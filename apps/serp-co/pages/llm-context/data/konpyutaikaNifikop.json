[
  {
    "owner": "konpyutaika",
    "repo": "nifikop",
    "content": "TITLE: Installing cert-manager with Helm 3 for NiFiKop\nDESCRIPTION: Installs cert-manager v1.7.2 using Helm 3, applying CustomResourceDefinitions first and then installing the Helm chart from the jetstack repository. Required for NiFiKop certificate management.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/2_deploy_nifikop/1_quick_start.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Install CustomResourceDefinitions first\nkubectl apply --validate=false -f \\\n   https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.crds.yaml\n\n# Add the jetstack helm repo\nhelm repo add jetstack https://charts.jetstack.io\nhelm repo update\n\n# You have to create the namespace before executing following command\nhelm install cert-manager \\\n    --namespace cert-manager \\\n    --version v1.7.2 jetstack/cert-manager\n```\n\n----------------------------------------\n\nTITLE: Defining NodeConfig for NiFiKop in YAML\nDESCRIPTION: This YAML snippet demonstrates how to define a NodeConfig for use with the NiFiKop operator, which manages Apache NiFi clusters in Kubernetes. It specifies storage properties (such as provenance repository size and external volumes), security settings (runAsUser), pod metadata (custom labels and annotations), image selection, affinity, and resource allocations. Required dependencies include a running Kubernetes cluster, the NiFiKop operator deployed, and appropriate storage classes and secrets as referenced. The main inputs are configuration keys such as provenanceStorage, runAsUser, podMetadata, and storageConfigs; outputs are the resulting pod and PVC resources in the cluster. Changing these values alters the pod and storage characteristics for NiFi nodes in the managed cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/5_references/1_nifi_cluster/3_node_config.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n   default_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      # Set this to true if the instance is a node in a cluster.\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#basic-cluster-setup\n      isNode: true\n      # Additionnal metadata to merge to the pod associated\n      podMetadata:\n        annotations:\n          node-annotation: \"node-annotation-value\"\n        labels:\n          node-label: \"node-label-value\"\n      # Docker image used by the operator to create the node associated\n      # https://hub.docker.com/r/apache/nifi/\n#      image: \"apache/nifi:1.11.2\"\n      # nodeAffinity can be specified, operator populates this value if new pvc added later to node\n      # https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity\n#      nodeAffinity:\n      # imagePullPolicy define the pull policy for NiFi cluster docker image\n      imagePullPolicy: IfNotPresent\n      # priorityClassName define the name of the priority class to be applied to these nodes\n      priorityClassName: \"example-priority-class-name\"\n      # externalVolumeConfigs specifies a list of volume to mount into the main container.\n      externalVolumeConfigs:\n        - name: example-volume\n          mountPath: \"/opt/nifi/example\"\n          secret:\n            secretName: \"raw-controller\"\n      # storageConfigs specifies the node related configs\n      storageConfigs:\n        # Name of the storage config, used to name PV to reuse into sidecars for example.\n        - name: provenance-repository\n          # Path where the volume will be mount into the main nifi container inside the pod.\n          mountPath: \"/opt/nifi/provenance_repository\"\n          # Metadata to attach to the PVC that gets created\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          # Kubernetes PVC spec\n          # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n\n```\n\n----------------------------------------\n\nTITLE: Defining Apache NiFi Cluster Configuration Using Kubernetes Custom Resource - YAML\nDESCRIPTION: This YAML snippet defines a custom resource of kind 'NifiCluster' designed to configure and deploy an Apache NiFi cluster using the nifikop Kubernetes operator. It includes metadata for identification, service configuration for cluster networking, and specifies cluster management via ZooKeeper with its address and path. External services, container images for NiFi and init containers, node configuration groups with storage and resource requests, and affinity policies are also declared. The specification sets up internal listeners for various NiFi services such as HTTP, cluster communication, site-to-site (S2S), Prometheus metrics, and load balancing. Key parameters include resource limits, storage volume definitions, pod annotations, and label propagation enabling. The snippet's inputs are the custom resource fields that map to the operator's deployment logic, resulting in an operational NiFi cluster setup. It requires the Kubernetes environment with the nifikop operator installed and ZooKeeper accessible at the specified address.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/5_references/1_nifi_cluster/1_nifi_cluster.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n    annotations:\n      tyty: ytyt\n    labels:\n      cluster-name: simplenifi\n      tete: titi\n  clusterManager: zookeeper\n  zkAddress: \"zookeeper.zookeeper:2181\"\n  zkPath: /simplenifi\n  externalServices:\n    - metadata:\n        annotations:\n          toto: tata\n        labels:\n          cluster-name: driver-simplenifi\n          titi: tutu\n      name: driver-ip\n      spec:\n        portConfigs:\n          - internalListenerName: http\n            port: 8080\n        type: ClusterIP\n  clusterImage: \"apache/nifi:1.28.0\"\n  initContainerImage: \"bash:5.2.2\"\n  oneNifiNodePerNode: true\n  readOnlyConfig:\n    nifiProperties:\n      overrideConfigs: |\n        nifi.sensitive.props.key=thisIsABadSensitiveKeyPassword\n  pod:\n    annotations:\n      toto: tata\n    labels:\n      cluster-name: simplenifi\n      titi: tutu\n  nodeConfigGroups:\n    default_group:\n      imagePullPolicy: IfNotPresent\n      isNode: true\n      serviceAccountName: default\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          reclaimPolicy: Delete\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      resourcesRequirements:\n        limits:\n          cpu: \"1\"\n          memory: 2Gi\n        requests:\n          cpu: \"1\"\n          memory: 2Gi\n  nodes:\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n    - id: 2\n      nodeConfigGroup: \"default_group\"\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - containerPort: 8080\n        type: http\n        name: http\n      - containerPort: 6007\n        type: cluster\n        name: cluster\n      - containerPort: 10000\n        type: s2s\n        name: s2s\n      - containerPort: 9090\n        type: prometheus\n        name: prometheus\n      - containerPort: 6342\n        type: load-balance\n        name: load-balance\n```\n\n----------------------------------------\n\nTITLE: Defining a NiFi Cluster with NifiCluster CRD in YAML\nDESCRIPTION: This YAML manifest defines a NiFi cluster resource named `simplenifi` using the `nifi.konpyutaika.com/v1` API version and `NifiCluster` kind. It configures various aspects like service properties (headless), pod metadata, ZooKeeper connection details (`zkAddress`, `zkPath`), NiFi image (`clusterImage`), node configurations (including storage, resources, service account), specific node definitions, listener configurations (http, cluster, s2s), and external services (ClusterIP). It showcases how to customize pod annotations/labels, storage volumes (PVC), resource limits/requests, and network exposure.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/5_references/1_nifi_cluster/1_nifi_cluster.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n    annotations:\n      tyty: ytyt\n    labels:\n      tete: titi  \n  pod:\n    annotations:\n      toto: tata\n    labels:\n      titi: tutu\n  zkAddress: 'zookeepercluster-client.zookeeper:2181'\n  zkPath: '/simplenifi'\n  clusterImage: 'apache/nifi:1.11.3'\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      podMetadata:\n        annotations:\n          node-annotation: \"node-annotation-value\"\n        labels:\n          node-label: \"node-label-value\"\n      externalVolumeConfigs:\n        - name: example-volume\n          mountPath: \"/opt/nifi/example\"\n          secret:\n            secretName: \"raw-controller\"\n      storageConfigs:\n        - mountPath: '/opt/nifi/nifi-current/logs'\n          name: logs\n          # Metadata to attach to the PVC that gets created\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: 'standard'\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: 'default'\n      resourcesRequirements:\n        limits:\n          cpu: '2'\n          memory: 3Gi\n        requests:\n          cpu: '1'\n          memory: 1Gi\n  nodes:\n    - id: 1\n      nodeConfigGroup: 'default_group'\n    - id: 2\n      nodeConfigGroup: 'default_group'\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: 'http'\n        name: 'http'\n        containerPort: 8080\n      - type: 'cluster'\n        name: 'cluster'\n        containerPort: 6007\n      - type: 's2s'\n        name: 's2s'\n        containerPort: 10000\n  externalServices:\n    - name: 'clusterip'\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8080\n            internalListenerName: 'http'\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Cluster Stateful Configuration using Kubernetes Custom Resource YAML\nDESCRIPTION: This YAML snippet describes the complete configuration of a NiFi cluster resource named 'simplenifi' to be managed via the NiFi operator. It defines metadata, service labels and annotations, the cluster manager type and ZooKeeper connection details, external services configuration, images for cluster and init containers, pod annotations and labels, node groups with resource requests and storage configuration, individual nodes, label propagation settings, retry durations for cluster tasks, and internal listener ports with their types. Dependencies include a Kubernetes cluster supporting custom resource definitions and the NiFi operator that interprets this spec. The input is a declarative YAML structure specifying cluster state, and the expected output is a corresponding NiFi cluster deployed as per this configuration. Constraints include compatibility with the NiFi operator version and Kubernetes environment specifics, and care should be taken to secure sensitive properties declared herein.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/5_references/1_nifi_cluster/1_nifi_cluster.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n    annotations:\n      tyty: ytyt\n    labels:\n      cluster-name: simplenifi\n      tete: titi\n  clusterManager: zookeeper\n  zkAddress: \"zookeeper.zookeeper:2181\"\n  zkPath: /simplenifi\n  externalServices:\n    - metadata:\n        annotations:\n          toto: tata\n        labels:\n          cluster-name: driver-simplenifi\n          titi: tutu\n      name: driver-ip\n      spec:\n        portConfigs:\n          - internalListenerName: http\n            port: 8080\n        type: ClusterIP\n  clusterImage: \"apache/nifi:1.28.0\"\n  initContainerImage: \"bash:5.2.2\"\n  oneNifiNodePerNode: true\n  readOnlyConfig:\n    nifiProperties:\n      overrideConfigs: |\n        nifi.sensitive.props.key=thisIsABadSensitiveKeyPassword\n  pod:\n    annotations:\n      toto: tata\n    labels:\n      cluster-name: simplenifi\n      titi: tutu\n  nodeConfigGroups:\n    default_group:\n      imagePullPolicy: IfNotPresent\n      isNode: true\n      serviceAccountName: default\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          reclaimPolicy: Delete\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      resourcesRequirements:\n        limits:\n          cpu: \"1\"\n          memory: 2Gi\n        requests:\n          cpu: \"1\"\n          memory: 2Gi\n  nodes:\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n    - id: 2\n      nodeConfigGroup: \"default_group\"\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - containerPort: 8080\n        type: http\n        name: http\n      - containerPort: 6007\n        type: cluster\n        name: cluster\n      - containerPort: 10000\n        type: s2s\n        name: s2s\n      - containerPort: 9090\n        type: prometheus\n        name: prometheus\n      - containerPort: 6342\n        type: load-balance\n        name: load-balance\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Cluster with Kubernetes Custom Resource (YAML)\nDESCRIPTION: This YAML snippet defines a Kubernetes Custom Resource named NifiCluster that specifies the desired state and configuration of an Apache NiFi cluster. The snippet includes details such as enabling headless services, setting cluster manager to Zookeeper with address and path, specifying external cluster IP services, container images, pod annotations and labels, node configuration groups with storage and resource configs, nodes list, label propagation, cluster task retry parameters, and internal listener configurations for HTTP, cluster communication, site-to-site, Prometheus metrics, and load balancing. Dependencies include a Kubernetes cluster with the NiFi operator installed to interpret and act upon this custom resource. Inputs include metadata for resource identification and the spec section describing the cluster desired state. The output is the creation and management of a NiFi cluster based on this configuration. Constraints involve correct definition according to the CRD schema and valid Kubernetes resource configurations.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/5_references/1_nifi_cluster/1_nifi_cluster.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n    annotations:\n      tyty: ytyt\n    labels:\n      cluster-name: simplenifi\n      tete: titi\n  clusterManager: zookeeper\n  zkAddress: \"zookeeper.zookeeper:2181\"\n  zkPath: /simplenifi\n  externalServices:\n    - metadata:\n        annotations:\n          toto: tata\n        labels:\n          cluster-name: driver-simplenifi\n          titi: tutu\n      name: driver-ip\n      spec:\n        portConfigs:\n          - internalListenerName: http\n            port: 8080\n        type: ClusterIP\n  clusterImage: \"apache/nifi:1.28.0\"\n  initContainerImage: \"bash:5.2.2\"\n  oneNifiNodePerNode: true\n  readOnlyConfig:\n    nifiProperties:\n      overrideConfigs: |\n        nifi.sensitive.props.key=thisIsABadSensitiveKeyPassword\n  pod:\n    annotations:\n      toto: tata\n    labels:\n      cluster-name: simplenifi\n      titi: tutu\n  nodeConfigGroups:\n    default_group:\n      imagePullPolicy: IfNotPresent\n      isNode: true\n      serviceAccountName: default\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          reclaimPolicy: Delete\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      resourcesRequirements:\n        limits:\n          cpu: \"1\"\n          memory: 2Gi\n        requests:\n          cpu: \"1\"\n          memory: 2Gi\n  nodes:\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n    - id: 2\n      nodeConfigGroup: \"default_group\"\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - containerPort: 8080\n        type: http\n        name: http\n      - containerPort: 6007\n        type: cluster\n        name: cluster\n      - containerPort: 10000\n        type: s2s\n        name: s2s\n      - containerPort: 9090\n        type: prometheus\n        name: prometheus\n      - containerPort: 6342\n        type: load-balance\n        name: load-balance\n```\n\n----------------------------------------\n\nTITLE: Testing Kubectl NiFiKop Plugin Installation (Console)\nDESCRIPTION: Executing the `kubectl nifikop` command after installation verifies that the plugin is correctly installed and accessible in the system's PATH. A successful execution displays the usage information for the plugin, listing the available subcommands for managing NiFiKop custom resources like `nificluster`, `nificonnection`, etc. This indicates the plugin is ready for use.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/2_deploy_nifikop/3_kubectl_plugin.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\n$ kubectl nifikop\nUsage:\n  nifikop [command]\n\nAvailable Commands:\n  completion          Generate the autocompletion script for the specified shell\n  help                Help about any command\n  nificluster         \n  nificonnection      \n  nifidataflow        \n  nifigroupautoscaler \n  nifiregistryclient  \n  nifiuser            \n  nifiusergroup \n\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Kubernetes StorageClass (YAML)\nDESCRIPTION: Creates a Kubernetes StorageClass named 'exampleStorageclass' using the 'gce-pd' provisioner with 'pd-standard' type. It sets the 'volumeBindingMode' to 'WaitForFirstConsumer' and 'reclaimPolicy' to 'Delete', recommended for NiFi deployments. This ensures persistent volumes are bound only when a pod needing them is scheduled.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: exampleStorageclass\nparameters:\n  type: pd-standard\nprovisioner: kubernetes.io/gce-pd\nreclaimPolicy: Delete\nvolumeBindingMode: WaitForFirstConsumer\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiUser Resource with Access Policies and Identity Mapping - YAML\nDESCRIPTION: This YAML code snippet provisions a NifiUser resource in Kubernetes, enabling managed user creation and policy-based access within an Apache NiFi cluster using the nifikop operator. Requires the nifikop operator and a previously defined NifiCluster resource. The 'identity' field allows overriding the default resource name for mapping to a NiFi user, while the 'accessPolicies' list grants component-level access (in this example, read access on all process groups). 'includeJKS' and 'createCert' flags govern certificate and keystore generation. Inputs include the resource metadata and spec parameters; outputs are the managed NiFi user and associated policies. Limitations include name constraints imposed by Kubernetes and optionality of certificate creation based on authentication method.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/3_manage_nifi/2_manage_users_and_accesses/1_users_management.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiUser\nmetadata:\n  name: aguitton\nspec:\n  # identity field is use to define the user identity on NiFi cluster side,\n  #\tit use full when the user's name doesn't suite with Kubernetes resource name.\n  identity: alexandre.guitton@konpyutaika.com\n  # Contains the reference to the NifiCluster with the one the registry client is linked.\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  # Whether or not the the operator also include a Java keystore format (JKS) with you secret\n  includeJKS: false\n  # Whether or not a certificate will be created for this user.\n  createCert: false\n  # defines the list of access policies that will be granted to the group.\n  accessPolicies:\n      # defines the kind of access policy, could be \"global\" or \"component\".\n    - type: component\n      # defines the kind of action that will be granted, could be \"read\" or \"write\"\n      action: read\n      # resource defines the kind of resource targeted by this access policies, please refer to the following page:\n      #\thttps://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#access-policies\n      resource: /data\n      # componentType is used if the type is \"component\", it's allow to define the kind of component on which is the\n      # access policy\n      componentType: \"process-groups\"\n      # componentId is used if the type is \"component\", it's allow to define the id of the component on which is the\n      # access policy\n      componentId: \"\"\n```\n\n----------------------------------------\n\nTITLE: Applying NiFiKop Cluster Configuration - Shell Command\nDESCRIPTION: This shell command applies the updated NiFiKop cluster YAML configuration to the Kubernetes cluster using kubectl. Required dependencies: kubectl installed and configured, access to the target Kubernetes cluster, and the NifiKop operator running. The main parameter is the YAML manifest file ('config/samples/simplenificluster.yaml') and the namespace (-n nifi). The input is the YAML file containing the cluster spec, and the output is an updated NiFi cluster resource which triggers Kubernetes changes for scaling.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\nkubectl -n nifi apply -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring SSL for NiFi Operator Cluster - YAML\nDESCRIPTION: This YAML manifest configures a NifiCluster custom resource in Kubernetes to enable SSL encryption using the konpyutaika NiFi Operator. It includes settings for read-only NiFi properties, HTTPS internal listeners, and the automatic management of TLS secrets. Key elements include 'webProxyHosts' for defining allowed hosts, as well as 'sslSecrets' for the TLS secret configuration, where setting 'create' to true allows the operator to generate certificates automatically or, if set to false, expects a pre-created secret. Dependencies include a running Kubernetes cluster, the NiFi Operator (nifikop) installed, and the CRDs for NiFi resources. The configuration expects relevant Kubernetes secrets for custom CA certificates when 'create' is false; otherwise, it will generate required certificates. Limitations may include proper alignment of DNS naming and secret key structures as required by both NiFi and Kubernetes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  readOnlyConfig:\n    # NifiProperties configuration that will be applied to the node.\n    nifiProperties:\n      webProxyHosts:\n        - nifistandard2.trycatchlearn.fr:8443\n        ...\n  ...\n  listenersConfig:\n    internalListeners:\n      - type: \"https\"\n        name: \"https\"\n        containerPort: 8443\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n```\n\n----------------------------------------\n\nTITLE: Declaring NifiCluster CRD for Kubernetes in YAML\nDESCRIPTION: This YAML snippet defines a Kubernetes custom resource of kind 'NifiCluster' used by the nifikop operator to deploy and manage an Apache NiFi cluster. It specifies cluster metadata, service configurations, integration details (such as ZooKeeper), external services, image versions, pod and node group settings, storage configurations, container resource limits, cluster topology, label propagation, and internal network listeners. Required dependencies include a running Kubernetes cluster, the nifikop CRD installed, and access to configured container images. Expected input is a valid YAML manifest; output is the instantiation and management of a NiFi cluster as specified. Limitations: must match the currently supported schema of the nifikop operator and the Kubernetes cluster's CRD version.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/5_references/1_nifi_cluster/1_nifi_cluster.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n    annotations:\n      tyty: ytyt\n    labels:\n      cluster-name: simplenifi\n      tete: titi\n  zkAddress: \"zookeeper.zookeeper:2181\"\n  zkPath: /simplenifi\n  externalServices:\n    - metadata:\n        annotations:\n          toto: tata\n        labels:\n          cluster-name: driver-simplenifi\n          titi: tutu\n      name: driver-ip\n      spec:\n        portConfigs:\n          - internalListenerName: http\n            port: 8080\n        type: ClusterIP\n  clusterImage: \"apache/nifi:1.26.0\"\n  initContainerImage: \"bash:5.2.2\"\n  oneNifiNodePerNode: true\n  readOnlyConfig:\n    nifiProperties:\n      overrideConfigs: |\n        nifi.sensitive.props.key=thisIsABadSensitiveKeyPassword\n  pod:\n    annotations:\n      toto: tata\n    labels:\n      cluster-name: simplenifi\n      titi: tutu\n  nodeConfigGroups:\n    default_group:\n      imagePullPolicy: IfNotPresent\n      isNode: true\n      serviceAccountName: default\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          reclaimPolicy: Delete\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      resourcesRequirements:\n        limits:\n          cpu: \"1\"\n          memory: 2Gi\n        requests:\n          cpu: \"1\"\n          memory: 2Gi\n  nodes:\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n    - id: 2\n      nodeConfigGroup: \"default_group\"\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - containerPort: 8080\n        type: http\n        name: http\n      - containerPort: 6007\n        type: cluster\n        name: cluster\n      - containerPort: 10000\n        type: s2s\n        name: s2s\n      - containerPort: 9090\n        type: prometheus\n        name: prometheus\n      - containerPort: 6342\n        type: load-balance\n        name: load-balance\n\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiUser resource for NiFi user management in YAML\nDESCRIPTION: This YAML snippet defines a NifiUser custom resource used to manage a NiFi user on a Kubernetes-managed NiFi cluster. It shows how to specify a user's identity different from the Kubernetes resource name, associate the user with a specific NiFi cluster, control certificate creation, and assign detailed access policies specifying actions and resource scopes. Required fields include metadata name, user identity, cluster reference, certificate inclusion flags, and accessPolicies list detailing the type, action, resource, and component details of permissions. This enables the NiFi operator to synchronize the Kubernetes resource with the actual NiFi user and apply the defined policies programmatically.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/2_manage_users_and_accesses/1_users_management.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: aguitton\nspec:\n  # identity field is use to define the user identity on NiFi cluster side,\n  #\tit use full when the user's name doesn't suite with Kubernetes resource name.\n  identity: alexandre.guitton@konpyutaika.com\n  # Contains the reference to the NifiCluster with the one the registry client is linked.\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  # Whether or not the the operator also include a Java keystore format (JKS) with you secret\n  includeJKS: false\n  # Whether or not a certificate will be created for this user.\n  createCert: false\n  # defines the list of access policies that will be granted to the group.\n  accessPolicies:\n      # defines the kind of access policy, could be \"global\" or \"component\".\n    - type: component\n      # defines the kind of action that will be granted, could be \"read\" or \"write\"\n      action: read\n      # resource defines the kind of resource targeted by this access policies, please refer to the following page:\n      #\thttps://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#access-policies\n      resource: /data\n      # componentType is used if the type is \"component\", it's allow to define the kind of component on which is the\n      # access policy\n      componentType: \"process-groups\"\n      # componentId is used if the type is \"component\", it's allow to define the id of the component on which is the\n      # access policy\n      componentId: \"\"\n```\n\n----------------------------------------\n\nTITLE: Defining NifiCluster YAML Configuration\nDESCRIPTION: This YAML snippet defines a NifiCluster resource, specifying the desired state of a NiFi cluster. It includes configurations for the service, ZooKeeper connection, external services, cluster image, node configurations, and listeners. Key parameters include `zkAddress` for the ZooKeeper connection string, `clusterImage` for the NiFi image version, and `nodeConfigGroups` for defining node-specific configurations.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/5_references/1_nifi_cluster/1_nifi_cluster.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n    annotations:\n      tyty: ytyt\n    labels:\n      cluster-name: simplenifi\n      tete: titi\n  zkAddress: \"zookeeper.zookeeper:2181\"\n  zkPath: /simplenifi\n  externalServices:\n    - metadata:\n        annotations:\n          toto: tata\n        labels:\n          cluster-name: driver-simplenifi\n          titi: tutu\n      name: driver-ip\n      spec:\n        portConfigs:\n          - internalListenerName: http\n            port: 8080\n        type: ClusterIP\n  clusterImage: \"apache/nifi:1.24.0\"\n  initContainerImage: \"bash:5.2.2\"\n  oneNifiNodePerNode: true\n  readOnlyConfig:\n    nifiProperties:\n      overrideConfigs: |\n        nifi.sensitive.props.key=thisIsABadSensitiveKeyPassword\n  pod:\n    annotations:\n      toto: tata\n    labels:\n      cluster-name: simplenifi\n      titi: tutu\n  nodeConfigGroups:\n    default_group:\n      imagePullPolicy: IfNotPresent\n      isNode: true\n      serviceAccountName: default\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          reclaimPolicy: Delete\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      resourcesRequirements:\n        limits:\n          cpu: \"1\"\n          memory: 2Gi\n        requests:\n          cpu: \"1\"\n          memory: 2Gi\n  nodes:\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n    - id: 2\n      nodeConfigGroup: \"default_group\"\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - containerPort: 8080\n        type: http\n        name: http\n      - containerPort: 6007\n        type: cluster\n        name: cluster\n      - containerPort: 10000\n        type: s2s\n        name: s2s\n      - containerPort: 9090\n        type: prometheus\n        name: prometheus\n      - containerPort: 6342\n        type: load-balance\n        name: load-balance\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes ServiceMonitor Resource for NiFi Metrics Endpoint Using YAML\nDESCRIPTION: Defines a ServiceMonitor resource to instruct Prometheus on how to discover and scrape NiFi cluster metrics. It selects services in the 'clusters' namespace matching labels 'app: nifi' and 'nifi_cr: cluster', and configures scraping on the '/metrics' path via the 'prometheus' port every 10 seconds. It also includes relabeling rules to map Kubernetes metadata such as pod IP, nodeId, and nifi_cr labels to Prometheus labels for enhanced querying. Requires Prometheus Operator support for ServiceMonitors and a running NiFi cluster exposing metrics.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: cluster\n  namespace: monitoring-system\n  labels:\n    app: nifi-cluster\n    nifi_cr: cluster\nspec:\n  selector:\n    matchLabels:\n      app: nifi\n      nifi_cr: cluster\n  namespaceSelector:\n    matchNames:\n      - clusters\n  endpoints:\n    - interval: 10s\n      port: prometheus\n      path: /metrics\n      honorLabels: true\n      relabelings:\n        - sourceLabels: [__meta_kubernetes_pod_ip]\n          separator: ;\n          regex: (.*)\n          targetLabel: pod_ip\n          replacement: $1\n          action: replace\n        - sourceLabels: [__meta_kubernetes_pod_label_nodeId]\n          separator: ;\n          regex: (.*)\n          targetLabel: nodeId\n          replacement: $1\n          action: replace\n        - sourceLabels: [__meta_kubernetes_pod_label_nifi_cr]\n          separator: ;\n          regex: (.*)\n          targetLabel: nifi_cr\n          replacement: $1\n          action: replace\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Node Group Autoscaler Resource - YAML\nDESCRIPTION: This YAML snippet defines a NifiNodeGroupAutoscaler custom resource for Kubernetes to manage the autoscaling of a NiFi cluster node group. It references a NiFi cluster and identifies the node group to be autoscaled by 'auto_scaling'. The spec allows overriding NiFi properties that trigger rolling upgrades, selects managed nodes based on labels, and specifies upscale and downscale strategies ('simple' and 'lifo' respectively). This resource requires a compatible NiFi operator installed in the cluster for full functionality.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiNodeGroupAutoscaler\nmetadata:\n  name: nifinodegroupautoscaler-sample\n  namespace: clusters\nspec:\n  # contains the reference to the NifiCluster with the one the node group autoscaler is linked.\n  clusterRef:\n    name: cluster\n    namespace: clusters\n  # defines the id of the NodeConfig contained in NifiCluster.Spec.NodeConfigGroups\n  nodeConfigGroupId: auto_scaling\n  # readOnlyConfig can be used to pass Nifi node config\n  # which has type read-only these config changes will trigger rolling upgrade\n  readOnlyConfig:\n    nifiProperties:\n      overrideConfigs: |\n        nifi.ui.banner.text=NiFiKop - Scale Group\n  # This is an example of a node config you can apply to each replica in this node group.\n  # Any settings here will override those in the configured nodeConfigGroupId\n#  nodeConfig:\n#    nodeSelector:\n#      node_type: high-mem\n  # The selector used to identify nodes in NifiCluster.Spec.Nodes this autoscaler will manage\n  # Use Node.Labels in combination with this selector to clearly define which nodes will be managed by this autoscaler \n  nodeLabelsSelector: \n    matchLabels:\n      nifi_cr: cluster\n      nifi_node_group: auto-scaling\n  # the strategy used to decide how to add nodes to a nifi cluster\n  upscaleStrategy: simple\n  # the strategy used to decide how to remove nodes from an existing cluster\n  downscaleStrategy: lifo\n```\n\n----------------------------------------\n\nTITLE: Defining a NiFi Cluster Custom Resource in YAML for Kubernetes Deployment\nDESCRIPTION: This YAML snippet specifies a NiFi cluster resource (`NifiCluster`) configuration for Kubernetes, detailing desired state including node groups, storage, network, and service configurations. It sets parameters such as Zookeeper connection, image version, node labels, resource limits, and external service ports, enabling declarative cluster setup and management.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/5_references/1_nifi_cluster/1_nifi_cluster.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n```yaml\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n    annotations:\n      tyty: ytyt\n    labels:\n      tete: titi  \n  pod:\n    annotations:\n      toto: tata\n    labels:\n      titi: tutu\n  zkAddress: 'zookeepercluster-client.zookeeper:2181'\n  zkPath: '/simplenifi'\n  clusterImage: 'apache/nifi:1.11.3'\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      podMetadata:\n        annotations:\n          node-annotation: \"node-annotation-value\"\n        labels:\n          node-label: \"node-label-value\"\n      externalVolumeConfigs:\n        - name: example-volume\n          mountPath: \"/opt/nifi/example\"\n          secret:\n            secretName: \"raw-controller\"\n      storageConfigs:\n        - mountPath: '/opt/nifi/nifi-current/logs'\n          name: logs\n          # Metadata to attach to the PVC that gets created\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: 'standard'\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: 'default'\n      resourcesRequirements:\n        limits:\n          cpu: '2'\n          memory: 3Gi\n        requests:\n          cpu: '1'\n          memory: 1Gi\n  nodes:\n    - id: 1\n      nodeConfigGroup: 'default_group'\n    - id: 2\n      nodeConfigGroup: 'default_group'\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: 'http'\n        name: 'http'\n        containerPort: 8080\n      - type: 'cluster'\n        name: 'cluster'\n        containerPort: 6007\n      - type: 's2s'\n        name: 's2s'\n        containerPort: 10000\n  externalServices:\n    - name: 'clusterip'\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8080\n            internalListenerName: 'http'\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n```\n\n----------------------------------------\n\nTITLE: Retrieving NiFi Namespace UID/GID for OpenShift Security Context - Bash\nDESCRIPTION: Extracts the supplementary groups UID/GID for the 'nifi' namespace from OpenShift annotations, processing with sed and tr, to adjust Pod security context values for NiFi deployments on OpenShift.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nuid=$(kubectl get namespace nifi -o=jsonpath='{.metadata.annotations.openshift\\.io/sa\\.scc\\.supplemental-groups}' | sed 's/\\/10000$//' | tr -d '[:space:]')\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi User Groups with Access Policies using NiFiKop Operator (YAML)\nDESCRIPTION: This YAML manifest defines a Kubernetes CustomResource for managing an Apache NiFi user group using the NifiKop operator. It specifies the NifiUserGroup resource, including references to the targeted NiFi cluster, a list of users (usersRef) to include in the group, and a set of access policies granting permissions (such as read access to counters). Before applying, ensure referenced NifiUser resources are created and the NifiKop operator is installed in your cluster. The resource expects user and cluster references (with required name and optional namespace fields), as well as specific access policy parameters: type (global or component), action (read or write), and resource path. ComponentId and componentType are optional for component type policies. Outputs are managed user groups and policies within the NiFi instance tied to the referenced cluster. Limitations: Users must be present as NifiUser resources; incomplete or missing references will result in an invalid configuration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/2_manage_users_and_accesses/2_groups_management.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  # Contains the reference to the NifiCluster with the one the registry client is linked.\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  # contains the list of reference to NifiUsers that are part to the group.\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n#     namespace: nifikop\n    - name: nc-controller.nifikop.mgt.cluster.local\n  # defines the list of access policies that will be granted to the group.\n  accessPolicies:\n      # defines the kind of access policy, could be \"global\" or \"component\".\n    - type: global\n      # defines the kind of action that will be granted, could be \"read\" or \"write\"\n      action: read\n      # resource defines the kind of resource targeted by this access policies, please refer to the following page:\n      #\thttps://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#access-policies\n      resource: /counters\n#      # componentType is used if the type is \"component\", it's allow to define the kind of component on which is the\n#      # access policy\n#      componentType: \"process-groups\"\n#      # componentId is used if the type is \"component\", it's allow to define the id of the component on which is the\n#      # access policy\n#      componentId: \"\"\n\n```\n\n----------------------------------------\n\nTITLE: Defining Node Configuration - YAML\nDESCRIPTION: This YAML code snippet defines the configuration for a NiFi node. It includes various settings like `provenanceStorage`, `runAsUser`, `isNode`, `podMetadata`, `imagePullPolicy`, `priorityClassName`, `externalVolumeConfigs`, and `storageConfigs`. These settings manage storage, user identity, cluster membership, pod metadata, image pulling policy, and volume configurations. The purpose is to provide a structured way to define the characteristics of a NiFi node in a Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/5_references/1_nifi_cluster/3_node_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n   default_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      # Set this to true if the instance is a node in a cluster.\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#basic-cluster-setup\n      isNode: true\n      # Additionnal metadata to merge to the pod associated\n      podMetadata:\n        annotations:\n          node-annotation: \"node-annotation-value\"\n        labels:\n          node-label: \"node-label-value\"\n      # Docker image used by the operator to create the node associated\n      # https://hub.docker.com/r/apache/nifi/\n#      image: \"apache/nifi:1.11.2\"\n      # nodeAffinity can be specified, operator populates this value if new pvc added later to node\n      # https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity\n#      nodeAffinity:\n      # imagePullPolicy define the pull policy for NiFi cluster docker image\n      imagePullPolicy: IfNotPresent\n      # priorityClassName define the name of the priority class to be applied to these nodes\n      priorityClassName: \"example-priority-class-name\"\n      # externalVolumeConfigs specifies a list of volume to mount into the main container.\n      externalVolumeConfigs:\n        - name: example-volume\n          mountPath: \"/opt/nifi/example\"\n          secret:\n            secretName: \"raw-controller\"\n      # storageConfigs specifies the node related configs\n      storageConfigs:\n        # Name of the storage config, used to name PV to reuse into sidecars for example.\n        - name: provenance-repository\n          # Retain this PVC throughout NifiCluster deletions.\n          reclaimPolicy: Retain\n          # Path where the volume will be mount into the main nifi container inside the pod.\n          mountPath: \"/opt/nifi/provenance_repository\"\n          # Metadata to attach to the PVC that gets created\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          # Kubernetes PVC spec\n          # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          reclaimPolicy: Delete\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Dataflow Resource in YAML\nDESCRIPTION: This YAML snippet defines a NifiDataflow custom resource, specifying its metadata and desired state. It includes configurations such as the parent process group ID, bucket ID, flow ID, and references to external resources like NifiCluster, NifiRegistryClient, and NifiParameterContext.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/5_references/5_nifi_dataflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  flowPosition:\n    posX: 0\n    posY: 0\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: squidflow\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: nifikop\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Define Basic NiFi Parameter Context YAML\nDESCRIPTION: This snippet demonstrates how to define a basic `NifiParameterContext` resource using YAML for deployment via the Nifikop operator. It shows how to specify metadata, a description, reference the target NiFi cluster, link to Kubernetes secrets for sensitive parameters, and define non-sensitive parameters directly.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/5_references/4_nifi_parameter_context.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      description: toto\n      sensistive: true\n\n```\n\n----------------------------------------\n\nTITLE: Installing cert-manager Directly Using kubectl - Kubernetes - Bash\nDESCRIPTION: Installs cert-manager and its CustomResourceDefinitions by applying the provided YAML manifest directly via kubectl. The code depends on kubectl being installed and configured with the correct Kubernetes context. No direct input parameters are needed; it downloads and applies version v1.7.2 of cert-manager. The output is an updated cluster with cert-manager deployed to manage TLS certificates.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/2_deploy_nifikop/1_quick_start.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install the CustomResourceDefinitions and cert-manager itself\nkubectl apply -f \\\n    https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiDataflow Resource (YAML)\nDESCRIPTION: Example YAML manifest for creating a `NifiDataflow` custom resource. This resource defines the deployment of a specific versioned flow from a NiFi Registry (`bucketId`, `flowId`, `flowVersion`) into a target NiFi cluster (`clusterRef`) and process group (`parentProcessGroupID`). It references the required `NifiRegistryClient` (`registryClientRef`) and optional `NifiParameterContext` (`parameterContextRef`), and defines the synchronization (`syncMode`) and update strategy (`updateStrategy`).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: demo\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Defining a NiFi User Resource in Kubernetes with Access Policies - YAML\nDESCRIPTION: This YAML snippet defines a NifiUser custom resource for managing a user and their access policies on an Apache NiFi cluster using Kubernetes. The resource includes the user's cluster reference, identity, options for including JKS and certificate creation, and a detailed specification of access policies (type, action, resource, and component filters). Dependencies include the nifikop operator and an existing NiFi cluster referenced by name and namespace. Required parameters are metadata.name (resource name), spec.identity (NiFi-side user identifier), spec.clusterRef (target NiFi cluster), and spec.accessPolicies (permissions): these control user mapping, certificate generation, and granted privileges. Outputs are a managed user reflected both on the NiFi cluster and within Kubernetes. Limitations: identity overrides are required if RFC 1123 constraints don't match NiFi requirements, and certificate creation can be toggled off for non-TLS users.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/2_manage_users_and_accesses/1_users_management.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: aguitton\nspec:\n  # identity field is use to define the user identity on NiFi cluster side,\n  #\tit use full when the user's name doesn't suite with Kubernetes resource name.\n  identity: alexandre.guitton@konpyutaika.com\n  # Contains the reference to the NifiCluster with the one the registry client is linked.\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  # Whether or not the the operator also include a Java keystore format (JKS) with you secret\n  includeJKS: false\n  # Whether or not a certificate will be created for this user.\n  createCert: false\n  # defines the list of access policies that will be granted to the group.\n  accessPolicies:\n      # defines the kind of access policy, could be \"global\" or \"component\".\n    - type: component\n      # defines the kind of action that will be granted, could be \"read\" or \"write\"\n      action: read\n      # resource defines the kind of resource targeted by this access policies, please refer to the following page:\n      #\thttps://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#access-policies\n      resource: /data\n      # componentType is used if the type is \"component\", it's allow to define the kind of component on which is the\n      # access policy\n      componentType: \"process-groups\"\n      # componentId is used if the type is \"component\", it's allow to define the id of the component on which is the\n      # access policy\n      componentId: \"\"\n```\n\n----------------------------------------\n\nTITLE: Defining NifiCluster Resource - YAML\nDESCRIPTION: This YAML snippet defines a `NifiCluster` custom resource. It specifies the API version, kind, and metadata, and includes a detailed `spec` section.  The `spec` section describes configurations for the service, cluster manager (Zookeeper in this case), image, node groups, node definitions, and listener configurations. It is a declarative definition of a NiFi cluster's desired state within a Kubernetes cluster. Key parameters include `zkAddress`, `clusterImage`, `nodeConfigGroups`, and `listenersConfig`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/5_references/1_nifi_cluster/1_nifi_cluster.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n    annotations:\n      tyty: ytyt\n    labels:\n      cluster-name: simplenifi\n      tete: titi\n  clusterManager: zookeeper\n  zkAddress: \"zookeeper.zookeeper:2181\"\n  zkPath: /simplenifi\n  externalServices:\n    - metadata:\n        annotations:\n          toto: tata\n        labels:\n          cluster-name: driver-simplenifi\n          titi: tutu\n      name: driver-ip\n      spec:\n        portConfigs:\n          - internalListenerName: http\n            port: 8080\n        type: ClusterIP\n  clusterImage: \"apache/nifi:1.28.0\"\n  initContainerImage: \"bash:5.2.2\"\n  oneNifiNodePerNode: true\n  readOnlyConfig:\n    nifiProperties:\n      overrideConfigs: |\n        nifi.sensitive.props.key=thisIsABadSensitiveKeyPassword\n  pod:\n    annotations:\n      toto: tata\n    labels:\n      cluster-name: simplenifi\n      titi: tutu\n  nodeConfigGroups:\n    default_group:\n      imagePullPolicy: IfNotPresent\n      isNode: true\n      serviceAccountName: default\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          reclaimPolicy: Delete\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      resourcesRequirements:\n        limits:\n          cpu: \"1\"\n          memory: 2Gi\n        requests:\n          cpu: \"1\"\n          memory: 2Gi\n  nodes:\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n    - id: 2\n      nodeConfigGroup: \"default_group\"\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - containerPort: 8080\n        type: http\n        name: http\n      - containerPort: 6007\n        type: cluster\n        name: cluster\n      - containerPort: 10000\n        type: s2s\n        name: s2s\n      - containerPort: 9090\n        type: prometheus\n        name: prometheus\n      - containerPort: 6342\n        type: load-balance\n        name: load-balance\n\n```\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubernetes RBAC Role and RoleBinding for NiFi Native State Management - YAML\nDESCRIPTION: Defines a Kubernetes Role granting full permissions on 'leases' and 'configmaps' resources needed by NiFi to manage its state natively on Kubernetes. It also creates a RoleBinding associating the Role to the 'default' ServiceAccount within the 'nifi' namespace. This setup ensures NiFi has the necessary cluster rights without requiring Zookeeper.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: simplenifi\n  namespace: nifi\nrules:\n- apiGroups: [\"coordination.k8s.io\"]\n  resources: [\"leases\"]\n  verbs: [\"*\"]\n- apiGroups: [\"\"]\n  resources: [\"configmaps\"]\n  verbs: [\"*\"]\n---\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: simplenifi\n  namespace: nifi\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: nifi\nroleRef:\n  kind: Role\n  name: simplenifi\n  apiGroup: rbac.authorization.k8s.io\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFiCluster SSL with Internal Generation (YAML)\nDESCRIPTION: Defines a `NifiCluster` custom resource configuration to enable SSL. It specifies `readOnlyConfig.nifiProperties.webProxyHosts`, configures internal listeners for `https`, `cluster`, and `s2s` protocols on specific ports within the `listenersConfig` section, and instructs the operator to automatically create the required TLS secrets by setting `listenersConfig.sslSecrets.create` to `true`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  readOnlyConfig:\n    # NifiProperties configuration that will be applied to the node.\n    nifiProperties:\n      webProxyHosts:\n        - nifistandard2.trycatchlearn.fr:8443\n        ...\n  ...\n  listenersConfig:\n    internalListeners:\n      - type: \"https\"\n        name: \"https\"\n        containerPort: 8443\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n```\n\n----------------------------------------\n\nTITLE: Defining a NiFi Cluster in Kubernetes with YAML\nDESCRIPTION: A complete YAML example of a NifiCluster custom resource definition that specifies a two-node NiFi cluster with storage configurations, resource requirements, and service definitions. This configuration includes headless service setup, node configuration groups, and multiple internal listeners.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/5_references/1_nifi_cluster/1_nifi_cluster.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n    annotations:\n      tyty: ytyt\n    labels:\n      tete: titi  \n  pod:\n    annotations:\n      toto: tata\n    labels:\n      titi: tutu\n  zkAddress: 'zookeepercluster-client.zookeeper:2181'\n  zkPath: '/simplenifi'\n  clusterImage: 'apache/nifi:1.11.3'\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      podMetadata:\n        annotations:\n          node-annotation: \"node-annotation-value\"\n        labels:\n          node-label: \"node-label-value\"\n      externalVolumeConfigs:\n        - name: example-volume\n          mountPath: \"/opt/nifi/example\"\n          secret:\n            secretName: \"raw-controller\"\n      storageConfigs:\n        - mountPath: '/opt/nifi/nifi-current/logs'\n          name: logs\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: 'standard'\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: 'default'\n      resourcesRequirements:\n        limits:\n          cpu: '2'\n          memory: 3Gi\n        requests:\n          cpu: '1'\n          memory: 1Gi\n  nodes:\n    - id: 1\n      nodeConfigGroup: 'default_group'\n    - id: 2\n      nodeConfigGroup: 'default_group'\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: 'http'\n        name: 'http'\n        containerPort: 8080\n      - type: 'cluster'\n        name: 'cluster'\n        containerPort: 6007\n      - type: 's2s'\n        name: 's2s'\n        containerPort: 10000\n  externalServices:\n    - name: 'clusterip'\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8080\n            internalListenerName: 'http'\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n\n----------------------------------------\n\nTITLE: Defining NiFiCluster Resource - YAML\nDESCRIPTION: This YAML snippet defines an instance of the `NifiCluster` custom resource. It specifies the desired configuration for a NiFi cluster managed by the nifikop operator, including service and pod metadata, Zookeeper connection details, cluster image, detailed node configurations (storage, external volumes, resources, service accounts), node IDs, listener ports, and external service definitions for accessing the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/5_references/1_nifi_cluster/1_nifi_cluster.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n    annotations:\n      tyty: ytyt\n    labels:\n      tete: titi  \n  pod:\n    annotations:\n      toto: tata\n    labels:\n      titi: tutu\n  zkAddress: 'zookeepercluster-client.zookeeper:2181'\n  zkPath: '/simplenifi'\n  clusterImage: 'apache/nifi:1.11.3'\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      podMetadata:\n        annotations:\n          node-annotation: \"node-annotation-value\"\n        labels:\n          node-label: \"node-label-value\"\n      externalVolumeConfigs:\n        - name: example-volume\n          mountPath: \"/opt/nifi/example\"\n          secret:\n            secretName: \"raw-controller\"\n      storageConfigs:\n        - mountPath: '/opt/nifi/nifi-current/logs'\n          name: logs\n          # Metadata to attach to the PVC that gets created\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: 'standard'\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: 'default'\n      resourcesRequirements:\n        limits:\n          cpu: '2'\n          memory: 3Gi\n        requests:\n          cpu: '1'\n          memory: 1Gi\n  nodes:\n    - id: 1\n      nodeConfigGroup: 'default_group'\n    - id: 2\n      nodeConfigGroup: 'default_group'\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: 'http'\n        name: 'http'\n        containerPort: 8080\n      - type: 'cluster'\n        name: 'cluster'\n        containerPort: 6007\n      - type: 's2s'\n        name: 's2s'\n        containerPort: 10000\n  externalServices:\n    - name: 'clusterip'\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8080\n            internalListenerName: 'http'\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiDataflow CRD in YAML\nDESCRIPTION: This YAML snippet defines a NifiDataflow custom resource, instructing NiFiKop to deploy and manage a specific versioned dataflow from NiFi Registry. It specifies flow details, synchronization mode, update strategies, and references the necessary NifiCluster, NifiRegistryClient, and NifiParameterContext resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: demo\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Deploying a NifiDataflow Resource referencing Registry Client and Parameter Context using Kubernetes YAML\nDESCRIPTION: This YAML snippet defines a NifiDataflow custom resource used by NiFiKop to deploy and manage a NiFi dataflow version. It references a NiFi cluster, a registry client, and a parameter context by name and namespace. It specifies necessary flow identifiers such as parent process group ID, bucket ID, flow ID, and version. Additional configuration includes synchronization mode, strategy for updates, and options to skip invalid components. Deployment via this resource enables lifecycle management and synchronization of NiFi dataflows through Kubernetes, allowing operators to control flow versions and updates declaratively.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: demo\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Defining a NiFi Cluster using YAML\nDESCRIPTION: This YAML snippet defines a NifiCluster resource. It configures the desired state of a NiFi cluster, including service settings (headless service, annotations, labels), cluster manager (zookeeper), zookeeper address and path, external services, cluster image, init container image, node configuration (one node per node), read-only configuration, pod configuration, node config groups (with storage and resource requirements), node definitions, label propagation, task specifications, and listener configurations.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/5_references/1_nifi_cluster/1_nifi_cluster.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n    annotations:\n      tyty: ytyt\n    labels:\n      cluster-name: simplenifi\n      tete: titi\n  clusterManager: zookeeper\n  zkAddress: \"zookeeper.zookeeper:2181\"\n  zkPath: /simplenifi\n  externalServices:\n    - metadata:\n        annotations:\n          toto: tata\n        labels:\n          cluster-name: driver-simplenifi\n          titi: tutu\n      name: driver-ip\n      spec:\n        portConfigs:\n          - internalListenerName: http\n            port: 8080\n        type: ClusterIP\n  clusterImage: \"apache/nifi:1.26.0\"\n  initContainerImage: \"bash:5.2.2\"\n  oneNifiNodePerNode: true\n  readOnlyConfig:\n    nifiProperties:\n      overrideConfigs: |\n        nifi.sensitive.props.key=thisIsABadSensitiveKeyPassword\n  pod:\n    annotations:\n      toto: tata\n    labels:\n      cluster-name: simplenifi\n      titi: tutu\n  nodeConfigGroups:\n    default_group:\n      imagePullPolicy: IfNotPresent\n      isNode: true\n      serviceAccountName: default\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          reclaimPolicy: Delete\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      resourcesRequirements:\n        limits:\n          cpu: \"1\"\n          memory: 2Gi\n        requests:\n          cpu: \"1\"\n          memory: 2Gi\n  nodes:\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n    - id: 2\n      nodeConfigGroup: \"default_group\"\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - containerPort: 8080\n        type: http\n        name: http\n      - containerPort: 6007\n        type: cluster\n        name: cluster\n      - containerPort: 10000\n        type: s2s\n        name: s2s\n      - containerPort: 9090\n        type: prometheus\n        name: prometheus\n      - containerPort: 6342\n        type: load-balance\n        name: load-balance\n```\n\n----------------------------------------\n\nTITLE: Example NifiCluster Resource Definition (YAML)\nDESCRIPTION: This YAML manifest defines a `NifiCluster` resource named 'simplenifi' for deployment via the `nifi.konpyutaika.com` operator. It specifies configurations for Kubernetes services, pods (including annotations and labels), ZooKeeper connection details (`zkAddress`, `zkPath`), the NiFi container image, node grouping (`nodeConfigGroups`) with specific storage (`storageConfigs`), volume mounts (`externalVolumeConfigs`), resource requirements, service accounts, listener configurations (`internalListeners`), and external service definitions (`externalServices`).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/5_references/1_nifi_cluster/1_nifi_cluster.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n    annotations:\n      tyty: ytyt\n    labels:\n      tete: titi  \n  pod:\n    annotations:\n      toto: tata\n    labels:\n      titi: tutu\n  zkAddress: 'zookeepercluster-client.zookeeper:2181'\n  zkPath: '/simplenifi'\n  clusterImage: 'apache/nifi:1.11.3'\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      podMetadata:\n        annotations:\n          node-annotation: \"node-annotation-value\"\n        labels:\n          node-label: \"node-label-value\"\n      externalVolumeConfigs:\n        - name: example-volume\n          mountPath: \"/opt/nifi/example\"\n          secret:\n            secretName: \"raw-controller\"\n      storageConfigs:\n        - mountPath: '/opt/nifi/nifi-current/logs'\n          name: logs\n          # Metadata to attach to the PVC that gets created\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: 'standard'\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: 'default'\n      resourcesRequirements:\n        limits:\n          cpu: '2'\n          memory: 3Gi\n        requests:\n          cpu: '1'\n          memory: 1Gi\n  nodes:\n    - id: 1\n      nodeConfigGroup: 'default_group'\n    - id: 2\n      nodeConfigGroup: 'default_group'\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: 'http'\n        name: 'http'\n        containerPort: 8080\n      - type: 'cluster'\n        name: 'cluster'\n        containerPort: 6007\n      - type: 's2s'\n        name: 's2s'\n        containerPort: 10000\n  externalServices:\n    - name: 'clusterip'\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8080\n            internalListenerName: 'http'\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop Operator with Helm\nDESCRIPTION: Installs the NiFiKop operator using its OCI Helm chart. Deploys the operator into the 'nifi' namespace, specifies the version and image tag, configures resource requests/limits, and limits the operator's scope to the 'nifi' namespace. Requires the 'nifi' namespace to exist.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/2_setup/1_getting_started.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# You have to create the namespace before executing following command\nhelm install nifikop \\\n    oci://ghcr.io/konpyutaika/helm-charts/nifikop \\\n    --namespace=nifi \\\n    --version 0.13.0 \\\n    --set image.tag=v0.13.0-release \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set namespaces={\"nifi\"}\n```\n\n----------------------------------------\n\nTITLE: NiFiKop Helm Chart Deployment with Resource and Namespace Configuration - Bash\nDESCRIPTION: This snippet installs the NiFiKop operator Helm chart from an OCI registry with specifications for namespace, version, image tags, and resource requests/limits for CPU and memory. It requires Helm and kubectl configured to interact with the target cluster, and that the target namespace is created beforehand. The snippet also shows how to declare the managed namespaces. This facilitates a flexible and reproducible deployment of the NiFiKop operator.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/2_deploy_nifikop/1_quick_start.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop \\\n    oci://ghcr.io/konpyutaika/helm-charts/nifikop \\\n    --namespace=nifi \\\n    --version 1.9.0 \\\n    --set image.tag=v1.9.0-release \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set namespaces={\"nifi\"}\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiUser resource\nDESCRIPTION: This YAML snippet demonstrates how to define a `NifiUser` resource in Kubernetes for managing users in a NiFi cluster using the Nifikop operator. It shows how to specify the user's identity, the target NiFi cluster, whether to include a Java keystore (JKS), whether to create a certificate, and a list of access policies. The example defines a user named `aguitton` with the identity `alexandre.guitton@konpyutaika.com` in the `nc` cluster and grants read access to the `/data` resource.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/2_manage_users_and_accesses/1_users_management.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: aguitton\nspec:\n  # identity field is use to define the user identity on NiFi cluster side,\n  #\tit use full when the user's name doesn't suite with Kubernetes resource name.\n  identity: alexandre.guitton@konpyutaika.com\n  # Contains the reference to the NifiCluster with the one the registry client is linked.\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  # Whether or not the the operator also include a Java keystore format (JKS) with you secret\n  includeJKS: false\n  # Whether or not a certificate will be created for this user.\n  createCert: false\n  # defines the list of access policies that will be granted to the group.\n  accessPolicies:\n      # defines the kind of access policy, could be \"global\" or \"component\".\n    - type: component\n      # defines the kind of action that will be granted, could be \"read\" or \"write\"\n      action: read\n      # resource defines the kind of resource targeted by this access policies, please refer to the following page:\n      #\thttps://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#access-policies\n      resource: /data\n      # componentType is used if the type is \"component\", it's allow to define the kind of component on which is the\n      # access policy\n      componentType: \"process-groups\"\n      # componentId is used if the type is \"component\", it's allow to define the id of the component on which is the\n      # access policy\n      componentId: \"\"\n```\n\n----------------------------------------\n\nTITLE: Defining a NiFiConnection Resource in YAML\nDESCRIPTION: Example YAML definition for a NiFiConnection resource that connects two dataflow components with specific configuration. It includes flow file expiration, back pressure thresholds, load balancing strategy, prioritizers, and position bends.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/5_references/8_nifi_connection.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiConnection\nmetadata:\n  name: connection\n  namespace: instances\nspec:\n  source:\n    name: input\n    namespace: instances\n    subName: output_1\n    type: dataflow\n  destination:\n    name: output\n    namespace: instances\n    subName: input_1\n    type: dataflow\n  configuration:\n    flowFileExpiration: 1 hour\n    backPressureDataSizeThreshold: 100 GB\n    backPressureObjectThreshold: 10000\n    loadBalanceStrategy: PARTITION_BY_ATTRIBUTE\n    loadBalancePartitionAttribute: partition_attribute\n    loadBalanceCompression: DO_NOT_COMPRESS\n    prioritizers: \n      - NewestFlowFileFirstPrioritizer\n      - FirstInFirstOutPrioritizer\n    labelIndex: 0\n    bends:\n      - posX: 550\n        posY: 550\n      - posX: 550\n        posY: 440\n      - posX: 550\n        posY: 88\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Implementing NifiNodeGroupAutoscaler for NiFi Cluster\nDESCRIPTION: YAML configuration for a NifiNodeGroupAutoscaler custom resource that defines how to autoscale the NiFi cluster. It references the NiFi cluster, specifies which node config group to use, and defines scaling strategies.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiNodeGroupAutoscaler\nmetadata:\n  name: nifinodegroupautoscaler-sample\n  namespace: clusters\nspec:\n  # contains the reference to the NifiCluster with the one the node group autoscaler is linked.\n  clusterRef:\n    name: cluster\n    namespace: clusters\n  # defines the id of the NodeConfig contained in NifiCluster.Spec.NodeConfigGroups\n  nodeConfigGroupId: auto_scaling\n  # readOnlyConfig can be used to pass Nifi node config\n  # which has type read-only these config changes will trigger rolling upgrade\n  readOnlyConfig:\n    nifiProperties:\n      overrideConfigs: |\n        nifi.ui.banner.text=NiFiKop - Scale Group\n  # This is an example of a node config you can apply to each replica in this node group.\n  # Any settings here will override those in the configured nodeConfigGroupId\n#  nodeConfig:\n#    nodeSelector:\n#      node_type: high-mem\n  # The selector used to identify nodes in NifiCluster.Spec.Nodes this autoscaler will manage\n  # Use Node.Labels in combination with this selector to clearly define which nodes will be managed by this autoscaler \n  nodeLabelsSelector: \n    matchLabels:\n      nifi_cr: cluster\n      nifi_node_group: auto-scaling\n  # the strategy used to decide how to add nodes to a nifi cluster\n  upscaleStrategy: simple\n  # the strategy used to decide how to remove nodes from an existing cluster\n  downscaleStrategy: lifo\n```\n\n----------------------------------------\n\nTITLE: Install cert-manager Helm Chart (bash)\nDESCRIPTION: Installs the cert-manager Helm chart from the jetstack repository into the cert-manager namespace. Specifies the version v1.7.2. This command deploys the cert-manager controller components after CRDs have been applied and the repo added/updated. Requires Helm 3+.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/2_deploy_nifikop/1_quick_start.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhelm install cert-manager \\\n    --namespace cert-manager \\\n    --version v1.7.2 jetstack/cert-manager\n```\n\n----------------------------------------\n\nTITLE: Defining a NiFi Dataflow in Kubernetes using YAML\nDESCRIPTION: Example YAML manifest for a NifiDataflow resource that defines a NiFi dataflow to be deployed to a NiFi cluster. It specifies the parent process group, bucket ID, flow ID, version, position, and sync mode, along with references to cluster, registry client, and parameter context resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/5_references/5_nifi_dataflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  flowPosition:\n    posX: 0\n    posY: 0\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: squidflow\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: nifikop\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Nodes in NiFiKop (YAML)\nDESCRIPTION: This YAML snippet demonstrates configuring two NiFi nodes (ID 0 and 2) within a NiFiKop deployment. It shows using `nodeConfigGroup` for shared settings (node 0), overriding read-only NiFi properties like `nifi.ui.banner.text` via `readOnlyConfig`, and specifying detailed `nodeConfig` including Kubernetes resource requests/limits (CPU, memory) and persistent storage volumes (`storageConfigs`) with specific PVC configurations (access modes, storage class, size, metadata) for node 2.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/5_references/1_nifi_cluster/4_node.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n    - id: 0\n      # nodeConfigGroup can be used to ease the node configuration, if set only the id is required\n      nodeConfigGroup: \"default_group\"\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        nifiProperties:\n          overrideConfigs: |\n            nifi.ui.banner.text=NiFiKop - Node 0\n      # node configuration\n#       nodeConfig:\n    - id: 2\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        overrideConfigs: |\n          nifi.ui.banner.text=NiFiKop - Node 2\n      # node configuration\n      nodeConfig:\n        resourcesRequirements:\n          limits:\n            cpu: \"2\"\n            memory: 3Gi\n          requests:\n            cpu: \"1\"\n            memory: 1Gi\n        storageConfigs:\n          # Name of the storage config, used to name PV to reuse into sidecars for example.\n          - name: provenance-repository\n            # Path where the volume will be mount into the main nifi container inside the pod.\n            mountPath: \"/opt/nifi/provenance_repository\"\n            # Metadata to attach to the PVC that gets created\n            metadata:\n              labels:\n                my-label: my-value\n              annotations:\n                my-annotation: my-value\n            # Kubernetes PVC spec\n            # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n            pvcSpec:\n              accessModes:\n                - ReadWriteOnce\n              storageClassName: \"standard\"\n              resources:\n                requests:\n                  storage: 8Gi\n```\n\n----------------------------------------\n\nTITLE: Implementing External DNS and Cluster Configuration with External Let's Encrypt Issuer\nDESCRIPTION: Shows partial cluster configuration with external DNS integration and referencing the external ACME issuer for automated SSL cert management, ensuring secure communication in the NiFi cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_4\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nspec:\n  clusterSecure: true\n  siteToSiteSecure: true\n  listenersConfig:\n    clusterDomain: <DNS zone name>\n    useExternalDNS: true\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n      issuerRef:\n        name: letsencrypt-staging\n        kind: Issuer\n```\n\n----------------------------------------\n\nTITLE: Setting the custom authorizer property in NiFi configuration\nDESCRIPTION: This snippet shows how to set the NiFi property that specifies which of the configured authorizers to use. In this example, it's configured to use the custom DatabaseAuthorizer.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/6_users_authorization/1_custom_user_authorizer.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nnifi.security.user.authorizer=custom-database-authorizer\n```\n\n----------------------------------------\n\nTITLE: Example NifiUser Resource Definition (YAML)\nDESCRIPTION: This YAML snippet demonstrates how to define a NifiUser resource in Kubernetes. It specifies the user's identity (email), links it to a NiFi cluster (`clusterRef`), and indicates that a certificate should not be automatically created (`createCert: false`). This resource is managed by the Nifikop operator.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/5_references/2_nifi_user.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: aguitton\nspec:\n  identity: alexandre.guitton@konpyutaika.com\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  createCert: false\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiRegistryClient Resource (YAML)\nDESCRIPTION: Example YAML manifest for creating a `NifiRegistryClient` custom resource. This resource connects the NiFi cluster managed by NiFiKop to a NiFi Registry instance, specified by `uri`. It requires a `clusterRef` pointing to the target `NifiCluster` resource within the specified namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiRegistryClient\nmetadata:\n  name: registry-client-example\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Registry client managed by NiFiKop\"\n  uri: \"http://nifi.hostname.com:18080\"\n```\n\n----------------------------------------\n\nTITLE: Defining NifiParameterContext API Version\nDESCRIPTION: This YAML snippet defines the API version for a `NifiParameterContext` resource. It specifies the kind of resource and provides metadata, including a name. It is a basic definition of the desired state using specific fields like description, cluster reference, secret references, and parameters.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/5_references/4_nifi_parameter_context.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      description: toto\n      sensistive: true\n```\n\n----------------------------------------\n\nTITLE: Patching CRDs for NiFiKop resource version migration\nDESCRIPTION: This YAML snippet demonstrates how to patch the CRDs associated with NiFiKop resources (NifiCluster, NifiDataflow, etc.) to enable version conversion.  It configures the `conversion` section of the CRD with a `Webhook` strategy, specifying the service details for the conversion webhook and the supported conversion review versions (v1 and v1alpha1). The webhook service is assumed to be deployed in the same namespace as the NiFiKop operator.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/7_upgrade_guides/3_v0.16.0_to_v1.0.0.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n```\n\n----------------------------------------\n\nTITLE: Defining an External NifiCluster Resource in YAML\nDESCRIPTION: This YAML snippet defines a NifiCluster resource for an external NiFi cluster. It specifies the root process group ID, node URI template, node IDs, cluster type (external), client type (basic or tls), and a reference to a secret containing authentication credentials. The `rootProcessGroupId` is required for root-level policy management. `nodeURITemplate` is used to generate node hostnames based on the provided IDs. `type` is set to 'external' to indicate that the cluster is not managed by the operator.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/1_manage_clusters/3_external_cluster.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: externalcluster\nspec:\n  # rootProcessGroupId contains the uuid of the root process group for this cluster.\n  rootProcessGroupId: 'd37bee03-017a-1000-cff7-4eaaa82266b7'\n  # nodeURITemplate used to dynamically compute node uri.\n  nodeURITemplate: 'nifi0%d.integ.mapreduce.m0.p.fti.net:9090'\n  # all node requiresunique id\n  nodes:\n    - id: 1\n    - id: 2\n    - id: 3\n  # type defines if the cluster is internal (i.e manager by the operator) or external.\n  # :Enum={\"external\",\"internal\"}\n  type: 'external'\n  # clientType defines if the operator will use basic or tls authentication to query the NiFi cluster.\n  # Enum={\"tls\",\"basic\"}\n  clientType: 'basic'\n  # secretRef reference the secret containing the informations required to authenticate to the cluster.\n  secretRef:\n    name: nifikop-credentials\n    namespace: nifikop-nifi\n```\n\n----------------------------------------\n\nTITLE: Example NifiNodeGroupAutoscaler Manifest (YAML)\nDESCRIPTION: This YAML manifest defines a `NifiNodeGroupAutoscaler` resource named `nifinodegroupautoscaler-sample`. It targets the `NifiCluster` named `nificluster-name` in the `nifikop` namespace, specifically managing the node group identified by `nodeConfigGroupId: default-node-group`. Nodes are selected based on matching labels `nifi_cr: nificluster-name` and `nifi_node_group: default-node-group`. The autoscaler uses the `simple` strategy for scaling up and the `lifo` (Last In, First Out) strategy for scaling down.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/5_references/7_nifi_nodegroup_autoscaler.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiNodeGroupAutoscaler\nmetadata:\n  name: nifinodegroupautoscaler-sample\nspec:\n  # contains the reference to the NifiCluster with the one the node group autoscaler is linked.\n  clusterRef:\n    name: nificluster-name\n    namespace: nifikop\n  # defines the id of the NodeConfig contained in NifiCluster.Spec.NodeConfigGroups\n  nodeConfigGroupId: default-node-group\n  # The selector used to identify nodes in NifiCluster.Spec.Nodes this autoscaler will manage\n  # Use Node.Labels in combination with this selector to clearly define which nodes will be managed by this autoscaler \n  nodeLabelsSelector: \n    matchLabels:\n      nifi_cr: nificluster-name\n      nifi_node_group: default-node-group\n  # the strategy used to decide how to add nodes to a nifi cluster\n  upscaleStrategy: simple\n  # the strategy used to decide how to remove nodes from an existing cluster\n  downscaleStrategy: lifo\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop with Custom Values Using Helm\nDESCRIPTION: Command for installing the NiFiKop Helm chart with a custom values file. This allows for overriding the default configuration settings with customized values defined in a YAML file.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ helm install nifikop \\\n      konpyutaika/nifikop \\\n      -f values.yaml\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFi Dataflow CRD in YAML with Sync and Update Strategies\nDESCRIPTION: This YAML snippet defines a NifiDataflow custom resource used to deploy and manage versioned NiFi dataflows through NiFiKop. It specifies the parent process group ID, bucket ID, flow ID and version, synchronization mode, and update strategy among other controls. References to the cluster, registry client, and parameter context provide necessary connectivity. The syncMode field controls automatic lifecycle management behavior, with options 'never', 'once', and 'always'.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: demo\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Defining KEDA ScaledObject for NiFi Autoscaling - YAML\nDESCRIPTION: Creates a KEDA ScaledObject resource that binds to a target NiFiNodeGroupAutoscaler and uses a Prometheus metric as the scaling trigger. Includes scaling boundaries, fallback config, polling and cooldown intervals, and a placeholder for Prometheus queries and thresholds. Prerequisites: KEDA installed, properly exposed Prometheus, and the related CRD definitions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: cluster\n  namespace: clusters\nspec:\n  scaleTargetRef:\n    apiVersion:    nifi.konpyutaika.com/v1alpha1     # Optional. Default: apps/v1\n    kind:          NifiNodeGroupAutoscaler           # Optional. Default: Deployment\n    name:          nifinodegroupautoscaler-sample    # Mandatory. Must be in the same namespace as the ScaledObject\n    envSourceContainerName: nifi                     # Optional. Default: .spec.template.spec.containers[0]\n  pollingInterval:  30                               # Optional. Default: 30 seconds\n  cooldownPeriod:   300                              # Optional. Default: 300 seconds\n  idleReplicaCount: 0                                # Optional. Default: ignored, must be less than minReplicaCount \n  minReplicaCount:  1                                # Optional. Default: 0\n  maxReplicaCount:  3                                # Optional. Default: 100\n  fallback:                                          # Optional. Section to specify fallback options\n    failureThreshold: 5                              # Mandatory if fallback section is included\n    replicas: 1                                      # Mandatory if fallback section is included\n  # advanced:                                          # Optional. Section to specify advanced options\n  #   restoreToOriginalReplicaCount: true              # Optional. Whether the target resource should be scaled back to original replicas count, after the ScaledObject is deleted\n  #   horizontalPodAutoscalerConfig:                   # Optional. Section to specify HPA related options\n  #     name: {name-of-hpa-resource}                   # Optional. Default: keda-hpa-{scaled-object-name}\n  #     behavior:                                      # Optional. Use to modify HPA's scaling behavior\n  #       scaleDown:\n  #         stabilizationWindowSeconds: 300 <--- \n  #         policies:\n  #         - type: Percent\n  #           value: 100\n  #           periodSeconds: 15\n  triggers:\n    - type: prometheus\n      metadata:\n        serverAddress: http://prometheus-operated.monitoring-system.svc:9090\n        metricName: http_requests_total\n        threshold: <threshold value>\n        query: <prometheus query>\n\n```\n\n----------------------------------------\n\nTITLE: Configuring IDE Environment Variables (Bash)\nDESCRIPTION: Sets required environment variables for running the NiFiKop operator directly from an IDE. 'KUBECONFIG' specifies the Kubernetes configuration file path, 'WATCH_NAMESPACE' defines the namespace the operator should monitor, 'POD_NAME' assigns a name to the operator pod instance, 'LOG_LEVEL' controls logging verbosity (e.g., 'Debug'), and 'OPERATOR_NAME' provides a name for the operator process.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/6_contributing/1_developer_guide.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nKUBECONFIG={path/to/your/kubeconfig}\nWATCH_NAMESPACE={namespace_to_watch}\nPOD_NAME={name for operator pod}\nLOG_LEVEL=Debug\nOPERATOR_NAME=ide\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Cluster Service with HTTPS Listener - YAML\nDESCRIPTION: Specifies a NiFi Kubernetes service section that exposes a ClusterIP service named 'nifi-cluster' on port 8443, to be used by the VirtualService as its route destination. Requires this configuration to be included within the overall NiFi deployment YAML. The 'internalListenerName' associates the port with the NiFi HTTPS listener.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_4\n\nLANGUAGE: YAML\nCODE:\n```\nspec:  \n  externalServices:  \n    - name: \"nifi-cluster\"\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8443\n            internalListenerName: \"https\"\n\n```\n\n----------------------------------------\n\nTITLE: Defining a NiFi User with NifiUser Resource in Kubernetes YAML\nDESCRIPTION: This YAML snippet demonstrates the declaration of a NifiUser custom resource for managing an Apache NiFi user in Kubernetes using the nifikop operator. Key fields include 'identity' (the user's NiFi-side identifier), 'clusterRef' for linking the user to a specific NiFiCluster, 'includeJKS' and 'createCert' for managing Java keystore and certificate creation respectively, and 'accessPolicies' for assigning permissions. The resource allows Kubernetes-NiFi identity mapping and supports granular access controls through policy configuration. Dependencies include nifikop installed on the cluster, a referenced NifiCluster, and correct values for required fields.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/2_manage_users_and_accesses/1_users_management.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: aguitton\nspec:\n  # identity field is use to define the user identity on NiFi cluster side,\n  #\tit use full when the user's name doesn't suite with Kubernetes resource name.\n  identity: alexandre.guitton@konpyutaika.com\n  # Contains the reference to the NifiCluster with the one the registry client is linked.\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  # Whether or not the the operator also include a Java keystore format (JKS) with you secret\n  includeJKS: false\n  # Whether or not a certificate will be created for this user.\n  createCert: false\n  # defines the list of access policies that will be granted to the group.\n  accessPolicies:\n      # defines the kind of access policy, could be \"global\" or \"component\".\n    - type: component\n      # defines the kind of action that will be granted, could be \"read\" or \"write\"\n      action: read\n      # resource defines the kind of resource targeted by this access policies, please refer to the following page:\n      #\thttps://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#access-policies\n      resource: /data\n      # componentType is used if the type is \"component\", it's allow to define the kind of component on which is the\n      # access policy\n      componentType: \"process-groups\"\n      # componentId is used if the type is \"component\", it's allow to define the id of the component on which is the\n      # access policy\n      componentId: \"\"\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFiKop Cluster to Add a Node - YAML\nDESCRIPTION: Defines an updated NifiCluster Custom Resource to add a new NiFi node (id 25) to the cluster. This YAML manifest must be applied to the Kubernetes cluster to scale up the NiFi cluster managed by NiFiKop. The configuration includes resource requests/limits, storage, node group references, Zookeeper integration, and internal network listeners. The Node.Id must be unique in the node list. Dependencies: NiFiKop CRDs installed, Zookeeper available. Inputs: Kubernetes YAML manifest. Outputs: A new NiFi pod, configmap, and PVC in the cluster. Limitations: Node ids must not be reused, and CR structure must conform to expected schema.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.12.1\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n    - id: 2\n      nodeConfigGroup: \"default_group\"\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenId Connect Settings in nifi.properties\nDESCRIPTION: This snippet shows the configuration parameters to enable OpenId Connect in NiFi. It includes the DN mapping pattern, value, and transformation settings needed for identity provider support. Dependencies include editing the nifi.properties file on NiFi nodes, and key parameters define the discovery URL, client ID, and secret.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/5_users_authentication/1_oidc.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\nnifi.security.identity.mapping.pattern.dn=CN=([^,]*)(?:, (?:O|OU)=.*)?\\nnifi.security.identity.mapping.value.dn=$1\\nnifi.security.identity.mapping.transform.dn=NONE\n```\n\n----------------------------------------\n\nTITLE: Defining a NiFi Cluster with NifiCluster CRD in YAML\nDESCRIPTION: This YAML manifest defines a `NifiCluster` custom resource named `simplenifi`. It configures a NiFi cluster managed by ZooKeeper, specifies headless service settings, defines external services, sets the NiFi image (`apache/nifi:1.28.0`), configures node properties (including sensitive properties override), defines storage requirements (`10Gi`), resource limits/requests, and sets up various internal listeners (HTTP, cluster, S2S, Prometheus, load-balance) on specific ports.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/5_references/1_nifi_cluster/1_nifi_cluster.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n    annotations:\n      tyty: ytyt\n    labels:\n      cluster-name: simplenifi\n      tete: titi\n  clusterManager: zookeeper\n  zkAddress: \"zookeeper.zookeeper:2181\"\n  zkPath: /simplenifi\n  externalServices:\n    - metadata:\n        annotations:\n          toto: tata\n        labels:\n          cluster-name: driver-simplenifi\n          titi: tutu\n      name: driver-ip\n      spec:\n        portConfigs:\n          - internalListenerName: http\n            port: 8080\n        type: ClusterIP\n  clusterImage: \"apache/nifi:1.28.0\"\n  initContainerImage: \"bash:5.2.2\"\n  oneNifiNodePerNode: true\n  readOnlyConfig:\n    nifiProperties:\n      overrideConfigs: |\n        nifi.sensitive.props.key=thisIsABadSensitiveKeyPassword\n  pod:\n    annotations:\n      toto: tata\n    labels:\n      cluster-name: simplenifi\n      titi: tutu\n  nodeConfigGroups:\n    default_group:\n      imagePullPolicy: IfNotPresent\n      isNode: true\n      serviceAccountName: default\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          reclaimPolicy: Delete\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      resourcesRequirements:\n        limits:\n          cpu: \"1\"\n          memory: 2Gi\n        requests:\n          cpu: \"1\"\n          memory: 2Gi\n  nodes:\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n    - id: 2\n      nodeConfigGroup: \"default_group\"\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - containerPort: 8080\n        type: http\n        name: http\n      - containerPort: 6007\n        type: cluster\n        name: cluster\n      - containerPort: 10000\n        type: s2s\n        name: s2s\n      - containerPort: 9090\n        type: prometheus\n        name: prometheus\n      - containerPort: 6342\n        type: load-balance\n        name: load-balance\n```\n\n----------------------------------------\n\nTITLE: Bash Commands to Extract and Save User Certificate Files\nDESCRIPTION: Retrieves stored secrets for a NiFi user, decodes base64-encoded data, and saves CA certificate, user certificate, and private key to local files. Dependencies include kubectl. Useful for mounting secrets into pods or local testing.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['ca\\.crt']}\" | base64 -d > ca.crt\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['tls\\.crt']}\" | base64 -d > tls.crt\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['tls\\.key']}\" | base64 -d > tls.key\n```\n\n----------------------------------------\n\nTITLE: Overriding nifi.properties for OIDC in NiFiKop Custom Resource YAML\nDESCRIPTION: This YAML snippet demonstrates how to configure the NiFiKop custom resource to override NiFi's default nifi.properties file, adding OpenId Connect authentication settings. It configures the web proxy hosts and specifies OIDC discovery URL, client ID, and secret within the overrideConfigs field. Additionally, it includes identity mapping configuration consistent with NiFi properties. Dependencies include a running NiFiKop operator managing NiFi clusters via these CRDs. Input parameters such as OIDC URLs and credentials must be supplied accurately; the output is a NiFi cluster configured to authenticate with OIDC.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/5_users_authentication/1_oidc.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  readOnlyConfig:\n    # NifiProperties configuration that will be applied to the node.\n    nifiProperties:\n      webProxyHosts:\n        - nifistandard2.trycatchlearn.fr:8443\n      # Additionnal nifi.properties configuration that will override the one produced based\n      # on template and configurations.\n      overrideConfigs: |\n        nifi.security.user.oidc.discovery.url=<oidc server discovery url>\n        nifi.security.user.oidc.client.id=<oidc client\\'s id>\n        nifi.security.user.oidc.client.secret=<oidc client\\'s secret>\n        nifi.security.identity.mapping.pattern.dn=CN=([^,]*)(?:, (?:O|OU)=.*)?\n        nifi.security.identity.mapping.value.dn=$1\n        nifi.security.identity.mapping.transform.dn=NONE\n      ...\n   ...\n...\n```\n\n----------------------------------------\n\nTITLE: Setting Custom Authorizer in NiFi Properties\nDESCRIPTION: A configuration setting for NiFi's properties file that specifies which authorizer to use from those configured in the authorizers.xml file. This example selects the custom database authorizer.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/6_users_authorization/1_custom_user_authorizer.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nnifi.security.user.authorizer=custom-database-authorizer\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Dataflow Configuration in YAML\nDESCRIPTION: This YAML snippet defines a `NifiDataflow` resource, specifying its metadata, spec, and relationships with other resources like `NifiCluster`, `ParameterContext`, and `RegistryClient`. It sets parameters such as the parent process group, bucket ID, flow ID, flow version, and update strategy. The configuration ensures correct dataflow deployment and synchronization based on the defined attributes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/5_references/5_nifi_dataflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  flowPosition:\n    posX: 0\n    posY: 0\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: squidflow\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: nifikop\n  updateStrategy: drain\n```\n```\n\n----------------------------------------\n\nTITLE: Listing Managed NiFi User Groups via Kubectl Console\nDESCRIPTION: This console command demonstrates how to use `kubectl` to retrieve a list of all `NifiUserGroup` resources managed by the NiFiKop operator within a specified namespace (`nifikop`). It serves as a verification step to confirm that the operator has successfully created and is managing the predefined groups, including the `managed-admins`, `managed-readers`, and the automatically created `managed-nodes` group.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/2_manage_users_and_accesses/3_managed_groups.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nkubectl get -n nifikop nifiusergroups.nifi.konpyutaika.com \n```\n\n----------------------------------------\n\nTITLE: Defining NifiDataflow CRD in YAML\nDESCRIPTION: This YAML snippet defines a `NifiDataflow` custom resource, specifying its `apiVersion`, `kind`, and `metadata`. It provides a template for defining a NiFi dataflow, including the parent process group, bucket and flow IDs, flow version, flow position, sync mode, update strategy, and references to other Kubernetes resources such as `ClusterRef`, `ParameterContextRef`, and `RegistryClientRef`. The resource specifies dataflow lifecycle in a declarative way.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/5_references/5_nifi_dataflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  flowPosition:\n    posX: 0\n    posY: 0\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: squidflow\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: nifikop\n  updateStrategy: drain\n```\n```\n\n----------------------------------------\n\nTITLE: Verifying New Node Resources - Console Command\nDESCRIPTION: This console command lists pods, configmaps, and persistent volume claims associated with the newly added NiFi node by filtering resources using the 'nodeId=25' label. It requires kubectl and sufficient access to list resources in the 'nifi' namespace. The command's input is the label selector, and the output details the status of the pod, configmap, and PVC for the specified node, allowing verification that the node was added successfully.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_2\n\nLANGUAGE: Console\nCODE:\n```\nkubectl get pods,configmap,pvc -l nodeId=25\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Listener Types - YAML\nDESCRIPTION: This YAML snippet configures the different listener types within the NiFi cluster. It defines internal listeners with their types (https, cluster, s2s, prometheus, load-balance), names, and container ports. It also specifies settings for SSL secrets, including the TLS secret name and whether to create secrets.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/5_references/1_nifi_cluster/6_listeners_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  listenersConfig:\n    internalListeners:\n      - type: \"https\"\n        name: \"https\"\n        containerPort: 8443\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n      - type: \"prometheus\"\n        name: \"prometheus\"\n        containerPort: 9090\n      - type: \"load-balance\"\n        name: \"load-balance\"\n        containerPort: 6342\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFi Cluster Using kubectl on Kubernetes - Bash\nDESCRIPTION: Deploys a simple NiFi cluster by applying a pre-configured Kubernetes manifest file that references the Zookeeper service name. This requires prior configuration of the NiFiCluster resource to ensure connectivity with Zookeeper and appropriate namespace context.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create -n nifi -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Deploying a NifiDataflow - Input\nDESCRIPTION: This YAML snippet defines a NifiDataflow resource named 'input'. It specifies the cluster reference, bucket ID, flow ID, flow version, registry client reference, and other configurations like skipping invalid components, sync mode, update strategy, and flow position. The namespace is 'nifikop'.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/4_manage_connections/1_deploy_connection.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiDataflow\nmetadata:\n  name: input\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  bucketId: deedb9f6-65a4-44e9-a1c9-722008fcd0ba\n  flowId: ab95431d-980d-41bd-904a-fac4bd864ba6\n  flowVersion: 1\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  skipInvalidComponent: true\n  skipInvalidControllerService: true\n  syncMode: always\n  updateStrategy: drain\n  flowPosition:\n    posX: 0\n    posY: 0\n```\n\n----------------------------------------\n\nTITLE: Example NiFi Node Configuration (YAML)\nDESCRIPTION: This YAML snippet demonstrates an example configuration for a NiFi node using the `NodeConfig` structure, likely within a custom resource definition managed by the nifikop operator. It specifies settings such as the maximum provenance storage size, the user ID for running the NiFi process, flags it as part of a cluster, adds custom Kubernetes metadata (annotations and labels) to the pod, defines the image pull policy, mounts an external volume from a Kubernetes secret, and configures persistent storage using PVCs for provenance and logs with specific reclaim policies and resource requests.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/5_references/1_nifi_cluster/3_node_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n   default_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      # Set this to true if the instance is a node in a cluster.\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#basic-cluster-setup\n      isNode: true\n      # Additionnal metadata to merge to the pod associated\n      podMetadata:\n        annotations:\n          node-annotation: \"node-annotation-value\"\n        labels:\n          node-label: \"node-label-value\"\n      # Docker image used by the operator to create the node associated\n      # https://hub.docker.com/r/apache/nifi/\n#      image: \"apache/nifi:1.11.2\"\n      # nodeAffinity can be specified, operator populates this value if new pvc added later to node\n      # https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity\n#      nodeAffinity:\n      # imagePullPolicy define the pull policy for NiFi cluster docker image\n      imagePullPolicy: IfNotPresent\n      # priorityClassName define the name of the priority class to be applied to these nodes\n      priorityClassName: \"example-priority-class-name\"\n      # externalVolumeConfigs specifies a list of volume to mount into the main container.\n      externalVolumeConfigs:\n        - name: example-volume\n          mountPath: \"/opt/nifi/example\"\n          secret:\n            secretName: \"raw-controller\"\n      # storageConfigs specifies the node related configs\n      storageConfigs:\n        # Name of the storage config, used to name PV to reuse into sidecars for example.\n        - name: provenance-repository\n          # Retain this PVC throughout NifiCluster deletions.\n          reclaimPolicy: Retain\n          # Path where the volume will be mount into the main nifi container inside the pod.\n          mountPath: \"/opt/nifi/provenance_repository\"\n          # Metadata to attach to the PVC that gets created\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          # Kubernetes PVC spec\n          # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          reclaimPolicy: Delete\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n```\n\n----------------------------------------\n\nTITLE: ReadOnlyConfig YAML Configuration\nDESCRIPTION: This YAML snippet defines the structure and configurable options for the ReadOnlyConfig object in Nifikop. It includes settings for thread management, logback configuration, authorizer configuration, NiFi properties, Zookeeper properties, and Bootstrap properties.  It shows how to override configurations using configmaps, secrets, and direct configuration strings.  The configuration is read-only at the cluster level but can be overridden per node.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/5_references/1_nifi_cluster/2_read_only_config.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\nreadOnlyConfig:\n  # MaximumTimerDrivenThreadCount define the maximum number of threads for timer driven processors available to the system.\n  maximumTimerDrivenThreadCount: 30\n  # MaximumEventDrivenThreadCount define the maximum number of threads for event driven processors available to the system.\n  maximumEventDrivenThreadCount: 10\n  # Logback configuration that will be applied to the node\n  logbackConfig:\n    # logback.xml configuration that will replace the one produced based on template\n    replaceConfigMap:\n      # The key of the value,in data content, that we want use.\n      data: logback.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # logback.xml configuration that will replace the one produced based on template and overrideConfigMap\n    replaceSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: logback.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n  # Authorizer configuration that will be applied to the node\n  authorizerConfig:\n    # An authorizers.xml configuration template that will replace the default template seen in authorizers.go\n    replaceTemplateConfigMap:\n      # The key of the value, in data content, that we want use.\n      data: authorizers.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # An authorizers.xml configuration template that will replace the default template seen in authorizers.go and the replaceTemplateConfigMap\n    replaceTemplateSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: authorizers.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n  # NifiProperties configuration that will be applied to the node.\n  nifiProperties:\n    # Additionnal nifi.properties configuration that will override the one produced based on template and\n    # configuration\n    overrideConfigMap:\n      # The key of the value,in data content, that we want use.\n      data: nifi.properties\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop.\n    # Additionnal nifi.properties configuration that will override the one produced based\n    #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    overrideSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: nifi.properties\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # Additionnal nifi.properties configuration that will override the one produced based\n    #\ton template, configurations and overrideConfigMap\n    overrideConfigs: |\n      nifi.ui.banner.text=NiFiKop\n    # A comma separated list of allowed HTTP Host header values to consider when NiFi\n    # is running securely and will be receiving requests to a different host[:port] than it is bound to.\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#web-properties\n    #      webProxyHosts:\n    # Nifi security client auth\n    needClientAuth: false\n    # Indicates which of the configured authorizers in the authorizers.xml file to use\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#authorizer-configuration\n  #      authorizer:\n  # ZookeeperProperties configuration that will be applied to the node.\n  zookeeperProperties:\n    #      # Additionnal zookeeeper.properties configuration that will override the one produced based on template and\n    #      # configuration\n    #      overrideConfigMap:\n    #        # The key of the value,in data content, that we want use.\n    #        data: zookeeeper.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop.\n    #      # Additionnal zookeeeper.properties configuration that will override the one produced based\n    #      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    #      overrideSecretConfig:\n    #        # The key of the value,in data content, that we want use.\n    #        data: zookeeeper.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop\n    # Additionnal zookeeper.properties configuration that will override the one produced based\n    # on template and configurations.\n    overrideConfigs: |\n      initLimit=15\n      autopurge.purgeInterval=24\n      syncLimit=5\n      tickTime=2000\n      dataDir=./state/zookeeper\n      autopurge.snapRetainCount=30\n  # BootstrapProperties configuration that will be applied to the node.\n  bootstrapProperties:\n    #      # Additionnal bootstrap.properties configuration that will override the one produced based on template and\n    #      # configuration\n    #      overrideConfigMap:\n    #        # The key of the value,in data content, that we want use.\n    #        data: bootstrap.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop.\n    #      # Additionnal bootstrap.properties configuration that will override the one produced based\n    #      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    #      overrideSecretConfig:\n    #        # The key of the value,in data content, that we want use.\n    #        data: bootstrap.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop\n    # JVM memory settings\n    nifiJvmMemory: \"512m\"\n    # Additionnal bootstrap.properties configuration that will override the one produced based\n    # on template and configurations.\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#bootstrap_properties\n    overrideConfigs: |\n      # java.arg.4=-Djava.net.preferIPv4Stack=true\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi NodeGroup Autoscaler with Kubernetes Custom Resource YAML\nDESCRIPTION: This YAML snippet defines an instance of the NifiNodeGroupAutoscaler custom resource to configure automatic scaling behavior for a node group within a NiFiCluster. It specifies references to the target NifiCluster, identifies the node group via nodeConfigGroupId and nodeLabelsSelector, and sets the upscale and downscale strategies for managing cluster nodes. The configuration is designed for Kubernetes environments and requires existing NiFiCluster and NodeConfigGroups. The autoscaler manages node scaling with strategies 'simple' (upscale) and 'lifo' (downscale).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/5_references/7_nifi_nodegroup_autoscaler.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiNodeGroupAutoscaler\nmetadata:\n  name: nifinodegroupautoscaler-sample\nspec:\n  # contains the reference to the NifiCluster with the one the node group autoscaler is linked.\n  clusterRef:\n    name: nificluster-name\n    namespace: nifikop\n  # defines the id of the NodeConfig contained in NifiCluster.Spec.NodeConfigGroups\n  nodeConfigGroupId: default-node-group\n  # The selector used to identify nodes in NifiCluster.Spec.Nodes this autoscaler will manage\n  # Use Node.Labels in combination with this selector to clearly define which nodes will be managed by this autoscaler \n  nodeLabelsSelector: \n    matchLabels:\n      nifi_cr: nificluster-name\n      nifi_node_group: default-node-group\n  # the strategy used to decide how to add nodes to a nifi cluster\n  upscaleStrategy: simple\n  # the strategy used to decide how to remove nodes from an existing cluster\n  downscaleStrategy: lifo\n```\n\n----------------------------------------\n\nTITLE: Overriding NiFi Properties with Kubernetes ConfigMap, Secret, and Inline YAML in NiFiCluster\nDESCRIPTION: This YAML snippet demonstrates how to specify NiFi properties overriding using three types of configuration sources in the NiFiCluster resource: a ConfigMap reference, a Kubernetes Secret reference, and an inline string override. The configuration keys specify which data entries to use, with namespace and resource names provided for secret and configmap lookup. Priority is given to Secret over ConfigMap over inline override strings to control configuration precedence. This allows secure and flexible property management for NiFi deployments.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n nifiProperties:\n      # Additionnal nifi.properties configuration that will override the one produced based on template and\n      # configuration\n      overrideConfigMap:\n        # The key of the value,in data content, that we want use.\n        data: nifi.properties\n        # Name of the configmap that we want to refer.\n        name: raw\n        # Namespace where is located the secret that we want to refer.\n        namespace: nifikop\n      # Additionnal nifi.properties configuration that will override the one produced based\n      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n      overrideSecretConfig:\n        # The key of the value,in data content, that we want use.\n        data: nifi.properties\n        # Name of the configmap that we want to refer.\n        name: raw\n        # Namespace where is located the secret that we want to refer.\n        namespace: nifikop\n      # Additionnal nifi.properties configuration that will override the one produced based\n      #\ton template, configurations and overrideConfigMap\n      overrideConfigs: |\n        nifi.ui.banner.text=NiFiKop\n        nifi.sensitive.props.key=thisIsABadSensitiveKeyPassword\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Connection Resource for Kubernetes in YAML\nDESCRIPTION: This YAML snippet defines a NiFiConnection custom resource manifest compliant with Kubernetes resource specifications. It illustrates the structure of a NiFi connection object including metadata, source and destination component references, detailed connection-specific configuration parameters like flow file expiration, backpressure thresholds, load balancing strategies, compression options, and prioritized queue handling. It also configures the update strategy to \"drain\" to specify how dataflow components should be treated during connection lifecycle events. This snippet is essential for deploying and managing NiFi connections via an operator in Kubernetes environments.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/5_references/8_nifi_connection.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiConnection\nmetadata:\n  name: connection\n  namespace: instances\nspec:\n  source:\n    name: input\n    namespace: instances\n    subName: output_1\n    type: dataflow\n  destination:\n    name: output\n    namespace: instances\n    subName: input_1\n    type: dataflow\n  configuration:\n    flowFileExpiration: 1 hour\n    backPressureDataSizeThreshold: 100 GB\n    backPressureObjectThreshold: 10000\n    loadBalanceStrategy: PARTITION_BY_ATTRIBUTE\n    loadBalancePartitionAttribute: partition_attribute\n    loadBalanceCompression: DO_NOT_COMPRESS\n    prioritizers: \n      - NewestFlowFileFirstPrioritizer\n      - FirstInFirstOutPrioritizer\n    labelIndex: 0\n    bends:\n      - posX: 550\n        posY: 550\n      - posX: 550\n        posY: 440\n      - posX: 550\n        posY: 88\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop Operator with Helm\nDESCRIPTION: This command deploys the NiFiKop operator via Helm from a specified OCI chart, setting the desired image tag, resource requests and limits, and deploying within the designated namespace. It includes a note on version customization.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/2_setup/1_getting_started.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop \\\n    oci://ghcr.io/konpyutaika/helm-charts/nifikop \\\n    --namespace=nifi \\\n    --version 0.14.0 \\\n    --set image.tag=v0.14.0-release \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set namespaces={\"nifi\"}\n```\n\n----------------------------------------\n\nTITLE: Defining NifiDataflow Resources in YAML\nDESCRIPTION: This YAML snippet defines two NifiDataflow resources named 'input' and 'output' for use with NiFiKop in a Kubernetes environment. Each resource specifies required parameters such as the associated NiFi cluster, registry client, bucket ID, flow ID, and version. Both dataflows include settings for skipping invalid components/services, synchronization mode, update strategy, and initial flow positioning. Ensure NiFiKop is installed and proper namespaces, clusters, and registry client resources exist before applying these manifests.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/4_manage_connections/1_deploy_connection.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiDataflow\nmetadata:\n  name: input\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  bucketId: deedb9f6-65a4-44e9-a1c9-722008fcd0ba\n  flowId: ab95431d-980d-41bd-904a-fac4bd864ba6\n  flowVersion: 1\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  skipInvalidComponent: true\n  skipInvalidControllerService: true\n  syncMode: always\n  updateStrategy: drain\n  flowPosition:\n    posX: 0\n    posY: 0\n---\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiDataflow\nmetadata:\n  name: output\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  bucketId: deedb9f6-65a4-44e9-a1c9-722008fcd0ba\n  flowId: fc5363eb-801e-432f-aa94-488838674d07\n  flowVersion: 2\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  skipInvalidComponent: true\n  skipInvalidControllerService: true\n  syncMode: always\n  updateStrategy: drain\n  flowPosition:\n    posX: 750\n    posY: 0\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Cluster Node with Prometheus and Storage in YAML\nDESCRIPTION: This snippet provides part of a Kubernetes manifest for deploying a NiFi cluster with Prometheus metrics enabled and ample persistent storage for different NiFi directories. Prerequisites include an operational Kubernetes cluster, a NiFi operator, and associated PVCs or StorageClasses defined. Key parameters control CPU/memory limits, storage mounts, and node group identity for autoscaling. The manifest should be integrated into a larger NiFi CRD definition for production use.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n...\nspec:\n  ...\n  listenersConfig:\n    internalListeners:\n    ...\n    - containerPort: 9090\n      name: prometheus\n      type: prometheus \n    ...\n  ...\n  nodeConfigGroups:\n    auto_scaling:\n      isNode: true\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 2Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n      serviceAccountName: external-dns\n      storageConfigs:\n        - mountPath: /opt/nifi/nifi-current/logs\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/data\n          name: data\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/extensions\n          name: extensions-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/flowfile_repository\n          name: flowfile-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/nifi-current/conf\n          name: conf\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/content_repository\n          name: content-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/provenance_repository\n          name: provenance-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n  ...\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi parameters at cluster/nodes level with ReadOnlyConfig\nDESCRIPTION: This snippet explains how to set global or node-specific NiFi configurations such as maximum thread counts using the `ReadOnlyConfig` field, which can accept parameters like `maximumTimerDrivenThreadCount` and other override configurations, possibly via secrets or config maps.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nReadOnlyConfig:\n  maximumTimerDrivenThreadCount: 10\n  maximumEventDrivenThreadCount: 4  # Deprecated from v1.9.0\n  # Additional configuration parameters can be added here, including secrets or configMap references.\n```\n\n----------------------------------------\n\nTITLE: Defining a NiFi Cluster Using NifiCluster Custom Resource in YAML\nDESCRIPTION: This YAML snippet specifies the desired state of a NiFi cluster through the NifiCluster custom resource. It includes cluster metadata, service and pod configurations, Zookeeper connection details, image, node configurations, resource requirements, nodes list, labels, retry settings, listener configurations, and external service definitions. This configuration manages deployment, resource allocation, and clustering behavior for NiFi in Kubernetes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/5_references/1_nifi_cluster/1_nifi_cluster.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n    annotations:\n      tyty: ytyt\n    labels:\n      tete: titi  \n  pod:\n    annotations:\n      toto: tata\n    labels:\n      titi: tutu\n  zkAddress: 'zookeepercluster-client.zookeeper:2181'\n  zkPath: '/simplenifi'\n  clusterImage: 'apache/nifi:1.11.3'\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      podMetadata:\n        annotations:\n          node-annotation: \"node-annotation-value\"\n        labels:\n          node-label: \"node-label-value\"\n      externalVolumeConfigs:\n        - name: example-volume\n          mountPath: \"/opt/nifi/example\"\n          secret:\n            secretName: \"raw-controller\"\n      storageConfigs:\n        - mountPath: '/opt/nifi/nifi-current/logs'\n          name: logs\n          # Metadata to attach to the PVC that gets created\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: 'standard'\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: 'default'\n      resourcesRequirements:\n        limits:\n          cpu: '2'\n          memory: 3Gi\n        requests:\n          cpu: '1'\n          memory: 1Gi\n  nodes:\n    - id: 1\n      nodeConfigGroup: 'default_group'\n    - id: 2\n      nodeConfigGroup: 'default_group'\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: 'http'\n        name: 'http'\n        containerPort: 8080\n      - type: 'cluster'\n        name: 'cluster'\n        containerPort: 6007\n      - type: 's2s'\n        name: 's2s'\n        containerPort: 10000\n  externalServices:\n    - name: 'clusterip'\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8080\n            internalListenerName: 'http'\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n\n----------------------------------------\n\nTITLE: Defining Storage Persistence Configurations for NiFi Nodes\nDESCRIPTION: This YAML snippet details how to define persistent storage for various NiFi data directories by specifying mount paths, PVC specifications, labels, annotations, and storage class, ensuring data durability across pod restarts.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\nstorageConfigs:\n  - mountPath: /opt/nifi/nifi-current/logs\n    name: logs\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/data\n    name: data\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 50Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/extensions\n    name: extensions-repository\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 5Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/flowfile_repository\n    name: flowfile-repository\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/nifi-current/conf\n    name: conf\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 5Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/content_repository\n    name: content-repository\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 500Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/provenance_repository\n    name: provenance-repository\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 500Gi\n```\n\n----------------------------------------\n\nTITLE: Configure NiFiCluster with External DNS and Issuer (YAML)\nDESCRIPTION: This YAML configures a `NifiCluster` resource to utilize an external DNS and an existing issuer for certificate management. It sets `clusterSecure` and `siteToSiteSecure` to true, and then defines `clusterDomain` and `useExternalDNS`.  It references an existing issuer (`letsencrypt-staging`) in `listenersConfig.sslSecrets`, implying the use of a certificate authority.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\n...\nspec:\n  ...\n  clusterSecure: true\n  siteToSiteSecure: true\n  ...\n  listenersConfig:\n    clusterDomain: <DNS zone name>\n    useExternalDNS: true\n    ...\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n      issuerRef:\n        name: letsencrypt-staging\n        kind: Issuer\n```\n\n----------------------------------------\n\nTITLE: Defining NifiNodeGroupAutoscaler as Kubernetes Custom Resource in YAML\nDESCRIPTION: This Kubernetes YAML manifest defines a NifiNodeGroupAutoscaler custom resource to configure autoscaling for a specific node group within a NiFi cluster. Requires the NiFi operator and CRDs already installed. Main parameters include cluster reference, target node config group ID, optional read-only NiFi configuration, node label selectors, and scaling strategies ('upscaleStrategy' and 'downscaleStrategy'). Inputs are resource and cluster metadata; outputs are dynamic scaling actions on the cluster nodes based on defined strategies.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiNodeGroupAutoscaler\nmetadata:\n  name: nifinodegroupautoscaler-sample\n  namespace: clusters\nspec:\n  # contains the reference to the NifiCluster with the one the node group autoscaler is linked.\n  clusterRef:\n    name: cluster\n    namespace: clusters\n  # defines the id of the NodeConfig contained in NifiCluster.Spec.NodeConfigGroups\n  nodeConfigGroupId: auto_scaling\n  # readOnlyConfig can be used to pass Nifi node config\n  # which has type read-only these config changes will trigger rolling upgrade\n  readOnlyConfig:\n    nifiProperties:\n      overrideConfigs: |\n        nifi.ui.banner.text=NiFiKop - Scale Group\n  # This is an example of a node config you can apply to each replica in this node group.\n  # Any settings here will override those in the configured nodeConfigGroupId\n#  nodeConfig:\n#    nodeSelector:\n#      node_type: high-mem\n  # The selector used to identify nodes in NifiCluster.Spec.Nodes this autoscaler will manage\n  # Use Node.Labels in combination with this selector to clearly define which nodes will be managed by this autoscaler \n  nodeLabelsSelector: \n    matchLabels:\n      nifi_cr: cluster\n      nifi_node_group: auto-scaling\n  # the strategy used to decide how to add nodes to a nifi cluster\n  upscaleStrategy: simple\n  # the strategy used to decide how to remove nodes from an existing cluster\n  downscaleStrategy: lifo\n```\n\n----------------------------------------\n\nTITLE: Overriding nifi.properties Configuration Methods in NiFiCluster (YAML)\nDESCRIPTION: Demonstrates the structure within the NiFiCluster resource for overriding `nifi.properties`. It shows how to reference Kubernetes ConfigMaps (`overrideConfigMap`) and Secrets (`overrideSecretConfig`) containing configuration files, and how to provide inline configuration overrides (`overrideConfigs`). The example illustrates the configuration fields and mentions the priority order: Secret > ConfigMap > Override > Default.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n nifiProperties:\n      # Additionnal nifi.properties configuration that will override the one produced based on template and\n      # configuration\n      overrideConfigMap:\n        # The key of the value,in data content, that we want use.\n        data: nifi.properties\n        # Name of the configmap that we want to refer.\n        name: raw\n        # Namespace where is located the secret that we want to refer.\n        namespace: nifikop\n      # Additionnal nifi.properties configuration that will override the one produced based\n      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n      overrideSecretConfig:\n        # The key of the value,in data content, that we want use.\n        data: nifi.properties\n        # Name of the configmap that we want to refer.\n        name: raw\n        # Namespace where is located the secret that we want to refer.\n        namespace: nifikop\n      # Additionnal nifi.properties configuration that will override the one produced based\n      #\ton template, configurations and overrideConfigMap\n      overrideConfigs: |\n        nifi.ui.banner.text=NiFiKop\n        nifi.sensitive.props.key=thisIsABadSensitiveKeyPassword\n```\n\n----------------------------------------\n\nTITLE: Defining NifiUser Custom Resource in YAML\nDESCRIPTION: This YAML snippet defines a NifiUser Kubernetes Custom Resource (CR) for use with the NiFi Operator (nifikop). It specifies the API version, resource kind, metadata, and the spec section that includes the user's identity (email), references to an associated cluster, and certificate creation options. Required dependencies include a running Kubernetes cluster with nifikop installed, configured NiFiOperator CRDs, and an existing referenced NifiCluster. The input is a YAML definition applied via kubectl or a similar controller to provision a user; it outputs a user that is tracked as a Kubernetes resource. The clusterRef fields (name and namespace) must match existing NiFi clusters, and createCert controls whether a certificate is generated (here, set to false). Optional parameters like DNSNames or secretName are omitted in this example.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/5_references/2_nifi_user.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiUser\nmetadata:\n  name: aguitton\nspec:\n  identity: alexandre.guitton@konpyutaika.com\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  createCert: false\n```\n\n----------------------------------------\n\nTITLE: Creating a NifiConnection between dataflows in NiFiKop with YAML\nDESCRIPTION: Example YAML configuration for a NifiConnection resource that connects the 'input' and 'output' dataflows. It includes connection configuration for backpressure thresholds, expiration, and bend positions for visual representation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/4_manage_connections/1_deploy_connection.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiConnection\nmetadata:\n  name: connection\n  namespace: nifikop\nspec:\n  source:\n    name: input\n    namespace: nifikop\n    subName: output\n    type: dataflow\n  destination:\n    name: output\n    namespace: nifikop\n    subName: input\n    type: dataflow\n  configuration:\n    backPressureDataSizeThreshold: 100 GB\n    backPressureObjectThreshold: 10000\n    flowFileExpiration: 1 hour\n    labelIndex: 0\n    bends:\n      - posX: 550\n        posY: 550\n      - posX: 550\n        posY: 440\n      - posX: 550\n        posY: 88\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Pod Ports Configuration in YAML\nDESCRIPTION: The resulting pod configuration showing how internal listeners are translated into container ports. Each port is defined with its name, container port number, and protocol (TCP).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n    ports:\n    - containerPort: 8443\n      name: https\n      protocol: TCP\n    - containerPort: 6007\n      name: cluster\n      protocol: TCP\n    - containerPort: 6342\n      name: load-balance\n      protocol: TCP\n    - containerPort: 10000\n      name: s2s\n      protocol: TCP\n    - containerPort: 9090\n      name: prometheus\n      protocol: TCP\n```\n\n----------------------------------------\n\nTITLE: Example NifiDataflow Resource Definition (YAML)\nDESCRIPTION: This YAML manifest defines a `NifiDataflow` custom resource for Kubernetes. It specifies the desired state for a NiFi dataflow, including its source in NiFi Registry (bucket, flow ID, version), target parent process group, synchronization behavior, update strategy, and references to associated `NifiCluster`, `NifiRegistryClient`, and `NifiParameterContext` resources within the `nifikop` namespace. This resource allows declarative management of a specific NiFi dataflow deployment through the nifikop operator.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/5_references/5_nifi_dataflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  flowPosition:\n    posX: 0\n    posY: 0\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: squidflow\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: nifikop\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Configuring Apache NiFi Node in Kubernetes - YAML\nDESCRIPTION: This YAML snippet defines the NodeConfig for an Apache NiFi node managed by the NiFi Kubernetes operator. It specifies resource limits like provenance storage size, security parameters such as runAsUser, node-specific flags like isNode, and Kubernetes pod config including pod metadata annotations and labels, image settings, pull policies, and priority classes. It also configures volume mounts through externalVolumeConfigs and storageConfigs, detailing PersistentVolumeClaims with access modes and storage class configurations. This snippet requires Kubernetes cluster setup and NiFi operator deployment. It expects the NiFi image and storage PVCs to be properly configured for persistent data handling. Limitations include adherence to Kubernetes storage provisioning and compatible NiFi Docker image versions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/5_references/1_nifi_cluster/3_node_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n   default_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      # Set this to true if the instance is a node in a cluster.\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#basic-cluster-setup\n      isNode: true\n      # Additionnal metadata to merge to the pod associated\n      podMetadata:\n        annotations:\n          node-annotation: \"node-annotation-value\"\n        labels:\n          node-label: \"node-label-value\"\n      # Docker image used by the operator to create the node associated\n      # https://hub.docker.com/r/apache/nifi/\n#      image: \"apache/nifi:1.11.2\"\n      # nodeAffinity can be specified, operator populates this value if new pvc added later to node\n      # https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity\n#      nodeAffinity:\n      # imagePullPolicy define the pull policy for NiFi cluster docker image\n      imagePullPolicy: IfNotPresent\n      # priorityClassName define the name of the priority class to be applied to these nodes\n      priorityClassName: \"example-priority-class-name\"\n      # externalVolumeConfigs specifies a list of volume to mount into the main container.\n      externalVolumeConfigs:\n        - name: example-volume\n          mountPath: \"/opt/nifi/example\"\n          secret:\n            secretName: \"raw-controller\"\n      # storageConfigs specifies the node related configs\n      storageConfigs:\n        # Name of the storage config, used to name PV to reuse into sidecars for example.\n        - name: provenance-repository\n          # Path where the volume will be mount into the main nifi container inside the pod.\n          mountPath: \"/opt/nifi/provenance_repository\"\n          # Kubernetes PVC spec\n          # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Nodes in YAML\nDESCRIPTION: Example YAML configuration for multiple NiFi nodes in a cluster. Shows how to configure nodes with different IDs, node config groups, read-only configurations for properties like UI banner text, and node-specific configurations like resource requirements and storage configurations.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/5_references/1_nifi_cluster/4_node.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n    - id: 0\n      # nodeConfigGroup can be used to ease the node configuration, if set only the id is required\n      nodeConfigGroup: \"default_group\"\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        nifiProperties:\n          overrideConfigs: |\n            nifi.ui.banner.text=NiFiKop - Node 0\n      # node configuration\n#       nodeConfig:\n    - id: 2\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        overrideConfigs: |\n          nifi.ui.banner.text=NiFiKop - Node 2\n      # node configuration\n      nodeConfig:\n        resourcesRequirements:\n          limits:\n            cpu: \"2\"\n            memory: 3Gi\n          requests:\n            cpu: \"1\"\n            memory: 1Gi\n        storageConfigs:\n          # Name of the storage config, used to name PV to reuse into sidecars for example.\n          - name: provenance-repository\n            # Path where the volume will be mount into the main nifi container inside the pod.\n            mountPath: \"/opt/nifi/provenance_repository\"\n            # Kubernetes PVC spec\n            # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n            pvcSpec:\n              accessModes:\n                - ReadWriteOnce\n              storageClassName: \"standard\"\n              resources:\n                requests:\n                  storage: 8Gi\n```\n\n----------------------------------------\n\nTITLE: Accessing Prometheus via Port Forwarding\nDESCRIPTION: Command to set up port forwarding to access the Prometheus web interface from localhost for verifying metrics collection from NiFi.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_12\n\nLANGUAGE: console\nCODE:\n```\nkubectl port-forward -n monitoring-system service/prometheus-operated 9090:9090\n```\n\n----------------------------------------\n\nTITLE: Defining Multiple Node Configurations in NiFiKop (YAML)\nDESCRIPTION: This YAML snippet illustrates how to define multiple node configuration groups such as 'default_group' and 'high_mem_group', specifying storage, user IDs, resource limits, and other pod settings for each group. These configurations allow tailored deployment of NiFi nodes with different performance and storage characteristics.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nnodeConfigGroups:\n  default_group:\n    provenanceStorage: \"10 GB\"\n    runAsUser: 1000\n    serviceAccountName: \"default\"\n    resourcesRequirements:\n      limits:\n        cpu: \"2\"\n        memory: 3Gi\n      requests:\n        cpu: \"1\"\n        memory: 3Gi\n  high_mem_group:\n    provenanceStorage: \"10 GB\"\n    runAsUser: 1000\n    serviceAccountName: \"default\"\n    resourcesRequirements:\n      limits:\n        cpu: \"2\"\n        memory: 30Gi\n      requests:\n        cpu: \"1\"\n        memory: 30Gi\n```\n\n----------------------------------------\n\nTITLE: Overriding NiFi Properties Using Kubernetes ConfigMap, Secret, and Inline YAML in NiFi Operator\nDESCRIPTION: This snippet shows how to override NiFi configuration using three sources: a ConfigMap, a Secret, and inline override properties defined directly in the NiFiCluster YAML resource. It demonstrates the priority order where Secret values take precedence over ConfigMap and inline strings. Key fields include 'data' specifying the data key within the referenced Kubernetes object, 'name' for the resource name, and 'namespace' specifying the Kubernetes namespace. Inline properties are provided as a multi-line string with standard property format. This configuration enables secure and flexible management of sensitive properties like passwords alongside general overrides.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n nifiProperties:\n      # Additionnal nifi.properties configuration that will override the one produced based on template and\n      # configuration\n      overrideConfigMap:\n        # The key of the value,in data content, that we want use.\n        data: nifi.properties\n        # Name of the configmap that we want to refer.\n        name: raw\n        # Namespace where is located the secret that we want to refer.\n        namespace: nifikop\n      # Additionnal nifi.properties configuration that will override the one produced based\n      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n      overrideSecretConfig:\n        # The key of the value,in data content, that we want use.\n        data: nifi.properties\n        # Name of the configmap that we want to refer.\n        name: raw\n        # Namespace where is located the secret that we want to refer.\n        namespace: nifikop\n      # Additionnal nifi.properties configuration that will override the one produced based\n      #\ton template, configurations and overrideConfigMap\n      overrideConfigs: |\n        nifi.ui.banner.text=NiFiKop\n        nifi.sensitive.props.key=thisIsABadSensitiveKeyPassword\n```\n\n----------------------------------------\n\nTITLE: Storage Configuration for NiFi Data Persistence\nDESCRIPTION: YAML configuration for setting up persistent storage for NiFi data. Defines storage configurations for logs, data, extensions, and repositories with specific volume sizes and storage classes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nstorageConfigs:\n  - mountPath: /opt/nifi/nifi-current/logs\n    name: logs\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/data\n    name: data\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 50Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/extensions\n    name: extensions-repository\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 5Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/flowfile_repository\n    name: flowfile-repository\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/nifi-current/conf\n    name: conf\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 5Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/content_repository\n    name: content-repository\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 500Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/provenance_repository\n    name: provenance-repository\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 500Gi\n```\n\n----------------------------------------\n\nTITLE: Defining NiFiParameterContext Kubernetes CRD with Secrets in YAML\nDESCRIPTION: Creates a NiFiParameterContext custom resource which models NiFi parameter contexts for use in NiFi dataflows. It includes cluster references, descriptions, and a list of parameters with their values. It can also reference Kubernetes secrets to store sensitive parameters securely, which NiFiKop converts into NiFi sensitive parameters internally.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\n  namespace: demo\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      value: toto\n      description: toto\n```\n\n----------------------------------------\n\nTITLE: Overriding nifi.properties for OpenId Connect configuration in NiFiKop using YAML\nDESCRIPTION: This YAML snippet demonstrates how to override specific nifi.properties configurations within a NiFiKop NifiCluster custom resource definition (CRD). It covers setting OIDC-related properties such as discovery URL, client ID and secret, and includes security identity mapping settings to support multiple identity providers in Kubernetes-managed NiFi clusters. Required dependencies include NiFiKop and a configured OIDC provider. The snippet must be applied within the spec.readOnlyConfig.nifiProperties.overrideConfigs field and expects NiFi nodes to adopt these properties dynamically.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/5_users_authentication/1_oidc.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  readOnlyConfig:\n    # NifiProperties configuration that will be applied to the node.\n    nifiProperties:\n      webProxyHosts:\n        - nifistandard2.trycatchlearn.fr:8443\n      # Additionnal nifi.properties configuration that will override the one produced based\n      # on template and configurations.\n      overrideConfigs: |\n        nifi.security.user.oidc.discovery.url=<oidc server discovery url>\n        nifi.security.user.oidc.client.id=<oidc client\\'s id>\n        nifi.security.user.oidc.client.secret=<oidc client\\'s secret>\n        nifi.security.identity.mapping.pattern.dn=CN=([^,]*)(?:, (?:O|OU)=.*)?\n        nifi.security.identity.mapping.value.dn=$1\n        nifi.security.identity.mapping.transform.dn=NONE\n      ...\n   ...\n...\n```\n\n----------------------------------------\n\nTITLE: Creating a Cert-Manager Issuer for Let's Encrypt (YAML)\nDESCRIPTION: YAML manifest for creating a cert-manager `Issuer` resource. This issuer is configured to use the Let's Encrypt staging server via the ACME protocol with an HTTP01 solver, integrating with external-dns via ingress annotations. Requires replacing the placeholder email address and ensuring the referenced private key secret exists.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: cert-manager.io/v1alpha2\nkind: Issuer\nmetadata:\n  name: letsencrypt-staging\nspec:\n  acme:\n    # You must replace this email address with your own.\n    # Let's Encrypt will use this to contact you about expiring\n    # certificates, and issues related to your account.\n    email: <your email address>\n    server: https://acme-staging-v02.api.letsencrypt.org/directory\n    privateKeySecretRef:\n      # Secret resource used to store the account's private key.\n      name: example-issuer-account-key\n    # Add a single challenge solver, HTTP01 using nginx\n    solvers:\n      - http01:\n          ingress:\n            ingressTemplate:\n              metadata:\n                annotations:\n                  \"external-dns.alpha.kubernetes.io/ttl\": \"5\"\n```\n\n----------------------------------------\n\nTITLE: Configuring External Exposure of NiFi Cluster in Kubernetes (YAML)\nDESCRIPTION: Defines external service configuration for exposing NiFi cluster components outside Kubernetes, including service type, ports, and associated internal listeners. Helps facilitate access to NiFi UI and Prometheus metrics externally, enabling client interactions with NiFi over load balancers or other Kubernetes service types.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_4\n\nLANGUAGE: YAML\nCODE:\n```\nexternalServices:\n  - name: cluster-access\n    spec:\n      portConfigs:\n        - internalListenerName: https\n          port: 443\n        - internalListenerName: http-tracking\n          port: 80\n      type: LoadBalancer\n```\n\n----------------------------------------\n\nTITLE: Defining NifiCluster with New Node (YAML)\nDESCRIPTION: Defines the `NifiCluster` custom resource manifest for scaling up a NiFi cluster using NiFiKop. This snippet shows how to add a node with ID 25 to the existing list of desired nodes by including a new entry in the `spec.nodes` array, referencing a predefined `nodeConfigGroup`. Applying this YAML to the Kubernetes cluster instructs the operator to create the new node.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.12.1\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n    - id: 2\n      nodeConfigGroup: \"default_group\"\n# >>>> START: The new node\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n# <<<< END\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: Installing cert-manager with Helm 3\nDESCRIPTION: This code snippet provides the steps to install `cert-manager` using Helm 3.  It first installs the CustomResourceDefinitions, then adds the jetstack helm repository and updates it. Finally, it installs cert-manager from the jetstack chart.  Requires Helm 3 and a Kubernetes cluster.  The namespace `cert-manager` needs to be created beforehand.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/2_deploy_nifikop/1_quick_start.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Install CustomResourceDefinitions first\nkubectl apply --validate=false -f \\\n   https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.crds.yaml\n\n# Add the jetstack helm repo\nhelm repo add jetstack https://charts.jetstack.io\nhelm repo update\n\n# You have to create the namespace before executing following command\nhelm install cert-manager \\\n    --namespace cert-manager \\\n    --version v1.7.2 jetstack/cert-manager\n```\n\n----------------------------------------\n\nTITLE: Defining NifiDataflow Custom Resource (YAML)\nDESCRIPTION: This YAML snippet defines a NifiDataflow Kubernetes Custom Resource to manage an Apache NiFi dataflow lifecycle. It specifies crucial parameters such as the parentProcessGroupID, bucketId, flowId, flowVersion, flowPosition, and synchronization options. Dependencies include a configured NifiCluster, ParameterContext, and RegistryClient references. Inputs are standard Kubernetes API fields, and the resource controls dataflow deployment, synchronization, and update strategy. Requires the NifiKop CRDs installed in the cluster; constraints involve conforming to API field requirements and valid UUIDs for flow, bucket, and reference objects.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/5_references/5_nifi_dataflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  flowPosition:\n    posX: 0\n    posY: 0\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: squidflow\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: nifikop\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Node Group Autoscaler\nDESCRIPTION: This YAML configuration defines the `NifiNodeGroupAutoscaler` custom resource, specifying the autoscaling configuration for the NiFi cluster. It links to a specific `NifiCluster`, selects the `auto_scaling` node group, and configures upscale and downscale strategies.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiNodeGroupAutoscaler\nmetadata:\n  name: nifinodegroupautoscaler-sample\n  namespace: clusters\nspec:\n  # contains the reference to the NifiCluster with the one the node group autoscaler is linked.\n  clusterRef:\n    name: cluster\n    namespace: clusters\n  # defines the id of the NodeConfig contained in NifiCluster.Spec.NodeConfigGroups\n  nodeConfigGroupId: auto_scaling\n  # readOnlyConfig can be used to pass Nifi node config\n  # which has type read-only these config changes will trigger rolling upgrade\n  readOnlyConfig:\n    nifiProperties:\n      overrideConfigs: |\n        nifi.ui.banner.text=NiFiKop - Scale Group\n  # This is an example of a node config you can apply to each replica in this node group.\n  # Any settings here will override those in the configured nodeConfigGroupId\n#  nodeConfig:\n#    nodeSelector:\n#      node_type: high-mem\n  # The selector used to identify nodes in NifiCluster.Spec.Nodes this autoscaler will manage\n  # Use Node.Labels in combination with this selector to clearly define which nodes will be managed by this autoscaler \n  nodeLabelsSelector: \n    matchLabels:\n      nifi_cr: cluster\n      nifi_node_group: auto-scaling\n  # the strategy used to decide how to add nodes to a nifi cluster\n  upscaleStrategy: simple\n  # the strategy used to decide how to remove nodes from an existing cluster\n  downscaleStrategy: lifo\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Dataflows Deployment with YAML\nDESCRIPTION: This snippet shows how to define two NiFi dataflows ('input' and 'output') as Kubernetes Custom Resources (CRDs). It specifies attributes such as cluster reference, flow ID, registry reference, and positional coordinates for visualization. This setup is a prerequisite for establishing connections.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/4_manage_connections/1_deploy_connection.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiDataflow\nmetadata:\n  name: input\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  bucketId: deedb9f6-65a4-44e9-a1c9-722008fcd0ba\n  flowId: ab95431d-980d-41bd-904a-fac4bd864ba6\n  flowVersion: 1\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  skipInvalidComponent: true\n  skipInvalidControllerService: true\n  syncMode: always\n  updateStrategy: drain\n  flowPosition:\n    posX: 0\n    posY: 0\n---\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiDataflow\nmetadata:\n  name: output\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  bucketId: deedb9f6-65a4-44e9-a1c9-722008fcd0ba\n  flowId: fc5363eb-801e-432f-aa94-488838674d07\n  flowVersion: 2\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  skipInvalidComponent: true\n  skipInvalidControllerService: true\n  syncMode: always\n  updateStrategy: drain\n  flowPosition:\n    posX: 750\n    posY: 0\n```\n\n----------------------------------------\n\nTITLE: Deploying Prometheus ClusterRole, ClusterRoleBinding, and ServiceAccount - yaml\nDESCRIPTION: This YAML manifest deploys Kubernetes RBAC resources required for Prometheus to access cluster-wide metrics and resources. It creates a ServiceAccount \"prometheus\" in the monitoring-system namespace, a ClusterRole with permissions on nodes, services, endpoints, pods, configmaps, ingresses, and the /metrics non-resource endpoint, and binds them together via a ClusterRoleBinding. These resources are essential for Prometheus to scrape metrics from cluster components securely.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: prometheus\n  namespace: monitoring-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: prometheus\nrules:\n- apiGroups: [\"\"]\n  resources:\n  - nodes\n  - nodes/metrics\n  - services\n  - endpoints\n  - pods\n  verbs: [\"get\", \"list\", \"watch\"]\n- apiGroups: [\"\"]\n  resources:\n  - configmaps\n  verbs: [\"get\"]\n- apiGroups:\n  - networking.k8s.io\n  resources:\n  - ingresses\n  verbs: [\"get\", \"list\", \"watch\"]\n- nonResourceURLs: [\"/metrics\"]\n  verbs: [\"get\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: prometheus\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: prometheus\nsubjects:\n- kind: ServiceAccount\n  name: prometheus\n  namespace: monitoring-system\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Registry Client - YAML\nDESCRIPTION: This YAML snippet defines a `NifiRegistryClient` custom resource. It specifies the API version, kind, and metadata (name).  The `spec` section configures the client, including a reference to a NiFi cluster (`clusterRef`), a description, and the URI of the NiFi Registry.  The `clusterRef` references a NifiCluster, located in the specified namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/5_references/3_nifi_registry_client.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiRegistryClient\nmetadata:\n  name: squidflow\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Squidflow demo\"\n  uri: \"http://nifi-registry:18080\"\n```\n```\n\n----------------------------------------\n\nTITLE: Deploying NifiDataflow CRD in YAML\nDESCRIPTION: This YAML snippet defines a NifiDataflow resource with all necessary specifications including process group IDs, bucket and flow IDs, version, synchronization mode, cluster, registry client, and parameter context references. It facilitates automated deployment and management of NiFi dataflows.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: demo\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Configuring SSL Listeners in NiFiCluster with Nifikop - YAML\nDESCRIPTION: This YAML manifest demonstrates how to configure a NifiCluster resource for SSL using the Nifikop operator. It specifies https, cluster, and site-to-site (s2s) listeners, configures webProxyHosts, and sets the operator to create a Kubernetes TLS secret. Dependencies include a Kubernetes cluster, the Nifikop CRDs installed, and proper RBAC. Key parameters include webProxyHosts for proxy configuration and sslSecrets for SSL handling; expected output is a secured NiFi cluster with SSL-enabled endpoints.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  readOnlyConfig:\n    # NifiProperties configuration that will be applied to the node.\n    nifiProperties:\n      webProxyHosts:\n        - nifistandard2.trycatchlearn.fr:8443\n        ...\n  ...\n  listenersConfig:\n    internalListeners:\n      - type: \"https\"\n        name: \"https\"\n        containerPort: 8443\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi NodeGroup Autoscaler - YAML\nDESCRIPTION: This YAML snippet demonstrates the configuration of a `NifiNodeGroupAutoscaler` resource. It defines the API version, kind, metadata, and specification, including references to the `NifiCluster`, node configuration group, label selector, and scaling strategies. This configures the autoscaler to manage a specific node group within a NiFi cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/5_references/7_nifi_nodegroup_autoscaler.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiNodeGroupAutoscaler\nmetadata:\n  name: nifinodegroupautoscaler-sample\nspec:\n  # contains the reference to the NifiCluster with the one the node group autoscaler is linked.\n  clusterRef:\n    name: nificluster-name\n    namespace: nifikop\n  # defines the id of the NodeConfig contained in NifiCluster.Spec.NodeConfigGroups\n  nodeConfigGroupId: default-node-group\n  # The selector used to identify nodes in NifiCluster.Spec.Nodes this autoscaler will manage\n  # Use Node.Labels in combination with this selector to clearly define which nodes will be managed by this autoscaler \n  nodeLabelsSelector: \n    matchLabels:\n      nifi_cr: nificluster-name\n      nifi_node_group: default-node-group\n  # the strategy used to decide how to add nodes to a nifi cluster\n  upscaleStrategy: simple\n  # the strategy used to decide how to remove nodes from an existing cluster\n  downscaleStrategy: lifo\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFi Cluster Using kubectl\nDESCRIPTION: Launches a NiFi cluster in the 'nifi' namespace by applying a sample NiFiCluster Custom Resource manifest. It assumes that the manifest file 'config/samples/simplenificluster.yaml' is preconfigured with the Zookeeper service details and desired cluster parameters. This command expects kubectl access to the target Kubernetes cluster with namespace privileges.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create -n nifi -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Overriding NiFi Properties with ConfigMap, Secret, and Inline Override (YAML)\nDESCRIPTION: This YAML snippet demonstrates how to override Apache NiFi properties in a NiFiCluster Kubernetes Custom Resource by referencing a ConfigMap, a Secret, and inline configuration values. The configuration uses the fields 'overrideConfigMap', 'overrideSecretConfig', and 'overrideConfigs' to specify which data keys and resources should supply the configuration values, as well as their namespaces. Dependencies include the existence of the specified ConfigMap and Secret in the referenced namespace. The key parameters ('data', 'name', 'namespace', and inline configuration) determine the source and precedence of overrides. This approach enables fine-grained, secure, and auditable configuration management; the effective configuration is selected based on the ordering: Secret > ConfigMap > Override > Default. All entries must be correctly scoped in Kubernetes with the appropriate RBAC and resource definitions. The configuration is written in YAML and used as part of a NiFiCluster CRD.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nnifiProperties:\n      # Additionnal nifi.properties configuration that will override the one produced based on template and\n      # configuration\n      overrideConfigMap:\n        # The key of the value,in data content, that we want use.\n        data: nifi.properties\n        # Name of the configmap that we want to refer.\n        name: raw\n        # Namespace where is located the secret that we want to refer.\n        namespace: nifikop\n      # Additionnal nifi.properties configuration that will override the one produced based\n      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n      overrideSecretConfig:\n        # The key of the value,in data content, that we want use.\n        data: nifi.properties\n        # Name of the configmap that we want to refer.\n        name: raw\n        # Namespace where is located the secret that we want to refer.\n        namespace: nifikop\n      # Additionnal nifi.properties configuration that will override the one produced based\n      #\ton template, configurations and overrideConfigMap\n      overrideConfigs: |\n        nifi.ui.banner.text=NiFiKop\n        nifi.sensitive.props.key=thisIsABadSensitiveKeyPassword\n```\n\n----------------------------------------\n\nTITLE: Defining ServiceMonitor for NiFi (YAML)\nDESCRIPTION: This YAML manifest defines a `ServiceMonitor` custom resource, instructing the Prometheus instance how to discover and scrape metrics endpoints from the NiFi cluster. It selects NiFi services based on labels (`app: nifi`, `nifi_cr: cluster`) within the `clusters` namespace and specifies the target port (`prometheus`), path (`/metrics`), scrape interval, and relabeling rules to extract relevant labels from the target pods.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: cluster\n  namespace: monitoring-system\n  labels:\n    app: nifi-cluster\n    nifi_cr: cluster\nspec:\n  selector:\n    matchLabels:\n      app: nifi\n      nifi_cr: cluster\n  namespaceSelector:\n    matchNames:\n      - clusters\n  endpoints:\n    - interval: 10s\n      port: prometheus\n      path: /metrics\n      honorLabels: true\n      relabelings:\n        - sourceLabels: [__meta_kubernetes_pod_ip]\n          separator: ;\n          regex: (.*)\n          targetLabel: pod_ip\n          replacement: $1\n          action: replace\n        - sourceLabels: [__meta_kubernetes_pod_label_nodeId]\n          separator: ;\n          regex: (.*)\n          targetLabel: nodeId\n          replacement: $1\n          action: replace\n        - sourceLabels: [__meta_kubernetes_pod_label_nifi_cr]\n          separator: ;\n          regex: (.*)\n          targetLabel: nifi_cr\n          replacement: $1\n          action: replace\n```\n\n----------------------------------------\n\nTITLE: Defining an Inherited NiFi Parameter Context in Kubernetes (YAML)\nDESCRIPTION: This YAML manifest defines a `NifiParameterContext` named `dataflow-lifecycle-child` that inherits parameters from the `dataflow-lifecycle` context defined in the same namespace. It overrides the inherited `test` parameter with a new value and description. It also references the same NiFi cluster and Kubernetes secret as the parent context.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/5_references/4_nifi_parameter_context.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle-child\nspec:\n  description: \"It is a child test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  inheritedParameterContexts:\n    - name: dataflow-lifecycle\n  parameters:\n    - name: test\n      value: toto-child\n      description: tutu (child)\n```\n\n----------------------------------------\n\nTITLE: Defining a NiFi Cluster Custom Resource for Kubernetes Operator - YAML\nDESCRIPTION: This YAML snippet defines a NifiCluster custom resource for deploying and managing an Apache NiFi cluster using the konpyutaika operator on Kubernetes. All cluster specifications, including service types, pod metadata, node configuration, storage settings, Zookeeper integration, listeners, and external services, are provided as fields under the spec key. Required dependencies include a functional konpyutaika operator, Kubernetes cluster with persistent volume support, and an accessible Zookeeper ensemble; expected input is a valid NifiCluster YAML manifest while output is a set of managed NiFi resources within the Kubernetes environment. Limitations may include compatibility with specific NiFi or Kubernetes versions and the need for properly configured RBAC and storage classes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/5_references/1_nifi_cluster/1_nifi_cluster.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n    annotations:\n      tyty: ytyt\n    labels:\n      tete: titi  \n  pod:\n    annotations:\n      toto: tata\n    labels:\n      titi: tutu\n  zkAddress: 'zookeepercluster-client.zookeeper:2181'\n  zkPath: '/simplenifi'\n  clusterImage: 'apache/nifi:1.11.3'\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      podMetadata:\n        annotations:\n          node-annotation: \"node-annotation-value\"\n        labels:\n          node-label: \"node-label-value\"\n      externalVolumeConfigs:\n        - name: example-volume\n          mountPath: \"/opt/nifi/example\"\n          secret:\n            secretName: \"raw-controller\"\n      storageConfigs:\n        - mountPath: '/opt/nifi/nifi-current/logs'\n          name: logs\n          # Metadata to attach to the PVC that gets created\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: 'standard'\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: 'default'\n      resourcesRequirements:\n        limits:\n          cpu: '2'\n          memory: 3Gi\n        requests:\n          cpu: '1'\n          memory: 1Gi\n  nodes:\n    - id: 1\n      nodeConfigGroup: 'default_group'\n    - id: 2\n      nodeConfigGroup: 'default_group'\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: 'http'\n        name: 'http'\n        containerPort: 8080\n      - type: 'cluster'\n        name: 'cluster'\n        containerPort: 6007\n      - type: 's2s'\n        name: 's2s'\n        containerPort: 10000\n  externalServices:\n    - name: 'clusterip'\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8080\n            internalListenerName: 'http'\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop Operator Using Helm - Kubernetes - Bash\nDESCRIPTION: Installs the NiFiKop operator leveraging a Helm chart, specifying the chart repository, version, resource requests and limits, and custom namespaces. Prerequisites include Helm 3, kubectl, and a created namespace. Inputs such as image.tag, memory/cpu allocation, and certManager.enabled (for unsecured clusters) should be tuned as needed. After execution, the NiFiKop operator will be running in the specified Kubernetes namespace, ready to manage NiFi clusters.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/2_deploy_nifikop/1_quick_start.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# You have to create the namespace before executing following command\nhelm install nifikop \\\n    oci://ghcr.io/konpyutaika/helm-charts/nifikop \\\n    --namespace=nifi \\\n    --version 1.11.4 \\\n    --set image.tag=v1.11.4-release \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set namespaces={\"nifi\"}\n```\n\n----------------------------------------\n\nTITLE: Setting the Custom Database Authorizer in NiFi Properties\nDESCRIPTION: This shell command shows how to configure the NiFi property to use the custom database authorizer instead of the default managed authorizer. This property must be set in the NiFi configuration to activate the custom authorizer.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/6_users_authorization/1_custom_user_authorizer.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nnifi.security.user.authorizer=custom-database-authorizer\n```\n\n----------------------------------------\n\nTITLE: Defining NifiUser Resource in YAML\nDESCRIPTION: This YAML snippet defines a NifiUser custom resource for nifikop. It specifies the user's identity within NiFi using the `identity` field, references the target NifiCluster via `clusterRef`, configures certificate and JKS creation options (`createCert`, `includeJKS`), and lists access policies (`accessPolicies`) defining permissions like resource type, action, and target component.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/2_manage_users_and_accesses/1_users_management.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: aguitton\nspec:\n  # identity field is use to define the user identity on NiFi cluster side,\n  #\tit use full when the user's name doesn't suite with Kubernetes resource name.\n  identity: alexandre.guitton@konpyutaika.com\n  # Contains the reference to the NifiCluster with the one the registry client is linked.\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  # Whether or not the the operator also include a Java keystore format (JKS) with you secret\n  includeJKS: false\n  # Whether or not a certificate will be created for this user.\n  createCert: false\n  # defines the list of access policies that will be granted to the group.\n  accessPolicies:\n      # defines the kind of access policy, could be \"global\" or \"component\".\n    - type: component\n      # defines the kind of action that will be granted, could be \"read\" or \"write\"\n      action: read\n      # resource defines the kind of resource targeted by this access policies, please refer to the following page:\n      #\thttps://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#access-policies\n      resource: /data\n      # componentType is used if the type is \"component\", it's allow to define the kind of component on which is the\n      # access policy\n      componentType: \"process-groups\"\n      # componentId is used if the type is \"component\", it's allow to define the id of the component on which is the\n      # access policy\n      componentId: \"\"\n\n```\n\n----------------------------------------\n\nTITLE: NiFi Cluster Configuration with NodeConfigGroup for Autoscaling - yaml\nDESCRIPTION: YAML snippet specifying the NiFi cluster deployment configuration. It defines internal listeners including a Prometheus listener for metrics exposure, sets resource limits and requests for autoscaling nodes in the 'auto_scaling' NodeConfigGroup, and configures multiple persistent volume claims with labels and annotations. This config prepares NiFi nodes for autoscaling and Prometheus monitoring integration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n...\nspec:\n  ...\n  listenersConfig:\n    internalListeners:\n    ...\n    - containerPort: 9090\n      name: prometheus\n      type: prometheus \n    ...\n  ...\n  nodeConfigGroups:\n    auto_scaling:\n      isNode: true\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 2Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n      serviceAccountName: external-dns\n      storageConfigs:\n        - mountPath: /opt/nifi/nifi-current/logs\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/data\n          name: data\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/extensions\n          name: extensions-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/flowfile_repository\n          name: flowfile-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/nifi-current/conf\n          name: conf\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/content_repository\n          name: content-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/provenance_repository\n          name: provenance-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n  ...\n```\n\n----------------------------------------\n\nTITLE: Deploy NiFi Cluster Autoscaler for Kubernetes\nDESCRIPTION: This YAML defines a NifiNodeGroupAutoscaler custom resource; it links to a NiFi cluster called 'cluster' and references the 'auto_scaling' NodeConfigGroup. The configuration specifies labels for node selection, autoscaling strategies (upscale and downscale), and optional read-only NiFi properties for UI branding. This resource manages dynamic scaling of NiFi nodes within the specified cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiNodeGroupAutoscaler\nmetadata:\n  name: nifinodegroupautoscaler-sample\n  namespace: clusters\nspec:\n  clusterRef:\n    name: cluster\n    namespace: clusters\n  nodeConfigGroupId: auto_scaling\n  readOnlyConfig:\n    nifiProperties:\n      overrideConfigs: |\n        nifi.ui.banner.text=NiFiKop - Scale Group\n  nodeLabelsSelector:\n    matchLabels:\n      nifi_cr: cluster\n      nifi_node_group: auto-scaling\n  upscaleStrategy: simple\n  downscaleStrategy: lifo\n```\n\n----------------------------------------\n\nTITLE: Overriding NiFi Properties Configuration using Kubernetes Resources - YAML\nDESCRIPTION: This YAML snippet demonstrates how to override NiFi properties in a NiFiKop-managed NiFiCluster by referencing external Kubernetes ConfigMaps and Secrets, as well as supplying direct inline configuration. Dependencies include a ConfigMap and Secret named 'raw' in the 'nifikop' namespace. The fields 'overrideConfigMap', 'overrideSecretConfig', and 'overrideConfigs' specify where to source the configuration, with the system giving precedence to the Secret over ConfigMap and inline config. The expected input is configuration values for nifi.properties, and the output is an overridden, merged configuration in the final NiFi deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\n nifiProperties:\n      # Additionnal nifi.properties configuration that will override the one produced based on template and\n      # configuration\n      overrideConfigMap:\n        # The key of the value,in data content, that we want use.\n        data: nifi.properties\n        # Name of the configmap that we want to refer.\n        name: raw\n        # Namespace where is located the secret that we want to refer.\n        namespace: nifikop\n      # Additionnal nifi.properties configuration that will override the one produced based\n      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n      overrideSecretConfig:\n        # The key of the value,in data content, that we want use.\n        data: nifi.properties\n        # Name of the configmap that we want to refer.\n        name: raw\n        # Namespace where is located the secret that we want to refer.\n        namespace: nifikop\n      # Additionnal nifi.properties configuration that will override the one produced based\n      #\ton template, configurations and overrideConfigMap\n      overrideConfigs: |\n        nifi.ui.banner.text=NiFiKop\n        nifi.sensitive.props.key=thisIsABadSensitiveKeyPassword\n```\n\n----------------------------------------\n\nTITLE: Defining ReadOnlyConfig for NiFiKOp Cluster - YAML\nDESCRIPTION: This YAML example demonstrates how to configure the ReadOnlyConfig object for cluster-wide, read-only NiFi settings managed by NiFiKOp. The snippet shows overriding default options for NiFi thread pools, logback and authorizer templates, nifi.properties, zookeeper, and bootstrap settings by referencing ConfigMaps or Secrets, enabling operators to use externalized Kubernetes resources for environment-specific configurations. Inputs include references to ConfigMaps and Secrets, literal configuration overrides, and key NiFi cluster parameters; outputs affect the active configuration in the cluster nodes. Dependencies: NiFiKOp CRD, Kubernetes ConfigMaps/Secrets, valid NiFi configuration files. Limitations: Some fields may be ignored in specific NiFiKOp versions, and invalid YAML formatting or missing references can result in configuration failures.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/5_references/1_nifi_cluster/2_read_only_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreadOnlyConfig:\n  maximumTimerDrivenThreadCount: 30\n  maximumEventDrivenThreadCount: 10\n  logbackConfig:\n    replaceConfigMap:\n      data: logback.xml\n      name: raw\n      namespace: nifikop\n    replaceSecretConfig:\n      data: logback.xml\n      name: raw\n      namespace: nifikop\n  authorizerConfig:\n    replaceTemplateConfigMap:\n      data: authorizers.xml\n      name: raw\n      namespace: nifikop\n    replaceTemplateSecretConfig:\n      data: authorizers.xml\n      name: raw\n      namespace: nifikop\n  nifiProperties:\n    overrideConfigMap:\n      data: nifi.properties\n      name: raw\n      namespace: nifikop.\n    overrideSecretConfig:\n      data: nifi.properties\n      name: raw\n      namespace: nifikop\n    overrideConfigs: |\n      nifi.ui.banner.text=NiFiKop\n    needClientAuth: false\n  zookeeperProperties:\n    overrideConfigs: |\n      initLimit=15\n      autopurge.purgeInterval=24\n      syncLimit=5\n      tickTime=2000\n      dataDir=./state/zookeeper\n      autopurge.snapRetainCount=30\n  bootstrapProperties:\n    nifiJvmMemory: \"512m\"\n    overrideConfigs: |\n      # java.arg.4=-Djava.net.preferIPv4Stack=true\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio DestinationRule for HTTPS NiFi with Sticky Sessions\nDESCRIPTION: This YAML defines a DestinationRule that redirects HTTP traffic to HTTPS by encrypting it before reaching the NiFi service. It also configures sticky sessions based on the __Secure-Authorization-Bearer cookie.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: DestinationRule\nmetadata:\n  name: nifi-dr\nspec:\n  host: <service-name>.<namespace>.svc.cluster.local\n  trafficPolicy:\n    tls:\n      mode: SIMPLE\n    loadBalancer:\n      consistentHash:\n        httpCookie:\n          name: __Secure-Authorization-Bearer\n          ttl: 0s\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiDataflow CRD with NiFiKop in YAML\nDESCRIPTION: Defines a NifiDataflow custom resource for managing an Apache NiFi versioned flow through the NiFiKop operator. This YAML specifies all required references (parent process group, bucket, flow, version, cluster, registry client, parameter context) and key options for synchronization and update strategies. Dependencies include previously created registry client and parameter context resources. Inputs are the referenced resource names and flow identifiers; the output is an automatically managed NiFi dataflow lifecycle according to spec. Ensure correct IDs as per NiFi Registry documentation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: demo\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiDataflow Resource (YAML)\nDESCRIPTION: This YAML snippet defines a `NifiDataflow` CR for NiFiKop. It specifies the details of a versioned dataflow to be deployed from a NiFi Registry, including the target parent process group (`parentProcessGroupID`), registry bucket (`bucketId`), flow identifier (`flowId`), and version (`flowVersion`). It also references the required `NifiRegistryClient` (`registryClientRef`), `NifiParameterContext` (`parameterContextRef`), and the target NiFi cluster (`clusterRef`), along with synchronization and update strategies (`syncMode`, `updateStrategy`).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: demo\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Configuring ReadOnly NiFi Cluster Options with YAML\nDESCRIPTION: This YAML snippet exemplifies a ReadOnlyConfig object for cluster-wide NiFi configuration, specifying default and override settings for core NiFi property files and operational parameters. Dependencies include the konpyutaika/nifikop Kubernetes operator for Apache NiFi and access to referenced Kubernetes ConfigMaps and Secrets. Key fields dictate thread allocations for processor types, config/secret references for logback.xml and authorizers.xml, override content for NiFi, Zookeeper, and bootstrap property files, JVM memory allocation, and per-node override support. Input is a structured YAML manifest; output is the application of the configuration to the NiFi cluster by the operator. Constraints: correctly referenced ConfigMaps/Secrets must exist; some sub-structures (especially for Zookeeper/bootstrap properties) are provided as commented examples and require manual un-commenting for use.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/5_references/1_nifi_cluster/2_read_only_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreadOnlyConfig:\n  # MaximumTimerDrivenThreadCount define the maximum number of threads for timer driven processors available to the system.\n  maximumTimerDrivenThreadCount: 30\n  # MaximumEventDrivenThreadCount define the maximum number of threads for event driven processors available to the system.\n  maximumEventDrivenThreadCount: 10\n  # Logback configuration that will be applied to the node\n  logbackConfig:\n    # logback.xml configuration that will replace the one produced based on template\n    replaceConfigMap:\n      # The key of the value,in data content, that we want use.\n      data: logback.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # logback.xml configuration that will replace the one produced based on template and overrideConfigMap\n    replaceSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: logback.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n  # Authorizer configuration that will be applied to the node\n  authorizerConfig:\n    # An authorizers.xml configuration template that will replace the default template seen in authorizers.go\n    replaceTemplateConfigMap:\n      # The key of the value, in data content, that we want use.\n      data: authorizers.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # An authorizers.xml configuration template that will replace the default template seen in authorizers.go and the replaceTemplateConfigMap\n    replaceTemplateSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: authorizers.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n  # NifiProperties configuration that will be applied to the node.\n  nifiProperties:\n    # Additionnal nifi.properties configuration that will override the one produced based on template and\n    # configuration\n    overrideConfigMap:\n      # The key of the value,in data content, that we want use.\n      data: nifi.properties\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop.\n    # Additionnal nifi.properties configuration that will override the one produced based\n    #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    overrideSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: nifi.properties\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # Additionnal nifi.properties configuration that will override the one produced based\n    #\ton template, configurations and overrideConfigMap\n    overrideConfigs: |\n      nifi.ui.banner.text=NiFiKop\n    # A comma separated list of allowed HTTP Host header values to consider when NiFi\n    # is running securely and will be receiving requests to a different host[:port] than it is bound to.\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#web-properties\n    #      webProxyHosts:\n    # Nifi security client auth\n    needClientAuth: false\n    # Indicates which of the configured authorizers in the authorizers.xml file to use\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#authorizer-configuration\n  #      authorizer:\n  # ZookeeperProperties configuration that will be applied to the node.\n  zookeeperProperties:\n    #      # Additionnal zookeeeper.properties configuration that will override the one produced based on template and\n    #      # configuration\n    #      overrideConfigMap:\n    #        # The key of the value,in data content, that we want use.\n    #        data: zookeeeper.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop.\n    #      # Additionnal zookeeeper.properties configuration that will override the one produced based\n    #      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    #      overrideSecretConfig:\n    #        # The key of the value,in data content, that we want use.\n    #        data: zookeeeper.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop\n    # Additionnal zookeeper.properties configuration that will override the one produced based\n    # on template and configurations.\n    overrideConfigs: |\n      initLimit=15\n      autopurge.purgeInterval=24\n      syncLimit=5\n      tickTime=2000\n      dataDir=./state/zookeeper\n      autopurge.snapRetainCount=30\n  # BootstrapProperties configuration that will be applied to the node.\n  bootstrapProperties:\n    #      # Additionnal bootstrap.properties configuration that will override the one produced based on template and\n    #      # configuration\n    #      overrideConfigMap:\n    #        # The key of the value,in data content, that we want use.\n    #        data: bootstrap.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop.\n    #      # Additionnal bootstrap.properties configuration that will override the one produced based\n    #      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    #      overrideSecretConfig:\n    #        # The key of the value,in data content, that we want use.\n    #        data: bootstrap.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop\n    # JVM memory settings\n    nifiJvmMemory: \"512m\"\n    # Additionnal bootstrap.properties configuration that will override the one produced based\n    # on template and configurations.\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#bootstrap_properties\n    overrideConfigs: |\n      # java.arg.4=-Djava.net.preferIPv4Stack=true\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Cluster with Prometheus and Storage - Kubernetes Custom Resource - yaml\nDESCRIPTION: Defines a NiFi cluster spec with Prometheus listener and multiple persistent storage mounts using YAML for Kubernetes custom resources. Requires the NiFi operator (NiFiKop), and is intended for setting up storage and Prometheus-based monitoring for each replica. Important parameters include internalListeners for Prometheus (port 9090) and nodeConfigGroups which detail storage, resource requirements, and annotations. Input: customized YAML; output: persistent volume claims and appropriate pod configuration for the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n...\nspec:\n  ...\n  listenersConfig:\n    internalListeners:\n    ...\n    - containerPort: 9090\n      name: prometheus\n      type: prometheus \n    ...\n  ...\n  nodeConfigGroups:\n    auto_scaling:\n      isNode: true\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 2Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n      serviceAccountName: external-dns\n      storageConfigs:\n        - mountPath: /opt/nifi/nifi-current/logs\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/data\n          name: data\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/extensions\n          name: extensions-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/flowfile_repository\n          name: flowfile-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/nifi-current/conf\n          name: conf\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/content_repository\n          name: content-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/provenance_repository\n          name: provenance-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n  ...\n```\n\n----------------------------------------\n\nTITLE: Monitoring Deployment Status with kubectl get pods (Console)\nDESCRIPTION: This console snippet checks the status of pods in the nifikop namespace. Key function is monitoring the progress of NiFiKop-related components, including cert-manager and solver pods. Dependency: the nifikop namespace must exist. Inputs: none, or specific namespace. Outputs shown are pod names, status, restart count, and uptime. Use to verify pod health and resource readiness post-deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/blog/2020-06-30-secured_nifi_cluster_on_gcp_with_external_dns.md#_snippet_17\n\nLANGUAGE: console\nCODE:\n```\nkubectl get pods -n nifikop\nNAME                            READY   STATUS    RESTARTS   AGE\ncm-acme-http-solver-4fg5b       1/1     Running   0          18s\ncm-acme-http-solver-6sw9x       1/1     Running   0          20s\ncm-acme-http-solver-bpzvm       1/1     Running   0          20s\ncm-acme-http-solver-f8xvs       1/1     Running   0          19s\ncm-acme-http-solver-k997c       1/1     Running   0          17s\ncm-acme-http-solver-l5fzz       1/1     Running   0          18s\nexternal-dns-569bf79b57-hjmtt   1/1     Running   0          9h\nnifikop-56cb587d96-p8vdf        1/1     Running   0          29s\n```\n\n----------------------------------------\n\nTITLE: Adding Prometheus Helm Repository - Console\nDESCRIPTION: Adds the Prometheus community Helm repository to Helm's list of available chart sources. This step is necessary before deploying the Prometheus Operator using Helm, ensuring access to official charts. The repository URL is specified as a required parameter.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nhelm repo add prometheus https://prometheus-community.github.io/helm-charts\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Nifi Listeners and SSL Secrets in Nifikop YAML\nDESCRIPTION: This YAML snippet illustrates how to configure the `listenersConfig` field within a Nifikop custom resource definition for a Nifi cluster. It defines multiple `internalListeners` with specified types, names, and container ports (e.g., HTTPS, cluster, S2S, Prometheus, load-balance). It also shows how to configure `sslSecrets`, including specifying the secret name (`tlsSecretName`) and enabling automatic creation (`create: true`). This configuration dictates how Nifi components communicate internally and handle secure connections.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/5_references/1_nifi_cluster/6_listeners_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  listenersConfig:\n    internalListeners:\n      - type: \"https\"\n        name: \"https\"\n        containerPort: 8443\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n      - type: \"prometheus\"\n        name: \"prometheus\"\n        containerPort: 9090\n      - type: \"load-balance\"\n        name: \"load-balance\"\n        containerPort: 6342\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Cluster with SSL (YAML)\nDESCRIPTION: This YAML snippet defines a `NifiCluster` custom resource for securing the NiFi cluster with SSL.  It configures listeners and specifies SSL settings, including the use of a pre-existing secret or the creation of new certificates. It relies on the NiFi operator and Kubernetes CRDs. Key parameters include `readOnlyConfig.nifiProperties.webProxyHosts`, `listenersConfig.sslSecrets.tlsSecretName`, and `listenersConfig.sslSecrets.create`. The output is a configured NiFi cluster with SSL enabled.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  readOnlyConfig:\n    # NifiProperties configuration that will be applied to the node.\n    nifiProperties:\n      webProxyHosts:\n        - nifistandard2.trycatchlearn.fr:8443\n        ...\n  ...\n  listenersConfig:\n    internalListeners:\n      - type: \"https\"\n        name: \"https\"\n        containerPort: 8443\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n```\n\n----------------------------------------\n\nTITLE: Defining a NiFi Connection Kubernetes Resource (YAML)\nDESCRIPTION: This snippet provides a YAML example demonstrating how to define a `NifiConnection` custom resource in Kubernetes. It specifies the API version, kind, metadata, and the desired state (`spec`), including the source and destination components (referencing `NifiDataflow` resources), various configuration options like back pressure, load balancing, prioritizers, and visual bend points, and the connection update strategy.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/5_references/8_nifi_connection.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiConnection\nmetadata:\n  name: connection\n  namespace: instances\nspec:\n  source:\n    name: input\n    namespace: instances\n    subName: output_1\n    type: dataflow\n  destination:\n    name: output\n    namespace: instances\n    subName: input_1\n    type: dataflow\n  configuration:\n    flowFileExpiration: 1 hour\n    backPressureDataSizeThreshold: 100 GB\n    backPressureObjectThreshold: 10000\n    loadBalanceStrategy: PARTITION_BY_ATTRIBUTE\n    loadBalancePartitionAttribute: partition_attribute\n    loadBalanceCompression: DO_NOT_COMPRESS\n    prioritizers: \n      - NewestFlowFileFirstPrioritizer\n      - FirstInFirstOutPrioritizer\n    labelIndex: 0\n    bends:\n      - posX: 550\n        posY: 550\n      - posX: 550\n        posY: 440\n      - posX: 550\n        posY: 88\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Setting Up NifiCluster with External DNS and Let's Encrypt - YAML\nDESCRIPTION: This YAML configures a NifiCluster resource to use external DNS and reference an existing cert-manager Issuer (letsencrypt-staging) for SSL certificate management. Key parameters include clusterSecure and siteToSiteSecure toggles, listenersConfig with clusterDomain and useExternalDNS enabled, and sslSecrets with issuerRef pointing to the external issuer. The result is a NifiCluster with automatic TLS certificate integration from Let's Encrypt via cert-manager. Limitations: DNS must be pre-configured and associated cert-manager resources must exist.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  clusterSecure: true\n  siteToSiteSecure: true\n  ...\n  listenersConfig:\n    clusterDomain: <DNS zone name>\n    useExternalDNS: true\n    ...\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n      issuerRef:\n        name: letsencrypt-staging\n        kind: Issuer\n```\n\n----------------------------------------\n\nTITLE: Deploying KEDA ScaledObject for NiFi Autoscaler (YAML)\nDESCRIPTION: This YAML snippet defines a Kubernetes ScaledObject resource for KEDA to autoscale NiFi nodes using Prometheus metrics, covering key fields such as min/max replicas, target references, triggers, and optional fallback/advanced configurations. Dependencies include an existing KEDA installation, a running Prometheus instance, and the NiFiNodeGroupAutoscaler resource in the target namespace. Adjustable parameters include pollingInterval, cooldownPeriod, replica counts, and trigger metadata (server address, metricName, and query), allowing robust and fine-grained scaling control of NiFi nodes based on Prometheus data.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: cluster\n  namespace: clusters\nspec:\n  scaleTargetRef:\n    apiVersion:    nifi.konpyutaika.com/v1alpha1     # Optional. Default: apps/v1\n    kind:          NifiNodeGroupAutoscaler           # Optional. Default: Deployment\n    name:          nifinodegroupautoscaler-sample    # Mandatory. Must be in the same namespace as the ScaledObject\n    envSourceContainerName: nifi                     # Optional. Default: .spec.template.spec.containers[0]\n  pollingInterval:  30                               # Optional. Default: 30 seconds\n  cooldownPeriod:   300                              # Optional. Default: 300 seconds\n  idleReplicaCount: 0                                # Optional. Default: ignored, must be less than minReplicaCount \n  minReplicaCount:  1                                # Optional. Default: 0\n  maxReplicaCount:  3                                # Optional. Default: 100\n  fallback:                                          # Optional. Section to specify fallback options\n    failureThreshold: 5                              # Mandatory if fallback section is included\n    replicas: 1                                      # Mandatory if fallback section is included\n  # advanced:                                          # Optional. Section to specify advanced options\n  #   restoreToOriginalReplicaCount: true              # Optional. Whether the target resource should be scaled back to original replicas count, after the ScaledObject is deleted\n  #   horizontalPodAutoscalerConfig:                   # Optional. Section to specify HPA related options\n  #     name: {name-of-hpa-resource}                   # Optional. Default: keda-hpa-{scaled-object-name}\n  #     behavior:                                      # Optional. Use to modify HPA's scaling behavior\n  #       scaleDown:\n  #         stabilizationWindowSeconds: 300 <--- \n  #         policies:\n  #         - type: Percent\n  #           value: 100\n  #           periodSeconds: 15\n  triggers:\n    - type: prometheus\n      metadata:\n        serverAddress: http://prometheus-operated.monitoring-system.svc:9090\n        metricName: http_requests_total\n        threshold: <threshold value>\n        query: <prometheus query>\n\n```\n\n----------------------------------------\n\nTITLE: Configuring External Kubernetes Services for NiFi Cluster Exposure in YAML\nDESCRIPTION: Specifies how to expose internal NiFi listeners externally through Kubernetes services. This YAML snippet shows the definition of externalServices with service name, type (such as LoadBalancer), and portConfigs. Each portConfig maps an external service port to an internal listener by name. This configuration enables external traffic to reach specific NiFi internal listeners like HTTPS UI access and a custom HTTP tracking port, facilitating external user access and monitoring integration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    - type: \"https\"\n      name: \"https\"\n      containerPort: 8443\n    - type: \"cluster\"\n      name: \"cluster\"\n      containerPort: 6007\n    - type: \"s2s\"\n      name: \"s2s\"\n      containerPort: 10000\n    - type: \"prometheus\"\n      name: \"prometheus\"\n      containerPort: 9090\n    - type: \"load-balance\"\n      name: \"load-balance\"\n      containerPort: 6342\n    - name: \"http-tracking\"\n      containerPort: 8081\nexternalServices:\n  - name: cluster-access\n    spec:\n      portConfigs:\n        - internalListenerName: https\n          port: 443\n        - internalListenerName: http-tracking\n          port: 80\n      type: LoadBalancer\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple NiFi Repositories and Storage Directories (YAML)\nDESCRIPTION: This YAML snippet showcases an advanced NiFiCluster configuration for setting up multiple content and provenance repository directories to enhance storage performance. The configuration overrides are set under 'readOnlyConfig.nifiProperties.overrideConfigs', where each repository directory is defined with a unique path. Corresponding PersistentVolumeClaims are specified in 'nodeConfigGroups.default_group.storageConfigs', with each mountPath matching the NiFi property override. Dependencies include the Kubernetes storage backend and correct PVC provisioning. Inputs are repository directory names and storage size requests; outputs are NiFi nodes with attached persistent storage as defined. This pattern enables scalable, high-performance NiFi cluster setups while requiring correct configuration of both Kubernetes storage resources and NiFi properties. The snippet is formatted in YAML for use in a Kubernetes CRD.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n...\n  readOnlyConfig:\n    nifiProperties:\n      overrideConfigs: |\n        nifi.content.repository.directory.dir1=../content-additional/dir1\n        nifi.content.repository.directory.dir2=../content-additional/dir2\n        nifi.content.repository.directory.dir3=../content-additional/dir3\n        nifi.provenance.repository.directory.dir1=../provenance-additional/dir1\n        nifi.provenance.repository.directory.dir2=../provenance-additional/dir2\n...\n  nodeConfigGroups:\n    default_group:\n      ...\n      storageConfigs:\n      - mountPath: \"/opt/nifi/content-additional/dir1\"\n        name: content-repository-dir1\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/content-additional/dir2\"\n        name: content-repository-dir2\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/content-additional/dir3\"\n        name: content-repository-dir3\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/provenance-additional/dir1\"\n        name: provenance-repository-dir1\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/provenance-additional/dir2\"\n        name: provenance-repository-dir2\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      ...\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop - Set Parameters\nDESCRIPTION: This command installs the NiFiKop chart and sets the namespace parameter using Helm. It allows you to configure the chart during installation by specifying key-value pairs using the `--set` flag.  In this example, it sets the namespace to \"nifikop\".\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop --set namespaces={\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop Operator with Inline Parameters via Helm in Bash\nDESCRIPTION: This snippet demonstrates installing the NiFiKop operator Helm chart while setting namespaces inline through the --set parameter. This allows customizing the Helm chart configuration without a separate YAML file. Helm 3 or higher is required and assumes Kubernetes cluster setup with required CRDs applied.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop --set namespaces={\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring External Kubernetes Services for NiFi Access with YAML\nDESCRIPTION: This comprehensive YAML snippet shows how to define both internalListeners and externalServices to enable external access to NiFi cluster endpoints. For each internal listener (e.g., https, http-tracking), an associated external Kubernetes service (e.g., named cluster-access) is created with portConfigs mapping external service ports to internal listener names. The service type (LoadBalancer) determines exposure method. Dependencies include a proper NiFi cluster deployment and Kubernetes with support for LoadBalancer services. Required parameters are name, port, and internalListenerName. This configuration enables external users to reach the NiFi UI and specific processors over public IPs.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    - type: \"https\"\n      name: \"https\"\n      containerPort: 8443\n    - type: \"cluster\"\n      name: \"cluster\"\n      containerPort: 6007\n    - type: \"s2s\"\n      name: \"s2s\"\n      containerPort: 10000\n    - type: \"prometheus\"\n      name: \"prometheus\"\n      containerPort: 9090\n    - type: \"load-balance\"\n      name: \"load-balance\"\n      containerPort: 6342\n    - name: \"http-tracking\"\n      containerPort: 8081\nexternalServices:\n  - name: cluster-access\n    spec:\n      portConfigs:\n        - internalListenerName: https\n          port: 443\n        - internalListenerName: http-tracking\n          port: 80\n      type: LoadBalancer\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFiNodeGroup Autoscaler (YAML)\nDESCRIPTION: This YAML snippet defines the configuration for a NiFiNodeGroupAutoscaler, specifying parameters for automatic scaling of a NiFi cluster.  It includes a reference to the NiFiCluster, a node configuration group ID, node label selectors, and upscale/downscale strategies. The `clusterRef` specifies the target NiFiCluster, `nodeConfigGroupId` indicates the node group, `nodeLabelsSelector` refines node selection, and the `upscaleStrategy` and `downscaleStrategy` determine how nodes are added and removed. This example is a minimal configuration suitable for understanding the basic structure.  Further customization is achievable using the configurable strategies. No dependencies or prerequisites are explicitly called out but this configuration is dependent on a running Kubernetes cluster with the NiFiKop operator installed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/5_references/7_nifi_nodegroup_autoscaler.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiNodeGroupAutoscaler\nmetadata:\n  name: nifinodegroupautoscaler-sample\nspec:\n  # contains the reference to the NifiCluster with the one the node group autoscaler is linked.\n  clusterRef:\n    name: nificluster-name\n    namespace: nifikop\n  # defines the id of the NodeConfig contained in NifiCluster.Spec.NodeConfigGroups\n  nodeConfigGroupId: default-node-group\n  # The selector used to identify nodes in NifiCluster.Spec.Nodes this autoscaler will manage\n  # Use Node.Labels in combination with this selector to clearly define which nodes will be managed by this autoscaler \n  nodeLabelsSelector: \n    matchLabels:\n      nifi_cr: nificluster-name\n      nifi_node_group: default-node-group\n  # the strategy used to decide how to add nodes to a nifi cluster\n  upscaleStrategy: simple\n  # the strategy used to decide how to remove nodes from an existing cluster\n  downscaleStrategy: lifo\n```\n\n----------------------------------------\n\nTITLE: Configuring NifiUser Kubernetes Custom Resource for NiFi User Management - YAML\nDESCRIPTION: Defines a NifiUser resource in YAML format for managing a NiFi user on a cluster via the Kubernetes operator. The resource specifies the user's identity used on the NiFi cluster side, a reference to the target NiFi cluster, flags for including a Java keystore and creating a certificate, and a detailed list of access policies granting permissions on specific NiFi components. The operator will check for existing users with the same name and bind or create them accordingly. This example user is configured with an identity that differs from the Kubernetes resource name to comply with NiFi's naming flexibility, and is granted read access on process groups. This resource requires the Nifi K8s operator and a configured NiFi cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/2_manage_users_and_accesses/1_users_management.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: aguitton\nspec:\n  # identity field is use to define the user identity on NiFi cluster side,\n  #\tit use full when the user's name doesn't suite with Kubernetes resource name.\n  identity: alexandre.guitton@konpyutaika.com\n  # Contains the reference to the NifiCluster with the one the registry client is linked.\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  # Whether or not the the operator also include a Java keystore format (JKS) with you secret\n  includeJKS: false\n  # Whether or not a certificate will be created for this user.\n  createCert: false\n  # defines the list of access policies that will be granted to the group.\n  accessPolicies:\n      # defines the kind of access policy, could be \"global\" or \"component\".\n    - type: component\n      # defines the kind of action that will be granted, could be \"read\" or \"write\"\n      action: read\n      # resource defines the kind of resource targeted by this access policies, please refer to the following page:\n      #\thttps://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#access-policies\n      resource: /data\n      # componentType is used if the type is \"component\", it's allow to define the kind of component on which is the\n      # access policy\n      componentType: \"process-groups\"\n      # componentId is used if the type is \"component\", it's allow to define the id of the component on which is the\n      # access policy\n      componentId: \"\"\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiParameterContext inheriting from another context - YAML\nDESCRIPTION: This YAML snippet shows how to define a `NifiParameterContext` that inherits parameters from a parent context. It references the parent `NifiParameterContext` by name in `inheritedParameterContexts` and can define its own parameters, potentially overriding those from the parent, while also linking to the target cluster and potentially using secrets.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/5_references/4_nifi_parameter_context.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle-child\nspec:\n  description: \"It is a child test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  inheritedParameterContexts:\n    - name: dataflow-lifecycle\n  parameters:\n    - name: test\n      value: toto-child\n      description: tutu (child)\n```\n\n----------------------------------------\n\nTITLE: Patching CRDs with Webhook Conversion in Kubernetes YAML\nDESCRIPTION: This YAML snippet shows how to patch NifiKop-related CRDs to enable webhook conversion, facilitating migration from v1alpha1 to v1 API versions. It includes required annotations for cert-manager integration and details the webhook service configuration, referencing deployment-specific variables for dynamic setup. The key parameters include 'namespace', 'certificate_name', and 'webhook_service_name', and the output is a properly configured CRD that supports seamless resource version conversion; ensure cert-manager and the operator pod are properly configured in your cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/7_upgrade_guides/3_v0.16.0_to_v1.0.0.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Nifi Listeners with SSL in YAML\nDESCRIPTION: Example configuration for setting up various internal listeners and SSL secrets in a Nifi cluster using NifiKop. Includes configurations for HTTPS, cluster communication, site-to-site, Prometheus monitoring, and load balancing endpoints.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/5_references/1_nifi_cluster/6_listeners_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  listenersConfig:\n    internalListeners:\n      - type: \"https\"\n        name: \"https\"\n        containerPort: 8443\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n      - type: \"prometheus\"\n        name: \"prometheus\"\n        containerPort: 9090\n      - type: \"load-balance\"\n        name: \"load-balance\"\n        containerPort: 6342\n      - name: \"my-custom-listener-port\"\n        containerPort: 1234\n        protocol: \"TCP\"\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n```\n\n----------------------------------------\n\nTITLE: NiFiParameterContext Controller Reconcile Loop\nDESCRIPTION: This snippet depicts the reconcile loop for the NiFiParameterContext controller. It manages parameter context resources, ensuring parameters are correctly configured in NiFi, and handles sensitive data securely during the reconciliation process.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/3_manage_dataflows/0_design_principles.md#_snippet_4\n\nLANGUAGE: Markdown\nCODE:\n```\n![NiFi parameter context's reconcile loop](/img/1_concepts/2_design_principes/parameter_context_reconcile_loop.jpeg)\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Connection Custom Resource with Configuration - YAML\nDESCRIPTION: This YAML snippet defines a NiFiConnection custom resource for use with the nifikop Kubernetes operator, establishing a connection between source and destination components within a specified namespace. It demonstrates setting advanced parameters such as flow file expiration, backpressure thresholds, load balancing strategy, prioritizers, and connection bends for customized data routing. The resource requires the nifikop operator installed in the cluster and is intended as part of a GitOps or manual deployment workflow. Expected input is a valid Kubernetes YAML manifest, and upon creation, it will be processed by the operator to manage NiFi dataflow connections. Parameter constraints include required references for source and destination, valid enum values for strategies, and optional tuning for performance and data routing.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/5_references/8_nifi_connection.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiConnection\nmetadata:\n  name: connection\n  namespace: instances\nspec:\n  source:\n    name: input\n    namespace: instances\n    subName: output_1\n    type: dataflow\n  destination:\n    name: output\n    namespace: instances\n    subName: input_1\n    type: dataflow\n  configuration:\n    flowFileExpiration: 1 hour\n    backPressureDataSizeThreshold: 100 GB\n    backPressureObjectThreshold: 10000\n    loadBalanceStrategy: PARTITION_BY_ATTRIBUTE\n    loadBalancePartitionAttribute: partition_attribute\n    loadBalanceCompression: DO_NOT_COMPRESS\n    prioritizers: \n      - NewestFlowFileFirstPrioritizer\n      - FirstInFirstOutPrioritizer\n    labelIndex: 0\n    bends:\n      - posX: 550\n        posY: 550\n      - posX: 550\n        posY: 440\n      - posX: 550\n        posY: 88\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple Node Groups in NiFiKop\nDESCRIPTION: YAML configuration defining two different node configuration groups with varying resource requirements. This example defines a default_group with 3GB of memory and a high_mem_group with 30GB of memory for different node types.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  # nodeConfigGroups specifies multiple node configs with unique name\n  nodeConfigGroups:\n    default_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      serviceAccountName: \"default\"\n      # resourceRequirements works exactly like Container resources, the user can specify the limit and the requests\n      # through this property\n      # https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 3Gi\n    high_mem_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      serviceAccountName: \"default\"\n      # resourceRequirements works exactly like Container resources, the user can specify the limit and the requests\n      # through this property\n      # https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 30Gi\n        requests:\n          cpu: \"1\"\n          memory: 30Gi\n```\n\n----------------------------------------\n\nTITLE: Defining a KEDA ScaledObject YAML for NiFi Autoscaling\nDESCRIPTION: This YAML snippet defines a KEDA ScaledObject resource for scaling NiFi nodes based on Prometheus metrics. It specifies the target resource, scaling parameters, fallback options, and trigger configuration needed for autoscaling. Dependencies include Kubernetes with KEDA and Prometheus metrics server, with main parameters such as scaleTargetRef, pollingInterval, thresholds, and trigger details.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: cluster\n  namespace: clusters\nspec:\n  scaleTargetRef:\n    apiVersion:    nifi.konpyutaika.com/v1alpha1     # Optional. Default: apps/v1\n    kind:          NifiNodeGroupAutoscaler           # Optional. Default: Deployment\n    name:          nifinodegroupautoscaler-sample    # Mandatory. Must be in the same namespace as the ScaledObject\n    envSourceContainerName: nifi\n  pollingInterval:  30\n  cooldownPeriod:   300\n  idleReplicaCount: 0\n  minReplicaCount:  1\n  maxReplicaCount:  3\n  fallback:\n    failureThreshold: 5\n    replicas: 1\n  # advanced:\n  #   restoreToOriginalReplicaCount: true\n  #   horizontalPodAutoscalerConfig:\n  #     name: {name-of-hpa-resource}\n  #     behavior:\n  #       scaleDown:\n  #         stabilizationWindowSeconds: 300\n  #         policies:\n  #         - type: Percent\n  #           value: 100\n  #           periodSeconds: 15\n  triggers:\n    - type: prometheus\n      metadata:\n        serverAddress: http://prometheus-operated.monitoring-system.svc:9090\n        metricName: http_requests_total\n        threshold: <threshold value>\n        query: <prometheus query>\n```\n\n----------------------------------------\n\nTITLE: Example NiFi Node Configuration (YAML)\nDESCRIPTION: A sample YAML configuration for a default NiFi node group within a Kubernetes cluster. It defines settings like provenance storage size, user ID, pod annotations/labels, image policy, priority class, external volume mounts (from a secret), and persistent storage configurations (for provenance and logs) using PersistentVolumeClaims (PVCs). This configuration is used by the Kubernetes operator to manage NiFi nodes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/5_references/1_nifi_cluster/3_node_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n   default_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      # Set this to true if the instance is a node in a cluster.\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#basic-cluster-setup\n      isNode: true\n      # Additionnal metadata to merge to the pod associated\n      podMetadata:\n        annotations:\n          node-annotation: \"node-annotation-value\"\n        labels:\n          node-label: \"node-label-value\"\n      # Docker image used by the operator to create the node associated\n      # https://hub.docker.com/r/apache/nifi/\n#      image: \"apache/nifi:1.11.2\"\n      # nodeAffinity can be specified, operator populates this value if new pvc added later to node\n      # https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity\n#      nodeAffinity:\n      # imagePullPolicy define the pull policy for NiFi cluster docker image\n      imagePullPolicy: IfNotPresent\n      # priorityClassName define the name of the priority class to be applied to these nodes\n      priorityClassName: \"example-priority-class-name\"\n      # externalVolumeConfigs specifies a list of volume to mount into the main container.\n      externalVolumeConfigs:\n        - name: example-volume\n          mountPath: \"/opt/nifi/example\"\n          secret:\n            secretName: \"raw-controller\"\n      # storageConfigs specifies the node related configs\n      storageConfigs:\n        # Name of the storage config, used to name PV to reuse into sidecars for example.\n        - name: provenance-repository\n          # Retain this PVC throughout NifiCluster deletions.\n          reclaimPolicy: Retain\n          # Path where the volume will be mount into the main nifi container inside the pod.\n          mountPath: \"/opt/nifi/provenance_repository\"\n          # Metadata to attach to the PVC that gets created\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          # Kubernetes PVC spec\n          # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          reclaimPolicy: Delete\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Node Group Autoscaler (YAML)\nDESCRIPTION: YAML configuration for a NifiNodeGroupAutoscaler custom resource. This resource links a NiFi node group (specified by `nodeConfigGroupId` and `nodeLabelsSelector`) to KEDA, enabling autoscaling based on defined triggers. It specifies the target cluster, node group ID, node selector labels, and scaling strategies (upscale/downscale). Requires the NiFiKop operator and a configured NiFiCluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiNodeGroupAutoscaler\nmetadata:\n  name: nifinodegroupautoscaler-sample\n  namespace: clusters\nspec:\n  # contains the reference to the NifiCluster with the one the node group autoscaler is linked.\n  clusterRef:\n    name: cluster\n    namespace: clusters\n  # defines the id of the NodeConfig contained in NifiCluster.Spec.NodeConfigGroups\n  nodeConfigGroupId: auto_scaling\n  # readOnlyConfig can be used to pass Nifi node config\n  # which has type read-only these config changes will trigger rolling upgrade\n  readOnlyConfig:\n    nifiProperties:\n      overrideConfigs: |\n        nifi.ui.banner.text=NiFiKop - Scale Group\n  # This is an example of a node config you can apply to each replica in this node group.\n  # Any settings here will override those in the configured nodeConfigGroupId\n#  nodeConfig:\n#    nodeSelector:\n#      node_type: high-mem\n  # The selector used to identify nodes in NifiCluster.Spec.Nodes this autoscaler will manage\n  # Use Node.Labels in combination with this selector to clearly define which nodes will be managed by this autoscaler \n  nodeLabelsSelector: \n    matchLabels:\n      nifi_cr: cluster\n      nifi_node_group: auto-scaling\n  # the strategy used to decide how to add nodes to a nifi cluster\n  upscaleStrategy: simple\n  # the strategy used to decide how to remove nodes from an existing cluster\n  downscaleStrategy: lifo\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Read-Only Settings (YAML)\nDESCRIPTION: This YAML snippet demonstrates the structure of the `readOnlyConfig` object used by NiFiKOp to define cluster-wide read-only configurations for NiFi. It includes settings for thread limits (`maximumTimerDrivenThreadCount`), logback (`logbackConfig`), authorizers (`authorizerConfig`), and provides examples of overriding `nifi.properties`, `zookeeper.properties`, and `bootstrap.conf` using inline configurations (`overrideConfigs`) or external ConfigMaps/Secrets. These settings serve as defaults but can be overridden at the node level.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/5_references/1_nifi_cluster/2_read_only_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreadOnlyConfig:\n  # MaximumTimerDrivenThreadCount define the maximum number of threads for timer driven processors available to the system.\n  maximumTimerDrivenThreadCount: 30\n  # MaximumEventDrivenThreadCount define the maximum number of threads for event driven processors available to the system (@DEPRECATED. This has no effect from NiFiKOp v1.9.0 or later).\n  maximumEventDrivenThreadCount: 10\n  # Logback configuration that will be applied to the node\n  logbackConfig:\n    # logback.xml configuration that will replace the one produced based on template\n    replaceConfigMap:\n      # The key of the value,in data content, that we want use.\n      data: logback.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # logback.xml configuration that will replace the one produced based on template and overrideConfigMap\n    replaceSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: logback.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n  # Authorizer configuration that will be applied to the node\n  authorizerConfig:\n    # An authorizers.xml configuration template that will replace the default template seen in authorizers.go\n    replaceTemplateConfigMap:\n      # The key of the value, in data content, that we want use.\n      data: authorizers.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # An authorizers.xml configuration template that will replace the default template seen in authorizers.go and the replaceTemplateConfigMap\n    replaceTemplateSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: authorizers.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n  # NifiProperties configuration that will be applied to the node.\n  nifiProperties:\n    # Additionnal nifi.properties configuration that will override the one produced based on template and\n    # configuration\n    overrideConfigMap:\n      # The key of the value,in data content, that we want use.\n      data: nifi.properties\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop.\n    # Additionnal nifi.properties configuration that will override the one produced based\n    #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    overrideSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: nifi.properties\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # Additionnal nifi.properties configuration that will override the one produced based\n    #\ton template, configurations and overrideConfigMap\n    overrideConfigs: |\n      nifi.ui.banner.text=NiFiKop\n    # A comma separated list of allowed HTTP Host header values to consider when NiFi\n    # is running securely and will be receiving requests to a different host[:port] than it is bound to.\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#web-properties\n    #      webProxyHosts:\n    # Nifi security client auth\n    needClientAuth: false\n    # Indicates which of the configured authorizers in the authorizers.xml file to use\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#authorizer-configuration\n  #      authorizer:\n  # ZookeeperProperties configuration that will be applied to the node.\n  zookeeperProperties:\n    #      # Additionnal zookeeeper.properties configuration that will override the one produced based on template and\n    #      # configuration\n    #      overrideConfigMap:\n    #        # The key of the value,in data content, that we want use.\n    #        data: zookeeeper.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop.\n    #      # Additionnal zookeeeper.properties configuration that will override the one produced based\n    #      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    #      overrideSecretConfig:\n    #        # The key of the value,in data content, that we want use.\n    #        data: zookeeeper.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop\n    # Additionnal zookeeper.properties configuration that will override the one produced based\n    # on template and configurations.\n    overrideConfigs: |\n      initLimit=15\n      autopurge.purgeInterval=24\n      syncLimit=5\n      tickTime=2000\n      dataDir=./state/zookeeper\n      autopurge.snapRetainCount=30\n  # BootstrapProperties configuration that will be applied to the node.\n  bootstrapProperties:\n    #      # Additionnal bootstrap.conf configuration that will override the one produced based on template and\n    #      # configuration\n    #      overrideConfigMap:\n    #        # The key of the value,in data content, that we want use.\n    #        data: bootstrap.conf\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop.\n    #      # Additionnal bootstrap.conf configuration that will override the one produced based\n    #      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    #      overrideSecretConfig:\n    #        # The key of the value,in data content, that we want use.\n    #        data: bootstrap.conf\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop\n    # JVM memory settings\n    nifiJvmMemory: \"512m\"\n    # Additionnal bootstrap.conf configuration that will override the one produced based\n    # on template and configurations.\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#bootstrap_properties\n    overrideConfigs: |\n      # java.arg.4=-Djava.net.preferIPv4Stack=true\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiUser Resource in YAML\nDESCRIPTION: This YAML snippet defines a `NifiUser` resource. It specifies the user's identity, the target NiFi cluster, whether to include a Java keystore (JKS), whether to create a certificate, and the access policies to grant to the user. The `clusterRef` specifies the NiFi cluster the user belongs to, `identity` defines the username inside of NiFi, `includeJKS` and `createCert` control the generation of keystore and user certificate respectively, `accessPolicies` specifies user permissions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/2_manage_users_and_accesses/1_users_management.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: aguitton\nspec:\n  # identity field is use to define the user identity on NiFi cluster side,\n  #\tit use full when the user's name doesn't suite with Kubernetes resource name.\n  identity: alexandre.guitton@konpyutaika.com\n  # Contains the reference to the NifiCluster with the one the registry client is linked.\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  # Whether or not the the operator also include a Java keystore format (JKS) with you secret\n  includeJKS: false\n  # Whether or not a certificate will be created for this user.\n  createCert: false\n  # defines the list of access policies that will be granted to the group.\n  accessPolicies:\n      # defines the kind of access policy, could be \"global\" or \"component\".\n    - type: component\n      # defines the kind of action that will be granted, could be \"read\" or \"write\"\n      action: read\n      # resource defines the kind of resource targeted by this access policies, please refer to the following page:\n      #\thttps://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#access-policies\n      resource: /data\n      # componentType is used if the type is \"component\", it's allow to define the kind of component on which is the\n      # access policy\n      componentType: \"process-groups\"\n      # componentId is used if the type is \"component\", it's allow to define the id of the component on which is the\n      # access policy\n      componentId: \"\"\n```\n\n----------------------------------------\n\nTITLE: Configuring NifiCluster with External DNS and IssuerRef (YAML)\nDESCRIPTION: Example YAML configuration for a `NifiCluster` resource integrating with an external DNS zone and a cert-manager `Issuer`. It enables `clusterSecure` and `siteToSiteSecure`, sets `useExternalDNS` to true, specifies the `clusterDomain`, enables automatic certificate creation (`create: true`), and references the `letsencrypt-staging` Issuer via `issuerRef`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  clusterSecure: true\n  siteToSiteSecure: true\n  ...\n  listenersConfig:\n    clusterDomain: <DNS zone name>\n    useExternalDNS: true\n    ...\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n      issuerRef:\n        name: letsencrypt-staging\n        kind: Issuer\n```\n\n----------------------------------------\n\nTITLE: Configuring Multi-Directory Content and Provenance Storage in NiFiCluster (YAML)\nDESCRIPTION: This YAML example demonstrates configuring multiple content and provenance repository directories in NiFi by setting the nifiProperties.overrideConfigs and corresponding storageConfigs for the nodeConfigGroups. This supports high performance installations by distributing repositories across multiple mount points, each backed by its own PersistentVolumeClaim. Prerequisites include a valid NiFiCluster resource, storage classes, and nifikop operator. Key inputs are the directory keys/paths and PVC specifications; outputs are NiFi nodes with the specified directory mounts. The storageConfigs must match the directories defined in the overrideConfigs for proper operation. Limitations include the need for consistent naming/mapping and resource availability.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\n...\n  readOnlyConfig:\n    nifiProperties:\n      overrideConfigs: |\n        nifi.content.repository.directory.dir1=../content-additional/dir1\n        nifi.content.repository.directory.dir2=../content-additional/dir2\n        nifi.content.repository.directory.dir3=../content-additional/dir3\n        nifi.provenance.repository.directory.dir1=../provenance-additional/dir1\n        nifi.provenance.repository.directory.dir2=../provenance-additional/dir2\n...\n  nodeConfigGroups:\n    default_group:\n      ...\n      storageConfigs:\n      - mountPath: \"/opt/nifi/content-additional/dir1\"\n        name: content-repository-dir1\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/content-additional/dir2\"\n        name: content-repository-dir2\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/content-additional/dir3\"\n        name: content-repository-dir3\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/provenance-additional/dir1\"\n        name: provenance-repository-dir1\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/provenance-additional/dir2\"\n        name: provenance-repository-dir2\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      ...\n\n```\n\n----------------------------------------\n\nTITLE: Configuring nifi.properties for OIDC identity mapping\nDESCRIPTION: This shell script snippet shows the configuration parameters added to nifi.properties to define how distinguished names from the identity provider are mapped for multiple IdP support. It specifies regex patterns and transformation rules for DN mapping.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/5_users_authentication/1_oidc.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nnifi.security.identity.mapping.pattern.dn=CN=([^,]*)(?:, (?:O|OU)=.*)?\n nifi.security.identity.mapping.value.dn=$1\n nifi.security.identity.mapping.transform.dn=NONE\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop Using Helm Charts - Bash\nDESCRIPTION: This snippet runs Helm install to deploy the NiFiKop operator using the specified Helm chart. Parameters set the image tag and target namespace. Helm v3.4.2+ and kubectl access to the cluster are prerequisites. Image repository and tag should match the ones defined during build and push processes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/6_contributing/1_developer_guide.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nhelm install skeleton ./helm/nifikop \\\n    --set image.tag=v0.5.1-release \\\n    --namespace-{\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Secret for Basic Authentication with kubectl CLI\nDESCRIPTION: Console command example for creating a Kubernetes secret containing credentials for basic authentication with an external NiFi cluster. The secret includes username, password, and optionally a CA certificate to trust the server's TLS certificate. This secret is referenced by the NifiCluster resource to allow the operator to authenticate its API requests against the NiFi cluster securely.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/1_manage_clusters/3_external_cluster.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nkubectl create secret generic nifikop-credentials \\\n  --from-file=username=./secrets/username\\\n  --from-file=password=./secrets/password\\\n  --from-file=ca.crt=./secrets/ca.crt\\\n  -n nifikop-nifi\n```\n\n----------------------------------------\n\nTITLE: Creating a Basic NiFi Parameter Context Resource (YAML)\nDESCRIPTION: This YAML manifest defines a `NifiParameterContext` Kubernetes resource named `dataflow-lifecycle`. It links to a NiFi cluster (`nc` in the `nifikop` namespace), references a Kubernetes secret (`secret-params`) for potentially sensitive parameters, and defines non-sensitive parameters directly within the `spec`. The `description` field provides metadata about the context's purpose.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/5_references/4_nifi_parameter_context.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      description: toto\n      sensistive: true\n```\n\n----------------------------------------\n\nTITLE: Defining NifiNodeGroupAutoscaler Custom Resource for Nifikop - YAML\nDESCRIPTION: This YAML snippet defines a NifiNodeGroupAutoscaler custom resource for Kubernetes, which is used to configure automatic scaling of NiFi node groups managed by Nifikop. It specifies the API version, resource kind, cluster and node group references, label selectors for targeting specific NiFi nodes, as well as the upscale and downscale strategies (e.g., 'simple' for adding and 'lifo' for removing nodes). Prerequisites include a running Kubernetes cluster with Nifikop installed and available NifiCluster and NodeConfigGroup resources. All required parameters such as clusterRef, nodeConfigGroupId, nodeLabelsSelector, and scaling strategies must be tailored to the target environment. The CRD will be reconciled by the Nifikop controller to manage NiFi node scaling. The initial number of replicas can be set, but is subsequently managed by Kubernetes HorizontalPodAutoscaler.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/5_references/7_nifi_nodegroup_autoscaler.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiNodeGroupAutoscaler\nmetadata:\n  name: nifinodegroupautoscaler-sample\nspec:\n  # contains the reference to the NifiCluster with the one the node group autoscaler is linked.\n  clusterRef:\n    name: nificluster-name\n    namespace: nifikop\n  # defines the id of the NodeConfig contained in NifiCluster.Spec.NodeConfigGroups\n  nodeConfigGroupId: default-node-group\n  # The selector used to identify nodes in NifiCluster.Spec.Nodes this autoscaler will manage\n  # Use Node.Labels in combination with this selector to clearly define which nodes will be managed by this autoscaler \n  nodeLabelsSelector: \n    matchLabels:\n      nifi_cr: nificluster-name\n      nifi_node_group: default-node-group\n  # the strategy used to decide how to add nodes to a nifi cluster\n  upscaleStrategy: simple\n  # the strategy used to decide how to remove nodes from an existing cluster\n  downscaleStrategy: lifo\n```\n\n----------------------------------------\n\nTITLE: Running the Migration Script via npm\nDESCRIPTION: Command to execute the migration script using npm, providing required command-line arguments to specify the NiFiKop resource type to migrate and the Kubernetes namespace where the resources exist. This command triggers the migration logic defined in the `index.js` script and should be run after stopping the old operator and ensuring both old and new CRDs are installed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nnpm start -- --type=<NIFIKOP_RESOURCE> --namespace=<K8S_NAMESPACE>\n```\n\n----------------------------------------\n\nTITLE: Pod Disruption Budget Configuration Schema\nDESCRIPTION: Defines the schema for creating and managing PodDisruptionBudgets (PDB) within Kubernetes to control pod disruptions. Includes parameters for create flag and budget type (static or percentage). No code implementation, just configuration schema with field descriptions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/5_references/1_nifi_cluster/1_nifi_cluster.md#_snippet_1\n\n\n\n----------------------------------------\n\nTITLE: Defining Basic and Child NiFi Parameter Contexts in YAML\nDESCRIPTION: Example YAML definitions for NifiParameterContext resources. The first example creates a basic parameter context with explicit parameters and secret references. The second example demonstrates a child parameter context that inherits from the first one and overrides some parameters.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/5_references/4_nifi_parameter_context.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      description: toto\n      sensistive: true\n---\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle-child\nspec:\n  description: \"It is a child test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  inheritedParameterContexts:\n    - name: dataflow-lifecycle\n  parameters:\n    - name: test\n      value: toto-child\n      description: tutu (child)\n```\n\n----------------------------------------\n\nTITLE: Configuring External Volumes in NiFiKop NodeConfigGroup\nDESCRIPTION: Provides guidance on mounting pre-existing volumes or external storage to NiFi nodes by specifying external volume configurations, facilitating integration with existing storage solutions outside operator management.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\n/* No specific code snippet provided in the text; reference to using StorageConfig with external volumes. */\n```\n\n----------------------------------------\n\nTITLE: Defining Nifikop NifiUserGroup Resource YAML\nDESCRIPTION: Shows how to define a Kubernetes resource for a NiFi user group managed by the Nifikop operator. It includes referencing the NiFi cluster, listing the associated `NifiUser` resources via `usersRef`, and applying access policies that will be granted to all members of the group.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/3_tasks/4_nifi_user_group.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  # Contains the reference to the NifiCluster with the one the registry client is linked.\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  # contains the list of reference to NifiUsers that are part to the group.\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n#     namespace: nifikop\n    - name: nc-controller.nifikop.mgt.cluster.local\n  # defines the list of access policies that will be granted to the group.\n  accessPolicies:\n      # defines the kind of access policy, could be \"global\" or \"component\".\n    - type: global\n      # defines the kind of action that will be granted, could be \"read\" or \"write\"\n      action: read\n      # resource defines the kind of resource targeted by this access policies, please refer to the following page:\n      #\thttps://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#access-policies\n      resource: /counters\n#      # componentType is used if the type is \"component\", it's allow to define the kind of component on which is the\n#      # access policy\n#      componentType: \"process-groups\"\n#      # componentId is used if the type is \"component\", it's allow to define the id of the component on which is the\n#      # access policy\n#      componentId: \"\"\n```\n\n----------------------------------------\n\nTITLE: Overriding NiFiCluster Configuration Sources in Kubernetes (YAML)\nDESCRIPTION: Demonstrates how to override the NiFi properties configuration using ConfigMaps, Secrets, and inline YAML fields within the NiFiCluster specification. The snippet shows the structure of 'nifiProperties' with 'overrideConfigMap', 'overrideSecretConfig', and 'overrideConfigs' fields, explaining their precedence and use cases. Required dependencies are a running NiFiKop operator, properly created referenced ConfigMaps and Secrets, and a correctly structured NiFiCluster manifest. Inputs include the name, namespace, and data keys for the ConfigMaps/Secrets; outputs are effective NiFi configuration overrides applied in priority order (Secret > ConfigMap > Inline > Default). Must be used with care to avoid misconfiguration that could impact operator behavior.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n nifiProperties:\n      # Additionnal nifi.properties configuration that will override the one produced based on template and\n      # configuration\n      overrideConfigMap:\n        # The key of the value,in data content, that we want use.\n        data: nifi.properties\n        # Name of the configmap that we want to refer.\n        name: raw\n        # Namespace where is located the secret that we want to refer.\n        namespace: nifikop\n      # Additionnal nifi.properties configuration that will override the one produced based\n      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n      overrideSecretConfig:\n        # The key of the value,in data content, that we want use.\n        data: nifi.properties\n        # Name of the configmap that we want to refer.\n        name: raw\n        # Namespace where is located the secret that we want to refer.\n        namespace: nifikop\n      # Additionnal nifi.properties configuration that will override the one produced based\n      #\ton template, configurations and overrideConfigMap\n      overrideConfigs: |\n        nifi.ui.banner.text=NiFiKop\n        nifi.sensitive.props.key=thisIsABadSensitiveKeyPassword\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFiKop Operator Using Helm with Resource and Namespace Settings in Bash\nDESCRIPTION: Shows the Helm install command for deploying the NiFiKop operator from an OCI Helm chart registry specifying namespace, operator image tag, resource requests and limits, and target namespaces. It requires Helm installed, the nifi namespace created, and optionally setting certManager.enabled to false to deploy an unsecured cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/2_deploy_nifikop/1_quick_start.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# You have to create the namespace before executing following command\nhelm install nifikop \\\n    oci://ghcr.io/konpyutaika/helm-charts/nifikop \\\n    --namespace=nifi \\\n    --version 1.13.0 \\\n    --set image.tag=v1.13.0-release \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set namespaces={\"nifi\"}\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiParameterContext Resource in YAML\nDESCRIPTION: Defines a `NifiParameterContext` custom resource for NiFiKop. This resource groups parameters to be applied to dataflows, optionally referencing Kubernetes secrets (`secretRefs`) for sensitive values. It is linked to a specific NiFi cluster (`clusterRef`).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\n  namespace: demo\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      value: toto\n      description: toto\n```\n\n----------------------------------------\n\nTITLE: Defining Managed Admin and Reader Users in NifiCluster CRD (YAML)\nDESCRIPTION: Example `NifiCluster` Custom Resource Definition (CRD) demonstrating how to configure managed admin and reader users. Users listed under `managedAdminUsers` are added to the 'managed-admins' group with full access, while users under `managedReaderUsers` are added to the 'managed-readers' group with view-only access. The operator automatically creates corresponding `NifiUser` resources and manages group memberships.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/2_manage_users_and_accesses/3_managed_groups.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  ...\n  oneNifiNodePerNode: false\n  propagateLabels: true\n  managedAdminUsers:\n    -  identity: \"alexandre.guitton@konpyutaika.com\"\n       name: \"aguitton\"\n    -  identity: \"nifiuser@konpyutaika.com\"\n       name: \"nifiuser\"\n  managedReaderUsers:\n    -  identity: \"toto@konpyutaika.com\"\n       name: \"toto\"\n    ...\n```\n\n----------------------------------------\n\nTITLE: NiFiKop Helm chart installation with custom resource limits and namespace in Bash\nDESCRIPTION: This snippet installs the NiFiKop operator Helm chart from the GitHub Container Registry using Helm 3. It specifies the namespace (nifi), version 1.11.0, image tag, and resource requests and limits for CPU and memory, optimizing resource allocation for the deployment. It also restricts the Helm chart to operate only within specified namespaces. This command requires Helm 3 and prior creation of the target namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/2_deploy_nifikop/1_quick_start.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# You have to create the namespace before executing following command\nhelm install nifikop \\\n    oci://ghcr.io/konpyutaika/helm-charts/nifikop \\\n    --namespace=nifi \\\n    --version 1.11.0 \\\n    --set image.tag=v1.11.0-release \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set namespaces={\"nifi\"}\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFiKop CRDs to Kubernetes Using Kubectl in Bash\nDESCRIPTION: This snippet applies all necessary NiFiKop Custom Resource Definitions (CRDs) to the Kubernetes cluster using kubectl. Each command targets a specific CRD YAML file required for NiFiKop's operation. Dependencies include kubectl and access credentials for the target cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/6_contributing/1_developer_guide.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining a NiFi User Group Custom Resource in Kubernetes - YAML\nDESCRIPTION: This YAML snippet demonstrates how to define a NiFiUserGroup custom resource for managing user group identity and permissions within an Apache NiFi cluster via the NiFiKop operator. Required dependencies include the NiFiKop CRDs installed in your Kubernetes cluster. Key parameters are 'metadata.name' for the group resource name, 'spec.identity' for the NiFi-side group name, 'spec.clusterRef' pointing to the relevant NiFiCluster, 'spec.usersRef' listing group user references, and 'spec.accessPolicies' specifying access control rules. The expected input is a valid YAML manifest; after application with kubectl, it will create or update the specified user group in NiFi. The snippet assumes all referenced users and clusters already exist, and misconfigured references may prevent proper group creation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/5_references/6_nifi_usergroup.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  identity: \"My Special Group\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n    - name: nc-controller.nifikop.mgt.cluster.local\n  accessPolicies:\n    - type: global\n      action: read\n      resource: /counters\n\n```\n\n----------------------------------------\n\nTITLE: Deploying ServiceMonitor Resource for NiFi Cluster Metrics Scraping - YAML\nDESCRIPTION: The ServiceMonitor resource configures how Prometheus scrapes metrics from NiFi cluster services. It selects services matching labels 'app: nifi' and 'nifi_cr: cluster' in the 'clusters' namespace and defines scrape endpoints on the 'prometheus' port at '/metrics' path with 10-second intervals. Relabeling rules extract pod IP, node ID, and NiFi custom resource labels to enrich metrics with pod metadata.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: cluster\n  namespace: monitoring-system\n  labels:\n    app: nifi-cluster\n    nifi_cr: cluster\nspec:\n  selector:\n    matchLabels:\n      app: nifi\n      nifi_cr: cluster\n  namespaceSelector:\n    matchNames:\n      - clusters\n  endpoints:\n    - interval: 10s\n      port: prometheus\n      path: /metrics\n      honorLabels: true\n      relabelings:\n        - sourceLabels: [__meta_kubernetes_pod_ip]\n          separator: ;\n          regex: (.*)\n          targetLabel: pod_ip\n          replacement: $1\n          action: replace\n        - sourceLabels: [__meta_kubernetes_pod_label_nodeId]\n          separator: ;\n          regex: (.*)\n          targetLabel: nodeId\n          replacement: $1\n          action: replace\n        - sourceLabels: [__meta_kubernetes_pod_label_nifi_cr]\n          separator: ;\n          regex: (.*)\n          targetLabel: nifi_cr\n          replacement: $1\n          action: replace\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFiCluster for Scale Up (YAML)\nDESCRIPTION: Modifies the NifiCluster custom resource definition (CRD) to add a new node definition to the 'nodes' list. This instructs the NiFiKop operator to create and join a new NiFi node with the specified ID and configuration group to the cluster. The 'id' field must be unique.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.12.1\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n    - id: 2\n      nodeConfigGroup: \"default_group\"\n# >>>> START: The new node\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n# <<<< END\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple NiFi Storage Repositories in NiFi Cluster YAML\nDESCRIPTION: This YAML snippet illustrates how to configure multiple content and provenance repository directories for NiFi. It uses the `overrideConfigs` field under `readOnlyConfig.nifiProperties` to define the repository paths using NiFi's property syntax and the `storageConfigs` section within a `nodeConfigGroups` entry to define the corresponding Persistent Volume Claims (PVCs) and mount paths for these directories.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n...\n  readOnlyConfig:\n    nifiProperties:\n      overrideConfigs: |\n        nifi.content.repository.directory.dir1=../content-additional/dir1\n        nifi.content.repository.directory.dir2=../content-additional/dir2\n        nifi.content.repository.directory.dir3=../content-additional/dir3\n        nifi.provenance.repository.directory.dir1=../provenance-additional/dir1\n        nifi.provenance.repository.directory.dir2=../provenance-additional/dir2\n...\n  nodeConfigGroups:\n    default_group:\n      ...\n      storageConfigs:\n      - mountPath: \"/opt/nifi/content-additional/dir1\"\n        name: content-repository-dir1\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/content-additional/dir2\"\n        name: content-repository-dir2\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/content-additional/dir3\"\n        name: content-repository-dir3\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/provenance-additional/dir1\"\n        name: provenance-repository-dir1\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/provenance-additional/dir2\"\n        name: provenance-repository-dir2\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      ...\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiRegistryClient Resource in YAML\nDESCRIPTION: Example YAML manifest for creating a `NifiRegistryClient` Kubernetes resource. This resource, managed by the Nifikop operator (apiVersion `nifi.konpyutaika.com/v1alpha1`), defines a client named `squidflow` associated with the NiFi cluster `nc` in the `nifikop` namespace. It specifies the target NiFi Registry URI (`http://nifi-registry:18080`) and includes a description. This CRD allows Kubernetes to manage NiFi registry client configurations declaratively.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/5_references/3_nifi_registry_client.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiRegistryClient\nmetadata:\n  name: squidflow\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Squidflow demo\"\n  uri: \"http://nifi-registry:18080\"\n```\n\n----------------------------------------\n\nTITLE: Defining Listeners Config in YAML\nDESCRIPTION: This YAML snippet defines the ListenersConfig for a NiFi instance. It specifies internal listeners for HTTPS, cluster communication, Site-to-Site (S2S), Prometheus metrics, and load balancing. It also configures SSL secrets, including the TLS secret name and a flag to create the secrets using cert-manager.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/5_references/1_nifi_cluster/6_listeners_config.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n  listenersConfig:\n    internalListeners:\n      - type: \"https\"\n        name: \"https\"\n        containerPort: 8443\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n      - type: \"prometheus\"\n        name: \"prometheus\"\n        containerPort: 9090\n      - type: \"load-balance\"\n        name: \"load-balance\"\n        containerPort: 6342\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n```\n\n----------------------------------------\n\nTITLE: Deploy NiFiKop with Helm\nDESCRIPTION: This command deploys the NiFiKop operator using a Helm chart from a container registry. It specifies the chart version, image tag, resource requests and limits for memory and CPU, and the namespaces where the operator should deploy. It also includes a parameter to disable cert-manager if needed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/2_deploy_nifikop/1_quick_start.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# You have to create the namespace before executing following command\nhelm install nifikop \\\n    oci://ghcr.io/konpyutaika/helm-charts/nifikop \\\n    --namespace=nifi \\\n    --version 1.7.0 \\\n    --set image.tag=v1.7.0-release \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set namespaces={\"nifi\"}\n```\n\n----------------------------------------\n\nTITLE: Defining KEDA ScaledObject for NiFi Autoscaling (YAML)\nDESCRIPTION: Defines a KEDA `ScaledObject` named `cluster` in the `clusters` namespace. This resource configures autoscaling for a target `NifiNodeGroupAutoscaler` resource. It specifies scaling parameters like polling interval, cooldown period, minimum/maximum replicas, fallback settings, and defines a Prometheus trigger. The trigger queries the deployed Prometheus instance for a specific metric (`http_requests_total` with a custom query) and scales the target based on a defined threshold.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: cluster\n  namespace: clusters\nspec:\n  scaleTargetRef:\n    apiVersion:    nifi.konpyutaika.com/v1alpha1     # Optional. Default: apps/v1\n    kind:          NifiNodeGroupAutoscaler           # Optional. Default: Deployment\n    name:          nifinodegroupautoscaler-sample    # Mandatory. Must be in the same namespace as the ScaledObject\n    envSourceContainerName: nifi                     # Optional. Default: .spec.template.spec.containers[0]\n  pollingInterval:  30                               # Optional. Default: 30 seconds\n  cooldownPeriod:   300                              # Optional. Default: 300 seconds\n  idleReplicaCount: 0                                # Optional. Default: ignored, must be less than minReplicaCount \n  minReplicaCount:  1                                # Optional. Default: 0\n  maxReplicaCount:  3                                # Optional. Default: 100\n  fallback:                                          # Optional. Section to specify fallback options\n    failureThreshold: 5                              # Mandatory if fallback section is included\n    replicas: 1                                      # Mandatory if fallback section is included\n  # advanced:                                          # Optional. Section to specify advanced options\n  #   restoreToOriginalReplicaCount: true              # Optional. Whether the target resource should be scaled back to original replicas count, after the ScaledObject is deleted\n  #   horizontalPodAutoscalerConfig:                   # Optional. Section to specify HPA related options\n  #     name: {name-of-hpa-resource}                   # Optional. Default: keda-hpa-{scaled-object-name}\n  #     behavior:                                      # Optional. Use to modify HPA's scaling behavior\n  #       scaleDown:\n  #         stabilizationWindowSeconds: 300 <--- \n  #         policies:\n  #         - type: Percent\n  #           value: 100\n  #           periodSeconds: 15\n  triggers:\n    - type: prometheus\n      metadata:\n        serverAddress: http://prometheus-operated.monitoring-system.svc:9090\n        metricName: http_requests_total\n        threshold: <threshold value>\n        query: <prometheus query>\n```\n\n----------------------------------------\n\nTITLE: Packaging NiFiKop Helm Chart (Bash)\nDESCRIPTION: Uses the 'make helm-package' command to package the NiFiKop Helm chart located in the 'helm/nifikop' directory into a versioned chart archive file (.tgz). This archive can then be distributed or added to a Helm repository. Requires Helm installed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/6_contributing/1_developer_guide.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nmake helm-package\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Internal Listener Without Type in NiFi Kubernetes YAML\nDESCRIPTION: Demonstrates how to add an additional internal listener without specifying a type in the NiFi Kubernetes configuration. This method is useful for exposing custom NiFi processors or endpoints that do not require a predefined listener type, for example, an HTTP tracking endpoint. This listener will be mapped as a container port accessible inside the pod.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    ...\n    - name: \"http-tracking\"\n      containerPort: 8081\n```\n\n----------------------------------------\n\nTITLE: Modifying OpenShift YAML for NiFi Cluster\nDESCRIPTION: This bash script modifies a YAML configuration file, replacing a placeholder UID/GID (1000690000) with a dynamically retrieved value.  It's designed for OpenShift environments, and prepares the configuration to use the correct user ID for NiFi pods. The script uses `sed` for the replacement. The script modifies the file referenced as the second argument.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nsed -i \"s/1000690000/$uid/g\" config/samples/openshift.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Node Configuration in YAML\nDESCRIPTION: This YAML snippet defines individual Apache NiFi nodes with unique IDs, optional grouping via nodeConfigGroup, immutable configuration parameters through readOnlyConfig, and detailed nodeConfig settings. The nodeConfig supports Kubernetes resource requirements, including CPU and memory limits/requests, and storage configurations with persistent volume claims specifying access modes, storage class, and metadata labels and annotations. This setup is essential for managing NiFi nodes in a container orchestration environment and facilitates rolling upgrades triggered by read-only configuration changes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/5_references/1_nifi_cluster/4_node.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n    - id: 0\n      # nodeConfigGroup can be used to ease the node configuration, if set only the id is required\n      nodeConfigGroup: \"default_group\"\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        nifiProperties:\n          overrideConfigs: |\n            nifi.ui.banner.text=NiFiKop - Node 0\n      # node configuration\n#       nodeConfig:\n    - id: 2\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        overrideConfigs: |\n          nifi.ui.banner.text=NiFiKop - Node 2\n      # node configuration\n      nodeConfig:\n        resourcesRequirements:\n          limits:\n            cpu: \"2\"\n            memory: 3Gi\n          requests:\n            cpu: \"1\"\n            memory: 1Gi\n        storageConfigs:\n          # Name of the storage config, used to name PV to reuse into sidecars for example.\n          - name: provenance-repository\n            # Path where the volume will be mount into the main nifi container inside the pod.\n            mountPath: \"/opt/nifi/provenance_repository\"\n            # Metadata to attach to the PVC that gets created\n            metadata:\n              labels:\n                my-label: my-value\n              annotations:\n                my-annotation: my-value\n            # Kubernetes PVC spec\n            # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n            pvcSpec:\n              accessModes:\n                - ReadWriteOnce\n              storageClassName: \"standard\"\n              resources:\n                requests:\n                  storage: 8Gi\n```\n\n----------------------------------------\n\nTITLE: Deploying KEDA ScaledObject for NiFi Autoscaling\nDESCRIPTION: Creates a KEDA ScaledObject that defines how to automatically scale a NiFi node group based on Prometheus metrics. Configures scaling parameters like polling interval, cooldown period, and replica count ranges.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: cluster\n  namespace: clusters\nspec:\n  scaleTargetRef:\n    apiVersion:    nifi.konpyutaika.com/v1alpha1     # Optional. Default: apps/v1\n    kind:          NifiNodeGroupAutoscaler           # Optional. Default: Deployment\n    name:          nifinodegroupautoscaler-sample    # Mandatory. Must be in the same namespace as the ScaledObject\n    envSourceContainerName: nifi                     # Optional. Default: .spec.template.spec.containers[0]\n  pollingInterval:  30                               # Optional. Default: 30 seconds\n  cooldownPeriod:   300                              # Optional. Default: 300 seconds\n  idleReplicaCount: 0                                # Optional. Default: ignored, must be less than minReplicaCount \n  minReplicaCount:  1                                # Optional. Default: 0\n  maxReplicaCount:  3                                # Optional. Default: 100\n  fallback:                                          # Optional. Section to specify fallback options\n    failureThreshold: 5                              # Mandatory if fallback section is included\n    replicas: 1                                      # Mandatory if fallback section is included\n  # advanced:                                          # Optional. Section to specify advanced options\n  #   restoreToOriginalReplicaCount: true              # Optional. Whether the target resource should be scaled back to original replicas count, after the ScaledObject is deleted\n  #   horizontalPodAutoscalerConfig:                   # Optional. Section to specify HPA related options\n  #     name: {name-of-hpa-resource}                   # Optional. Default: keda-hpa-{scaled-object-name}\n  #     behavior:                                      # Optional. Use to modify HPA's scaling behavior\n  #       scaleDown:\n  #         stabilizationWindowSeconds: 300 <--- \n  #         policies:\n  #         - type: Percent\n  #           value: 100\n  #           periodSeconds: 15\n  triggers:\n    - type: prometheus\n      metadata:\n        serverAddress: http://prometheus-operated.monitoring-system.svc:9090\n        metricName: http_requests_total\n        threshold: <threshold value>\n        query: <prometheus query>\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Nodes in NiFiKop (YAML)\nDESCRIPTION: This YAML snippet demonstrates how to define configurations for individual NiFi nodes within a NiFiKop custom resource. It shows examples of using `nodeConfigGroup`, setting `readOnlyConfig` for NiFi properties like the UI banner, and specifying detailed `nodeConfig` including resource requirements (CPU, memory) and persistent storage configurations for repositories like provenance.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/5_references/1_nifi_cluster/4_node.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n    - id: 0\n      # nodeConfigGroup can be used to ease the node configuration, if set only the id is required\n      nodeConfigGroup: \"default_group\"\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        nifiProperties:\n          overrideConfigs: |\n            nifi.ui.banner.text=NiFiKop - Node 0\n      # node configuration\n#       nodeConfig:\n    - id: 2\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        overrideConfigs: |\n          nifi.ui.banner.text=NiFiKop - Node 2\n      # node configuration\n      nodeConfig:\n        resourcesRequirements:\n          limits:\n            cpu: \"2\"\n            memory: 3Gi\n          requests:\n            cpu: \"1\"\n            memory: 1Gi\n        storageConfigs:\n          # Name of the storage config, used to name PV to reuse into sidecars for example.\n          - name: provenance-repository\n            # Path where the volume will be mount into the main nifi container inside the pod.\n            mountPath: \"/opt/nifi/provenance_repository\"\n            # Metadata to attach to the PVC that gets created\n            metadata:\n              labels:\n                my-label: my-value\n              annotations:\n                my-annotation: my-value\n            # Kubernetes PVC spec\n            # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n            pvcSpec:\n              accessModes:\n                - ReadWriteOnce\n              storageClassName: \"standard\"\n              resources:\n                requests:\n                  storage: 8Gi\n```\n\n----------------------------------------\n\nTITLE: Advanced NiFi Storage Repository Configuration Using Inline YAML Overrides in NiFiCluster\nDESCRIPTION: This YAML snippet demonstrates advanced configuration for NiFi content and provenance repository directories by specifying multiple storage paths as inline override properties under 'readOnlyConfig'. It shows how to map these directories to Persistent Volume Claims (PVCs) through the 'nodeConfigGroups' storageConfigs entries, specifying mount paths, PVC names, access modes, storage class, and storage requests. This setup addresses high-performance NiFi installations requiring multiple repository directories and persistent storage. It requires the NiFi operator to support the referenced 'readOnlyConfig' and node group storage configuration mechanisms.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n...\n  readOnlyConfig:\n    nifiProperties:\n      overrideConfigs: |\n        nifi.content.repository.directory.dir1=../content-additional/dir1\n        nifi.content.repository.directory.dir2=../content-additional/dir2\n        nifi.content.repository.directory.dir3=../content-additional/dir3\n        nifi.provenance.repository.directory.dir1=../provenance-additional/dir1\n        nifi.provenance.repository.directory.dir2=../provenance-additional/dir2\n...\n  nodeConfigGroups:\n    default_group:\n      ...\n      storageConfigs:\n      - mountPath: \"/opt/nifi/content-additional/dir1\"\n        name: content-repository-dir1\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/content-additional/dir2\"\n        name: content-repository-dir2\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/content-additional/dir3\"\n        name: content-repository-dir3\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/provenance-additional/dir1\"\n        name: provenance-repository-dir1\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/provenance-additional/dir2\"\n        name: provenance-repository-dir2\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      ...\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiRegistryClient Resource in YAML\nDESCRIPTION: Example YAML manifest for creating a `NifiRegistryClient` custom resource in Kubernetes using NiFiKop. This resource connects NiFiKop to a specific NiFi Registry instance, identified by its URI, and associates it with a NiFi cluster (`clusterRef`). It is a prerequisite for managing dataflows via the `NifiDataflow` CRD.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/3_tasks/3_nifi_dataflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiRegistryClient\nmetadata:\n  name: registry-client-example\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Registry client managed by NiFiKop\"\n  uri: \"http://nifi.hostname.com:18080\"\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFiKop Operator using Helm\nDESCRIPTION: Installs the NiFiKop operator using its Helm chart from the OCI registry. Specifies the target namespace, operator version, image tag, resource requests/limits, and the namespaces the operator should watch. Requires Helm 3 and the 'nifi' namespace to exist.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/2_deploy_nifikop/1_quick_start.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n# You have to create the namespace before executing following command\nhelm install nifikop \\\n    oci://ghcr.io/konpyutaika/helm-charts/nifikop \\\n    --namespace=nifi \\\n    --version 1.10.0 \\\n    --set image.tag=v1.10.0-release \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set namespaces={\"nifi\"}\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFi Cluster\nDESCRIPTION: This command deploys a simple NiFi cluster using `kubectl create`.  It assumes a `simplenificluster.yaml` file is available in the `config/samples/` directory, and deploys it within the `nifi` namespace.  This assumes zookeeper has been installed and configured to enable the NiFi cluster to start.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create -n nifi -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Automating NiFi Sensitive Algorithm Migration with initContainer - Kubernetes YAML\nDESCRIPTION: Defines a Kubernetes initContainer to automate updating sensitive properties encryption algorithm on NiFi cluster startup. Relies on the apache/nifi-toolkit container image, shell scripting for extracting property values, and the encrypt-config.sh tool for updating flow files. Requires appropriate volume mounts for data and configuration directories, and expects Kubernetes deployment with the specified image and mount paths.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/7_upgrade_guides/4_v1.3.1_to_v1.4.0.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ninitContainers:\n  - image: \"apache/nifi-toolkit:latest\"\n    name: nifi-toolkit\n    imagePullPolicy: Always\n    command:\n      - \"sh\"\n      - \"-c\"\n      - \"NIFI_SENSITIVE_PROPS_KEY=$(grep 'nifi.sensitive.props.key' /opt/nifi/nifi-current/conf/nifi.properties | cut -d'=' -f2) && bin/encrypt-config.sh -n /opt/nifi/nifi-current/conf/nifi.properties -f /opt/nifi/data/flow.json.gz -x -A NIFI_PBKDF2_AES_GCM_256 -s $NIFI_SENSITIVE_PROPS_KEY; bin/encrypt-config.sh -n /opt/nifi/nifi-current/conf/nifi.properties -f /opt/nifi/data/flow.xml.gz -x -A NIFI_PBKDF2_AES_GCM_256 -s $NIFI_SENSITIVE_PROPS_KEY\"\n    volumeMounts:\n      - name: data\n        mountPath: /opt/nifi/data\n      - name: conf\n        mountPath: /opt/nifi/nifi-current/conf\n```\n\n----------------------------------------\n\nTITLE: Defining ZookeeperProperties Configuration Overrides (Markdown)\nDESCRIPTION: Specifies properties for customizing Zookeeper configuration (zookeeper.properties). Allows overriding default settings using external Kubernetes ConfigMaps (`overrideConfigMap`), direct string overrides (`overrideConfigs`), or external Kubernetes Secrets (`overrideSecretConfig`) in a cascading manner.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/5_references/1_nifi_cluster/2_read_only_config.md#_snippet_1\n\nLANGUAGE: Markdown\nCODE:\n```\n## ZookeeperProperties\n\n|Field|Type|Description|Required|Default|\n|-----|----|-----------|--------|--------|\n|overrideConfigMap|[ConfigmapReference](#configmapreference)|Additionnal zookeeper.properties configuration that will override the one produced based on template and configuration.|No|nil|\n|overrideConfigs|string|Additionnal zookeeper.properties configuration that will override the one produced based on template, configurations and overrideConfigMap.|No|\"\"|\n|overrideSecretConfig|[SecretConfigReference](#secretconfigreference)|Additionnal zookeeper.properties configuration that will override the one produced based on template, configurations, overrideConfigMap and overrideConfigs.|No|nil|\n```\n\n----------------------------------------\n\nTITLE: Defining NodeConfigGroups in NiFiKop (YAML)\nDESCRIPTION: This YAML snippet demonstrates how to define multiple NodeConfigGroups within the NiFiKop Spec. It allocates different memory and CPU resources, provenance storage, and service account names for each group. The field resourcesRequirements configures Kubernetes container resource requests and limits, and provenanceStorage sets the storage size for NiFi provenance data. Each group can be referenced by name when assigning configurations to cluster nodes. No external dependencies are needed, but this must be part of a NiFiKop CRD manifest. All resource limits and group names should be chosen to match cluster requirements.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  # nodeConfigGroups specifies multiple node configs with unique name\n  nodeConfigGroups:\n    default_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      serviceAccountName: \"default\"\n      # resourceRequirements works exactly like Container resources, the user can specify the limit and the requests\n      # through this property\n      # https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 3Gi\n    high_mem_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      serviceAccountName: \"default\"\n      # resourceRequirements works exactly like Container resources, the user can specify the limit and the requests\n      # through this property\n      # https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 30Gi\n        requests:\n          cpu: \"1\"\n          memory: 30Gi\n\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Dataflow in YAML\nDESCRIPTION: This YAML snippet defines a `NifiDataflow` resource, specifying the configuration for a NiFi dataflow. It includes metadata, such as the name of the dataflow, and specifies the desired state through the `spec` field, including references to a cluster, registry client, and parameter context. The `spec` field also defines other crucial properties such as `bucketId`, `flowId`, `flowVersion`, `syncMode`, and update strategy. The defined `skipInvalidControllerService` and `skipInvalidComponent` properties control how the flow behaves in the face of invalid components.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/5_references/5_nifi_dataflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  flowPosition:\n    posX: 0\n    posY: 0\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: squidflow\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: nifikop\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image For NiFiKop Operator Using Makefile (Bash)\nDESCRIPTION: This snippet builds a Docker image for the NiFiKop operator using a Makefile target, tagging it with the registry name stored in the DOCKER_REGISTRY_BASE environment variable. It assumes Docker is installed and running locally, and is intended for generating an image to deploy via Helm charts or other container orchestration workflows.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/6_contributing/1_developer_guide.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nexport DOCKER_REGISTRY_BASE={your-docker-repo}\nmake docker-build\n```\n\n----------------------------------------\n\nTITLE: Applying NiFi Cluster Scale-Down Configuration using kubectl\nDESCRIPTION: Executes the `kubectl apply` command to apply the modified `NifiCluster` configuration (presumably `config/samples/simplenificluster.yaml` with the node removed) to the Kubernetes cluster in the 'nifi' namespace, initiating the scale-down procedure.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\nkubectl -n nifi apply -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Adding a Node to NiFiKop Cluster - YAML Configuration\nDESCRIPTION: This YAML snippet demonstrates how to update the NifiCluster custom resource to add a new node to the cluster. The new node is appended to the 'nodes' array with a unique 'id' and links to an existing 'nodeConfigGroup'. Dependencies include a running Kubernetes cluster with NiFiKop installed, existing NiFiCluster resource, and proper configuration of referenced resources (storage class, service account, Zookeeper). 'Node.Id' values must be unique. Input is a YAML file, and output is an updated state resulting in a new NiFi node being created.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n  clusterManager: zookeeper\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.12.1\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n    - id: 2\n      nodeConfigGroup: \"default_group\"\n# >>>> START: The new node\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n# <<<< END\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: Extracting Allowed UID/GID for NiFi Namespace (OpenShift) - Bash\nDESCRIPTION: Extracts the supplemental group ID for the NiFi namespace on OpenShift, which is necessary to set pod security contexts for NiFi deployment. Utilizes kubectl, sed, and tr for string manipulation. Output is used to parameterize YAML manifests or Helm charts.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nuid=$(kubectl get namespace nifi -o=jsonpath='{.metadata.annotations.openshift\\.io/sa\\.scc\\.supplemental-groups}' | sed 's/\\/10000$//' | tr -d '[:space:]')\n\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Service in Cluster Deployment\nDESCRIPTION: This YAML snippet shows how to define an external service in the NiFi cluster deployment configuration. It creates a ClusterIP service that exposes port 8443 for the HTTPS listener used by Istio.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nspec:  \n  externalServices:  \n    - name: \"nifi-cluster\"\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8443\n            internalListenerName: \"https\"\n```\n\n----------------------------------------\n\nTITLE: Defining NifiDataflow Custom Resource - YAML\nDESCRIPTION: This YAML snippet demonstrates how to define a NifiDataflow custom resource for use with the konpyutaika NiFi operator in a Kubernetes environment. All required fields (such as bucketId, flowId, flowVersion, clusterRef, registryClientRef, and updateStrategy) are specified, along with optional settings like flowPosition, syncMode, and skipInvalidComponent. The manifest allows operator-based management of versioned NiFi dataflows by mapping flows from a registry bucket into the target NiFi cluster, with operational modes such as drain-based updates. Prerequisites include a running NifiCluster and appropriate registry clients, and field values must correspond to existing resources. The YAML must be applied to a Kubernetes cluster with the NiFi operator installed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/5_references/5_nifi_dataflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  flowPosition:\n    posX: 0\n    posY: 0\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: squidflow\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: nifikop\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Building and Installing the NiFiKop Kubectl Plugin (Console)\nDESCRIPTION: Compiles the NiFiKop kubectl plugin using the 'make' command and copies the resulting executable binary to the '/usr/local/bin' directory using 'sudo' for system-wide availability on UNIX-like systems. Requires 'make' and appropriate build tools to be installed, as well as 'sudo' privileges.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/2_deploy_nifikop/3_kubectl_plugin.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nmake kubectl-nifikop && sudo cp ./bin/kubectl-nifikop /usr/local/bin\n```\n\n----------------------------------------\n\nTITLE: Update NiFi OpenShift Config with UID\nDESCRIPTION: This snippet updates the `openshift.yaml` configuration file with the retrieved UID using `sed`. It replaces the placeholder value `1000690000` with the actual UID.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nsed -i \"s/1000690000/$uid/g\" config/samples/openshift.yaml\n```\n\n----------------------------------------\n\nTITLE: Creating Managed User Groups: Admins and Readers\nDESCRIPTION: The operator manages user groups named 'managed-admins' and 'managed-readers', containing designated users such as 'aguitton', 'nifiuser', and 'toto'. This setup streamlines access permissions by associating users with their respective role groups. These groups are created dynamically based on the specified user lists, simplifying large-scale user management and role assignment within the NiFi environment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/3_manage_nifi/2_manage_users_and_accesses/3_managed_groups.md#_snippet_2\n\n\n\n----------------------------------------\n\nTITLE: Defining NiFi Node Configurations in YAML\nDESCRIPTION: This YAML snippet demonstrates configuring individual NiFi nodes within a NiFiKop deployment. It shows using `nodeConfigGroup` for simplified setup (Node 0) and detailed `nodeConfig` including `resourcesRequirements` (CPU, memory) and `storageConfigs` with PVC specifications (Node 2). It also illustrates overriding read-only properties like `nifi.ui.banner.text` via `readOnlyConfig`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/5_references/1_nifi_cluster/4_node.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n    - id: 0\n      # nodeConfigGroup can be used to ease the node configuration, if set only the id is required\n      nodeConfigGroup: \"default_group\"\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        nifiProperties:\n          overrideConfigs: |\n            nifi.ui.banner.text=NiFiKop - Node 0\n      # node configuration\n#       nodeConfig:\n    - id: 2\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        overrideConfigs: |\n          nifi.ui.banner.text=NiFiKop - Node 2\n      # node configuration\n      nodeConfig:\n        resourcesRequirements:\n          limits:\n            cpu: \"2\"\n            memory: 3Gi\n          requests:\n            cpu: \"1\"\n            memory: 1Gi\n        storageConfigs:\n          # Name of the storage config, used to name PV to reuse into sidecars for example.\n          - name: provenance-repository\n            # Path where the volume will be mount into the main nifi container inside the pod.\n            mountPath: \"/opt/nifi/provenance_repository\"\n            # Kubernetes PVC spec\n            # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n            pvcSpec:\n              accessModes:\n                - ReadWriteOnce\n              storageClassName: \"standard\"\n              resources:\n                requests:\n                  storage: 8Gi\n```\n\n----------------------------------------\n\nTITLE: Creating a Kubernetes Secret for Sensitive NiFi Parameters using kubectl (Console)\nDESCRIPTION: Demonstrates how to create a generic Kubernetes secret using kubectl, which is then referenced in the NifiParameterContext for securely injecting sensitive parameter values into NiFi flows. Dependencies include access to a Kubernetes cluster, kubectl installed, and appropriate permissions in the target namespace. Inputs are literal key-value pairs to store, and the output is a secret manifest stored in the specified namespace. Avoid storing secrets in version control or in clear-text YAML.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl create secret generic secret-params \\\n    --from-literal=secret1=yop \\\n    --from-literal=secret2=yep \\\n    -n nifikop\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Cluster with Prometheus and Storage (YAML)\nDESCRIPTION: This snippet defines a set of Kubernetes manifest values for a NiFi cluster, demonstrating how to enable a Prometheus internal listener and configure persistent volumes for logs, data, extensions, repositories, and configuration. Prerequisites include a NiFi operator or custom resources YAML-based deployment and a storage class named 'ssd-wait'. The example shows parameter settings for CPU and memory requests, provides custom labels and annotations, and enables service account configuration. Input is a standard Kubernetes YAML manifest; output is a NiFi cluster with persistent storage and Prometheus monitoring capabilities. Storage and nodeSelector settings can be adjusted according to hardware needs.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n...\nspec:\n  ...\n  listenersConfig:\n    internalListeners:\n    ...\n    - containerPort: 9090\n      name: prometheus\n      type: prometheus \n    ...\n  ...\n  nodeConfigGroups:\n    auto_scaling:\n      isNode: true\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 2Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n      serviceAccountName: external-dns\n      storageConfigs:\n        - mountPath: /opt/nifi/nifi-current/logs\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/data\n          name: data\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/extensions\n          name: extensions-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/flowfile_repository\n          name: flowfile-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/nifi-current/conf\n          name: conf\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/content_repository\n          name: content-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/provenance_repository\n          name: provenance-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n  ...\n```\n\n----------------------------------------\n\nTITLE: Example NiFi Cluster YAML Definition\nDESCRIPTION: This YAML manifest defines a `NifiCluster` resource named `simplenifi` using the `nifi.konpyutaika.com/v1alpha1` API. It specifies configurations for headless services, pod metadata, ZooKeeper connection (`zkAddress`, `zkPath`), the NiFi image (`clusterImage`), node setup including storage (`storageConfigs`) and resources (`resourcesRequirements`), internal listeners (`listenersConfig`), and external service exposure (`externalServices`). This resource is intended to be applied to a Kubernetes cluster with the Konpyutaika NiFi operator installed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/5_references/1_nifi_cluster/1_nifi_cluster.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n    annotations:\n      tyty: ytyt\n    labels:\n      tete: titi  \n  pod:\n    annotations:\n      toto: tata\n    labels:\n      titi: tutu\n  zkAddress: 'zookeepercluster-client.zookeeper:2181'\n  zkPath: '/simplenifi'\n  clusterImage: 'apache/nifi:1.11.3'\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      podMetadata:\n        annotations:\n          node-annotation: \"node-annotation-value\"\n        labels:\n          node-label: \"node-label-value\"\n      externalVolumeConfigs:\n        - name: example-volume\n          mountPath: \"/opt/nifi/example\"\n          secret:\n            secretName: \"raw-controller\"\n      storageConfigs:\n        - mountPath: '/opt/nifi/nifi-current/logs'\n          name: logs\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: 'standard'\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: 'default'\n      resourcesRequirements:\n        limits:\n          cpu: '2'\n          memory: 3Gi\n        requests:\n          cpu: '1'\n          memory: 1Gi\n  nodes:\n    - id: 1\n      nodeConfigGroup: 'default_group'\n    - id: 2\n      nodeConfigGroup: 'default_group'\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: 'http'\n        name: 'http'\n        containerPort: 8080\n      - type: 'cluster'\n        name: 'cluster'\n        containerPort: 6007\n      - type: 's2s'\n        name: 's2s'\n        containerPort: 10000\n  externalServices:\n    - name: 'clusterip'\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8080\n            internalListenerName: 'http'\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n\n----------------------------------------\n\nTITLE: Listing Deployed Helm Charts in Bash\nDESCRIPTION: This Bash command displays all currently deployed Helm chart releases in the active Kubernetes cluster. Requires Helm CLI; no parameters. Outputs a tabular summary of releases.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nhelm list\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Node Configurations in NiFiKop YAML\nDESCRIPTION: This YAML snippet illustrates how to define configurations for individual NiFi nodes managed by NiFiKop. It shows examples of setting unique `id`s, applying predefined configurations via `nodeConfigGroup` (for node 0), customizing read-only properties like the UI banner text in `readOnlyConfig` (which triggers rolling upgrades), and specifying resource requests/limits (`nodeConfig.resourcesRequirements`) and persistent storage volumes (`nodeConfig.storageConfigs`) using Kubernetes PVC specs (for node 2). This allows granular control over each node's behavior and resource footprint within the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/5_references/1_nifi_cluster/4_node.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n    - id: 0\n      # nodeConfigGroup can be used to ease the node configuration, if set only the id is required\n      nodeConfigGroup: \"default_group\"\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        nifiProperties:\n          overrideConfigs: |\n            nifi.ui.banner.text=NiFiKop - Node 0\n      # node configuration\n#       nodeConfig:\n    - id: 2\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        overrideConfigs: |\n          nifi.ui.banner.text=NiFiKop - Node 2\n      # node configuration\n      nodeConfig:\n        resourcesRequirements:\n          limits:\n            cpu: \"2\"\n            memory: 3Gi\n          requests:\n            cpu: \"1\"\n            memory: 1Gi\n        storageConfigs:\n          # Name of the storage config, used to name PV to reuse into sidecars for example.\n          - name: provenance-repository\n            # Path where the volume will be mount into the main nifi container inside the pod.\n            mountPath: \"/opt/nifi/provenance_repository\"\n            # Metadata to attach to the PVC that gets created\n            metadata:\n              labels:\n                my-label: my-value\n              annotations:\n                my-annotation: my-value\n            # Kubernetes PVC spec\n            # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n            pvcSpec:\n              accessModes:\n                - ReadWriteOnce\n              storageClassName: \"standard\"\n              resources:\n                requests:\n                  storage: 8Gi\n```\n\n----------------------------------------\n\nTITLE: Defining Managed Admin/Reader Users in NiFiCluster YAML\nDESCRIPTION: This YAML snippet demonstrates how to define managed admin and reader users within the `NifiCluster.Spec`. The operator will create NifiUsers based on these definitions and manage their membership in the respective groups.  The identities are used for authentication, and names are internal to NiFi.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/2_manage_users_and_accesses/3_managed_groups.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  ...\n  oneNifiNodePerNode: false\n  propagateLabels: true\n  managedAdminUsers:\n    -  identity: \"alexandre.guitton@konpyutaika.com\"\n       name: \"aguitton\"\n    -  identity: \"nifiuser@konpyutaika.com\"\n       name: \"nifiuser\"\n  managedReaderUsers:\n    -  identity: \"toto@konpyutaika.com\"\n       name: \"toto\"\n    ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Storage for NiFi Nodes - YAML\nDESCRIPTION: This YAML snippet provides an example of configuring storage for a NiFi cluster. It utilizes the `storageConfigs` field within a `NodeConfigGroup` to define persistent volumes for various NiFi data directories. Includes configuration for persistent volume claims, mount paths and storage classes. Output is a series of storage configuration entries for a Kubernetes CRD.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n...\nstorageConfigs:\n  - mountPath: /opt/nifi/nifi-current/logs\n    name: logs\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/data\n    name: data\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 50Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/extensions\n    name: extensions-repository\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 5Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/flowfile_repository\n    name: flowfile-repository\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/nifi-current/conf\n    name: conf\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 5Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/content_repository\n    name: content-repository\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 500Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/provenance_repository\n    name: provenance-repository\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 500Gi\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Cluster Configuration - YAML\nDESCRIPTION: This YAML snippet defines a `NifiCluster` custom resource, specifying its configuration within a Kubernetes environment. It utilizes the `apiVersion`, `kind`, and `metadata` fields for Kubernetes resource definition. The `spec` section configures the cluster's behavior. Key aspects include service settings, pod configurations with annotations and labels, Zookeeper connection details, image, node setup, listener configurations and external services. It's used to instruct the operator to create and manage the specified NiFi cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/5_references/1_nifi_cluster/1_nifi_cluster.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n    annotations:\n      tyty: ytyt\n    labels:\n      tete: titi  \n  pod:\n    annotations:\n      toto: tata\n    labels:\n      titi: tutu\n  zkAddress: 'zookeepercluster-client.zookeeper:2181'\n  zkPath: '/simplenifi'\n  clusterImage: 'apache/nifi:1.11.3'\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      podMetadata:\n        annotations:\n          node-annotation: \"node-annotation-value\"\n        labels:\n          node-label: \"node-label-value\"\n      externalVolumeConfigs:\n        - name: example-volume\n          mountPath: \"/opt/nifi/example\"\n          secret:\n            secretName: \"raw-controller\"\n      storageConfigs:\n        - mountPath: '/opt/nifi/nifi-current/logs'\n          name: logs\n          # Metadata to attach to the PVC that gets created\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: 'standard'\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: 'default'\n      resourcesRequirements:\n        limits:\n          cpu: '2'\n          memory: 3Gi\n        requests:\n          cpu: '1'\n          memory: 1Gi\n  nodes:\n    - id: 1\n      nodeConfigGroup: 'default_group'\n    - id: 2\n      nodeConfigGroup: 'default_group'\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: 'http'\n        name: 'http'\n        containerPort: 8080\n      - type: 'cluster'\n        name: 'cluster'\n        containerPort: 6007\n      - type: 's2s'\n        name: 's2s'\n        containerPort: 10000\n  externalServices:\n    - name: 'clusterip'\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8080\n            internalListenerName: 'http'\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n```\n\n----------------------------------------\n\nTITLE: Install NiFiKop with Helm\nDESCRIPTION: This snippet installs NiFiKop using Helm. It specifies the chart location, namespace, version, image tag, and resource requirements.  It also shows how to disable certManager if you only want to deploy unsecured clusters.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/2_deploy_nifikop/1_quick_start.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# You have to create the namespace before executing following command\nhelm install nifikop \\\n    oci://ghcr.io/konpyutaika/helm-charts/nifikop \\\n    --namespace=nifi \\\n    --version 1.3.1 \\\n    --set image.tag=v1.3.1-release \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set namespaces={\\\"nifi\\\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFiKop for OIDC Authentication via OverrideConfigs in YAML\nDESCRIPTION: Demonstrates how to configure OIDC authentication for a NiFi cluster managed by NiFiKop by overriding `nifi.properties` using the `Spec.NifiProperties.OverrideConfigs` field within the `NifiCluster` custom resource definition. This example includes settings for the OIDC discovery URL, client ID, client secret, web proxy hosts, and the recommended identity mapping properties.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/5_users_authentication/1_oidc.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  readOnlyConfig:\n    # NifiProperties configuration that will be applied to the node.\n    nifiProperties:\n      webProxyHosts:\n        - nifistandard2.trycatchlearn.fr:8443\n      # Additionnal nifi.properties configuration that will override the one produced based\n      # on template and configurations.\n      overrideConfigs: |\n        nifi.security.user.oidc.discovery.url=<oidc server discovery url>\n        nifi.security.user.oidc.client.id=<oidc client's id>\n        nifi.security.user.oidc.client.secret=<oidc client's secret>\n        nifi.security.identity.mapping.pattern.dn=CN=([^,]*)(?:, (?:O|OU)=.*)?\n        nifi.security.identity.mapping.value.dn=$1\n        nifi.security.identity.mapping.transform.dn=NONE\n      ...\n   ...\n...\n```\n\n----------------------------------------\n\nTITLE: Configuring Individual NiFi Nodes in NiFiKop YAML\nDESCRIPTION: This YAML snippet shows how to configure individual NiFi nodes within a NiFiKop custom resource. It demonstrates using `nodeConfigGroup` for shared settings, applying `readOnlyConfig` for properties that trigger rolling upgrades (like UI banner text), and defining `nodeConfig` for resource requests, limits, and persistent storage volumes specific to a node. This allows granular control over each node's configuration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/5_references/1_nifi_cluster/4_node.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n    - id: 0\n      # nodeConfigGroup can be used to ease the node configuration, if set only the id is required\n      nodeConfigGroup: \"default_group\"\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        nifiProperties:\n          overrideConfigs: |\n            nifi.ui.banner.text=NiFiKop - Node 0\n      # node configuration\n#       nodeConfig:\n    - id: 2\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        overrideConfigs: |\n          nifi.ui.banner.text=NiFiKop - Node 2\n      # node configuration\n      nodeConfig:\n        resourcesRequirements:\n          limits:\n            cpu: \"2\"\n            memory: 3Gi\n          requests:\n            cpu: \"1\"\n            memory: 1Gi\n        storageConfigs:\n          # Name of the storage config, used to name PV to reuse into sidecars for example.\n          - name: provenance-repository\n            # Path where the volume will be mount into the main nifi container inside the pod.\n            mountPath: \"/opt/nifi/provenance_repository\"\n            # Metadata to attach to the PVC that gets created\n            metadata:\n              labels:\n                my-label: my-value\n              annotations:\n                my-annotation: my-value\n            # Kubernetes PVC spec\n            # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n            pvcSpec:\n              accessModes:\n                - ReadWriteOnce\n              storageClassName: \"standard\"\n              resources:\n                requests:\n                  storage: 8Gi\n```\n\n----------------------------------------\n\nTITLE: Defining Internal Listeners in Kubernetes Service Configuration - YAML\nDESCRIPTION: This YAML snippet defines multiple internal listeners for a NiFi cluster within a Kubernetes environment. Each listener specifies a `type` (such as https, cluster, s2s, prometheus, load-balance), a `name` which is used as the port identifier in the pod, and the `containerPort` which corresponds to the port inside the pod that NiFi uses for that listener. These internal listeners facilitate various NiFi functionalities like cluster communication, UI exposure, site-to-site communication, load balancing, and monitoring. Additional listeners without a `type` may also be defined for custom access needs (e.g., HTTP tracking endpoints). Dependencies include a NiFi container configured to use these ports internally.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    - type: \"https\"\n      name: \"https\"\n      containerPort: 8443\n    - type: \"cluster\"\n      name: \"cluster\"\n      containerPort: 6007\n    - type: \"s2s\"\n      name: \"s2s\"\n      containerPort: 10000\n    - type: \"prometheus\"\n      name: \"prometheus\"\n      containerPort: 9090\n    - type: \"load-balance\"\n      name: \"load-balance\"\n      containerPort: 6342\n```\n\n----------------------------------------\n\nTITLE: Defining Istio VirtualService for HTTP in YAML\nDESCRIPTION: This YAML snippet defines an Istio `VirtualService` resource. It redirects traffic intercepted by the previously defined `Gateway` to a specific service running the NiFi cluster. The `gateways` field associates this virtual service with the `nifi-gateway`. The `hosts` field specifies the domain. The `http` section defines the routing rules, in this case, directing all traffic (`/`) to the NiFi service on port 8080. Input is HTTP traffic intercepted by the Gateway, and the output is traffic routed to the NiFi service.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: nifi-vs\nspec:\n  gateways:\n  - nifi-gateway\n  hosts:\n  - nifi.my-domain.com\n  http:\n  - match:\n    - uri:\n        prefix: /\n    route:\n    - destination:\n        host: nifi\n        port:\n          number: 8080\n```\n```\n\n----------------------------------------\n\nTITLE: Creating an Istio VirtualService for HTTP Request Routing to NiFi\nDESCRIPTION: Configures an Istio VirtualService that associates with the 'nifi-gateway' to route all HTTP requests with a URI prefix '/' to the NiFi service listening on port 8080. This enables traffic from the external world to reach the NiFi deployment inside the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: nifi-vs\nspec:\n  gateways:\n  - nifi-gateway\n  hosts:\n  - nifi.my-domain.com\n  http:\n  - match:\n    - uri:\n        prefix: /\n    route:\n    - destination:\n        host: nifi\n        port:\n          number: 8080\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Node Configurations in YAML (NiFiKop)\nDESCRIPTION: Example YAML configuration defining a list of NiFi nodes for a NiFiKop cluster. It demonstrates setting the mandatory `id`, optionally using `nodeConfigGroup` for shared settings (Node 0), applying `readOnlyConfig` to override specific NiFi properties like `nifi.ui.banner.text` (Nodes 0 and 2), and defining detailed `nodeConfig` including Kubernetes resource requests/limits and persistent volume claim (`pvcSpec`) specifications for storage (Node 2).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/5_references/1_nifi_cluster/4_node.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n    - id: 0\n      # nodeConfigGroup can be used to ease the node configuration, if set only the id is required\n      nodeConfigGroup: \"default_group\"\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        nifiProperties:\n          overrideConfigs: |\n            nifi.ui.banner.text=NiFiKop - Node 0\n      # node configuration\n#       nodeConfig:\n    - id: 2\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        overrideConfigs: |\n          nifi.ui.banner.text=NiFiKop - Node 2\n      # node configuration\n      nodeConfig:\n        resourcesRequirements:\n          limits:\n            cpu: \"2\"\n            memory: 3Gi\n          requests:\n            cpu: \"1\"\n            memory: 1Gi\n        storageConfigs:\n          # Name of the storage config, used to name PV to reuse into sidecars for example.\n          - name: provenance-repository\n            # Path where the volume will be mount into the main nifi container inside the pod.\n            mountPath: \"/opt/nifi/provenance_repository\"\n            # Metadata to attach to the PVC that gets created\n            metadata:\n              labels:\n                my-label: my-value\n              annotations:\n                my-annotation: my-value\n            # Kubernetes PVC spec\n            # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n            pvcSpec:\n              accessModes:\n                - ReadWriteOnce\n              storageClassName: \"standard\"\n              resources:\n                requests:\n                  storage: 8Gi\n```\n\n----------------------------------------\n\nTITLE: Configuring NifiUser Resource in Kubernetes for NiFi User Management\nDESCRIPTION: This snippet demonstrates how to define a NifiUser resource in Kubernetes to manage a user in a NiFi cluster. It shows how to specify user identity, cluster reference, certificate preferences, and access policies for components in the NiFi cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/2_manage_users_and_accesses/1_users_management.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: aguitton\nspec:\n  # identity field is use to define the user identity on NiFi cluster side,\n  #\tit use full when the user's name doesn't suite with Kubernetes resource name.\n  identity: alexandre.guitton@konpyutaika.com\n  # Contains the reference to the NifiCluster with the one the registry client is linked.\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  # Whether or not the the operator also include a Java keystore format (JKS) with you secret\n  includeJKS: false\n  # Whether or not a certificate will be created for this user.\n  createCert: false\n  # defines the list of access policies that will be granted to the group.\n  accessPolicies:\n      # defines the kind of access policy, could be \"global\" or \"component\".\n    - type: component\n      # defines the kind of action that will be granted, could be \"read\" or \"write\"\n      action: read\n      # resource defines the kind of resource targeted by this access policies, please refer to the following page:\n      #\thttps://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#access-policies\n      resource: /data\n      # componentType is used if the type is \"component\", it's allow to define the kind of component on which is the\n      # access policy\n      componentType: \"process-groups\"\n      # componentId is used if the type is \"component\", it's allow to define the id of the component on which is the\n      # access policy\n      componentId: \"\"\n```\n\n----------------------------------------\n\nTITLE: Helm Install NiFiKop\nDESCRIPTION: This command demonstrates how to install the NiFiKop Helm chart with a specified release name and custom namespace. It uses the `helm install` command to deploy the chart and sets the `namespaces` parameter.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/helm/nifikop/README.md#_snippet_3\n\nLANGUAGE: console\nCODE:\n```\n$ helm install nifikop konpyutaika-incubator/nifikop --set namespaces={\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring Persistent Storage for NiFi Nodes (NiFiKop YAML)\nDESCRIPTION: This YAML snippet shows how to define persistent storage volumes for a NiFi node configuration using the `storageConfigs` field within a `NodeConfigGroup` or inline `nodeConfig`. Each entry specifies a `mountPath` inside the container, a unique `name`, optional `metadata` (labels/annotations for the generated PVC), and the Kubernetes `pvcSpec` including access modes, resource requests, and storage class. This is essential for ensuring data persistence for critical NiFi repositories and directories.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n...\nstorageConfigs:\n  - mountPath: /opt/nifi/nifi-current/logs\n    name: logs\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/data\n    name: data\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 50Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/extensions\n    name: extensions-repository\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 5Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/flowfile_repository\n    name: flowfile-repository\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/nifi-current/conf\n    name: conf\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 5Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/content_repository\n    name: content-repository\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 500Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/provenance_repository\n    name: provenance-repository\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 500Gi\n      storageClassName: ssd-wait\n```\n\n----------------------------------------\n\nTITLE: Creating a NiFi Client User Resource - YAML\nDESCRIPTION: This YAML snippet can be piped into kubectl to create a NifiUser custom resource, automating certificate issuance for application-level authentication with a NiFi cluster. The manifest details the cluster reference, output secret name, and metadata for unique identification. Applying this resource triggers the operator to generate a client certificate, CA certificate, and private key, all stored in a specified Kubernetes secret. Dependencies include the active deployment of the NiFi Operator and CRDs. Important parameters are 'spec.clusterRef.name' (target cluster) and 'spec.secretName' (destination secret). The output secret will contain base64-encoded SSL artifacts for integration into workloads.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion:  nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: example-client\n  namespace: nifi\nspec:\n  clusterRef:\n    name: nifi\n  secretName: example-client-secret\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFiKop Helm chart with specific version and resource requests\nDESCRIPTION: This snippet shows how to deploy the NiFiKop operator Helm chart with a specific image tag and resource requests/limits. It also creates the required namespace and can be customized for different versions and resource configurations. It requires Helm and Kubernetes cluster access.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/2_deploy_nifikop/1_quick_start.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# You have to create the namespace before executing following command\nhelm install nifikop \\\n    oci://ghcr.io/konpyutaika/helm-charts/nifikop \\\n    --namespace=nifi \\\n    --version 1.11.1 \\\n    --set image.tag=v1.11.1-release \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set namespaces={\"nifi\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Internal Listeners using Nifikop YAML\nDESCRIPTION: Defines the internal listeners for a NiFi cluster within the `listenersConfig` spec of a NifiCluster custom resource. Each listener specifies a `type` (e.g., 'https', 'cluster', 's2s', 'prometheus', 'load-balance'), a `name` used for the pod's port definition, and the `containerPort` the NiFi process will listen on internally.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    - type: \"https\"\n      name: \"https\"\n      containerPort: 8443\n    - type: \"cluster\"\n      name: \"cluster\"\n      containerPort: 6007\n    - type: \"s2s\"\n      name: \"s2s\"\n      containerPort: 10000\n    - type: \"prometheus\"\n      name: \"prometheus\"\n      containerPort: 9090\n    - type: \"load-balance\"\n      name: \"load-balance\"\n      containerPort: 6342\n```\n\n----------------------------------------\n\nTITLE: Verifying Operator Pods\nDESCRIPTION: This console command uses `kubectl` to retrieve the status of pods in the `nifikop` namespace. It verifies the operator is running correctly after deployment. It requires `kubectl` to be configured with access to the Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/6_contributing/1_developer_guide.md#_snippet_9\n\nLANGUAGE: console\nCODE:\n```\n$ kubectl get pods -n nifikop\n```\n\n----------------------------------------\n\nTITLE: Defining Prerequisite NifiDataflow Resources in YAML\nDESCRIPTION: Defines two `NifiDataflow` custom resources named `input` and `output` in the `nifikop` namespace using YAML. These resources represent NiFi dataflows managed by NiFiKop and are required prerequisites before creating a `NifiConnection` between them. Each specifies details like the target NiFi cluster (`nc`), registry information (`registry-client-example`), flow IDs, versions, synchronization strategies (`always`), update strategies (`drain`), and initial positions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/4_manage_connections/1_deploy_connection.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiDataflow\nmetadata:\n  name: input\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  bucketId: deedb9f6-65a4-44e9-a1c9-722008fcd0ba\n  flowId: ab95431d-980d-41bd-904a-fac4bd864ba6\n  flowVersion: 1\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  skipInvalidComponent: true\n  skipInvalidControllerService: true\n  syncMode: always\n  updateStrategy: drain\n  flowPosition:\n    posX: 0\n    posY: 0\n---\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiDataflow\nmetadata:\n  name: output\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  bucketId: deedb9f6-65a4-44e9-a1c9-722008fcd0ba\n  flowId: fc5363eb-801e-432f-aa94-488838674d07\n  flowVersion: 2\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  skipInvalidComponent: true\n  skipInvalidControllerService: true\n  syncMode: always\n  updateStrategy: drain\n  flowPosition:\n    posX: 750\n    posY: 0\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiUser Resource in YAML\nDESCRIPTION: This YAML manifest defines a `NifiUser` custom resource for Kubernetes managed by the nifikop operator. It specifies the user's identity (`alexandre.guitton@konpyutaika.com`), links it to a specific NiFi cluster (`nc` in `nifikop` namespace), controls certificate generation (`createCert: false`, `includeJKS: false`), and grants read access to the `/data` resource for components of type `process-groups`. The operator uses this definition to automatically create or manage the corresponding user in the target NiFi cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/2_manage_users_and_accesses/1_users_management.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: aguitton\nspec:\n  # identity field is use to define the user identity on NiFi cluster side,\n  #\tit use full when the user's name doesn't suite with Kubernetes resource name.\n  identity: alexandre.guitton@konpyutaika.com\n  # Contains the reference to the NifiCluster with the one the registry client is linked.\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  # Whether or not the the operator also include a Java keystore format (JKS) with you secret\n  includeJKS: false\n  # Whether or not a certificate will be created for this user.\n  createCert: false\n  # defines the list of access policies that will be granted to the group.\n  accessPolicies:\n      # defines the kind of access policy, could be \"global\" or \"component\".\n    - type: component\n      # defines the kind of action that will be granted, could be \"read\" or \"write\"\n      action: read\n      # resource defines the kind of resource targeted by this access policies, please refer to the following page:\n      #\thttps://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#access-policies\n      resource: /data\n      # componentType is used if the type is \"component\", it's allow to define the kind of component on which is the\n      # access policy\n      componentType: \"process-groups\"\n      # componentId is used if the type is \"component\", it's allow to define the id of the component on which is the\n      # access policy\n      componentId: \"\"\n```\n\n----------------------------------------\n\nTITLE: Defining Istio VirtualService for HTTP Routing - YAML\nDESCRIPTION: Creates a VirtualService to route incoming traffic from the defined Gateway to the internal NiFi service over HTTP. This resource ensures that requests for the specified domain are matched (by URI prefix) and routed to the NiFi service on port 8080. Prerequisites include a running NiFi service with proper DNS and the previously defined Gateway.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: nifi-vs\nspec:\n  gateways:\n  - nifi-gateway\n  hosts:\n  - nifi.my-domain.com\n  http:\n  - match:\n    - uri:\n        prefix: /\n    route:\n    - destination:\n        host: nifi\n        port:\n          number: 8080\n\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFiKop Operator Helm Chart Using Bash and Helm\nDESCRIPTION: This snippet demonstrates how to deploy the NiFiKop operator Helm chart into a Kubernetes cluster using Helm commands. The snippet requires the Helm client and kubectl configured to access the target namespace. It installs the Helm chart from the GitHub Container Registry using an OCI-based URI, specifying version 1.4.0 and resource requests and limits for CPU and memory. The parameter to restrict cert-manager usage is also noted. This installation method streamlines the operator deployment and management via Helm charts.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/2_deploy_nifikop/1_quick_start.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# You have to create the namespace before executing following command\nhelm install nifikop \\\n    oci://ghcr.io/konpyutaika/helm-charts/nifikop \\\n    --namespace=nifi \\\n    --version 1.4.0 \\\n    --set image.tag=v1.4.0-release \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set namespaces={\"nifi\"}\n```\n\n----------------------------------------\n\nTITLE: Securing NiFi Cluster with Existing Let's Encrypt Issuer in NiFiCluster YAML Configuration\nDESCRIPTION: This snippet shows how to configure NiFiCluster custom resource to use an existing cert-manager Issuer for SSL certificate management. Fields enabled include cluster-wide SSL secures flags, use of external DNS for domain resolution, and referencing the issuer by name and kind within the sslSecrets block. This setup relies on cert-manager integration and existing DNS services enabling automatic certificate renewal and provisioning via Let's Encrypt.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  clusterSecure: true\n  siteToSiteSecure: true\n  ...\n  listenersConfig:\n    clusterDomain: <DNS zone name>\n    useExternalDNS: true\n    ...\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n      issuerRef:\n        name: letsencrypt-staging\n        kind: Issuer\n```\n\n----------------------------------------\n\nTITLE: Defining NifiParameterContext with Inheritance\nDESCRIPTION: This YAML snippet defines a `NifiParameterContext` named `dataflow-lifecycle-child` that inherits from another context. It references the `dataflow-lifecycle` parameter context. It also includes its own parameter, `test`, which overrides the value from the inherited context. The `clusterRef` specifies the NiFi cluster to which this context applies, and the `secretRefs` define which secrets to use for sensitive parameters.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/5_references/4_nifi_parameter_context.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle-child\nspec:\n  description: \"It is a child test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  inheritedParameterContexts:\n    - name: dataflow-lifecycle\n  parameters:\n    - name: test\n      value: toto-child\n      description: tutu (child)\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFi Cluster on Kubernetes (kubectl YAML)\nDESCRIPTION: Creates a NiFi cluster by applying a predefined configuration YAML file within the 'nifi' namespace. This step assumes Zookeeper is already configured and accessible for the NiFi cluster to connect.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create -n nifi -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining NifiRegistryClient Resource with YAML\nDESCRIPTION: This snippet defines a Kubernetes custom resource for a NifiRegistryClient, which establishes a connection from NiFi to a NiFi Registry instance. This resource is necessary for NiFiKop to manage versioned dataflows. Required fields include apiVersion, kind, metadata (name, namespace), and spec (clusterRef to a NiFi cluster, description, URI to the NiFi Registry). The cluster referenced in clusterRef must exist, and the Registry endpoint must be accessible from the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiRegistryClient\nmetadata:\n  name: registry-client-example\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Registry client managed by NiFiKop\"\n  uri: \"http://nifi.hostname.com:18080\"\n\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Dataflow Kubernetes Custom Resource in YAML\nDESCRIPTION: This YAML snippet defines a NifiDataflow custom resource representing a NiFi dataflow deployment within Kubernetes. It specifies API version, resource kind, metadata, and a spec section that declares key parameters including the parent process group UUID, bucket and flow identifiers, flow version, position on the NiFi canvas, synchronization mode, and references to cluster, registry, and parameter context resources. It also defines update strategies and controls for handling invalid components. This resource facilitates declarative management of NiFi flows through Kubernetes operators, enabling lifecycle actions such as updates and synchronization.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/5_references/5_nifi_dataflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  flowPosition:\n    posX: 0\n    posY: 0\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: squidflow\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: nifikop\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Initializing NiFi NodeGroup Autoscaler with YAML\nDESCRIPTION: This YAML snippet defines a NifiNodeGroupAutoscaler Kubernetes custom resource that configures automatic scaling behavior for a specific node group within a NiFiCluster. It requires referencing the target NifiCluster by name and namespace, specifying the node configuration group ID, and defining the node selector labels used to identify managed nodes. The snippet specifies upscale and downscale strategies ('simple' and 'lifo' respectively) that control how nodes are added or removed during autoscaling.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/5_references/7_nifi_nodegroup_autoscaler.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiNodeGroupAutoscaler\nmetadata:\n  name: nifinodegroupautoscaler-sample\nspec:\n  # contains the reference to the NifiCluster with the one the node group autoscaler is linked.\n  clusterRef:\n    name: nificluster-name\n    namespace: nifikop\n  # defines the id of the NodeConfig contained in NifiCluster.Spec.NodeConfigGroups\n  nodeConfigGroupId: default-node-group\n  # The selector used to identify nodes in NifiCluster.Spec.Nodes this autoscaler will manage\n  # Use Node.Labels in combination with this selector to clearly define which nodes will be managed by this autoscaler \n  nodeLabelsSelector: \n    matchLabels:\n      nifi_cr: nificluster-name\n      nifi_node_group: default-node-group\n  # the strategy used to decide how to add nodes to a nifi cluster\n  upscaleStrategy: simple\n  # the strategy used to decide how to remove nodes from an existing cluster\n  downscaleStrategy: lifo\n```\n\n----------------------------------------\n\nTITLE: Deploying NifiNodeGroupAutoscaler for Auto-Scaling - Kubernetes Custom Resource - yaml\nDESCRIPTION: This Kubernetes YAML defines a NifiNodeGroupAutoscaler custom resource, linking it to a NiFi cluster for managing node group autoscaling. Dependency: NiFiKop CRDs must be installed. The spec includes references to the target NiFi cluster, the NodeConfigGroup to scale, node label selectors, and strategies for upscaling and downscaling. Inputs include the cluster and node group names, label selectors, and scaling strategies; output is an autoscaler controller that adds or removes NiFi nodes dynamically based on these configs.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiNodeGroupAutoscaler\nmetadata:\n  name: nifinodegroupautoscaler-sample\n  namespace: clusters\nspec:\n  # contains the reference to the NifiCluster with the one the node group autoscaler is linked.\n  clusterRef:\n    name: cluster\n    namespace: clusters\n  # defines the id of the NodeConfig contained in NifiCluster.Spec.NodeConfigGroups\n  nodeConfigGroupId: auto_scaling\n  # readOnlyConfig can be used to pass Nifi node config\n  # which has type read-only these config changes will trigger rolling upgrade\n  readOnlyConfig:\n    nifiProperties:\n      overrideConfigs: |\n        nifi.ui.banner.text=NiFiKop - Scale Group\n  # This is an example of a node config you can apply to each replica in this node group.\n  # Any settings here will override those in the configured nodeConfigGroupId\n#  nodeConfig:\n#    nodeSelector:\n#      node_type: high-mem\n  # The selector used to identify nodes in NifiCluster.Spec.Nodes this autoscaler will manage\n  # Use Node.Labels in combination with this selector to clearly define which nodes will be managed by this autoscaler \n  nodeLabelsSelector: \n    matchLabels:\n      nifi_cr: cluster\n      nifi_node_group: auto-scaling\n  # the strategy used to decide how to add nodes to a nifi cluster\n  upscaleStrategy: simple\n  # the strategy used to decide how to remove nodes from an existing cluster\n  downscaleStrategy: lifo\n```\n\n----------------------------------------\n\nTITLE: Verifying NiFiKop Operator Pod Status (Console/kubectl)\nDESCRIPTION: Checks the status of pods within the 'nifikop' namespace using 'kubectl get pods'. This command is used to verify that the operator pod, deployed via Helm (e.g., 'skeleton-nifikop-...'), is running correctly.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/6_contributing/1_developer_guide.md#_snippet_10\n\nLANGUAGE: console\nCODE:\n```\nkubectl get pods -n nifikop\n```\n\n----------------------------------------\n\nTITLE: Provisioning a NifiUser for SSL Client Certificates - YAML/Console\nDESCRIPTION: This snippet provides a YAML manifest for a NifiUser CRD, piped to kubectl for application, which will request the operator to generate a new user certificate, store credentials in a Kubernetes secret, and bind the user to a targeted Nifi cluster. It requires Nifikop and its CRDs installed and configured within Kubernetes. The secret will store the CA certificate, user certificate, and private key (and optionally a Java keystore), enabling secure client authentication to the NiFi cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_3\n\nLANGUAGE: console\nCODE:\n```\ncat << EOF | kubectl apply -n nifi -f -\napiVersion:  nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: example-client\n  namespace: nifi\nspec:\n  clusterRef:\n    name: nifi\n  secretName: example-client-secret\nEOF\n```\n\n----------------------------------------\n\nTITLE: Configuring data persistence with storage configs in NiFiKop\nDESCRIPTION: This YAML example displays multiple storage configurations used to define persistent volumes for NiFi nodes, including parameters such as mount paths, PVC specifications, access modes, resource requests, and storage classes, ensuring data durability across pod restarts.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nstorageConfigs:\n  - mountPath: /opt/nifi/nifi-current/logs\n    name: logs\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/data\n    name: data\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 50Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/extensions\n    name: extensions-repository\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 5Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/flowfile_repository\n    name: flowfile-repository\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/nifi-current/conf\n    name: conf\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 5Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/content_repository\n    name: content-repository\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 500Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/provenance_repository\n    name: provenance-repository\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 500Gi\n```\n\n----------------------------------------\n\nTITLE: Configuring External NiFi Cluster with NifiKop Operator using YAML\nDESCRIPTION: This YAML configuration defines an external NiFi cluster resource, setting parameters such as cluster ID, node URIs, authentication type, and secret references. It enables the operator to manage and communicate with an existing NiFi cluster by specifying connection details and security credentials.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/3_manage_nifi/1_manage_clusters/3_external_cluster.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: externalcluster\nspec:\n  rootProcessGroupId: 'd37bee03-017a-1000-cff7-4eaaa82266b7'\n  nodeURITemplate: 'nifi0%d.integ.mapreduce.m0.p.fti.net:9090'\n  nodes:\n    - id: 1\n    - id: 2\n    - id: 3\n  type: 'external'\n  clientType: 'basic'\n  secretRef:\n    name: nifikop-credentials\n    namespace: nifikop-nifi\n```\n\n----------------------------------------\n\nTITLE: Defining NifiParameterContext Custom Resource in Kubernetes YAML\nDESCRIPTION: This YAML snippet defines a Kubernetes custom resource of kind NifiParameterContext for the nifikop operator. It sets up a parameter context named \"dataflow-lifecycle\" with a description, a cluster reference, secret references for storing sensitive parameters, and a list of individual parameters (including both normal and sensitive ones). To use this snippet, you must have the nifikop operator and its CRDs installed in your Kubernetes cluster. Key fields include metadata.name (the context identifier), spec.clusterRef (target NifiCluster), secretRefs (referencing secrets), and the parameters array, where each entry specifies the parameter's name, value, description, and sensitivity. Applying this manifest configures the specified parameter context for use in NiFi workflows; limitations may include required field presence based on the operator version.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/5_references/4_nifi_parameter_context.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      description: toto\n      sensistive: true\n```\n\n----------------------------------------\n\nTITLE: Apply OIDC and Identity Mapping Configs in NiFiKop NifiCluster CRD (YAML)\nDESCRIPTION: Demonstrates how to include OpenID Connect configuration and the recommended identity mapping properties within a NiFiKop `NifiCluster` custom resource definition. The properties are specified under `spec.readOnlyConfig.nifiProperties.overrideConfigs`, allowing NiFiKop to apply custom settings to the NiFi nodes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/1_deploy_cluster/5_users_authentication/1_oidc.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  readOnlyConfig:\n    # NifiProperties configuration that will be applied to the node.\n    nifiProperties:\n      webProxyHosts:\n        - nifistandard2.trycatchlearn.fr:8443\n      # Additionnal nifi.properties configuration that will override the one produced based\n      # on template and configurations.\n      overrideConfigs: |\n        nifi.security.user.oidc.discovery.url=<oidc server discovery url>\n        nifi.security.user.oidc.client.id=<oidc client's id>\n        nifi.security.user.oidc.client.secret=<oidc client's secret>\n        nifi.security.identity.mapping.pattern.dn=CN=([^,]*)(?:, (?:O|OU)=.*)?\n        nifi.security.identity.mapping.value.dn=$1\n        nifi.security.identity.mapping.transform.dn=NONE\n      ...\n   ...\n...\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFi Cluster on OpenShift with kubectl - Bash\nDESCRIPTION: Deploys the NiFi cluster on OpenShift using a manifest updated for the correct user ID. Requires prior update of config/samples/openshift.yaml and a functioning NiFi operator. Must set namespace and validate Zookeeper connectivity in the manifest before use.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create -n nifi -f config/samples/openshift.yaml\n\n```\n\n----------------------------------------\n\nTITLE: Automating NiFi Sensitive Property Encryption Upgrade with Kubernetes InitContainer - YAML\nDESCRIPTION: This YAML snippet configures a Kubernetes initContainer in a NiFi cluster deployment to automate the sensitive property encryption algorithm upgrade using the NiFi Toolkit Docker image. The initContainer runs a shell command that extracts the sensitive properties key from the NiFi configuration and executes the 'encrypt-config.sh' script to re-encrypt both 'flow.json.gz' and 'flow.xml.gz' files with the new algorithm 'NIFI_PBKDF2_AES_GCM_256'. It mounts configuration and data volumes into the container to access necessary files. The snippet emphasizes adapting volumeMount paths to match specific cluster environments. This approach is intended to be used during an operator-controlled upgrade process to minimize manual intervention.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/7_upgrade_guides/4_v1.3.1_to_v1.4.0.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ninitContainers:\n  - image: \"apache/nifi-toolkit:latest\"\n    name: nifi-toolkit\n    imagePullPolicy: Always\n    command:\n      - \"sh\"\n      - \"-c\"\n      - \"NIFI_SENSITIVE_PROPS_KEY=$(grep 'nifi.sensitive.props.key' /opt/nifi/nifi-current/conf/nifi.properties | cut -d'=' -f2) && bin/encrypt-config.sh -n /opt/nifi/nifi-current/conf/nifi.properties -f /opt/nifi/data/flow.json.gz -x -A NIFI_PBKDF2_AES_GCM_256 -s $NIFI_SENSITIVE_PROPS_KEY; bin/encrypt-config.sh -n /opt/nifi/nifi-current/conf/nifi.properties -f /opt/nifi/data/flow.xml.gz -x -A NIFI_PBKDF2_AES_GCM_256 -s $NIFI_SENSITIVE_PROPS_KEY\"\n    volumeMounts:\n      - name: data\n        mountPath: /opt/nifi/data\n      - name: conf\n        mountPath: /opt/nifi/nifi-current/conf\n```\n\n----------------------------------------\n\nTITLE: Verifying Kubernetes Resources - Shell\nDESCRIPTION: This `kubectl` command retrieves information about pods, configmaps, and persistent volume claims that are associated with the node with `nodeId=25`. It helps verify that the resources for the new node were created successfully.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl get pods,configmap,pvc -l nodeId=25\nNAME                          READY   STATUS    RESTARTS   AGE\npod/simplenifi-25-nodem5jh4   1/1     Running   0          11m\n\nNAME                             DATA   AGE\nconfigmap/simplenifi-config-25   7      11m\n\nNAME                                               STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\npersistentvolumeclaim/simplenifi-25-storagehwn24   Bound    pvc-7da86076-728e-11ea-846d-42010a8400f2   10Gi       RWO            standard       11m\n```\n\n----------------------------------------\n\nTITLE: Defining NiFiParameterContext Custom Resource\nDESCRIPTION: This snippet details the CRD for NiFiParameterContext, which enables declaration and management of parameter contexts for NiFi. It supports both non-sensitive parameters in a map and sensitive parameters as secrets, allowing secure parameter handling during dataflow deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/3_manage_dataflows/0_design_principles.md#_snippet_1\n\nLANGUAGE: Markdown\nCODE:\n```\n- **NiFiParameterContext:** Allowing you to create parameter context, with two kinds of parameters, a simple `map[string]string` for non-sensitive parameters and a `list of secrets` containing sensitive parameters.\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Registry Client Resource in YAML\nDESCRIPTION: This YAML snippet defines a NiFiRegistryClient custom resource instance for Kubernetes. It specifies the API version and kind, metadata such as the resource's name, and a spec block that includes cluster reference, description, and the URI of the NiFi registry. The snippet serves as an example of how to configure the client to connect with a specific NiFi registry endpoint.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/5_references/3_nifi_registry_client.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiRegistryClient\nmetadata:\n  name: squidflow\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Squidflow demo\"\n  uri: \"http://nifi-registry:18080\"\n```\n\n----------------------------------------\n\nTITLE: Configuring RBAC Role and RoleBinding for Kubernetes State Management\nDESCRIPTION: Creates Role and RoleBinding that grants permissions to manage 'leases' and 'configmaps' resources within the 'nifi' namespace for the NiFi ServiceAccount. Necessary for NiFi to utilize Kubernetes native state management features.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n---\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: simplenifi\n  namespace: nifi\nrules:\n- apiGroups: [\"coordination.k8s.io\"]\n  resources: [\"leases\"]\n  verbs: [\"*\"]\n- apiGroups: [\"\"]\n  resources: [\"configmaps\"]\n  verbs: [\"*\"]\n---\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: simplenifi\n  namespace: nifi\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: nifi\nroleRef:\n  kind: Role\n  name: simplenifi\n  apiGroup: rbac.authorization.k8s.io\n```\n\n----------------------------------------\n\nTITLE: Configuring Apache NiFi Node with Resource and Storage Settings - YAML\nDESCRIPTION: This YAML snippet demonstrates the configuration structure for Apache NiFi nodes managed via Kubernetes, focusing on node identity, group assignment, read-only property overrides, resource limits, and persistent storage. It requires a Kubernetes cluster and assumes knowledge of NiFi node operation, including usage of PersistentVolumeClaims and resource requirements. Inputs include node IDs, optional nodeConfigGroup names, and configuration for containers (CPU, memory, PVC, mount paths, and metadata). Some changes (such as updates to readOnlyConfig) may trigger rolling upgrades of the node. Limitations include strict YAML syntax, and required fields like id must be unique across nodes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/5_references/1_nifi_cluster/4_node.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n- id: 0\n  # nodeConfigGroup can be used to ease the node configuration, if set only the id is required\n  nodeConfigGroup: \"default_group\"\n  # readOnlyConfig can be used to pass Nifi node config\n  # which has type read-only these config changes will trigger rolling upgrade\n  readOnlyConfig:\n    nifiProperties:\n      overrideConfigs: |\n        nifi.ui.banner.text=NiFiKop - Node 0\n  # node configuration\n# nodeConfig:\n- id: 2\n  # readOnlyConfig can be used to pass Nifi node config\n  # which has type read-only these config changes will trigger rolling upgrade\n  readOnlyConfig:\n    overrideConfigs: |\n      nifi.ui.banner.text=NiFiKop - Node 2\n  # node configuration\n  nodeConfig:\n    resourcesRequirements:\n      limits:\n        cpu: \"2\"\n        memory: 3Gi\n      requests:\n        cpu: \"1\"\n        memory: 1Gi\n    storageConfigs:\n      # Name of the storage config, used to name PV to reuse into sidecars for example.\n      - name: provenance-repository\n        # Path where the volume will be mount into the main nifi container inside the pod.\n        mountPath: \"/opt/nifi/provenance_repository\"\n        # Metadata to attach to the PVC that gets created\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        # Kubernetes PVC spec\n        # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: \"standard\"\n          resources:\n            requests:\n              storage: 8Gi\n```\n\n----------------------------------------\n\nTITLE: Retrieving NiFi User Certificate (Console)\nDESCRIPTION: This console commands retrieve the NiFi user certificates, encoded in base64, and writes them to files. This uses `kubectl` to get the data from the secret created by the `NifiUser` resource. It uses `base64 -d` to decode the certificate data. The output is three files `ca.crt`, `tls.crt`, and `tls.key` containing the certificates.  This relies on having the `kubectl` command available.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['ca\\.crt']}\" | base64 -d > ca.crt\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['tls\\.crt']}\" | base64 -d > tls.crt\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['tls\\.key']}\" | base64 -d > tls.key\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple Node Groups in NiFiKop\nDESCRIPTION: YAML configuration showing how to define multiple node configuration groups with different resource requirements. This example defines a default_group with 3GB RAM and a high_mem_group with 30GB RAM.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  # nodeConfigGroups specifies multiple node configs with unique name\n  nodeConfigGroups:\n    default_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      serviceAccountName: \"default\"\n      # resourceRequirements works exactly like Container resources, the user can specify the limit and the requests\n      # through this property\n      # https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 3Gi\n    high_mem_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      serviceAccountName: \"default\"\n      # resourceRequirements works exactly like Container resources, the user can specify the limit and the requests\n      # through this property\n      # https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 30Gi\n        requests:\n          cpu: \"1\"\n          memory: 30Gi\n```\n\n----------------------------------------\n\nTITLE: Defining Node Configurations based on Groups - YAML\nDESCRIPTION: This YAML snippet shows how to declare nodes of the NiFi cluster using previously defined node configurations groups. It assigns different node groups and individual resource requirements to specific nodes by specifying the `nodeConfigGroup` and `nodeConfig` fields within the node definitions. The output defines the nodes and their configurations within the Kubernetes CRD.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n- id: 0\n  nodeConfigGroup: \"default_group\"\n- id: 2\n  nodeConfigGroup: \"high_mem_group\"\n- id: 3\n  nodeConfigGroup: \"high_mem_group\"\n- id: 5\n  nodeConfig:\n    resourcesRequirements:\n      limits:\n        cpu: \"2\"\n        memory: 3Gi\n      requests:\n        cpu: \"1\"\n        memory: 1Gi\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Parameter Context Custom Resource in YAML\nDESCRIPTION: This YAML snippet defines two NiFiParameterContext custom resources used by the nifikop operator to manage NiFi parameter contexts in Kubernetes. The first resource configures a parameter context named 'dataflow-lifecycle' with description, cluster reference, secret references for sensitive parameters, and parameter lists with names, values, descriptions, and sensitivity flags. The second defines a child parameter context 'dataflow-lifecycle-child' that inherits from the first context and overrides parameters. Dependencies include the nifikop operator and underlying Kubernetes cluster to apply these YAML manifests. Inputs are the field values in metadata and spec sections; outputs are Kubernetes CRD objects representing NiFi parameter contexts.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/5_references/4_nifi_parameter_context.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      description: toto\n      sensistive: true\n---\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle-child\nspec:\n  description: \"It is a child test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  inheritedParameterContexts:\n    - name: dataflow-lifecycle\n  parameters:\n    - name: test\n      value: toto-child\n      description: tutu (child)\n```\n\n----------------------------------------\n\nTITLE: Defining NifiParameterContext resource - parent\nDESCRIPTION: This YAML snippet defines a NifiParameterContext custom resource named `dataflow-lifecycle`. It specifies a description, a cluster reference, a secret reference, and two parameters (`test` and `test2`). The `clusterRef` points to a NifiCluster named `nc` in the `nifikop` namespace. `secretRefs` points to a secret named `secret-params` in the `nifikop` namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/5_references/4_nifi_parameter_context.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      description: toto\n      sensistive: true\n```\n\n----------------------------------------\n\nTITLE: Configuring an External NiFiCluster Resource in YAML\nDESCRIPTION: Defines the YAML configuration for creating an external NiFi cluster resource, specifying cluster identification, node URIs, authentication type, and secret reference for credentials. This setup enables the operator to manage or interact with an external NiFi deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/3_external_cluster.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: externalcluster\nspec:\n  rootProcessGroupId: 'd37bee03-017a-1000-cff7-4eaaa82266b7'\n  nodeURITemplate: 'nifi0%d.integ.mapreduce.m0.p.fti.net:9090'\n  nodes:\n    - id: 1\n    - id: 2\n    - id: 3\n  type: 'external'\n  clientType: 'basic'\n  secretRef:\n    name: nifikop-credentials\n    namespace: nifikop-nifi\n```\n\n----------------------------------------\n\nTITLE: Removing a Node from NiFi Cluster Configuration in YAML\nDESCRIPTION: YAML configuration for removing a node (ID: 2) from an existing NiFi cluster. This configuration triggers the graceful decommissioning process to safely remove the node without data loss.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  headlessServiceEnabled: true\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.11.3\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n# >>>> START: node removed\n#    - id: 2\n#      nodeConfigGroup: \"default_group\"\n# <<<< END\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFiKop Helm Chart with Specific Version and Resources\nDESCRIPTION: This command installs the NiFiKop operator Helm chart from GitHub OCI registry, specifying version, image tag, resource requests and limits, and target namespace. Use this for straightforward deployment with custom resource allocations and to target a specific NiFiKop release.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/2_deploy_nifikop/1_quick_start.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Install NiFiKop Helm chart with custom parameters\nhelm install nifikop \\\n    oci://ghcr.io/konpyutaika/helm-charts/nifikop \\\n    --namespace=nifi \\\n    --version 1.3.0 \\\n    --set image.tag=v1.3.0-release \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set namespaces={\"nifi\"}\n```\n\n----------------------------------------\n\nTITLE: Defining External NifiCluster resource in YAML\nDESCRIPTION: This YAML snippet demonstrates how to define a NifiCluster resource for an external NiFi cluster. It includes essential configurations such as the root process group ID, node URI template, node IDs, cluster type (external), client type (basic), and a reference to a Kubernetes secret containing authentication credentials. This configuration enables NiFiKop to manage dataflows on the external NiFi cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/3_manage_nifi/1_manage_clusters/3_external_cluster.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: externalcluster\nspec:\n  # rootProcessGroupId contains the uuid of the root process group for this cluster.\n  rootProcessGroupId: 'd37bee03-017a-1000-cff7-4eaaa82266b7'\n  # nodeURITemplate used to dynamically compute node uri.\n  nodeURITemplate: 'nifi0%d.integ.mapreduce.m0.p.fti.net:9090'\n  # all node requiresunique id\n  nodes:\n    - id: 1\n    - id: 2\n    - id: 3\n  # type defines if the cluster is internal (i.e manager by the operator) or external.\n  # :Enum={\"external\",\"internal\"}\n  type: 'external'\n  # clientType defines if the operator will use basic or tls authentication to query the NiFi cluster.\n  # Enum={\"tls\",\"basic\"}\n  clientType: 'basic'\n  # secretRef reference the secret containing the informations required to authenticate to the cluster.\n  secretRef:\n    name: nifikop-credentials\n    namespace: nifikop-nifi\n```\n\n----------------------------------------\n\nTITLE: Declaring cluster nodes with specific configuration groups\nDESCRIPTION: This YAML snippet illustrates how to assign node IDs to specific configuration groups or individual configurations, enabling differentiated node setups within the cluster, by specifying 'nodeConfigGroup' or 'nodeConfig' per node.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n- id: 0\n  nodeConfigGroup: \"default_group\"\n- id: 2\n  nodeConfigGroup: \"high_mem_group\"\n- id: 3\n  nodeConfigGroup: \"high_mem_group\"\n- id: 5\n  nodeConfig:\n    resourcesRequirements:\n      limits:\n        cpu: \"2\"\n        memory: 3Gi\n      requests:\n        cpu: \"1\"\n        memory: 1Gi\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiParameterContext Resource (YAML)\nDESCRIPTION: Example YAML manifest for creating a `NifiParameterContext` custom resource. This defines a parameter context within the target NiFi cluster (`clusterRef`), allowing parameters (including sensitive ones sourced from Kubernetes secrets via `secretRefs`) to be applied to dataflows. Parameters are defined under the `parameters` list.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\n  namespace: demo\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      value: toto\n      description: toto\n```\n\n----------------------------------------\n\nTITLE: Deploying a NiFi Connection Between Dataflows Using YAML Kubernetes CRD\nDESCRIPTION: Defines a NifiConnection CRD resource in YAML format named 'connection' to establish a connection between two NiFiDataflows named 'input' and 'output' within the 'nifikop' namespace. The spec includes source and destination references to dataflows with subNames specifying ports (output port on 'input' dataflow and input port on 'output' dataflow). Connection configurations such as back pressure thresholds, flow file expiration, label positioning with bends (coordinates), and update strategy are specified. Users must ensure that the dataflows have the corresponding ports for this connection to work. This CRD enables declarative management of NiFi connections in a Kubernetes environment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/4_manage_connections/1_deploy_connection.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiConnection\nmetadata:\n  name: connection\n  namespace: nifikop\nspec:\n  source:\n    name: input\n    namespace: nifikop\n    subName: output\n    type: dataflow\n  destination:\n    name: output\n    namespace: nifikop\n    subName: input\n    type: dataflow\n  configuration:\n    backPressureDataSizeThreshold: 100 GB\n    backPressureObjectThreshold: 10000\n    flowFileExpiration: 1 hour\n    labelIndex: 0\n    bends:\n      - posX: 550\n        posY: 550\n      - posX: 550\n        posY: 440\n      - posX: 550\n        posY: 88\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Defining ReadOnlyConfig in NiFiKop (YAML)\nDESCRIPTION: This YAML snippet shows how to define a ReadOnlyConfig object in NiFiKop. It includes configurations for maximum thread counts, logback, authorizer, NiFi properties, Zookeeper properties, and bootstrap properties, all merged with node-specific configurations. It shows how to override configs using configmaps, secrets, and direct overrides.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/5_references/1_nifi_cluster/2_read_only_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreadOnlyConfig:\n  # MaximumTimerDrivenThreadCount define the maximum number of threads for timer driven processors available to the system.\n  maximumTimerDrivenThreadCount: 30\n  # MaximumEventDrivenThreadCount define the maximum number of threads for event driven processors available to the system (@DEPRECATED. This has no effect from NiFiKOp v1.9.0 or later).\n  maximumEventDrivenThreadCount: 10\n  # Logback configuration that will be applied to the node\n  logbackConfig:\n    # logback.xml configuration that will replace the one produced based on template\n    replaceConfigMap:\n      # The key of the value,in data content, that we want use.\n      data: logback.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # logback.xml configuration that will replace the one produced based on template and overrideConfigMap\n    replaceSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: logback.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n  # Authorizer configuration that will be applied to the node\n  authorizerConfig:\n    # An authorizers.xml configuration template that will replace the default template seen in authorizers.go\n    replaceTemplateConfigMap:\n      # The key of the value, in data content, that we want use.\n      data: authorizers.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # An authorizers.xml configuration template that will replace the default template seen in authorizers.go and the replaceTemplateConfigMap\n    replaceTemplateSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: authorizers.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n  # NifiProperties configuration that will be applied to the node.\n  nifiProperties:\n    # Additionnal nifi.properties configuration that will override the one produced based on template and\n    # configuration\n    overrideConfigMap:\n      # The key of the value,in data content, that we want use.\n      data: nifi.properties\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop.\n    # Additionnal nifi.properties configuration that will override the one produced based\n    #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    overrideSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: nifi.properties\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # Additionnal nifi.properties configuration that will override the one produced based\n    #\ton template, configurations and overrideConfigMap\n    overrideConfigs: |\n      nifi.ui.banner.text=NiFiKop\n    # A comma separated list of allowed HTTP Host header values to consider when NiFi\n    # is running securely and will be receiving requests to a different host[:port] than it is bound to.\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#web-properties\n    #      webProxyHosts:\n    # Nifi security client auth\n    needClientAuth: false\n    # Indicates which of the configured authorizers in the authorizers.xml file to use\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#authorizer-configuration\n  #      authorizer:\n  # ZookeeperProperties configuration that will be applied to the node.\n  zookeeperProperties:\n    #      # Additionnal zookeeeper.properties configuration that will override the one produced based on template and\n    #      # configuration\n    #      overrideConfigMap:\n    #        # The key of the value,in data content, that we want use.\n    #        data: zookeeeper.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop.\n    #      # Additionnal zookeeeper.properties configuration that will override the one produced based\n    #      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    #      overrideSecretConfig:\n    #        # The key of the value,in data content, that we want use.\n    #        data: zookeeeper.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop\n    # Additionnal zookeeper.properties configuration that will override the one produced based\n    # on template and configurations.\n    overrideConfigs: |\n      initLimit=15\n      autopurge.purgeInterval=24\n      syncLimit=5\n      tickTime=2000\n      dataDir=./state/zookeeper\n      autopurge.snapRetainCount=30\n  # BootstrapProperties configuration that will be applied to the node.\n  bootstrapProperties:\n    #      # Additionnal bootstrap.conf configuration that will override the one produced based on template and\n    #      # configuration\n    #      overrideConfigMap:\n    #        # The key of the value,in data content, that we want use.\n    #        data: bootstrap.conf\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop.\n    #      # Additionnal bootstrap.conf configuration that will override the one produced based\n    #      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    #      overrideSecretConfig:\n    #        # The key of the value,in data content, that we want use.\n    #        data: bootstrap.conf\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop\n    # JVM memory settings\n    nifiJvmMemory: \"512m\"\n    # Additionnal bootstrap.conf configuration that will override the one produced based\n    # on template and configurations.\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#bootstrap_properties\n    overrideConfigs: |\n      # java.arg.4=-Djava.net.preferIPv4Stack=true\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiDataflow Resource in YAML\nDESCRIPTION: Example YAML manifest for creating a `NifiDataflow` custom resource in Kubernetes using NiFiKop. This resource instructs NiFiKop to deploy and manage a specific versioned flow from a NiFi Registry (`bucketId`, `flowId`, `flowVersion`). It references the target NiFi cluster (`clusterRef`), the registry client (`registryClientRef`), an optional parameter context (`parameterContextRef`), and defines synchronization (`syncMode`) and update strategies (`updateStrategy`).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/3_tasks/3_nifi_dataflow.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: demo\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Get UID/GID on OpenShift for NiFi\nDESCRIPTION: This bash command retrieves the UID/GID for the NiFi deployment namespace using `kubectl get namespace` command. This UID is then used to correctly configure security contexts for the NiFi cluster to work correctly inside the OpenShift environment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nuid=$(kubectl get namespace nifi -o=jsonpath='{.metadata.annotations.openshift\\.io/sa\\.scc\\.supplemental-groups}' | sed 's//10000$//' | tr -d '[:space:]')\n```\n\n----------------------------------------\n\nTITLE: Declaring NiFi Cluster Nodes with Group and Per-Node Configurations in YAML\nDESCRIPTION: This YAML example shows how to declare individual NiFi cluster nodes using previously defined node configuration groups or custom per-node settings. Each node entry binds an ID to its config group or overrides resource parameters directly for one-off customization. Dependencies are valid nodeConfigGroups and the Kubernetes cluster managed by NiFiKop. IDs define cluster membership, and the nodeConfig property accepts the same resource structure as reusable groups. Uniqueness of IDs is required, and nodes not property configured may result in misconfigured pods or scheduling failures.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n- id: 0\n  nodeConfigGroup: \"default_group\"\n- id: 2\n  nodeConfigGroup: \"high_mem_group\"\n- id: 3\n  nodeConfigGroup: \"high_mem_group\"\n- id: 5\n  nodeConfig:\n    resourcesRequirements:\n      limits:\n        cpu: \"2\"\n        memory: 3Gi\n      requests:\n        cpu: \"1\"\n        memory: 1Gi\n```\n\n----------------------------------------\n\nTITLE: Creating NiFiRegistryClient Kubernetes CRD in YAML\nDESCRIPTION: Defines a NiFiRegistryClient custom resource to configure connectivity from NiFiKop to a NiFi Registry instance. The resource specifies cluster references, namespace, and the URI of the NiFi Registry service. This setup is required for NiFiKop to manage dataflows via the NiFi Registry feature.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiRegistryClient\nmetadata:\n  name: registry-client-example\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Registry client managed by NiFiKop\"\n  uri: \"http://nifi.hostname.com:18080\"\n```\n\n----------------------------------------\n\nTITLE: Configuring External NiFi Cluster in YAML\nDESCRIPTION: This YAML example demonstrates how to configure an external NiFi cluster. It includes specifications for the root process group ID, node URI template, node IDs, cluster type, authentication type, and secret reference for credentials.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/3_external_cluster.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: externalcluster\nspec:\n  # rootProcessGroupId contains the uuid of the root process group for this cluster.\n  rootProcessGroupId: 'd37bee03-017a-1000-cff7-4eaaa82266b7'\n  # nodeURITemplate used to dynamically compute node uri.\n  nodeURITemplate: 'nifi0%d.integ.mapreduce.m0.p.fti.net:9090'\n  # all node requiresunique id\n  nodes:\n    - id: 1\n    - id: 2\n    - id: 3\n  # type defines if the cluster is internal (i.e manager by the operator) or external.\n  # :Enum={\"external\",\"internal\"}\n  type: 'external'\n  # clientType defines if the operator will use basic or tls authentication to query the NiFi cluster.\n  # Enum={\"tls\",\"basic\"}\n  clientType: 'basic'\n  # secretRef reference the secret containing the informations required to authenticate to the cluster.\n  secretRef:\n    name: nifikop-credentials\n    namespace: nifikop-nifi\n```\n\n----------------------------------------\n\nTITLE: Installing Prometheus Operator Using Helm in Kubernetes Console\nDESCRIPTION: This Helm install command deploys the kube-prometheus-stack package into the 'monitoring-system' namespace with customized values disabling most default exporters and dashboard features except the operator and its basic monitoring functionality. Flags like 'prometheusOperator.createCustomResource' and disabling alertmanager and Grafana tailor the deployment for integration with the NiFi cluster autoscaling scenario. Helm and Kubernetes access with sufficient privileges are prerequisites.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_7\n\nLANGUAGE: console\nCODE:\n```\nhelm install prometheus prometheus/kube-prometheus-stack --namespace monitoring-system \\\n    --set prometheusOperator.createCustomResource=false \\\n    --set prometheusOperator.logLevel=debug \\\n    --set prometheusOperator.alertmanagerInstanceNamespaces=monitoring-system \\\n    --set prometheusOperator.namespaces.additional[0]=monitoring-system \\\n    --set prometheusOperator.prometheusInstanceNamespaces=monitoring-system \\\n    --set prometheusOperator.thanosRulerInstanceNamespaces=monitoring-system \\\n    --set defaultRules.enable=false \\\n    --set alertmanager.enabled=false \\\n    --set grafana.enabled=false \\\n    --set kubeApiServer.enabled=false \\\n    --set kubelet.enabled=false \\\n    --set kubeControllerManager.enabled=false \\\n    --set coreDNS.enabled=false \\\n    --set kubeEtcd.enabled=false \\\n    --set kubeScheduler.enabled=false \\\n    --set kubeProxy.enabled=false \\\n    --set kubeStateMetrics.enabled=false \\\n    --set prometheus.enabled=false\n```\n\n----------------------------------------\n\nTITLE: Defining multiple node configuration groups in NiFiKop\nDESCRIPTION: This snippet demonstrates how to define multiple node configuration groups with different resource and storage requirements in YAML format. Each group specifies parameters such as storage limits, user IDs, service accounts, and resource limits, enabling flexible cluster node setups.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nnodeConfigGroups:\n  default_group:\n    provenanceStorage: \"10 GB\"\n    runAsUser: 1000\n    serviceAccountName: \"default\"\n    resourcesRequirements:\n      limits:\n        cpu: \"2\"\n        memory: 3Gi\n      requests:\n        cpu: \"1\"\n        memory: 3Gi\n  high_mem_group:\n    provenanceStorage: \"10 GB\"\n    runAsUser: 1000\n    serviceAccountName: \"default\"\n    resourcesRequirements:\n      limits:\n        cpu: \"2\"\n        memory: 30Gi\n      requests:\n        cpu: \"1\"\n        memory: 30Gi\n```\n\n----------------------------------------\n\nTITLE: Removing Node Configuration (YAML)\nDESCRIPTION: This YAML snippet demonstrates how to remove a node from the NiFi cluster configuration.  It involves modifying the `NifiCluster.Spec.Nodes` list by removing the node definition that corresponds to the `id` you want to remove. The only prerequisite is a running Kubernetes cluster with the NiFiKop operator. The output is the configuration of the NiFi cluster with the node removed, ready to be applied via kubectl.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  headlessServiceEnabled: true\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.11.3\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n# >>>> START: node removed\n#    - id: 2\n#      nodeConfigGroup: \"default_group\"\n# <<<< END\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n```\n\n----------------------------------------\n\nTITLE: Configuring Persistent Storage for NiFi Nodes using NiFiKop StorageConfigs in YAML\nDESCRIPTION: Provides an example of configuring persistent storage for various essential NiFi directories (`logs`, `data`, `extensions`, `flowfile_repository`, `conf`, `content_repository`, `provenance_repository`) using the `storageConfigs` field within a `NodeConfigGroup`. Each entry defines a `mountPath` inside the NiFi container, a unique `name`, and a Kubernetes PersistentVolumeClaim specification (`pvcSpec`) detailing access modes, requested storage size, and the `storageClassName`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n...\nstorageConfigs:\n  - mountPath: /opt/nifi/nifi-current/logs\n    name: logs\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/data\n    name: data\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 50Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/extensions\n    name: extensions-repository\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 5Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/flowfile_repository\n    name: flowfile-repository\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/nifi-current/conf\n    name: conf\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 5Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/content_repository\n    name: content-repository\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 500Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/provenance_repository\n    name: provenance-repository\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 500Gi\n```\n\n----------------------------------------\n\nTITLE: Configure NiFiCluster to use existing Issuer\nDESCRIPTION: This YAML snippet configures a NifiCluster resource to use an existing cert-manager Issuer for SSL certificates. It sets `clusterSecure` and `siteToSiteSecure` to `true`, configures the `clusterDomain` and `useExternalDNS` flags, and defines an `issuerRef` pointing to the Let's Encrypt issuer. The `create: true` flag specifies that the TLS secret should be created using the provided issuer.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  clusterSecure: true\n  siteToSiteSecure: true\n  ...\n  listenersConfig:\n    clusterDomain: <DNS zone name>\n    useExternalDNS: true\n    ...\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n      issuerRef:\n        name: letsencrypt-staging\n        kind: Issuer\n```\n\n----------------------------------------\n\nTITLE: Uninstalling the Nifikop Helm Chart\nDESCRIPTION: Uses the `helm del` (alias for `helm uninstall`) command to remove the Nifikop Helm release named 'nifikop' and its associated Kubernetes resources managed by Helm. Note that CRDs installed by the chart might need separate manual deletion.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nhelm del nifikop\n```\n\n----------------------------------------\n\nTITLE: Example NiFi Node Configuration (YAML)\nDESCRIPTION: This YAML snippet illustrates a sample `NodeConfig` configuration for an Apache NiFi node within a Kubernetes cluster managed by the nifikop operator. It demonstrates setting provenance storage size, user ID, cluster node status, pod metadata (annotations, labels), image pull policy, priority class, mounting external volumes from secrets, and configuring persistent storage using PVCs for provenance and logs with specific access modes, storage classes, and sizes. The configuration references Kubernetes concepts like PVCs, Secrets, Priority Classes, and Pod Metadata.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/5_references/1_nifi_cluster/3_node_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n   default_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      # Set this to true if the instance is a node in a cluster.\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#basic-cluster-setup\n      isNode: true\n      # Additionnal metadata to merge to the pod associated\n      podMetadata:\n        annotations:\n          node-annotation: \"node-annotation-value\"\n        labels:\n          node-label: \"node-label-value\"\n      # Docker image used by the operator to create the node associated\n      # https://hub.docker.com/r/apache/nifi/\n#      image: \"apache/nifi:1.11.2\"\n      # nodeAffinity can be specified, operator populates this value if new pvc added later to node\n      # https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity\n#      nodeAffinity:\n      # imagePullPolicy define the pull policy for NiFi cluster docker image\n      imagePullPolicy: IfNotPresent\n      # priorityClassName define the name of the priority class to be applied to these nodes\n      priorityClassName: \"example-priority-class-name\"\n      # externalVolumeConfigs specifies a list of volume to mount into the main container.\n      externalVolumeConfigs:\n        - name: example-volume\n          mountPath: \"/opt/nifi/example\"\n          secret:\n            secretName: \"raw-controller\"\n      # storageConfigs specifies the node related configs\n      storageConfigs:\n        # Name of the storage config, used to name PV to reuse into sidecars for example.\n        - name: provenance-repository\n          # Retain this PVC throughout NifiCluster deletions.\n          reclaimPolicy: Retain\n          # Path where the volume will be mount into the main nifi container inside the pod.\n          mountPath: \"/opt/nifi/provenance_repository\"\n          # Metadata to attach to the PVC that gets created\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          # Kubernetes PVC spec\n          # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          reclaimPolicy: Delete\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiUser resource in YAML for Kubernetes\nDESCRIPTION: Example YAML configuration for creating a NifiUser resource that references a NiFi cluster. This example creates a user without certificate generation, using an email address as the identity.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/5_references/2_nifi_user.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: aguitton\nspec:\n  identity: alexandre.guitton@konpyutaika.com\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  createCert: false\n```\n\n----------------------------------------\n\nTITLE: Sample YAML configuration for NiFi Registry Client resource\nDESCRIPTION: This YAML snippet defines a NiFiRegistryClient custom resource, specifying metadata, cluster reference, description, and URI. It is intended to configure and deploy a NiFi registry client within a Kubernetes environment, enabling interaction with a NiFi registry at the specified URI.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/5_references/3_nifi_registry_client.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiRegistryClient\nmetadata:\n  name: squidflow\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Squidflow demo\"\n  uri: \"http://nifi-registry:18080\"\n```\n\n----------------------------------------\n\nTITLE: Creating NifiParameterContext YAML\nDESCRIPTION: This YAML defines a NifiParameterContext resource.  It configures a parameter context that can be applied to a NiFi dataflow. It allows you to define parameters, including sensitive parameters referenced from secrets. The example includes a cluster reference, a description, and a reference to a secret containing sensitive parameters.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\n  namespace: demo\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      value: toto\n      description: toto\n```\n\n----------------------------------------\n\nTITLE: Example Node Declaration Using NodeConfigGroups\nDESCRIPTION: Shows how to declare individual nodes linked to predefined node configuration groups, including assigning specific configuration groups or defining unique node-specific configurations directly within the node spec.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\n- id: 0\n  nodeConfigGroup: \"default_group\"\n- id: 2\n  nodeConfigGroup: \"high_mem_group\"\n- id: 3\n  nodeConfigGroup: \"high_mem_group\"\n- id: 5\n  nodeConfig:\n    resourcesRequirements:\n      limits:\n        cpu: \"2\"\n        memory: 3Gi\n      requests:\n        cpu: \"1\"\n        memory: 1Gi\n```\n\n----------------------------------------\n\nTITLE: Configuring NifiParameterContext CRD in YAML\nDESCRIPTION: This YAML snippet creates a NifiParameterContext resource containing parameters and secret references. The parameters include simple key-value pairs, and secretRefs connect to Kubernetes secrets containing sensitive data. It enables dynamic and secure configuration of NiFi dataflows.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\n  namespace: demo\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      value: toto\n      description: toto\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Cluster with External DNS and Issuer (YAML)\nDESCRIPTION: This YAML snippet configures the NiFi cluster to use an external DNS and the Let's Encrypt issuer. It relies on setting the `clusterDomain`, `useExternalDNS` and  `issuerRef` in `listenersConfig.sslSecrets`. It assumes that the issuer and external DNS are already configured.  The output is a NiFi cluster configured to leverage the Let's Encrypt issued certificate.  It needs `cert-manager` and external DNS to be correctly configured.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  clusterSecure: true\n  siteToSiteSecure: true\n  ...\n  listenersConfig:\n    clusterDomain: <DNS zone name>\n    useExternalDNS: true\n    ...\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n      issuerRef:\n        name: letsencrypt-staging\n        kind: Issuer\n```\n\n----------------------------------------\n\nTITLE: Configuring Listeners for NiFi Cluster using YAML\nDESCRIPTION: Example YAML configuration defining internal listeners with various types (https, cluster, s2s, prometheus, load-balance, custom) and SSL secret configuration. This setup specifies container ports for different communication channels and enables automatic TLS certificate creation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/5_references/1_nifi_cluster/6_listeners_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  listenersConfig:\n    internalListeners:\n      - type: \"https\"\n        name: \"https\"\n        containerPort: 8443\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n      - type: \"prometheus\"\n        name: \"prometheus\"\n        containerPort: 9090\n      - type: \"load-balance\"\n        name: \"load-balance\"\n        containerPort: 6342\n      - name: \"my-custom-listener-port\"\n        containerPort: 1234\n        protocol: \"TCP\"\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n```\n\n----------------------------------------\n\nTITLE: Configuring ClusterIP External Service (YAML)\nDESCRIPTION: This YAML snippet demonstrates how to define an `externalService` of type `ClusterIP` within the Nifikop Custom Resource. It configures a service named \"clusterip\" that exposes ports 8080 and 7182, mapping them to internal Nifi listeners named \"http\" and \"my-custom-listener\" respectively, with the latter using the TCP protocol. Optional Kubernetes service annotations and labels are also included.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/5_references/1_nifi_cluster/7_external_service_config.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n  externalServices:\n    - name: \"clusterip\"\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8080\n            internalListenerName: \"http\"\n          - port: 7182\n            internalListenerName: \"my-custom-listener\"\n            protocol: TCP\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n\n----------------------------------------\n\nTITLE: Defining Node Configuration Parameters in YAML for Kubernetes NiFi Deployment\nDESCRIPTION: This YAML snippet describes the node configuration schema for deploying an Apache NiFi node on Kubernetes. It includes settings for storage, user permissions, image details, affinity, node selection, resource requirements, and custom metadata. Dependencies include Kubernetes API objects and NiFi deployment guidelines. Key parameters include 'provenanceStorage' for data provenance limits, 'runAsUser' for user ID, and 'storageConfigs' for persistent storage setup.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/5_references/1_nifi_cluster/3_node_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n   default_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      # Set this to true if the instance is a node in a cluster.\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#basic-cluster-setup\n      isNode: true\n      # Additionnal metadata to merge to the pod associated\n      podMetadata:\n        annotations:\n          node-annotation: \"node-annotation-value\"\n        labels:\n          node-label: \"node-label-value\"\n      # Docker image used by the operator to create the node associated\n      # https://hub.docker.com/r/apache/nifi/\n#      image: \"apache/nifi:1.11.2\"\n      # nodeAffinity can be specified, operator populates this value if new pvc added later to node\n      # https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity\n#      nodeAffinity:\n      # imagePullPolicy define the pull policy for NiFi cluster docker image\n      imagePullPolicy: IfNotPresent\n      # priorityClassName define the name of the priority class to be applied to these nodes\n      priorityClassName: \"example-priority-class-name\"\n      # externalVolumeConfigs specifies a list of volume to mount into the main container.\n      externalVolumeConfigs:\n        - name: example-volume\n          mountPath: \"/opt/nifi/example\"\n          secret:\n            secretName: \"raw-controller\"\n      # storageConfigs specifies the node related configs\n      storageConfigs:\n        # Name of the storage config, used to name PV to reuse into sidecars for example.\n        - name: provenance-repository\n          # Path where the volume will be mount into the main nifi container inside the pod.\n          mountPath: \"/opt/nifi/provenance_repository\"\n          # Metadata to attach to the PVC that gets created\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          # Kubernetes PVC spec\n          # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n```\n\n----------------------------------------\n\nTITLE: Running NiFiKop Operator Locally via Make (Bash)\nDESCRIPTION: Executes the 'make run' command, which typically builds and runs the NiFiKop operator Go binary locally. It connects to the Kubernetes cluster specified in the default kubeconfig file ($HOME/.kube/config) and operates within the 'default' namespace unless overridden by environment variables like WATCH_NAMESPACE.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/6_contributing/1_developer_guide.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmake run\n```\n\n----------------------------------------\n\nTITLE: Removing a node from the NiFi cluster configuration in YAML\nDESCRIPTION: This YAML snippet demonstrates how to remove a node (with id 2) from the NiFi cluster by excluding it from the 'nodes' list. It is used to configure a graceful scale-down procedure, adhering to decommissioning steps. Dependencies include the cluster CRD and Kubernetes. After modification, the configuration is applied to trigger node removal and resource cleanup.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  headlessServiceEnabled: true\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.11.3\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n# >>>> START: node removed\n#    - id: 2\n#      nodeConfigGroup: \"default_group\"\n# <<<< END\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiRegistryClient resource in YAML\nDESCRIPTION: This example demonstrates how to create a NifiRegistryClient Kubernetes resource that connects a NiFi cluster to a NiFi Registry instance. It references an existing NiFi cluster 'nc' in the 'nifikop' namespace and specifies the Registry's URI.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/5_references/3_nifi_registry_client.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiRegistryClient\nmetadata:\n  name: squidflow\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Squidflow demo\"\n  uri: \"http://nifi-registry:18080\"\n```\n\n----------------------------------------\n\nTITLE: Defining NifiRegistryClient Resource in YAML\nDESCRIPTION: This YAML snippet demonstrates how to define a `NifiRegistryClient` custom resource for the nifikop operator. It specifies the API version, kind, metadata, and the `spec` including the `clusterRef` to the associated NiFi cluster, a descriptive name, and the URI of the NiFi Registry instance.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/5_references/3_nifi_registry_client.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiRegistryClient\nmetadata:\n  name: squidflow\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Squidflow demo\"\n  uri: \"http://nifi-registry:18080\"\n```\n\n----------------------------------------\n\nTITLE: Overriding NiFi Properties with ConfigMap, Secret, and Inline YAML (YAML)\nDESCRIPTION: This YAML snippet shows how to override NiFi configuration properties in a NiFiCluster object using overrideConfigMap, overrideSecretConfig, and overrideConfigs fields. It demonstrates referencing a data key in a ConfigMap or Secret for sensitive or general configurations and providing inline property overrides directly in the resource specification. Requires a NiFiCluster custom resource, Kubernetes, the specified ConfigMaps and Secrets, and nifikop operator. The priority of values is Secret > ConfigMap > Inline Override > Default, and the structure supports specifying the data key, resource name, and namespace. Inputs are Kubernetes resource references and configuration values; outputs are resolved NiFi property values in the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nnifiProperties:\n      # Additionnal nifi.properties configuration that will override the one produced based on template and\n      # configuration\n      overrideConfigMap:\n        # The key of the value,in data content, that we want use.\n        data: nifi.properties\n        # Name of the configmap that we want to refer.\n        name: raw\n        # Namespace where is located the secret that we want to refer.\n        namespace: nifikop\n      # Additionnal nifi.properties configuration that will override the one produced based\n      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n      overrideSecretConfig:\n        # The key of the value,in data content, that we want use.\n        data: nifi.properties\n        # Name of the configmap that we want to refer.\n        name: raw\n        # Namespace where is located the secret that we want to refer.\n        namespace: nifikop\n      # Additionnal nifi.properties configuration that will override the one produced based\n      #\ton template, configurations and overrideConfigMap\n      overrideConfigs: |\n        nifi.ui.banner.text=NiFiKop\n        nifi.sensitive.props.key=thisIsABadSensitiveKeyPassword\n\n```\n\n----------------------------------------\n\nTITLE: Configuring NifiCluster for Basic SSL (YAML)\nDESCRIPTION: Example YAML configuration for a `NifiCluster` custom resource to enable SSL. It specifies HTTPS internal listeners on port 8443, enables automatic SSL certificate creation (`create: true`), defines the secret name (`tlsSecretName`) for storing certificates, and configures `webProxyHosts` for secure web access.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  readOnlyConfig:\n    # NifiProperties configuration that will be applied to the node.\n    nifiProperties:\n      webProxyHosts:\n        - nifistandard2.trycatchlearn.fr:8443\n        ...\n  ...\n  listenersConfig:\n    internalListeners:\n      - type: \"https\"\n        name: \"https\"\n        containerPort: 8443\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n```\n\n----------------------------------------\n\nTITLE: Creating Role and RoleBinding for Kubernetes State Management\nDESCRIPTION: This YAML defines a Kubernetes Role and RoleBinding to provide the NiFi ServiceAccount with the necessary permissions to manage leases and configmaps. This allows the NiFi cluster to manage its own state using Kubernetes resources. The Role defines the specific permissions required and the RoleBinding binds those permissions to the 'default' ServiceAccount within the 'nifi' namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: simplenifi\n  namespace: nifi\nrules:\n- apiGroups: [\"coordination.k8s.io\"]\n  resources: [\"leases\"]\n  verbs: [\"*\"]\n- apiGroups: [\"\"]\n  resources: [\"configmaps\"]\n  verbs: [\"*\"]\n---\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: simplenifi\n  namespace: nifi\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: nifi\nroleRef:\n  kind: Role\n  name: simplenifi\n  apiGroup: rbac.authorization.k8s.io\n```\n\n----------------------------------------\n\nTITLE: Declaring External NiFi Cluster Connection with NifiKop Operator (YAML)\nDESCRIPTION: This YAML snippet defines a NifiCluster Kubernetes resource configured for referencing an external Apache NiFi cluster with basic authentication. Key fields include apiVersion, kind, metadata, and spec. Under spec, fields such as rootProcessGroupId, nodeURITemplate, nodes array (with unique int32 IDs), cluster type (set to 'external'), clientType, and secretRef (pointing to the authentication secret) are critical. Dependencies: Kubernetes cluster with NifiKop operator installed, valid NiFi cluster, existing secret containing authentication data. Inputs: field values tailored to your cluster environment. Outputs: Kubernetes resource for managing Dataflow lifecycle through the operator. Limitation: Node IDs must be int32, and this configuration targets clusters accessible via the provided host/naming structure.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/1_manage_clusters/3_external_cluster.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: externalcluster\nspec:\n  # rootProcessGroupId contains the uuid of the root process group for this cluster.\n  rootProcessGroupId: 'd37bee03-017a-1000-cff7-4eaaa82266b7'\n  # nodeURITemplate used to dynamically compute node uri.\n  nodeURITemplate: 'nifi0%d.integ.mapreduce.m0.p.fti.net:9090'\n  # all node requiresunique id\n  nodes:\n    - id: 1\n    - id: 2\n    - id: 3\n  # type defines if the cluster is internal (i.e manager by the operator) or external.\n  # :Enum={\"external\",\"internal\"}\n  type: 'external'\n  # clientType defines if the operator will use basic or tls authentication to query the NiFi cluster.\n  # Enum={\"tls\",\"basic\"}\n  clientType: 'basic'\n  # secretRef reference the secret containing the informations required to authenticate to the cluster.\n  secretRef:\n    name: nifikop-credentials\n    namespace: nifikop-nifi\n\n```\n\n----------------------------------------\n\nTITLE: Defining Kubernetes Custom StorageClass with WaitForFirstConsumer Binding in YAML\nDESCRIPTION: This snippet defines a Kubernetes StorageClass resource in YAML format to create a custom storage configuration optimized for dynamic provisioning. It specifies the storage type, provisioner, reclaim policy, and volume binding mode set to WaitForFirstConsumer to delay volume binding until a pod consumes it. This configuration is essential for workloads like NiFi to ensure volume scheduling compatibility. Requires Kubernetes cluster with storage provisioner support.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: exampleStorageclass\nparameters:\n  type: pd-standard\nprovisioner: kubernetes.io/gce-pd\nreclaimPolicy: Delete\nvolumeBindingMode: WaitForFirstConsumer\n```\n\n----------------------------------------\n\nTITLE: Configuring package.json Script for NiFiKop Migration Tool\nDESCRIPTION: Adds a start script to package.json to run the migration script with warnings disabled. The complete package.json includes metadata, dependencies, and script configuration for the migration tool.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"start\": \"node --no-warnings index.js\"\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"nifikop_crd_migration\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Script to migrate from the old CRDs to the new CRDs.\",\n  \"main\": \"index.js\",\n  \"scripts\": {\n    \"start\": \"node --no-warnings index.js\",\n    \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\"\n  },\n  \"keywords\": [\n    \"K8S\",\n    \"NiFiKop\",\n    \"CRDs\"\n  ],\n  \"license\": \"ISC\",\n  \"dependencies\": {\n    \"@kubernetes/client-node\": \"^0.16.3\",\n    \"minimist\": \"^1.2.6\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Deploying Two NiFi Dataflows Using YAML Kubernetes CRDs\nDESCRIPTION: Defines two NiFiDataflow CRD resources in YAML format named 'input' and 'output' within the 'nifikop' namespace to be deployed in a Kubernetes cluster. These describe references to NiFi clusters, flow versioning, registry clients, and include positioning for UI representation. Prerequisites include a Kubernetes cluster and the NiFiKop operator managing these custom resources. Each dataflow spec includes bucketId, flowId, sync mode, update strategy, and flags to skip invalid components or controller services. These YAML definitions are used as initial building blocks to create dataflows that can be connected via NiFiConnection.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/4_manage_connections/1_deploy_connection.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiDataflow\nmetadata:\n  name: input\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  bucketId: deedb9f6-65a4-44e9-a1c9-722008fcd0ba\n  flowId: ab95431d-980d-41bd-904a-fac4bd864ba6\n  flowVersion: 1\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  skipInvalidComponent: true\n  skipInvalidControllerService: true\n  syncMode: always\n  updateStrategy: drain\n  flowPosition:\n    posX: 0\n    posY: 0\n---\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiDataflow\nmetadata:\n  name: output\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  bucketId: deedb9f6-65a4-44e9-a1c9-722008fcd0ba\n  flowId: fc5363eb-801e-432f-aa94-488838674d07\n  flowVersion: 2\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  skipInvalidComponent: true\n  skipInvalidControllerService: true\n  syncMode: always\n  updateStrategy: drain\n  flowPosition:\n    posX: 750\n    posY: 0\n```\n\n----------------------------------------\n\nTITLE: Configuring Persistent Storage with storageConfigs in NiFiKop YAML\nDESCRIPTION: This YAML snippet exemplifies configuring persistent storage for a NiFi node using the `storageConfigs` array within a `NodeConfigGroup`. It defines multiple persistent volumes, each specifying a `mountPath` corresponding to essential NiFi directories (logs, data, extensions, repositories, conf), a unique `name`, metadata (labels, annotations), and a `pvcSpec` detailing access modes, requested storage size, and storage class name (`storageClassName`). The `reclaimPolicy` determines the fate of the volume when the PVC is deleted.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n...\nstorageConfigs:\n  - mountPath: /opt/nifi/nifi-current/logs\n    name: logs\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/data\n    name: data\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 50Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/extensions\n    name: extensions-repository\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 5Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/flowfile_repository\n    name: flowfile-repository\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/nifi-current/conf\n    name: conf\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 5Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/content_repository\n    name: content-repository\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 500Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/provenance_repository\n    name: provenance-repository\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 500Gi\n```\n\n----------------------------------------\n\nTITLE: AccessPolicyAction Enum Definitions for Permission Types\nDESCRIPTION: This schema defines permission actions, namely 'read' and 'write', indicating whether a user can view or modify resources or components within NiFi.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/5_references/2_nifi_user.md#_snippet_7\n\nLANGUAGE: YAML\nCODE:\n```\n`AccessPolicyAction:\n  ReadAccessPolicyAction: read\n  WriteAccessPolicyAction: write`\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFiNodeGroupAutoscaler\nDESCRIPTION: This YAML defines a `NifiNodeGroupAutoscaler` resource to manage the autoscaling of NiFi nodes.  It references the NiFi cluster and specifies the `auto_scaling` node group.  `nodeLabelsSelector` selects nodes with specific labels.  `upscaleStrategy` and `downscaleStrategy` define the scaling behavior. This configuration sets up the autoscaling logic, defining how NiFi nodes are scaled up or down based on resource utilization or other metrics (in conjunction with KEDA).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiNodeGroupAutoscaler\nmetadata:\n  name: nifinodegroupautoscaler-sample\n  namespace: clusters\nspec:\n  # contains the reference to the NifiCluster with the one the node group autoscaler is linked.\n  clusterRef:\n    name: cluster\n    namespace: clusters\n  # defines the id of the NodeConfig contained in NifiCluster.Spec.NodeConfigGroups\n  nodeConfigGroupId: auto_scaling\n  # readOnlyConfig can be used to pass Nifi node config\n  # which has type read-only these config changes will trigger rolling upgrade\n  readOnlyConfig:\n    nifiProperties:\n      overrideConfigs: |\n        nifi.ui.banner.text=NiFiKop - Scale Group\n  # This is an example of a node config you can apply to each replica in this node group.\n  # Any settings here will override those in the configured nodeConfigGroupId\n#  nodeConfig:\n#    nodeSelector:\n#      node_type: high-mem\n  # The selector used to identify nodes in NifiCluster.Spec.Nodes this autoscaler will manage\n  # Use Node.Labels in combination with this selector to clearly define which nodes will be managed by this autoscaler \n  nodeLabelsSelector: \n    matchLabels:\n      nifi_cr: cluster\n      nifi_node_group: auto-scaling\n  # the strategy used to decide how to add nodes to a nifi cluster\n  upscaleStrategy: simple\n  # the strategy used to decide how to remove nodes from an existing cluster\n  downscaleStrategy: lifo\n```\n\n----------------------------------------\n\nTITLE: NifiCluster Managed Groups Configuration\nDESCRIPTION: This YAML snippet demonstrates configuring managed admin and reader users within a NifiCluster resource's specification. It defines the identities and names of users who will be automatically added to the 'managed-admins' and 'managed-readers' groups respectively, simplifying access control management in NiFi.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/3_tasks/4_nifi_user_group.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  ...\n  oneNifiNodePerNode: false\n  propagateLabels: true\n  managedAdminUsers:\n    -  identity: \"alexandre.guitton@konpyutaika.com\"\n       name: \"aguitton\"\n    -  identity: \"nifiuser@konpyutaika.com\"\n       name: \"nifiuser\"\n  managedReaderUsers:\n    -  identity: \"toto@konpyutaika.com\"\n       name: \"toto\"\n    ...\n```\n\n----------------------------------------\n\nTITLE: Kubernetes RBAC Role and RoleBinding for NiFi Kubernetes State Management\nDESCRIPTION: Defines a Role and RoleBinding in Kubernetes to grant a ServiceAccount named 'default' in the 'nifi' namespace full permissions on leases and configmaps resources, enabling NiFi to leverage Kubernetes native state management. The Role includes verbs set to '*' for the specified resources, and the RoleBinding binds this role to the target ServiceAccount. This RBAC setup is required to operate NiFi cluster state without external Zookeeper dependencies.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: simplenifi\n  namespace: nifi\nrules:\n- apiGroups: [\"coordination.k8s.io\"]\n  resources: [\"leases\"]\n  verbs: [\"*\"]\n- apiGroups: [\"\"]\n  resources: [\"configmaps\"]\n  verbs: [\"*\"]\n---\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: simplenifi\n  namespace: nifi\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: nifi\nroleRef:\n  kind: Role\n  name: simplenifi\n  apiGroup: rbac.authorization.k8s.io\n```\n\n----------------------------------------\n\nTITLE: Defining Nifi Listeners\nDESCRIPTION: This YAML snippet defines the configuration for Nifi listeners, specifying types, names, container ports, and associated SSL secrets. It outlines different listener types such as 'https', 'cluster', 's2s', 'prometheus', and 'load-balance'. The `internalListeners` section configures each listener with its type, name, and container port.  The `sslSecrets` section specifies the TLS secret name and whether to create the secret.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/5_references/1_nifi_cluster/6_listeners_config.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n  listenersConfig:\n    internalListeners:\n      - type: \"https\"\n        name: \"https\"\n        containerPort: 8443\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n      - type: \"prometheus\"\n        name: \"prometheus\"\n        containerPort: 9090\n      - type: \"load-balance\"\n        name: \"load-balance\"\n        containerPort: 6342\n      - name: \"my-custom-listener-port\"\n        containerPort: 1234\n        protocol: \"TCP\"\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n```\n\n----------------------------------------\n\nTITLE: Deploying KEDA with Helm - Adding repository\nDESCRIPTION: Command to add the KEDA Helm repository to your Kubernetes cluster's Helm configuration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nhelm repo add kedacore https://kedacore.github.io/charts\n```\n\n----------------------------------------\n\nTITLE: Defining LoadBalancer External Service - YAML\nDESCRIPTION: This YAML snippet defines an external service of type LoadBalancer, specifically using an AWS Network Load Balancer (NLB). It exposes ports 8080 and 7890 (UDP) with corresponding internal listener names and associated metadata.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/5_references/1_nifi_cluster/7_external_service_config.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nexternalServices:\n    - name: \"nlb\"\n      spec:\n        type: LoadBalancer\n        loadBalancerClass: \"service.k8s.aws/nlb\"\n        portConfigs:\n          - port: 8080\n            internalListenerName: \"http\"\n          - port: 7890\n            internalListenerName: \"my-custom-udp-listener\"\n            protocol: UDP\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio VirtualService for HTTP (YAML)\nDESCRIPTION: This YAML snippet defines an Istio VirtualService that redirects HTTP traffic intercepted by the gateway to a specific service.  It specifies the gateway, host (nifi.my-domain.com), the matching prefix ('/'), and the destination service (nifi on port 8080). This configuration enables external access to the NiFi cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: nifi-vs\nspec:\n  gateways:\n  - nifi-gateway\n  hosts:\n  - nifi.my-domain.com\n  http:\n  - match:\n    - uri:\n        prefix: /\n    route:\n    - destination:\n        host: nifi\n        port:\n          number: 8080\n```\n```\n\n----------------------------------------\n\nTITLE: Creating a NiFi User via Kubernetes CustomResource in YAML\nDESCRIPTION: This YAML snippet demonstrates how to define a NiFi user by creating a NifiUser CustomResource for the nifikop operator. The resource specifies the user identity, associated NiFi cluster reference, certificate settings, and necessary metadata. The core fields include 'apiVersion', 'kind', 'metadata', and 'spec', where 'spec.identity' is used for the NiFi-side identity and 'spec.clusterRef' links the user to a specific NiFiCluster. This snippet requires that the nifikop CRDs (Custom Resource Definitions) are installed in the cluster, and the operator is running. Expected input is a YAML manifest, applied using kubectl; output is the creation of the CustomResource and, consequently, provisioning of the user in NiFi. Key parameters include 'identity' for user login, 'createCert' to toggle certificate creation, and 'clusterRef' to reference the target NiFi cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/5_references/2_nifi_user.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: aguitton\nspec:\n  identity: alexandre.guitton@konpyutaika.com\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  createCert: false\n```\n\n----------------------------------------\n\nTITLE: Updating Helm Repositories - Console\nDESCRIPTION: Refreshes Helm to ensure the most recent versions of all available charts are fetched. This command should be run after adding a new repository. There are no optional parameters.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_5\n\nLANGUAGE: console\nCODE:\n```\nhelm repo update\n\n```\n\n----------------------------------------\n\nTITLE: Configuring ClusterIP External Service in NiFiKop YAML\nDESCRIPTION: This YAML snippet demonstrates configuring a basic `ClusterIP` service named \"clusterip\" within the `externalServices` list of a NiFiKop custom resource. It shows how to map external port 8080 to the internal port used by the NiFi \"http\" listener and how to include custom Kubernetes annotations and labels in the generated Service metadata. This configuration is used by the NiFiKop operator to create and manage the corresponding Kubernetes Service.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/5_references/1_nifi_cluster/7_external_service_config.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n  externalServices:\n    - name: \"clusterip\"\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8080\n            internalListenerName: \"http\"\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Cluster Internal Service for HTTPS in YAML\nDESCRIPTION: This YAML snippet demonstrates the `externalServices` configuration within a NiFi cluster deployment specification. It defines an internal Kubernetes `ClusterIP` service named \"nifi-cluster\" that exposes the NiFi instance's HTTPS port (8443), mapped to the internal listener named \"https\". This service serves as the internal destination for Istio's routing rules.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\nspec:  \n  externalServices:  \n    - name: \"nifi-cluster\"\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8443\n            internalListenerName: \"https\"\n```\n\n----------------------------------------\n\nTITLE: Scaling Down a NiFi Cluster by Removing a Node (YAML)\nDESCRIPTION: This YAML snippet illustrates how to scale down a NiFi cluster by removing a node (ID 2). This is achieved by commenting out or deleting the node's definition from the `spec.nodes` list in the `NifiCluster` custom resource. Applying this change initiates the graceful node decommissioning process managed by NiFiKop.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  headlessServiceEnabled: true\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.11.3\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n# >>>> START: node removed\n#    - id: 2\n#      nodeConfigGroup: \"default_group\"\n# <<<< END\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: Deploying a Sample NiFi Cluster using Kubectl (Bash)\nDESCRIPTION: Deploys a NiFi cluster into the 'nifi' namespace using 'kubectl create'. It applies the configuration defined in the 'config/samples/simplenificluster.yaml' file. Before running, ensure the Zookeeper service name is correctly configured within this YAML file.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Add your zookeeper svc name to the configuration\nkubectl create -n nifi -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Applying NiFiCluster Configuration in Kubernetes\nDESCRIPTION: A shell command to apply the NiFiCluster configuration to the Kubernetes cluster in the 'nifi' namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nkubectl -n nifi apply -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Deploying Prometheus Server Configuration\nDESCRIPTION: Creates a Prometheus custom resource that defines the Prometheus server configuration, including resources, scrape intervals, and service monitor selection.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: Prometheus\nmetadata:\n  name: prometheus\n  namespace: monitoring-system\nspec:\n  enableAdminAPI: false\n  evaluationInterval: 30s\n  logLevel: debug\n  podMonitorSelector:\n    matchExpressions:\n    - key: release\n      operator: In\n      values:\n      - prometheus\n  resources:\n    requests:\n      memory: 400Mi\n  scrapeInterval: 30s\n  serviceAccountName: prometheus\n  serviceMonitorSelector:\n    matchExpressions:\n    - key: app\n      operator: In\n      values:\n      - nifi-cluster\n```\n\n----------------------------------------\n\nTITLE: Defining a NiFi User resource in YAML\nDESCRIPTION: This YAML snippet defines a `NifiUser` resource for the NiFi Kubernetes operator. It specifies the API version, kind, metadata, and the desired state in the `spec` section, including the user's identity, cluster reference, and certificate creation flag. The `clusterRef` defines which NiFi cluster this user is associated with.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/5_references/2_nifi_user.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: aguitton\nspec:\n  identity: alexandre.guitton@konpyutaika.com\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  createCert: false\n```\n\n----------------------------------------\n\nTITLE: Defining Managed NiFi Cluster User Groups in Kubernetes YAML\nDESCRIPTION: This YAML snippet shows how to specify managed administrator and reader users in the NifiCluster custom resource. By listing users under `managedAdminUsers` and `managedReaderUsers`, the nifikop operator creates corresponding NifiUser objects and manages group memberships automatically. This streamlines user access configuration for multiple NiFi dataflows and clusters. Required dependencies include a Kubernetes cluster with the nifikop operator installed. The YAML must be applied to configure the cluster access, and users are identified by their email identity and a short internal name.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/2_manage_users_and_accesses/3_managed_groups.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  ...\n  oneNifiNodePerNode: false\n  propagateLabels: true\n  managedAdminUsers:\n    -  identity: \"alexandre.guitton@konpyutaika.com\"\n       name: \"aguitton\"\n    -  identity: \"nifiuser@konpyutaika.com\"\n       name: \"nifiuser\"\n  managedReaderUsers:\n    -  identity: \"toto@konpyutaika.com\"\n       name: \"toto\"\n  ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio HTTP Gateway in YAML\nDESCRIPTION: Defines an Istio `Gateway` resource. This gateway is configured to listen on HTTP port 80, targeting the `istio-ingressgateway` selector. It specifies `nifi.my-domain.com` as the host it will handle, making it the entry point for external HTTP traffic to the NiFi cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: nifi-gateway\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - nifi.my-domain.com\n```\n\n----------------------------------------\n\nTITLE: Defining NifiUser Kubernetes Resource - YAML\nDESCRIPTION: This YAML snippet provides an example definition of a NifiUser custom resource for Kubernetes, managed by the Nifikop operator. It specifies the user's desired identity, links it to a target NiFi cluster using clusterRef (including name and namespace), and indicates whether a certificate should be automatically generated. This resource defines the desired state of a NiFi user within the Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/5_references/2_nifi_user.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiUser\nmetadata:\n  name: aguitton\nspec:\n  identity: alexandre.guitton@konpyutaika.com\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  createCert: false\n```\n\n----------------------------------------\n\nTITLE: Defining Managed NiFi User Groups Using Custom Resource YAML\nDESCRIPTION: This YAML snippet shows how to configure managed admin and reader user lists inside the spec of a NifiCluster custom resource for the nifikop operator. The configuration includes lists of user identities and their names, which the operator will use to create corresponding NifiUser and NifiUserGroup resources automatically. Key fields include 'managedAdminUsers' and 'managedReaderUsers' under 'spec', defining which users belong to the admin and reader groups respectively. The input is a YAML manifest applied to a Kubernetes cluster where the operator reconciles these resources to ensure correct user and group management.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/2_manage_users_and_accesses/3_managed_groups.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  ...\n  oneNifiNodePerNode: false\n  propagateLabels: true\n  managedAdminUsers:\n    -  identity: \"alexandre.guitton@konpyutaika.com\"\n       name: \"aguitton\"\n    -  identity: \"nifiuser@konpyutaika.com\"\n       name: \"nifiuser\"\n  managedReaderUsers:\n    -  identity: \"toto@konpyutaika.com\"\n       name: \"toto\"\n    ...\n```\n\n----------------------------------------\n\nTITLE: Explanation of Configuration Overriding Methods in NifiKop\nDESCRIPTION: Provides an overview of the four primary methods to override default NiFi configurations in Kubernetes: default, Secrets, ConfigMaps, and inline override fields, along with their priority order. It also includes an example YAML showcasing how to specify these overrides for nifi.properties, with detailed explanations of data key, name, and namespace parameters.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\nnifiProperties:\n  overrideConfigMap:\n    data: nifi.properties\n    name: raw\n    namespace: nifikop\n  overrideSecretConfig:\n    data: nifi.properties\n    name: raw\n    namespace: nifikop\n  overrideConfigs: |\n    nifi.ui.banner.text=NiFiKop\n    nifi.sensitive.props.key=thisIsABadSensitiveKeyPassword\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Internal and External Services in YAML\nDESCRIPTION: Provides a comprehensive example showing both internal listener configuration and how to expose specific internal listeners externally using the `externalServices` field. It defines a LoadBalancer service (`cluster-access`) that maps external ports (`443`, `80`) to internal listener names (`https`, `http-tracking`).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    - type: \"https\"\n      name: \"https\"\n      containerPort: 8443\n    - type: \"cluster\"\n      name: \"cluster\"\n      containerPort: 6007\n    - type: \"s2s\"\n      name: \"s2s\"\n      containerPort: 10000\n    - type: \"prometheus\"\n      name: \"prometheus\"\n      containerPort: 9090\n    - type: \"load-balance\"\n      name: \"load-balance\"\n      containerPort: 6342\n    - name: \"http-tracking\"\n      containerPort: 8081\nexternalServices:\n  - name: cluster-access\n    spec:\n      portConfigs:\n        - internalListenerName: https\n          port: 443\n        - internalListenerName: http-tracking\n          port: 80\n      type: LoadBalancer\n```\n\n----------------------------------------\n\nTITLE: Configuring Nifi External Service with LoadBalancer in YAML\nDESCRIPTION: This YAML snippet demonstrates defining an external Nifi service using the `LoadBalancer` type. The service, named \"nlb\", specifies a `loadBalancerClass` (e.g., for AWS NLB). It maps external port 8080 to the internal listener \"http\" (TCP implied) and external port 7890 to the internal listener \"my-custom-udp-listener\" using the UDP protocol. Additional metadata like annotations and labels are included for the service.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/5_references/1_nifi_cluster/7_external_service_config.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nexternalServices:\n    - name: \"nlb\"\n      spec:\n        type: LoadBalancer\n        loadBalancerClass: \"service.k8s.aws/nlb\"\n        portConfigs:\n          - port: 8080\n            internalListenerName: \"http\"\n          - port: 7890\n            internalListenerName: \"my-custom-udp-listener\"\n            protocol: UDP\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFiKop Helm chart\nDESCRIPTION: This command installs the NiFiKop operator using Helm from an OCI registry, specifying the release name, namespace, version, image tag, resource requests/limits, and target namespaces. It is used for deploying the operator in a packaged manner with consistent configuration, suitable for production environments.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/2_deploy_nifikop/1_quick_start.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# You have to create the namespace before executing following command\nhelm install nifikop \\\n    oci://ghcr.io/konpyutaika/helm-charts/nifikop \\\n    --namespace=nifi \\\n    --version 1.1.0 \\\n    --set image.tag=v1.1.0-release \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set namespaces={\"nifi\"}\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Standard Internal NiFi Listeners in YAML\nDESCRIPTION: This YAML snippet shows the configuration for standard internal listeners within the `listenersConfig.internalListeners` field of the NiFiCluster custom resource. It defines critical ports for NiFi's internal operations like clustering, UI access (HTTPS), Site-to-Site (s2s), load balancing, and Prometheus metrics, specifying the `type`, `name`, and `containerPort` for each.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    - type: \"https\"\n      name: \"https\"\n      containerPort: 8443\n    - type: \"cluster\"\n      name: \"cluster\"\n      containerPort: 6007\n    - type: \"s2s\"\n      name: \"s2s\"\n      containerPort: 10000\n    - type: \"prometheus\"\n      name: \"prometheus\"\n      containerPort: 9090\n    - type: \"load-balance\"\n      name: \"load-balance\"\n      containerPort: 6342\n```\n\n----------------------------------------\n\nTITLE: Configuring NifiCluster with Operator-Managed SSL\nDESCRIPTION: Configures the NifiCluster custom resource to enable HTTPS listeners and instructs the NiFi operator to automatically create SSL certificates. It specifies the port for the HTTPS listener (8443) and includes an optional configuration for `webProxyHosts` in `nifi.properties`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  readOnlyConfig:\n    # NifiProperties configuration that will be applied to the node.\n    nifiProperties:\n      webProxyHosts:\n        - nifistandard2.trycatchlearn.fr:8443\n        ...\n  ...\n  listenersConfig:\n    internalListeners:\n      - type: \"https\"\n        name: \"https\"\n        containerPort: 8443\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n```\n\n----------------------------------------\n\nTITLE: Creating a NiFi Registry Client (YAML)\nDESCRIPTION: This YAML snippet defines a `NifiRegistryClient` named \"squidflow\". It specifies the API version, kind, and metadata.  The `spec` section includes a reference to a NiFi cluster, a description of the client, and the URI of the NiFi registry. This configuration allows interaction with a specified NiFi registry.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/5_references/3_nifi_registry_client.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiRegistryClient\nmetadata:\n  name: squidflow\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Squidflow demo\"\n  uri: \"http://nifi-registry:18080\"\n```\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop with custom values.yaml using Helm\nDESCRIPTION: This command installs the NiFiKop Helm chart from the konpyutaika repository, utilizing a custom `values.yaml` file to configure the installation parameters. The `nifikop` argument specifies the release name for the installation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n$ helm install nifikop \\\n      konpyutaika/nifikop \\\n      -f values.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi UserGroup Custom Resource YAML Definition\nDESCRIPTION: This YAML snippet shows how to configure a NifiUserGroup custom resource for Kubernetes using the nifikop API. It defines the group identity, cluster reference, user references, and associated access policies granting permissions within a NiFi cluster. This resource enables Kubernetes-native management of NiFi user groups and their access controls. The snippet requires a Kubernetes cluster with nifikop operator installed and CustomResourceDefinitions (CRDs) for nifikop resources. Key fields include `metadata.name` for the resource name, `spec.identity` for the NiFi group identity, and `spec.accessPolicies` which govern permissions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/5_references/6_nifi_usergroup.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  identity: \"My Special Group\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n    - name: nc-controller.nifikop.mgt.cluster.local\n  accessPolicies:\n    - type: global\n      action: read\n      resource: /counters\n```\n\n----------------------------------------\n\nTITLE: Enabling TLS Origination and Sticky Sessions with Istio DestinationRule for NiFi (YAML)\nDESCRIPTION: Defines an Istio DestinationRule named `nifi-dr` targeting the NiFi ClusterIP service host (`<service-name>.<namespace>.svc.cluster.local`). It configures the traffic policy to re-encrypt traffic using `SIMPLE` TLS (TLS origination) before forwarding it to NiFi pods on port 8443. It also sets up load balancing with a consistent hash based on the `__Secure-Authorization-Bearer` HTTP cookie to ensure sticky sessions, which is crucial for NiFi UI authentication.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: DestinationRule\nmetadata:\n  name: nifi-dr\nspec:\n  host: <service-name>.<namespace>.svc.cluster.local\n  trafficPolicy:\n    tls:\n      mode: SIMPLE\n    loadBalancer:\n      consistentHash:\n        httpCookie:\n          name: __Secure-Authorization-Bearer\n          ttl: 0s\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio DestinationRule for HTTPS (YAML)\nDESCRIPTION: This YAML defines a DestinationRule to encrypt traffic to the service and manage the sticky session using the `httpCookie` property. The destination rule is applied to the service and uses the `consistentHash` load balancing policy with the cookie name `__Secure-Authorization-Bearer` to maintain session affinity. The service must be a ClusterIP type.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\napiVersion: networking.istio.io/v1beta1\nkind: DestinationRule\nmetadata:\n  name: nifi-dr\nspec:\n  host: <service-name>.<namespace>.svc.cluster.local\n  trafficPolicy:\n    tls:\n      mode: SIMPLE\n    loadBalancer:\n      consistentHash:\n        httpCookie:\n          name: __Secure-Authorization-Bearer\n          ttl: 0s\n```\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop with custom values via Helm\nDESCRIPTION: This command demonstrates how to install the NiFiKop Helm chart, specifying a custom YAML file (`values.yaml`) containing overrides for the default configuration parameters. It uses `helm install` with the specified release name (`nifikop`), the chart name (`konpyutaika/nifikop`), and the `-f` flag to provide the custom values file. This method is an alternative to specifying individual parameters using `--set`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n$ helm install nifikop \\\n      konpyutaika/nifikop \\\n      -f values.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining KEDA ScaledObject Resource for NiFi Autoscaling - YAML\nDESCRIPTION: Defines a KEDA ScaledObject for autoscaling a NiFiNodeGroupAutoscaler resource using Prometheus metrics. Customizes scaling parameters such as pollingInterval, cooldownPeriod, min/max replica count, and fallback logic. Requires prior deployment of Prometheus with ServiceMonitor and HPA/KEDA installed. Users must specify the Prometheus server address, metric name, threshold, and query used for scaling decisions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: cluster\n  namespace: clusters\nspec:\n  scaleTargetRef:\n    apiVersion:    nifi.konpyutaika.com/v1alpha1     # Optional. Default: apps/v1\n    kind:          NifiNodeGroupAutoscaler           # Optional. Default: Deployment\n    name:          nifinodegroupautoscaler-sample    # Mandatory. Must be in the same namespace as the ScaledObject\n    envSourceContainerName: nifi                     # Optional. Default: .spec.template.spec.containers[0]\n  pollingInterval:  30                               # Optional. Default: 30 seconds\n  cooldownPeriod:   300                              # Optional. Default: 300 seconds\n  idleReplicaCount: 0                                # Optional. Default: ignored, must be less than minReplicaCount \n  minReplicaCount:  1                                # Optional. Default: 0\n  maxReplicaCount:  3                                # Optional. Default: 100\n  fallback:                                          # Optional. Section to specify fallback options\n    failureThreshold: 5                              # Mandatory if fallback section is included\n    replicas: 1                                      # Mandatory if fallback section is included\n  # advanced:                                          # Optional. Section to specify advanced options\n  #   restoreToOriginalReplicaCount: true              # Optional. Whether the target resource should be scaled back to original replicas count, after the ScaledObject is deleted\n  #   horizontalPodAutoscalerConfig:                   # Optional. Section to specify HPA related options\n  #     name: {name-of-hpa-resource}                   # Optional. Default: keda-hpa-{scaled-object-name}\n  #     behavior:                                      # Optional. Use to modify HPA's scaling behavior\n  #       scaleDown:\n  #         stabilizationWindowSeconds: 300 <--- \n  #         policies:\n  #         - type: Percent\n  #           value: 100\n  #           periodSeconds: 15\n  triggers:\n    - type: prometheus\n      metadata:\n        serverAddress: http://prometheus-operated.monitoring-system.svc:9090\n        metricName: http_requests_total\n        threshold: <threshold value>\n        query: <prometheus query>\n```\n\n----------------------------------------\n\nTITLE: Defining a NiFiRegistryClient Resource in YAML\nDESCRIPTION: Defines a `NifiRegistryClient` custom resource for NiFiKop. This resource specifies the connection details (URI) for a NiFi Registry instance and associates it with a NiFi cluster (`clusterRef`). NiFiKop uses this to interact with the registry for managing versioned dataflows.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiRegistryClient\nmetadata:\n  name: registry-client-example\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Registry client managed by NiFiKop\"\n  uri: \"http://nifi.hostname.com:18080\"\n```\n\n----------------------------------------\n\nTITLE: Routing Decrypted HTTP Traffic with VirtualService (YAML)\nDESCRIPTION: This YAML snippet defines an Istio VirtualService for routing decrypted HTTP traffic originating from the HTTPS Gateway. It matches requests for `nifi.my-domain.com` and routes them to the specified internal service host (`<service-name>.<namespace>.svc.cluster.local`) on port 8443, which corresponds to the NiFi service.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: nifi-vs\nspec:\n  gateways:\n  - nifi-gateway\n  hosts:\n  - nifi.my-domain.com\n  http:\n  - match:\n    - uri:\n        prefix: /\n    route:\n    - destination:\n        host: <service-name>.<namespace>.svc.cluster.local\n        port:\n          number: 8443\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi OIDC with NiFiKop OverrideConfigs (YAML)\nDESCRIPTION: Illustrates how to configure NiFi for OIDC authentication within a NiFiKop deployment using the `Spec.NifiProperties.OverrideConfigs` field in the `NifiCluster` custom resource. This YAML snippet includes placeholders for OIDC discovery URL, client ID, client secret, and includes the recommended identity mapping settings.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/5_users_authentication/1_oidc.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  readOnlyConfig:\n    # NifiProperties configuration that will be applied to the node.\n    nifiProperties:\n      webProxyHosts:\n        - nifistandard2.trycatchlearn.fr:8443\n      # Additionnal nifi.properties configuration that will override the one produced based\n      # on template and configurations.\n      overrideConfigs: |\n        nifi.security.user.oidc.discovery.url=<oidc server discovery url>\n        nifi.security.user.oidc.client.id=<oidc client's id>\n        nifi.security.user.oidc.client.secret=<oidc client's secret>\n        nifi.security.identity.mapping.pattern.dn=CN=([^,]*)(?:, (?:O|OU)=.*)?\n        nifi.security.identity.mapping.value.dn=$1\n        nifi.security.identity.mapping.transform.dn=NONE\n      ...\n   ...\n...\n```\n\n----------------------------------------\n\nTITLE: NiFi Cluster Configuration YAML with Zookeeper and Keycloak Integration\nDESCRIPTION: The 'cluster.yaml' manifest defines the NiFi cluster resource configuration featuring integration points such as the Zookeeper service operating at 'zookeeper:2181' and Keycloak identity provider hosted at 'sso.MY_DOMAIN.com'. It specifies the client ID used for OIDC authentication (requires replacement with actual client ID), and references the Keycloak realm 'MY_REALM' with a configured callback URL for seamless OAuth2/OIDC flows. This file controls the core NiFi deployment and OIDC federation settings.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/config/samples/keycloak-example/README.md#_snippet_4\n\nLANGUAGE: YAML\nCODE:\n```\ncluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring External Services to Expose NiFi Internal Listeners - YAML\nDESCRIPTION: This YAML snippet configures external Kubernetes services to expose NiFi internal listeners outside of the cluster. The configuration defines an external service named `cluster-access` of type `LoadBalancer`. It maps the service ports (e.g., 443, 80) to the corresponding internal listener names (`https` and `http-tracking`). This enables external access to the NiFi UI via HTTPS and a custom HTTP tracking endpoint through Kubernetes' LoadBalancer service. Dependencies include proper NiFi configuration to recognize and use these external addresses, such as setting `WebProxyHosts` appropriately.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    - type: \"https\"\n      name: \"https\"\n      containerPort: 8443\n    - type: \"cluster\"\n      name: \"cluster\"\n      containerPort: 6007\n    - type: \"s2s\"\n      name: \"s2s\"\n      containerPort: 10000\n    - type: \"prometheus\"\n      name: \"prometheus\"\n      containerPort: 9090\n    - type: \"load-balance\"\n      name: \"load-balance\"\n      containerPort: 6342\n    - name: \"http-tracking\"\n      containerPort: 8081\nexternalServices:\n  - name: cluster-access\n    spec:\n      portConfigs:\n        - internalListenerName: https\n          port: 443\n        - internalListenerName: http-tracking\n          port: 80\n      type: LoadBalancer\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFiKop Operator using Helm (Bash)\nDESCRIPTION: Deploys the NiFiKop operator version 1.6.0 using Helm 3 from the OCI registry `ghcr.io/konpyutaika/helm-charts/nifikop`. Requires the `nifi` namespace to exist beforehand. Specifies the operator image tag, resource requests/limits, and the namespaces (`nifi`) the operator should watch.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/2_deploy_nifikop/1_quick_start.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# You have to create the namespace before executing following command\nhelm install nifikop \\\n    oci://ghcr.io/konpyutaika/helm-charts/nifikop \\\n    --namespace=nifi \\\n    --version 1.6.0 \\\n    --set image.tag=v1.6.0-release \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set namespaces={\"nifi\"}\n```\n\n----------------------------------------\n\nTITLE: Defining NifiDataflow Kubernetes CRD\nDESCRIPTION: This YAML snippet defines a NifiDataflow custom resource. It specifies the API version, kind, metadata, and the desired state (spec) of the NiFi dataflow. It includes configurations such as the parent process group ID, bucket ID, flow ID, flow version, flow position, sync mode, cluster reference, registry client reference, parameter context reference, and update strategy.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/5_references/5_nifi_dataflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  flowPosition:\n    posX: 0\n    posY: 0\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: squidflow\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: nifikop\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Applying NifiCluster Configuration (kubectl)\nDESCRIPTION: This command applies the NiFi cluster configuration defined in a YAML file. It uses `kubectl` to create or update a `NifiCluster` resource in the 'nifi' namespace. The prerequisite is a properly formatted YAML configuration file and the Kubernetes cluster configured for `kubectl`. The input is a YAML file path, and the output is the creation or modification of the NiFi cluster resources. This is essential for deploying or updating the cluster definition.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nkubectl -n nifi apply -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: YAML Definition for NiFi Dataflow Deployment\nDESCRIPTION: Defines a NiFi dataflow Kubernetes resource with parameters for cluster reference, flow ID, version, registry, and display position. This configuration is used to deploy individual dataflows such as 'input' and 'output' in the NiFi cluster, ensuring that they are correctly initialized before creating connections.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/4_manage_connections/1_deploy_connection.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiDataflow\nmetadata:\n  name: input\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  bucketId: deedb9f6-65a4-44e9-a1c9-722008fcd0ba\n  flowId: ab95431d-980d-41bd-904a-fac4bd864ba6\n  flowVersion: 1\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  skipInvalidComponent: true\n  skipInvalidControllerService: true\n  syncMode: always\n  updateStrategy: drain\n  flowPosition:\n    posX: 0\n    posY: 0\n```\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiDataflow\nmetadata:\n  name: output\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  bucketId: deedb9f6-65a4-44e9-a1c9-722008fcd0ba\n  flowId: fc5363eb-801e-432f-aa94-488838674d07\n  flowVersion: 2\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  skipInvalidComponent: true\n  skipInvalidControllerService: true\n  syncMode: always\n  updateStrategy: drain\n  flowPosition:\n    posX: 750\n    posY: 0\n```\n\n----------------------------------------\n\nTITLE: Scaling Up: Adding a Node to NifiCluster Definition (YAML)\nDESCRIPTION: This YAML snippet demonstrates how to scale up a NiFi cluster by adding a new node definition (id: 25) to the `spec.nodes` list within the `NifiCluster` custom resource. The new node uses the 'default_group' configuration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n  clusterManager: zookeeper\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.12.1\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n    - id: 2\n      nodeConfigGroup: \"default_group\"\n# >>>> START: The new node\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n# <<<< END\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: Configuring External Services with Internal Listeners in YAML\nDESCRIPTION: Complete example showing how to configure both internal listeners and external services for a NiFi cluster. The example maps internal HTTPS and HTTP-tracking listeners to external load balancer ports 443 and 80, respectively.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    - type: \"https\"\n      name: \"https\"\n      containerPort: 8443\n    - type: \"cluster\"\n      name: \"cluster\"\n      containerPort: 6007\n    - type: \"s2s\"\n      name: \"s2s\"\n      containerPort: 10000\n    - type: \"prometheus\"\n      name: \"prometheus\"\n      containerPort: 9090\n    - type: \"load-balance\"\n      name: \"load-balance\"\n      containerPort: 6342\n    - name: \"http-tracking\"\n      containerPort: 8081\nexternalServices:\n  - name: cluster-access\n    spec:\n      portConfigs:\n        - internalListenerName: https\n          port: 443\n        - internalListenerName: http-tracking\n          port: 80\n      type: LoadBalancer\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiDataflow Resource in YAML\nDESCRIPTION: This YAML manifest defines a `NifiDataflow` custom resource named `dataflow-lifecycle` for use with the NiFiKop operator. It specifies deployment parameters such as the parent process group ID, the NiFi Registry bucket and flow identifiers, the desired flow version, synchronization behavior (`syncMode`), update strategy (`updateStrategy`), and references to associated `NifiCluster`, `NifiRegistryClient`, and `NifiParameterContext` resources within the specified namespaces. It also includes options to skip invalid components during deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/5_references/5_nifi_dataflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  flowPosition:\n    posX: 0\n    posY: 0\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: squidflow\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: nifikop\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Installing Nifikop with Custom Namespaces - Bash\nDESCRIPTION: Installs the Nifikop operator Helm chart with explicit setting of the namespace parameter, allowing you to specify which Kubernetes namespaces will be managed by the operator. Dependencies: Helm CLI and write access to targeted namespaces. The --set flag takes a comma-separated list of namespaces within curly braces.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop --set namespaces={\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring NifiNodeGroupAutoscaler for Auto-Scaling NiFi Clusters in YAML\nDESCRIPTION: This YAML snippet defines a sample NifiNodeGroupAutoscaler resource configuration, used by the Nifikop Kubernetes operator to automatically scale NiFi node groups based on specified upscale and downscale strategies. It requires a reference to the target NifiCluster object, identification of the node config group, and a label selector that uniquely identifies nodes managed by this autoscaler to avoid overlap. The upscaleStrategy and downscaleStrategy fields specify how nodes should be added or removed from the cluster. Dependencies include having the target NifiCluster deployed with appropriate node config groups and matching labels. Inputs are metadata and spec fields conforming to the NifiNodeGroupAutoscaler schema, and outputs are managed node replicas controlled by Kubernetes HPA and the autoscaler logic.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/5_references/7_nifi_nodegroup_autoscaler.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiNodeGroupAutoscaler\nmetadata:\n  name: nifinodegroupautoscaler-sample\nspec:\n  # contains the reference to the NifiCluster with the one the node group autoscaler is linked.\n  clusterRef:\n    name: nificluster-name\n    namespace: nifikop\n  # defines the id of the NodeConfig contained in NifiCluster.Spec.NodeConfigGroups\n  nodeConfigGroupId: default-node-group\n  # The selector used to identify nodes in NifiCluster.Spec.Nodes this autoscaler will manage\n  # Use Node.Labels in combination with this selector to clearly define which nodes will be managed by this autoscaler \n  nodeLabelsSelector: \n    matchLabels:\n      nifi_cr: nificluster-name\n      nifi_node_group: default-node-group\n  # the strategy used to decide how to add nodes to a nifi cluster\n  upscaleStrategy: simple\n  # the strategy used to decide how to remove nodes from an existing cluster\n  downscaleStrategy: lifo\n```\n\n----------------------------------------\n\nTITLE: Installing Zookeeper on OpenShift\nDESCRIPTION: This bash script installs Zookeeper on OpenShift using Helm, customizing the deployment for OpenShift's security context. It retrieves the uid/gid for the RunAsUser and fsGroup using `kubectl` and sets it within the Helm install command for proper user identity within the OpenShift environment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nhelm install zookeeper oci://registry-1.docker.io/bitnamicharts/zookeeper \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set global.storageClass=standard \\\n    --set networkPolicy.enabled=true \\\n    --set replicaCount=3 \\\n    --set containerSecurityContext.runAsUser=$zookeper_uid \\\n    --set podSecurityContext.fsGroup=$zookeper_uid\n```\n\n----------------------------------------\n\nTITLE: Configuring Individual NiFi Nodes YAML\nDESCRIPTION: Illustrates configuring multiple NiFi nodes within a NiFiKop custom resource definition. It shows setting node IDs, using `nodeConfigGroup`, applying `readOnlyConfig` for NiFi properties (like banner text), and using `nodeConfig` to define Kubernetes resource requests/limits and persistent storage for repositories like provenance.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/5_references/1_nifi_cluster/4_node.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n    - id: 0\n      # nodeConfigGroup can be used to ease the node configuration, if set only the id is required\n      nodeConfigGroup: \"default_group\"\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        nifiProperties:\n          overrideConfigs: |\n            nifi.ui.banner.text=NiFiKop - Node 0\n      # node configuration\n#       nodeConfig:\n    - id: 2\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        overrideConfigs: |\n          nifi.ui.banner.text=NiFiKop - Node 2\n      # node configuration\n      nodeConfig:\n        resourcesRequirements:\n          limits:\n            cpu: \"2\"\n            memory: 3Gi\n          requests:\n            cpu: \"1\"\n            memory: 1Gi\n        storageConfigs:\n          # Name of the storage config, used to name PV to reuse into sidecars for example.\n          - name: provenance-repository\n            # Path where the volume will be mount into the main nifi container inside the pod.\n            mountPath: \"/opt/nifi/provenance_repository\"\n            # Metadata to attach to the PVC that gets created\n            metadata:\n              labels:\n                my-label: my-value\n              annotations:\n                my-annotation: my-value\n            # Kubernetes PVC spec\n            # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n            pvcSpec:\n              accessModes:\n                - ReadWriteOnce\n              storageClassName: \"standard\"\n              resources:\n                requests:\n                  storage: 8Gi\n```\n\n----------------------------------------\n\nTITLE: Configuring Internal Listeners for NiFi Cluster in YAML\nDESCRIPTION: Defines the internal listeners configuration for a NiFi cluster, including ports for HTTPS, cluster communication, site-to-site communication, Prometheus metrics, and load balancing. Each listener specifies its type, name, and container port.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    - type: \"https\"\n      name: \"https\"\n      containerPort: 8443\n    - type: \"cluster\"\n      name: \"cluster\"\n      containerPort: 6007\n    - type: \"s2s\"\n      name: \"s2s\"\n      containerPort: 10000\n    - type: \"prometheus\"\n      name: \"prometheus\"\n      containerPort: 9090\n    - type: \"load-balance\"\n      name: \"load-balance\"\n      containerPort: 6342\n```\n\n----------------------------------------\n\nTITLE: Extracting Client Secret Certificates to Files\nDESCRIPTION: These shell commands decode and save the certificate and key data from the Kubernetes secret `example-client-secret` into local files (`ca.crt`, `tls.crt`, `tls.key`) for use in client authentication or configuration outside of Kubernetes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_5\n\nLANGUAGE: console\nCODE:\n```\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['ca\\.crt']}\" | base64 -d > ca.crt\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['tls\\.crt']}\" | base64 -d > tls.crt\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['tls\\.key']}\" | base64 -d > tls.key\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio HTTP VirtualService in YAML\nDESCRIPTION: Defines an Istio `VirtualService` resource. This service is associated with the `nifi-gateway` and handles requests for `nifi.my-domain.com`, routing all traffic (`prefix: /`) to an internal service named `nifi` on port 8080. It directs traffic intercepted by the gateway to the backend NiFi service.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: nifi-vs\nspec:\n  gateways:\n  - nifi-gateway\n  hosts:\n  - nifi.my-domain.com\n  http:\n  - match:\n    - uri:\n        prefix: /\n    route:\n    - destination:\n        host: nifi\n        port:\n          number: 8080\n```\n\n----------------------------------------\n\nTITLE: Defining Istio Gateway for HTTPS in YAML\nDESCRIPTION: This YAML snippet defines an Istio `Gateway` resource for HTTPS traffic. It is configured to accept HTTPS traffic on port 443, decrypting it to HTTP. The `tls` section specifies the mode (`SIMPLE`) and credential name (`my-secret`) for TLS termination.  The `hosts` field indicates the domain.  This Gateway handles the HTTPS traffic and is a crucial step to decrypt the traffic. The input is HTTPS traffic; the output is decrypted HTTP traffic.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: nifi-gateway\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 443\n      name: https\n      protocol: HTTPS\n    tls:\n      mode: SIMPLE\n      credentialName: my-secret\n    hosts:\n    - nifi.my-domain.com\n```\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop Helm Chart with a YAML Values File (Console)\nDESCRIPTION: This command installs the NiFiKop Helm chart using a user-defined YAML file for configuration (`values.yaml`). The snippet assumes Helm v3+ is installed and that the YAML file includes overridden parameters as necessary. It accepts YAML configuration as input and produces a Helm deployment or reports errors if configuration is invalid or dependencies are missing.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/2_setup/3_install/1_customizable_install_with_helm.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nhelm install nifikop \\\n      konpyutaika/nifikop \\\n      -f values.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Node Configuration in YAML\nDESCRIPTION: This YAML snippet demonstrates how to configure individual NiFi nodes within a cluster. Key properties include a unique node id, optional grouping through nodeConfigGroup to simplify configurations, and readOnlyConfig which allows passing NiFi node properties marked as read-only; changes here trigger rolling upgrades for the node. The example shows how to override the NiFi UI banner text through nifiProperties. For nodes with detailed configurations, resources requirements are specified with CPU and memory limits and requests, and storageConfigs define Kubernetes persistent volume claims, specifying access modes, storage class, and requested capacity. This configuration is essential for managing node-specific settings and integrating persistent storage in Kubernetes environments.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/5_references/1_nifi_cluster/4_node.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n    - id: 0\n      # nodeConfigGroup can be used to ease the node configuration, if set only the id is required\n      nodeConfigGroup: \"default_group\"\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        nifiProperties:\n          overrideConfigs: |\n            nifi.ui.banner.text=NiFiKop - Node 0\n      # node configuration\n#       nodeConfig:\n    - id: 2\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        overrideConfigs: |\n          nifi.ui.banner.text=NiFiKop - Node 2\n      # node configuration\n      nodeConfig:\n        resourcesRequirements:\n          limits:\n            cpu: \"2\"\n            memory: 3Gi\n          requests:\n            cpu: \"1\"\n            memory: 1Gi\n        storageConfigs:\n          # Name of the storage config, used to name PV to reuse into sidecars for example.\n          - name: provenance-repository\n            # Path where the volume will be mount into the main nifi container inside the pod.\n            mountPath: \"/opt/nifi/provenance_repository\"\n            # Kubernetes PVC spec\n            # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n            pvcSpec:\n              accessModes:\n                - ReadWriteOnce\n              storageClassName: \"standard\"\n              resources:\n                requests:\n                  storage: 8Gi\n```\n\n----------------------------------------\n\nTITLE: Creating NifiUser Custom Resource for User Management in Kubernetes (YAML)\nDESCRIPTION: Defines a NifiUser custom resource manifest that instructs the operator to manage a NiFi user with specified identity, cluster reference, certificate creation options, and access policies. The snippet includes fields such as 'identity' for the NiFi user identity that can differ from the Kubernetes resource name to comply with naming rules. It references the NiFi cluster with 'clusterRef' and specifies whether to create certificates ('createCert') or include Java keystore format secrets ('includeJKS'). The 'accessPolicies' list defines the types of actions ('read', 'write') and components on which the user will have access. This manifest is used by the Kubernetes operator to synchronize user state with the NiFi cluster, either binding to existing users or creating new ones as necessary.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/2_manage_users_and_accesses/1_users_management.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: aguitton\nspec:\n  # identity field is use to define the user identity on NiFi cluster side,\n  #\tit use full when the user's name doesn\\'t suite with Kubernetes resource name.\n  identity: alexandre.guitton@konpyutaika.com\n  # Contains the reference to the NifiCluster with the one the registry client is linked.\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  # Whether or not the the operator also include a Java keystore format (JKS) with you secret\n  includeJKS: false\n  # Whether or not a certificate will be created for this user.\n  createCert: false\n  # defines the list of access policies that will be granted to the group.\n  accessPolicies:\n      # defines the kind of access policy, could be \"global\" or \"component\".\n    - type: component\n      # defines the kind of action that will be granted, could be \"read\" or \"write\"\n      action: read\n      # resource defines the kind of resource targeted by this access policies, please refer to the following page:\n      #\thttps://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#access-policies\n      resource: /data\n      # componentType is used if the type is \"component\", it\\'s allow to define the kind of component on which is the\n      # access policy\n      componentType: \"process-groups\"\n      # componentId is used if the type is \"component\", it\\'s allow to define the id of the component on which is the\n      # access policy\n      componentId: \"\"\n```\n\n----------------------------------------\n\nTITLE: Defining NifiUserGroup for Apache NiFi Access Management in YAML\nDESCRIPTION: This YAML snippet defines a NifiUserGroup custom resource used to create and manage a user group in Apache NiFi through the Kubernetes operator. It specifies references to the target NiFi cluster and the users included in the group via 'usersRef'. It also sets access policies that grant specific permissions (such as read access to the /counters resource) to the group. Key fields include 'clusterRef' for linking the NiFi cluster, 'usersRef' listing user resource names, and 'accessPolicies' detailing policy type, action, and target resources. The operator creates the group named using the pattern '${resource namespace}-${resource name}'. Dependencies include having NifiUser resources for each user. This resource enables centralized and declarative access management within NiFi via Kubernetes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/2_manage_users_and_accesses/2_groups_management.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  # Contains the reference to the NifiCluster with the one the registry client is linked.\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  # contains the list of reference to NifiUsers that are part to the group.\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n#     namespace: nifikop\n    - name: nc-controller.nifikop.mgt.cluster.local\n  # defines the list of access policies that will be granted to the group.\n  accessPolicies:\n      # defines the kind of access policy, could be \"global\" or \"component\".\n    - type: global\n      # defines the kind of action that will be granted, could be \"read\" or \"write\"\n      action: read\n      # resource defines the kind of resource targeted by this access policies, please refer to the following page:\n      #\thttps://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#access-policies\n      resource: /counters\n#      # componentType is used if the type is \"component\", it's allow to define the kind of component on which is the\n#      # access policy\n#      componentType: \"process-groups\"\n#      # componentId is used if the type is \"component\", it's allow to define the id of the component on which is the\n#      # access policy\n#      componentId: \"\"\n```\n\n----------------------------------------\n\nTITLE: Defining ServiceMonitor Custom Resource for NiFi Metrics Scraping with Prometheus in YAML\nDESCRIPTION: This ServiceMonitor resource in Kubernetes defines how Prometheus should scrape metrics endpoints from the NiFi cluster pods. It selects services labeled with 'app: nifi' and 'nifi_cr: cluster' in the 'clusters' namespace and scrapes the 'prometheus' port at the '/metrics' path every 10 seconds. Label relabelings are configured to add pod IP, nodeId, and nifi_cr labels to scraped metrics for identification in Prometheus. This resource must exist along with Prometheus CRD to enable monitoring of NiFi cluster metrics for autoscaling and visualization.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: cluster\n  namespace: monitoring-system\n  labels:\n    app: nifi-cluster\n    nifi_cr: cluster\nspec:\n  selector:\n    matchLabels:\n      app: nifi\n      nifi_cr: cluster\n  namespaceSelector:\n    matchNames:\n      - clusters\n  endpoints:\n    - interval: 10s\n      port: prometheus\n      path: /metrics\n      honorLabels: true\n      relabelings:\n        - sourceLabels: [__meta_kubernetes_pod_ip]\n          separator: ;\n          regex: (.*)\n          targetLabel: pod_ip\n          replacement: $1\n          action: replace\n        - sourceLabels: [__meta_kubernetes_pod_label_nodeId]\n          separator: ;\n          regex: (.*)\n          targetLabel: nodeId\n          replacement: $1\n          action: replace\n        - sourceLabels: [__meta_kubernetes_pod_label_nifi_cr]\n          separator: ;\n          regex: (.*)\n          targetLabel: nifi_cr\n          replacement: $1\n          action: replace\n```\n\n----------------------------------------\n\nTITLE: Defining a ConfigMap Reference Structure (Markdown)\nDESCRIPTION: Describes the structure used to reference a specific key within a Kubernetes ConfigMap. Requires the ConfigMap name (`name`) and the data key (`data`), optionally specifying the namespace (`namespace`). Used for overriding or replacing configurations.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/5_references/1_nifi_cluster/2_read_only_config.md#_snippet_6\n\nLANGUAGE: Markdown\nCODE:\n```\n## ConfigmapReference\n\n|Field|Type|Description|Required|Default|\n|-----|----|-----------|--------|--------|\n|name|string|Name of the configmap that we want to refer.|Yes|\"\"|\n|namespace|string|Namespace where is located the configmap that we want to refer.|No|\"\"|\n|data|string|The key of the value,in data content, that we want use.|Yes|\"\"|\n```\n\n----------------------------------------\n\nTITLE: Configuring NifiNodeGroupAutoscaler with Kubernetes CRD in YAML\nDESCRIPTION: This snippet demonstrates how to configure a NifiNodeGroupAutoscaler resource in YAML for Kubernetes, providing automatic scaling for specific node groups in a NiFi cluster using NiFiKop. Dependencies include a deployed NifiCluster and NiFiKop operator with permission to manage CRDs. Key parameters include metadata.name for the autoscaler name, spec.clusterRef to reference the target NifiCluster, nodeConfigGroupId for selecting the node config group, nodeLabelsSelector to specify nodes managed by labels, and strategies for scaling up and down. Inputs are Kubernetes CR fields, and the output is an autoscaler resource that integrates with the HorizontalPodAutoscaler for stateful scaling control. Proper labeling and avoiding overlap between autoscalers is required.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/5_references/7_nifi_nodegroup_autoscaler.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiNodeGroupAutoscaler\nmetadata:\n  name: nifinodegroupautoscaler-sample\nspec:\n  # contains the reference to the NifiCluster with the one the node group autoscaler is linked.\n  clusterRef:\n    name: nificluster-name\n    namespace: nifikop\n  # defines the id of the NodeConfig contained in NifiCluster.Spec.NodeConfigGroups\n  nodeConfigGroupId: default-node-group\n  # The selector used to identify nodes in NifiCluster.Spec.Nodes this autoscaler will manage\n  # Use Node.Labels in combination with this selector to clearly define which nodes will be managed by this autoscaler \n  nodeLabelsSelector: \n    matchLabels:\n      nifi_cr: nificluster-name\n      nifi_node_group: default-node-group\n  # the strategy used to decide how to add nodes to a nifi cluster\n  upscaleStrategy: simple\n  # the strategy used to decide how to remove nodes from an existing cluster\n  downscaleStrategy: lifo\n\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiDataflow Kubernetes Resource in YAML\nDESCRIPTION: Example YAML definition for creating a NifiDataflow resource that deploys a versioned flow from a NiFi Registry to a NiFi cluster with specific positioning, sync mode, and update strategy.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/5_references/5_nifi_dataflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  flowPosition:\n    posX: 0\n    posY: 0\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: squidflow\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: nifikop\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Listing Managed NiFi User Groups with kubectl\nDESCRIPTION: This console command snippet shows how to list the NiFi user groups managed by the operator within the Kubernetes namespace 'nifikop'. Running 'kubectl get' for the custom resource 'nifiusergroups.nifi.konpyutaika.com' returns the names and ages of the managed groups (admin, nodes, readers). This requires access to a Kubernetes cluster with the NiFi operator and respective custom resources installed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/2_manage_users_and_accesses/3_managed_groups.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nkubectl get -n nifikop nifiusergroups.nifi.konpyutaika.com \nNAME              AGE\nmanaged-admins    6d7h\nmanaged-nodes     6d7h\nmanaged-readers   6d7h\n```\n\n----------------------------------------\n\nTITLE: Configuring Internal Listeners for NiFi Cluster in YAML\nDESCRIPTION: Defines multiple internal listener configurations for NiFi cluster pods using YAML. Each listener has a type (such as https, cluster, s2s, prometheus, load-balance), a name that identifies the port attached to the pod, and a containerPort representing the port number used inside the container. This snippet is used to specify ports needed for NiFi internal communication, UI access, Site-to-Site communication, load balancing, and metrics exposure. Additional listeners without types can be added for custom NiFi processors.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    - type: \"https\"\n      name: \"https\"\n      containerPort: 8443\n    - type: \"cluster\"\n      name: \"cluster\"\n      containerPort: 6007\n    - type: \"s2s\"\n      name: \"s2s\"\n      containerPort: 10000\n    - type: \"prometheus\"\n      name: \"prometheus\"\n      containerPort: 9090\n    - type: \"load-balance\"\n      name: \"load-balance\"\n      containerPort: 6342\n```\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    ...\n    - name: \"http-tracking\"\n      containerPort: 8081\n```\n\n----------------------------------------\n\nTITLE: Viewing Kubernetes Horizontal Pod Autoscaler Status for NiFi Nodes Using kubectl Console\nDESCRIPTION: This command retrieves the current Horizontal Pod Autoscaler (HPA) objects in the 'clusters' namespace, showing their references, target utilization metrics, minimum and maximum pod replicas, current replicas count, and age. It helps verify that KEDA deployed HPA is actively managing scaling of the NifiNodeGroupAutoscaler resource based on the defined metrics.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_13\n\nLANGUAGE: console\nCODE:\n```\nkubectl get -n clusters hpa\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi User Management with NifiUser Custom Resource in YAML\nDESCRIPTION: This YAML snippet defines a NifiUser resource for managing a NiFi user within a Kubernetes environment. It specifies user identity overrides, references the target NiFi cluster, controls certificate creation options, and sets up granular access policies, including actions and resource component details. The resource name adheres to Kubernetes naming conventions while the identity field supports more permissive naming for NiFi user creation. Access policies include types, actions, and targeted resources with component-specific identifiers. Dependencies include the NiFi Operator managing synchronization between the Kubernetes resource and the NiFi cluster. Inputs are the YAML fields defining user parameters, and the output is the created or bound NiFi user with specified policies. Constraints include naming validity and optional certificate creation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/2_manage_users_and_accesses/1_users_management.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: aguitton\nspec:\n  # identity field is use to define the user identity on NiFi cluster side,\n  #\tit use full when the user's name doesn't suite with Kubernetes resource name.\n  identity: alexandre.guitton@konpyutaika.com\n  # Contains the reference to the NifiCluster with the one the registry client is linked.\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  # Whether or not the the operator also include a Java keystore format (JKS) with you secret\n  includeJKS: false\n  # Whether or not a certificate will be created for this user.\n  createCert: false\n  # defines the list of access policies that will be granted to the group.\n  accessPolicies:\n      # defines the kind of access policy, could be \"global\" or \"component\".\n    - type: component\n      # defines the kind of action that will be granted, could be \"read\" or \"write\"\n      action: read\n      # resource defines the kind of resource targeted by this access policies, please refer to the following page:\n      #\thttps://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#access-policies\n      resource: /data\n      # componentType is used if the type is \"component\", it's allow to define the kind of component on which is the\n      # access policy\n      componentType: \"process-groups\"\n      # componentId is used if the type is \"component\", it's allow to define the id of the component on which is the\n      # access policy\n      componentId: \"\"\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiParameterContext Resource (YAML)\nDESCRIPTION: This YAML snippet defines a `NifiParameterContext` CR for NiFiKop. It represents a NiFi Parameter Context, allowing the definition of parameters (`parameters`) to be used by dataflows. It references the target NiFi cluster (`clusterRef`) and can optionally reference Kubernetes secrets (`secretRefs`) to manage sensitive parameters securely.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\n  namespace: demo\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      value: toto\n      description: toto\n```\n\n----------------------------------------\n\nTITLE: Manually Deploying NiFiKop Custom Resource Definitions\nDESCRIPTION: Commands to manually deploy the Custom Resource Definitions (CRDs) required by NiFiKop. This is needed when deploying the Helm chart with the --skip-crds flag.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/2_deploy_nifikop/1_quick_start.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\n```\n\n----------------------------------------\n\nTITLE: Listing Deployed Helm Charts\nDESCRIPTION: Command to list all current Helm releases in the cluster, including active and possibly failed or deleted releases. Helps in managing Helm deployment states.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nhelm list\n```\n\n----------------------------------------\n\nTITLE: Deploying a NiFi Dataflow with NiFiKop using YAML CRD\nDESCRIPTION: This YAML snippet defines the NifiDataflow custom resource that deploys a NiFi dataflow managed by NiFiKop. It includes references to the parent process group, bucket, flow identifiers, flow version, synchronization mode, skip flags for invalid components, cluster reference, registry client, parameter context, and update strategy. SyncMode controls how the operator manages lifecycle; updateStrategy modifies how updates are applied. This resource enables declarative control over NiFi flow deployments within Kubernetes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: demo\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Cert-Manager Issuer Creation\nDESCRIPTION: This YAML snippet shows how to create a Cert-Manager Issuer for Let's Encrypt. It configures the ACME server, email, private key secret, and HTTP01 solver using ingress. This issuer can be used to obtain SSL certificates for the NiFi cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: cert-manager.io/v1alpha2\nkind: Issuer\nmetadata:\n  name: letsencrypt-staging\nspec:\n  acme:\n    # You must replace this email address with your own.\n    # Let's Encrypt will use this to contact you about expiring\n    # certificates, and issues related to your account.\n    email: <your email address>\n    server: https://acme-staging-v02.api.letsencrypt.org/directory\n    privateKeySecretRef:\n      # Secret resource used to store the account's private key.\n      name: example-issuer-account-key\n    # Add a single challenge solver, HTTP01 using nginx\n    solvers:\n      - http01:\n          ingress:\n            ingressTemplate:\n              metadata:\n                annotations:\n                  \"external-dns.alpha.kubernetes.io/ttl\": \"5\"\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop Helm Chart with Custom Values File\nDESCRIPTION: This command installs the NiFiKop Helm chart from the 'konpyutaika/nifikop' repository, naming the release 'nifikop'. It uses a custom YAML file named 'values.yaml' (specified with the -f flag) to override default chart parameters, allowing for a tailored installation. This is an alternative to setting individual parameters using '--set'.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n$ helm install nifikop \\\n      konpyutaika/nifikop \\\n      -f values.yaml\n```\n\n----------------------------------------\n\nTITLE: Running NiFiKop Migration Script - Bash\nDESCRIPTION: This snippet shows how to execute the migration script using npm with custom type and namespace options. Replace '<NIFIKOP_RESOURCE>' with the resource type (e.g., cluster, dataflow) and '<K8S_NAMESPACE>' with the Kubernetes namespace to migrate. The script reads arguments using 'minimist' and validates type; running this command initiates the resource migration procedure.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nnpm start -- --type=<NIFIKOP_RESOURCE> --namespace=<K8S_NAMESPACE>\n```\n\n----------------------------------------\n\nTITLE: Creating a NifiUserGroup resource in Kubernetes for NiFi access management\nDESCRIPTION: This YAML example defines a NifiUserGroup custom resource that creates a user group in Apache NiFi with specific users and access policies. It includes cluster reference, user references, and access policy configuration for viewing counters information.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/2_manage_users_and_accesses/2_groups_management.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  # Contains the reference to the NifiCluster with the one the registry client is linked.\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  # contains the list of reference to NifiUsers that are part to the group.\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n#     namespace: nifikop\n    - name: nc-controller.nifikop.mgt.cluster.local\n  # defines the list of access policies that will be granted to the group.\n  accessPolicies:\n      # defines the kind of access policy, could be \"global\" or \"component\".\n    - type: global\n      # defines the kind of action that will be granted, could be \"read\" or \"write\"\n      action: read\n      # resource defines the kind of resource targeted by this access policies, please refer to the following page:\n      #\thttps://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#access-policies\n      resource: /counters\n#      # componentType is used if the type is \"component\", it's allow to define the kind of component on which is the\n#      # access policy\n#      componentType: \"process-groups\"\n#      # componentId is used if the type is \"component\", it's allow to define the id of the component on which is the\n#      # access policy\n#      componentId: \"\"\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Internal Listeners in NiFi on Kubernetes\nDESCRIPTION: This YAML snippet demonstrates adding a custom internal listener without a predefined `type`.  This is useful for exposing custom NiFi processors through a specific port (e.g., an HTTP endpoint). It defines the `name` and `containerPort` for the custom listener.  This example shows an `http-tracking` listener on port 8081.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    ...\n    - name: \"http-tracking\"\n      containerPort: 8081\n```\n\n----------------------------------------\n\nTITLE: Configuring External Services for NiFi Cluster in YAML\nDESCRIPTION: Shows a complete example of both internal listener configuration and external service exposure through a LoadBalancer. The external service maps internal ports to externally accessible ones for UI and custom processor endpoints.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    - type: \"https\"\n      name: \"https\"\n      containerPort: 8443\n    - type: \"cluster\"\n      name: \"cluster\"\n      containerPort: 6007\n    - type: \"s2s\"\n      name: \"s2s\"\n      containerPort: 10000\n    - type: \"prometheus\"\n      name: \"prometheus\"\n      containerPort: 9090\n    - type: \"load-balance\"\n      name: \"load-balance\"\n      containerPort: 6342\n    - name: \"http-tracking\"\n      containerPort: 8081\nexternalServices:\n  - name: cluster-access\n    spec:\n      portConfigs:\n        - internalListenerName: https\n          port: 443\n        - internalListenerName: http-tracking\n          port: 80\n      type: LoadBalancer\n```\n\n----------------------------------------\n\nTITLE: NiFi Cluster Service Configuration for Istio\nDESCRIPTION: This YAML snippet shows the service configuration in the NiFi cluster deployment YAML that creates the ClusterIP service targeted by the Istio VirtualService. It exposes port 8443 for the HTTPS internal listener.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nspec:  \n  externalServices:  \n    - name: \"nifi-cluster\"\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8443\n            internalListenerName: \"https\"\n```\n\n----------------------------------------\n\nTITLE: Example NifiNodeGroupAutoscaler Resource Definition (YAML)\nDESCRIPTION: This YAML snippet illustrates a sample configuration for a `NifiNodeGroupAutoscaler` resource. It specifies the target `NifiCluster` via `clusterRef`, identifies the node group using `nodeConfigGroupId` and `nodeLabelsSelector`, and defines the scaling strategies (`upscaleStrategy: simple`, `downscaleStrategy: lifo`). This resource is used by NiFiKop to manage autoscaling for a specific group of nodes within a NiFi cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/5_references/7_nifi_nodegroup_autoscaler.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiNodeGroupAutoscaler\nmetadata:\n  name: nifinodegroupautoscaler-sample\nspec:\n  # contains the reference to the NifiCluster with the one the node group autoscaler is linked.\n  clusterRef:\n    name: nificluster-name\n    namespace: nifikop\n  # defines the id of the NodeConfig contained in NifiCluster.Spec.NodeConfigGroups\n  nodeConfigGroupId: default-node-group\n  # The selector used to identify nodes in NifiCluster.Spec.Nodes this autoscaler will manage\n  # Use Node.Labels in combination with this selector to clearly define which nodes will be managed by this autoscaler \n  nodeLabelsSelector: \n    matchLabels:\n      nifi_cr: nificluster-name\n      nifi_node_group: default-node-group\n  # the strategy used to decide how to add nodes to a nifi cluster\n  upscaleStrategy: simple\n  # the strategy used to decide how to remove nodes from an existing cluster\n  downscaleStrategy: lifo\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Cluster SSL with Cert-Manager Issuer (YAML)\nDESCRIPTION: This YAML snippet shows how to configure a NifiCluster custom resource to use an existing cert-manager Issuer (like the one for Let's Encrypt) to obtain SSL certificates. It enables clusterSecure and siteToSiteSecure and references the desired Issuer by name and kind.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  clusterSecure: true\n  siteToSiteSecure: true\n  ...\n  listenersConfig:\n    clusterDomain: <DNS zone name>\n    useExternalDNS: true\n    ...\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n      issuerRef:\n        name: letsencrypt-staging\n        kind: Issuer\n```\n\n----------------------------------------\n\nTITLE: Defining NifiDataflow Resource in YAML\nDESCRIPTION: This YAML snippet defines a NifiDataflow custom resource for the Nifikop Kubernetes operator. It specifies the dataflow to be deployed by referencing its bucket ID, flow ID, and version from a NiFi Registry. It also links the dataflow to a specific NiFi cluster, registry client, and parameter context, controlling its deployment location and configuration within the NiFi instance. This requires a Kubernetes cluster with the Nifikop operator installed and potentially existing referenced resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/5_references/5_nifi_dataflow.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  flowPosition:\n    posX: 0\n    posY: 0\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: squidflow\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: nifikop\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Creating an Istio DestinationRule for TLS-enabled Traffic with Session Sticky Management\nDESCRIPTION: Establishes a DestinationRule that enforces TLS settings for the specified host, using SIMPLE mode for TLS encryption. It also sets a load balancer policy with cookie-based session affinity using the '__Secure-Authorization-Bearer' cookie, ensuring requests from the same client are consistently routed to the same backend.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_5\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: DestinationRule\nmetadata:\n  name: nifi-dr\nspec:\n  host: <service-name>.<namespace>.svc.cluster.local\n  trafficPolicy:\n    tls:\n      mode: SIMPLE\n    loadBalancer:\n      consistentHash:\n        httpCookie:\n          name: __Secure-Authorization-Bearer\n          ttl: 0s\n```\n\n----------------------------------------\n\nTITLE: Defining a NiFi Registry Client Resource in Kubernetes using YAML\nDESCRIPTION: This YAML snippet demonstrates how to create a NifiRegistryClient CRD instance in Kubernetes to connect an Apache NiFi cluster to a NiFi Registry. It requires the custom resource (provided by the nifikop operator), and references an existing NiFi cluster by name and namespace. The 'uri' specifies the URL of the corresponding NiFi Registry, while optional fields allow you to set a human-readable description. The manifest must be applied to a Kubernetes cluster with the nifikop operator installed; required parameters include spec.clusterRef and spec.uri. The output is a new NifiRegistryClient resource in the given namespace, ready to be managed by the operator.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/5_references/3_nifi_registry_client.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiRegistryClient\nmetadata:\n  name: squidflow\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Squidflow demo\"\n  uri: \"http://nifi-registry:18080\"\n```\n\n----------------------------------------\n\nTITLE: NiFi ClusterIP Service Configuration\nDESCRIPTION: This YAML snippet defines the configuration of a ClusterIP service to expose NiFi with HTTPS, using port 8443 for secure communication. It specifies the service type and internal listener name.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nspec:  \n  externalServices:  \n    - name: \"nifi-cluster\"\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8443\n            internalListenerName: \"https\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio VirtualService for HTTPS\nDESCRIPTION: This YAML snippet defines an Istio VirtualService that routes HTTP traffic to a specific ClusterIP service for NiFi, which handles the encryption back to HTTPS. It uses the decrypted traffic from the Gateway.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: nifi-vs\nspec:\n  gateways:\n  - nifi-gateway\n  hosts:\n  - nifi.my-domain.com\n  http:\n  - match:\n    - uri:\n        prefix: /\n    route:\n    - destination:\n        host: <service-name>.<namespace>.svc.cluster.local\n        port:\n          number: 8443\n```\n\n----------------------------------------\n\nTITLE: Overriding NiFiCluster Configuration with ConfigMap, Secret, and Inline Fields - YAML\nDESCRIPTION: This YAML snippet demonstrates how to customize 'nifiProperties' in a NiFiCluster spec using three mechanisms: referencing Kubernetes ConfigMaps and Secrets, and providing inline override configuration. Required dependencies are a NiFiKop operator and predefined ConfigMap and Secret resources in the appropriate namespace. Key parameters are 'overrideConfigMap' and 'overrideSecretConfig', which refer to the data key, resource name, and namespace, and 'overrideConfigs', which applies direct key-value pairs. Inputs are the referenced ConfigMap/Secret and provided override strings; outputs are the effective NiFi properties applied at runtime based on the documented priority order. Secret values always take precedence over ConfigMaps and direct overrides.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nnifiProperties:\n      # Additionnal nifi.properties configuration that will override the one produced based on template and\n      # configuration\n      overrideConfigMap:\n        # The key of the value,in data content, that we want use.\n        data: nifi.properties\n        # Name of the configmap that we want to refer.\n        name: raw\n        # Namespace where is located the secret that we want to refer.\n        namespace: nifikop\n      # Additionnal nifi.properties configuration that will override the one produced based\n      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n      overrideSecretConfig:\n        # The key of the value,in data content, that we want use.\n        data: nifi.properties\n        # Name of the configmap that we want to refer.\n        name: raw\n        # Namespace where is located the secret that we want to refer.\n        namespace: nifikop\n      # Additionnal nifi.properties configuration that will override the one produced based\n      #\ton template, configurations and overrideConfigMap\n      overrideConfigs: |\n        nifi.ui.banner.text=NiFiKop\n        nifi.sensitive.props.key=thisIsABadSensitiveKeyPassword\n```\n\n----------------------------------------\n\nTITLE: Deploying NifiDataflow in Kubernetes with YAML\nDESCRIPTION: Example YAML configuration for creating a NifiDataflow resource that references existing NifiRegistryClient and NifiParameterContext resources. This defines a versioned flow to be deployed from the NiFi Registry with specific synchronization options.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: demo\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Configuring KEDA ScaledObject for NiFi Node Scaling in YAML\nDESCRIPTION: This YAML snippet defines a KEDA ScaledObject for scaling NiFi node groups based on Prometheus metrics. It specifies target resources, scaling parameters, fallback options, and optional advanced configurations. Dependencies include the Kubernetes CustomResourceDefinition for ScaledObject and a Prometheus metrics server. Input parameters include threshold values, queries, and scaling limits, which influence how the NiFi cluster scales dynamically.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_10\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: cluster\n  namespace: clusters\nspec:\n  scaleTargetRef:\n    apiVersion:    nifi.konpyutaika.com/v1alpha1     # Optional. Default: apps/v1\n    kind:          NifiNodeGroupAutoscaler           # Optional. Default: Deployment\n    name:          nifinodegroupautoscaler-sample    # Mandatory. Must be in the same namespace as the ScaledObject\n    envSourceContainerName: nifi                     # Optional. Default: .spec.template.spec.containers[0]\n  pollingInterval:  30                               # Optional. Default: 30 seconds\n  cooldownPeriod:   300                              # Optional. Default: 300 seconds\n  idleReplicaCount: 0                                # Optional. Default: ignored, must be less than minReplicaCount \n  minReplicaCount:  1                                # Optional. Default: 0\n  maxReplicaCount:  3                                # Optional. Default: 100\n  fallback:                                          # Optional. Section to specify fallback options\n    failureThreshold: 5                              # Mandatory if fallback section is included\n    replicas: 1                                      # Mandatory if fallback section is included\n  # advanced:                                          # Optional. Section to specify advanced options\n  #   restoreToOriginalReplicaCount: true              # Optional. Whether the target resource should be scaled back to original replicas count, after the ScaledObject is deleted\n  #   horizontalPodAutoscalerConfig:                   # Optional. Section to specify HPA related options\n  #     name: {name-of-hpa-resource}                   # Optional. Default: keda-hpa-{scaled-object-name}\n  #     behavior:                                      # Optional. Use to modify HPA's scaling behavior\n  #       scaleDown:\n  #         stabilizationWindowSeconds: 300 <--- \n  #         policies:\n  #         - type: Percent\n  #           value: 100\n  #           periodSeconds: 15\n  triggers:\n    - type: prometheus\n      metadata:\n        serverAddress: http://prometheus-operated.monitoring-system.svc:9090\n        metricName: http_requests_total\n        threshold: <threshold value>\n        query: <prometheus query>\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Secret for Sensitive NiFi Parameters via CLI\nDESCRIPTION: Shows a command to create a generic Kubernetes secret containing key-value pairs for sensitive parameters referenced by NiFiParameterContext. These secrets store sensitive configuration values that cannot be retrieved from NiFi's REST API, and require careful update procedures to reflect in NiFi parameter contexts.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl create secret generic secret-params \\\n    --from-literal=secret1=yop \\\n    --from-literal=secret2=yep \\\n    -n nifikop\n```\n\n----------------------------------------\n\nTITLE: Install NiFiKop Operator via Helm (Standard)\nDESCRIPTION: Deploys the NiFiKop Kubernetes operator using the official Helm chart from an OCI registry. The command specifies the chart name, namespace (`nifi`), version, image tag, resource requests/limits, and target namespaces for the operator to watch. Requires Helm installed and configured.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/2_deploy_nifikop/1_quick_start.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop     oci://ghcr.io/konpyutaika/helm-charts/nifikop     --namespace=nifi     --version 1.11.3     --set image.tag=v1.11.3-release     --set resources.requests.memory=256Mi     --set resources.requests.cpu=250m     --set resources.limits.memory=256Mi     --set resources.limits.cpu=250m     --set namespaces={\"nifi\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Cluster with External Issuer\nDESCRIPTION: YAML configuration for setting up a NiFi cluster to use an external certificate issuer with external DNS support.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/3_tasks/2_security/1_ssl.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\n...\nspec:\n  ...\n  clusterSecure: true\n  siteToSiteSecure: true\n  ...\n  listenersConfig:\n    clusterDomain: <DNS zone name>\n    useExternalDNS: true\n    ...\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n      issuerRef:\n        name: letsencrypt-staging\n        kind: Issuer\n```\n\n----------------------------------------\n\nTITLE: Defining a NiFi Registry Client using Kubernetes CRD in YAML\nDESCRIPTION: This YAML snippet defines a NifiRegistryClient custom resource for NiFiKop, which represents a connection to a NiFi Registry instance. The resource specifies the cluster reference, a description, and the URI of the NiFi Registry server. It is required as a prerequisite to manage dataflows through NiFi Registry integration. The snippet must be applied to the Kubernetes cluster before deploying dataflows that rely on registry capabilities.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiRegistryClient\nmetadata:\n  name: registry-client-example\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Registry client managed by NiFiKop\"\n  uri: \"http://nifi.hostname.com:18080\"\n```\n\n----------------------------------------\n\nTITLE: Deploying a Sample NiFi Cluster\nDESCRIPTION: Deploys a simple NiFi cluster instance using a sample NiFiCluster custom resource definition (CR). This command applies the sample YAML file to the 'nifi' namespace using kubectl. You need to ensure the Zookeeper service name is correctly configured in the sample file.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/2_setup/1_getting_started.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n# Add your zookeeper svc name to the configuration\nkubectl create -n nifi -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining NodeConfigGroups in YAML\nDESCRIPTION: This YAML snippet defines two `NodeConfigGroup` configurations: `default_group` and `high_mem_group`.  `NodeConfigGroups` are used to set various properties for a NiFi pod. They specify resource requirements, the user the NiFi process runs as, and service account and the amount of data provenance information to store.  The main purpose of NodeConfigGroups is to define the purely technical requirements for the pod that will be deployed, allowing for different resource allocations and settings for different node types within the NiFi cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  # nodeConfigGroups specifies multiple node configs with unique name\n  nodeConfigGroups:\n    default_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      serviceAccountName: \"default\"\n      # resourceRequirements works exactly like Container resources, the user can specify the limit and the requests\n      # through this property\n      # https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 3Gi\n    high_mem_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      serviceAccountName: \"default\"\n      # resourceRequirements works exactly like Container resources, the user can specify the limit and the requests\n      # through this property\n      # https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 30Gi\n        requests:\n          cpu: \"1\"\n          memory: 30Gi\n```\n\n----------------------------------------\n\nTITLE: Mapping Internal Listeners to External Service Ports (YAML)\nDESCRIPTION: Shows how to expose specific internal listeners through an external Kubernetes service by mapping internal listener names to external ports, enabling external access to NiFi endpoints and ensuring seamless external communication.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_5\n\nLANGUAGE: YAML\nCODE:\n```\nkubectl get services\nNAME                TYPE           CLUSTER-IP    EXTERNAL-IP      PORT(S)                                AGE\ncluster-access      LoadBalancer   10.88.21.98   35.180.241.132   443:30421/TCP,80:30521/TCP             20d\n```\n\n----------------------------------------\n\nTITLE: Defining NodeConfig for NiFi Operator in YAML\nDESCRIPTION: This YAML snippet demonstrates how to define a NodeConfig for deploying a node in an Apache NiFi cluster managed by a Kubernetes operator. It includes configuration for provenance storage, specifying user IDs, node roles, associated Docker images, volume mounts (both external and storage-specific), pod metadata annotations and labels, affinity, priority class, and other runtime settings. To utilize this configuration, ensure you have access to a Kubernetes cluster with NiFi operator installed, and modify the properties based on your infrastructure needs and the documentation references supplied in comments. Key parameters include provenanceStorage (limiting provenance data size), runAsUser (the user ID for the container), isNode (whether this config is for a node), imagePullPolicy (how container images are fetched), externalVolumeConfigs (mounting additional volumes/secrets), and storageConfigs (defining persistent storage requirements for provenance and logs). Limitations may apply based on underlying Kubernetes storage classes and resource quotas.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/5_references/1_nifi_cluster/3_node_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n   default_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      # Set this to true if the instance is a node in a cluster.\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#basic-cluster-setup\n      isNode: true\n      # Additionnal metadata to merge to the pod associated\n      podMetadata:\n        annotations:\n          node-annotation: \"node-annotation-value\"\n        labels:\n          node-label: \"node-label-value\"\n      # Docker image used by the operator to create the node associated\n      # https://hub.docker.com/r/apache/nifi/\n#      image: \"apache/nifi:1.11.2\"\n      # nodeAffinity can be specified, operator populates this value if new pvc added later to node\n      # https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity\n#      nodeAffinity:\n      # imagePullPolicy define the pull policy for NiFi cluster docker image\n      imagePullPolicy: IfNotPresent\n      # priorityClassName define the name of the priority class to be applied to these nodes\n      priorityClassName: \"example-priority-class-name\"\n      # externalVolumeConfigs specifies a list of volume to mount into the main container.\n      externalVolumeConfigs:\n        - name: example-volume\n          mountPath: \"/opt/nifi/example\"\n          secret:\n            secretName: \"raw-controller\"\n      # storageConfigs specifies the node related configs\n      storageConfigs:\n        # Name of the storage config, used to name PV to reuse into sidecars for example.\n        - name: provenance-repository\n          # Retain this PVC throughout NifiCluster deletions.\n          reclaimPolicy: Retain\n          # Path where the volume will be mount into the main nifi container inside the pod.\n          mountPath: \"/opt/nifi/provenance_repository\"\n          # Metadata to attach to the PVC that gets created\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          # Kubernetes PVC spec\n          # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          reclaimPolicy: Delete\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Cluster for Autoscaling (Partial)\nDESCRIPTION: Defines a specific 'auto_scaling' node configuration group within a NiFiCluster Custom Resource. This group includes a Prometheus listener port and standard resource requests/limits and storage configurations for the nodes intended for autoscaling.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n...\nspec:\n  ...\n  listenersConfig:\n    internalListeners:\n    ...\n    - containerPort: 9090\n      name: prometheus\n      type: prometheus \n    ...\n  ...\n  nodeConfigGroups:\n    auto_scaling:\n      isNode: true\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 2Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n      serviceAccountName: external-dns\n      storageConfigs:\n        - mountPath: /opt/nifi/nifi-current/logs\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/data\n          name: data\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/extensions\n          name: extensions-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/flowfile_repository\n          name: flowfile-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/nifi-current/conf\n          name: conf\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/content_repository\n          name: content-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/provenance_repository\n          name: provenance-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n  ...\n\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Internal Listener Configuration in YAML\nDESCRIPTION: Demonstrates how to define an additional internal listener port that is not tied to specific core NiFi functions, useful for exposing custom endpoints like processor-specific HTTP listeners. These listeners are defined within the `internalListeners` list but omit the 'type' field.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    ...\n    - name: \"http-tracking\"\n      containerPort: 8081\n```\n\n----------------------------------------\n\nTITLE: Scaling Down a NiFi Cluster by Removing a Node in YAML\nDESCRIPTION: Shows the modified `NifiCluster` YAML configuration where a node (ID 2) is commented out from the `spec.nodes` list, effectively signaling NiFiKop to remove it from the cluster and trigger a graceful scale-down process.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  headlessServiceEnabled: true\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.11.3\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n# >>>> START: node removed\n#    - id: 2\n#      nodeConfigGroup: \"default_group\"\n# <<<< END\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: Removing Node from NiFiCluster Spec - NiFiKop - YAML\nDESCRIPTION: This YAML snippet shows the `NifiCluster` custom resource definition modified for a scale-down operation. It demonstrates removing node 2 by commenting out or deleting its entry from the `spec.nodes` list. Applying this configuration instructs the NiFiKop operator to begin the graceful decommissioning process for the specified node.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  headlessServiceEnabled: true\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.11.3\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n# >>>> START: node removed\n#    - id: 2\n#      nodeConfigGroup: \"default_group\"\n# <<<< END\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: Installing CRDs Manually\nDESCRIPTION: This snippet shows how to manually apply Custom Resource Definitions (CRDs) using `kubectl`. This is necessary if the user chooses to skip CRD deployment through Helm.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifinodegroupautoscalers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificonnections.yaml\n```\n\n----------------------------------------\n\nTITLE: Declaring External NiFi Cluster Resource YAML\nDESCRIPTION: YAML snippet to declare an external NiFi cluster resource for the Nifikop operator. It defines the root process group, node URI template, node IDs, cluster type, client authentication type, and references a Kubernetes secret for authentication. This configuration enables the operator to manage resources on an external NiFi cluster by specifying connection details and credentials. Required fields include rootProcessGroupId, nodes with integer IDs, and secretRef with access credentials.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/1_manage_clusters/3_external_cluster.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: externalcluster\nspec:\n  # rootProcessGroupId contains the uuid of the root process group for this cluster.\n  rootProcessGroupId: 'd37bee03-017a-1000-cff7-4eaaa82266b7'\n  # nodeURITemplate used to dynamically compute node uri.\n  nodeURITemplate: 'nifi0%d.integ.mapreduce.m0.p.fti.net:9090'\n  # all node requiresunique id\n  nodes:\n    - id: 1\n    - id: 2\n    - id: 3\n  # type defines if the cluster is internal (i.e manager by the operator) or external.\n  # :Enum={\"external\",\"internal\"}\n  type: 'external'\n  # clientType defines if the operator will use basic or tls authentication to query the NiFi cluster.\n  # Enum={\"tls\",\"basic\"}\n  clientType: 'basic'\n  # secretRef reference the secret containing the informations required to authenticate to the cluster.\n  secretRef:\n    name: nifikop-credentials\n    namespace: nifikop-nifi\n```\n\n----------------------------------------\n\nTITLE: Configuring ServiceMonitor for NiFi Metrics\nDESCRIPTION: Defines a ServiceMonitor custom resource that configures how Prometheus scrapes metrics from the NiFi cluster. Includes relabeling configurations to capture pod IP, node ID, and NiFi CR labels.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: cluster\n  namespace: monitoring-system\n  labels:\n    app: nifi-cluster\n    nifi_cr: cluster\nspec:\n  selector:\n    matchLabels:\n      app: nifi\n      nifi_cr: cluster\n  namespaceSelector:\n    matchNames:\n      - clusters\n  endpoints:\n    - interval: 10s\n      port: prometheus\n      path: /metrics\n      honorLabels: true\n      relabelings:\n        - sourceLabels: [__meta_kubernetes_pod_ip]\n          separator: ;\n          regex: (.*)\n          targetLabel: pod_ip\n          replacement: $1\n          action: replace\n        - sourceLabels: [__meta_kubernetes_pod_label_nodeId]\n          separator: ;\n          regex: (.*)\n          targetLabel: nodeId\n          replacement: $1\n          action: replace\n        - sourceLabels: [__meta_kubernetes_pod_label_nifi_cr]\n          separator: ;\n          regex: (.*)\n          targetLabel: nifi_cr\n          replacement: $1\n          action: replace\n```\n\n----------------------------------------\n\nTITLE: Overriding NiFi Properties via ConfigMap, Secret, and Inline in Kubernetes\nDESCRIPTION: This YAML snippet demonstrates how to specify multiple configuration override methods for NiFi properties, such as ConfigMap references, Secret references, and inline configuration content. It highlights how the system resolves conflicts based on security priority: Secret > ConfigMap > Override > Default.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\nnifiProperties:\n  overrideConfigMap:\n    data: nifi.properties\n    name: raw\n    namespace: nifikop\n  overrideSecretConfig:\n    data: nifi.properties\n    name: raw\n    namespace: nifikop\n  overrideConfigs: |\n    nifi.ui.banner.text=NiFiKop\n    nifi.sensitive.props.key=thisIsABadSensitiveKeyPassword\n```\n\n----------------------------------------\n\nTITLE: Define NodeConfigGroups with Resource Requirements - YAML\nDESCRIPTION: This YAML snippet defines two node configuration groups: `default_group` and `high_mem_group`. Each group specifies resource requirements such as CPU and memory limits and requests, along with provenance storage and the user to run the NiFi image as.  The `default_group` requests 3Gi of memory while `high_mem_group` requests 30Gi.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  # nodeConfigGroups specifies multiple node configs with unique name\n  nodeConfigGroups:\n    default_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      serviceAccountName: \"default\"\n      # resourceRequirements works exactly like Container resources, the user can specify the limit and the requests\n      # through this property\n      # https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 3Gi\n    high_mem_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      serviceAccountName: \"default\"\n      # resourceRequirements works exactly like Container resources, the user can specify the limit and the requests\n      # through this property\n      # https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 30Gi\n        requests:\n          cpu: \"1\"\n          memory: 30Gi\n```\n\n----------------------------------------\n\nTITLE: ZookeeperProperties Configuration Structure in Markdown\nDESCRIPTION: This table defines the fields available for configuring Zookeeper properties in NiFiKop, including override options via ConfigMap, direct string configuration, and Secret references.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/5_references/1_nifi_cluster/2_read_only_config.md#_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n## ZookeeperProperties\n\n|Field|Type|Description|Required|Default|\n|-----|----|-----------|--------|--------|\n|overrideConfigMap|[ConfigmapReference](#configmapreference)|Additionnal zookeeper.properties configuration that will override the one produced based on template and configuration.|No|nil|\n|overrideConfigs|string|Additionnal zookeeper.properties configuration that will override the one produced based on template, configurations and overrideConfigMap.|No|\"\"|\n|overrideSecretConfig|[SecretConfigReference](#secretconfigreference)|Additionnal zookeeper.properties configuration that will override the one produced based on template, configurations, overrideConfigMap and overrideConfigs.|No|nil|\n```\n\n----------------------------------------\n\nTITLE: Defining Multiple NodeConfigGroups in NiFiKop YAML\nDESCRIPTION: This YAML snippet shows the definition of two distinct `NodeConfigGroup` resources within a NiFiKop `NifiCluster` specification. The `default_group` requests 3Gi of memory, while the `high_mem_group` requests 30Gi, illustrating how to create differentiated node types based on resource needs. It also includes configurations for provenance storage size (`provenanceStorage`), the user ID for the NiFi container (`runAsUser`), the service account name (`serviceAccountName`), and Kubernetes resource requests/limits.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  # nodeConfigGroups specifies multiple node configs with unique name\n  nodeConfigGroups:\n    default_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      serviceAccountName: \"default\"\n      # resourceRequirements works exactly like Container resources, the user can specify the limit and the requests\n      # through this property\n      # https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 3Gi\n    high_mem_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      serviceAccountName: \"default\"\n      # resourceRequirements works exactly like Container resources, the user can specify the limit and the requests\n      # through this property\n      # https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 30Gi\n        requests:\n          cpu: \"1\"\n          memory: 30Gi\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Namespace for Monitoring - console\nDESCRIPTION: This snippet creates a dedicated namespace called 'monitoring-system' in the Kubernetes cluster for isolating monitoring resources. It requires kubectl to be installed and authenticated against your cluster. No input parameters are required, and the output is the creation of a new namespace object.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_5\n\nLANGUAGE: console\nCODE:\n```\nkubectl create namespace monitoring-system\n\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Namespace for Prometheus Monitoring Using kubectl Console\nDESCRIPTION: Creates a dedicated Kubernetes namespace called 'monitoring-system' to isolate Prometheus monitoring resources. This namespace ensures that all Prometheus-related components operate within their own scope, avoiding conflicts with other namespaces. Dependent on having a working kubectl context connected to the target cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_5\n\nLANGUAGE: console\nCODE:\n```\nkubectl create namespace monitoring-system\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFiKop Operator with Helm\nDESCRIPTION: This command installs the NiFiKop operator using a Helm chart. It specifies the chart's location (OCI registry), namespace, version, image tag, resource requests and limits, and the namespaces to watch. The `--set` parameters are used to customize the deployment. Requires Helm and a Kubernetes cluster. The `nifi` namespace must be created before running the install command. This will deploy all the necessary CRDs too if they are not already deployed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/2_deploy_nifikop/1_quick_start.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# You have to create the namespace before executing following command\nhelm install nifikop \\\n    oci://ghcr.io/konpyutaika/helm-charts/nifikop \\\n    --namespace=nifi \\\n    --version 1.8.0 \\\n    --set image.tag=v1.8.0-release \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set namespaces={\"nifi\"}\n```\n\n----------------------------------------\n\nTITLE: Migrating NiFiKop Resources Using Kubernetes API - JavaScript\nDESCRIPTION: This JavaScript script migrates NiFiKop custom resources from 'nifi.orange.com/v1alpha1' to 'nifi.konpyutaika.com/v1alpha1'. It uses the '@kubernetes/client-node' and 'minimist' libraries to connect to the Kubernetes API, list resources, recreate them in the new CRD, and copy status fields. Dependencies include Node.js 15.3.0+, '@kubernetes/client-node', and 'minimist'. Requires the old operator stopped, both CRDs installed, and appropriate kubeconfig. Expects '--type' and optionally '--namespace' CLI parameters; outputs logs for each major operation and handles API errors gracefully. Limitations: It refuses to copy resources with ownerReferences set, supports only explicit NiFiKop resource types, and assumes matching kinds between source and destination.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nprocess.env['NODE_TLS_REJECT_UNAUTHORIZED'] = 0;\nconst k8s = require('@kubernetes/client-node');\n\nconst kc = new k8s.KubeConfig();\nkc.loadFromDefault();\n\nconst k8sApi = kc.makeApiClient(k8s.CustomObjectsApi);\n\nconst KONPYUTAIKA_GROUP = 'nifi.konpyutaika.com';\nconst KONPYUTAIKA_GROUP_VERSION = 'v1alpha1';\nconst ORANGE_GROUP = 'nifi.orange.com';\nconst ORANGE_GROUP_VERSION = 'v1alpha1';\n\nconst call = async (SRC_GRP, SRC_GRP_VER, DST_GRP, DST_GRP_VER, KIND_PLURAL, NAMESPACE) => {\n\tconsole.log(`Listing ${KIND_PLURAL} of ${SRC_GRP}/${SRC_GRP_VER} in ${NAMESPACE}...`);\n\tconst listResources = (await k8sApi.listNamespacedCustomObject(SRC_GRP, SRC_GRP_VER, NAMESPACE, KIND_PLURAL)).body.items;\n\treturn Promise.all(listResources.map(async (resource) => {\n\t\ttry {\n\t\t\tconsole.log(`Found ${resource.kind} \\\"${resource.metadata.name}\\\" of ${resource.apiVersion} in ${NAMESPACE}`);\n\n\t\t\tif (resource.metadata.ownerReferences) {\n\t\t\t\tconsole.log(`${resource.kind} ${resource.metadata.name} mananged by something else (ownerRefereces is set).`);\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tconst bodyResource = {\n\t\t\t\tapiVersion: `${DST_GRP}/${DST_GRP_VER}`,\n\t\t\t\tkind: resource.kind,\n\t\t\t\tmetadata: {\n\t\t\t\t\tname: resource.metadata.name,\n\t\t\t\t\tannotations: resource.metadata.annotations,\n\t\t\t\t\tlabels: resource.metadata.labels\n\t\t\t\t},\n\t\t\t\tspec: resource.spec\n\t\t\t};\n\n\t\t\tconsole.log(`Creating ${bodyResource.kind} \\\"${bodyResource.metadata.name}\\\" of ${bodyResource.apiVersion} in ${NAMESPACE}...`);\n\t\t\tconst newResource = (await k8sApi.createNamespacedCustomObject(DST_GRP, DST_GRP_VER, NAMESPACE, KIND_PLURAL, bodyResource)).body;\n\t\t\tconsole.log('...done creating.');\n\n\t\t\tconst bodyStatus = {\n\t\t\t\tapiVersion: newResource.apiVersion,\n\t\t\t\tkind: newResource.kind,\n\t\t\t\tmetadata: {\n\t\t\t\t\tname: newResource.metadata.name,\n\t\t\t\t\tresourceVersion: newResource.metadata.resourceVersion\n\t\t\t\t},\n\t\t\t\tstatus: resource.status\n\t\t\t};\n\n\t\t\tconsole.log(`Copying status from ${resource.kind} \\\"${resource.metadata.name}\\\" of ${newResource.apiVersion} to ${newResource.kind} \\\"${newResource.metadata.name}\\\" of ${newResource.apiVersion} in ${NAMESPACE}...`);\n\t\t\tconst newResourceWithStatus = (await k8sApi.replaceNamespacedCustomObjectStatus(DST_GRP, DST_GRP_VER, NAMESPACE, KIND_PLURAL, bodyStatus.metadata.name, bodyStatus)).body;\n\t\t\tconsole.log('...done copying.');\n\t\t\treturn newResourceWithStatus;\n\t\t}\n\t\tcatch (e) {\n\t\t\tconsole.error(e.body ? e.body.message ? e.body.message : e.body : e);\n\t\t}\n\t}));\n};\n\nconst argv = require('minimist')(process.argv.slice(2));\n\nlet NAMESPACE = argv.namespace ? argv.namespace.length > 0 ? argv.namespace : 'default' : 'default';\nlet KIND_PLURAL = {\n\tcluster: 'nificlusters',\n\tdataflow: 'nifidataflows',\n\tparametercontext: 'nifiparametercontexts',\n\tregistryclient: 'nifiregistryclients',\n\tuser: 'nifiusers',\n\tusergroup: 'nifiusergroups',\n};\n\nif (!argv.type) {\n\tconsole.error('Type not provided');\n\tprocess.exit(1);\n}\n\nif (!KIND_PLURAL[argv.type]) {\n\tconsole.error(`Type ${argv.type} is not one of the following types: ${Object.keys(KIND_PLURAL)}`);\n\tprocess.exit(1);\n}\n\nconsole.log(`########### START: ${KIND_PLURAL[argv.type]} ###########`);\ncall( ORANGE_GROUP, ORANGE_GROUP_VERSION, KONPYUTAIKA_GROUP, KONPYUTAIKA_GROUP_VERSION, KIND_PLURAL[argv.type], NAMESPACE)\n\t.then(r => console.log('############ END ############'))\n\t.catch(e => console.error(e));\n```\n\n----------------------------------------\n\nTITLE: Declaring an External NiFi Cluster Resource with Nifikop - YAML\nDESCRIPTION: This YAML manifest defines a NifiCluster resource referencing an external NiFi cluster for use with the Nifikop operator. Key parameters include 'rootProcessGroupId' for the root group UUID, 'nodeURITemplate' for dynamic host address generation, a list of integer node IDs, 'type' set to 'external', 'clientType' for authentication mode ('basic' or 'tls'), and 'secretRef' specifying the Kubernetes secret for credentials. Dependencies include the proper installation of the Nifikop operator CRDs and an existing external NiFi cluster accessible according to the provided configuration. The expected input is a set of cluster details; output is a Kubernetes resource recognized by Nifikop. Limitations: Node IDs must be int32, and host template formatting must conform to node addressing requirements.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/1_manage_clusters/3_external_cluster.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: externalcluster\nspec:\n  # rootProcessGroupId contains the uuid of the root process group for this cluster.\n  rootProcessGroupId: 'd37bee03-017a-1000-cff7-4eaaa82266b7'\n  # nodeURITemplate used to dynamically compute node uri.\n  nodeURITemplate: 'nifi0%d.integ.mapreduce.m0.p.fti.net:9090'\n  # all node requiresunique id\n  nodes:\n    - id: 1\n    - id: 2\n    - id: 3\n  # type defines if the cluster is internal (i.e manager by the operator) or external.\n  # :Enum={\"external\",\"internal\"}\n  type: 'external'\n  # clientType defines if the operator will use basic or tls authentication to query the NiFi cluster.\n  # Enum={\"tls\",\"basic\"}\n  clientType: 'basic'\n  # secretRef reference the secret containing the informations required to authenticate to the cluster.\n  secretRef:\n    name: nifikop-credentials\n    namespace: nifikop-nifi\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop Helm Chart (Setting Parameters)\nDESCRIPTION: This command installs the NiFiKop Helm chart and sets the `namespaces` parameter to `{\"nifikop\"}`. The `--set` option allows overriding default values in the chart.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/2_setup/3_install/1_customizable_install_with_helm.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop --set namespaces={\\\"nifikop\\\"}\n```\n\n----------------------------------------\n\nTITLE: Verifying Kubernetes Resources after NiFi Cluster Scale-Up\nDESCRIPTION: Demonstrates using `kubectl get` to list the pods, configmaps, and persistent volume claims associated with the newly added NiFi node (ID 25) by filtering with the label `nodeId=25`. This confirms the creation of necessary Kubernetes resources for the new node.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl get pods,configmap,pvc -l nodeId=25\nNAME                          READY   STATUS    RESTARTS   AGE\npod/simplenifi-25-nodem5jh4   1/1     Running   0          11m\n\nNAME                             DATA   AGE\nconfigmap/simplenifi-config-25   7      11m\n\nNAME                                               STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\npersistentvolumeclaim/simplenifi-25-storagehwn24   Bound    pvc-7da86076-728e-11ea-846d-42010a8400f2   10Gi       RWO            standard       11m\n```\n\n----------------------------------------\n\nTITLE: Installing Helm Chart - Bash\nDESCRIPTION: This command installs the NiFiKop operator using Helm. It specifies the image tag, namespace, and chart name.  It uses `helm install` command to deploy the operator to your Kubernetes cluster.  The `--set` flag is used to customize the deployment, setting the image tag and namespace.  The chart name is set to the branch name.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/6_contributing/1_developer_guide.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nhelm install skeleton ./helm/nifikop \\\n    --set image.tag=v0.5.1-release \\\n    --namespace-{\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Declaring a NifiUserGroup Kubernetes Resource in YAML\nDESCRIPTION: This YAML snippet demonstrates how to define a NifiUserGroup resource for use with the nifikop NiFi operator. It specifies the apiVersion, kind, and metadata, along with a spec designating the NiFi cluster reference, the users to include in the group, and a list of access policies. Required fields include 'metadata.name' and 'spec.clusterRef'. This resource enables automated creation and management of NiFi user groups, with inputs defined as cluster and user references and the output as a fully managed resource in the Kubernetes API.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/5_references/6_nifi_usergroup.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n    - name: nc-controller.nifikop.mgt.cluster.local\n  accessPolicies:\n    - type: global\n      action: read\n      resource: /counters\n```\n\n----------------------------------------\n\nTITLE: Defining NifiUser Resource in YAML\nDESCRIPTION: This YAML snippet defines a `NifiUser` resource within a Kubernetes environment using the custom resource definition (CRD).  The resource specifies an identity, cluster reference, and other settings. It utilizes the `apiVersion`, `kind`, and `metadata` fields to identify the resource and its properties.  The `spec` section includes the user's identity, and cluster reference, and also allows to manage certificates for the user.   It's a building block for creating and managing NiFi users in a declarative way using Kubernetes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/5_references/2_nifi_user.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiUser\nmetadata:\n  name: aguitton\nspec:\n  identity: alexandre.guitton@konpyutaika.com\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  createCert: false\n```\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio VirtualService for HTTPS NiFi Routing\nDESCRIPTION: This YAML configuration defines an Istio VirtualService for HTTPS that routes decrypted traffic to the ClusterIP Service of the NiFi cluster. It directs all requests for the specified host to port 8443 on the service.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: nifi-vs\nspec:\n  gateways:\n  - nifi-gateway\n  hosts:\n  - nifi.my-domain.com\n  http:\n  - match:\n    - uri:\n        prefix: /\n    route:\n    - destination:\n        host: <service-name>.<namespace>.svc.cluster.local\n        port:\n          number: 8443\n```\n\n----------------------------------------\n\nTITLE: Defining a NiFi Cluster with YAML in NiFiKop\nDESCRIPTION: A complete example of a NifiCluster resource definition that sets up a two-node NiFi cluster with various configurations including service definitions, storage, resource requirements, and network listeners.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/5_references/1_nifi_cluster/1_nifi_cluster.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n    annotations:\n      tyty: ytyt\n    labels:\n      cluster-name: simplenifi\n      tete: titi\n  zkAddress: \"zookeeper.zookeeper:2181\"\n  zkPath: /simplenifi\n  externalServices:\n    - metadata:\n        annotations:\n          toto: tata\n        labels:\n          cluster-name: driver-simplenifi\n          titi: tutu\n      name: driver-ip\n      spec:\n        portConfigs:\n          - internalListenerName: http\n            port: 8080\n        type: ClusterIP\n  clusterImage: \"apache/nifi:1.26.0\"\n  initContainerImage: \"bash:5.2.2\"\n  oneNifiNodePerNode: true\n  readOnlyConfig:\n    nifiProperties:\n      overrideConfigs: |\n        nifi.sensitive.props.key=thisIsABadSensitiveKeyPassword\n  pod:\n    annotations:\n      toto: tata\n    labels:\n      cluster-name: simplenifi\n      titi: tutu\n  nodeConfigGroups:\n    default_group:\n      imagePullPolicy: IfNotPresent\n      isNode: true\n      serviceAccountName: default\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          reclaimPolicy: Delete\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      resourcesRequirements:\n        limits:\n          cpu: \"1\"\n          memory: 2Gi\n        requests:\n          cpu: \"1\"\n          memory: 2Gi\n  nodes:\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n    - id: 2\n      nodeConfigGroup: \"default_group\"\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - containerPort: 8080\n        type: http\n        name: http\n      - containerPort: 6007\n        type: cluster\n        name: cluster\n      - containerPort: 10000\n        type: s2s\n        name: s2s\n      - containerPort: 9090\n        type: prometheus\n        name: prometheus\n      - containerPort: 6342\n        type: load-balance\n        name: load-balance\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi UserGroup using YAML\nDESCRIPTION: This YAML snippet defines a NiFi UserGroup resource that specifies cluster reference, user references, and access policies. It includes metadata and specification details, enabling management of user groups via Kubernetes-style manifests. Dependencies include cluster and user reference schemas; inputs are cluster and user details, outputs are a configured UserGroup resource.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/5_references/6_nifi_usergroup.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n    - name: nc-controller.nifikop.mgt.cluster.local\n  accessPolicies:\n    - type: global\n      action: read\n      resource: /counters\n```\n\n----------------------------------------\n\nTITLE: Adding Bitnami Helm Chart Repository - Bash\nDESCRIPTION: Adds the Bitnami repository to the local Helm configuration, enabling access to Bitnami charts such as Zookeeper. Requires Helm CLI to be installed and configured. No parameters; repository is referenced by subsequent helm install commands.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhelm repo add bitnami https://charts.bitnami.com/bitnami\n\n```\n\n----------------------------------------\n\nTITLE: Defining NifiNodeGroupAutoscaler Resource\nDESCRIPTION: YAML definition for a NifiNodeGroupAutoscaler custom resource that configures how NiFi nodes should be auto-scaled, including cluster reference, node config group, scaling strategies, and label selectors.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiNodeGroupAutoscaler\nmetadata:\n  name: nifinodegroupautoscaler-sample\n  namespace: clusters\nspec:\n  # contains the reference to the NifiCluster with the one the node group autoscaler is linked.\n  clusterRef:\n    name: cluster\n    namespace: clusters\n  # defines the id of the NodeConfig contained in NifiCluster.Spec.NodeConfigGroups\n  nodeConfigGroupId: auto_scaling\n  # readOnlyConfig can be used to pass Nifi node config\n  # which has type read-only these config changes will trigger rolling upgrade\n  readOnlyConfig:\n    nifiProperties:\n      overrideConfigs: |\n        nifi.ui.banner.text=NiFiKop - Scale Group\n  # This is an example of a node config you can apply to each replica in this node group.\n  # Any settings here will override those in the configured nodeConfigGroupId\n#  nodeConfig:\n#    nodeSelector:\n#      node_type: high-mem\n  # The selector used to identify nodes in NifiCluster.Spec.Nodes this autoscaler will manage\n  # Use Node.Labels in combination with this selector to clearly define which nodes will be managed by this autoscaler \n  nodeLabelsSelector: \n    matchLabels:\n      nifi_cr: cluster\n      nifi_node_group: auto-scaling\n  # the strategy used to decide how to add nodes to a nifi cluster\n  upscaleStrategy: simple\n  # the strategy used to decide how to remove nodes from an existing cluster\n  downscaleStrategy: lifo\n```\n\n----------------------------------------\n\nTITLE: Deploy CRDs Manually\nDESCRIPTION: This command deploys Custom Resource Definitions (CRDs) manually using `kubectl`.  It's intended for use when the Helm chart's default CRD installation is skipped (e.g., using `--skip-crds`). The CRDs are essential for defining the custom resources managed by the Nifikop operator.  Dependencies: kubectl, Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifinodegroupautoscalers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificonnections.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring LoadBalancer External Service with AWS NLB in YAML\nDESCRIPTION: This YAML example demonstrates configuring an external service with type LoadBalancer and specifying a loadBalancerClass to use AWS Network Load Balancer (NLB). It defines multiple port configurations with distinct internal listener names and protocols (TCP and UDP), plus metadata annotations and labels. This configuration is intended to expose Nifi listeners externally via a cloud provider-managed load balancer with fine-grained control over ports and traffic protocols.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/5_references/1_nifi_cluster/7_external_service_config.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nexternalServices:\n    - name: \"nlb\"\n      spec:\n        type: LoadBalancer\n        loadBalancerClass: \"service.k8s.aws/nlb\"\n        portConfigs:\n          - port: 8080\n            internalListenerName: \"http\"\n          - port: 7890\n            internalListenerName: \"my-custom-udp-listener\"\n            protocol: UDP\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Cluster NodeConfigGroup for Prometheus Metrics Exposure in YAML\nDESCRIPTION: This YAML snippet is part of a NiFi cluster specification defining a custom node configuration group named 'auto_scaling'. It sets resource limits and requests for CPU and memory, assigns a service account, and configures multiple persistent volume claims (PVCs) for logs, data, extensions, and other NiFi directories. Importantly, it exposes Prometheus metrics by adding an internal listener on container port 9090. This configuration is essential to enable Prometheus-based monitoring and autoscaling of NiFi nodes. Assumes existing NiFi cluster resources management.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n...\nspec:\n  ...\n  listenersConfig:\n    internalListeners:\n    ...\n    - containerPort: 9090\n      name: prometheus\n      type: prometheus \n    ...\n  ...\n  nodeConfigGroups:\n    auto_scaling:\n      isNode: true\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 2Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n      serviceAccountName: external-dns\n      storageConfigs:\n        - mountPath: /opt/nifi/nifi-current/logs\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/data\n          name: data\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/extensions\n          name: extensions-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/flowfile_repository\n          name: flowfile-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/nifi-current/conf\n          name: conf\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/content_repository\n          name: content-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/provenance_repository\n          name: provenance-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n  ...\n```\n\n----------------------------------------\n\nTITLE: Defining Prometheus RBAC (YAML)\nDESCRIPTION: This YAML manifest defines the necessary Kubernetes Role-Based Access Control (RBAC) resources for the Prometheus instance to operate. It creates a `ServiceAccount`, a `ClusterRole` granting permissions to read various cluster resources (nodes, services, pods, ingresses, configmaps, and metrics endpoints), and a `ClusterRoleBinding` to associate the service account with the cluster role.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: prometheus\n  namespace: monitoring-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: prometheus\nrules:\n- apiGroups: [\"\"]\n  resources:\n  - nodes\n  - nodes/metrics\n  - services\n  - endpoints\n  - pods\n  verbs: [\"get\", \"list\", \"watch\"]\n- apiGroups: [\"\"]\n  resources:\n  - configmaps\n  verbs: [\"get\"]\n- apiGroups:\n  - networking.k8s.io\n  resources:\n  - ingresses\n  verbs: [\"get\", \"list\", \"watch\"]\n- nonResourceURLs: [\"/metrics\"]\n  verbs: [\"get\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: prometheus\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: prometheus\nsubjects:\n- kind: ServiceAccount\n  name: prometheus\n  namespace: monitoring-system\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Internal Listener Without Type for NiFi Processor Exposure - YAML\nDESCRIPTION: This YAML snippet demonstrates how to add a custom internal listener to the NiFi Kubernetes configuration without specifying a listener `type`. This is useful when exposing a NiFi processor with a custom port, for example, to handle HTTP requests directly within NiFi. The listener is defined with a `name` and a `containerPort`, but no `type`, indicating it is an additional port not related to NiFi's core internal listener types. This flexibility allows extending NiFi cluster functionality via Kubernetes ports.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    ...\n    - name: \"http-tracking\"\n      containerPort: 8081\n```\n\n----------------------------------------\n\nTITLE: Accessing Prometheus UI via Port Forwarding Console\nDESCRIPTION: This command sets up port forwarding from the local machine's port 9090 to the 'prometheus-operated' service's port 9090 within the 'monitoring-system' namespace. This allows accessing the Prometheus web UI from a local browser at `http://localhost:9090`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_12\n\nLANGUAGE: console\nCODE:\n```\nkubectl port-forward -n monitoring-system service/prometheus-operated 9090:9090\n```\n\n----------------------------------------\n\nTITLE: Adding KEDA Helm Repository - Helm Console Commands - console\nDESCRIPTION: This snippet adds the official KEDA Helm chart repository to your Helm environment using the console. Helm must be installed as a prerequisite. The command \"helm repo add kedacore https://kedacore.github.io/charts\" sets up the Helm repository for KEDA charts, enabling further actions like installation or updating charts. There are no input parameters; output is a confirmation from Helm.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nhelm repo add kedacore https://kedacore.github.io/charts\n```\n\n----------------------------------------\n\nTITLE: Configuring Persistent Storage Volumes for NiFi Nodes in NiFiKop YAML\nDESCRIPTION: This configuration snippet provides several StorageConfig entries under NodeConfigGroup to define persistent volume claims (PVCs) for data persistence in Kubernetes pods running NiFi. Each storage config specifies mountPath within the NiFi container, metadata labels and annotations, and pvcSpec details such as access modes, storage request sizes, and storageClassName. These volumes ensure critical NiFi directories (logs, data, config, repositories) persist data across pod restarts, which is essential for stateful workloads. Inputs include volume sizes and mount points; outputs are persistent volumes mounted inside pods. Pre-requisite is Kubernetes cluster with storage classes supporting PVCs.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n...\nstorageConfigs:\n  - mountPath: /opt/nifi/nifi-current/logs\n    name: logs\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/data\n    name: data\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 50Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/extensions\n    name: extensions-repository\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 5Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/flowfile_repository\n    name: flowfile-repository\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/nifi-current/conf\n    name: conf\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 5Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/content_repository\n    name: content-repository\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 500Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/provenance_repository\n    name: provenance-repository\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 500Gi\n```\n\n----------------------------------------\n\nTITLE: Applying NiFi Property Overrides in NiFi Cluster YAML\nDESCRIPTION: This YAML snippet demonstrates the three primary methods for overriding NiFi properties within the `NiFiCluster` custom resource: referencing a Kubernetes ConfigMap, referencing a Kubernetes Secret, and providing inline configuration. It shows how to specify the resource name, namespace, and data key, and provides example inline configurations.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n nifiProperties:\n      # Additionnal nifi.properties configuration that will override the one produced based on template and\n      # configuration\n      overrideConfigMap:\n        # The key of the value,in data content, that we want use.\n        data: nifi.properties\n        # Name of the configmap that we want to refer.\n        name: raw\n        # Namespace where is located the secret that we want to refer.\n        namespace: nifikop\n      # Additionnal nifi.properties configuration that will override the one produced based\n      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n      overrideSecretConfig:\n        # The key of the value,in data content, that we want use.\n        data: nifi.properties\n        # Name of the configmap that we want to refer.\n        name: raw\n        # Namespace where is located the secret that we want to refer.\n        namespace: nifikop\n      # Additionnal nifi.properties configuration that will override the one produced based\n      #\ton template, configurations and overrideConfigMap\n      overrideConfigs: |\n        nifi.ui.banner.text=NiFiKop\n        nifi.sensitive.props.key=thisIsABadSensitiveKeyPassword\n```\n\n----------------------------------------\n\nTITLE: Configuring Identity Mapping in NiFiKop\nDESCRIPTION: Identity mapping configuration recommended for supporting multiple identity providers in NiFi. These configurations should be added to nifi.properties to handle Distinguished Name (DN) mapping.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/5_users_authentication/1_oidc.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nnifi.security.identity.mapping.pattern.dn=CN=([^,]*)(?:, (?:O|OU)=.*)?\\nnifi.security.identity.mapping.value.dn=$1\\nnifi.security.identity.mapping.transform.dn=NONE\n```\n\n----------------------------------------\n\nTITLE: Deploying Prometheus Custom Resource for NiFi Cluster Monitoring - yaml\nDESCRIPTION: This YAML manifest creates a Prometheus custom resource named \"prometheus\" in the monitoring-system namespace to configure its operation. It defines settings such as disabling the admin API, setting evaluation and scrape intervals to 30 seconds, specifying pod and service monitor selectors to target the NiFi cluster metrics, and resource requests. It uses a dedicated ServiceAccount for permissions. This resource directs Prometheus to monitor NiFi cluster metrics specifically.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: Prometheus\nmetadata:\n  name: prometheus\n  namespace: monitoring-system\nspec:\n  enableAdminAPI: false\n  evaluationInterval: 30s\n  logLevel: debug\n  podMonitorSelector:\n    matchExpressions:\n    - key: release\n      operator: In\n      values:\n      - prometheus\n  resources:\n    requests:\n      memory: 400Mi\n  scrapeInterval: 30s\n  serviceAccountName: prometheus\n  serviceMonitorSelector:\n    matchExpressions:\n    - key: app\n      operator: In\n      values:\n      - nifi-cluster\n```\n\n----------------------------------------\n\nTITLE: Defining Managed Groups in NifiCluster Specification\nDESCRIPTION: Example YAML configuration showing how to define managed admin and reader users directly in the NifiCluster specification. The operator will automatically create and manage the corresponding NifiUsers and NifiUserGroups.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/2_manage_users_and_accesses/3_managed_groups.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  ...\n  oneNifiNodePerNode: false\n  propagateLabels: true\n  managedAdminUsers:\n    -  identity: \"alexandre.guitton@konpyutaika.com\"\n       name: \"aguitton\"\n    -  identity: \"nifiuser@konpyutaika.com\"\n       name: \"nifiuser\"\n  managedReaderUsers:\n    -  identity: \"toto@konpyutaika.com\"\n       name: \"toto\"\n    ...\n```\n\n----------------------------------------\n\nTITLE: Creating a NiFi User with SSL Credentials in Kubernetes\nDESCRIPTION: Command for creating a NiFi user resource with SSL credentials. This will generate client certificates signed by the cluster CA and store them in a Kubernetes secret.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_3\n\nLANGUAGE: console\nCODE:\n```\ncat << EOF | kubectl apply -n nifi -f -\napiVersion:  nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: example-client\n  namespace: nifi\nspec:\n  clusterRef:\n    name: nifi\n  secretName: example-client-secret\nEOF\n```\n\n----------------------------------------\n\nTITLE: Creating NifiDataflow YAML\nDESCRIPTION: This YAML defines a NifiDataflow resource, which specifies the configuration and deployment details for a NiFi dataflow managed by NiFiKop.  It references a parent process group, the bucket and flow IDs from the NiFi registry, the flow version, and the sync mode for managing the dataflow lifecycle.  It also includes references to a registry client and a parameter context.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: demo\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Applying OIDC and Identity Mapping Configs in NiFiKop (YAML)\nDESCRIPTION: Example YAML configuration for a NiFiKop `NifiCluster` custom resource, demonstrating how to use the `spec.readOnlyConfig.nifiProperties.overrideConfigs` field to apply both OpenId Connect (OIDC) and identity mapping configurations directly to the NiFi instance's `nifi.properties` file.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/3_tasks/2_security/2_authentication/1_oidc.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\n...\nspec:\n  ...\n  readOnlyConfig:\n    # NifiProperties configuration that will be applied to the node.\n    nifiProperties:\n      webProxyHosts:\n        - nifistandard2.trycatchlearn.fr:8443\n      # Additionnal nifi.properties configuration that will override the one produced based\n      # on template and configurations.\n      overrideConfigs: |\n        nifi.security.user.oidc.discovery.url=<oidc server discovery url>\n        nifi.security.user.oidc.client.id=<oidc client's id>\n        nifi.security.user.oidc.client.secret=<oidc client's secret>\n        nifi.security.identity.mapping.pattern.dn=CN=([^,]*)(?:, (?:O|OU)=.*)?\n        nifi.security.identity.mapping.value.dn=$1\n        nifi.security.identity.mapping.transform.dn=NONE\n      ...\n   ...\n...\n```\n\n----------------------------------------\n\nTITLE: Installing cert-manager with Helm 3 - Bash\nDESCRIPTION: This code snippet demonstrates installing cert-manager using Helm 3, including steps to apply CRDs, add the Jetstack Helm repository, and install cert-manager in the 'cert-manager' namespace. It requires a properly configured Helm 3 client, kubectl, and that the cert-manager namespace exists prior to installation. This method provides more flexible management of cert-manager deployments and easily supports upgrades.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/2_deploy_nifikop/1_quick_start.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply --validate=false -f \\\n   https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.crds.yaml\n\nhelm repo add jetstack https://charts.jetstack.io\nhelm repo update\n\nhelm install cert-manager \\\n    --namespace cert-manager \\\n    --version v1.7.2 jetstack/cert-manager\n```\n\n----------------------------------------\n\nTITLE: Configuring Persistent Storage for NiFi Nodes - YAML\nDESCRIPTION: This YAML snippet shows how to set up persistent storage for NiFi node directories using the storageConfigs field under NodeConfigGroup. Each entry defines the mount path within the container, volume name, label and annotation metadata, and a pvcSpec describing access mode, requested size, and storage class. Dependencies include access to a StorageClass (e.g., ssd-wait) in the Kubernetes cluster and appropriate quota. All storage-related customizations should match actual pod requirements and mount paths; misaligned volume mounts can result in lost data or failed pods.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n...\nstorageConfigs:\n  - mountPath: /opt/nifi/nifi-current/logs\n    name: logs\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/data\n    name: data\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 50Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/extensions\n    name: extensions-repository\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 5Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/flowfile_repository\n    name: flowfile-repository\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/nifi-current/conf\n    name: conf\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 5Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/content_repository\n    name: content-repository\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 500Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/provenance_repository\n    name: provenance-repository\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 500Gi\n\n```\n\n----------------------------------------\n\nTITLE: Declaring NiFi Cluster Nodes Using NodeConfigGroups - YAML\nDESCRIPTION: This YAML snippet shows how to declare individual NiFi cluster nodes and assign them to predefined node configuration groups or set custom configurations at node level. Nodes can be referenced by a group name for shared configuration, or specify an inline configuration using the nodeConfig field. Parameters include node ID and optionally a resourcesRequirements block. This approach facilitates both reusable and one-off configuration patterns within the same NiFiKop cluster. Requires prior definition of referenced nodeConfigGroups.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\n- id: 0\n  nodeConfigGroup: \"default_group\"\n- id: 2\n  nodeConfigGroup: \"high_mem_group\"\n- id: 3\n  nodeConfigGroup: \"high_mem_group\"\n- id: 5\n  nodeConfig:\n    resourcesRequirements:\n      limits:\n        cpu: \"2\"\n        memory: 3Gi\n      requests:\n        cpu: \"1\"\n        memory: 1Gi\n```\n\n----------------------------------------\n\nTITLE: Configuring Persistent Storage for NiFi Cluster Nodes - YAML\nDESCRIPTION: This YAML snippet configures persistent volumes for various NiFi directories to ensure data durability within a Kubernetes-managed NiFi cluster. Each entry in the storageConfigs array defines a mount path, a unique name, PVC specifications (including size, access mode, and storage class), and metadata for labeling and annotating the resulting k8s PersistentVolumeClaims. Key directories like logs, data, repositories, and configuration are included. Proper PVC and k8s storage class setup is required. Volumes are configured to be deleted when no longer used unless the reclaim policy is changed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\n...\nstorageConfigs:\n  - mountPath: /opt/nifi/nifi-current/logs\n    name: logs\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/data\n    name: data\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 50Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/extensions\n    name: extensions-repository\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 5Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/flowfile_repository\n    name: flowfile-repository\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/nifi-current/conf\n    name: conf\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 5Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/content_repository\n    name: content-repository\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 500Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/provenance_repository\n    name: provenance-repository\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 500Gi\n```\n\n----------------------------------------\n\nTITLE: Applying NiFi OIDC Configuration via NiFiKop NifiCluster (YAML)\nDESCRIPTION: Demonstrates how to configure OpenID Connect and identity mapping settings for Apache NiFi using the `readOnlyConfig.nifiProperties.overrideConfigs` field within the NiFiKop `NifiCluster` Kubernetes custom resource. This YAML snippet shows how to embed standard `nifi.properties` content directly, including the OIDC discovery URL, client ID, client secret, and the previously described identity mapping properties. This allows NiFiKop to deploy a NiFi cluster with these OIDC settings pre-applied.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/5_users_authentication/1_oidc.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  readOnlyConfig:\n    # NifiProperties configuration that will be applied to the node.\n    nifiProperties:\n      webProxyHosts:\n        - nifistandard2.trycatchlearn.fr:8443\n      # Additionnal nifi.properties configuration that will override the one produced based\n      # on template and configurations.\n      overrideConfigs: |\n        nifi.security.user.oidc.discovery.url=<oidc server discovery url>\n        nifi.security.user.oidc.client.id=<oidc client's id>\n        nifi.security.user.oidc.client.secret=<oidc client's secret>\n        nifi.security.identity.mapping.pattern.dn=CN=([^,]*)(?:, (?:O|OU)=.*)?\n        nifi.security.identity.mapping.value.dn=$1\n        nifi.security.identity.mapping.transform.dn=NONE\n      ...\n   ...\n...\n```\n\n----------------------------------------\n\nTITLE: Configuring NifiUserGroup Resource in YAML\nDESCRIPTION: Example YAML configuration for creating a NifiUserGroup resource that defines a group with two users and grants them read access to counters. The configuration includes references to NifiUsers and specifies access policy details.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/3_manage_nifi/2_manage_users_and_accesses/2_groups_management.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  # Contains the reference to the NifiCluster with the one the registry client is linked.\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  # contains the list of reference to NifiUsers that are part to the group.\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n#     namespace: nifikop\n    - name: nc-controller.nifikop.mgt.cluster.local\n  # defines the list of access policies that will be granted to the group.\n  accessPolicies:\n      # defines the kind of access policy, could be \"global\" or \"component\".\n    - type: global\n      # defines the kind of action that will be granted, could be \"read\" or \"write\"\n      action: read\n      # resource defines the kind of resource targeted by this access policies, please refer to the following page:\n      #\thttps://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#access-policies\n      resource: /counters\n#      # componentType is used if the type is \"component\", it's allow to define the kind of component on which is the\n#      # access policy\n#      componentType: \"process-groups\"\n#      # componentId is used if the type is \"component\", it's allow to define the id of the component on which is the\n#      # access policy\n#      componentId: \"\"\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFi Cluster\nDESCRIPTION: This bash script deploys a NiFi cluster to a Kubernetes cluster using `kubectl`. It assumes a configuration file named `simplenificluster.yaml` is available in the `config/samples/` directory, and that the user has configured the zookeeper service name to the configuration. It needs kubectl configured and connected to the cluster.  The command creates the resources defined in the specified YAML file in the `nifi` namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create -n nifi -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFiCluster to Use External Issuer for SSL Certificates\nDESCRIPTION: This YAML configuration shows how to link the NiFi cluster to an external issuer, such as Let's Encrypt, by setting `issuerRef` within `sslSecrets`. It ensures certificates are automatically issued and rotated, improving security and reducing manual certificate management.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  clusterSecure: true\n  siteToSiteSecure: true\n  ...\n  listenersConfig:\n    clusterDomain: <DNS zone name>\n    useExternalDNS: true\n    ...\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n      issuerRef:\n        name: letsencrypt-staging\n        kind: Issuer\n```\n\n----------------------------------------\n\nTITLE: YAML Example for Using an External Issuer with cert-manager\nDESCRIPTION: Illustrates how to reference an external certificate issuer (e.g., Let's Encrypt) within the NiFi cluster configuration by setting the issuerRef field. Dependencies include cert-manager and an existing Issuer resource. The key parameters are issuerRef.name and issuerRef.kind, which link to the external CA.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nsslSecrets:\n  tlsSecretName: \"test-nifikop\"\n  create: true\n  issuerRef:\n    name: letsencrypt-staging\n    kind: Issuer\n```\n\n----------------------------------------\n\nTITLE: Scaling Down: Removing a Node from NifiCluster Definition (YAML)\nDESCRIPTION: This YAML snippet demonstrates how to scale down a NiFi cluster by removing a node definition (id: 2, shown commented out) from the `spec.nodes` list in the `NifiCluster` custom resource. Applying this change triggers the graceful decommissioning process managed by NiFiKop.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  headlessServiceEnabled: true\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.11.3\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n# >>>> START: node removed\n#    - id: 2\n#      nodeConfigGroup: \"default_group\"\n# <<<< END\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Node Group Autoscaler Resource (YAML)\nDESCRIPTION: Defines a `NifiNodeGroupAutoscaler` custom resource for NiFiKop. It specifies the target NiFi cluster (`clusterRef`), the node group to scale (`nodeConfigGroupId: auto_scaling`), optional read-only configuration overrides, a label selector (`nodeLabelsSelector`) to identify the nodes managed by this autoscaler, and the strategies for adding (`upscaleStrategy: simple`) and removing (`downscaleStrategy: lifo`) nodes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiNodeGroupAutoscaler\nmetadata:\n  name: nifinodegroupautoscaler-sample\n  namespace: clusters\nspec:\n  # contains the reference to the NifiCluster with the one the node group autoscaler is linked.\n  clusterRef:\n    name: cluster\n    namespace: clusters\n  # defines the id of the NodeConfig contained in NifiCluster.Spec.NodeConfigGroups\n  nodeConfigGroupId: auto_scaling\n  # readOnlyConfig can be used to pass Nifi node config\n  # which has type read-only these config changes will trigger rolling upgrade\n  readOnlyConfig:\n    nifiProperties:\n      overrideConfigs: |\n        nifi.ui.banner.text=NiFiKop - Scale Group\n  # This is an example of a node config you can apply to each replica in this node group.\n  # Any settings here will override those in the configured nodeConfigGroupId\n#  nodeConfig:\n#    nodeSelector:\n#      node_type: high-mem\n  # The selector used to identify nodes in NifiCluster.Spec.Nodes this autoscaler will manage\n  # Use Node.Labels in combination with this selector to clearly define which nodes will be managed by this autoscaler \n  nodeLabelsSelector: \n    matchLabels:\n      nifi_cr: cluster\n      nifi_node_group: auto-scaling\n  # the strategy used to decide how to add nodes to a nifi cluster\n  upscaleStrategy: simple\n  # the strategy used to decide how to remove nodes from an existing cluster\n  downscaleStrategy: lifo\n```\n\n----------------------------------------\n\nTITLE: Setting up Self-Signed SSL Certificates via YAML\nDESCRIPTION: Specifies the SSL secret configuration for the NiFi operator, indicating whether to create new self-signed certificates or to reference an existing secret. If create is set to false, the secret must contain the CA certificate, CA key, client certificate, and client key.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/3_tasks/2_security/1_ssl.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\nsslSecrets:\n  tlsSecretName: \"test-nifikop\"\n  create: true\n```\n\n----------------------------------------\n\nTITLE: Defining Managed Users in NifiCluster YAML\nDESCRIPTION: This YAML snippet demonstrates how to configure managed users within a `NifiCluster` specification.  It defines the `managedAdminUsers` and `managedReaderUsers` fields, each containing a list of users. Each user is specified with an `identity` and a `name`.  The operator uses this information to create and manage `NifiUser` resources and assign them to corresponding groups. This simplifies the management of NiFi cluster users by automating user and group creation and membership.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/2_manage_users_and_accesses/3_managed_groups.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  ...\n  oneNifiNodePerNode: false\n  propagateLabels: true\n  managedAdminUsers:\n    -  identity: \"alexandre.guitton@konpyutaika.com\"\n       name: \"aguitton\"\n    -  identity: \"nifiuser@konpyutaika.com\"\n       name: \"nifiuser\"\n  managedReaderUsers:\n    -  identity: \"toto@konpyutaika.com\"\n       name: \"toto\"\n    ...\n```\n```\n\n----------------------------------------\n\nTITLE: Defining NiFiDataflow Custom Resource\nDESCRIPTION: This snippet presents the CRD for NiFiDataflow, which allows users to declare a dataflow based on a NiFiRegistryClient and optionally a ParameterContext. The controller manages deployment and lifecycle operations on the targeted NiFi cluster, integrating all components involved in the dataflow.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/3_manage_dataflows/0_design_principles.md#_snippet_2\n\nLANGUAGE: Markdown\nCODE:\n```\n- **NiFiDataflow:** Allowing you to declare a Dataflow based on a `NiFiRegistryClient` and optionally a `ParameterContext`, which will be deployed and managed by the operator on the `targeted NiFi cluster`.\n```\n\n----------------------------------------\n\nTITLE: Defining Managed Admin and Reader Users in NifiCluster Resource - YAML\nDESCRIPTION: This YAML snippet shows how to define users for the operator's predefined 'Admins' and 'Readers' managed groups within the NifiCluster custom resource specification. The operator will automatically create corresponding NifiUser resources and add them to the 'managed-admins' and 'managed-readers' groups, simplifying access control configuration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/3_manage_nifi/2_manage_users_and_accesses/3_managed_groups.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  ...\n  oneNifiNodePerNode: false\n  propagateLabels: true\n  managedAdminUsers:\n    -  identity: \"alexandre.guitton@konpyutaika.com\"\n       name: \"aguitton\"\n    -  identity: \"nifiuser@konpyutaika.com\"\n       name: \"nifiuser\"\n  managedReaderUsers:\n    -  identity: \"toto@konpyutaika.com\"\n       name: \"toto\"\n    ...\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Node Configuration - YAML\nDESCRIPTION: This YAML snippet defines the configuration for two NiFi nodes. It includes the node ID, read-only configuration for setting UI banner text, resource requirements (CPU and memory limits/requests), and storage configurations using PersistentVolumeClaims (PVCs). The `readOnlyConfig` will trigger rolling upgrades when changed. Node 0 utilizes `nodeConfigGroup` while node 2 specifies resources and storage.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/5_references/1_nifi_cluster/4_node.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n    - id: 0\n      # nodeConfigGroup can be used to ease the node configuration, if set only the id is required\n      nodeConfigGroup: \"default_group\"\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        nifiProperties:\n          overrideConfigs: |\n            nifi.ui.banner.text=NiFiKop - Node 0\n      # node configuration\n#       nodeConfig:\n    - id: 2\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        overrideConfigs: |\n          nifi.ui.banner.text=NiFiKop - Node 2\n      # node configuration\n      nodeConfig:\n        resourcesRequirements:\n          limits:\n            cpu: \"2\"\n            memory: 3Gi\n          requests:\n            cpu: \"1\"\n            memory: 1Gi\n        storageConfigs:\n          # Name of the storage config, used to name PV to reuse into sidecars for example.\n          - name: provenance-repository\n            # Path where the volume will be mount into the main nifi container inside the pod.\n            mountPath: \"/opt/nifi/provenance_repository\"\n            # Kubernetes PVC spec\n            # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n            pvcSpec:\n              accessModes:\n                - ReadWriteOnce\n              storageClassName: \"standard\"\n              resources:\n                requests:\n                  storage: 8Gi\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFi Cluster in Kubernetes\nDESCRIPTION: Command to create a simple NiFi cluster by applying a sample YAML configuration file. This assumes Zookeeper is already running.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Add your zookeeper svc name to the configuration\nkubectl create -n nifi -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring Cluster-Wide ReadOnlyConfig in NiFiKop with YAML\nDESCRIPTION: This YAML snippet demonstrates how to define ReadOnlyConfig in NiFiKop by specifying properties for cluster-wide read-only configuration, including CPU thread allocation, logback logging settings, authorizer templates, NiFi and Zookeeper settings, and bootstrap options. Dependencies include the NiFiKop operator deployed in Kubernetes and pre-existing ConfigMaps/Secrets for referenced configuration files. Key parameters include thread counts, config map/secret references (with required names, data keys, and namespaces), property overrides, JVM memory sizing, and NiFi/ Zookeeper/Bootstrap property overrides. Inputs must match NiFiKop's schema; output is a cluster read-only configuration applied either globally or per-node. Limitations: proper naming and namespace referencing for K8s objects is essential, deprecated properties are noted, and security-specific options (e.g., client auth) must be correctly set.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/5_references/1_nifi_cluster/2_read_only_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreadOnlyConfig:\n  # MaximumTimerDrivenThreadCount define the maximum number of threads for timer driven processors available to the system.\n  maximumTimerDrivenThreadCount: 30\n  # MaximumEventDrivenThreadCount define the maximum number of threads for event driven processors available to the system (@DEPRECATED. This has no effect from NiFiKOp v1.9.0 or later).\n  maximumEventDrivenThreadCount: 10\n  # Logback configuration that will be applied to the node\n  logbackConfig:\n    # logback.xml configuration that will replace the one produced based on template\n    replaceConfigMap:\n      # The key of the value,in data content, that we want use.\n      data: logback.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # logback.xml configuration that will replace the one produced based on template and overrideConfigMap\n    replaceSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: logback.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n  # Authorizer configuration that will be applied to the node\n  authorizerConfig:\n    # An authorizers.xml configuration template that will replace the default template seen in authorizers.go\n    replaceTemplateConfigMap:\n      # The key of the value, in data content, that we want use.\n      data: authorizers.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # An authorizers.xml configuration template that will replace the default template seen in authorizers.go and the replaceTemplateConfigMap\n    replaceTemplateSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: authorizers.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n  # NifiProperties configuration that will be applied to the node.\n  nifiProperties:\n    # Additionnal nifi.properties configuration that will override the one produced based on template and\n    # configuration\n    overrideConfigMap:\n      # The key of the value,in data content, that we want use.\n      data: nifi.properties\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop.\n    # Additionnal nifi.properties configuration that will override the one produced based\n    #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    overrideSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: nifi.properties\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # Additionnal nifi.properties configuration that will override the one produced based\n    #\ton template, configurations and overrideConfigMap\n    overrideConfigs: |\n      nifi.ui.banner.text=NiFiKop\n    # A comma separated list of allowed HTTP Host header values to consider when NiFi\n    # is running securely and will be receiving requests to a different host[:port] than it is bound to.\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#web-properties\n    #      webProxyHosts:\n    # Nifi security client auth\n    needClientAuth: false\n    # Indicates which of the configured authorizers in the authorizers.xml file to use\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#authorizer-configuration\n  #      authorizer:\n  # ZookeeperProperties configuration that will be applied to the node.\n  zookeeperProperties:\n    #      # Additionnal zookeeeper.properties configuration that will override the one produced based on template and\n    #      # configuration\n    #      overrideConfigMap:\n    #        # The key of the value,in data content, that we want use.\n    #        data: zookeeeper.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop.\n    #      # Additionnal zookeeeper.properties configuration that will override the one produced based\n    #      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    #      overrideSecretConfig:\n    #        # The key of the value,in data content, that we want use.\n    #        data: zookeeeper.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop\n    # Additionnal zookeeper.properties configuration that will override the one produced based\n    # on template and configurations.\n    overrideConfigs: |\n      initLimit=15\n      autopurge.purgeInterval=24\n      syncLimit=5\n      tickTime=2000\n      dataDir=./state/zookeeper\n      autopurge.snapRetainCount=30\n  # BootstrapProperties configuration that will be applied to the node.\n  bootstrapProperties:\n    #      # Additionnal bootstrap.conf configuration that will override the one produced based on template and\n    #      # configuration\n    #      overrideConfigMap:\n    #        # The key of the value,in data content, that we want use.\n    #        data: bootstrap.conf\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop.\n    #      # Additionnal bootstrap.conf configuration that will override the one produced based\n    #      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    #      overrideSecretConfig:\n    #        # The key of the value,in data content, that we want use.\n    #        data: bootstrap.conf\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop\n    # JVM memory settings\n    nifiJvmMemory: \"512m\"\n    # Additionnal bootstrap.conf configuration that will override the one produced based\n    # on template and configurations.\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#bootstrap_properties\n    overrideConfigs: |\n      # java.arg.4=-Djava.net.preferIPv4Stack=true\n```\n\n----------------------------------------\n\nTITLE: Deploying KEDA ScaledObject for NiFi Autoscaling - yaml\nDESCRIPTION: Defines a KEDA ScaledObject for autoscaling NiFi nodes based on Prometheus metrics in the 'clusters' namespace. Requires the KEDA operator, NiFi CRDs, and a configured Prometheus instance. Key functionality includes pointing at the NiFi autoscaler resource, specifying polling and cooldown intervals, replica counts, fallback policy, and a Prometheus metric as the scaling trigger. Inputs are scaling configuration fields; outputs are dynamic scaling (HPA) behavior for NiFi workloads.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: cluster\n  namespace: clusters\nspec:\n  scaleTargetRef:\n    apiVersion:    nifi.konpyutaika.com/v1alpha1     # Optional. Default: apps/v1\n    kind:          NifiNodeGroupAutoscaler           # Optional. Default: Deployment\n    name:          nifinodegroupautoscaler-sample    # Mandatory. Must be in the same namespace as the ScaledObject\n    envSourceContainerName: nifi                     # Optional. Default: .spec.template.spec.containers[0]\n  pollingInterval:  30                               # Optional. Default: 30 seconds\n  cooldownPeriod:   300                              # Optional. Default: 300 seconds\n  idleReplicaCount: 0                                # Optional. Default: ignored, must be less than minReplicaCount \n  minReplicaCount:  1                                # Optional. Default: 0\n  maxReplicaCount:  3                                # Optional. Default: 100\n  fallback:                                          # Optional. Section to specify fallback options\n    failureThreshold: 5                              # Mandatory if fallback section is included\n    replicas: 1                                      # Mandatory if fallback section is included\n  # advanced:                                          # Optional. Section to specify advanced options\n  #   restoreToOriginalReplicaCount: true              # Optional. Whether the target resource should be scaled back to original replicas count, after the ScaledObject is deleted\n  #   horizontalPodAutoscalerConfig:                   # Optional. Section to specify HPA related options\n  #     name: {name-of-hpa-resource}                   # Optional. Default: keda-hpa-{scaled-object-name}\n  #     behavior:                                      # Optional. Use to modify HPA's scaling behavior\n  #       scaleDown:\n  #         stabilizationWindowSeconds: 300 <--- \n  #         policies:\n  #         - type: Percent\n  #           value: 100\n  #           periodSeconds: 15\n  triggers:\n    - type: prometheus\n      metadata:\n        serverAddress: http://prometheus-operated.monitoring-system.svc:9090\n        metricName: http_requests_total\n        threshold: <threshold value>\n        query: <prometheus query>\n\n```\n\n----------------------------------------\n\nTITLE: Creating a Let's Encrypt Issuer for NiFi SSL Certificates Using cert-manager in YAML\nDESCRIPTION: This YAML snippet defines a cert-manager Issuer resource configured to obtain certificates from Let's Encrypt's staging environment using ACME protocol. It requires specifying an email for communication, a private key secret for ACME account key storage, and HTTP-01 challenge solver integrated with Nginx ingress. Prerequisites include cert-manager installed in the Kubernetes cluster and an ingress controller capable of handling HTTP01 challenges. The snippet enables automated certificate issuance for use by the NiFi operator.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: cert-manager.io/v1alpha2\nkind: Issuer\nmetadata:\n  name: letsencrypt-staging\nspec:\n  acme:\n    # You must replace this email address with your own.\n    # Let's Encrypt will use this to contact you about expiring\n    # certificates, and issues related to your account.\n    email: <your email address>\n    server: https://acme-staging-v02.api.letsencrypt.org/directory\n    privateKeySecretRef:\n      # Secret resource used to store the account's private key.\n      name: example-issuer-account-key\n    # Add a single challenge solver, HTTP01 using nginx\n    solvers:\n      - http01:\n          ingress:\n            ingressTemplate:\n              metadata:\n                annotations:\n                  \"external-dns.alpha.kubernetes.io/ttl\": \"5\"\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiParameterContext CRD with NiFiKop in YAML\nDESCRIPTION: Defines a NifiParameterContext custom resource for managing parameter contexts in NiFi via NiFiKop. Parameters can reference Kubernetes secrets for sensitive values, enabling secure storage and injection of those parameters into NiFi flows. The resource references a cluster, a list of parameters (with names, values, and descriptions), and any required secrets. All referenced names and namespaces must pre-exist. Used in combination with other resources, this assists in modular, secure flow configuration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\n  namespace: demo\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      value: toto\n      description: toto\n```\n\n----------------------------------------\n\nTITLE: Example NiFi Node Configuration YAML\nDESCRIPTION: This YAML snippet provides an example configuration for a default group within a NiFiKop `NodeConfig`. It demonstrates setting parameters like provenance storage size, user ID, cluster node status, pod metadata, image pull policy, priority class name, and defining external and persistent storage volumes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/5_references/1_nifi_cluster/3_node_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n   default_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      # Set this to true if the instance is a node in a cluster.\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#basic-cluster-setup\n      isNode: true\n      # Additionnal metadata to merge to the pod associated\n      podMetadata:\n        annotations:\n          node-annotation: \"node-annotation-value\"\n        labels:\n          node-label: \"node-label-value\"\n      # Docker image used by the operator to create the node associated\n      # https://hub.docker.com/r/apache/nifi/\n#      image: \"apache/nifi:1.11.2\"\n      # nodeAffinity can be specified, operator populates this value if new pvc added later to node\n      # https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity\n#      nodeAffinity:\n      # imagePullPolicy define the pull policy for NiFi cluster docker image\n      imagePullPolicy: IfNotPresent\n      # priorityClassName define the name of the priority class to be applied to these nodes\n      priorityClassName: \"example-priority-class-name\"\n      # externalVolumeConfigs specifies a list of volume to mount into the main container.\n      externalVolumeConfigs:\n        - name: example-volume\n          mountPath: \"/opt/nifi/example\"\n          secret:\n            secretName: \"raw-controller\"\n      # storageConfigs specifies the node related configs\n      storageConfigs:\n        # Name of the storage config, used to name PV to reuse into sidecars for example.\n        - name: provenance-repository\n          # Retain this PVC throughout NifiCluster deletions.\n          reclaimPolicy: Retain\n          # Path where the volume will be mount into the main nifi container inside the pod.\n          mountPath: \"/opt/nifi/provenance_repository\"\n          # Metadata to attach to the PVC that gets created\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          # Kubernetes PVC spec\n          # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          reclaimPolicy: Delete\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n```\n\n----------------------------------------\n\nTITLE: Installing Nifikop Helm Chart with Specific Parameters\nDESCRIPTION: Shows how to install the Nifikop Helm chart while overriding default configuration values using the `--set` flag. This example specifically configures the `namespaces` parameter to `{\"nifikop\"}`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop --set namespaces={\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Kubernetes Pod Ports Declaration for NiFi Listeners in YAML\nDESCRIPTION: Specifies the ports section of a deployed NiFi pod manifest, matching internal listeners and assigning each a protocol, name, and container port. This snippet must be included in the pod spec generated by the NiFi operator or deployment mechanism. Each port must correspond to a configured internal listener and use TCP as the protocol; without correct mapping, service routing may fail.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n    ports:\n    - containerPort: 8443\n      name: https\n      protocol: TCP\n    - containerPort: 6007\n      name: cluster\n      protocol: TCP\n    - containerPort: 6342\n      name: load-balance\n      protocol: TCP\n    - containerPort: 10000\n      name: s2s\n      protocol: TCP\n    - containerPort: 9090\n      name: prometheus\n      protocol: TCP\n```\n\n----------------------------------------\n\nTITLE: Implementing OIDC Configuration in NiFiCluster Resource\nDESCRIPTION: Example YAML configuration for a NiFiCluster resource that includes OpenID Connect settings. This demonstrates how to use the overrideConfigs field to specify OIDC server details and identity mapping.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/5_users_authentication/1_oidc.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\\nkind: NifiCluster\\n...\\nspec:\\n  ...\\n  readOnlyConfig:\\n    # NifiProperties configuration that will be applied to the node.\\n    nifiProperties:\\n      webProxyHosts:\\n        - nifistandard2.trycatchlearn.fr:8443\\n      # Additionnal nifi.properties configuration that will override the one produced based\\n      # on template and configurations.\\n      overrideConfigs: |\\n        nifi.security.user.oidc.discovery.url=<oidc server discovery url>\\n        nifi.security.user.oidc.client.id=<oidc client's id>\\n        nifi.security.user.oidc.client.secret=<oidc client's secret>\\n        nifi.security.identity.mapping.pattern.dn=CN=([^,]*)(?:, (?:O|OU)=.*)?\\n        nifi.security.identity.mapping.value.dn=$1\\n        nifi.security.identity.mapping.transform.dn=NONE\\n      ...\\n   ...\\n...\n```\n\n----------------------------------------\n\nTITLE: Configuring ServiceAccount, ClusterRole, and ClusterRoleBinding for Prometheus - YAML\nDESCRIPTION: Defines required Kubernetes RBAC resources: a ServiceAccount (for Prometheus in the monitoring-system namespace), a ClusterRole granting read access to nodes, pods, endpoints, configmaps, and ingresses, and a ClusterRoleBinding connecting the ServiceAccount to the ClusterRole. These permissions enable Prometheus to discover and scrape targets as needed. YAML resource manifests, applied with kubectl or a GitOps tool, are required.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: prometheus\n  namespace: monitoring-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: prometheus\nrules:\n- apiGroups: [\"\"]\n  resources:\n  - nodes\n  - nodes/metrics\n  - services\n  - endpoints\n  - pods\n  verbs: [\"get\", \"list\", \"watch\"]\n- apiGroups: [\"\"]\n  resources:\n  - configmaps\n  verbs: [\"get\"]\n- apiGroups:\n  - networking.k8s.io\n  resources:\n  - ingresses\n  verbs: [\"get\", \"list\", \"watch\"]\n- nonResourceURLs: [\"/metrics\"]\n  verbs: [\"get\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: prometheus\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: prometheus\nsubjects:\n- kind: ServiceAccount\n  name: prometheus\n  namespace: monitoring-system\n\n```\n\n----------------------------------------\n\nTITLE: Defining a KEDA ScaledObject for NiFi Node Auto-Scaling (YAML)\nDESCRIPTION: This YAML configuration defines a KEDA ScaledObject to automatically scale a NiFi node group based on Prometheus metrics. It specifies the target resource, scaling parameters including minimum and maximum replicas, fallback options, and triggers for measuring the workload, primarily using Prometheus server queries.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_6\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: cluster\n  namespace: clusters\nspec:\n  scaleTargetRef:\n    apiVersion: nifi.konpyutaika.com/v1alpha1\n    kind: NifiNodeGroupAutoscaler\n    name: nifinodegroupautoscaler-sample\n    envSourceContainerName: nifi\n  pollingInterval: 30\n  cooldownPeriod: 300\n  idleReplicaCount: 0\n  minReplicaCount: 1\n  maxReplicaCount: 3\n  fallback:\n    failureThreshold: 5\n    replicas: 1\n  triggers:\n    - type: prometheus\n      metadata:\n        serverAddress: http://prometheus-operated.monitoring-system.svc:9090\n        metricName: http_requests_total\n        threshold: <threshold value>\n        query: <prometheus query>\n```\n\n----------------------------------------\n\nTITLE: Deploying a NiFi Dataflow (Input)\nDESCRIPTION: This YAML configuration defines a NifiDataflow named 'input' within the 'nifikop' namespace. It specifies the cluster reference, bucket ID, flow ID, flow version, registry client reference, and other settings such as skipping invalid components, sync mode, and update strategy. It also sets the flow position on the canvas.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/4_manage_connections/1_deploy_connection.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiDataflow\nmetadata:\n  name: input\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  bucketId: deedb9f6-65a4-44e9-a1c9-722008fcd0ba\n  flowId: ab95431d-980d-41bd-904a-fac4bd864ba6\n  flowVersion: 1\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  skipInvalidComponent: true\n  skipInvalidControllerService: true\n  syncMode: always\n  updateStrategy: drain\n  flowPosition:\n    posX: 0\n    posY: 0\n```\n\n----------------------------------------\n\nTITLE: Install Nifikop Chart - Set Parameters\nDESCRIPTION: This command installs the Nifikop chart, setting the namespaces parameter. The `--set` flag allows for setting custom values for chart parameters.  It overrides the default values defined in the chart's values.yaml file. Dependencies: Helm, Kubernetes cluster, and the Nifikop chart repository.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop --set namespaces={\\\"nifikop\\\"}\n```\n\n----------------------------------------\n\nTITLE: Sample YAML Configuration for NifiNodeGroupAutoscaler\nDESCRIPTION: Provides an example YAML manifest for creating a NifiNodeGroupAutoscaler resource, defining cluster reference, label selectors, and scaling strategies. It illustrates how to associate this autoscaler with a specific NiFi cluster and node group, along with scaling behaviors.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/5_references/7_nifi_nodegroup_autoscaler.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiNodeGroupAutoscaler\nmetadata:\n  name: nifinodegroupautoscaler-sample\nspec:\n  clusterRef:\n    name: nificluster-name\n    namespace: nifikop\n  nodeConfigGroupId: default-node-group\n  nodeLabelsSelector:\n    matchLabels:\n      nifi_cr: nificluster-name\n      nifi_node_group: default-node-group\n  upscaleStrategy: simple\n  downscaleStrategy: lifo\n```\n\n----------------------------------------\n\nTITLE: Defining Cluster-Wide ReadOnlyConfig in Nifikop YAML\nDESCRIPTION: This YAML snippet demonstrates the structure for defining cluster-wide read-only NiFi configurations using the `readOnlyConfig` block within a Nifikop Custom Resource. It includes examples for setting thread pools, referencing external ConfigMaps/Secrets for configuration files like logback and authorizers, and providing inline property overrides for `nifi.properties`, `zookeeper.properties`, and `bootstrap.conf`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/5_references/1_nifi_cluster/2_read_only_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreadOnlyConfig:\n  # MaximumTimerDrivenThreadCount define the maximum number of threads for timer driven processors available to the system.\n  maximumTimerDrivenThreadCount: 30\n  # MaximumEventDrivenThreadCount define the maximum number of threads for event driven processors available to the system.\n  maximumEventDrivenThreadCount: 10\n  # Logback configuration that will be applied to the node\n  logbackConfig:\n    # logback.xml configuration that will replace the one produced based on template\n    replaceConfigMap:\n      # The key of the value,in data content, that we want use.\n      data: logback.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # logback.xml configuration that will replace the one produced based on template and overrideConfigMap\n    replaceSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: logback.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n  # Authorizer configuration that will be applied to the node\n  authorizerConfig:\n    # An authorizers.xml configuration template that will replace the default template seen in authorizers.go\n    replaceTemplateConfigMap:\n      # The key of the value, in data content, that we want use.\n      data: authorizers.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # An authorizers.xml configuration template that will replace the default template seen in authorizers.go and the replaceTemplateConfigMap\n    replaceTemplateSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: authorizers.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n  # NifiProperties configuration that will be applied to the node.\n  nifiProperties:\n    # Additionnal nifi.properties configuration that will override the one produced based on template and\n    # configuration\n    overrideConfigMap:\n      # The key of the value,in data content, that we want use.\n      data: nifi.properties\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop.\n    # Additionnal nifi.properties configuration that will override the one produced based\n    #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    overrideSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: nifi.properties\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # Additionnal nifi.properties configuration that will override the one produced based\n    #\ton template, configurations and overrideConfigMap\n    overrideConfigs: |\n      nifi.ui.banner.text=NiFiKop\n    # A comma separated list of allowed HTTP Host header values to consider when NiFi\n    # is running securely and will be receiving requests to a different host[:port] than it is bound to.\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#web-properties\n    #      webProxyHosts:\n    # Nifi security client auth\n    needClientAuth: false\n    # Indicates which of the configured authorizers in the authorizers.xml file to use\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#authorizer-configuration\n  #      authorizer:\n  # ZookeeperProperties configuration that will be applied to the node.\n  zookeeperProperties:\n    #      # Additionnal zookeeeper.properties configuration that will override the one produced based on template and\n    #      # configuration\n    #      overrideConfigMap:\n    #        # The key of the value,in data content, that we want use.\n    #        data: zookeeeper.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop.\n    #      # Additionnal zookeeeper.properties configuration that will override the one produced based\n    #      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    #      overrideSecretConfig:\n    #        # The key of the value,in data content, that we want use.\n    #        data: zookeeeper.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop\n    # Additionnal zookeeper.properties configuration that will override the one produced based\n    # on template and configurations.\n    overrideConfigs: |\n      initLimit=15\n      autopurge.purgeInterval=24\n      syncLimit=5\n      tickTime=2000\n      dataDir=./state/zookeeper\n      autopurge.snapRetainCount=30\n  # BootstrapProperties configuration that will be applied to the node.\n  bootstrapProperties:\n    #      # Additionnal bootstrap.properties configuration that will override the one produced based on template and\n    #      # configuration\n    #      overrideConfigMap:\n    #        # The key of the value,in data content, that we want use.\n    #        data: bootstrap.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop.\n    #      # Additionnal bootstrap.properties configuration that will override the one produced based\n    #      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    #      overrideSecretConfig:\n    #        # The key of the value,in data content, that we want use.\n    #        data: bootstrap.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop\n    # JVM memory settings\n    nifiJvmMemory: \"512m\"\n    # Additionnal bootstrap.properties configuration that will override the one produced based\n    # on template and configurations.\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#bootstrap_properties\n    overrideConfigs: |\n      # java.arg.4=-Djava.net.preferIPv4Stack=true\n```\n\n----------------------------------------\n\nTITLE: Defining NiFiUser Custom Resource in YAML\nDESCRIPTION: YAML snippet illustrating the definition of a NiFiUser Kubernetes custom resource, specifying properties such as user identity, cluster reference, and certificate creation toggle. It serves as a template for creating NiFiUser resources in the nifikop operator environment. The example includes apiVersion, kind, metadata with resource name, and spec containing identity, cluster reference details (name and namespace), and a boolean flag to determine whether to create a certificate for the user.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/5_references/2_nifi_user.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: aguitton\nspec:\n  identity: alexandre.guitton@konpyutaika.com\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  createCert: false\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Listeners Configuration - YAML\nDESCRIPTION: This YAML snippet defines the configuration for NiFi listeners using the ListenersConfig structure. It specifies the internal listeners (HTTPS, Cluster, S2S, Prometheus, Load Balance) with their respective types, names, and container ports. It also configures SSL secrets with a TLS secret name and indicates that the cert-manager should create the necessary certificates.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/5_references/1_nifi_cluster/6_listeners_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    - type: \"https\"\n      name: \"https\"\n      containerPort: 8443\n    - type: \"cluster\"\n      name: \"cluster\"\n      containerPort: 6007\n    - type: \"s2s\"\n      name: \"s2s\"\n      containerPort: 10000\n    - type: \"prometheus\"\n      name: \"prometheus\"\n      containerPort: 9090\n    - type: \"load-balance\"\n      name: \"load-balance\"\n      containerPort: 6342\n  sslSecrets:\n    tlsSecretName: \"test-nifikop\"\n    create: true\n```\n\n----------------------------------------\n\nTITLE: Applying Custom Resource Definitions (CRDs)\nDESCRIPTION: Deploys the CRD YAML files into Kubernetes, establishing the custom resource schemas for NiFiKop components such as clusters, data flows, parameter contexts, registry clients, user groups, and users.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/6_contributing/1_developer_guide.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\n```\n\n----------------------------------------\n\nTITLE: Applying NiFiKop CRDs using Bash and kubectl\nDESCRIPTION: Uses `kubectl apply` to install the Custom Resource Definitions (CRDs) required by NiFiKop into the connected Kubernetes cluster. This defines the custom resources like `NifiCluster`, `NifiDataflow`, `NifiParameterContext`, `NifiRegistryClient`, `NifiUserGroup`, and `NifiUser` that the operator manages. Requires kubectl v1.16+ and access to a Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/6_contributing/1_developer_guide.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\n```\n\n----------------------------------------\n\nTITLE: Applying NiFiKop CRDs Manually\nDESCRIPTION: Applies all required NiFiKop CustomResourceDefinitions (CRDs) directly to the Kubernetes cluster using kubectl. This step is necessary if deploying the NiFiKop Helm chart with the '--skip-crds' flag.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/2_deploy_nifikop/1_quick_start.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifinodegroupautoscalers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificonnections.yaml\n```\n\n----------------------------------------\n\nTITLE: Manually Applying NiFiKop Operator Custom Resource Definitions with kubectl in Bash\nDESCRIPTION: This snippet provides commands for manually applying required Custom Resource Definitions (CRDs) for NiFiKop operator using kubectl. This is necessary if the Helm install command is run with --skip-crds to avoid automatic CRD installation. Each CRD relevant to various NiFiKop custom resources is applied from the official GitHub repository.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifinodegroupautoscalers.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining Istio Gateway for HTTPS\nDESCRIPTION: This YAML snippet defines an Istio Gateway that accepts HTTPS traffic on port 443, using TLS configuration with a specified credential name. The traffic is decrypted at the Gateway.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: nifi-gateway\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 443\n      name: https\n      protocol: HTTPS\n    tls:\n      mode: SIMPLE\n      credentialName: my-secret\n    hosts:\n    - nifi.my-domain.com\n```\n\n----------------------------------------\n\nTITLE: Configuring Persistent Storage for NiFi Nodes (YAML)\nDESCRIPTION: This YAML example details how to provide persistent storage for various directories in NiFi pods by defining the storageConfigs array. Each entry specifies a mountPath, a unique name, and a corresponding Kubernetes PVC (pvcSpec) with access modes, storage class, and size. This ensures that important data (logs, repositories, configs) survives pod restarts or failures. Prerequisites include existing storage classes (e.g., ssd-wait) and adequate storage quota. The snippet should be placed under the node or cluster spec where storageConfigs are accepted.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n... \nstorageConfigs:\n  - mountPath: /opt/nifi/nifi-current/logs\n    name: logs\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/data\n    name: data\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 50Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/extensions\n    name: extensions-repository\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 5Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/flowfile_repository\n    name: flowfile-repository\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/nifi-current/conf\n    name: conf\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 5Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/content_repository\n    name: content-repository\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 500Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/provenance_repository\n    name: provenance-repository\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 500Gi\n```\n\n----------------------------------------\n\nTITLE: Templated NiFi Authorizers XML Configuration\nDESCRIPTION: This YAML template demonstrates how to replace the default `authorizers.xml` configuration with a custom configuration using NiFiKOp. It configures both file-based and database-based user group and access policy providers, as well as the corresponding authorizers. It leverages templating to dynamically configure node identities.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/6_users_authorization/1_custom_user_authorizer.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n{{- $nodeList := .NodeList }}\n{{- $clusterName := .ClusterName }}\n{{- $namespace := .Namespace }}<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>\n<authorizers>\n    <userGroupProvider>\n        <identifier>file-user-group-provider</identifier>\n        <class>org.apache.nifi.authorization.FileUserGroupProvider</class>\n        <property name=\"Users File\">../data/users.xml</property>\n        <property name=\"Legacy Authorized Users File\"></property>\n        <property name=\"Initial User Identity admin\">{{ .ControllerUser }}</property>\n{{- range $i, $host := .NodeList }}\n        <property name=\"Initial User Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n    </userGroupProvider>\n    <userGroupProvider>\n        <identifier>database-user-group-provider</identifier>\n        <class>my.custom.DatabaseUserGroupProvider</class>\n        <!-- Any extra configuration for this provider goes here -->\n{{- range $i, $host := .NodeList }}\n        <property name=\"Initial User Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n    </userGroupProvider>\n    <accessPolicyProvider>\n        <identifier>file-access-policy-provider</identifier>\n        <class>org.apache.nifi.authorization.FileAccessPolicyProvider</class>\n        <property name=\"User Group Provider\">file-user-group-provider</property>\n        <property name=\"Authorizations File\">../data/authorizations.xml</property>\n        <property name=\"Initial Admin Identity\">{{ .ControllerUser }}</property>\n        <property name=\"Legacy Authorized Users File\"></property>\n{{- range $i, $host := .NodeList }}\n        <property name=\"Node Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n\t\t<property name=\"Node Group\"></property>\n    </accessPolicyProvider>\n    <accessPolicyProvider>\n        <identifier>database-access-policy-provider</identifier>\n        <class>my.custom.DatabaseAccessPolicyProvider</class>\n        <!-- Any extra configuration for this provider goes here -->\n{{- range $i, $host := .NodeList }}\n        <property name=\"Node Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n\t\t<property name=\"Node Group\"></property>\n    </accessPolicyProvider>\n    <authorizer>\n        <identifier>managed-authorizer</identifier>\n        <class>org.apache.nifi.authorization.StandardManagedAuthorizer</class>\n        <property name=\"Access Policy Provider\">file-access-policy-provider</property>\n    </authorizer>\n    <authorizer>\n        <identifier>custom-database-authorizer</identifier>\n        <class>my.custom.DatabaseAuthorizer</class>\n        <property name=\"Access Policy Provider\">database-access-policy-provider</property>\n    </authorizer>\n</authorizers>\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Listener for NiFi Processor in YAML\nDESCRIPTION: Example of adding a custom internal listener without a specific type, useful for exposing NiFi processors that need to receive incoming connections, such as HTTP requests.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    ...\n    - name: \"http-tracking\"\n      containerPort: 8081\n```\n\n----------------------------------------\n\nTITLE: Defining Cluster-wide Read-Only NiFi Configuration with NiFiKop YAML\nDESCRIPTION: This YAML snippet provides an example of a cluster-wide ReadOnlyConfig configuration for NiFiKop. It demonstrates how to set global values for max thread counts, logging, security (authorizer), NiFi property overrides, Zookeeper, and bootstrap settings. Depending on configuration options, values may reference Kubernetes ConfigMaps, Secrets, or be embedded directly as override strings. Required dependencies include a running Kubernetes cluster, deployed ConfigMaps/Secrets as referenced, and the NiFiKop operator. Key parameters include maximum thread counts, JVM memory, and configuration override settings, with expected input as correctly structured YAML. Output is effective cluster-wide NiFi configuration, and certain fields or overrides may be deprecated or version-specific (such as maximumEventDrivenThreadCount).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/5_references/1_nifi_cluster/2_read_only_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreadOnlyConfig:\n  # MaximumTimerDrivenThreadCount define the maximum number of threads for timer driven processors available to the system.\n  maximumTimerDrivenThreadCount: 30\n  # MaximumEventDrivenThreadCount define the maximum number of threads for event driven processors available to the system (@DEPRECATED. This has no effect from NiFiKOp v1.9.0 or later).\n  maximumEventDrivenThreadCount: 10\n  # Logback configuration that will be applied to the node\n  logbackConfig:\n    # logback.xml configuration that will replace the one produced based on template\n    replaceConfigMap:\n      # The key of the value,in data content, that we want use.\n      data: logback.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # logback.xml configuration that will replace the one produced based on template and overrideConfigMap\n    replaceSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: logback.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n  # Authorizer configuration that will be applied to the node\n  authorizerConfig:\n    # An authorizers.xml configuration template that will replace the default template seen in authorizers.go\n    replaceTemplateConfigMap:\n      # The key of the value, in data content, that we want use.\n      data: authorizers.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # An authorizers.xml configuration template that will replace the default template seen in authorizers.go and the replaceTemplateConfigMap\n    replaceTemplateSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: authorizers.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n  # NifiProperties configuration that will be applied to the node.\n  nifiProperties:\n    # Additionnal nifi.properties configuration that will override the one produced based on template and\n    # configuration\n    overrideConfigMap:\n      # The key of the value,in data content, that we want use.\n      data: nifi.properties\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop.\n    # Additionnal nifi.properties configuration that will override the one produced based\n    #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    overrideSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: nifi.properties\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # Additionnal nifi.properties configuration that will override the one produced based\n    #\ton template, configurations and overrideConfigMap\n    overrideConfigs: |\n      nifi.ui.banner.text=NiFiKop\n    # A comma separated list of allowed HTTP Host header values to consider when NiFi\n    # is running securely and will be receiving requests to a different host[:port] than it is bound to.\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#web-properties\n    #      webProxyHosts:\n    # Nifi security client auth\n    needClientAuth: false\n    # Indicates which of the configured authorizers in the authorizers.xml file to use\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#authorizer-configuration\n  #      authorizer:\n  # ZookeeperProperties configuration that will be applied to the node.\n  zookeeperProperties:\n    #      # Additionnal zookeeeper.properties configuration that will override the one produced based on template and\n    #      # configuration\n    #      overrideConfigMap:\n    #        # The key of the value,in data content, that we want use.\n    #        data: zookeeeper.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop.\n    #      # Additionnal zookeeeper.properties configuration that will override the one produced based\n    #      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    #      overrideSecretConfig:\n    #        # The key of the value,in data content, that we want use.\n    #        data: zookeeeper.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop\n    # Additionnal zookeeper.properties configuration that will override the one produced based\n    # on template and configurations.\n    overrideConfigs: |\n      initLimit=15\n      autopurge.purgeInterval=24\n      syncLimit=5\n      tickTime=2000\n      dataDir=./state/zookeeper\n      autopurge.snapRetainCount=30\n  # BootstrapProperties configuration that will be applied to the node.\n  bootstrapProperties:\n    #      # Additionnal bootstrap.conf configuration that will override the one produced based on template and\n    #      # configuration\n    #      overrideConfigMap:\n    #        # The key of the value,in data content, that we want use.\n    #        data: bootstrap.conf\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop.\n    #      # Additionnal bootstrap.conf configuration that will override the one produced based\n    #      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    #      overrideSecretConfig:\n    #        # The key of the value,in data content, that we want use.\n    #        data: bootstrap.conf\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop\n    # JVM memory settings\n    nifiJvmMemory: \"512m\"\n    # Additionnal bootstrap.conf configuration that will override the one produced based\n    # on template and configurations.\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#bootstrap_properties\n    overrideConfigs: |\n      # java.arg.4=-Djava.net.preferIPv4Stack=true\n\n```\n\n----------------------------------------\n\nTITLE: Specifying SSL Secrets Key Values for Existing Secrets\nDESCRIPTION: This YAML configuration specifies the keys expected within an existing SSL secret when `create` is set to false. These keys include CA and client certificates and private keys necessary for establishing trusted SSL connections to the NiFi cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nsslSecrets:\n  tlsSecretName: \"test-nifikop\"\n  create: false\n\n# Keys within the secret:\n# caCert: The CA certificate\n# caKey: The CA private key\n# clientCert: Client certificate for NiFi operations\n# clientKey: Private key for the clientCert\n```\n\n----------------------------------------\n\nTITLE: Install cert-manager with Helm 3\nDESCRIPTION: This snippet demonstrates the installation of cert-manager using Helm 3. It first installs the CustomResourceDefinitions, adds the jetstack helm repository, updates the repository, and then installs the cert-manager chart. Before executing these commands, a namespace must be created.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/2_deploy_nifikop/1_quick_start.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Install CustomResourceDefinitions first\nkubectl apply --validate=false -f \\\n   https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.crds.yaml\n\n# Add the jetstack helm repo\nhelm repo add jetstack https://charts.jetstack.io\nhelm repo update\n\n# You have to create the namespace before executing following command\nhelm install cert-manager \\\n    --namespace cert-manager \\\n    --version v1.7.2 jetstack/cert-manager\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiDataflow Resource in YAML\nDESCRIPTION: Example YAML manifest for creating a `NifiDataflow` custom resource in Kubernetes. This resource defines a specific NiFi dataflow to be deployed and managed by the Nifikop operator, specifying details like the parent process group (`parentProcessGroupID`), flow source (`bucketId`, `flowId`, `flowVersion`), synchronization mode (`syncMode`), update strategy (`updateStrategy`), canvas position (`flowPosition`), error handling flags (`skipInvalidControllerService`, `skipInvalidComponent`), and references to related Nifikop resources (`clusterRef`, `registryClientRef`, `parameterContextRef`).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/5_references/5_nifi_dataflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  flowPosition:\n    posX: 0\n    posY: 0\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: squidflow\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: nifikop\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Assigning NiFiKop NodeConfigGroups to Nodes in YAML\nDESCRIPTION: Shows how to assign previously defined `NodeConfigGroups` (e.g., `default_group`, `high_mem_group`) to specific NiFi nodes using their `id` and the `nodeConfigGroup` field within the NiFiCluster spec. It also illustrates defining node-specific configurations directly using the `nodeConfig` field, overriding or supplementing group settings (shown for node 5).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n- id: 0\n  nodeConfigGroup: \"default_group\"\n- id: 2\n  nodeConfigGroup: \"high_mem_group\"\n- id: 3\n  nodeConfigGroup: \"high_mem_group\"\n- id: 5\n  nodeConfig:\n    resourcesRequirements:\n      limits:\n        cpu: \"2\"\n        memory: 3Gi\n      requests:\n        cpu: \"1\"\n        memory: 1Gi\n```\n\n----------------------------------------\n\nTITLE: Resulting Kubernetes Pod Port Configuration in YAML\nDESCRIPTION: The YAML representation of port configurations in a NiFi pod after internal listeners are defined, showing how container ports are mapped to named ports with TCP protocol.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n    ports:\n    - containerPort: 8443\n      name: https\n      protocol: TCP\n    - containerPort: 6007\n      name: cluster\n      protocol: TCP\n    - containerPort: 6342\n      name: load-balance\n      protocol: TCP\n    - containerPort: 10000\n      name: s2s\n      protocol: TCP\n    - containerPort: 9090\n      name: prometheus\n      protocol: TCP\n```\n\n----------------------------------------\n\nTITLE: Running the NiFiKop CRD Migration Script - Bash\nDESCRIPTION: This command executes the migration process via npm, forwarding command-line arguments for resource type and namespace. Replace <NIFIKOP_RESOURCE> with the specific resource kind (e.g., cluster) and <K8S_NAMESPACE> with the Kubernetes namespace to target. Requires that setup and configuration steps have been completed and the Node.js script is present in the project directory.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nnpm start -- --type=<NIFIKOP_RESOURCE> --namespace=<K8S_NAMESPACE>\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiDataflow CRD in YAML\nDESCRIPTION: This YAML manifest defines a `NifiDataflow` Custom Resource for NiFiKop. It specifies the target process group (`parentProcessGroupID`), registry flow details (`bucketId`, `flowId`, `flowVersion`), sync behavior (`syncMode`), error handling flags (`skipInvalidControllerService`, `skipInvalidComponent`), cluster reference (`clusterRef`), associated registry client (`registryClientRef`), parameter context (`parameterContextRef`), and update strategy (`updateStrategy`). This resource manages the deployment and lifecycle of a specific NiFi dataflow version from a NiFi Registry.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/3_tasks/3_nifi_dataflow.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: demo\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio Gateway for HTTPS with TLS Termination\nDESCRIPTION: Defines an Istio Gateway 'nifi-gateway' that accepts HTTPS traffic on port 443 with TLS mode 'SIMPLE' and associated certificates managed via 'credentialName'. The gateway offloads SSL termination, enabling HTTPS connections to be decrypted at the ingress point.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: nifi-gateway\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 443\n      name: https\n      protocol: HTTPS\n    tls:\n      mode: SIMPLE\n      credentialName: my-secret\n    hosts:\n    - nifi.my-domain.com\n```\n\n----------------------------------------\n\nTITLE: SecretResourceVersion Data Model\nDESCRIPTION: Tracks the resource version of a secret for caching or validation purposes, including secret name, namespace, and resource version string. Ensures the integrity and consistency of secret data.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/5_references/4_nifi_parameter_context.md#_snippet_8\n\nLANGUAGE: YAML\nCODE:\n```\nSecretResourceVersion:\n  name: string\n  namespace: string\n  resourceVersion: string\n```\n\n----------------------------------------\n\nTITLE: Defining Child NifiParameterContext Resource with Inheritance - YAML\nDESCRIPTION: This YAML snippet defines a NifiParameterContext custom resource named 'dataflow-lifecycle-child' which inherits parameters from the 'dataflow-lifecycle' context. It references the same secret for sensitive values but overrides the 'test' parameter with a different value ('toto-child'). This demonstrates how parameter contexts can inherit settings from others, simplifying management.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/5_references/4_nifi_parameter_context.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle-child\nspec:\n  description: \"It is a child test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  inheritedParameterContexts:\n    - name: dataflow-lifecycle\n  parameters:\n    - name: test\n      value: toto-child\n      description: tutu (child)\n```\n\n----------------------------------------\n\nTITLE: Declaring NiFi Nodes with NodeConfigGroups and Inline Configuration (YAML)\nDESCRIPTION: This YAML snippet demonstrates how individual NiFi cluster nodes can be assigned to previously defined nodeConfigGroups or directly overridden with a custom configuration. Each entry configures a node by referencing a group (using nodeConfigGroup) or provides a node-specific configuration (via nodeConfig). Inputs are node definitions within the NifiCluster spec, and the output will be deployment of pods with resource requirements and storage as defined. Ensure all referenced nodeConfigGroups are declared before use and that Kubernetes cluster nodes have adequate resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n- id: 0\n  nodeConfigGroup: \"default_group\"\n- id: 2\n  nodeConfigGroup: \"high_mem_group\"\n- id: 3\n  nodeConfigGroup: \"high_mem_group\"\n- id: 5\n  nodeConfig:\n    resourcesRequirements:\n      limits:\n        cpu: \"2\"\n        memory: 3Gi\n      requests:\n        cpu: \"1\"\n        memory: 1Gi\n```\n\n----------------------------------------\n\nTITLE: Creating Basic Authentication Secret (kubectl)\nDESCRIPTION: This console command demonstrates how to create a Kubernetes Secret using `kubectl` that contains the necessary credentials for the NiFiKop operator to authenticate to an external NiFi cluster using basic authentication. The secret `nifikop-credentials` is created in the `nifikop-nifi` namespace and populated with `username`, `password`, and an optional `ca.crt` from local files.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/3_tasks/1_nifi_cluster/4_external_cluster.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nkubectl create secret generic nifikop-credentials \\\n  --from-file=username=./secrets/username\\\n  --from-file=password=./secrets/password\\\n  --from-file=ca.crt=./secrets/ca.crt\\\n  -n nifikop-nifi\n```\n\n----------------------------------------\n\nTITLE: Describing NifiCluster for Node State Verification - Console\nDESCRIPTION: Issues a kubectl describe command to inspect the status and action steps of a NifiCluster resource after initiating a node removal. This allows operators to verify the progress and state transitions of the target node within the cluster status, observing fields like Configuration State and Graceful Action State. Prerequisites: kubectl, NifiCluster custom resource in the Kubernetes cluster. Input: NifiCluster name. Output: Full status section including details of nodes, action states, and any errors. Limitations: Status information is read-only and may lag slightly behind actual events.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nkubectl describe nificluster simplenifi\n\n...\nStatus:\n  Nodes State:\n    ...\n    2:\n      Configuration State:  ConfigInSync\n      Graceful Action State:\n        Action State:   GracefulDownscaleRequired\n        Error Message:\n    ...\n...\n```\n\n----------------------------------------\n\nTITLE: Deploying NifiNodeGroupAutoscaler Custom Resource with YAML\nDESCRIPTION: Defines a 'NifiNodeGroupAutoscaler' custom resource in Kubernetes to specify how NiFi nodes are auto-scaled. This resource links to an existing NifiCluster, references the NodeConfigGroup for targeted scaling, and sets strategies for both upscaling and downscaling the cluster. Required dependencies include the nifikop operator and custom resource definitions installed in the Kubernetes cluster. Parameters include clusterRef, nodeConfigGroupId, nodeLabelsSelector, upscaleStrategy, and downscaleStrategy. Input is a YAML manifest; output is automated node management. The spec allows further customization using readOnlyConfig and nodeConfig.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiNodeGroupAutoscaler\nmetadata:\n  name: nifinodegroupautoscaler-sample\n  namespace: clusters\nspec:\n  # contains the reference to the NifiCluster with the one the node group autoscaler is linked.\n  clusterRef:\n    name: cluster\n    namespace: clusters\n  # defines the id of the NodeConfig contained in NifiCluster.Spec.NodeConfigGroups\n  nodeConfigGroupId: auto_scaling\n  # readOnlyConfig can be used to pass Nifi node config\n  # which has type read-only these config changes will trigger rolling upgrade\n  readOnlyConfig:\n    nifiProperties:\n      overrideConfigs: |\n        nifi.ui.banner.text=NiFiKop - Scale Group\n  # This is an example of a node config you can apply to each replica in this node group.\n  # Any settings here will override those in the configured nodeConfigGroupId\n#  nodeConfig:\n#    nodeSelector:\n#      node_type: high-mem\n  # The selector used to identify nodes in NifiCluster.Spec.Nodes this autoscaler will manage\n  # Use Node.Labels in combination with this selector to clearly define which nodes will be managed by this autoscaler \n  nodeLabelsSelector: \n    matchLabels:\n      nifi_cr: cluster\n      nifi_node_group: auto-scaling\n  # the strategy used to decide how to add nodes to a nifi cluster\n  upscaleStrategy: simple\n  # the strategy used to decide how to remove nodes from an existing cluster\n  downscaleStrategy: lifo\n```\n\n----------------------------------------\n\nTITLE: Creating an Inherited NifiParameterContext Resource (YAML)\nDESCRIPTION: This YAML snippet illustrates creating a child `NifiParameterContext` named `dataflow-lifecycle-child` that inherits parameters from `dataflow-lifecycle`. It defines its own description, cluster reference, secret reference, specifies the inherited context via `inheritedParameterContexts`, and overrides the 'test' parameter value.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/5_references/4_nifi_parameter_context.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle-child\nspec:\n  description: \"It is a child test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  inheritedParameterContexts:\n    - name: dataflow-lifecycle\n  parameters:\n    - name: test\n      value: toto-child\n      description: tutu (child)\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Namespace (console)\nDESCRIPTION: This command creates a dedicated Kubernetes namespace named `monitoring-system` to isolate the monitoring components, such as Prometheus and its associated resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_5\n\nLANGUAGE: console\nCODE:\n```\nkubectl create namespace monitoring-system\n```\n\n----------------------------------------\n\nTITLE: Deploying KEDA with Helm - Updating repository\nDESCRIPTION: Command to update the Helm repositories to ensure the latest KEDA charts are available.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nhelm repo update\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Autoscaler ScaledObject KEDA/YAML\nDESCRIPTION: This YAML manifest defines a KEDA ScaledObject that enables autoscaling for a 'NifiNodeGroupAutoscaler' custom resource. It configures polling intervals, cooldown periods, replica counts, and defines a Prometheus trigger. KEDA will query the specified Prometheus metric from the configured server address and scale the target resource based on the metric's value compared to the defined threshold.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: cluster\n  namespace: clusters\nspec:\n  scaleTargetRef:\n    apiVersion:    nifi.konpyutaika.com/v1alpha1     # Optional. Default: apps/v1\n    kind:          NifiNodeGroupAutoscaler           # Optional. Default: Deployment\n    name:          nifinodegroupautoscaler-sample    # Mandatory. Must be in the same namespace as the ScaledObject\n    envSourceContainerName: nifi                     # Optional. Default: .spec.template.spec.containers[0]\n  pollingInterval:  30                               # Optional. Default: 30 seconds\n  cooldownPeriod:   300                              # Optional. Default: 300 seconds\n  idleReplicaCount: 0                                # Optional. Default: ignored, must be less than minReplicaCount \n  minReplicaCount:  1                                # Optional. Default: 0\n  maxReplicaCount:  3                                # Optional. Default: 100\n  fallback:                                          # Optional. Section to specify fallback options\n    failureThreshold: 5                              # Mandatory if fallback section is included\n    replicas: 1                                      # Mandatory if fallback section is included\n  # advanced:                                          # Optional. Section to specify advanced options\n  #   restoreToOriginalReplicaCount: true              # Optional. Whether the target resource should be scaled back to original replicas count, after the ScaledObject is deleted\n  #   horizontalPodAutoscalerConfig:                   # Optional. Section to specify HPA related options\n  #     name: {name-of-hpa-resource}                   # Optional. Default: keda-hpa-{scaled-object-name}\n  #     behavior:                                      # Optional. Use to modify HPA's scaling behavior\n  #       scaleDown:\n  #         stabilizationWindowSeconds: 300 <--- \n  #         policies:\n  #         - type: Percent\n  #           value: 100\n  #           periodSeconds: 15\n  triggers:\n    - type: prometheus\n      metadata:\n        serverAddress: http://prometheus-operated.monitoring-system.svc:9090\n        metricName: http_requests_total\n        threshold: <threshold value>\n        query: <prometheus query>\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFi Cluster on OpenShift with UID/GID Adjustment\nDESCRIPTION: Modifies the sample NiFi cluster configuration to include the retrieved UID by substituting placeholder values in the YAML file. Then deploys the NiFi cluster with 'kubectl create', ensuring the security contexts align with OpenShift requirements.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nsed -i \"s/1000690000/$uid/g\" config/samples/openshift.yaml\n```\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create -n nifi -f config/samples/openshift.yaml\n```\n\n----------------------------------------\n\nTITLE: Identity Mapping Configuration for NiFi Properties\nDESCRIPTION: Recommended identity mapping configuration to add to nifi.properties to ensure multiple identity provider support.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/5_users_authentication/1_oidc.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nnifi.security.identity.mapping.pattern.dn=CN=([^,]*)(?:, (?:O|OU)=.*)?\\nnifi.security.identity.mapping.value.dn=$1\\nnifi.security.identity.mapping.transform.dn=NONE\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi ServiceMonitor - YAML\nDESCRIPTION: YAML definition for a `ServiceMonitor` custom resource. This resource instructs the Prometheus server to discover and scrape metrics from NiFi services. It uses label selectors and namespace selectors to find the correct services and defines the endpoint configuration (port, path, interval) and relabeling rules for metric processing. Requires the Prometheus Operator CRDs.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: cluster\n  namespace: monitoring-system\n  labels:\n    app: nifi-cluster\n    nifi_cr: cluster\nspec:\n  selector:\n    matchLabels:\n      app: nifi\n      nifi_cr: cluster\n  namespaceSelector:\n    matchNames:\n      - clusters\n  endpoints:\n    - interval: 10s\n      port: prometheus\n      path: /metrics\n      honorLabels: true\n      relabelings:\n        - sourceLabels: [__meta_kubernetes_pod_ip]\n          separator: ;\n          regex: (.*)\n          targetLabel: pod_ip\n          replacement: $1\n          action: replace\n        - sourceLabels: [__meta_kubernetes_pod_label_nodeId]\n          separator: ;\n          regex: (.*)\n          targetLabel: nodeId\n          replacement: $1\n          action: replace\n        - sourceLabels: [__meta_kubernetes_pod_label_nifi_cr]\n          separator: ;\n          regex: (.*)\n          targetLabel: nifi_cr\n          replacement: $1\n          action: replace\n```\n\n----------------------------------------\n\nTITLE: Defining cluster-wide ReadOnlyConfig for NiFi with YAML\nDESCRIPTION: This YAML snippet represents the ReadOnlyConfig object which provides a read-only configuration for a NiFi cluster managed by NiFiKop, specifying overrides and replacements for various NiFi components. It supports threading constraints, logback logging configurations, authorizer templates, NiFi properties overrides, Zookeeper settings, and bootstrap properties. The snippet references Kubernetes ConfigMap and Secret resources to specify replacement configurations by providing the data key, resource name, and namespace. The configuration is designed to be merged with node-specific read-only configs and supports deprecated threading settings with notes on their version constraints. This setup lets operators manage secure and customized NiFi environments declaratively via YAML.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/5_references/1_nifi_cluster/2_read_only_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreadOnlyConfig:\n  # MaximumTimerDrivenThreadCount define the maximum number of threads for timer driven processors available to the system.\n  maximumTimerDrivenThreadCount: 30\n  # MaximumEventDrivenThreadCount define the maximum number of threads for event driven processors available to the system (@DEPRECATED. This has no effect from NiFiKOp v1.9.0 or later).\n  maximumEventDrivenThreadCount: 10\n  # Logback configuration that will be applied to the node\n  logbackConfig:\n    # logback.xml configuration that will replace the one produced based on template\n    replaceConfigMap:\n      # The key of the value,in data content, that we want use.\n      data: logback.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # logback.xml configuration that will replace the one produced based on template and overrideConfigMap\n    replaceSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: logback.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n  # Authorizer configuration that will be applied to the node\n  authorizerConfig:\n    # An authorizers.xml configuration template that will replace the default template seen in authorizers.go\n    replaceTemplateConfigMap:\n      # The key of the value, in data content, that we want use.\n      data: authorizers.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # An authorizers.xml configuration template that will replace the default template seen in authorizers.go and the replaceTemplateConfigMap\n    replaceTemplateSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: authorizers.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n  # NifiProperties configuration that will be applied to the node.\n  nifiProperties:\n    # Additionnal nifi.properties configuration that will override the one produced based on template and\n    # configuration\n    overrideConfigMap:\n      # The key of the value,in data content, that we want use.\n      data: nifi.properties\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop.\n    # Additionnal nifi.properties configuration that will override the one produced based\n    #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    overrideSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: nifi.properties\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # Additionnal nifi.properties configuration that will override the one produced based\n    #\ton template, configurations and overrideConfigMap\n    overrideConfigs: |\n      nifi.ui.banner.text=NiFiKop\n    # A comma separated list of allowed HTTP Host header values to consider when NiFi\n    # is running securely and will be receiving requests to a different host[:port] than it is bound to.\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#web-properties\n    #      webProxyHosts:\n    # Nifi security client auth\n    needClientAuth: false\n    # Indicates which of the configured authorizers in the authorizers.xml file to use\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#authorizer-configuration\n  #      authorizer:\n  # ZookeeperProperties configuration that will be applied to the node.\n  zookeeperProperties:\n    #      # Additionnal zookeeeper.properties configuration that will override the one produced based on template and\n    #      # configuration\n    #      overrideConfigMap:\n    #        # The key of the value,in data content, that we want use.\n    #        data: zookeeeper.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop.\n    #      # Additionnal zookeeeper.properties configuration that will override the one produced based\n    #      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    #      overrideSecretConfig:\n    #        # The key of the value,in data content, that we want use.\n    #        data: zookeeeper.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop\n    # Additionnal zookeeper.properties configuration that will override the one produced based\n    # on template and configurations.\n    overrideConfigs: |\n      initLimit=15\n      autopurge.purgeInterval=24\n      syncLimit=5\n      tickTime=2000\n      dataDir=./state/zookeeper\n      autopurge.snapRetainCount=30\n  # BootstrapProperties configuration that will be applied to the node.\n  bootstrapProperties:\n    #      # Additionnal bootstrap.conf configuration that will override the one produced based on template and\n    #      # configuration\n    #      overrideConfigMap:\n    #        # The key of the value,in data content, that we want to use.\n    #        data: bootstrap.conf\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop.\n    #      # Additionnal bootstrap.conf configuration that will override the one produced based\n    #      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    #      overrideSecretConfig:\n    #        # The key of the value,in data content, that we want use.\n    #        data: bootstrap.conf\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop\n    # JVM memory settings\n    nifiJvmMemory: \"512m\"\n    # Additionnal bootstrap.conf configuration that will override the one produced based\n    # on template and configurations.\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#bootstrap_properties\n    overrideConfigs: |\n      # java.arg.4=-Djava.net.preferIPv4Stack=true\n```\n\n----------------------------------------\n\nTITLE: Creating Secret with kubectl\nDESCRIPTION: This console command creates a Kubernetes secret named `secret-params` within the `nifikop` namespace.  The secret stores sensitive parameters as key-value pairs, which can be referenced by a `NifiParameterContext`. The example includes two key-value pairs: `secret1` and `secret2`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl create secret generic secret-params \\\n    --from-literal=secret1=yop \\\n    --from-literal=secret2=yep \\\n    -n nifikop\n```\n\n----------------------------------------\n\nTITLE: Example YAML Configuration for NifiNodeGroupAutoscaler Resource\nDESCRIPTION: This YAML snippet demonstrates the declaration of a NifiNodeGroupAutoscaler resource, specifying cluster references, node label selectors, and scaling strategies for automating node management within a NiFi cluster. Dependencies include the Kubernetes API and custom resource definitions for NiFi autoscaler management.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/5_references/7_nifi_nodegroup_autoscaler.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiNodeGroupAutoscaler\nmetadata:\n  name: nifinodegroupautoscaler-sample\nspec:\n  clusterRef:\n    name: nificluster-name\n    namespace: nifikop\n  nodeConfigGroupId: default-node-group\n  nodeLabelsSelector:\n    matchLabels:\n      nifi_cr: nificluster-name\n      nifi_node_group: default-node-group\n  upscaleStrategy: simple\n  downscaleStrategy: lifo\n```\n\n----------------------------------------\n\nTITLE: Configuring Nifi LoadBalancer External Service in YAML\nDESCRIPTION: This YAML example defines an external Nifi service of type LoadBalancer, including settings for specifying a loadBalancerClass appropriate for AWS NLB, and multiple listener port configurations with different protocols (TCP/UDP). The snippet requires a Kubernetes environment with LoadBalancer support and appropriate RBAC, as well as access to any referenced cloud infrastructure. Main inputs are the external service name, service type, loadBalancerClass, portConfigs, and optional metadata. Applying the manifest deploys a Service resource exposed via a cloud load balancer, potentially constrained by any specified class, port, or protocol attributes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/5_references/1_nifi_cluster/7_external_service_config.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\nexternalServices:\n    - name: \"nlb\"\n      spec:\n        type: LoadBalancer\n        loadBalancerClass: \"service.k8s.aws/nlb\"\n        portConfigs:\n          - port: 8080\n            internalListenerName: \"http\"\n          - port: 7890\n            internalListenerName: \"my-custom-udp-listener\"\n            protocol: UDP\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n\n----------------------------------------\n\nTITLE: Adding a Node to NiFiKop Cluster - YAML\nDESCRIPTION: This YAML manifest adds a new node with unique id (25) to the NifiCluster resource. It specifies node groups, storage config, resource requirements, service accounts, listener ports, and other necessary fields required by the NiFiKop operator for proper orchestration. To apply, ensure prior installation of NiFiKop and an existing cluster, then append the additional node definition as shown. The Node.Id value must be unique within the cluster. Inputs: Custom values for ids, resources, storage. Output: Updated cluster state with a new NiFi node. Required: Kubernetes, the NiFiKop CRDs deployed, and cluster access rights.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.12.1\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n    - id: 2\n      nodeConfigGroup: \"default_group\"\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: Describing NiFiKop Cluster State - Console Command\nDESCRIPTION: This command requests a detailed description of the current NiFi cluster custom resource in Kubernetes, showing the status of each node and the progress or errors for any ongoing scaling, offload, or decommission steps. Requires kubectl, appropriate RBAC permissions, and the existence of a corresponding NifiCluster resource. Main input is the cluster name ('simplenifi'), and the output details node states and cluster health, key for troubleshooting scaledown and decommission processes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_4\n\nLANGUAGE: Console\nCODE:\n```\nkubectl describe nificluster simplenifi\n```\n\n----------------------------------------\n\nTITLE: Configuring Persistent Storage for NiFiKop Nodes Using YAML\nDESCRIPTION: This YAML snippet illustrates how to configure multiple persistent storage volumes for different NiFi data and configuration paths through the storageConfigs array in the NiFiKop CRD. Each storageConfig entry defines mountPath, volume name, PVC spec, reclaim policy, and metadata (labels/annotations). Dependencies are pre-existing Kubernetes StorageClasses and the ability to create PersistentVolumeClaims. Key parameters include the storage size, mount location in the pod, and PVC labels/annotations. The expected result is robust, persistent storage of stateful NiFi data (logs, repositories, configurations) across pod restarts or recreations. PVC creation failures or misalignment with NiFi file path requirements may affect cluster durability.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n...\nstorageConfigs:\n  - mountPath: /opt/nifi/nifi-current/logs\n    name: logs\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/data\n    name: data\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 50Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/extensions\n    name: extensions-repository\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 5Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/flowfile_repository\n    name: flowfile-repository\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/nifi-current/conf\n    name: conf\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 5Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/content_repository\n    name: content-repository\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 500Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/provenance_repository\n    name: provenance-repository\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 500Gi\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Cluster with Auto-scaling Node Group\nDESCRIPTION: YAML configuration for a NiFi cluster with a dedicated NodeConfigGroup for auto-scaling and Prometheus metrics exposure through an internal listener.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n...\nspec:\n  ...\n  listenersConfig:\n    internalListeners:\n    ...\n    - containerPort: 9090\n      name: prometheus\n      type: prometheus \n    ...\n  ...\n  nodeConfigGroups:\n    auto_scaling:\n      isNode: true\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 2Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n      serviceAccountName: external-dns\n      storageConfigs:\n        - mountPath: /opt/nifi/nifi-current/logs\n          name: logs\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/data\n          name: data\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/extensions\n          name: extensions-repository\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/flowfile_repository\n          name: flowfile-repository\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/nifi-current/conf\n          name: conf\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/content_repository\n          name: content-repository\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/provenance_repository\n          name: provenance-repository\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n  ...\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Parameter Context Resource in Kubernetes YAML\nDESCRIPTION: This YAML snippet defines a custom resource named NifiParameterContext for Kubernetes, specifying metadata and the desired state (spec) for a NiFi Parameter Context. It includes details such as description, cluster reference, secret references for sensitive parameters, and a list of parameters with names, values, descriptions, and sensitivity flags. This resource is used by the Nifikop operator to manage and configure NiFi parameter contexts within a Kubernetes environment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/5_references/4_nifi_parameter_context.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      description: toto\n      sensistive: true\n```\n\n----------------------------------------\n\nTITLE: Defining NifiParameterContext with Parameters\nDESCRIPTION: This YAML snippet defines a `NifiParameterContext` named `dataflow-lifecycle`.  It includes a description, a reference to a NifiCluster, a reference to a secret containing parameters, and a list of non-sensitive parameters with names, values, and descriptions.  The `clusterRef` specifies the NiFi cluster to which this context applies, and the `secretRefs` define which secrets to use for sensitive parameters.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/5_references/4_nifi_parameter_context.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      description: toto\n      sensistive: true\n```\n\n----------------------------------------\n\nTITLE: Configuring Managed Groups for NiFi Cluster via NifiCluster Resource (YAML)\nDESCRIPTION: Demonstrates how to use the 'managedAdminUsers' and 'managedReaderUsers' fields in a NifiCluster Kubernetes CR to declaratively create preconfigured admin and reader groups in NiFi. Each user is defined with both an identity (for NiFi) and a resource name (for Kubernetes). The operator will automatically add these users to the managed groups, create necessary NifiUser resources, and sync with NiFi's internal user/group registry. Dependencies: nifikop operator managing the specified NifiCluster. Inputs: users for admin and reader roles in 'spec'. Outputs: Managed admin and reader user groups plus their constituent users. You can expand users and identities as needed for your organization.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/3_tasks/4_nifi_user_group.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  ...\n  oneNifiNodePerNode: false\n  propagateLabels: true\n  managedAdminUsers:\n    -  identity: \"alexandre.guitton@konpyutaika.com\"\n       name: \"aguitton\"\n    -  identity: \"nifiuser@konpyutaika.com\"\n       name: \"nifiuser\"\n  managedReaderUsers:\n    -  identity: \"toto@konpyutaika.com\"\n       name: \"toto\"\n    ...\n\n```\n\n----------------------------------------\n\nTITLE: NiFiCluster YAML configuration for scale up\nDESCRIPTION: This YAML configuration shows how to add a new node to the NiFi cluster by adding a new entry to the `NifiCluster.Spec.Nodes` list.  The `Node.Id` field must be unique. This configuration defines the new node with id 25 using the default node config group.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.12.1\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n    - id: 2\n      nodeConfigGroup: \"default_group\"\n# >>>> START: The new node\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n# <<<< END\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi NodeGroup Autoscaler with Kubernetes YAML\nDESCRIPTION: This YAML snippet defines a Kubernetes custom resource of kind NifiNodeGroupAutoscaler, enabling automatic scaling of NiFi nodes within a cluster using the nifikop operator. It requires the nifikop CRDs to be installed and correctly references a NifiCluster by name and namespace. The configuration specifies the node group selector, scaling strategies (simple for upscale and lifo for downscale), and associates nodes to be managed via labels. Input requirements include existing NifiCluster, correctly set label selectors, and appropriate role permissions. The output will be automated node scaling behavior for the referenced NiFi node group.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/5_references/7_nifi_nodegroup_autoscaler.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiNodeGroupAutoscaler\nmetadata:\n  name: nifinodegroupautoscaler-sample\nspec:\n  # contains the reference to the NifiCluster with the one the node group autoscaler is linked.\n  clusterRef:\n    name: nificluster-name\n    namespace: nifikop\n  # defines the id of the NodeConfig contained in NifiCluster.Spec.NodeConfigGroups\n  nodeConfigGroupId: default-node-group\n  # The selector used to identify nodes in NifiCluster.Spec.Nodes this autoscaler will manage\n  # Use Node.Labels in combination with this selector to clearly define which nodes will be managed by this autoscaler \n  nodeLabelsSelector: \n    matchLabels:\n      nifi_cr: nificluster-name\n      nifi_node_group: default-node-group\n  # the strategy used to decide how to add nodes to a nifi cluster\n  upscaleStrategy: simple\n  # the strategy used to decide how to remove nodes from an existing cluster\n  downscaleStrategy: lifo\n```\n\n----------------------------------------\n\nTITLE: Configuring NifiNodeGroupAutoscaler for KEDA Integration\nDESCRIPTION: YAML configuration for a NifiNodeGroupAutoscaler that defines how and what to autoscale in the NiFi cluster using the auto_scaling NodeConfigGroup.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiNodeGroupAutoscaler\nmetadata:\n  name: nifinodegroupautoscaler-sample\n  namespace: clusters\nspec:\n  # contains the reference to the NifiCluster with the one the node group autoscaler is linked.\n  clusterRef:\n    name: cluster\n    namespace: clusters\n  # defines the id of the NodeConfig contained in NifiCluster.Spec.NodeConfigGroups\n  nodeConfigGroupId: auto_scaling\n  # readOnlyConfig can be used to pass Nifi node config\n  # which has type read-only these config changes will trigger rolling upgrade\n  readOnlyConfig:\n    nifiProperties:\n      overrideConfigs: |\n        nifi.ui.banner.text=NiFiKop - Scale Group\n  # This is an example of a node config you can apply to each replica in this node group.\n  # Any settings here will override those in the configured nodeConfigGroupId\n#  nodeConfig:\n#    nodeSelector:\n#      node_type: high-mem\n  # The selector used to identify nodes in NifiCluster.Spec.Nodes this autoscaler will manage\n  # Use Node.Labels in combination with this selector to clearly define which nodes will be managed by this autoscaler \n  nodeLabelsSelector: \n    matchLabels:\n      nifi_cr: cluster\n      nifi_node_group: auto-scaling\n  # the strategy used to decide how to add nodes to a nifi cluster\n  upscaleStrategy: simple\n  # the strategy used to decide how to remove nodes from an existing cluster\n  downscaleStrategy: lifo\n```\n\n----------------------------------------\n\nTITLE: Configuring External Services in NiFi on Kubernetes\nDESCRIPTION: This YAML snippet demonstrates configuring external services to expose NiFi's internal listeners to the outside world. It defines the `name` of the external service, the `type` of service (e.g., LoadBalancer), and the `portConfigs` which map internal listener names to external ports. The example exposes the `https` listener on port 443 and the `http-tracking` listener on port 80.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    - type: \"https\"\n      name: \"https\"\n      containerPort: 8443\n    - type: \"cluster\"\n      name: \"cluster\"\n      containerPort: 6007\n    - type: \"s2s\"\n      name: \"s2s\"\n      containerPort: 10000\n    - type: \"prometheus\"\n      name: \"prometheus\"\n      containerPort: 9090\n    - type: \"load-balance\"\n      name: \"load-balance\"\n      containerPort: 6342\n    - name: \"http-tracking\"\n      containerPort: 8081\nexternalServices:\n  - name: cluster-access\n    spec:\n      portConfigs:\n        - internalListenerName: https\n          port: 443\n        - internalListenerName: http-tracking\n          port: 80\n      type: LoadBalancer\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Listeners in YAML\nDESCRIPTION: This YAML snippet configures different types of NiFi listeners. It defines internal listeners with their types (https, cluster, s2s, prometheus, load-balance), names, and container ports. It also specifies the `sslSecrets`, including the TLS secret name and whether to create the secrets.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/5_references/1_nifi_cluster/6_listeners_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  listenersConfig:\n    internalListeners:\n      - type: \"https\"\n        name: \"https\"\n        containerPort: 8443\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n      - type: \"prometheus\"\n        name: \"prometheus\"\n        containerPort: 9090\n      - type: \"load-balance\"\n        name: \"load-balance\"\n        containerPort: 6342\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n```\n\n----------------------------------------\n\nTITLE: Creating NifiDataflow YAML Example\nDESCRIPTION: This YAML snippet illustrates the creation of a `NifiDataflow` resource. It specifies the API version, kind, metadata (name), and specification including the parent process group ID, bucket ID, flow ID, flow version, sync mode, references to the registry client, parameter context, and update strategy. The `syncMode` parameter defines how the operator will synchronize with the dataflow on the NiFi cluster. This snippet uses `always` to maintain the dataflow lifecycle.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: demo\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Cluster with Prometheus Support\nDESCRIPTION: YAML configuration for a NiFi cluster with a dedicated auto_scaling NodeConfigGroup and Prometheus listener to enable metrics collection for autoscaling.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n...\nspec:\n  ...\n  listenersConfig:\n    internalListeners:\n    ...\n    - containerPort: 9090\n      name: prometheus\n      type: prometheus \n    ...\n  ...\n  nodeConfigGroups:\n    auto_scaling:\n      isNode: true\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 2Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n      serviceAccountName: external-dns\n      storageConfigs:\n        - mountPath: /opt/nifi/nifi-current/logs\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/data\n          name: data\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/extensions\n          name: extensions-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/flowfile_repository\n          name: flowfile-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/nifi-current/conf\n          name: conf\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/content_repository\n          name: content-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/provenance_repository\n          name: provenance-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n  ...\n```\n\n----------------------------------------\n\nTITLE: Example NifiUser Resource Definition (YAML)\nDESCRIPTION: This YAML snippet shows an example manifest for creating a `NifiUser` Kubernetes resource. It specifies the API version, kind, metadata (like the user's resource name), and the spec, including the user's identity within NiFi, a reference to the target `NifiCluster`, and the option to disable automatic certificate creation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/5_references/2_nifi_user.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: aguitton\nspec:\n  identity: alexandre.guitton@konpyutaika.com\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  createCert: false\n```\n\n----------------------------------------\n\nTITLE: Configuring Nifi Listeners (YAML)\nDESCRIPTION: This YAML snippet defines the configuration for various Nifi listeners. It specifies listener types like 'https', 'cluster', 's2s', 'prometheus', and 'load-balance'. Each listener has a name and container port. The configuration also includes SSL-related settings through `sslSecrets`, such as `tlsSecretName` and `create` flag. This snippet is essential for configuring how Nifi components communicate internally and securely.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/5_references/1_nifi_cluster/6_listeners_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  listenersConfig:\n    internalListeners:\n      - type: \"https\"\n        name: \"https\"\n        containerPort: 8443\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n      - type: \"prometheus\"\n        name: \"prometheus\"\n        containerPort: 9090\n      - type: \"load-balance\"\n        name: \"load-balance\"\n        containerPort: 6342\n      - name: \"my-custom-listener-port\"\n        containerPort: 1234\n        protocol: \"TCP\"\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n```\n\n----------------------------------------\n\nTITLE: Removing a Node from NifiCluster Definition (YAML)\nDESCRIPTION: YAML manifest for a `NifiCluster` resource, illustrating how to remove a node (id: 2, commented out) from the `spec.nodes` list to initiate a graceful scale-down. Removing the node entry triggers the NiFiKop decommissioning process.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  headlessServiceEnabled: true\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.11.3\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n# >>>> START: node removed\n#    - id: 2\n#      nodeConfigGroup: \"default_group\"\n# <<<< END\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: NiFiCluster Resource with OIDC Configuration\nDESCRIPTION: Example YAML configuration for a NifiCluster custom resource with OpenID Connect settings in the overrideConfigs section of nifiProperties.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/5_users_authentication/1_oidc.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\\nkind: NifiCluster\\n...\\nspec:\\n  ...\\n  readOnlyConfig:\\n    # NifiProperties configuration that will be applied to the node.\\n    nifiProperties:\\n      webProxyHosts:\\n        - nifistandard2.trycatchlearn.fr:8443\\n      # Additionnal nifi.properties configuration that will override the one produced based\\n      # on template and configurations.\\n      overrideConfigs: |\\n        nifi.security.user.oidc.discovery.url=<oidc server discovery url>\\n        nifi.security.user.oidc.client.id=<oidc client's id>\\n        nifi.security.user.oidc.client.secret=<oidc client's secret>\\n        nifi.security.identity.mapping.pattern.dn=CN=([^,]*)(?:, (?:O|OU)=.*)?\\n        nifi.security.identity.mapping.value.dn=$1\\n        nifi.security.identity.mapping.transform.dn=NONE\\n      ...\\n   ...\\n...\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Cluster with Prometheus Support\nDESCRIPTION: YAML configuration for a NiFi cluster with Prometheus metrics support and a dedicated node config group for autoscaling. This includes configuration for internal listeners and detailed storage settings.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n...\nspec:\n  ...\n  listenersConfig:\n    internalListeners:\n    ...\n    - containerPort: 9090\n      name: prometheus\n      type: prometheus \n    ...\n  ...\n  nodeConfigGroups:\n    auto_scaling:\n      isNode: true\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 2Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n      serviceAccountName: external-dns\n      storageConfigs:\n        - mountPath: /opt/nifi/nifi-current/logs\n          name: logs\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/data\n          name: data\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/extensions\n          name: extensions-repository\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/flowfile_repository\n          name: flowfile-repository\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/nifi-current/conf\n          name: conf\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/content_repository\n          name: content-repository\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/provenance_repository\n          name: provenance-repository\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n  ...\n```\n\n----------------------------------------\n\nTITLE: NifiNodeGroupAutoscalerStatus Schema Explanation\nDESCRIPTION: This schema details the status information exposed by the autoscaler, including current state, number of replicas, and selector used. This data reflects the autoscaler's runtime state and is set automatically by the autoscaler controller.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/5_references/7_nifi_nodegroup_autoscaler.md#_snippet_3\n\n\n\n----------------------------------------\n\nTITLE: Configuring Conversion Webhook for CRDs\nDESCRIPTION: Provides YAML snippet for configuring the conversion webhook in CRDs to handle resource version conversions from v1alpha1 to v1, including setting the webhook service and certificate annotations. Necessary for proper resource handling during API upgrades.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n... \nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n\n```\n\n----------------------------------------\n\nTITLE: Defining Istio VirtualService for HTTP Routing in YAML\nDESCRIPTION: This Istio VirtualService routes HTTP traffic received by the 'nifi-gateway' for host 'nifi.my-domain.com' with any URI prefix ('/') to the internal NiFi service (named 'nifi') on port 8080.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: nifi-vs\nspec:\n  gateways:\n  - nifi-gateway\n  hosts:\n  - nifi.my-domain.com\n  http:\n  - match:\n    - uri:\n        prefix: /\n    route:\n    - destination:\n        host: nifi\n        port:\n          number: 8080\n```\n\n----------------------------------------\n\nTITLE: Defining NifiCluster CR to Scale Up | YAML\nDESCRIPTION: This YAML snippet shows how to update the `NifiCluster` Custom Resource definition to add a new node with ID 25. It demonstrates adding an entry to the `spec.nodes` list to trigger a scale-up operation by the NiFiKop operator. Requires a pre-existing `simplenifi` cluster and `default_group` node configuration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.12.1\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n    - id: 2\n      nodeConfigGroup: \"default_group\"\n# >>>> START: The new node\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n# <<<< END\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: Defining NiFiUser Resource\nDESCRIPTION: This YAML snippet defines a NifiUser resource in Kubernetes, specifying the user's identity, the cluster it belongs to, and whether a certificate should be created for the user.  It uses the `nifi.konpyutaika.com/v1` API version.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/5_references/2_nifi_user.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: aguitton\nspec:\n  identity: alexandre.guitton@konpyutaika.com\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  createCert: false\n```\n\n----------------------------------------\n\nTITLE: Defining Prometheus Custom Resource (CR) in Kubernetes (YAML)\nDESCRIPTION: Deploys a Prometheus instance using the `Prometheus` Custom Resource Definition (CRD) managed by the Prometheus Operator. This manifest specifies configuration details such as disabling the admin API, setting scrape and evaluation intervals, defining resource requests, specifying the service account (`prometheus` created earlier), and setting selectors to discover relevant `PodMonitor` and `ServiceMonitor` resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: Prometheus\nmetadata:\n  name: prometheus\n  namespace: monitoring-system\nspec:\n  enableAdminAPI: false\n  evaluationInterval: 30s\n  logLevel: debug\n  podMonitorSelector:\n    matchExpressions:\n    - key: release\n      operator: In\n      values:\n      - prometheus\n  resources:\n    requests:\n      memory: 400Mi\n  scrapeInterval: 30s\n  serviceAccountName: prometheus\n  serviceMonitorSelector:\n    matchExpressions:\n    - key: app\n      operator: In\n      values:\n      - nifi-cluster\n```\n\n----------------------------------------\n\nTITLE: Assigning Configurations to Individual NiFi Nodes (NiFiKop YAML)\nDESCRIPTION: This YAML snippet demonstrates how to define individual nodes within the `nodes` list of a NiFiCluster resource specification. It shows how to apply a predefined `nodeConfigGroup` by referencing its name (`nodeConfigGroup: \"group_name\"`). It also illustrates the flexibility to define a unique, non-reusable configuration directly at the node level using the `nodeConfig` field.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n- id: 0\n  nodeConfigGroup: \"default_group\"\n- id: 2\n  nodeConfigGroup: \"high_mem_group\"\n- id: 3\n  nodeConfigGroup: \"high_mem_group\"\n- id: 5\n  nodeConfig:\n    resourcesRequirements:\n      limits:\n        cpu: \"2\"\n        memory: 3Gi\n      requests:\n        cpu: \"1\"\n        memory: 1Gi\n```\n\n----------------------------------------\n\nTITLE: Configure NiFiCluster to Use External Cert-Manager Issuer (YAML)\nDESCRIPTION: This YAML snippet configures a `NifiCluster` resource to use a cert-manager `Issuer` named `letsencrypt-staging` (as defined in the previous example) for generating SSL certificates. It sets `sslSecrets.create` to `true` but adds an `issuerRef` to delegate certificate creation to cert-manager. It also enables secure cluster and site-to-site communication and configures external DNS usage with a specified cluster domain. Requires cert-manager, the referenced Issuer, and external-dns setup.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  clusterSecure: true\n  siteToSiteSecure: true\n  ...\n  listenersConfig:\n    clusterDomain: <DNS zone name>\n    useExternalDNS: true\n    ...\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n      issuerRef:\n        name: letsencrypt-staging\n        kind: Issuer\n```\n\n----------------------------------------\n\nTITLE: Installing cert-manager Directly (Bash/kubectl)\nDESCRIPTION: Installs cert-manager and its CustomResourceDefinitions (CRDs) directly using `kubectl apply` from the official release YAML. This method applies version v1.7.2.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/2_deploy_nifikop/1_quick_start.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Install the CustomResourceDefinitions and cert-manager itself\nkubectl apply -f \\\n    https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.yaml\n```\n\n----------------------------------------\n\nTITLE: Port-Forwarding Prometheus Service to Localhost Using kubectl Console\nDESCRIPTION: This console command sets up port forwarding from the Prometheus service 'prometheus-operated' running in the 'monitoring-system' namespace to localhost port 9090, allowing local access to the Prometheus UI for querying NiFi metrics. Requires kubectl configured to access the Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_11\n\nLANGUAGE: console\nCODE:\n```\nkubectl port-forward -n monitoring-system service/prometheus-operated 9090:9090\n```\n\n----------------------------------------\n\nTITLE: Set NiFi User Authorizer Property\nDESCRIPTION: Shows the NiFi property that needs to be set to specify which authorizer configured in authorizers.xml should be used. This property value must match the identifier of the desired authorizer element in the XML configuration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/3_tasks/2_security/2_authorization/1_custom_authorizer.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\nnifi.security.user.authorizer=custom-database-authorizer\n```\n\n----------------------------------------\n\nTITLE: Extracting SSL Certificates from a Kubernetes Secret - Console Commands\nDESCRIPTION: Provides shell commands to extract and decode the CA certificate, user certificate, and user private key from a generated Kubernetes secret. Uses kubectl and jsonpath queries to fetch base64-encoded data fields, then decodes and writes them to local files. Requires the secret to already exist in the cluster (created via NiFi Operator or as part of user creation). Suitable for integrating the credentials into external applications or for local testing.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['ca\\.crt']}\" | base64 -d > ca.crt\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['tls\\.crt']}\" | base64 -d > tls.crt\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['tls\\.key']}\" | base64 -d > tls.key\n\n```\n\n----------------------------------------\n\nTITLE: Installing KEDA with Helm\nDESCRIPTION: This command installs the KEDA Helm chart into the specified namespace (`keda`). It pulls the chart from the configured Helm repositories and deploys the KEDA components to the Kubernetes cluster.  The namespace `keda` is created before installation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl create namespace keda\nhelm install keda kedacore/keda --namespace keda\n```\n\n----------------------------------------\n\nTITLE: Retrieve Client SSL Credentials from Secret (Console)\nDESCRIPTION: These console commands use `kubectl get secret` to retrieve the contents of the `example-client-secret`, which was created by the NifiUser resource. They use `jsonpath` to extract the base64-encoded certificate and key data (`ca.crt`, `tls.crt`, `tls.key`) and pipe the output to `base64 -d` to decode it, saving the decoded content into local files (`ca.crt`, `tls.crt`, `tls.key`). Requires kubectl and the secret to exist.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['ca\\.crt']}\" | base64 -d > ca.crt\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['tls\\.crt']}\" | base64 -d > tls.crt\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['tls\\.key']}\" | base64 -d > tls.key\n```\n\n----------------------------------------\n\nTITLE: Packaging NiFiKop Helm Chart - Bash\nDESCRIPTION: Packages the NiFiKop operator Helm chart into a deployable format using a make command. This prepares the chart for distribution and installation from the helm repository 'konpyutaika-incubator'.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/6_contributing/1_developer_guide.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nmake helm-package\n```\n\n----------------------------------------\n\nTITLE: Creating Cert-Manager Issuer for Let's Encrypt - YAML\nDESCRIPTION: This YAML configuration creates a cert-manager Issuer resource for Let's Encrypt using the ACME protocol, configuring HTTP01 challenge solvers with custom ingress annotations. Dependencies include cert-manager installed with CRDs in the Kubernetes cluster, external DNS controller, and a valid email address for Let's Encrypt notifications. The output is an Issuer resource named letsencrypt-staging, ready to issue SSL certificates using your external DNS integration. Limitations: replace the email and ensure HTTP01 solving is correctly configured.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: cert-manager.io/v1alpha2\nkind: Issuer\nmetadata:\n  name: letsencrypt-staging\nspec:\n  acme:\n    # You must replace this email address with your own.\n    # Let's Encrypt will use this to contact you about expiring\n    # certificates, and issues related to your account.\n    email: <your email address>\n    server: https://acme-staging-v02.api.letsencrypt.org/directory\n    privateKeySecretRef:\n      # Secret resource used to store the account's private key.\n      name: example-issuer-account-key\n    # Add a single challenge solver, HTTP01 using nginx\n    solvers:\n      - http01:\n          ingress:\n            ingressTemplate:\n              metadata:\n                annotations:\n                  \"external-dns.alpha.kubernetes.io/ttl\": \"5\"\n```\n\n----------------------------------------\n\nTITLE: Specifying OIDC configuration in NifiKop Custom Resource (YAML)\nDESCRIPTION: This YAML snippet illustrates how to embed OIDC settings within a NifiCluster resource for NifiKop by overriding nifi.properties. It sets parameters such as the discovery URL, client ID, and secret, along with DN mapping patterns to facilitate OIDC integration in NiFi clusters managed via Kubernetes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/5_users_authentication/1_oidc.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  readOnlyConfig:\n    nifiProperties:\n      webProxyHosts:\n        - nifistandard2.trycatchlearn.fr:8443\n      overrideConfigs: |\n        nifi.security.user.oidc.discovery.url=<oidc server discovery url>\n        nifi.security.user.oidc.client.id=<oidc client's id>\n        nifi.security.user.oidc.client.secret=<oidc client's secret>\n        nifi.security.identity.mapping.pattern.dn=CN=([^,]*)(?:, (?:O|OU)=.*)?\n        nifi.security.identity.mapping.value.dn=$1\n        nifi.security.identity.mapping.transform.dn=NONE\n      ...\n...\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Secret Basic Auth Console\nDESCRIPTION: This console command demonstrates how to create a generic Kubernetes secret named 'nifikop-credentials' in the 'nifikop-nifi' namespace. The secret is populated from local files named 'username', 'password', and optionally 'ca.crt', providing the necessary credentials for NifiKop to authenticate with an external NiFi cluster using basic authentication.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/3_manage_nifi/1_manage_clusters/3_external_cluster.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nkubectl create secret generic nifikop-credentials \\\n  --from-file=username=./secrets/username\\\n  --from-file=password=./secrets/password\\\n  --from-file=ca.crt=./secrets/ca.crt\\\n  -n nifikop-nifi\n```\n\n----------------------------------------\n\nTITLE: Defining Istio DestinationRule for HTTPS in YAML\nDESCRIPTION: This YAML defines an Istio `DestinationRule` to manage sticky sessions and encrypt HTTP traffic back to HTTPS. The `host` targets a specific service. The `trafficPolicy` specifies TLS mode and configures a consistent hash load balancer for sticky sessions, based on the httpCookie property. Input is HTTP traffic from the VirtualService, and output is HTTPS traffic routed to the NiFi service. This ensures that users stay on the same NiFi node for a session.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\napiVersion: networking.istio.io/v1beta1\nkind: DestinationRule\nmetadata:\n  name: nifi-dr\nspec:\n  host: <service-name>.<namespace>.svc.cluster.local\n  trafficPolicy:\n    tls:\n      mode: SIMPLE\n    loadBalancer:\n      consistentHash:\n        httpCookie:\n          name: __Secure-Authorization-Bearer\n          ttl: 0s\n```\n```\n\n----------------------------------------\n\nTITLE: Defining NifiParameterContext resource - child\nDESCRIPTION: This YAML snippet defines a NifiParameterContext custom resource named `dataflow-lifecycle-child`.  It includes a description, a cluster reference, a secret reference, an inherited parameter context referencing `dataflow-lifecycle`, and one parameter (`test`). The `clusterRef` points to a NifiCluster named `nc` in the `nifikop` namespace. `secretRefs` points to a secret named `secret-params` in the `nifikop` namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/5_references/4_nifi_parameter_context.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle-child\nspec:\n  description: \"It is a child test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  inheritedParameterContexts:\n    - name: dataflow-lifecycle\n  parameters:\n    - name: test\n      value: toto-child\n      description: tutu (child)\n```\n\n----------------------------------------\n\nTITLE: Custom Authorizer Template for NiFi with YAML\nDESCRIPTION: This YAML snippet demonstrates a custom authorizer template designed to replace the default NiFi authorizer configuration using NiFiKOp. It defines custom `DatabaseUserGroupProvider` and `DatabaseAccessPolicyProvider` configurations alongside the default file-based providers. It uses Go templating to dynamically set node identities at deployment time.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/1_deploy_cluster/6_users_authorization/1_custom_user_authorizer.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n{{- $nodeList := .NodeList }}\n{{- $clusterName := .ClusterName }}\n{{- $namespace := .Namespace }}<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>\n<authorizers>\n    <userGroupProvider>\n        <identifier>file-user-group-provider</identifier>\n        <class>org.apache.nifi.authorization.FileUserGroupProvider</class>\n        <property name=\"Users File\">../data/users.xml</property>\n        <property name=\"Legacy Authorized Users File\"></property>\n        <property name=\"Initial User Identity admin\">{{ .ControllerUser }}</property>\n{{- range $i, $host := .NodeList }}\n        <property name=\"Initial User Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n    </userGroupProvider>\n    <userGroupProvider>\n        <identifier>database-user-group-provider</identifier>\n        <class>my.custom.DatabaseUserGroupProvider</class>\n        <!-- Any extra configuration for this provider goes here -->\n{{- range $i, $host := .NodeList }}\n        <property name=\"Initial User Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n    </userGroupProvider>\n    <accessPolicyProvider>\n        <identifier>file-access-policy-provider</identifier>\n        <class>org.apache.nifi.authorization.FileAccessPolicyProvider</class>\n        <property name=\"User Group Provider\">file-user-group-provider</property>\n        <property name=\"Authorizations File\">../data/authorizations.xml</property>\n        <property name=\"Initial Admin Identity\">{{ .ControllerUser }}</property>\n        <property name=\"Legacy Authorized Users File\"></property>\n{{- range $i, $host := .NodeList }}\n        <property name=\"Node Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n\t\t<property name=\"Node Group\"></property>\n    </accessPolicyProvider>\n    <accessPolicyProvider>\n        <identifier>database-access-policy-provider</identifier>\n        <class>my.custom.DatabaseAccessPolicyProvider</class>\n        <!-- Any extra configuration for this provider goes here -->\n{{- range $i, $host := .NodeList }}\n        <property name=\"Node Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n\t\t<property name=\"Node Group\"></property>\n    </accessPolicyProvider>\n    <authorizer>\n        <identifier>managed-authorizer</identifier>\n        <class>org.apache.nifi.authorization.StandardManagedAuthorizer</class>\n        <property name=\"Access Policy Provider\">file-access-policy-provider</property>\n    </authorizer>\n    <authorizer>\n        <identifier>custom-database-authorizer</identifier>\n        <class>my.custom.DatabaseAuthorizer</class>\n        <property name=\"Access Policy Provider\">database-access-policy-provider</property>\n    </authorizer>\n</authorizers>\n\n```\n\n----------------------------------------\n\nTITLE: Installing Nifikop Helm Chart Skipping CRDs (Bash)\nDESCRIPTION: Installs the Nifikop Helm chart from a local directory (`./helm/nifikop`) using `helm install`. It specifies the release name (`nifikop`), sets the target `namespaces`, and includes the `--skip-crds` flag to prevent Helm from installing or managing the CRDs. Requires CRDs to be applied manually beforehand.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nhelm install --name nifikop ./helm/nifikop --set namespaces={\"nifikop\"} --skip-crds\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiUserGroup Resource for User Group Management in Kubernetes\nDESCRIPTION: This YAML snippet defines a NifiUserGroup resource for managing user groups and access policies in NiFi via Kubernetes. It specifies the cluster reference, associated users, and access policies (like read access to counters). Dependencies include the NifiUserGroup custom resource definition and NiFi cluster resources. Parameters include metadata, clusterRef, usersRef, and accessPolicies, controlling group identity, members, and permissions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/2_manage_users_and_accesses/2_groups_management.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  # Contains the reference to the NifiCluster with the one the registry client is linked.\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  # contains the list of reference to NifiUsers that are part to the group.\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n#     namespace: nifikop\n    - name: nc-controller.nifikop.mgt.cluster.local\n  # defines the list of access policies that will be granted to the group.\n  accessPolicies:\n      # defines the kind of access policy, could be \"global\" or \"component\".\n    - type: global\n      # defines the kind of action that will be granted, could be \"read\" or \"write\"\n      action: read\n      # resource defines the kind of resource targeted by this access policies, please refer to the following page:\n      #\thttps://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#access-policies\n      resource: /counters\n#      # componentType is used if the type is \"component\", it's allow to define the kind of component on which is the\n#      # access policy\n#      componentType: \"process-groups\"\n#      # componentId is used if the type is \"component\", it's allow to define the id of the component on which is the\n#      # access policy\n#      componentId: \"\"\n```\n\n----------------------------------------\n\nTITLE: Setting the Active NiFi Authorizer Property (Shell)\nDESCRIPTION: This shell snippet shows how to set the `nifi.security.user.authorizer` property within NiFi's configuration (typically `nifi.properties`). This property specifies which authorizer, identified by its `identifier` in the `authorizers.xml` file, should be actively used by NiFi for managing user access and policies. In this example, it's set to `custom-database-authorizer` to activate the custom authorizer configured in the previous XML template.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/1_manage_clusters/1_deploy_cluster/6_users_authorization/1_custom_user_authorizer.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nnifi.security.user.authorizer=custom-database-authorizer\n```\n\n----------------------------------------\n\nTITLE: Getting UID/GID for NiFi Cluster on OpenShift\nDESCRIPTION: This bash script gets the UID/GID for the NiFi deployment on OpenShift using `kubectl` and `jsonpath`. This is used to set the `runAsUser` and `fsGroup` parameters for the NiFi cluster's pods on OpenShift. It retrieves this from OpenShift annotations. It depends on the correct OpenShift annotations being available, and that the `nifi` namespace exists.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nuid=$(kubectl get namespace nifi -o=jsonpath='{.metadata.annotations.openshift\\.io/sa\\.scc\\.supplemental-groups}' | sed 's//10000$//' | tr -d '[:space:]')\n```\n\n----------------------------------------\n\nTITLE: Manually Deleting Nifikop CRDs using kubectl\nDESCRIPTION: Manually removes the Nifikop Custom Resource Definitions (CRDs) from the Kubernetes cluster using `kubectl delete crd`. This is typically done after the associated Helm release is deleted, as the CRDs are not removed by `helm del` by default. **Caution:** Deleting a CRD will delete *all* custom resources defined by them. Requires `kubectl`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete crd nificlusters.nifi.konpyutaika.com\nkubectl delete crd nifiusers.nifi.konpyutaika.com\nkubectl delete crd nifiusergroups.nifi.konpyutaika.com\nkubectl delete crd nifiregistryclients.nifi.konpyutaika.com\nkubectl delete crd nifiparametercontexts.nifi.konpyutaika.com\nkubectl delete crd nifidataflows.nifi.konpyutaika.com\n```\n\n----------------------------------------\n\nTITLE: Defining StorageConfigs in YAML\nDESCRIPTION: This YAML snippet demonstrates how to define storage configurations using `storageConfigs` within a `NodeConfigGroup`. It configures persistent volumes for various NiFi data directories by defining a persistent volume claim (PVC) spec including access modes, resources requests, and storage class name. Each configuration specifies the mount path, storage claim configuration and metadata for a specific volume. This is critical to ensure data persistence for NiFi and avoids data loss when pods are deleted or restarted in Kubernetes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n... \nstorageConfigs:\n  - mountPath: /opt/nifi/nifi-current/logs\n    name: logs\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/data\n    name: data\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 50Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/extensions\n    name: extensions-repository\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 5Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/flowfile_repository\n    name: flowfile-repository\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/nifi-current/conf\n    name: conf\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 5Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/content_repository\n    name: content-repository\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 500Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/provenance_repository\n    name: provenance-repository\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 500Gi\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for the Operator\nDESCRIPTION: Builds a Docker image from the current branch of the NiFiKop project to be pushed to a container registry, facilitating Helm chart deployment with custom images.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/6_contributing/1_developer_guide.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nexport DOCKER_REPO_BASE={your-docker-repo}\nmake docker-build\n```\n\n----------------------------------------\n\nTITLE: Adding Helm Repo for KEDA Deployment\nDESCRIPTION: This snippet adds the KEDA Helm repository to the Helm configuration. It's a prerequisite step before installing KEDA via Helm.  The command specifies the repository name and the URL where the Helm charts are located.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nhelm repo add kedacore https://kedacore.github.io/charts\n```\n\n----------------------------------------\n\nTITLE: Patching CRDs for Webhook Conversion in Kubernetes Using YAML\nDESCRIPTION: This YAML snippet demonstrates how to patch NifiKop CRDs to enable conversion webhooks during migration from v0.16.0 to v1.0.0. It includes annotations for cert-manager to inject a CA certificate from a specified secret, and specifies a conversion strategy of 'Webhook' with client configuration details such as namespace, service name, and path. Key parameters like 'namespace', 'certificate_name', and 'webhook_service_name' must be set according to your Helm release deployment. Inputs are patch data for the CRDs, and this configuration must be applied after upgrading the Helm chart. Prerequisites include cert-manager and the relevant webhook services being deployed. Limitations: YAML is a patch and not a full resource definition; placeholders require substitution before deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/7_upgrade_guides/3_v0.16.0_to_v1.0.0.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n```\n\n----------------------------------------\n\nTITLE: Listing Kubernetes resources associated with the new node\nDESCRIPTION: The command retrieves the current pods, configmaps, and persistent volume claims associated with the node having 'nodeId=25' to verify its deployment and resource allocation within the Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get pods,configmap,pvc -l nodeId=25\n```\n\n----------------------------------------\n\nTITLE: Defining ServiceMonitor for NiFi Metrics (YAML)\nDESCRIPTION: YAML definition for a `ServiceMonitor` custom resource used by Prometheus Operator to discover and configure scraping for NiFi metrics. It targets services labeled `app: nifi` and `nifi_cr: cluster` within the `clusters` namespace, scrapes the `/metrics` endpoint on the `prometheus` port (9090) every 10 seconds, and uses relabeling rules to add pod IP, node ID, and NiFi cluster name labels to the metrics.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: cluster\n  namespace: monitoring-system\n  labels:\n    app: nifi-cluster\n    nifi_cr: cluster\nspec:\n  selector:\n    matchLabels:\n      app: nifi\n      nifi_cr: cluster\n  namespaceSelector:\n    matchNames:\n      - clusters\n  endpoints:\n    - interval: 10s\n      port: prometheus\n      path: /metrics\n      honorLabels: true\n      relabelings:\n        - sourceLabels: [__meta_kubernetes_pod_ip]\n          separator: ;\n          regex: (.*)\n          targetLabel: pod_ip\n          replacement: $1\n          action: replace\n        - sourceLabels: [__meta_kubernetes_pod_label_nodeId]\n          separator: ;\n          regex: (.*)\n          targetLabel: nodeId\n          replacement: $1\n          action: replace\n        - sourceLabels: [__meta_kubernetes_pod_label_nifi_cr]\n          separator: ;\n          regex: (.*)\n          targetLabel: nifi_cr\n          replacement: $1\n          action: replace\n```\n\n----------------------------------------\n\nTITLE: Deploy NiFi cluster from YAML (Bash)\nDESCRIPTION: This command deploys a NiFi cluster using a YAML configuration file (`config/samples/simplenificluster.yaml`). It assumes that you have already added your Zookeeper service name to the configuration file. It creates the resources in the `nifi` namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create -n nifi -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio DestinationRule for HTTPS Re-encryption and Sticky Sessions (YAML)\nDESCRIPTION: This YAML snippet defines an Istio DestinationRule for the internal NiFi service. It configures `SIMPLE` TLS mode to re-encrypt traffic before sending it to the destination port 8443. It also sets up a consistent hash load balancing policy based on the `__Secure-Authorization-Bearer` HTTP cookie to ensure sticky sessions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: DestinationRule\nmetadata:\n  name: nifi-dr\nspec:\n  host: <service-name>.<namespace>.svc.cluster.local\n  trafficPolicy:\n    tls:\n      mode: SIMPLE\n    loadBalancer:\n      consistentHash:\n        httpCookie:\n          name: __Secure-Authorization-Bearer\n          ttl: 0s\n```\n\n----------------------------------------\n\nTITLE: Purging a Helm Release\nDESCRIPTION: This snippet demonstrates how to purge a release in Helm, removing it completely and its history. This action is irreversible and requires Helm to be installed and configured.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nhelm delete --purge nifikop\n```\n\n----------------------------------------\n\nTITLE: Testing NiFiKop Kubectl Plugin Installation - Console\nDESCRIPTION: This command tests whether the installed 'kubectl-nifikop' plugin is available and functioning by invoking it via 'kubectl nifikop'. The expected output lists available subcommands and confirms successful installation. No additional dependencies are required beyond 'kubectl' and the installed plugin. Input is the shell command; output is a usage summary with available commands.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/2_deploy_nifikop/3_kubectl_plugin.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nkubectl nifikop\nUsage:\n  nifikop [command]\n\nAvailable Commands:\n  completion          Generate the autocompletion script for the specified shell\n  help                Help about any command\n  nificluster         \n  nificonnection      \n  nifidataflow        \n  nifigroupautoscaler \n  nifiregistryclient  \n  nifiuser            \n  nifiusergroup \n```\n\n----------------------------------------\n\nTITLE: Updating NiFi Configuration File for OpenShift\nDESCRIPTION: Sed command to replace the default UID in the OpenShift sample configuration file with the extracted UID from the namespace, ensuring proper security context for the NiFi cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nsed -i \"s/1000690000/$uid/g\" config/samples/openshift.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring LoadBalancer External Service in Kubernetes YAML\nDESCRIPTION: This YAML example demonstrates setting up an external service of type LoadBalancer with specified load balancer class, multiple port configurations including protocols, and metadata annotations/labels. It is used to expose services externally via cloud provider load balancers with custom configurations. Key parameters include the loadBalancerClass, ports, and traffic policies. Inputs include service name, ports, and metadata; the output is a Kubernetes LoadBalancer service object ready for deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/5_references/1_nifi_cluster/7_external_service_config.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nexternalServices:\n    - name: \"nlb\"\n      spec:\n        type: LoadBalancer\n        loadBalancerClass: \"service.k8s.aws/nlb\"\n        portConfigs:\n          - port: 8080\n            internalListenerName: \"http\"\n          - port: 7890\n            internalListenerName: \"my-custom-udp-listener\"\n            protocol: UDP\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiRegistryClient YAML\nDESCRIPTION: This YAML defines a NifiRegistryClient custom resource. It specifies the API version, kind, metadata (name and namespace), and the specification, including a reference to the NiFi cluster, a description, and the URI of the NiFi Registry instance. It requires a NiFi cluster to be already deployed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiRegistryClient\nmetadata:\n  name: registry-client-example\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Registry client managed by NiFiKop\"\n  uri: \"http://nifi.hostname.com:18080\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple Storage Volumes for NiFi Repositories (YAML)\nDESCRIPTION: Illustrates an advanced configuration for setting up multiple directories for NiFi's content and provenance repositories, often needed for high-performance installations. It shows how to override specific `nifi.properties` keys (`nifi.content.repository.directory.*`, `nifi.provenance.repository.directory.*`) using `readOnlyConfig.nifiProperties.overrideConfigs`. Subsequently, it defines the corresponding PersistentVolumeClaims (PVCs) and mount paths within `nodeConfigGroups.storageConfigs` to make these directories available to the NiFi pods.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n...\n  readOnlyConfig:\n    nifiProperties:\n      overrideConfigs: |\n        nifi.content.repository.directory.dir1=../content-additional/dir1\n        nifi.content.repository.directory.dir2=../content-additional/dir2\n        nifi.content.repository.directory.dir3=../content-additional/dir3\n        nifi.provenance.repository.directory.dir1=../provenance-additional/dir1\n        nifi.provenance.repository.directory.dir2=../provenance-additional/dir2\n...\n  nodeConfigGroups:\n    default_group:\n      ...\n      storageConfigs:\n      - mountPath: \"/opt/nifi/content-additional/dir1\"\n        name: content-repository-dir1\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/content-additional/dir2\"\n        name: content-repository-dir2\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/content-additional/dir3\"\n        name: content-repository-dir3\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/provenance-additional/dir1\"\n        name: provenance-repository-dir1\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/provenance-additional/dir2\"\n        name: provenance-repository-dir2\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      ...\n```\n\n----------------------------------------\n\nTITLE: NifiUser Creation (YAML)\nDESCRIPTION: This YAML snippet defines a NifiUser resource to create a new user certificate for a NiFi cluster. It specifies the `clusterRef` to link the user to the target cluster and `secretName` to store the generated credentials.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion:  nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: example-client\n  namespace: nifi\nspec:\n  clusterRef:\n    name: nifi\n  secretName: example-client-secret\nEOF\n```\n\n----------------------------------------\n\nTITLE: Example External Service Configuration (YAML)\nDESCRIPTION: This YAML snippet demonstrates defining an external service named 'clusterip' of type `ClusterIP`. It configures the service to expose port 8080, forwarding traffic to the internal NiFi listener named 'http'. Additionally, it shows how to apply custom annotations and labels to the generated Kubernetes Service object via the `metadata` field.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/5_references/1_nifi_cluster/7_external_service_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  externalServices:\n    - name: \"clusterip\"\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8080\n            internalListenerName: \"http\"\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n\n----------------------------------------\n\nTITLE: Creating StorageClass for Kubernetes\nDESCRIPTION: This YAML snippet defines a Kubernetes StorageClass.  It sets the `volumeBindingMode` to `WaitForFirstConsumer` to ensure volumes are created only when pods are scheduled.  The `provisioner` and `type` parameters specify the storage provider and volume type, respectively.  Dependencies include a running Kubernetes cluster and a configured storage provider.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: exampleStorageclass\nparameters:\n  type: pd-standard\nprovisioner: kubernetes.io/gce-pd\nreclaimPolicy: Delete\nvolumeBindingMode: WaitForFirstConsumer\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiConnection between Dataflows in YAML\nDESCRIPTION: Defines a NifiConnection Custom Resource (CR) named 'connection' within the 'nifikop' namespace to link the 'output' port of the 'input' NifiDataflow to the 'input' port of the 'output' NifiDataflow. It specifies source and destination details, configuration like back pressure thresholds, flow file expiration, visual layout ('bends' coordinates and 'labelIndex'), and the update strategy ('drain').\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/4_manage_connections/1_deploy_connection.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiConnection\nmetadata:\n  name: connection\n  namespace: nifikop\nspec:\n  source:\n    name: input\n    namespace: nifikop\n    subName: output\n    type: dataflow\n  destination:\n    name: output\n    namespace: nifikop\n    subName: input\n    type: dataflow\n  configuration:\n    backPressureDataSizeThreshold: 100 GB\n    backPressureObjectThreshold: 10000\n    flowFileExpiration: 1 hour\n    labelIndex: 0\n    bends:\n      - posX: 550\n        posY: 550\n      - posX: 550\n        posY: 440\n      - posX: 550\n        posY: 88\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Configuring Nifi Listeners with SSL in YAML\nDESCRIPTION: Example YAML configuration for Nifi listeners, including HTTPS, cluster, site-to-site (s2s), Prometheus, and load balancing ports. Also demonstrates SSL secret configuration with cert-manager integration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/5_references/1_nifi_cluster/6_listeners_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  listenersConfig:\n    internalListeners:\n      - type: \"https\"\n        name: \"https\"\n        containerPort: 8443\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n      - type: \"prometheus\"\n        name: \"prometheus\"\n        containerPort: 9090\n      - type: \"load-balance\"\n        name: \"load-balance\"\n        containerPort: 6342\n      - name: \"my-custom-listener-port\"\n        containerPort: 1234\n        protocol: \"TCP\"\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n```\n\n----------------------------------------\n\nTITLE: NiFiCluster SSL Configuration\nDESCRIPTION: This YAML snippet demonstrates how to configure SSL for a NiFi cluster using the `NifiCluster` resource. It sets up internal listeners, specifies SSL secrets, and configures web proxy hosts. `tlsSecretName` defines the secret to use for SSL certificates, and `create: true` instructs the operator to create the secret.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  readOnlyConfig:\n    # NifiProperties configuration that will be applied to the node.\n    nifiProperties:\n      webProxyHosts:\n        - nifistandard2.trycatchlearn.fr:8443\n        ...\n  ...\n  listenersConfig:\n    internalListeners:\n      - type: \"https\"\n        name: \"https\"\n        containerPort: 8443\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n```\n\n----------------------------------------\n\nTITLE: Applying NiFiCluster Configuration (Bash)\nDESCRIPTION: This command applies the modified NiFiCluster configuration (in YAML format) to the Kubernetes cluster using `kubectl`. The `-n nifi` flag specifies the namespace. It requires a running Kubernetes cluster and `kubectl` configured to interact with it, and the NiFi cluster configuration file (e.g., `config/samples/simplenificluster.yaml`).  The command applies the changes described in the YAML configuration file.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n```sh \nkubectl -n nifi apply -f config/samples/simplenificluster.yaml\n```\n```\n\n----------------------------------------\n\nTITLE: Scaling Up a NiFi Cluster by Adding a Node in YAML\nDESCRIPTION: Defines a `NifiCluster` custom resource configuration in YAML to scale up a NiFi cluster by adding a new node (ID 25) with the 'default_group' configuration. This configuration specifies cluster details like ZooKeeper address, image, node groups, storage, resources, listeners, and the updated list of nodes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.12.1\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n    - id: 2\n      nodeConfigGroup: \"default_group\"\n# >>>> START: The new node\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n# <<<< END\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: Removing a node from NiFiCluster with YAML configuration\nDESCRIPTION: This YAML snippet updates the NiFiCluster configuration by removing the node with 'id: 2' from the 'nodes' list. It maintains other nodes and cluster settings, facilitating an orderly decommissioning. Applying this configuration begins the node removal process, initiating decommission steps such as data offloading and pod termination.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  headlessServiceEnabled: true\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.11.3\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n# >>>> START: node removed\n#    - id: 2\n#      nodeConfigGroup: \"default_group\"\n# <<<< END\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: Defining NifiNodeGroupAutoscaler Resource (YAML)\nDESCRIPTION: YAML definition for a `NifiNodeGroupAutoscaler` custom resource provided by NiFiKop. It specifies the target NiFi cluster (`clusterRef`), the `nodeConfigGroupId` ('auto_scaling') to manage, optional read-only configurations, a label selector to identify the nodes to autoscale, and the strategies for upscaling ('simple') and downscaling ('lifo').\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiNodeGroupAutoscaler\nmetadata:\n  name: nifinodegroupautoscaler-sample\n  namespace: clusters\nspec:\n  # contains the reference to the NifiCluster with the one the node group autoscaler is linked.\n  clusterRef:\n    name: cluster\n    namespace: clusters\n  # defines the id of the NodeConfig contained in NifiCluster.Spec.NodeConfigGroups\n  nodeConfigGroupId: auto_scaling\n  # readOnlyConfig can be used to pass Nifi node config\n  # which has type read-only these config changes will trigger rolling upgrade\n  readOnlyConfig:\n    nifiProperties:\n      overrideConfigs: |\n        nifi.ui.banner.text=NiFiKop - Scale Group\n  # This is an example of a node config you can apply to each replica in this node group.\n  # Any settings here will override those in the configured nodeConfigGroupId\n#  nodeConfig:\n#    nodeSelector:\n#      node_type: high-mem\n  # The selector used to identify nodes in NifiCluster.Spec.Nodes this autoscaler will manage\n  # Use Node.Labels in combination with this selector to clearly define which nodes will be managed by this autoscaler \n  nodeLabelsSelector: \n    matchLabels:\n      nifi_cr: cluster\n      nifi_node_group: auto-scaling\n  # the strategy used to decide how to add nodes to a nifi cluster\n  upscaleStrategy: simple\n  # the strategy used to decide how to remove nodes from an existing cluster\n  downscaleStrategy: lifo\n```\n\n----------------------------------------\n\nTITLE: Configuring External NiFi Cluster with YAML in Kubernetes\nDESCRIPTION: This YAML snippet demonstrates how to declare an external NiFi cluster resource that can be used to manage dataflows. It specifies the root process group ID, node URI template, node IDs, authentication type, and secret reference for connecting to an external NiFi cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/3_manage_nifi/1_manage_clusters/3_external_cluster.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: externalcluster\nspec:\n  # rootProcessGroupId contains the uuid of the root process group for this cluster.\n  rootProcessGroupId: 'd37bee03-017a-1000-cff7-4eaaa82266b7'\n  # nodeURITemplate used to dynamically compute node uri.\n  nodeURITemplate: 'nifi0%d.integ.mapreduce.m0.p.fti.net:9090'\n  # all node requiresunique id\n  nodes:\n    - id: 1\n    - id: 2\n    - id: 3\n  # type defines if the cluster is internal (i.e manager by the operator) or external.\n  # :Enum={\"external\",\"internal\"}\n  type: 'external'\n  # clientType defines if the operator will use basic or tls authentication to query the NiFi cluster.\n  # Enum={\"tls\",\"basic\"}\n  clientType: 'basic'\n  # secretRef reference the secret containing the informations required to authenticate to the cluster.\n  secretRef:\n    name: nifikop-credentials\n    namespace: nifikop-nifi\n```\n\n----------------------------------------\n\nTITLE: Defining Nifi Listeners Configuration in YAML\nDESCRIPTION: This YAML snippet demonstrates the structure of the `listenersConfig` object. It defines multiple `internalListeners` (https, cluster, s2s, prometheus) with their respective types, names, and container ports. It also includes an `sslSecrets` section specifying the TLS secret name (`test-nifikop`) and enabling its creation (`create: true`) for secure communication.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/5_references/1_nifi_cluster/6_listeners_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  listenersConfig:\n    internalListeners:\n      - type: \"https\"\n        name: \"https\"\n        containerPort: 8443\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n      - type: \"prometheus\"\n        name: \"prometheus\"\n        containerPort: 9090\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n```\n\n----------------------------------------\n\nTITLE: Configuring Internal Listeners in NiFi Kubernetes Cluster - YAML\nDESCRIPTION: This YAML snippet demonstrates how to set up the listenersConfig.internalListeners field for a NiFi cluster on Kubernetes. It declares the ports and names for six different internal listener types, specifying the communication channels required for cluster operation (HTTPS UI, inter-node communication, Site-to-Site, load balancing, and Prometheus monitoring). No external dependencies are required beyond Kubernetes cluster access and nifikop CRDs. Key parameters: type (listener classification), name (port identifier), containerPort (port number inside the pod). Valid types include https, cluster, s2s, prometheus, and load-balance. The expected output is correct exposure of named ports in each NiFi pod.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    - type: \"https\"\n      name: \"https\"\n      containerPort: 8443\n    - type: \"cluster\"\n      name: \"cluster\"\n      containerPort: 6007\n    - type: \"s2s\"\n      name: \"s2s\"\n      containerPort: 10000\n    - type: \"prometheus\"\n      name: \"prometheus\"\n      containerPort: 9090\n    - type: \"load-balance\"\n      name: \"load-balance\"\n      containerPort: 6342\n```\n\n----------------------------------------\n\nTITLE: Configuring Cluster-Wide Read-Only NiFi Properties in YAML\nDESCRIPTION: This YAML snippet illustrates the structure and key fields for the `readOnlyConfig` section of a NifiKop `NifiCluster` custom resource. It defines settings like maximum thread counts and provides various mechanisms (`overrideConfigs`, `overrideConfigMap`, `overrideSecretConfig`) to customize logging, authorizer, and core NiFi, Zookeeper, and Bootstrap properties using inline values or references to Kubernetes ConfigMaps and Secrets.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/5_references/1_nifi_cluster/2_read_only_config.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\nreadOnlyConfig:\n  # MaximumTimerDrivenThreadCount define the maximum number of threads for timer driven processors available to the system.\n  maximumTimerDrivenThreadCount: 30\n  # MaximumEventDrivenThreadCount define the maximum number of threads for event driven processors available to the system.\n  maximumEventDrivenThreadCount: 10\n  # Logback configuration that will be applied to the node\n  logbackConfig:\n    # logback.xml configuration that will replace the one produced based on template\n    replaceConfigMap:\n      # The key of the value,in data content, that we want use.\n      data: logback.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # logback.xml configuration that will replace the one produced based on template and overrideConfigMap\n    replaceSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: logback.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n  # Authorizer configuration that will be applied to the node\n  authorizerConfig:\n    # An authorizers.xml configuration template that will replace the default template seen in authorizers.go\n    replaceTemplateConfigMap:\n      # The key of the value, in data content, that we want use.\n      data: authorizers.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # An authorizers.xml configuration template that will replace the default template seen in authorizers.go and the replaceTemplateConfigMap\n    replaceTemplateSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: authorizers.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n  # NifiProperties configuration that will be applied to the node.\n  nifiProperties:\n    # Additionnal nifi.properties configuration that will override the one produced based on template and\n    # configuration\n    overrideConfigMap:\n      # The key of the value,in data content, that we want use.\n      data: nifi.properties\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop.\n    # Additionnal nifi.properties configuration that will override the one produced based\n    #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    overrideSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: nifi.properties\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # Additionnal nifi.properties configuration that will override the one produced based\n    #\ton template, configurations and overrideConfigMap\n    overrideConfigs: |\n      nifi.ui.banner.text=NiFiKop\n    # A comma separated list of allowed HTTP Host header values to consider when NiFi\n    # is running securely and will be receiving requests to a different host[:port] than it is bound to.\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#web-properties\n    #      webProxyHosts:\n    # Nifi security client auth\n    needClientAuth: false\n    # Indicates which of the configured authorizers in the authorizers.xml file to use\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#authorizer-configuration\n  #      authorizer:\n  # ZookeeperProperties configuration that will be applied to the node.\n  zookeeperProperties:\n    #      # Additionnal zookeeeper.properties configuration that will override the one produced based on template and\n    #      # configuration\n    #      overrideConfigMap:\n    #        # The key of the value,in data content, that we want use.\n    #        data: zookeeeper.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop.\n    #      # Additionnal zookeeeper.properties configuration that will override the one produced based\n    #      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    #      overrideSecretConfig:\n    #        # The key of the value,in data content, that we want use.\n    #        data: zookeeeper.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop\n    # Additionnal zookeeper.properties configuration that will override the one produced based\n    # on template and configurations.\n    overrideConfigs: |\n      initLimit=15\n      autopurge.purgeInterval=24\n      syncLimit=5\n      tickTime=2000\n      dataDir=./state/zookeeper\n      autopurge.snapRetainCount=30\n  # BootstrapProperties configuration that will be applied to the node.\n  bootstrapProperties:\n    #      # Additionnal bootstrap.properties configuration that will override the one produced based on template and\n    #      # configuration\n    #      overrideConfigMap:\n    #        # The key of the value,in data content, that we want use.\n    #        data: bootstrap.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop.\n    #      # Additionnal bootstrap.properties configuration that will override the one produced based\n    #      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    #      overrideSecretConfig:\n    #        # The key of the value,in data content, that we want use.\n    #        data: bootstrap.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop\n    # JVM memory settings\n    nifiJvmMemory: \"512m\"\n    # Additionnal bootstrap.properties configuration that will override the one produced based\n    # on template and configurations.\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#bootstrap_properties\n    overrideConfigs: |\n      # java.arg.4=-Djava.net.preferIPv4Stack=true\n```\n\n----------------------------------------\n\nTITLE: Creating NifiConnection Resource via NiFiKop YAML\nDESCRIPTION: This YAML defines a NifiConnection resource named 'connection' that links the previously defined 'input' and 'output' NifiDataflows. It specifies the source and destination dataflows by name and namespace, importantly indicating the specific output ('output') and input ('input') ports to connect using the 'subName' field. The 'configuration' block sets connection parameters like back pressure thresholds and visual bends on the NiFi canvas.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/4_manage_connections/1_deploy_connection.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiConnection\nmetadata:\n  name: connection\n  namespace: nifikop\nspec:\n  source:\n    name: input\n    namespace: nifikop\n    subName: output\n    type: dataflow\n  destination:\n    name: output\n    namespace: nifikop\n    subName: input\n    type: dataflow\n  configuration:\n    backPressureDataSizeThreshold: 100 GB\n    backPressureObjectThreshold: 10000\n    flowFileExpiration: 1 hour\n    labelIndex: 0\n    bends:\n      - posX: 550\n        posY: 550\n      - posX: 550\n        posY: 440\n      - posX: 550\n        posY: 88\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Deploying Prometheus Resource CRD - yaml\nDESCRIPTION: Configures a Prometheus custom resource in the monitoring-system namespace using the 'monitoring.coreos.com/v1' API. This resource controls Prometheus deployment settings, such as scrape intervals, evaluation intervals, logging, resource requests, service account, and selectors for monitoring NiFi services. Requires Prometheus Operator CRDs to be installed. The spec block allows detailed configuration; outputs are custom Prometheus pods and configuration in the namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: Prometheus\nmetadata:\n  name: prometheus\n  namespace: monitoring-system\nspec:\n  enableAdminAPI: false\n  evaluationInterval: 30s\n  logLevel: debug\n  podMonitorSelector:\n    matchExpressions:\n    - key: release\n      operator: In\n      values:\n      - prometheus\n  resources:\n    requests:\n      memory: 400Mi\n  scrapeInterval: 30s\n  serviceAccountName: prometheus\n  serviceMonitorSelector:\n    matchExpressions:\n    - key: app\n      operator: In\n      values:\n      - nifi-cluster\n\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop Helm Chart with Custom Image Tag - Bash\nDESCRIPTION: Deploys the NiFiKop operator Helm chart into the 'nifikop' Kubernetes namespace specifying the operator image tag 'v0.5.1-release'. The image.repository and image.tag must match the Docker image pushed previously. This requires Helm installed and configured with permission to install in the target namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/6_contributing/1_developer_guide.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nhelm install skeleton ./helm/nifikop \\\n    --set image.tag=v0.5.1-release \\\n    --namespace-{\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring Nifi Listeners (YAML)\nDESCRIPTION: This YAML snippet configures the various listener types for a Nifi instance, specifying their names, container ports, and SSL settings.  It defines internal listeners such as https, cluster, s2s, prometheus, and load-balance. It also configures SSL secrets by specifying the name of the TLS secret and a flag indicating whether to create it.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/5_references/1_nifi_cluster/6_listeners_config.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n  listenersConfig:\n    internalListeners:\n      - type: \"https\"\n        name: \"https\"\n        containerPort: 8443\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n      - type: \"prometheus\"\n        name: \"prometheus\"\n        containerPort: 9090\n      - type: \"load-balance\"\n        name: \"load-balance\"\n        containerPort: 6342\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n```\n\n----------------------------------------\n\nTITLE: Defining NifiParameterContext in Kubernetes\nDESCRIPTION: This YAML configuration defines a NifiParameterContext resource in Kubernetes. It includes a description, cluster reference, secret references, and parameter definitions. NiFiKop will convert this into a NiFi Parameter Context.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\n  namespace: demo\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      value: toto\n      description: toto\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop Helm Chart via Console\nDESCRIPTION: This snippet demonstrates how to install the NiFiKop Helm chart using the Helm client through the command line. It assumes Helm version 3 or higher is installed and ready to use. The command installs the chart with a custom values.yaml file to override default configuration parameters, enabling tailored deployment of the NiFi Kubernetes operator.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n$ helm install nifikop \\\n      konpyutaika/nifikop \\\n      -f values.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFiKop Read-Only Settings YAML\nDESCRIPTION: This YAML snippet demonstrates how to define the `readOnlyConfig` section within a NiFiKop `NifiCluster` custom resource. It shows examples of setting global thread counts, configuring logback, authorizer, nifi.properties, zookeeper.properties, and bootstrap.conf either inline or by referencing Kubernetes ConfigMaps/Secrets. This configuration applies cluster-wide but can be overridden by node-specific settings.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/5_references/1_nifi_cluster/2_read_only_config.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\nreadOnlyConfig:\n  # MaximumTimerDrivenThreadCount define the maximum number of threads for timer driven processors available to the system.\n  maximumTimerDrivenThreadCount: 30\n  # MaximumEventDrivenThreadCount define the maximum number of threads for event driven processors available to the system (@DEPRECATED. This has no effect from NiFiKOp v1.9.0 or later).\n  maximumEventDrivenThreadCount: 10\n  # Logback configuration that will be applied to the node\n  logbackConfig:\n    # logback.xml configuration that will replace the one produced based on template\n    replaceConfigMap:\n      # The key of the value,in data content, that we want use.\n      data: logback.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # logback.xml configuration that will replace the one produced based on template and overrideConfigMap\n    replaceSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: logback.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n  # Authorizer configuration that will be applied to the node\n  authorizerConfig:\n    # An authorizers.xml configuration template that will replace the default template seen in authorizers.go\n    replaceTemplateConfigMap:\n      # The key of the value, in data content, that we want use.\n      data: authorizers.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # An authorizers.xml configuration template that will replace the default template seen in authorizers.go and the replaceTemplateConfigMap\n    replaceTemplateSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: authorizers.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n  # NifiProperties configuration that will be applied to the node.\n  nifiProperties:\n    # Additionnal nifi.properties configuration that will override the one produced based on template and\n    # configuration\n    overrideConfigMap:\n      # The key of the value,in data content, that we want use.\n      data: nifi.properties\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop.\n    # Additionnal nifi.properties configuration that will override the one produced based\n    #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    overrideSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: nifi.properties\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # Additionnal nifi.properties configuration that will override the one produced based\n    #\ton template, configurations and overrideConfigMap\n    overrideConfigs: |\n      nifi.ui.banner.text=NiFiKop\n    # A comma separated list of allowed HTTP Host header values to consider when NiFi\n    # is running securely and will be receiving requests to a different host[:port] than it is bound to.\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#web-properties\n    #      webProxyHosts:\n    # Nifi security client auth\n    needClientAuth: false\n    # Indicates which of the configured authorizers in the authorizers.xml file to use\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#authorizer-configuration\n  #      authorizer:\n  # ZookeeperProperties configuration that will be applied to the node.\n  zookeeperProperties:\n    #      # Additionnal zookeeeper.properties configuration that will override the one produced based on template and\n    #      # configuration\n    #      overrideConfigMap:\n    #        # The key of the value,in data content, that we want use.\n    #        data: zookeeeper.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop.\n    #      # Additionnal zookeeeper.properties configuration that will override the one produced based\n    #      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    #      overrideSecretConfig:\n    #        # The key of the value,in data content, that we want use.\n    #        data: zookeeeper.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop\n    # Additionnal zookeeper.properties configuration that will override the one produced based\n    # on template and configurations.\n    overrideConfigs: |\n      initLimit=15\n      autopurge.purgeInterval=24\n      syncLimit=5\n      tickTime=2000\n      dataDir=./state/zookeeper\n      autopurge.snapRetainCount=30\n  # BootstrapProperties configuration that will be applied to the node.\n  bootstrapProperties:\n    #      # Additionnal bootstrap.conf configuration that will override the one produced based on template and\n    #      # configuration\n    #      overrideConfigMap:\n    #        # The key of the value,in data content, that we want use.\n    #        data: bootstrap.conf\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop.\n    #      # Additionnal bootstrap.conf configuration that will override the one produced based\n    #      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    #      overrideSecretConfig:\n    #        # The key of the value,in data content, that we want use.\n    #        data: bootstrap.conf\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop\n    # JVM memory settings\n    nifiJvmMemory: \"512m\"\n    # Additionnal bootstrap.conf configuration that will override the one produced based\n    # on template and configurations.\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#bootstrap_properties\n    overrideConfigs: |\n      # java.arg.4=-Djava.net.preferIPv4Stack=true\n```\n\n----------------------------------------\n\nTITLE: Defining Multiple NodeConfigGroups in NiFiKop YAML\nDESCRIPTION: Illustrates how to define multiple node configuration groups, specifying parameters like provenance storage, user ID, service account, and resource requirements. Enables customized node setups within a NiFi cluster deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\nnodeConfigGroups:\n  default_group:\n    provenanceStorage: \"10 GB\"\n    runAsUser: 1000\n    serviceAccountName: \"default\"\n    resourcesRequirements:\n      limits:\n        cpu: \"2\"\n        memory: 3Gi\n      requests:\n        cpu: \"1\"\n        memory: 3Gi\n  high_mem_group:\n    provenanceStorage: \"10 GB\"\n    runAsUser: 1000\n    serviceAccountName: \"default\"\n    resourcesRequirements:\n      limits:\n        cpu: \"2\"\n        memory: 30Gi\n      requests:\n        cpu: \"1\"\n        memory: 30Gi\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Node Configuration Groups (NiFiKop YAML)\nDESCRIPTION: This YAML snippet defines reusable node configurations (`NodeConfigGroups`) within a NiFiCluster resource specification. It shows two examples, 'default_group' and 'high_mem_group', primarily differentiating resource requirements (CPU/Memory) while including common settings for provenance storage, user, and service account. These defined groups can then be referenced by individual nodes to apply the configuration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  # nodeConfigGroups specifies multiple node configs with unique name\n  nodeConfigGroups:\n    default_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      serviceAccountName: \"default\"\n      # resourceRequirements works exactly like Container resources, the user can specify the limit and the requests\n      # through this property\n      # https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 3Gi\n    high_mem_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      serviceAccountName: \"default\"\n      # resourceRequirements works exactly like Container resources, the user can specify the limit and the requests\n      # through this property\n      # https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 30Gi\n        requests:\n          cpu: \"1\"\n          memory: 30Gi\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Secret for Sensitive Parameters Using Console Command\nDESCRIPTION: Demonstrates how to create a Kubernetes generic secret containing sensitive parameters that can be referenced by NifiParameterContext CRDs. The command includes literal key-value pairs representing sensitive data. This secret must be in the specified namespace and formatted to be compatible with NiFiKop's sensitive parameter handling.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/3_tasks/3_nifi_dataflow.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl create secret generic secret-params \\\n    --from-literal=secret1=yop \\\n    --from-literal=secret2=yep \\\n    -n nifikop\n```\n\n----------------------------------------\n\nTITLE: Monitoring node decommission status in NiFiCluster\nDESCRIPTION: This command describes the 'simplenifi' NiFiCluster resource to monitor the status of the node decommissioning process. It shows the current node states, including whether a node has entered 'GracefulDownscaleRequired' state, indicating ongoing decommission steps and guiding operational decisions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_5\n\nLANGUAGE: Shell\nCODE:\n```\nkubectl describe nificluster simplenifi\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Internal Listeners in YAML\nDESCRIPTION: YAML configuration for setting up internal listeners in a NiFi cluster, defining various communication ports including HTTPS, cluster communication, site-to-site, Prometheus metrics, and load balancing.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    - type: \"https\"\n      name: \"https\"\n      containerPort: 8443\n    - type: \"cluster\"\n      name: \"cluster\"\n      containerPort: 6007\n    - type: \"s2s\"\n      name: \"s2s\"\n      containerPort: 10000\n    - type: \"prometheus\"\n      name: \"prometheus\"\n      containerPort: 9090\n    - type: \"load-balance\"\n      name: \"load-balance\"\n      containerPort: 6342\n```\n\n----------------------------------------\n\nTITLE: Defining Nifi Listener Types and SSL Secrets in ListenersConfig YAML\nDESCRIPTION: This YAML snippet configures multiple Nifi internal listeners (https, cluster, s2s, prometheus, load-balance) by specifying their types, names, and container ports under the 'internalListeners' key, and sets up SSL secrets management with the 'sslSecrets' block. Dependencies include a running Kubernetes environment and appropriate cert-manager setup for handling TLS secrets. Inputs required are listener type definitions, port assignments, and TLS secret parameters, while outputs are a Nifi cluster with all configured endpoints available and TLS certificates managed according to the 'create' flag. Limitations may arise if required keys are missing or misconfigured, potentially preventing Nifi startup or secure communication.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/5_references/1_nifi_cluster/6_listeners_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  listenersConfig:\n    internalListeners:\n      - type: \"https\"\n        name: \"https\"\n        containerPort: 8443\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n      - type: \"prometheus\"\n        name: \"prometheus\"\n        containerPort: 9090\n      - type: \"load-balance\"\n        name: \"load-balance\"\n        containerPort: 6342\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Internal Listeners in YAML\nDESCRIPTION: Defines the internal ports used by NiFi components for cluster communication, UI access, Site-to-Site, load balancing, and Prometheus monitoring within the NiFiCluster custom resource specification. Each listener requires a type (except for custom listeners), a name, and a container port.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    - type: \"https\"\n      name: \"https\"\n      containerPort: 8443\n    - type: \"cluster\"\n      name: \"cluster\"\n      containerPort: 6007\n    - type: \"s2s\"\n      name: \"s2s\"\n      containerPort: 10000\n    - type: \"prometheus\"\n      name: \"prometheus\"\n      containerPort: 9090\n    - type: \"load-balance\"\n      name: \"load-balance\"\n      containerPort: 6342\n```\n\n----------------------------------------\n\nTITLE: Deploying NifiDataflow in Kubernetes\nDESCRIPTION: This YAML configuration defines a NifiDataflow resource in Kubernetes. It specifies the parent process group ID, bucket ID, flow ID, flow version, synchronization mode, cluster reference, registry client reference, parameter context reference, and update strategy.  These parameters tell NiFiKop how to manage the dataflow.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: demo\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Defining a NiFi Connection in YAML\nDESCRIPTION: This YAML snippet defines a NifiConnection resource, specifying the source and destination components, along with configuration details such as flow file expiration, back pressure thresholds, and load balancing strategies. It also includes prioritizers and bend points for the connection.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/5_references/8_nifi_connection.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiConnection\nmetadata:\n  name: connection\n  namespace: instances\nspec:\n  source:\n    name: input\n    namespace: instances\n    subName: output_1\n    type: dataflow\n  destination:\n    name: output\n    namespace: instances\n    subName: input_1\n    type: dataflow\n  configuration:\n    flowFileExpiration: 1 hour\n    backPressureDataSizeThreshold: 100 GB\n    backPressureObjectThreshold: 10000\n    loadBalanceStrategy: PARTITION_BY_ATTRIBUTE\n    loadBalancePartitionAttribute: partition_attribute\n    loadBalanceCompression: DO_NOT_COMPRESS\n    prioritizers: \n      - NewestFlowFileFirstPrioritizer\n      - FirstInFirstOutPrioritizer\n    labelIndex: 0\n    bends:\n      - posX: 550\n        posY: 550\n      - posX: 550\n        posY: 440\n      - posX: 550\n        posY: 88\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Cluster for Autoscaling and Prometheus\nDESCRIPTION: YAML snippet showing additions to a NiFi cluster custom resource specification. It enables Prometheus metrics scraping by adding a listener on port 9090 with type 'prometheus' under `listenersConfig.internalListeners`. It also defines a specific `nodeConfigGroups` named 'auto_scaling' with defined resource requests/limits and dedicated storage configurations, intended for nodes managed by KEDA autoscaling.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n...\nspec:\n  ...\n  listenersConfig:\n    internalListeners:\n    ...\n    - containerPort: 9090\n      name: prometheus\n      type: prometheus \n    ...\n  ...\n  nodeConfigGroups:\n    auto_scaling:\n      isNode: true\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 2Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n      serviceAccountName: external-dns\n      storageConfigs:\n        - mountPath: /opt/nifi/nifi-current/logs\n          name: logs\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/data\n          name: data\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/extensions\n          name: extensions-repository\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/flowfile_repository\n          name: flowfile-repository\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/nifi-current/conf\n          name: conf\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/content_repository\n          name: content-repository\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/provenance_repository\n          name: provenance-repository\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n  ...\n```\n\n----------------------------------------\n\nTITLE: Defining a NiFi Connection CRD Instance in Kubernetes - YAML\nDESCRIPTION: This YAML snippet shows how to define a NifiConnection Kubernetes Custom Resource for managing connections between NiFi components via the nifikop operator. It specifies both source and destination, detailed configuration options for connection behavior (such as back pressure, load balancing, and prioritizers), and the update strategy. The resource must be applied to a Kubernetes cluster where the nifikop operator is installed, and the referenced sources/destinations must exist. Key parameters include connection direction, thresholds, and queue strategies. Expected input is a valid YAML applied to the target namespace, producing a connection object managed by the operator. This resource is constrained to existing namespaces/components and requires appropriate CRD/cluster permissions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/5_references/8_nifi_connection.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiConnection\nmetadata:\n  name: connection\n  namespace: instances\nspec:\n  source:\n    name: input\n    namespace: instances\n    subName: output_1\n    type: dataflow\n  destination:\n    name: output\n    namespace: instances\n    subName: input_1\n    type: dataflow\n  configuration:\n    flowFileExpiration: 1 hour\n    backPressureDataSizeThreshold: 100 GB\n    backPressureObjectThreshold: 10000\n    loadBalanceStrategy: PARTITION_BY_ATTRIBUTE\n    loadBalancePartitionAttribute: partition_attribute\n    loadBalanceCompression: DO_NOT_COMPRESS\n    prioritizers: \n      - NewestFlowFileFirstPrioritizer\n      - FirstInFirstOutPrioritizer\n    labelIndex: 0\n    bends:\n      - posX: 550\n        posY: 550\n      - posX: 550\n        posY: 440\n      - posX: 550\n        posY: 88\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Secret for Sensitive Parameters using kubectl Console Command\nDESCRIPTION: This console command creates a Kubernetes generic secret named 'secret-params' in the 'nifikop' namespace. The secret contains literal key-value pairs representing sensitive parameters (such as 'secret1' and 'secret2') which are referenced by the NifiParameterContext resource. This facilitates secure storage and injection of sensitive data into NiFi parameter contexts managed by NiFiKop. It requires kubectl configured with access to the target Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl create secret generic secret-params \\\n    --from-literal=secret1=yop \\\n    --from-literal=secret2=yep \\\n    -n nifikop\n```\n\n----------------------------------------\n\nTITLE: Configuring NifiParameterContext Resource in YAML\nDESCRIPTION: Example of a NifiParameterContext custom resource that defines parameters for NiFi dataflows. This allows for configuration of dataflows with both regular and sensitive parameters that can be referenced from secrets.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\n  namespace: demo\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      value: toto\n      description: toto\n```\n\n----------------------------------------\n\nTITLE: Configuring Internal Listeners in NiFi Cluster Specification - YAML\nDESCRIPTION: This YAML snippet configures the internalListeners field in the listenersConfig for a NiFi cluster, specifying ports for key cluster functions such as HTTPS UI access, cluster-node communication, Site-to-Site transfer, Prometheus monitoring, and load balancing. Each listener entry requires a type (from the provided six types), a unique name, and the containerPort to use within the pod. This configuration informs the NiFi operator how to expose and manage internal ports in the deployed pods. Required dependencies include the NiFi operator and proper RBAC permissions. Inputs: the types and names of required listeners; Outputs: configured network ports inside the NiFi pods.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    - type: \"https\"\n      name: \"https\"\n      containerPort: 8443\n    - type: \"cluster\"\n      name: \"cluster\"\n      containerPort: 6007\n    - type: \"s2s\"\n      name: \"s2s\"\n      containerPort: 10000\n    - type: \"prometheus\"\n      name: \"prometheus\"\n      containerPort: 9090\n    - type: \"load-balance\"\n      name: \"load-balance\"\n      containerPort: 6342\n```\n\n----------------------------------------\n\nTITLE: Configuring Managed Groups in NifiCluster Specification\nDESCRIPTION: YAML configuration example showing how to define managed admin and reader users directly in the NifiCluster specification. This eliminates the need to create separate user definition files.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/2_manage_users_and_accesses/3_managed_groups.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  ...\n  oneNifiNodePerNode: false\n  propagateLabels: true\n  managedAdminUsers:\n    -  identity: \"alexandre.guitton@konpyutaika.com\"\n       name: \"aguitton\"\n    -  identity: \"nifiuser@konpyutaika.com\"\n       name: \"nifiuser\"\n  managedReaderUsers:\n    -  identity: \"toto@konpyutaika.com\"\n       name: \"toto\"\n    ...\n```\n\n----------------------------------------\n\nTITLE: Creating a basic NifiParameterContext using YAML\nDESCRIPTION: This YAML manifest defines a NifiParameterContext resource named 'dataflow-lifecycle'. It specifies a description, links it to a NifiCluster named 'nc' in the 'nifikop' namespace, references a Kubernetes Secret 'secret-params' for sensitive data, and includes two non-sensitive parameters ('test' and 'test2'). This example illustrates the basic structure for defining a parameter context.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/5_references/4_nifi_parameter_context.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      description: toto\n      sensistive: true\n```\n\n----------------------------------------\n\nTITLE: Deploying KEDA ScaledObject for NiFi Autoscaling\nDESCRIPTION: Creates a KEDA ScaledObject resource that defines how to scale the NiFi node group based on Prometheus metrics, including scaling limits and behavior.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: cluster\n  namespace: clusters\nspec:\n  scaleTargetRef:\n    apiVersion:    nifi.konpyutaika.com/v1alpha1     # Optional. Default: apps/v1\n    kind:          NifiNodeGroupAutoscaler           # Optional. Default: Deployment\n    name:          nifinodegroupautoscaler-sample    # Mandatory. Must be in the same namespace as the ScaledObject\n    envSourceContainerName: nifi                     # Optional. Default: .spec.template.spec.containers[0]\n  pollingInterval:  30                               # Optional. Default: 30 seconds\n  cooldownPeriod:   300                              # Optional. Default: 300 seconds\n  idleReplicaCount: 0                                # Optional. Default: ignored, must be less than minReplicaCount \n  minReplicaCount:  1                                # Optional. Default: 0\n  maxReplicaCount:  3                                # Optional. Default: 100\n  fallback:                                          # Optional. Section to specify fallback options\n    failureThreshold: 5                              # Mandatory if fallback section is included\n    replicas: 1                                      # Mandatory if fallback section is included\n  # advanced:                                          # Optional. Section to specify advanced options\n  #   restoreToOriginalReplicaCount: true              # Optional. Whether the target resource should be scaled back to original replicas count, after the ScaledObject is deleted\n  #   horizontalPodAutoscalerConfig:                   # Optional. Section to specify HPA related options\n  #     name: {name-of-hpa-resource}                   # Optional. Default: keda-hpa-{scaled-object-name}\n  #     behavior:                                      # Optional. Use to modify HPA's scaling behavior\n  #       scaleDown:\n  #         stabilizationWindowSeconds: 300 <--- \n  #         policies:\n  #         - type: Percent\n  #           value: 100\n  #           periodSeconds: 15\n  triggers:\n    - type: prometheus\n      metadata:\n        serverAddress: http://prometheus-operated.monitoring-system.svc:9090\n        metricName: http_requests_total\n        threshold: <threshold value>\n        query: <prometheus query>\n```\n\n----------------------------------------\n\nTITLE: Deploying ScaledObject for NiFi Autoscaling\nDESCRIPTION: This YAML snippet defines a KEDA ScaledObject, which is used to automatically scale a NiFi node group. The `spec` section defines how the NiFi node group scales, referencing a `NifiNodeGroupAutoscaler` and uses a Prometheus trigger to scale based on metrics.  Dependencies: Kubernetes, KEDA, NiFiNodeGroupAutoscaler.  Key parameters include `scaleTargetRef`, `pollingInterval`, `minReplicaCount`, `maxReplicaCount`, and the `triggers` section. The output is a ScaledObject resource ready for deployment to a Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_9\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: cluster\n  namespace: clusters\nspec:\n  scaleTargetRef:\n    apiVersion:    nifi.konpyutaika.com/v1alpha1     # Optional. Default: apps/v1\n    kind:          NifiNodeGroupAutoscaler           # Optional. Default: Deployment\n    name:          nifinodegroupautoscaler-sample    # Mandatory. Must be in the same namespace as the ScaledObject\n    envSourceContainerName: nifi                     # Optional. Default: .spec.template.spec.containers[0]\n  pollingInterval:  30                               # Optional. Default: 30 seconds\n  cooldownPeriod:   300                              # Optional. Default: 300 seconds\n  idleReplicaCount: 0                                # Optional. Default: ignored, must be less than minReplicaCount \n  minReplicaCount:  1                                # Optional. Default: 0\n  maxReplicaCount:  3                                # Optional. Default: 100\n  fallback:                                          # Optional. Section to specify fallback options\n    failureThreshold: 5                              # Mandatory if fallback section is included\n    replicas: 1                                      # Mandatory if fallback section is included\n  # advanced:                                          # Optional. Section to specify advanced options\n  #   restoreToOriginalReplicaCount: true              # Optional. Whether the target resource should be scaled back to original replicas count, after the ScaledObject is deleted\n  #   horizontalPodAutoscalerConfig:                   # Optional. Section to specify HPA related options\n  #     name: {name-of-hpa-resource}                   # Optional. Default: keda-hpa-{scaled-object-name}\n  #     behavior:                                      # Optional. Use to modify HPA's scaling behavior\n  #       scaleDown:\n  #         stabilizationWindowSeconds: 300 <--- \n  #         policies:\n  #         - type: Percent\n  #           value: 100\n  #           periodSeconds: 15\n  triggers:\n    - type: prometheus\n      metadata:\n        serverAddress: http://prometheus-operated.monitoring-system.svc:9090\n        metricName: http_requests_total\n        threshold: <threshold value>\n        query: <prometheus query>\n```\n\n----------------------------------------\n\nTITLE: Creating and Configuring NiFi User with NifiUser Custom Resource in YAML\nDESCRIPTION: This YAML snippet defines a NifiUser custom resource for managing a NiFi user within a Kubernetes environment. It specifies the user identity on the NiFi cluster, references the target NiFi cluster, and sets flags for certificate creation and inclusion of JKS keystore. The accessPolicies list grants the user specific permissions on NiFi components. Dependencies include a running Nifi operator managing synchronization between Kubernetes and NiFi clusters. Key parameters include 'identity' for the NiFi username, 'clusterRef' defining the NiFi cluster namespace and name, and accessPolicies detailing access types, actions, and target resources. Inputs are the resource declarations; outputs include automatic user creation or binding with existing NiFi users. Constraints include Kubernetes resource naming limitations mitigated by the identity field override.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/3_manage_nifi/2_manage_users_and_accesses/1_users_management.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: aguitton\nspec:\n  # identity field is use to define the user identity on NiFi cluster side,\n  #\tit use full when the user's name doesn't suite with Kubernetes resource name.\n  identity: alexandre.guitton@konpyutaika.com\n  # Contains the reference to the NifiCluster with the one the registry client is linked.\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  # Whether or not the the operator also include a Java keystore format (JKS) with you secret\n  includeJKS: false\n  # Whether or not a certificate will be created for this user.\n  createCert: false\n  # defines the list of access policies that will be granted to the group.\n  accessPolicies:\n      # defines the kind of access policy, could be \"global\" or \"component\".\n    - type: component\n      # defines the kind of action that will be granted, could be \"read\" or \"write\"\n      action: read\n      # resource defines the kind of resource targeted by this access policies, please refer to the following page:\n      #\thttps://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#access-policies\n      resource: /data\n      # componentType is used if the type is \"component\", it's allow to define the kind of component on which is the\n      # access policy\n      componentType: \"process-groups\"\n      # componentId is used if the type is \"component\", it's allow to define the id of the component on which is the\n      # access policy\n      componentId: \"\"\n```\n\n----------------------------------------\n\nTITLE: Example NifiConnection Custom Resource Definition (YAML)\nDESCRIPTION: This YAML manifest defines a `NifiConnection` custom resource named 'connection' in the 'instances' namespace. It specifies the source and destination dataflow components (input to output), including specific sub-component ports (output_1 to input_1), and configures various connection settings like flow file expiration (1 hour), back pressure thresholds (100 GB data size, 10000 objects), load balancing strategy (PARTITION_BY_ATTRIBUTE using 'partition_attribute'), prioritizers (NewestFlowFileFirstPrioritizer, FirstInFirstOutPrioritizer), label index, bend points for visual layout, and the update strategy (drain).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/5_references/8_nifi_connection.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiConnection\nmetadata:\n  name: connection\n  namespace: instances\nspec:\n  source:\n    name: input\n    namespace: instances\n    subName: output_1\n    type: dataflow\n  destination:\n    name: output\n    namespace: instances\n    subName: input_1\n    type: dataflow\n  configuration:\n    flowFileExpiration: 1 hour\n    backPressureDataSizeThreshold: 100 GB\n    backPressureObjectThreshold: 10000\n    loadBalanceStrategy: PARTITION_BY_ATTRIBUTE\n    loadBalancePartitionAttribute: partition_attribute\n    loadBalanceCompression: DO_NOT_COMPRESS\n    prioritizers: \n      - NewestFlowFileFirstPrioritizer\n      - FirstInFirstOutPrioritizer\n    labelIndex: 0\n    bends:\n      - posX: 550\n        posY: 550\n      - posX: 550\n        posY: 440\n      - posX: 550\n        posY: 88\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Listing Deployed Helm Charts\nDESCRIPTION: This snippet uses the Helm CLI to list all deployed Helm charts in the current Kubernetes context.  It's a simple command, requiring only Helm to be installed and configured.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nhelm list\n```\n\n----------------------------------------\n\nTITLE: Adding and Updating Prometheus Helm Repository Using Helm CLI\nDESCRIPTION: Adds the official Prometheus community Helm chart repository to Helm local configuration and updates it to fetch the latest available charts. These commands prepare the Helm client for installing Prometheus Operator charts. Requires Helm installed and access to the internet.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_6\n\nLANGUAGE: console\nCODE:\n```\nhelm repo add prometheus https://prometheus-community.github.io/helm-charts\n```\n\nLANGUAGE: console\nCODE:\n```\nhelm repo update\n```\n\n----------------------------------------\n\nTITLE: Listing Managed NiFi User Groups Using kubectl Console Command\nDESCRIPTION: This console command example demonstrates how to retrieve the existing managed NiFi user groups created and maintained by the operator within the Kubernetes cluster. Running `kubectl get` on the nifiusergroups custom resource in the 'nifikop' namespace lists groups such as 'managed-admins', 'managed-nodes', and 'managed-readers' along with their ages. This allows administrators to verify the presence and status of automatically managed user groups for NiFi cluster access control.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/2_manage_users_and_accesses/3_managed_groups.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nkubectl get -n nifikop nifiusergroups.nifi.konpyutaika.com \nNAME              AGE\nmanaged-admins    6d7h\nmanaged-nodes     6d7h\nmanaged-readers   6d7h\n```\n\n----------------------------------------\n\nTITLE: Installing Zookeeper on OpenShift with Custom runAsUser and fsGroup - Bash\nDESCRIPTION: Installs Zookeeper on OpenShift using Helm while passing the runAsUser and fsGroup parameters obtained from the namespace to ensure proper security context. This approach adapts the standard Zookeeper Helm install for OpenShift's stricter security requirements while setting resource limits, replica count, and network policies.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhelm install zookeeper oci://registry-1.docker.io/bitnamicharts/zookeeper \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set global.storageClass=standard \\\n    --set networkPolicy.enabled=true \\\n    --set replicaCount=3 \\\n    --set containerSecurityContext.runAsUser=$zookeper_uid \\\n    --set podSecurityContext.fsGroup=$zookeper_uid\n```\n\n----------------------------------------\n\nTITLE: Uninstalling Helm Release for Nifikop Operator in Bash\nDESCRIPTION: This Bash command uninstalls the nifikop operator Helm release from the Kubernetes cluster. It removes all the Kubernetes components and resources that were deployed as part of the Helm chart for nifikop. The command requires Helm to be installed and properly configured to access the appropriate Kubernetes context. The only input is the release name, 'nifikop'; there are no additional parameters.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nhelm del nifikop\n```\n\n----------------------------------------\n\nTITLE: Deploying Prometheus Custom Resource to Configure Scraping - YAML\nDESCRIPTION: This YAML manifest defines a Prometheus custom resource configured for the 'monitoring-system' namespace. It disables admin API, sets evaluation and scrape intervals to 30 seconds, requests 400Mi memory, and selects pods and services with labels matching Prometheus and the NiFi cluster. The Prometheus operator uses this definition to manage Prometheus pods that scrape NiFi cluster metrics.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: Prometheus\nmetadata:\n  name: prometheus\n  namespace: monitoring-system\nspec:\n  enableAdminAPI: false\n  evaluationInterval: 30s\n  logLevel: debug\n  podMonitorSelector:\n    matchExpressions:\n    - key: release\n      operator: In\n      values:\n      - prometheus\n  resources:\n    requests:\n      memory: 400Mi\n  scrapeInterval: 30s\n  serviceAccountName: prometheus\n  serviceMonitorSelector:\n    matchExpressions:\n    - key: app\n      operator: In\n      values:\n      - nifi-cluster\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio Gateway for HTTP Traffic to NiFi\nDESCRIPTION: Defines an Istio Gateway resource named 'nifi-gateway' that listens on port 80 for HTTP traffic destined for 'nifi.my-domain.com'. It uses the default Istio ingress gateway selector.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: nifi-gateway\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - nifi.my-domain.com\n```\n\n----------------------------------------\n\nTITLE: Configuring Conversion Webhook for nifikop CRDs - YAML\nDESCRIPTION: Adds configuration for a Kubernetes CRD to enable version conversion using a webhook. This YAML snippet includes necessary annotations for cert-manager and specifies webhook conversion strategy details. Required dependencies: cert-manager for certificate injection and a conversion webhook service. Key parameters: namespace, certificate_name, and webhook_service_name must be set appropriately.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n```\n\n----------------------------------------\n\nTITLE: Listing Managed NiFi User Groups\nDESCRIPTION: This command shows how to list all NifiUserGroups in the nifikop namespace, which includes the three managed groups created by the operator: managed-admins, managed-nodes, and managed-readers.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/2_manage_users_and_accesses/3_managed_groups.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nkubectl get -n nifikop nifiusergroups.nifi.konpyutaika.com\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Secret for Basic Authentication - Kubectl Command\nDESCRIPTION: This 'kubectl create secret' command creates a Kubernetes secret named 'nifikop-credentials' for use with the Nifikop operator's basic authentication. The secret must contain the username and password files and can optionally include a CA certificate file. These files should exist at the specified paths and will be mounted to authenticate the Nifikop operator to the NiFi API. The secret is created in the 'nifikop-nifi' namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/1_manage_clusters/3_external_cluster.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nkubectl create secret generic nifikop-credentials \\\n  --from-file=username=./secrets/username\\\n  --from-file=password=./secrets/password\\\n  --from-file=ca.crt=./secrets/ca.crt\\\n  -n nifikop-nifi\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Secret for Basic Authentication to NiFi\nDESCRIPTION: This command creates a Kubernetes secret containing credentials for basic authentication to an external NiFi cluster. It includes username, password, and an optional CA certificate for server certificate validation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/3_manage_nifi/1_manage_clusters/3_external_cluster.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create secret generic nifikop-credentials \\\n  --from-file=username=./secrets/username\\\n  --from-file=password=./secrets/password\\\n  --from-file=ca.crt=./secrets/ca.crt\\\n  -n nifikop-nifi\n```\n\n----------------------------------------\n\nTITLE: Applying NiFiKop CRDs manually using kubectl in Bash\nDESCRIPTION: This snippet shows the manual application of CustomResourceDefinitions (CRDs) for NiFiKop resources using kubectl. This is particularly useful when installing NiFiKop without using Helm's built-in CRD management (`--skip-crds`). It applies multiple CRD files hosted in the NiFiKop GitHub repository to the cluster, enabling the Kubernetes API server to recognize NiFiKop custom resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/2_deploy_nifikop/1_quick_start.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifinodegroupautoscalers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificonnections.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Internal and External Kubernetes Services with YAML\nDESCRIPTION: Defines both internal listeners and external Kubernetes Service objects ('externalServices') to expose NiFi cluster ports outside the Kubernetes cluster. Each listener is matched to a service port configuration, with the service type (e.g., 'LoadBalancer') set. Requires integration with the NiFi operator and a cluster supporting external service types, such as cloud-managed Kubernetes. Listens on HTTPS (port 443) and a custom HTTP-tracking endpoint (port 80) via the named LoadBalancer.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    - type: \"https\"\n      name: \"https\"\n      containerPort: 8443\n    - type: \"cluster\"\n      name: \"cluster\"\n      containerPort: 6007\n    - type: \"s2s\"\n      name: \"s2s\"\n      containerPort: 10000\n    - type: \"prometheus\"\n      name: \"prometheus\"\n      containerPort: 9090\n    - type: \"load-balance\"\n      name: \"load-balance\"\n      containerPort: 6342\n    - name: \"http-tracking\"\n      containerPort: 8081\nexternalServices:\n  - name: cluster-access\n    spec:\n      portConfigs:\n        - internalListenerName: https\n          port: 443\n        - internalListenerName: http-tracking\n          port: 80\n      type: LoadBalancer\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple Content and Provenance Repositories in NiFiCluster - YAML\nDESCRIPTION: This YAML snippet illustrates advanced NiFi configuration for high-performance setups by assigning multiple content and provenance repository directories within a NiFiKop deployment. The sample sets properties in 'readOnlyConfig.nifiProperties.overrideConfigs' and maps each logical directory to a PersistentVolumeClaim under 'nodeConfigGroups'. Dependencies are specific storage classes and PVCs defined by the user. Key parameters include the multiple repository property names and mount paths. Inputs are directory property configurations and PVC specs; outputs are logically separated and mounted storage directories at runtime.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_4\n\nLANGUAGE: YAML\nCODE:\n```\n...\n  readOnlyConfig:\n    nifiProperties:\n      overrideConfigs: |\n        nifi.content.repository.directory.dir1=../content-additional/dir1\n        nifi.content.repository.directory.dir2=../content-additional/dir2\n        nifi.content.repository.directory.dir3=../content-additional/dir3\n        nifi.provenance.repository.directory.dir1=../provenance-additional/dir1\n        nifi.provenance.repository.directory.dir2=../provenance-additional/dir2\n...\n  nodeConfigGroups:\n    default_group:\n      ...\n      storageConfigs:\n      - mountPath: \"/opt/nifi/content-additional/dir1\"\n        name: content-repository-dir1\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/content-additional/dir2\"\n        name: content-repository-dir2\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/content-additional/dir3\"\n        name: content-repository-dir3\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/provenance-additional/dir1\"\n        name: provenance-repository-dir1\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/provenance-additional/dir2\"\n        name: provenance-repository-dir2\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      ...\n```\n\n----------------------------------------\n\nTITLE: Console command to list managed NifiUserGroups in Kubernetes\nDESCRIPTION: This console command retrieves the list of managed NifiUserGroups within the specified namespace, enabling users to verify the presence and status of groups like managed-admins, managed-readers, and managed-nodes, which are automatically created and managed by the operator.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/2_manage_users_and_accesses/3_managed_groups.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nkubectl get -n nifikop nifiusergroups.nifi.konpyutaika.com\n```\n\n----------------------------------------\n\nTITLE: Listing and Managing Helm Releases Using Bash Commands\nDESCRIPTION: These bash commands provide ways to list currently deployed Helm releases, check release status, list deleted or all releases (including failed), and delete Helm releases in a Kubernetes environment. The commands assume Helm CLI is installed and configured. Additional flags like --deleted and --all extend listing functionality. The deletion commands remove the operator deployments without deleting non-removed CRDs, which require separate manual deletion commands.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nhelm list\n```\n\nLANGUAGE: bash\nCODE:\n```\nhelm status nifikop\n```\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --deleted\n```\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --all\n```\n\nLANGUAGE: bash\nCODE:\n```\nhelm del nifikop\n```\n\nLANGUAGE: bash\nCODE:\n```\nhelm delete --purge nifikop\n```\n\n----------------------------------------\n\nTITLE: Configuring Data Persistence with StorageConfig in NiFiKop YAML\nDESCRIPTION: Demonstrates configuring persistent storage for NiFi nodes using StorageConfig objects, including setting storage paths, PVC specifications, metadata, labels, and annotations. Ensures data durability across pod restarts.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\nstorageConfigs:\n  - mountPath: /opt/nifi/nifi-current/logs\n    name: logs\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/data\n    name: data\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 50Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/extensions\n    name: extensions-repository\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 5Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/flowfile_repository\n    name: flowfile-repository\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/nifi-current/conf\n    name: conf\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 5Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/content_repository\n    name: content-repository\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 500Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/provenance_repository\n    name: provenance-repository\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 500Gi\n```\n\n----------------------------------------\n\nTITLE: Extracting Allowed UID/GID for Zookeeper Namespace (OpenShift) - Bash\nDESCRIPTION: Extracts the supplemental group ID for the Zookeeper namespace from OpenShift annotations using kubectl, sed, and tr. This UID is required to securely run the Zookeeper pods in OpenShift. Dependencies: kubectl, proper permissions, sed and tr utilities.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nzookeper_uid=$(kubectl get namespace zookeeper -o=jsonpath='{.metadata.annotations.openshift\\.io/sa\\.scc\\.supplemental-groups}' | sed 's/\\/10000$//' | tr -d '[:space:]')\n\n```\n\n----------------------------------------\n\nTITLE: Defining Node Configuration Groups in NiFiKop YAML\nDESCRIPTION: This YAML snippet demonstrates how to define multiple NodeConfigGroups within the NiFiKop operator spec to customize pod technical requirements. It includes settings for provenance storage size, container user ID, resource limits and requests for CPU and memory, and service account name. The nodeConfigGroups allow reuse of configurations across multiple nodes, enabling differentiated resource allocation such as default 3Gi memory nodes and high memory nodes with 30Gi. Dependencies include a Kubernetes cluster and NiFiKop operator. Inputs are the resource and user parameters specified per group; outputs are Kubernetes pods with node-specific configurations.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  # nodeConfigGroups specifies multiple node configs with unique name\n  nodeConfigGroups:\n    default_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      serviceAccountName: \"default\"\n      # resourceRequirements works exactly like Container resources, the user can specify the limit and the requests\n      # through this property\n      # https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 3Gi\n    high_mem_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      serviceAccountName: \"default\"\n      # resourceRequirements works exactly like Container resources, the user can specify the limit and the requests\n      # through this property\n      # https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 30Gi\n        requests:\n          cpu: \"1\"\n          memory: 30Gi\n```\n\n----------------------------------------\n\nTITLE: Pushing Docker Image to Registry - Bash\nDESCRIPTION: This Makefile command pushes the previously built NiFiKop Docker image to the specified Docker registry. The user should ensure proper login credentials and image naming before execution. Outputs the image to a remote registry for deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/6_contributing/1_developer_guide.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nmake docker-push\n```\n\n----------------------------------------\n\nTITLE: Creating a Kubernetes Secret\nDESCRIPTION: This command creates a Kubernetes secret named `secret-params` in the `nifikop` namespace.  The secret contains two key-value pairs (`secret1` and `secret2`) that can be referenced by the NifiParameterContext. This allows sensitive parameters to be managed securely.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl create secret generic secret-params \\\n    --from-literal=secret1=yop \\\n    --from-literal=secret2=yep \\\n    -n nifikop\n```\n\n----------------------------------------\n\nTITLE: Configuring External NiFi Cluster in Yaml\nDESCRIPTION: YAML configuration for declaring an external NiFi cluster resource that allows the operator to communicate with clusters outside Kubernetes. Includes settings for node URI templates, authentication type, and secret references.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/3_manage_nifi/1_manage_clusters/3_external_cluster.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: externalcluster\nspec:\n  # rootProcessGroupId contains the uuid of the root process group for this cluster.\n  rootProcessGroupId: 'd37bee03-017a-1000-cff7-4eaaa82266b7'\n  # nodeURITemplate used to dynamically compute node uri.\n  nodeURITemplate: 'nifi0%d.integ.mapreduce.m0.p.fti.net:9090'\n  # all node requiresunique id\n  nodes:\n    - id: 1\n    - id: 2\n    - id: 3\n  # type defines if the cluster is internal (i.e manager by the operator) or external.\n  # :Enum={\"external\",\"internal\"}\n  type: 'external'\n  # clientType defines if the operator will use basic or tls authentication to query the NiFi cluster.\n  # Enum={\"tls\",\"basic\"}\n  clientType: 'basic'\n  # secretRef reference the secret containing the informations required to authenticate to the cluster.\n  secretRef:\n    name: nifikop-credentials\n    namespace: nifikop-nifi\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi User Groups with Kubernetes Custom Resource Definition YAML\nDESCRIPTION: This snippet defines a NiFiUserGroup Kubernetes Custom Resource using YAML syntax to configure a NiFi user group within a Kubernetes cluster. It specifies metadata like the resource name, the group's identity, references to the associated NiFi cluster, user references belonging to the group, and the access policies granted. Dependencies include a Kubernetes environment that recognizes the nifi.konpyutaika.com/v1 API group and a NiFi cluster managed under the specified namespace. Inputs include the group identity, cluster references, user references, and access policies. The output is a declarative resource representing a NiFi user group, which Kubernetes can apply to manage access control in NiFi clusters.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/5_references/6_nifi_usergroup.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  identity: \"My Special Group\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n    - name: nc-controller.nifikop.mgt.cluster.local\n  accessPolicies:\n    - type: global\n      action: read\n      resource: /counters\n```\n\n----------------------------------------\n\nTITLE: Viewing NiFiKop Node Resources with kubectl in Shell\nDESCRIPTION: This shell command queries Kubernetes to list pods, config maps, and persistent volume claims labeled with a specific nodeId, verifying that new resources for an added node have been provisioned. Replace 'nodeId=25' as appropriate for your environment. It expects no arguments and displays current Kubernetes resources related to the given node.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl get pods,configmap,pvc -l nodeId=25\nNAME                          READY   STATUS    RESTARTS   AGE\npod/simplenifi-25-nodem5jh4   1/1     Running   0          11m\n\nNAME                             DATA   AGE\nconfigmap/simplenifi-config-25   7      11m\n\nNAME                                               STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\npersistentvolumeclaim/simplenifi-25-storagehwn24   Bound    pvc-7da86076-728e-11ea-846d-42010a8400f2   10Gi       RWO            standard       11m\n```\n\n----------------------------------------\n\nTITLE: Defining ServiceMonitor for NiFi Cluster in Kubernetes (YAML)\nDESCRIPTION: Creates a `ServiceMonitor` resource in the `monitoring-system` namespace. This resource instructs the Prometheus instance (selected via `serviceMonitorSelector` in the Prometheus CR) how to discover and scrape metrics from NiFi pods labeled with `app: nifi` and `nifi_cr: cluster` within the `clusters` namespace. It specifies the metrics endpoint path, port, scrape interval, and includes relabeling rules to enrich metrics with pod IP and specific Kubernetes labels.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: cluster\n  namespace: monitoring-system\n  labels:\n    app: nifi-cluster\n    nifi_cr: cluster\nspec:\n  selector:\n    matchLabels:\n      app: nifi\n      nifi_cr: cluster\n  namespaceSelector:\n    matchNames:\n      - clusters\n  endpoints:\n    - interval: 10s\n      port: prometheus\n      path: /metrics\n      honorLabels: true\n      relabelings:\n        - sourceLabels: [__meta_kubernetes_pod_ip]\n          separator: ;\n          regex: (.*)\n          targetLabel: pod_ip\n          replacement: $1\n          action: replace\n        - sourceLabels: [__meta_kubernetes_pod_label_nodeId]\n          separator: ;\n          regex: (.*)\n          targetLabel: nodeId\n          replacement: $1\n          action: replace\n        - sourceLabels: [__meta_kubernetes_pod_label_nifi_cr]\n          separator: ;\n          regex: (.*)\n          targetLabel: nifi_cr\n          replacement: $1\n          action: replace\n```\n\n----------------------------------------\n\nTITLE: Defining Istio Gateway for HTTP Access to NiFi Cluster\nDESCRIPTION: This YAML configuration creates an Istio Gateway that intercepts all HTTP requests on port 80 for a specific domain host. The Gateway routes traffic to the NiFi cluster service.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: nifi-gateway\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - nifi.my-domain.com\n```\n\n----------------------------------------\n\nTITLE: YAML configuration to specify NifiCluster metadata and spec\nDESCRIPTION: This snippet defines a NifiCluster resource in YAML, including metadata and specification of the initContainerImage repository and tag. It demonstrates how users currently set custom init container images before the upgrade.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/7_upgrade_guides/2_v0.14.1_to_v0.15.0.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  initContainerImage:\n    repository: busybox\n    tag: \"1.34.0\"\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop Operator Helm Chart Using a Values YAML File in Console\nDESCRIPTION: This example demonstrates the command-line usage to install the NiFiKop Helm chart by specifying a custom configuration file (values.yaml). This allows users to customize Helm chart parameters using a YAML file rather than inline --set options. Requires Helm CLI version 3 or above.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_6\n\nLANGUAGE: console\nCODE:\n```\n$ helm install nifikop \\\n      konpyutaika/nifikop \\\n      -f values.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring Internal Listeners for NiFi Cluster\nDESCRIPTION: YAML configuration for NiFi's internal listeners, defining the ports for HTTPS, cluster communication, site-to-site, Prometheus metrics, and load balancing. Each listener specifies a type, name, and container port.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    - type: \"https\"\n      name: \"https\"\n      containerPort: 8443\n    - type: \"cluster\"\n      name: \"cluster\"\n      containerPort: 6007\n    - type: \"s2s\"\n      name: \"s2s\"\n      containerPort: 10000\n    - type: \"prometheus\"\n      name: \"prometheus\"\n      containerPort: 9090\n    - type: \"load-balance\"\n      name: \"load-balance\"\n      containerPort: 6342\n```\n\n----------------------------------------\n\nTITLE: Deploying a NifiConnection\nDESCRIPTION: This YAML snippet defines a NifiConnection resource named 'connection'. It configures a connection between the 'input' and 'output' NifiDataflows defined in the 'nifikop' namespace. It specifies the source and destination, along with configuration details such as back pressure thresholds, flow file expiration, label index, and connection bends. The update strategy is also set to 'drain'.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/4_manage_connections/1_deploy_connection.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiConnection\nmetadata:\n  name: connection\n  namespace: nifikop\nspec:\n  source:\n    name: input\n    namespace: nifikop\n    subName: output\n    type: dataflow\n  destination:\n    name: output\n    namespace: nifikop\n    subName: input\n    type: dataflow\n  configuration:\n    backPressureDataSizeThreshold: 100 GB\n    backPressureObjectThreshold: 10000\n    flowFileExpiration: 1 hour\n    labelIndex: 0\n    bends:\n      - posX: 550\n        posY: 550\n      - posX: 550\n        posY: 440\n      - posX: 550\n        posY: 88\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Example Kubernetes Pod Ports YAML Configuration for NiFi Internal Listeners\nDESCRIPTION: Demonstrates the expected pod port configuration in Kubernetes YAML based on the internal listeners defined earlier. Each port entry specifies the containerPort, a matching name, and the TCP protocol. This snippet reflects how Kubernetes exposes these internal listener ports on the NiFi pods for internal cluster communication and external access through services.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n    ports:\n    - containerPort: 8443\n      name: https\n      protocol: TCP\n    - containerPort: 6007\n      name: cluster\n      protocol: TCP\n    - containerPort: 6342\n      name: load-balance\n      protocol: TCP\n    - containerPort: 10000\n      name: s2s\n      protocol: TCP\n    - containerPort: 9090\n      name: prometheus\n      protocol: TCP\n```\n\n----------------------------------------\n\nTITLE: Listing Deleted Helm Releases\nDESCRIPTION: This command lists the Helm releases that have been deleted. This allows users to view previously deployed releases even after they are removed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --deleted\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop Helm Chart with Values File - Helm - Console\nDESCRIPTION: This console command demonstrates how to deploy the NiFiKop Kubernetes Operator using the Helm package manager. It utilizes the -f flag to supply a custom YAML file, enabling users to override default chart parameters for advanced configuration. Required dependencies include a Helm client (version 3 or above), access to the konpyutaika/nifikop Helm repository, and any prerequisite Kubernetes setup. The main parameters to customize are passed through the referenced values.yaml file. Expected input is a configuration YAML file, and the output is a deployed NiFiKop Operator in the Kubernetes cluster. Ensure all prerequisites in the document are satisfied before executing.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n$ helm install nifikop \\\n      konpyutaika/nifikop \\\n      -f values.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining Prometheus Custom Resource\nDESCRIPTION: YAML configuration for the Prometheus custom resource that defines the Prometheus server instance, including scrape configuration and resource requests.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: Prometheus\nmetadata:\n  name: prometheus\n  namespace: monitoring-system\nspec:\n  enableAdminAPI: false\n  evaluationInterval: 30s\n  logLevel: debug\n  podMonitorSelector:\n    matchExpressions:\n    - key: release\n      operator: In\n      values:\n      - prometheus\n  resources:\n    requests:\n      memory: 400Mi\n  scrapeInterval: 30s\n  serviceAccountName: prometheus\n  serviceMonitorSelector:\n    matchExpressions:\n    - key: app\n      operator: In\n      values:\n      - nifi-cluster\n```\n\n----------------------------------------\n\nTITLE: Defining the NifiUser Kubernetes Custom Resource Metadata\nDESCRIPTION: This YAML snippet specifies the metadata section of the NifiUser resource, including the resource's name and sidebar label for UI display. It provides identification details used by Kubernetes to manage and display the custom resource.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/5_references/2_nifi_user.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n`id: 2_nifi_user\ntitle: NiFi User\nsidebar_label: NiFi User`\n```\n\n----------------------------------------\n\nTITLE: Adding and Updating Helm Repositories for Prometheus Deployment - console\nDESCRIPTION: Commands to add the Prometheus community Helm charts repository and update local Helm repositories. These commands are prerequisites for installing the Prometheus operator chart.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_5\n\nLANGUAGE: console\nCODE:\n```\nhelm repo add prometheus https://prometheus-community.github.io/helm-charts\n```\n\nLANGUAGE: console\nCODE:\n```\nhelm repo update\n```\n\n----------------------------------------\n\nTITLE: Defining NifiParameterContext CRD with Parameters and Secret References in YAML\nDESCRIPTION: This YAML manifest creates a NifiParameterContext resource which NiFiKop converts into a NiFi Parameter Context. It supports defining parameters directly, with names, values, and descriptions, and allows referencing Kubernetes secrets for sensitive parameters through secretRefs. The example includes a cluster reference to specify the NiFi cluster. This enables encapsulated parameter management for NiFi dataflows.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\n  namespace: demo\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      value: toto\n      description: toto\n```\n\n----------------------------------------\n\nTITLE: Defining a NiFi Registry Client via Kubernetes YAML manifest - YAML\nDESCRIPTION: This YAML snippet demonstrates how to define a NifiRegistryClient custom resource for use with the nifikop operator on Kubernetes. It specifies the API version, kind, metadata, and core spec fields including clusterRef, description, and the URI of the target NiFi registry. Inputs include cluster name and namespace, a textual description, and a URI to the associated registry; outputs are the creation or update of a NifiRegistryClient in the cluster. Prerequisites: a running Kubernetes cluster with nifikop CRDs installed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/5_references/3_nifi_registry_client.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiRegistryClient\nmetadata:\n  name: squidflow\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Squidflow demo\"\n  uri: \"http://nifi-registry:18080\"\n```\n\n----------------------------------------\n\nTITLE: Defining Prometheus Custom Resource - YAML\nDESCRIPTION: Creates a Prometheus monitoring instance scoped to the 'monitoring-system' namespace. This resource configures scrape intervals, resource requests, selectors for PodMonitors and ServiceMonitors, disables admin API, and sets debug log level. Deploy with 'kubectl apply -f'. Requires prior installation of Prometheus Operator and the RBAC resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: Prometheus\nmetadata:\n  name: prometheus\n  namespace: monitoring-system\nspec:\n  enableAdminAPI: false\n  evaluationInterval: 30s\n  logLevel: debug\n  podMonitorSelector:\n    matchExpressions:\n    - key: release\n      operator: In\n      values:\n      - prometheus\n  resources:\n    requests:\n      memory: 400Mi\n  scrapeInterval: 30s\n  serviceAccountName: prometheus\n  serviceMonitorSelector:\n    matchExpressions:\n    - key: app\n      operator: In\n      values:\n      - nifi-cluster\n```\n\n----------------------------------------\n\nTITLE: Complete External Service Configuration\nDESCRIPTION: Comprehensive configuration demonstrating how to expose NiFi services externally using LoadBalancer. This example shows internal listener configuration and external service mapping with custom port assignments.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    - type: \"https\"\n      name: \"https\"\n      containerPort: 8443\n    - type: \"cluster\"\n      name: \"cluster\"\n      containerPort: 6007\n    - type: \"s2s\"\n      name: \"s2s\"\n      containerPort: 10000\n    - type: \"prometheus\"\n      name: \"prometheus\"\n      containerPort: 9090\n    - type: \"load-balance\"\n      name: \"load-balance\"\n      containerPort: 6342\n    - name: \"http-tracking\"\n      containerPort: 8081\nexternalServices:\n  - name: cluster-access\n    spec:\n      portConfigs:\n        - internalListenerName: https\n          port: 443\n        - internalListenerName: http-tracking\n          port: 80\n      type: LoadBalancer\n```\n\n----------------------------------------\n\nTITLE: Configuring External Services for NiFi in Kubernetes using YAML\nDESCRIPTION: This YAML code snippet demonstrates how to configure an external service for NiFi, specifying service type (ClusterIP), port configuration, and metadata fields such as annotations and labels. Required dependencies are Kubernetes and the nifikop operator, and the snippet serves as an example custom resource definition field that details how to map listeners and expose service ports. Key parameters include service name, port number, internal listener name, and optional label/annotation metadata. The YAML is intended to be used as part of a larger Kubernetes manifest for deploying NiFi services and may require adaptation for integration into specific clusters.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/5_references/1_nifi_cluster/7_external_service_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  externalServices:\n    - name: \"clusterip\"\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8080\n            internalListenerName: \"http\"\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n\n```\n\n----------------------------------------\n\nTITLE: Defining NifiRegistryClient in YAML for NiFiKop\nDESCRIPTION: This snippet shows how to define a NifiRegistryClient resource that connects to a NiFi Registry instance. The registry client is required for NiFiKop to manage dataflows using NiFi Registry features.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiRegistryClient\nmetadata:\n  name: registry-client-example\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Registry client managed by NiFiKop\"\n  uri: \"http://nifi.hostname.com:18080\"\n```\n\n----------------------------------------\n\nTITLE: Defining NifiParameterContext CRD in YAML\nDESCRIPTION: This YAML manifest defines a `NifiParameterContext` custom resource for NiFiKop. It allows defining parameters, including sensitive ones sourced from Kubernetes secrets (`secretRefs`), which can be associated with a `NifiDataflow`. It references the target NiFi cluster (`clusterRef`).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\n  namespace: demo\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      value: toto\n      description: toto\n```\n\n----------------------------------------\n\nTITLE: Defining Node Configuration\nDESCRIPTION: This YAML snippet configures a NiFi node, specifying settings for storage, user ID, and Kubernetes integration. It defines properties like provenanceStorage, runAsUser, imagePullPolicy, externalVolumeConfigs (including secret name and mount path), and storageConfigs (including PVC specifications and mount paths). These settings are used to customize the behavior and resources of the NiFi node within a Kubernetes cluster. The configuration includes node affinity and pod metadata.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/5_references/1_nifi_cluster/3_node_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n   default_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      # Set this to true if the instance is a node in a cluster.\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#basic-cluster-setup\n      isNode: true\n      # Additionnal metadata to merge to the pod associated\n      podMetadata:\n        annotations:\n          node-annotation: \"node-annotation-value\"\n        labels:\n          node-label: \"node-label-value\"\n      # Docker image used by the operator to create the node associated\n      # https://hub.docker.com/r/apache/nifi/\n#      image: \"apache/nifi:1.11.2\"\n      # nodeAffinity can be specified, operator populates this value if new pvc added later to node\n      # https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity\n#      nodeAffinity:\n      # imagePullPolicy define the pull policy for NiFi cluster docker image\n      imagePullPolicy: IfNotPresent\n      # priorityClassName define the name of the priority class to be applied to these nodes\n      priorityClassName: \"example-priority-class-name\"\n      # externalVolumeConfigs specifies a list of volume to mount into the main container.\n      externalVolumeConfigs:\n        - name: example-volume\n          mountPath: \"/opt/nifi/example\"\n          secret:\n            secretName: \"raw-controller\"\n      # storageConfigs specifies the node related configs\n      storageConfigs:\n        # Name of the storage config, used to name PV to reuse into sidecars for example.\n        - name: provenance-repository\n          # Path where the volume will be mount into the main nifi container inside the pod.\n          mountPath: \"/opt/nifi/provenance_repository\"\n          # Kubernetes PVC spec\n          # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n```\n\n----------------------------------------\n\nTITLE: Defining Nodes with Different Node Configuration Groups\nDESCRIPTION: YAML configuration showing how to declare cluster nodes using different node configuration groups. This example includes nodes that reference predefined groups and one node with inline configuration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n- id: 0\n  nodeConfigGroup: \"default_group\"\n- id: 2\n  nodeConfigGroup: \"high_mem_group\"\n- id: 3\n  nodeConfigGroup: \"high_mem_group\"\n- id: 5\n  nodeConfig:\n    resourcesRequirements:\n      limits:\n        cpu: \"2\"\n        memory: 3Gi\n      requests:\n        cpu: \"1\"\n        memory: 1Gi\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Node with Basic Configuration in YAML\nDESCRIPTION: This YAML snippet illustrates the setup of multiple NiFi nodes with unique IDs, shared configuration groups, and specific read-only configurations for customizing UI banners. It also demonstrates how to specify resource limits, storage volumes, and PVC specifications for each node. The configuration allows for scalable and manageable deployment within Kubernetes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/5_references/1_nifi_cluster/4_node.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n    - id: 0\n      # nodeConfigGroup can be used to ease the node configuration, if set only the id is required\n      nodeConfigGroup: \"default_group\"\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        nifiProperties:\n          overrideConfigs: |\n            nifi.ui.banner.text=NiFiKop - Node 0\n  \n    - id: 2\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        overrideConfigs: |\n          nifi.ui.banner.text=NiFiKop - Node 2\n      # node configuration\n      nodeConfig:\n        resourcesRequirements:\n          limits:\n            cpu: \"2\"\n            memory: 3Gi\n          requests:\n            cpu: \"1\"\n            memory: 1Gi\n        storageConfigs:\n          # Name of the storage config, used to name PV to reuse into sidecars for example.\n          - name: provenance-repository\n            # Path where the volume will be mount into the main nifi container inside the pod.\n            mountPath: \"/opt/nifi/provenance_repository\"\n            # Metadata to attach to the PVC that gets created\n            metadata:\n              labels:\n                my-label: my-value\n              annotations:\n                my-annotation: my-value\n            # Kubernetes PVC spec\n            # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n            pvcSpec:\n              accessModes:\n                - ReadWriteOnce\n              storageClassName: \"standard\"\n              resources:\n                requests:\n                  storage: 8Gi\n```\n\n----------------------------------------\n\nTITLE: Creating NifiRegistryClient YAML Example\nDESCRIPTION: This YAML snippet demonstrates the creation of a `NifiRegistryClient` resource. It defines the API version, kind, metadata (name and namespace), and specification including the cluster reference and the URI of the NiFi instance.  This client is a prerequisite for managing dataflows.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiRegistryClient\nmetadata:\n  name: registry-client-example\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Registry client managed by NiFiKop\"\n  uri: \"http://nifi.hostname.com:18080\"\n```\n\n----------------------------------------\n\nTITLE: Schema Definition for NifiUserGroup in YAML\nDESCRIPTION: This YAML snippet describes the schema for the NiFi UserGroup resource, including API versioning, metadata, specification fields for cluster references, user references, and access policies, as well as status indicators for current state tracking. Dependencies include Kubernetes Custom Resource Definitions (CRDs); key parameters include clusterRef, usersRef, and accessPolicies, which control user group behavior and permissions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/5_references/6_nifi_usergroup.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n`apiVersion: nifi.konpyutaika.com/v1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n    - name: nc-controller.nifikop.mgt.cluster.local\n  accessPolicies:\n    - type: global\n      action: read\n      resource: /counters`\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Cluster Autoscaler\nDESCRIPTION: This YAML snippet defines the `NifiNodeGroupAutoscaler` custom resource. It specifies the NiFi cluster to autoscale, the node config group, read-only configurations, and how to select nodes for scaling using node labels.  The autoscaler will use this configuration to manage the scaling of NiFi nodes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_4\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiNodeGroupAutoscaler\nmetadata:\n  name: nifinodegroupautoscaler-sample\n  namespace: clusters\nspec:\n  # contains the reference to the NifiCluster with the one the node group autoscaler is linked.\n  clusterRef:\n    name: cluster\n    namespace: clusters\n  # defines the id of the NodeConfig contained in NifiCluster.Spec.NodeConfigGroups\n  nodeConfigGroupId: auto_scaling\n  # readOnlyConfig can be used to pass Nifi node config\n  # which has type read-only these config changes will trigger rolling upgrade\n  readOnlyConfig:\n    nifiProperties:\n      overrideConfigs: |\n        nifi.ui.banner.text=NiFiKop - Scale Group\n  # This is an example of a node config you can apply to each replica in this node group.\n  # Any settings here will override those in the configured nodeConfigGroupId\n#  nodeConfig:\n#    nodeSelector:\n#      node_type: high-mem\n  # The selector used to identify nodes in NifiCluster.Spec.Nodes this autoscaler will manage\n  # Use Node.Labels in combination with this selector to clearly define which nodes will be managed by this autoscaler \n  nodeLabelsSelector: \n    matchLabels:\n      nifi_cr: cluster\n      nifi_node_group: auto-scaling\n  # the strategy used to decide how to add nodes to a nifi cluster\n  upscaleStrategy: simple\n  # the strategy used to decide how to remove nodes from an existing cluster\n  downscaleStrategy: lifo\n```\n\n----------------------------------------\n\nTITLE: Applying nifikop CRDs using kubectl - Bash\nDESCRIPTION: Applies all required Kubernetes CRDs for nifikop manually using kubectl. This is necessary if Helm is installed with the --skip-crds flag, which prevents automatic CRD creation. Each kubectl apply command points to a public CRD YAML from the nifikop GitHub repository. Dependencies: kubectl CLI and relevant cluster context.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifinodegroupautoscalers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificonnections.yaml\n```\n\n----------------------------------------\n\nTITLE: Applying removal of a node to Kubernetes\nDESCRIPTION: This CLI command applies the updated NiFiCluster configuration YAML that omits the node with 'id: 2'. It initiates the decommissioning and removal sequence in the cluster, beginning with data offload, pod termination, and node deregistration, aligned with best practices for graceful shutdown.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_4\n\nLANGUAGE: Shell\nCODE:\n```\nkubectl -n nifi apply -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining a ScaledObject for NiFi Autoscaling using YAML\nDESCRIPTION: This YAML configuration defines a ScaledObject for autoscaling a NiFi node based on Prometheus metrics. It specifies the target NiFiNodeGroupAutoscaler, polling interval, cooldown period, replica counts, fallback options, and Prometheus trigger.  The Prometheus trigger uses server address, metric name, threshold, and a query to determine scaling.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: cluster\n  namespace: clusters\nspec:\n  scaleTargetRef:\n    apiVersion:    nifi.konpyutaika.com/v1alpha1     # Optional. Default: apps/v1\n    kind:          NifiNodeGroupAutoscaler           # Optional. Default: Deployment\n    name:          nifinodegroupautoscaler-sample    # Mandatory. Must be in the same namespace as the ScaledObject\n    envSourceContainerName: nifi                     # Optional. Default: .spec.template.spec.containers[0]\n  pollingInterval:  30                               # Optional. Default: 30 seconds\n  cooldownPeriod:   300                              # Optional. Default: 300 seconds\n  idleReplicaCount: 0                                # Optional. Default: ignored, must be less than minReplicaCount \n  minReplicaCount:  1                                # Optional. Default: 0\n  maxReplicaCount:  3                                # Optional. Default: 100\n  fallback:                                          # Optional. Section to specify fallback options\n    failureThreshold: 5                              # Mandatory if fallback section is included\n    replicas: 1                                      # Mandatory if fallback section is included\n  # advanced:                                          # Optional. Section to specify advanced options\n  #   restoreToOriginalReplicaCount: true              # Optional. Whether the target resource should be scaled back to original replicas count, after the ScaledObject is deleted\n  #   horizontalPodAutoscalerConfig:                   # Optional. Section to specify HPA related options\n  #     name: {name-of-hpa-resource}                   # Optional. Default: keda-hpa-{scaled-object-name}\n  #     behavior:                                      # Optional. Use to modify HPA's scaling behavior\n  #       scaleDown:\n  #         stabilizationWindowSeconds: 300 <--- \n  #         policies:\n  #         - type: Percent\n  #           value: 100\n  #           periodSeconds: 15\n  triggers:\n    - type: prometheus\n      metadata:\n        serverAddress: http://prometheus-operated.monitoring-system.svc:9090\n        metricName: http_requests_total\n        threshold: <threshold value>\n        query: <prometheus query>\n```\n\n----------------------------------------\n\nTITLE: Configuring LoadBalancer External Service in YAML\nDESCRIPTION: This YAML snippet defines an external service of type LoadBalancer, specifically using an AWS Network Load Balancer (NLB). It configures the service name, the load balancer class, port mappings including TCP and UDP protocols, and associated metadata.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/5_references/1_nifi_cluster/7_external_service_config.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nexternalServices:\n    - name: \"nlb\"\n      spec:\n        type: LoadBalancer\n        loadBalancerClass: \"service.k8s.aws/nlb\"\n        portConfigs:\n          - port: 8080\n            internalListenerName: \"http\"\n          - port: 7890\n            internalListenerName: \"my-custom-udp-listener\"\n            protocol: UDP\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n\n----------------------------------------\n\nTITLE: Creating NifiRegistryClient YAML\nDESCRIPTION: This YAML snippet defines a NifiRegistryClient Custom Resource Definition (CRD) that is managed by NiFiKop. It specifies the cluster, a description and the URI of the NiFi Registry to connect to. This dependency must be deployed before a NifiDataflow.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiRegistryClient\nmetadata:\n  name: registry-client-example\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Registry client managed by NiFiKop\"\n  uri: \"http://nifi.hostname.com:18080\"\n```\n\n----------------------------------------\n\nTITLE: Defining External Services Configuration in YAML\nDESCRIPTION: This YAML snippet defines an external service named 'clusterip' of type ClusterIP for a NiFi cluster. It configures the service to expose port 8080, mapping it to the internal listener named 'http', and also defines metadata such as annotations and labels.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/5_references/1_nifi_cluster/7_external_service_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nexternalServices:\n    - name: \"clusterip\"\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8080\n            internalListenerName: \"http\"\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n\n----------------------------------------\n\nTITLE: Deploying KEDA ScaledObject Resource for NiFi Autoscaling - YAML\nDESCRIPTION: This YAML manifest defines a KEDA ScaledObject for controlling the scaling behavior of a NifiNodeGroupAutoscaler custom resource in the 'clusters' namespace. It specifies various scaling parameters such as polling and cooldown intervals, replica count limits, and fallback procedures as well as Prometheus-based metric triggers. Required dependencies include a running Kubernetes cluster, the KEDA operator installed, the targeted NifiNodeGroupAutoscaler resource present, and Prometheus metrics available at the specified address. The main input is the configuration fields within the spec, and the output is the scaled NiFi deployment according to desired metrics and boundaries. The configuration must be adjusted to fit the environment, especially mandatory fields and Prometheus query details.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: cluster\n  namespace: clusters\nspec:\n  scaleTargetRef:\n    apiVersion:    nifi.konpyutaika.com/v1alpha1     # Optional. Default: apps/v1\n    kind:          NifiNodeGroupAutoscaler           # Optional. Default: Deployment\n    name:          nifinodegroupautoscaler-sample    # Mandatory. Must be in the same namespace as the ScaledObject\n    envSourceContainerName: nifi                     # Optional. Default: .spec.template.spec.containers[0]\n  pollingInterval:  30                               # Optional. Default: 30 seconds\n  cooldownPeriod:   300                              # Optional. Default: 300 seconds\n  idleReplicaCount: 0                                # Optional. Default: ignored, must be less than minReplicaCount \n  minReplicaCount:  1                                # Optional. Default: 0\n  maxReplicaCount:  3                                # Optional. Default: 100\n  fallback:                                          # Optional. Section to specify fallback options\n    failureThreshold: 5                              # Mandatory if fallback section is included\n    replicas: 1                                      # Mandatory if fallback section is included\n  # advanced:                                          # Optional. Section to specify advanced options\n  #   restoreToOriginalReplicaCount: true              # Optional. Whether the target resource should be scaled back to original replicas count, after the ScaledObject is deleted\n  #   horizontalPodAutoscalerConfig:                   # Optional. Section to specify HPA related options\n  #     name: {name-of-hpa-resource}                   # Optional. Default: keda-hpa-{scaled-object-name}\n  #     behavior:                                      # Optional. Use to modify HPA's scaling behavior\n  #       scaleDown:\n  #         stabilizationWindowSeconds: 300 <--- \n  #         policies:\n  #         - type: Percent\n  #           value: 100\n  #           periodSeconds: 15\n  triggers:\n    - type: prometheus\n      metadata:\n        serverAddress: http://prometheus-operated.monitoring-system.svc:9090\n        metricName: http_requests_total\n        threshold: <threshold value>\n        query: <prometheus query>\n\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiUserGroup Resource - YAML\nDESCRIPTION: This YAML snippet defines a NifiUserGroup resource named `group-test` within the `nifikop` namespace. It specifies the NiFi cluster to which the group belongs, a list of users (referenced by name) that are members of the group, and a list of access policies that grant the group read access to the `/counters` resource. The clusterRef field refers to the NiFi cluster name and namespace. usersRef contains an array of users, referenced by name, to be included in the group. accessPolicies define the permissions granted to the group.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/2_manage_users_and_accesses/2_groups_management.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  # Contains the reference to the NifiCluster with the one the registry client is linked.\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  # contains the list of reference to NifiUsers that are part to the group.\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n#     namespace: nifikop\n    - name: nc-controller.nifikop.mgt.cluster.local\n  # defines the list of access policies that will be granted to the group.\n  accessPolicies:\n      # defines the kind of access policy, could be \"global\" or \"component\".\n    - type: global\n      # defines the kind of action that will be granted, could be \"read\" or \"write\"\n      action: read\n      # resource defines the kind of resource targeted by this access policies, please refer to the following page:\n      #\thttps://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#access-policies\n      resource: /counters\n#      # componentType is used if the type is \"component\", it's allow to define the kind of component on which is the\n#      # access policy\n#      componentType: \"process-groups\"\n#      # componentId is used if the type is \"component\", it's allow to define the id of the component on which is the\n#      # access policy\n#      componentId: \"\"\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi User Groups via Kubernetes Custom Resource (YAML)\nDESCRIPTION: This YAML snippet demonstrates the configuration for a NifiUserGroup custom resource in Kubernetes, allowing operators to manage NiFi groups declaratively. It requires the nifikop CRDs to be installed and existing NifiUser resources referenced in usersRef. Key parameters include clusterRef (linking to the target NifiCluster), usersRef (referencing NifiUser members), and accessPolicies (defining which actions on which resources are granted to the group). The output is the automatic management of the group and its permissions within the specified NiFi cluster. Limitations include the need to predefine referenced users and clusters, and commented fields indicate how to further restrict access to specific NiFi components.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/2_manage_users_and_accesses/2_groups_management.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  # Contains the reference to the NifiCluster with the one the registry client is linked.\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  # contains the list of reference to NifiUsers that are part to the group.\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n#     namespace: nifikop\n    - name: nc-controller.nifikop.mgt.cluster.local\n  # defines the list of access policies that will be granted to the group.\n  accessPolicies:\n      # defines the kind of access policy, could be \"global\" or \"component\".\n    - type: global\n      # defines the kind of action that will be granted, could be \"read\" or \"write\"\n      action: read\n      # resource defines the kind of resource targeted by this access policies, please refer to the following page:\n      #\thttps://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#access-policies\n      resource: /counters\n#      # componentType is used if the type is \"component\", it's allow to define the kind of component on which is the\n#      # access policy\n#      componentType: \"process-groups\"\n#      # componentId is used if the type is \"component\", it's allow to define the id of the component on which is the\n#      # access policy\n#      componentId: \"\"\n\n```\n\n----------------------------------------\n\nTITLE: Defining NifiUserGroup resource\nDESCRIPTION: This YAML snippet defines a NifiUserGroup resource with its API version, kind, metadata, and specification. The specification includes a reference to a NifiCluster named 'nc', references to two NifiUsers, and an access policy granting read access to the '/counters' resource.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/5_references/6_nifi_usergroup.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n    - name: nc-controller.nifikop.mgt.cluster.local\n  accessPolicies:\n    - type: global\n      action: read\n      resource: /counters\n```\n\n----------------------------------------\n\nTITLE: Creating NiFiUser for Client SSL Credentials\nDESCRIPTION: YAML and console commands for creating a NiFi user with SSL credentials. This creates a Kubernetes secret containing certificate and key for a client to securely connect to the NiFi cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_3\n\nLANGUAGE: console\nCODE:\n```\ncat << EOF | kubectl apply -n nifi -f -\napiVersion:  nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: example-client\n  namespace: nifi\nspec:\n  clusterRef:\n    name: nifi\n  secretName: example-client-secret\nEOF\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFi Cluster with kubectl in Bash\nDESCRIPTION: This command deploys a NiFi cluster from a sample configuration file (`simplenificluster.yaml`) using `kubectl`. The command creates the NiFi cluster within the `nifi` namespace. It assumes the user has a configuration file available in the `config/samples/` directory that defines the NiFi cluster. Prior to this command the user needs to set up Zookeeper and create the nifi namespace, and modify the `simplenificluster.yaml` to include the zookeeper svc name.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create -n nifi -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining NifiDataflow Custom Resource - YAML\nDESCRIPTION: This YAML snippet defines an instance of the NifiDataflow Kubernetes Custom Resource for managing a NiFi dataflow via the konpyutaika NiFi Operator. Dependencies include access to a running NiFi cluster managed by nifikop, valid references to a NiFi bucket, registry client, parameter context, and cluster, all of which must exist within the specified namespaces. Key parameters such as 'parentProcessGroupID', 'bucketId', 'flowId', and 'flowVersion' control where and which dataflow is deployed; additional parameters like 'syncMode', 'skipInvalidControllerService', and 'updateStrategy' specify operational behavior during deployment and updates. Inputs include the referenced resource IDs and configuration flags, while the expected output is the creation, synchronization, and lifecycle management of the specified NiFi dataflow; limitations include the need for corresponding referenced resources to preexist in the specified namespaces.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/5_references/5_nifi_dataflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  flowPosition:\n    posX: 0\n    posY: 0\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: squidflow\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: nifikop\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: NifiCluster InitContainer Configuration (After)\nDESCRIPTION: This YAML snippet shows the NifiCluster configuration after the upgrade. It demonstrates an example where the `initContainerImage` is updated to `bash`. This ensures compatibility with Nifikop v0.15.0, as `bash` is now the default. The repository and tag of the image are specified.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/7_upgrade_guides/2_v0.14.1_to_v0.15.0.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  initContainerImage:\n    repository: bash\n    tag: \"5.2.2\"\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFiKop Custom Resource Definitions (CRDs) (kubectl)\nDESCRIPTION: Applies the NiFiKop Custom Resource Definitions (CRDs) to the target Kubernetes cluster using 'kubectl apply'. These CRDs define the custom resources (like NiFiClusters, NiFiDataFlows, etc.) that the operator manages. This step must be completed before the operator can manage NiFi resources in the cluster. It requires kubectl configured to connect to your cluster and the CRD YAML files to be present at the specified paths.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/6_contributing/1_developer_guide.md#_snippet_4\n\nLANGUAGE: Bash\nCODE:\n```\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\n```\n\n----------------------------------------\n\nTITLE: NiFiKop Custom Resource YAML for OIDC Configuration\nDESCRIPTION: This YAML snippet demonstrates how to override NiFi properties, including OIDC discovery URL, client ID, secret, and identity mapping pattern, in a NiFiKop deployment. It configures the cluster to use OIDC authentication with specific mapping rules.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/5_users_authentication/1_oidc.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  readOnlyConfig:\n    # NifiProperties configuration that will be applied to the node.\n    nifiProperties:\n      webProxyHosts:\n        - nifistandard2.trycatchlearn.fr:8443\n      # Additionnal nifi.properties configuration that will override the one produced based\n      # on template and configurations.\n      overrideConfigs: |\n        nifi.security.user.oidc.discovery.url=<oidc server discovery url>\n        nifi.security.user.oidc.client.id=<oidc client's id>\n        nifi.security.user.oidc.client.secret=<oidc client's secret>\n        nifi.security.identity.mapping.pattern.dn=CN=([^,]*)(?:, (?:O|OU)=.*)?\n        nifi.security.identity.mapping.value.dn=$1\n        nifi.security.identity.mapping.transform.dn=NONE\n      ...\n   ...\n...\n```\n\n----------------------------------------\n\nTITLE: Defining NifiUserGroup Resource\nDESCRIPTION: This YAML snippet defines a `NifiUserGroup` resource named `group-test`. It specifies the NiFi cluster to which the group is linked, the users that are part of the group, and the access policies granted to the group. The `clusterRef` points to a NiFi cluster named `nc` in the `nifikop` namespace. `usersRef` lists two users. `accessPolicies` grants read access to the `/counters` resource.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/3_manage_nifi/2_manage_users_and_accesses/2_groups_management.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  # Contains the reference to the NifiCluster with the one the registry client is linked.\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  # contains the list of reference to NifiUsers that are part to the group.\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n#     namespace: nifikop\n    - name: nc-controller.nifikop.mgt.cluster.local\n  # defines the list of access policies that will be granted to the group.\n  accessPolicies:\n      # defines the kind of access policy, could be \"global\" or \"component\".\n    - type: global\n      # defines the kind of action that will be granted, could be \"read\" or \"write\"\n      action: read\n      # resource defines the kind of resource targeted by this access policies, please refer to the following page:\n      #\thttps://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#access-policies\n      resource: /counters\n#      # componentType is used if the type is \"component\", it's allow to define the kind of component on which is the\n#      # access policy\n#      componentType: \"process-groups\"\n#      # componentId is used if the type is \"component\", it's allow to define the id of the component on which is the\n#      # access policy\n#      componentId: \"\"\n```\n\n----------------------------------------\n\nTITLE: Listing All Helm Releases (Including Deleted/Failed)\nDESCRIPTION: Displays a comprehensive list of all Helm releases, including those currently deployed, previously deleted, and those that failed during deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --all\n```\n\n----------------------------------------\n\nTITLE: Schema Documentation for InternalListener\nDESCRIPTION: This schema defines an InternalListener object with fields for type, name, and containerPort, including allowed values and descriptions. It ensures proper validation of listener configurations, specifying ports and listener roles such as HTTPS, HTTP, or S2S communication within the Nifi deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/5_references/1_nifi_cluster/6_listeners_config.md#_snippet_2\n\n\n\n----------------------------------------\n\nTITLE: Purge Helm Release\nDESCRIPTION: This command purges a Helm release named `nifikop`. Purging a release completely removes it from Helm's history. After purging, the release cannot be rolled back. Dependencies: Helm.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nhelm delete --purge nifikop\n```\n\n----------------------------------------\n\nTITLE: Defining Istio HTTP Gateway (YAML)\nDESCRIPTION: This YAML snippet defines an Istio Gateway resource. It configures the gateway to listen on HTTP port 80, targeting the `istio: ingressgateway` selector. It specifies that the gateway will handle traffic for the host `nifi.my-domain.com`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: nifi-gateway\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - nifi.my-domain.com\n```\n\n----------------------------------------\n\nTITLE: Checking Kubernetes Horizontal Pod Autoscaler Status for NiFi ScaledObject - console\nDESCRIPTION: This console command lists the Horizontal Pod Autoscaler (HPA) resources in the \"clusters\" namespace, showing the status of the HPA that manages scaling of the NifiNodeGroupAutoscaler resource named \"nifinodegroupautoscaler-sample\". It displays current CPU/memory targets, minimum and maximum pod counts, current replicas, and age, which verifies that autoscaling is active.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_13\n\nLANGUAGE: console\nCODE:\n```\nkubectl get -n clusters hpa\nNAME                REFERENCE                                                TARGETS         MINPODS   MAXPODS   REPLICAS   AGE\nkeda-hpa-cluster    NifiNodeGroupAutoscaler/nifinodegroupautoscaler-sample     1833m/2 (avg)   1         3         2          25d\n```\n\n----------------------------------------\n\nTITLE: Advanced NiFi Content and Provenance Repository Directory Partitioning - YAML\nDESCRIPTION: This YAML snippet illustrates advanced NiFi deployment configuration within a NiFiCluster, setting multiple directories for content and provenance repositories and mapping them to storage volumes. It leverages the 'readOnlyConfig.nifiProperties.overrideConfigs' field to specify directory mappings, and 'nodeConfigGroups.storageConfigs' to assign persistent volume claims to each directory. Dependencies are a valid NiFiCluster CRD and properly provisioned storage classes. Key parameters include overrideConfigs for mapping repository directories, mountPath and pvcSpec for storage volumes. Input is NiFiCluster YAML; output is a high-performance NiFi installation with multiple storage-backed repository directories. Custom volume provisioning and unique storage class integration may be required.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\n...\n  readOnlyConfig:\n    nifiProperties:\n      overrideConfigs: |\n        nifi.content.repository.directory.dir1=../content-additional/dir1\n        nifi.content.repository.directory.dir2=../content-additional/dir2\n        nifi.content.repository.directory.dir3=../content-additional/dir3\n        nifi.provenance.repository.directory.dir1=../provenance-additional/dir1\n        nifi.provenance.repository.directory.dir2=../provenance-additional/dir2\n...\n  nodeConfigGroups:\n    default_group:\n      ...\n      storageConfigs:\n      - mountPath: \"/opt/nifi/content-additional/dir1\"\n        name: content-repository-dir1\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/content-additional/dir2\"\n        name: content-repository-dir2\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/content-additional/dir3\"\n        name: content-repository-dir3\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/provenance-additional/dir1\"\n        name: provenance-repository-dir1\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/provenance-additional/dir2\"\n        name: provenance-repository-dir2\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      ...\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFiKop on OpenShift with custom UID\nDESCRIPTION: Installs NiFiKop on OpenShift using Helm with the correct UID value extracted from the namespace annotation. This ensures the operator can run with OpenShift's restricted Security Context Constraint (SCC).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/2_deploy_nifikop/1_quick_start.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop \\\n    nifikop \\\n    --namespace=nifi \\\n    --version 1.1.1 \\\n    --set image.tag=v1.1.1-release \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set namespaces={\"nifi\"} \\\n    --set runAsUser=$uid\n```\n\n----------------------------------------\n\nTITLE: Building the NiFiKop Docker Image using Bash\nDESCRIPTION: Sets the `DOCKER_REPO_BASE` environment variable to specify the target Docker repository prefix (e.g., `your-dockerhub-username/nifikop`) and then runs `make docker-build` to build the operator's Docker image using the project's Makefile. Requires Docker v18.09+.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/6_contributing/1_developer_guide.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nexport DOCKER_REPO_BASE={your-docker-repo}\nmake docker-build\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables for IDE Execution in Bash\nDESCRIPTION: Defines necessary environment variables for running the NiFiKop operator directly from an Integrated Development Environment (IDE). These variables configure the Kubernetes connection (`KUBECONFIG`), the namespace to monitor (`WATCH_NAMESPACE`), operator pod identification (`POD_NAME`), logging verbosity (`LOG_LEVEL`), and an operator identifier (`OPERATOR_NAME`). Replace placeholders with actual values.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/6_contributing/1_developer_guide.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nKUBECONFIG={path/to/your/kubeconfig}\nWATCH_NAMESPACE={namespace_to_watch}\nPOD_NAME={name for operator pod}\nLOG_LEVEL=Debug\nOPERATOR_NAME=ide\n```\n\n----------------------------------------\n\nTITLE: List Deployed Helm Releases - Bash\nDESCRIPTION: Lists all Helm releases currently deployed within the active Kubernetes context using the `helm list` command. Provides an overview of installed charts, their versions, status, and the namespaces they occupy.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/2_setup/3_install/1_customizable_install_with_helm.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nhelm list\n```\n\n----------------------------------------\n\nTITLE: Executing kubectl apply to update NiFi Cluster with new node\nDESCRIPTION: This shell command applies the updated NiFi cluster configuration YAML to the Kubernetes cluster, triggering the deployment of the new node resource. Dependencies include the kubectl CLI and access to the Kubernetes cluster with appropriate permissions. The operation results in the provisioning of a new NiFi node within the cluster environment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\nkubectl -n nifi apply -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining a Standalone NiFi Parameter Context with Kubernetes YAML\nDESCRIPTION: This YAML snippet defines a NiFi parameter context as a Kubernetes custom resource using the NifiParameterContext kind. It specifies a unique context name, descriptive metadata, a cluster reference, references to Kubernetes secrets for sensitive values, and a list of both sensitive and non-sensitive parameters with their values and descriptions. Dependencies include the nifikop operator and a configured NiFi cluster. The expected input is a YAML manifest applied to a cluster; the output is the creation or update of the parameter context resource within NiFi. Ensure the referenced secrets and cluster exist prior to deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/5_references/4_nifi_parameter_context.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      description: toto\n      sensistive: true\n```\n\n----------------------------------------\n\nTITLE: Test Kubectl NiFiKop Plugin Installation\nDESCRIPTION: This command invokes the `kubectl-nifikop` plugin to verify its installation.  It expects the plugin to be in the PATH and display usage information.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/2_deploy_nifikop/3_kubectl_plugin.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\n$ kubectl nifikop\nUsage:\n  nifikop [command]\n\nAvailable Commands:\n  completion          Generate the autocompletion script for the specified shell\n  help                Help about any command\n  nificluster         \n  nificonnection      \n  nifidataflow        \n  nifigroupautoscaler \n  nifiregistryclient  \n  nifiuser            \n  nifiusergroup \n```\n\n----------------------------------------\n\nTITLE: Defining Cluster-Wide Read-Only NiFi Configurations in YAML\nDESCRIPTION: This YAML snippet demonstrates the structure of the `readOnlyConfig` object used by NiFiKop to define cluster-wide read-only configurations for Apache NiFi. It allows setting maximum thread counts (`maximumTimerDrivenThreadCount`, `maximumEventDrivenThreadCount`) and configuring overrides for logback, authorizers, nifi.properties, zookeeper.properties, and bootstrap.properties. Configurations can be sourced directly, from ConfigMaps (`overrideConfigMap`, `replaceConfigMap`, `replaceTemplateConfigMap`), or Secrets (`overrideSecretConfig`, `replaceSecretConfig`, `replaceTemplateSecretConfig`) within a specified Kubernetes namespace. These settings are merged with node-specific configurations.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/5_references/1_nifi_cluster/2_read_only_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreadOnlyConfig:\n  # MaximumTimerDrivenThreadCount define the maximum number of threads for timer driven processors available to the system.\n  maximumTimerDrivenThreadCount: 30\n  # MaximumEventDrivenThreadCount define the maximum number of threads for event driven processors available to the system.\n  maximumEventDrivenThreadCount: 10\n  # Logback configuration that will be applied to the node\n  logbackConfig:\n    # logback.xml configuration that will replace the one produced based on template\n    replaceConfigMap:\n      # The key of the value,in data content, that we want use.\n      data: logback.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # logback.xml configuration that will replace the one produced based on template and overrideConfigMap\n    replaceSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: logback.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n  # Authorizer configuration that will be applied to the node\n  authorizerConfig:\n    # An authorizers.xml configuration template that will replace the default template seen in authorizers.go\n    replaceTemplateConfigMap:\n      # The key of the value, in data content, that we want use.\n      data: authorizers.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # An authorizers.xml configuration template that will replace the default template seen in authorizers.go and the replaceTemplateConfigMap\n    replaceTemplateSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: authorizers.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n  # NifiProperties configuration that will be applied to the node.\n  nifiProperties:\n    # Additionnal nifi.properties configuration that will override the one produced based on template and\n    # configuration\n    overrideConfigMap:\n      # The key of the value,in data content, that we want use.\n      data: nifi.properties\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop.\n    # Additionnal nifi.properties configuration that will override the one produced based\n    #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    overrideSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: nifi.properties\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # Additionnal nifi.properties configuration that will override the one produced based\n    #\ton template, configurations and overrideConfigMap\n    overrideConfigs: |\n      nifi.ui.banner.text=NiFiKop\n    # A comma separated list of allowed HTTP Host header values to consider when NiFi\n    # is running securely and will be receiving requests to a different host[:port] than it is bound to.\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#web-properties\n    #      webProxyHosts:\n    # Nifi security client auth\n    needClientAuth: false\n    # Indicates which of the configured authorizers in the authorizers.xml file to use\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#authorizer-configuration\n  #      authorizer:\n  # ZookeeperProperties configuration that will be applied to the node.\n  zookeeperProperties:\n    #      # Additionnal zookeeeper.properties configuration that will override the one produced based on template and\n    #      # configuration\n    #      overrideConfigMap:\n    #        # The key of the value,in data content, that we want use.\n    #        data: zookeeeper.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop.\n    #      # Additionnal zookeeeper.properties configuration that will override the one produced based\n    #      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    #      overrideSecretConfig:\n    #        # The key of the value,in data content, that we want use.\n    #        data: zookeeeper.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop\n    # Additionnal zookeeper.properties configuration that will override the one produced based\n    # on template and configurations.\n    overrideConfigs: |\n      initLimit=15\n      autopurge.purgeInterval=24\n      syncLimit=5\n      tickTime=2000\n      dataDir=./state/zookeeper\n      autopurge.snapRetainCount=30\n  # BootstrapProperties configuration that will be applied to the node.\n  bootstrapProperties:\n    #      # Additionnal bootstrap.properties configuration that will override the one produced based on template and\n    #      # configuration\n    #      overrideConfigMap:\n    #        # The key of the value,in data content, that we want use.\n    #        data: bootstrap.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop.\n    #      # Additionnal bootstrap.properties configuration that will override the one produced based\n    #      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    #      overrideSecretConfig:\n    #        # The key of the value,in data content, that we want use.\n    #        data: bootstrap.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop\n    # JVM memory settings\n    nifiJvmMemory: \"512m\"\n    # Additionnal bootstrap.properties configuration that will override the one produced based\n    # on template and configurations.\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#bootstrap_properties\n    overrideConfigs: |\n      # java.arg.4=-Djava.net.preferIPv4Stack=true\n```\n\n----------------------------------------\n\nTITLE: Monitoring Node Decommission Status (Console)\nDESCRIPTION: Uses the `kubectl describe` command to inspect the detailed status of the `NifiCluster` resource. This snippet shows relevant parts of the output, specifically the `Status.Nodes State` section. Observing the `Graceful Action State` (e.g., `GracefulDownscaleRequired`) allows tracking the progress of the graceful decommission procedure initiated by the NiFiKop operator. Requires `kubectl` access to the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nkubectl describe nificluster simplenifi\n\n...\nStatus:\n  Nodes State:\n    ...\n    2:\n      Configuration State:  ConfigInSync\n      Graceful Action State:\n        Action State:   GracefulDownscaleRequired\n        Error Message:\n    ...\n...\n```\n\n----------------------------------------\n\nTITLE: Verifying NiFiKop Operator Pod Status (kubectl)\nDESCRIPTION: Uses the 'kubectl get pods' command to list the pods in the 'nifikop' namespace. This command is used to verify that the NiFiKop operator pod has been successfully deployed by Helm and is running in the cluster. It requires 'kubectl' configured to communicate with the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/6_contributing/1_developer_guide.md#_snippet_9\n\nLANGUAGE: Bash\nCODE:\n```\nkubectl get pods -n nifikop\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Node Group Autoscaler - YAML\nDESCRIPTION: YAML definition for the `NifiNodeGroupAutoscaler` custom resource. It links to a `NifiCluster`, specifies the `nodeConfigGroupId` (`auto_scaling`) to manage, uses `nodeLabelsSelector` to identify target nodes, and defines scaling strategies. This resource configures NiFiKop's autoscaling behavior for a specific node group. Requires the NiFiKop operator.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiNodeGroupAutoscaler\nmetadata:\n  name: nifinodegroupautoscaler-sample\n  namespace: clusters\nspec:\n  # contains the reference to the NifiCluster with the one the node group autoscaler is linked.\n  clusterRef:\n    name: cluster\n    namespace: clusters\n  # defines the id of the NodeConfig contained in NifiCluster.Spec.NodeConfigGroups\n  nodeConfigGroupId: auto_scaling\n  # readOnlyConfig can be used to pass Nifi node config\n  # which has type read-only these config changes will trigger rolling upgrade\n  readOnlyConfig:\n    nifiProperties:\n      overrideConfigs: |\n        nifi.ui.banner.text=NiFiKop - Scale Group\n  # This is an example of a node config you can apply to each replica in this node group.\n  # Any settings here will override those in the configured nodeConfigGroupId\n#  nodeConfig:\n#    nodeSelector:\n#      node_type: high-mem\n  # The selector used to identify nodes in NifiCluster.Spec.Nodes this autoscaler will manage\n  # Use Node.Labels in combination with this selector to clearly define which nodes will be managed by this autoscaler \n  nodeLabelsSelector: \n    matchLabels:\n      nifi_cr: cluster\n      nifi_node_group: auto-scaling\n  # the strategy used to decide how to add nodes to a nifi cluster\n  upscaleStrategy: simple\n  # the strategy used to decide how to remove nodes from an existing cluster\n  downscaleStrategy: lifo\n```\n\n----------------------------------------\n\nTITLE: Defining NifiDataflow in YAML for NiFiKop\nDESCRIPTION: This snippet shows how to define a NifiDataflow resource that references a registry client and parameter context. It includes versioning information, sync mode, and update strategy for managing the dataflow lifecycle.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: demo\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Creating Nifikop NiFi User Resource - YAML\nDESCRIPTION: Example YAML manifest to create a `NifiUser` resource managed by the Nifikop operator. It specifies the user's identity and links it to an existing NiFi cluster using `clusterRef`. The `createCert` field is set to `false`, disabling automatic certificate generation for this user.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/5_references/2_nifi_user.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: aguitton\nspec:\n  identity: alexandre.guitton@konpyutaika.com\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  createCert: false\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio Gateway for HTTPS NiFi Access\nDESCRIPTION: This YAML configuration defines an Istio Gateway that accepts HTTPS traffic on port 443 and transforms it to HTTP. It uses TLS in SIMPLE mode with credentials from a specified secret for securing external access to a NiFi cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: nifi-gateway\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 443\n      name: https\n      protocol: HTTPS\n    tls:\n      mode: SIMPLE\n      credentialName: my-secret\n    hosts:\n    - nifi.my-domain.com\n```\n\n----------------------------------------\n\nTITLE: Deploy NiFiKop on OpenShift with Helm\nDESCRIPTION: This command deploys the NiFiKop operator on OpenShift using Helm and addresses security context constraints (SCC). The command first gets the uid for the `nifi` namespace, and then installs NiFiKop specifying the `runAsUser` parameter with the retrieved uid. This is crucial for proper deployment on OpenShift.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/2_deploy_nifikop/1_quick_start.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nuid=$(kubectl get namespace nifi -o=jsonpath='{.metadata.annotations.openshift\\.io/sa\\.scc\\.supplemental-groups}' | sed 's/\\/10000$//' | tr -d '[:space:]')\nhelm install nifikop \\\n    nifikop \\\n    --namespace=nifi \\\n    --version 1.1.1 \\\n    --set image.tag=v1.1.1-release \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set namespaces={\"nifi\"} \\\n    --set runAsUser=$uid\n```\n\n----------------------------------------\n\nTITLE: Defining Prometheus Custom Resource for Monitoring - YAML\nDESCRIPTION: Specifies a Prometheus custom resource in the monitoring-system namespace to control Prometheus configuration and behavior (resource requests, scraping intervals, selectors for PodMonitors and ServiceMonitors). The manifest expects all CRDs from the Prometheus Operator to be installed. Parameters include metrics selectors, resource limits, log level, and attached ServiceAccount.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: Prometheus\nmetadata:\n  name: prometheus\n  namespace: monitoring-system\nspec:\n  enableAdminAPI: false\n  evaluationInterval: 30s\n  logLevel: debug\n  podMonitorSelector:\n    matchExpressions:\n    - key: release\n      operator: In\n      values:\n      - prometheus\n  resources:\n    requests:\n      memory: 400Mi\n  scrapeInterval: 30s\n  serviceAccountName: prometheus\n  serviceMonitorSelector:\n    matchExpressions:\n    - key: app\n      operator: In\n      values:\n      - nifi-cluster\n\n```\n\n----------------------------------------\n\nTITLE: Defining NifiUserGroup YAML\nDESCRIPTION: This YAML snippet defines a NifiUserGroup resource. It specifies the API version, kind, metadata, and spec. The spec includes the identity, cluster reference, user references, and access policies to configure user group settings within a Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/5_references/6_nifi_usergroup.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  identity: \"My Special Group\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n    - name: nc-controller.nifikop.mgt.cluster.local\n  accessPolicies:\n    - type: global\n      action: read\n      resource: /counters\n```\n```\n\n----------------------------------------\n\nTITLE: Retrieving User SSL Credentials from Kubernetes Secret - Console\nDESCRIPTION: This set of kubectl commands extracts the CA certificate, user certificate, and user private key for a NifiUser from a Kubernetes secret into local files, decoding their base64-encoded values. Requirements: a running Kubernetes cluster, kubectl installed and configured, and the secret (example-client-secret) pre-created by the operator. Outputs are plain PEM files suitable for client authentication. Limitation: secrets must exist in the current context/namespace and commands must be run with sufficient privileges.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['ca\\.crt']}\" | base64 -d > ca.crt\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['tls\\.crt']}\" | base64 -d > tls.crt\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['tls\\.key']}\" | base64 -d > tls.key\n```\n\n----------------------------------------\n\nTITLE: Deploying NifiDataflow CRD with Versioned Flow and Lifecycle Management in YAML\nDESCRIPTION: This YAML snippet defines a NifiDataflow resource that NiFiKop uses to deploy and manage a NiFi flow. It includes fields like parentProcessGroupID, bucketId, flowId, and flowVersion to specify the target flow version in the NiFi Registry. The spec also controls deployment behavior with syncMode (never, once, always), skip flags for invalid controllers/components, and references to the NiFi cluster, registry client, and parameter context. The updateStrategy controls how the operator updates the flow lifecycle.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: demo\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Automating NiFi Flow Encryption Update with Kubernetes InitContainer (YAML)\nDESCRIPTION: This YAML snippet defines a Kubernetes `initContainer` using the `apache/nifi-toolkit` image to automatically update the NiFi flow configuration's encryption algorithm during pod initialization. It extracts the sensitive properties key from `nifi.properties`, runs `encrypt-config.sh` for both `flow.json.gz` and `flow.xml.gz` to apply the `NIFI_PBKDF2_AES_GCM_256` algorithm, and requires correctly configured volume mounts (`data`, `conf`) for accessing the necessary files within the container. The `volumeMounts` and `mountPath` should be adjusted based on the specific deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/7_upgrade_guides/4_v1.3.1_to_v1.4.0.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ninitContainers:\n  - image: \"apache/nifi-toolkit:latest\"\n    name: nifi-toolkit\n    imagePullPolicy: Always\n    command:\n      - \"sh\"\n      - \"-c\"\n      - \"NIFI_SENSITIVE_PROPS_KEY=$(grep 'nifi.sensitive.props.key' /opt/nifi/nifi-current/conf/nifi.properties | cut -d'=' -f2) && bin/encrypt-config.sh -n /opt/nifi/nifi-current/conf/nifi.properties -f /opt/nifi/data/flow.json.gz -x -A NIFI_PBKDF2_AES_GCM_256 -s $NIFI_SENSITIVE_PROPS_KEY; bin/encrypt-config.sh -n /opt/nifi/nifi-current/conf/nifi.properties -f /opt/nifi/data/flow.xml.gz -x -A NIFI_PBKDF2_AES_GCM_256 -s $NIFI_SENSITIVE_PROPS_KEY\"\n    volumeMounts:\n      - name: data\n        mountPath: /opt/nifi/data\n      - name: conf\n        mountPath: /opt/nifi/nifi-current/conf\n```\n\n----------------------------------------\n\nTITLE: Deploying a NifiConnection YAML\nDESCRIPTION: This YAML configuration defines a NifiConnection to connect two previously deployed NifiDataflows. It specifies the source and destination dataflows, including their names, namespaces, and sub-names (ports). It also configures connection properties like back pressure thresholds, flow file expiration, and label index.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/4_manage_connections/1_deploy_connection.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiConnection\nmetadata:\n  name: connection\n  namespace: nifikop\nspec:\n  source:\n    name: input\n    namespace: nifikop\n    subName: output\n    type: dataflow\n  destination:\n    name: output\n    namespace: nifikop\n    subName: input\n    type: dataflow\n  configuration:\n    backPressureDataSizeThreshold: 100 GB\n    backPressureObjectThreshold: 10000\n    flowFileExpiration: 1 hour\n    labelIndex: 0\n    bends:\n      - posX: 550\n        posY: 550\n      - posX: 550\n        posY: 440\n      - posX: 550\n        posY: 88\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Deploying a NiFi Dataflow (Output)\nDESCRIPTION: This YAML configuration defines a NifiDataflow named 'output' within the 'nifikop' namespace. Similar to the 'input' dataflow, it specifies cluster reference, bucket ID, flow ID, flow version, registry client reference, and settings like skipping invalid components, sync mode, and update strategy. It also sets the flow position, differing from the input dataflow.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/4_manage_connections/1_deploy_connection.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiDataflow\nmetadata:\n  name: output\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  bucketId: deedb9f6-65a4-44e9-a1c9-722008fcd0ba\n  flowId: fc5363eb-801e-432f-aa94-488838674d07\n  flowVersion: 2\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  skipInvalidComponent: true\n  skipInvalidControllerService: true\n  syncMode: always\n  updateStrategy: drain\n  flowPosition:\n    posX: 750\n    posY: 0\n```\n\n----------------------------------------\n\nTITLE: Create Secret for Basic Authentication - Bash\nDESCRIPTION: This bash command demonstrates how to create a Kubernetes secret containing the credentials required for basic authentication against the NiFi cluster. It uses `kubectl create secret generic` to create a secret named `nifikop-credentials` from files containing the username, password, and optionally the CA certificate.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/1_manage_clusters/3_external_cluster.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create secret generic nifikop-credentials \\\n  --from-file=username=./secrets/username\\\n  --from-file=password=./secrets/password\\\n  --from-file=ca.crt=./secrets/ca.crt\\\n  -n nifikop-nifi\n```\n\n----------------------------------------\n\nTITLE: NiFi Cluster Autoscaling Group Deployment Specification\nDESCRIPTION: This YAML configuration defines a 'NifiNodeGroupAutoscaler' custom resource that links to a specific NiFi cluster and its node config group. It sets parameters such as the node labels selector, scaling strategy, and optional read-only node configuration, enabling fine-grained control over autoscaling behaviors for NiFi nodes within Kubernetes. Dependencies include the Custom Resource Definition for autoscalers and NiFi cluster setup.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiNodeGroupAutoscaler\nmetadata:\n  name: nifinodegroupautoscaler-sample\n  namespace: clusters\nspec:\n  clusterRef:\n    name: cluster\n    namespace: clusters\n  nodeConfigGroupId: auto_scaling\n  readOnlyConfig:\n    nifiProperties:\n      overrideConfigs: | \n        nifi.ui.banner.text=NiFiKop - Scale Group\n  nodeLabelsSelector:\n    matchLabels:\n      nifi_cr: cluster\n      nifi_node_group: auto-scaling\n  upscaleStrategy: simple\n  downscaleStrategy: lifo\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Secret for NiFi Sensitive Parameters\nDESCRIPTION: Command for creating a Kubernetes secret that contains sensitive parameters to be used with NifiParameterContext. These parameters will be converted into sensitive parameters in NiFi.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nkubectl create secret generic secret-params \\\n    --from-literal=secret1=yop \\\n    --from-literal=secret2=yep \\\n    -n nifikop\n```\n\n----------------------------------------\n\nTITLE: Obtaining UID from OpenShift Namespace Annotation for SCC Compliance - Bash\nDESCRIPTION: This snippet extracts the UID range annotation for the 'nifi' namespace in OpenShift, parsing and formatting it to comply with OpenShift Security Context Constraints (SCC) that restrict pod permissions. The extracted UID is then used in configuring NiFiKop operator deployment to set the RunAsUser security context, ensuring pods run within allowed identity ranges. It requires a configured kubectl context with permissions to read namespace annotations on OpenShift clusters.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/2_deploy_nifikop/1_quick_start.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nuid=$(kubectl get namespace nifi -o=jsonpath='{.metadata.annotations.openshift\\.io/sa\\.scc\\.supplemental-groups}' | sed 's/\\/10000$//' | tr -d '[:space:]')\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenId Connect in NiFiCluster - YAML\nDESCRIPTION: This YAML configuration snippet demonstrates how to configure OIDC settings within a NiFi cluster managed by NiFiKop.  It defines `overrideConfigs` in the `NifiCluster` specification, allowing modifications to the `nifi.properties` file.  It specifies properties such as the OIDC discovery URL, client ID, and client secret, along with identity mapping configurations. The `apiVersion`, `kind`, and other settings are required for correct deployment. \nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/5_users_authentication/1_oidc.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  readOnlyConfig:\n    # NifiProperties configuration that will be applied to the node.\n    nifiProperties:\n      webProxyHosts:\n        - nifistandard2.trycatchlearn.fr:8443\n      # Additionnal nifi.properties configuration that will override the one produced based\n      # on template and configurations.\n      overrideConfigs: |\n        nifi.security.user.oidc.discovery.url=<oidc server discovery url>\n        nifi.security.user.oidc.client.id=<oidc client's id>\n        nifi.security.user.oidc.client.secret=<oidc client's secret>\n        nifi.security.identity.mapping.pattern.dn=CN=([^,]*)(?:, (?:O|OU)=.*)?\n        nifi.security.identity.mapping.value.dn=$1\n        nifi.security.identity.mapping.transform.dn=NONE\n      ...\n   ...\n...\n```\n\n----------------------------------------\n\nTITLE: Configuring Webhook Conversion for custom resources\nDESCRIPTION: This YAML snippet shows the annotations and conversion strategy needed for webhook-based resource version conversion from 'v1alpha1' to 'v1'. It specifies the webhook service details, namespace, and path, which are essential for enabling seamless resource version upgrades in the CRDs used by NiFiKop.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/2_deploy_nifikop/1_quick_start.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n\n```\n\n----------------------------------------\n\nTITLE: Configuring package.json Script for Node.js Migration\nDESCRIPTION: Shows how to add a start script to the `package.json` file that runs the migration script with Node.js, suppressing warnings. This snippet also provides a complete example of the relevant fields—including dependencies and keywords—defining the project context. This configuration is necessary to easily execute the migration script using `npm start`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"nifikop_crd_migration\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Script to migrate from the old CRDs to the new CRDs.\",\n  \"main\": \"index.js\",\n  \"scripts\": {\n    \"start\": \"node --no-warnings index.js\",\n    \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\"\n  },\n  \"keywords\": [\n    \"K8S\",\n    \"NiFiKop\",\n    \"CRDs\"\n  ],\n  \"license\": \"ISC\",\n  \"dependencies\": {\n    \"@kubernetes/client-node\": \"^0.16.3\",\n    \"minimist\": \"^1.2.6\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop Helm Chart in Bash\nDESCRIPTION: This helm install command deploys the NiFiKop operator using a Helm chart. It specifies a release name, the chart location, image tag, and target namespace. Prerequisites include Helm v3+ and correct values for image and namespace. The chart's image repository/tag should match the previously built and pushed Docker image for successful deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/6_contributing/1_developer_guide.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nhelm install skeleton ./helm/nifikop \\\n    --set image.tag=v0.5.1-release \\\n    --namespace-{\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Manually Deleting NiFiKop CRDs\nDESCRIPTION: Commands to manually delete the Custom Resource Definitions created by the NiFiKop operator. This should be done with caution as it will delete all resources created using these CRDs.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete crd nificlusters.nifi.konpyutaika.com\nkubectl delete crd nifiusers.nifi.konpyutaika.com\nkubectl delete crd nifiusergroups.nifi.konpyutaika.com\nkubectl delete crd nifiregistryclients.nifi.konpyutaika.com\nkubectl delete crd nifiparametercontexts.nifi.konpyutaika.com\nkubectl delete crd nifidataflows.nifi.konpyutaika.com\n```\n\n----------------------------------------\n\nTITLE: Configuring LoadBalancer External Service (YAML)\nDESCRIPTION: This YAML snippet demonstrates how to define an `externalService` of type `LoadBalancer` within the Nifikop Custom Resource. It configures a service named \"nlb\" and specifies a `loadBalancerClass` (e.g., \"service.k8s.aws/nlb\"). It exposes ports 8080 and 7890, mapping them to internal Nifi listeners \"http\" and \"my-custom-udp-listener\" respectively, with the latter using the UDP protocol. Optional Kubernetes service annotations and labels are also included.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/5_references/1_nifi_cluster/7_external_service_config.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\nexternalServices:\n    - name: \"nlb\"\n      spec:\n        type: LoadBalancer\n        loadBalancerClass: \"service.k8s.aws/nlb\"\n        portConfigs:\n          - port: 8080\n            internalListenerName: \"http\"\n          - port: 7890\n            internalListenerName: \"my-custom-udp-listener\"\n            protocol: UDP\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n\n----------------------------------------\n\nTITLE: Creating an Inherited NiFi Parameter Context Resource (YAML)\nDESCRIPTION: This YAML manifest defines a `NifiParameterContext` resource named `dataflow-lifecycle-child`. It demonstrates inheritance by specifying the `dataflow-lifecycle` context within the `inheritedParameterContexts` field. Parameters defined in this child context can override those inherited from the parent, as shown with the `test` parameter. It also references the same cluster and secret as the parent.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/5_references/4_nifi_parameter_context.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle-child\nspec:\n  description: \"It is a child test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  inheritedParameterContexts:\n    - name: dataflow-lifecycle\n  parameters:\n    - name: test\n      value: toto-child\n      description: tutu (child)\n```\n\n----------------------------------------\n\nTITLE: Creating a Basic NifiParameterContext Resource (YAML)\nDESCRIPTION: This YAML snippet demonstrates the creation of a `NifiParameterContext` Kubernetes resource named `dataflow-lifecycle`. It specifies the API version, kind, metadata, and spec, including a description, cluster reference, secret reference, and a list of parameters (one non-sensitive, one marked sensitive).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/5_references/4_nifi_parameter_context.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      description: toto\n      sensistive: true\n```\n\n----------------------------------------\n\nTITLE: Configuring CRD Webhook Conversion for cert-manager - Kubernetes - YAML\nDESCRIPTION: Provides the required YAML configuration to enable a conversion webhook for NiFiKop CRDs using cert-manager. This snippet adds necessary annotations and webhook spec blocks. Input parameters include the Kubernetes namespace, certificate name (from Helm release), and webhook service name. The YAML should be incorporated into CRD definitions when deploying cert-manager with webhook conversion enabled, ensuring proper resource version conversions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/2_deploy_nifikop/1_quick_start.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiDataflow YAML\nDESCRIPTION: This YAML defines a NifiDataflow custom resource. It specifies the API version, kind, metadata (name), and the specification, including the parent process group ID, bucket ID, flow ID, flow version, synchronization mode, flags to skip invalid components, references to the NiFi cluster, registry client, and parameter context, and the update strategy. The referenced registry client and parameter context must exist.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: demo\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Define cert-manager Issuer for Let's Encrypt Staging (YAML)\nDESCRIPTION: This YAML snippet defines a `cert-manager.io/v1alpha2` `Issuer` resource named `letsencrypt-staging`. It's configured to use the Let's Encrypt ACME staging server, specifies an email for notifications, references a secret for the account key, and uses an HTTP01 challenge solver with ingress annotations for external-dns. This issuer can be used by other resources (like `NifiCluster`) to request certificates from Let's Encrypt. Requires cert-manager and external-dns.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: cert-manager.io/v1alpha2\nkind: Issuer\nmetadata:\n  name: letsencrypt-staging\nspec:\n  acme:\n    # You must replace this email address with your own.\n    # Let's Encrypt will use this to contact you about expiring\n    # certificates, and issues related to your account.\n    email: <your email address>\n    server: https://acme-staging-v02.api.letsencrypt.org/directory\n    privateKeySecretRef:\n      # Secret resource used to store the account's private key.\n      name: example-issuer-account-key\n    # Add a single challenge solver, HTTP01 using nginx\n    solvers:\n      - http01:\n          ingress:\n            ingressTemplate:\n              metadata:\n                annotations:\n                  \"external-dns.alpha.kubernetes.io/ttl\": \"5\"\n```\n\n----------------------------------------\n\nTITLE: Deploying a NiFi Dataflow in YAML with registry and parameter references\nDESCRIPTION: This YAML example defines a NifiDataflow resource, with essential components such as parentProcessGroupID, bucketId, flowId, flowVersion, syncMode, and control flags. It references the registry client, parameter context, and specifies update strategy, enabling automated lifecycle management of the NiFi flow within Kubernetes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/3_tasks/3_nifi_dataflow.md#_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: demo\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Defining Istio Gateway for HTTPS Traffic with TLS Termination in YAML\nDESCRIPTION: This Istio Gateway configuration listens for HTTPS traffic on port 443 for 'nifi.my-domain.com'. It performs TLS termination ('mode: SIMPLE') using credentials stored in the Kubernetes secret 'my-secret'. Traffic is forwarded internally as HTTP.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: nifi-gateway\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 443\n      name: https\n      protocol: HTTPS\n    tls:\n      mode: SIMPLE\n      credentialName: my-secret\n    hosts:\n    - nifi.my-domain.com\n```\n\n----------------------------------------\n\nTITLE: Manually Apply NiFiKop Custom Resource Definitions (CRDs)\nDESCRIPTION: Applies all required NiFiKop Custom Resource Definitions (CRDs) to the Kubernetes cluster using `kubectl apply`. This step is necessary if the Helm installation is performed with `--skip-crds` or if installing CRDs separately from the operator chart. Each command applies a specific CRD from a remote URL.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/2_deploy_nifikop/1_quick_start.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\n```\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\n```\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\n```\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\n```\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\n```\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\n```\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifinodegroupautoscalers.yaml\n```\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificonnections.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing Nifikop Helm Chart with Various Parameters Using Bash\nDESCRIPTION: These bash snippets provide multiple examples of installing the Nifikop Helm chart with different configurations. The first snippet demonstrates a dry-run installation with debug log level and specified namespaces. The second is a basic install example indicating how to specify a custom release name. The third installs the chart while explicitly setting the namespaces parameter. These commands require Helm and Kubernetes cluster access. They illustrate essential Helm install flags like --dry-run, --set for parameterization, and positional arguments for release and chart names.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop \\\n    --dry-run \\\n    --set logLevel=Debug \\\n    --set namespaces={\"nifikop\"}\n```\n\nLANGUAGE: bash\nCODE:\n```\nhelm install <release name> konpyutaika/nifikop\n```\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop --set namespaces={\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring authorizers.xml with Custom Authorizer (YAML Template)\nDESCRIPTION: This YAML template demonstrates how to replace the default `authorizers.xml` in NiFi with a custom configuration that uses both file-based and database-backed UserGroupProviders and AccessPolicyProviders. The template leverages NiFiKop variables such as `NodeList`, `ClusterName`, and `Namespace` to dynamically configure node identities. It defines the structure of the `authorizers.xml` file which is used to configure different authorizers and providers in NiFi. The template defines two user group providers (`file-user-group-provider` and `database-user-group-provider`), two access policy providers (`file-access-policy-provider` and `database-access-policy-provider`), and two authorizers (`managed-authorizer` and `custom-database-authorizer`).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/6_users_authorization/1_custom_user_authorizer.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n{{- $nodeList := .NodeList }}\n{{- $clusterName := .ClusterName }}\n{{- $namespace := .Namespace }}<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>\n<authorizers>\n    <userGroupProvider>\n        <identifier>file-user-group-provider</identifier>\n        <class>org.apache.nifi.authorization.FileUserGroupProvider</class>\n        <property name=\"Users File\">../data/users.xml</property>\n        <property name=\"Legacy Authorized Users File\"></property>\n        <property name=\"Initial User Identity admin\">{{ .ControllerUser }}</property>\n{{- range $i, $host := .NodeList }}\n        <property name=\"Initial User Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n    </userGroupProvider>\n    <userGroupProvider>\n        <identifier>database-user-group-provider</identifier>\n        <class>my.custom.DatabaseUserGroupProvider</class>\n        <!-- Any extra configuration for this provider goes here -->\n{{- range $i, $host := .NodeList }}\n        <property name=\"Initial User Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n    </userGroupProvider>\n    <accessPolicyProvider>\n        <identifier>file-access-policy-provider</identifier>\n        <class>org.apache.nifi.authorization.FileAccessPolicyProvider</class>\n        <property name=\"User Group Provider\">file-user-group-provider</property>\n        <property name=\"Authorizations File\">../data/authorizations.xml</property>\n        <property name=\"Initial Admin Identity\">{{ .ControllerUser }}</property>\n        <property name=\"Legacy Authorized Users File\"></property>\n{{- range $i, $host := .NodeList }}\n        <property name=\"Node Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n\t\t<property name=\"Node Group\"></property>\n    </accessPolicyProvider>\n    <accessPolicyProvider>\n        <identifier>database-access-policy-provider</identifier>\n        <class>my.custom.DatabaseAccessPolicyProvider</class>\n        <!-- Any extra configuration for this provider goes here -->\n{{- range $i, $host := .NodeList }}\n        <property name=\"Node Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n\t\t<property name=\"Node Group\"></property>\n    </accessPolicyProvider>\n    <authorizer>\n        <identifier>managed-authorizer</identifier>\n        <class>org.apache.nifi.authorization.StandardManagedAuthorizer</class>\n        <property name=\"Access Policy Provider\">file-access-policy-provider</property>\n    </authorizer>\n    <authorizer>\n        <identifier>custom-database-authorizer</identifier>\n        <class>my.custom.DatabaseAuthorizer</class>\n        <property name=\"Access Policy Provider\">database-access-policy-provider</property>\n    </authorizer>\n</authorizers>\n\n```\n\n----------------------------------------\n\nTITLE: Creating a NifiRegistryClient in YAML\nDESCRIPTION: Example of defining a NifiRegistryClient custom resource that connects to a NiFi Registry server. This is required before deploying dataflows as NiFiKop manages dataflows using the NiFi Registry feature.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiRegistryClient\nmetadata:\n  name: registry-client-example\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Registry client managed by NiFiKop\"\n  uri: \"http://nifi.hostname.com:18080\"\n```\n\n----------------------------------------\n\nTITLE: Verifying Kubernetes HPA Resource for NiFi Node Autoscaling in Console\nDESCRIPTION: Shows the console command output from running \"kubectl get hpa\" in the clusters namespace to verify the status of the Kubernetes Horizontal Pod Autoscaler resource created by KEDA. It confirms the HPA is referencing the correct NiFi node group autoscaler, provides current metric values driving scaling, and lists configured minimum, maximum, and current replica counts along with the HPA age. This snippet depends on having kubectl access and a configured Kubernetes context targeting the correct cluster and namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_8\n\nLANGUAGE: console\nCODE:\n```\nkubectl get -n clusters hpa\nNAME                REFERENCE                                                TARGETS         MINPODS   MAXPODS   REPLICAS   AGE\nkeda-hpa-cluster    NifiNodeGroupAutoscaler/nifinodegroupautoscaler-sample     1833m/2 (avg)   1         3         2          25d\n```\n\n----------------------------------------\n\nTITLE: Uninstalling/Purging a Helm Release (Bash)\nDESCRIPTION: Shows how to remove a Helm release. `helm del <release_name>` (or `helm uninstall <release_name>`) removes the Kubernetes components associated with the chart but keeps the release history. `helm delete --purge <release_name>` (deprecated in Helm 3, equivalent to `helm uninstall <release_name>`) permanently removes the release and its history, allowing the name to be reused.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nhelm del nifikop\n```\n\nLANGUAGE: bash\nCODE:\n```\nhelm delete --purge nifikop\n```\n\n----------------------------------------\n\nTITLE: Creating NiFi Registry Client in YAML\nDESCRIPTION: This YAML snippet defines a `NifiRegistryClient` resource.  It specifies the API version, kind, and metadata like the name. The `spec` section includes the `clusterRef` (referencing a NiFi cluster), a description, and the `uri` of the NiFi registry.  This resource likely configures a connection to a specific NiFi registry instance.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/5_references/3_nifi_registry_client.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiRegistryClient\nmetadata:\n  name: squidflow\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Squidflow demo\"\n  uri: \"http://nifi-registry:18080\"\n```\n```\n\n----------------------------------------\n\nTITLE: Deploying Prometheus Instance in Kubernetes\nDESCRIPTION: Creates a Prometheus instance configured to scrape metrics from NiFi cluster. Specifies resource requirements, evaluation intervals, and selectors to identify NiFi-related monitoring targets.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: Prometheus\nmetadata:\n  name: prometheus\n  namespace: monitoring-system\nspec:\n  enableAdminAPI: false\n  evaluationInterval: 30s\n  logLevel: debug\n  podMonitorSelector:\n    matchExpressions:\n    - key: release\n      operator: In\n      values:\n      - prometheus\n  resources:\n    requests:\n      memory: 400Mi\n  scrapeInterval: 30s\n  serviceAccountName: prometheus\n  serviceMonitorSelector:\n    matchExpressions:\n    - key: app\n      operator: In\n      values:\n      - nifi-cluster\n```\n\n----------------------------------------\n\nTITLE: Advanced NiFi Storage Directory Customization for High Performance\nDESCRIPTION: This YAML example shows how to configure multiple directories for NiFi's content and provenance repositories by setting custom directory paths and persistent volume claims. It illustrates how to extend NiFi's storage options for high-performance and scalable deployments, referencing storage class and size parameters.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_4\n\nLANGUAGE: YAML\nCODE:\n```\n... \nreadOnlyConfig:\n  nifiProperties:\n    overrideConfigs: |\n      nifi.content.repository.directory.dir1=../content-additional/dir1\n      nifi.content.repository.directory.dir2=../content-additional/dir2\n      nifi.content.repository.directory.dir3=../content-additional/dir3\n      nifi.provenance.repository.directory.dir1=../provenance-additional/dir1\n      nifi.provenance.repository.directory.dir2=../provenance-additional/dir2\n...\nnodeConfigGroups:\n  default_group:\n    ...\n    storageConfigs:\n    - mountPath: \"/opt/nifi/content-additional/dir1\"\n      name: content-repository-dir1\n      metadata:\n        labels:\n          my-label: my-value\n        annotations:\n          my-annotation: my-value\n      pvcSpec:\n        accessModes:\n          - ReadWriteOnce\n        storageClassName: {{ storageClassName }}\n        resources:\n          requests:\n            storage: 100G\n    - mountPath: \"/opt/nifi/content-additional/dir2\"\n      name: content-repository-dir2\n      metadata:\n        labels:\n          my-label: my-value\n        annotations:\n          my-annotation: my-value\n      pvcSpec:\n        accessModes:\n          - ReadWriteOnce\n        storageClassName: {{ storageClassName }}\n        resources:\n          requests:\n            storage: 100G\n    - mountPath: \"/opt/nifi/content-additional/dir3\"\n      name: content-repository-dir3\n      metadata:\n        labels:\n          my-label: my-value\n        annotations:\n          my-annotation: my-value\n      pvcSpec:\n        accessModes:\n          - ReadWriteOnce\n        storageClassName: {{ storageClassName }}\n        resources:\n          requests:\n            storage: 100G\n    - mountPath: \"/opt/nifi/provenance-additional/dir1\"\n      name: provenance-repository-dir1\n      metadata:\n        labels:\n          my-label: my-value\n        annotations:\n          my-annotation: my-value\n    - mountPath: \"/opt/nifi/provenance-additional/dir2\"\n      name: provenance-repository-dir2\n      metadata:\n        labels:\n          my-label: my-value\n        annotations:\n          my-annotation: my-value\n      pvcSpec:\n        accessModes:\n          - ReadWriteOnce\n        storageClassName: {{ storageClassName }}\n        resources:\n          requests:\n            storage: 100G\n```\n\n----------------------------------------\n\nTITLE: YAML configuration for NifiUserGroup API resource\nDESCRIPTION: Defines a NifiUserGroup resource in YAML format to specify a user group with cluster references, user references, and access policies. This snippet includes the API version, kind, metadata, and specification sections, serving as a template for creating user group instances in a Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/5_references/6_nifi_usergroup.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n`apiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n    - name: nc-controller.nifikop.mgt.cluster.local\n  accessPolicies:\n    - type: global\n      action: read\n      resource: /counters`\n```\n\n----------------------------------------\n\nTITLE: Templated Authorizer Configuration for NiFi\nDESCRIPTION: This YAML snippet demonstrates a templated authorizer configuration intended to replace NiFi's default `authorizers.xml` file. It includes definitions for both file-based and database-based user group and access policy providers, as well as authorizers. The template uses Go templating to dynamically inject node identities based on the `NodeList` variable, ensuring that the configuration is tailored to the NiFi cluster's specific deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/1_manage_clusters/1_deploy_cluster/6_users_authorization/1_custom_user_authorizer.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n{{- $nodeList := .NodeList }}\n{{- $clusterName := .ClusterName }}\n{{- $namespace := .Namespace }}<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>\n<authorizers>\n    <userGroupProvider>\n        <identifier>file-user-group-provider</identifier>\n        <class>org.apache.nifi.authorization.FileUserGroupProvider</class>\n        <property name=\"Users File\">../data/users.xml</property>\n        <property name=\"Legacy Authorized Users File\"></property>\n        <property name=\"Initial User Identity admin\">{{ .ControllerUser }}</property>\n{{- range $i, $host := .NodeList }}\n        <property name=\"Initial User Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n    </userGroupProvider>\n    <userGroupProvider>\n        <identifier>database-user-group-provider</identifier>\n        <class>my.custom.DatabaseUserGroupProvider</class>\n        <!-- Any extra configuration for this provider goes here -->\n{{- range $i, $host := .NodeList }}\n        <property name=\"Initial User Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n    </userGroupProvider>\n    <accessPolicyProvider>\n        <identifier>file-access-policy-provider</identifier>\n        <class>org.apache.nifi.authorization.FileAccessPolicyProvider</class>\n        <property name=\"User Group Provider\">file-user-group-provider</property>\n        <property name=\"Authorizations File\">../data/authorizations.xml</property>\n        <property name=\"Initial Admin Identity\">{{ .ControllerUser }}</property>\n        <property name=\"Legacy Authorized Users File\"></property>\n{{- range $i, $host := .NodeList }}\n        <property name=\"Node Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n\t\t<property name=\"Node Group\"></property>\n    </accessPolicyProvider>\n    <accessPolicyProvider>\n        <identifier>database-access-policy-provider</identifier>\n        <class>my.custom.DatabaseAccessPolicyProvider</class>\n        <!-- Any extra configuration for this provider goes here -->\n{{- range $i, $host := .NodeList }}\n        <property name=\"Node Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n\t\t<property name=\"Node Group\"></property>\n    </accessPolicyProvider>\n    <authorizer>\n        <identifier>managed-authorizer</identifier>\n        <class>org.apache.nifi.authorization.StandardManagedAuthorizer</class>\n        <property name=\"Access Policy Provider\">file-access-policy-provider</property>\n    </authorizer>\n    <authorizer>\n        <identifier>custom-database-authorizer</identifier>\n        <class>my.custom.DatabaseAuthorizer</class>\n        <property name=\"Access Policy Provider\">database-access-policy-provider</property>\n    </authorizer>\n</authorizers>\n\n```\n\n----------------------------------------\n\nTITLE: Create NifiUser Resource for Client Credentials (Console)\nDESCRIPTION: This console command uses `kubectl apply` with a heredoc to create a `NifiUser` custom resource named `example-client` in the `nifi` namespace. The resource references the `nifi` cluster and specifies that the generated client certificate and credentials should be stored in a Kubernetes secret named `example-client-secret`. The operator watches for this resource and automatically generates the required SSL credentials. Requires kubectl and the NifiUser CRD.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_3\n\nLANGUAGE: console\nCODE:\n```\ncat << EOF | kubectl apply -n nifi -f -\napiVersion:  nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: example-client\n  namespace: nifi\nspec:\n  clusterRef:\n    name: nifi\n  secretName: example-client-secret\nEOF\n```\n\n----------------------------------------\n\nTITLE: NifiUser Creation\nDESCRIPTION: This YAML configuration defines a NifiUser resource to create a new user certificate for accessing the NiFi cluster. It specifies the cluster reference, secret name to store the credentials, and includes options like JKS format.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion:  nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: example-client\n  namespace: nifi\nspec:\n  clusterRef:\n    name: nifi\n  secretName: example-client-secret\n```\n\n----------------------------------------\n\nTITLE: Pushing NiFiKop Operator Docker Image to Registry (Bash)\nDESCRIPTION: Executes the 'make docker-push' command to upload the locally built Docker image of the NiFiKop operator to the specified Docker registry. The image is tagged based on the project version and branch. This makes the image available for Kubernetes deployments using tools like Helm. Requires Docker to be logged in to the target registry.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/6_contributing/1_developer_guide.md#_snippet_7\n\nLANGUAGE: Bash\nCODE:\n```\nmake docker-push\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop with Custom Parameters\nDESCRIPTION: Helm command to install NiFiKop with custom namespace configuration. Demonstrates how to set specific parameters during installation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop --set namespaces={\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Installing Nifikop Chart with Custom Parameter Set\nDESCRIPTION: Helm command to install the Nifikop chart while specifying namespace parameters. Enables customizing deployment configurations at install time.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop --set namespaces={\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Defining Managed Users in NifiCluster CRD (YAML)\nDESCRIPTION: This YAML snippet demonstrates how to define users for the automatically managed 'admins' and 'readers' groups within the NifiCluster custom resource specification. Users listed under `managedAdminUsers` gain full administrative access, while those under `managedReaderUsers` receive read-only access. The operator automatically creates corresponding NifiUser resources and manages their membership in the respective NifiUserGroup resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/2_manage_users_and_accesses/3_managed_groups.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  ...\n  oneNifiNodePerNode: false\n  propagateLabels: true\n  managedAdminUsers:\n    -  identity: \"alexandre.guitton@konpyutaika.com\"\n       name: \"aguitton\"\n    -  identity: \"nifiuser@konpyutaika.com\"\n       name: \"nifiuser\"\n  managedReaderUsers:\n    -  identity: \"toto@konpyutaika.com\"\n       name: \"toto\"\n    ...\n```\n\n----------------------------------------\n\nTITLE: Node Configuration YAML Example\nDESCRIPTION: This YAML snippet demonstrates the structure and configuration options available for a NiFi node. It sets properties like `provenanceStorage`, `runAsUser`, `isNode`, `imagePullPolicy`, `externalVolumeConfigs`, and `storageConfigs`. This configuration is crucial for defining how a NiFi node operates within a Kubernetes cluster and manages its resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/5_references/1_nifi_cluster/3_node_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n   default_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      # Set this to true if the instance is a node in a cluster.\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#basic-cluster-setup\n      isNode: true\n      # Additionnal metadata to merge to the pod associated\n      podMetadata:\n        annotations:\n          node-annotation: \"node-annotation-value\"\n        labels:\n          node-label: \"node-label-value\"\n      # Docker image used by the operator to create the node associated\n      # https://hub.docker.com/r/apache/nifi/\n#      image: \"apache/nifi:1.11.2\"\n      # nodeAffinity can be specified, operator populates this value if new pvc added later to node\n      # https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity\n#      nodeAffinity:\n      # imagePullPolicy define the pull policy for NiFi cluster docker image\n      imagePullPolicy: IfNotPresent\n      # priorityClassName define the name of the priority class to be applied to these nodes\n      priorityClassName: \"example-priority-class-name\"\n      # externalVolumeConfigs specifies a list of volume to mount into the main container.\n      externalVolumeConfigs:\n        - name: example-volume\n          mountPath: \"/opt/nifi/example\"\n          secret:\n            secretName: \"raw-controller\"\n      # storageConfigs specifies the node related configs\n      storageConfigs:\n        # Name of the storage config, used to name PV to reuse into sidecars for example.\n        - name: provenance-repository\n          # Retain this PVC throughout NifiCluster deletions.\n          reclaimPolicy: Retain\n          # Path where the volume will be mount into the main nifi container inside the pod.\n          mountPath: \"/opt/nifi/provenance_repository\"\n          # Metadata to attach to the PVC that gets created\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          # Kubernetes PVC spec\n          # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          reclaimPolicy: Delete\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n```\n\n----------------------------------------\n\nTITLE: Configuring External Services in NiFiKop with YAML\nDESCRIPTION: Example YAML configuration for setting up an external service of type ClusterIP with HTTP port 8080 mapping to an internal listener, including custom annotations and labels.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/5_references/1_nifi_cluster/7_external_service_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  externalServices:\n    - name: \"clusterip\"\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8080\n            internalListenerName: \"http\"\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n\n----------------------------------------\n\nTITLE: Creating a NiFi User with SSL Credentials\nDESCRIPTION: YAML configuration for creating a NiFi user with SSL credentials. This creates a user and stores its credentials (certificates and keys) in a Kubernetes secret, allowing secure access to the NiFi cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion:  nifi.konpyutaika.com/v1alpha1\nkind: NifiUser\nmetadata:\n  name: example-client\n  namespace: nifi\nspec:\n  clusterRef:\n    name: nifi\n  secretName: example-client-secret\n```\n\n----------------------------------------\n\nTITLE: Removing a Node from NiFiKop Cluster - YAML\nDESCRIPTION: This YAML manifest revision demonstrates graceful downscale by removing a node entry from the NifiCluster spec.nodes section. The configuration maintains group definitions and listener settings, and disables a node by removing it (here, id 2 is commented out). Prerequisites: operator readiness and awareness of data migration/offload steps. Input: Adjusted list of ids. Output: Kubernetes triggers node decommissioning via NiFiKop, and data offload occurs. Note: The retryDurationMinutes parameter influences rollback behavior if process is slow.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  headlessServiceEnabled: true\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.11.3\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n#    - id: 2\n#      nodeConfigGroup: \"default_group\"\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: Defining Managed Admin and Reader Users in NifiKop (YAML)\nDESCRIPTION: This YAML configuration snippet shows how to declare managed admin and reader users in the spec of a NifiCluster resource for use with the NifiKop Kubernetes operator. The 'managedAdminUsers' and 'managedReaderUsers' fields accept lists of user objects, each requiring an 'identity' (email or system identity) and a 'name', to automate RBAC group management within NiFi. Prerequisites include a functioning NifiKop installation and access to a Kubernetes cluster. Inputs consist of user identity and name details; output is the automated creation and management of NiFi users and their groups. The snippet should be added or modified in your NifiCluster YAML custom resource definitions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/3_manage_nifi/2_manage_users_and_accesses/3_managed_groups.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  ...\n  oneNifiNodePerNode: false\n  propagateLabels: true\n  managedAdminUsers:\n    -  identity: \"alexandre.guitton@konpyutaika.com\"\n       name: \"aguitton\"\n    -  identity: \"nifiuser@konpyutaika.com\"\n       name: \"nifiuser\"\n  managedReaderUsers:\n    -  identity: \"toto@konpyutaika.com\"\n       name: \"toto\"\n  ...\n\n```\n\n----------------------------------------\n\nTITLE: Configuring NifiNodeGroupAutoscaler Resource for NiFiKop in YAML\nDESCRIPTION: This YAML snippet defines an instance of the NifiNodeGroupAutoscaler custom resource for use with the NiFiKop operator on Kubernetes. It references the NiFiCluster object, specifies the node config group, and uses label selectors to determine which nodes are affected. The snippet configures upscale and downscale strategies as well as other customization options such as the node labels selector and initial replicas. Required dependencies include an operational Kubernetes cluster with the NiFiKop CRDs installed, an existing NiFiCluster, and appropriate node groups defined within that cluster. Inputs include the cluster reference, nodeConfigGroupId, nodeLabelsSelector, and scaling strategies; the resource controls node scaling behavior and cannot be applied unless all referenced components exist.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/5_references/7_nifi_nodegroup_autoscaler.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiNodeGroupAutoscaler\nmetadata:\n  name: nifinodegroupautoscaler-sample\nspec:\n  # contains the reference to the NifiCluster with the one the node group autoscaler is linked.\n  clusterRef:\n    name: nificluster-name\n    namespace: nifikop\n  # defines the id of the NodeConfig contained in NifiCluster.Spec.NodeConfigGroups\n  nodeConfigGroupId: default-node-group\n  # The selector used to identify nodes in NifiCluster.Spec.Nodes this autoscaler will manage\n  # Use Node.Labels in combination with this selector to clearly define which nodes will be managed by this autoscaler \n  nodeLabelsSelector: \n    matchLabels:\n      nifi_cr: nificluster-name\n      nifi_node_group: default-node-group\n  # the strategy used to decide how to add nodes to a nifi cluster\n  upscaleStrategy: simple\n  # the strategy used to decide how to remove nodes from an existing cluster\n  downscaleStrategy: lifo\n```\n\n----------------------------------------\n\nTITLE: Installing cert-manager Directly using kubectl (Bash)\nDESCRIPTION: Applies the cert-manager manifest directly from its GitHub release URL using kubectl. This installs both the CustomResourceDefinitions (CRDs) and the cert-manager components into the cluster. Version v1.7.2 is used in this example.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/2_deploy_nifikop/1_quick_start.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Install the CustomResourceDefinitions and cert-manager itself\nkubectl apply -f \\\n    https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.yaml\n```\n\n----------------------------------------\n\nTITLE: Creating a Kubernetes Secret for Sensitive Parameters using kubectl\nDESCRIPTION: Demonstrates using the `kubectl create secret generic` command to create a Kubernetes secret. This secret holds sensitive key-value pairs (`--from-literal`) which can be referenced by a `NifiParameterContext` to manage sensitive NiFi parameters. The `-n` flag specifies the namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl create secret generic secret-params \\\n    --from-literal=secret1=yop \\\n    --from-literal=secret2=yep \\\n    -n nifikop\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiRegistryClient Resource in YAML\nDESCRIPTION: This YAML snippet defines a NifiRegistryClient Kubernetes resource. It specifies the API version and kind for the NiFi operator, sets the resource metadata (name), and configures the client's connection details including the associated NiFi cluster reference, a description, and the URI of the NiFi Registry instance.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/5_references/3_nifi_registry_client.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiRegistryClient\nmetadata:\n  name: squidflow\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Squidflow demo\"\n  uri: \"http://nifi-registry:18080\"\n```\n\n----------------------------------------\n\nTITLE: Deploying ServiceMonitor for NiFi Metrics\nDESCRIPTION: Creates a ServiceMonitor resource that configures Prometheus to scrape NiFi metrics exposed through the Prometheus port, with relabeling rules to capture pod information.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: cluster\n  namespace: monitoring-system\n  labels:\n    app: nifi-cluster\n    nifi_cr: cluster\nspec:\n  selector:\n    matchLabels:\n      app: nifi\n      nifi_cr: cluster\n  namespaceSelector:\n    matchNames:\n      - clusters\n  endpoints:\n    - interval: 10s\n      port: prometheus\n      path: /metrics\n      honorLabels: true\n      relabelings:\n        - sourceLabels: [__meta_kubernetes_pod_ip]\n          separator: ;\n          regex: (.*)\n          targetLabel: pod_ip\n          replacement: $1\n          action: replace\n        - sourceLabels: [__meta_kubernetes_pod_label_nodeId]\n          separator: ;\n          regex: (.*)\n          targetLabel: nodeId\n          replacement: $1\n          action: replace\n        - sourceLabels: [__meta_kubernetes_pod_label_nifi_cr]\n          separator: ;\n          regex: (.*)\n          targetLabel: nifi_cr\n          replacement: $1\n          action: replace\n```\n\n----------------------------------------\n\nTITLE: Defining NifiUser Custom Resource in YAML\nDESCRIPTION: This YAML snippet defines a NifiUser custom resource instance for the nifikop NiFi operator. It specifies metadata including the resource name and the user identity through the 'spec' field, references the target NiFi cluster by name and namespace, and sets preferences for certificate creation. The snippet serves as an example for creating a NifiUser resource to be managed within Kubernetes with no certificate generation requested here.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/5_references/2_nifi_user.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiUser\nmetadata:\n  name: aguitton\nspec:\n  identity: alexandre.guitton@konpyutaika.com\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  createCert: false\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Registry Client Custom Resource in YAML\nDESCRIPTION: This YAML snippet demonstrates how to define a NifiRegistryClient custom resource for the nifikop operator in Kubernetes. The configuration sets the apiVersion, kind, metadata, and spec properties that specify the connection to a NiFi Registry endpoint, referencing a cluster and providing a URI and description. Required dependencies include a functional Kubernetes cluster, the Nifi KOP operator installed, and an existing NifiCluster referred by clusterRef. Key parameters are the clusterRef for linking to a NiFi cluster, the URI for the Registry endpoint, and the description for identification. Inputs are Kubernetes YAML syntax objects, and outputs are Kubernetes resources managed by the operator. The manifest must be applied using kubectl, and fields like uri and clusterRef are required.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/5_references/3_nifi_registry_client.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiRegistryClient\nmetadata:\n  name: squidflow\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Squidflow demo\"\n  uri: \"http://nifi-registry:18080\"\n```\n\n----------------------------------------\n\nTITLE: Adding a new node to NiFiCluster with YAML configuration\nDESCRIPTION: This YAML snippet defines a new node with 'id: 25' added to the existing NiFiCluster specification. It specifies the node's configuration group, storage, resources, and labels, facilitating an automated cluster scale-up by applying this configuration via kubectl. The configuration ensures the new node integrates seamlessly with existing nodes, maintaining cluster consistency.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n  clusterManager: zookeeper\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.12.1\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n    - id: 2\n      nodeConfigGroup: \"default_group\"\n# >>>> START: The new node\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n# <<<< END\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: Describing NiFiCluster Status - Kubectl - Shell\nDESCRIPTION: This shell command uses `kubectl describe` to retrieve detailed information about the `simplenifi` `NifiCluster` custom resource in the `nifi` namespace. This is particularly useful during a scale-down operation to monitor the `Status` field, which shows the state of each node, including the graceful action steps being performed during decommissioning.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nkubectl describe nificluster simplenifi\n```\n\n----------------------------------------\n\nTITLE: Declaring NiFi Cluster Nodes with Specific NodeConfigGroup or Inline Configurations in YAML\nDESCRIPTION: This YAML snippet illustrates how to declare individual NiFi cluster nodes using either a predefined nodeConfigGroup or an inline nodeConfig. Nodes reference one of the groups defined previously (e.g., default_group or high_mem_group) or specify custom resource requirements inline. The id field identifies the node, nodeConfigGroup specifies reusable configuration groups, and nodeConfig allows node-level overrides. This enables heterogeneous cluster topologies combining multiple resource profiles. Inputs include node ids and configuration references; outputs are node specifications deployed as pods in Kubernetes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n- id: 0\n  nodeConfigGroup: \"default_group\"\n- id: 2\n  nodeConfigGroup: \"high_mem_group\"\n- id: 3\n  nodeConfigGroup: \"high_mem_group\"\n- id: 5\n  nodeConfig:\n    resourcesRequirements:\n      limits:\n        cpu: \"2\"\n        memory: 3Gi\n      requests:\n        cpu: \"1\"\n        memory: 1Gi\n```\n\n----------------------------------------\n\nTITLE: YAML Definition of NodeConfig for Kubernetes NiFi Deployment\nDESCRIPTION: This YAML snippet defines the structure of a node configuration for deploying NiFi on Kubernetes. It specifies parameters such as provenance storage limits, user IDs, node clustering options, container images, volume mounts, and persistent volume claims. The configuration enables customized deployment of NiFi nodes with support for external volumes and storage management, relying on standard Kubernetes resources and annotations.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/5_references/1_nifi_cluster/3_node_config.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n   default_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      # Set this to true if the instance is a node in a cluster.\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#basic-cluster-setup\n      isNode: true\n      # Additionnal metadata to merge to the pod associated\n      podMetadata:\n        annotations:\n          node-annotation: \"node-annotation-value\"\n        labels:\n          node-label: \"node-label-value\"\n      # Docker image used by the operator to create the node associated\n      # https://hub.docker.com/r/apache/nifi/\n#      image: \"apache/nifi:1.11.2\"\n      # nodeAffinity can be specified, operator populates this value if new pvc added later to node\n      # https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity\n#      nodeAffinity:\n      # imagePullPolicy define the pull policy for NiFi cluster docker image\n      imagePullPolicy: IfNotPresent\n      # priorityClassName define the name of the priority class to be applied to these nodes\n      priorityClassName: \"example-priority-class-name\"\n      # externalVolumeConfigs specifies a list of volume to mount into the main container.\n      externalVolumeConfigs:\n        - name: example-volume\n          mountPath: \"/opt/nifi/example\"\n          secret:\n            secretName: \"raw-controller\"\n      # storageConfigs specifies the node related configs\n      storageConfigs:\n        # Name of the storage config, used to name PV to reuse into sidecars for example.\n        - name: provenance-repository\n          # Path where the volume will be mount into the main nifi container inside the pod.\n          mountPath: \"/opt/nifi/provenance_repository\"\n          # Metadata to attach to the PVC that gets created\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          # Kubernetes PVC spec\n          # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Nodes with YAML in NiFiKop\nDESCRIPTION: Example YAML configuration for multiple NiFi nodes, demonstrating the use of nodeConfigGroup for simplified configuration, readOnlyConfig for properties that trigger rolling upgrades, and nodeConfig for resource and storage specifications.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/5_references/1_nifi_cluster/4_node.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n    - id: 0\n      # nodeConfigGroup can be used to ease the node configuration, if set only the id is required\n      nodeConfigGroup: \"default_group\"\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        nifiProperties:\n          overrideConfigs: |\n            nifi.ui.banner.text=NiFiKop - Node 0\n      # node configuration\n#       nodeConfig:\n    - id: 2\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        overrideConfigs: |\n          nifi.ui.banner.text=NiFiKop - Node 2\n      # node configuration\n      nodeConfig:\n        resourcesRequirements:\n          limits:\n            cpu: \"2\"\n            memory: 3Gi\n          requests:\n            cpu: \"1\"\n            memory: 1Gi\n        storageConfigs:\n          # Name of the storage config, used to name PV to reuse into sidecars for example.\n          - name: provenance-repository\n            # Path where the volume will be mount into the main nifi container inside the pod.\n            mountPath: \"/opt/nifi/provenance_repository\"\n            # Metadata to attach to the PVC that gets created\n            metadata:\n              labels:\n                my-label: my-value\n              annotations:\n                my-annotation: my-value\n            # Kubernetes PVC spec\n            # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n            pvcSpec:\n              accessModes:\n                - ReadWriteOnce\n              storageClassName: \"standard\"\n              resources:\n                requests:\n                  storage: 8Gi\n```\n\n----------------------------------------\n\nTITLE: Deploying KEDA ScaledObject for NiFi Node Autoscaling Using YAML\nDESCRIPTION: Defines a KEDA ScaledObject that enables event-driven horizontal scaling for NiFi nodes via Prometheus metrics. The resource targets a 'NifiNodeGroupAutoscaler' custom resource named 'nifinodegroupautoscaler-sample' within the 'clusters' namespace. It configures parameters such as polling interval (30s), cooldown period (300s), minimum and maximum replicas (1 to 3), and a fallback scaling policy for failure scenarios. The trigger uses a Prometheus metric query to determine scaling thresholds dynamically. This manifest requires KEDA installed in the cluster and a running Prometheus instance accessible at the specified server address.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: cluster\n  namespace: clusters\nspec:\n  scaleTargetRef:\n    apiVersion:    nifi.konpyutaika.com/v1alpha1     # Optional. Default: apps/v1\n    kind:          NifiNodeGroupAutoscaler           # Optional. Default: Deployment\n    name:          nifinodegroupautoscaler-sample    # Mandatory. Must be in the same namespace as the ScaledObject\n    envSourceContainerName: nifi                     # Optional. Default: .spec.template.spec.containers[0]\n  pollingInterval:  30                               # Optional. Default: 30 seconds\n  cooldownPeriod:   300                              # Optional. Default: 300 seconds\n  idleReplicaCount: 0                                # Optional. Default: ignored, must be less than minReplicaCount \n  minReplicaCount:  1                                # Optional. Default: 0\n  maxReplicaCount:  3                                # Optional. Default: 100\n  fallback:                                          # Optional. Section to specify fallback options\n    failureThreshold: 5                              # Mandatory if fallback section is included\n    replicas: 1                                      # Mandatory if fallback section is included\n  # advanced:                                          # Optional. Section to specify advanced options\n  #   restoreToOriginalReplicaCount: true              # Optional. Whether the target resource should be scaled back to original replicas count, after the ScaledObject is deleted\n  #   horizontalPodAutoscalerConfig:                   # Optional. Section to specify HPA related options\n  #     name: {name-of-hpa-resource}                   # Optional. Default: keda-hpa-{scaled-object-name}\n  #     behavior:                                      # Optional. Use to modify HPA's scaling behavior\n  #       scaleDown:\n  #         stabilizationWindowSeconds: 300 <--- \n  #         policies:\n  #         - type: Percent\n  #           value: 100\n  #           periodSeconds: 15\n  triggers:\n    - type: prometheus\n      metadata:\n        serverAddress: http://prometheus-operated.monitoring-system.svc:9090\n        metricName: http_requests_total\n        threshold: <threshold value>\n        query: <prometheus query>\n```\n\n----------------------------------------\n\nTITLE: Defining a NiFi User in YAML for NiFiKOP\nDESCRIPTION: An example YAML configuration for creating a NiFi user in Kubernetes using NiFiKOP. This definition specifies the user's identity, references a NiFi cluster, and sets certificate generation to false.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/5_references/2_nifi_user.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: aguitton\nspec:\n  identity: alexandre.guitton@konpyutaika.com\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  createCert: false\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Nodes with YAML in Kubernetes\nDESCRIPTION: Example YAML configuration for NiFi nodes that specifies provenance storage, user permissions, cluster settings, storage configuration, and external volume mounting. This configuration demonstrates how to set up NiFi nodes in a Kubernetes environment using NiFiKop.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/5_references/1_nifi_cluster/3_node_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ndefault_group:\n   # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n   # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n   provenanceStorage: \"10 GB\"\n   #RunAsUser define the id of the user to run in the Nifi image\n   # +kubebuilder:validation:Minimum=1\n   runAsUser: 1000\n   # Set this to true if the instance is a node in a cluster.\n   # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#basic-cluster-setup\n   isNode: true\n   # Additionnal metadata to merge to the pod associated\n   podMetadata:\n     annotations:\n       node-annotation: \"node-annotation-value\"\n     labels:\n       node-label: \"node-label-value\"\n   # Docker image used by the operator to create the node associated\n   # https://hub.docker.com/r/apache/nifi/\n#      image: \"apache/nifi:1.11.2\"\n   # nodeAffinity can be specified, operator populates this value if new pvc added later to node\n   # https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity\n#      nodeAffinity:\n   # imagePullPolicy define the pull policy for NiFi cluster docker image\n   imagePullPolicy: IfNotPresent\n   # priorityClassName define the name of the priority class to be applied to these nodes\n   priorityClassName: \"example-priority-class-name\"\n   # externalVolumeConfigs specifies a list of volume to mount into the main container.\n   externalVolumeConfigs:\n     - name: example-volume\n       mountPath: \"/opt/nifi/example\"\n       secret:\n         secretName: \"raw-controller\"\n   # storageConfigs specifies the node related configs\n   storageConfigs:\n     # Name of the storage config, used to name PV to reuse into sidecars for example.\n     - name: provenance-repository\n       # Path where the volume will be mount into the main nifi container inside the pod.\n       mountPath: \"/opt/nifi/provenance_repository\"\n       # Metadata to attach to the PVC that gets created\n       metadata:\n         labels:\n           my-label: my-value\n         annotations:\n           my-annotation: my-value\n       # Kubernetes PVC spec\n       # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n       pvcSpec:\n         accessModes:\n           - ReadWriteOnce\n         storageClassName: \"standard\"\n         resources:\n           requests:\n             storage: 10Gi\n     - mountPath: \"/opt/nifi/nifi-current/logs\"\n       name: logs\n       pvcSpec:\n         accessModes:\n           - ReadWriteOnce\n         storageClassName: \"standard\"\n         resources:\n           requests:\n             storage: 10Gi\n```\n\n----------------------------------------\n\nTITLE: Configure NiFiKop CRD Conversion Webhook (YAML)\nDESCRIPTION: Provides a YAML snippet showing annotations and conversion strategy settings needed for NiFiKop CRDs when the conversion webhook is enabled. This configuration allows the API server to convert resource objects between different API versions using a webhook service, often related to cert-manager for securing the webhook. Requires the webhook service and certificate to be correctly deployed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/2_deploy_nifikop/1_quick_start.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n```\n\n----------------------------------------\n\nTITLE: NiFiCluster with Let's Encrypt Integration\nDESCRIPTION: This YAML configuration defines a NiFiCluster that integrates with Let's Encrypt for SSL certificate management. It sets `clusterSecure` and `siteToSiteSecure` to true, specifies the `clusterDomain` and `useExternalDNS`, and references the cert-manager Issuer in the `sslSecrets.issuerRef` field.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  clusterSecure: true\n  siteToSiteSecure: true\n  ...\n  listenersConfig:\n    clusterDomain: <DNS zone name>\n    useExternalDNS: true\n    ...\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n      issuerRef:\n        name: letsencrypt-staging\n        kind: Issuer\n```\n\n----------------------------------------\n\nTITLE: Installing Zookeeper with Helm in Bash\nDESCRIPTION: This command installs a Zookeeper cluster using the Bitnami Helm chart. It sets the namespace to `zookeeper`, requests and limits resources for CPU and memory, sets the storage class to `standard`, enables network policy, sets the replica count to 3, and creates the namespace if it doesn't exist. Replace the `storageClass` parameter value with your own StorageClass if you are not using `standard`. Requires a functioning Helm installation and access to a Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nhelm install zookeeper bitnami/zookeeper \\\n    --namespace=zookeeper \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set global.storageClass=standard \\\n    --set networkPolicy.enabled=true \\\n    --set replicaCount=3 \\\n    --create-namespace\n```\n\n----------------------------------------\n\nTITLE: Running NiFiKop Operator Locally Using Make Run Target - Bash\nDESCRIPTION: This command runs the NiFiKop operator locally in the default namespace using the Kubernetes config file from '$HOME/.kube/config'. It relies on the 'make run' target to launch the compiled operator binary with appropriate settings, suitable for local development and testing.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/6_contributing/1_developer_guide.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmake run\n```\n\n----------------------------------------\n\nTITLE: NiFiDataflow Controller Reconcile Loop\nDESCRIPTION: This snippet describes the reconcile loop for NiFiDataflow. It manages the deployment and lifecycle of dataflows by orchestrating the resources, monitoring state changes, and ensuring the NiFi cluster reflects the desired configuration, as depicted in the associated diagram.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/3_manage_dataflows/0_design_principles.md#_snippet_5\n\nLANGUAGE: Markdown\nCODE:\n```\n![NiFi dataflow's reconcile loop](/img/1_concepts/2_design_principes/dataflow_reconcile_loop.jpeg)\n```\n\n----------------------------------------\n\nTITLE: Adding KEDA Helm Repository (Console)\nDESCRIPTION: Adds the official KEDA Helm chart repository named 'kedacore' to your local Helm configuration. This step is necessary to locate and install the KEDA chart.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nhelm repo add kedacore https://kedacore.github.io/charts\n```\n\n----------------------------------------\n\nTITLE: Re-encrypting NiFi Flow Config with Encrypt-Config Tool (sh)\nDESCRIPTION: Demonstrates how to use the NiFi Encrypt-Config Tool command-line script to re-encrypt flow configuration files (`flow.xml.gz` and `flow.json.gz`) using the new default sensitive properties algorithm, `NIFI_PBKDF2_AES_GCM_256`. This requires access to the `nifi.properties` file, the flow files, and the original sensitive properties key. The `-x` flag performs decryption followed by encryption.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/7_upgrade_guides/4_v1.3.1_to_v1.4.0.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nencrypt-config.sh -n nifi.properties -f flow.xml.gz -x -s PROPERTIES_KEY -A NIFI_PBKDF2_AES_GCM_256\n```\n\nLANGUAGE: sh\nCODE:\n```\nencrypt-config.sh -n nifi.properties -f flow.json.gz -x -s PROPERTIES_KEY -A NIFI_PBKDF2_AES_GCM_256\n```\n\n----------------------------------------\n\nTITLE: Installing Helm Chart Skipping CRDs (bash)\nDESCRIPTION: Installs the Helm chart, explicitly naming the release `nifikop`, while preventing the chart from installing its associated Custom Resource Definitions (CRDs). This assumes the CRDs are already installed or managed separately. Note: The `--name` flag is deprecated in Helm 3+ in favor of placing the name argument before the chart name. Requires Helm installed and configured.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\n$ helm install --name nifikop ./helm/nifikop --set namespaces={\"nifikop\"} --skip-crds\n```\n\n----------------------------------------\n\nTITLE: Configuring Internal Listeners for NiFi in Kubernetes with YAML\nDESCRIPTION: Defines internal listeners for a NiFi cluster within a Kubernetes deployment by setting types, names, and container ports. Dependencies include a running Kubernetes environment and NiFi operator support for these configuration fields. The snippet shows fields for HTTPS, cluster communication, Site-to-Site, load-balancing, and Prometheus metrics; each internal listener is assigned a container port, which must match NiFi's internal and service-level configuration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    - type: \"https\"\n      name: \"https\"\n      containerPort: 8443\n    - type: \"cluster\"\n      name: \"cluster\"\n      containerPort: 6007\n    - type: \"s2s\"\n      name: \"s2s\"\n      containerPort: 10000\n    - type: \"prometheus\"\n      name: \"prometheus\"\n      containerPort: 9090\n    - type: \"load-balance\"\n      name: \"load-balance\"\n      containerPort: 6342\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Cluster Node Groups with Prometheus and Storage - YAML\nDESCRIPTION: This YAML snippet is part of a NiFi cluster custom resource specification defining a NodeConfigGroup named 'auto_scaling'. It configures internal listeners to expose a Prometheus metrics endpoint on port 9090, sets CPU and memory requests and limits for nodes, assigns a service account named 'external-dns', and defines multiple persistent volume claims for logs, data, extensions, and other NiFi directories. Each PVC includes labels, annotations, access modes, requested storage, and storage class settings for flexible and scalable storage provisioning in Kubernetes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n...\nspec:\n  ...\n  listenersConfig:\n    internalListeners:\n    ...\n    - containerPort: 9090\n      name: prometheus\n      type: prometheus \n    ...\n  ...\n  nodeConfigGroups:\n    auto_scaling:\n      isNode: true\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 2Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n      serviceAccountName: external-dns\n      storageConfigs:\n        - mountPath: /opt/nifi/nifi-current/logs\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/data\n          name: data\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/extensions\n          name: extensions-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/flowfile_repository\n          name: flowfile-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/nifi-current/conf\n          name: conf\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/content_repository\n          name: content-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/provenance_repository\n          name: provenance-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n  ...\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Node Configuration with YAML\nDESCRIPTION: This YAML snippet provides a sample configuration under the default_group key for setting up an Apache NiFi node using the NodeConfig structure in Kubernetes. It demonstrates how to specify provenance storage size, user and group IDs, node flags, pod metadata (annotations and labels), Docker image and pull policy, storage configurations, and mounting of external volumes. Required dependencies include Kubernetes (for deployment), NiFi Docker images, and custom resource definitions supported by NiFi's operator. Parameters such as provenanceStorage, runAsUser, isNode, imagePullPolicy, externalVolumeConfigs, and storageConfigs control the node's storage, execution security, metadata, and persistence. The configuration expects the values for paths, secrets, and image tags to be customized as per user requirements, and should be used within a Kubernetes environment supporting the referenced APIs and resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/5_references/1_nifi_cluster/3_node_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n   default_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      # Set this to true if the instance is a node in a cluster.\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#basic-cluster-setup\n      isNode: true\n      # Additionnal metadata to merge to the pod associated\n      podMetadata:\n        annotations:\n          node-annotation: \"node-annotation-value\"\n        labels:\n          node-label: \"node-label-value\"\n      # Docker image used by the operator to create the node associated\n      # https://hub.docker.com/r/apache/nifi/\n#      image: \"apache/nifi:1.11.2\"\n      # nodeAffinity can be specified, operator populates this value if new pvc added later to node\n      # https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity\n#      nodeAffinity:\n      # imagePullPolicy define the pull policy for NiFi cluster docker image\n      imagePullPolicy: IfNotPresent\n      # priorityClassName define the name of the priority class to be applied to these nodes\n      priorityClassName: \"example-priority-class-name\"\n      # externalVolumeConfigs specifies a list of volume to mount into the main container.\n      externalVolumeConfigs:\n        - name: example-volume\n          mountPath: \"/opt/nifi/example\"\n          secret:\n            secretName: \"raw-controller\"\n      # storageConfigs specifies the node related configs\n      storageConfigs:\n        # Name of the storage config, used to name PV to reuse into sidecars for example.\n        - name: provenance-repository\n          # Path where the volume will be mount into the main nifi container inside the pod.\n          mountPath: \"/opt/nifi/provenance_repository\"\n          # Kubernetes PVC spec\n          # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop with Helm Dry Run\nDESCRIPTION: Command to perform a dry run installation of the NiFiKop chart, setting the log level to Debug and constraining the operator to watch only the 'nifikop' namespace. Useful for testing configuration before actual deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop \\\n    --dry-run \\\n    --set logLevel=Debug \\\n    --set namespaces={\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Installing Helm Chart Without CRDs - Bash\nDESCRIPTION: This command installs the 'nifikop' Helm chart while skipping the installation of CRDs, by using the '--skip-crds' parameter. Prerequisites: Helm CLI access, existing CRDs in the cluster if required, and an understanding of the namespace parameter. This is useful in multi-tenant clusters or when CRDs are already installed and should not be modified by this chart deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nhelm install --name nifikop ./helm/nifikop --set namespaces={\"nifikop\"} --skip-crds\n```\n\n----------------------------------------\n\nTITLE: Getting OpenShift UID for NiFi Cluster\nDESCRIPTION: Command to extract the allowed UID/GID for the NiFi namespace in OpenShift, which is needed for proper security context configuration of the NiFi cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nuid=$(kubectl get namespace nifi -o=jsonpath='{.metadata.annotations.openshift\\.io/sa\\.scc\\.supplemental-groups}' | sed 's/\\/10000$//' | tr -d '[:space:]')\n```\n\n----------------------------------------\n\nTITLE: Packaging Helm Chart - Bash\nDESCRIPTION: This command packages the Helm chart for the NiFiKop operator.  It uses the `make helm-package` command, which generates a .tgz archive of the chart. The output is a deployable package.  This command must be run before deploying the chart.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/6_contributing/1_developer_guide.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nmake helm-package\n```\n\n----------------------------------------\n\nTITLE: Defining NifiUserGroup Resource Manifest in YAML\nDESCRIPTION: This YAML snippet defines a NifiUserGroup custom resource instance for the Nifi operator. It specifies the API version, metadata with a resource name, and the spec section containing references to a Nifi cluster, associated user nodes, and access policies granted to the group. The snippet serves as a template to create or modify user groups within a Nifi cluster managed by the nifikop controller. It requires the operator and relevant cluster/user references to exist beforehand.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/5_references/6_nifi_usergroup.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n    - name: nc-controller.nifikop.mgt.cluster.local\n  accessPolicies:\n    - type: global\n      action: read\n      resource: /counters\n```\n\n----------------------------------------\n\nTITLE: Declaring an External NiFiCluster Resource in YAML\nDESCRIPTION: This YAML snippet defines an 'external' type NifiCluster resource, setting parameters such as root process group ID, node URI template, node list, cluster type, authentication method, and secret reference. It enables the operator to manage and communicate with an external NiFi cluster by specifying how to connect and authenticate.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/3_tasks/1_nifi_cluster/4_external_cluster.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: externalcluster\nspec:\n  rootProcessGroupId: 'd37bee03-017a-1000-cff7-4eaaa82266b7'\n  nodeURITemplate: 'nifi0%d.integ.mapreduce.m0.p.fti.net:9090'\n  nodes:\n    - id: 1\n    - id: 2\n    - id: 3\n  type: 'external'\n  clientType: 'basic'\n  secretRef:\n    name: nifikop-credentials\n    namespace: nifikop-nifi\n```\n\n----------------------------------------\n\nTITLE: Creating an Istio VirtualService for HTTPS Traffic to NiFi with TLS passthrough\nDESCRIPTION: Sets up an Istio VirtualService to route HTTPS traffic accepted by the Gateway to a specific ClusterIP service within the cluster, using port 8443. This configuration handles encrypted traffic, forwarding requests to the internal NiFi service, and supporting TLS-based communication.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: nifi-vs\nspec:\n  gateways:\n  - nifi-gateway\n  hosts:\n  - nifi.my-domain.com\n  http:\n  - match:\n    - uri:\n        prefix: /\n    route:\n    - destination:\n        host: <service-name>.<namespace>.svc.cluster.local\n        port:\n          number: 8443\n```\n\n----------------------------------------\n\nTITLE: Deploying ServiceMonitor for NiFi\nDESCRIPTION: This YAML defines a `ServiceMonitor` resource. It specifies how Prometheus should discover and scrape metrics from NiFi, using a matching label and specifying a service port, path and namespace to gather those metrics from NiFi. The `relabelings` section specifies how Prometheus relabel collected metrics. This configuration is used by Prometheus to discover the NiFi service endpoints and start scraping metrics for use in scaling.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: cluster\n  namespace: monitoring-system\n  labels:\n    app: nifi-cluster\n    nifi_cr: cluster\nspec:\n  selector:\n    matchLabels:\n      app: nifi\n      nifi_cr: cluster\n  namespaceSelector:\n    matchNames:\n      - clusters\n  endpoints:\n    - interval: 10s\n      port: prometheus\n      path: /metrics\n      honorLabels: true\n      relabelings:\n        - sourceLabels: [__meta_kubernetes_pod_ip]\n          separator: ;\n          regex: (.*)\n          targetLabel: pod_ip\n          replacement: $1\n          action: replace\n        - sourceLabels: [__meta_kubernetes_pod_label_nodeId]\n          separator: ;\n          regex: (.*)\n          targetLabel: nodeId\n          replacement: $1\n          action: replace\n        - sourceLabels: [__meta_kubernetes_pod_label_nifi_cr]\n          separator: ;\n          regex: (.*)\n          targetLabel: nifi_cr\n          replacement: $1\n          action: replace\n```\n\n----------------------------------------\n\nTITLE: Deploying a NifiDataflow - Output\nDESCRIPTION: This YAML snippet defines a NifiDataflow resource named 'output'. It mirrors the 'input' dataflow configuration but uses different flow ID and flow version, as well as flow position. The namespace is 'nifikop'.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/4_manage_connections/1_deploy_connection.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiDataflow\nmetadata:\n  name: output\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  bucketId: deedb9f6-65a4-44e9-a1c9-722008fcd0ba\n  flowId: fc5363eb-801e-432f-aa94-488838674d07\n  flowVersion: 2\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  skipInvalidComponent: true\n  skipInvalidControllerService: true\n  syncMode: always\n  updateStrategy: drain\n  flowPosition:\n    posX: 750\n    posY: 0\n```\n\n----------------------------------------\n\nTITLE: Configuring External Service for NiFi Cluster in Kubernetes (YAML)\nDESCRIPTION: This configuration defines an external service for a NiFi cluster in Kubernetes, exposing internal listeners (HTTPS and HTTP-tracking) through a Kubernetes LoadBalancer service. It specifies the internal listener names, the ports for the external service, and the service type.  This allows external access to the NiFi cluster UI and custom NiFi processors via specified ports.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    - type: \"https\"\n      name: \"https\"\n      containerPort: 8443\n    - type: \"cluster\"\n      name: \"cluster\"\n      containerPort: 6007\n    - type: \"s2s\"\n      name: \"s2s\"\n      containerPort: 10000\n    - type: \"prometheus\"\n      name: \"prometheus\"\n      containerPort: 9090\n    - type: \"load-balance\"\n      name: \"load-balance\"\n      containerPort: 6342\n    - name: \"http-tracking\"\n      containerPort: 8081\nexternalServices:\n  - name: cluster-access\n    spec:\n      portConfigs:\n        - internalListenerName: https\n          port: 443\n        - internalListenerName: http-tracking\n          port: 80\n      type: LoadBalancer\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop via Helm Chart (Bash/Helm)\nDESCRIPTION: Uses the Helm CLI to install the NiFiKop operator from the local Helm chart directory (`./helm/nifikop`). It assigns the release name `skeleton`, specifies the image tag to use via `--set`, and targets the `nifikop` namespace. The `image.repository` must also match the pushed image.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/6_contributing/1_developer_guide.md#_snippet_8\n\nLANGUAGE: Bash\nCODE:\n```\nhelm install skeleton ./helm/nifikop \\\n    --set image.tag=v0.5.1-release \\\n    --namespace-{\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Properties with Multiple Override Methods in YAML\nDESCRIPTION: Example showing how to override NiFi properties using different methods with priority order (Secret > ConfigMap > Override > Default). This demonstrates referencing external ConfigMaps and Secrets along with inline property definitions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n nifiProperties:\n      # Additionnal nifi.properties configuration that will override the one produced based on template and\n      # configuration\n      overrideConfigMap:\n        # The key of the value,in data content, that we want use.\n        data: nifi.properties\n        # Name of the configmap that we want to refer.\n        name: raw\n        # Namespace where is located the secret that we want to refer.\n        namespace: nifikop\n      # Additionnal nifi.properties configuration that will override the one produced based\n      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n      overrideSecretConfig:\n        # The key of the value,in data content, that we want use.\n        data: nifi.properties\n        # Name of the configmap that we want to refer.\n        name: raw\n        # Namespace where is located the secret that we want to refer.\n        namespace: nifikop\n      # Additionnal nifi.properties configuration that will override the one produced based\n      #\ton template, configurations and overrideConfigMap\n      overrideConfigs: |\n        nifi.ui.banner.text=NiFiKop\n        nifi.sensitive.props.key=thisIsABadSensitiveKeyPassword\n```\n\n----------------------------------------\n\nTITLE: Defining NifiNodeGroupAutoscaler Resource\nDESCRIPTION: Creates a NifiNodeGroupAutoscaler Custom Resource. It references a specific NiFiCluster and the 'auto_scaling' node configuration group, defines scaling strategies (simple upscale, LIFO downscale), and uses a node label selector to identify the target nodes for autoscaling.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiNodeGroupAutoscaler\nmetadata:\n  name: nifinodegroupautoscaler-sample\n  namespace: clusters\nspec:\n  # contains the reference to the NifiCluster with the one the node group autoscaler is linked.\n  clusterRef:\n    name: cluster\n    namespace: clusters\n  # defines the id of the NodeConfig contained in NifiCluster.Spec.NodeConfigGroups\n  nodeConfigGroupId: auto_scaling\n  # readOnlyConfig can be used to pass Nifi node config\n  # which has type read-only these config changes will trigger rolling upgrade\n  readOnlyConfig:\n    nifiProperties:\n      overrideConfigs: |\n        nifi.ui.banner.text=NiFiKop - Scale Group\n  # This is an example of a node config you can apply to each replica in this node group.\n  # Any settings here will override those in the configured nodeConfigGroupId\n#  nodeConfig:\n#    nodeSelector:\n#      node_type: high-mem\n  # The selector used to identify nodes in NifiCluster.Spec.Nodes this autoscaler will manage\n  # Use Node.Labels in combination with this selector to clearly define which nodes will be managed by this autoscaler \n  nodeLabelsSelector: \n    matchLabels:\n      nifi_cr: cluster\n      nifi_node_group: auto-scaling\n  # the strategy used to decide how to add nodes to a nifi cluster\n  upscaleStrategy: simple\n  # the strategy used to decide how to remove nodes from an existing cluster\n  downscaleStrategy: lifo\n```\n\n----------------------------------------\n\nTITLE: Creating NifiParameterContext in Kubernetes with YAML\nDESCRIPTION: Example YAML configuration for defining a NifiParameterContext resource that contains parameters and references to secret parameters. This allows configuring dataflows with both regular and sensitive parameters.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\n  namespace: demo\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      value: toto\n      description: toto\n```\n\n----------------------------------------\n\nTITLE: NiFiCluster using Cert-Manager Issuer\nDESCRIPTION: This YAML snippet configures a `NifiCluster` to use a Cert-Manager Issuer for SSL certificates. It sets `clusterSecure` and `siteToSiteSecure` to `true`, specifies the cluster domain, enables external DNS, and references the Cert-Manager Issuer using `issuerRef`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  clusterSecure: true\n  siteToSiteSecure: true\n  ...\n  listenersConfig:\n    clusterDomain: <DNS zone name>\n    useExternalDNS: true\n    ...\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n      issuerRef:\n        name: letsencrypt-staging\n        kind: Issuer\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiUserGroup Resource in YAML\nDESCRIPTION: This YAML example shows how to create a NifiUserGroup resource that contains references to NiFi users and defines access policies for the group. The resource establishes a global read permission for counters to two different users.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/2_manage_users_and_accesses/2_groups_management.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  # Contains the reference to the NifiCluster with the one the registry client is linked.\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  # contains the list of reference to NifiUsers that are part to the group.\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n#     namespace: nifikop\n    - name: nc-controller.nifikop.mgt.cluster.local\n  # defines the list of access policies that will be granted to the group.\n  accessPolicies:\n      # defines the kind of access policy, could be \"global\" or \"component\".\n    - type: global\n      # defines the kind of action that will be granted, could be \"read\" or \"write\"\n      action: read\n      # resource defines the kind of resource targeted by this access policies, please refer to the following page:\n      #\thttps://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#access-policies\n      resource: /counters\n#      # componentType is used if the type is \"component\", it's allow to define the kind of component on which is the\n#      # access policy\n#      componentType: \"process-groups\"\n#      # componentId is used if the type is \"component\", it's allow to define the id of the component on which is the\n#      # access policy\n#      componentId: \"\"\n```\n\n----------------------------------------\n\nTITLE: Activating Custom NiFi Authorizer (Shell)\nDESCRIPTION: This shell command demonstrates how to set the `nifi.security.user.authorizer` property within NiFi's configuration (e.g., `nifi.properties`). This property directs NiFi to use the authorizer identified as `custom-database-authorizer` (as defined in the `authorizers.xml` file) instead of the default.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/6_users_authorization/1_custom_user_authorizer.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nnifi.security.user.authorizer=custom-database-authorizer\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFiKop Operator using Helm\nDESCRIPTION: Deploys the NiFiKop operator version 0.15.0 into the `nifi` namespace using Helm 3 from an OCI registry. It sets specific resource requests/limits and configures the operator to watch the 'nifi' namespace. Requires Helm 3 and the `nifi` namespace must be created beforehand. Optionally, `--set certManager.enabled=false` can be added for unsecured clusters.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/2_deploy_nifikop/1_quick_start.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# You have to create the namespace before executing following command\nhelm install nifikop \\\n    oci://ghcr.io/konpyutaika/helm-charts/nifikop \\\n    --namespace=nifi \\\n    --version 0.15.0 \\\n    --set image.tag=v0.15.0-release \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set namespaces={\\\"nifi\\\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring kubectl for EKS Cluster Access (Bash)\nDESCRIPTION: Writes the kubeconfig file for the specified EKS cluster, making its context available to kubectl. Requires AWS CLI to be configured and eksctl installed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/2_deploy_nifikop/1_quick_start.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\neksctl utils write-kubeconfig --cluster=${CLUSTER NAME}\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes secret for Basic Authentication\nDESCRIPTION: This command demonstrates how to create a Kubernetes secret containing the credentials required for basic authentication to the NiFi cluster. It uses kubectl to create a generic secret named `nifikop-credentials` in the `nifikop-nifi` namespace, populating it with the username, password, and optional CA certificate from local files. This secret is then referenced by the NifiCluster resource.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/3_manage_nifi/1_manage_clusters/3_external_cluster.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nkubectl create secret generic nifikop-credentials \\\n  --from-file=username=./secrets/username\\\n  --from-file=password=./secrets/password\\\n  --from-file=ca.crt=./secrets/ca.crt\\\n  -n nifikop-nifi\n```\n\n----------------------------------------\n\nTITLE: Installing Nifikop Helm Chart (Standard)\nDESCRIPTION: Install the Nifikop Helm chart with a specified release name. Replace `<release name>` with your desired name for the Helm release. This is the basic command to deploy the operator using Helm.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nhelm install <release name> konpyutaika/nifikop\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi User Custom Resource YAML Schema\nDESCRIPTION: This YAML snippet defines a sample NiFi User custom resource for Kubernetes, which specifies user metadata, identity, and cluster references within the 'nifikop' operator framework. It illustrates the minimal resource needed to instantiate a NiFi User named 'aguitton' without certificate creation. Dependencies include the NiFi K8s custom resource definition and the operational context of the nifikop operator. Key fields are 'apiVersion' for API grouping, 'kind' for resource type, 'metadata.name' for resource naming, and 'spec' properties such as 'identity', 'clusterRef', and 'createCert'. This snippet's output is a configured Kubernetes resource that is used by the operator to manage user lifecycle in the NiFi cluster. Constraints are that the referenced cluster must exist and the user identity must be valid within NiFi.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/5_references/2_nifi_user.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: aguitton\nspec:\n  identity: alexandre.guitton@konpyutaika.com\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  createCert: false\n```\n\n----------------------------------------\n\nTITLE: Defining an Inherited NifiParameterContext in YAML\nDESCRIPTION: This YAML manifest defines a `NifiParameterContext` named `dataflow-lifecycle-child`. It inherits parameters from the `dataflow-lifecycle` context (defined in the same or another namespace). It connects to the same NiFi cluster and uses the same secret (`secret-params`). The `test` parameter is overridden with a new value ('toto-child') specific to this child context.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/5_references/4_nifi_parameter_context.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle-child\nspec:\n  description: \"It is a child test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  inheritedParameterContexts:\n    - name: dataflow-lifecycle\n  parameters:\n    - name: test\n      value: toto-child\n      description: tutu (child)\n```\n\n----------------------------------------\n\nTITLE: Installing KEDA with Helm and Namespace Preparation (Console)\nDESCRIPTION: This snippet creates a dedicated Kubernetes namespace called 'keda' and deploys the KEDA Helm chart into it using Helm v3. The 'kubectl' and 'helm' CLIs must be installed and configured. Inputs include the 'keda' namespace and 'kedacore/keda' chart reference. Output is a running KEDA deployment in the 'keda' namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl create namespace keda\nhelm install keda kedacore/keda --namespace keda\n```\n\n----------------------------------------\n\nTITLE: Configuring CRD Conversion Webhook Annotations and Spec - YAML\nDESCRIPTION: This YAML snippet shows how to configure annotations and conversion webhook details in the definition of NiFiKop CRDs. These settings are required if the conversion webhook is enabled to allow Kubernetes to convert CR resources between API versions 'v1alpha1' and 'v1'. Key parameters include namespace and certificate names for injecting the CA bundle, and webhook service details specifying the endpoint for conversion requests. Proper configuration ensures smooth API version transition without resource incompatibility issues.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/2_deploy_nifikop/1_quick_start.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n```\n\n----------------------------------------\n\nTITLE: YAML Definition for NiFi Connection between Dataflows\nDESCRIPTION: Configures a NiFi connection resource linking 'input' and 'output' dataflows with customizable parameters such as backpressure thresholds, expiration, bends for connection routing, and label placement via labelIndex. This setup facilitates establishing data flow pathways and managing traffic control within the NiFi cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/4_manage_connections/1_deploy_connection.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiConnection\nmetadata:\n  name: connection\n  namespace: nifikop\nspec:\n  source:\n    name: input\n    namespace: nifikop\n    subName: output\n    type: dataflow\n  destination:\n    name: output\n    namespace: nifikop\n    subName: input\n    type: dataflow\n  configuration:\n    backPressureDataSizeThreshold: 100 GB\n    backPressureObjectThreshold: 10000\n    flowFileExpiration: 1 hour\n    labelIndex: 0\n    bends:\n      - posX: 550\n        posY: 550\n      - posX: 550\n        posY: 440\n      - posX: 550\n        posY: 88\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: NiFiCluster SSL with IssuerRef\nDESCRIPTION: This YAML snippet configures a NiFi cluster to use an existing cert-manager Issuer for SSL certificates. It sets clusterSecure and siteToSiteSecure to true, specifies the cluster domain and enables external DNS. The sslSecrets section refers to the existing Issuer using issuerRef.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  clusterSecure: true\n  siteToSiteSecure: true\n  ...\n  listenersConfig:\n    clusterDomain: <DNS zone name>\n    useExternalDNS: true\n    ...\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n      issuerRef:\n        name: letsencrypt-staging\n        kind: Issuer\n```\n\n----------------------------------------\n\nTITLE: Kubernetes Pod Port Configuration Result in YAML\nDESCRIPTION: This YAML snippet illustrates how the `listenersConfig.internalListeners` configuration from the NiFiCluster custom resource translates into the `ports` definition within the resulting Kubernetes Pod specification. It shows the configured `containerPort`, `name`, and `protocol` (TCP by default) for each defined internal listener, demonstrating the effect of the custom resource configuration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n    ports:\n    - containerPort: 8443\n      name: https\n      protocol: TCP\n    - containerPort: 6007\n      name: cluster\n      protocol: TCP\n    - containerPort: 6342\n      name: load-balance\n      protocol: TCP\n    - containerPort: 10000\n      name: s2s\n      protocol: TCP\n    - containerPort: 9090\n      name: prometheus\n      protocol: TCP\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTPS Gateway for NiFi in Istio\nDESCRIPTION: Defines an Istio Gateway that accepts HTTPS traffic on port 443 and transforms it to HTTP, using a TLS certificate stored in a Kubernetes secret.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: nifi-gateway\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 443\n      name: https\n      protocol: HTTPS\n    tls:\n      mode: SIMPLE\n      credentialName: my-secret\n    hosts:\n    - nifi.my-domain.com\n```\n\n----------------------------------------\n\nTITLE: Deploying CRDs - Bash\nDESCRIPTION: This snippet applies the Custom Resource Definitions (CRDs) for the NiFiKop operator using `kubectl apply`.  CRDs define the custom resources that the operator manages. These commands ensure that the Kubernetes cluster is aware of the custom resources used by the operator and are prerequisites before deploying the operator itself. Each command applies a specific CRD definition file.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/6_contributing/1_developer_guide.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring SSL for NiFi Cluster in YAML\nDESCRIPTION: YAML configuration for securing a NiFi cluster with SSL. The config sets up internal listeners for HTTPS, cluster communication, and site-to-site protocol, and instructs the operator to create the required SSL certificates.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\n...\nspec:\n  ...\n  readOnlyConfig:\n    # NifiProperties configuration that will be applied to the node.\n    nifiProperties:\n      webProxyHosts:\n        - nifistandard2.trycatchlearn.fr:8443\n        ...\n  ...\n  listenersConfig:\n    internalListeners:\n      - type: \"https\"\n        name: \"https\"\n        containerPort: 8443\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiUser Resource in YAML\nDESCRIPTION: This YAML snippet defines a NifiUser resource in Kubernetes. It specifies the user's identity, the associated NiFi cluster, and configurations for certificate creation and access policies. The identity field allows overriding the Kubernetes resource name for the NiFi user.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/2_manage_users_and_accesses/1_users_management.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: aguitton\nspec:\n  # identity field is use to define the user identity on NiFi cluster side,\n  #\tit use full when the user's name doesn't suite with Kubernetes resource name.\n  identity: alexandre.guitton@konpyutaika.com\n  # Contains the reference to the NifiCluster with the one the registry client is linked.\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  # Whether or not the the operator also include a Java keystore format (JKS) with you secret\n  includeJKS: false\n  # Whether or not a certificate will be created for this user.\n  createCert: false\n  # defines the list of access policies that will be granted to the group.\n  accessPolicies:\n      # defines the kind of access policy, could be \"global\" or \"component\".\n    - type: component\n      # defines the kind of action that will be granted, could be \"read\" or \"write\"\n      action: read\n      # resource defines the kind of resource targeted by this access policies, please refer to the following page:\n      #\thttps://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#access-policies\n      resource: /data\n      # componentType is used if the type is \"component\", it's allow to define the kind of component on which is the\n      # access policy\n      componentType: \"process-groups\"\n      # componentId is used if the type is \"component\", it's allow to define the id of the component on which is the\n      # access policy\n      componentId: \"\"\n```\n\n----------------------------------------\n\nTITLE: Installing Zookeeper with Helm on Kubernetes\nDESCRIPTION: Installs Zookeeper using Bitnami's Helm chart with specific resource requests, limits, and custom storage class. Dependencies include Helm CLI and access to Kubernetes cluster; parameters can be adjusted for resource needs and storage preferences.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhelm install zookeeper oci://registry-1.docker.io/bitnamicharts/zookeeper \\\n    --namespace=zookeeper \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set global.storageClass=standard \\\n    --set networkPolicy.enabled=true \\\n    --set replicaCount=3 \\\n    --create-namespace\n```\n\n----------------------------------------\n\nTITLE: Creating a Kubernetes Secret for Basic Authentication\nDESCRIPTION: This `kubectl` command demonstrates how to create a Kubernetes secret named `nifikop-credentials` in the `nifikop-nifi` namespace for basic authentication. It utilizes `--from-file` to read the `username`, `password`, and optional `ca.crt` from specified files, which are required for authenticating to the external NiFi cluster's API. The secret needs to be created before deploying the NifiCluster resource.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/3_external_cluster.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nkubectl create secret generic nifikop-credentials \\\n  --from-file=username=./secrets/username\\\n  --from-file=password=./secrets/password\\\n  --from-file=ca.crt=./secrets/ca.crt\\\n  -n nifikop-nifi\n```\n\n----------------------------------------\n\nTITLE: Configuring Managed Groups in NifiCluster Spec\nDESCRIPTION: YAML configuration example showing how to define managedAdminUsers and managedReaderUsers in a NifiCluster specification to automatically create and manage user access groups.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/2_manage_users_and_accesses/3_managed_groups.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  ...\n  oneNifiNodePerNode: false\n  propagateLabels: true\n  managedAdminUsers:\n    -  identity: \"alexandre.guitton@konpyutaika.com\"\n       name: \"aguitton\"\n    -  identity: \"nifiuser@konpyutaika.com\"\n       name: \"nifiuser\"\n  managedReaderUsers:\n    -  identity: \"toto@konpyutaika.com\"\n       name: \"toto\"\n    ...\n```\n\n----------------------------------------\n\nTITLE: Configuring managed groups in NifiCluster resource\nDESCRIPTION: YAML configuration snippet showing how to set up managed admin and reader users directly in the NifiCluster resource for simplified user management. This approach automatically creates and manages the specified users and assigns them to appropriate groups.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/3_tasks/4_nifi_user_group.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  ...\n  oneNifiNodePerNode: false\n  propagateLabels: true\n  managedAdminUsers:\n    -  identity: \"alexandre.guitton@konpyutaika.com\"\n       name: \"aguitton\"\n    -  identity: \"nifiuser@konpyutaika.com\"\n       name: \"nifiuser\"\n  managedReaderUsers:\n    -  identity: \"toto@konpyutaika.com\"\n       name: \"toto\"\n    ...\n```\n\n----------------------------------------\n\nTITLE: Configuring NifiCluster with External Issuer\nDESCRIPTION: YAML configuration showing how to configure a NifiCluster to use an external certificate issuer like Let's Encrypt. This example enables cluster and site-to-site security with external DNS integration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/3_tasks/2_security/1_ssl.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\n...\nspec:\n  ...\n  clusterSecure: true\n  siteToSiteSecure: true\n  ...\n  listenersConfig:\n    clusterDomain: <DNS zone name>\n    useExternalDNS: true\n    ...\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n      issuerRef:\n        name: letsencrypt-staging\n        kind: Issuer\n```\n\n----------------------------------------\n\nTITLE: Deploying Prometheus ServiceMonitor Resource - yaml\nDESCRIPTION: Defines a ServiceMonitor custom resource to configure Prometheus scraping NiFi cluster metrics. Requires Prometheus operator, corresponding ServiceAccount and necessary RBAC privileges. Parameters include label selectors, endpoints, custom relabelings for identifying pods and labels in Prometheus metrics. Inputs must match deployed NiFi service labels.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: cluster\n  namespace: monitoring-system\n  labels:\n    app: nifi-cluster\n    nifi_cr: cluster\nspec:\n  selector:\n    matchLabels:\n      app: nifi\n      nifi_cr: cluster\n  namespaceSelector:\n    matchNames:\n      - clusters\n  endpoints:\n    - interval: 10s\n      port: prometheus\n      path: /metrics\n      honorLabels: true\n      relabelings:\n        - sourceLabels: [__meta_kubernetes_pod_ip]\n          separator: ;\n          regex: (.*)\n          targetLabel: pod_ip\n          replacement: $1\n          action: replace\n        - sourceLabels: [__meta_kubernetes_pod_label_nodeId]\n          separator: ;\n          regex: (.*)\n          targetLabel: nodeId\n          replacement: $1\n          action: replace\n        - sourceLabels: [__meta_kubernetes_pod_label_nifi_cr]\n          separator: ;\n          regex: (.*)\n          targetLabel: nifi_cr\n          replacement: $1\n          action: replace\n```\n\n----------------------------------------\n\nTITLE: Configuring ServiceMonitor for NiFi Metrics Collection\nDESCRIPTION: Defines a ServiceMonitor resource to tell Prometheus how to scrape metrics from the NiFi cluster. Includes label selectors, endpoint configurations, and relabeling rules to properly identify and tag NiFi metrics.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: cluster\n  namespace: monitoring-system\n  labels:\n    app: nifi-cluster\n    nifi_cr: cluster\nspec:\n  selector:\n    matchLabels:\n      app: nifi\n      nifi_cr: cluster\n  namespaceSelector:\n    matchNames:\n      - clusters\n  endpoints:\n    - interval: 10s\n      port: prometheus\n      path: /metrics\n      honorLabels: true\n      relabelings:\n        - sourceLabels: [__meta_kubernetes_pod_ip]\n          separator: ;\n          regex: (.*)\n          targetLabel: pod_ip\n          replacement: $1\n          action: replace\n        - sourceLabels: [__meta_kubernetes_pod_label_nodeId]\n          separator: ;\n          regex: (.*)\n          targetLabel: nodeId\n          replacement: $1\n          action: replace\n        - sourceLabels: [__meta_kubernetes_pod_label_nifi_cr]\n          separator: ;\n          regex: (.*)\n          targetLabel: nifi_cr\n          replacement: $1\n          action: replace\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi User with Access Policies using NifiUser Kubernetes Custom Resource - YAML\nDESCRIPTION: This YAML snippet defines a NifiUser custom resource for managing a user named 'aguitton' in the NiFi cluster. It specifies the user identity which overrides the Kubernetes resource name, a reference to the NiFi cluster, options to include a Java keystore or create certificates, and a list of access policies granting read permissions on NiFi components. The operator uses this specification to either bind to existing NiFi users or create/manage new ones synchronizing with the NiFi cluster. Key parameters include 'identity' for NiFi-side username, 'clusterRef' for cluster linkage, 'createCert' boolean for certificate creation, and 'accessPolicies' detailing resource-level access control.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/3_manage_nifi/2_manage_users_and_accesses/1_users_management.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: aguitton\nspec:\n  # identity field is use to define the user identity on NiFi cluster side,\n  #\tit use full when the user's name doesn't suite with Kubernetes resource name.\n  identity: alexandre.guitton@konpyutaika.com\n  # Contains the reference to the NifiCluster with the one the registry client is linked.\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  # Whether or not the the operator also include a Java keystore format (JKS) with you secret\n  includeJKS: false\n  # Whether or not a certificate will be created for this user.\n  createCert: false\n  # defines the list of access policies that will be granted to the group.\n  accessPolicies:\n      # defines the kind of access policy, could be \"global\" or \"component\".\n    - type: component\n      # defines the kind of action that will be granted, could be \"read\" or \"write\"\n      action: read\n      # resource defines the kind of resource targeted by this access policies, please refer to the following page:\n      #\thttps://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#access-policies\n      resource: /data\n      # componentType is used if the type is \"component\", it's allow to define the kind of component on which is the\n      # access policy\n      componentType: \"process-groups\"\n      # componentId is used if the type is \"component\", it's allow to define the id of the component on which is the\n      # access policy\n      componentId: \"\"\n```\n\n----------------------------------------\n\nTITLE: Defining ReadOnlyConfig for Apache NiFi Cluster-wide Configuration in YAML\nDESCRIPTION: This YAML snippet defines the ReadOnlyConfig object that sets cluster-wide read-only settings for an Apache NiFi deployment. It specifies maximum thread counts for timer-driven and event-driven processors to control concurrency levels. It configures logging via logback.xml through ConfigMaps and Secrets references and authorizer settings for security templates. The snippet also includes NiFi-specific properties, Zookeeper settings, and bootstrap properties with the ability to override configuration files and apply JVM memory settings. Fields reference Kubernetes ConfigMaps and Secrets by name and namespace, allowing secure and centralized configuration management.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/5_references/1_nifi_cluster/2_read_only_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreadOnlyConfig:\n  # MaximumTimerDrivenThreadCount define the maximum number of threads for timer driven processors available to the system.\n  maximumTimerDrivenThreadCount: 30\n  # MaximumEventDrivenThreadCount define the maximum number of threads for event driven processors available to the system.\n  maximumEventDrivenThreadCount: 10\n  # Logback configuration that will be applied to the node\n  logbackConfig:\n    # logback.xml configuration that will replace the one produced based on template\n    replaceConfigMap:\n      # The key of the value,in data content, that we want use.\n      data: logback.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # logback.xml configuration that will replace the one produced based on template and overrideConfigMap\n    replaceSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: logback.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n  # Authorizer configuration that will be applied to the node\n  authorizerConfig:\n    # An authorizers.xml configuration template that will replace the default template seen in authorizers.go\n    replaceTemplateConfigMap:\n      # The key of the value, in data content, that we want use.\n      data: authorizers.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # An authorizers.xml configuration template that will replace the default template seen in authorizers.go and the replaceTemplateConfigMap\n    replaceTemplateSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: authorizers.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n  # NifiProperties configuration that will be applied to the node.\n  nifiProperties:\n    # Additionnal nifi.properties configuration that will override the one produced based on template and\n    # configuration\n    overrideConfigMap:\n      # The key of the value,in data content, that we want use.\n      data: nifi.properties\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop.\n    # Additionnal nifi.properties configuration that will override the one produced based\n    #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    overrideSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: nifi.properties\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # Additionnal nifi.properties configuration that will override the one produced based\n    #\ton template, configurations and overrideConfigMap\n    overrideConfigs: |\n      nifi.ui.banner.text=NiFiKop\n    # A comma separated list of allowed HTTP Host header values to consider when NiFi\n    # is running securely and will be receiving requests to a different host[:port] than it is bound to.\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#web-properties\n    #      webProxyHosts:\n    # Nifi security client auth\n    needClientAuth: false\n    # Indicates which of the configured authorizers in the authorizers.xml file to use\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#authorizer-configuration\n    #      authorizer:\n  # ZookeeperProperties configuration that will be applied to the node.\n  zookeeperProperties:\n    #      # Additionnal zookeeeper.properties configuration that will override the one produced based on template and\n    #      # configuration\n    #      overrideConfigMap:\n    #        # The key of the value,in data content, that we want use.\n    #        data: zookeeeper.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop.\n    #      # Additionnal zookeeeper.properties configuration that will override the one produced based\n    #      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    #      overrideSecretConfig:\n    #        # The key of the value,in data content, that we want use.\n    #        data: zookeeeper.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop\n    # Additionnal zookeeper.properties configuration that will override the one produced based\n    # on template and configurations.\n    overrideConfigs: |\n      initLimit=15\n      autopurge.purgeInterval=24\n      syncLimit=5\n      tickTime=2000\n      dataDir=./state/zookeeper\n      autopurge.snapRetainCount=30\n  # BootstrapProperties configuration that will be applied to the node.\n  bootstrapProperties:\n    #      # Additionnal bootstrap.properties configuration that will override the one produced based on template and\n    #      # configuration\n    #      overrideConfigMap:\n    #        # The key of the value,in data content, that we want use.\n    #        data: bootstrap.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop.\n    #      # Additionnal bootstrap.properties configuration that will override the one produced based\n    #      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    #      overrideSecretConfig:\n    #        # The key of the value,in data content, that we want use.\n    #        data: bootstrap.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop\n    # JVM memory settings\n    nifiJvmMemory: \"512m\"\n    # Additionnal bootstrap.properties configuration that will override the one produced based\n    # on template and configurations.\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#bootstrap_properties\n    overrideConfigs: |\n      # java.arg.4=-Djava.net.preferIPv4Stack=true\n```\n\n----------------------------------------\n\nTITLE: Configuring NifiUserGroup Resource for NiFi Access Management in YAML\nDESCRIPTION: This YAML snippet defines a Kubernetes custom resource 'NifiUserGroup' for managing user groups and their access within an Apache NiFi cluster, as handled by the konpyutaika/nifikop operator. It sets the group metadata, references the associated NiFi cluster, specifies user references (which must be defined as NifiUser resources), and grants a read-only 'global' access policy for the '/counters' resource. Prerequisites include existing NifiUser resources for each user and an active NiFi cluster referenced by 'clusterRef'. The configuration expects cluster and user names, and optional restriction of access policies by component or ID if needed. The output is a managed NiFi group with the designated access policies synchronized by the operator.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/2_manage_users_and_accesses/2_groups_management.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  # Contains the reference to the NifiCluster with the one the registry client is linked.\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  # contains the list of reference to NifiUsers that are part to the group.\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n#     namespace: nifikop\n    - name: nc-controller.nifikop.mgt.cluster.local\n  # defines the list of access policies that will be granted to the group.\n  accessPolicies:\n      # defines the kind of access policy, could be \"global\" or \"component\".\n    - type: global\n      # defines the kind of action that will be granted, could be \"read\" or \"write\"\n      action: read\n      # resource defines the kind of resource targeted by this access policies, please refer to the following page:\n      #\thttps://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#access-policies\n      resource: /counters\n#      # componentType is used if the type is \"component\", it's allow to define the kind of component on which is the\n#      # access policy\n#      componentType: \"process-groups\"\n#      # componentId is used if the type is \"component\", it's allow to define the id of the component on which is the\n#      # access policy\n#      componentId: \"\"\n\n```\n\n----------------------------------------\n\nTITLE: NiFi ClusterIP Service Definition for HTTPS\nDESCRIPTION: Defines the NiFi ClusterIP service specification that should be included in the NiFi cluster deployment YAML. This service exposes the HTTPS listener on port 8443 and is referenced in the VirtualService configuration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nspec:  \n  externalServices:  \n    - name: \"nifi-cluster\"\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8443\n            internalListenerName: \"https\"\n```\n\n----------------------------------------\n\nTITLE: Defining Persistent Storage for NiFi Pods with PVCs - YAML\nDESCRIPTION: This YAML block illustrates configuring persistent storage for NiFi node containers using storageConfigs. Each entry maps specific directories inside the NiFi container to a PersistentVolumeClaim (PVC), specifying mount path, storage class, resource requests, and PVC metadata like labels and annotations. This setup ensures data directories, logs, repositories, and configurations persist across pod restarts—a must for stateful NiFi workloads. It assumes access to suitable StorageClasses and sufficient cluster storage resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\nstorageConfigs:\n  - mountPath: /opt/nifi/nifi-current/logs\n    name: logs\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/data\n    name: data\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 50Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/extensions\n    name: extensions-repository\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 5Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/flowfile_repository\n    name: flowfile-repository\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/nifi-current/conf\n    name: conf\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 5Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/content_repository\n    name: content-repository\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 500Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/provenance_repository\n    name: provenance-repository\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 500Gi\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Cluster Custom Resource YAML Specification\nDESCRIPTION: This YAML snippet defines the desired state of a NiFi cluster leveraging the 'NifiCluster' custom resource in Kubernetes. It includes configurations for services, pods, storage, resource limits, node specifications, listener ports, and external services. Required dependencies include having the Konpyutaika NiFi operator installed for managing this CRD. Key parameters such as zkAddress and zkPath specify ZooKeeper connectivity, while nodeConfigGroups dictate per-node configurations. Inputs are declarative YAML fields describing cluster setup, and outputs are the configured running NiFi cluster resources. Limitations include the necessity of Kubernetes cluster and operator support for this custom resource.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/5_references/1_nifi_cluster/1_nifi_cluster.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n    annotations:\n      tyty: ytyt\n    labels:\n      tete: titi  \n  pod:\n    annotations:\n      toto: tata\n    labels:\n      titi: tutu\n  zkAddress: 'zookeepercluster-client.zookeeper:2181'\n  zkPath: '/simplenifi'\n  clusterImage: 'apache/nifi:1.11.3'\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      podMetadata:\n        annotations:\n          node-annotation: \"node-annotation-value\"\n        labels:\n          node-label: \"node-label-value\"\n      externalVolumeConfigs:\n        - name: example-volume\n          mountPath: \"/opt/nifi/example\"\n          secret:\n            secretName: \"raw-controller\"\n      storageConfigs:\n        - mountPath: '/opt/nifi/nifi-current/logs'\n          name: logs\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: 'standard'\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: 'default'\n      resourcesRequirements:\n        limits:\n          cpu: '2'\n          memory: 3Gi\n        requests:\n          cpu: '1'\n          memory: 1Gi\n  nodes:\n    - id: 1\n      nodeConfigGroup: 'default_group'\n    - id: 2\n      nodeConfigGroup: 'default_group'\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: 'http'\n        name: 'http'\n        containerPort: 8080\n      - type: 'cluster'\n        name: 'cluster'\n        containerPort: 6007\n      - type: 's2s'\n        name: 's2s'\n        containerPort: 10000\n  externalServices:\n    - name: 'clusterip'\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8080\n            internalListenerName: 'http'\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n\n----------------------------------------\n\nTITLE: Metadata Schema for NifiCluster Resource in Kubernetes\nDESCRIPTION: This section defines the schema for the metadata field of the NifiCluster resource, conforming to Kubernetes ObjectMetadata standards. It includes common identifiers like name, namespace, labels, and annotations, which are optional. This schema ensures consistent configuration of resource metadata across the cluster deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/5_references/1_nifi_cluster/1_nifi_cluster.md#_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Field    | Type                                                                                | Description                                                                                       | Required | Default |\n| -------- | ----------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------- | -------- | ------- |\n| metadata | [ObjectMetadata](https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1#ObjectMeta) | is metadata that all persisted resources must have, which includes all objects users must create. | No       | nil     |\n```\n\n----------------------------------------\n\nTITLE: Port-Forwarding Prometheus Service for Local Access - console\nDESCRIPTION: Enables local port forwarding from the Prometheus-operated Kubernetes service in the 'monitoring-system' namespace to port 9090 on the local machine. Assumes kubectl access to the cluster. Input parameters are service name and namespace; output is a locally accessible Prometheus UI at http://localhost:9090.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_12\n\nLANGUAGE: console\nCODE:\n```\nkubectl port-forward -n monitoring-system service/prometheus-operated 9090:9090\n\n```\n\n----------------------------------------\n\nTITLE: Creating NiFi Registry Client Resource\nDESCRIPTION: This YAML snippet demonstrates how to define a NifiRegistryClient Kubernetes resource. It specifies the API version, kind, metadata (name), and the desired state (`spec`), which includes a reference to the associated NiFi cluster (`clusterRef`), a user-defined description, and the URI of the NiFi Registry endpoint.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/5_references/3_nifi_registry_client.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiRegistryClient\nmetadata:\n  name: squidflow\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Squidflow demo\"\n  uri: \"http://nifi-registry:18080\"\n```\n\n----------------------------------------\n\nTITLE: NiFi NodeGroup Autoscaler YAML Configuration\nDESCRIPTION: This YAML configuration defines a NifiNodeGroupAutoscaler resource. It specifies the target NiFi cluster, the node config group to manage, the node label selector for identifying nodes, and the upscale/downscale strategies.  The `clusterRef` points to the target NiFiCluster. The `nodeLabelsSelector` identifies which nodes will be managed. The `upscaleStrategy` and `downscaleStrategy` define the scaling behavior.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/5_references/7_nifi_nodegroup_autoscaler.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiNodeGroupAutoscaler\nmetadata:\n  name: nifinodegroupautoscaler-sample\nspec:\n  # contains the reference to the NifiCluster with the one the node group autoscaler is linked.\n  clusterRef:\n    name: nificluster-name\n    namespace: nifikop\n  # defines the id of the NodeConfig contained in NifiCluster.Spec.NodeConfigGroups\n  nodeConfigGroupId: default-node-group\n  # The selector used to identify nodes in NifiCluster.Spec.Nodes this autoscaler will manage\n  # Use Node.Labels in combination with this selector to clearly define which nodes will be managed by this autoscaler \n  nodeLabelsSelector: \n    matchLabels:\n      nifi_cr: nificluster-name\n      nifi_node_group: default-node-group\n  # the strategy used to decide how to add nodes to a nifi cluster\n  upscaleStrategy: simple\n  # the strategy used to decide how to remove nodes from an existing cluster\n  downscaleStrategy: lifo\n```\n\n----------------------------------------\n\nTITLE: Extracting NiFi Client Certificates and Keys from Kubernetes Secret Using kubectl CLI\nDESCRIPTION: These kubectl commands extract the CA certificate, user certificate, and user private key from a Kubernetes secret created by NifiUser resource. They use jsonpath expressions to access base64-encoded secret data fields and decode them to local files for application usage. Prerequisites include kubectl access to the cluster namespace and the presence of the specified secret. This enables client applications or local users to retrieve SSL credentials for secure NiFi communication.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['ca\\.crt']}\" | base64 -d > ca.crt\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['tls\\.crt']}\" | base64 -d > tls.crt\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['tls\\.key']}\" | base64 -d > tls.key\n```\n\n----------------------------------------\n\nTITLE: Defining BootstrapProperties Configuration Overrides and JVM Settings (Markdown)\nDESCRIPTION: Specifies properties for customizing NiFi bootstrap configuration (bootstrap.properties) and JVM memory. Allows overriding default settings using external Kubernetes ConfigMaps (`overrideConfigMap`), direct string overrides (`overrideConfigs`), or external Kubernetes Secrets (`overrideSecretConfig`). Includes an option to set NiFi JVM memory (`NifiJvmMemory`).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/5_references/1_nifi_cluster/2_read_only_config.md#_snippet_2\n\nLANGUAGE: Markdown\nCODE:\n```\n## BootstrapProperties\n\n|Field|Type|Description|Required|Default|\n|-----|----|-----------|--------|--------|\n|overrideConfigMap|[ConfigmapReference](#configmapreference)|Additionnal bootstrap.properties configuration that will override the one produced based on template and configuration.|No|nil|\n|overrideConfigs|string|Additionnal bootstrap.properties configuration that will override the one produced based on template, configurations and overrideConfigMap.|No|\"\"|\n|overrideSecretConfig|[SecretConfigReference](#secretconfigreference)|Additionnal bootstrap.properties configuration that will override the one produced based on template, configurations, overrideConfigMap and overrideConfigs.|No|nil|\n|NifiJvmMemory|string|JVM memory settings.|No|\"512m\"|\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiParameterContext including Sensitive Parameters using Kubernetes YAML\nDESCRIPTION: This YAML snippet defines a NifiParameterContext custom resource, used to create and manage parameter contexts in NiFi through NiFiKop. It references the target cluster and includes parameters with values and descriptions. Additionally, it references Kubernetes secrets that contain sensitive parameters, which NiFiKop converts into NiFi-sensitive properties. Users must create referenced secrets separately in the Kubernetes cluster. This configuration enables secure management of sensitive data in NiFi parameter contexts.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\n  namespace: demo\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      value: toto\n      description: toto\n```\n\n----------------------------------------\n\nTITLE: Getting Zookeeper UID/GID for OpenShift Deployment\nDESCRIPTION: Bash command to extract the appropriate UID/GID from an OpenShift namespace for Zookeeper deployment, ensuring proper permissions are set.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nzookeper_uid=$(kubectl get namespace zookeeper -o=jsonpath='{.metadata.annotations.openshift\\.io/sa\\.scc\\.supplemental-groups}' | sed 's/\\/10000$//' | tr -d '[:space:]')\n```\n\n----------------------------------------\n\nTITLE: Recalculating NiFi Sensitive Property Encryption Using Encrypt-Config Tool - Bash\nDESCRIPTION: This snippet demonstrates how to manually update NiFi flow configuration files to use the new sensitive property encryption algorithm 'NIFI_PBKDF2_AES_GCM_256' using the NiFi Encrypt-Config Tool. It requires the flow files ('flow.xml.gz' and/or 'flow.json.gz'), the 'nifi.properties' configuration file, and the sensitive properties key ('PROPERTIES_KEY'). The commands decrypt and re-encrypt the sensitive properties using the specified algorithm, thereby ensuring flows are compatible with the upgraded encryption settings. The user must replace 'PROPERTIES_KEY' with their actual sensitive properties key.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/7_upgrade_guides/4_v1.3.1_to_v1.4.0.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nencrypt-config.sh -n nifi.properties -f flow.xml.gz -x -s PROPERTIES_KEY -A NIFI_PBKDF2_AES_GCM_256\nencrypt-config.sh -n nifi.properties -f flow.json.gz -x -s PROPERTIES_KEY -A NIFI_PBKDF2_AES_GCM_256\n```\n\n----------------------------------------\n\nTITLE: Install NiFiKop Helm Chart Basic - Bash\nDESCRIPTION: Installs the NiFiKop Helm chart into the Kubernetes cluster using a specified release name. This is the fundamental command for deploying the chart with its default configurations. Customizations can be added using `--set` flags or `-f` for a values file.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/2_setup/3_install/1_customizable_install_with_helm.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhelm install <release name> konpyutaika/nifikop\n```\n\n----------------------------------------\n\nTITLE: Removing Node from NiFi Cluster Custom Resource (YAML)\nDESCRIPTION: Defines a `NifiCluster` Custom Resource configuration. It shows how to remove a node by commenting out or deleting its entry (`id: 2`) from the `spec.nodes` list. This modification signals the NiFiKop operator to initiate the graceful decommission process for that node.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  headlessServiceEnabled: true\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.11.3\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n# >>>> START: node removed\n#    - id: 2\n#      nodeConfigGroup: \"default_group\"\n# <<<< END\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple Repository Directories and Storage in NiFiCluster (YAML)\nDESCRIPTION: Provides an example of advanced NiFiCluster configuration enabling multiple directories for content and provenance repositories to support high performance deployments. The snippet defines additional repository directories in 'readOnlyConfig.nifiProperties.overrideConfigs' and maps them to PVC-backed paths in 'nodeConfigGroups.default_group.storageConfigs'. Dependencies include a NiFiKop setup, matching PVCs available via Kubernetes, and appropriately substituted storage class names. All directory mount paths and sizes must be aligned across the configuration, and potential errors can arise if PVCs are not present or not sized correctly.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n...\n  readOnlyConfig:\n    nifiProperties:\n      overrideConfigs: |\n        nifi.content.repository.directory.dir1=../content-additional/dir1\n        nifi.content.repository.directory.dir2=../content-additional/dir2\n        nifi.content.repository.directory.dir3=../content-additional/dir3\n        nifi.provenance.repository.directory.dir1=../provenance-additional/dir1\n        nifi.provenance.repository.directory.dir2=../provenance-additional/dir2\n...\n  nodeConfigGroups:\n    default_group:\n      ...\n      storageConfigs:\n      - mountPath: \"/opt/nifi/content-additional/dir1\"\n        name: content-repository-dir1\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/content-additional/dir2\"\n        name: content-repository-dir2\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/content-additional/dir3\"\n        name: content-repository-dir3\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/provenance-additional/dir1\"\n        name: provenance-repository-dir1\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/provenance-additional/dir2\"\n        name: provenance-repository-dir2\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      ...\n```\n\n----------------------------------------\n\nTITLE: Running NiFiKop Resource Migration Script via NPM (bash)\nDESCRIPTION: Demonstrates how to trigger the migration script using the npm start command and pass required arguments. 'NIFIKOP_RESOURCE' specifies the resource type to migrate (e.g., cluster, user) and 'K8S_NAMESPACE' refers to the Kubernetes namespace. This assumes the rest of the setup has completed and migrates selected resource types accordingly.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nnpm start -- --type=<NIFIKOP_RESOURCE> --namespace=<K8S_NAMESPACE>\n```\n\n----------------------------------------\n\nTITLE: Defining a NiFi Dataflow Resource ('output') in YAML\nDESCRIPTION: Defines a `NifiDataflow` custom resource named 'output' in the 'nifikop' namespace using YAML for Kubernetes. This dataflow references a NiFi cluster (`nc`), a different flow version (2) from the same NiFi Registry bucket, and sets synchronization/update strategies. It serves as the destination dataflow for the connection example and requires an input port.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/4_manage_connections/1_deploy_connection.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiDataflow\nmetadata:\n  name: output\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  bucketId: deedb9f6-65a4-44e9-a1c9-722008fcd0ba\n  flowId: fc5363eb-801e-432f-aa94-488838674d07\n  flowVersion: 2\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  skipInvalidComponent: true\n  skipInvalidControllerService: true\n  syncMode: always\n  updateStrategy: drain\n  flowPosition:\n    posX: 750\n    posY: 0\n```\n\n----------------------------------------\n\nTITLE: Defining a basic NiFi Cluster Resource\nDESCRIPTION: This YAML snippet defines a `NifiCluster` custom resource named `simplenifi`. It configures various aspects of the NiFi cluster, including enabling a headless service, adding annotations and labels to service and pods, specifying Zookeeper connection details, setting the NiFi cluster image, defining node configuration groups with storage, resource limits/requests, and external volumes, listing individual nodes, configuring internal listeners (http, cluster, s2s), and defining an external ClusterIP service.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/5_references/1_nifi_cluster/1_nifi_cluster.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n    annotations:\n      tyty: ytyt\n    labels:\n      tete: titi  \n  pod:\n    annotations:\n      toto: tata\n    labels:\n      titi: tutu\n  zkAddress: 'zookeepercluster-client.zookeeper:2181'\n  zkPath: '/simplenifi'\n  clusterImage: 'apache/nifi:1.11.3'\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      podMetadata:\n        annotations:\n          node-annotation: \"node-annotation-value\"\n        labels:\n          node-label: \"node-label-value\"\n      externalVolumeConfigs:\n        - name: example-volume\n          mountPath: \"/opt/nifi/example\"\n          secret:\n            secretName: \"raw-controller\"\n      storageConfigs:\n        - mountPath: '/opt/nifi/nifi-current/logs'\n          name: logs\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: 'standard'\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: 'default'\n      resourcesRequirements:\n        limits:\n          cpu: '2'\n          memory: 3Gi\n        requests:\n          cpu: '1'\n          memory: 1Gi\n  nodes:\n    - id: 1\n      nodeConfigGroup: 'default_group'\n    - id: 2\n      nodeConfigGroup: 'default_group'\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: 'http'\n        name: 'http'\n        containerPort: 8080\n      - type: 'cluster'\n        name: 'cluster'\n        containerPort: 6007\n      - type: 's2s'\n        name: 's2s'\n        containerPort: 10000\n  externalServices:\n    - name: 'clusterip'\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8080\n            internalListenerName: 'http'\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n\n----------------------------------------\n\nTITLE: Example NiFiKop Read-Only Configuration in YAML\nDESCRIPTION: This YAML snippet demonstrates the structure of the `readOnlyConfig` object for specifying cluster-wide read-only configurations in NiFiKop. It covers settings for maximum thread counts, Logback configuration (referencing ConfigMaps/Secrets), Authorizer configuration (referencing ConfigMaps/Secrets), NiFi properties (overrides via ConfigMap/Secret/inline), ZooKeeper properties (overrides via inline), and Bootstrap properties (JVM memory, overrides via inline). These configurations are applied cluster-wide but can be overridden at the node level.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/5_references/1_nifi_cluster/2_read_only_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreadOnlyConfig:\n  # MaximumTimerDrivenThreadCount define the maximum number of threads for timer driven processors available to the system.\n  maximumTimerDrivenThreadCount: 30\n  # MaximumEventDrivenThreadCount define the maximum number of threads for event driven processors available to the system.\n  maximumEventDrivenThreadCount: 10\n  # Logback configuration that will be applied to the node\n  logbackConfig:\n    # logback.xml configuration that will replace the one produced based on template\n    replaceConfigMap:\n      # The key of the value,in data content, that we want use.\n      data: logback.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # logback.xml configuration that will replace the one produced based on template and overrideConfigMap\n    replaceSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: logback.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n  # Authorizer configuration that will be applied to the node\n  authorizerConfig:\n    # An authorizers.xml configuration template that will replace the default template seen in authorizers.go\n    replaceTemplateConfigMap:\n      # The key of the value, in data content, that we want use.\n      data: authorizers.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # An authorizers.xml configuration template that will replace the default template seen in authorizers.go and the replaceTemplateConfigMap\n    replaceTemplateSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: authorizers.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n  # NifiProperties configuration that will be applied to the node.\n  nifiProperties:\n    # Additionnal nifi.properties configuration that will override the one produced based on template and\n    # configuration\n    overrideConfigMap:\n      # The key of the value,in data content, that we want use.\n      data: nifi.properties\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop.\n    # Additionnal nifi.properties configuration that will override the one produced based\n    #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    overrideSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: nifi.properties\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # Additionnal nifi.properties configuration that will override the one produced based\n    #\ton template, configurations and overrideConfigMap\n    overrideConfigs: |\n      nifi.ui.banner.text=NiFiKop\n    # A comma separated list of allowed HTTP Host header values to consider when NiFi\n    # is running securely and will be receiving requests to a different host[:port] than it is bound to.\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#web-properties\n    #      webProxyHosts:\n    # Nifi security client auth\n    needClientAuth: false\n    # Indicates which of the configured authorizers in the authorizers.xml file to use\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#authorizer-configuration\n  #      authorizer:\n  # ZookeeperProperties configuration that will be applied to the node.\n  zookeeperProperties:\n    #      # Additionnal zookeeeper.properties configuration that will override the one produced based on template and\n    #      # configuration\n    #      overrideConfigMap:\n    #        # The key of the value,in data content, that we want use.\n    #        data: zookeeeper.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop.\n    #      # Additionnal zookeeeper.properties configuration that will override the one produced based\n    #      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    #      overrideSecretConfig:\n    #        # The key of the value,in data content, that we want use.\n    #        data: zookeeeper.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop\n    # Additionnal zookeeper.properties configuration that will override the one produced based\n    # on template and configurations.\n    overrideConfigs: |\n      initLimit=15\n      autopurge.purgeInterval=24\n      syncLimit=5\n      tickTime=2000\n      dataDir=./state/zookeeper\n      autopurge.snapRetainCount=30\n  # BootstrapProperties configuration that will be applied to the node.\n  bootstrapProperties:\n    #      # Additionnal bootstrap.properties configuration that will override the one produced based on template and\n    #      # configuration\n    #      overrideConfigMap:\n    #        # The key of the value,in data content, that we want use.\n    #        data: bootstrap.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop.\n    #      # Additionnal bootstrap.properties configuration that will override the one produced based\n    #      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    #      overrideSecretConfig:\n    #        # The key of the value,in data content, that we want use.\n    #        data: bootstrap.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop\n    # JVM memory settings\n    nifiJvmMemory: \"512m\"\n    # Additionnal bootstrap.properties configuration that will override the one produced based\n    # on template and configurations.\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#bootstrap_properties\n    overrideConfigs: |\n      # java.arg.4=-Djava.net.preferIPv4Stack=true\n```\n\n----------------------------------------\n\nTITLE: Defining Logging Flow Filters (JSON)\nDESCRIPTION: Shows an example of the `filters` array within a Logging Operator Flow configuration. It specifies a `parser` filter using a regular expression to structure standard NiFi log lines into distinct fields.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/helm/nifi-cluster/README.md#_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n[{\"parser\":{\"parse\":{\"expression\":\"/^(?<time>\\\\d{4}-\\\\d{2}-\\\\d{2} \\\\d{2}:\\\\d{2}:\\\\d{2},\\\\d{3}) (?<level>[^\\\\s]+) \\\\[(?<thread>.*)\\\\] (?<message>.*)$/im\",\"keep_time_key\":true,\"time_format\":\"%iso8601\",\"time_key\":\"time\",\"time_type\":\"string\",\"type\":\"regexp\"}}}]\n```\n\n----------------------------------------\n\nTITLE: Declare NiFi Cluster Nodes using NodeConfigGroups - YAML\nDESCRIPTION: This YAML snippet demonstrates how to declare nodes in a NiFi cluster, assigning them to specific `NodeConfigGroup` configurations or defining resource requirements directly at the node level. Nodes 0, 2, and 3 are configured using `default_group` and `high_mem_group` respectively, while node 5 has its resource requirements defined inline.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n- id: 0\n  nodeConfigGroup: \"default_group\"\n- id: 2\n  nodeConfigGroup: \"high_mem_group\"\n- id: 3\n  nodeConfigGroup: \"high_mem_group\"\n- id: 5\n  nodeConfig:\n    resourcesRequirements:\n      limits:\n        cpu: \"2\"\n        memory: 3Gi\n      requests:\n        cpu: \"1\"\n        memory: 1Gi\n```\n\n----------------------------------------\n\nTITLE: Verifying Operator Running - Bash\nDESCRIPTION: Verifies that the NiFiKop operator is running by listing pods in the specified namespace.  It uses `kubectl get pods` command and the namespace is set to `nifikop`. Requires kubectl and access to the Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/6_contributing/1_developer_guide.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n$ kubectl get pods -n nifikop\n```\n\n----------------------------------------\n\nTITLE: Setting NiFi Authorizer Property in Shell\nDESCRIPTION: This shell command sets the `nifi.security.user.authorizer` property to `custom-database-authorizer`. This configuration indicates that NiFi should use the custom database authorizer defined in the `authorizers.xml` file. This is the final step required to activate the custom authorizer.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/1_deploy_cluster/6_users_authorization/1_custom_user_authorizer.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nnifi.security.user.authorizer=custom-database-authorizer\n```\n\n----------------------------------------\n\nTITLE: Port-Forwarding Prometheus Service with kubectl - Console\nDESCRIPTION: Forwards local port 9090 to the Prometheus-operated Kubernetes service in the monitoring-system namespace. This allows web UI or API access to Prometheus from localhost. Prerequisites include a running Prometheus instance and cluster network access.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_10\n\nLANGUAGE: console\nCODE:\n```\nkubectl port-forward -n monitoring-system service/prometheus-operated 9090:9090\n\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi ServiceMonitor Kubernetes/YAML\nDESCRIPTION: This YAML manifest defines a ServiceMonitor resource used by the Prometheus instance to discover and scrape metrics from NiFi pods. It targets services/pods with specific labels (`app: nifi`, `nifi_cr: cluster`) within the 'clusters' namespace and configures scraping on the 'prometheus' port and '/metrics' path, including relabeling rules to capture useful Kubernetes metadata.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: cluster\n  namespace: monitoring-system\n  labels:\n    app: nifi-cluster\n    nifi_cr: cluster\nspec:\n  selector:\n    matchLabels:\n      app: nifi\n      nifi_cr: cluster\n  namespaceSelector:\n    matchNames:\n      - clusters\n  endpoints:\n    - interval: 10s\n      port: prometheus\n      path: /metrics\n      honorLabels: true\n      relabelings:\n        - sourceLabels: [__meta_kubernetes_pod_ip]\n          separator: \";\"\n          regex: (.*)\n          targetLabel: pod_ip\n          replacement: \"$1\"\n          action: replace\n        - sourceLabels: [__meta_kubernetes_pod_label_nodeId]\n          separator: \";\"\n          regex: (.*)\n          targetLabel: nodeId\n          replacement: \"$1\"\n          action: replace\n        - sourceLabels: [__meta_kubernetes_pod_label_nifi_cr]\n          separator: \";\"\n          regex: (.*)\n          targetLabel: nifi_cr\n          replacement: \"$1\"\n          action: replace\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi User Resource Using Kubernetes CRD YAML\nDESCRIPTION: This YAML snippet defines a NiFiUser custom resource for managing a NiFi user within the Kubernetes-based Nifikop operator. It includes metadata about the user resource, the user's identity, reference to the targeted NiFiCluster, and options controlling certificate creation. The snippet illustrates how to declare the resource with apiVersion, kind, metadata, and spec fields, enabling operators to create or update NiFi users declaratively. Key parameters include 'identity' for user identification, 'clusterRef' to specify the NiFi cluster linkage, and 'createCert' to determine if certificates should be generated for the user.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/5_references/2_nifi_user.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: aguitton\nspec:\n  identity: alexandre.guitton@konpyutaika.com\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  createCert: false\n```\n\n----------------------------------------\n\nTITLE: Deploying NifiDataflow Example\nDESCRIPTION: This snippet provides an example of a `NifiDataflow` resource definition in YAML.  It defines metadata like name and namespace and specifies details such as cluster reference, bucket ID, flow ID, flow version, registry client, and update strategy.  The `flowPosition` allows defining the visual placement in the NiFi UI.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/4_manage_connections/1_deploy_connection.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiDataflow\nmetadata:\n  name: input\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  bucketId: deedb9f6-65a4-44e9-a1c9-722008fcd0ba\n  flowId: ab95431d-980d-41bd-904a-fac4bd864ba6\n  flowVersion: 1\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  skipInvalidComponent: true\n  skipInvalidControllerService: true\n  syncMode: always\n  updateStrategy: drain\n  flowPosition:\n    posX: 0\n    posY: 0\n```\n\n----------------------------------------\n\nTITLE: Verifying Operator Pod Status (Console/kubectl)\nDESCRIPTION: Uses `kubectl get pods` to list pods in the `nifikop` namespace, allowing verification that the operator pod (e.g., `skeleton-nifikop-...`) is running correctly after Helm installation. The output shows an example of a running pod.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/6_contributing/1_developer_guide.md#_snippet_9\n\nLANGUAGE: Console\nCODE:\n```\n$ kubectl get pods -n nifikop\nNAME                                                READY   STATUS    RESTARTS   AGE\nskeleton-nifikop-8946b89dc-4cfs9   1/1     Running   0          7m45s\n```\n\n----------------------------------------\n\nTITLE: Running NiFiKop Locally via Makefile (Bash)\nDESCRIPTION: Executes the `run` target in the Makefile, which typically compiles and runs the operator Go binary locally, connecting to the Kubernetes cluster specified in the default kubeconfig file (`$HOME/.kube/config`).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/6_contributing/1_developer_guide.md#_snippet_5\n\nLANGUAGE: Bash\nCODE:\n```\nmake run\n```\n\n----------------------------------------\n\nTITLE: Declaring NiFi Cluster Nodes with Node Groups or Inline Config YAML\nDESCRIPTION: This snippet illustrates how to declare individual nodes within a NiFi cluster specification. Nodes can reference a predefined configuration using `nodeConfigGroup` or define their technical configuration directly using `nodeConfig`. Each node is identified by a unique `id`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\n- id: 0\n  nodeConfigGroup: \"default_group\"\n- id: 2\n  nodeConfigGroup: \"high_mem_group\"\n- id: 3\n  nodeConfigGroup: \"high_mem_group\"\n- id: 5\n  nodeConfig:\n    resourcesRequirements:\n      limits:\n        cpu: \"2\"\n        memory: 3Gi\n      requests:\n        cpu: \"1\"\n        memory: 1Gi\n```\n\n----------------------------------------\n\nTITLE: Defining a NiFi Connection Resource (YAML)\nDESCRIPTION: This YAML snippet defines a NifiConnection resource, specifying the source and destination components, along with configuration details like flow file expiration, back pressure thresholds, load balancing strategy, and prioritizers. It also sets the update strategy to 'drain'.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/5_references/8_nifi_connection.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiConnection\nmetadata:\n  name: connection\n  namespace: instances\nspec:\n  source:\n    name: input\n    namespace: instances\n    subName: output_1\n    type: dataflow\n  destination:\n    name: output\n    namespace: instances\n    subName: input_1\n    type: dataflow\n  configuration:\n    flowFileExpiration: 1 hour\n    backPressureDataSizeThreshold: 100 GB\n    backPressureObjectThreshold: 10000\n    loadBalanceStrategy: PARTITION_BY_ATTRIBUTE\n    loadBalancePartitionAttribute: partition_attribute\n    loadBalanceCompression: DO_NOT_COMPRESS\n    prioritizers: \n      - NewestFlowFileFirstPrioritizer\n      - FirstInFirstOutPrioritizer\n    labelIndex: 0\n    bends:\n      - posX: 550\n        posY: 550\n      - posX: 550\n        posY: 440\n      - posX: 550\n        posY: 88\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Defining Istio Gateway for HTTPS Termination - YAML\nDESCRIPTION: Specifies an Istio Gateway for terminating HTTPS traffic from external clients on port 443, using a provided TLS credential. The Gateway forwards decrypted HTTP traffic to the service mesh. Prerequisites are Istio configured with TLS secrets (credentialName) and DNS for the selected domain. Outputs a Gateway suitable for translating HTTPS to HTTP before further processing.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: nifi-gateway\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 443\n      name: https\n      protocol: HTTPS\n    tls:\n      mode: SIMPLE\n      credentialName: my-secret\n    hosts:\n    - nifi.my-domain.com\n\n```\n\n----------------------------------------\n\nTITLE: Configuring cert-manager Issuer for Let's Encrypt - YAML\nDESCRIPTION: This YAML sample defines a cert-manager Issuer resource for Let's Encrypt ACME certificate automation in a Kubernetes cluster. The configuration includes ACME-specific fields such as the registration email, endpoint for the staging environment, and a reference to a secret for the account private key. An HTTP01 solver using an ingress resource is included for domain validation. To use this, cert-manager must be installed in the cluster, and the email and secret names must be customized. Limitations include reliance on DNS accessibility and cert-manager's webhook constraints, such as maximum hostname lengths.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: cert-manager.io/v1alpha2\nkind: Issuer\nmetadata:\n  name: letsencrypt-staging\nspec:\n  acme:\n    # You must replace this email address with your own.\n    # Let's Encrypt will use this to contact you about expiring\n    # certificates, and issues related to your account.\n    email: <your email address>\n    server: https://acme-staging-v02.api.letsencrypt.org/directory\n    privateKeySecretRef:\n      # Secret resource used to store the account's private key.\n      name: example-issuer-account-key\n    # Add a single challenge solver, HTTP01 using nginx\n    solvers:\n      - http01:\n          ingress:\n            ingressTemplate:\n              metadata:\n                annotations:\n                  \"external-dns.alpha.kubernetes.io/ttl\": \"5\"\n```\n\n----------------------------------------\n\nTITLE: NiFiKop CRD Migration Script\nDESCRIPTION: This JavaScript code migrates NiFiKop resources from older CRDs (nifi.orange.com/v1alpha1) to newer ones (nifi.konpyutaika.com/v1alpha1). It uses the Kubernetes API to list resources from the old CRDs, copy their specifications and statuses to new resources in the new CRDs.  The script takes the resource type and namespace as command-line arguments, utilizing `@kubernetes/client-node` and `minimist` libraries.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nprocess.env['NODE_TLS_REJECT_UNAUTHORIZED'] = 0;\nconst k8s = require('@kubernetes/client-node');\n\nconst kc = new k8s.KubeConfig();\nkc.loadFromDefault();\n\nconst k8sApi = kc.makeApiClient(k8s.CustomObjectsApi);\n\nconst KONPYUTAIKA_GROUP = 'nifi.konpyutaika.com';\nconst KONPYUTAIKA_GROUP_VERSION = 'v1alpha1';\nconst ORANGE_GROUP = 'nifi.orange.com';\nconst ORANGE_GROUP_VERSION = 'v1alpha1';\n\nconst call = async (SRC_GRP, SRC_GRP_VER, DST_GRP, DST_GRP_VER, KIND_PLURAL, NAMESPACE) => {\n\tconsole.log(`Listing ${KIND_PLURAL} of ${SRC_GRP}/${SRC_GRP_VER} in ${NAMESPACE}...`);\n\tconst listResources = (await k8sApi.listNamespacedCustomObject(SRC_GRP, SRC_GRP_VER, NAMESPACE, KIND_PLURAL)).body.items;\n\treturn Promise.all(listResources.map(async (resource) => {\n\t\ttry {\n\t\t\tconsole.log(`Found ${resource.kind} \"${resource.metadata.name}\" of ${resource.apiVersion} in ${NAMESPACE}`);\n\n\t\t\tif (resource.metadata.ownerReferences) {\n\t\t\t\tconsole.log(`${resource.kind} ${resource.metadata.name} mananged by something else (ownerRefereces is set).`);\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tconst bodyResource = {\n\t\t\t\tapiVersion: `${DST_GRP}/${DST_GRP_VER}`,\n\t\t\t\tkind: resource.kind,\n\t\t\t\tmetadata: {\n\t\t\t\t\tname: resource.metadata.name,\n\t\t\t\t\tannotations: resource.metadata.annotations,\n\t\t\t\t\tlabels: resource.metadata.labels\n\t\t\t\t},\n\t\t\t\tspec: resource.spec\n\t\t\t};\n\n\t\t\tconsole.log(`Creating ${bodyResource.kind} \"${bodyResource.metadata.name}\" of ${bodyResource.apiVersion} in ${NAMESPACE}...`);\n\t\t\tconst newResource = (await k8sApi.createNamespacedCustomObject(DST_GRP, DST_GRP_VER, NAMESPACE, KIND_PLURAL, bodyResource)).body;\n\t\t\tconsole.log('...done creating.');\n\n\t\t\tconst bodyStatus = {\n\t\t\t\tapiVersion: newResource.apiVersion,\n\t\t\t\tkind: newResource.kind,\n\t\t\t\tmetadata: {\n\t\t\t\t\tname: newResource.metadata.name,\n\t\t\t\t\tresourceVersion: newResource.metadata.resourceVersion\n\t\t\t\t},\n\t\t\t\tstatus: resource.status\n\t\t\t};\n\n\t\t\tconsole.log(`Copying status from ${resource.kind} \"${resource.metadata.name}\" of ${newResource.apiVersion} to ${newResource.kind} \"${newResource.metadata.name}\" of ${newResource.apiVersion} in ${NAMESPACE}...`);\n\t\t\tconst newResourceWithStatus = (await k8sApi.replaceNamespacedCustomObjectStatus(DST_GRP, DST_GRP_VER, NAMESPACE, KIND_PLURAL, bodyStatus.metadata.name, bodyStatus)).body;\n\t\t\tconsole.log('...done copying.');\n\t\t\treturn newResourceWithStatus;\n\t\t}\n\t\tcatch (e) {\n\t\t\tconsole.error(e.body ? e.body.message ? e.body.message : e.body : e);\n\t\t}\n\t}));\n};\n\nconst argv = require('minimist')(process.argv.slice(2));\n\nlet NAMESPACE = argv.namespace ? argv.namespace.length > 0 ? argv.namespace : 'default' : 'default';\nlet KIND_PLURAL = {\n\tcluster: 'nificlusters',\n\tdataflow: 'nifidataflows',\n\tparametercontext: 'nifiparametercontexts',\n\tregistryclient: 'nifiregistryclients',\n\tuser: 'nifiusers',\n\tusergroup: 'nifiusergroups',\n};\n\nif (!argv.type) {\n\tconsole.error('Type not provided');\n\tprocess.exit(1);\n}\n\nif (!KIND_PLURAL[argv.type]) {\n\tconsole.error(`Type ${argv.type} is not one of the following types: ${Object.keys(KIND_PLURAL)}`);\n\tprocess.exit(1);\n}\n\nconsole.log(`########### START: ${KIND_PLURAL[argv.type]} ###########`);\ncall( ORANGE_GROUP, ORANGE_GROUP_VERSION, KONPYUTAIKA_GROUP, KONPYUTAIKA_GROUP_VERSION, KIND_PLURAL[argv.type], NAMESPACE)\n\t.then(r => console.log('############ END ############'))\n\t.catch(e => console.error(e));\n```\n\n----------------------------------------\n\nTITLE: Defining Prometheus RBAC Resources in Kubernetes\nDESCRIPTION: Deploys the necessary RBAC (Role-Based Access Control) resources for Prometheus, including ServiceAccount, ClusterRole, and ClusterRoleBinding, to allow Prometheus to access the metrics from NiFi pods.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: prometheus\n  namespace: monitoring-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: prometheus\nrules:\n- apiGroups: [\"\"]\n  resources:\n  - nodes\n  - nodes/metrics\n  - services\n  - endpoints\n  - pods\n  verbs: [\"get\", \"list\", \"watch\"]\n- apiGroups: [\"\"]\n  resources:\n  - configmaps\n  verbs: [\"get\"]\n- apiGroups:\n  - networking.k8s.io\n  resources:\n  - ingresses\n  verbs: [\"get\", \"list\", \"watch\"]\n- nonResourceURLs: [\"/metrics\"]\n  verbs: [\"get\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: prometheus\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: prometheus\nsubjects:\n- kind: ServiceAccount\n  name: prometheus\n  namespace: monitoring-system\n```\n\n----------------------------------------\n\nTITLE: Installing Helm Chart with --skip-crds Parameter - Bash\nDESCRIPTION: Executes the installation of the Nifikop chart with the --skip-crds flag, preventing CRDs from being installed if they already exist. Inputs include the release name and namespace list, with CRDs being managed outside of Helm. Dependencies: Helm CLI and pre-existing CRDs in the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\n$ helm install --name nifikop ./helm/nifikop --set namespaces={\"nifikop\"} --skip-crds\n```\n\n----------------------------------------\n\nTITLE: Creating a Kubernetes Secret for Sensitive Parameters using kubectl\nDESCRIPTION: This console command uses kubectl to create a generic Kubernetes secret named secret-params in the nifikop namespace. It populates the secret with literal key-value pairs, which can then be referenced by a NifiParameterContext resource for sensitive parameters.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl create secret generic secret-params \\\n    --from-literal=secret1=yop \\\n    --from-literal=secret2=yep \\\n    -n nifikop\n```\n\n----------------------------------------\n\nTITLE: Create Custom StorageClass - Kubernetes\nDESCRIPTION: This Kubernetes YAML defines a custom StorageClass named `exampleStorageclass` using the `pd-standard` type and Google Compute Engine persistent disks.  The `volumeBindingMode` is set to `WaitForFirstConsumer` to delay volume binding until a pod is scheduled. The reclaim policy is set to `Delete` to delete the volume when the PVC is deleted. This is required to leverage the volume binding mode `WaitForFirstConsumer`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: exampleStorageclass\nparameters:\n  type: pd-standard\nprovisioner: kubernetes.io/gce-pd\nreclaimPolicy: Delete\nvolumeBindingMode: WaitForFirstConsumer\n```\n\n----------------------------------------\n\nTITLE: Configuring NifiConnection (YAML)\nDESCRIPTION: This YAML snippet defines a NifiConnection resource named `connection`. It establishes a connection between the `input` and `output` dataflows, specifying source and destination. It sets several configuration options: back-pressure thresholds, flow file expiration, label index, and bends.  The bends configure the visual layout.  The `updateStrategy` is also set.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/4_manage_connections/1_deploy_connection.md#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiConnection\nmetadata:\n  name: connection\n  namespace: nifikop\nspec:\n  source:\n    name: input\n    namespace: nifikop\n    subName: output\n    type: dataflow\n  destination:\n    name: output\n    namespace: nifikop\n    subName: input\n    type: dataflow\n  configuration:\n    backPressureDataSizeThreshold: 100 GB\n    backPressureObjectThreshold: 10000\n    flowFileExpiration: 1 hour\n    labelIndex: 0\n    bends:\n      - posX: 550\n        posY: 550\n      - posX: 550\n        posY: 440\n      - posX: 550\n        posY: 88\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Configure CRD Conversion Webhook\nDESCRIPTION: This YAML snippet shows how to configure the CRD conversion webhook for handling resource version conversions from `v1alpha1` to `v1`. The webhook needs `cert-manager.io/inject-ca-from` annotation which references the namespace and certificate. `conversion` section with `strategy: Webhook` specifies the use of a webhook, linking it to a Kubernetes service for handling the conversion.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop Helm Chart Bash\nDESCRIPTION: This command installs the NiFiKop operator onto Kubernetes using Helm. The 'helm install' command references the chart directory and sets the image tag and namespace. Variables such as 'image.tag' and 'namespace' must match the previously pushed image and desired deployment scope. Requires Helm 3.x+, a valid Kubernetes config, and previously pushed operator image.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/6_contributing/1_developer_guide.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nhelm install skeleton ./helm/nifikop \\\n    --set image.tag=v0.5.1-release \\\n    --namespace-{\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Define NiFi Internal Listeners Config (YAML)\nDESCRIPTION: This YAML snippet demonstrates how to configure internal listeners for a NiFi cluster using the ListenersConfig custom resource definition. It specifies different listener types (https, cluster, s2s, prometheus, load-balance) with their corresponding container ports. Additionally, it configures SSL secret settings, including the TLS secret name and whether to create the secrets.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/5_references/1_nifi_cluster/6_listeners_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    - type: \"https\"\n      name: \"https\"\n      containerPort: 8443\n    - type: \"cluster\"\n      name: \"cluster\"\n      containerPort: 6007\n    - type: \"s2s\"\n      name: \"s2s\"\n      containerPort: 10000\n    - type: \"prometheus\"\n      name: \"prometheus\"\n      containerPort: 9090\n    - type: \"load-balance\"\n      name: \"load-balance\"\n      containerPort: 6342\n    - name: \"my-custom-listener-port\"\n      containerPort: 1234\n      protocol: \"TCP\"\n  sslSecrets:\n    tlsSecretName: \"test-nifikop\"\n    create: true\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Secret for Sensitive NiFi Parameter Values via Console\nDESCRIPTION: This console command snippet demonstrates how to create a Kubernetes generic secret named 'secret-params' containing key-value pairs for sensitive parameters used in NiFi parameter contexts. It is a prerequisite for managing sensitive parameters securely in NiFiKop-managed dataflows. Modifications to sensitive values require secret updates followed by reconciliation by the operator or recreation of the parameter context.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl create secret generic secret-params \\\n    --from-literal=secret1=yop \\\n    --from-literal=secret2=yep \\\n    -n nifikop\n```\n\n----------------------------------------\n\nTITLE: Defining Managed Groups in NifiCluster (YAML)\nDESCRIPTION: This YAML snippet demonstrates how to define managed users and groups within the `NifiCluster.Spec` field. It specifies the identities and names of users who should be granted admin and reader access to the NiFi cluster. The operator automatically creates and manages the corresponding `NifiUsers` and `NifiUserGroups` based on this configuration.  It assumes the existence of the NiFi operator and the custom resource definition (CRD) for `NifiCluster`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/2_manage_users_and_accesses/3_managed_groups.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  ...\n  oneNifiNodePerNode: false\n  propagateLabels: true\n  managedAdminUsers:\n    -  identity: \"alexandre.guitton@konpyutaika.com\"\n       name: \"aguitton\"\n    -  identity: \"nifiuser@konpyutaika.com\"\n       name: \"nifiuser\"\n  managedReaderUsers:\n    -  identity: \"toto@konpyutaika.com\"\n       name: \"toto\"\n    ...\n```\n\n----------------------------------------\n\nTITLE: SecretReference Data Model\nDESCRIPTION: Specifies a reference to a Secret resource containing sensitive parameters, including secret name and namespace. Ensures secure handling of confidential data in parameter contexts.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/5_references/4_nifi_parameter_context.md#_snippet_5\n\nLANGUAGE: YAML\nCODE:\n```\nSecretReference:\n  name: string\n  namespace: string\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop using Helm - Console command\nDESCRIPTION: This snippet demonstrates the command-line process for installing the NiFiKop operator with Helm on a Kubernetes cluster. Dependencies include a Helm client (version 3 or higher) and valid access to the 'konpyutaika/nifikop' Helm chart repository. Key parameters such as release name and custom configuration via a YAML values file can be specified. Input consists of the desired release name, chart repository, and YAML file ('values.yaml'); output is the creation of Kubernetes resources for NiFiKop.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n$ helm install nifikop \\\n      konpyutaika/nifikop \\\n      -f values.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining Self-signed Certificates and Certificate Issuers for Internal HTTPS in YAML\nDESCRIPTION: These YAML manifests define self-signed certificate issuers and certificates to enable HTTPS for cluster-internal addresses (e.g., cluster.local), which is a prerequisite for OIDC integration. They provide secure communication within the cluster where public CA certificates are not applicable, ensuring services like NiFi and Keycloak can authenticate securely over HTTPS.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/config/samples/keycloak-example/README.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\nself-signed-issuer.yaml\n```\n\nLANGUAGE: YAML\nCODE:\n```\nself-signed-cert.yaml\n```\n\nLANGUAGE: YAML\nCODE:\n```\nnifi-issuer.yaml\n```\n\n----------------------------------------\n\nTITLE: Checking the status of node decommissioning in NiFiCluster\nDESCRIPTION: This command displays detailed status information about the NiFi cluster, particularly the nodes' states and their progress in the decommissioning process. It helps verify that the targeted node has completed its graceful shutdown and removal.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nkubectl describe nificluster simplenifi\n```\n\n----------------------------------------\n\nTITLE: Cleanup: Remove NiFi Cluster and Operator Using kubectl and helm (Bash)\nDESCRIPTION: This bash sequence deletes the NiFiCluster instance, NiFiKop operator, CRDs, and issuer via kubectl and helm commands. Dependencies: targeted resources must exist and kubectl, helm CLIs must be installed. No parameters are needed as resource names are given. Results in removal of cluster, operator, custom resource definitions, and certificate issuers. Execution order is important for resource and dependency cleanup.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/blog/2020-06-30-secured_nifi_cluster_on_gcp_with_external_dns.md#_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete nificlusters.nifi.orange.com nifi -n nifikop\nhelm del nifikop\nkubectl delete crds nificlusters.nifi.orange.com\nkubectl delete crds nifiusers.nifi.orange.com\nkubectl delete issuers.cert-manager.io letsencrypt-staging -n nifikop\n```\n\n----------------------------------------\n\nTITLE: Retrieving Managed NiFi User Groups Using kubectl (Console Command)\nDESCRIPTION: A command-line example for listing all managed NiFi user groups in the 'nifikop' namespace using kubectl. This is useful for verifying that the nifikop operator has properly created and synced user groups ('managed-admins', 'managed-nodes', 'managed-readers') according to your resource definitions. Dependencies: Access to the target Kubernetes cluster and kubectl CLI configured. Input: N/A. Output: Table of group names and their ages.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/3_tasks/4_nifi_user_group.md#_snippet_3\n\nLANGUAGE: console\nCODE:\n```\nkubectl get -n nifikop nifiusergroups.nifi.konpyutaika.com \nNAME              AGE\nmanaged-admins    6d7h\nmanaged-nodes     6d7h\nmanaged-readers   6d7h\n\n```\n\n----------------------------------------\n\nTITLE: Deploying an Output NifiDataflow YAML\nDESCRIPTION: This YAML configuration defines an output NifiDataflow within a NiFiKop managed NiFi cluster. Similar to the input dataflow, it specifies cluster reference, bucket ID, flow ID, flow version, and synchronization mode.  The `output` dataflow must have an `input port`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/4_manage_connections/1_deploy_connection.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiDataflow\nmetadata:\n  name: output\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  bucketId: deedb9f6-65a4-44e9-a1c9-722008fcd0ba\n  flowId: fc5363eb-801e-432f-aa94-488838674d07\n  flowVersion: 2\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  skipInvalidComponent: true\n  skipInvalidControllerService: true\n  syncMode: always\n  updateStrategy: drain\n  flowPosition:\n    posX: 750\n    posY: 0\n```\n\n----------------------------------------\n\nTITLE: Defining NifiDataflow YAML Manifest\nDESCRIPTION: This YAML snippet defines a NifiDataflow resource. It specifies metadata like the name, and then the specifications for the dataflow, including parent process group ID, bucket ID, flow ID, flow version, and cluster/registry client references. The `spec` section outlines the desired state of the dataflow deployment, while the `metadata` section contains standard Kubernetes object metadata.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/5_references/5_nifi_dataflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  flowPosition:\n    posX: 0\n    posY: 0\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: squidflow\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: nifikop\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Defining Node Configuration Groups in NiFiKop - YAML\nDESCRIPTION: This YAML snippet demonstrates how to define multiple node configuration groups (default_group, high_mem_group) for NiFiKop. Each group specifies provenance storage, user ID, service account, and Kubernetes container resource requirements (CPU/memory for limits and requests). These groups let you quickly assign resource profiles to nodes, promoting reusability and consistency across your cluster. All keys and fields conform to NiFiKop custom resource spec attributes; Kubernetes must be available and NiFiKop installed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n  nodeConfigGroups:\n    default_group:\n      provenanceStorage: \"10 GB\"\n      runAsUser: 1000\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 3Gi\n    high_mem_group:\n      provenanceStorage: \"10 GB\"\n      runAsUser: 1000\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 30Gi\n        requests:\n          cpu: \"1\"\n          memory: 30Gi\n```\n\n----------------------------------------\n\nTITLE: Creating an External Issuer for Let's Encrypt with YAML\nDESCRIPTION: Provides YAML configuration for creating a cert-manager issuer resource using Let's Encrypt staging environment. The issuer is referenced in NiFi cluster configuration to obtain SSL certificates from an external authority.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/3_tasks/2_security/1_ssl.md#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: cert-manager.io/v1alpha2\nkind: Issuer\nmetadata:\n  name: letsencrypt-staging\nspec:\n  acme:\n    email: <your email address>\n    server: https://acme-staging-v02.api.letsencrypt.org/directory\n    privateKeySecretRef:\n      name: example-issuer-account-key\n    solvers:\n      - http01:\n          ingress:\n            ingressTemplate:\n              metadata:\n                annotations:\n                  \"external-dns.alpha.kubernetes.io/ttl\": \"5\"\n```\n\n----------------------------------------\n\nTITLE: Configuring NifiRegistryClient Resource in YAML\nDESCRIPTION: Example of a NifiRegistryClient custom resource definition that connects to a NiFi Registry instance. This resource is required before deploying dataflows as NiFiKop manages dataflows using the NiFi Registry feature.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiRegistryClient\nmetadata:\n  name: registry-client-example\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Registry client managed by NiFiKop\"\n  uri: \"http://nifi.hostname.com:18080\"\n```\n\n----------------------------------------\n\nTITLE: Defining Node Configuration Groups in NiFiKop YAML\nDESCRIPTION: This snippet demonstrates how to define reusable technical configurations for NiFi nodes using the `nodeConfigGroups` field in a NiFiKop `NifiCluster` resource. It covers setting resource requirements (CPU, memory), provenance storage limits, the user ID to run the NiFi process, and the Kubernetes service account name.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n  # nodeConfigGroups specifies multiple node configs with unique name\n  nodeConfigGroups:\n    default_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      serviceAccountName: \"default\"\n      # resourceRequirements works exactly like Container resources, the user can specify the limit and the requests\n      # through this property\n      # https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 3Gi\n    high_mem_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      serviceAccountName: \"default\"\n      # resourceRequirements works exactly like Container resources, the user can specify the limit and the requests\n      # through this property\n      # https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 30Gi\n        requests:\n          cpu: \"1\"\n          memory: 30Gi\n```\n\n----------------------------------------\n\nTITLE: Configuring VirtualService for HTTPS Traffic Routing\nDESCRIPTION: Creates a VirtualService that routes HTTPS traffic arriving at the 'nifi-gateway' to a specified ClusterIP service endpoint on port 8443. The destination host is referenced with the full service namespace, facilitating secure communication via TLS.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: nifi-vs\nspec:\n  gateways:\n  - nifi-gateway\n  hosts:\n  - nifi.my-domain.com\n  http:\n  - match:\n    - uri:\n        prefix: /\n    route:\n    - destination:\n        host: <service-name>.<namespace>.svc.cluster.local\n        port:\n          number: 8443\n```\n\n----------------------------------------\n\nTITLE: Installing cert-manager Helm Chart Using Bash and Helm 3\nDESCRIPTION: This set of bash commands installs cert-manager using Helm 3. It first applies the necessary CRD manifests without validation, then adds and updates the jetstack Helm repository. After creating the required namespace, it installs the cert-manager Helm chart at version v1.7.2 in the cert-manager namespace. Prerequisites include having Helm 3 installed and kubectl access to the cluster. This approach facilitates easier upgrades and management of cert-manager via Helm.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/2_deploy_nifikop/1_quick_start.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Install CustomResourceDefinitions first\nkubectl apply --validate=false -f \\\n   https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.crds.yaml\n\n# Add the jetstack helm repo\nhelm repo add jetstack https://charts.jetstack.io\nhelm repo update\n\n# You have to create the namespace before executing following command\nhelm install cert-manager \\\n    --namespace cert-manager \\\n    --version v1.7.2 jetstack/cert-manager\n```\n\n----------------------------------------\n\nTITLE: Building NiFiKop Operator Locally Using Make in Bash\nDESCRIPTION: This make command builds the NiFiKop operator using the project's Makefile. A compatible Go toolchain must be installed and configured. No arguments are required, and the output will be a compiled operator binary. Limitations include the need for local Go dependencies and access to the project's files.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/6_contributing/1_developer_guide.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmake build\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop - Release Name\nDESCRIPTION: This command installs the NiFiKop chart with a specified release name using Helm.  Replace `<release name>` with the desired name for your deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nhelm install <release name> konpyutaika/nifikop\n```\n\n----------------------------------------\n\nTITLE: Creating Custom StorageClass in Kubernetes\nDESCRIPTION: This YAML snippet defines a custom StorageClass with volumeBindingMode set to WaitForFirstConsumer, which is recommended for NiFi deployments on Kubernetes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: exampleStorageclass\nparameters:\n  type: pd-standard\nprovisioner: kubernetes.io/gce-pd\nreclaimPolicy: Delete\nvolumeBindingMode: WaitForFirstConsumer\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Read-Only Configuration (YAML)\nDESCRIPTION: Example YAML structure for the `readOnlyConfig` object. This section defines cluster-wide NiFi settings that are generally static, such as maximum thread counts and configurations for logback, authorizers, nifi.properties, zookeeper.properties, and bootstrap.properties. These settings can be overridden at the node level and allow customization through external ConfigMaps/Secrets or direct inline overrides.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/5_references/1_nifi_cluster/2_read_only_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreadOnlyConfig:\n  # MaximumTimerDrivenThreadCount define the maximum number of threads for timer driven processors available to the system.\n  maximumTimerDrivenThreadCount: 30\n  # MaximumEventDrivenThreadCount define the maximum number of threads for event driven processors available to the system.\n  maximumEventDrivenThreadCount: 10\n  # Logback configuration that will be applied to the node\n  logbackConfig:\n    # logback.xml configuration that will replace the one produced based on template\n    replaceConfigMap:\n      # The key of the value,in data content, that we want use.\n      data: logback.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # logback.xml configuration that will replace the one produced based on template and overrideConfigMap\n    replaceSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: logback.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n  # Authorizer configuration that will be applied to the node\n  authorizerConfig:\n    # An authorizers.xml configuration template that will replace the default template seen in authorizers.go\n    replaceTemplateConfigMap:\n      # The key of the value, in data content, that we want use.\n      data: authorizers.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # An authorizers.xml configuration template that will replace the default template seen in authorizers.go and the replaceTemplateConfigMap\n    replaceTemplateSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: authorizers.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n  # NifiProperties configuration that will be applied to the node.\n  nifiProperties:\n    # Additionnal nifi.properties configuration that will override the one produced based on template and\n    # configuration\n    overrideConfigMap:\n      # The key of the value,in data content, that we want use.\n      data: nifi.properties\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop.\n    # Additionnal nifi.properties configuration that will override the one produced based\n    #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    overrideSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: nifi.properties\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # Additionnal nifi.properties configuration that will override the one produced based\n    #\ton template, configurations and overrideConfigMap\n    overrideConfigs: |\n      nifi.ui.banner.text=NiFiKop\n    # A comma separated list of allowed HTTP Host header values to consider when NiFi\n    # is running securely and will be receiving requests to a different host[:port] than it is bound to.\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#web-properties\n    #      webProxyHosts:\n    # Nifi security client auth\n    needClientAuth: false\n    # Indicates which of the configured authorizers in the authorizers.xml file to use\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#authorizer-configuration\n  #      authorizer:\n  # ZookeeperProperties configuration that will be applied to the node.\n  zookeeperProperties:\n    #      # Additionnal zookeeeper.properties configuration that will override the one produced based on template and\n    #      # configuration\n    #      overrideConfigMap:\n    #        # The key of the value,in data content, that we want use.\n    #        data: zookeeeper.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop.\n    #      # Additionnal zookeeeper.properties configuration that will override the one produced based\n    #      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    #      overrideSecretConfig:\n    #        # The key of the value,in data content, that we want use.\n    #        data: zookeeeper.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop\n    # Additionnal zookeeper.properties configuration that will override the one produced based\n    # on template and configurations.\n    overrideConfigs: |\n      initLimit=15\n      autopurge.purgeInterval=24\n      syncLimit=5\n      tickTime=2000\n      dataDir=./state/zookeeper\n      autopurge.snapRetainCount=30\n  # BootstrapProperties configuration that will be applied to the node.\n  bootstrapProperties:\n    #      # Additionnal bootstrap.properties configuration that will override the one produced based on template and\n    #      # configuration\n    #      overrideConfigMap:\n    #        # The key of the value,in data content, that we want use.\n    #        data: bootstrap.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop.\n    #      # Additionnal bootstrap.properties configuration that will override the one produced based\n    #      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    #      overrideSecretConfig:\n    #        # The key of the value,in data content, that we want use.\n    #        data: bootstrap.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop\n    # JVM memory settings\n    nifiJvmMemory: \"512m\"\n    # Additionnal bootstrap.properties configuration that will override the one produced based\n    # on template and configurations.\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#bootstrap_properties\n    overrideConfigs: |\n      # java.arg.4=-Djava.net.preferIPv4Stack=true\n```\n\n----------------------------------------\n\nTITLE: Creating a NifiUser Resource in Kubernetes (YAML)\nDESCRIPTION: This YAML manifest defines a `NifiUser` custom resource named `aguitton` managed by the Nifikop operator. It specifies the user's identity (`alexandre.guitton@konpyutaika.com`), links it to a NiFi cluster named `nc` within the `nifikop` namespace, and explicitly sets `createCert` to `false`, indicating that a client certificate should not be automatically generated for this user.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/5_references/2_nifi_user.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiUser\nmetadata:\n  name: aguitton\nspec:\n  identity: alexandre.guitton@konpyutaika.com\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  createCert: false\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiRegistryClient Resource (YAML)\nDESCRIPTION: This YAML snippet defines a `NifiRegistryClient` Custom Resource (CR) for NiFiKop. It specifies how NiFiKop should connect to an external NiFi Registry instance, providing the cluster reference (`clusterRef`) and the URI (`uri`) of the registry.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiRegistryClient\nmetadata:\n  name: registry-client-example\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Registry client managed by NiFiKop\"\n  uri: \"http://nifi.hostname.com:18080\"\n```\n\n----------------------------------------\n\nTITLE: Defining NiFiRegistryClient CRD in YAML\nDESCRIPTION: This YAML snippet defines a NifiRegistryClient custom resource that connects to a NiFi Registry service. It includes metadata such as name, namespace, description, and the registry URI. It ensures NiFiKop can manage and interact with the specified NiFi Registry instance.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiRegistryClient\nmetadata:\n  name: registry-client-example\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Registry client managed by NiFiKop\"\n  uri: \"http://nifi.hostname.com:18080\"\n```\n\n----------------------------------------\n\nTITLE: Creating a NifiUser Resource (YAML)\nDESCRIPTION: This YAML snippet provides an example definition for a NifiUser custom resource. It specifies the API version, kind, metadata (like name), and desired state in the spec field, including linking to a NiFi cluster and configuring certificate creation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/5_references/2_nifi_user.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: aguitton\nspec:\n  identity: alexandre.guitton@konpyutaika.com\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  createCert: false\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi User Custom Resource Using YAML\nDESCRIPTION: This YAML snippet defines a NiFiUser custom resource for the Nifikop Kubernetes operator using the apiVersion \"nifi.konpyutaika.com/v1\". The resource specifies user identification details, the associated NiFi cluster reference, and whether a certificate should be created. It serves as an example manifest to instantiate NiFi users managed by the operator.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/5_references/2_nifi_user.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: aguitton\nspec:\n  identity: alexandre.guitton@konpyutaika.com\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  createCert: false\n```\n\n----------------------------------------\n\nTITLE: Defining External Services with LoadBalancer in YAML\nDESCRIPTION: This YAML snippet demonstrates setting up an external load balancer service named \"nlb\" with specific load balancer class and port configurations including UDP protocol. Metadata annotations and labels are included to provide additional configuration context, suitable for exposing services externally via cloud provider load balancers.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/5_references/1_nifi_cluster/7_external_service_config.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nexternalServices:\n    - name: \"nlb\"\n      spec:\n        type: LoadBalancer\n        loadBalancerClass: \"service.k8s.aws/nlb\"\n        portConfigs:\n          - port: 8080\n            internalListenerName: \"http\"\n          - port: 7890\n            internalListenerName: \"my-custom-udp-listener\"\n            protocol: UDP\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n\n----------------------------------------\n\nTITLE: Defining Secured NiFi Cluster with NiFiKop in YAML\nDESCRIPTION: This YAML snippet configures a secured NiFi cluster resource for deployment on Kubernetes through the NiFiKop operator. Key properties include admin user email, OAuth2 client credentials, DNS hostnames, and SSL sources. Dependencies: Kubernetes cluster with NiFiKop operator, cert-manager, ExternalDNS, valid OAuth client, and Let's Encrypt issuer. You must replace placeholder values (such as admin email, OAuth credentials, and DNS names) with environment-specific details. Output is a Kubernetes custom resource for a running NiFi cluster; constraints include correct Kubernetes setup and working external dependencies.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/blog/2020-06-30-secured_nifi_cluster_on_gcp_with_external_dns.md#_snippet_15\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.orange.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: securednificluster\n  namespace: nifi\nspec:\n  ...\n  initialAdminUser: <your google account email>\n  readOnlyConfig:\n    # NifiProperties configuration that will be applied to the node.\n    nifiProperties:\n      webProxyHosts:\n        - <nifi's hostname>:8443\n      # Additionnal nifi.properties configuration that will override the one produced based\n      # on template and configurations.\n      overrideConfigs: |\n        ...\n        nifi.security.user.oidc.client.id=<oidc client's id>\n        nifi.security.user.oidc.client.secret=<oidc client's secret>\n        ...\n    ...\n  ...\n  listenersConfig:\n    useExternalDNS: true\n    clusterDomain: <nifi's domain name>\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n      issuerRef:\n        name: letsencrypt-staging\n        kind: Issuer\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Cluster with SSL using YAML\nDESCRIPTION: Defines the YAML configuration for deploying a secure NiFi cluster with SSL enabled. Includes admin user management, web proxy hosts, listener configurations, and SSL secret creation settings. Requires the Kubernetes operator for NiFi and appropriate certificates.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/3_tasks/2_security/1_ssl.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\n...\nspec:\n  ...\n  managedAdminUsers:\n    - identity: \"alexandre.guitton@konpyutaika.com\"\n      name: \"aguitton\"\n  ...\n  readOnlyConfig:\n    # NifiProperties configuration that will be applied to the node.\n    nifiProperties:\n      webProxyHosts:\n        - nifistandard2.trycatchlearn.fr:8443\n        ...\n  ...\n  listenersConfig:\n    internalListeners:\n      - type: \"https\"\n        name: \"https\"\n        containerPort: 8443\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n```\n\n----------------------------------------\n\nTITLE: Configuring ClusterIP External Service in YAML\nDESCRIPTION: This YAML snippet defines an external service of type ClusterIP. It specifies the service name, port configurations including mapping external ports to internal listener names, and metadata such as annotations and labels.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/5_references/1_nifi_cluster/7_external_service_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nexternalServices:\n    - name: \"clusterip\"\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8080\n            internalListenerName: \"http\"\n          - port: 7182\n            internalListenerName: \"my-custom-listener\"\n            protocol: TCP\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes StorageClass with WaitForFirstConsumer Binding Mode - YAML\nDESCRIPTION: Defines a custom Kubernetes StorageClass resource configured with the 'WaitForFirstConsumer' volumeBindingMode to delay volume binding until a Pod consumes it, improving scheduling efficiency. The StorageClass uses the GCE Persistent Disk provisioner with 'pd-standard' type and sets the reclaim policy to 'Delete'. This snippet requires Kubernetes cluster with support for StorageClass resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: exampleStorageclass\nparameters:\n  type: pd-standard\nprovisioner: kubernetes.io/gce-pd\nreclaimPolicy: Delete\nvolumeBindingMode: WaitForFirstConsumer\n```\n\n----------------------------------------\n\nTITLE: Installing Zookeeper on Kubernetes Using Bitnami Helm Chart\nDESCRIPTION: Installs Zookeeper using the Bitnami Helm chart with specified resource requests and limits, setting the storage class to 'standard', enabling network policies, and deploying three replicas. Requires Helm repository addition and Helm install command. Replace 'storageClass' value as needed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhelm repo add bitnami https://charts.bitnami.com/bitnami\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Registry Client Configuration (YAML)\nDESCRIPTION: This YAML snippet defines a `NifiRegistryClient` custom resource. It sets the API version, kind, and metadata, including the name. The `spec` section contains the configuration for the client, referring to a NiFi cluster named 'nc' in the 'nifikop' namespace, along with a description and the URI of the NiFi registry.  This configuration is used to connect and interact with a NiFi Registry instance.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/5_references/3_nifi_registry_client.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n```yaml\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiRegistryClient\nmetadata:\n  name: squidflow\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Squidflow demo\"\n  uri: \"http://nifi-registry:18080\"\n```\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio VirtualService for HTTP Access to NiFi\nDESCRIPTION: Defines an Istio VirtualService that routes traffic from the Gateway to the NiFi service running on port 8080. This complements the Gateway configuration and completes the HTTP routing path.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: nifi-vs\nspec:\n  gateways:\n  - nifi-gateway\n  hosts:\n  - nifi.my-domain.com\n  http:\n  - match:\n    - uri:\n        prefix: /\n    route:\n    - destination:\n        host: nifi\n        port:\n          number: 8080\n```\n\n----------------------------------------\n\nTITLE: Installing Zookeeper using Helm (Bash)\nDESCRIPTION: Installs the Bitnami Zookeeper chart using Helm into the 'zookeeper' namespace (creating it if necessary). It configures resource requests/limits (CPU, memory), sets the storage class (which should be replaced with your custom one), enables network policy, and specifies 3 replicas. Zookeeper is a prerequisite for NiFi.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nhelm install zookeeper bitnami/zookeeper \\\n    --namespace=zookeeper \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set global.storageClass=standard \\\n    --set networkPolicy.enabled=true \\\n    --set replicaCount=3 \\\n    --create-namespace\n```\n\n----------------------------------------\n\nTITLE: Creating a Secret for Basic Authentication - Bash\nDESCRIPTION: This bash command demonstrates how to create a Kubernetes secret containing the username, password, and optional CA certificate required for basic authentication against the external NiFi cluster. This secret is referenced by the NifiCluster resource to provide the operator with the necessary credentials to interact with the NiFi API.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/1_manage_clusters/3_external_cluster.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create secret generic nifikop-credentials \\\n  --from-file=username=./secrets/username\\\n  --from-file=password=./secrets/password\\\n  --from-file=ca.crt=./secrets/ca.crt\\\n  -n nifikop-nifi\n```\n\n----------------------------------------\n\nTITLE: Checking NiFiKop Deployment Status\nDESCRIPTION: Command to check the detailed status of the NiFiKop Helm deployment, including information about pods, services, and other resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nhelm status nifikop\n```\n\n----------------------------------------\n\nTITLE: Applying the updated NiFiCluster configuration to Kubernetes for node removal\nDESCRIPTION: This bash command applies the modified NiFiCluster YAML, prompting the cluster operator to initiate the decommission process for the specified node, including resource cleanup and graceful shutdown steps.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl -n nifi apply -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining Apache NiFi Node Configuration in YAML\nDESCRIPTION: This YAML snippet demonstrates the structure for configuring NiFi nodes within the nifikop operator. It shows usage of fields such as id, nodeConfigGroup, readOnlyConfig (with overrideConfigs for NiFi properties), resource and storage allocations, and Kubernetes PVC parameters. The configuration requires YAML syntax and depends on the nifikop operator and a working Kubernetes cluster; overrides and resource parameters are provided as key-value pairs under their respective sections. The snippet expects unique node ids and allows specifying both read-only and mutable options, with required and optional parameters as documented.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/5_references/1_nifi_cluster/4_node.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n    - id: 0\n      # nodeConfigGroup can be used to ease the node configuration, if set only the id is required\n      nodeConfigGroup: \"default_group\"\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        nifiProperties:\n          overrideConfigs: |\n            nifi.ui.banner.text=NiFiKop - Node 0\n      # node configuration\n#       nodeConfig:\n    - id: 2\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        overrideConfigs: |\n          nifi.ui.banner.text=NiFiKop - Node 2\n      # node configuration\n      nodeConfig:\n        resourcesRequirements:\n          limits:\n            cpu: \"2\"\n            memory: 3Gi\n          requests:\n            cpu: \"1\"\n            memory: 1Gi\n        storageConfigs:\n          # Name of the storage config, used to name PV to reuse into sidecars for example.\n          - name: provenance-repository\n            # Path where the volume will be mount into the main nifi container inside the pod.\n            mountPath: \"/opt/nifi/provenance_repository\"\n            # Metadata to attach to the PVC that gets created\n            metadata:\n              labels:\n                my-label: my-value\n              annotations:\n                my-annotation: my-value\n            # Kubernetes PVC spec\n            # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n            pvcSpec:\n              accessModes:\n                - ReadWriteOnce\n              storageClassName: \"standard\"\n              resources:\n                requests:\n                  storage: 8Gi\n\n```\n\n----------------------------------------\n\nTITLE: Deploying Prometheus Operator Stack - Console\nDESCRIPTION: Installs the Prometheus Operator stack into the 'monitoring-system' namespace using Helm. Disables several default components (Prometheus, Grafana, Alertmanager, etc.) and configures logging as well as all instance namespaces. Requires Helm v3+, Kubernetes cluster access, and the prometheus-community repository configured.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_8\n\nLANGUAGE: console\nCODE:\n```\nhelm install prometheus prometheus/kube-prometheus-stack --namespace monitoring-system \\\n    --set prometheusOperator.createCustomResource=false \\\n    --set prometheusOperator.logLevel=debug \\\n    --set prometheusOperator.alertmanagerInstanceNamespaces=monitoring-system \\\n    --set prometheusOperator.namespaces.additional[0]=monitoring-system \\\n    --set prometheusOperator.prometheusInstanceNamespaces=monitoring-system \\\n    --set prometheusOperator.thanosRulerInstanceNamespaces=monitoring-system \\\n    --set defaultRules.enable=false \\\n    --set alertmanager.enabled=false \\\n    --set grafana.enabled=false \\\n    --set kubeApiServer.enabled=false \\\n    --set kubelet.enabled=false \\\n    --set kubeControllerManager.enabled=false \\\n    --set coreDNS.enabled=false \\\n    --set kubeEtcd.enabled=false \\\n    --set kubeScheduler.enabled=false \\\n    --set kubeProxy.enabled=false \\\n    --set kubeStateMetrics.enabled=false \\\n    --set prometheus.enabled=false\n```\n\n----------------------------------------\n\nTITLE: Configuring Internal Listeners in NiFi on Kubernetes (YAML)\nDESCRIPTION: This code snippet demonstrates how to configure internal listeners for a NiFi cluster deployed on Kubernetes.  It defines the types, names, and container ports for various NiFi internal services, such as HTTPS, cluster communication, Site-to-Site communication, Prometheus metrics, and load balancing.  These listeners are crucial for the internal operation of the NiFi cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    - type: \"https\"\n      name: \"https\"\n      containerPort: 8443\n    - type: \"cluster\"\n      name: \"cluster\"\n      containerPort: 6007\n    - type: \"s2s\"\n      name: \"s2s\"\n      containerPort: 10000\n    - type: \"prometheus\"\n      name: \"prometheus\"\n      containerPort: 9090\n    - type: \"load-balance\"\n      name: \"load-balance\"\n      containerPort: 6342\n```\n\n----------------------------------------\n\nTITLE: Defining Prometheus Instance Kubernetes/YAML\nDESCRIPTION: This YAML manifest defines a custom Prometheus resource managed by the Prometheus Operator. It configures basic settings for the Prometheus instance, including logging level, scrape/evaluation intervals, and importantly, specifies selectors (`podMonitorSelector` and `serviceMonitorSelector`) to discover monitoring targets defined by PodMonitor and ServiceMonitor resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: Prometheus\nmetadata:\n  name: prometheus\n  namespace: monitoring-system\nspec:\n  enableAdminAPI: false\n  evaluationInterval: 30s\n  logLevel: debug\n  podMonitorSelector:\n    matchExpressions:\n    - key: release\n      operator: In\n      values:\n      - prometheus\n  resources:\n    requests:\n      memory: 400Mi\n  scrapeInterval: 30s\n  serviceAccountName: prometheus\n  serviceMonitorSelector:\n    matchExpressions:\n    - key: app\n      operator: In\n      values:\n      - nifi-cluster\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Node with YAML\nDESCRIPTION: This YAML snippet configures a NiFi node. It sets the node ID, references a configuration group, and defines read-only configurations to trigger rolling upgrades. The readOnlyConfig includes custom NiFi properties such as UI banner text. This section defines the basic configuration of a NiFi node, utilizing groups for configuration and specifying a read-only configuration for controlled updates.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/5_references/1_nifi_cluster/4_node.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n    - id: 0\n      # nodeConfigGroup can be used to ease the node configuration, if set only the id is required\n      nodeConfigGroup: \"default_group\"\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        nifiProperties:\n          overrideConfigs: |\n            nifi.ui.banner.text=NiFiKop - Node 0\n      # node configuration\n#       nodeConfig:\n\n```\n\n----------------------------------------\n\nTITLE: Adding Helm Repository prometheus-community/helm-charts Console\nDESCRIPTION: This command adds the 'prometheus-community' Helm chart repository to the local Helm configuration. This repository contains the 'kube-prometheus-stack' chart required to deploy the Prometheus operator and related components.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_6\n\nLANGUAGE: console\nCODE:\n```\nhelm repo add prometheus https://prometheus-community.github.io/helm-charts\n```\n\n----------------------------------------\n\nTITLE: Listing Managed NiFi User Groups via kubectl Command\nDESCRIPTION: This console command retrieves the list of NiFiUserGroups managed by the NiFi Operator within the Kubernetes namespace 'nifikop'. It confirms the presence and age of the automatically managed groups such as 'managed-admins', 'managed-nodes', and 'managed-readers'. This step is used to verify the cluster's user access control configuration as maintained by the operator.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/2_manage_users_and_accesses/3_managed_groups.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nkubectl get -n nifikop nifiusergroups.nifi.konpyutaika.com \nNAME              AGE\nmanaged-admins    6d7h\nmanaged-nodes     6d7h\nmanaged-readers   6d7h\n```\n\n----------------------------------------\n\nTITLE: Configuring NifiUserGroup resource in YAML for NiFiKop\nDESCRIPTION: Example YAML configuration for a NifiUserGroup resource that creates a group with two users and grants them read access to the counters resource. This demonstrates how to reference NifiUsers and define access policies for the group.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/3_manage_nifi/2_manage_users_and_accesses/2_groups_management.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  # Contains the reference to the NifiCluster with the one the registry client is linked.\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  # contains the list of reference to NifiUsers that are part to the group.\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n#     namespace: nifikop\n    - name: nc-controller.nifikop.mgt.cluster.local\n  # defines the list of access policies that will be granted to the group.\n  accessPolicies:\n      # defines the kind of access policy, could be \"global\" or \"component\".\n    - type: global\n      # defines the kind of action that will be granted, could be \"read\" or \"write\"\n      action: read\n      # resource defines the kind of resource targeted by this access policies, please refer to the following page:\n      #\thttps://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#access-policies\n      resource: /counters\n#      # componentType is used if the type is \"component\", it's allow to define the kind of component on which is the\n#      # access policy\n#      componentType: \"process-groups\"\n#      # componentId is used if the type is \"component\", it's allow to define the id of the component on which is the\n#      # access policy\n#      componentId: \"\"\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Cluster for Prometheus and Autoscaling - Kubernetes Manifest - yaml\nDESCRIPTION: This YAML configuration snippet demonstrates how to configure a NiFi cluster custom resource in Kubernetes to enable Prometheus monitoring and prepare for autoscaling. It defines an 'auto_scaling' NodeConfigGroup with resource requirements, service account settings, and several persistent volume claim (PVC) specifications for different types of storage. At least one NiFi listener is configured to expose internal Prometheus metrics on port 9090. Custom labels and annotations can be set for each volume. Requires the relevant NiFi Kubernetes operator, configured storage classes, and appropriate cluster permissions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n...\nspec:\n  ...\n  listenersConfig:\n    internalListeners:\n    ...\n    - containerPort: 9090\n      name: prometheus\n      type: prometheus \n    ...\n  ...\n  nodeConfigGroups:\n    auto_scaling:\n      isNode: true\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 2Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n      serviceAccountName: external-dns\n      storageConfigs:\n        - mountPath: /opt/nifi/nifi-current/logs\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/data\n          name: data\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/extensions\n          name: extensions-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/flowfile_repository\n          name: flowfile-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/nifi-current/conf\n          name: conf\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/content_repository\n          name: content-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/provenance_repository\n          name: provenance-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n  ...\n```\n\n----------------------------------------\n\nTITLE: Verifying the deployment of the new NiFi node pods and resources\nDESCRIPTION: This shell command lists the pods, configmaps, and persistent volume claims associated with the new node (id 25) in the 'nifi' namespace, verifying its successful deployment. It requires kubectl configured to communicate with the cluster. The output includes resource status and ensures the node is operational and properly configured.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\nkubectl get pods,configmap,pvc -l nodeId=25\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi at the System Level Using ReadOnlyConfig\nDESCRIPTION: Explains how to set NiFi-wide or node-specific configurations via the ReadOnlyConfig field, including parameters like maximum thread counts, using ConfigMaps, Secrets, or overrides to customize NiFi behavior.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_4\n\nLANGUAGE: YAML\nCODE:\n```\n/* No specific code snippet provided in the text; description of NiFi configuration options via ReadOnlyConfig. */\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi User Resource Using YAML\nDESCRIPTION: This YAML snippet demonstrates how to define a NiFiUser resource for managing user identities and their association with a NiFi cluster in Kubernetes via the nifikop operator. It specifies metadata such as the resource name, the user's identity within the NiFi cluster, the reference to the target NiFi cluster (name and namespace), and whether a certificate should be created for the user.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/5_references/2_nifi_user.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: aguitton\nspec:\n  identity: alexandre.guitton@konpyutaika.com\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  createCert: false\n```\n\n----------------------------------------\n\nTITLE: Implementing initContainer for Automatic Algorithm Update\nDESCRIPTION: Kubernetes YAML configuration for an initContainer that automatically updates the NiFi sensitive algorithm. The container runs the NiFi Toolkit to encrypt the configuration with the new algorithm before NiFi starts.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/7_upgrade_guides/4_v1.3.1_to_v1.4.0.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ninitContainers:\n  - image: \"apache/nifi-toolkit:latest\"\n    name: nifi-toolkit\n    imagePullPolicy: Always\n    command:\n      - \"sh\"\n      - \"-c\"\n      - \"NIFI_SENSITIVE_PROPS_KEY=$(grep 'nifi.sensitive.props.key' /opt/nifi/nifi-current/conf/nifi.properties | cut -d'=' -f2) && bin/encrypt-config.sh -n /opt/nifi/nifi-current/conf/nifi.properties -f /opt/nifi/data/flow.json.gz -x -A NIFI_PBKDF2_AES_GCM_256 -s $NIFI_SENSITIVE_PROPS_KEY; bin/encrypt-config.sh -n /opt/nifi/nifi-current/conf/nifi.properties -f /opt/nifi/data/flow.xml.gz -x -A NIFI_PBKDF2_AES_GCM_256 -s $NIFI_SENSITIVE_PROPS_KEY\"\n    volumeMounts:\n      - name: data\n        mountPath: /opt/nifi/data\n      - name: conf\n        mountPath: /opt/nifi/nifi-current/conf\n```\n\n----------------------------------------\n\nTITLE: Setting Operator Name Environment Variable for Local Run - Bash\nDESCRIPTION: Defines the 'OPERATOR_NAME' environment variable with the identifier 'nifi-operator' to specify the operator's name in a local run context. This variable helps the operator identify itself during execution outside the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/6_contributing/1_developer_guide.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport OPERATOR_NAME=nifi-operator\n```\n\n----------------------------------------\n\nTITLE: Specifying Multiple Content and Provenance Storage Directories in NiFiCluster YAML\nDESCRIPTION: This YAML snippet shows an advanced configuration to setup multiple directories for NiFi's content and provenance repositories for improved storage performance. It overrides default NiFi properties by specifying directory paths via overrideConfigs under nifiProperties within readOnlyConfig. The nodeConfigGroups section then defines corresponding persistent volume claims mounted at these paths. This configuration requires Kubernetes PVCs provisioned with proper access modes and storage classes. Inputs include directory paths and PVC specs; outputs are configured storage volumes accessible by NiFi nodes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n...\n  readOnlyConfig:\n    nifiProperties:\n      overrideConfigs: |\n        nifi.content.repository.directory.dir1=..\\/content-additional\\/dir1\n        nifi.content.repository.directory.dir2=..\\/content-additional\\/dir2\n        nifi.content.repository.directory.dir3=..\\/content-additional\\/dir3\n        nifi.provenance.repository.directory.dir1=..\\/provenance-additional\\/dir1\n        nifi.provenance.repository.directory.dir2=..\\/provenance-additional\\/dir2\n...\n  nodeConfigGroups:\n    default_group:\n      ...\n      storageConfigs:\n      - mountPath: \"/opt/nifi/content-additional/dir1\"\n        name: content-repository-dir1\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/content-additional/dir2\"\n        name: content-repository-dir2\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/content-additional/dir3\"\n        name: content-repository-dir3\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/provenance-additional/dir1\"\n        name: provenance-repository-dir1\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/provenance-additional/dir2\"\n        name: provenance-repository-dir2\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      ...\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Node Settings in YAML\nDESCRIPTION: This YAML snippet demonstrates defining configuration settings for a NiFi node within a Kubernetes cluster, likely managed by the nifikop operator. It showcases parameters like `provenanceStorage` size, `runAsUser` ID, enabling cluster node mode (`isNode`), adding custom `podMetadata` (annotations and labels), specifying `imagePullPolicy`, assigning a `priorityClassName`, mounting external volumes (`externalVolumeConfigs`) from secrets, and configuring persistent storage (`storageConfigs`) with specific mount paths, reclaim policies, and PersistentVolumeClaim specifications (`pvcSpec`) including access modes, storage class, and resource requests.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/5_references/1_nifi_cluster/3_node_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n   default_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      # Set this to true if the instance is a node in a cluster.\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#basic-cluster-setup\n      isNode: true\n      # Additionnal metadata to merge to the pod associated\n      podMetadata:\n        annotations:\n          node-annotation: \"node-annotation-value\"\n        labels:\n          node-label: \"node-label-value\"\n      # Docker image used by the operator to create the node associated\n      # https://hub.docker.com/r/apache/nifi/\n#      image: \"apache/nifi:1.11.2\"\n      # nodeAffinity can be specified, operator populates this value if new pvc added later to node\n      # https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity\n#      nodeAffinity:\n      # imagePullPolicy define the pull policy for NiFi cluster docker image\n      imagePullPolicy: IfNotPresent\n      # priorityClassName define the name of the priority class to be applied to these nodes\n      priorityClassName: \"example-priority-class-name\"\n      # externalVolumeConfigs specifies a list of volume to mount into the main container.\n      externalVolumeConfigs:\n        - name: example-volume\n          mountPath: \"/opt/nifi/example\"\n          secret:\n            secretName: \"raw-controller\"\n      # storageConfigs specifies the node related configs\n      storageConfigs:\n        # Name of the storage config, used to name PV to reuse into sidecars for example.\n        - name: provenance-repository\n          # Retain this PVC throughout NifiCluster deletions.\n          reclaimPolicy: Retain\n          # Path where the volume will be mount into the main nifi container inside the pod.\n          mountPath: \"/opt/nifi/provenance_repository\"\n          # Metadata to attach to the PVC that gets created\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          # Kubernetes PVC spec\n          # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          reclaimPolicy: Delete\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n```\n\n----------------------------------------\n\nTITLE: Declaring NiFi Cluster Nodes in YAML\nDESCRIPTION: This YAML snippet shows how to define the nodes in a NiFi cluster and assign them to a configuration group, or define the configuration directly at the node level. It demonstrates how to assign different node configurations to different nodes using `nodeConfigGroup`. Each node is identified by an `id` and references a `nodeConfigGroup` or defines its own `nodeConfig` properties.  This allows for different resource allocations and settings for each node within the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n- id: 0\n  nodeConfigGroup: \"default_group\"\n- id: 2\n  nodeConfigGroup: \"high_mem_group\"\n- id: 3\n  nodeConfigGroup: \"high_mem_group\"\n- id: 5\n  nodeConfig:\n    resourcesRequirements:\n      limits:\n        cpu: \"2\"\n        memory: 3Gi\n      requests:\n        cpu: \"1\"\n        memory: 1Gi\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Listeners with ListenersConfig YAML - YAML\nDESCRIPTION: This YAML snippet demonstrates how to define multiple internal listeners (such as HTTPS, Cluster, S2S, Prometheus, and custom ports) as part of the ListenersConfig for Apache NiFi on Kubernetes. It also shows how to specify associated SSL secrets for secure communication. Dependencies include a NiFi Kubernetes operator and the existence or automatic creation of relevant SSL secrets, as well as correct integration into a NiFi custom resource. Key parameters include 'type', 'name', 'containerPort', and 'protocol' for each listener, with 'sslSecrets' specifying certificate management details. The configuration expects input as a YAML document and is used to provision NiFi clusters with proper networking setup. Limitation: Ensure that listener ports do not conflict and that SSL secret names reference valid Kubernetes secrets.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/5_references/1_nifi_cluster/6_listeners_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  listenersConfig:\n    internalListeners:\n      - type: \"https\"\n        name: \"https\"\n        containerPort: 8443\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n      - type: \"prometheus\"\n        name: \"prometheus\"\n        containerPort: 9090\n      - type: \"load-balance\"\n        name: \"load-balance\"\n        containerPort: 6342\n      - name: \"my-custom-listener-port\"\n        containerPort: 1234\n        protocol: \"TCP\"\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n```\n\n----------------------------------------\n\nTITLE: Cloning NiFiKop Project Repository Using Git Bash\nDESCRIPTION: This snippet demonstrates the initial project checkout by cloning the NiFiKop GitHub repository and changing into the project directory. It requires Git to be installed and configured on the system. These commands set up the local workspace for further development and build tasks.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/6_contributing/1_developer_guide.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/konpyutaika/nifikop.git\ncd nifikop\n```\n\n----------------------------------------\n\nTITLE: Defining ReadOnlyConfig for NifiCluster in Kubernetes with YAML\nDESCRIPTION: This YAML snippet configures read-only cluster-wide settings for an Apache NiFi cluster managed with Nifikop on Kubernetes. It defines thread limits, logback, authorizer, nifi.properties, zookeeper.properties, and bootstrap.properties, each supporting overrides using config maps and secrets pulled from specific namespaces. Key parameters allow fine-tuning of processor threads, JVM memory, client authentication, and advanced configuration via multiline values under overrideConfigs. To use, ensure referenced configmaps/secrets exist in the indicated namespaces; required fields and their defaults are described in the surrounding documentation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/5_references/1_nifi_cluster/2_read_only_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreadOnlyConfig:\n  maximumTimerDrivenThreadCount: 30\n  maximumEventDrivenThreadCount: 10\n  logbackConfig:\n    replaceConfigMap:\n      data: logback.xml\n      name: raw\n      namespace: nifikop\n    replaceSecretConfig:\n      data: logback.xml\n      name: raw\n      namespace: nifikop\n  authorizerConfig:\n    replaceTemplateConfigMap:\n      data: authorizers.xml\n      name: raw\n      namespace: nifikop\n    replaceTemplateSecretConfig:\n      data: authorizers.xml\n      name: raw\n      namespace: nifikop\n  nifiProperties:\n    overrideConfigMap:\n      data: nifi.properties\n      name: raw\n      namespace: nifikop.\n    overrideSecretConfig:\n      data: nifi.properties\n      name: raw\n      namespace: nifikop\n    overrideConfigs: |\n      nifi.ui.banner.text=NiFiKop\n    needClientAuth: false\n  zookeeperProperties:\n    overrideConfigs: |\n      initLimit=15\n      autopurge.purgeInterval=24\n      syncLimit=5\n      tickTime=2000\n      dataDir=./state/zookeeper\n      autopurge.snapRetainCount=30\n  bootstrapProperties:\n    nifiJvmMemory: \"512m\"\n    overrideConfigs: |\n      # java.arg.4=-Djava.net.preferIPv4Stack=true\n```\n\n----------------------------------------\n\nTITLE: Install cert-manager with Helm 3\nDESCRIPTION: This snippet installs cert-manager using Helm 3. It first applies the CustomResourceDefinitions (CRDs), then adds the jetstack helm repository, updates it, and finally installs the cert-manager chart in the cert-manager namespace. The namespace must be created prior to running the helm install command.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/2_deploy_nifikop/1_quick_start.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Install CustomResourceDefinitions first\nkubectl apply --validate=false -f \\\n   https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.crds.yaml\n\n# Add the jetstack helm repo\nhelm repo add jetstack https://charts.jetstack.io\nhelm repo update\n\n# You have to create the namespace before executing following command\nhelm install cert-manager \\\n    --namespace cert-manager \\\n    --version v1.7.2 jetstack/cert-manager\n```\n\n----------------------------------------\n\nTITLE: Applying NiFiKop CRDs (Bash)\nDESCRIPTION: Uses `kubectl apply` to install the necessary NiFiKop Custom Resource Definitions (CRDs) into the connected Kubernetes cluster. These definitions (`NifiCluster`, `NifiDataflow`, etc.) are required for the operator to manage NiFi resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/6_contributing/1_developer_guide.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\n```\n\n----------------------------------------\n\nTITLE: BootstrapNotificationServicesConfig Configuration Structure in Markdown\nDESCRIPTION: This table details configuration options for bootstrap notification services in NiFiKop, allowing replacement of the default template via ConfigMap or Secret references.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/5_references/1_nifi_cluster/2_read_only_config.md#_snippet_5\n\nLANGUAGE: markdown\nCODE:\n```\n## BootstrapNotificationServicesConfig\n\n|Field|Type|Description|Required|Default|\n|-----|----|-----------|--------|--------|\n|replaceConfigMap|[ConfigmapReference](#configmapreference)|bootstrap_notifications_services.xml configuration that will replace the one produced based on template.|No|nil|\n|replaceSecretConfig|[SecretConfigReference](#secretconfigreference)|bootstrap_notifications_services.xml configuration that will replace the one produced based on template and overrideConfigMap.|No|nil|\n```\n\n----------------------------------------\n\nTITLE: Port Forwarding Prometheus Service - Console\nDESCRIPTION: Sets up a local port forward from 'service/prometheus-operated' in the 'monitoring-system' namespace to local port 9090, allowing access to the Prometheus dashboard via http://localhost:9090. Keep this process running to maintain connectivity. Useful for testing and development environments.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_12\n\nLANGUAGE: console\nCODE:\n```\nkubectl port-forward -n monitoring-system service/prometheus-operated 9090:9090\n```\n\n----------------------------------------\n\nTITLE: Defining Node Configuration Groups in NiFiKop - YAML\nDESCRIPTION: This YAML snippet demonstrates how to define multiple node configuration groups for a NiFiKop cluster using the \"nodeConfigGroups\" field. Each group (e.g., default_group, high_mem_group) specifies storage, computation limits, service accounts, and runtime user IDs. The resourceRequirements section follows Kubernetes conventions for defining pod resource requests and limits. This configuration is referenced later when declaring which nodes should use which group, effectively allowing templated node profiles for consistent cluster setups. Prerequisites: NiFiKop CRD properly installed in Kubernetes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n  # nodeConfigGroups specifies multiple node configs with unique name\n  nodeConfigGroups:\n    default_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      serviceAccountName: \"default\"\n      # resourceRequirements works exactly like Container resources, the user can specify the limit and the requests\n      # through this property\n      # https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 3Gi\n    high_mem_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      serviceAccountName: \"default\"\n      # resourceRequirements works exactly like Container resources, the user can specify the limit and the requests\n      # through this property\n      # https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 30Gi\n        requests:\n          cpu: \"1\"\n          memory: 30Gi\n```\n\n----------------------------------------\n\nTITLE: Defining a NiFi Registry Client in YAML\nDESCRIPTION: YAML definition for creating a NifiRegistryClient resource that connects an Apache NiFi cluster to a NiFi Registry instance. This resource requires a reference to an existing NiFi cluster, a description, and the URI of the target registry.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/5_references/3_nifi_registry_client.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiRegistryClient\nmetadata:\n  name: squidflow\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Squidflow demo\"\n  uri: \"http://nifi-registry:18080\"\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi ClusterIP Service in Kubernetes\nDESCRIPTION: This YAML excerpt defines the ClusterIP Service configuration in the NiFi cluster deployment, specifying port 8443 for the HTTPS listener.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nspec:  \n  externalServices:  \n    - name: \"nifi-cluster\"\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8443\n            internalListenerName: \"https\"\n```\n\n----------------------------------------\n\nTITLE: Defining Kubernetes Node Configuration for NiFi in YAML\nDESCRIPTION: This YAML snippet specifies the configuration for a NiFi node within a Kubernetes cluster, including storage, user permissions, labels, affinity, image policies, and volume mounts. It facilitates the deployment of a NiFi node with customizable parameters for cluster setup and storage management.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/5_references/1_nifi_cluster/3_node_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n   default_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      # Set this to true if the instance is a node in a cluster.\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#basic-cluster-setup\n      isNode: true\n      # Additionnal metadata to merge to the pod associated\n      podMetadata:\n        annotations:\n          node-annotation: \"node-annotation-value\"\n        labels:\n          node-label: \"node-label-value\"\n      # Docker image used by the operator to create the node associated\n      # https://hub.docker.com/r/apache/nifi/\n#      image: \"apache/nifi:1.11.2\"\n      # nodeAffinity can be specified, operator populates this value if new pvc added later to node\n      # https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity\n#      nodeAffinity:\n      # imagePullPolicy define the pull policy for NiFi cluster docker image\n      imagePullPolicy: IfNotPresent\n      # priorityClassName define the name of the priority class to be applied to these nodes\n      priorityClassName: \"example-priority-class-name\"\n      # externalVolumeConfigs specifies a list of volume to mount into the main container.\n      externalVolumeConfigs:\n        - name: example-volume\n          mountPath: \"/opt/nifi/example\"\n          secret:\n            secretName: \"raw-controller\"\n      # storageConfigs specifies the node related configs\n      storageConfigs:\n        # Name of the storage config, used to name PV to reuse into sidecars for example.\n        - name: provenance-repository\n          # Path where the volume will be mount into the main nifi container inside the pod.\n          mountPath: \"/opt/nifi/provenance_repository\"\n          # Kubernetes PVC spec\n          # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n```\n\n----------------------------------------\n\nTITLE: Verifying and Monitoring HorizontalPodAutoscaler\nDESCRIPTION: Retrieves the current HPA status for the NiFi autoscaler, showing target metrics, min/max pods, and current replicas. Ensures autoscaling is functioning as configured. Requires kubectl access to the specific namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get -n clusters hpa\n```\n\n----------------------------------------\n\nTITLE: Deploying Another NifiDataflow (YAML)\nDESCRIPTION: This YAML defines a second NifiDataflow resource named `output`. It mirrors the configuration of the `input` dataflow, including cluster reference, registry client, and other settings. The main difference is the `flowId` and `flowVersion`. This setup facilitates the creation of connections between different dataflows. The flow position is configured to visually separate the two dataflows.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/4_manage_connections/1_deploy_connection.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\n--- \napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiDataflow\nmetadata:\n  name: output\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  bucketId: deedb9f6-65a4-44e9-a1c9-722008fcd0ba\n  flowId: fc5363eb-801e-432f-aa94-488838674d07\n  flowVersion: 2\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  skipInvalidComponent: true\n  skipInvalidControllerService: true\n  syncMode: always\n  updateStrategy: drain\n  flowPosition:\n    posX: 750\n    posY: 0\n```\n\n----------------------------------------\n\nTITLE: Defining Managed Admin and Reader Users in NiFi Operator Using YAML\nDESCRIPTION: This YAML snippet shows how to specify lists of managed admin and reader users within the NifiCluster custom resource specification. The 'managedAdminUsers' and 'managedReaderUsers' fields contain user identities and names that the NiFi Operator uses to create corresponding NiFiUsers and group memberships automatically. This configuration enables streamlined role-based access control setup without having to individually manage multiple YAML files for each user or cluster. The snippet requires the NiFi operator CRDs to be installed and properly running to act upon these definitions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/2_manage_users_and_accesses/3_managed_groups.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  ...\n  oneNifiNodePerNode: false\n  propagateLabels: true\n  managedAdminUsers:\n    -  identity: \"alexandre.guitton@konpyutaika.com\"\n       name: \"aguitton\"\n    -  identity: \"nifiuser@konpyutaika.com\"\n       name: \"nifiuser\"\n  managedReaderUsers:\n    -  identity: \"toto@konpyutaika.com\"\n       name: \"toto\"\n    ...\n```\n\n----------------------------------------\n\nTITLE: Cloning NiFiKop Project Repository - Bash\nDESCRIPTION: This snippet clones the NiFiKop GitHub repository and changes the directory into the cloned repository, preparing the environment for subsequent building or development commands. It requires git to be installed and a network connection to GitHub.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/6_contributing/1_developer_guide.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/konpyutaika/nifikop.git\ncd nifikop\n```\n\n----------------------------------------\n\nTITLE: Creating Secret via kubectl\nDESCRIPTION: This command creates a Kubernetes secret containing key-value pairs that are referenced by the NifiParameterContext. The secret values are protected and used within the NiFi dataflow configuration. The values can be referenced by using the `secretRefs` field in `NifiParameterContext`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl create secret generic secret-params \\\n    --from-literal=secret1=yop \\\n    --from-literal=secret2=yep \\\n    -n nifikop\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFi Cluster on OpenShift\nDESCRIPTION: This bash script deploys the NiFi cluster using kubectl with the updated OpenShift-specific configuration file. It is required to have first updated the configuration file. This requires `kubectl` to be correctly configured to talk to the Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create -n nifi -f config/samples/openshift.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring External Services for NiFi Cluster Access\nDESCRIPTION: Complete example of configuring both internal listeners and external services to expose a NiFi cluster externally. The example demonstrates how to expose HTTPS and a custom HTTP tracking endpoint through a LoadBalancer service.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    - type: \"https\"\n      name: \"https\"\n      containerPort: 8443\n    - type: \"cluster\"\n      name: \"cluster\"\n      containerPort: 6007\n    - type: \"s2s\"\n      name: \"s2s\"\n      containerPort: 10000\n    - type: \"prometheus\"\n      name: \"prometheus\"\n      containerPort: 9090\n    - type: \"load-balance\"\n      name: \"load-balance\"\n      containerPort: 6342\n    - name: \"http-tracking\"\n      containerPort: 8081\nexternalServices:\n  - name: cluster-access\n    spec:\n      portConfigs:\n        - internalListenerName: https\n          port: 443\n        - internalListenerName: http-tracking\n          port: 80\n      type: LoadBalancer\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi UserGroup Custom Resource with Access Policies in Kubernetes YAML\nDESCRIPTION: This YAML snippet demonstrates the definition of a NifiUserGroup custom resource for the nifikop API in a Kubernetes cluster. The resource specifies metadata such as name, a reference to a NiFi cluster, a list of user references that belong to the group, and access policies dictating group permissions. Required parameters include 'apiVersion', 'kind', 'metadata', and 'spec' with nested fields for clusterRef and usersRef. Dependencies: nifikop CRDs must be installed in the Kubernetes cluster. Input is a YAML manifest to be applied via kubectl; output is a UserGroup object managed by the NiFi operator. All referenced user and cluster objects must exist in the specified namespaces.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/5_references/6_nifi_usergroup.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n    - name: nc-controller.nifikop.mgt.cluster.local\n  accessPolicies:\n    - type: global\n      action: read\n      resource: /counters\n```\n\n----------------------------------------\n\nTITLE: Deploying Prerequisite NifiDataflows via NiFiKop YAML\nDESCRIPTION: This YAML defines two NifiDataflow resources, named 'input' and 'output', which serve as the source and destination respectively for a future connection. It specifies their association with a NiFi cluster ('nc'), the dataflow's details like bucket ID, flow ID, version, and registry client reference, and placement within the NiFi canvas.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/4_manage_connections/1_deploy_connection.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiDataflow\nmetadata:\n  name: input\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  bucketId: deedb9f6-65a4-44e9-a1c9-722008fcd0ba\n  flowId: ab95431d-980d-41bd-904a-fac4bd864ba6\n  flowVersion: 1\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  skipInvalidComponent: true\n  skipInvalidControllerService: true\n  syncMode: always\n  updateStrategy: drain\n  flowPosition:\n    posX: 0\n    posY: 0\n---\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiDataflow\nmetadata:\n  name: output\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  bucketId: deedb9f6-65a4-44e9-a1c9-722008fcd0ba\n  flowId: fc5363eb-801e-432f-aa94-488838674d07\n  flowVersion: 2\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  skipInvalidComponent: true\n  skipInvalidControllerService: true\n  syncMode: always\n  updateStrategy: drain\n  flowPosition:\n    posX: 750\n    posY: 0\n```\n\n----------------------------------------\n\nTITLE: Building NiFiKop Docker Image From Local Branch - Bash\nDESCRIPTION: Builds the NiFiKop operator's Docker image using the make utility with a user-defined Docker repository base environment variable. This creates a container image based on the locally checked-out code branch, preparing it for deployment to a container registry.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/6_contributing/1_developer_guide.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nexport DOCKER_REPO_BASE={your-docker-repo}\nmake docker-build\n```\n\n----------------------------------------\n\nTITLE: Applying Kubernetes CRD Definitions\nDESCRIPTION: Commands to deploy Custom Resource Definitions (CRDs) for NiFiKop into the Kubernetes cluster, enabling the operator to manage specific NiFi resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/6_contributing/1_developer_guide.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Local NiFiKop Operator Development - Bash\nDESCRIPTION: This snippet sets environment variables necessary for running the NiFiKop operator locally with an IDE or manually, including Kubernetes config path, namespace to watch, pod name, log verbosity, and operator name. These variables configure connectivity and behavior of the operator during development.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/6_contributing/1_developer_guide.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nKUBECONFIG={path/to/your/kubeconfig}\nWATCH_NAMESPACE={namespace_to_watch}\nPOD_NAME={name for operator pod}\nLOG_LEVEL=Debug\nOPERATOR_NAME=ide\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom StorageClass for Persistent Volume Claims (Kubernetes YAML)\nDESCRIPTION: Defines a StorageClass resource with volume binding mode set to 'WaitForFirstConsumer' to optimize volume provisioning based on pod scheduling. It is recommended to set this StorageClass in the NiFiCluster CR for proper volume management.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: exampleStorageclass\nparameters:\n  type: pd-standard\nprovisioner: kubernetes.io/gce-pd\nreclaimPolicy: Delete\nvolumeBindingMode: WaitForFirstConsumer\n```\n\n----------------------------------------\n\nTITLE: Creating Templated Authorizers.xml Configuration for Database Authorizer in NiFiKOp\nDESCRIPTION: This YAML template demonstrates how to configure a custom database authorizer in NiFi by replacing the standard authorizers.xml file. It defines both the default file-based providers and custom database providers for user groups and access policies, with templated node identities that will be populated at deployment time.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/6_users_authorization/1_custom_user_authorizer.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n{{- $nodeList := .NodeList }}\n{{- $clusterName := .ClusterName }}\n{{- $namespace := .Namespace }}<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>\n<authorizers>\n    <userGroupProvider>\n        <identifier>file-user-group-provider</identifier>\n        <class>org.apache.nifi.authorization.FileUserGroupProvider</class>\n        <property name=\"Users File\">../data/users.xml</property>\n        <property name=\"Legacy Authorized Users File\"></property>\n        <property name=\"Initial User Identity admin\">{{ .ControllerUser }}</property>\n{{- range $i, $host := .NodeList }}\n        <property name=\"Initial User Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n    </userGroupProvider>\n    <userGroupProvider>\n        <identifier>database-user-group-provider</identifier>\n        <class>my.custom.DatabaseUserGroupProvider</class>\n        <!-- Any extra configuration for this provider goes here -->\n{{- range $i, $host := .NodeList }}\n        <property name=\"Initial User Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n    </userGroupProvider>\n    <accessPolicyProvider>\n        <identifier>file-access-policy-provider</identifier>\n        <class>org.apache.nifi.authorization.FileAccessPolicyProvider</class>\n        <property name=\"User Group Provider\">file-user-group-provider</property>\n        <property name=\"Authorizations File\">../data/authorizations.xml</property>\n        <property name=\"Initial Admin Identity\">{{ .ControllerUser }}</property>\n        <property name=\"Legacy Authorized Users File\"></property>\n{{- range $i, $host := .NodeList }}\n        <property name=\"Node Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n\t\t<property name=\"Node Group\"></property>\n    </accessPolicyProvider>\n    <accessPolicyProvider>\n        <identifier>database-access-policy-provider</identifier>\n        <class>my.custom.DatabaseAccessPolicyProvider</class>\n        <!-- Any extra configuration for this provider goes here -->\n{{- range $i, $host := .NodeList }}\n        <property name=\"Node Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n\t\t<property name=\"Node Group\"></property>\n    </accessPolicyProvider>\n    <authorizer>\n        <identifier>managed-authorizer</identifier>\n        <class>org.apache.nifi.authorization.StandardManagedAuthorizer</class>\n        <property name=\"Access Policy Provider\">file-access-policy-provider</property>\n    </authorizer>\n    <authorizer>\n        <identifier>custom-database-authorizer</identifier>\n        <class>my.custom.DatabaseAuthorizer</class>\n        <property name=\"Access Policy Provider\">database-access-policy-provider</property>\n    </authorizer>\n</authorizers>\n```\n\n----------------------------------------\n\nTITLE: Defining an External NifiCluster Resource\nDESCRIPTION: This YAML snippet demonstrates how to define a NifiCluster resource representing an external NiFi cluster. Key fields include `rootProcessGroupId`, `nodeURITemplate`, `nodes`, `type` (set to 'external'), `clientType` (specifying the authentication method), and `secretRef` (referencing a secret containing authentication credentials). The `nodeURITemplate` and `nodes` fields are used to dynamically compute node URIs.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/3_external_cluster.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: externalcluster\nspec:\n  # rootProcessGroupId contains the uuid of the root process group for this cluster.\n  rootProcessGroupId: 'd37bee03-017a-1000-cff7-4eaaa82266b7'\n  # nodeURITemplate used to dynamically compute node uri.\n  nodeURITemplate: 'nifi0%d.integ.mapreduce.m0.p.fti.net:9090'\n  # all node requiresunique id\n  nodes:\n    - id: 1\n    - id: 2\n    - id: 3\n  # type defines if the cluster is internal (i.e manager by the operator) or external.\n  # :Enum={\"external\",\"internal\"}\n  type: 'external'\n  # clientType defines if the operator will use basic or tls authentication to query the NiFi cluster.\n  # Enum={\"tls\",\"basic\"}\n  clientType: 'basic'\n  # secretRef reference the secret containing the informations required to authenticate to the cluster.\n  secretRef:\n    name: nifikop-credentials\n    namespace: nifikop-nifi\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Cluster for Prometheus\nDESCRIPTION: This YAML configuration defines a NiFi cluster with a Prometheus listener and node configuration groups for autoscaling. The `listenersConfig` section exposes a Prometheus endpoint at port 9090. The `nodeConfigGroups` section specifies a node config group named `auto_scaling`, defining resources limits and storage configurations for nodes.  This sets up NiFi to expose metrics that Prometheus can scrape for autoscaling, and defines configurations for nodes in auto-scaling groups.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n...\nspec:\n  ...\n  listenersConfig:\n    internalListeners:\n    ...\n    - containerPort: 9090\n      name: prometheus\n      type: prometheus \n    ...\n  ...\n  nodeConfigGroups:\n    auto_scaling:\n      isNode: true\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 2Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n      serviceAccountName: external-dns\n      storageConfigs:\n        - mountPath: /opt/nifi/nifi-current/logs\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/data\n          name: data\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/extensions\n          name: extensions-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/flowfile_repository\n          name: flowfile-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/nifi-current/conf\n          name: conf\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/content_repository\n          name: content-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/provenance_repository\n          name: provenance-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n  ...\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi User Group in YAML\nDESCRIPTION: This YAML snippet defines a `NifiUserGroup` resource. It specifies the cluster reference, users in the group, and access policies. The `clusterRef` links to the NiFi cluster, `usersRef` lists users, and `accessPolicies` defines access permissions. The example grants read access to counters for the specified users.  This resource requires the `nifi.konpyutaika.com/v1` API version and the `NifiUserGroup` kind.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/2_manage_users_and_accesses/2_groups_management.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  # Contains the reference to the NifiCluster with the one the registry client is linked.\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  # contains the list of reference to NifiUsers that are part to the group.\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n#     namespace: nifikop\n    - name: nc-controller.nifikop.mgt.cluster.local\n  # defines the list of access policies that will be granted to the group.\n  accessPolicies:\n      # defines the kind of access policy, could be \"global\" or \"component\".\n    - type: global\n      # defines the kind of action that will be granted, could be \"read\" or \"write\"\n      action: read\n      # resource defines the kind of resource targeted by this access policies, please refer to the following page:\n      #\thttps://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#access-policies\n      resource: /counters\n#      # componentType is used if the type is \"component\", it's allow to define the kind of component on which is the\n#      # access policy\n#      componentType: \"process-groups\"\n#      # componentId is used if the type is \"component\", it's allow to define the id of the component on which is the\n#      # access policy\n#      componentId: \"\"\n```\n```\n\n----------------------------------------\n\nTITLE: Deploying Prometheus Operator with Helm\nDESCRIPTION: Installs the Prometheus Operator using Helm with custom configuration to disable unnecessary components and set appropriate namespaces for monitoring.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_8\n\nLANGUAGE: console\nCODE:\n```\nhelm install prometheus prometheus/kube-prometheus-stack --namespace monitoring-system \\\n    --set prometheusOperator.createCustomResource=false \\\n    --set prometheusOperator.logLevel=debug \\\n    --set prometheusOperator.alertmanagerInstanceNamespaces=monitoring-system \\\n    --set prometheusOperator.namespaces.additional[0]=monitoring-system \\\n    --set prometheusOperator.prometheusInstanceNamespaces=monitoring-system \\\n    --set prometheusOperator.thanosRulerInstanceNamespaces=monitoring-system \\\n    --set defaultRules.enable=false \\\n    --set alertmanager.enabled=false \\\n    --set grafana.enabled=false \\\n    --set kubeApiServer.enabled=false \\\n    --set kubelet.enabled=false \\\n    --set kubeControllerManager.enabled=false \\\n    --set coreDNS.enabled=false \\\n    --set kubeEtcd.enabled=false \\\n    --set kubeScheduler.enabled=false \\\n    --set kubeProxy.enabled=false \\\n    --set kubeStateMetrics.enabled=false \\\n    --set prometheus.enabled=false\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Cluster with SSL in Nifikop - YAML\nDESCRIPTION: Defines a Kubernetes custom resource for a NiFiCluster with SSL endpoints using the Nifikop operator. Main dependencies are Nifikop operator and a Kubernetes cluster. It sets up HTTPS, S2S, and cluster listener ports, specifies how SSL secrets should be created or referenced, and allows proxy host configuration. The nifiProperties.webProxyHosts field lists allowed HTTP host headers for secure web access. Inputs are configuration YAML fields; outputs are a secured NiFi cluster managed by the operator.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  readOnlyConfig:\n    # NifiProperties configuration that will be applied to the node.\n    nifiProperties:\n      webProxyHosts:\n        - nifistandard2.trycatchlearn.fr:8443\n        ...\n  ...\n  listenersConfig:\n    internalListeners:\n      - type: \"https\"\n        name: \"https\"\n        containerPort: 8443\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes ServiceAccount and RBAC Roles for Prometheus Using YAML\nDESCRIPTION: Defines a ServiceAccount 'prometheus' in the 'monitoring-system' namespace along with a ClusterRole granting it permissions to 'get', 'list', and 'watch' various core Kubernetes resources like nodes, pods, services, and ingresses, as well as access to the /metrics endpoint. It also establishes a ClusterRoleBinding that binds this ClusterRole to the ServiceAccount, ensuring Prometheus has the necessary privileges to scrape metrics cluster-wide. Prerequisite knowledge of Kubernetes RBAC and access to apply YAML manifests is required.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: prometheus\n  namespace: monitoring-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: prometheus\nrules:\n- apiGroups: [\"\"]\n  resources:\n  - nodes\n  - nodes/metrics\n  - services\n  - endpoints\n  - pods\n  verbs: [\"get\", \"list\", \"watch\"]\n- apiGroups: [\"\"]\n  resources:\n  - configmaps\n  verbs: [\"get\"]\n- apiGroups:\n  - networking.k8s.io\n  resources:\n  - ingresses\n  verbs: [\"get\", \"list\", \"watch\"]\n- nonResourceURLs: [\"/metrics\"]\n  verbs: [\"get\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: prometheus\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: prometheus\nsubjects:\n- kind: ServiceAccount\n  name: prometheus\n  namespace: monitoring-system\n```\n\n----------------------------------------\n\nTITLE: Building NiFiKop Locally - Bash\nDESCRIPTION: This command builds the NiFiKop operator binary in a local development environment using the provided Makefile. It requires make and a properly set up Go development environment according to project prerequisites. No parameters are required, and the output is the compiled operator binary.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/6_contributing/1_developer_guide.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmake build\n```\n\n----------------------------------------\n\nTITLE: Getting Status of Nifikop Helm Deployment - Bash\nDESCRIPTION: This command retrieves detailed status information for the Nifikop Helm release, including resource states, deployment progress, and any errors. A valid release name and cluster access are required. Use this for diagnosing deployment status or troubleshooting issues.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nhelm status nifikop\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiUser Resource in YAML\nDESCRIPTION: This YAML snippet defines a `NifiUser` resource in the `nifi.konpyutaika.com/v1alpha1` API version.  It specifies the user's identity, a reference to a NiFi cluster, and whether to create a certificate for the user. The `clusterRef` points to a NifiCluster named `nc` in the `nifikop` namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/5_references/2_nifi_user.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiUser\nmetadata:\n  name: aguitton\nspec:\n  identity: alexandre.guitton@konpyutaika.com\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  createCert: false\n```\n\n----------------------------------------\n\nTITLE: Installing Zookeeper using Helm\nDESCRIPTION: This bash script demonstrates the installation of Zookeeper using the Helm package manager.  It specifies the Zookeeper chart from Bitnami's repository and sets several parameters, including resource requests/limits, storage class, network policy, replica count, and the creation of a namespace. Replace the `storageClass` parameter with your own storage class name.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhelm install zookeeper oci://registry-1.docker.io/bitnamicharts/zookeeper \\\n    --namespace=zookeeper \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set global.storageClass=standard \\\n    --set networkPolicy.enabled=true \\\n    --set replicaCount=3 \\\n    --create-namespace\n```\n\n----------------------------------------\n\nTITLE: Updating Helm Repository with Console\nDESCRIPTION: This snippet updates the local Helm chart repository cache to ensure the latest charts (including KEDA) are available. This is a standard Helm maintenance step. Requires an existing Helm installation. No specific parameters or dependencies except access to the internet.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nhelm repo update\n```\n\n----------------------------------------\n\nTITLE: Listing All Helm Releases in Kubernetes\nDESCRIPTION: Helm command to view all releases including deleted, deployed, and failed ones.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --all\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiUserGroup Resource in YAML\nDESCRIPTION: This YAML manifest defines a `NifiUserGroup` custom resource named 'group-test'. It links to a `NifiCluster` named 'nc' in the 'nifikop' namespace, includes references to two `NifiUser` resources, and grants the group read access to the '/counters' global resource in NiFi. This resource facilitates managing NiFi user groups and their permissions declaratively through Kubernetes. Pre-existing `NifiUser` resources referenced in `usersRef` are required.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/2_manage_users_and_accesses/2_groups_management.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  # Contains the reference to the NifiCluster with the one the registry client is linked.\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  # contains the list of reference to NifiUsers that are part to the group.\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n#     namespace: nifikop\n    - name: nc-controller.nifikop.mgt.cluster.local\n  # defines the list of access policies that will be granted to the group.\n  accessPolicies:\n      # defines the kind of access policy, could be \"global\" or \"component\".\n    - type: global\n      # defines the kind of action that will be granted, could be \"read\" or \"write\"\n      action: read\n      # resource defines the kind of resource targeted by this access policies, please refer to the following page:\n      #\thttps://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#access-policies\n      resource: /counters\n#      # componentType is used if the type is \"component\", it's allow to define the kind of component on which is the\n#      # access policy\n#      componentType: \"process-groups\"\n#      # componentId is used if the type is \"component\", it's allow to define the id of the component on which is the\n#      # access policy\n#      componentId: \"\"\n```\n\n----------------------------------------\n\nTITLE: Uninstalling NiFiKop\nDESCRIPTION: Command to remove the NiFiKop operator and associated Kubernetes components. This command removes the operator but preserves the CRDs.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nhelm del nifikop\n```\n\n----------------------------------------\n\nTITLE: Routing Decrypted HTTPS Traffic with Istio VirtualService for NiFi (YAML)\nDESCRIPTION: Configures an Istio VirtualService named `nifi-vs` linked to the `nifi-gateway` configured for HTTPS. It routes all incoming HTTP requests (after TLS termination at the gateway) for `nifi.my-domain.com` matching the root URI prefix (`/`) to the specified NiFi ClusterIP service (e.g., `<service-name>.<namespace>.svc.cluster.local`) on port 8443.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: nifi-vs\nspec:\n  gateways:\n  - nifi-gateway\n  hosts:\n  - nifi.my-domain.com\n  http:\n  - match:\n    - uri:\n        prefix: /\n    route:\n    - destination:\n        host: <service-name>.<namespace>.svc.cluster.local\n        port:\n          number: 8443\n```\n\n----------------------------------------\n\nTITLE: Updating Helm Repositories\nDESCRIPTION: Updates the local cache of available charts from all configured Helm repositories using `helm repo update`. This ensures that Helm has the latest information about available chart versions before installation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nhelm repo update\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop with Custom Namespace Parameter in Kubernetes\nDESCRIPTION: Helm command to install NiFiKop with a specific namespace configuration parameter.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop --set namespaces={\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Cluster for Autoscaling/Prometheus (YAML)\nDESCRIPTION: Partial YAML configuration for a NiFiCluster custom resource. It defines a specific `NodeConfigGroup` named 'auto_scaling' intended for nodes managed by KEDA, specifying resource requirements and storage configurations (PVCs). It also configures an internal listener of type 'prometheus' on container port 9090 to expose NiFi metrics.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n...\nspec:\n  ...\n  listenersConfig:\n    internalListeners:\n    ...\n    - containerPort: 9090\n      name: prometheus\n      type: prometheus \n    ...\n  ...\n  nodeConfigGroups:\n    auto_scaling:\n      isNode: true\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 2Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n      serviceAccountName: external-dns\n      storageConfigs:\n        - mountPath: /opt/nifi/nifi-current/logs\n          name: logs\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/data\n          name: data\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/extensions\n          name: extensions-repository\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/flowfile_repository\n          name: flowfile-repository\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/nifi-current/conf\n          name: conf\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/content_repository\n          name: content-repository\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/provenance_repository\n          name: provenance-repository\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n  ...\n```\n\n----------------------------------------\n\nTITLE: Installing Prometheus Operator with Custom Settings - Console\nDESCRIPTION: Deploys the Prometheus Operator using Helm with explicit configuration overrides for RBAC, monitoring, and various Prometheus subcomponents. The command installs in the \"monitoring-system\" namespace, disables several built-in integrations (e.g., Grafana, Alertmanager), and sets debug log level. Helm and cluster RBAC permissions are prerequisites.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_6\n\nLANGUAGE: console\nCODE:\n```\nhelm install prometheus prometheus/kube-prometheus-stack --namespace monitoring-system \\\n    --set prometheusOperator.createCustomResource=false \\\n    --set prometheusOperator.logLevel=debug \\\n    --set prometheusOperator.alertmanagerInstanceNamespaces=monitoring-system \\\n    --set prometheusOperator.namespaces.additional[0]=monitoring-system \\\n    --set prometheusOperator.prometheusInstanceNamespaces=monitoring-system \\\n    --set prometheusOperator.thanosRulerInstanceNamespaces=monitoring-system \\\n    --set defaultRules.enable=false \\\n    --set alertmanager.enabled=false \\\n    --set grafana.enabled=false \\\n    --set kubeApiServer.enabled=false \\\n    --set kubelet.enabled=false \\\n    --set kubeControllerManager.enabled=false \\\n    --set coreDNS.enabled=false \\\n    --set kubeEtcd.enabled=false \\\n    --set kubeScheduler.enabled=false \\\n    --set kubeProxy.enabled=false \\\n    --set kubeStateMetrics.enabled=false \\\n    --set prometheus.enabled=false\n\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Namespace for Monitoring - Console\nDESCRIPTION: Creates a dedicated namespace 'monitoring-system' in Kubernetes, which isolates all Prometheus-related resources from other cluster workloads. Run this command before installing any monitoring components to avoid resource conflicts. No parameters are required.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_5\n\nLANGUAGE: console\nCODE:\n```\nkubectl create namespace monitoring-system\n```\n\n----------------------------------------\n\nTITLE: Applying the new node configuration to Kubernetes\nDESCRIPTION: This CLI command applies the updated NiFiCluster configuration YAML to the Kubernetes cluster, effectively adding the new node to the existing cluster. It ensures that the cluster state in Kubernetes reflects the configuration changes, triggering the deployment and initialization of the new node pod and associated resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\nkubectl -n nifi apply -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: YAML Snippet for Cluster SSL Secrets and Proxy Hosts\nDESCRIPTION: Specifies SSL secrets configuration, including secret name and whether to create a new secret, along with web proxy host settings used for secure web requests. Dependencies include Kubernetes secrets and cluster setup via Nifikop. Key parameters are tlsSecretName and create flag, which determine if the operator should generate or use existing secrets.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nsslSecrets:\n  tlsSecretName: \"test-nifikop\"\n  create: true\n```\n\n----------------------------------------\n\nTITLE: Creating Cert-Manager Issuer for Let's Encrypt\nDESCRIPTION: Defines a Kubernetes `Issuer` resource for `cert-manager` to obtain certificates from the Let's Encrypt staging environment using the ACME protocol. It configures the email for notifications, the ACME server URL, a secret to store the account key, and uses the HTTP01 challenge solver with an ingress template.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: cert-manager.io/v1alpha2\nkind: Issuer\nmetadata:\n  name: letsencrypt-staging\nspec:\n  acme:\n    # You must replace this email address with your own.\n    # Let's Encrypt will use this to contact you about expiring\n    # certificates, and issues related to your account.\n    email: <your email address>\n    server: https://acme-staging-v02.api.letsencrypt.org/directory\n    privateKeySecretRef:\n      # Secret resource used to store the account's private key.\n      name: example-issuer-account-key\n    # Add a single challenge solver, HTTP01 using nginx\n    solvers:\n      - http01:\n          ingress:\n            ingressTemplate:\n              metadata:\n                annotations:\n                  \"external-dns.alpha.kubernetes.io/ttl\": \"5\"\n```\n\n----------------------------------------\n\nTITLE: NiFiNodeGroupAutoscaler Custom Resource - YAML\nDESCRIPTION: This YAML defines a `NifiNodeGroupAutoscaler` custom resource.  It references the NiFi cluster and the node configuration group to be autoscaled.  It specifies the node selector, upscale and downscale strategies, and configurations for the NiFi properties.  This defines the autoscaling behavior for the NiFi cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiNodeGroupAutoscaler\nmetadata:\n  name: nifinodegroupautoscaler-sample\n  namespace: clusters\nspec:\n  # contains the reference to the NifiCluster with the one the node group autoscaler is linked.\n  clusterRef:\n    name: cluster\n    namespace: clusters\n  # defines the id of the NodeConfig contained in NifiCluster.Spec.NodeConfigGroups\n  nodeConfigGroupId: auto_scaling\n  # readOnlyConfig can be used to pass Nifi node config\n  # which has type read-only these config changes will trigger rolling upgrade\n  readOnlyConfig:\n    nifiProperties:\n      overrideConfigs: |\n        nifi.ui.banner.text=NiFiKop - Scale Group\n  # This is an example of a node config you can apply to each replica in this node group.\n  # Any settings here will override those in the configured nodeConfigGroupId\n#  nodeConfig:\n#    nodeSelector:\n#      node_type: high-mem\n  # The selector used to identify nodes in NifiCluster.Spec.Nodes this autoscaler will manage\n  # Use Node.Labels in combination with this selector to clearly define which nodes will be managed by this autoscaler \n  nodeLabelsSelector: \n    matchLabels:\n      nifi_cr: cluster\n      nifi_node_group: auto-scaling\n  # the strategy used to decide how to add nodes to a nifi cluster\n  upscaleStrategy: simple\n  # the strategy used to decide how to remove nodes from an existing cluster\n  downscaleStrategy: lifo\n```\n\n----------------------------------------\n\nTITLE: Replacing Logback Configuration (Markdown)\nDESCRIPTION: Defines options for replacing the default Logback configuration file (logback.xml). Allows replacement using an external Kubernetes ConfigMap (`replaceConfigMap`) or a Kubernetes Secret (`replaceSecretConfig`), with the Secret taking precedence.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/5_references/1_nifi_cluster/2_read_only_config.md#_snippet_3\n\nLANGUAGE: Markdown\nCODE:\n```\n## LogbackConfig\n\n|Field|Type|Description|Required|Default|\n|-----|----|-----------|--------|--------|\n|replaceConfigMap|[ConfigmapReference](#configmapreference)|logback.xml configuration that will replace the one produced based on template.|No|nil|\n|replaceSecretConfig|[SecretConfigReference](#secretconfigreference)|logback.xml configuration that will replace the one produced based on template and overrideConfigMap.|No|nil|\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop via Helm in Bash\nDESCRIPTION: Command to install the NiFiKop operator using Helm charts with a custom image tag and namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/6_contributing/1_developer_guide.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nhelm install skeleton ./helm/nifikop \\\n    --set image.tag=v0.5.1-release \\\n    --namespace-{\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Creating Custom StorageClass in Kubernetes - YAML\nDESCRIPTION: Defines a StorageClass with parameters tailored for Google Cloud Engine Persistent Disk and sets the volumeBindingMode to WaitForFirstConsumer. Dependencies include a Kubernetes cluster with GCE integration. Users must specify a unique StorageClass name and adjust provisioner type as needed. The StorageClass must be referenced in the NiFiCluster custom resource for effective usage.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: exampleStorageclass\nparameters:\n  type: pd-standard\nprovisioner: kubernetes.io/gce-pd\nreclaimPolicy: Delete\nvolumeBindingMode: WaitForFirstConsumer\n\n```\n\n----------------------------------------\n\nTITLE: Uninstalling Nifikop Chart\nDESCRIPTION: This snippet demonstrates uninstalling the Nifikop chart using Helm. It removes the chart deployment from the cluster. It requires Helm to be installed and configured, plus knowledge of the release name.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nhelm del nifikop\n```\n\n----------------------------------------\n\nTITLE: Referencing Let's Encrypt Issuer in NiFi Cluster Secure ListenerConfig - YAML\nDESCRIPTION: Updates the NifiCluster custom resource to reference an existing ACME Issuer for SSL certificate management. This configuration enables integration with Let's Encrypt via Cert-Manager and supports secure clustering and site-to-site communication. Dependencies include an active Issuer, Cert-Manager, and Nifikop operator. Key parameters: clusterDomain, useExternalDNS, sslSecrets.tlsSecretName, sslSecrets.issuerRef. Inputs are appropriate Composer YAML fields; output is a secure NiFiCluster configured for automatic SSL certificate management.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  clusterSecure: true\n  siteToSiteSecure: true\n  ...\n  listenersConfig:\n    clusterDomain: <DNS zone name>\n    useExternalDNS: true\n    ...\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n      issuerRef:\n        name: letsencrypt-staging\n        kind: Issuer\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Registry Client YAML Schema\nDESCRIPTION: This YAML code snippet defines a Custom Resource (CR) for the NiFi Registry Client, including API version, kind, metadata, and specification. It specifies the cluster reference, description, and URI for connecting to the NiFi registry, serving as the primary configuration template for the resource.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/5_references/3_nifi_registry_client.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiRegistryClient\nmetadata:\n  name: squidflow\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Squidflow demo\"\n  uri: \"http://nifi-registry:18080\"\n```\n\n----------------------------------------\n\nTITLE: Extracting client certificate and key from secret\nDESCRIPTION: These kubectl commands extract and decode the secret data for CA certificate, client certificate, and private key, writing each to separate files. This allows external applications or manual configuration to use the generated SSL credentials for secure communication with the NiFi cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_5\n\nLANGUAGE: Console\nCODE:\n```\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['ca\\.crt']}\" | base64 -d > ca.crt\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['tls\\.crt']}\" | base64 -d > tls.crt\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['tls\\.key']}\" | base64 -d > tls.key\n```\n\n----------------------------------------\n\nTITLE: Sample NifiNodeGroupAutoscaler Configuration (YAML)\nDESCRIPTION: This YAML snippet provides an example configuration for the `NifiNodeGroupAutoscaler` Custom Resource. It demonstrates how to reference a `NifiCluster`, specify the target `nodeConfigGroupId`, use `nodeLabelsSelector` to identify nodes, and set `upscaleStrategy` and `downscaleStrategy`. This configuration is used to automatically manage the replica count of a specific node group within the referenced NiFi cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/5_references/7_nifi_nodegroup_autoscaler.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiNodeGroupAutoscaler\nmetadata:\n  name: nifinodegroupautoscaler-sample\nspec:\n  # contains the reference to the NifiCluster with the one the node group autoscaler is linked.\n  clusterRef:\n    name: nificluster-name\n    namespace: nifikop\n  # defines the id of the NodeConfig contained in NifiCluster.Spec.NodeConfigGroups\n  nodeConfigGroupId: default-node-group\n  # The selector used to identify nodes in NifiCluster.Spec.Nodes this autoscaler will manage\n  # Use Node.Labels in combination with this selector to clearly define which nodes will be managed by this autoscaler \n  nodeLabelsSelector: \n    matchLabels:\n      nifi_cr: nificluster-name\n      nifi_node_group: default-node-group\n  # the strategy used to decide how to add nodes to a nifi cluster\n  upscaleStrategy: simple\n  # the strategy used to decide how to remove nodes from an existing cluster\n  downscaleStrategy: lifo\n```\n\n----------------------------------------\n\nTITLE: Deploying KEDA ScaledObject for NiFi Autoscaling - yaml\nDESCRIPTION: Defines a KEDA ScaledObject to autoscale NiFi node resources based on Prometheus metric triggers. Requires KEDA installed in the cluster and Prometheus stack operational. Key parameters include min/max replica counts, fallback logic, target deployment reference, and the Prometheus metric/query used for scaling decisions. Trigger query must match available Prometheus metrics.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: cluster\n  namespace: clusters\nspec:\n  scaleTargetRef:\n    apiVersion:    nifi.konpyutaika.com/v1alpha1     # Optional. Default: apps/v1\n    kind:          NifiNodeGroupAutoscaler           # Optional. Default: Deployment\n    name:          nifinodegroupautoscaler-sample    # Mandatory. Must be in the same namespace as the ScaledObject\n    envSourceContainerName: nifi                     # Optional. Default: .spec.template.spec.containers[0]\n  pollingInterval:  30                               # Optional. Default: 30 seconds\n  cooldownPeriod:   300                              # Optional. Default: 300 seconds\n  idleReplicaCount: 0                                # Optional. Default: ignored, must be less than minReplicaCount \n  minReplicaCount:  1                                # Optional. Default: 0\n  maxReplicaCount:  3                                # Optional. Default: 100\n  fallback:                                          # Optional. Section to specify fallback options\n    failureThreshold: 5                              # Mandatory if fallback section is included\n    replicas: 1                                      # Mandatory if fallback section is included\n  # advanced:                                          # Optional. Section to specify advanced options\n  #   restoreToOriginalReplicaCount: true              # Optional. Whether the target resource should be scaled back to original replicas count, after the ScaledObject is deleted\n  #   horizontalPodAutoscalerConfig:                   # Optional. Section to specify HPA related options\n  #     name: {name-of-hpa-resource}                   # Optional. Default: keda-hpa-{scaled-object-name}\n  #     behavior:                                      # Optional. Use to modify HPA's scaling behavior\n  #       scaleDown:\n  #         stabilizationWindowSeconds: 300 <--- \n  #         policies:\n  #         - type: Percent\n  #           value: 100\n  #           periodSeconds: 15\n  triggers:\n    - type: prometheus\n      metadata:\n        serverAddress: http://prometheus-operated.monitoring-system.svc:9090\n        metricName: http_requests_total\n        threshold: <threshold value>\n        query: <prometheus query>\n```\n\n----------------------------------------\n\nTITLE: Listing Deployed Helm Charts\nDESCRIPTION: Uses the `helm list` command to display all currently deployed Helm releases within the active Kubernetes context (namespace or cluster-wide, depending on configuration).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nhelm list\n```\n\n----------------------------------------\n\nTITLE: Updating OpenShift Configuration File for NiFi Deployment\nDESCRIPTION: Command to replace the default UID in the OpenShift configuration file with the one retrieved from the namespace. This ensures that the NiFi deployment uses the correct security context.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nsed -i \"s/1000690000/$uid/g\" config/samples/openshift.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing Helm Chart with Custom Image Tag\nDESCRIPTION: Deploys the NiFiKop Helm chart to Kubernetes, specifying the image tag and namespace. Ensures the Helm chart uses the correct image version matching the built image.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/6_contributing/1_developer_guide.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nhelm install skeleton ./helm/nifikop \\\n    --set image.tag=v0.5.1-release \\\n    --namespace-{\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Defining ServiceMonitor for NiFi Cluster\nDESCRIPTION: YAML manifest for creating a `ServiceMonitor` custom resource (apiVersion `monitoring.coreos.com/v1`) named 'cluster' in the 'monitoring-system' namespace. It configures Prometheus to discover Kubernetes services matching the labels `app: nifi` and `nifi_cr: cluster` within the 'clusters' namespace. It specifies scraping the 'prometheus' port on the '/metrics' path every 10 seconds and includes `relabelings` rules to add pod IP, `nodeId` label, and `nifi_cr` label as Prometheus labels on the scraped metrics.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: cluster\n  namespace: monitoring-system\n  labels:\n    app: nifi-cluster\n    nifi_cr: cluster\nspec:\n  selector:\n    matchLabels:\n      app: nifi\n      nifi_cr: cluster\n  namespaceSelector:\n    matchNames:\n      - clusters\n  endpoints:\n    - interval: 10s\n      port: prometheus\n      path: /metrics\n      honorLabels: true\n      relabelings:\n        - sourceLabels: [__meta_kubernetes_pod_ip]\n          separator: ;\n          regex: (.*)\n          targetLabel: pod_ip\n          replacement: $1\n          action: replace\n        - sourceLabels: [__meta_kubernetes_pod_label_nodeId]\n          separator: ;\n          regex: (.*)\n          targetLabel: nodeId\n          replacement: $1\n          action: replace\n        - sourceLabels: [__meta_kubernetes_pod_label_nifi_cr]\n          separator: ;\n          regex: (.*)\n          targetLabel: nifi_cr\n          replacement: $1\n          action: replace\n```\n\n----------------------------------------\n\nTITLE: Configuring RBAC for Kubernetes State Management\nDESCRIPTION: YAML definition for creating a Role and RoleBinding to allow NiFi's ServiceAccount to manage Leases and ConfigMaps, which are required for Kubernetes-native state management.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: simplenifi\n  namespace: nifi\nrules:\n- apiGroups: [\"coordination.k8s.io\"]\n  resources: [\"leases\"]\n  verbs: [\"*\"]\n- apiGroups: [\"\"]\n  resources: [\"configmaps\"]\n  verbs: [\"*\"]\n---\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: simplenifi\n  namespace: nifi\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: nifi\nroleRef:\n  kind: Role\n  name: simplenifi\n  apiGroup: rbac.authorization.k8s.io\n```\n\n----------------------------------------\n\nTITLE: Installing cert-manager Directly using kubectl - Bash\nDESCRIPTION: This snippet shows how to install cert-manager by applying the complete cert-manager CustomResourceDefinitions (CRDs) and deployment manifests directly with kubectl. It is necessary for securing NiFiKop clusters with TLS and authentication enabled. The snippet requires kubectl installed and connectivity to the target Kubernetes cluster. It installs cert-manager version v1.7.2 from upstream GitHub releases, setting up certificate issuance support for the operator.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/2_deploy_nifikop/1_quick_start.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f \\\n    https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.yaml\n```\n\n----------------------------------------\n\nTITLE: Apply NifiUser Resource\nDESCRIPTION: This console command applies the NifiUser resource defined in the previous code snippet to the Kubernetes cluster, creating the new user certificate for NiFi.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\ncat << EOF | kubectl apply -n nifi -f -\napiVersion:  nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: example-client\n  namespace: nifi\nspec:\n  clusterRef:\n    name: nifi\n  secretName: example-client-secret\nEOF\n```\n\n----------------------------------------\n\nTITLE: Deploying NifiNodeGroupAutoscaler Resource with YAML\nDESCRIPTION: This YAML manifest deploys a custom resource 'NifiNodeGroupAutoscaler', which sets up autoscaling for a specific NiFi node group based on defined strategies and labels. Dependencies include the NiFi Operator, an existing NiFiCluster, and matching NodeConfigGroup and labels. Key parameters specify cluster references, NodeConfigGroup id, Prometheus configuration, strategies for upscaling and downscaling, and resource selectors. Input is Kubernetes-compatible YAML; output is dynamic NiFi node group autoscaling according to real-time workload.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiNodeGroupAutoscaler\nmetadata:\n  name: nifinodegroupautoscaler-sample\n  namespace: clusters\nspec:\n  # contains the reference to the NifiCluster with the one the node group autoscaler is linked.\n  clusterRef:\n    name: cluster\n    namespace: clusters\n  # defines the id of the NodeConfig contained in NifiCluster.Spec.NodeConfigGroups\n  nodeConfigGroupId: auto_scaling\n  # readOnlyConfig can be used to pass Nifi node config\n  # which has type read-only these config changes will trigger rolling upgrade\n  readOnlyConfig:\n    nifiProperties:\n      overrideConfigs: |\n        nifi.ui.banner.text=NiFiKop - Scale Group\n  # This is an example of a node config you can apply to each replica in this node group.\n  # Any settings here will override those in the configured nodeConfigGroupId\n#  nodeConfig:\n#    nodeSelector:\n#      node_type: high-mem\n  # The selector used to identify nodes in NifiCluster.Spec.Nodes this autoscaler will manage\n  # Use Node.Labels in combination with this selector to clearly define which nodes will be managed by this autoscaler \n  nodeLabelsSelector: \n    matchLabels:\n      nifi_cr: cluster\n      nifi_node_group: auto-scaling\n  # the strategy used to decide how to add nodes to a nifi cluster\n  upscaleStrategy: simple\n  # the strategy used to decide how to remove nodes from an existing cluster\n  downscaleStrategy: lifo\n```\n\n----------------------------------------\n\nTITLE: NifiUser Creation\nDESCRIPTION: This YAML snippet shows how to create a NifiUser resource to generate client certificates for applications. It specifies the cluster reference and the secret name where the certificates will be stored.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion:  nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: example-client\n  namespace: nifi\nspec:\n  clusterRef:\n    name: nifi\n  secretName: example-client-secret\n```\n\n----------------------------------------\n\nTITLE: Creating a NifiUser resource for client SSL certificates\nDESCRIPTION: This snippet applies a Kubernetes YAML manifest to create a 'NifiUser' resource which generates client certificates signed by the NiFi CA. It creates a secret with CA certificate, user certificate, and private key. These credentials can be mounted into pods or exported for external use. Optionally, a JKS keystore can be included by setting 'includeJKS' to true in the spec.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_4\n\nLANGUAGE: Console\nCODE:\n```\ncat << EOF | kubectl apply -n nifi -f -\napiVersion:  nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: example-client\n  namespace: nifi\nspec:\n  clusterRef:\n    name: nifi\n  secretName: example-client-secret\nEOF\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for NiFiKop\nDESCRIPTION: Commands to build a Docker image of the NiFiKop operator from the current local branch, necessary before pushing or deploying via Helm.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/6_contributing/1_developer_guide.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport DOCKER_REPO_BASE={your-docker-repo}\nmake docker-build\n```\n\n----------------------------------------\n\nTITLE: InitContainer Configuration - YAML\nDESCRIPTION: This YAML snippet defines an `initContainer` for a Kubernetes deployment. The initContainer leverages the `apache/nifi-toolkit` Docker image and executes a shell script. This script retrieves the sensitive property key and runs the `encrypt-config.sh` tool against the NiFi configuration and flow configuration.  It mounts necessary volumes for configuration files and the data directory. The script ensures that sensitive properties in `nifi.properties`, `flow.json.gz`, and `flow.xml.gz` are encrypted with the specified algorithm during pod initialization.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/7_upgrade_guides/4_v1.3.1_to_v1.4.0.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ninitContainers:\n  - image: \"apache/nifi-toolkit:latest\"\n    name: nifi-toolkit\n    imagePullPolicy: Always\n    command:\n      - \"sh\"\n      - \"-c\"\n      - \"NIFI_SENSITIVE_PROPS_KEY=$(grep 'nifi.sensitive.props.key' /opt/nifi/nifi-current/conf/nifi.properties | cut -d'=' -f2) && bin/encrypt-config.sh -n /opt/nifi/nifi-current/conf/nifi.properties -f /opt/nifi/data/flow.json.gz -x -A NIFI_PBKDF2_AES_GCM_256 -s $NIFI_SENSITIVE_PROPS_KEY; bin/encrypt-config.sh -n /opt/nifi/nifi-current/conf/nifi.properties -f /opt/nifi/data/flow.xml.gz -x -A NIFI_PBKDF2_AES_GCM_256 -s $NIFI_SENSITIVE_PROPS_KEY\"\n    volumeMounts:\n      - name: data\n        mountPath: /opt/nifi/data\n      - name: conf\n        mountPath: /opt/nifi/nifi-current/conf\n```\n\n----------------------------------------\n\nTITLE: Example NifiNodeGroupAutoscaler Resource Definition (YAML)\nDESCRIPTION: This YAML snippet demonstrates how to define a `NifiNodeGroupAutoscaler` resource in Kubernetes. It specifies the target `NifiCluster` via `clusterRef`, identifies the node group using `nodeConfigGroupId` and `nodeLabelsSelector`, and sets the scaling strategies (`upscaleStrategy`, `downscaleStrategy`). This resource interacts with Kubernetes HorizontalPodAutoscaler to manage the number of NiFi nodes in the specified group.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/5_references/7_nifi_nodegroup_autoscaler.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiNodeGroupAutoscaler\nmetadata:\n  name: nifinodegroupautoscaler-sample\nspec:\n  # contains the reference to the NifiCluster with the one the node group autoscaler is linked.\n  clusterRef:\n    name: nificluster-name\n    namespace: nifikop\n  # defines the id of the NodeConfig contained in NifiCluster.Spec.NodeConfigGroups\n  nodeConfigGroupId: default-node-group\n  # The selector used to identify nodes in NifiCluster.Spec.Nodes this autoscaler will manage\n  # Use Node.Labels in combination with this selector to clearly define which nodes will be managed by this autoscaler \n  nodeLabelsSelector: \n    matchLabels:\n      nifi_cr: nificluster-name\n      nifi_node_group: default-node-group\n  # the strategy used to decide how to add nodes to a nifi cluster\n  upscaleStrategy: simple\n  # the strategy used to decide how to remove nodes from an existing cluster\n  downscaleStrategy: lifo\n```\n\n----------------------------------------\n\nTITLE: Defining NifiDataflow CRD in YAML\nDESCRIPTION: This YAML manifest defines a `NifiDataflow` custom resource for NiFiKop. It orchestrates the deployment of a specific versioned flow from a NiFi Registry (`bucketId`, `flowId`, `flowVersion`). It references the required `NifiRegistryClient` and `NifiParameterContext`, specifies the target cluster (`clusterRef`), and defines the management behavior (`syncMode`, `updateStrategy`).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: demo\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Installing Zookeeper using Helm\nDESCRIPTION: Helm command to install Zookeeper with specific resource limits, storage class configuration, and network policy settings. Zookeeper is required for NiFi cluster coordination.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nhelm install zookeeper bitnami/zookeeper \\\n    --namespace=zookeeper \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set global.storageClass=standard \\\n    --set networkPolicy.enabled=true \\\n    --set replicaCount=3 \\\n    --create-namespace\n```\n\n----------------------------------------\n\nTITLE: Packaging the Helm Chart for NiFiKop Using Make in Bash\nDESCRIPTION: Packages the NiFiKop Helm chart for distribution or deployment using a Makefile target. Dependencies include GNU Make and Helm CLI. No input parameters; output is a packaged Helm chart archive, ready to be pushed to a Helm repository.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/6_contributing/1_developer_guide.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nmake helm-package\n```\n\n----------------------------------------\n\nTITLE: Applying NiFiKop CRDs\nDESCRIPTION: Applies the Custom Resource Definitions (CRDs) for NiFiKop to the Kubernetes cluster. These CRDs define the custom resources that the operator manages, such as NiFiClusters, NiFiDataflows, and NiFiUsers.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/6_contributing/1_developer_guide.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop Helm Chart with YAML Configuration (Console)\nDESCRIPTION: This console command demonstrates how to install the NiFiKop Helm chart from the 'konpyutaika/nifikop' repository. It names the release 'nifikop' and applies custom configurations specified in the 'values.yaml' file, offering an alternative to setting parameters individually via the command line.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n$ helm install nifikop \\\n      konpyutaika/nifikop \\\n      -f values.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiDataflow Resource in YAML\nDESCRIPTION: This YAML snippet provides an example manifest for creating a `NifiDataflow` resource using the nifikop operator. It demonstrates how to specify the API version, kind, metadata, and various configuration options within the `spec`, including the parent process group, flow details (bucket, flow ID, version), sync mode, invalid component handling, update strategy, and references to dependent resources like the NiFi cluster, registry client, and parameter context.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/5_references/5_nifi_dataflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  flowPosition:\n    posX: 0\n    posY: 0\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: squidflow\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: nifikop\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Applying Scaledown NiFi Cluster Configuration (Shell)\nDESCRIPTION: Applies the updated `NifiCluster` custom resource definition to the Kubernetes cluster using `kubectl`. This command instructs the NiFiKop operator to reconcile the cluster state based on the modified YAML file, triggering the graceful scaledown procedure for the removed node.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nkubectl -n nifi apply -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining Prometheus ServiceMonitor Resource for NiFi Cluster Metrics in YAML\nDESCRIPTION: This ServiceMonitor resource instructs Prometheus on which services to scrape metrics from, specifically targeting NiFi cluster pods labeled 'app: nifi' and 'nifi_cr: cluster' in the 'clusters' namespace. It defines scrape endpoints on the 'prometheus' port at path '/metrics' every 10s, with label relabeling to extract pod IP, node ID, and NiFi custom resource labels. This configuration is essential for accurate Prometheus metric collection from NiFi pods within Kubernetes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: cluster\n  namespace: monitoring-system\n  labels:\n    app: nifi-cluster\n    nifi_cr: cluster\nspec:\n  selector:\n    matchLabels:\n      app: nifi\n      nifi_cr: cluster\n  namespaceSelector:\n    matchNames:\n      - clusters\n  endpoints:\n    - interval: 10s\n      port: prometheus\n      path: /metrics\n      honorLabels: true\n      relabelings:\n        - sourceLabels: [__meta_kubernetes_pod_ip]\n          separator: ;\n          regex: (.*)\n          targetLabel: pod_ip\n          replacement: $1\n          action: replace\n        - sourceLabels: [__meta_kubernetes_pod_label_nodeId]\n          separator: ;\n          regex: (.*)\n          targetLabel: nodeId\n          replacement: $1\n          action: replace\n        - sourceLabels: [__meta_kubernetes_pod_label_nifi_cr]\n          separator: ;\n          regex: (.*)\n          targetLabel: nifi_cr\n          replacement: $1\n          action: replace\n```\n\n----------------------------------------\n\nTITLE: Deploy NiFi Cluster on OpenShift\nDESCRIPTION: This snippet deploys the NiFi cluster on OpenShift using the updated `openshift.yaml` file. The UID/GID has been configured in the previous steps.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create -n nifi -f config/samples/openshift.yaml\n```\n\n----------------------------------------\n\nTITLE: Sample Deployment YAML for NiFi Cluster Service with External Service Definition\nDESCRIPTION: Defines the NiFi cluster service within Kubernetes, specifying a ClusterIP type and exposing port 8443. This service acts as the internal endpoint for TLS termination and communication from the VirtualService route, as referenced in the destination configuration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_4\n\nLANGUAGE: YAML\nCODE:\n```\nspec:  \n  externalServices:  \n    - name: \"nifi-cluster\"\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8443\n            internalListenerName: \"https\"\n```\n\n----------------------------------------\n\nTITLE: Installing Zookeeper Cluster with Helm\nDESCRIPTION: Installs a 3-replica Zookeeper cluster using the Bitnami Helm chart into the 'zookeeper' namespace. Configures resource requests/limits and sets the storage class. Requires the Bitnami Helm repo to be added first.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/2_setup/1_getting_started.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhelm install zookeeper bitnami/zookeeper \\\n    --namespace=zookeeper \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set global.storageClass=standard \\\n    --set networkPolicy.enabled=true \\\n    --set replicaCount=3 \\\n    --create-namespace\n```\n\n----------------------------------------\n\nTITLE: Installing Zookeeper with Helm - Bash\nDESCRIPTION: Installs the Bitnami Zookeeper chart into the `zookeeper` namespace using Helm. It sets resource requests and limits, configures the storage class (note: replace `standard` with your desired value), enables network policy, and sets the replica count to 3. The `--create-namespace` flag ensures the namespace exists.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nhelm install zookeeper bitnami/zookeeper \\\n    --namespace=zookeeper \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set global.storageClass=standard \\\n    --set networkPolicy.enabled=true \\\n    --set replicaCount=3 \\\n    --create-namespace\n```\n\n----------------------------------------\n\nTITLE: Deploying Prometheus Custom Resource - yaml\nDESCRIPTION: Declares the Prometheus custom resource for the monitoring-system namespace, specifying operational parameters such as scrape intervals, resource requests, pod monitor selection, and account usage. Requires the Prometheus Operator to be installed in the cluster. Parameters configure resource requests, selector logic, and disables the Admin API. Should be applied after operator and RBAC setup.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: Prometheus\nmetadata:\n  name: prometheus\n  namespace: monitoring-system\nspec:\n  enableAdminAPI: false\n  evaluationInterval: 30s\n  logLevel: debug\n  podMonitorSelector:\n    matchExpressions:\n    - key: release\n      operator: In\n      values:\n      - prometheus\n  resources:\n    requests:\n      memory: 400Mi\n  scrapeInterval: 30s\n  serviceAccountName: prometheus\n  serviceMonitorSelector:\n    matchExpressions:\n    - key: app\n      operator: In\n      values:\n      - nifi-cluster\n```\n\n----------------------------------------\n\nTITLE: Retrieve User Credentials from Secret\nDESCRIPTION: These kubectl commands retrieve the CA certificate, user certificate, and user private key from the specified secret, decode them from base64, and write them to local files. These files can then be used by client applications to authenticate with the NiFi cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nkubectl get secret example-client-secret -o jsonpath=\"{[\\'data\\'][\\'ca\\.crt\\']}\" | base64 -d > ca.crt\nkubectl get secret example-client-secret -o jsonpath=\"{[\\'data\\'][\\'tls\\.crt\\']}\" | base64 -d > tls.crt\nkubectl get secret example-client-secret -o jsonpath=\"{[\\'data\\'][\\'tls\\.key\\']}\" | base64 -d > tls.key\n```\n\n----------------------------------------\n\nTITLE: Creating a Kubernetes Secret for Sensitive Parameters (kubectl)\nDESCRIPTION: Demonstrates using `kubectl` to create a Kubernetes secret named `secret-params` in the `nifikop` namespace. This secret holds key-value pairs (`secret1`, `secret2`) that can be referenced as sensitive parameters within a `NifiParameterContext` defined in the same or different namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create secret generic secret-params \\\n    --from-literal=secret1=yop \\\n    --from-literal=secret2=yep \\\n    -n nifikop\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio VirtualService for HTTPS NiFi Routing\nDESCRIPTION: This YAML defines a VirtualService for HTTPS that redirects traffic from the Gateway to the NiFi ClusterIP Service on port 8443. It handles all traffic with the '/' prefix.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: nifi-vs\nspec:\n  gateways:\n  - nifi-gateway\n  hosts:\n  - nifi.my-domain.com\n  http:\n  - match:\n    - uri:\n        prefix: /\n    route:\n    - destination:\n        host: <service-name>.<namespace>.svc.cluster.local\n        port:\n          number: 8443\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes StorageClass YAML\nDESCRIPTION: Defines a Kubernetes StorageClass resource intended for persistent volume provisioning. It specifies the `WaitForFirstConsumer` volume binding mode, which delays volume binding until a pod is scheduled, which is recommended for stateful applications like NiFi. The example is configured for GCP Persistent Disks (`kubernetes.io/gce-pd`) with a `Delete` reclaim policy.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: exampleStorageclass\nparameters:\n  type: pd-standard\nprovisioner: kubernetes.io/gce-pd\nreclaimPolicy: Delete\nvolumeBindingMode: WaitForFirstConsumer\n```\n\n----------------------------------------\n\nTITLE: Adding Prometheus Helm Repository\nDESCRIPTION: Commands to add and update the Prometheus community Helm chart repository for installing Prometheus components.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nhelm repo add prometheus https://prometheus-community.github.io/helm-charts\n```\n\nLANGUAGE: console\nCODE:\n```\nhelm repo update\n```\n\n----------------------------------------\n\nTITLE: Uninstalling the Nifikop Helm Chart\nDESCRIPTION: Command to delete the Nifikop Helm deployment, removing associated resources from the cluster. Note: CRDs are not deleted automatically and require manual cleanup.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nhelm del nifikop\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Node Group Autoscaler - Kubernetes CRD Manifest - yaml\nDESCRIPTION: This YAML snippet defines a NifiNodeGroupAutoscaler custom resource to specify the autoscaling settings for a NiFi node group in Kubernetes. The resource links to an existing NiFi cluster, references a NodeConfigGroup for auto-scaling ('auto_scaling'), and sets strategies for scaling up and down. It also allows optional configuration overrides (such as custom NiFi properties) and node label selectors to target the group. Requires the NiFi Kubernetes operator and CRDs for NifiNodeGroupAutoscaler to be installed and available in the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiNodeGroupAutoscaler\nmetadata:\n  name: nifinodegroupautoscaler-sample\n  namespace: clusters\nspec:\n  # contains the reference to the NifiCluster with the one the node group autoscaler is linked.\n  clusterRef:\n    name: cluster\n    namespace: clusters\n  # defines the id of the NodeConfig contained in NifiCluster.Spec.NodeConfigGroups\n  nodeConfigGroupId: auto_scaling\n  # readOnlyConfig can be used to pass Nifi node config\n  # which has type read-only these config changes will trigger rolling upgrade\n  readOnlyConfig:\n    nifiProperties:\n      overrideConfigs: |\n        nifi.ui.banner.text=NiFiKop - Scale Group\n  # This is an example of a node config you can apply to each replica in this node group.\n  # Any settings here will override those in the configured nodeConfigGroupId\n#  nodeConfig:\n#    nodeSelector:\n#      node_type: high-mem\n  # The selector used to identify nodes in NifiCluster.Spec.Nodes this autoscaler will manage\n  # Use Node.Labels in combination with this selector to clearly define which nodes will be managed by this autoscaler \n  nodeLabelsSelector: \n    matchLabels:\n      nifi_cr: cluster\n      nifi_node_group: auto-scaling\n  # the strategy used to decide how to add nodes to a nifi cluster\n  upscaleStrategy: simple\n  # the strategy used to decide how to remove nodes from an existing cluster\n  downscaleStrategy: lifo\n```\n\n----------------------------------------\n\nTITLE: Creating secret for sensitive parameters using kubectl\nDESCRIPTION: This console command creates a Kubernetes secret named 'secret-params' with literal key-value pairs for sensitive parameters, which can be referenced by the NifiParameterContext resource. This approach keeps sensitive information secure outside of CRD specifications.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/3_tasks/3_nifi_dataflow.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl create secret generic secret-params \\\n    --from-literal=secret1=yop \\\n    --from-literal=secret2=yep \\\n    -n nifikop\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Listener for NiFi Processors in YAML\nDESCRIPTION: Demonstrates how to add a custom internal listener without a specific type to expose NiFi processors through additional ports, allowing direct access to processor endpoints such as HTTP receivers.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    ...\n    - name: \"http-tracking\"\n      containerPort: 8081\n```\n\n----------------------------------------\n\nTITLE: Create NifiUser Resource\nDESCRIPTION: This YAML snippet defines a NifiUser resource. It references the NiFi cluster by name (`nifi`), specifies the secret name where the user credentials will be stored (`example-client-secret`), and indicates the namespace (`nifi`). This will trigger the operator to create a new user certificate signed by the CA.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion:  nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: example-client\n  namespace: nifi\nspec:\n  clusterRef:\n    name: nifi\n  secretName: example-client-secret\n```\n\n----------------------------------------\n\nTITLE: Assigning an External Issuer to the NiFiCluster for SSL - Kubernetes YAML\nDESCRIPTION: Configures the NiFiCluster resource to use an external issuer (such as the previously defined cert-manager Issuer) for SSL certificate management. Enables secure cluster and site-to-site communication, external DNS, and sets the \"issuerRef\" field to reference the certificate authority by name and kind. Assumes that the referenced issuer already exists in the cluster. Required for integrating with Let's Encrypt or other external certificate authorities.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\n...\nspec:\n  ...\n  clusterSecure: true\n  siteToSiteSecure: true\n  ...\n  listenersConfig:\n    clusterDomain: <DNS zone name>\n    useExternalDNS: true\n    ...\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n      issuerRef:\n        name: letsencrypt-staging\n        kind: Issuer\n\n```\n\n----------------------------------------\n\nTITLE: Configuring ClusterIP External Service for Nifi (YAML)\nDESCRIPTION: This YAML snippet demonstrates how to configure a Kubernetes ClusterIP service for a Nifi cluster managed by Nifikop. It defines a service named 'clusterip' that exposes ports 8080 and 7182, mapping them to internal Nifi listeners 'http' and 'my-custom-listener' respectively, and includes example metadata.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/5_references/1_nifi_cluster/7_external_service_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nexternalServices:\n    - name: \"clusterip\"\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8080\n            internalListenerName: \"http\"\n          - port: 7182\n            internalListenerName: \"my-custom-listener\"\n            protocol: TCP\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Dataflow Configuration YAML\nDESCRIPTION: This YAML snippet defines a `NifiDataflow` custom resource, representing a NiFi dataflow configuration. It specifies details such as the parent process group, bucket ID, flow ID, flow version, position, sync mode, update strategy, and references to other Kubernetes resources like clusters, registry clients, and parameter contexts.  The dataflow will run based on the provided configuration. The `spec` field encapsulates the desired dataflow configuration, and it depends on external Kubernetes resources, and the NiFi operator.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/5_references/5_nifi_dataflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  flowPosition:\n    posX: 0\n    posY: 0\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: squidflow\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: nifikop\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Defining NifiCluster CR to Scale Down | YAML\nDESCRIPTION: This YAML snippet shows how to update the `NifiCluster` Custom Resource definition to remove a node with ID 2 by commenting out or removing its entry from the `spec.nodes` list. This signals the NiFiKop operator to initiate a graceful scale-down (decommission) process for that node. Requires a pre-existing `simplenifi` cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  headlessServiceEnabled: true\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.11.3\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n# >>>> START: node removed\n#    - id: 2\n#      nodeConfigGroup: \"default_group\"\n# <<<< END\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi NodeGroup Autoscaler Using Kubernetes CustomResource YAML\nDESCRIPTION: This YAML snippet defines a NifiNodeGroupAutoscaler CustomResource used to configure automatic scaling for a NifiCluster deployment. The spec includes references to the target NifiCluster, node configuration group IDs, and label selectors which identify the nodes controlled by this autoscaler. It specifies upscale and downscale strategies to govern how nodes are added or removed from the cluster. This configuration requires a Kubernetes environment with CRD support for NiFiKop components and proper permissions to manage the specified cluster. The key inputs are clusterRef to link the NifiCluster, nodeConfigGroupId to select the node configuration, and nodeLabelsSelector to target nodes. The output is the autoscaler's managed scaling behavior reflected in cluster node counts.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/5_references/7_nifi_nodegroup_autoscaler.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiNodeGroupAutoscaler\nmetadata:\n  name: nifinodegroupautoscaler-sample\nspec:\n  # contains the reference to the NifiCluster with the one the node group autoscaler is linked.\n  clusterRef:\n    name: nificluster-name\n    namespace: nifikop\n  # defines the id of the NodeConfig contained in NifiCluster.Spec.NodeConfigGroups\n  nodeConfigGroupId: default-node-group\n  # The selector used to identify nodes in NifiCluster.Spec.Nodes this autoscaler will manage\n  # Use Node.Labels in combination with this selector to clearly define which nodes will be managed by this autoscaler \n  nodeLabelsSelector: \n    matchLabels:\n      nifi_cr: nificluster-name\n      nifi_node_group: default-node-group\n  # the strategy used to decide how to add nodes to a nifi cluster\n  upscaleStrategy: simple\n  # the strategy used to decide how to remove nodes from an existing cluster\n  downscaleStrategy: lifo\n```\n\n----------------------------------------\n\nTITLE: Performing a Dry Run Installation of Nifikop Helm Chart\nDESCRIPTION: Demonstrates how to perform a dry run installation of the Nifikop Helm chart using `helm install`. The `--dry-run` flag simulates the installation without deploying resources, useful for validation. It also shows setting parameters like `logLevel` and target `namespaces` using `--set`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop \\\n    --dry-run \\\n    --set logLevel=Debug \\\n    --set namespaces={\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Deploying ServiceMonitor to Scrape NiFi Cluster Metrics from Kubernetes Services - yaml\nDESCRIPTION: This YAML manifest defines a ServiceMonitor custom resource named \"cluster\" in the monitoring-system namespace. It targets Kubernetes services labeled with \"app: nifi\" and \"nifi_cr: cluster\" in the \"clusters\" namespace for Prometheus to scrape metrics. The endpoint configuration specifies scraping every 10 seconds on the prometheus port and relabeling pod IP, nodeId, and nifi_cr labels to Prometheus labels. This resource enables Prometheus to discover and collect NiFi cluster metrics automatically.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: cluster\n  namespace: monitoring-system\n  labels:\n    app: nifi-cluster\n    nifi_cr: cluster\nspec:\n  selector:\n    matchLabels:\n      app: nifi\n      nifi_cr: cluster\n  namespaceSelector:\n    matchNames:\n      - clusters\n  endpoints:\n    - interval: 10s\n      port: prometheus\n      path: /metrics\n      honorLabels: true\n      relabelings:\n        - sourceLabels: [__meta_kubernetes_pod_ip]\n          separator: ;\n          regex: (.*)\n          targetLabel: pod_ip\n          replacement: $1\n          action: replace\n        - sourceLabels: [__meta_kubernetes_pod_label_nodeId]\n          separator: ;\n          regex: (.*)\n          targetLabel: nodeId\n          replacement: $1\n          action: replace\n        - sourceLabels: [__meta_kubernetes_pod_label_nifi_cr]\n          separator: ;\n          regex: (.*)\n          targetLabel: nifi_cr\n          replacement: $1\n          action: replace\n```\n\n----------------------------------------\n\nTITLE: Creating Let's Encrypt Issuer using Cert-Manager - YAML\nDESCRIPTION: Defines an ACME Issuer resource for use with Cert-Manager, issuing signed certificates via Let's Encrypt in a Kubernetes environment. Dependency: Cert-Manager must be installed. Required parameters include ACME registration email, secret for private key storage, and ingress template annotations for DNS management. Input is the email and related ACME info; output is an Issuer resource ready to create certificates automatically.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: cert-manager.io/v1alpha2\nkind: Issuer\nmetadata:\n  name: letsencrypt-staging\nspec:\n  acme:\n    # You must replace this email address with your own.\n    # Let's Encrypt will use this to contact you about expiring\n    # certificates, and issues related to your account.\n    email: <your email address>\n    server: https://acme-staging-v02.api.letsencrypt.org/directory\n    privateKeySecretRef:\n      # Secret resource used to store the account's private key.\n      name: example-issuer-account-key\n    # Add a single challenge solver, HTTP01 using nginx\n    solvers:\n      - http01:\n          ingress:\n            ingressTemplate:\n              metadata:\n                annotations:\n                  \"external-dns.alpha.kubernetes.io/ttl\": \"5\"\n```\n\n----------------------------------------\n\nTITLE: Applying NifiCluster Configuration Changes for Scale Down (Shell)\nDESCRIPTION: This command uses `kubectl` to apply the updated configuration from the `simplenificluster.yaml` file (where a node has been removed) to the `nifi` namespace. This signals the NiFiKop operator to start the graceful decommission process for the removed node.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/3_tasks/1_nifi_cluster/2_cluster_scaling.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\nkubectl -n nifi apply -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Applying NiFiKop Cluster Configuration - kubectl / Shell\nDESCRIPTION: This shell command applies the updated NifiCluster YAML manifest using kubectl within the specified namespace (nifi). Ensure the kubectl CLI is installed and configured, and you have permissions to the target cluster and namespace. The input is a path to the YAML file (config/samples/simplenificluster.yaml), and the effect is to trigger scaling changes in the cluster. Output: Successful apply triggers operator reconciliation. Limitations: Requires NiFiKop CRDs and operator running.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nkubectl -n nifi apply -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring Persistent Storage for NiFi Nodes (YAML)\nDESCRIPTION: This YAML snippet defines multiple storageConfigs entries for persisting NiFi data, logs, repositories, and configurations using Kubernetes PVCs. Each storageConfig sets mountPath, PVC attributes, reclaimPolicy, and custom metadata via labels and annotations. The field storageClassName selects the desired storage class, and resources.requests.storage determines the persistent volume size. This configuration should be inserted under the storageConfigs section of a NodeConfigGroup and is dependent on the presence of appropriate storage classes and sufficient storage capacity in the Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n...\nstorageConfigs:\n  - mountPath: /opt/nifi/nifi-current/logs\n    name: logs\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/data\n    name: data\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 50Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/extensions\n    name: extensions-repository\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 5Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/flowfile_repository\n    name: flowfile-repository\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/nifi-current/conf\n    name: conf\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 5Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/content_repository\n    name: content-repository\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 500Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/provenance_repository\n    name: provenance-repository\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 500Gi\n\n```\n\n----------------------------------------\n\nTITLE: Creating a NiFi User with Custom Access Policies in Kubernetes (YAML)\nDESCRIPTION: Defines a Kubernetes Custom Resource (CR) of kind 'NifiUser' for managing an Apache NiFi user via the nifikop operator. This declaration allows you to specify the user's identity in NiFi, reference the associated cluster, control certificate management settings ('includeJKS', 'createCert'), and declare a list of access policies including type, action, target resource, component information, etc. Dependencies: Requires a running nifikop operator and a referenced NifiCluster. Inputs: YAML with CRD fields. Outputs: Synced user entity and access policies in both NiFi and Kubernetes. Fields under 'metadata' and 'spec' must match your setup constraints; componentId can be left empty if not required.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/3_tasks/4_nifi_user_group.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiUser\nmetadata:\n  name: aguitton\nspec:\n  identity: alexandre.guitton@konpyutaika.com\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  includeJKS: false\n  createCert: false\n  accessPolicies:\n    - type: component\n      action: read\n      resource: /data\n      componentType: \"process-groups\"\n      componentId: \"\"\n\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi NodeConfig Resource in YAML\nDESCRIPTION: This YAML snippet defines the NodeConfig specification used by a Kubernetes NiFi operator to configure individual nodes within a NiFi cluster. It specifies parameters such as the maximum provenance storage size, user ID to run as, node clustering flag, pod metadata (including annotations and labels), Docker image and image pull policy, volume mounts for external volumes and storage configurations, and persistent volume claim specifications. The configuration facilitates node-level customization in the deployment of NiFi on Kubernetes and includes references to official NiFi and Kubernetes documentation for detailed property explanations.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/5_references/1_nifi_cluster/3_node_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n   default_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      # Set this to true if the instance is a node in a cluster.\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#basic-cluster-setup\n      isNode: true\n      # Additionnal metadata to merge to the pod associated\n      podMetadata:\n        annotations:\n          node-annotation: \"node-annotation-value\"\n        labels:\n          node-label: \"node-label-value\"\n      # Docker image used by the operator to create the node associated\n      # https://hub.docker.com/r/apache/nifi/\n#      image: \"apache/nifi:1.11.2\"\n      # nodeAffinity can be specified, operator populates this value if new pvc added later to node\n      # https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity\n#      nodeAffinity:\n      # imagePullPolicy define the pull policy for NiFi cluster docker image\n      imagePullPolicy: IfNotPresent\n      # priorityClassName define the name of the priority class to be applied to these nodes\n      priorityClassName: \"example-priority-class-name\"\n      # externalVolumeConfigs specifies a list of volume to mount into the main container.\n      externalVolumeConfigs:\n        - name: example-volume\n          mountPath: \"/opt/nifi/example\"\n          secret:\n            secretName: \"raw-controller\"\n      # storageConfigs specifies the node related configs\n      storageConfigs:\n        # Name of the storage config, used to name PV to reuse into sidecars for example.\n        - name: provenance-repository\n          # Path where the volume will be mount into the main nifi container inside the pod.\n          mountPath: \"/opt/nifi/provenance_repository\"\n          # Metadata to attach to the PVC that gets created\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          # Kubernetes PVC spec\n          # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n```\n\n----------------------------------------\n\nTITLE: Defining NifiParameterContext CRD with Parameters and Secret References in YAML\nDESCRIPTION: Specifies a NifiParameterContext custom resource that defines parameter contexts for NiFi dataflows, including plain and sensitive parameters. It references Kubernetes secrets to handle sensitive parameter values securely. This CRD relies on a cluster reference and includes descriptive metadata and parameter name-value pairs to be applied to the NiFi dataflow.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/3_tasks/3_nifi_dataflow.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\n  namespace: demo\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      value: toto\n      description: toto\n```\n\n----------------------------------------\n\nTITLE: Checking Status of NiFiKop Helm Deployment\nDESCRIPTION: Command to check the current status of the NiFiKop Helm deployment, which provides information about the deployed resources and their condition.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nhelm status nifikop\n```\n\n----------------------------------------\n\nTITLE: Deploy KEDA with Helm on Kubernetes\nDESCRIPTION: This snippet illustrates adding the KEDA Helm repository, updating it, and installing KEDA into a dedicated namespace 'keda'. It automates the deployment process for KEDA, a Kubernetes Event Driven Autoscaler. No dependencies beyond Helm and kubectl are required. It sets up KEDA to enable event-driven scaling for other applications.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_0\n\nLANGUAGE: Console\nCODE:\n```\nhelm repo add kedacore https://kedacore.github.io/charts\nhelm repo update\nkubectl create namespace keda\nhelm install keda kedacore/keda --namespace keda\n```\n\n----------------------------------------\n\nTITLE: Defining Nifi Listener Types Using YAML\nDESCRIPTION: This YAML snippet defines the Nifi listener configuration, specifying multiple internal listener types such as HTTPS, cluster communication, S2S protocol, Prometheus monitoring, and load balancing. Each listener configures a name, type, and container port for internal access. The snippet also includes SSL secrets configuration with TLS secret name and an option to create the cert manager secrets, which are necessary for secure communication within Nifi in Kubernetes environments. This configuration snippet is crucial for setting up Nifi's internal networking and securing traffic between components.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/5_references/1_nifi_cluster/6_listeners_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  listenersConfig:\n    internalListeners:\n      - type: \"https\"\n        name: \"https\"\n        containerPort: 8443\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n      - type: \"prometheus\"\n        name: \"prometheus\"\n        containerPort: 9090\n      - type: \"load-balance\"\n        name: \"load-balance\"\n        containerPort: 6342\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Node with Kubernetes NodeConfig in YAML\nDESCRIPTION: This YAML snippet defines the NodeConfig for a NiFi node within a Kubernetes cluster. It specifies parameters such as the maximum provenance storage size, the user ID under which the NiFi process runs, whether the instance is part of a cluster, pod metadata annotations and labels, Docker image details, image pull policy, priority class names, external volume mounts, and persistent storage configurations. It includes examples of mounting volumes with Kubernetes PersistentVolumeClaims and managing pod affinity and security context settings. The snippet depends on Kubernetes environment concepts and NiFi deployment best practices.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/5_references/1_nifi_cluster/3_node_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ndefault_group:\n   # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n   # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n   provenanceStorage: \"10 GB\"\n   #RunAsUser define the id of the user to run in the Nifi image\n   # +kubebuilder:validation:Minimum=1\n   runAsUser: 1000\n   # Set this to true if the instance is a node in a cluster.\n   # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#basic-cluster-setup\n   isNode: true\n   # Additionnal metadata to merge to the pod associated\n   podMetadata:\n     annotations:\n       node-annotation: \"node-annotation-value\"\n     labels:\n       node-label: \"node-label-value\"\n   # Docker image used by the operator to create the node associated\n   # https://hub.docker.com/r/apache/nifi/\n#      image: \"apache/nifi:1.11.2\"\n   # nodeAffinity can be specified, operator populates this value if new pvc added later to node\n   # https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity\n#      nodeAffinity:\n   # imagePullPolicy define the pull policy for NiFi cluster docker image\n   imagePullPolicy: IfNotPresent\n   # priorityClassName define the name of the priority class to be applied to these nodes\n   priorityClassName: \"example-priority-class-name\"\n   # externalVolumeConfigs specifies a list of volume to mount into the main container.\n   externalVolumeConfigs:\n     - name: example-volume\n       mountPath: \"/opt/nifi/example\"\n       secret:\n         secretName: \"raw-controller\"\n   # storageConfigs specifies the node related configs\n   storageConfigs:\n     # Name of the storage config, used to name PV to reuse into sidecars for example.\n     - name: provenance-repository\n       # Path where the volume will be mount into the main nifi container inside the pod.\n       mountPath: \"/opt/nifi/provenance_repository\"\n       # Metadata to attach to the PVC that gets created\n       metadata:\n         labels:\n           my-label: my-value\n         annotations:\n           my-annotation: my-value\n       # Kubernetes PVC spec\n       # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n       pvcSpec:\n         accessModes:\n           - ReadWriteOnce\n         storageClassName: \"standard\"\n         resources:\n           requests:\n             storage: 10Gi\n     - mountPath: \"/opt/nifi/nifi-current/logs\"\n       name: logs\n       pvcSpec:\n         accessModes:\n           - ReadWriteOnce\n         storageClassName: \"standard\"\n         resources:\n           requests:\n             storage: 10Gi\n```\n\n----------------------------------------\n\nTITLE: Describing NifiCluster to see node's action step status - Console\nDESCRIPTION: This shell command describes the NiFiCluster and extracts the graceful action state of each node. This status is critical to monitor the progress of the scaling down process.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/3_tasks/1_nifi_cluster/2_cluster_scaling.md#_snippet_5\n\nLANGUAGE: console\nCODE:\n```\nkubectl describe nificluster simplenifi\n\n...\nStatus:\n  Nodes State:\n    ...\n    2:\n      Configuration State:  ConfigInSync\n      Graceful Action State:\n        Action State:   GracefulDownscaleRequired\n        Error Message:\n    ...\n...\n```\n\n----------------------------------------\n\nTITLE: Adding KEDA Helm Repo\nDESCRIPTION: This command adds the KEDA Helm repository to your local Helm configuration, allowing you to install KEDA charts from this repository. It's a prerequisite for installing KEDA via Helm.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nhelm repo add kedacore https://kedacore.github.io/charts\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Namespace for Prometheus Operator - console\nDESCRIPTION: Command to create a dedicated Kubernetes namespace called 'monitoring-system' for deploying the Prometheus operator and related resources. This segregation isolates monitoring components.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nkubectl create namespace monitoring-system\n```\n\n----------------------------------------\n\nTITLE: Encrypting NiFi Configuration - Shell\nDESCRIPTION: This shell script uses the `encrypt-config.sh` tool from the NiFi toolkit to update the encryption algorithm for NiFi properties and flow configuration files. It takes `nifi.properties`, `flow.xml.gz`, and `flow.json.gz` as input, and uses the provided sensitive properties key and target algorithm to encrypt the sensitive properties. It assumes the toolkit is available and that the necessary files are present.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/7_upgrade_guides/4_v1.3.1_to_v1.4.0.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nencrypt-config.sh -n nifi.properties -f flow.xml.gz -x -s PROPERTIES_KEY -A NIFI_PBKDF2_AES_GCM_256\nencrypt-config.sh -n nifi.properties -f flow.json.gz -x -s PROPERTIES_KEY -A NIFI_PBKDF2_AES_GCM_256\n```\n\n----------------------------------------\n\nTITLE: Defining NifiNodeGroupAutoscaler Resource for NiFi Autoscaling - yaml\nDESCRIPTION: Kubernetes custom resource definition for NifiNodeGroupAutoscaler specifying autoscaling behavior for NiFi node groups. It references the NiFi cluster to scale, identifies the NodeConfigGroup to target, optionally overrides read-only NiFi properties, selects nodes matching specified labels, and sets upscale and downscale strategies. This controls how NiFi nodes are added or removed dynamically.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiNodeGroupAutoscaler\nmetadata:\n  name: nifinodegroupautoscaler-sample\n  namespace: clusters\nspec:\n  clusterRef:\n    name: cluster\n    namespace: clusters\n  nodeConfigGroupId: auto_scaling\n  readOnlyConfig:\n    nifiProperties:\n      overrideConfigs: |\n        nifi.ui.banner.text=NiFiKop - Scale Group\n  # nodeConfig:\n  #   nodeSelector:\n  #     node_type: high-mem\n  nodeLabelsSelector: \n    matchLabels:\n      nifi_cr: cluster\n      nifi_node_group: auto-scaling\n  upscaleStrategy: simple\n  downscaleStrategy: lifo\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Encryption Update with an InitContainer - YAML\nDESCRIPTION: This YAML snippet defines a Kubernetes initContainer that runs the Apache NiFi Toolkit's encrypt-config.sh script inside the pod to update the flow.xml.gz and flow.json.gz files on startup. It mounts necessary data and configuration volumes, extracts the sensitive properties key from nifi.properties, and performs in-place encryption updates to use the new algorithm (NIFI_PBKDF2_AES_GCM_256). Dependencies are the nifikop operator, NiFi Toolkit image, required files at the specified paths, and correct volume mounts; successful execution ensures that sensitive properties meet new security requirements on upgrade.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/7_upgrade_guides/4_v1.3.1_to_v1.4.0.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ninitContainers:\n  - image: \"apache/nifi-toolkit:latest\"\n    name: nifi-toolkit\n    imagePullPolicy: Always\n    command:\n      - \"sh\"\n      - \"-c\"\n      - \"NIFI_SENSITIVE_PROPS_KEY=$(grep 'nifi.sensitive.props.key' /opt/nifi/nifi-current/conf/nifi.properties | cut -d'=' -f2) && bin/encrypt-config.sh -n /opt/nifi/nifi-current/conf/nifi.properties -f /opt/nifi/data/flow.json.gz -x -A NIFI_PBKDF2_AES_GCM_256 -s $NIFI_SENSITIVE_PROPS_KEY; bin/encrypt-config.sh -n /opt/nifi/nifi-current/conf/nifi.properties -f /opt/nifi/data/flow.xml.gz -x -A NIFI_PBKDF2_AES_GCM_256 -s $NIFI_SENSITIVE_PROPS_KEY\"\n    volumeMounts:\n      - name: data\n        mountPath: /opt/nifi/data\n      - name: conf\n        mountPath: /opt/nifi/nifi-current/conf\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Cluster External Service for Istio Integration\nDESCRIPTION: Shows the relevant section within a NiFi cluster deployment YAML to define an external Kubernetes Service of type ClusterIP. This service, named 'nifi-cluster', exposes port 8443 and maps it to the internal NiFi listener named 'https'. This service FQDN is used as the destination host in the HTTPS VirtualService.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nspec:  \n  externalServices:  \n    - name: \"nifi-cluster\"\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8443\n            internalListenerName: \"https\"\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Secret for Sensitive Parameters via kubectl Console\nDESCRIPTION: This console command demonstrates how to create a Kubernetes generic secret named 'secret-params' with two literal key-value pairs. This secret can then be referenced within a NifiParameterContext for managing sensitive parameter values in NiFi dataflows deployed by NiFiKop. It requires kubectl CLI access with permissions to create secrets in the specified namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl create secret generic secret-params \\\n    --from-literal=secret1=yop \\\n    --from-literal=secret2=yep \\\n    -n nifikop\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Cluster Configuration (YAML)\nDESCRIPTION: This YAML snippet defines the configuration for a NiFi cluster. It specifies the API version, kind, metadata (name), and various specifications for the cluster.  These specs include service definitions, cluster manager settings, image configurations, node configurations, and listener configurations. The configuration sets up a NiFi cluster with two nodes, persistent storage, and various listeners.  Dependencies are Kubernetes and the NiFi operator.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/5_references/1_nifi_cluster/1_nifi_cluster.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n    annotations:\n      tyty: ytyt\n    labels:\n      cluster-name: simplenifi\n      tete: titi\n  clusterManager: zookeeper\n  zkAddress: \"zookeeper.zookeeper:2181\"\n  zkPath: /simplenifi\n  externalServices:\n    - metadata:\n        annotations:\n          toto: tata\n        labels:\n          cluster-name: driver-simplenifi\n          titi: tutu\n      name: driver-ip\n      spec:\n        portConfigs:\n          - internalListenerName: http\n            port: 8080\n        type: ClusterIP\n  clusterImage: \"apache/nifi:1.28.0\"\n  initContainerImage: \"bash:5.2.2\"\n  oneNifiNodePerNode: true\n  readOnlyConfig:\n    nifiProperties:\n      overrideConfigs: |\n        nifi.sensitive.props.key=thisIsABadSensitiveKeyPassword\n  pod:\n    annotations:\n      toto: tata\n    labels:\n      cluster-name: simplenifi\n      titi: tutu\n  nodeConfigGroups:\n    default_group:\n      imagePullPolicy: IfNotPresent\n      isNode: true\n      serviceAccountName: default\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          reclaimPolicy: Delete\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      resourcesRequirements:\n        limits:\n          cpu: \"1\"\n          memory: 2Gi\n        requests:\n          cpu: \"1\"\n          memory: 2Gi\n  nodes:\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n    - id: 2\n      nodeConfigGroup: \"default_group\"\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - containerPort: 8080\n        type: http\n        name: http\n      - containerPort: 6007\n        type: cluster\n        name: cluster\n      - containerPort: 10000\n        type: s2s\n        name: s2s\n      - containerPort: 9090\n        type: prometheus\n        name: prometheus\n      - containerPort: 6342\n        type: load-balance\n        name: load-balance\n\n```\n\n----------------------------------------\n\nTITLE: Adding KEDA Helm Repository - Console\nDESCRIPTION: Adds the official KEDA Helm chart repository to your local Helm configuration. This makes the KEDA charts available for installation using Helm. Requires Helm to be installed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nhelm repo add kedacore https://kedacore.github.io/charts\n```\n\n----------------------------------------\n\nTITLE: ParameterContextUpdateRequest Structure\nDESCRIPTION: Represents a request to update a NiFi Parameter Context, including identifiers, timestamps, status flags, failure reasons, progress percentage, and retry count. Facilitates tracking and managing update operations.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/5_references/4_nifi_parameter_context.md#_snippet_6\n\nLANGUAGE: YAML\nCODE:\n```\nParameterContextUpdateRequest:\n  id: string\n  uri: string\n  submissionTime: string\n  lastUpdated: string\n  complete: bool\n  failureReason: string\n  percentCompleted: int32\n  state: string\n  notFound: bool\n  notFoundRetryCount: int32\n```\n\n----------------------------------------\n\nTITLE: Accessing Prometheus via Kubernetes Port Forwarding - Console\nDESCRIPTION: This command sets up a local port forward from port 9090 on the user's machine to the Prometheus operated service within the 'monitoring-system' namespace. It enables local browser access to the Prometheus web UI at http://localhost:9090 for querying NiFi metrics currently scraped by Prometheus.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_11\n\nLANGUAGE: console\nCODE:\n```\nkubectl port-forward -n monitoring-system service/prometheus-operated 9090:9090\n```\n\n----------------------------------------\n\nTITLE: Getting UID/GID for OpenShift\nDESCRIPTION: This bash script retrieves the user ID (UID) required for running Zookeeper on OpenShift. It uses `kubectl` and `jsonpath` to extract the UID from the annotations of the `zookeeper` namespace.  This is necessary for setting the `runAsUser` and `fsGroup` in the Zookeeper deployment. It assumes the zookeeper namespace exists. This relies on the specific OpenShift annotations for retrieving the user ID.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nzookeper_uid=$(kubectl get namespace zookeeper -o=jsonpath='{.metadata.annotations.openshift\\.io/sa\\.scc\\.supplemental-groups}' | sed 's//10000$//' | tr -d '[:space:]')\n```\n\n----------------------------------------\n\nTITLE: Conversion Webhook Configuration in NiFiKop CRDs Using YAML\nDESCRIPTION: This YAML snippet provides an example configuration to enable the conversion webhook feature for NiFiKop CRDs, allowing resource version conversion between v1alpha1 and v1. It specifies annotations to inject CA certificates from cert-manager, and the conversion strategy using a webhook service with a defined path and conversion review versions. This configuration requires the namespace, certificate name, and webhook service name placeholders to be replaced with specific deployment values, important for handling CRD upgrades and backward compatibility.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/2_deploy_nifikop/1_quick_start.md#_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n```\n\n----------------------------------------\n\nTITLE: Defining a NiFi Cluster Kubernetes Resource (YAML)\nDESCRIPTION: This YAML manifest defines a `NifiCluster` Custom Resource named `simplenifi`. It specifies the desired state for deploying a NiFi cluster, including connection details for Zookeeper, the container image to use, default and specific node configurations (storage, resources, metadata), network listeners (HTTP, cluster, S2S), and external service definitions. This resource is processed by the NiFi operator to provision and manage the NiFi cluster within a Kubernetes environment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/5_references/1_nifi_cluster/1_nifi_cluster.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n    annotations:\n      tyty: ytyt\n    labels:\n      tete: titi  \n  pod:\n    annotations:\n      toto: tata\n    labels:\n      titi: tutu\n  zkAddress: 'zookeepercluster-client.zookeeper:2181'\n  zkPath: '/simplenifi'\n  clusterImage: 'apache/nifi:1.11.3'\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      podMetadata:\n        annotations:\n          node-annotation: \"node-annotation-value\"\n        labels:\n          node-label: \"node-label-value\"\n      externalVolumeConfigs:\n        - name: example-volume\n          mountPath: \"/opt/nifi/example\"\n          secret:\n            secretName: \"raw-controller\"\n      storageConfigs:\n        - mountPath: '/opt/nifi/nifi-current/logs'\n          name: logs\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: 'standard'\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: 'default'\n      resourcesRequirements:\n        limits:\n          cpu: '2'\n          memory: 3Gi\n        requests:\n          cpu: '1'\n          memory: 1Gi\n  nodes:\n    - id: 1\n      nodeConfigGroup: 'default_group'\n    - id: 2\n      nodeConfigGroup: 'default_group'\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: 'http'\n        name: 'http'\n        containerPort: 8080\n      - type: 'cluster'\n        name: 'cluster'\n        containerPort: 6007\n      - type: 's2s'\n        name: 's2s'\n        containerPort: 10000\n  externalServices:\n    - name: 'clusterip'\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8080\n            internalListenerName: 'http'\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Secret for Basic Authentication (kubectl CLI)\nDESCRIPTION: This command creates a Kubernetes generic secret named 'nifikop-credentials' in the 'nifikop-nifi' namespace. The secret includes username, password, and optionally a CA certificate file. This secret is referenced by the NifiCluster resource for basic authentication to the NiFi cluster REST API. The username and password files must contain the credentials allowing the operator to authenticate. The presence of 'ca.crt' is optional and can be used to verify the server certificate when needed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/3_external_cluster.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nkubectl create secret generic nifikop-credentials \\\n  --from-file=username=./secrets/username\\\n  --from-file=password=./secrets/password\\\n  --from-file=ca.crt=./secrets/ca.crt\\\n  -n nifikop-nifi\n```\n\n----------------------------------------\n\nTITLE: Overriding NiFiCluster Configuration Using ConfigMap and Secret - YAML\nDESCRIPTION: This YAML snippet demonstrates how to override NiFi configuration files within a Kubernetes-based NiFiCluster by specifying 'overrideConfigMap', 'overrideSecretConfig', and inline 'overrideConfigs' fields. The example shows referencing specific data keys and resource names/namespaces for both ConfigMap and Secret. Dependencies include an existing Kubernetes NiFiCluster custom resource, ConfigMap and Secret objects properly defined in the target namespaces. Key parameters are 'overrideConfigMap' (ConfigMap name, namespace, data key), 'overrideSecretConfig' (Secret or ConfigMap name, namespace, data key), and 'overrideConfigs' (literal override string). Inputs are resource metadata and configuration strings; outputs are merged NiFi configuration with Secret taking precedence as specified. Ensure referenced resources and data keys exist, and consider potential operator behavior impacts with overrides.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\nnifiProperties:\n      # Additionnal nifi.properties configuration that will override the one produced based on template and\n      # configuration\n      overrideConfigMap:\n        # The key of the value,in data content, that we want use.\n        data: nifi.properties\n        # Name of the configmap that we want to refer.\n        name: raw\n        # Namespace where is located the secret that we want to refer.\n        namespace: nifikop\n      # Additionnal nifi.properties configuration that will override the one produced based\n      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n      overrideSecretConfig:\n        # The key of the value,in data content, that we want use.\n        data: nifi.properties\n        # Name of the configmap that we want to refer.\n        name: raw\n        # Namespace where is located the secret that we want to refer.\n        namespace: nifikop\n      # Additionnal nifi.properties configuration that will override the one produced based\n      #\ton template, configurations and overrideConfigMap\n      overrideConfigs: |\n        nifi.ui.banner.text=NiFiKop\n        nifi.sensitive.props.key=thisIsABadSensitiveKeyPassword\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiUser Resource in Kubernetes (YAML)\nDESCRIPTION: YAML manifest demonstrating the creation of a `NifiUser` custom resource. It specifies the Kubernetes resource name (`aguitton`), the corresponding NiFi identity (`alexandre.guitton@konpyutaika.com`), the target NiFi cluster (`nc` in `nifikop` namespace), disables Java Keystore (JKS) inclusion and certificate creation (`createCert: false`), and grants read access to the `/data` resource for components of type 'process-groups'.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/2_manage_users_and_accesses/1_users_management.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: aguitton\nspec:\n  # identity field is use to define the user identity on NiFi cluster side,\n  #\tit use full when the user's name doesn't suite with Kubernetes resource name.\n  identity: alexandre.guitton@konpyutaika.com\n  # Contains the reference to the NifiCluster with the one the registry client is linked.\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  # Whether or not the the operator also include a Java keystore format (JKS) with you secret\n  includeJKS: false\n  # Whether or not a certificate will be created for this user.\n  createCert: false\n  # defines the list of access policies that will be granted to the group.\n  accessPolicies:\n      # defines the kind of access policy, could be \"global\" or \"component\".\n    - type: component\n      # defines the kind of action that will be granted, could be \"read\" or \"write\"\n      action: read\n      # resource defines the kind of resource targeted by this access policies, please refer to the following page:\n      #\thttps://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#access-policies\n      resource: /data\n      # componentType is used if the type is \"component\", it's allow to define the kind of component on which is the\n      # access policy\n      componentType: \"process-groups\"\n      # componentId is used if the type is \"component\", it's allow to define the id of the component on which is the\n      # access policy\n      componentId: \"\"\n```\n\n----------------------------------------\n\nTITLE: Describing NiFiKop Cluster Status with kubectl in Console\nDESCRIPTION: This command retrieves the status and description of the specified NifiCluster resource, focusing on the status of nodes and ongoing actions such as graceful removal. The output provides detailed insights into the state transitions and decommissioning steps, helping operators monitor and troubleshoot scale down operations. No arguments are required except the resource name.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nkubectl describe nificluster simplenifi\n\n...\nStatus:\n  Nodes State:\n    ...\n    2:\n      Configuration State:  ConfigInSync\n      Graceful Action State:\n        Action State:   GracefulDownscaleRequired\n        Error Message:\n    ...\n...\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio VirtualService for HTTP NiFi Routing\nDESCRIPTION: This YAML defines a VirtualService that works with the Gateway to route all intercepted HTTP requests to the NiFi service on port 8080. It handles URI paths with the '/' prefix.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: nifi-vs\nspec:\n  gateways:\n  - nifi-gateway\n  hosts:\n  - nifi.my-domain.com\n  http:\n  - match:\n    - uri:\n        prefix: /\n    route:\n    - destination:\n        host: nifi\n        port:\n          number: 8080\n```\n\n----------------------------------------\n\nTITLE: Configuring Node Groups in NiFiKop Using YAML\nDESCRIPTION: This YAML snippet demonstrates how to define multiple node configuration groups under the nodeConfigGroups field in a NiFiKop custom resource. Each group specifies resource requirements, provenance storage, user ID, and service account for pods running the NiFi application. Dependencies include a working NiFiKop operator and a Kubernetes cluster. Key parameters are resource limits, requests, provenanceStorage size, and serviceAccountName. The output is a NiFiKop CRD that allows reusable grouping of node specs across the cluster. Each group is referenced by name when declaring nodes; group configs are not automatically validated beyond syntax.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  # nodeConfigGroups specifies multiple node configs with unique name\n  nodeConfigGroups:\n    default_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      serviceAccountName: \"default\"\n      # resourceRequirements works exactly like Container resources, the user can specify the limit and the requests\n      # through this property\n      # https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 3Gi\n    high_mem_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      serviceAccountName: \"default\"\n      # resourceRequirements works exactly like Container resources, the user can specify the limit and the requests\n      # through this property\n      # https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 30Gi\n        requests:\n          cpu: \"1\"\n          memory: 30Gi\n```\n\n----------------------------------------\n\nTITLE: Creating NifiDataflow YAML\nDESCRIPTION: This YAML defines a NifiDataflow CRD, which deploys and manages the NiFi dataflow.  It references the parent process group, bucket and flow IDs, the flow version, sync mode, various refs, and an update strategy. The `syncMode` controls how the operator manages the dataflow lifecycle.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: demo\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Creating NifiRegistryClient CRD in YAML\nDESCRIPTION: Defines a NifiRegistryClient custom resource that connects NiFiKop to a NiFi Registry instance. This CRD requires cluster reference information and the URI of the NiFi Registry service. It enables dataflow versioning and lifecycle management by integrating NiFi Registry into the NiFiKop operator's control plane.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/3_tasks/3_nifi_dataflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiRegistryClient\nmetadata:\n  name: registry-client-example\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Registry client managed by NiFiKop\"\n  uri: \"http://nifi.hostname.com:18080\"\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Kubernetes StorageClass YAML\nDESCRIPTION: Defines a Kubernetes StorageClass resource for persistent volume provisioning using Google Compute Engine persistent disks. The manifest specifies the storage type, reclaim policy, and volume binding mode to optimize volume provisioning delays. This StorageClass is intended to be referenced in the NiFiCluster Custom Resource for persistent storage allocation. The resource must be created in Kubernetes before use.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: exampleStorageclass\nparameters:\n  type: pd-standard\nprovisioner: kubernetes.io/gce-pd\nreclaimPolicy: Delete\nvolumeBindingMode: WaitForFirstConsumer\n```\n\n----------------------------------------\n\nTITLE: Defining Prometheus Resource (YAML)\nDESCRIPTION: This YAML manifest defines the `Prometheus` custom resource managed by the Prometheus Operator. It configures the Prometheus instance itself, including settings for logging, evaluation/scrape intervals, resource requests, and specifies which `PodMonitor` and `ServiceMonitor` resources it should use to discover scrape targets based on labels.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: Prometheus\nmetadata:\n  name: prometheus\n  namespace: monitoring-system\nspec:\n  enableAdminAPI: false\n  evaluationInterval: 30s\n  logLevel: debug\n  podMonitorSelector:\n    matchExpressions:\n    - key: release\n      operator: In\n      values:\n      - prometheus\n  resources:\n    requests:\n      memory: 400Mi\n  scrapeInterval: 30s\n  serviceAccountName: prometheus\n  serviceMonitorSelector:\n    matchExpressions:\n    - key: app\n      operator: In\n      values:\n      - nifi-cluster\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFi Cluster on OpenShift\nDESCRIPTION: kubectl command to create a NiFi cluster on OpenShift using the updated configuration file with proper security context settings.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create -n nifi -f config/samples/openshift.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Cluster for Prometheus and Autoscaling Group (YAML)\nDESCRIPTION: Partial YAML configuration for a NiFiKop `NifiCluster` custom resource. It adds an internal listener on port 9090 to expose Prometheus metrics and defines a `NodeConfigGroup` named 'auto_scaling' which specifies resource requirements and persistent storage configurations for the NiFi nodes intended for autoscaling.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n...\nspec:\n  ...\n  listenersConfig:\n    internalListeners:\n    ...\n    - containerPort: 9090\n      name: prometheus\n      type: prometheus \n    ...\n  ...\n  nodeConfigGroups:\n    auto_scaling:\n      isNode: true\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 2Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n      serviceAccountName: external-dns\n      storageConfigs:\n        - mountPath: /opt/nifi/nifi-current/logs\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/data\n          name: data\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/extensions\n          name: extensions-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/flowfile_repository\n          name: flowfile-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/nifi-current/conf\n          name: conf\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/content_repository\n          name: content-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/provenance_repository\n          name: provenance-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n  ...\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image - Bash\nDESCRIPTION: Builds a Docker image from the current branch of the NiFiKop project. This command uses the `make docker-build` command. It requires Docker to be installed and configured and it pulls relevant dependencies. The image can then be pushed to a registry.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/6_contributing/1_developer_guide.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nexport DOCKER_REGISTRY_BASE={your-docker-repo}\nmake docker-build\n```\n\n----------------------------------------\n\nTITLE: Configuring Issuer for Let's Encrypt (YAML)\nDESCRIPTION: This YAML snippet shows an example of setting up a Let's Encrypt issuer using `cert-manager`. It requires a valid email address for contact and defines the ACME server. It uses HTTP01 challenge using nginx as a solver. Dependencies include `cert-manager` and a DNS server with external DNS deployed.  The output is an issuer resource within the Kubernetes cluster ready to issue certificates from Let's Encrypt.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: cert-manager.io/v1alpha2\nkind: Issuer\nmetadata:\n  name: letsencrypt-staging\nspec:\n  acme:\n    # You must replace this email address with your own.\n    # Let's Encrypt will use this to contact you about expiring\n    # certificates, and issues related to your account.\n    email: <your email address>\n    server: https://acme-staging-v02.api.letsencrypt.org/directory\n    privateKeySecretRef:\n      # Secret resource used to store the account's private key.\n      name: example-issuer-account-key\n    # Add a single challenge solver, HTTP01 using nginx\n    solvers:\n      - http01:\n          ingress:\n            ingressTemplate:\n              metadata:\n                annotations:\n                  \"external-dns.alpha.kubernetes.io/ttl\": \"5\"\n```\n\n----------------------------------------\n\nTITLE: Viewing Kubernetes Pod Ports Configuration in YAML\nDESCRIPTION: Shows how the internal listeners defined in the NiFiCluster specification are reflected in the `ports` configuration of the generated Kubernetes Pods. This is an example of the resulting Pod definition, not a configuration snippet to be applied directly.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n    ports:\n    - containerPort: 8443\n      name: https\n      protocol: TCP\n    - containerPort: 6007\n      name: cluster\n      protocol: TCP\n    - containerPort: 6342\n      name: load-balance\n      protocol: TCP\n    - containerPort: 10000\n      name: s2s\n      protocol: TCP\n    - containerPort: 9090\n      name: prometheus\n      protocol: TCP\n```\n\n----------------------------------------\n\nTITLE: Updating NifiCluster InitContainerImage to bash - YAML\nDESCRIPTION: Shows the required YAML manifest change to update the initContainerImage of a NifiCluster resource from 'busybox' to 'bash' version '5.2.2'. This change is necessary for compatibility with version 0.15.0 and later of the nifikop operator, ensuring the init container has a bash shell available. Similar prerequisites as the previous snippet apply, with the replaced image repository and tag values reflecting the new default.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/7_upgrade_guides/2_v0.14.1_to_v0.15.0.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  initContainerImage:\n    repository: bash\n    tag: \"5.2.2\"\n```\n\n----------------------------------------\n\nTITLE: Installing Bitnami Zookeeper Chart with Helm in Bash\nDESCRIPTION: This snippet provides bash commands to add the Bitnami Helm chart repository and install a Zookeeper cluster on a Kubernetes cluster. It configures resource requests and limits for CPU and memory, sets the storage class, enables network policy, and specifies a 3-node replication. This setup prepares Zookeeper as a prerequisite external service for NiFi without operator management. Helm and Kubernetes CLI are required.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhelm repo add bitnami https://charts.bitnami.com/bitnami\n```\n\nLANGUAGE: bash\nCODE:\n```\nhelm install zookeeper bitnami/zookeeper \\\n    --namespace=zookeeper \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set global.storageClass=standard \\\n    --set networkPolicy.enabled=true \\\n    --set replicaCount=3 \\\n    --create-namespace\n```\n\n----------------------------------------\n\nTITLE: Executing the NiFiKop CRD Migration Script\nDESCRIPTION: Shows the command to execute the Node.js migration script using `npm start`. It requires passing the NiFiKop resource type to migrate (e.g., 'cluster', 'dataflow') via the `--type` argument and the target Kubernetes namespace via the `--namespace` argument.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nnpm start -- --type=<NIFIKOP_RESOURCE> --namespace=<K8S_NAMESPACE>\n```\n\n----------------------------------------\n\nTITLE: Applying NiFi Cluster Configuration in Kubernetes\nDESCRIPTION: Shell command to apply the updated NiFi cluster configuration to Kubernetes, which will trigger the scale-up operation to add the new node.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nkubectl -n nifi apply -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing Nifikop Helm Chart with a Release Name\nDESCRIPTION: Provides the basic `helm install` command structure to deploy the Nifikop chart from the `konpyutaika/nifikop` repository. The user must replace `<release name>` with their desired name for the Helm release.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nhelm install <release name> konpyutaika/nifikop\n```\n\n----------------------------------------\n\nTITLE: Defining NifiNodeGroupAutoscaler Resource (YAML)\nDESCRIPTION: This YAML snippet provides an example definition of a `NifiNodeGroupAutoscaler` Kubernetes resource. It demonstrates how to reference a target `NifiCluster` and node group, specify the labels used to identify nodes within that group, and configure the desired upscale (`simple`) and downscale (`lifo`) strategies. This resource is used by the NiFiKop operator to automatically manage the replica count of the specified node group.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/5_references/7_nifi_nodegroup_autoscaler.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiNodeGroupAutoscaler\nmetadata:\n  name: nifinodegroupautoscaler-sample\nspec:\n  # contains the reference to the NifiCluster with the one the node group autoscaler is linked.\n  clusterRef:\n    name: nificluster-name\n    namespace: nifikop\n  # defines the id of the NodeConfig contained in NifiCluster.Spec.NodeConfigGroups\n  nodeConfigGroupId: default-node-group\n  # The selector used to identify nodes in NifiCluster.Spec.Nodes this autoscaler will manage\n  # Use Node.Labels in combination with this selector to clearly define which nodes will be managed by this autoscaler \n  nodeLabelsSelector: \n    matchLabels:\n      nifi_cr: nificluster-name\n      nifi_node_group: default-node-group\n  # the strategy used to decide how to add nodes to a nifi cluster\n  upscaleStrategy: simple\n  # the strategy used to decide how to remove nodes from an existing cluster\n  downscaleStrategy: lifo\n```\n\n----------------------------------------\n\nTITLE: Deploying KEDA ScaledObject for NiFi Node Auto-Scaling (YAML configuration)\nDESCRIPTION: Defines a KEDA ScaledObject to manage the auto-scaling of a NiFi node group based on Prometheus metrics. It includes configuration options for scaling targets, polling intervals, cooldown periods, replica counts, fallback options, and optional advanced settings. Dependencies include the Kubernetes Cluster, KEDA operator, and Prometheus metrics server.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: cluster\n  namespace: clusters\nspec:\n  scaleTargetRef:\n    apiVersion:    nifi.konpyutaika.com/v1alpha1     # Optional. Default: apps/v1\n    kind:          NifiNodeGroupAutoscaler           # Optional. Default: Deployment\n    name:          nifinodegroupautoscaler-sample    # Mandatory. Must be in the same namespace as the ScaledObject\n    envSourceContainerName: nifi                     # Optional. Default: .spec.template.spec.containers[0]\n  pollingInterval:  30                               # Optional. Default: 30 seconds\n  cooldownPeriod:   300                              # Optional. Default: 300 seconds\n  idleReplicaCount: 0                                # Optional. Default: ignored, must be less than minReplicaCount \n  minReplicaCount:  1                                # Optional. Default: 0\n  maxReplicaCount:  3                                # Optional. Default: 100\n  fallback:                                          # Optional. Section to specify fallback options\n    failureThreshold: 5                              # Mandatory if fallback section is included\n    replicas: 1                                      # Mandatory if fallback section is included\n  # advanced:                                          # Optional. Section to specify advanced options\n  #   restoreToOriginalReplicaCount: true              # Optional. Whether the target resource should be scaled back to original replicas count, after the ScaledObject is deleted\n  #   horizontalPodAutoscalerConfig:                   # Optional. Section to specify HPA related options\n  #     name: {name-of-hpa-resource}                   # Optional. Default: keda-hpa-{scaled-object-name}\n  #     behavior:                                      # Optional. Use to modify HPA's scaling behavior\n  #       scaleDown:\n  #         stabilizationWindowSeconds: 300 <--- \n  #         policies:\n  #         - type: Percent\n  #           value: 100\n  #           periodSeconds: 15\n  triggers:\n    - type: prometheus\n      metadata:\n        serverAddress: http://prometheus-operated.monitoring-system.svc:9090\n        metricName: http_requests_total\n        threshold: <threshold value>\n        query: <prometheus query>\n```\n\n----------------------------------------\n\nTITLE: Deploying a NiFi Cluster using Kubectl (Bash)\nDESCRIPTION: Deploys a NiFi cluster into the 'nifi' namespace by applying the configuration defined in the 'config/samples/simplenificluster.yaml' file. This command assumes the YAML file has been pre-configured, including pointing to the correct Zookeeper service endpoint. Requires kubectl, a configured Kubernetes context, and a running Zookeeper cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Add your zookeeper svc name to the configuration\nkubectl create -n nifi -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining External Services with ClusterIP in YAML\nDESCRIPTION: This YAML snippet demonstrates how to define an external service configuration, specifically using the ClusterIP service type. It showcases defining a port configuration and associated metadata (annotations and labels) for the service. This config allows exposing a NiFi listener as a Kubernetes ClusterIP service.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/5_references/1_nifi_cluster/7_external_service_config.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n  externalServices:\n    - name: \"clusterip\"\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8080\n            internalListenerName: \"http\"\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n\n----------------------------------------\n\nTITLE: Adding the Bitnami Helm Repository (Bash)\nDESCRIPTION: Adds the Bitnami Helm chart repository to your local Helm configuration using the 'helm repo add' command. This repository is required to install the recommended Zookeeper chart in the next step.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhelm repo add bitnami https://charts.bitnami.com/bitnami\n```\n\n----------------------------------------\n\nTITLE: Test Kubectl NiFiKop Plugin\nDESCRIPTION: This command executes the installed kubectl-nifikop plugin and displays its usage instructions and available commands.  It assumes the plugin has been successfully installed in the user's PATH.  The output shows available subcommands such as nificluster, nificonnection, etc.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/2_deploy_nifikop/3_kubectl_plugin.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\n$ kubectl nifikop\nUsage:\n  nifikop [command]\n\nAvailable Commands:\n  completion          Generate the autocompletion script for the specified shell\n  help                Help about any command\n  nificluster         \n  nificonnection      \n  nifidataflow        \n  nifigroupautoscaler \n  nifiregistryclient  \n  nifiuser            \n  nifiusergroup \n```\n\n----------------------------------------\n\nTITLE: Configuring NiFiCluster with SSL (YAML)\nDESCRIPTION: This YAML configuration defines a `NifiCluster` resource, setting up SSL for secure communication. It includes configurations for internal listeners, specifically setting `https` type for the container port 8443. The configuration uses `listenersConfig.sslSecrets` which allows you to specify whether to create the SSL certificates.  `readOnlyConfig.nifiProperties.webProxyHosts` lists allowed hostnames for the NiFi web UI in a secured environment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\n...\nspec:\n  ...\n  readOnlyConfig:\n    # NifiProperties configuration that will be applied to the node.\n    nifiProperties:\n      webProxyHosts:\n        - nifistandard2.trycatchlearn.fr:8443\n        ...\n  ...\n  listenersConfig:\n    internalListeners:\n      - type: \"https\"\n        name: \"https\"\n        containerPort: 8443\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n```\n\n----------------------------------------\n\nTITLE: Defining Prometheus RBAC Resources in Kubernetes (YAML)\nDESCRIPTION: Defines the necessary Kubernetes RBAC resources (`ServiceAccount`, `ClusterRole`, `ClusterRoleBinding`) for Prometheus within the `monitoring-system` namespace. This grants the Prometheus service account permissions to discover and scrape metrics from nodes, pods, services, endpoints, configmaps, and ingresses across the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: prometheus\n  namespace: monitoring-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: prometheus\nrules:\n- apiGroups: [\"\"]\n  resources:\n  - nodes\n  - nodes/metrics\n  - services\n  - endpoints\n  - pods\n  verbs: [\"get\", \"list\", \"watch\"]\n- apiGroups: [\"\"]\n  resources:\n  - configmaps\n  verbs: [\"get\"]\n- apiGroups:\n  - networking.k8s.io\n  resources:\n  - ingresses\n  verbs: [\"get\", \"list\", \"watch\"]\n- nonResourceURLs: [\"/metrics\"]\n  verbs: [\"get\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: prometheus\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: prometheus\nsubjects:\n- kind: ServiceAccount\n  name: prometheus\n  namespace: monitoring-system\n```\n\n----------------------------------------\n\nTITLE: Starting Minikube with Specific Memory Allocation (Console)\nDESCRIPTION: Command to start a Minikube cluster, specifying a minimum memory allocation of 4000MB. This is recommended for running NiFiKop effectively.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/2_deploy_nifikop/1_quick_start.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nminikube start --memory=4000\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Registry Clients (JSON)\nDESCRIPTION: Shows the structure for configuring `NiFiRegistryClient` custom resources. It defines properties like description, enabling, endpoint URL, and name, allowing the operator to connect to NiFi Registry instances.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/helm/nifi-cluster/README.md#_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n[{\"description\":\"Default NiFi Registry client\",\"enabled\":false,\"endpoint\":\"http://nifi-registry\",\"name\":\"default\"},{\"description\":\"Alternate NiFi Registry client\",\"enabled\":false,\"endpoint\":\"http://nifi-registry\",\"name\":\"alternate\"}]\n```\n\n----------------------------------------\n\nTITLE: Install NiFiKop Kubectl Plugin via Make\nDESCRIPTION: This command uses `make` to build the `kubectl-nifikop` executable and then copies the resulting binary into `/usr/local/bin`. This directory is typically included in the system's PATH, making the plugin accessible as a `kubectl` subcommand. It requires a Unix-like operating system and `sudo` privileges for the copy operation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/2_deploy_nifikop/3_kubectl_plugin.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nmake kubectl-nifikop && sudo cp ./bin/kubectl-nifikop /usr/local/bin\n```\n\n----------------------------------------\n\nTITLE: Creating NifiUserGroup Resource in Kubernetes (YAML)\nDESCRIPTION: This YAML snippet defines a NifiUserGroup custom resource object. It demonstrates how to specify the group's identity, reference the target NiFi cluster, list member users by referencing NifiUser resources, and assign initial access policies. This resource is managed by the nifikop operator.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/5_references/6_nifi_usergroup.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  identity: \"My Special Group\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n    - name: nc-controller.nifikop.mgt.cluster.local\n  accessPolicies:\n    - type: global\n      action: read\n      resource: /counters\n```\n\n----------------------------------------\n\nTITLE: Configuring an Istio Gateway for HTTP Traffic to NiFi\nDESCRIPTION: Defines an Istio Gateway to intercept incoming HTTP requests destined for nifi.my-domain.com on port 80. It specifies the ingress gateway selector and sets up the server to listen on HTTP port with the host matching the domain. This allows external access to the NiFi cluster via HTTP.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: nifi-gateway\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - nifi.my-domain.com\n```\n\n----------------------------------------\n\nTITLE: Example package.json for Migration Script (JSON)\nDESCRIPTION: Provides a complete example `package.json` file for the migration project. It includes project metadata, the `start` script for running the migration, and lists the required Node.js dependencies (`@kubernetes/client-node`, `minimist`).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/7_upgrade/1_v0.7.x_to_v0.8.0.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"nifikop_crd_migration\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Script to migrate from the old CRDs to the new CRDs.\",\n  \"main\": \"index.js\",\n  \"scripts\": {\n    \"start\": \"node --no-warnings index.js\",\n    \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\"\n  },\n  \"keywords\": [\n    \"K8S\",\n    \"NiFiKop\",\n    \"CRDs\"\n  ],\n  \"license\": \"ISC\",\n  \"dependencies\": {\n    \"@kubernetes/client-node\": \"^0.16.3\",\n    \"minimist\": \"^1.2.6\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining NifiNodeGroupAutoscaler Resource\nDESCRIPTION: YAML manifest for creating a `NifiNodeGroupAutoscaler` custom resource (apiVersion `nifi.konpyutaika.com/v1alpha1`). It links to a specific NiFi cluster (`clusterRef`), targets the nodes associated with the 'auto_scaling' `nodeConfigGroupId`, optionally applies read-only NiFi property overrides (e.g., banner text), selects target nodes using labels (`nodeLabelsSelector`), and specifies the `upscaleStrategy` ('simple') and `downscaleStrategy` ('lifo') for managing the nodes in the group.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiNodeGroupAutoscaler\nmetadata:\n  name: nifinodegroupautoscaler-sample\n  namespace: clusters\nspec:\n  # contains the reference to the NifiCluster with the one the node group autoscaler is linked.\n  clusterRef:\n    name: cluster\n    namespace: clusters\n  # defines the id of the NodeConfig contained in NifiCluster.Spec.NodeConfigGroups\n  nodeConfigGroupId: auto_scaling\n  # readOnlyConfig can be used to pass Nifi node config\n  # which has type read-only these config changes will trigger rolling upgrade\n  readOnlyConfig:\n    nifiProperties:\n      overrideConfigs: |\n        nifi.ui.banner.text=NiFiKop - Scale Group\n  # This is an example of a node config you can apply to each replica in this node group.\n  # Any settings here will override those in the configured nodeConfigGroupId\n#  nodeConfig:\n#    nodeSelector:\n#      node_type: high-mem\n  # The selector used to identify nodes in NifiCluster.Spec.Nodes this autoscaler will manage\n  # Use Node.Labels in combination with this selector to clearly define which nodes will be managed by this autoscaler \n  nodeLabelsSelector: \n    matchLabels:\n      nifi_cr: cluster\n      nifi_node_group: auto-scaling\n  # the strategy used to decide how to add nodes to a nifi cluster\n  upscaleStrategy: simple\n  # the strategy used to decide how to remove nodes from an existing cluster\n  downscaleStrategy: lifo\n```\n\n----------------------------------------\n\nTITLE: Building the NiFiKop kubectl plugin using Make\nDESCRIPTION: This snippet shows how to compile the NiFiKop kubectl plugin using the Makefile target, then copying the resulting executable into the system's PATH for easy access. Dependencies include a working Makefile with the 'kubectl-nifikop' target and a UNIX-based operating system.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/2_deploy_nifikop/3_kubectl_plugin.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\nmake kubectl-nifikop && sudo cp ./bin/kubectl-nifikop /usr/local/bin\n```\n\n----------------------------------------\n\nTITLE: Deleting the Nifikop Helm Chart in Bash\nDESCRIPTION: This Bash command removes the Nifikop Helm release and all associated Kubernetes resources. Dependency: Helm CLI. Parameter: release name. Output: Helm chart and resources are deleted. Limit: CRDs are not removed by default and should be manually cleaned.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nhelm del nifikop\n```\n\n----------------------------------------\n\nTITLE: Deploying KEDA ScaledObject for NiFi Autoscaling\nDESCRIPTION: Creates a KEDA ScaledObject to define the autoscaling behavior for the NiFi cluster based on Prometheus metrics. Configures scaling parameters including min/max replicas, polling intervals, and the Prometheus query to use for scaling decisions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: cluster\n  namespace: clusters\nspec:\n  scaleTargetRef:\n    apiVersion:    nifi.konpyutaika.com/v1alpha1     # Optional. Default: apps/v1\n    kind:          NifiNodeGroupAutoscaler           # Optional. Default: Deployment\n    name:          nifinodegroupautoscaler-sample    # Mandatory. Must be in the same namespace as the ScaledObject\n    envSourceContainerName: nifi                     # Optional. Default: .spec.template.spec.containers[0]\n  pollingInterval:  30                               # Optional. Default: 30 seconds\n  cooldownPeriod:   300                              # Optional. Default: 300 seconds\n  idleReplicaCount: 0                                # Optional. Default: ignored, must be less than minReplicaCount \n  minReplicaCount:  1                                # Optional. Default: 0\n  maxReplicaCount:  3                                # Optional. Default: 100\n  fallback:                                          # Optional. Section to specify fallback options\n    failureThreshold: 5                              # Mandatory if fallback section is included\n    replicas: 1                                      # Mandatory if fallback section is included\n  # advanced:                                          # Optional. Section to specify advanced options\n  #   restoreToOriginalReplicaCount: true              # Optional. Whether the target resource should be scaled back to original replicas count, after the ScaledObject is deleted\n  #   horizontalPodAutoscalerConfig:                   # Optional. Section to specify HPA related options\n  #     name: {name-of-hpa-resource}                   # Optional. Default: keda-hpa-{scaled-object-name}\n  #     behavior:                                      # Optional. Use to modify HPA's scaling behavior\n  #       scaleDown:\n  #         stabilizationWindowSeconds: 300 <--- \n  #         policies:\n  #         - type: Percent\n  #           value: 100\n  #           periodSeconds: 15\n  triggers:\n    - type: prometheus\n      metadata:\n        serverAddress: http://prometheus-operated.monitoring-system.svc:9090\n        metricName: http_requests_total\n        threshold: <threshold value>\n        query: <prometheus query>\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Listeners (YAML)\nDESCRIPTION: This YAML snippet configures various types of NiFi listeners, including HTTPS, cluster, S2S, Prometheus, and load-balance.  It specifies the listener type, name, and container port. It also defines the configuration for SSL secrets, including the secret name and whether to create the secrets. This configuration is essential for defining network access to the NiFi instance.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/5_references/1_nifi_cluster/6_listeners_config.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n  listenersConfig:\n    internalListeners:\n      - type: \"https\"\n        name: \"https\"\n        containerPort: 8443\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n      - type: \"prometheus\"\n        name: \"prometheus\"\n        containerPort: 9090\n      - type: \"load-balance\"\n        name: \"load-balance\"\n        containerPort: 6342\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n```\n\n----------------------------------------\n\nTITLE: Defining a ClusterIP External Service for Kubernetes in YAML\nDESCRIPTION: This snippet defines a Kubernetes 'ClusterIP' external service named 'clusterip' with specific port configurations and metadata annotations and labels. It includes details such as service type, port number, and internal listener name for service targeting.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/5_references/1_nifi_cluster/7_external_service_config.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n  externalServices:\n    - name: \"clusterip\"\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8080\n            internalListenerName: \"http\"\n          - port: 7182\n            internalListenerName: \"my-custom-listener\"\n            protocol: TCP\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n\n----------------------------------------\n\nTITLE: Defining Prometheus Custom Resource (YAML)\nDESCRIPTION: YAML definition for a `Prometheus` custom resource managed by the Prometheus Operator. It creates a Prometheus instance named 'prometheus' in the 'monitoring-system' namespace, configuring scrape/evaluation intervals, log level, resource requests, the service account, and selectors to discover `PodMonitor` and `ServiceMonitor` resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: Prometheus\nmetadata:\n  name: prometheus\n  namespace: monitoring-system\nspec:\n  enableAdminAPI: false\n  evaluationInterval: 30s\n  logLevel: debug\n  podMonitorSelector:\n    matchExpressions:\n    - key: release\n      operator: In\n      values:\n      - prometheus\n  resources:\n    requests:\n      memory: 400Mi\n  scrapeInterval: 30s\n  serviceAccountName: prometheus\n  serviceMonitorSelector:\n    matchExpressions:\n    - key: app\n      operator: In\n      values:\n      - nifi-cluster\n```\n\n----------------------------------------\n\nTITLE: Creating a Kubernetes Secret for Sensitive Parameters using kubectl\nDESCRIPTION: Example `kubectl` command to create a Kubernetes generic secret named `secret-params` in the `nifikop` namespace. This secret holds key-value pairs (`secret1=yop`, `secret2=yep`) intended to be used as sensitive parameters within a `NifiParameterContext`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/3_tasks/3_nifi_dataflow.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl create secret generic secret-params \\\n    --from-literal=secret1=yop \\\n    --from-literal=secret2=yep \\\n    -n nifikop\n```\n\n----------------------------------------\n\nTITLE: Listing Deployed Helm Charts in Kubernetes\nDESCRIPTION: Helm command to list all deployed charts in the Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nhelm list\n```\n\n----------------------------------------\n\nTITLE: Creating Role for Kubernetes State Management\nDESCRIPTION: This YAML snippet defines a Kubernetes Role, granting permissions to manage `leases` and `configmaps` within a specified namespace. It uses `coordination.k8s.io` and the empty string to specify the apiGroups. It requires a Kubernetes cluster and `rbac.authorization.k8s.io/v1` to be supported.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: simplenifi\n  namespace: nifi\nrules:\n- apiGroups: [\"coordination.k8s.io\"]\n  resources: [\"leases\"]\n  verbs: [\"*\"]\n- apiGroups: [\"\"]\n  resources: [\"configmaps\"]\n  verbs: [\"*\"]\n```\n\n----------------------------------------\n\nTITLE: Adding and Updating Helm Repository for KEDA Installation Using Console\nDESCRIPTION: This snippet demonstrates the commands to add the official KEDA Helm chart repository and update the local Helm repositories cache. These commands are prerequisites for installing KEDA via Helm, enabling Kubernetes clusters to access the latest KEDA charts.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nhelm repo add kedacore https://kedacore.github.io/charts\n```\n\nLANGUAGE: console\nCODE:\n```\nhelm repo update\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Local Operator Development - Bash\nDESCRIPTION: This snippet sets environment variables which configure the operator's connection details and runtime behavior when running locally through an IDE or directly with the Go binary. Variables include KUBECONFIG (path to Kubernetes config), WATCH_NAMESPACE (namespace to monitor), POD_NAME (operator pod name), LOG_LEVEL (logging verbosity), and OPERATOR_NAME (name identifier).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/6_contributing/1_developer_guide.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nKUBECONFIG={path/to/your/kubeconfig}\nWATCH_NAMESPACE={namespace_to_watch}\nPOD_NAME={name for operator pod}\nLOG_LEVEL=Debug\nOPERATOR_NAME=ide\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Node with Resource and Storage - YAML\nDESCRIPTION: This YAML snippet defines a NiFi node with resource requirements (CPU and memory limits and requests) and storage configurations. It defines the required CPU and memory resources, and configure persistent volume claims (PVCs) for storage with configurations like mount path, metadata (labels and annotations), and the PVC specification, including access modes, storage class, and resource requests for storage capacity.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/5_references/1_nifi_cluster/4_node.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n    - id: 2\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        overrideConfigs: |\n          nifi.ui.banner.text=NiFiKop - Node 2\n      # node configuration\n      nodeConfig:\n        resourcesRequirements:\n          limits:\n            cpu: \"2\"\n            memory: 3Gi\n          requests:\n            cpu: \"1\"\n            memory: 1Gi\n        storageConfigs:\n          # Name of the storage config, used to name PV to reuse into sidecars for example.\n          - name: provenance-repository\n            # Path where the volume will be mount into the main nifi container inside the pod.\n            mountPath: \"/opt/nifi/provenance_repository\"\n            # Metadata to attach to the PVC that gets created\n            metadata:\n              labels:\n                my-label: my-value\n              annotations:\n                my-annotation: my-value\n            # Kubernetes PVC spec\n            # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n            pvcSpec:\n              accessModes:\n                - ReadWriteOnce\n              storageClassName: \"standard\"\n              resources:\n                requests:\n                  storage: 8Gi\n```\n\n----------------------------------------\n\nTITLE: Creating Basic Authentication Secret for NifiKop\nDESCRIPTION: A kubectl command to create a Kubernetes secret containing credentials for basic authentication to the NiFi cluster. The secret includes username, password, and optional CA certificate for secure connections.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/1_manage_clusters/3_external_cluster.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nkubectl create secret generic nifikop-credentials \\\n  --from-file=username=./secrets/username\\\n  --from-file=password=./secrets/password\\\n  --from-file=ca.crt=./secrets/ca.crt\\\n  -n nifikop-nifi\n```\n\n----------------------------------------\n\nTITLE: Setting Operator Name for Local Execution\nDESCRIPTION: Command to set the operator name as an environment variable, which is required before running the operator locally outside the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/6_contributing/1_developer_guide.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport OPERATOR_NAME=nifi-operator\n```\n\n----------------------------------------\n\nTITLE: NiFi Cluster Service (YAML)\nDESCRIPTION: This YAML snippet configures the NiFi cluster's Service, and it defines a service of type ClusterIP with two port configs. The example is a section of the cluster Deployment YAML. This service definition is then used in the Istio configuration as the destination for the HTTP traffic. The internalListenerName is set to https.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\nspec:  \n  externalServices:  \n    - name: \"nifi-cluster\"\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8443\n            internalListenerName: \"https\"\n```\n```\n\n----------------------------------------\n\nTITLE: Installing cert-manager using Helm 3\nDESCRIPTION: Commands for installing cert-manager v1.7.2 using Helm 3, including applying CRDs first and then deploying the chart from the jetstack repository.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/2_deploy_nifikop/1_quick_start.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Install CustomResourceDefinitions first\nkubectl apply --validate=false -f \\\n   https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.crds.yaml\n\n# Add the jetstack helm repo\nhelm repo add jetstack https://charts.jetstack.io\nhelm repo update\n\n# You have to create the namespace before executing following command\nhelm install cert-manager \\\n    --namespace cert-manager \\\n    --version v1.7.2 jetstack/cert-manager\n```\n\n----------------------------------------\n\nTITLE: Defining Istio Gateway for HTTPS\nDESCRIPTION: This snippet defines an Istio `Gateway` for HTTPS traffic on port 443. It specifies the use of the `SIMPLE` TLS mode and references a Kubernetes secret named `my-secret` for handling TLS certificates. This gateway receives HTTPS traffic, which will then be decrypted.  The `hosts` field specifies the domain. The configuration assumes a pre-existing Kubernetes secret containing the TLS certificate and key.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\n```yaml\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: nifi-gateway\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 443\n      name: https\n      protocol: HTTPS\n    tls:\n      mode: SIMPLE\n      credentialName: my-secret\n    hosts:\n    - nifi.my-domain.com\n```\n```\n\n----------------------------------------\n\nTITLE: Creating a NifiDataflow in YAML\nDESCRIPTION: Example of defining a NifiDataflow custom resource that references a versioned flow from NiFi Registry. It includes references to the required NifiRegistryClient and NifiParameterContext, along with sync mode and update strategy settings.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: demo\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio DestinationRule for Re-encryption and Sticky Sessions\nDESCRIPTION: Defines an Istio DestinationRule named 'nifi-dr' targeting the internal ClusterIP service FQDN '<service-name>.<namespace>.svc.cluster.local'. It configures the traffic policy to re-encrypt traffic using SIMPLE TLS mode before reaching the NiFi pods. It also sets up load balancing with a consistent hash based on the '__Secure-Authorization-Bearer' HTTP cookie to maintain sticky sessions, essential for NiFi UI/API access. Placeholders need replacement.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: DestinationRule\nmetadata:\n  name: nifi-dr\nspec:\n  host: <service-name>.<namespace>.svc.cluster.local\n  trafficPolicy:\n    tls:\n      mode: SIMPLE\n    loadBalancer:\n      consistentHash:\n        httpCookie:\n          name: __Secure-Authorization-Bearer\n          ttl: 0s\n```\n\n----------------------------------------\n\nTITLE: Defining Apache NiFi User Groups with Access Policies in YAML\nDESCRIPTION: This YAML snippet defines a NifiUserGroup custom resource used to simplify access management in Apache NiFi by grouping users and assigning access policies. It requires a reference to a NifiCluster and a list of NifiUser references that belong to the group. The accessPolicies section specifies which global or component-level permissions (read or write) are granted on NiFi resources, such as \"/counters\" in the example. The operator creates and manages the corresponding NiFi group using the Kubernetes resource details. Users must be declared beforehand as NifiUser resources, including their name and namespace for proper synchronization by the operator.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/2_manage_users_and_accesses/2_groups_management.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  # Contains the reference to the NifiCluster with the one the registry client is linked.\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  # contains the list of reference to NifiUsers that are part to the group.\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n#     namespace: nifikop\n    - name: nc-controller.nifikop.mgt.cluster.local\n  # defines the list of access policies that will be granted to the group.\n  accessPolicies:\n      # defines the kind of access policy, could be \"global\" or \"component\".\n    - type: global\n      # defines the kind of action that will be granted, could be \"read\" or \"write\"\n      action: read\n      # resource defines the kind of resource targeted by this access policies, please refer to the following page:\n      #\thttps://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#access-policies\n      resource: /counters\n#      # componentType is used if the type is \"component\", it's allow to define the kind of component on which is the\n#      # access policy\n#      componentType: \"process-groups\"\n#      # componentId is used if the type is \"component\", it's allow to define the id of the component on which is the\n#      # access policy\n#      componentId: \"\"\n```\n\n----------------------------------------\n\nTITLE: Example NodeConfig YAML for NiFi Node Configuration\nDESCRIPTION: This YAML snippet provides an example configuration for a NiFi node using the `NodeConfig` structure. It demonstrates setting provenance storage size, user ID, enabling node clustering, adding pod metadata, specifying image pull policy, assigning a priority class, mounting external volumes (like secrets), and defining persistent storage configurations (`storageConfigs`) for provenance and logs using Kubernetes PVCs. Key fields like `image`, `nodeAffinity` are commented out but show potential configurations.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/5_references/1_nifi_cluster/3_node_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n   default_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      # Set this to true if the instance is a node in a cluster.\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#basic-cluster-setup\n      isNode: true\n      # Additionnal metadata to merge to the pod associated\n      podMetadata:\n        annotations:\n          node-annotation: \"node-annotation-value\"\n        labels:\n          node-label: \"node-label-value\"\n      # Docker image used by the operator to create the node associated\n      # https://hub.docker.com/r/apache/nifi/\n#      image: \"apache/nifi:1.11.2\"\n      # nodeAffinity can be specified, operator populates this value if new pvc added later to node\n      # https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity\n#      nodeAffinity:\n      # imagePullPolicy define the pull policy for NiFi cluster docker image\n      imagePullPolicy: IfNotPresent\n      # priorityClassName define the name of the priority class to be applied to these nodes\n      priorityClassName: \"example-priority-class-name\"\n      # externalVolumeConfigs specifies a list of volume to mount into the main container.\n      externalVolumeConfigs:\n        - name: example-volume\n          mountPath: \"/opt/nifi/example\"\n          secret:\n            secretName: \"raw-controller\"\n      # storageConfigs specifies the node related configs\n      storageConfigs:\n        # Name of the storage config, used to name PV to reuse into sidecars for example.\n        - name: provenance-repository\n          # Retain this PVC throughout NifiCluster deletions.\n          reclaimPolicy: Retain\n          # Path where the volume will be mount into the main nifi container inside the pod.\n          mountPath: \"/opt/nifi/provenance_repository\"\n          # Metadata to attach to the PVC that gets created\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          # Kubernetes PVC spec\n          # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          reclaimPolicy: Delete\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi NodeGroup Autoscalers (JSON)\nDESCRIPTION: Provides an example configuration structure for `NiFiNodeGroupAutoscaler` custom resources. It includes settings for enabling, scaling strategies, horizontal autoscaler parameters (min/max replicas), node group ID, and label selectors for targeting node groups.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/helm/nifi-cluster/README.md#_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n[{\"downscaleStrategy\":\"lifo\",\"enabled\":false,\"horizontalAutoscaler\":{\"maxReplicas\":2,\"minReplicas\":1},\"name\":\"default-group-autoscaler\",\"nodeConfig\":{},\"nodeConfigGroupId\":\"default-group\",\"nodeLabelsSelector\":{\"matchLabels\":{\"default-scale-group\":\"true\"}},\"readOnlyConfig\":{},\"replicas\":null,\"upscaleStrategy\":\"simple\"}]\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop Helm Chart with Custom Values File\nDESCRIPTION: Command for installing the NiFiKop Helm chart using a custom values YAML file. This allows for customizing any of the operator's parameters by specifying them in the values.yaml file rather than directly in the command line.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n$ helm install nifikop \\\n      konpyutaika/nifikop \\\n      -f values.yaml\n```\n\n----------------------------------------\n\nTITLE: Install NiFiKop Helm Chart with Custom Values File\nDESCRIPTION: This command uses the Helm client to install the NiFiKop operator chart. It specifies the release name as 'nifikop' and pulls the chart from the 'konpyutaika/nifikop' repository. The '-f values.yaml' flag instructs Helm to load configuration values from the 'values.yaml' file, overriding default chart parameters. Requires a Helm client (v3+) installed and the 'konpyutaika' Helm repository added.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n$ helm install nifikop \\\n      konpyutaika/nifikop \\\n      -f values.yaml\n```\n\n----------------------------------------\n\nTITLE: Cloning the NiFiKop Repository\nDESCRIPTION: Clones the NiFiKop GitHub repository and navigates into the project directory for further setup and development.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/6_contributing/1_developer_guide.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/konpyutaika/nifikop.git\ncd nifikop\n```\n\n----------------------------------------\n\nTITLE: Define NiFi Cluster Node Configuration YAML\nDESCRIPTION: This YAML snippet defines the configuration for a NiFi cluster, including node specifications. It includes the necessary parameters such as `apiVersion`, `kind`, `metadata`, `spec`, `zkAddress`, `zkPath`, `clusterImage`, `nodes` and more. The nodes section is modified to add a new node (id: 25) to scale up the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.12.1\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n    - id: 2\n      nodeConfigGroup: \"default_group\"\n# >>>> START: The new node\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n# <<<< END\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: Configuring ReadOnlyConfig in NiFiKop\nDESCRIPTION: A YAML configuration example for ReadOnlyConfig in NiFiKop, showing how to set thread counts, logback configuration, authorizer settings, NiFi properties, Zookeeper properties, and bootstrap properties.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/5_references/1_nifi_cluster/2_read_only_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreadOnlyConfig:\n  # MaximumTimerDrivenThreadCount define the maximum number of threads for timer driven processors available to the system.\n  maximumTimerDrivenThreadCount: 30\n  # MaximumEventDrivenThreadCount define the maximum number of threads for event driven processors available to the system.\n  maximumEventDrivenThreadCount: 10\n  # Logback configuration that will be applied to the node\n  logbackConfig:\n    # logback.xml configuration that will replace the one produced based on template\n    replaceConfigMap:\n      # The key of the value,in data content, that we want use.\n      data: logback.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # logback.xml configuration that will replace the one produced based on template and overrideConfigMap\n    replaceSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: logback.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n  # Authorizer configuration that will be applied to the node\n  authorizerConfig:\n    # An authorizers.xml configuration template that will replace the default template seen in authorizers.go\n    replaceTemplateConfigMap:\n      # The key of the value, in data content, that we want use.\n      data: authorizers.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # An authorizers.xml configuration template that will replace the default template seen in authorizers.go and the replaceTemplateConfigMap\n    replaceTemplateSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: authorizers.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n  # NifiProperties configuration that will be applied to the node.\n  nifiProperties:\n    # Additionnal nifi.properties configuration that will override the one produced based on template and\n    # configuration\n    overrideConfigMap:\n      # The key of the value,in data content, that we want use.\n      data: nifi.properties\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop.\n    # Additionnal nifi.properties configuration that will override the one produced based\n    #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    overrideSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: nifi.properties\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # Additionnal nifi.properties configuration that will override the one produced based\n    #\ton template, configurations and overrideConfigMap\n    overrideConfigs: |\n      nifi.ui.banner.text=NiFiKop\n    # A comma separated list of allowed HTTP Host header values to consider when NiFi\n    # is running securely and will be receiving requests to a different host[:port] than it is bound to.\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#web-properties\n    #      webProxyHosts:\n    # Nifi security client auth\n    needClientAuth: false\n    # Indicates which of the configured authorizers in the authorizers.xml file to use\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#authorizer-configuration\n  #      authorizer:\n  # ZookeeperProperties configuration that will be applied to the node.\n  zookeeperProperties:\n    #      # Additionnal zookeeeper.properties configuration that will override the one produced based on template and\n    #      # configuration\n    #      overrideConfigMap:\n    #        # The key of the value,in data content, that we want use.\n    #        data: zookeeeper.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop.\n    #      # Additionnal zookeeeper.properties configuration that will override the one produced based\n    #      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    #      overrideSecretConfig:\n    #        # The key of the value,in data content, that we want use.\n    #        data: zookeeeper.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop\n    # Additionnal zookeeper.properties configuration that will override the one produced based\n    # on template and configurations.\n    overrideConfigs: |\n      initLimit=15\n      autopurge.purgeInterval=24\n      syncLimit=5\n      tickTime=2000\n      dataDir=./state/zookeeper\n      autopurge.snapRetainCount=30\n  # BootstrapProperties configuration that will be applied to the node.\n  bootstrapProperties:\n    #      # Additionnal bootstrap.conf configuration that will override the one produced based on template and\n    #      # configuration\n    #      overrideConfigMap:\n    #        # The key of the value,in data content, that we want use.\n    #        data: bootstrap.conf\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop.\n    #      # Additionnal bootstrap.conf configuration that will override the one produced based\n    #      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    #      overrideSecretConfig:\n    #        # The key of the value,in data content, that we want use.\n    #        data: bootstrap.conf\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop\n    # JVM memory settings\n    nifiJvmMemory: \"512m\"\n    # Additionnal bootstrap.conf configuration that will override the one produced based\n    # on template and configurations.\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#bootstrap_properties\n    overrideConfigs: |\n      # java.arg.4=-Djava.net.preferIPv4Stack=true\n```\n\n----------------------------------------\n\nTITLE: Helm Chart Upgrade and Webhook Enablement for CRD Migration\nDESCRIPTION: This section explains upgrading the NifiKop helm chart to enable the webhook that facilitates CRD conversion during migration from v0.16.0 to v1.0.0. It mentions setting 'webhook.enabled' to true to activate the conversion webhook in the operator pod.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/7_upgrade_guides/3_v0.16.0_to_v1.0.0.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n\n```\n\n----------------------------------------\n\nTITLE: Example: Kubernetes YAML Configuration for External Cert-Manager Issuer\nDESCRIPTION: Shows how to reference an external Issuer (e.g., Let's Encrypt) in the NiFi cluster configuration by setting `issuerRef`. This allows using externally managed certificates for SSL termination in Kubernetes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\nlistenersConfig:\n  clusterDomain: <DNS zone name>\n  useExternalDNS: true\n  sslSecrets:\n    tlsSecretName: \"test-nifikop\"\n    create: true\n    issuerRef:\n      name: letsencrypt-staging\n      kind: Issuer\n```\n\n----------------------------------------\n\nTITLE: Listing Deleted Helm Releases - Bash\nDESCRIPTION: Shows all Helm releases that have been deleted but whose records still exist. Useful for auditing or recovering previous Helm deployments. Requires helm CLI.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --deleted\n```\n\n----------------------------------------\n\nTITLE: Installing nifikop Helm Chart with --skip-crds Option - Bash\nDESCRIPTION: Installs the nifikop chart with the --skip-crds flag, preventing the chart from automatically creating CRDs. You must apply CRDs manually before this operation. The --set namespaces argument specifies the namespace for installation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nhelm install --name nifikop ./helm/nifikop --set namespaces={\"nifikop\"} --skip-crds\n```\n\n----------------------------------------\n\nTITLE: Installing Prometheus Operator via Helm (Console)\nDESCRIPTION: Installs the `kube-prometheus-stack` chart (which includes the Prometheus Operator) into the 'monitoring-system' namespace using Helm. Several parameters (`--set`) are used to customize the installation, such as disabling default rules, Alertmanager, Grafana, and specific metric exporters, while configuring the operator's scope and log level.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_7\n\nLANGUAGE: console\nCODE:\n```\nhelm install prometheus prometheus/kube-prometheus-stack --namespace monitoring-system \\\n    --set prometheusOperator.createCustomResource=false \\\n    --set prometheusOperator.logLevel=debug \\\n    --set prometheusOperator.alertmanagerInstanceNamespaces=monitoring-system \\\n    --set prometheusOperator.namespaces.additional[0]=monitoring-system \\\n    --set prometheusOperator.prometheusInstanceNamespaces=monitoring-system \\\n    --set prometheusOperator.thanosRulerInstanceNamespaces=monitoring-system \\\n    --set defaultRules.enable=false \\\n    --set alertmanager.enabled=false \\\n    --set grafana.enabled=false \\\n    --set kubeApiServer.enabled=false \\\n    --set kubelet.enabled=false \\\n    --set kubeControllerManager.enabled=false \\\n    --set coreDNS.enabled=false \\\n    --set kubeEtcd.enabled=false \\\n    --set kubeScheduler.enabled=false \\\n    --set kubeProxy.enabled=false \\\n    --set kubeStateMetrics.enabled=false \\\n    --set prometheus.enabled=false\n```\n\n----------------------------------------\n\nTITLE: NifiCluster Configuration Example (busybox)\nDESCRIPTION: Example NifiCluster configuration showing the `initContainerImage` set to `busybox`. This configuration needs to be updated when upgrading to Nifikop v0.15.0.  This snippet demonstrates the pre-upgrade configuration that needs modification.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/7_upgrade_guides/2_v0.14.1_to_v0.15.0.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  initContainerImage:\n    repository: busybox\n    tag: \"1.34.0\"\n```\n\n----------------------------------------\n\nTITLE: Defining a NiFi Connection\nDESCRIPTION: This YAML configuration defines a NifiConnection named 'connection' within the 'nifikop' namespace. It establishes a connection between the 'input' and 'output' NifiDataflows, specifying the source and destination dataflows along with their respective sub-names (output and input ports). It also configures connection properties such as back pressure thresholds, flow file expiration, bends, and update strategy.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/4_manage_connections/1_deploy_connection.md#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiConnection\nmetadata:\n  name: connection\n  namespace: nifikop\nspec:\n  source:\n    name: input\n    namespace: nifikop\n    subName: output\n    type: dataflow\n  destination:\n    name: output\n    namespace: nifikop\n    subName: input\n    type: dataflow\n  configuration:\n    backPressureDataSizeThreshold: 100 GB\n    backPressureObjectThreshold: 10000\n    flowFileExpiration: 1 hour\n    labelIndex: 0\n    bends:\n      - posX: 550\n        posY: 550\n      - posX: 550\n        posY: 440\n      - posX: 550\n        posY: 88\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Deploying KEDA ScaledObject Resource for NiFi Node Autoscaling in Kubernetes YAML\nDESCRIPTION: This ScaledObject configures KEDA autoscaling by targeting a custom resource 'NifiNodeGroupAutoscaler' named 'nifinodegroupautoscaler-sample' in the 'clusters' namespace. It specifies polling interval, cooldown period, min/max replica counts, an optional fallback scale configuration, and a Prometheus trigger using a metric query threshold to scale the NiFi nodes based on Prometheus metrics. The resource requires KEDA operator installed and configured properly in the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: cluster\n  namespace: clusters\nspec:\n  scaleTargetRef:\n    apiVersion:    nifi.konpyutaika.com/v1alpha1     # Optional. Default: apps/v1\n    kind:          NifiNodeGroupAutoscaler           # Optional. Default: Deployment\n    name:          nifinodegroupautoscaler-sample    # Mandatory. Must be in the same namespace as the ScaledObject\n    envSourceContainerName: nifi                     # Optional. Default: .spec.template.spec.containers[0]\n  pollingInterval:  30                               # Optional. Default: 30 seconds\n  cooldownPeriod:   300                              # Optional. Default: 300 seconds\n  idleReplicaCount: 0                                # Optional. Default: ignored, must be less than minReplicaCount \n  minReplicaCount:  1                                # Optional. Default: 0\n  maxReplicaCount:  3                                # Optional. Default: 100\n  fallback:                                          # Optional. Section to specify fallback options\n    failureThreshold: 5                              # Mandatory if fallback section is included\n    replicas: 1                                      # Mandatory if fallback section is included\n  triggers:\n    - type: prometheus\n      metadata:\n        serverAddress: http://prometheus-operated.monitoring-system.svc:9090\n        metricName: http_requests_total\n        threshold: <threshold value>\n        query: <prometheus query>\n```\n\n----------------------------------------\n\nTITLE: Running the NiFiKop Migration Script (Bash)\nDESCRIPTION: Shows the command to execute the Node.js migration script via `npm start`. It passes command-line arguments to the script: `--type` specifies the NiFiKop resource type to migrate (e.g., `cluster`, `dataflow`) and `--namespace` specifies the target Kubernetes namespace. The `--` separates npm arguments from script arguments.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nnpm start -- --type=<NIFIKOP_RESOURCE> --namespace=<K8S_NAMESPACE>\n```\n\n----------------------------------------\n\nTITLE: Installing Nifikop Helm Chart while Skipping CRD Installation\nDESCRIPTION: Illustrates installing the Nifikop Helm chart using `helm install` with the `--skip-crds` flag. This prevents Helm from managing the CRDs, useful if they exist or are managed externally. The example uses `--name` (Helm 2 syntax; Helm 3 uses positional release name) and sets the target `namespaces`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nhelm install --name nifikop ./helm/nifikop --set namespaces={\"nifikop\"} --skip-crds\n```\n\n----------------------------------------\n\nTITLE: Installing Nifikop Helm Chart Skipping CRDs\nDESCRIPTION: Illustrates installing the Nifikop Helm chart from a local path (`./helm/nifikop`) while setting target `namespaces` and preventing CRD installation via `--skip-crds`. This requires CRDs to be present or applied manually beforehand.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\n$ helm install --name nifikop ./helm/nifikop --set namespaces={\"nifikop\"} --skip-crds\n```\n\n----------------------------------------\n\nTITLE: Deploying KEDA ScaledObject for NiFi NodeGroup Autoscaling - YAML\nDESCRIPTION: This ScaledObject resource defines autoscaling behavior for the NiFiNodeGroupAutoscaler custom resource within the 'clusters' namespace using KEDA. It references the target autoscaler deployment, sets polling and cooldown intervals, and configures min and max replica counts. The trigger uses Prometheus metrics, providing server address, metric name, threshold, and query parameters to dynamically scale based on real-time NiFi traffic or load metrics.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: cluster\n  namespace: clusters\nspec:\n  scaleTargetRef:\n    apiVersion:    nifi.konpyutaika.com/v1alpha1     # Optional. Default: apps/v1\n    kind:          NifiNodeGroupAutoscaler           # Optional. Default: Deployment\n    name:          nifinodegroupautoscaler-sample    # Mandatory. Must be in the same namespace as the ScaledObject\n    envSourceContainerName: nifi                     # Optional. Default: .spec.template.spec.containers[0]\n  pollingInterval:  30                               # Optional. Default: 30 seconds\n  cooldownPeriod:   300                              # Optional. Default: 300 seconds\n  idleReplicaCount: 0                                # Optional. Default: ignored, must be less than minReplicaCount \n  minReplicaCount:  1                                # Optional. Default: 0\n  maxReplicaCount:  3                                # Optional. Default: 100\n  fallback:                                          # Optional. Section to specify fallback options\n    failureThreshold: 5                              # Mandatory if fallback section is included\n    replicas: 1                                      # Mandatory if fallback section is included\n  # advanced:                                          # Optional. Section to specify advanced options\n  #   restoreToOriginalReplicaCount: true              # Optional. Whether the target resource should be scaled back to original replicas count, after the ScaledObject is deleted\n  #   horizontalPodAutoscalerConfig:                   # Optional. Section to specify HPA related options\n  #     name: {name-of-hpa-resource}                   # Optional. Default: keda-hpa-{scaled-object-name}\n  #     behavior:                                      # Optional. Use to modify HPA's scaling behavior\n  #       scaleDown:\n  #         stabilizationWindowSeconds: 300 <--- \n  #         policies:\n  #         - type: Percent\n  #           value: 100\n  #           periodSeconds: 15\n  triggers:\n    - type: prometheus\n      metadata:\n        serverAddress: http://prometheus-operated.monitoring-system.svc:9090\n        metricName: http_requests_total\n        threshold: <threshold value>\n        query: <prometheus query>\n```\n\n----------------------------------------\n\nTITLE: Defining a NiFiUserGroup YAML Resource for Access Management in NiFiKop\nDESCRIPTION: A YAML definition of a NiFiUserGroup resource that creates a user group with specific members and access policies. The example shows how to reference a NiFi cluster, associate users with the group, and grant specific access permissions (read access to the /counters resource).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/2_manage_users_and_accesses/2_groups_management.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  # Contains the reference to the NifiCluster with the one the registry client is linked.\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  # contains the list of reference to NifiUsers that are part to the group.\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n#     namespace: nifikop\n    - name: nc-controller.nifikop.mgt.cluster.local\n  # defines the list of access policies that will be granted to the group.\n  accessPolicies:\n      # defines the kind of access policy, could be \"global\" or \"component\".\n    - type: global\n      # defines the kind of action that will be granted, could be \"read\" or \"write\"\n      action: read\n      # resource defines the kind of resource targeted by this access policies, please refer to the following page:\n      #\thttps://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#access-policies\n      resource: /counters\n#      # componentType is used if the type is \"component\", it's allow to define the kind of component on which is the\n#      # access policy\n#      componentType: \"process-groups\"\n#      # componentId is used if the type is \"component\", it's allow to define the id of the component on which is the\n#      # access policy\n#      componentId: \"\"\n```\n\n----------------------------------------\n\nTITLE: Deploying CRDs Manually\nDESCRIPTION: This set of `kubectl apply` commands manually deploys the Custom Resource Definitions (CRDs) required by the NiFiKop operator. This is required if you are skipping the CRD deployment during the helm install using `--skip-crds`. This step is essential to allow the operator to manage custom resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifinodegroupautoscalers.yaml\n```\n\n----------------------------------------\n\nTITLE: Deploying a Simple NiFi Cluster\nDESCRIPTION: kubectl command to create a NiFi cluster using a sample configuration file, assuming you've already configured your Zookeeper service.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# Add your zookeeper svc name to the configuration\nkubectl create -n nifi -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Prometheus Resource Configuration YAML - yaml\nDESCRIPTION: YAML manifest for a Prometheus custom resource defining scraping configuration targeted at NiFi cluster metrics. Specifies evaluation interval, scraping interval, resource requests, logging level, serviceAccount, and pod and service monitor selectors to ensure Prometheus scrapes the correct metrics endpoints.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: Prometheus\nmetadata:\n  name: prometheus\n  namespace: monitoring-system\nspec:\n  enableAdminAPI: false\n  evaluationInterval: 30s\n  logLevel: debug\n  podMonitorSelector:\n    matchExpressions:\n    - key: release\n      operator: In\n      values:\n      - prometheus\n  resources:\n    requests:\n      memory: 400Mi\n  scrapeInterval: 30s\n  serviceAccountName: prometheus\n  serviceMonitorSelector:\n    matchExpressions:\n    - key: app\n      operator: In\n      values:\n      - nifi-cluster\n```\n\n----------------------------------------\n\nTITLE: Applying NifiCluster Configuration with kubectl - Shell\nDESCRIPTION: Executes a kubectl command to apply a new or updated NifiCluster YAML configuration file. This command triggers the NiFiKop operator to reconcile the cluster state to match the desired specification, such as adding or removing nodes. Prerequisites: kubectl CLI installed and configured, access to target Kubernetes namespace, NiFiKop operator is running. Inputs: Path to the YAML manifest file. Outputs: Kubernetes resources are created or updated. Limitations: User must have sufficient privileges, and manifests must be valid.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nkubectl -n nifi apply -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFiKop Cluster to Remove a Node - YAML\nDESCRIPTION: Updates the NifiCluster Custom Resource definition to remove a specified NiFi node (id 2) from the cluster, as part of a controlled scale-down/decommissioning process. This YAML manifest should be applied to the Kubernetes cluster, following the NiFiKop and Apache NiFi node decommissioning procedure. Prerequisites: NiFiKop installed, node exists in running cluster. Inputs: YAML manifest minus the node to be removed. Outputs: Triggers graceful node removal, including pod deletion and data redistribution. Limitations: Removal is asynchronous and subject to operator-configured retry durations.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  headlessServiceEnabled: true\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.11.3\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: Creating Custom StorageClass for NiFi in Kubernetes\nDESCRIPTION: YAML configuration for a custom StorageClass using volumeBindingMode 'WaitForFirstConsumer', which is recommended for NiFi clusters. This storage class uses Google Cloud's persistent disk standard provisioner.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: exampleStorageclass\nparameters:\n  type: pd-standard\nprovisioner: kubernetes.io/gce-pd\nreclaimPolicy: Delete\nvolumeBindingMode: WaitForFirstConsumer\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFiKop on OpenShift using Helm with Specific UID (Bash)\nDESCRIPTION: Deploys NiFiKop operator v1.1.1 on OpenShift using Helm. It specifically sets the `runAsUser` parameter using a pre-obtained UID (stored in `$uid`) to comply with OpenShift's Security Context Constraints (SCC). Also configures namespace, image tag, resource limits, and watched namespaces.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/2_deploy_nifikop/1_quick_start.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop \\\n    nifikop \\\n    --namespace=nifi \\\n    --version 1.1.1 \\\n    --set image.tag=v1.1.1-release \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set namespaces={\"nifi\"} \\\n    --set runAsUser=$uid\n```\n\n----------------------------------------\n\nTITLE: Install Zookeeper - Helm\nDESCRIPTION: This command installs Zookeeper using the Bitnami Helm chart. It sets resource requests and limits for CPU and memory, specifies a storage class, enables network policy, sets the replica count to 3, and creates a new namespace for Zookeeper.  It is crucial to replace `standard` with your own StorageClass.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhelm install zookeeper oci://registry-1.docker.io/bitnamicharts/zookeeper \\\n    --namespace=zookeeper \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set global.storageClass=standard \\\n    --set networkPolicy.enabled=true \\\n    --set replicaCount=3 \\\n    --create-namespace\n```\n\n----------------------------------------\n\nTITLE: Helm Install Dry Run\nDESCRIPTION: This command shows how to perform a dry run of the NiFiKop Helm chart installation. It allows you to preview the changes that would be made without actually deploying the chart.  The command also shows how to set the log level and namespaces during the dry run.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/helm/nifikop/README.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nhelm install nifikop konpyutaika-incubator/nifikop \\\n    --dry-run \\\n    --debug.enabled \\\n    --set logLevel=\"Debug\" \\\n    --set namespaces={\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Adding a Node to NiFi Cluster Configuration in YAML\nDESCRIPTION: YAML configuration showing how to add a new node (id: 25) to an existing NiFi cluster by updating the NifiCluster custom resource definition.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/3_tasks/1_nifi_cluster/2_cluster_scaling.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.12.1\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n    - id: 2\n      nodeConfigGroup: \"default_group\"\n# >>>> START: The new node\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n# <<<< END\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFiKop CRDs with kubectl - Bash\nDESCRIPTION: Applies the Custom Resource Definitions (CRDs) required by NiFiKop to the Kubernetes cluster using kubectl. The commands must be executed from the repository root where the CRD YAML files exist. Requires access to a Kubernetes cluster and appropriate privileges. Successfully runs with no parameterization needed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/6_contributing/1_developer_guide.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing cert-manager with Helm 3\nDESCRIPTION: Installs cert-manager using Helm 3. First applies the CRDs using kubectl, then adds the jetstack Helm repo, updates repos, and finally installs the cert-manager chart into the 'cert-manager' namespace. Requires the namespace to be created beforehand.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/2_setup/1_getting_started.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Install CustomResourceDefinitions first\nkubectl apply --validate=false -f \\\n   https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.crds.yaml\n\n# Add the jetstack helm repo\nhelm repo add jetstack https://charts.jetstack.io\nhelm repo update\n\n# You have to create the namespace before executing following command\nhelm install cert-manager \\\n    --namespace cert-manager \\\n    --version v1.7.2 jetstack/cert-manager\n```\n\n----------------------------------------\n\nTITLE: Getting Status for a Helm Deployment\nDESCRIPTION: Retrieves the status and details of a specific Helm release, identified by its name (e.g., `nifikop`).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nhelm status nifikop\n```\n\n----------------------------------------\n\nTITLE: Defining Nifi External Service Listeners in YAML\nDESCRIPTION: This snippet demonstrates how to configure Nifi external service listeners using YAML, specifying service name, type, port, and associated listener name, along with metadata annotations and labels for management purposes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/5_references/1_nifi_cluster/7_external_service_config.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n  externalServices:\n    - name: \"clusterip\"\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8080\n            internalListenerName: \"http\"\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n\n----------------------------------------\n\nTITLE: Helm CRD Installation Skipping\nDESCRIPTION: Helm installation command with the '--skip-crds' flag, which skips CRD deployment, assuming they are already present in the cluster. Useful for avoiding modifications to existing CRDs.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nhelm install --name nifikop ./helm/nifikop --set namespaces={\"nifikop\"} --skip-crds\n```\n\n----------------------------------------\n\nTITLE: Applying NiFi Cluster Scale-Down Configuration (Shell)\nDESCRIPTION: This shell command uses `kubectl` to apply the `NifiCluster` configuration (from `config/samples/simplenificluster.yaml`) where a node has been removed or commented out. This triggers the NiFiKop operator to start the graceful decommissioning process for the specified node within the `nifi` namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\nkubectl -n nifi apply -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining Istio Gateway for HTTPS\nDESCRIPTION: This YAML snippet defines an Istio Gateway for HTTPS traffic. It configures the gateway to accept HTTPS traffic on port 443, specifies the protocol as HTTPS, and uses TLS with mode SIMPLE. It uses a credential with credentialName 'my-secret', and accepts traffic for the host 'nifi.my-domain.com'.  This configuration is designed to decrypt HTTPS traffic from the external world, allowing it to be routed and processed as HTTP internally within the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: nifi-gateway\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 443\n      name: https\n      protocol: HTTPS\n    tls:\n      mode: SIMPLE\n      credentialName: my-secret\n    hosts:\n    - nifi.my-domain.com\n```\n```\n\n----------------------------------------\n\nTITLE: Defining Service with ClusterIP Type for HTTPS Traffic\nDESCRIPTION: Specifies a Kubernetes Service named 'nifi-cluster' of type ClusterIP, exposing port 8443 with an internal listener name 'https'. This service acts as the backend endpoint for HTTPS traffic routed via the VirtualService.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nspec:\n  externalServices:\n    - name: \"nifi-cluster\"\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8443\n            internalListenerName: \"https\"\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiParameterContext CRD in YAML\nDESCRIPTION: This YAML manifest defines a `NifiParameterContext` custom resource. NiFiKop uses this to create and manage a Parameter Context within the target NiFi cluster. It references the cluster (`clusterRef`), defines regular parameters, and can reference Kubernetes Secrets (`secretRefs`) to populate sensitive parameters.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\n  namespace: demo\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      value: toto\n      description: toto\n```\n\n----------------------------------------\n\nTITLE: Configuring Scale Down for NiFiKop Cluster in YAML\nDESCRIPTION: This YAML manifest modifies the NifiCluster resource to remove a node from the cluster by commenting out or deleting its entry from the 'NifiCluster.Spec.Nodes' list. The configuration keeps all other cluster parameters and node groups intact. Removing the node triggers the decommissioning steps managed by NiFiKop, which sequentially removes and cleans up the node across cluster and Kubernetes resources. Input is a valid manifest; output is the intended updated NiFi cluster topology.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  headlessServiceEnabled: true\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.11.3\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n# >>>> START: node removed\n#    - id: 2\n#      nodeConfigGroup: \"default_group\"\n# <<<< END\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Registry Client in YAML\nDESCRIPTION: Example YAML configuration for creating a NifiRegistryClient resource that connects a NiFi cluster to a registry. This defines the connection to a registry named 'Squidflow demo' with the specified URI, linked to a NiFi cluster named 'nc'.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/5_references/3_nifi_registry_client.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiRegistryClient\nmetadata:\n  name: squidflow\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Squidflow demo\"\n  uri: \"http://nifi-registry:18080\"\n```\n\n----------------------------------------\n\nTITLE: Deploying a Basic NiFi Cluster\nDESCRIPTION: Creates a NiFi cluster deployment by applying a sample YAML configuration, specifying the Zookeeper service name to connect NiFi nodes. Ensure your configuration matches your environment setup.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/2_setup/1_getting_started.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# Add your zookeeper svc name to the configuration\\nkubectl create -n nifi -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing KEDA Helm Chart with Console and kubectl\nDESCRIPTION: Creates a new Kubernetes namespace named 'keda' and installs the KEDA Helm chart into that namespace using Helm. Prerequisites include an active kubectl context with cluster access and Helm installed. The key parameters are the namespace ('keda') and the chart source ('kedacore/keda'). The expected outcome is a running instance of KEDA in the specified namespace, ready for event-driven autoscaling. Output includes resource creation messages in the terminal.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nkubectl create namespace keda\nhelm install keda kedacore/keda --namespace keda\n```\n\n----------------------------------------\n\nTITLE: Defining NifiRegistryClient in YAML for NiFiKop\nDESCRIPTION: This snippet shows how to create a NifiRegistryClient resource that connects to a NiFi Registry instance. The registry client is required for managing dataflows as it enables version control capabilities.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiRegistryClient\nmetadata:\n  name: registry-client-example\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Registry client managed by NiFiKop\"\n  uri: \"http://nifi.hostname.com:18080\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio HTTPS Decrypting Gateway YAML\nDESCRIPTION: This YAML snippet defines an Istio Gateway resource for handling HTTPS traffic. It configures the Istio ingress gateway to listen on port 443 for the specified host, using `SIMPLE` TLS mode and a referenced credential secret (`my-secret`) to terminate TLS and pass traffic as HTTP within the mesh.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: nifi-gateway\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 443\n      name: https\n      protocol: HTTPS\n    tls:\n      mode: SIMPLE\n      credentialName: my-secret\n    hosts:\n    - nifi.my-domain.com\n```\n\n----------------------------------------\n\nTITLE: Defining a NiFi Registry Client in YAML\nDESCRIPTION: This snippet demonstrates how to create a NifiRegistryClient resource in Kubernetes to connect to a NiFi Registry. It includes required fields such as the API version, kind, metadata, and specification for the registry client connection.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/5_references/3_nifi_registry_client.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiRegistryClient\nmetadata:\n  name: squidflow\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Squidflow demo\"\n  uri: \"http://nifi-registry:18080\"\n```\n\n----------------------------------------\n\nTITLE: Installing cert-manager Using Helm 3 Bash Commands\nDESCRIPTION: This snippet guides on installing cert-manager with Helm 3 by first applying CRDs separately, then adding the jetstack Helm repository, and finally installing cert-manager version v1.7.2 into the cluster's cert-manager namespace. It requires Helm 3 installed and a pre-created cert-manager namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/2_deploy_nifikop/1_quick_start.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Install CustomResourceDefinitions first\nkubectl apply --validate=false -f \\\n   https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.crds.yaml\n\n# Add the jetstack helm repo\nhelm repo add jetstack https://charts.jetstack.io\nhelm repo update\n\n# You have to create the namespace before executing following command\nhelm install cert-manager \\\n    --namespace cert-manager \\\n    --version v1.7.2 jetstack/cert-manager\n```\n\n----------------------------------------\n\nTITLE: Defining Istio DestinationRule for HTTPS with Sticky Session - YAML\nDESCRIPTION: Creates a DestinationRule for traffic to the NiFi ClusterIP service, enabling SIMPLE TLS mode and configuring consistent hash session affinity based on the '__Secure-Authorization-Bearer' cookie. Requires that the VirtualService directs traffic to the same service, and Istio's ingress controller is managing flow. Enables sticky sessions required for user authentication via HTTP cookies.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_5\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: DestinationRule\nmetadata:\n  name: nifi-dr\nspec:\n  host: <service-name>.<namespace>.svc.cluster.local\n  trafficPolicy:\n    tls:\n      mode: SIMPLE\n    loadBalancer:\n      consistentHash:\n        httpCookie:\n          name: __Secure-Authorization-Bearer\n          ttl: 0s\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio HTTPS DestinationRule YAML\nDESCRIPTION: This YAML snippet defines an Istio DestinationRule applied to the NiFi service destination. It configures the traffic policy to enforce `SIMPLE` TLS mode, ensuring traffic is re-encrypted before reaching the service endpoints. It also enables sticky sessions by configuring a consistent hash load balancer based on the `__Secure-Authorization-Bearer` HTTP cookie.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_5\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: DestinationRule\nmetadata:\n  name: nifi-dr\nspec:\n  host: <service-name>.<namespace>.svc.cluster.local\n  trafficPolicy:\n    tls:\n      mode: SIMPLE\n    loadBalancer:\n      consistentHash:\n        httpCookie:\n          name: __Secure-Authorization-Bearer\n          ttl: 0s\n```\n\n----------------------------------------\n\nTITLE: Defining Prometheus Custom Resource for NiFi Cluster Monitoring in YAML\nDESCRIPTION: This YAML snippet defines a Prometheus custom resource in the 'monitoring-system' namespace with parameters optimized for scraping NiFi cluster metrics. It disables the admin API, sets evaluation and scrape intervals to 30 seconds, and configures selection criteria for pod monitors and service monitors labeled for NiFi. Requests memory resources of 400Mi and specifies the service account 'prometheus' for authentication. Required for enabling Prometheus to collect metrics from the NiFi cluster. Requires Prometheus operator installed and configured.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: Prometheus\nmetadata:\n  name: prometheus\n  namespace: monitoring-system\nspec:\n  enableAdminAPI: false\n  evaluationInterval: 30s\n  logLevel: debug\n  podMonitorSelector:\n    matchExpressions:\n    - key: release\n      operator: In\n      values:\n      - prometheus\n  resources:\n    requests:\n      memory: 400Mi\n  scrapeInterval: 30s\n  serviceAccountName: prometheus\n  serviceMonitorSelector:\n    matchExpressions:\n    - key: app\n      operator: In\n      values:\n      - nifi-cluster\n```\n\n----------------------------------------\n\nTITLE: Installing Zookeeper with Helm on Kubernetes\nDESCRIPTION: Installs a Zookeeper cluster using the Bitnami Helm chart with specified resource requests and limits, a storage class, network policies, and a replica count of 3. This Helm command requires the Helm package manager and Kubernetes cluster access with namespace creation privileges. The user must customize the storageClass parameter to match their environment. This Zookeeper cluster is necessary when using Zookeeper for NiFi state management.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhelm install zookeeper oci://registry-1.docker.io/bitnamicharts/zookeeper \\\n    --namespace=zookeeper \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set global.storageClass=standard \\\n    --set networkPolicy.enabled=true \\\n    --set replicaCount=3 \\\n    --create-namespace\n```\n\n----------------------------------------\n\nTITLE: Applying NifiCluster Configuration - Shell\nDESCRIPTION: This shell command applies the updated NifiCluster configuration to the Kubernetes cluster within the 'nifi' namespace. It uses `kubectl` to apply the changes defined in the `config/samples/simplenificluster.yaml` file.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nkubectl -n nifi apply -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Creating a Kubernetes Secret for Sensitive Parameters using kubectl\nDESCRIPTION: This command-line snippet demonstrates how to create a generic Kubernetes Secret containing key-value pairs for sensitive NiFi parameter values. Use this command before applying NifiParameterContext manifests that reference this secret. Replace secret keys and values as needed for your configuration. The -n flag specifies the namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl create secret generic secret-params \\\n    --from-literal=secret1=yop \\\n    --from-literal=secret2=yep \\\n    -n nifikop\n\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi User via NifiUser Custom Resource in Kubernetes (YAML)\nDESCRIPTION: This YAML manifest defines a NifiUser custom resource for the nifikop operator, specifying user identity, cluster association, PKI generation options, and detailed access control policies. It requires the nifikop operator deployed in the cluster, and the referenced NifiCluster must exist and be running. The resource maps the Kubernetes user resource to a NiFi user, optionally overrides identity for compatibility, and configures RBAC (read permissions on '/data' for process-groups). Set 'createCert' to false if no client certificate is needed for authentication.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/3_manage_nifi/2_manage_users_and_accesses/1_users_management.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiUser\nmetadata:\n  name: aguitton\nspec:\n  # identity field is use to define the user identity on NiFi cluster side,\n  #\tit use full when the user's name doesn't suite with Kubernetes resource name.\n  identity: alexandre.guitton@konpyutaika.com\n  # Contains the reference to the NifiCluster with the one the registry client is linked.\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  # Whether or not the the operator also include a Java keystore format (JKS) with you secret\n  includeJKS: false\n  # Whether or not a certificate will be created for this user.\n  createCert: false\n  # defines the list of access policies that will be granted to the group.\n  accessPolicies:\n      # defines the kind of access policy, could be \"global\" or \"component\".\n    - type: component\n      # defines the kind of action that will be granted, could be \"read\" or \"write\"\n      action: read\n      # resource defines the kind of resource targeted by this access policies, please refer to the following page:\n      #\thttps://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#access-policies\n      resource: /data\n      # componentType is used if the type is \"component\", it's allow to define the kind of component on which is the\n      # access policy\n      componentType: \"process-groups\"\n      # componentId is used if the type is \"component\", it's allow to define the id of the component on which is the\n      # access policy\n      componentId: \"\"\n```\n\n----------------------------------------\n\nTITLE: Deploying Prometheus Operator using Helm\nDESCRIPTION: Installs the `kube-prometheus-stack` Helm chart into the `monitoring-system` namespace. This command uses various `--set` flags to customize the installation, disabling components like Grafana and Alertmanager, configuring the Prometheus Operator's namespace scope, and setting the log level, while explicitly disabling the deployment of a Prometheus instance managed by this chart (as it will be deployed separately via CR).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_8\n\nLANGUAGE: console\nCODE:\n```\nhelm install prometheus prometheus/kube-prometheus-stack --namespace monitoring-system \\\n    --set prometheusOperator.createCustomResource=false \\\n    --set prometheusOperator.logLevel=debug \\\n    --set prometheusOperator.alertmanagerInstanceNamespaces=monitoring-system \\\n    --set prometheusOperator.namespaces.additional[0]=monitoring-system \\\n    --set prometheusOperator.prometheusInstanceNamespaces=monitoring-system \\\n    --set prometheusOperator.thanosRulerInstanceNamespaces=monitoring-system \\\n    --set defaultRules.enable=false \\\n    --set alertmanager.enabled=false \\\n    --set grafana.enabled=false \\\n    --set kubeApiServer.enabled=false \\\n    --set kubelet.enabled=false \\\n    --set kubeControllerManager.enabled=false \\\n    --set coreDNS.enabled=false \\\n    --set kubeEtcd.enabled=false \\\n    --set kubeScheduler.enabled=false \\\n    --set kubeProxy.enabled=false \\\n    --set kubeStateMetrics.enabled=false \\\n    --set prometheus.enabled=false\n```\n\n----------------------------------------\n\nTITLE: Building and Pushing NiFiKop Docker Image for Helm Deployment in Bash\nDESCRIPTION: This snippet shows how to build a Docker image of the NiFiKop operator from the local branch by setting the base Docker repository environment variable and invoking make targets to build and push the image to the specified repository. This workflow facilitates deploying the operator Helm chart with a custom image. Docker, Make, and access to a Docker registry are required.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/6_contributing/1_developer_guide.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nexport DOCKER_REPO_BASE={your-docker-repo}\nmake docker-build\n```\n\nLANGUAGE: bash\nCODE:\n```\nmake docker-push\n```\n\n----------------------------------------\n\nTITLE: Listing All Helm Releases Including Failed and Deleted - Bash\nDESCRIPTION: Lists all Helm chart releases, including those currently deployed, deleted, or failed, providing comprehensive visibility into a cluster's Helm release history. Helm must be installed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --all\n```\n\n----------------------------------------\n\nTITLE: Deploying Prometheus Instance Resource\nDESCRIPTION: Creates a Prometheus custom resource that defines the configuration for the Prometheus server, including the service monitor selector for NiFi metrics.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: Prometheus\nmetadata:\n  name: prometheus\n  namespace: monitoring-system\nspec:\n  enableAdminAPI: false\n  evaluationInterval: 30s\n  logLevel: debug\n  podMonitorSelector:\n    matchExpressions:\n    - key: release\n      operator: In\n      values:\n      - prometheus\n  resources:\n    requests:\n      memory: 400Mi\n  scrapeInterval: 30s\n  serviceAccountName: prometheus\n  serviceMonitorSelector:\n    matchExpressions:\n    - key: app\n      operator: In\n      values:\n      - nifi-cluster\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFiKop CRDs Using kubectl - Bash\nDESCRIPTION: Applies multiple Custom Resource Definitions (CRDs) related to NiFiKop by using kubectl. These CRDs define Kubernetes custom resources managed by the operator. This step is necessary to enable usage of NiFiKop-specific resource types in the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/6_contributing/1_developer_guide.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\n```\n\n----------------------------------------\n\nTITLE: Building and Pushing NiFiKop Docker Image (Bash)\nDESCRIPTION: Builds a Docker image of the NiFiKop operator from the current branch and pushes it to a specified Docker registry. Requires `DOCKER_REGISTRY_BASE` environment variable to be set. The image tag combines the version from `version/version.go` and the branch name.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/6_contributing/1_developer_guide.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nexport DOCKER_REGISTRY_BASE={your-docker-repo}\nmake docker-build\n```\n\nLANGUAGE: bash\nCODE:\n```\nmake docker-push\n```\n\n----------------------------------------\n\nTITLE: Listing Deleted Helm Releases in Bash\nDESCRIPTION: This Bash command lists all Helm releases marked as deleted but retained in the cluster's Helm metadata. Dependency: Helm CLI. Output: deleted Helm releases and metadata.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --deleted\n```\n\n----------------------------------------\n\nTITLE: Checking cert-manager Pods Status in Kubernetes Using kubectl Console Command\nDESCRIPTION: The command lists pods running in the cert-manager namespace, showing readiness and current status. This confirms cert-manager components are active and operational, which is essential for TLS certificate generation and security management within the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/blog/2020-05-20-secured_nifi_cluster_on_gcp.md#_snippet_5\n\nLANGUAGE: console\nCODE:\n```\nkubectl -n cert-manager get pods\nNAME                                       READY   STATUS    RESTARTS   AGE\ncert-manager-55fff7f85f-74nf5              1/1     Running   0          72m\ncert-manager-cainjector-54c4796c5d-mfbbx   1/1     Running   0          72m\ncert-manager-webhook-77ccf5c8b4-m6ws4      1/1     Running   2          72m\n```\n\n----------------------------------------\n\nTITLE: Running NiFiKop Operator Locally\nDESCRIPTION: Command to run the NiFiKop operator on a local machine using the default Kubernetes configuration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/6_contributing/1_developer_guide.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmake run\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Cluster NodeGroup for Autoscaling with Prometheus Support in YAML\nDESCRIPTION: Defines a partial Kubernetes custom resource for a NiFi cluster, adding a NodeConfigGroup named 'auto_scaling' and multiple persistent storage configurations. The listener port for Prometheus metrics is included for external autoscaler integration. Prerequisites are a running NiFi operator and existing cluster CRD definitions. Key parameters are containerPorts, storage mountPaths, PVC specifications, and the node group name. Inputs are the YAML resource definition; outputs are NiFi pods with Prometheus-exposed metrics and autoscaling-ready configurations. Limitations: example is partial and must be integrated with the full NiFiCluster manifest.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n...\nspec:\n  ...\n  listenersConfig:\n    internalListeners:\n    ...\n    - containerPort: 9090\n      name: prometheus\n      type: prometheus \n    ...\n  ...\n  nodeConfigGroups:\n    auto_scaling:\n      isNode: true\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 2Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n      serviceAccountName: external-dns\n      storageConfigs:\n        - mountPath: /opt/nifi/nifi-current/logs\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/data\n          name: data\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/extensions\n          name: extensions-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/flowfile_repository\n          name: flowfile-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/nifi-current/conf\n          name: conf\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/content_repository\n          name: content-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/provenance_repository\n          name: provenance-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n  ...\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Namespace for Monitoring\nDESCRIPTION: Creates a dedicated Kubernetes namespace named `monitoring-system` using `kubectl`. This namespace will house the Prometheus monitoring components, isolating them from other applications.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_5\n\nLANGUAGE: console\nCODE:\n```\nkubectl create namespace monitoring-system\n```\n\n----------------------------------------\n\nTITLE: Node.js Script for Migrating NiFiKop CRDs\nDESCRIPTION: This JavaScript code defines a Node.js script (`index.js`) for migrating Kubernetes custom resources. It uses the `@kubernetes/client-node` library to connect to the Kubernetes API (disabling TLS verification), lists resources from a source API group (`nifi.orange.com/v1alpha1`), creates corresponding resources in a destination API group (`nifi.konpyutaika.com/v1alpha1`), copies relevant metadata and spec, and then updates the status of the newly created resource. It utilizes `minimist` to parse command-line arguments specifying the resource type (`--type`) and namespace (`--namespace`) for the migration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nprocess.env['NODE_TLS_REJECT_UNAUTHORIZED'] = 0;\nconst k8s = require('@kubernetes/client-node');\n\nconst kc = new k8s.KubeConfig();\nkc.loadFromDefault();\n\nconst k8sApi = kc.makeApiClient(k8s.CustomObjectsApi);\n\nconst KONPYUTAIKA_GROUP = 'nifi.konpyutaika.com';\nconst KONPYUTAIKA_GROUP_VERSION = 'v1alpha1';\nconst ORANGE_GROUP = 'nifi.orange.com';\nconst ORANGE_GROUP_VERSION = 'v1alpha1';\n\nconst call = async (SRC_GRP, SRC_GRP_VER, DST_GRP, DST_GRP_VER, KIND_PLURAL, NAMESPACE) => {\n\tconsole.log(`Listing ${KIND_PLURAL} of ${SRC_GRP}/${SRC_GRP_VER} in ${NAMESPACE}...`);\n\tconst listResources = (await k8sApi.listNamespacedCustomObject(SRC_GRP, SRC_GRP_VER, NAMESPACE, KIND_PLURAL)).body.items;\n\treturn Promise.all(listResources.map(async (resource) => {\n\t\ttry {\n\t\t\tconsole.log(`Found ${resource.kind} \"${resource.metadata.name}\" of ${resource.apiVersion} in ${NAMESPACE}`);\n\n\t\t\tif (resource.metadata.ownerReferences) {\n\t\t\t\tconsole.log(`${resource.kind} ${resource.metadata.name} mananged by something else (ownerRefereces is set).`);\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tconst bodyResource = {\n\t\t\t\tapiVersion: `${DST_GRP}/${DST_GRP_VER}`,\n\t\t\t\tkind: resource.kind,\n\t\t\t\tmetadata: {\n\t\t\t\t\tname: resource.metadata.name,\n\t\t\t\t\tannotations: resource.metadata.annotations,\n\t\t\t\t\tlabels: resource.metadata.labels\n\t\t\t\t},\n\t\t\t\tspec: resource.spec\n\t\t\t};\n\n\t\t\tconsole.log(`Creating ${bodyResource.kind} \"${bodyResource.metadata.name}\" of ${bodyResource.apiVersion} in ${NAMESPACE}...`);\n\t\t\tconst newResource = (await k8sApi.createNamespacedCustomObject(DST_GRP, DST_GRP_VER, NAMESPACE, KIND_PLURAL, bodyResource)).body;\n\t\t\tconsole.log('...done creating.');\n\n\t\t\tconst bodyStatus = {\n\t\t\t\tapiVersion: newResource.apiVersion,\n\t\t\t\tkind: newResource.kind,\n\t\t\t\tmetadata: {\n\t\t\t\t\tname: newResource.metadata.name,\n\t\t\t\t\tresourceVersion: newResource.metadata.resourceVersion\n\t\t\t\t},\n\t\t\t\tstatus: resource.status\n\t\t\t};\n\n\t\t\tconsole.log(`Copying status from ${resource.kind} \"${resource.metadata.name}\" of ${newResource.apiVersion} to ${newResource.kind} \"${newResource.metadata.name}\" of ${newResource.apiVersion} in ${NAMESPACE}...`);\n\t\t\tconst newResourceWithStatus = (await k8sApi.replaceNamespacedCustomObjectStatus(DST_GRP, DST_GRP_VER, NAMESPACE, KIND_PLURAL, bodyStatus.metadata.name, bodyStatus)).body;\n\t\t\tconsole.log('...done copying.');\n\t\t\treturn newResourceWithStatus;\n\t\t}\n\t\tcatch (e) {\n\t\t\tconsole.error(e.body ? e.body.message ? e.body.message : e.body : e);\n\t\t}\n\t}));\n};\n\nconst argv = require('minimist')(process.argv.slice(2));\n\nlet NAMESPACE = argv.namespace ? argv.namespace.length > 0 ? argv.namespace : 'default' : 'default';\nlet KIND_PLURAL = {\n\tcluster: 'nificlusters',\n\tdataflow: 'nifidataflows',\n\tparametercontext: 'nifiparametercontexts',\n\tregistryclient: 'nifiregistryclients',\n\tuser: 'nifiusers',\n\tusergroup: 'nifiusergroups',\n};\n\nif (!argv.type) {\n\tconsole.error('Type not provided');\n\tprocess.exit(1);\n}\n\nif (!KIND_PLURAL[argv.type]) {\n\tconsole.error(`Type ${argv.type} is not one of the following types: ${Object.keys(KIND_PLURAL)}`);\n\tprocess.exit(1);\n}\n\nconsole.log(`########### START: ${KIND_PLURAL[argv.type]} ###########`);\ncall( ORANGE_GROUP, ORANGE_GROUP_VERSION, KONPYUTAIKA_GROUP, KONPYUTAIKA_GROUP_VERSION, KIND_PLURAL[argv.type], NAMESPACE)\n\t.then(r => console.log('############ END ############'))\n\t.catch(e => console.error(e));\n```\n\n----------------------------------------\n\nTITLE: Specifying NifiCluster InitContainerImage Using Bash YAML\nDESCRIPTION: This YAML snippet demonstrates the updated NifiCluster CRD specification where the init container image has been changed from busybox to bash with tag 5.2.2. This change is mandatory for versions 0.15.0 and onward if the user had previously overridden the initContainerImage. The snippet preserves the same structural fields as before but updates the repository and tag to point to a bash image that includes a bash shell, which is required for proper initialization.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/7_upgrade_guides/2_v0.14.1_to_v0.15.0.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  initContainerImage:\n    repository: bash\n    tag: \"5.2.2\"\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiUserGroup Resource in Kubernetes (YAML)\nDESCRIPTION: This YAML manifest defines a `NifiUserGroup` custom resource named `group-test` for the NifiKop operator. It specifies the target NiFi cluster via `clusterRef`, lists members using references (`usersRef`) to existing `NifiUser` resources, and assigns access policies (e.g., read access to global `/counters`). The operator uses this definition to create and manage the corresponding group and its policies within the specified NiFi cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/2_manage_users_and_accesses/2_groups_management.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  # Contains the reference to the NifiCluster with the one the registry client is linked.\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  # contains the list of reference to NifiUsers that are part to the group.\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n#     namespace: nifikop\n    - name: nc-controller.nifikop.mgt.cluster.local\n  # defines the list of access policies that will be granted to the group.\n  accessPolicies:\n      # defines the kind of access policy, could be \"global\" or \"component\".\n    - type: global\n      # defines the kind of action that will be granted, could be \"read\" or \"write\"\n      action: read\n      # resource defines the kind of resource targeted by this access policies, please refer to the following page:\n      #\thttps://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#access-policies\n      resource: /counters\n#      # componentType is used if the type is \"component\", it's allow to define the kind of component on which is the\n#      # access policy\n#      componentType: \"process-groups\"\n#      # componentId is used if the type is \"component\", it's allow to define the id of the component on which is the\n#      # access policy\n#      componentId: \"\"\n```\n\n----------------------------------------\n\nTITLE: Configuring LoadBalancer External Service for Nifi (YAML)\nDESCRIPTION: This YAML snippet provides an example configuration for a Kubernetes LoadBalancer service for a Nifi cluster managed by Nifikop. It defines a service named 'nlb' with a specific load balancer class, exposing ports 8080 (TCP) and 7890 (UDP), mapping them to internal Nifi listeners 'http' and 'my-custom-udp-listener'.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/5_references/1_nifi_cluster/7_external_service_config.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nexternalServices:\n    - name: \"nlb\"\n      spec:\n        type: LoadBalancer\n        loadBalancerClass: \"service.k8s.aws/nlb\"\n        portConfigs:\n          - port: 8080\n            internalListenerName: \"http\"\n          - port: 7890\n            internalListenerName: \"my-custom-udp-listener\"\n            protocol: UDP\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiUser Resource in YAML\nDESCRIPTION: This YAML snippet demonstrates how to define a `NifiUser` custom resource in Kubernetes. It specifies the API version and kind, sets metadata like the user's name (`aguitton`), and defines the spec, including the user's identity (`alexandre.guitton@konpyutaika.com`), a reference to the target NiFi cluster (`nc` in the `nifikop` namespace), and explicitly disables automatic certificate creation (`createCert: false`).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/5_references/2_nifi_user.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: aguitton\nspec:\n  identity: alexandre.guitton@konpyutaika.com\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  createCert: false\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop Operator Helm Chart Into Kubernetes Cluster (Bash)\nDESCRIPTION: This snippet installs the NiFiKop operator Helm chart into a Kubernetes cluster with specified image tag and namespace. It uses Helm CLI version 3 or above, deploying the locally built operator image via chart configuration. The namespace name is set to 'nifikop' and the image tag to 'v0.5.1-release' accordingly. Proper image repository and tag matching is critical for successful deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/6_contributing/1_developer_guide.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nhelm install skeleton ./helm/nifikop \\\n    --set image.tag=v0.5.1-release \\\n    --namespace-{\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Installing KEDA using Helm\nDESCRIPTION: Commands to add KEDA's Helm repository, update the repositories, and install KEDA in its own namespace using Helm.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nhelm repo add kedacore https://kedacore.github.io/charts\n```\n\nLANGUAGE: console\nCODE:\n```\nhelm repo update\n```\n\nLANGUAGE: console\nCODE:\n```\nkubectl create namespace keda\nhelm install keda kedacore/keda --namespace keda\n```\n\n----------------------------------------\n\nTITLE: Configuring NifiCluster for Removing a Node (YAML)\nDESCRIPTION: This YAML snippet demonstrates the configuration for removing a node from an existing NiFi cluster. The node with ID 2 is commented out. This involves modifying the `NifiCluster.Spec.Nodes` field. It is crucial for gracefully removing a node. Prerequisites include a running NiFi cluster managed by the NiFiKop operator. The main parameter is the removal of the node ID from the `nodes` list. The output will be the decommission of the node with ID 2.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  headlessServiceEnabled: true\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.11.3\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n# >>>> START: node removed\n#    - id: 2\n#      nodeConfigGroup: \"default_group\"\n# <<<< END\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: Building and Installing Kubectl NiFiKop Plugin (Console)\nDESCRIPTION: This command sequence builds the kubectl-nifikop executable from source using `make` and then copies the resulting binary to a standard directory in the system's PATH (`/usr/local/bin`). It requires the source code, `make`, and administrative privileges (`sudo`) to install.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/2_deploy_nifikop/3_kubectl_plugin.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nmake kubectl-nifikop && sudo cp ./bin/kubectl-nifikop /usr/local/bin\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Cluster SSL Listeners using YAML\nDESCRIPTION: Defines the SSL-related configuration for the NiFi cluster resource using Kubernetes YAML. It specifies internal HTTPS listeners with assigned ports, configures TLS certificate secrets for the cluster, and sets allowed webProxyHosts. This snippet depends on the NiFi custom resource definitions and cluster operator to apply these SSL settings. Input includes TLS secret details like the secret name and creation flag. Outputs a secured NiFi cluster with HTTPS access on specified ports.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  readOnlyConfig:\n    # NifiProperties configuration that will be applied to the node.\n    nifiProperties:\n      webProxyHosts:\n        - nifistandard2.trycatchlearn.fr:8443\n        ...\n  ...\n  listenersConfig:\n    internalListeners:\n      - type: \"https\"\n        name: \"https\"\n        containerPort: 8443\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n```\n\n----------------------------------------\n\nTITLE: Viewing Kubernetes Services for NiFi Cluster Exposure in Console\nDESCRIPTION: Example of a kubectl command output showing the created external Kubernetes services exposing NiFi cluster ports via a LoadBalancer type, including cluster IP, external IP, and port mappings. Validates the external exposure configuration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nkubectl get services\nNAME                TYPE           CLUSTER-IP    EXTERNAL-IP      PORT(S)                                AGE\ncluster-access      LoadBalancer   10.88.21.98   35.180.241.132   443:30421/TCP,80:30521/TCP             20d\n```\n\n----------------------------------------\n\nTITLE: Configuring custom DatabaseAuthorizer in authorizers.xml template for NiFiKOp\nDESCRIPTION: This XML template shows how to configure a custom DatabaseAuthorizer alongside the default file-based authorizer. It demonstrates the required structure for user group providers, access policy providers, and authorizer definitions with templated node identities that NiFiKOp will populate at runtime.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/6_users_authorization/1_custom_user_authorizer.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n{{- $nodeList := .NodeList }}\n{{- $clusterName := .ClusterName }}\n{{- $namespace := .Namespace }}<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>\n<authorizers>\n    <userGroupProvider>\n        <identifier>file-user-group-provider</identifier>\n        <class>org.apache.nifi.authorization.FileUserGroupProvider</class>\n        <property name=\"Users File\">../data/users.xml</property>\n        <property name=\"Legacy Authorized Users File\"></property>\n        <property name=\"Initial User Identity admin\">{{ .ControllerUser }}</property>\n{{- range $i, $host := .NodeList }}\n        <property name=\"Initial User Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n    </userGroupProvider>\n    <userGroupProvider>\n        <identifier>database-user-group-provider</identifier>\n        <class>my.custom.DatabaseUserGroupProvider</class>\n        <!-- Any extra configuration for this provider goes here -->\n{{- range $i, $host := .NodeList }}\n        <property name=\"Initial User Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n    </userGroupProvider>\n    <accessPolicyProvider>\n        <identifier>file-access-policy-provider</identifier>\n        <class>org.apache.nifi.authorization.FileAccessPolicyProvider</class>\n        <property name=\"User Group Provider\">file-user-group-provider</property>\n        <property name=\"Authorizations File\">../data/authorizations.xml</property>\n        <property name=\"Initial Admin Identity\">{{ .ControllerUser }}</property>\n        <property name=\"Legacy Authorized Users File\"></property>\n{{- range $i, $host := .NodeList }}\n        <property name=\"Node Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n\t\t<property name=\"Node Group\"></property>\n    </accessPolicyProvider>\n    <accessPolicyProvider>\n        <identifier>database-access-policy-provider</identifier>\n        <class>my.custom.DatabaseAccessPolicyProvider</class>\n        <!-- Any extra configuration for this provider goes here -->\n{{- range $i, $host := .NodeList }}\n        <property name=\"Node Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n\t\t<property name=\"Node Group\"></property>\n    </accessPolicyProvider>\n    <authorizer>\n        <identifier>managed-authorizer</identifier>\n        <class>org.apache.nifi.authorization.StandardManagedAuthorizer</class>\n        <property name=\"Access Policy Provider\">file-access-policy-provider</property>\n    </authorizer>\n    <authorizer>\n        <identifier>custom-database-authorizer</identifier>\n        <class>my.custom.DatabaseAuthorizer</class>\n        <property name=\"Access Policy Provider\">database-access-policy-provider</property>\n    </authorizer>\n</authorizers>\n```\n\n----------------------------------------\n\nTITLE: Configuring Persistent Storage for NiFi Nodes using Kubernetes PVCs in YAML\nDESCRIPTION: Defines an array of storageConfigs for NiFiKop-managed nodes, specifying persistent volumes for various directories using Kubernetes PersistentVolumeClaims (PVCs). Each storageConfig assigns a mountPath, PVC metadata (labels/annotations), PVC specification (size, access mode, storage class), and a reclaim policy. Requires: NiFiKop Operator, underlying Kubernetes cluster with matching storage classes. Parameters include storage size, mount path inside the container, and cluster-wide storage policies. Output is an array of volume mounts for each node, ensuring persistence for logs, NiFi data, flow files, configurations, content, and provenance repositories. Volume creation depends on having the necessary cluster permissions and available storage.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nstorageConfigs:\n  - mountPath: /opt/nifi/nifi-current/logs\n    name: logs\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/data\n    name: data\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 50Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/extensions\n    name: extensions-repository\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 5Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/flowfile_repository\n    name: flowfile-repository\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/nifi-current/conf\n    name: conf\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 5Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/content_repository\n    name: content-repository\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 500Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/provenance_repository\n    name: provenance-repository\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 500Gi\n```\n\n----------------------------------------\n\nTITLE: NiFiKop CRD Migration Script (Node.js/JavaScript)\nDESCRIPTION: The main Node.js script using the `@kubernetes/client-node` library to migrate NiFiKop resources. It connects to the Kubernetes cluster using the default Kubeconfig, lists resources based on provided type (`--type`) and namespace (`--namespace`) arguments from the `nifi.orange.com` API group, and creates equivalent resources under the `nifi.konpyutaika.com` API group, copying metadata, spec, and status. It handles potential errors during the process. Requires `NODE_TLS_REJECT_UNAUTHORIZED=0` for potentially insecure connections (e.g., self-signed certs) and uses `minimist` to parse command-line arguments.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nprocess.env['NODE_TLS_REJECT_UNAUTHORIZED'] = 0;\nconst k8s = require('@kubernetes/client-node');\n\nconst kc = new k8s.KubeConfig();\nkc.loadFromDefault();\n\nconst k8sApi = kc.makeApiClient(k8s.CustomObjectsApi);\n\nconst KONPYUTAIKA_GROUP = 'nifi.konpyutaika.com';\nconst KONPYUTAIKA_GROUP_VERSION = 'v1alpha1';\nconst ORANGE_GROUP = 'nifi.orange.com';\nconst ORANGE_GROUP_VERSION = 'v1alpha1';\n\nconst call = async (SRC_GRP, SRC_GRP_VER, DST_GRP, DST_GRP_VER, KIND_PLURAL, NAMESPACE) => {\n\tconsole.log(`Listing ${KIND_PLURAL} of ${SRC_GRP}/${SRC_GRP_VER} in ${NAMESPACE}...`);\n\tconst listResources = (await k8sApi.listNamespacedCustomObject(SRC_GRP, SRC_GRP_VER, NAMESPACE, KIND_PLURAL)).body.items;\n\treturn Promise.all(listResources.map(async (resource) => {\n\t\ttry {\n\t\t\tconsole.log(`Found ${resource.kind} \"${resource.metadata.name}\" of ${resource.apiVersion} in ${NAMESPACE}`);\n\n\t\t\tif (resource.metadata.ownerReferences) {\n\t\t\t\tconsole.log(`${resource.kind} ${resource.metadata.name} mananged by something else (ownerRefereces is set).`);\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tconst bodyResource = {\n\t\t\t\tapiVersion: `${DST_GRP}/${DST_GRP_VER}`,\n\t\t\t\tkind: resource.kind,\n\t\t\t\tmetadata: {\n\t\t\t\t\tname: resource.metadata.name,\n\t\t\t\t\tannotations: resource.metadata.annotations,\n\t\t\t\t\tlabels: resource.metadata.labels\n\t\t\t\t},\n\t\t\t\tspec: resource.spec\n\t\t\t};\n\n\t\t\tconsole.log(`Creating ${bodyResource.kind} \"${bodyResource.metadata.name}\" of ${bodyResource.apiVersion} in ${NAMESPACE}...`);\n\t\t\tconst newResource = (await k8sApi.createNamespacedCustomObject(DST_GRP, DST_GRP_VER, NAMESPACE, KIND_PLURAL, bodyResource)).body;\n\t\t\tconsole.log('...done creating.');\n\n\t\t\tconst bodyStatus = {\n\t\t\t\tapiVersion: newResource.apiVersion,\n\t\t\t\tkind: newResource.kind,\n\t\t\t\tmetadata: {\n\t\t\t\t\tname: newResource.metadata.name,\n\t\t\t\t\tresourceVersion: newResource.metadata.resourceVersion\n\t\t\t\t},\n\t\t\t\tstatus: resource.status\n\t\t\t};\n\n\t\t\tconsole.log(`Copying status from ${resource.kind} \"${resource.metadata.name}\" of ${newResource.apiVersion} to ${newResource.kind} \"${newResource.metadata.name}\" of ${newResource.apiVersion} in ${NAMESPACE}...`);\n\t\t\tconst newResourceWithStatus = (await k8sApi.replaceNamespacedCustomObjectStatus(DST_GRP, DST_GRP_VER, NAMESPACE, KIND_PLURAL, bodyStatus.metadata.name, bodyStatus)).body;\n\t\t\tconsole.log('...done copying.');\n\t\t\treturn newResourceWithStatus;\n\t\t}\n\t\tcatch (e) {\n\t\t\tconsole.error(e.body ? e.body.message ? e.body.message : e.body : e);\n\t\t}\n\t}));\n};\n\nconst argv = require('minimist')(process.argv.slice(2));\n\nlet NAMESPACE = argv.namespace ? argv.namespace.length > 0 ? argv.namespace : 'default' : 'default';\nlet KIND_PLURAL = {\n\tcluster: 'nificlusters',\n\tdataflow: 'nifidataflows',\n\tparametercontext: 'nifiparametercontexts',\n\tregistryclient: 'nifiregistryclients',\n\tuser: 'nifiusers',\n\tusergroup: 'nifiusergroups',\n};\n\nif (!argv.type) {\n\tconsole.error('Type not provided');\n\tprocess.exit(1);\n}\n\nif (!KIND_PLURAL[argv.type]) {\n\tconsole.error(`Type ${argv.type} is not one of the following types: ${Object.keys(KIND_PLURAL)}`);\n\tprocess.exit(1);\n}\n\nconsole.log(`########### START: ${KIND_PLURAL[argv.type]} ###########`);\ncall( ORANGE_GROUP, ORANGE_GROUP_VERSION, KONPYUTAIKA_GROUP, KONPYUTAIKA_GROUP_VERSION, KIND_PLURAL[argv.type], NAMESPACE)\n\t.then(r => console.log('############ END ############'))\n\t.catch(e => console.error(e));\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiConnection Resource in YAML\nDESCRIPTION: Defines a NifiConnection custom resource ('connection') using YAML to connect the 'input' (source) and 'output' (destination) NifiDataflows within the 'nifikop' namespace. Specifies source/destination details (name, namespace, subName, type), connection configuration (back pressure thresholds, expiration, label index, visual bends), and update strategy. This resource is processed by NiFiKop to create the actual connection in NiFi.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/4_manage_connections/1_deploy_connection.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiConnection\nmetadata:\n  name: connection\n  namespace: nifikop\nspec:\n  source:\n    name: input\n    namespace: nifikop\n    subName: output\n    type: dataflow\n  destination:\n    name: output\n    namespace: nifikop\n    subName: input\n    type: dataflow\n  configuration:\n    backPressureDataSizeThreshold: 100 GB\n    backPressureObjectThreshold: 10000\n    flowFileExpiration: 1 hour\n    labelIndex: 0\n    bends:\n      - posX: 550\n        posY: 550\n      - posX: 550\n        posY: 440\n      - posX: 550\n        posY: 88\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Defining Prometheus Custom Resource for NiFi Metrics Monitoring Using YAML\nDESCRIPTION: Specifies a Prometheus custom resource configured to operate within the 'monitoring-system' namespace, with parameters such as evaluation interval (30s), debug logging, resource requests for memory, and selectors to match PodMonitors and ServiceMonitors that relate to NiFi metrics collection. The ServiceAccount named 'prometheus' is assigned for execution. This manifest requires Prometheus Operator installed and CRDs available in the cluster and defines scraping behavior specific to the NiFi cluster for precise metrics collection.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: Prometheus\nmetadata:\n  name: prometheus\n  namespace: monitoring-system\nspec:\n  enableAdminAPI: false\n  evaluationInterval: 30s\n  logLevel: debug\n  podMonitorSelector:\n    matchExpressions:\n    - key: release\n      operator: In\n      values:\n      - prometheus\n  resources:\n    requests:\n      memory: 400Mi\n  scrapeInterval: 30s\n  serviceAccountName: prometheus\n  serviceMonitorSelector:\n    matchExpressions:\n    - key: app\n      operator: In\n      values:\n      - nifi-cluster\n```\n\n----------------------------------------\n\nTITLE: Updating KEDA Helm Repository - Console\nDESCRIPTION: This command updates the Helm repository with the latest information from the configured repositories, ensuring that you have access to the latest KEDA chart versions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nhelm repo update\n```\n\n----------------------------------------\n\nTITLE: Defining a NiFi User Group with Access Policies (YAML)\nDESCRIPTION: This YAML snippet defines a `NifiUserGroup` resource named `group-test`. It specifies the NiFi cluster to which the group belongs, the users that are members of the group, and the access policies granted to the group. The access policy grants read access to the `/counters` resource.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/2_manage_users_and_accesses/2_groups_management.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  # Contains the reference to the NifiCluster with the one the registry client is linked.\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  # contains the list of reference to NifiUsers that are part to the group.\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n#     namespace: nifikop\n    - name: nc-controller.nifikop.mgt.cluster.local\n  # defines the list of access policies that will be granted to the group.\n  accessPolicies:\n      # defines the kind of access policy, could be \"global\" or \"component\".\n    - type: global\n      # defines the kind of action that will be granted, could be \"read\" or \"write\"\n      action: read\n      # resource defines the kind of resource targeted by this access policies, please refer to the following page:\n      #\thttps://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#access-policies\n      resource: /counters\n#      # componentType is used if the type is \"component\", it's allow to define the kind of component on which is the\n#      # access policy\n#      componentType: \"process-groups\"\n#      # componentId is used if the type is \"component\", it's allow to define the id of the component on which is the\n#      # access policy\n#      componentId: \"\"\n```\n\n----------------------------------------\n\nTITLE: Manually Applying Nifikop CRDs with kubectl\nDESCRIPTION: Provides the `kubectl apply` commands to manually install all necessary Nifikop Custom Resource Definitions (CRDs) from their GitHub source URLs. This step is required if installing the Helm chart with the `--skip-crds` flag to avoid automatic CRD deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifinodegroupautoscalers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificonnections.yaml\n```\n\n----------------------------------------\n\nTITLE: Deleting Helm Release (bash)\nDESCRIPTION: Deletes a specific Helm release and removes all associated Kubernetes resources from the cluster using the `helm del` command. The example uses `nifikop` as the release name. This marks the release as deleted but keeps its history by default. Requires Helm installed and configured for the target Kubernetes cluster, and the specified release must exist.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nhelm del nifikop\n```\n\n----------------------------------------\n\nTITLE: Installing Nifikop with Parameters\nDESCRIPTION: This command installs the Nifikop chart and sets the `namespaces` parameter.  It sets the namespace where the chart will be deployed.  The installation utilizes the `--set` flag to define the parameter value.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop --set namespaces={\\\"nifikop\\\"}\n```\n\n----------------------------------------\n\nTITLE: Adding a new node configuration in Kubernetes YAML for NiFi Cluster\nDESCRIPTION: This YAML snippet illustrates how to add a new node to the existing NiFi cluster by extending the 'nodes' list in the NifiCluster custom resource. It specifies a unique node ID, configuration group, and resource settings to provision a new NiFi pod within the cluster. Dependencies include the NifiCluster CRD and Kubernetes API. The snippet facilitates dynamic scaling by application of the updated configuration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n  clusterManager: zookeeper\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.12.1\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n    - id: 2\n      nodeConfigGroup: \"default_group\"\n# >>>> START: The new node\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n# <<<< END\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: Installing Nifikop Helm Chart with Custom Release Name - Bash\nDESCRIPTION: This command installs the Nifikop chart to your Kubernetes cluster using a custom release name. No additional parameters are set, so defaults are used except for the explicit release name. Helm and cluster access are required; output is a deployed set of Nifikop resources under the specified release.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nhelm install <release name> konpyutaika/nifikop\n```\n\n----------------------------------------\n\nTITLE: Purging Nifikop Helm Release Completely in Bash\nDESCRIPTION: This Bash command permanently deletes the Nifikop Helm release, including all associated metadata, thus making the release name reusable. Dependency: Helm CLI. Parameters: release name, --purge flag. Output: full Helm release removal from cluster history.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nhelm delete --purge nifikop\n```\n\n----------------------------------------\n\nTITLE: Defining Prometheus RBAC Resources - yaml\nDESCRIPTION: Defines a ServiceAccount, ClusterRole, and ClusterRoleBinding for Prometheus within Kubernetes. Necessary dependencies are a working Kubernetes cluster and RBAC enabled. ServiceAccount grants identity, ClusterRole grants read access to cluster resources for scraping metrics, and ClusterRoleBinding binds the role to the account. Inputs are standard Kubernetes RBAC resource definitions; outputs are the existence of service accounts and roles tied to Prometheus.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: prometheus\n  namespace: monitoring-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: prometheus\nrules:\n- apiGroups: [\"\"]\n  resources:\n  - nodes\n  - nodes/metrics\n  - services\n  - endpoints\n  - pods\n  verbs: [\"get\", \"list\", \"watch\"]\n- apiGroups: [\"\"]\n  resources:\n  - configmaps\n  verbs: [\"get\"]\n- apiGroups:\n  - networking.k8s.io\n  resources:\n  - ingresses\n  verbs: [\"get\", \"list\", \"watch\"]\n- nonResourceURLs: [\"/metrics\"]\n  verbs: [\"get\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: prometheus\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: prometheus\nsubjects:\n- kind: ServiceAccount\n  name: prometheus\n  namespace: monitoring-system\n\n```\n\n----------------------------------------\n\nTITLE: Configuring nifi.properties via ConfigMap, Secret, and Override (YAML)\nDESCRIPTION: This YAML snippet demonstrates how to override the default `nifi.properties` configuration using a ConfigMap (`overrideConfigMap`), a Secret (`overrideSecretConfig`), and direct configuration overrides (`overrideConfigs`). It shows the precedence order: Secret > ConfigMap > Override > Default.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nnifiProperties:\n      # Additionnal nifi.properties configuration that will override the one produced based on template and\n      # configuration\n      overrideConfigMap:\n        # The key of the value,in data content, that we want use.\n        data: nifi.properties\n        # Name of the configmap that we want to refer.\n        name: raw\n        # Namespace where is located the secret that we want to refer.\n        namespace: nifikop\n      # Additionnal nifi.properties configuration that will override the one produced based\n      #\t on template, configurations, overrideConfigMap and overrideConfigs.\n      overrideSecretConfig:\n        # The key of the value,in data content, that we want use.\n        data: nifi.properties\n        # Name of the configmap that we want to refer.\n        name: raw\n        # Namespace where is located the secret that we want to refer.\n        namespace: nifikop\n      # Additionnal nifi.properties configuration that will override the one produced based\n      #\t on template, configurations and overrideConfigMap\n      overrideConfigs: |\n        nifi.ui.banner.text=NiFiKop\n        nifi.sensitive.props.key=thisIsABadSensitiveKeyPassword\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop CRDs manually\nDESCRIPTION: Applies the Custom Resource Definitions for NiFiKop components including NiFiClusters, NiFiUsers, NiFiUserGroups, NiFiDataflows, NiFiParameterContexts, and NiFiRegistryClients when not using Helm's CRD installation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/2_deploy_nifikop/1_quick_start.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing Zookeeper Cluster using Helm\nDESCRIPTION: Helm command to install a 3-node Zookeeper cluster with specified resource limits, storage class, and network policy settings. This Zookeeper cluster is required for NiFi operation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nhelm install zookeeper bitnami/zookeeper \\\n    --namespace=zookeeper \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set global.storageClass=standard \\\n    --set networkPolicy.enabled=true \\\n    --set replicaCount=3 \\\n    --create-namespace\n```\n\n----------------------------------------\n\nTITLE: Install NiFiKop Chart (dry run) - Helm\nDESCRIPTION: This command performs a dry run installation of the NiFiKop Helm chart, simulating the installation process without actually deploying resources to the cluster.  It sets the logLevel to Debug and specifies the namespaces.  This is useful for validating the chart configuration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop \\\n    --dry-run \\\n    --set logLevel=Debug \\\n    --set namespaces={\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Verifying NiFiKop Operator Pod Status - Bash\nDESCRIPTION: This 'kubectl get pods' command checks the status of pods in the 'nifikop' namespace to confirm the NiFiKop operator is running correctly. The output displays pod name, readiness, status, restart count, and uptime.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/6_contributing/1_developer_guide.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get pods -n nifikop\nNAME                                                READY   STATUS    RESTARTS   AGE\nskeleton-nifikop-8946b89dc-4cfs9   1/1     Running   0          7m45s\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio DestinationRule for HTTPS NiFi Traffic\nDESCRIPTION: This YAML configuration defines an Istio DestinationRule that re-encrypts HTTP traffic to HTTPS before reaching NiFi nodes. It also implements sticky sessions using cookies to ensure consistent routing of user requests.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: DestinationRule\nmetadata:\n  name: nifi-dr\nspec:\n  host: <service-name>.<namespace>.svc.cluster.local\n  trafficPolicy:\n    tls:\n      mode: SIMPLE\n    loadBalancer:\n      consistentHash:\n        httpCookie:\n          name: __Secure-Authorization-Bearer\n          ttl: 0s\n```\n\n----------------------------------------\n\nTITLE: Creating Secured NiFi Cluster Resource Using kubectl Console Command\nDESCRIPTION: This command creates the NifiCluster resource in Kubernetes from the previously configured YAML manifest. It triggers NiFiKop operator to deploy and configure the secured NiFi cluster with OIDC authentication and load balancing as specified.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/blog/2020-05-20-secured_nifi_cluster_on_gcp.md#_snippet_9\n\nLANGUAGE: console\nCODE:\n```\nkubectl create -f kubernetes/nifi/secured_nifi_cluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining Managed User Groups in YAML\nDESCRIPTION: This YAML snippet defines managed user groups (Admins and Readers) within a NiFi cluster's specification. It specifies users with their identities and names, enabling the operator to automatically create and manage the corresponding NifiUsers and NifiUserGroups. The `managedAdminUsers` and `managedReaderUsers` fields are used to configure the user groups.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/3_manage_nifi/2_manage_users_and_accesses/3_managed_groups.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  ...\n  oneNifiNodePerNode: false\n  propagateLabels: true\n  managedAdminUsers:\n    -  identity: \"alexandre.guitton@konpyutaika.com\"\n       name: \"aguitton\"\n    -  identity: \"nifiuser@konpyutaika.com\"\n       name: \"nifiuser\"\n  managedReaderUsers:\n    -  identity: \"toto@konpyutaika.com\"\n       name: \"toto\"\n    ...\n```\n\n----------------------------------------\n\nTITLE: Installing cert-manager with Helm 3 (YAML Deployment Method)\nDESCRIPTION: This sequence sets up cert-manager via Helm, first adding the Jetstack Helm repository, then installing cert-manager in the 'cert-manager' namespace after deploying its CRDs. It requires Helm v3 and Kubernetes cluster access.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/2_deploy_nifikop/1_quick_start.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Install CustomResourceDefinitions first\nkubectl apply --validate=false -f \\\n   https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.crds.yaml\n\n# Add the jetstack helm repo\nhelm repo add jetstack https://charts.jetstack.io\nhelm repo update\n\n# Create namespace and install cert-manager\nhelm install cert-manager \\\n    --namespace cert-manager \\\n    --version v1.7.2 jetstack/cert-manager\n```\n\n----------------------------------------\n\nTITLE: Defining NiFiDataflow Custom Resource in YAML\nDESCRIPTION: This snippet demonstrates how to define a NiFiDataflow Custom Resource for the nifikop Kubernetes operator. It requires the nifikop CRDs to be installed in the cluster beforehand. Key parameters include parentProcessGroupID (deployment context), bucketId, flowId, flowVersion, and update strategies such as syncMode and updateStrategy. Dependencies include a deployed NiFiCluster, linked RegistryClient, and optional ParameterContext, all specified via references. Inputs are YAML fields as defined per nifikop CRD, and outputs are a Kubernetes manifest for applying via kubectl. The configuration must be applied in a namespace with the proper RBAC permissions; missing or invalid references will lead to creation errors.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/5_references/5_nifi_dataflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  flowPosition:\n    posX: 0\n    posY: 0\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: squidflow\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: nifikop\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Defining NifiConnection Schema in YAML\nDESCRIPTION: This snippet defines a `NifiConnection` resource using YAML, specifying its API version, kind, metadata, and spec. It outlines the source and destination components, configuration details such as flow file expiration, and update strategy. The purpose is to describe a NiFi connection for the Konpyutaika platform.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/5_references/8_nifi_connection.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiConnection\nmetadata:\n  name: connection\n  namespace: instances\nspec:\n  source:\n    name: input\n    namespace: instances\n    subName: output_1\n    type: dataflow\n  destination:\n    name: output\n    namespace: instances\n    subName: input_1\n    type: dataflow\n  configuration:\n    flowFileExpiration: 1 hour\n    backPressureDataSizeThreshold: 100 GB\n    backPressureObjectThreshold: 10000\n    loadBalanceStrategy: PARTITION_BY_ATTRIBUTE\n    loadBalancePartitionAttribute: partition_attribute\n    loadBalanceCompression: DO_NOT_COMPRESS\n    prioritizers: \n      - NewestFlowFileFirstPrioritizer\n      - FirstInFirstOutPrioritizer\n    labelIndex: 0\n    bends:\n      - posX: 550\n        posY: 550\n      - posX: 550\n        posY: 440\n      - posX: 550\n        posY: 88\n  updateStrategy: drain\n```\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio Gateway for HTTP Access to NiFi (YAML)\nDESCRIPTION: Defines an Istio Gateway named `nifi-gateway` to listen on port 80 for HTTP traffic destined for `nifi.my-domain.com`, using the default `istio: ingressgateway` selector. This serves as the entry point for external HTTP requests to the NiFi cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: nifi-gateway\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - nifi.my-domain.com\n```\n\n----------------------------------------\n\nTITLE: Kubectl Get Services Output Example\nDESCRIPTION: This provides an example output of `kubectl get services` command. It depicts the configuration of `cluster-access` service, exposing on ports 443 (HTTPS) and 80 (HTTP), using the type 'LoadBalancer' and shows cluster IP and external IP.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nkubectl get services\nNAME                TYPE           CLUSTER-IP    EXTERNAL-IP      PORT(S)                                AGE\ncluster-access      LoadBalancer   10.88.21.98   35.180.241.132   443:30421/TCP,80:30521/TCP             20d\n```\n\n----------------------------------------\n\nTITLE: NiFi Cluster Service Configuration\nDESCRIPTION: YAML configuration for the ClusterIP Service that handles NiFi traffic, defining ports and listener configurations.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nspec:  \n  externalServices:  \n    - name: \"nifi-cluster\"\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8443\n            internalListenerName: \"https\"\n```\n\n----------------------------------------\n\nTITLE: Listing Deployed Helm Charts\nDESCRIPTION: Displays a list of all currently deployed Helm releases within the active Kubernetes context/namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nhelm list\n```\n\n----------------------------------------\n\nTITLE: Build and Install Kubectl NiFiKop Plugin\nDESCRIPTION: This command builds the `kubectl-nifikop` executable and copies it to `/usr/local/bin`, making it accessible in the user's PATH. It requires `make` and `sudo` privileges.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/2_deploy_nifikop/3_kubectl_plugin.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nmake kubectl-nifikop && sudo cp ./bin/kubectl-nifikop /usr/local/bin\n```\n\n----------------------------------------\n\nTITLE: Migrating NiFiKop CRDs Using Kubernetes Client in Node.js\nDESCRIPTION: A Node.js script that migrates NiFiKop custom resources from the old `nifi.orange.com/v1alpha1` group to the new `nifi.konpyutaika.com/v1alpha1` group inside a specified Kubernetes namespace. The script lists all resources of a specified type in the old CRD group, filters out resources with ownerReferences to avoid uncontrolled changes, copies essential metadata (name, labels, annotations) and spec to the new CRDs, and replicates the resource status. It uses `@kubernetes/client-node` for Kubernetes API interactions and `minimist` for command-line argument parsing. It requires running with the old operator stopped and both CRDs installed. Key parameters include resource type and namespace, specified on the command line. Expected output includes console logs detailing the migration progress and error messages if operations fail.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nprocess.env['NODE_TLS_REJECT_UNAUTHORIZED'] = 0;\nconst k8s = require('@kubernetes/client-node');\n\nconst kc = new k8s.KubeConfig();\nkc.loadFromDefault();\n\nconst k8sApi = kc.makeApiClient(k8s.CustomObjectsApi);\n\nconst KONPYUTAIKA_GROUP = 'nifi.konpyutaika.com';\nconst KONPYUTAIKA_GROUP_VERSION = 'v1alpha1';\nconst ORANGE_GROUP = 'nifi.orange.com';\nconst ORANGE_GROUP_VERSION = 'v1alpha1';\n\nconst call = async (SRC_GRP, SRC_GRP_VER, DST_GRP, DST_GRP_VER, KIND_PLURAL, NAMESPACE) => {\n\tconsole.log(`Listing ${KIND_PLURAL} of ${SRC_GRP}/${SRC_GRP_VER} in ${NAMESPACE}...`);\n\tconst listResources = (await k8sApi.listNamespacedCustomObject(SRC_GRP, SRC_GRP_VER, NAMESPACE, KIND_PLURAL)).body.items;\n\treturn Promise.all(listResources.map(async (resource) => {\n\t\ttry {\n\t\t\tconsole.log(`Found ${resource.kind} \\\"${resource.metadata.name}\\\" of ${resource.apiVersion} in ${NAMESPACE}`);\n\n\t\t\tif (resource.metadata.ownerReferences) {\n\t\t\t\tconsole.log(`${resource.kind} ${resource.metadata.name} mananged by something else (ownerRefereces is set).`);\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tconst bodyResource = {\n\t\t\t\tapiVersion: `${DST_GRP}/${DST_GRP_VER}`,\n\t\t\t\tkind: resource.kind,\n\t\t\t\tmetadata: {\n\t\t\t\t\tname: resource.metadata.name,\n\t\t\t\t\tannotations: resource.metadata.annotations,\n\t\t\t\t\tlabels: resource.metadata.labels\n\t\t\t\t},\n\t\t\t\tspec: resource.spec\n\t\t\t};\n\n\t\t\tconsole.log(`Creating ${bodyResource.kind} \\\"${bodyResource.metadata.name}\\\" of ${bodyResource.apiVersion} in ${NAMESPACE}...`);\n\t\t\tconst newResource = (await k8sApi.createNamespacedCustomObject(DST_GRP, DST_GRP_VER, NAMESPACE, KIND_PLURAL, bodyResource)).body;\n\t\t\tconsole.log('...done creating.');\n\n\t\t\tconst bodyStatus = {\n\t\t\t\tapiVersion: newResource.apiVersion,\n\t\t\t\tkind: newResource.kind,\n\t\t\t\tmetadata: {\n\t\t\t\t\tname: newResource.metadata.name,\n\t\t\t\t\tresourceVersion: newResource.metadata.resourceVersion\n\t\t\t\t},\n\t\t\t\tstatus: resource.status\n\t\t\t};\n\n\t\t\tconsole.log(`Copying status from ${resource.kind} \\\"${resource.metadata.name}\\\" of ${newResource.apiVersion} to ${newResource.kind} \\\"${newResource.metadata.name}\\\" of ${newResource.apiVersion} in ${NAMESPACE}...`);\n\t\t\tconst newResourceWithStatus = (await k8sApi.replaceNamespacedCustomObjectStatus(DST_GRP, DST_GRP_VER, NAMESPACE, KIND_PLURAL, bodyStatus.metadata.name, bodyStatus)).body;\n\t\t\tconsole.log('...done copying.');\n\t\t\treturn newResourceWithStatus;\n\t\t}\n\t\tcatch (e) {\n\t\t\tconsole.error(e.body ? e.body.message ? e.body.message : e.body : e);\n\t\t}\n\t}));\n};\n\nconst argv = require('minimist')(process.argv.slice(2));\n\nlet NAMESPACE = argv.namespace ? argv.namespace.length > 0 ? argv.namespace : 'default' : 'default';\nlet KIND_PLURAL = {\n\tcluster: 'nificlusters',\n\tdataflow: 'nifidataflows',\n\tparametercontext: 'nifiparametercontexts',\n\tregistryclient: 'nifiregistryclients',\n\tuser: 'nifiusers',\n\tusergroup: 'nifiusergroups',\n};\n\nif (!argv.type) {\n\tconsole.error('Type not provided');\n\tprocess.exit(1);\n}\n\nif (!KIND_PLURAL[argv.type]) {\n\tconsole.error(`Type ${argv.type} is not one of the following types: ${Object.keys(KIND_PLURAL)}`);\n\tprocess.exit(1);\n}\n\nconsole.log(`########### START: ${KIND_PLURAL[argv.type]} ###########`);\ncall( ORANGE_GROUP, ORANGE_GROUP_VERSION, KONPYUTAIKA_GROUP, KONPYUTAIKA_GROUP_VERSION, KIND_PLURAL[argv.type], NAMESPACE)\n\t.then(r => console.log('############ END ############'))\n\t.catch(e => console.error(e));\n```\n\n----------------------------------------\n\nTITLE: NifiUserGroup Data Model Schema\nDESCRIPTION: This section specifies the data model schema for NifiUserGroup, including metadata, specification, and status fields. It provides detailed field descriptions, data types, and default or required constraints, essential for developers implementing or interacting with the NiFi UserGroup API.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/5_references/6_nifi_usergroup.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n# NifiUserGroup root schema\n# Defines the API resource including metadata, desired state, and observed state.\n\n```\n\n----------------------------------------\n\nTITLE: Running NiFiKop Locally\nDESCRIPTION: Command to run the NiFiKop operator locally using make. This executes the operator in the default namespace using the Kubernetes config at $HOME/.kube/config.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/6_contributing/1_developer_guide.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmake run\n```\n\n----------------------------------------\n\nTITLE: Installing Nifikop Helm Chart Skipping CRDs\nDESCRIPTION: Installs the Nifikop Helm chart from a local directory (`./helm/nifikop`) while skipping the automatic installation of Custom Resource Definitions (CRDs) using the `--skip-crds` flag. Requires CRDs to be manually installed beforehand.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nhelm install --name nifikop ./helm/nifikop --set namespaces={\"nifikop\"} --skip-crds\n```\n\n----------------------------------------\n\nTITLE: Example NifiNodeGroupAutoscaler Manifest (YAML)\nDESCRIPTION: This YAML manifest defines a `NifiNodeGroupAutoscaler` resource named `nifinodegroupautoscaler-sample`. It targets the `nificluster-name` NifiCluster in the `nifikop` namespace, specifically managing nodes in the `default-node-group` identified by the provided `nodeLabelsSelector`. It configures the autoscaler to use the 'simple' strategy for upscaling and 'lifo' (Last-In, First-Out) for downscaling.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/5_references/7_nifi_nodegroup_autoscaler.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiNodeGroupAutoscaler\nmetadata:\n  name: nifinodegroupautoscaler-sample\nspec:\n  # contains the reference to the NifiCluster with the one the node group autoscaler is linked.\n  clusterRef:\n    name: nificluster-name\n    namespace: nifikop\n  # defines the id of the NodeConfig contained in NifiCluster.Spec.NodeConfigGroups\n  nodeConfigGroupId: default-node-group\n  # The selector used to identify nodes in NifiCluster.Spec.Nodes this autoscaler will manage\n  # Use Node.Labels in combination with this selector to clearly define which nodes will be managed by this autoscaler \n  nodeLabelsSelector: \n    matchLabels:\n      nifi_cr: nificluster-name\n      nifi_node_group: default-node-group\n  # the strategy used to decide how to add nodes to a nifi cluster\n  upscaleStrategy: simple\n  # the strategy used to decide how to remove nodes from an existing cluster\n  downscaleStrategy: lifo\n```\n\n----------------------------------------\n\nTITLE: NiFiKop CRD Migration Script using Kubernetes Client (JavaScript)\nDESCRIPTION: A Node.js script utilizing the `@kubernetes/client-node` library to migrate NiFiKop custom resources within a Kubernetes cluster. It connects to the Kubernetes API (disabling TLS verification), lists resources from the old API group (`nifi.orange.com/v1alpha1`), creates corresponding resources (copying spec, labels, annotations) in the new group (`nifi.konpyutaika.com/v1alpha1`), and then copies the status subresource. It uses `minimist` to accept the resource type (`--type`) and namespace (`--namespace`) via command-line arguments.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/7_upgrade/1_v0.7.x_to_v0.8.0.md#_snippet_3\n\nLANGUAGE: js\nCODE:\n```\nprocess.env['NODE_TLS_REJECT_UNAUTHORIZED'] = 0;\nconst k8s = require('@kubernetes/client-node');\n\nconst kc = new k8s.KubeConfig();\nkc.loadFromDefault();\n\nconst k8sApi = kc.makeApiClient(k8s.CustomObjectsApi);\n\nconst KONPYUTAIKA_GROUP = 'nifi.konpyutaika.com';\nconst KONPYUTAIKA_GROUP_VERSION = 'v1alpha1';\nconst ORANGE_GROUP = 'nifi.orange.com';\nconst ORANGE_GROUP_VERSION = 'v1alpha1';\n\nconst call = async (SRC_GRP, SRC_GRP_VER, DST_GRP, DST_GRP_VER, KIND_PLURAL, NAMESPACE) => {\n\tconsole.log(`Listing ${KIND_PLURAL} of ${SRC_GRP}/${SRC_GRP_VER} in ${NAMESPACE}...`);\n\tconst listResources = (await k8sApi.listNamespacedCustomObject(SRC_GRP, SRC_GRP_VER, NAMESPACE, KIND_PLURAL)).body.items;\n\treturn Promise.all(listResources.map(async (resource) => {\n\t\ttry {\n\t\t\tconsole.log(`Found ${resource.kind} \"${resource.metadata.name}\" of ${resource.apiVersion} in ${NAMESPACE}`);\n\n\t\t\tif (resource.metadata.ownerReferences) {\n\t\t\t\tconsole.log(`${resource.kind} ${resource.metadata.name} mananged by something else (ownerRefereces is set).`);\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tconst bodyResource = {\n\t\t\t\tapiVersion: `${DST_GRP}/${DST_GRP_VER}`,\n\t\t\t\tkind: resource.kind,\n\t\t\t\tmetadata: {\n\t\t\t\t\tname: resource.metadata.name,\n\t\t\t\t\tannotations: resource.metadata.annotations,\n\t\t\t\t\tlabels: resource.metadata.labels\n\t\t\t\t},\n\t\t\t\tspec: resource.spec\n\t\t\t};\n\n\t\t\tconsole.log(`Creating ${bodyResource.kind} \"${bodyResource.metadata.name}\" of ${bodyResource.apiVersion} in ${NAMESPACE}...`);\n\t\t\tconst newResource = (await k8sApi.createNamespacedCustomObject(DST_GRP, DST_GRP_VER, NAMESPACE, KIND_PLURAL, bodyResource)).body;\n\t\t\tconsole.log('...done creating.');\n\n\t\t\tconst bodyStatus = {\n\t\t\t\tapiVersion: newResource.apiVersion,\n\t\t\t\tkind: newResource.kind,\n\t\t\t\tmetadata: {\n\t\t\t\t\tname: newResource.metadata.name,\n\t\t\t\t\tresourceVersion: newResource.metadata.resourceVersion\n\t\t\t\t},\n\t\t\t\tstatus: resource.status\n\t\t\t};\n\n\t\t\tconsole.log(`Copying status from ${resource.kind} \"${resource.metadata.name}\" of ${newResource.apiVersion} to ${newResource.kind} \"${newResource.metadata.name}\" of ${newResource.apiVersion} in ${NAMESPACE}...`);\n\t\t\tconst newResourceWithStatus = (await k8sApi.replaceNamespacedCustomObjectStatus(DST_GRP, DST_GRP_VER, NAMESPACE, KIND_PLURAL, bodyStatus.metadata.name, bodyStatus)).body;\n\t\t\tconsole.log('...done copying.');\n\t\t\treturn newResourceWithStatus;\n\t\t}\n\t\tcatch (e) {\n\t\t\tconsole.error(e.body ? e.body.message ? e.body.message : e.body : e);\n\t\t}\n\t}));\n};\n\nconst argv = require('minimist')(process.argv.slice(2));\n\nlet NAMESPACE = argv.namespace ? argv.namespace.length > 0 ? argv.namespace : 'default' : 'default';\nlet KIND_PLURAL = {\n\tcluster: 'nificlusters',\n\tdataflow: 'nifidataflows',\n\tparametercontext: 'nifiparametercontexts',\n\tregistryclient: 'nifiregistryclients',\n\tuser: 'nifiusers',\n\tusergroup: 'nifiusergroups',\n};\n\nif (!argv.type) {\n\tconsole.error('Type not provided');\n\tprocess.exit(1);\n}\n\nif (!KIND_PLURAL[argv.type]) {\n\tconsole.error(`Type ${argv.type} is not one of the following types: ${Object.keys(KIND_PLURAL)}`);\n\tprocess.exit(1);\n}\n\nconsole.log(`########### START: ${KIND_PLURAL[argv.type]} ###########`);\ncall( ORANGE_GROUP, ORANGE_GROUP_VERSION, KONPYUTAIKA_GROUP, KONPYUTAIKA_GROUP_VERSION, KIND_PLURAL[argv.type], NAMESPACE)\n\t.then(r => console.log('############ END ############'))\n\t.catch(e => console.error(e));\n```\n\n----------------------------------------\n\nTITLE: Listing Managed NiFiUserGroups using kubectl\nDESCRIPTION: This command shows how to retrieve the list of managed NifiUserGroups created by the operator using `kubectl`. This allows you to verify the creation and management of the `managed-admins`, `managed-nodes`, and `managed-readers` groups. Requires `kubectl` configured to connect to the Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/2_manage_users_and_accesses/3_managed_groups.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nkubectl get -n nifikop nifiusergroups.nifi.konpyutaika.com \nNAME              AGE\nmanaged-admins    6d7h\nmanaged-nodes     6d7h\nmanaged-readers   6d7h\n```\n\n----------------------------------------\n\nTITLE: Sample NiFiKop Migration package.json Configuration - JSON\nDESCRIPTION: This snippet is a complete package.json for the migration project. It specifies the project metadata, entry point, dependencies, scripts, and descriptive fields. Required dependencies include @kubernetes/client-node and minimist. The 'start' script executes index.js with Node.js, and the 'test' script is a placeholder. No special parameters are needed; adjust names or descriptions as desired.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"nifikop_crd_migration\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Script to migrate from the old CRDs to the new CRDs.\",\n  \"main\": \"index.js\",\n  \"scripts\": {\n    \"start\": \"node --no-warnings index.js\",\n    \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\"\n  },\n  \"keywords\": [\n    \"K8S\",\n    \"NiFiKop\",\n    \"CRDs\"\n  ],\n  \"license\": \"ISC\",\n  \"dependencies\": {\n    \"@kubernetes/client-node\": \"^0.16.3\",\n    \"minimist\": \"^1.2.6\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenID Connect in nifi.properties - Shell\nDESCRIPTION: This shell snippet provides the necessary configuration lines that should be added to the `nifi.properties` file to enable OpenID Connect (OIDC) authentication for a NiFi cluster. This includes defining the mapping pattern for the Distinguished Name (DN) to extract the Common Name (CN) and configuring transformations. This configuration supports multiple identity providers.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/5_users_authentication/1_oidc.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nnifi.security.identity.mapping.pattern.dn=CN=([^,]*)(?:, (?:O|OU)=.*)?\nnifi.security.identity.mapping.value.dn=$1\nnifi.security.identity.mapping.transform.dn=NONE\n```\n\n----------------------------------------\n\nTITLE: Defining an inherited NifiParameterContext using YAML\nDESCRIPTION: This YAML manifest defines a NifiParameterContext named 'dataflow-lifecycle-child'. It links to the same NifiCluster and references the same secret as the first example. Critically, it demonstrates inheritance by referencing the 'dataflow-lifecycle' parameter context, allowing the child context to inherit its parameters while also defining or overriding its own (like the 'test' parameter here).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/5_references/4_nifi_parameter_context.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle-child\nspec:\n  description: \"It is a child test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  inheritedParameterContexts:\n    - name: dataflow-lifecycle\n  parameters:\n    - name: test\n      value: toto-child\n      description: tutu (child)\n```\n\n----------------------------------------\n\nTITLE: NiFiCluster with External DNS and IssuerRef (YAML)\nDESCRIPTION: This YAML snippet demonstrates configuring a NiFiCluster to use an existing cert-manager Issuer for SSL certificate management. It sets `clusterSecure` and `siteToSiteSecure` to `true`, enables `useExternalDNS`, and specifies the `issuerRef` to point to the desired Issuer resource.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  clusterSecure: true\n  siteToSiteSecure: true\n  ...\n  listenersConfig:\n    clusterDomain: <DNS zone name>\n    useExternalDNS: true\n    ...\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n      issuerRef:\n        name: letsencrypt-staging\n        kind: Issuer\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for NiFiKop Operator from Local Branch - Bash\nDESCRIPTION: This snippet sets the environment variable 'DOCKER_REPO_BASE' to specify the Docker repository base, then invokes the 'make docker-build' target to build a Docker image of the NiFiKop operator from the current branch. It requires Docker installed and configured locally.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/6_contributing/1_developer_guide.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nexport DOCKER_REPO_BASE={your-docker-repo}\nmake docker-build\n```\n\n----------------------------------------\n\nTITLE: Initializing Node.js Project and Installing Dependencies for Kubernetes CRD Migration Script\nDESCRIPTION: Initial setup commands to initialize a Node.js project and install required dependencies: '@kubernetes/client-node' for Kubernetes API interaction and 'minimist' for command-line argument parsing. These dependencies are prerequisites to run the migration script that copies NiFiKop CRDs between API groups on Kubernetes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm init -y\nnpm install @kubernetes/client-node@0.16.3 minimist@1.2.6\n```\n\n----------------------------------------\n\nTITLE: Deploying Prometheus RBAC Resources in Kubernetes\nDESCRIPTION: Creates the ServiceAccount, ClusterRole, and ClusterRoleBinding resources required for Prometheus to access and scrape metrics from Kubernetes resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: prometheus\n  namespace: monitoring-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: prometheus\nrules:\n- apiGroups: [\"\"]\n  resources:\n  - nodes\n  - nodes/metrics\n  - services\n  - endpoints\n  - pods\n  verbs: [\"get\", \"list\", \"watch\"]\n- apiGroups: [\"\"]\n  resources:\n  - configmaps\n  verbs: [\"get\"]\n- apiGroups:\n  - networking.k8s.io\n  resources:\n  - ingresses\n  verbs: [\"get\", \"list\", \"watch\"]\n- nonResourceURLs: [\"/metrics\"]\n  verbs: [\"get\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: prometheus\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: prometheus\nsubjects:\n- kind: ServiceAccount\n  name: prometheus\n  namespace: monitoring-system\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Node Group Autoscaler Configuration in YAML\nDESCRIPTION: This YAML snippet defines a Kubernetes custom resource of kind 'NifiNodeGroupAutoscaler' used to manage autoscaling of NiFi cluster nodes based on configuration group 'auto_scaling'. It includes references to the NiFi cluster namespace and name, node configuration overrides for NiFi properties, node label selectors for managed nodes, and strategies for upscale (simple) and downscale (lifo). This configuration enables dynamic, event-driven scaling of NiFi node groups.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiNodeGroupAutoscaler\nmetadata:\n  name: nifinodegroupautoscaler-sample\n  namespace: clusters\nspec:\n  # contains the reference to the NifiCluster with the one the node group autoscaler is linked.\n  clusterRef:\n    name: cluster\n    namespace: clusters\n  # defines the id of the NodeConfig contained in NifiCluster.Spec.NodeConfigGroups\n  nodeConfigGroupId: auto_scaling\n  # readOnlyConfig can be used to pass Nifi node config\n  # which has type read-only these config changes will trigger rolling upgrade\n  readOnlyConfig:\n    nifiProperties:\n      overrideConfigs: |\n        nifi.ui.banner.text=NiFiKop - Scale Group\n  # This is an example of a node config you can apply to each replica in this node group.\n  # Any settings here will override those in the configured nodeConfigGroupId\n#  nodeConfig:\n#    nodeSelector:\n#      node_type: high-mem\n  # The selector used to identify nodes in NifiCluster.Spec.Nodes this autoscaler will manage\n  # Use Node.Labels in combination with this selector to clearly define which nodes will be managed by this autoscaler \n  nodeLabelsSelector: \n    matchLabels:\n      nifi_cr: cluster\n      nifi_node_group: auto-scaling\n  # the strategy used to decide how to add nodes to a nifi cluster\n  upscaleStrategy: simple\n  # the strategy used to decide how to remove nodes from an existing cluster\n  downscaleStrategy: lifo\n```\n\n----------------------------------------\n\nTITLE: Defining Pod Ports for NiFi on Kubernetes (YAML)\nDESCRIPTION: This snippet shows the ports configuration of a deployed pod. It defines the ports for various NiFi internal services, such as HTTPS, cluster communication, Site-to-Site communication, Prometheus metrics, and load balancing. This should align with the ports defined in the internal listeners config.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nports:\n    - containerPort: 8443\n      name: https\n      protocol: TCP\n    - containerPort: 6007\n      name: cluster\n      protocol: TCP\n    - containerPort: 6342\n      name: load-balance\n      protocol: TCP\n    - containerPort: 10000\n      name: s2s\n      protocol: TCP\n    - containerPort: 9090\n      name: prometheus\n      protocol: TCP\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for NiFiKop Operator Bash\nDESCRIPTION: This sequence sets the Docker registry base via environment variable and invokes the Makefile target to build a Docker image from the local branch. Dependencies include Docker and GNU Make. The DOCKER_REGISTRY_BASE parameter should be set to your desired container registry. Outputs a locally built image ready for pushing.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/6_contributing/1_developer_guide.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nexport DOCKER_REGISTRY_BASE={your-docker-repo}\nmake docker-build\n```\n\n----------------------------------------\n\nTITLE: Installing Zookeeper with Bitnami Helm Chart (Bash)\nDESCRIPTION: Adds the Bitnami Helm repository and installs Zookeeper in the 'zookeeper' namespace with specified resource requests and limits, enabling network policies and setting the number of replicas. Users should customize the storageClass parameter to match their environment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhelm repo add bitnami https://charts.bitnami.com/bitnami\nhelm install zookeeper bitnami/zookeeper \\\n    --namespace=zookeeper \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set global.storageClass=standard \\\n    --set networkPolicy.enabled=true \\\n    --set replicaCount=3 \\\n    --create-namespace\n```\n\n----------------------------------------\n\nTITLE: Configuring LoadBalancer External Service\nDESCRIPTION: This YAML snippet provides an example of configuring an external service using the LoadBalancer type.  It defines a service named \"nlb\" using the loadBalancerClass \"service.k8s.aws/nlb\".  It specifies the port configurations for HTTP and UDP listener. The annotations and labels are included in metadata section.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/5_references/1_nifi_cluster/7_external_service_config.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nexternalServices:\n    - name: \"nlb\"\n      spec:\n        type: LoadBalancer\n        loadBalancerClass: \"service.k8s.aws/nlb\"\n        portConfigs:\n          - port: 8080\n            internalListenerName: \"http\"\n          - port: 7890\n            internalListenerName: \"my-custom-udp-listener\"\n            protocol: UDP\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFi Cluster on Kubernetes Using kubectl with Preconfigured YAML\nDESCRIPTION: This snippet uses a bash command to deploy a NiFi cluster on Kubernetes by applying a sample YAML configuration file referencing an existing Zookeeper service. It assumes namespace 'nifi' and provides a quick start deployment leveraging the previously defined custom storage and compatible Zookeeper setup. The command requires kubectl configured with appropriate cluster access and the sample YAML file located at 'config/samples/simplenificluster.yaml'.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create -n nifi -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio Gateway for HTTPS (YAML)\nDESCRIPTION: This YAML defines an Istio Gateway configured for HTTPS traffic. The gateway intercepts HTTPS traffic on port 443 and decrypts it. It specifies the `tls` mode as `SIMPLE` and uses `credentialName` (`my-secret`) for the TLS certificate. The `hosts` parameter specifies the domain.  This setup is designed to handle HTTPS termination at the gateway before traffic is routed internally as HTTP.  Dependencies include a valid TLS certificate and Istio's ingress gateway.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: nifi-gateway\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 443\n      name: https\n      protocol: HTTPS\n    tls:\n      mode: SIMPLE\n      credentialName: my-secret\n    hosts:\n    - nifi.my-domain.com\n```\n```\n\n----------------------------------------\n\nTITLE: Creating NifiConnection Resource to Link Dataflows with Deployment Spec\nDESCRIPTION: This YAML snippet defines a NifiConnection custom resource to establish a connection between 'input' and 'output' dataflows, specifying source and destination dataflows, connection configuration parameters such as backpressure thresholds, flow expiration, bends for routing, and update strategy. The configuration also details the placement of connection labels via 'labelIndex' and the importance of prioritizers order. This setup facilitates automated connection management within a NiFi cluster using CRDs.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/4_manage_connections/1_deploy_connection.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiConnection\nmetadata:\n  name: connection\n  namespace: nifikop\nspec:\n  source:\n    name: input\n    namespace: nifikop\n    subName: output\n    type: dataflow\n  destination:\n    name: output\n    namespace: nifikop\n    subName: input\n    type: dataflow\n  configuration:\n    backPressureDataSizeThreshold: 100 GB\n    backPressureObjectThreshold: 10000\n    flowFileExpiration: 1 hour\n    labelIndex: 0\n    bends:\n      - posX: 550\n        posY: 550\n      - posX: 550\n        posY: 440\n      - posX: 550\n        posY: 88\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Defining NiFiDataflow API using YAML\nDESCRIPTION: This YAML snippet defines the structure of a `NifiDataflow` custom resource, including its API version, kind, and metadata. It specifies the configuration required to deploy and manage a NiFi dataflow within a Kubernetes environment. The snippet showcases the required fields like `parentProcessGroupID`, `bucketId`, `flowId`, `flowVersion`, `clusterRef`, `registryClientRef` and  `parameterContextRef` which are essential for dataflow deployment and management. This yaml defines the desired state of the dataflow. There is a section for updating the dataflow with several modes like drain or drop.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/5_references/5_nifi_dataflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  flowPosition:\n    posX: 0\n    posY: 0\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: squidflow\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: nifikop\n  updateStrategy: drain\n```\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFiKop on OpenShift using Helm\nDESCRIPTION: Installs the NiFiKop operator on OpenShift using Helm. This command is similar to the standard Helm install but includes the '--set runAsUser=$uid' parameter, using the UID retrieved from the namespace annotations to comply with OpenShift's security constraints.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/2_deploy_nifikop/1_quick_start.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop \\\n    nifikop \\\n    --namespace=nifi \\\n    --version 1.1.1 \\\n    --set image.tag=v1.1.1-release \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set namespaces={\"nifi\"} \\\n    --set runAsUser=$uid\n```\n\n----------------------------------------\n\nTITLE: Creating Monitoring Namespace in Kubernetes\nDESCRIPTION: Creates a dedicated Kubernetes namespace for monitoring resources where Prometheus and related components will be deployed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_5\n\nLANGUAGE: console\nCODE:\n```\nkubectl create namespace monitoring-system\n```\n\n----------------------------------------\n\nTITLE: Cloning NiFiKop Repository\nDESCRIPTION: This snippet clones the NiFiKop repository from GitHub and navigates into the project directory. It's the first step in setting up the development environment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/6_contributing/1_developer_guide.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/konpyutaika/nifikop.git\ncd nifikop\n```\n\n----------------------------------------\n\nTITLE: Listing All Helm Releases (Deployed, Deleted, Failed)\nDESCRIPTION: Uses the `helm list --all` command to provide a comprehensive list of all Helm releases known to Helm, including those currently deployed, previously deleted, and those that failed during deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --all\n```\n\n----------------------------------------\n\nTITLE: Defining NiFiNodeGroupAutoscaler Kubernetes Custom Resource in YAML for Auto-Scaling NiFi Nodes\nDESCRIPTION: This YAML defines a custom resource of kind NifiNodeGroupAutoscaler to manage auto-scaling of NiFi cluster node groups. It specifies the target NiFiCluster via clusterRef, identifies the nodeConfigGroup to scale ('auto_scaling'), includes optional readOnlyConfig overrides for NiFi properties, and sets nodeLabelsSelector to target specific NiFi nodes. The upscaleStrategy and downscaleStrategy control how nodes are added or removed, using simple and lifo strategies respectively. This resource enables Kubernetes to automatically adjust NiFi node counts based on cluster metrics and policies. Requires the corresponding Nifi CRD installed and cluster configured.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiNodeGroupAutoscaler\nmetadata:\n  name: nifinodegroupautoscaler-sample\n  namespace: clusters\nspec:\n  clusterRef:\n    name: cluster\n    namespace: clusters\n  nodeConfigGroupId: auto_scaling\n  readOnlyConfig:\n    nifiProperties:\n      overrideConfigs: |\n        nifi.ui.banner.text=NiFiKop - Scale Group\n  # nodeConfig example commented out\n  # nodeConfig:\n  #   nodeSelector:\n  #     node_type: high-mem\n  nodeLabelsSelector: \n    matchLabels:\n      nifi_cr: cluster\n      nifi_node_group: auto-scaling\n  upscaleStrategy: simple\n  downscaleStrategy: lifo\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Dataflow Resource (YAML)\nDESCRIPTION: Define a `NifiDataflow` custom resource to instruct NiFiKop on deploying and managing a specific versioned flow from NiFi Registry. It requires details about the flow (bucket, flow ID, version), references the associated NiFi cluster, registry client, and parameter context, and defines management behavior like `syncMode` and `updateStrategy`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: demo\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Declaring External NiFi Cluster in Kubernetes YAML\nDESCRIPTION: This YAML snippet demonstrates how to declare an external NiFi cluster using the NifiCluster custom resource. It includes specifying the root process group ID, a template for node URIs, a list of node IDs, the cluster type (set to external), client authentication type (basic or tls), and a reference to a Kubernetes secret holding authentication data. Required fields enable the operator to connect and authenticate to the external NiFi cluster for lifecycle management. The rootProcessGroupId is mandatory to manage root policies, and the nodeURITemplate combined with node IDs dynamically constructs node addresses.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/3_external_cluster.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: externalcluster\nspec:\n  # rootProcessGroupId contains the uuid of the root process group for this cluster.\n  rootProcessGroupId: 'd37bee03-017a-1000-cff7-4eaaa82266b7'\n  # nodeURITemplate used to dynamically compute node uri.\n  nodeURITemplate: 'nifi0%d.integ.mapreduce.m0.p.fti.net:9090'\n  # all node requiresunique id\n  nodes:\n    - id: 1\n    - id: 2\n    - id: 3\n  # type defines if the cluster is internal (i.e manager by the operator) or external.\n  # :Enum={\"external\",\"internal\"}\n  type: 'external'\n  # clientType defines if the operator will use basic or tls authentication to query the NiFi cluster.\n  # Enum={\"tls\",\"basic\"}\n  clientType: 'basic'\n  # secretRef reference the secret containing the informations required to authenticate to the cluster.\n  secretRef:\n    name: nifikop-credentials\n    namespace: nifikop-nifi\n```\n\n----------------------------------------\n\nTITLE: Adding a Node to NifiCluster Definition (YAML)\nDESCRIPTION: YAML manifest defining a `NifiCluster` resource, showing how to add a new node (id: 25) to the `spec.nodes` list to scale up the cluster. The new node uses the 'default_group' configuration and requires a unique ID within the list.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.12.1\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n    - id: 2\n      nodeConfigGroup: \"default_group\"\n# >>>> START: The new node\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n# <<<< END\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: Installing KEDA Helm Chart in Kubernetes Using Console\nDESCRIPTION: This snippet provides the commands to create a dedicated Kubernetes namespace named 'keda' and install the KEDA Helm chart within that namespace. It is necessary to isolate KEDA components and ensure proper deployment; Helm manages chart releases and dependencies.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nkubectl create namespace keda\nhelm install keda kedacore/keda --namespace keda\n```\n\n----------------------------------------\n\nTITLE: Listing All Helm Releases (Including Deleted/Failed)\nDESCRIPTION: Demonstrates the `helm list --all` command to retrieve a comprehensive list of all Helm release records known to Helm, including currently deployed, deleted, and failed ones.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --all\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiRegistryClient CRD in YAML\nDESCRIPTION: This YAML manifest defines a `NifiRegistryClient` custom resource. This resource is required by NiFiKop to interact with a NiFi Registry instance. It specifies the target NiFi cluster via `clusterRef` and the URI of the NiFi Registry.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiRegistryClient\nmetadata:\n  name: registry-client-example\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Registry client managed by NiFiKop\"\n  uri: \"http://nifi.hostname.com:18080\"\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Cluster for Autoscaling (YAML)\nDESCRIPTION: Partial YAML configuration for a NiFiCluster custom resource managed by NiFiKop. It shows how to enable a Prometheus listener on port 9090 for metrics exposition and define a 'auto_scaling' node group with specific resource limits, requests, and storage configurations, designating it for autoscaling.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n...spec:\n  ...\n  listenersConfig:\n    internalListeners:\n    ...\n    - containerPort: 9090\n      name: prometheus\n      type: prometheus \n    ...\n  ...\n  nodeConfigGroups:\n    auto_scaling:\n      isNode: true\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 2Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n      serviceAccountName: external-dns\n      storageConfigs:\n        - mountPath: /opt/nifi/nifi-current/logs\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/data\n          name: data\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/extensions\n          name: extensions-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/flowfile_repository\n          name: flowfile-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/nifi-current/conf\n          name: conf\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/content_repository\n          name: content-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/provenance_repository\n          name: provenance-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n  ...\n```\n\n----------------------------------------\n\nTITLE: Running NiFiKop Operator\nDESCRIPTION: This command runs the NiFiKop operator, likely using the compiled binary. It assumes the CRDs have been applied.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/6_contributing/1_developer_guide.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmake run\n```\n\n----------------------------------------\n\nTITLE: Configuring RBAC for Kubernetes State Management in NiFi\nDESCRIPTION: YAML configuration that defines the necessary Role and RoleBinding to allow NiFi to use Kubernetes native state management. Grants permissions to manage Leases and ConfigMaps resources in the NiFi namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: simplenifi\n  namespace: nifi\nrules:\n- apiGroups: [\"coordination.k8s.io\"]\n  resources: [\"leases\"]\n  verbs: [\"*\"]\n- apiGroups: [\"\"]\n  resources: [\"configmaps\"]\n  verbs: [\"*\"]\n---\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: simplenifi\n  namespace: nifi\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: nifi\nroleRef:\n  kind: Role\n  name: simplenifi\n  apiGroup: rbac.authorization.k8s.io\n```\n\n----------------------------------------\n\nTITLE: Installing Nifikop Helm Chart with Namespaces Parameter in Bash\nDESCRIPTION: This Bash command demonstrates installing the Nifikop Helm chart with a specified namespace using the --set flag. Prerequisite: Helm configured to access the relevant repository. Parameters: release name, chart, and namespaces. Inputs: none, outputs: deployment in the designated namespace(s).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop --set namespaces={\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Creating Storage Class for Kubernetes\nDESCRIPTION: This snippet defines a Kubernetes StorageClass resource. It configures the storage class with a specific provisioner (gce-pd in this case) and volume binding mode (`WaitForFirstConsumer`).  This ensures that the storage volume is only provisioned when a pod using the volume is scheduled.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: exampleStorageclass\nparameters:\n  type: pd-standard\nprovisioner: kubernetes.io/gce-pd\nreclaimPolicy: Delete\nvolumeBindingMode: WaitForFirstConsumer\n```\n\n----------------------------------------\n\nTITLE: Verifying HPA Deployment (kubectl)\nDESCRIPTION: This snippet demonstrates how to check the status of the Horizontal Pod Autoscaler (HPA) created by KEDA and associated with the ScaledObject.  It utilizes the `kubectl get hpa` command to retrieve information about the HPA resource, including the number of replicas, target utilization, and other relevant metrics. Dependency: kubectl command-line tool. The output is the HPA status information.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_10\n\nLANGUAGE: console\nCODE:\n```\nkubectl get -n clusters hpa\nNAME                REFERENCE                                                TARGETS         MINPODS   MAXPODS   REPLICAS   AGE\nkeda-hpa-cluster    NifiNodeGroupAutoscaler/nifinodegroupautoscaler-sample     1833m/2 (avg)   1         3         2          25d\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus Custom Resource in Kubernetes YAML\nDESCRIPTION: This YAML defines a Prometheus resource under the Prometheus Operator CRD that configures scraping intervals, resource requests, pod and service monitor selectors limited to pods labeled with 'prometheus' and 'nifi-cluster' respectively, and disables the admin API. Resource requests help with resource management, and selectors determine which targets Prometheus scrapes for metrics. This enables Prometheus to monitor NiFi cluster specifics with desired log levels and scraping behavior.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: Prometheus\nmetadata:\n  name: prometheus\n  namespace: monitoring-system\nspec:\n  enableAdminAPI: false\n  evaluationInterval: 30s\n  logLevel: debug\n  podMonitorSelector:\n    matchExpressions:\n    - key: release\n      operator: In\n      values:\n      - prometheus\n  resources:\n    requests:\n      memory: 400Mi\n  scrapeInterval: 30s\n  serviceAccountName: prometheus\n  serviceMonitorSelector:\n    matchExpressions:\n    - key: app\n      operator: In\n      values:\n      - nifi-cluster\n```\n\n----------------------------------------\n\nTITLE: Deploying Prometheus ServiceAccount, ClusterRole, and ClusterRoleBinding - yaml\nDESCRIPTION: Defines the Kubernetes RBAC resources (ServiceAccount, ClusterRole, ClusterRoleBinding) necessary for Prometheus to access required cluster resources and metadata. No external dependencies but requires cluster-admin permissions to apply. Applies correct permissions for scraping metrics across multiple resource types. Sensitive to correct namespace and resource names.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: prometheus\n  namespace: monitoring-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: prometheus\nrules:\n- apiGroups: [\"\"]\n  resources:\n  - nodes\n  - nodes/metrics\n  - services\n  - endpoints\n  - pods\n  verbs: [\"get\", \"list\", \"watch\"]\n- apiGroups: [\"\"]\n  resources:\n  - configmaps\n  verbs: [\"get\"]\n- apiGroups:\n  - networking.k8s.io\n  resources:\n  - ingresses\n  verbs: [\"get\", \"list\", \"watch\"]\n- nonResourceURLs: [\"/metrics\"]\n  verbs: [\"get\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: prometheus\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: prometheus\nsubjects:\n- kind: ServiceAccount\n  name: prometheus\n  namespace: monitoring-system\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom StorageClass in Kubernetes\nDESCRIPTION: Creates a StorageClass with volume binding mode 'WaitForFirstConsumer' to enable delayed volume provisioning. Dependencies include Kubernetes cluster with storage provider; parameters like 'type' can be customized based on storage backend.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: exampleStorageclass\nparameters:\n  type: pd-standard\nprovisioner: kubernetes.io/gce-pd\nreclaimPolicy: Delete\nvolumeBindingMode: WaitForFirstConsumer\n```\n\n----------------------------------------\n\nTITLE: Defining Istio VirtualService for Decrypted HTTPS Traffic Routing in YAML\nDESCRIPTION: This Istio VirtualService routes the now-HTTP traffic (decrypted by the HTTPS Gateway) for 'nifi.my-domain.com' to the specified NiFi ClusterIP service '<service-name>.<namespace>.svc.cluster.local' on port 8443. Replace placeholders with actual service name and namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: nifi-vs\nspec:\n  gateways:\n  - nifi-gateway\n  hosts:\n  - nifi.my-domain.com\n  http:\n  - match:\n    - uri:\n        prefix: /\n    route:\n    - destination:\n        host: <service-name>.<namespace>.svc.cluster.local\n        port:\n          number: 8443\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio VirtualService for HTTPS (YAML)\nDESCRIPTION: This YAML configures a VirtualService that redirects HTTP traffic to a specific `ClusterIP` service. It targets traffic for `nifi.my-domain.com` and directs traffic (prefix `/`) to the service, with a port of 8443. This step is crucial after decrypting HTTPS at the Gateway to ensure that internal traffic uses the correct port. Prerequisites include a configured Istio gateway and a running service. The destination host is configured in the <service-name>.<namespace>.svc.cluster.local format, depending on your setup.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: nifi-vs\nspec:\n  gateways:\n  - nifi-gateway\n  hosts:\n  - nifi.my-domain.com\n  http:\n  - match:\n    - uri:\n        prefix: /\n    route:\n    - destination:\n        host: <service-name>.<namespace>.svc.cluster.local\n        port:\n          number: 8443\n```\n```\n\n----------------------------------------\n\nTITLE: Defining Nifi ClusterIP External Service Configuration in YAML\nDESCRIPTION: This YAML snippet configures an external Nifi service of type ClusterIP, specifying a set of ports and associated internal listener names for HTTP and custom traffic. The configuration allows for detailed specification of ports, protocols, and service metadata (annotations and labels). Required dependencies include a Kubernetes cluster and permissions to create Service resources; key parameters are the external service name, type, portConfigs (with internalListenerName and protocol), and metadata. Takes no input parameters at runtime; edits should be applied via a manifest; results in a Kubernetes Service resource being created with specific listeners exposed within the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/5_references/1_nifi_cluster/7_external_service_config.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n  externalServices:\n    - name: \"clusterip\"\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8080\n            internalListenerName: \"http\"\n          - port: 7182\n            internalListenerName: \"my-custom-listener\"\n            protocol: TCP\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n\n----------------------------------------\n\nTITLE: Defining a NiFi UserGroup resource in YAML\nDESCRIPTION: This YAML snippet defines a NifiUserGroup resource named \"group-test\". It specifies the group's identity, a reference to a NiFi cluster named \"nc\" in the \"nifikop\" namespace, references to two NiFi users, and an access policy that grants read access to the /counters resource.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/5_references/6_nifi_usergroup.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  identity: \"My Special Group\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n    - name: nc-controller.nifikop.mgt.cluster.local\n  accessPolicies:\n    - type: global\n      action: read\n      resource: /counters\n```\n\n----------------------------------------\n\nTITLE: Configuring External NiFi Cluster with YAML in NifiKop\nDESCRIPTION: A YAML configuration example for declaring an external NiFi cluster. It specifies the root process group ID, node URI template, node IDs, cluster type, authentication method, and secret reference for credentials.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/1_manage_clusters/3_external_cluster.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: externalcluster\nspec:\n  # rootProcessGroupId contains the uuid of the root process group for this cluster.\n  rootProcessGroupId: 'd37bee03-017a-1000-cff7-4eaaa82266b7'\n  # nodeURITemplate used to dynamically compute node uri.\n  nodeURITemplate: 'nifi0%d.integ.mapreduce.m0.p.fti.net:9090'\n  # all node requiresunique id\n  nodes:\n    - id: 1\n    - id: 2\n    - id: 3\n  # type defines if the cluster is internal (i.e manager by the operator) or external.\n  # :Enum={\"external\",\"internal\"}\n  type: 'external'\n  # clientType defines if the operator will use basic or tls authentication to query the NiFi cluster.\n  # Enum={\"tls\",\"basic\"}\n  clientType: 'basic'\n  # secretRef reference the secret containing the informations required to authenticate to the cluster.\n  secretRef:\n    name: nifikop-credentials\n    namespace: nifikop-nifi\n```\n\n----------------------------------------\n\nTITLE: Listing Nifi Listener Types and Ports in YAML\nDESCRIPTION: This YAML snippet defines various internal listener configurations for Nifi, specifying types, ports, protocols, and custom listener settings. It also includes SSL secret references with creation flags, providing core connection and security parameters required for Nifi's network setup.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/5_references/1_nifi_cluster/6_listeners_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  listenersConfig:\n    internalListeners:\n      - type: \"https\"\n        name: \"https\"\n        containerPort: 8443\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n      - type: \"prometheus\"\n        name: \"prometheus\"\n        containerPort: 9090\n      - type: \"load-balance\"\n        name: \"load-balance\"\n        containerPort: 6342\n      - name: \"my-custom-listener-port\"\n        containerPort: 1234\n        protocol: \"TCP\"\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n```\n\n----------------------------------------\n\nTITLE: Deploying Prometheus ServiceAccount, ClusterRole, and ClusterRoleBinding - YAML\nDESCRIPTION: This YAML manifest defines Kubernetes RBAC resources to grant Prometheus the necessary permissions to scrape metrics from nodes, pods, endpoints, services, configmaps, ingresses, and read from the /metrics non-resource endpoint. It creates a ServiceAccount named 'prometheus', a ClusterRole with appropriate API resource access verbs, and binds them via ClusterRoleBinding within the 'monitoring-system' namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: prometheus\n  namespace: monitoring-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: prometheus\nrules:\n- apiGroups: [\"\"]\n  resources:\n  - nodes\n  - nodes/metrics\n  - services\n  - endpoints\n  - pods\n  verbs: [\"get\", \"list\", \"watch\"]\n- apiGroups: [\"\"]\n  resources:\n  - configmaps\n  verbs: [\"get\"]\n- apiGroups:\n  - networking.k8s.io\n  resources:\n  - ingresses\n  verbs: [\"get\", \"list\", \"watch\"]\n- nonResourceURLs: [\"/metrics\"]\n  verbs: [\"get\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: prometheus\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: prometheus\nsubjects:\n- kind: ServiceAccount\n  name: prometheus\n  namespace: monitoring-system\n```\n\n----------------------------------------\n\nTITLE: Defining NifiUser Resource in YAML\nDESCRIPTION: This YAML snippet defines a `NifiUser` Kubernetes resource. It specifies the desired state for a NiFi user, including its identity (`identity`), the NiFi cluster it belongs to (`clusterRef`), whether to create a certificate and include a JKS keystore (`createCert`, `includeJKS`), and a list of access policies to grant (`accessPolicies`). The example configures a user with specific read access on a process group.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/2_manage_users_and_accesses/1_users_management.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: aguitton\nspec:\n  # identity field is use to define the user identity on NiFi cluster side,\n  #\tit use full when the user's name doesn't suite with Kubernetes resource name.\n  identity: alexandre.guitton@konpyutaika.com\n  # Contains the reference to the NifiCluster with the one the registry client is linked.\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  # Whether or not the the operator also include a Java keystore format (JKS) with you secret\n  includeJKS: false\n  # Whether or not a certificate will be created for this user.\n  createCert: false\n  # defines the list of access policies that will be granted to the group.\n  accessPolicies:\n      # defines the kind of access policy, could be \"global\" or \"component\".\n    - type: component\n      # defines the kind of action that will be granted, could be \"read\" or \"write\"\n      action: read\n      # resource defines the kind of resource targeted by this access policies, please refer to the following page:\n      #\thttps://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#access-policies\n      resource: /data\n      # componentType is used if the type is \"component\", it's allow to define the kind of component on which is the\n      # access policy\n      componentType: \"process-groups\"\n      # componentId is used if the type is \"component\", it's allow to define the id of the component on which is the\n      # access policy\n      componentId: \"\"\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom StorageClass in Kubernetes YAML\nDESCRIPTION: This YAML snippet creates a custom StorageClass in Kubernetes that uses the 'pd-standard' type and sets the volume binding mode to 'WaitForFirstConsumer'. It specifies reclaim policy as 'Delete'. Proper configuration of this StorageClass allows dynamic volume provisioning optimized for latency-sensitive workloads. Dependencies include Kubernetes cluster access and the ability to apply YAML manifests.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: exampleStorageclass\nparameters:\n  type: pd-standard\nprovisioner: kubernetes.io/gce-pd\nreclaimPolicy: Delete\nvolumeBindingMode: WaitForFirstConsumer\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Cluster NodeConfigGroup with Prometheus Listener and Persistent Storage in YAML\nDESCRIPTION: This YAML snippet defines part of the configuration for a NiFi cluster node group tailored for autoscaling. It specifies a Prometheus listener on port 9090 for metrics exposure, and detailed persistent volume claims (PVCs) for logs, data, extensions, flowfile repository, configuration, content repository, and provenance repository. Resource requests and limits are configured to allocate CPU and memory, with associated storage classes for dynamic provisioning.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n...\nspec:\n  ...\n  listenersConfig:\n    internalListeners:\n    ...\n    - containerPort: 9090\n      name: prometheus\n      type: prometheus \n    ...\n  ...\n  nodeConfigGroups:\n    auto_scaling:\n      isNode: true\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 2Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n      serviceAccountName: external-dns\n      storageConfigs:\n        - mountPath: /opt/nifi/nifi-current/logs\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/data\n          name: data\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/extensions\n          name: extensions-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/flowfile_repository\n          name: flowfile-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/nifi-current/conf\n          name: conf\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/content_repository\n          name: content-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/provenance_repository\n          name: provenance-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n  ...\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Secret for Sensitive Parameters\nDESCRIPTION: This console command creates a Kubernetes secret named 'secret-params' with two sensitive literals ‘secret1’ and ‘secret2’. These secrets are referenced in NifiParameterContext to securely manage sensitive data required by dataflows.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_2\n\nLANGUAGE: Console\nCODE:\n```\nkubectl create secret generic secret-params \\\n    --from-literal=secret1=yop \\\n    --from-literal=secret2=yep \\\n    -n nifikop\n```\n\n----------------------------------------\n\nTITLE: Creating a Cert-Manager Issuer for Let's Encrypt Staging (YAML)\nDESCRIPTION: Defines a cert-manager `Issuer` resource named `letsencrypt-staging` to issue certificates using the Let's Encrypt staging ACME server. Requires replacing the placeholder email address. It specifies the ACME server URL, a secret (`example-issuer-account-key`) to store the private key, and configures an HTTP01 challenge solver using an Ingress template with annotations for external-dns integration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: cert-manager.io/v1alpha2\nkind: Issuer\nmetadata:\n  name: letsencrypt-staging\nspec:\n  acme:\n    # You must replace this email address with your own.\n    # Let's Encrypt will use this to contact you about expiring\n    # certificates, and issues related to your account.\n    email: <your email address>\n    server: https://acme-staging-v02.api.letsencrypt.org/directory\n    privateKeySecretRef:\n      # Secret resource used to store the account's private key.\n      name: example-issuer-account-key\n    # Add a single challenge solver, HTTP01 using nginx\n    solvers:\n      - http01:\n          ingress:\n            ingressTemplate:\n              metadata:\n                annotations:\n                  \"external-dns.alpha.kubernetes.io/ttl\": \"5\"\n```\n\n----------------------------------------\n\nTITLE: Defining a NiFiUser Custom Resource in YAML\nDESCRIPTION: This YAML snippet specifies a NiFiUser resource, including metadata, user identity, cluster reference, security settings, and access policies. It ensures proper management and synchronization of users within a NiFi cluster managed by Kubernetes. Dependencies include the NifiUser Custom Resource Definition (CRD) from the nifi.konpyutaika.com API group. Key parameters include 'identity' for user identification, 'clusterRef' to associate with a NiFi cluster, security options like 'includeJKS' and 'createCert', and 'accessPolicies' for defining user permissions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/3_manage_nifi/2_manage_users_and_accesses/1_users_management.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiUser\nmetadata:\n  name: aguitton\nspec:\n  # identity field is use to define the user identity on NiFi cluster side,\n  # it use full when the user's name doesn't suite with Kubernetes resource name.\n  identity: alexandre.guitton@konpyutaika.com\n  # Contains the reference to the NifiCluster with the one the registry client is linked.\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  # Whether or not the the operator also include a Java keystore format (JKS) with you secret\n  includeJKS: false\n  # Whether or not a certificate will be created for this user.\n  createCert: false\n  # defines the list of access policies that will be granted to the group.\n  accessPolicies:\n      # defines the kind of access policy, could be \"global\" or \"component\".\n    - type: component\n      # defines the kind of action that will be granted, could be \"read\" or \"write\"\n      action: read\n      # resource defines the kind of resource targeted by this access policies, please refer to the following page:\n      #\thttps://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#access-policies\n      resource: /data\n      # componentType is used if the type is \"component\", it's allow to define the kind of component on which is the\n      # access policy\n      componentType: \"process-groups\"\n      # componentId is used if the type is \"component\", it's allow to define the id of the component on which is the\n      # access policy\n      componentId: \"\"\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi security identity mapping properties for OpenId Connect - Shell\nDESCRIPTION: This shell snippet shows the necessary entries to add into nifi.properties for enabling identity mapping when using OpenId Connect authentication with NiFi. It configures the identity mapping pattern, value, and transformation behavior to support multiple identity providers by extracting the common name (CN) from the distinguished name (DN). No external dependencies are required aside from NiFi itself, and the expected output is proper identity extraction facilitating OIDC user authentication.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/5_users_authentication/1_oidc.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nnifi.security.identity.mapping.pattern.dn=CN=([^,]*)(?:, (?:O|OU)=.*)?\nnifi.security.identity.mapping.value.dn=$1\nnifi.security.identity.mapping.transform.dn=NONE\n```\n\n----------------------------------------\n\nTITLE: Defining a Basic NiFi Cluster Resource YAML\nDESCRIPTION: This YAML manifest defines a NifiCluster custom resource for deploying a NiFi cluster using the Nifikop Kubernetes operator. It specifies metadata, Zookeeper connection details (`zkAddress`, `zkPath`), the NiFi image (`clusterImage`), node configurations (`nodeConfigGroups`, `nodes`) including resource limits, storage, and service accounts, listener ports (`listenersConfig`), and external service definitions (`externalServices`). It declares the desired state for the NiFi cluster, which the operator will then reconcile. Requires a running Nifikop operator and a Zookeeper cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/5_references/1_nifi_cluster/1_nifi_cluster.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n    annotations:\n      tyty: ytyt\n    labels:\n      tete: titi  \n  pod:\n    annotations:\n      toto: tata\n    labels:\n      titi: tutu\n  zkAddress: 'zookeepercluster-client.zookeeper:2181'\n  zkPath: '/simplenifi'\n  clusterImage: 'apache/nifi:1.11.3'\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      podMetadata:\n        annotations:\n          node-annotation: \"node-annotation-value\"\n        labels:\n          node-label: \"node-label-value\"\n      externalVolumeConfigs:\n        - name: example-volume\n          mountPath: \"/opt/nifi/example\"\n          secret:\n            secretName: \"raw-controller\"\n      storageConfigs:\n        - mountPath: '/opt/nifi/nifi-current/logs'\n          name: logs\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: 'standard'\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: 'default'\n      resourcesRequirements:\n        limits:\n          cpu: '2'\n          memory: 3Gi\n        requests:\n          cpu: '1'\n          memory: 1Gi\n  nodes:\n    - id: 1\n      nodeConfigGroup: 'default_group'\n    - id: 2\n      nodeConfigGroup: 'default_group'\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: 'http'\n        name: 'http'\n        containerPort: 8080\n      - type: 'cluster'\n        name: 'cluster'\n        containerPort: 6007\n      - type: 's2s'\n        name: 's2s'\n        containerPort: 10000\n  externalServices:\n    - name: 'clusterip'\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8080\n            internalListenerName: 'http'\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n\n----------------------------------------\n\nTITLE: Example YAML Configuration for NiFi NodeGroup Autoscaler\nDESCRIPTION: This YAML snippet provides a sample configuration for the `NifiNodeGroupAutoscaler` custom resource. It demonstrates how to define an autoscaler that links to a specific `NifiCluster` (`clusterRef`), targets a particular node configuration group (`nodeConfigGroupId`) using label selectors (`nodeLabelsSelector`), and specifies scaling strategies (`upscaleStrategy`, `downscaleStrategy`). This resource is used by the NiFiKop operator to manage the replica count of the identified NiFi nodes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/5_references/7_nifi_nodegroup_autoscaler.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiNodeGroupAutoscaler\nmetadata:\n  name: nifinodegroupautoscaler-sample\nspec:\n  # contains the reference to the NifiCluster with the one the node group autoscaler is linked.\n  clusterRef:\n    name: nificluster-name\n    namespace: nifikop\n  # defines the id of the NodeConfig contained in NifiCluster.Spec.NodeConfigGroups\n  nodeConfigGroupId: default-node-group\n  # The selector used to identify nodes in NifiCluster.Spec.Nodes this autoscaler will manage\n  # Use Node.Labels in combination with this selector to clearly define which nodes will be managed by this autoscaler \n  nodeLabelsSelector: \n    matchLabels:\n      nifi_cr: nificluster-name\n      nifi_node_group: default-node-group\n  # the strategy used to decide how to add nodes to a nifi cluster\n  upscaleStrategy: simple\n  # the strategy used to decide how to remove nodes from an existing cluster\n  downscaleStrategy: lifo\n\n```\n\n----------------------------------------\n\nTITLE: Setting Development Environment Variables for NiFiKop - Bash\nDESCRIPTION: This snippet sets the environment variables required to configure the NiFiKop operator for development. These variables specify Kubernetes configuration paths, namespaces, pod names, log levels, and the operator name. The variables must be set before starting the operator from an IDE or binary.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/6_contributing/1_developer_guide.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nKUBECONFIG={path/to/your/kubeconfig}\nWATCH_NAMESPACE={namespace_to_watch}\nPOD_NAME={name for operator pod}\nLOG_LEVEL=Debug\nOPERATOR_NAME=ide\n```\n\n----------------------------------------\n\nTITLE: Getting Zookeeper UID/GID for OpenShift Deployment\nDESCRIPTION: Bash command to extract the allowed user ID from OpenShift namespace annotations, which is required for configuring security context for Zookeeper deployment on OpenShift.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nzookeper_uid=$(kubectl get namespace zookeeper -o=jsonpath='{.metadata.annotations.openshift\\.io/sa\\.scc\\.supplemental-groups}' | sed 's/\\/10000$//' | tr -d '[:space:]')\n```\n\n----------------------------------------\n\nTITLE: Deploying KEDA ScaledObject for Autoscaling NiFi Node Group - yaml\nDESCRIPTION: This YAML manifest defines a KEDA ScaledObject resource named \"cluster\" in the \"clusters\" namespace to enable autoscaling of NiFi nodes based on Prometheus metrics. It references the NifiNodeGroupAutoscaler resource as the target scale resource and configures polling interval, cooldown period, replica counts, and fallback behavior. The scaling trigger is based on Prometheus query metrics with placeholders for server address, metric name, threshold, and query expression. This setup automates horizontal scaling according to NiFi cluster workload.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: cluster\n  namespace: clusters\nspec:\n  scaleTargetRef:\n    apiVersion:    nifi.konpyutaika.com/v1alpha1     # Optional. Default: apps/v1\n    kind:          NifiNodeGroupAutoscaler           # Optional. Default: Deployment\n    name:          nifinodegroupautoscaler-sample    # Mandatory. Must be in the same namespace as the ScaledObject\n    envSourceContainerName: nifi                     # Optional. Default: .spec.template.spec.containers[0]\n  pollingInterval:  30                               # Optional. Default: 30 seconds\n  cooldownPeriod:   300                              # Optional. Default: 300 seconds\n  idleReplicaCount: 0                                # Optional. Default: ignored, must be less than minReplicaCount \n  minReplicaCount:  1                                # Optional. Default: 0\n  maxReplicaCount:  3                                # Optional. Default: 100\n  fallback:                                          # Optional. Section to specify fallback options\n    failureThreshold: 5                              # Mandatory if fallback section is included\n    replicas: 1                                      # Mandatory if fallback section is included\n  # advanced:                                          # Optional. Section to specify advanced options\n  #   restoreToOriginalReplicaCount: true              # Optional. Whether the target resource should be scaled back to original replicas count, after the ScaledObject is deleted\n  #   horizontalPodAutoscalerConfig:                   # Optional. Section to specify HPA related options\n  #     name: {name-of-hpa-resource}                   # Optional. Default: keda-hpa-{scaled-object-name}\n  #     behavior:                                      # Optional. Use to modify HPA's scaling behavior\n  #       scaleDown:\n  #         stabilizationWindowSeconds: 300 <--- \n  #         policies:\n  #         - type: Percent\n  #           value: 100\n  #           periodSeconds: 15\n  triggers:\n    - type: prometheus\n      metadata:\n        serverAddress: http://prometheus-operated.monitoring-system.svc:9090\n        metricName: http_requests_total\n        threshold: <threshold value>\n        query: <prometheus query>\n```\n\n----------------------------------------\n\nTITLE: Deploying NifiRegistryClient in Kubernetes with YAML\nDESCRIPTION: Example YAML configuration for creating a NifiRegistryClient resource that connects to a NiFi Registry. The registry client is required before deploying dataflows since NiFiKop manages dataflows using the NiFi Registry feature.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiRegistryClient\nmetadata:\n  name: registry-client-example\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Registry client managed by NiFiKop\"\n  uri: \"http://nifi.hostname.com:18080\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Internal Listeners and External Services for NiFiCluster in YAML\nDESCRIPTION: This comprehensive YAML snippet combines the configuration for both internal NiFi listeners (`listenersConfig.internalListeners`) and external Kubernetes services (`externalServices`). It shows how to define standard and custom internal listeners and then expose specific internal listeners (`https`, `http-tracking`) externally via a Kubernetes Service (`LoadBalancer` type) using the `externalServices` field, mapping internal listener names to external service ports.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    - type: \"https\"\n      name: \"https\"\n      containerPort: 8443\n    - type: \"cluster\"\n      name: \"cluster\"\n      containerPort: 6007\n    - type: \"s2s\"\n      name: \"s2s\"\n      containerPort: 10000\n    - type: \"prometheus\"\n      name: \"prometheus\"\n      containerPort: 9090\n    - type: \"load-balance\"\n      name: \"load-balance\"\n      containerPort: 6342\n    - name: \"http-tracking\"\n      containerPort: 8081\nexternalServices:\n  - name: cluster-access\n    spec:\n      portConfigs:\n        - internalListenerName: https\n          port: 443\n        - internalListenerName: http-tracking\n          port: 80\n      type: LoadBalancer\n```\n\n----------------------------------------\n\nTITLE: Performing a Dry Run Helm Install of NiFiKop Operator in Bash\nDESCRIPTION: This bash snippet demonstrates performing a Helm dry run installation of the NiFiKop operator, allowing users to validate the configuration before actual deployment. It showcases setting the log level to Debug and restricting the operator to watch the \"nifikop\" namespace. Helm version 3 or higher is required.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop \\\n    --dry-run \\\n    --set logLevel=Debug \\\n    --set namespaces={\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Local Development in Bash\nDESCRIPTION: Environment variables required for configuring your IDE for local development of the NiFiKop operator.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/6_contributing/1_developer_guide.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nKUBECONFIG={path/to/your/kubeconfig}\nWATCH_NAMESPACE={namespace_to_watch}\nPOD_NAME={name for operator pod}\nLOG_LEVEL=Debug\nOPERATOR_NAME=ide\n```\n\n----------------------------------------\n\nTITLE: Deploying Prometheus RBAC Resources - YAML\nDESCRIPTION: Defines Kubernetes resources for Prometheus RBAC: a ServiceAccount, a ClusterRole with wide read and watch permissions on core monitoring objects (nodes, pods, configmaps, ingresses), and a ClusterRoleBinding associating the ServiceAccount with the ClusterRole. Required for Prometheus to access cluster-level metrics and resources. Apply this manifest using 'kubectl apply -f'.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: prometheus\n  namespace: monitoring-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: prometheus\nrules:\n- apiGroups: [\"\"]\n  resources:\n  - nodes\n  - nodes/metrics\n  - services\n  - endpoints\n  - pods\n  verbs: [\"get\", \"list\", \"watch\"]\n- apiGroups: [\"\"]\n  resources:\n  - configmaps\n  verbs: [\"get\"]\n- apiGroups:\n  - networking.k8s.io\n  resources:\n  - ingresses\n  verbs: [\"get\", \"list\", \"watch\"]\n- nonResourceURLs: [\"/metrics\"]\n  verbs: [\"get\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: prometheus\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: prometheus\nsubjects:\n- kind: ServiceAccount\n  name: prometheus\n  namespace: monitoring-system\n```\n\n----------------------------------------\n\nTITLE: NifiCluster InitContainer Configuration (Before)\nDESCRIPTION: This YAML snippet shows the NifiCluster configuration before the upgrade.  It demonstrates an example where the `initContainerImage` is explicitly set to `busybox`. Users with such configurations must update their settings during the upgrade to v0.15.0. The repository and tag of the image are specified.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/7_upgrade_guides/2_v0.14.1_to_v0.15.0.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  initContainerImage:\n    repository: busybox\n    tag: \"1.34.0\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio DestinationRule for HTTPS in NiFi\nDESCRIPTION: This YAML configuration defines an Istio DestinationRule that handles TLS re-encryption and configures sticky sessions using a consistent hash based on a secure authorization cookie.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: DestinationRule\nmetadata:\n  name: nifi-dr\nspec:\n  host: <service-name>.<namespace>.svc.cluster.local\n  trafficPolicy:\n    tls:\n      mode: SIMPLE\n    loadBalancer:\n      consistentHash:\n        httpCookie:\n          name: __Secure-Authorization-Bearer\n          ttl: 0s\n```\n\n----------------------------------------\n\nTITLE: Verifying NiFi External Service Exposure Using kubectl\nDESCRIPTION: Demonstrates how to verify the status and external IP addresses of the Kubernetes services exposing NiFi by issuing the 'kubectl get services' command. The console output lists services, their types, cluster and external IPs, exposed ports, and the age of the services. This verification is essential for confirming that the LoadBalancer service is properly created and accessible from outside the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nkubectl get services\nNAME                TYPE           CLUSTER-IP    EXTERNAL-IP      PORT(S)                                AGE\ncluster-access      LoadBalancer   10.88.21.98   35.180.241.132   443:30421/TCP,80:30521/TCP             20d\n```\n\n----------------------------------------\n\nTITLE: Installing Zookeeper with Helm\nDESCRIPTION: This snippet uses Helm to install the Bitnami Zookeeper chart. It configures resource requests and limits for CPU and memory, sets the storage class, enables network policies, and sets the replica count to 3 for a Zookeeper ensemble.  It requires Helm to be installed and configured to interact with the Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhelm install zookeeper oci://registry-1.docker.io/bitnamicharts/zookeeper \\\n    --namespace=zookeeper \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set global.storageClass=standard \\\n    --set networkPolicy.enabled=true \\\n    --set replicaCount=3 \\\n    --create-namespace\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop Helm Chart with Custom Image Tag - Bash\nDESCRIPTION: Installs the NiFiKop operator using Helm, specifying the image tag and target namespace. Assumes the Helm chart directory is present locally and image.tag matches the previously built and pushed Docker image. Replace values as necessary to match environment and chart naming conventions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/6_contributing/1_developer_guide.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nhelm install skeleton ./helm/nifikop \\\n    --set image.tag=v0.5.1-release \\\n    --namespace-{\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFi Cluster on OpenShift\nDESCRIPTION: Applies the OpenShift-specific NiFi cluster sample manifest to create the NiFi cluster within the 'nifi' namespace. This command assumes prior adjustment of security context UIDs in the manifest and cluster access permissions. It enables deployment of NiFi under OpenShift’s security constraints.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create -n nifi -f config/samples/openshift.yaml\n```\n\n----------------------------------------\n\nTITLE: Accessing Prometheus UI via Port Forwarding\nDESCRIPTION: Command to set up port forwarding from the Kubernetes Prometheus service to the local machine, allowing access to the Prometheus web interface for querying NiFi metrics.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_8\n\nLANGUAGE: console\nCODE:\n```\nkubectl port-forward -n monitoring-system service/prometheus-operated 9090:9090\n```\n\n----------------------------------------\n\nTITLE: Node Declaration Using NodeConfigGroups\nDESCRIPTION: This snippet illustrates how individual cluster nodes are associated with predefined NodeConfigGroups or custom configurations. It assigns nodes by their ID to specific groups or directly specifies resource attributes if a custom setup is needed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n- id: 0\n  nodeConfigGroup: \"default_group\"\n- id: 2\n  nodeConfigGroup: \"high_mem_group\"\n- id: 3\n  nodeConfigGroup: \"high_mem_group\"\n- id: 5\n  nodeConfig:\n    resourcesRequirements:\n      limits:\n        cpu: \"2\"\n        memory: 3Gi\n      requests:\n        cpu: \"1\"\n        memory: 1Gi\n```\n\n----------------------------------------\n\nTITLE: Setting Custom Authorizer in NiFi Configuration\nDESCRIPTION: This command sets the NiFi property to use the custom database authorizer instead of the default managed authorizer. This property tells NiFi which of the configured authorizers in the authorizers.xml file to use for authorization decisions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/6_users_authorization/1_custom_user_authorizer.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nnifi.security.user.authorizer=custom-database-authorizer\n```\n\n----------------------------------------\n\nTITLE: Packaging NiFiKop Helm Chart\nDESCRIPTION: Command to package the NiFiKop Helm chart for distribution. This creates a distributable chart archive that can be used for installation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/6_contributing/1_developer_guide.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nmake helm-package\n```\n\n----------------------------------------\n\nTITLE: Example NiFi Node Configurations in YAML\nDESCRIPTION: This YAML snippet demonstrates how to define configurations for individual NiFi nodes managed by NiFiKop. It shows two nodes (id 0 and 2), utilizing `nodeConfigGroup` for simplified setup, `readOnlyConfig` to override specific NiFi properties like the UI banner text (triggering rolling upgrades), and `nodeConfig` to specify detailed settings like CPU/memory resource requirements and persistent storage volumes (PVCs) for data like the provenance repository. Requires NiFiKop operator and underlying Kubernetes environment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/5_references/1_nifi_cluster/4_node.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n    - id: 0\n      # nodeConfigGroup can be used to ease the node configuration, if set only the id is required\n      nodeConfigGroup: \"default_group\"\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        nifiProperties:\n          overrideConfigs: |\n            nifi.ui.banner.text=NiFiKop - Node 0\n      # node configuration\n#       nodeConfig:\n    - id: 2\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        overrideConfigs: |\n          nifi.ui.banner.text=NiFiKop - Node 2\n      # node configuration\n      nodeConfig:\n        resourcesRequirements:\n          limits:\n            cpu: \"2\"\n            memory: 3Gi\n          requests:\n            cpu: \"1\"\n            memory: 1Gi\n        storageConfigs:\n          # Name of the storage config, used to name PV to reuse into sidecars for example.\n          - name: provenance-repository\n            # Path where the volume will be mount into the main nifi container inside the pod.\n            mountPath: \"/opt/nifi/provenance_repository\"\n            # Kubernetes PVC spec\n            # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n            pvcSpec:\n              accessModes:\n                - ReadWriteOnce\n              storageClassName: \"standard\"\n              resources:\n                requests:\n                  storage: 8Gi\n```\n\n----------------------------------------\n\nTITLE: Declaring External NiFi Cluster Resource (YAML)\nDESCRIPTION: This YAML snippet defines a `NifiCluster` resource configured to reference an existing external Apache NiFi cluster. It specifies the cluster's root process group ID, a template for generating node URIs, a list of node IDs, the type as 'external', the client authentication method ('basic'), and a reference to a Kubernetes Secret containing authentication credentials.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/1_manage_clusters/3_external_cluster.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: externalcluster\nspec:\n  # rootProcessGroupId contains the uuid of the root process group for this cluster.\n  rootProcessGroupId: 'd37bee03-017a-1000-cff7-4eaaa82266b7'\n  # nodeURITemplate used to dynamically compute node uri.\n  nodeURITemplate: 'nifi0%d.integ.mapreduce.m0.p.fti.net:9090'\n  # all node requiresunique id\n  nodes:\n    - id: 1\n    - id: 2\n    - id: 3\n  # type defines if the cluster is internal (i.e manager by the operator) or external.\n  # :Enum={\"external\",\"internal\"}\n  type: 'external'\n  # clientType defines if the operator will use basic or tls authentication to query the NiFi cluster.\n  # Enum={\"tls\",\"basic\"}\n  clientType: 'basic'\n  # secretRef reference the secret containing the informations required to authenticate to the cluster.\n  secretRef:\n    name: nifikop-credentials\n    namespace: nifikop-nifi\n```\n\n----------------------------------------\n\nTITLE: Installing cert-manager directly using kubectl\nDESCRIPTION: Command for direct installation of cert-manager v1.7.2 which is required by NiFiKop for issuing certificates to users and nodes in secured clusters.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/2_deploy_nifikop/1_quick_start.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Install the CustomResourceDefinitions and cert-manager itself\nkubectl apply -f \\\n    https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring High-Performance NiFi Storage with Multiple Repositories in YAML\nDESCRIPTION: Advanced configuration example showing how to set up multiple content and provenance repositories for high-performance NiFi installation. Includes both the property definitions and the corresponding persistent volume claims for storage.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n...\n  readOnlyConfig:\n    nifiProperties:\n      overrideConfigs: |\n        nifi.content.repository.directory.dir1=../content-additional/dir1\n        nifi.content.repository.directory.dir2=../content-additional/dir2\n        nifi.content.repository.directory.dir3=../content-additional/dir3\n        nifi.provenance.repository.directory.dir1=../provenance-additional/dir1\n        nifi.provenance.repository.directory.dir2=../provenance-additional/dir2\n...\n  nodeConfigGroups:\n    default_group:\n      ...\n      storageConfigs:\n      - mountPath: \"/opt/nifi/content-additional/dir1\"\n        name: content-repository-dir1\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/content-additional/dir2\"\n        name: content-repository-dir2\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/content-additional/dir3\"\n        name: content-repository-dir3\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/provenance-additional/dir1\"\n        name: provenance-repository-dir1\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/provenance-additional/dir2\"\n        name: provenance-repository-dir2\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      ...\n```\n\n----------------------------------------\n\nTITLE: Installing Prometheus Operator with Helm for Monitoring NiFi Metrics using Console Commands\nDESCRIPTION: This snippet provides console commands to create a dedicated Kubernetes namespace 'monitoring-system', add the Prometheus community Helm chart repository, update Helm repos, and install the Prometheus Operator via Helm. The installation disables many default components (Grafana, Alertmanager, kube-system exporters) to customize for a NiFi cluster monitoring use case. This setup is required to deploy Prometheus operator for subsequent custom Prometheus and ServiceMonitor resources. Assumes Helm and kubectl configured with necessary cluster permissions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_3\n\nLANGUAGE: console\nCODE:\n```\nkubectl create namespace monitoring-system\nhelm repo add prometheus https://prometheus-community.github.io/helm-charts\nhelm repo update\nhelm install prometheus prometheus/kube-prometheus-stack --namespace monitoring-system \\\n    --set prometheusOperator.createCustomResource=false \\\n    --set prometheusOperator.logLevel=debug \\\n    --set prometheusOperator.alertmanagerInstanceNamespaces=monitoring-system \\\n    --set prometheusOperator.namespaces.additional[0]=monitoring-system \\\n    --set prometheusOperator.prometheusInstanceNamespaces=monitoring-system \\\n    --set prometheusOperator.thanosRulerInstanceNamespaces=monitoring-system \\\n    --set defaultRules.enable=false \\\n    --set alertmanager.enabled=false \\\n    --set grafana.enabled=false \\\n    --set kubeApiServer.enabled=false \\\n    --set kubelet.enabled=false \\\n    --set kubeControllerManager.enabled=false \\\n    --set coreDNS.enabled=false \\\n    --set kubeEtcd.enabled=false \\\n    --set kubeScheduler.enabled=false \\\n    --set kubeProxy.enabled=false \\\n    --set kubeStateMetrics.enabled=false \\\n    --set prometheus.enabled=false\n```\n\n----------------------------------------\n\nTITLE: Deploying Prometheus Operator Helm Console\nDESCRIPTION: This command installs the 'kube-prometheus-stack' Helm chart into the 'monitoring-system' namespace. It deploys the Prometheus Operator but explicitly disables several default components (like Alertmanager, Grafana, default rules, and various kube-state-metrics) to focus solely on deploying the operator that manages custom Prometheus resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_8\n\nLANGUAGE: console\nCODE:\n```\nhelm install prometheus prometheus/kube-prometheus-stack --namespace monitoring-system \\\n    --set prometheusOperator.createCustomResource=false \\\n    --set prometheusOperator.logLevel=debug \\\n    --set prometheusOperator.alertmanagerInstanceNamespaces=monitoring-system \\\n    --set prometheusOperator.namespaces.additional[0]=monitoring-system \\\n    --set prometheusOperator.prometheusInstanceNamespaces=monitoring-system \\\n    --set prometheusOperator.thanosRulerInstanceNamespaces=monitoring-system \\\n    --set defaultRules.enable=false \\\n    --set alertmanager.enabled=false \\\n    --set grafana.enabled=false \\\n    --set kubeApiServer.enabled=false \\\n    --set kubelet.enabled=false \\\n    --set kubeControllerManager.enabled=false \\\n    --set coreDNS.enabled=false \\\n    --set kubeEtcd.enabled=false \\\n    --set kubeScheduler.enabled=false \\\n    --set kubeProxy.enabled=false \\\n    --set kubeStateMetrics.enabled=false \\\n    --set prometheus.enabled=false\n```\n\n----------------------------------------\n\nTITLE: Defining Nifikop NiFi UserGroup Resource in YAML\nDESCRIPTION: This YAML snippet provides an example manifest for creating a `NifiUserGroup` resource in Kubernetes using the Nifikop operator. It defines a user group named 'group-test' with a custom identity, links it to a specific NiFi cluster, includes references to two existing NiFi users, and grants a global read policy on the '/counters' resource. This resource requires the referenced `NifiCluster` and `NifiUser` resources to exist.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/5_references/6_nifi_usergroup.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  identity: \"My Special Group\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n    - name: nc-controller.nifikop.mgt.cluster.local\n  accessPolicies:\n    - type: global\n      action: read\n      resource: /counters\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFiKop CRDs manually via kubectl\nDESCRIPTION: These commands deploy custom resource definitions for NiFiKop components directly using kubectl, bypassing Helm for CRD installation. It covers necessary components including clusters, users, groups, dataflows, parameter contexts, and registry clients, facilitating custom deployment configurations.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/2_deploy_nifikop/1_quick_start.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring ReadOnlyConfig in YAML for NiFiKop\nDESCRIPTION: A YAML configuration example for ReadOnlyConfig in NiFiKop, specifying thread counts, logback, authorizer, and various properties configurations. This snippet demonstrates how to define configuration maps, secrets, and overrides for NiFi cluster components.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/5_references/1_nifi_cluster/2_read_only_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreadOnlyConfig:\n  # MaximumTimerDrivenThreadCount define the maximum number of threads for timer driven processors available to the system.\n  maximumTimerDrivenThreadCount: 30\n  # MaximumEventDrivenThreadCount define the maximum number of threads for event driven processors available to the system.\n  maximumEventDrivenThreadCount: 10\n  # Logback configuration that will be applied to the node\n  logbackConfig:\n    # logback.xml configuration that will replace the one produced based on template\n    replaceConfigMap:\n      # The key of the value,in data content, that we want use.\n      data: logback.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # logback.xml configuration that will replace the one produced based on template and overrideConfigMap\n    replaceSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: logback.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n  # Authorizer configuration that will be applied to the node\n  authorizerConfig:\n    # An authorizers.xml configuration template that will replace the default template seen in authorizers.go\n    replaceTemplateConfigMap:\n      # The key of the value, in data content, that we want use.\n      data: authorizers.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # An authorizers.xml configuration template that will replace the default template seen in authorizers.go and the replaceTemplateConfigMap\n    replaceTemplateSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: authorizers.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n  # NifiProperties configuration that will be applied to the node.\n  nifiProperties:\n    # Additionnal nifi.properties configuration that will override the one produced based on template and\n    # configuration\n    overrideConfigMap:\n      # The key of the value,in data content, that we want use.\n      data: nifi.properties\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop.\n    # Additionnal nifi.properties configuration that will override the one produced based\n    #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    overrideSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: nifi.properties\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # Additionnal nifi.properties configuration that will override the one produced based\n    #\ton template, configurations and overrideConfigMap\n    overrideConfigs: |\n      nifi.ui.banner.text=NiFiKop\n    # A comma separated list of allowed HTTP Host header values to consider when NiFi\n    # is running securely and will be receiving requests to a different host[:port] than it is bound to.\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#web-properties\n    #      webProxyHosts:\n    # Nifi security client auth\n    needClientAuth: false\n    # Indicates which of the configured authorizers in the authorizers.xml file to use\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#authorizer-configuration\n  #      authorizer:\n  # ZookeeperProperties configuration that will be applied to the node.\n  zookeeperProperties:\n    #      # Additionnal zookeeeper.properties configuration that will override the one produced based on template and\n    #      # configuration\n    #      overrideConfigMap:\n    #        # The key of the value,in data content, that we want use.\n    #        data: zookeeeper.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop.\n    #      # Additionnal zookeeeper.properties configuration that will override the one produced based\n    #      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    #      overrideSecretConfig:\n    #        # The key of the value,in data content, that we want use.\n    #        data: zookeeeper.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop\n    # Additionnal zookeeper.properties configuration that will override the one produced based\n    # on template and configurations.\n    overrideConfigs: |\n      initLimit=15\n      autopurge.purgeInterval=24\n      syncLimit=5\n      tickTime=2000\n      dataDir=./state/zookeeper\n      autopurge.snapRetainCount=30\n  # BootstrapProperties configuration that will be applied to the node.\n  bootstrapProperties:\n    #      # Additionnal bootstrap.properties configuration that will override the one produced based on template and\n    #      # configuration\n    #      overrideConfigMap:\n    #        # The key of the value,in data content, that we want use.\n    #        data: bootstrap.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop.\n    #      # Additionnal bootstrap.properties configuration that will override the one produced based\n    #      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    #      overrideSecretConfig:\n    #        # The key of the value,in data content, that we want use.\n    #        data: bootstrap.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop\n    # JVM memory settings\n    nifiJvmMemory: \"512m\"\n    # Additionnal bootstrap.properties configuration that will override the one produced based\n    # on template and configurations.\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#bootstrap_properties\n    overrideConfigs: |\n      # java.arg.4=-Djava.net.preferIPv4Stack=true\n```\n\n----------------------------------------\n\nTITLE: Installing Nifikop with Skip CRDs\nDESCRIPTION: This snippet installs the Nifikop chart using Helm and the `--skip-crds` parameter, which prevents the chart from installing the CRDs.  This is useful when CRDs are already installed or when you want to manage them separately. It requires Helm, a valid Helm repository, and a file path where the helm chart is located.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\n$ helm install --name nifikop ./helm/nifikop --set namespaces={\\\"nifikop\\\"} --skip-crds\n```\n\n----------------------------------------\n\nTITLE: Patching NifiKop CRDs for Webhook Conversion - YAML\nDESCRIPTION: Demonstrates how to patch existing NifiKop CRDs with the necessary annotations for cert-manager and conversion webhook configuration. This snippet updates the CRD spec to use 'Webhook' strategy, specifies the webhook service and its endpoints, and lists supported conversion review versions. Required dependencies include cert-manager, valid Kubernetes CRDs, and a Helm release with webhook enabled. Parameters such as namespace, certificate_name, and webhook_service_name must be set to match the deployed environment. Input is a CRD manifest patched with these fields; output is a CRD ready for webhook-based version conversion. Ensure cert-manager and the operator's webhook service are accessible; otherwise, CRD conversion may fail.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/7_upgrade_guides/3_v0.16.0_to_v1.0.0.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n```\n\n----------------------------------------\n\nTITLE: Advanced NiFi Content and Provenance Directory Configuration\nDESCRIPTION: This section shows how to customize NiFi's content and provenance repository directories for performance tuning. It demonstrates inline configuration of directory paths and the setup of persistent volume claims (PVCs) in Kubernetes for multiple directories, with metadata and storage specifications.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\n...\n  readOnlyConfig:\n    nifiProperties:\n      overrideConfigs: |\n        nifi.content.repository.directory.dir1=../content-additional/dir1\n        nifi.content.repository.directory.dir2=../content-additional/dir2\n        nifi.content.repository.directory.dir3=../content-additional/dir3\n        nifi.provenance.repository.directory.dir1=../provenance-additional/dir1\n        nifi.provenance.repository.directory.dir2=../provenance-additional/dir2\n...\n  nodeConfigGroups:\n    default_group:\n      ...\n      storageConfigs:\n      - mountPath: \"/opt/nifi/content-additional/dir1\"\n        name: content-repository-dir1\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/content-additional/dir2\"\n        name: content-repository-dir2\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/content-additional/dir3\"\n        name: content-repository-dir3\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/provenance-additional/dir1\"\n        name: provenance-repository-dir1\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/provenance-additional/dir2\"\n        name: provenance-repository-dir2\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Connection API Schema\nDESCRIPTION: This YAML snippet defines a NiFi connection configuration. It specifies the API version, kind, metadata including name and namespace, and the specifications that define source and destination components, configuration, and update strategies. The configuration includes flow file expiration, back pressure settings, load balancing strategy, prioritizers, label index and connection bends. This configuration is crucial for setting up data flow connections within a NiFi environment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/5_references/8_nifi_connection.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiConnection\nmetadata:\n  name: connection\n  namespace: instances\nspec:\n  source:\n    name: input\n    namespace: instances\n    subName: output_1\n    type: dataflow\n  destination:\n    name: output\n    namespace: instances\n    subName: input_1\n    type: dataflow\n  configuration:\n    flowFileExpiration: 1 hour\n    backPressureDataSizeThreshold: 100 GB\n    backPressureObjectThreshold: 10000\n    loadBalanceStrategy: PARTITION_BY_ATTRIBUTE\n    loadBalancePartitionAttribute: partition_attribute\n    loadBalanceCompression: DO_NOT_COMPRESS\n    prioritizers: \n      - NewestFlowFileFirstPrioritizer\n      - FirstInFirstOutPrioritizer\n    labelIndex: 0\n    bends:\n      - posX: 550\n        posY: 550\n      - posX: 550\n        posY: 440\n      - posX: 550\n        posY: 88\n  updateStrategy: drain\n```\n```\n\n----------------------------------------\n\nTITLE: Defining Nifi Listener Configuration in YAML\nDESCRIPTION: This YAML snippet configures multiple internal Nifi listeners, specifying their types (e.g., https, cluster, s2s, prometheus, load-balance), names, container ports, and optionally protocols. It also defines TLS secrets for SSL setup with options to specify secret names and whether to create certificates automatically. Required fields include listener type, containerPort, and TLS secret name. The snippet is intended for configuring Nifi listeners within a Kubernetes environment where ports and protocols are exposed securely.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/5_references/1_nifi_cluster/6_listeners_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    - type: \"https\"\n      name: \"https\"\n      containerPort: 8443\n    - type: \"cluster\"\n      name: \"cluster\"\n      containerPort: 6007\n    - type: \"s2s\"\n      name: \"s2s\"\n      containerPort: 10000\n    - type: \"prometheus\"\n      name: \"prometheus\"\n      containerPort: 9090\n    - type: \"load-balance\"\n      name: \"load-balance\"\n      containerPort: 6342\n    - name: \"my-custom-listener-port\"\n      containerPort: 1234\n      protocol: \"TCP\"\n  sslSecrets:\n    tlsSecretName: \"test-nifikop\"\n    create: true\n```\n\n----------------------------------------\n\nTITLE: Migrating NiFiKop CRDs Kubernetes Client Node.js\nDESCRIPTION: This JavaScript script performs the core migration logic. It connects to the Kubernetes API, lists custom resources of a specified type from the old API group (`nifi.orange.com`), creates new resources in the target API group (`nifi.konpyutaika.com`) by copying relevant fields (metadata and spec), and then updates the status of the new resource by copying it from the old one. It skips resources with owner references and requires specifying the resource type via command-line arguments.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nprocess.env['NODE_TLS_REJECT_UNAUTHORIZED'] = 0;\nconst k8s = require('@kubernetes/client-node');\n\nconst kc = new k8s.KubeConfig();\nkc.loadFromDefault();\n\nconst k8sApi = kc.makeApiClient(k8s.CustomObjectsApi);\n\nconst KONPYUTAIKA_GROUP = 'nifi.konpyutaika.com';\nconst KONPYUTAIKA_GROUP_VERSION = 'v1alpha1';\nconst ORANGE_GROUP = 'nifi.orange.com';\nconst ORANGE_GROUP_VERSION = 'v1alpha1';\n\nconst call = async (SRC_GRP, SRC_GRP_VER, DST_GRP, DST_GRP_VER, KIND_PLURAL, NAMESPACE) => {\n\tconsole.log(`Listing ${KIND_PLURAL} of ${SRC_GRP}/${SRC_GRP_VER} in ${NAMESPACE}...`);\n\tconst listResources = (await k8sApi.listNamespacedCustomObject(SRC_GRP, SRC_GRP_VER, NAMESPACE, KIND_PLURAL)).body.items;\n\treturn Promise.all(listResources.map(async (resource) => {\n\t\ttry {\n\t\t\tconsole.log(`Found ${resource.kind} \"${resource.metadata.name}\" of ${resource.apiVersion} in ${NAMESPACE}`);\n\n\t\t\tif (resource.metadata.ownerReferences) {\n\t\t\t\tconsole.log(`${resource.kind} ${resource.metadata.name} mananged by something else (ownerRefereces is set).`);\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tconst bodyResource = {\n\t\t\t\tapiVersion: `${DST_GRP}/${DST_GRP_VER}`,\n\t\t\t\tkind: resource.kind,\n\t\t\t\tmetadata: {\n\t\t\t\t\tname: resource.metadata.name,\n\t\t\t\t\tannotations: resource.metadata.annotations,\n\t\t\t\t\tlabels: resource.metadata.labels\n\t\t\t\t},\n\t\t\t\tspec: resource.spec\n\t\t\t};\n\n\t\t\tconsole.log(`Creating ${bodyResource.kind} \"${bodyResource.metadata.name}\" of ${bodyResource.apiVersion} in ${NAMESPACE}...`);\n\t\t\tconst newResource = (await k8sApi.createNamespacedCustomObject(DST_GRP, DST_GRP_VER, NAMESPACE, KIND_PLURAL, bodyResource)).body;\n\t\t\tconsole.log('...done creating.');\n\n\t\t\tconst bodyStatus = {\n\t\t\t\tapiVersion: newResource.apiVersion,\n\t\t\t\tkind: newResource.kind,\n\t\t\t\tmetadata: {\n\t\t\t\t\tname: newResource.metadata.name,\n\t\t\t\t\tresourceVersion: newResource.metadata.resourceVersion\n\t\t\t\t},\n\t\t\t\tstatus: resource.status\n\t\t\t};\n\n\t\t\tconsole.log(`Copying status from ${resource.kind} \"${resource.metadata.name}\" of ${newResource.apiVersion} to ${newResource.kind} \"${newResource.metadata.name}\" of ${newResource.apiVersion} in ${NAMESPACE}...`);\n\t\t\tconst newResourceWithStatus = (await k8sApi.replaceNamespacedCustomObjectStatus(DST_GRP, DST_GRP_VER, NAMESPACE, KIND_PLURAL, bodyStatus.metadata.name, bodyStatus)).body;\n\t\t\tconsole.log('...done copying.');\n\t\t\treturn newResourceWithStatus;\n\t\t}\n\t\tcatch (e) {\n\t\t\tconsole.error(e.body ? e.body.message ? e.body.message : e.body : e);\n\t\t}\n\t}));\n};\n\nconst argv = require('minimist')(process.argv.slice(2));\n\nlet NAMESPACE = argv.namespace ? argv.namespace.length > 0 ? argv.namespace : 'default' : 'default';\nlet KIND_PLURAL = {\n\tcluster: 'nificlusters',\n\tdatabflow: 'nifidataflows',\n\tparametercontext: 'nifiparametercontexts',\n\tregistryclient: 'nifiregistryclients',\n\tuser: 'nifiusers',\n\tusergroup: 'nifiusergroups'\n};\n\nif (!argv.type) {\n\tconsole.error('Type not provided');\n\tprocess.exit(1);\n}\n\nif (!KIND_PLURAL[argv.type]) {\n\tconsole.error(`Type ${argv.type} is not one of the following types: ${Object.keys(KIND_PLURAL)}`);\n\tprocess.exit(1);\n}\n\nconsole.log(`########### START: ${KIND_PLURAL[argv.type]} ###########`);\ncall( ORANGE_GROUP, ORANGE_GROUP_VERSION, KONPYUTAIKA_GROUP, KONPYUTAIKA_GROUP_VERSION, KIND_PLURAL[argv.type], NAMESPACE)\n\t.then(r => console.log('############ END ############'))\n\t.catch(e => console.error(e));\n```\n\n----------------------------------------\n\nTITLE: Retrieving Namespace UID Range on OpenShift (Bash)\nDESCRIPTION: Gets the supplemental group UID range annotation ('openshift.io/sa.scc.supplemental-groups') for the specified namespace ('nifi') on an OpenShift cluster using kubectl and processes it with sed and tr to extract the base UID. This UID is needed for deploying NiFiKop with the correct permissions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/2_deploy_nifikop/1_quick_start.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nuid=$(kubectl get namespace nifi -o=jsonpath='{.metadata.annotations.openshift\\.io/sa\\.scc\\.supplemental-groups}' | sed 's/\\/10000$//' | tr -d '[:space:]')\n```\n\n----------------------------------------\n\nTITLE: Listing All Helm Releases\nDESCRIPTION: This command lists all Helm releases, including those that are currently deployed, deleted, and those that have failed. This provides a comprehensive view of all release history.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --all\n```\n\n----------------------------------------\n\nTITLE: Creating a Kubernetes Secret for Sensitive Parameters using kubectl\nDESCRIPTION: This `kubectl` command creates a Kubernetes Secret named `secret-params` in the `nifikop` namespace. It stores sensitive key-value pairs (`secret1=yop`, `secret2=yep`) which can be referenced by a `NifiParameterContext` CRD to manage sensitive parameters for NiFi dataflows securely.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/3_tasks/3_nifi_dataflow.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create secret generic secret-params \\\n    --from-literal=secret1=yop \\\n    --from-literal=secret2=yep \\\n    -n nifikop\n```\n\n----------------------------------------\n\nTITLE: Executing NiFiKop CR migration script (Bash)\nDESCRIPTION: Provides the bash command to run the Node.js migration script using the `npm start` alias configured in `package.json`. It requires specifying the desired NiFiKop resource type (`--type`) and the Kubernetes namespace (`--namespace`) where the resources reside and should be migrated.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nnpm start -- --type=<NIFIKOP_RESOURCE> --namespace=<K8S_NAMESPACE>\n```\n\n----------------------------------------\n\nTITLE: Installing Nifikop Helm Chart in Dry Run Mode - Bash\nDESCRIPTION: This Helm command installs the Nifikop chart in dry-run mode, which simulates the installation without changing any Kubernetes resources. Parameters include logLevel and namespaces. No resources are created; output is a rendered manifest preview useful for validation and debugging. Helm must be installed and kubeconfig valid.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop \\\n    --dry-run \\\n    --set logLevel=Debug \\\n    --set namespaces={\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Defining NifiUserGroup for Apache NiFi Access Management in YAML\nDESCRIPTION: This YAML snippet defines a NifiUserGroup resource to manage user groups in Apache NiFi via Kubernetes. It declares the target NiFi cluster, lists the users by referencing their NifiUser resources, and specifies access policies granted to the group. Dependencies include the NifiUserGroup CRD and existing NifiUser resources. Key parameters include clusterRef (link to NiFi cluster), usersRef (list of NifiUser references by name and optionally namespace), and accessPolicies (defining policy type, action, and resource). The expected input is a valid Kubernetes YAML resource, and the output is creation and management of a corresponding NiFi user group by the operator. Limitations include the requirement to create NifiUser resources beforehand even if users exist in NiFi cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/2_manage_users_and_accesses/2_groups_management.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  # Contains the reference to the NifiCluster with the one the registry client is linked.\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  # contains the list of reference to NifiUsers that are part to the group.\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n#     namespace: nifikop\n    - name: nc-controller.nifikop.mgt.cluster.local\n  # defines the list of access policies that will be granted to the group.\n  accessPolicies:\n      # defines the kind of access policy, could be \"global\" or \"component\".\n    - type: global\n      # defines the kind of action that will be granted, could be \"read\" or \"write\"\n      action: read\n      # resource defines the kind of resource targeted by this access policies, please refer to the following page:\n      #\thttps://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#access-policies\n      resource: /counters\n#      # componentType is used if the type is \"component\", it's allow to define the kind of component on which is the\n#      # access policy\n#      componentType: \"process-groups\"\n#      # componentId is used if the type is \"component\", it's allow to define the id of the component on which is the\n#      # access policy\n#      componentId: \"\"\n```\n\n----------------------------------------\n\nTITLE: Applying NifiCluster CR Update | Shell\nDESCRIPTION: This command uses `kubectl apply` to send the modified `NifiCluster` Custom Resource definition for scaledown to the Kubernetes API server. The NiFiKop operator detects the removal of the node from the configuration and triggers the graceful decommissioning steps. Assumes the cluster is in the `nifi` namespace and the file is `config/samples/simplenificluster.yaml`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nkubectl -n nifi apply -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Listeners Configuration in YAML\nDESCRIPTION: This YAML snippet defines the NiFi listeners configuration, including internal listeners (https, cluster, s2s, prometheus, load-balance) and SSL secrets. The internal listeners specify the type, name, and container port for each listener. The SSL secrets section defines the TLS secret name and whether to create the secrets.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/5_references/1_nifi_cluster/6_listeners_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    - type: \"https\"\n      name: \"https\"\n      containerPort: 8443\n    - type: \"cluster\"\n      name: \"cluster\"\n      containerPort: 6007\n    - type: \"s2s\"\n      name: \"s2s\"\n      containerPort: 10000\n    - type: \"prometheus\"\n      name: \"prometheus\"\n      containerPort: 9090\n    - type: \"load-balance\"\n      name: \"load-balance\"\n      containerPort: 6342\n  sslSecrets:\n    tlsSecretName: \"test-nifikop\"\n    create: true\n```\n\n----------------------------------------\n\nTITLE: Adding Prometheus Helm Repository - console\nDESCRIPTION: Adds the Prometheus community Helm chart repository for later dependency installations. Requires helm CLI and internet access. No inputs required. Repository will not be added more than once unless removed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_6\n\nLANGUAGE: console\nCODE:\n```\nhelm repo add prometheus https://prometheus-community.github.io/helm-charts\n```\n\n----------------------------------------\n\nTITLE: Configuring External Services in NiFiKop with YAML\nDESCRIPTION: Example YAML configuration for defining an external service in NiFiKop. This snippet shows how to configure a ClusterIP service exposing port 8080 mapped to the NiFi 'http' internal listener, along with custom annotations and labels.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/5_references/1_nifi_cluster/7_external_service_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  externalServices:\n    - name: \"clusterip\"\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8080\n            internalListenerName: \"http\"\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n\n----------------------------------------\n\nTITLE: Adding Start Script to package.json\nDESCRIPTION: Defines a new npm script named `start` in the `package.json` file. This script executes the `index.js` file using Node.js, disabling SSL warnings, which is useful in certain Kubernetes environments or development setups. This makes running the migration script easier via `npm start`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"start\": \"node --no-warnings index.js\"\n```\n\n----------------------------------------\n\nTITLE: Defining Node Configuration Groups in NiFiKop YAML\nDESCRIPTION: This snippet shows how to set up multiple node configuration groups, specifying parameters like provenance storage, user IDs, service account names, and resource requirements. These groups enable flexible node deployment tailored to different resource needs.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\nnodeConfigGroups:\n  default_group:\n    provenanceStorage: \"10 GB\"\n    runAsUser: 1000\n    serviceAccountName: \"default\"\n    resourcesRequirements:\n      limits:\n        cpu: \"2\"\n        memory: 3Gi\n      requests:\n        cpu: \"1\"\n        memory: 3Gi\n  high_mem_group:\n    provenanceStorage: \"10 GB\"\n    runAsUser: 1000\n    serviceAccountName: \"default\"\n    resourcesRequirements:\n      limits:\n        cpu: \"2\"\n        memory: 30Gi\n      requests:\n        cpu: \"1\"\n        memory: 30Gi\n```\n\n----------------------------------------\n\nTITLE: Listing Managed NiFiUserGroups (Console)\nDESCRIPTION: This console command retrieves and lists the `NifiUserGroup` resources within the `nifikop` namespace.  It allows you to verify the managed groups created by the operator: `managed-admins`, `managed-nodes`, and `managed-readers`. Requires `kubectl` to be configured and connected to the Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/3_manage_nifi/2_manage_users_and_accesses/3_managed_groups.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nkubectl get -n nifikop nifiusergroups.nifi.konpyutaika.com \nNAME              AGE\nmanaged-admins    6d7h\nmanaged-nodes     6d7h\nmanaged-readers   6d7h\n```\n\n----------------------------------------\n\nTITLE: Listing Managed NifiUserGroups Kubernetes Console\nDESCRIPTION: This console command demonstrates how to use `kubectl` to list the `NifiUserGroup` resources managed by the NiFiKop operator. The output shows the default managed groups created by the operator: `managed-admins`, `managed-nodes`, and `managed-readers`, allowing users to verify their existence and current state within the Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/2_manage_users_and_accesses/3_managed_groups.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nkubectl get -n nifikop nifiusergroups.nifi.konpyutaika.com \nNAME              AGE\nmanaged-admins    6d7h\nmanaged-nodes     6d7h\nmanaged-readers   6d7h\n```\n\n----------------------------------------\n\nTITLE: Configuring Webhook Conversion for NiFiKop CRDs (YAML snippet)\nDESCRIPTION: This YAML snippet adds annotations and conversion strategy settings to CRD manifests to enable resource conversion webhook functionality from 'v1alpha1' to 'v1'. It specifies the webhook service location and required review versions for seamless resource management.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/2_deploy_nifikop/1_quick_start.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n... \nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n```\n\n----------------------------------------\n\nTITLE: Defining a LoadBalancer External Service for Kubernetes in YAML\nDESCRIPTION: This snippet illustrates the configuration of a Kubernetes 'LoadBalancer' external service named 'nlb', specifying load balancer class, port configurations, annotations, and labels. It demonstrates setting up a network load balancer with UDP and TCP protocols on different ports.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/5_references/1_nifi_cluster/7_external_service_config.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\nexternalServices:\n    - name: \"nlb\"\n      spec:\n        type: LoadBalancer\n        loadBalancerClass: \"service.k8s.aws/nlb\"\n        portConfigs:\n          - port: 8080\n            internalListenerName: \"http\"\n          - port: 7890\n            internalListenerName: \"my-custom-udp-listener\"\n            protocol: UDP\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n\n----------------------------------------\n\nTITLE: Defining NifiUser Kubernetes CRD\nDESCRIPTION: This YAML snippet defines a NifiUser Kubernetes Custom Resource. It specifies the API version, kind (NifiUser), metadata (name), and the desired specification, including the user's identity, cluster reference, and whether to create a certificate.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/5_references/2_nifi_user.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: aguitton\nspec:\n  identity: alexandre.guitton@konpyutaika.com\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  createCert: false\n```\n\n----------------------------------------\n\nTITLE: Installing cert-manager directly with kubectl (YAML Deployment Method)\nDESCRIPTION: This script installs cert-manager by applying its YAML manifest using kubectl, necessary for certificate management in a secured NiFiKop deployment. It assumes access to the cert-manager YAML release URL for version v1.7.2.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/2_deploy_nifikop/1_quick_start.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install the CustomResourceDefinitions and cert-manager itself\nkubectl apply -f \\\n    https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing Nifikop Chart (Release Name)\nDESCRIPTION: This snippet demonstrates a standard installation of the Nifikop chart with a specified release name, via Helm. Requires Helm and a valid Helm repository.  The user must replace `<release name>` with their desired release name.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nhelm install <release name> konpyutaika/nifikop\n```\n\n----------------------------------------\n\nTITLE: Building NiFiKop Locally in Bash\nDESCRIPTION: Command to build the NiFiKop operator using the Makefile in your local Go environment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/6_contributing/1_developer_guide.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmake build\n```\n\n----------------------------------------\n\nTITLE: Adding a Node to NiFiCluster Configuration in YAML\nDESCRIPTION: A YAML configuration example showing how to add a new node (id: 25) to an existing NiFi cluster by modifying the NifiCluster resource definition.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n  clusterManager: zookeeper\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.12.1\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n    - id: 2\n      nodeConfigGroup: \"default_group\"\n# >>>> START: The new node\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n# <<<< END\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: Pushing the Docker Image to Registry\nDESCRIPTION: Uploads the built Docker image to a Docker registry, such as Docker Hub, preparing it for deployment via Helm or other methods.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/6_contributing/1_developer_guide.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n$ make docker-push\n```\n\n----------------------------------------\n\nTITLE: Deploying two NifiDataflows for connection in NiFiKop with YAML\nDESCRIPTION: Example YAML configuration for deploying two NifiDataflow resources named 'input' and 'output' that can be later connected. The input dataflow must have an output port and the output dataflow must have an input port.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/4_manage_connections/1_deploy_connection.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiDataflow\nmetadata:\n  name: input\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  bucketId: deedb9f6-65a4-44e9-a1c9-722008fcd0ba\n  flowId: ab95431d-980d-41bd-904a-fac4bd864ba6\n  flowVersion: 1\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  skipInvalidComponent: true\n  skipInvalidControllerService: true\n  syncMode: always\n  updateStrategy: drain\n  flowPosition:\n    posX: 0\n    posY: 0\n---\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiDataflow\nmetadata:\n  name: output\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  bucketId: deedb9f6-65a4-44e9-a1c9-722008fcd0ba\n  flowId: fc5363eb-801e-432f-aa94-488838674d07\n  flowVersion: 2\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  skipInvalidComponent: true\n  skipInvalidControllerService: true\n  syncMode: always\n  updateStrategy: drain\n  flowPosition:\n    posX: 750\n    posY: 0\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiConnection Resource in YAML\nDESCRIPTION: This YAML snippet defines a NifiConnection CRD that connects two existing NifiDataflows ('input' and 'output') using NiFiKop in Kubernetes. It configures source and destination references, connection configuration including backpressure thresholds, flow file expiration, bends for the connection path, label index, and update strategy. Required dependencies include pre-created NifiDataflow resources, and that both flows have correct input/output port definitions. Inputs are the referenced dataflows; the output is an active connection managed within the NiFi cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/4_manage_connections/1_deploy_connection.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiConnection\nmetadata:\n  name: connection\n  namespace: nifikop\nspec:\n  source:\n    name: input\n    namespace: nifikop\n    subName: output\n    type: dataflow\n  destination:\n    name: output\n    namespace: nifikop\n    subName: input\n    type: dataflow\n  configuration:\n    backPressureDataSizeThreshold: 100 GB\n    backPressureObjectThreshold: 10000\n    flowFileExpiration: 1 hour\n    labelIndex: 0\n    bends:\n      - posX: 550\n        posY: 550\n      - posX: 550\n        posY: 440\n      - posX: 550\n        posY: 88\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Defining NodeConfigGroups in NiFiKop - YAML\nDESCRIPTION: This YAML snippet demonstrates how to configure multiple node groups using the Spec.nodeConfigGroups field in NiFiKop. Each group defines resource limits such as CPU and memory, provenance storage, user IDs, and service accounts. Dependencies include a Kubernetes environment with NiFiKop installed and basic understanding of NiFi node requirements. Specify CPU and memory in accurate units, and ensure serviceAccountName and PVCs (if used) exist in the same namespace; incorrect resource keys or missing fields may prevent successful deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  # nodeConfigGroups specifies multiple node configs with unique name\n  nodeConfigGroups:\n    default_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      serviceAccountName: \"default\"\n      # resourceRequirements works exactly like Container resources, the user can specify the limit and the requests\n      # through this property\n      # https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 3Gi\n    high_mem_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      serviceAccountName: \"default\"\n      # resourceRequirements works exactly like Container resources, the user can specify the limit and the requests\n      # through this property\n      # https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 30Gi\n        requests:\n          cpu: \"1\"\n          memory: 30Gi\n\n```\n\n----------------------------------------\n\nTITLE: Defining an External NiFiCluster Resource in YAML\nDESCRIPTION: This YAML snippet defines a `NifiCluster` resource for an external NiFi cluster. It specifies the root process group ID, node URI template, node IDs, cluster type (external), client authentication type (basic), and a reference to a secret containing the credentials. This configuration allows NiFiKop to interact with the external NiFi cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/3_external_cluster.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: externalcluster\nspec:\n  # rootProcessGroupId contains the uuid of the root process group for this cluster.\n  rootProcessGroupId: 'd37bee03-017a-1000-cff7-4eaaa82266b7'\n  # nodeURITemplate used to dynamically compute node uri.\n  nodeURITemplate: 'nifi0%d.integ.mapreduce.m0.p.fti.net:9090'\n  # all node requiresunique id\n  nodes:\n    - id: 1\n    - id: 2\n    - id: 3\n  # type defines if the cluster is internal (i.e manager by the operator) or external.\n  # :Enum={\"external\",\"internal\"}\n  type: 'external'\n  # clientType defines if the operator will use basic or tls authentication to query the NiFi cluster.\n  # Enum={\"tls\",\"basic\"}\n  clientType: 'basic'\n  # secretRef reference the secret containing the informations required to authenticate to the cluster.\n  secretRef:\n    name: nifikop-credentials\n    namespace: nifikop-nifi\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Cluster to Use External Issuer in YAML\nDESCRIPTION: Modifies the NiFi cluster YAML configuration to reference an external issuer for SSL certificates, replacing self-signed with externally issued certificates. Sets `useExternalDNS` and `issuerRef` fields accordingly.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/3_tasks/2_security/1_ssl.md#_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\nlistenersConfig:\n  clusterDomain: <DNS zone name>\n  useExternalDNS: true\n  ...\n  sslSecrets:\n    tlsSecretName: \"test-nifikop\"\n    create: true\n    issuerRef:\n      name: letsencrypt-staging\n      kind: Issuer\n```\n\n----------------------------------------\n\nTITLE: Installing cert-manager Directly\nDESCRIPTION: Installs cert-manager and its CustomResourceDefinitions (CRDs) directly into the Kubernetes cluster using kubectl apply with the official manifest URL. This method is suitable for direct installation without Helm.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/2_deploy_nifikop/1_quick_start.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Install the CustomResourceDefinitions and cert-manager itself\nkubectl apply -f \\\n    https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing Prometheus Operator via Helm with Custom Disabled Components - Console\nDESCRIPTION: This Helm install command deploys the kube-prometheus-stack chart in the 'monitoring-system' namespace with fine-grained settings to disable most default exporters and services, leaving only essential Prometheus operator features active. It also configures namespaces for operator components and disables alertmanager, grafana, and Kubernetes default metrics collection components to tailor Prometheus strictly for NiFi cluster scraping.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_7\n\nLANGUAGE: console\nCODE:\n```\nhelm install prometheus prometheus/kube-prometheus-stack --namespace monitoring-system \\\n    --set prometheusOperator.createCustomResource=false \\\n    --set prometheusOperator.logLevel=debug \\\n    --set prometheusOperator.alertmanagerInstanceNamespaces=monitoring-system \\\n    --set prometheusOperator.namespaces.additional[0]=monitoring-system \\\n    --set prometheusOperator.prometheusInstanceNamespaces=monitoring-system \\\n    --set prometheusOperator.thanosRulerInstanceNamespaces=monitoring-system \\\n    --set defaultRules.enable=false \\\n    --set alertmanager.enabled=false \\\n    --set grafana.enabled=false \\\n    --set kubeApiServer.enabled=false \\\n    --set kubelet.enabled=false \\\n    --set kubeControllerManager.enabled=false \\\n    --set coreDNS.enabled=false \\\n    --set kubeEtcd.enabled=false \\\n    --set kubeScheduler.enabled=false \\\n    --set kubeProxy.enabled=false \\\n    --set kubeStateMetrics.enabled=false \\\n    --set prometheus.enabled=false\n```\n\n----------------------------------------\n\nTITLE: Deploying Prometheus Operator with Custom Settings via Helm - console\nDESCRIPTION: This helm install command deploys the Prometheus operator using the kube-prometheus-stack Helm chart into the \"monitoring-system\" namespace. The command disables several default components like Alertmanager, Grafana, and various exporters, enabling a scoped deployment focused on monitoring the NiFi cluster namespace. It also sets log levels and namespace access controls to customize the operator's behavior.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_7\n\nLANGUAGE: console\nCODE:\n```\nhelm install prometheus prometheus/kube-prometheus-stack --namespace monitoring-system \\\n    --set prometheusOperator.createCustomResource=false \\\n    --set prometheusOperator.logLevel=debug \\\n    --set prometheusOperator.alertmanagerInstanceNamespaces=monitoring-system \\\n    --set prometheusOperator.namespaces.additional[0]=monitoring-system \\\n    --set prometheusOperator.prometheusInstanceNamespaces=monitoring-system \\\n    --set prometheusOperator.thanosRulerInstanceNamespaces=monitoring-system \\\n    --set defaultRules.enable=false \\\n    --set alertmanager.enabled=false \\\n    --set grafana.enabled=false \\\n    --set kubeApiServer.enabled=false \\\n    --set kubelet.enabled=false \\\n    --set kubeControllerManager.enabled=false \\\n    --set coreDNS.enabled=false \\\n    --set kubeEtcd.enabled=false \\\n    --set kubeScheduler.enabled=false \\\n    --set kubeProxy.enabled=false \\\n    --set kubeStateMetrics.enabled=false \\\n    --set prometheus.enabled=false\n```\n\n----------------------------------------\n\nTITLE: Building NiFiKop Docker Image\nDESCRIPTION: Builds a Docker image for NiFiKop using the make command.  The DOCKER_REPO_BASE variable must be set beforehand to specify the Docker repository to use.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/6_contributing/1_developer_guide.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nexport DOCKER_REPO_BASE={your-docker-repo}\nmake docker-build\n```\n\n----------------------------------------\n\nTITLE: Defining Node Configuration Snippets in YAML for NiFi Nodes\nDESCRIPTION: This YAML snippet provides configuration details for NiFi nodes, including node IDs, optional configuration groups, read-only configurations for rolling upgrades, resource requirements with CPU and memory limits, and storage settings specifying volume mount paths and persistent volume claims. It is used to configure individual NiFi nodes with customizable properties and resource management parameters.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/5_references/1_nifi_cluster/4_node.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n    - id: 0\n      # nodeConfigGroup can be used to ease the node configuration, if set only the id is required\n      nodeConfigGroup: \"default_group\"\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        nifiProperties:\n          overrideConfigs: |\n            nifi.ui.banner.text=NiFiKop - Node 0\n    - id: 2\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        overrideConfigs: |\n          nifi.ui.banner.text=NiFiKop - Node 2\n      # node configuration\n#       nodeConfig:\n    - id: 2\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        overrideConfigs: |\n          nifi.ui.banner.text=NiFiKop - Node 2\n      # node configuration\n      nodeConfig:\n        resourcesRequirements:\n          limits:\n            cpu: \"2\"\n            memory: 3Gi\n          requests:\n            cpu: \"1\"\n            memory: 1Gi\n        storageConfigs:\n          # Name of the storage config, used to name PV to reuse into sidecars for example.\n          - name: provenance-repository\n            # Path where the volume will be mount into the main nifi container inside the pod.\n            mountPath: \"/opt/nifi/provenance_repository\"\n            # Kubernetes PVC spec\n            # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n            pvcSpec:\n              accessModes:\n                - ReadWriteOnce\n              storageClassName: \"standard\"\n              resources:\n                requests:\n                  storage: 8Gi\n```\n\n----------------------------------------\n\nTITLE: Describing NifiCluster Status (kubectl)\nDESCRIPTION: This command retrieves detailed information about the status of a NiFi cluster, including the state of each node and any errors encountered during scaling operations.  It utilizes `kubectl describe` to inspect the `NifiCluster` resource. Prerequisites:  a running Kubernetes cluster and the `kubectl` command-line tool. The main output is the cluster's current status, including the graceful actions performed by the operator. This is used to monitor the scaling process.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nkubectl describe nificluster simplenifi\n```\n\n----------------------------------------\n\nTITLE: Installing Zookeeper with Helm\nDESCRIPTION: This bash script uses Helm to install a Zookeeper chart from a Docker registry. It sets parameters for resource requests/limits (memory and CPU), storage class, network policy, replica count, and creates a namespace.  It requires Helm installed and configured, and access to a Kubernetes cluster. The `storageClass` needs to be replaced with the user's environment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhelm install zookeeper oci://registry-1.docker.io/bitnamicharts/zookeeper \\\n    --namespace=zookeeper \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set global.storageClass=standard \\\n    --set networkPolicy.enabled=true \\\n    --set replicaCount=3 \\\n    --create-namespace\n```\n\n----------------------------------------\n\nTITLE: Defining External Service Configuration in YAML\nDESCRIPTION: This YAML defines an external service for a NiFi cluster. It configures how to expose NiFi pods externally through a Kubernetes service.  It specifies the `name`, `spec`, `type`, and `portConfigs`. It also maps internal listener names to external ports.  The example shows the `https` and `http-tracking` listeners being exposed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    - type: \"https\"\n      name: \"https\"\n      containerPort: 8443\n    - type: \"cluster\"\n      name: \"cluster\"\n      containerPort: 6007\n    - type: \"s2s\"\n      name: \"s2s\"\n      containerPort: 10000\n    - type: \"prometheus\"\n      name: \"prometheus\"\n      containerPort: 9090\n    - type: \"load-balance\"\n      name: \"load-balance\"\n      containerPort: 6342\n    - name: \"http-tracking\"\n      containerPort: 8081\nexternalServices:\n  - name: cluster-access\n    spec:\n      portConfigs:\n        - internalListenerName: https\n          port: 443\n        - internalListenerName: http-tracking\n          port: 80\n      type: LoadBalancer\n```\n\n----------------------------------------\n\nTITLE: Creating Monitoring Namespace - Console\nDESCRIPTION: Creates a dedicated Kubernetes namespace named `monitoring-system`. This is recommended for organizing and isolating the Prometheus deployment and related monitoring components within the cluster. Requires `kubectl` installed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_5\n\nLANGUAGE: console\nCODE:\n```\nkubectl create namespace monitoring-system\n```\n\n----------------------------------------\n\nTITLE: Deleting Nifikop CRDs in Kubernetes Bash\nDESCRIPTION: This bash snippet manually deletes multiple CustomResourceDefinitions (CRDs) related to Nifikop from a Kubernetes cluster. Requires 'kubectl' configured with sufficient permissions for CRD deletion. Key parameters are the fully qualified CRD names. Use with caution, as removing these CRDs will delete all managed resources of these kinds.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/2_setup/3_install/1_customizable_install_with_helm.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete crd nificlusters.nifi.konpyutaika.com\nkubectl delete crd nifiusers.nifi.konpyutaika.com\nkubectl delete crd nifiusergroups.nifi.konpyutaika.com\nkubectl delete crd nifiregistryclients.nifi.konpyutaika.com\nkubectl delete crd nifiparametercontexts.nifi.konpyutaika.com\nkubectl delete crd nifidataflows.nifi.konpyutaika.com\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Node with YAML\nDESCRIPTION: This YAML snippet demonstrates the configuration of a NiFi node, defining its ID, and applying read-only configurations to trigger rolling upgrades. The `readOnlyConfig` section allows for setting NiFi properties like the UI banner text. The node's ID is a unique identifier. This is a high-level overview and doesn't provide complete node configuration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/5_references/1_nifi_cluster/4_node.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n    - id: 0\n      # nodeConfigGroup can be used to ease the node configuration, if set only the id is required\n      nodeConfigGroup: \"default_group\"\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        nifiProperties:\n          overrideConfigs: |\n            nifi.ui.banner.text=NiFiKop - Node 0\n      # node configuration\n#       nodeConfig:\n    - id: 2\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        overrideConfigs: |\n          nifi.ui.banner.text=NiFiKop - Node 2\n      # node configuration\n      nodeConfig:\n        resourcesRequirements:\n          limits:\n            cpu: \"2\"\n            memory: 3Gi\n          requests:\n            cpu: \"1\"\n            memory: 1Gi\n        storageConfigs:\n          # Name of the storage config, used to name PV to reuse into sidecars for example.\n          - name: provenance-repository\n            # Path where the volume will be mount into the main nifi container inside the pod.\n            mountPath: \"/opt/nifi/provenance_repository\"\n            # Metadata to attach to the PVC that gets created\n            metadata:\n              labels:\n                my-label: my-value\n              annotations:\n                my-annotation: my-value\n            # Kubernetes PVC spec\n            # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n            pvcSpec:\n              accessModes:\n                - ReadWriteOnce\n              storageClassName: \"standard\"\n              resources:\n                requests:\n                  storage: 8Gi\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Cluster Internal Service for HTTPS (YAML)\nDESCRIPTION: This YAML snippet shows a fragment of the NiFi cluster deployment configuration. It defines an internal Kubernetes Service of type `ClusterIP` named \"nifi-cluster\". This service exposes port 8443, mapped to an internal HTTPS listener within the NiFi pods, and is the destination for the HTTPS VirtualService.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nspec:  \n  externalServices:  \n    - name: \"nifi-cluster\"\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8443\n            internalListenerName: \"https\"\n```\n\n----------------------------------------\n\nTITLE: Install NiFiKop Chart with Release Name - Helm\nDESCRIPTION: This command installs the NiFiKop Helm chart with a specified release name.  Replace `<release name>` with the desired name for the deployment.  Helm and kubectl must be properly configured.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhelm install <release name> konpyutaika/nifikop\n```\n\n----------------------------------------\n\nTITLE: List Deployed Helm Charts - Helm\nDESCRIPTION: This command lists all deployed Helm charts in the current Kubernetes context. It requires Helm to be installed and configured.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nhelm list\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio HTTPS Gateway (SSL Termination) in YAML\nDESCRIPTION: Defines an Istio `Gateway` resource configured for HTTPS traffic on port 443. It performs SSL termination (`mode: SIMPLE`) using a secret named `my-secret`. The gateway targets the `istio-ingressgateway` and handles the host `nifi.my-domain.com`, decrypting incoming HTTPS traffic for further processing within the mesh.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: nifi-gateway\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 443\n      name: https\n      protocol: HTTPS\n    tls:\n      mode: SIMPLE\n      credentialName: my-secret\n    hosts:\n    - nifi.my-domain.com\n```\n\n----------------------------------------\n\nTITLE: Implementing OIDC in NiFiCluster Custom Resource\nDESCRIPTION: YAML configuration for a NiFiCluster custom resource with OIDC authentication settings applied through the overrideConfigs field in the readOnlyConfig section.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/5_users_authentication/1_oidc.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\\nkind: NifiCluster\\n...\\nspec:\\n  ...\\n  readOnlyConfig:\\n    # NifiProperties configuration that will be applied to the node.\\n    nifiProperties:\\n      webProxyHosts:\\n        - nifistandard2.trycatchlearn.fr:8443\\n      # Additionnal nifi.properties configuration that will override the one produced based\\n      # on template and configurations.\\n      overrideConfigs: |\\n        nifi.security.user.oidc.discovery.url=<oidc server discovery url>\\n        nifi.security.user.oidc.client.id=<oidc client's id>\\n        nifi.security.user.oidc.client.secret=<oidc client's secret>\\n        nifi.security.identity.mapping.pattern.dn=CN=([^,]*)(?:, (?:O|OU)=.*)?\\n        nifi.security.identity.mapping.value.dn=$1\\n        nifi.security.identity.mapping.transform.dn=NONE\\n      ...\\n   ...\\n...\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio Gateway for HTTP (YAML)\nDESCRIPTION: This YAML snippet defines an Istio Gateway to intercept HTTP traffic for a NiFi cluster.  It specifies the port (80) and host (nifi.my-domain.com) that the gateway should listen on. The `selector` targets the Istio ingress gateway.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: nifi-gateway\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - nifi.my-domain.com\n```\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFi Cluster with Kubectl - Bash\nDESCRIPTION: Deploys a simple NiFi cluster resource definition to the Kubernetes cluster using `kubectl`. It applies a sample configuration file (`config/samples/simplenificluster.yaml`) to create the NiFi cluster resources within the `nifi` namespace. Ensure the Zookeeper service name is correctly configured in the YAML file before execution.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Add your zookeeper svc name to the configuration\nkubectl create -n nifi -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Uninstalling the Nifikop Helm Chart\nDESCRIPTION: Demonstrates how to uninstall a Helm release (named 'nifikop') using the `helm del` command (equivalent to `helm uninstall` in Helm 3). This removes all Kubernetes components associated with the specified chart release.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nhelm del nifikop\n```\n\n----------------------------------------\n\nTITLE: Defining NifiCluster Resource for Scaledown (YAML)\nDESCRIPTION: This YAML snippet shows the NifiCluster configuration after removing a node for a scale-down operation. Node with ID 2 is commented out in the `spec.nodes` list. Applying this configuration tells the operator to gracefully decommission and remove this specific node.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  headlessServiceEnabled: true\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.11.3\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n# >>>> START: node removed\n#    - id: 2\n#      nodeConfigGroup: \"default_group\"\n# <<<< END\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFi Cluster Using kubectl with OpenShift Configuration - Bash\nDESCRIPTION: Deploys a NiFi cluster on OpenShift by applying a customized YAML manifest that includes the correct UID for security context. This follows UID adjustment and ensures cluster components run with compliant permissions on OpenShift.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create -n nifi -f config/samples/openshift.yaml\n```\n\n----------------------------------------\n\nTITLE: Deploying Prometheus Custom Resource\nDESCRIPTION: Defines the Prometheus resource for deploying Prometheus server with specified evaluation interval, log level, resource requests, and service account. Requires Kubernetes CRD support and controllers installed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_9\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: Prometheus\nmetadata:\n  name: prometheus\n  namespace: monitoring-system\nspec:\n  enableAdminAPI: false\n  evaluationInterval: 30s\n  logLevel: debug\n  podMonitorSelector:\n    matchExpressions:\n    - key: release\n      operator: In\n      values:\n      - prometheus\n  resources:\n    requests:\n      memory: 400Mi\n  scrapeInterval: 30s\n  serviceAccountName: prometheus\n  serviceMonitorSelector:\n    matchExpressions:\n    - key: app\n      operator: In\n      values:\n      - nifi-cluster\n```\n\n----------------------------------------\n\nTITLE: Listing Deployed Helm Charts - Bash\nDESCRIPTION: Shows all currently deployed Helm releases in the cluster. Requires Helm CLI and read access to the cluster. No parameters required; outputs a table listing all deployed release names, namespaces, statuses, and more.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nhelm list\n```\n\n----------------------------------------\n\nTITLE: Installing cert-manager Directly Using kubectl - Bash\nDESCRIPTION: This snippet installs cert-manager and its CustomResourceDefinitions directly using kubectl by applying the official manifest from Jetstack. cert-manager is a required dependency for NiFiKop to enable certificate management for clusters. No parameters are required, but kubectl must be installed and configured to access your target Kubernetes cluster. It fetches the specified version (v1.7.2) manifest and applies it in a single step.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/2_deploy_nifikop/1_quick_start.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install the CustomResourceDefinitions and cert-manager itself\nkubectl apply -f \\\n    https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiUserGroup Resource in YAML\nDESCRIPTION: This YAML snippet defines a NifiUserGroup resource named `group-test`. It specifies the NiFi cluster to which the group is linked (`nc`), the users that are members of the group (`nc-0-node.nc-headless.nifikop.svc.cluster.local` and `nc-controller.nifikop.mgt.cluster.local`), and the access policies granted to the group (read access to /counters).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/3_manage_nifi/2_manage_users_and_accesses/2_groups_management.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  # Contains the reference to the NifiCluster with the one the registry client is linked.\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  # contains the list of reference to NifiUsers that are part to the group.\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n#     namespace: nifikop\n    - name: nc-controller.nifikop.mgt.cluster.local\n  # defines the list of access policies that will be granted to the group.\n  accessPolicies:\n      # defines the kind of access policy, could be \"global\" or \"component\".\n    - type: global\n      # defines the kind of action that will be granted, could be \"read\" or \"write\"\n      action: read\n      # resource defines the kind of resource targeted by this access policies, please refer to the following page:\n      #\thttps://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#access-policies\n      resource: /counters\n#      # componentType is used if the type is \"component\", it's allow to define the kind of component on which is the\n#      # access policy\n#      componentType: \"process-groups\"\n#      # componentId is used if the type is \"component\", it's allow to define the id of the component on which is the\n#      # access policy\n#      componentId: \"\"\n```\n\n----------------------------------------\n\nTITLE: Building the NiFiKop Docker Image and Setting Registry - Bash\nDESCRIPTION: Sets the Docker registry base as an environment variable and triggers the Docker image build for the operator from the local branch, using Makefile automation. DOCKER_REGISTRY_BASE should be replaced with the intended container registry name. Produces a Docker image tagged according to branching and versioning policies.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/6_contributing/1_developer_guide.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nexport DOCKER_REGISTRY_BASE={your-docker-repo}\nmake docker-build\n```\n\n----------------------------------------\n\nTITLE: NiFi Cluster Deployment with Prometheus Monitoring and Storage Configuration\nDESCRIPTION: This YAML configuration deploys a NiFi cluster with internal Prometheus metrics listener enabled on port 9090. It defines a node configuration group 'auto_scaling' with resource limits and persistent volume claims for logs, data, extensions, flowfile, configuration, content, and provenance repositories, each with specific storage class and size. Dependencies include Kubernetes YAML deployment tools and NiFi setup. This setup facilitates autoscaling by exposing metrics via Prometheus.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nspec:\n  ...\n  listenersConfig:\n    internalListeners:\n    ...\n    - containerPort: 9090\n      name: prometheus\n      type: prometheus \n  ...\n  nodeConfigGroups:\n    auto_scaling:\n      isNode: true\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 2Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n      serviceAccountName: external-dns\n      storageConfigs:\n        - mountPath: /opt/nifi/nifi-current/logs\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/data\n          name: data\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/extensions\n          name: extensions-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/flowfile_repository\n          name: flowfile-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/nifi-current/conf\n          name: conf\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/content_repository\n          name: content-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/provenance_repository\n          name: provenance-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n```\n\n----------------------------------------\n\nTITLE: Deploying a Simple NiFi Cluster\nDESCRIPTION: Command to create a NiFi cluster in the 'nifi' namespace using a sample configuration file. Before running this command, you should ensure the Zookeeper service name is properly configured.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Add your zookeeper svc name to the configuration\nkubectl create -n nifi -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining Child NiFi Parameter Context - YAML Manifest\nDESCRIPTION: This YAML snippet defines another `NifiParameterContext` resource, inheriting from the previous one. It utilizes the `inheritedParameterContexts` field to reference an existing parameter context. This demonstrates inheritance of parameters and configurations.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/5_references/4_nifi_parameter_context.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n--- \napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle-child\nspec:\n  description: \"It is a child test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  inheritedParameterContexts:\n    - name: dataflow-lifecycle\n  parameters:\n    - name: test\n      value: toto-child\n      description: tutu (child)\n```\n\n----------------------------------------\n\nTITLE: Defining Multiple Listener Types with ListenersConfig in Nifikop (YAML)\nDESCRIPTION: This YAML snippet demonstrates how to specify different Nifi internal listeners, including standard types like https, cluster, s2s, prometheus, load-balance, and a custom TCP listener by configuring the listenersConfig object. It also integrates sslSecrets for secure communication, referencing a tlsSecretName and indicating that required SSL certificates should be automatically generated. Required dependencies include a running Kubernetes cluster with Nifikop and (optionally) cert-manager for SSL cert automation. The snippet expects valid port numbers, correct type and name values, and a corresponding secret to exist if SSL is enabled. Customize the containerPort and protocol values as needed to match cluster requirements.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/5_references/1_nifi_cluster/6_listeners_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  listenersConfig:\n    internalListeners:\n      - type: \"https\"\n        name: \"https\"\n        containerPort: 8443\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n      - type: \"prometheus\"\n        name: \"prometheus\"\n        containerPort: 9090\n      - type: \"load-balance\"\n        name: \"load-balance\"\n        containerPort: 6342\n      - name: \"my-custom-listener-port\"\n        containerPort: 1234\n        protocol: \"TCP\"\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n\n```\n\n----------------------------------------\n\nTITLE: Listing Deleted Helm Releases Using Bash\nDESCRIPTION: This Helm command lists all releases that have been deleted but still have their record kept by Helm. It helps in audit and debugging by showing the history of releases including those that are no longer active.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --deleted\n```\n\n----------------------------------------\n\nTITLE: Listing Managed Groups using Kubectl\nDESCRIPTION: This snippet demonstrates how to list the managed user groups created by the operator using the `kubectl` command-line tool. It targets `NifiUserGroup` resources within the `nifikop` namespace.  The output displays the names of the groups and their ages. This command is useful for verifying that the groups have been created and are being managed correctly by the operator.  It requires `kubectl` to be configured to connect to a Kubernetes cluster with the NiFiKop operator installed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/2_manage_users_and_accesses/3_managed_groups.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\n```console\nkubectl get -n nifikop nifiusergroups.nifi.konpyutaika.com \nNAME              AGE\nmanaged-admins    6d7h\nmanaged-nodes     6d7h\nmanaged-readers   6d7h\n```\n```\n\n----------------------------------------\n\nTITLE: Extracting OpenShift UID and Installing NiFiKop Operator with runAsUser - OpenShift - Bash\nDESCRIPTION: Demonstrates how to retrieve the OpenShift project supplemental group UID and use it as runAsUser when installing the NiFiKop operator via Helm. This ensures compliance with OpenShift's restricted SCCs. Dependencies include kubectl, Helm 3, and OpenShift environment. The Helm command sets image version, resource requests, and limits, and injects the extracted UID to ensure pods run with correct permissions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/2_deploy_nifikop/1_quick_start.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nuid=$(kubectl get namespace nifi -o=jsonpath='{.metadata.annotations.openshift\\.io/sa\\.scc\\.supplemental-groups}' | sed 's/\\/10000$//' | tr -d '[:space:]')\n```\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop \\\n    nifikop \\\n    --namespace=nifi \\\n    --version 1.1.1 \\\n    --set image.tag=v1.1.1-release \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set namespaces={\"nifi\"} \\\n    --set runAsUser=$uid\n```\n\n----------------------------------------\n\nTITLE: Installing Zookeeper with Helm on OpenShift (Custom UID/GID) - Bash\nDESCRIPTION: Installs Zookeeper via Helm while specifying custom containerSecurityContext and podSecurityContext user/group IDs for OpenShift compliance. Requires OpenShift cluster, Helm, and extracted zookeeper_uid variable. Key parameters include runAsUser and fsGroup, which must match project security constraints.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nhelm install zookeeper bitnami/zookeeper \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set global.storageClass=standard \\\n    --set networkPolicy.enabled=true \\\n    --set replicaCount=3 \\\n    --set containerSecurityContext.runAsUser=$zookeper_uid \\\n    --set podSecurityContext.fsGroup=$zookeper_uid\n\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Parameter Contexts (JSON)\nDESCRIPTION: Defines the structure for configuring `NiFiParameterContext` custom resources. It includes properties for enabling, inheriting contexts, name, and parameters (with description, name, sensitivity, value). This is necessary for deploying versioned dataflows that utilize parameter contexts.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/helm/nifi-cluster/README.md#_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n[{\"enabled\":false,\"inheritedParameterContexts\":[],\"name\":\"default\",\"parameters\":[{\"description\":\"my foo bar property\",\"name\":\"foo-prop\",\"sensitive\":false,\"value\":\"bar-value\"},{\"description\":\"my foo bar property\",\"name\":\"foo-prop-2\",\"sensitive\":true,\"value\":\"bar-value-2\"}]}]\n```\n\n----------------------------------------\n\nTITLE: Removing a Node from NiFiKop Cluster - YAML Configuration\nDESCRIPTION: This YAML snippet illustrates removing a node (with id: 2) from the 'nodes' array in the NifiCluster custom resource to initiate a graceful scaledown. Prerequisites include a deployed NifiCluster managed by NiFiKop and understanding that removal process should follow documented decommission procedures. The primary parameter is the list of nodes; deleting an entry starts the node decommission workflow. Input is a modified YAML, and output is transition of the node to decommissioned state with Kubernetes managing pod and data migration steps.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  headlessServiceEnabled: true\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.11.3\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n# >>>> START: node removed\n#    - id: 2\n#      nodeConfigGroup: \"default_group\"\n# <<<< END\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: Defining Multiple Node Groups in NiFiKop (YAML)\nDESCRIPTION: This YAML snippet defines two distinct node groups (default_group and high_mem_group) within the nodeConfigGroups section of a NiFiKop cluster specification. Each group specifies provenance storage limits, security context (runAsUser), service account, and resource requirements, allowing flexible deployment of NiFi nodes with different resource profiles. Prerequisites include a Kubernetes cluster, NiFiKop installed, and appropriate storage classes. The expected input is the nodeConfigGroups YAML definition within a NifiCluster custom resource, and output is that NiFiKop will deploy pods per these configurations. Note limitations on resource availability within the given Kubernetes environment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  # nodeConfigGroups specifies multiple node configs with unique name\n  nodeConfigGroups:\n    default_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      runAsUser: 1000\n      serviceAccountName: \"default\"\n      # resourceRequirements works exactly like Container resources, the user can specify the limit and the requests\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 3Gi\n    high_mem_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      runAsUser: 1000\n      serviceAccountName: \"default\"\n      # resourceRequirements works exactly like Container resources, the user can specify the limit and the requests\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 30Gi\n        requests:\n          cpu: \"1\"\n          memory: 30Gi\n```\n\n----------------------------------------\n\nTITLE: ParameterContextReference Data Model\nDESCRIPTION: Contains references to other NiFi Parameter Contexts by name and optional namespace. Enables inheritance and modular configuration of parameter contexts.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/5_references/4_nifi_parameter_context.md#_snippet_7\n\nLANGUAGE: YAML\nCODE:\n```\nParameterContextReference:\n  name: string\n  namespace: string\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi UserGroup Resource Instance (YAML)\nDESCRIPTION: This YAML snippet defines a `NifiUserGroup` custom resource instance named `group-test`. It specifies the target NiFi cluster using `clusterRef`, lists the users belonging to the group via `usersRef`, and defines access policies granted to the group, such as read access to global counters.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/5_references/6_nifi_usergroup.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n    - name: nc-controller.nifikop.mgt.cluster.local\n  accessPolicies:\n    - type: global\n      action: read\n      resource: /counters\n```\n\n----------------------------------------\n\nTITLE: Defining Prerequisite NifiDataflows for Connections in YAML\nDESCRIPTION: Defines two prerequisite NifiDataflow Custom Resources (CRs), 'input' and 'output', required for establishing a NiFi connection using NiFiKop. Each CR specifies cluster reference, NiFi Registry details (bucket ID, flow ID, flow version), synchronization settings, update strategy, and initial canvas position within the 'nifikop' namespace. These dataflows must exist before a NifiConnection can be created between them.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/4_manage_connections/1_deploy_connection.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiDataflow\nmetadata:\n  name: input\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  bucketId: deedb9f6-65a4-44e9-a1c9-722008fcd0ba\n  flowId: ab95431d-980d-41bd-904a-fac4bd864ba6\n  flowVersion: 1\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  skipInvalidComponent: true\n  skipInvalidControllerService: true\n  syncMode: always\n  updateStrategy: drain\n  flowPosition:\n    posX: 0\n    posY: 0\n---\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiDataflow\nmetadata:\n  name: output\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  bucketId: deedb9f6-65a4-44e9-a1c9-722008fcd0ba\n  flowId: fc5363eb-801e-432f-aa94-488838674d07\n  flowVersion: 2\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  skipInvalidComponent: true\n  skipInvalidControllerService: true\n  syncMode: always\n  updateStrategy: drain\n  flowPosition:\n    posX: 750\n    posY: 0\n```\n\n----------------------------------------\n\nTITLE: Installing KEDA Helm Chart\nDESCRIPTION: This command installs the KEDA Helm chart into the `keda` namespace. It deploys the necessary Kubernetes resources to enable event-driven autoscaling for your cluster.  It assumes that the `keda` namespace already exists.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl create namespace keda\nhelm install keda kedacore/keda --namespace keda\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi NodeGroup Autoscaler using YAML\nDESCRIPTION: This YAML snippet defines an instance of NifiNodeGroupAutoscaler, setting up the necessary references, labels, and strategies for automatic scaling of NiFi cluster node groups. It includes metadata, cluster reference, node selection labels, and scaling strategies for up and down actions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/5_references/7_nifi_nodegroup_autoscaler.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n`apiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiNodeGroupAutoscaler\nmetadata:\n  name: nifinodegroupautoscaler-sample\nspec:\n  clusterRef:\n    name: nificluster-name\n    namespace: nifikop\n  nodeConfigGroupId: default-node-group\n  nodeLabelsSelector:\n    matchLabels:\n      nifi_cr: nificluster-name\n      nifi_node_group: default-node-group\n  upscaleStrategy: simple\n  downscaleStrategy: lifo`\n```\n\n----------------------------------------\n\nTITLE: Executing Encrypt-Config Tool for NiFi Property Encryption - Shell\nDESCRIPTION: This shell command sequence runs the encrypt-config.sh utility to re-encrypt sensitive values in Apache NiFi's flow configuration files (flow.xml.gz or flow.json.gz) using the new NIFI_PBKDF2_AES_GCM_256 algorithm. Dependencies include the NiFi Toolkit and access to the appropriate nifi.properties and flow files. The parameters -n, -f, -x, -s, and -A define the source properties, target file, key, and encryption algorithm. Outputs are updated, re-encrypted flow files; sensitive properties key must be provided.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/7_upgrade_guides/4_v1.3.1_to_v1.4.0.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nencrypt-config.sh -n nifi.properties -f flow.xml.gz -x -s PROPERTIES_KEY -A NIFI_PBKDF2_AES_GCM_256\nencrypt-config.sh -n nifi.properties -f flow.json.gz -x -s PROPERTIES_KEY -A NIFI_PBKDF2_AES_GCM_256\n```\n\n----------------------------------------\n\nTITLE: Creating an Issuer for Certificate Authority (YAML)\nDESCRIPTION: This YAML defines a `Issuer` resource, configured to use Let's Encrypt for certificate issuance. It specifies the email address for contact, the Let's Encrypt staging server URL, and a reference to a secret for storing the account's private key. It configures a HTTP01 solver for the challenges and uses annotations for external DNS integration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: cert-manager.io/v1alpha2\nkind: Issuer\nmetadata:\n  name: letsencrypt-staging\nspec:\n  acme:\n    # You must replace this email address with your own.\n    # Let's Encrypt will use this to contact you about expiring\n    # certificates, and issues related to your account.\n    email: <your email address>\n    server: https://acme-staging-v02.api.letsencrypt.org/directory\n    privateKeySecretRef:\n      # Secret resource used to store the account's private key.\n      name: example-issuer-account-key\n    # Add a single challenge solver, HTTP01 using nginx\n    solvers:\n      - http01:\n          ingress:\n            ingressTemplate:\n              metadata:\n                annotations:\n                  \"external-dns.alpha.kubernetes.io/ttl\": \"5\"\n```\n\n----------------------------------------\n\nTITLE: Creating a Secret for Basic Authentication\nDESCRIPTION: This command demonstrates how to create a Kubernetes secret containing the username, password, and optional CA certificate required for basic authentication to the NiFi cluster. The secret is then referenced in the `secretRef` field of the NifiCluster resource. This setup is crucial for the operator to securely authenticate with the external NiFi cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/3_external_cluster.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nkubectl create secret generic nifikop-credentials \\\n  --from-file=username=./secrets/username\\\n  --from-file=password=./secrets/password\\\n  --from-file=ca.crt=./secrets/ca.crt\\\n  -n nifikop-nifi\n```\n\n----------------------------------------\n\nTITLE: Automating NiFi Flow Re-encryption with `initContainer` (YAML)\nDESCRIPTION: This YAML snippet defines a Kubernetes `initContainer` to automate the re-encryption of NiFi flow configuration files (`flow.json.gz`, `flow.xml.gz`) during pod startup.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/7_upgrade_guides/4_v1.3.1_to_v1.4.0.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ninitContainers:\n  - image: \"apache/nifi-toolkit:latest\"\n    name: nifi-toolkit\n    imagePullPolicy: Always\n    command:\n      - \"sh\"\n      - \"-c\"\n      - \"NIFI_SENSITIVE_PROPS_KEY=$(grep 'nifi.sensitive.props.key' /opt/nifi/nifi-current/conf/nifi.properties | cut -d'=' -f2) && bin/encrypt-config.sh -n /opt/nifi/nifi-current/conf/nifi.properties -f /opt/nifi/data/flow.json.gz -x -A NIFI_PBKDF2_AES_GCM_256 -s $NIFI_SENSITIVE_PROPS_KEY; bin/encrypt-config.sh -n /opt/nifi/nifi-current/conf/nifi.properties -f /opt/nifi/data/flow.xml.gz -x -A NIFI_PBKDF2_AES_GCM_256 -s $NIFI_SENSITIVE_PROPS_KEY\"\n    volumeMounts:\n      - name: data\n        mountPath: /opt/nifi/data\n      - name: conf\n        mountPath: /opt/nifi/nifi-current/conf\n```\n\n----------------------------------------\n\nTITLE: Defining Internal Listeners in YAML\nDESCRIPTION: This YAML snippet defines internal listeners for a NiFi cluster.  It specifies the type, name, and container port for various listeners like `https`, `cluster`, `s2s`, `prometheus`, and `load-balance`.  These listeners are used for internal communication and exposing the NiFi UI and other functionalities within the Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    - type: \"https\"\n      name: \"https\"\n      containerPort: 8443\n    - type: \"cluster\"\n      name: \"cluster\"\n      containerPort: 6007\n    - type: \"s2s\"\n      name: \"s2s\"\n      containerPort: 10000\n    - type: \"prometheus\"\n      name: \"prometheus\"\n      containerPort: 9090\n    - type: \"load-balance\"\n      name: \"load-balance\"\n      containerPort: 6342\n```\n\n----------------------------------------\n\nTITLE: Verifying Kubernetes HorizontalPodAutoscaler for NiFi Autoscaling\nDESCRIPTION: This console command retrieves the current status of an HPA managing NiFi node scaling, displaying target metrics, current replica count, and age. It depends on the kubectl CLI and requires access to the Kubernetes cluster with the appropriate namespace and HPA resource. The output helps confirm that autoscaling is functioning as configured in the ScaledObject.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_12\n\nLANGUAGE: console\nCODE:\n```\nkubectl get -n clusters hpa\nNAME                REFERENCE                                                TARGETS         MINPODS   MAXPODS   REPLICAS   AGE\nkeda-hpa-cluster    NifiNodeGroupAutoscaler/nifinodegroupautoscaler-sample     1833m/2 (avg)   1         3         2          25d\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for NiFiKop with Make and Environment Variable in Bash\nDESCRIPTION: This sequence sets the target Docker repository and builds a container image for the operator from the current branch using a Makefile. The environment variable DOCKER_REPO_BASE must be defined with the desired Docker registry. Outputs a container image tagged with version and branch details, requiring local Docker access.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/6_contributing/1_developer_guide.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nexport DOCKER_REPO_BASE={your-docker-repo}\nmake docker-build\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom NiFi Authorizers Template (YAML)\nDESCRIPTION: This YAML snippet shows a templated `authorizers.xml` configuration for NiFi, intended to be used with NiFiKOp via `replaceTemplateConfigMap` or `replaceTemplateSecretConfig`. It defines both the standard file-based providers and a custom `DatabaseAuthorizer` example (`my.custom.*` classes), using Go templating (`{{ ... }}`) to dynamically inject node identities and the controller user at deployment time.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/6_users_authorization/1_custom_user_authorizer.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n{{- $nodeList := .NodeList }}\n{{- $clusterName := .ClusterName }}\n{{- $namespace := .Namespace }}<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>\n<authorizers>\n    <userGroupProvider>\n        <identifier>file-user-group-provider</identifier>\n        <class>org.apache.nifi.authorization.FileUserGroupProvider</class>\n        <property name=\"Users File\">../data/users.xml</property>\n        <property name=\"Legacy Authorized Users File\"></property>\n        <property name=\"Initial User Identity admin\">{{ .ControllerUser }}</property>\n{{- range $i, $host := .NodeList }}\n        <property name=\"Initial User Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n    </userGroupProvider>\n    <userGroupProvider>\n        <identifier>database-user-group-provider</identifier>\n        <class>my.custom.DatabaseUserGroupProvider</class>\n        <!-- Any extra configuration for this provider goes here -->\n{{- range $i, $host := .NodeList }}\n        <property name=\"Initial User Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n    </userGroupProvider>\n    <accessPolicyProvider>\n        <identifier>file-access-policy-provider</identifier>\n        <class>org.apache.nifi.authorization.FileAccessPolicyProvider</class>\n        <property name=\"User Group Provider\">file-user-group-provider</property>\n        <property name=\"Authorizations File\">../data/authorizations.xml</property>\n        <property name=\"Initial Admin Identity\">{{ .ControllerUser }}</property>\n        <property name=\"Legacy Authorized Users File\"></property>\n{{- range $i, $host := .NodeList }}\n        <property name=\"Node Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n\t\t<property name=\"Node Group\"></property>\n    </accessPolicyProvider>\n    <accessPolicyProvider>\n        <identifier>database-access-policy-provider</identifier>\n        <class>my.custom.DatabaseAccessPolicyProvider</class>\n        <!-- Any extra configuration for this provider goes here -->\n{{- range $i, $host := .NodeList }}\n        <property name=\"Node Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n\t\t<property name=\"Node Group\"></property>\n    </accessPolicyProvider>\n    <authorizer>\n        <identifier>managed-authorizer</identifier>\n        <class>org.apache.nifi.authorization.StandardManagedAuthorizer</class>\n        <property name=\"Access Policy Provider\">file-access-policy-provider</property>\n    </authorizer>\n    <authorizer>\n        <identifier>custom-database-authorizer</identifier>\n        <class>my.custom.DatabaseAuthorizer</class>\n        <property name=\"Access Policy Provider\">database-access-policy-provider</property>\n    </authorizer>\n</authorizers>\n```\n\n----------------------------------------\n\nTITLE: Defining Node Configuration in YAML\nDESCRIPTION: This YAML snippet defines the configuration for a NiFi node. It sets parameters such as `provenanceStorage`, `runAsUser`, and Kubernetes-related configurations like `isNode` and `podMetadata`. It uses a nested structure to configure storage, including details about volume mounts and persistent volume claims (PVCs).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/5_references/1_nifi_cluster/3_node_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n   default_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      # Set this to true if the instance is a node in a cluster.\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#basic-cluster-setup\n      isNode: true\n      # Additionnal metadata to merge to the pod associated\n      podMetadata:\n        annotations:\n          node-annotation: \"node-annotation-value\"\n        labels:\n          node-label: \"node-label-value\"\n      # Docker image used by the operator to create the node associated\n      # https://hub.docker.com/r/apache/nifi/\n#      image: \"apache/nifi:1.11.2\"\n      # nodeAffinity can be specified, operator populates this value if new pvc added later to node\n      # https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity\n#      nodeAffinity:\n      # imagePullPolicy define the pull policy for NiFi cluster docker image\n      imagePullPolicy: IfNotPresent\n      # priorityClassName define the name of the priority class to be applied to these nodes\n      priorityClassName: \"example-priority-class-name\"\n      # externalVolumeConfigs specifies a list of volume to mount into the main container.\n      externalVolumeConfigs:\n        - name: example-volume\n          mountPath: \"/opt/nifi/example\"\n          secret:\n            secretName: \"raw-controller\"\n      # storageConfigs specifies the node related configs\n      storageConfigs:\n        # Name of the storage config, used to name PV to reuse into sidecars for example.\n        - name: provenance-repository\n          # Retain this PVC throughout NifiCluster deletions.\n          reclaimPolicy: Retain\n          # Path where the volume will be mount into the main nifi container inside the pod.\n          mountPath: \"/opt/nifi/provenance_repository\"\n          # Metadata to attach to the PVC that gets created\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          # Kubernetes PVC spec\n          # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          reclaimPolicy: Delete\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n```\n\n----------------------------------------\n\nTITLE: Defining External NiFi Cluster Configuration - YAML\nDESCRIPTION: This YAML snippet defines a `NifiCluster` resource to configure an external NiFi cluster. It specifies the cluster's root process group ID, node URI template, node IDs, cluster type, client type, and a reference to a secret containing authentication credentials.  Dependencies include the Kubernetes operator and a running NiFi cluster. The output is a configured Kubernetes resource.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/3_external_cluster.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: externalcluster\nspec:\n  # rootProcessGroupId contains the uuid of the root process group for this cluster.\n  rootProcessGroupId: 'd37bee03-017a-1000-cff7-4eaaa82266b7'\n  # nodeURITemplate used to dynamically compute node uri.\n  nodeURITemplate: 'nifi0%d.integ.mapreduce.m0.p.fti.net:9090'\n  # all node requiresunique id\n  nodes:\n    - id: 1\n    - id: 2\n    - id: 3\n  # type defines if the cluster is internal (i.e manager by the operator) or external.\n  # :Enum={\"external\",\"internal\"}\n  type: 'external'\n  # clientType defines if the operator will use basic or tls authentication to query the NiFi cluster.\n  # Enum={\"tls\",\"basic\"}\n  clientType: 'basic'\n  # secretRef reference the secret containing the informations required to authenticate to the cluster.\n  secretRef:\n    name: nifikop-credentials\n    namespace: nifikop-nifi\n```\n```\n\n----------------------------------------\n\nTITLE: Creating an External Issuer with Let's Encrypt for SSL\nDESCRIPTION: This YAML example guides the creation of a cert-manager Issuer resource to obtain SSL certificates via Let's Encrypt's staging environment, suitable for testing. The issuer is referenced in the NiFi cluster's `issuerRef` configuration to automate certificate management.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: cert-manager.io/v1alpha2\nkind: Issuer\nmetadata:\n  name: letsencrypt-staging\nspec:\n  acme:\n    email: <your email address>\n    server: https://acme-staging-v02.api.letsencrypt.org/directory\n    privateKeySecretRef:\n      name: example-issuer-account-key\n    solvers:\n      - http01:\n          ingress:\n            ingressTemplate:\n              metadata:\n                annotations:\n                  \"external-dns.alpha.kubernetes.io/ttl\": \"5\"\n```\n\n----------------------------------------\n\nTITLE: Building NiFiKop Docker Image\nDESCRIPTION: This command builds a Docker image for the NiFiKop operator. It uses the `DOCKER_REPO_BASE` environment variable to determine the repository for the image.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/6_contributing/1_developer_guide.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nexport DOCKER_REPO_BASE={your-docker-repo}\nmake docker-build\n```\n\n----------------------------------------\n\nTITLE: Advanced NiFi Content and Provenance Repository Configuration (YAML)\nDESCRIPTION: This YAML snippet illustrates how to configure multiple directories for the NiFi content and provenance repositories. It shows how to define `nifi.content.repository.directory.default*` and `nifi.provenance.repository.directory.default*` properties within the `readOnlyConfig` section.  It also demonstrates how to configure associated persistent volume claims within the `nodeConfigGroups` section for each directory.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nreadOnlyConfig:\n    nifiProperties:\n      overrideConfigs: |\n        nifi.content.repository.directory.dir1=../content-additional/dir1\n        nifi.content.repository.directory.dir2=../content-additional/dir2\n        nifi.content.repository.directory.dir3=../content-additional/dir3\n        nifi.provenance.repository.directory.dir1=../provenance-additional/dir1\n        nifi.provenance.repository.directory.dir2=../provenance-additional/dir2\nnodeConfigGroups:\n    default_group:\n      storageConfigs:\n      - mountPath: \"/opt/nifi/content-additional/dir1\"\n        name: content-repository-dir1\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/content-additional/dir2\"\n        name: content-repository-dir2\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/content-additional/dir3\"\n        name: content-repository-dir3\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/provenance-additional/dir1\"\n        name: provenance-repository-dir1\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/provenance-additional/dir2\"\n        name: provenance-repository-dir2\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n```\n\n----------------------------------------\n\nTITLE: Listing Deleted Helm Releases - Bash\nDESCRIPTION: Displays a list of Helm releases that have been deleted but records are still retained by Helm. The --deleted flag ensures only deleted releases are shown. Dependencies: Helm CLI and cluster access.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --deleted\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio VirtualService for HTTPS Access to NiFi\nDESCRIPTION: Defines an Istio VirtualService that routes decrypted traffic from the Gateway to the NiFi ClusterIP service running on port 8443. This is part of the HTTPS configuration that manages the internal routing path.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: nifi-vs\nspec:\n  gateways:\n  - nifi-gateway\n  hosts:\n  - nifi.my-domain.com\n  http:\n  - match:\n    - uri:\n        prefix: /\n    route:\n    - destination:\n        host: <service-name>.<namespace>.svc.cluster.local\n        port:\n          number: 8443\n```\n\n----------------------------------------\n\nTITLE: Deleting Helm Release - Helm & Kubernetes (bash)\nDESCRIPTION: This command deletes the Helm release named \"nifikop\". It removes all Kubernetes components associated with the chart and deletes the Helm release. It requires Helm CLI installed and configured to connect to the Kubernetes cluster. The input is the release name, and the output is the removal of the chart's resources from the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nhelm del nifikop\n```\n\n----------------------------------------\n\nTITLE: Install NiFiKop Operator via Helm on OpenShift\nDESCRIPTION: Deploys the NiFiKop Kubernetes operator using the Helm chart specifically for OpenShift environments with a restricted Security Context Constraint (SCC). It's similar to the standard Helm install but includes `--set runAsUser=$uid` to specify the UID obtained from the namespace annotation, complying with SCC requirements. Requires the `uid` variable to be set from the previous command and Helm installed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/2_deploy_nifikop/1_quick_start.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop     nifikop     --namespace=nifi     --version 1.1.1     --set image.tag=v1.1.1-release     --set resources.requests.memory=256Mi     --set resources.requests.cpu=250m     --set resources.limits.memory=256Mi     --set resources.limits.cpu=250m     --set namespaces={\"nifi\"}     --set runAsUser=$uid\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Cluster Configuration - YAML\nDESCRIPTION: This YAML snippet defines the configuration for a NiFi cluster named `simplenifi`. It specifies details such as service annotations and labels, pod annotations and labels, Zookeeper connection information, the NiFi cluster image, node configuration groups with storage and resource requirements, and listener configurations. This configuration is crucial for deploying and managing a NiFi cluster within a Kubernetes environment using an operator.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/5_references/1_nifi_cluster/1_nifi_cluster.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n    annotations:\n      tyty: ytyt\n    labels:\n      tete: titi  \n  pod:\n    annotations:\n      toto: tata\n    labels:\n      titi: tutu\n  zkAddress: 'zookeepercluster-client.zookeeper:2181'\n  zkPath: '/simplenifi'\n  clusterImage: 'apache/nifi:1.11.3'\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      podMetadata:\n        annotations:\n          node-annotation: \"node-annotation-value\"\n        labels:\n          node-label: \"node-label-value\"\n      externalVolumeConfigs:\n        - name: example-volume\n          mountPath: \"/opt/nifi/example\"\n          secret:\n            secretName: \"raw-controller\"\n      storageConfigs:\n        - mountPath: '/opt/nifi/nifi-current/logs'\n          name: logs\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: 'standard'\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: 'default'\n      resourcesRequirements:\n        limits:\n          cpu: '2'\n          memory: 3Gi\n        requests:\n          cpu: '1'\n          memory: 1Gi\n  nodes:\n    - id: 1\n      nodeConfigGroup: 'default_group'\n    - id: 2\n      nodeConfigGroup: 'default_group'\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: 'http'\n        name: 'http'\n        containerPort: 8080\n      - type: 'cluster'\n        name: 'cluster'\n        containerPort: 6007\n      - type: 's2s'\n        name: 's2s'\n        containerPort: 10000\n  externalServices:\n    - name: 'clusterip'\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8080\n            internalListenerName: 'http'\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n\n----------------------------------------\n\nTITLE: Updating nifi.provenance.repository.debug.frequency parameter\nDESCRIPTION: This snippet changes the debug frequency for the provenance repository, adjusting the interval from '1_000_000' to '1000000' to optimize performance or debugging verbosity within NiFi. It modifies configuration parameters relevant to NiFi runtime.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/CHANGELOG.md#_snippet_8\n\nLANGUAGE: YAML\nCODE:\n```\nnifi.provenance.repository.debug.frequency: 1000000\n```\n\n----------------------------------------\n\nTITLE: Cloning NiFiKop Repository - Bash\nDESCRIPTION: This snippet clones the NiFiKop project repository from GitHub and navigates into the project directory. It's a prerequisite for setting up the development environment.  It uses the `git clone` command to retrieve the project and then changes the current directory to the project root.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/6_contributing/1_developer_guide.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/konpyutaika/nifikop.git\ncd nifikop\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Parameter Contexts with nifikop Kubernetes CRD YAML\nDESCRIPTION: This YAML snippet demonstrates how to define NifiParameterContext resources for the nifikop operator, specifying parameter values, descriptions, secret references, and optional context inheritance. Dependencies include an active nifikop operator, a cluster referenced under clusterRef, and pre-created secrets for sensitive parameters. The main input is a valid Kubernetes YAML manifest using apiVersion 'nifi.konpyutaika.com/v1alpha1'; the output is creation or update of NiFi parameter contexts in the cluster with named parameters, inheritance, and secret mapping. All referenced secrets and clusters must exist, and 'inheritedParameterContexts' enables hierarchical parameter management.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/5_references/4_nifi_parameter_context.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      description: toto\n      sensistive: true\n---\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle-child\nspec:\n  description: \"It is a child test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  inheritedParameterContexts:\n    - name: dataflow-lifecycle\n  parameters:\n    - name: test\n      value: toto-child\n      description: tutu (child)\n```\n\n----------------------------------------\n\nTITLE: Extracting Client SSL Certificates from Kubernetes Secret\nDESCRIPTION: Console commands to extract the CA certificate, client certificate, and client private key from a Kubernetes secret. These can be used to configure a client for secure communication with the NiFi cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['ca\\.crt']}\" | base64 -d > ca.crt\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['tls\\.crt']}\" | base64 -d > tls.crt\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['tls\\.key']}\" | base64 -d > tls.key\n```\n\n----------------------------------------\n\nTITLE: Dry-running Helm Install for nifikop - Bash\nDESCRIPTION: Simulates the installation of the nifikop Helm chart using the --dry-run flag. The command shows what resources would be created, sets logLevel to Debug, and specifies namespaces. No Kubernetes resources are actually created. Prerequisites: helm CLI must be installed and repositories added.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop \\\n    --dry-run \\\n    --set logLevel=Debug \\\n    --set namespaces={\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: NiFiCluster CRD YAML Snippet for SSL-enabled NiFi Cluster\nDESCRIPTION: Defines a Kubernetes Custom Resource for a NiFi cluster with SSL enabled. It includes configuration for read-only properties, listener ports, and SSL secret management. The `sslSecrets.create` flag determines whether the operator generates certificates or uses existing ones.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nspec:\n  readOnlyConfig:\n    nifiProperties:\n      webProxyHosts:\n        - nifistandard2.trycatchlearn.fr:8443\n  listenersConfig:\n    internalListeners:\n      - type: \"https\"\n        name: \"https\"\n        containerPort: 8443\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop with CRD Installation Skip Option\nDESCRIPTION: Command to install NiFiKop Helm chart while skipping the CRD installation. This is useful when you don't want to modify existing CRDs in the cluster or when managing CRDs separately.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nhelm install --name nifikop ./helm/nifikop --set namespaces={\"nifikop\"} --skip-crds\n```\n\n----------------------------------------\n\nTITLE: Retrieving Client SSL Credentials from Kubernetes Secret\nDESCRIPTION: Commands to decode and save client CA certificate, user certificate, and private key from the Kubernetes secret. These files can be used for external client configuration or mounted into containers for secure communication.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_6\n\nLANGUAGE: Console Commands\nCODE:\n```\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['ca\\.crt']}\" | base64 -d > ca.crt\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['tls\\.crt']}\" | base64 -d > tls.crt\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['tls\\.key']}\" | base64 -d > tls.key\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFiKop with Helm (Bash)\nDESCRIPTION: This command installs the NiFiKop operator using Helm.  It specifies the chart repository, namespace, version, and other configuration parameters. Requires a configured Kubernetes cluster, Helm installed and configured and the `nifi` namespace created.  The `--set` flags customize the deployment such as image tag and resource requests and limits.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/2_deploy_nifikop/1_quick_start.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# You have to create the namespace before executing following command\nhelm install nifikop \\\n    oci://ghcr.io/konpyutaika/helm-charts/nifikop \\\n    --namespace=nifi \\\n    --version 1.5.0 \\\n    --set image.tag=v1.5.0-release \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set namespaces={\"{nifi}\"}\n```\n\n----------------------------------------\n\nTITLE: Pushing NiFiKop Operator Docker Image To Container Registry Using Makefile (Bash)\nDESCRIPTION: This snippet uploads the locally built NiFiKop operator Docker image to a container registry such as Docker Hub using the 'make docker-push' Makefile target. This step requires prior Docker image build and valid authentication to the registry.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/6_contributing/1_developer_guide.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nmake docker-push\n```\n\n----------------------------------------\n\nTITLE: Defining Istio Gateway for HTTPS Traffic with TLS Credentials\nDESCRIPTION: This YAML snippet configures an Istio Gateway to accept HTTPS traffic on port 443, specifying TLS mode as SIMPLE and referencing a secret containing TLS credentials. It is used to decrypt HTTPS requests at the Gateway before routing.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: nifi-gateway\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 443\n      name: https\n      protocol: HTTPS\n    tls:\n      mode: SIMPLE\n      credentialName: my-secret\n    hosts:\n    - nifi.my-domain.com\n```\n\n----------------------------------------\n\nTITLE: Monitoring Node State During Scale Down (Console)\nDESCRIPTION: Shows how to use `kubectl describe nificluster simplenifi` to inspect the `Status.Nodes State` section of the `NifiCluster` resource. This allows monitoring the `GracefulActionState` (e.g., `GracefulDownscaleRequired`) of the node being removed (node id 2) during the decommissioning process.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_5\n\nLANGUAGE: console\nCODE:\n```\nkubectl describe nificluster simplenifi\n\n...\nStatus:\n  Nodes State:\n    ...\n    2:\n      Configuration State:  ConfigInSync\n      Graceful Action State:\n        Action State:   GracefulDownscaleRequired\n        Error Message:\n    ...\n...\n```\n\n----------------------------------------\n\nTITLE: Defining NifiDataflow Custom Resource\nDESCRIPTION: This YAML snippet defines a NifiDataflow custom resource. It specifies the API version, kind, metadata (name), and the desired state (spec) of the NiFi dataflow, including parent process group ID, bucket ID, flow ID, flow version, flow position, sync mode, skip invalid controller service, skip invalid component, cluster reference, registry client reference, parameter context reference, and update strategy.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/5_references/5_nifi_dataflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  flowPosition:\n    posX: 0\n    posY: 0\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: squidflow\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: nifikop\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Pushing Docker Image - Bash\nDESCRIPTION: This command pushes the Docker image to a specified repository. It leverages the `make docker-push` command, which is likely defined in a Makefile. This step is necessary to make the image available for deployment in a Kubernetes cluster. The tag is derived from the version and branch.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/6_contributing/1_developer_guide.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n$ make docker-push\n```\n\n----------------------------------------\n\nTITLE: Automating NiFi Encryption Update Using InitContainer in Kubernetes - YAML\nDESCRIPTION: This YAML snippet configures a Kubernetes initContainer for automating the upgrade of NiFi sensitive property encryption algorithm within a NiFi cluster deployment. It uses the apache/nifi-toolkit Docker image to run the encrypt-config.sh tool on the cluster's flow.json.gz and flow.xml.gz files inside mounted volumes. The container fetches the sensitive properties key from nifi.properties dynamically and applies the new algorithm NIFI_PBKDF2_AES_GCM_256. Volumes for data and configuration are mounted at appropriate paths. This container should be run prior to operator restart and removed after upgrade.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/7_upgrade_guides/4_v1.3.1_to_v1.4.0.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ninitContainers:\n  - image: \"apache/nifi-toolkit:latest\"\n    name: nifi-toolkit\n    imagePullPolicy: Always\n    command:\n      - \"sh\"\n      - \"-c\"\n      - \"NIFI_SENSITIVE_PROPS_KEY=$(grep 'nifi.sensitive.props.key' /opt/nifi/nifi-current/conf/nifi.properties | cut -d'=' -f2) && bin/encrypt-config.sh -n /opt/nifi/nifi-current/conf/nifi.properties -f /opt/nifi/data/flow.json.gz -x -A NIFI_PBKDF2_AES_GCM_256 -s $NIFI_SENSITIVE_PROPS_KEY; bin/encrypt-config.sh -n /opt/nifi/nifi-current/conf/nifi.properties -f /opt/nifi/data/flow.xml.gz -x -A NIFI_PBKDF2_AES_GCM_256 -s $NIFI_SENSITIVE_PROPS_KEY\"\n    volumeMounts:\n      - name: data\n        mountPath: /opt/nifi/data\n      - name: conf\n        mountPath: /opt/nifi/nifi-current/conf\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFi Cluster\nDESCRIPTION: This snippet uses `kubectl` to create a NiFi cluster from a YAML configuration file. It assumes that the `simplenificluster.yaml` file is located in the `config/samples` directory and that the Zookeeper service name has been added to the configuration.  The NiFi namespace must be created beforehand.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Add your zookeeper svc name to the configuration\nkubectl create -n nifi -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Applying NiFiKop Cluster Changes with kubectl in Shell\nDESCRIPTION: This shell command applies the modified NifiCluster YAML manifest to the Kubernetes cluster using kubectl. The namespace and file path must be specified appropriately. After running this command, NiFiKop will reconcile the state and perform node addition or removal. Input is the path to the updated manifest file; output is the update operation's status in the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nkubectl -n nifi apply -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio DestinationRule for HTTPS (YAML)\nDESCRIPTION: This YAML defines a DestinationRule to encrypt HTTP traffic destined to the `ClusterIP` service. It sets the traffic policy to `tls: SIMPLE` and uses a consistent hash-based load balancer with an `httpCookie` for sticky sessions. This rule is essential for the HTTPS setup and to manage the sticky sessions. It requires a deployed ClusterIP service and Istio.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\napiVersion: networking.istio.io/v1beta1\nkind: DestinationRule\nmetadata:\n  name: nifi-dr\nspec:\n  host: <service-name>.<namespace>.svc.cluster.local\n  trafficPolicy:\n    tls:\n      mode: SIMPLE\n    loadBalancer:\n      consistentHash:\n        httpCookie:\n          name: __Secure-Authorization-Bearer\n          ttl: 0s\n```\n```\n\n----------------------------------------\n\nTITLE: Configuring Managed Users in Nifikop NifiCluster Spec YAML\nDESCRIPTION: Demonstrates how to add users to the pre-defined 'managed-admins' and 'managed-readers' groups by listing their identities and names within the `NifiCluster` resource's `spec`. This approach allows the operator to automatically create and manage the corresponding `NifiUser` resources and user groups with appropriate permissions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/3_tasks/4_nifi_user_group.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  ...\n  oneNifiNodePerNode: false\n  propagateLabels: true\n  managedAdminUsers:\n    -  identity: \"alexandre.guitton@konpyutaika.com\"\n       name: \"aguitton\"\n    -  identity: \"nifiuser@konpyutaika.com\"\n       name: \"nifiuser\"\n  managedReaderUsers:\n    -  identity: \"toto@konpyutaika.com\"\n       name: \"toto\"\n    ...\n```\n\n----------------------------------------\n\nTITLE: Node.js Migration Script (index.js)\nDESCRIPTION: This Node.js script uses the Kubernetes client library to automate the migration of NiFiKop Custom Resources. It connects to the Kubernetes cluster, lists resources from the old API group (`nifi.orange.com`), creates corresponding resources in the new API group (`nifi.konpyutaika.com`) with relevant fields (metadata, spec), and then copies the status from the old resource to the new one. It handles different resource types and takes namespace and type as command-line arguments.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nprocess.env['NODE_TLS_REJECT_UNAUTHORIZED'] = 0;\nconst k8s = require('@kubernetes/client-node');\n\nconst kc = new k8s.KubeConfig();\nkc.loadFromDefault();\n\nconst k8sApi = kc.makeApiClient(k8s.CustomObjectsApi);\n\nconst KONPYUTAIKA_GROUP = 'nifi.konpyutaika.com';\nconst KONPYUTAIKA_GROUP_VERSION = 'v1alpha1';\nconst ORANGE_GROUP = 'nifi.orange.com';\nconst ORANGE_GROUP_VERSION = 'v1alpha1';\n\nconst call = async (SRC_GRP, SRC_GRP_VER, DST_GRP, DST_GRP_VER, KIND_PLURAL, NAMESPACE) => {\n\tconsole.log(`Listing ${KIND_PLURAL} of ${SRC_GRP}/${SRC_GRP_VER} in ${NAMESPACE}...`);\n\tconst listResources = (await k8sApi.listNamespacedCustomObject(SRC_GRP, SRC_GRP_VER, NAMESPACE, KIND_PLURAL)).body.items;\n\treturn Promise.all(listResources.map(async (resource) => {\n\t\ttry {\n\t\t\tconsole.log(`Found ${resource.kind} \"${resource.metadata.name}\" of ${resource.apiVersion} in ${NAMESPACE}`);\n\n\t\t\tif (resource.metadata.ownerReferences) {\n\t\t\t\tconsole.log(`${resource.kind} ${resource.metadata.name} mananged by something else (ownerRefereces is set).`);\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tconst bodyResource = {\n\t\t\t\tapiVersion: `${DST_GRP}/${DST_GRP_VER}`,\n\t\t\t\tkind: resource.kind,\n\t\t\t\tmetadata: {\n\t\t\t\t\tname: resource.metadata.name,\n\t\t\t\t\tannotations: resource.metadata.annotations,\n\t\t\t\t\tlabels: resource.metadata.labels\n\t\t\t\t},\n\t\t\t\tspec: resource.spec\n\t\t\t};\n\n\t\t\tconsole.log(`Creating ${bodyResource.kind} \"${bodyResource.metadata.name}\" of ${bodyResource.apiVersion} in ${NAMESPACE}...`);\n\t\t\tconst newResource = (await k8sApi.createNamespacedCustomObject(DST_GRP, DST_GRP_VER, NAMESPACE, KIND_PLURAL, bodyResource)).body;\n\t\t\tconsole.log('...done creating.');\n\n\t\t\tconst bodyStatus = {\n\t\t\t\tapiVersion: newResource.apiVersion,\n\t\t\t\tkind: newResource.kind,\n\t\t\t\tmetadata: {\n\t\t\t\t\tname: newResource.metadata.name,\n\t\t\t\t\tresourceVersion: newResource.metadata.resourceVersion\n\t\t\t\t},\n\t\t\t\tstatus: resource.status\n\t\t\t};\n\n\t\t\tconsole.log(`Copying status from ${resource.kind} \"${resource.metadata.name}\" of ${newResource.apiVersion} to ${newResource.kind} \"${newResource.metadata.name}\" of ${newResource.apiVersion} in ${NAMESPACE}...`);\n\t\t\tconst newResourceWithStatus = (await k8sApi.replaceNamespacedCustomObjectStatus(DST_GRP, DST_GRP_VER, NAMESPACE, KIND_PLURAL, bodyStatus.metadata.name, bodyStatus)).body;\n\t\t\tconsole.log('...done copying.');\n\t\t\treturn newResourceWithStatus;\n\t\t}\n\t\tcatch (e) {\n\t\t\tconsole.error(e.body ? e.body.message ? e.body.message : e.body : e);\n\t\t}\n\t}));\n};\n\nconst argv = require('minimist')(process.argv.slice(2));\n\nlet NAMESPACE = argv.namespace ? argv.namespace.length > 0 ? argv.namespace : 'default' : 'default';\nlet KIND_PLURAL = {\n\tcluster: 'nificlusters',\n\tdataflow: 'nifidataflows',\n\tparametercontext: 'nifiparametercontexts',\n\tregistryclient: 'nifiregistryclients',\n\tuser: 'nifiusers',\n\tusergroup: 'nifiusergroups',\n};\n\nif (!argv.type) {\n\tconsole.error('Type not provided');\t\n\tprocess.exit(1);\n}\n\nif (!KIND_PLURAL[argv.type]) {\n\tconsole.error(`Type ${argv.type} is not one of the following types: ${Object.keys(KIND_PLURAL)}`);\n\tprocess.exit(1);\n}\n\nconsole.log(`########### START: ${KIND_PLURAL[argv.type]} ###########`);\ncall( ORANGE_GROUP, ORANGE_GROUP_VERSION, KONPYUTAIKA_GROUP, KONPYUTAIKA_GROUP_VERSION, KIND_PLURAL[argv.type], NAMESPACE)\n\t.then(r => console.log('############ END ############'))\n\t.catch(e => console.error(e));\n```\n\n----------------------------------------\n\nTITLE: Defining a NiFi Parameter Context in YAML\nDESCRIPTION: This YAML snippet defines a NiFi Parameter Context named `dataflow-lifecycle` with specified parameters and secret references. It includes a description, cluster reference, and parameters. It demonstrates a basic configuration within a Kubernetes cluster, using the `apiVersion`, `kind`, and `metadata` fields to specify the object's type and attributes. The code includes a child example which inherits parameters.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/5_references/4_nifi_parameter_context.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      description: toto\n      sensistive: true\n--- \napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle-child\nspec:\n  description: \"It is a child test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  inheritedParameterContexts:\n    - name: dataflow-lifecycle\n  parameters:\n    - name: test\n      value: toto-child\n      description: tutu (child)\n```\n```\n\n----------------------------------------\n\nTITLE: Defining NifiRegistryClient CRD in YAML\nDESCRIPTION: This YAML manifest defines a `NifiRegistryClient` custom resource for NiFiKop. It specifies the target NiFi cluster (`clusterRef`) and the URI of the NiFi Registry instance (`uri`) to enable dataflow management via the registry. This resource is a prerequisite for deploying `NifiDataflow` resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiRegistryClient\nmetadata:\n  name: registry-client-example\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Registry client managed by NiFiKop\"\n  uri: \"http://nifi.hostname.com:18080\"\n```\n\n----------------------------------------\n\nTITLE: Defining Managed Users in NifiCluster YAML Configuration\nDESCRIPTION: This YAML snippet demonstrates how to specify admin and reader user lists within the NifiCluster custom resource, enabling the operator to automatically create and manage corresponding NifiUsers and user groups. It requires the NifiCluster CRD and a Kubernetes environment with the operator installed. Key parameters include 'managedAdminUsers' and 'managedReaderUsers', which list user identities and names, affecting user and group resource creation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/3_manage_nifi/2_manage_users_and_accesses/3_managed_groups.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  ...\n  oneNifiNodePerNode: false\n  propagateLabels: true\n  managedAdminUsers:\n    -  identity: \"alexandre.guitton@konpyutaika.com\"\n       name: \"aguitton\"\n    -  identity: \"nifiuser@konpyutaika.com\"\n       name: \"nifiuser\"\n  managedReaderUsers:\n    -  identity: \"toto@konpyutaika.com\"\n       name: \"toto\"\n```\n\n----------------------------------------\n\nTITLE: Describe NiFiCluster with kubectl\nDESCRIPTION: This command describes the NiFiCluster resource named \"simplenifi\". It retrieves the current status of the cluster, including node states, and potential error messages related to graceful actions. This command is crucial to monitor and understand the decommissions steps status.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nkubectl describe nificluster simplenifi\n```\n\n----------------------------------------\n\nTITLE: Installing Zookeeper with Helm (Bash)\nDESCRIPTION: Installs a Zookeeper cluster using the Bitnami Helm chart into the 'zookeeper' namespace. It configures resource requests/limits, specifies a storage class (replace 'standard' with your custom class), enables network policy, and sets the replica count to 3. Requires Helm, the Bitnami repo added, and a Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nhelm install zookeeper bitnami/zookeeper \\\n    --namespace=zookeeper \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set global.storageClass=standard \\\n    --set networkPolicy.enabled=true \\\n    --set replicaCount=3 \\\n    --create-namespace\n```\n\n----------------------------------------\n\nTITLE: Running the Operator in Development Mode with Make\nDESCRIPTION: Command to build and run the NiFiKop operator locally using Make, facilitating development and testing outside the Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/6_contributing/1_developer_guide.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmake run\n```\n\n----------------------------------------\n\nTITLE: Purge NiFiKop Helm Release - Bash\nDESCRIPTION: Permanently deletes a specific Helm release (e.g., `nifikop`) and completely removes its record from Helm's storage using the `helm delete --purge` command. This action allows the release name to be reused for a new installation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/2_setup/3_install/1_customizable_install_with_helm.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nhelm delete --purge nifikop\n```\n\n----------------------------------------\n\nTITLE: Creating a NiFiUser Resource for Client Certificate Generation\nDESCRIPTION: This console command applies a YAML manifest to create a NifiUser resource named `example-client`, linked to the Nifi cluster, and generates associated secrets containing CA and user certificates. The secret can be mounted into pods or exported for use in client authentication.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\ncat << EOF | kubectl apply -n nifi -f -\napiVersion:  nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: example-client\n  namespace: nifi\nspec:\n  clusterRef:\n    name: nifi\n  secretName: example-client-secret\nEOF\n```\n\n----------------------------------------\n\nTITLE: NiFi UserGroup YAML Definition\nDESCRIPTION: This YAML defines a NifiUserGroup custom resource. It specifies the cluster reference, a list of user references that belong to the group, and a set of access policies to be granted to the group.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/5_references/6_nifi_usergroup.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n    - name: nc-controller.nifikop.mgt.cluster.local\n  accessPolicies:\n    - type: global\n      action: read\n      resource: /counters\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi for High Performance with Multiple Storage Directories in YAML\nDESCRIPTION: An advanced configuration example showing how to set up multiple content and provenance repository directories for high-performance NiFi installations. Includes volume mount configurations for each directory.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\n...\n  readOnlyConfig:\n    nifiProperties:\n      overrideConfigs: |\n        nifi.content.repository.directory.dir1=../content-additional/dir1\n        nifi.content.repository.directory.dir2=../content-additional/dir2\n        nifi.content.repository.directory.dir3=../content-additional/dir3\n        nifi.provenance.repository.directory.dir1=../provenance-additional/dir1\n        nifi.provenance.repository.directory.dir2=../provenance-additional/dir2\n...\n  nodeConfigGroups:\n    default_group:\n      ...\n      storageConfigs:\n      - mountPath: \"/opt/nifi/content-additional/dir1\"\n        name: content-repository-dir1\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/content-additional/dir2\"\n        name: content-repository-dir2\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/content-additional/dir3\"\n        name: content-repository-dir3\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/provenance-additional/dir1\"\n        name: provenance-repository-dir1\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/provenance-additional/dir2\"\n        name: provenance-repository-dir2\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      ...\n```\n\n----------------------------------------\n\nTITLE: Creating NiFi User Client Certificates with NifiUser CRD - kubectl console commands\nDESCRIPTION: Shows how to generate client TLS certificates for applications to authenticate with the NiFi cluster using the NifiUser custom resource definition. The YAML manifest defines a NifiUser referencing the cluster and secrets for credential storage. Applying it with kubectl creates a Kubernetes secret with CA, client certificate, and private key. Additional instructions provide kubectl commands to extract the secret data in decoded form for mounting or local use. Optional inclusion of Java KeyStore format is also described. Prerequisites: NiFi operator installed and cluster running with SSL enabled.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_3\n\nLANGUAGE: console\nCODE:\n```\ncat << EOF | kubectl apply -n nifi -f -\napiVersion:  nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: example-client\n  namespace: nifi\nspec:\n  clusterRef:\n    name: nifi\n  secretName: example-client-secret\nEOF\n```\n\n----------------------------------------\n\nTITLE: Defining a NiFi Registry Client in Kubernetes\nDESCRIPTION: Example YAML configuration for creating a NifiRegistryClient resource that connects a NiFi cluster to a registry. It includes references to the target cluster, a description, and the registry URI.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/5_references/3_nifi_registry_client.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiRegistryClient\nmetadata:\n  name: squidflow\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Squidflow demo\"\n  uri: \"http://nifi-registry:18080\"\n```\n\n----------------------------------------\n\nTITLE: Defining External Service Configuration in YAML\nDESCRIPTION: This YAML snippet demonstrates how to define an external service configuration within NiFiKop. It specifies the service name, type (ClusterIP), port configurations, and metadata (annotations and labels) for a Kubernetes Service. The portConfigs section associates a port with an internal listener.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/5_references/1_nifi_cluster/7_external_service_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nexternalServices:\n    - name: \"clusterip\"\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8080\n            internalListenerName: \"http\"\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n\n----------------------------------------\n\nTITLE: Creating a Certificate Issuer with Let's Encrypt for NiFi SSL\nDESCRIPTION: This snippet shows how to define a cert-manager Issuer resource using the ACME protocol with Let's Encrypt staging environment, including email for contact and secret reference for private keys. It demonstrates setup for external DNS validation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/3_tasks/2_security/1_ssl.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: cert-manager.io/v1alpha2\nkind: Issuer\nmetadata:\n  name: letsencrypt-staging\nspec:\n  acme:\n    email: <your email address>\n    server: https://acme-staging-v02.api.letsencrypt.org/directory\n    privateKeySecretRef:\n      name: example-issuer-account-key\n    solvers:\n      - http01:\n          ingress:\n            ingressTemplate:\n              metadata:\n                annotations:\n                  \"external-dns.alpha.kubernetes.io/ttl\": \"5\"\n```\n\n----------------------------------------\n\nTITLE: Creating Let's Encrypt Issuer for SSL\nDESCRIPTION: YAML configuration for setting up a Let's Encrypt issuer for SSL certificates. This example uses the staging server and configures HTTP01 challenge solver with external DNS integration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/3_tasks/2_security/1_ssl.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: cert-manager.io/v1alpha2\nkind: Issuer\nmetadata:\n  name: letsencrypt-staging\nspec:\n  acme:\n    # You must replace this email address with your own.\n    # Let's Encrypt will use this to contact you about expiring\n    # certificates, and issues related to your account.\n    email: <your email address>\n    server: https://acme-staging-v02.api.letsencrypt.org/directory\n    privateKeySecretRef:\n      # Secret resource used to store the account's private key.\n      name: example-issuer-account-key\n    # Add a single challenge solver, HTTP01 using nginx\n    solvers:\n      - http01:\n          ingress:\n            ingressTemplate:\n              metadata:\n                annotations:\n                  \"external-dns.alpha.kubernetes.io/ttl\": \"5\"\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Make\nDESCRIPTION: This bash command leverages the `make docker-build` command to build a Docker image for the NiFiKop operator.  It requires a `DOCKER_REPO_BASE` environment variable to be set. It uses the Dockerfile in the project and assumes the user has Docker installed and configured.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/6_contributing/1_developer_guide.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nexport DOCKER_REPO_BASE={your-docker-repo}\nmake docker-build\n```\n\n----------------------------------------\n\nTITLE: Verifying HorizontalPodAutoscaler for NiFi Node Group Using kubectl Console\nDESCRIPTION: Shows a sample command and output for retrieving HorizontalPodAutoscaler (HPA) resources in the 'clusters' namespace to confirm that the autoscaler named 'keda-hpa-cluster' is managing the target 'NifiNodeGroupAutoscaler' resource. It displays current scaling metrics, minimum and maximum pod counts, and replica status, verifying the KEDA-based autoscaling setup. Requires kubectl access to the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_13\n\nLANGUAGE: console\nCODE:\n```\nkubectl get -n clusters hpa\nNAME                REFERENCE                                                TARGETS         MINPODS   MAXPODS   REPLICAS   AGE\nkeda-hpa-cluster    NifiNodeGroupAutoscaler/nifinodegroupautoscaler-sample     1833m/2 (avg)   1         3         2          25d\n```\n\n----------------------------------------\n\nTITLE: Installing cert-manager Directly\nDESCRIPTION: This command installs cert-manager directly using `kubectl`. It downloads and applies the necessary CustomResourceDefinitions and cert-manager itself from a specified URL. Dependencies include `kubectl` and a Kubernetes cluster.  The output is the installation status from `kubectl apply`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/2_deploy_nifikop/1_quick_start.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install the CustomResourceDefinitions and cert-manager itself\nkubectl apply -f \\\n    https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.yaml\n```\n\n----------------------------------------\n\nTITLE: Viewing Kubernetes Services in Console\nDESCRIPTION: Shows the command line output of `kubectl get services`, listing available Kubernetes services including the NiFi cluster-access service. The snippet provides information about service type, cluster IP, external IP, ports mapping, and service age. This output helps verify that the NiFi cluster is accessible externally through the correctly configured LoadBalancer service.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_3\n\nLANGUAGE: console\nCODE:\n```\nkubectl get services\nNAME                TYPE           CLUSTER-IP    EXTERNAL-IP      PORT(S)                                AGE\ncluster-access      LoadBalancer   10.88.21.98   35.180.241.132   443:30421/TCP,80:30521/TCP             20d\n```\n\n----------------------------------------\n\nTITLE: Defining a NiFi Parameter Context CRD in YAML with Secret References\nDESCRIPTION: This YAML snippet defines a NifiParameterContext custom resource that NiFiKop converts into a NiFi Parameter Context for parameterizing dataflows. It includes references to Kubernetes secrets which are used for sensitive parameters. The resource specifies metadata, cluster references, descriptive details, and a list of named parameters with their values and descriptions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\n  namespace: demo\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      value: toto\n      description: toto\n```\n\n----------------------------------------\n\nTITLE: Patching CRDs for Version Migration (YAML)\nDESCRIPTION: This YAML snippet demonstrates how to patch the CRDs. It configures the conversion webhook to handle version migrations from v1alpha1 to v1. The `annotations` section configures cert-manager to inject the CA from a specified namespace and certificate. The `spec.conversion` section uses a webhook strategy, specifying the service, path, and versions. The `namespace`, `certificate_name`, and `webhook_service_name` are variables that need to be replaced with actual values appropriate to the deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/7_upgrade_guides/3_v0.16.0_to_v1.0.0.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n```\n```\n\n----------------------------------------\n\nTITLE: Granting cluster administrator permissions in GKE\nDESCRIPTION: Grants cluster administrator permissions to the current user by creating a cluster role binding. This allows the user to create the necessary RBAC rules for NiFiKop. The command uses kubectl and gcloud to retrieve the current user's account. This step is essential for NiFiKop to function correctly.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/2_setup/2_platform_setup/1_gke.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nkubectl create clusterrolebinding cluster-admin-binding \\\n    --clusterrole=cluster-admin \\\n    --user=$(gcloud config get-value core/account)\n```\n\n----------------------------------------\n\nTITLE: Installing Nifikop Helm Chart with --skip-crds Option in Bash\nDESCRIPTION: This Bash example shows how to install the Nifikop Helm chart while skipping CRD installation, using the --skip-crds flag. Requires Helm CLI and preinstalled CRDs. Parameters include --name for chart name, --set for namespaces, and --skip-crds. Output: chart installed except for CRDs.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\n$ helm install --name nifikop ./helm/nifikop --set namespaces={\"nifikop\"} --skip-crds\n```\n\n----------------------------------------\n\nTITLE: Applying Nifikop CRDs with Kubectl in Bash\nDESCRIPTION: This Bash series applies all required Nifikop Custom Resource Definitions (CRDs) using kubectl with remote YAML files. Prerequisites: access to a Kubernetes cluster, kubectl installed and configured, and the URLs for the official CRD YAMLs. This operation must occur if the --skip-crds flag is used during Helm installation. Inputs are the provided URLs; output is the CRD resources in the cluster. Limitations: the URLs must remain accessible and accurate.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifinodegroupautoscalers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificonnections.yaml\n```\n\n----------------------------------------\n\nTITLE: Applying Nifikop CRDs Using kubectl - Bash\nDESCRIPTION: This snippet shows the manual application of all necessary Nifikop CustomResourceDefinitions (CRDs) using kubectl before installing the operator with Helm, specifically when the --skip-crds flag is used. Each line downloads and applies a CRD YAML from the Nifikop repository. Dependencies: kubectl CLI, access to a Kubernetes cluster, and internet access to the CRD files. Each CRD is required for the operator to manage its respective resource types within the Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifinodegroupautoscalers.yaml\n```\n\n----------------------------------------\n\nTITLE: Granting Cluster Administrator Permissions (Shell)\nDESCRIPTION: This `kubectl` command grants cluster administrator (admin) permissions to the current user, which is required for creating the necessary RBAC rules for NiFiKop.  It creates a `clusterrolebinding` named `cluster-admin-binding` that binds the `cluster-admin` cluster role to the user specified by `$(gcloud config get-value core/account)`.  This gives the current user full access to the cluster. Prerequisites: `kubectl` configured and connected to the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/2_setup/2_platform_setup/1_gke.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nkubectl create clusterrolebinding cluster-admin-binding \\\n    --clusterrole=cluster-admin \\\n    --user=$(gcloud config get-value core/account)\n```\n\n----------------------------------------\n\nTITLE: Deleting Nifikop Helm Release (Bash)\nDESCRIPTION: This command uses the Helm CLI to uninstall and delete the specified release named 'nifikop'. It removes all Kubernetes resources associated with this release as defined by the chart, except for CRDs which are typically not removed by default. This is the primary step for uninstalling a Helm-managed application.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/2_setup/3_install/1_customizable_install_with_helm.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nhelm del nifikop\n```\n\n----------------------------------------\n\nTITLE: Deploying Prometheus Operator with Helm\nDESCRIPTION: Installs the Prometheus Operator using Helm with specific configurations, disabling unnecessary components to customize the deployment for NiFi monitoring.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_6\n\nLANGUAGE: console\nCODE:\n```\nhelm install prometheus prometheus/kube-prometheus-stack --namespace monitoring-system \\\n    --set prometheusOperator.createCustomResource=false \\\n    --set prometheusOperator.logLevel=debug \\\n    --set prometheusOperator.alertmanagerInstanceNamespaces=monitoring-system \\\n    --set prometheusOperator.namespaces.additional[0]=monitoring-system \\\n    --set prometheusOperator.prometheusInstanceNamespaces=monitoring-system \\\n    --set prometheusOperator.thanosRulerInstanceNamespaces=monitoring-system \\\n    --set defaultRules.enable=false \\\n    --set alertmanager.enabled=false \\\n    --set grafana.enabled=false \\\n    --set kubeApiServer.enabled=false \\\n    --set kubelet.enabled=false \\\n    --set kubeControllerManager.enabled=false \\\n    --set coreDNS.enabled=false \\\n    --set kubeEtcd.enabled=false \\\n    --set kubeScheduler.enabled=false \\\n    --set kubeProxy.enabled=false \\\n    --set kubeStateMetrics.enabled=false \\\n    --set prometheus.enabled=false\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Identity Mapping Properties - Shell\nDESCRIPTION: These properties are recommended additions to the NiFi `nifi.properties` file to enhance support for multiple identity providers by defining a pattern for extracting a common name (CN) from a distinguished name (DN) for identity mapping. They specify the pattern, the replacement value using a backreference to the captured group, and indicate that no further transformation is needed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/5_users_authentication/1_oidc.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nnifi.security.identity.mapping.pattern.dn=CN=([^,]*)(?:, (?:O|OU)=.*)?\nnifi.security.identity.mapping.value.dn=$1\nnifi.security.identity.mapping.transform.dn=NONE\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Authorizer Template in NiFiKOp (YAML/XML)\nDESCRIPTION: Provides a template for NiFi's `authorizers.xml` file, designed for use with NiFiKOp. It defines both default file-based providers and a custom `DatabaseUserGroupProvider`, `DatabaseAccessPolicyProvider`, and `DatabaseAuthorizer`. The template utilizes Go templating syntax (e.g., `{{ .NodeList }}`) to dynamically inject node and controller user identities provided by NiFiKOp during deployment. This template should be stored in a ConfigMap or Secret referenced by `Spec.readOnlyConfig.authorizerConfig` in the NiFiCluster CRD.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/3_tasks/2_security/2_authorization/1_custom_authorizer.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n{{- $nodeList := .NodeList }}\n{{- $clusterName := .ClusterName }}\n{{- $namespace := .Namespace }}<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>\n<authorizers>\n    <userGroupProvider>\n        <identifier>file-user-group-provider</identifier>\n        <class>org.apache.nifi.authorization.FileUserGroupProvider</class>\n        <property name=\"Users File\">../data/users.xml</property>\n        <property name=\"Legacy Authorized Users File\"></property>\n        <property name=\"Initial User Identity admin\">{{ .ControllerUser }}</property>\n{{- range $i, $host := .NodeList }}\n        <property name=\"Initial User Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n    </userGroupProvider>\n    <userGroupProvider>\n        <identifier>database-user-group-provider</identifier>\n        <class>my.custom.DatabaseUserGroupProvider</class>\n        <!-- Any extra configuration for this provider goes here -->\n{{- range $i, $host := .NodeList }}\n        <property name=\"Initial User Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n    </userGroupProvider>\n    <accessPolicyProvider>\n        <identifier>file-access-policy-provider</identifier>\n        <class>org.apache.nifi.authorization.FileAccessPolicyProvider</class>\n        <property name=\"User Group Provider\">file-user-group-provider</property>\n        <property name=\"Authorizations File\">../data/authorizations.xml</property>\n        <property name=\"Initial Admin Identity\">{{ .ControllerUser }}</property>\n        <property name=\"Legacy Authorized Users File\"></property>\n{{- range $i, $host := .NodeList }}\n        <property name=\"Node Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n\t\t<property name=\"Node Group\"></property>\n    </accessPolicyProvider>\n    <accessPolicyProvider>\n        <identifier>database-access-policy-provider</identifier>\n        <class>my.custom.DatabaseAccessPolicyProvider</class>\n        <!-- Any extra configuration for this provider goes here -->\n{{- range $i, $host := .NodeList }}\n        <property name=\"Node Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n\t\t<property name=\"Node Group\"></property>\n    </accessPolicyProvider>\n    <authorizer>\n        <identifier>managed-authorizer</identifier>\n        <class>org.apache.nifi.authorization.StandardManagedAuthorizer</class>\n        <property name=\"Access Policy Provider\">file-access-policy-provider</property>\n    </authorizer>\n    <authorizer>\n        <identifier>custom-database-authorizer</identifier>\n        <class>my.custom.DatabaseAuthorizer</class>\n        <property name=\"Access Policy Provider\">database-access-policy-provider</property>\n    </authorizer>\n</authorizers>\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop Operator Helm Chart in Kubernetes Namespace in Bash\nDESCRIPTION: This Helm command installs the NiFiKop operator chart named 'skeleton', specifying the image tag and target Kubernetes namespace. It requires the previously pushed Docker image to have repository and tag names matching the Helm chart values. The installation schedules the operator pods on the specified namespace with the provided image version.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/6_contributing/1_developer_guide.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nhelm install skeleton ./helm/nifikop \\\n    --set image.tag=v0.5.1-release \\\n    --namespace-{\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Installing Nifikop Helm Chart with Custom Release Name in Bash\nDESCRIPTION: This Bash command installs the Nifikop Helm chart with a user-defined Helm release name. Required dependencies include Helm and access to the konpyutaika/nifikop chart repository. Arguments: <release name> (user-chosen), chart identifier. Inputs: none, outputs: chart resources created under selected release. Limitation: release name must be unique unless using --replace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nhelm install <release name> konpyutaika/nifikop\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFi Cluster on OpenShift\nDESCRIPTION: Command to deploy a NiFi cluster on OpenShift using the configured OpenShift-specific sample. Creates the NiFi resources with the proper security context in the specified namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create -n nifi -f config/samples/openshift.yaml\n```\n\n----------------------------------------\n\nTITLE: OpenShift UID extraction and NiFiKop Helm install with RunAsUser setting in Bash\nDESCRIPTION: This sequence extracts the UID range annotation from the OpenShift namespace `nifi` to comply with restricted Security Context Constraints (SCC). It stores the numeric UID for safe pod execution. Then it installs the NiFiKop Helm chart with the `runAsUser` parameter set to this UID, along with resource requests, limits, image tag, and targeted namespace. This deployment approach respects OpenShift's security restrictions requiring explicit user ID and SELinux context.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/2_deploy_nifikop/1_quick_start.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nuid=$(kubectl get namespace nifi -o=jsonpath='{.metadata.annotations.openshift\\.io/sa\\.scc\\.supplemental-groups}' | sed 's/\\/10000$//' | tr -d '[:space:]')\n```\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop \\\n    nifikop \\\n    --namespace=nifi \\\n    --version 1.1.1 \\\n    --set image.tag=v1.1.1-release \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set namespaces={\"nifi\"} \\\n    --set runAsUser=$uid\n```\n\n----------------------------------------\n\nTITLE: Deploying a Simple NiFi Cluster\nDESCRIPTION: Command to deploy a NiFi cluster using a predefined configuration. Creates NiFi resources in the specified namespace using the sample configuration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# Add your zookeeper svc name to the configuration\nkubectl create -n nifi -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Helm Install with Values File\nDESCRIPTION: This command demonstrates how to install the NiFiKop Helm chart using a values file. The values file allows for customization of various parameters during the installation process.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/helm/nifikop/README.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n$ helm install nifikop \\\n    oci://ghcr.io/konpyutaika/helm-charts/nifikop \\\n    -f values.yaml\n```\n\n----------------------------------------\n\nTITLE: Deploying Prometheus Resource\nDESCRIPTION: This YAML configures a Prometheus instance within the `monitoring-system` namespace.  It sets the evaluation interval and scrape interval.  It configures a `podMonitorSelector` to discover pods and scrape metrics. It sets the `serviceAccountName`.  This deploys a Prometheus instance to scrape NiFi metrics, which can then be used by KEDA for autoscaling decisions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: Prometheus\nmetadata:\n  name: prometheus\n  namespace: monitoring-system\nspec:\n  enableAdminAPI: false\n  evaluationInterval: 30s\n  logLevel: debug\n  podMonitorSelector:\n    matchExpressions:\n    - key: release\n      operator: In\n      values:\n      - prometheus\n  resources:\n    requests:\n      memory: 400Mi\n  scrapeInterval: 30s\n  serviceAccountName: prometheus\n  serviceMonitorSelector:\n    matchExpressions:\n    - key: app\n      operator: In\n      values:\n      - nifi-cluster\n```\n\n----------------------------------------\n\nTITLE: Defining ClusterIP External Service - YAML\nDESCRIPTION: This YAML snippet defines an external service of type ClusterIP, exposing ports 8080 and 7182 with corresponding internal listener names. It also includes metadata annotations and labels to be merged with the associated service.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/5_references/1_nifi_cluster/7_external_service_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nexternalServices:\n    - name: \"clusterip\"\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8080\n            internalListenerName: \"http\"\n          - port: 7182\n            internalListenerName: \"my-custom-listener\"\n            protocol: TCP\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Parameter Context - YAML Manifest\nDESCRIPTION: This YAML snippet defines a `NifiParameterContext` resource. It specifies the API version, kind, metadata (name), and the `spec` which contains parameters, descriptions, and references to other resources. The example showcases how to set parameters and references.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/5_references/4_nifi_parameter_context.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      description: toto\n      sensitive: true\n```\n\n----------------------------------------\n\nTITLE: Installing cert-manager via YAML Manifest\nDESCRIPTION: This script deploys cert-manager by applying the official YAML manifest, installing necessary CustomResourceDefinitions and cert-manager components directly into the cluster. Suitable for direct deployment without Helm.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/2_setup/1_getting_started.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Install the CustomResourceDefinitions and cert-manager itself\nkubectl apply -f \\\n    https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.yaml\n```\n\n----------------------------------------\n\nTITLE: Install Nifikop Chart - Dry Run\nDESCRIPTION: This command installs the Nifikop Helm chart in a dry-run mode, which simulates the installation without actually deploying the resources. It is used for testing configurations. It sets the `logLevel` to `Debug` and specifies the `namespaces` using `--set`.  Dependencies: Helm, Kubernetes cluster, and the Nifikop chart repository.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop \\\n    --dry-run \\\n    --set logLevel=Debug \\\n    --set namespaces={\\\"nifikop\\\"}\n```\n\n----------------------------------------\n\nTITLE: Installing KEDA Helm Chart in Kubernetes Namespace - console\nDESCRIPTION: Commands to create the 'keda' namespace in Kubernetes and install the KEDA Helm chart into that namespace. This sets up KEDA components needed for event-driven autoscaling functionality.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nkubectl create namespace keda\nhelm install keda kedacore/keda --namespace keda\n```\n\n----------------------------------------\n\nTITLE: Cert-Manager Issuer Creation\nDESCRIPTION: This YAML snippet shows how to create a cert-manager Issuer for Let's Encrypt. It configures the ACME server, email address, and private key secret. It also defines a challenge solver using HTTP01 with Nginx ingress, including an annotation for external-dns.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: cert-manager.io/v1alpha2\nkind: Issuer\nmetadata:\n  name: letsencrypt-staging\nspec:\n  acme:\n    # You must replace this email address with your own.\n    # Let's Encrypt will use this to contact you about expiring\n    # certificates, and issues related to your account.\n    email: <your email address>\n    server: https://acme-staging-v02.api.letsencrypt.org/directory\n    privateKeySecretRef:\n      # Secret resource used to store the account's private key.\n      name: example-issuer-account-key\n    # Add a single challenge solver, HTTP01 using nginx\n    solvers:\n      - http01:\n          ingress:\n            ingressTemplate:\n              metadata:\n                annotations:\n                  \"external-dns.alpha.kubernetes.io/ttl\": \"5\"\n```\n\n----------------------------------------\n\nTITLE: Configuring ServiceMonitor for NiFi Cluster Scraping - YAML\nDESCRIPTION: Defines a ServiceMonitor resource instructing Prometheus to scrape metrics from NiFi cluster services, with specific pod relabeling rules and scrape intervals. Endpoint definitions and label selectors are customized for NiFi. Requires Prometheus Operator CRDs and a running NiFi cluster with metrics endpoints.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: cluster\n  namespace: monitoring-system\n  labels:\n    app: nifi-cluster\n    nifi_cr: cluster\nspec:\n  selector:\n    matchLabels:\n      app: nifi\n      nifi_cr: cluster\n  namespaceSelector:\n    matchNames:\n      - clusters\n  endpoints:\n    - interval: 10s\n      port: prometheus\n      path: /metrics\n      honorLabels: true\n      relabelings:\n        - sourceLabels: [__meta_kubernetes_pod_ip]\n          separator: ;\n          regex: (.*)\n          targetLabel: pod_ip\n          replacement: $1\n          action: replace\n        - sourceLabels: [__meta_kubernetes_pod_label_nodeId]\n          separator: ;\n          regex: (.*)\n          targetLabel: nodeId\n          replacement: $1\n          action: replace\n        - sourceLabels: [__meta_kubernetes_pod_label_nifi_cr]\n          separator: ;\n          regex: (.*)\n          targetLabel: nifi_cr\n          replacement: $1\n          action: replace\n\n```\n\n----------------------------------------\n\nTITLE: Defining a NiFi Registry Client in YAML\nDESCRIPTION: This YAML snippet defines a `NifiRegistryClient` resource. It specifies the API version, kind, metadata (name), and the specification, including the cluster reference, description, and URI of the NiFi registry. The `clusterRef` specifies the name and namespace of the associated NiFi cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/5_references/3_nifi_registry_client.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiRegistryClient\nmetadata:\n  name: squidflow\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Squidflow demo\"\n  uri: \"http://nifi-registry:18080\"\n```\n\n----------------------------------------\n\nTITLE: Re-encrypting NiFi Flows using encrypt-config.sh (Shell)\nDESCRIPTION: Demonstrates manual re-encryption of NiFi flow configuration files (.xml.gz and .json.gz) using the NiFi Encrypt-Config Tool. This is necessary during the upgrade if the default sensitive properties algorithm was used in v1.3.1. It requires the NiFi Toolkit and the sensitive properties key.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/7_upgrade_guides/4_v1.3.1_to_v1.4.0.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nencrypt-config.sh -n nifi.properties -f flow.xml.gz -x -s PROPERTIES_KEY -A NIFI_PBKDF2_AES_GCM_256\nencrypt-config.sh -n nifi.properties -f flow.json.gz -x -s PROPERTIES_KEY -A NIFI_PBKDF2_AES_GCM_256\n```\n\n----------------------------------------\n\nTITLE: Automating NiFi Configuration Re-encryption using Kubernetes initContainer (YAML)\nDESCRIPTION: Defines a Kubernetes `initContainer` using the `apache/nifi-toolkit` image to automatically re-encrypt NiFi flow configurations (`flow.json.gz`, `flow.xml.gz`) with the `NIFI_PBKDF2_AES_GCM_256` algorithm during pod initialization. It extracts the sensitive properties key from `nifi.properties` and uses `volumeMounts` to access the configuration and data directories. The `volumeMounts` and `mountPath` need to be adapted based on the specific deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/7_upgrade_guides/4_v1.3.1_to_v1.4.0.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ninitContainers:\n  - image: \"apache/nifi-toolkit:latest\"\n    name: nifi-toolkit\n    imagePullPolicy: Always\n    command:\n      - \"sh\"\n      - \"-c\"\n      - \"NIFI_SENSITIVE_PROPS_KEY=$(grep 'nifi.sensitive.props.key' /opt/nifi/nifi-current/conf/nifi.properties | cut -d'=' -f2) && bin/encrypt-config.sh -n /opt/nifi/nifi-current/conf/nifi.properties -f /opt/nifi/data/flow.json.gz -x -A NIFI_PBKDF2_AES_GCM_256 -s $NIFI_SENSITIVE_PROPS_KEY; bin/encrypt-config.sh -n /opt/nifi/nifi-current/conf/nifi.properties -f /opt/nifi/data/flow.xml.gz -x -A NIFI_PBKDF2_AES_GCM_256 -s $NIFI_SENSITIVE_PROPS_KEY\"\n    volumeMounts:\n      - name: data\n        mountPath: /opt/nifi/data\n      - name: conf\n        mountPath: /opt/nifi/nifi-current/conf\n```\n\n----------------------------------------\n\nTITLE: Using NiFi Encrypt-Config Tool for Algorithm Change\nDESCRIPTION: Shell commands to update the flow configuration using the NiFi Encrypt-Config Tool to change the sensitive algorithm from the default to NIFI_PBKDF2_AES_GCM_256.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/7_upgrade_guides/4_v1.3.1_to_v1.4.0.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nencrypt-config.sh -n nifi.properties -f flow.xml.gz -x -s PROPERTIES_KEY -A NIFI_PBKDF2_AES_GCM_256\nencrypt-config.sh -n nifi.properties -f flow.json.gz -x -s PROPERTIES_KEY -A NIFI_PBKDF2_AES_GCM_256\n```\n\n----------------------------------------\n\nTITLE: Adding Node Configuration (YAML)\nDESCRIPTION: This YAML snippet demonstrates how to add a new node to a NiFi cluster configuration. It involves modifying the `NifiCluster.Spec.Nodes` list by including a new node definition with a unique `id` and associated `nodeConfigGroup`. Prerequisites include a running Kubernetes cluster with the NiFiKop operator and a deployed NiFi cluster. The output is the configuration of the NiFi cluster with the addition of a new node, ready for application via kubectl.  The main parameter is `id`, it must be unique.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.12.1\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n    - id: 2\n      nodeConfigGroup: \"default_group\"\n# >>>> START: The new node\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n# <<<< END\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n```\n\n----------------------------------------\n\nTITLE: Configuring ClusterIP External Service\nDESCRIPTION: This YAML snippet demonstrates the configuration for an external service of type ClusterIP.  It defines a service named \"clusterip\" with a specific port configuration. It sets the type to ClusterIP, specifies the port 8080 for the HTTP listener and 7182 with TCP protocol for custom listener. It includes annotations and labels as well.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/5_references/1_nifi_cluster/7_external_service_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  externalServices:\n    - name: \"clusterip\"\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8080\n            internalListenerName: \"http\"\n          - port: 7182\n            internalListenerName: \"my-custom-listener\"\n            protocol: TCP\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n\n----------------------------------------\n\nTITLE: Verifying NiFiKop Operator Pod Status - Console\nDESCRIPTION: Runs a kubectl command to retrieve the list and readiness status of pods deploying the NiFiKop operator within a given namespace. Expected to be run after deployment for troubleshooting and verification. Output is the tabular listing of pods, their statuses, and restart counts.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/6_contributing/1_developer_guide.md#_snippet_9\n\nLANGUAGE: console\nCODE:\n```\nkubectl get pods -n nifikop\nNAME                                                READY   STATUS    RESTARTS   AGE\nskeleton-nifikop-8946b89dc-4cfs9   1/1     Running   0          7m45s\n```\n\n----------------------------------------\n\nTITLE: Configuring VirtualService to Route Requests to NiFi Service\nDESCRIPTION: This YAML snippet creates an Istio VirtualService that associates the previously defined Gateway and routes all incoming requests with a URI prefix '/' to the NiFi service on port 8080, based on hostname matching.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: nifi-vs\nspec:\n  gateways:\n  - nifi-gateway\n  hosts:\n  - nifi.my-domain.com\n  http:\n  - match:\n    - uri:\n        prefix: /\n    route:\n    - destination:\n        host: nifi\n        port:\n          number: 8080\n```\n\n----------------------------------------\n\nTITLE: Importing Documentation Tabs UI Components in JavaScript\nDESCRIPTION: Imports the Tabs and TabItem components from the '@theme' library to create tabbed navigation within the documentation UI. These components facilitate the presentation of different installation methods interactively and rely on the documentation framework providing the '@theme/Tabs' and '@theme/TabItem' modules.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/2_deploy_nifikop/1_quick_start.md#_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n```\n\n----------------------------------------\n\nTITLE: Listing Managed NifiUserGroups using kubectl\nDESCRIPTION: This shell command example demonstrates how to list all managed NifiUserGroups created by the operator. The command returns three managed groups: managed-admins, managed-nodes, and managed-readers.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/2_manage_users_and_accesses/3_managed_groups.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nkubectl get -n nifikop nifiusergroups.nifi.konpyutaika.com\n```\n\n----------------------------------------\n\nTITLE: Configuring SSL Listeners for NiFi Cluster - Kubernetes YAML\nDESCRIPTION: Demonstrates how to define the NiFiCluster resource to enable SSL-secured communication using internal HTTPS, cluster, and Site-to-Site listeners. The configuration includes a list of web proxy hosts for secure header handling and specifies that SSL certificates should either be auto-generated or sourced from a Kubernetes secret named \"test-nifikop\". Requires NiFi Operator (nifikop) installed in the Kubernetes cluster, and references to the appropriate secret fields if not auto-creating the secret. Users must provide or ensure the existence of secrets with keys: caCert, caKey, clientCert, and clientKey.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\n...\nspec:\n  ...\n  readOnlyConfig:\n    # NifiProperties configuration that will be applied to the node.\n    nifiProperties:\n      webProxyHosts:\n        - nifistandard2.trycatchlearn.fr:8443\n        ...\n  ...\n  listenersConfig:\n    internalListeners:\n      - type: \"https\"\n        name: \"https\"\n        containerPort: 8443\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Cluster Kubernetes Custom Resource YAML\nDESCRIPTION: This YAML snippet defines a NifiCluster custom resource for Kubernetes, describing the desired state and configuration parameters for an Apache NiFi cluster managed by the NiFi operator. It includes fields to configure cluster-wide services, pod metadata annotations and labels, Zookeeper connection details, container images, storage configuration with PersistentVolumeClaims, custom node groups with pod metadata and resources, internal listeners with container ports, and external service definitions. Dependencies include a Kubernetes cluster with the NiFi operator installed and available Custom Resource Definitions (CRDs) for NifiCluster. The inputs are Kubernetes resource metadata and specification properties; the output is the creation or updating of NiFi cluster resources in the cluster. It is constrained by valid Kubernetes resource conventions and the operator's capabilities.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/5_references/1_nifi_cluster/1_nifi_cluster.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n    annotations:\n      tyty: ytyt\n    labels:\n      tete: titi  \n  pod:\n    annotations:\n      toto: tata\n    labels:\n      titi: tutu\n  zkAddress: 'zookeepercluster-client.zookeeper:2181'\n  zkPath: '/simplenifi'\n  clusterImage: 'apache/nifi:1.11.3'\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      podMetadata:\n        annotations:\n          node-annotation: \"node-annotation-value\"\n        labels:\n          node-label: \"node-label-value\"\n      externalVolumeConfigs:\n        - name: example-volume\n          mountPath: \"/opt/nifi/example\"\n          secret:\n            secretName: \"raw-controller\"\n      storageConfigs:\n        - mountPath: '/opt/nifi/nifi-current/logs'\n          name: logs\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: 'standard'\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: 'default'\n      resourcesRequirements:\n        limits:\n          cpu: '2'\n          memory: 3Gi\n        requests:\n          cpu: '1'\n          memory: 1Gi\n  nodes:\n    - id: 1\n      nodeConfigGroup: 'default_group'\n    - id: 2\n      nodeConfigGroup: 'default_group'\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: 'http'\n        name: 'http'\n        containerPort: 8080\n      - type: 'cluster'\n        name: 'cluster'\n        containerPort: 6007\n      - type: 's2s'\n        name: 's2s'\n        containerPort: 10000\n  externalServices:\n    - name: 'clusterip'\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8080\n            internalListenerName: 'http'\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n\n----------------------------------------\n\nTITLE: Checking NiFi Node Resources After Scale Up - Console\nDESCRIPTION: Runs a kubectl get command to list pods, configmaps, and PVCs labeled with the new nodeId (25). This command helps operators verify that the resources for the new NiFi node have been provisioned as intended by the cluster specification. Prerequisites: kubectl CLI access, NiFiKop operator active, correct namespace. Input: nodeId label selector. Output: Table showing status of pods, configmaps, and persistent volume claims for the node. Limitations: Output is for inspection/verification only.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl get pods,configmap,pvc -l nodeId=25\nNAME                          READY   STATUS    RESTARTS   AGE\npod/simplenifi-25-nodem5jh4   1/1     Running   0          11m\n\nNAME                             DATA   AGE\nconfigmap/simplenifi-config-25   7      11m\n\nNAME                                               STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\npersistentvolumeclaim/simplenifi-25-storagehwn24   Bound    pvc-7da86076-728e-11ea-846d-42010a8400f2   10Gi       RWO            standard       11m\n```\n\n----------------------------------------\n\nTITLE: Pod Ports Definition for NiFi Kubernetes Deployment - YAML\nDESCRIPTION: This YAML segment illustrates how the defined listeners translate to pod-level port configurations in the resulting Kubernetes deployment or pod specification. Each entry describes a port, its corresponding name, and specifies the TCP protocol required by NiFi. No external dependencies are needed; this is an outcome of proper internal listener setup. Key parameters are containerPort (as set previously), name (referencing the internal listener), and protocol (always TCP). Applied correctly, the pods will accept traffic on each of the specified named ports.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n    ports:\n    - containerPort: 8443\n      name: https\n      protocol: TCP\n    - containerPort: 6007\n      name: cluster\n      protocol: TCP\n    - containerPort: 6342\n      name: load-balance\n      protocol: TCP\n    - containerPort: 10000\n      name: s2s\n      protocol: TCP\n    - containerPort: 9090\n      name: prometheus\n      protocol: TCP\n```\n\n----------------------------------------\n\nTITLE: Uninstalling a Helm Release (Nifikop)\nDESCRIPTION: Shows the `helm del` (alias for `helm uninstall`) command followed by the release name (`nifikop`) to remove all Kubernetes resources associated with that Helm chart release. Note that CRDs might not be removed by default.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nhelm del nifikop\n```\n\n----------------------------------------\n\nTITLE: Defining a Basic NifiParameterContext in YAML\nDESCRIPTION: This YAML manifest defines a `NifiParameterContext` Kubernetes custom resource named `dataflow-lifecycle`. It links to a NiFi cluster (`nc` in the `nifikop` namespace), specifies a description, references a Kubernetes secret named `secret-params` for sensitive parameters, and explicitly defines parameters `test` (with value 'toto') and `test2` (marked as sensitive, value likely sourced from the secret).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/5_references/4_nifi_parameter_context.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      description: toto\n      sensistive: true\n```\n\n----------------------------------------\n\nTITLE: Defining a NiFiConnection Custom Resource in Kubernetes - YAML\nDESCRIPTION: This YAML snippet demonstrates how to define a NiFiConnection custom resource for use with the konpyutaika NiFi operator in Kubernetes. It specifies metadata such as name and namespace, as well as the complete specification of the connection including source and destination components, connection configuration (such as backpressure thresholds, flow expiration, load balancing strategy, prioritizers, and connection bends), and update strategy. Required dependencies include a running Kubernetes cluster with the konpyutaika NiFi operator installed. The most important parameters are under the spec section (source, destination, configuration), each of which determine how data flows between NiFi components. Inputs are the CRD fields; output is creation of the connection in the NiFi cluster. Completion of relevant source and destination components is required for a valid configuration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/5_references/8_nifi_connection.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiConnection\nmetadata:\n  name: connection\n  namespace: instances\nspec:\n  source:\n    name: input\n    namespace: instances\n    subName: output_1\n    type: dataflow\n  destination:\n    name: output\n    namespace: instances\n    subName: input_1\n    type: dataflow\n  configuration:\n    flowFileExpiration: 1 hour\n    backPressureDataSizeThreshold: 100 GB\n    backPressureObjectThreshold: 10000\n    loadBalanceStrategy: PARTITION_BY_ATTRIBUTE\n    loadBalancePartitionAttribute: partition_attribute\n    loadBalanceCompression: DO_NOT_COMPRESS\n    prioritizers: \n      - NewestFlowFileFirstPrioritizer\n      - FirstInFirstOutPrioritizer\n    labelIndex: 0\n    bends:\n      - posX: 550\n        posY: 550\n      - posX: 550\n        posY: 440\n      - posX: 550\n        posY: 88\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Verifying Node Resources in Kubernetes\nDESCRIPTION: A Kubernetes command to check the pods, configmaps, and persistent volume claims associated with a specific node (nodeId=25).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl get pods,configmap,pvc -l nodeId=25\nNAME                          READY   STATUS    RESTARTS   AGE\npod/simplenifi-25-nodem5jh4   1/1     Running   0          11m\n\nNAME                             DATA   AGE\nconfigmap/simplenifi-config-25   7      11m\n\nNAME                                               STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\npersistentvolumeclaim/simplenifi-25-storagehwn24   Bound    pvc-7da86076-728e-11ea-846d-42010a8400f2   10Gi       RWO            standard       11m\n```\n\n----------------------------------------\n\nTITLE: Pushing Docker Image to Registry - Bash\nDESCRIPTION: This snippet uses Makefile to push the previously built NiFiKop Docker image to a remote registry such as Docker Hub. Docker must be authenticated to the destination registry, and the image details should match the one built locally.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/6_contributing/1_developer_guide.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nmake docker-push\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFi Cluster with kubectl in Bash\nDESCRIPTION: Deploys a simple NiFi cluster onto the 'nifi' namespace by applying a sample NiFi custom resource configuration via kubectl. This command assumes the user has already created or configured the NiFi cluster YAML manifest and that the Zookeeper service name is correctly specified within it. Requires kubectl installed and access to the Kubernetes cluster with necessary RBAC permissions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create -n nifi -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Listing Kubernetes Namespaces Including cert-manager, nifi, and zookeeper Using kubectl Console Command\nDESCRIPTION: This kubectl command lists all namespaces present in the Kubernetes cluster with their current status and age. The presence of cert-manager, nifi, and zookeeper namespaces indicates successful deployment of critical components required for securing and managing the NiFi cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/blog/2020-05-20-secured_nifi_cluster_on_gcp.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nkubectl get namespaces\nNAME              STATUS   AGE\ncert-manager      Active   106m\ndefault           Active   116m\nkube-node-lease   Active   116m\nkube-public       Active   116m\nkube-system       Active   116m\nnifi              Active   106m\nzookeeper         Active   106m\n```\n\n----------------------------------------\n\nTITLE: Cert-Manager Issuer Configuration (YAML)\nDESCRIPTION: This YAML snippet shows how to create a cert-manager Issuer for Let's Encrypt. It defines the ACME server, email address, and private key secret. It also configures a solver for HTTP01 challenges, using an ingress template to manage DNS records.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: cert-manager.io/v1alpha2\nkind: Issuer\nmetadata:\n  name: letsencrypt-staging\nspec:\n  acme:\n    # You must replace this email address with your own.\n    # Let's Encrypt will use this to contact you about expiring\n    # certificates, and issues related to your account.\n    email: <your email address>\n    server: https://acme-staging-v02.api.letsencrypt.org/directory\n    privateKeySecretRef:\n      # Secret resource used to store the account's private key.\n      name: example-issuer-account-key\n    # Add a single challenge solver, HTTP01 using nginx\n    solvers:\n      - http01:\n          ingress:\n            ingressTemplate:\n              metadata:\n                annotations:\n                  \"external-dns.alpha.kubernetes.io/ttl\": \"5\"\n```\n\n----------------------------------------\n\nTITLE: Building and Pushing NiFiKop Docker Image\nDESCRIPTION: Commands to build a Docker image for NiFiKop from the local branch and push it to a Docker registry.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/6_contributing/1_developer_guide.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nexport DOCKER_REGISTRY_BASE={your-docker-repo}\nmake docker-build\nmake docker-push\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Secret for Basic Auth NiFi Access (kubectl command, console)\nDESCRIPTION: This kubectl command creates a Kubernetes secret named 'nifikop-credentials' for use by the NifiKop operator to connect to an external NiFi cluster via basic authentication. The command expects local files for username, password, and an optional CA certificate, mapping each into the secret. Prerequisites: Access to kubectl, required credential files, and proper namespace permissions. Inputs: ./secrets/username, ./secrets/password, and case-optional ./secrets/ca.crt files. Output: Kubernetes Secret resource ready for operator reference. Limitations: Secret file paths and filenames must be correct, and namespace must exist.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/1_manage_clusters/3_external_cluster.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nkubectl create secret generic nifikop-credentials \\\n  --from-file=username=./secrets/username\\\n  --from-file=password=./secrets/password\\\n  --from-file=ca.crt=./secrets/ca.crt\\\n  -n nifikop-nifi\n\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFiKop CRDs Manually Using kubectl - Kubernetes - Bash\nDESCRIPTION: Manually applies all necessary NiFiKop CustomResourceDefinition YAML files using kubectl for environments where CRD deployment via Helm is skipped. Requires kubectl, a running Kubernetes cluster, and direct internet access to fetch YAML files. No special parameters are required beyond cluster authentication; after running these commands, NiFiKop CRDs will be present for custom resource management.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/2_deploy_nifikop/1_quick_start.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifinodegroupautoscalers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificonnections.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring Managed Groups in NifiCluster Definition\nDESCRIPTION: This YAML snippet demonstrates how to configure managed admin and reader users directly in the NifiCluster specification. The operator automatically creates NifiUsers for each identity and assigns them to appropriate managed groups.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/2_manage_users_and_accesses/3_managed_groups.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  ...\n  oneNifiNodePerNode: false\n  propagateLabels: true\n  managedAdminUsers:\n    -  identity: \"alexandre.guitton@konpyutaika.com\"\n       name: \"aguitton\"\n    -  identity: \"nifiuser@konpyutaika.com\"\n       name: \"nifiuser\"\n  managedReaderUsers:\n    -  identity: \"toto@konpyutaika.com\"\n       name: \"toto\"\n    ...\n```\n\n----------------------------------------\n\nTITLE: NiFiParameterContext Configuration YAML\nDESCRIPTION: Defines YAML schemas for NiFi Parameter Context and its child, including metadata, specifications, parameters, secret references, and inheritance. Provides a template for creating or updating NiFi parameter contexts with detailed configuration options.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/5_references/4_nifi_parameter_context.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n```yaml\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      description: toto\n      sensistive: true\n---\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle-child\nspec:\n  description: \"It is a child test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  inheritedParameterContexts:\n    - name: dataflow-lifecycle\n  parameters:\n    - name: test\n      value: toto-child\n      description: tutu (child)```\n```\n\n----------------------------------------\n\nTITLE: Manually Deleting NiFiKop CRDs\nDESCRIPTION: Commands to manually remove NiFiKop Custom Resource Definitions after uninstalling the operator. Note that this will delete all associated resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete crd nificlusters.nifi.konpyutaika.com\nkubectl delete crd nifiusers.nifi.konpyutaika.com\nkubectl delete crd nifiusergroups.nifi.konpyutaika.com\nkubectl delete crd nifiregistryclients.nifi.konpyutaika.com\nkubectl delete crd nifiparametercontexts.nifi.konpyutaika.com\nkubectl delete crd nifidataflows.nifi.konpyutaika.com\n```\n\n----------------------------------------\n\nTITLE: Uninstalling the nifikop Helm Chart - Bash\nDESCRIPTION: Deletes the nifikop Helm release and removes associated Kubernetes resources. Note that CRDs created by the chart are left intact and must be manually removed if desired. Requires helm CLI.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nhelm del nifikop\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom StorageClass in Kubernetes for NiFi\nDESCRIPTION: Defines a Kubernetes StorageClass with WaitForFirstConsumer binding mode, which is recommended for NiFi deployments. This ensures that persistent volumes are only bound when pods requesting them are scheduled to nodes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: exampleStorageclass\nparameters:\n  type: pd-standard\nprovisioner: kubernetes.io/gce-pd\nreclaimPolicy: Delete\nvolumeBindingMode: WaitForFirstConsumer\n```\n\n----------------------------------------\n\nTITLE: Defining ReadOnlyConfig in YAML\nDESCRIPTION: This YAML snippet defines the structure and configuration options for the ReadOnlyConfig object in Nifikop. It shows how to set maximum thread counts, configure logging, authorizers, NiFi properties, Zookeeper properties, and bootstrap properties. The configurations specified here act as cluster-wide read-only settings that can be overridden on individual nodes. It utilizes ConfigMaps and Secrets to inject configurations into NiFi instances.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/5_references/1_nifi_cluster/2_read_only_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreadOnlyConfig:\n  # MaximumTimerDrivenThreadCount define the maximum number of threads for timer driven processors available to the system.\n  maximumTimerDrivenThreadCount: 30\n  # MaximumEventDrivenThreadCount define the maximum number of threads for event driven processors available to the system.\n  maximumEventDrivenThreadCount: 10\n  # Logback configuration that will be applied to the node\n  logbackConfig:\n    # logback.xml configuration that will replace the one produced based on template\n    replaceConfigMap:\n      # The key of the value,in data content, that we want use.\n      data: logback.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # logback.xml configuration that will replace the one produced based on template and overrideConfigMap\n    replaceSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: logback.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n  # Authorizer configuration that will be applied to the node\n  authorizerConfig:\n    # An authorizers.xml configuration template that will replace the default template seen in authorizers.go\n    replaceTemplateConfigMap:\n      # The key of the value, in data content, that we want use.\n      data: authorizers.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # An authorizers.xml configuration template that will replace the default template seen in authorizers.go and the replaceTemplateConfigMap\n    replaceTemplateSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: authorizers.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n  # NifiProperties configuration that will be applied to the node.\n  nifiProperties:\n    # Additionnal nifi.properties configuration that will override the one produced based on template and\n    # configuration\n    overrideConfigMap:\n      # The key of the value,in data content, that we want use.\n      data: nifi.properties\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop.\n    # Additionnal nifi.properties configuration that will override the one produced based\n    #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    overrideSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: nifi.properties\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # Additionnal nifi.properties configuration that will override the one produced based\n    #\ton template, configurations and overrideConfigMap\n    overrideConfigs: |\n      nifi.ui.banner.text=NiFiKop\n    # A comma separated list of allowed HTTP Host header values to consider when NiFi\n    # is running securely and will be receiving requests to a different host[:port] than it is bound to.\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#web-properties\n    #      webProxyHosts:\n    # Nifi security client auth\n    needClientAuth: false\n    # Indicates which of the configured authorizers in the authorizers.xml file to use\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#authorizer-configuration\n  #      authorizer:\n  # ZookeeperProperties configuration that will be applied to the node.\n  zookeeperProperties:\n    #      # Additionnal zookeeeper.properties configuration that will override the one produced based on template and\n    #      # configuration\n    #      overrideConfigMap:\n    #        # The key of the value,in data content, that we want use.\n    #        data: zookeeeper.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop.\n    #      # Additionnal zookeeeper.properties configuration that will override the one produced based\n    #      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    #      overrideSecretConfig:\n    #        # The key of the value,in data content, that we want use.\n    #        data: zookeeeper.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop\n    # Additionnal zookeeper.properties configuration that will override the one produced based\n    # on template and configurations.\n    overrideConfigs: |\n      initLimit=15\n      autopurge.purgeInterval=24\n      syncLimit=5\n      tickTime=2000\n      dataDir=./state/zookeeper\n      autopurge.snapRetainCount=30\n  # BootstrapProperties configuration that will be applied to the node.\n  bootstrapProperties:\n    #      # Additionnal bootstrap.properties configuration that will override the one produced based on template and\n    #      # configuration\n    #      overrideConfigMap:\n    #        # The key of the value,in data content, that we want use.\n    #        data: bootstrap.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop.\n    #      # Additionnal bootstrap.properties configuration that will override the one produced based\n    #      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    #      overrideSecretConfig:\n    #        # The key of the value,in data content, that we want use.\n    #        data: bootstrap.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop\n    # JVM memory settings\n    nifiJvmMemory: \"512m\"\n    # Additionnal bootstrap.properties configuration that will override the one produced based\n    # on template and configurations.\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#bootstrap_properties\n    overrideConfigs: |\n      # java.arg.4=-Djava.net.preferIPv4Stack=true\n```\n\n----------------------------------------\n\nTITLE: Configure NiFi Authorizers with Custom Provider Template (XML + Go Template)\nDESCRIPTION: This snippet provides a templated XML configuration for NiFi's `authorizers.xml` file, intended for use with NiFiKOp. It defines standard file-based authorization components alongside custom database-based components, utilizing Go templating variables (`.NodeList`, `.ControllerUser`) to dynamically include NiFi cluster details. This template is used to replace the default configuration via a Kubernetes ConfigMap or Secret.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/6_users_authorization/1_custom_user_authorizer.md#_snippet_0\n\nLANGUAGE: XML + Go Template\nCODE:\n```\n{{- $nodeList := .NodeList }}\n{{- $clusterName := .ClusterName }}\n{{- $namespace := .Namespace }}<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>\n<authorizers>\n    <userGroupProvider>\n        <identifier>file-user-group-provider</identifier>\n        <class>org.apache.nifi.authorization.FileUserGroupProvider</class>\n        <property name=\"Users File\">../data/users.xml</property>\n        <property name=\"Legacy Authorized Users File\"></property>\n        <property name=\"Initial User Identity admin\">{{ .ControllerUser }}</property>\n{{- range $i, $host := .NodeList }}\n        <property name=\"Initial User Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n    </userGroupProvider>\n    <userGroupProvider>\n        <identifier>database-user-group-provider</identifier>\n        <class>my.custom.DatabaseUserGroupProvider</class>\n        <!-- Any extra configuration for this provider goes here -->\n{{- range $i, $host := .NodeList }}\n        <property name=\"Initial User Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n    </userGroupProvider>\n    <accessPolicyProvider>\n        <identifier>file-access-policy-provider</identifier>\n        <class>org.apache.nifi.authorization.FileAccessPolicyProvider</class>\n        <property name=\"User Group Provider\">file-user-group-provider</property>\n        <property name=\"Authorizations File\">../data/authorizations.xml</property>\n        <property name=\"Initial Admin Identity\">{{ .ControllerUser }}</property>\n        <property name=\"Legacy Authorized Users File\"></property>\n{{- range $i, $host := .NodeList }}\n        <property name=\"Node Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n\t\t<property name=\"Node Group\"></property>\n    </accessPolicyProvider>\n    <accessPolicyProvider>\n        <identifier>database-access-policy-provider</identifier>\n        <class>my.custom.DatabaseAccessPolicyProvider</class>\n        <!-- Any extra configuration for this provider goes here -->\n{{- range $i, $host := .NodeList }}\n        <property name=\"Node Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n\t\t<property name=\"Node Group\"></property>\n    </accessPolicyProvider>\n    <authorizer>\n        <identifier>managed-authorizer</identifier>\n        <class>org.apache.nifi.authorization.StandardManagedAuthorizer</class>\n        <property name=\"Access Policy Provider\">file-access-policy-provider</property>\n    </authorizer>\n    <authorizer>\n        <identifier>custom-database-authorizer</identifier>\n        <class>my.custom.DatabaseAuthorizer</class>\n        <property name=\"Access Policy Provider\">database-access-policy-provider</property>\n    </authorizer>\n</authorizers>\n```\n\n----------------------------------------\n\nTITLE: Pushing NiFiKop Operator Docker Image to Repository - Bash\nDESCRIPTION: This command pushes the locally built Docker image of the NiFiKop operator to the configured remote image repository, such as Docker Hub. It assumes 'make docker-push' is setup to use the correct repository and image tags.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/6_contributing/1_developer_guide.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n$ make docker-push\n```\n\n----------------------------------------\n\nTITLE: Defining Internal Listeners for NiFi Pods in Kubernetes (YAML)\nDESCRIPTION: This YAML snippet demonstrates how to configure a range of internal listeners for an Apache NiFi cluster inside Kubernetes. Each listener is defined with a type, name, and containerPort, supporting HTTPS, cluster communication, Site-to-Site (s2s), Prometheus integration, and load balancing. Required dependency includes the NiFi operator and access to the listenersConfig.internalListeners field in the NiFi CRD. The expected input is a YAML configuration section applied as part of the NiFi custom resource. Proper configuration of these listeners is necessary for internal cluster operations, security, and monitoring. Limitations: Ensure port numbers do not conflict and each type is only specified once, except for custom endpoints.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    - type: \"https\"\n      name: \"https\"\n      containerPort: 8443\n    - type: \"cluster\"\n      name: \"cluster\"\n      containerPort: 6007\n    - type: \"s2s\"\n      name: \"s2s\"\n      containerPort: 10000\n    - type: \"prometheus\"\n      name: \"prometheus\"\n      containerPort: 9090\n    - type: \"load-balance\"\n      name: \"load-balance\"\n      containerPort: 6342\n```\n\n----------------------------------------\n\nTITLE: Referencing NiFi Parameter Context (JSON)\nDESCRIPTION: Shows the structure for referencing a NiFi Parameter Context custom resource associated with a dataflow configuration. It requires specifying the `name` and `namespace` of the target `NiFiParameterContext` object.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/helm/nifi-cluster/README.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\"name\":\"default\",\"namespace\":\"nifi\"}\n```\n\n----------------------------------------\n\nTITLE: Recalculating NiFi Sensitive Properties Encryption Using Encrypt-Config Tool - Shell\nDESCRIPTION: This shell snippet demonstrates how to use the NiFi Encrypt-Config Tool to update sensitive property encryption in flow.xml.gz and flow.json.gz files by converting encryption to NIFI_PBKDF2_AES_GCM_256 algorithm. Dependencies include having nifi.properties, flow.xml.gz, and flow.json.gz files available, and providing the PROPERTIES_KEY for decrypting and re-encrypting sensitive values. Input files are the existing flow configuration files, and output is the upgraded flow files with updated encryption. This procedure is necessary for users upgrading from NiFi v1.3.1 to v1.4.0 when not overriding the algorithm property.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/7_upgrade_guides/4_v1.3.1_to_v1.4.0.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nencrypt-config.sh -n nifi.properties -f flow.xml.gz -x -s PROPERTIES_KEY -A NIFI_PBKDF2_AES_GCM_256\nencrypt-config.sh -n nifi.properties -f flow.json.gz -x -s PROPERTIES_KEY -A NIFI_PBKDF2_AES_GCM_256\n```\n\n----------------------------------------\n\nTITLE: Configure Multiple NiFi Repositories with Overrides and Storage YAML\nDESCRIPTION: This YAML snippet provides an advanced example for configuring multiple NiFi content and provenance repository directories. It shows how to set the necessary `nifi.properties` via the `readOnlyConfig.nifiProperties.overrideConfigs` field and define the corresponding Persistent Volume Claims (PVCs) and mount paths within the `nodeConfigGroups.storageConfigs` section to provision the required storage.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n...\n  readOnlyConfig:\n    nifiProperties:\n      overrideConfigs: |\n        nifi.content.repository.directory.dir1=../content-additional/dir1\n        nifi.content.repository.directory.dir2=../content-additional/dir2\n        nifi.content.repository.directory.dir3=../content-additional/dir3\n        nifi.provenance.repository.directory.dir1=../provenance-additional/dir1\n        nifi.provenance.repository.directory.dir2=../provenance-additional/dir2\n...\n  nodeConfigGroups:\n    default_group:\n      ...\n      storageConfigs:\n      - mountPath: \"/opt/nifi/content-additional/dir1\"\n        name: content-repository-dir1\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/content-additional/dir2\"\n        name: content-repository-dir2\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/content-additional/dir3\"\n        name: content-repository-dir3\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/provenance-additional/dir1\"\n        name: provenance-repository-dir1\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/provenance-additional/dir2\"\n        name: provenance-repository-dir2\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      ...\n```\n\n----------------------------------------\n\nTITLE: NifiUser Resource Definition\nDESCRIPTION: This YAML defines a NifiUser resource named 'aguitton'. It specifies the user's identity, the associated NiFi cluster, whether to include a Java keystore, whether to create a certificate for the user, and defines access policies for the user to have read access to the /data resource on process-groups component type with an empty component ID.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/3_tasks/4_nifi_user_group.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiUser\nmetadata:\n  name: aguitton\nspec:\n  # identity field is use to define the user identity on NiFi cluster side,\n  #\t it use full when the user's name doesn't suite with Kubernetes resource name.\n  identity: alexandre.guitton@konpyutaika.com\n  # Contains the reference to the NifiCluster with the one the registry client is linked.\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  # Whether or not the the operator also include a Java keystore format (JKS) with you secret\n  includeJKS: false\n  # Whether or not a certificate will be created for this user.\n  createCert: false\n  # defines the list of access policies that will be granted to the group.\n  accessPolicies:\n      # defines the kind of access policy, could be \"global\" or \"component\".\n    - type: component\n      # defines the kind of action that will be granted, could be \"read\" or \"write\"\n      action: read\n      # resource defines the kind of resource targeted by this access policies, please refer to the following page:\n      #\thttps://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#access-policies\n      resource: /data\n      # componentType is used if the type is \"component\", it's allow to define the kind of component on which is the\n      # access policy\n      componentType: \"process-groups\"\n      # componentId is used if the type is \"component\", it's allow to define the id of the component on which is the\n      # access policy\n      componentId: \"\"\n```\n\n----------------------------------------\n\nTITLE: Defining NiFiKop Read-Only Configurations (YAML)\nDESCRIPTION: Example YAML configuration for the `readOnlyConfig` object in NiFiKop. Demonstrates setting maximum thread counts, configuring Logback and authorizers via external ConfigMaps/Secrets, overriding `nifi.properties`, `zookeeper.properties`, and `bootstrap.conf` using ConfigMaps, Secrets, or inline values, and setting JVM memory (`nifiJvmMemory`). These configurations apply cluster-wide but can be overridden by node-specific settings.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/5_references/1_nifi_cluster/2_read_only_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreadOnlyConfig:\n  # MaximumTimerDrivenThreadCount define the maximum number of threads for timer driven processors available to the system.\n  maximumTimerDrivenThreadCount: 30\n  # MaximumEventDrivenThreadCount define the maximum number of threads for event driven processors available to the system.\n  maximumEventDrivenThreadCount: 10\n  # Logback configuration that will be applied to the node\n  logbackConfig:\n    # logback.xml configuration that will replace the one produced based on template\n    replaceConfigMap:\n      # The key of the value,in data content, that we want use.\n      data: logback.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # logback.xml configuration that will replace the one produced based on template and overrideConfigMap\n    replaceSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: logback.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n  # Authorizer configuration that will be applied to the node\n  authorizerConfig:\n    # An authorizers.xml configuration template that will replace the default template seen in authorizers.go\n    replaceTemplateConfigMap:\n      # The key of the value, in data content, that we want use.\n      data: authorizers.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # An authorizers.xml configuration template that will replace the default template seen in authorizers.go and the replaceTemplateConfigMap\n    replaceTemplateSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: authorizers.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n  # NifiProperties configuration that will be applied to the node.\n  nifiProperties:\n    # Additionnal nifi.properties configuration that will override the one produced based on template and\n    # configuration\n    overrideConfigMap:\n      # The key of the value,in data content, that we want use.\n      data: nifi.properties\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop.\n    # Additionnal nifi.properties configuration that will override the one produced based\n    #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    overrideSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: nifi.properties\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # Additionnal nifi.properties configuration that will override the one produced based\n    #\ton template, configurations and overrideConfigMap\n    overrideConfigs: |\n      nifi.ui.banner.text=NiFiKop\n    # A comma separated list of allowed HTTP Host header values to consider when NiFi\n    # is running securely and will be receiving requests to a different host[:port] than it is bound to.\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#web-properties\n    #      webProxyHosts:\n    # Nifi security client auth\n    needClientAuth: false\n    # Indicates which of the configured authorizers in the authorizers.xml file to use\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#authorizer-configuration\n  #      authorizer:\n  # ZookeeperProperties configuration that will be applied to the node.\n  zookeeperProperties:\n    #      # Additionnal zookeeeper.properties configuration that will override the one produced based on template and\n    #      # configuration\n    #      overrideConfigMap:\n    #        # The key of the value,in data content, that we want use.\n    #        data: zookeeeper.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop.\n    #      # Additionnal zookeeeper.properties configuration that will override the one produced based\n    #      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    #      overrideSecretConfig:\n    #        # The key of the value,in data content, that we want use.\n    #        data: zookeeeper.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop\n    # Additionnal zookeeper.properties configuration that will override the one produced based\n    # on template and configurations.\n    overrideConfigs: |\n      initLimit=15\n      autopurge.purgeInterval=24\n      syncLimit=5\n      tickTime=2000\n      dataDir=./state/zookeeper\n      autopurge.snapRetainCount=30\n  # BootstrapProperties configuration that will be applied to the node.\n  bootstrapProperties:\n    #      # Additionnal bootstrap.conf configuration that will override the one produced based on template and\n    #      # configuration\n    #      overrideConfigMap:\n    #        # The key of the value,in data content, that we want use.\n    #        data: bootstrap.conf\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop.\n    #      # Additionnal bootstrap.conf configuration that will override the one produced based\n    #      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    #      overrideSecretConfig:\n    #        # The key of the value,in data content, that we want use.\n    #        data: bootstrap.conf\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop\n    # JVM memory settings\n    nifiJvmMemory: \"512m\"\n    # Additionnal bootstrap.conf configuration that will override the one produced based\n    # on template and configurations.\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#bootstrap_properties\n    overrideConfigs: |\n      # java.arg.4=-Djava.net.preferIPv4Stack=true\n```\n\n----------------------------------------\n\nTITLE: Purging a Deleted Helm Release (Helm 2 Syntax)\nDESCRIPTION: Demonstrates using `helm delete --purge <release_name>` (primarily Helm 2 syntax) to permanently remove a deleted Helm release record. In Helm 3, `helm uninstall <release_name>` handles the deletion, and this explicit purge step for deleted releases is generally not needed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nhelm delete --purge nifikop\n```\n\n----------------------------------------\n\nTITLE: Installing cert-manager with Helm 3 - Kubernetes - Bash\nDESCRIPTION: Applies cert-manager CRDs, adds the Jetstack Helm repository, updates it, and installs cert-manager using Helm 3. Dependencies include Helm 3, kubectl, and a created Kubernetes namespace. Key parameters are the target namespace and cert-manager version (v1.7.2). This approach is recommended for environments using Helm for lifecycle management.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/2_deploy_nifikop/1_quick_start.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Install CustomResourceDefinitions first\nkubectl apply --validate=false -f \\\n   https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.crds.yaml\n\n# Add the jetstack helm repo\nhelm repo add jetstack https://charts.jetstack.io\nhelm repo update\n\n# You have to create the namespace before executing following command\nhelm install cert-manager \\\n    --namespace cert-manager \\\n    --version v1.7.2 jetstack/cert-manager\n```\n\n----------------------------------------\n\nTITLE: Defining Node Configuration Groups in YAML\nDESCRIPTION: This YAML snippet defines two node configuration groups (`default_group` and `high_mem_group`) within NiFiKop.  Each group specifies resource requirements like CPU, memory limits, provenance storage, and service account name. It defines resource requests and limits. The `runAsUser` parameter specifies the user ID for the NiFi image. Dependencies: NiFiKop, Kubernetes.  Input: YAML.  Output:  Configuration of node groups.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  # nodeConfigGroups specifies multiple node configs with unique name\n  nodeConfigGroups:\n    default_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      serviceAccountName: \"default\"\n      # resourceRequirements works exactly like Container resources, the user can specify the limit and the requests\n      # through this property\n      # https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 3Gi\n    high_mem_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      serviceAccountName: \"default\"\n      # resourceRequirements works exactly like Container resources, the user can specify the limit and the requests\n      # through this property\n      # https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 30Gi\n        requests:\n          cpu: \"1\"\n          memory: 30Gi\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Registry Client Resource - YAML\nDESCRIPTION: This YAML snippet defines a `NifiRegistryClient` custom resource.  It specifies the API version, kind, metadata (name), and spec. The `spec` section contains configurations such as the `clusterRef`, `description`, and `uri` for connecting to a NiFi registry instance. It's a declarative configuration used to instantiate a NiFi Registry client.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/5_references/3_nifi_registry_client.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiRegistryClient\nmetadata:\n  name: squidflow\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Squidflow demo\"\n  uri: \"http://nifi-registry:18080\"\n```\n```\n\n----------------------------------------\n\nTITLE: Getting OpenShift namespace UID for NiFiKop deployment\nDESCRIPTION: Extracts the UID range from the OpenShift namespace annotation for the nifi namespace, which is required to set the runAsUser parameter when deploying NiFiKop on OpenShift with its restricted security context constraints.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/2_deploy_nifikop/1_quick_start.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nuid=$(kubectl get namespace nifi -o=jsonpath='{.metadata.annotations.openshift\\.io/sa\\.scc\\.supplemental-groups}' | sed 's/\\/10000$//' | tr -d '[:space:]')\n```\n\n----------------------------------------\n\nTITLE: Configuring CRD Conversion Webhook in YAML\nDESCRIPTION: Shows the YAML structure needed within a CRD definition to configure the conversion webhook, used for handling resource version conversions (e.g., v1alpha1 to v1). It includes annotations for `cert-manager` CA injection and the `spec.conversion` block specifying the webhook strategy and client configuration. Placeholders `${namespace}`, `${certificate_name}`, and `${webhook_service_name}` must be replaced with actual values.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n```\n\n----------------------------------------\n\nTITLE: Example NiFi Node Configuration (NodeConfig) in YAML\nDESCRIPTION: This YAML snippet demonstrates a sample `NodeConfig` block used within a Nifikop custom resource. It showcases configuration options like provenance storage size, user ID (`runAsUser`), cluster node designation (`isNode`), custom pod annotations and labels (`podMetadata`), image pull policy, priority class, external volume mounting (`externalVolumeConfigs`), and persistent volume claim configurations for storage (`storageConfigs`) like provenance and logs.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/5_references/1_nifi_cluster/3_node_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n   default_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      # Set this to true if the instance is a node in a cluster.\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#basic-cluster-setup\n      isNode: true\n      # Additionnal metadata to merge to the pod associated\n      podMetadata:\n        annotations:\n          node-annotation: \"node-annotation-value\"\n        labels:\n          node-label: \"node-label-value\"\n      # Docker image used by the operator to create the node associated\n      # https://hub.docker.com/r/apache/nifi/\n#      image: \"apache/nifi:1.11.2\"\n      # nodeAffinity can be specified, operator populates this value if new pvc added later to node\n      # https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity\n#      nodeAffinity:\n      # imagePullPolicy define the pull policy for NiFi cluster docker image\n      imagePullPolicy: IfNotPresent\n      # priorityClassName define the name of the priority class to be applied to these nodes\n      priorityClassName: \"example-priority-class-name\"\n      # externalVolumeConfigs specifies a list of volume to mount into the main container.\n      externalVolumeConfigs:\n        - name: example-volume\n          mountPath: \"/opt/nifi/example\"\n          secret:\n            secretName: \"raw-controller\"\n      # storageConfigs specifies the node related configs\n      storageConfigs:\n        # Name of the storage config, used to name PV to reuse into sidecars for example.\n        - name: provenance-repository\n          # Path where the volume will be mount into the main nifi container inside the pod.\n          mountPath: \"/opt/nifi/provenance_repository\"\n          # Kubernetes PVC spec\n          # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n```\n\n----------------------------------------\n\nTITLE: Defining a NiFi Registry Client CRD in YAML\nDESCRIPTION: This YAML snippet defines a NifiRegistryClient custom resource used by NiFiKop to connect and manage NiFi registries within a Kubernetes cluster. It specifies the API version, cluster reference, and the URI endpoint of the NiFi registry server. This resource is a prerequisite to deploy NiFi dataflows managed by NiFiKop.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiRegistryClient\nmetadata:\n  name: registry-client-example\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Registry client managed by NiFiKop\"\n  uri: \"http://nifi.hostname.com:18080\"\n```\n\n----------------------------------------\n\nTITLE: Upgrading NiFi Sensitive Property Algorithm with Encrypt-Config Tool - Shell\nDESCRIPTION: Uses the NiFi Encrypt-Config Tool (encrypt-config.sh) to update sensitive property values from the old algorithm to NIFI_PBKDF2_AES_GCM_256. Requires access to the relevant nifi.properties, flow.xml.gz or flow.json.gz files, and knowledge of the sensitive properties key. The inputs are the configuration and flow files along with the new algorithm, and the output is re-encrypted configuration using the new standard.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/7_upgrade_guides/4_v1.3.1_to_v1.4.0.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nencrypt-config.sh -n nifi.properties -f flow.xml.gz -x -s PROPERTIES_KEY -A NIFI_PBKDF2_AES_GCM_256\nencrypt-config.sh -n nifi.properties -f flow.json.gz -x -s PROPERTIES_KEY -A NIFI_PBKDF2_AES_GCM_256\n```\n\n----------------------------------------\n\nTITLE: Adding KEDA Helm Repository in Kubernetes\nDESCRIPTION: This snippet adds the KEDA Helm chart repository to Helm, enabling subsequent installation of KEDA components into the Kubernetes cluster. It requires Helm CLI setup and is essential for deploying KEDA via Helm.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nhelm repo add kedacore https://kedacore.github.io/charts\n```\n\n----------------------------------------\n\nTITLE: Defining ReadOnly NiFi Cluster Configuration YAML\nDESCRIPTION: This YAML snippet demonstrates the full structure of the ReadOnlyConfig object, which specifies cluster-wide read-only configuration properties for a NiFi cluster managed with nifikop. It includes thread counts for timer-driven and event-driven processors, logback and authorizer XML configurations sourced from ConfigMaps and Secrets, and override options for nifi.properties, zookeeper.properties, and bootstrap.properties. Key optional parameters support secure cluster operation settings and custom JVM memory configuration. Dependencies include Kubernetes ConfigMaps and Secrets referenced by name and namespace. Inputs are YAML keys representing overriding configuration files or inline configuration strings, and outputs are applied to respective nodes' configurations, allowing for cluster-level defaults with node-level overrides.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/5_references/1_nifi_cluster/2_read_only_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreadOnlyConfig:\n  # MaximumTimerDrivenThreadCount define the maximum number of threads for timer driven processors available to the system.\n  maximumTimerDrivenThreadCount: 30\n  # MaximumEventDrivenThreadCount define the maximum number of threads for event driven processors available to the system.\n  maximumEventDrivenThreadCount: 10\n  # Logback configuration that will be applied to the node\n  logbackConfig:\n    # logback.xml configuration that will replace the one produced based on template\n    replaceConfigMap:\n      # The key of the value,in data content, that we want use.\n      data: logback.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # logback.xml configuration that will replace the one produced based on template and overrideConfigMap\n    replaceSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: logback.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n  # Authorizer configuration that will be applied to the node\n  authorizerConfig:\n    # An authorizers.xml configuration template that will replace the default template seen in authorizers.go\n    replaceTemplateConfigMap:\n      # The key of the value, in data content, that we want use.\n      data: authorizers.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # An authorizers.xml configuration template that will replace the default template seen in authorizers.go and the replaceTemplateConfigMap\n    replaceTemplateSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: authorizers.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n  # NifiProperties configuration that will be applied to the node.\n  nifiProperties:\n    # Additionnal nifi.properties configuration that will override the one produced based on template and\n    # configuration\n    overrideConfigMap:\n      # The key of the value,in data content, that we want use.\n      data: nifi.properties\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop.\n    # Additionnal nifi.properties configuration that will override the one produced based\n    #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    overrideSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: nifi.properties\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # Additionnal nifi.properties configuration that will override the one produced based\n    #\ton template, configurations and overrideConfigMap\n    overrideConfigs: |\n      nifi.ui.banner.text=NiFiKop\n    # A comma separated list of allowed HTTP Host header values to consider when NiFi\n    # is running securely and will be receiving requests to a different host[:port] than it is bound to.\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#web-properties\n    #      webProxyHosts:\n    # Nifi security client auth\n    needClientAuth: false\n    # Indicates which of the configured authorizers in the authorizers.xml file to use\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#authorizer-configuration\n    #      authorizer:\n  # ZookeeperProperties configuration that will be applied to the node.\n  zookeeperProperties:\n    #      # Additionnal zookeeeper.properties configuration that will override the one produced based on template and\n    #      # configuration\n    #      overrideConfigMap:\n    #        # The key of the value,in data content, that we want use.\n    #        data: zookeeeper.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop.\n    #      # Additionnal zookeeeper.properties configuration that will override the one produced based\n    #      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    #      overrideSecretConfig:\n    #        # The key of the value,in data content, that we want use.\n    #        data: zookeeeper.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop\n    # Additionnal zookeeper.properties configuration that will override the one produced based\n    # on template and configurations.\n    overrideConfigs: |\n      initLimit=15\n      autopurge.purgeInterval=24\n      syncLimit=5\n      tickTime=2000\n      dataDir=./state/zookeeper\n      autopurge.snapRetainCount=30\n  # BootstrapProperties configuration that will be applied to the node.\n  bootstrapProperties:\n    #      # Additionnal bootstrap.properties configuration that will override the one produced based on template and\n    #      # configuration\n    #      overrideConfigMap:\n    #        # The key of the value,in data content, that we want use.\n    #        data: bootstrap.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop.\n    #      # Additionnal bootstrap.properties configuration that will override the one produced based\n    #      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    #      overrideSecretConfig:\n    #        # The key of the value,in data content, that we want use.\n    #        data: bootstrap.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop\n    # JVM memory settings\n    nifiJvmMemory: \"512m\"\n    # Additionnal bootstrap.properties configuration that will override the one produced based\n    # on template and configurations.\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#bootstrap_properties\n    overrideConfigs: |\n      # java.arg.4=-Djava.net.preferIPv4Stack=true\n```\n\n----------------------------------------\n\nTITLE: Verifying Operator Pod Status\nDESCRIPTION: Command to list Pods in the 'nifikop' namespace to confirm that the NiFiKop operator is running correctly.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/6_contributing/1_developer_guide.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n$ kubectl get pods -n nifikop\nNAME                                                READY   STATUS    RESTARTS   AGE\nskeleton-nifikop-8946b89dc-4cfs9   1/1     Running   0          7m45s\n```\n\n----------------------------------------\n\nTITLE: Extracting Client Certificates from Secret - Console\nDESCRIPTION: This set of console commands retrieves base64-encoded certificate data from a Kubernetes secret and decodes it to create standard certificate and key files for client authentication. The commands use 'kubectl get secret' with jsonpath to access individual secret keys, decoding each value (CA cert, client cert, private key) and saving them to appropriately named files. Prerequisites include kubectl access to the relevant namespace and the expectation that the secret already exists with proper keys. Users should ensure secret permissions and file security best practices.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_4\n\nLANGUAGE: Console\nCODE:\n```\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['ca\\.crt']}\" | base64 -d > ca.crt\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['tls\\.crt']}\" | base64 -d > tls.crt\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['tls\\.key']}\" | base64 -d > tls.key\n```\n\n----------------------------------------\n\nTITLE: Running NiFiKop Operator Locally Using Make - Bash\nDESCRIPTION: This 'make run' command launches the NiFiKop operator locally in the default Kubernetes namespace. It leverages the local kubeconfig to connect to the cluster and execute the operator binary in development mode.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/6_contributing/1_developer_guide.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmake run\n```\n\n----------------------------------------\n\nTITLE: Configuring External Services for NiFi Cluster Exposure - YAML\nDESCRIPTION: Combining internalListeners and externalServices, this YAML block configures both internal and external access for the NiFi cluster. The externalServices field defines a Kubernetes LoadBalancer service named cluster-access, mapping the https and http-tracking internal listeners to external ports 443 and 80, respectively. This configuration requires having the underlying cloud provider support for LoadBalancer type services. Key inputs are internal listener names and targeted public ports; outputs are created Kubernetes service objects with external IPs for access. Dependencies: NiFi Helm Operator, proper cluster permissions, and supported Kubernetes provider.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    - type: \"https\"\n      name: \"https\"\n      containerPort: 8443\n    - type: \"cluster\"\n      name: \"cluster\"\n      containerPort: 6007\n    - type: \"s2s\"\n      name: \"s2s\"\n      containerPort: 10000\n    - type: \"prometheus\"\n      name: \"prometheus\"\n      containerPort: 9090\n    - type: \"load-balance\"\n      name: \"load-balance\"\n      containerPort: 6342\n    - name: \"http-tracking\"\n      containerPort: 8081\nexternalServices:\n  - name: cluster-access\n    spec:\n      portConfigs:\n        - internalListenerName: https\n          port: 443\n        - internalListenerName: http-tracking\n          port: 80\n      type: LoadBalancer\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi User Custom Resource in YAML\nDESCRIPTION: This YAML snippet defines a NifiUser custom resource instance for the Nifikop operator. It specifies the API version, kind, metadata such as the resource name, and the spec which includes user identity, the referenced NiFi cluster namespace and name, and whether a certificate should be created for the user. The snippet is used to create or manage a NiFi user resource on Kubernetes clusters managed by Nifikop.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/5_references/2_nifi_user.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: aguitton\nspec:\n  identity: alexandre.guitton@konpyutaika.com\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  createCert: false\n```\n\n----------------------------------------\n\nTITLE: Declaring Nodes with Config Groups and Per-Node Specs in NiFiKop YAML\nDESCRIPTION: Shows how to declare individual NiFi cluster nodes in a NiFiKop Custom Resource with references to reusable NodeConfigGroups or direct per-node configuration. Nodes are assigned by their identifier and either reference a config group or define an in-place resourcesRequirements block. Dependencies: NiFiKop Operator, and existing NodeConfigGroups if referenced. Key parameters are node IDs, referenced group name, and inline node-specific resource settings (CPU, memory). The output assigns configuration per node and supports mixing group-based and ad hoc configuration. Only valid within a NiFiKop CR under the 'nodes' or similar field.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n- id: 0\n  nodeConfigGroup: \"default_group\"\n- id: 2\n  nodeConfigGroup: \"high_mem_group\"\n- id: 3\n  nodeConfigGroup: \"high_mem_group\"\n- id: 5\n  nodeConfig:\n    resourcesRequirements:\n      limits:\n        cpu: \"2\"\n        memory: 3Gi\n      requests:\n        cpu: \"1\"\n        memory: 1Gi\n```\n\n----------------------------------------\n\nTITLE: Cloning NiFiKop Repository\nDESCRIPTION: Clones the NiFiKop repository from GitHub and changes the directory to the cloned repository. This is the first step in setting up the development environment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/6_contributing/1_developer_guide.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/konpyutaika/nifikop.git\ncd nifikop\n```\n\n----------------------------------------\n\nTITLE: Adding a Custom Internal Listener Without Type in NiFi - YAML\nDESCRIPTION: This example shows how to define an additional internal listener in the listenersConfig without specifying a type. Such listeners allow NiFi processors to listen on custom ports (e.g., for processor HTTP endpoints). Only the name and containerPort fields are required, and these listeners are not managed by the default internal NiFi service logic. This is useful for advanced use cases where processors require direct external connectivity. Inputs: name and custom port; Outputs: new exposed port inside the pod.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    ...\n    - name: \"http-tracking\"\n      containerPort: 8081\n```\n\n----------------------------------------\n\nTITLE: Packaging NiFiKop Helm Chart\nDESCRIPTION: Command to package the NiFiKop Helm chart for distribution.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/6_contributing/1_developer_guide.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nmake helm-package\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Node Configuration using YAML\nDESCRIPTION: This YAML snippet demonstrates how to define the configuration for a NiFi node using the NodeConfig. It specifies various parameters such as the default group, provenance storage, runAsUser, isNode, podMetadata, imagePullPolicy, priorityClassName, externalVolumeConfigs, and storageConfigs. The configuration includes detailed specifications for persistent volume claims (PVCs) for both provenance repository and logs.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/5_references/1_nifi_cluster/3_node_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n   default_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      # Set this to true if the instance is a node in a cluster.\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#basic-cluster-setup\n      isNode: true\n      # Additionnal metadata to merge to the pod associated\n      podMetadata:\n        annotations:\n          node-annotation: \"node-annotation-value\"\n        labels:\n          node-label: \"node-label-value\"\n      # Docker image used by the operator to create the node associated\n      # https://hub.docker.com/r/apache/nifi/\n#      image: \"apache/nifi:1.11.2\"\n      # nodeAffinity can be specified, operator populates this value if new pvc added later to node\n      # https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity\n#      nodeAffinity:\n      # imagePullPolicy define the pull policy for NiFi cluster docker image\n      imagePullPolicy: IfNotPresent\n      # priorityClassName define the name of the priority class to be applied to these nodes\n      priorityClassName: \"example-priority-class-name\"\n      # externalVolumeConfigs specifies a list of volume to mount into the main container.\n      externalVolumeConfigs:\n        - name: example-volume\n          mountPath: \"/opt/nifi/example\"\n          secret:\n            secretName: \"raw-controller\"\n      # storageConfigs specifies the node related configs\n      storageConfigs:\n        # Name of the storage config, used to name PV to reuse into sidecars for example.\n        - name: provenance-repository\n          # Retain this PVC throughout NifiCluster deletions.\n          reclaimPolicy: Retain\n          # Path where the volume will be mount into the main nifi container inside the pod.\n          mountPath: \"/opt/nifi/provenance_repository\"\n          # Metadata to attach to the PVC that gets created\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          # Kubernetes PVC spec\n          # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          reclaimPolicy: Delete\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n```\n\n----------------------------------------\n\nTITLE: Verifying New Node Resources after Scale Up (Console)\nDESCRIPTION: Demonstrates using `kubectl get` with a label selector (`nodeId=25`) to list the Kubernetes pods, configmaps, and persistent volume claims associated with the newly added NiFi node, verifying its creation and status.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl get pods,configmap,pvc -l nodeId=25\nNAME                          READY   STATUS    RESTARTS   AGE\npod/simplenifi-25-nodem5jh4   1/1     Running   0          11m\n\nNAME                             DATA   AGE\nconfigmap/simplenifi-config-25   7      11m\n\nNAME                                               STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\npersistentvolumeclaim/simplenifi-25-storagehwn24   Bound    pvc-7da86076-728e-11ea-846d-42010a8400f2   10Gi       RWO            standard       11m\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio VirtualService for HTTPS to Internal Service\nDESCRIPTION: Defines an Istio VirtualService named 'nifi-vs' associated with the 'nifi-gateway'. It routes the now-HTTP traffic (after gateway termination) for 'nifi.my-domain.com' to a specific internal ClusterIP service FQDN '<service-name>.<namespace>.svc.cluster.local' on port 8443. Placeholders need to be replaced with actual service name and namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: nifi-vs\nspec:\n  gateways:\n  - nifi-gateway\n  hosts:\n  - nifi.my-domain.com\n  http:\n  - match:\n    - uri:\n        prefix: /\n    route:\n    - destination:\n        host: <service-name>.<namespace>.svc.cluster.local\n        port:\n          number: 8443\n```\n\n----------------------------------------\n\nTITLE: Pushing NiFiKop Docker Image (Bash)\nDESCRIPTION: Uses the 'make docker-push' command to push the previously built NiFiKop operator Docker image to the configured Docker registry (specified by 'DOCKER_REGISTRY_BASE' and the image tag). Requires prior authentication to the target Docker registry.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/6_contributing/1_developer_guide.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nmake docker-push\n```\n\n----------------------------------------\n\nTITLE: Installing Nifikop Helm Chart with Release Name (Bash)\nDESCRIPTION: Shows the basic command to install the Nifikop Helm chart from the `konpyutaika` repository using `helm install`. Requires replacing `<release name>` with a unique name for this deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nhelm install <release name> konpyutaika/nifikop\n```\n\n----------------------------------------\n\nTITLE: Deploying a Simple NiFi Cluster in Kubernetes\nDESCRIPTION: Kubectl command to create a basic NiFi cluster using a sample configuration file. This requires first configuring the Zookeeper service name in the sample file.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Add your zookeeper svc name to the configuration\nkubectl create -n nifi -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Manually Deleting nifikop CRDs - Bash\nDESCRIPTION: Deletes all nifikop-related CRDs from the Kubernetes cluster using kubectl. This operation is destructive and will remove all related custom resources, causing all managed clusters to be deleted. Used for full cleanup after uninstalling the operator.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete crd nificlusters.nifi.konpyutaika.com\nkubectl delete crd nifiusers.nifi.konpyutaika.com\nkubectl delete crd nifiusergroups.nifi.konpyutaika.com\nkubectl delete crd nifiregistryclients.nifi.konpyutaika.com\nkubectl delete crd nifiparametercontexts.nifi.konpyutaika.com\nkubectl delete crd nifidataflows.nifi.konpyutaika.com\n```\n\n----------------------------------------\n\nTITLE: Installing KEDA Helm Chart - Console\nDESCRIPTION: This command installs the KEDA Helm chart into a specified namespace. It creates the namespace if it does not exist.  This deploys KEDA within the Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl create namespace keda\nhelm install keda kedacore/keda --namespace keda\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio Gateway for HTTP (YAML)\nDESCRIPTION: This YAML snippet configures an Istio Gateway to intercept HTTP traffic. It defines the gateway's metadata and specification, including the ingress gateway selector, port configuration (port 80, HTTP), and the host domain (`nifi.my-domain.com`). This gateway will intercept all requests for the specified domain on port 80, allowing access to the NiFi cluster from the external world. The key parameter is `hosts`, which specifies the domain. It requires Istio to be installed and running in your Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: nifi-gateway\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - nifi.my-domain.com\n```\n```\n\n----------------------------------------\n\nTITLE: Applying Nifikop Custom Resource Definitions (CRDs) Manually with Bash\nDESCRIPTION: This snippet shows a set of kubectl commands to manually deploy the Nifikop CRDs if the user opts to skip CRD deployment during Helm installation using the --skip-crds flag. Applying these CRDs beforehand ensures Kubernetes recognizes the custom resource types used by Nifikop. This is necessary to avoid Helm attempting to create them and to maintain control over CRD management. Each command applies one of the CRD yaml manifests hosted in the Nifikop GitHub repository.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifinodegroupautoscalers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificonnections.yaml\n```\n\n----------------------------------------\n\nTITLE: Setting ReclaimPolicy for StorageConfig in NifiCluster\nDESCRIPTION: This code adds the ability to specify ReclaimPolicy, such as 'Retain' or 'Delete', for persistent volumes associated with StorageConfig within a NifiCluster resource, aiding in storage lifecycle management.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/CHANGELOG.md#_snippet_12\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.k8s.io/v1alpha1\nkind: NifiCluster\nspec:\n  storage:\n    persistentVolume:\n      reclaimPolicy: Retain\n```\n\n----------------------------------------\n\nTITLE: Defining Multiple NodeConfigGroups for NiFi Cluster\nDESCRIPTION: This snippet demonstrates how to specify multiple NodeConfigGroups to configure different types of nodes within a NiFi cluster. Each group defines resource requirements, storage options, and user settings to tailor nodes for specific roles or performance needs.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nnodeConfigGroups:\n  default_group:\n    provenanceStorage: \"10 GB\"\n    runAsUser: 1000\n    serviceAccountName: \"default\"\n    resourcesRequirements:\n      limits:\n        cpu: \"2\"\n        memory: 3Gi\n      requests:\n        cpu: \"1\"\n        memory: 3Gi\n  high_mem_group:\n    provenanceStorage: \"10 GB\"\n    runAsUser: 1000\n    serviceAccountName: \"default\"\n    resourcesRequirements:\n      limits:\n        cpu: \"2\"\n        memory: 30Gi\n      requests:\n        cpu: \"1\"\n        memory: 30Gi\n```\n\n----------------------------------------\n\nTITLE: Example NifiDataflow Kubernetes Resource Definition (YAML)\nDESCRIPTION: This YAML manifest demonstrates how to define a `NifiDataflow` custom resource in Kubernetes using the Nifikop operator. It specifies the target NiFi cluster (`clusterRef`), registry client (`registryClientRef`), parameter context (`parameterContextRef`), the flow details (parent process group, bucket ID, flow ID, version), synchronization mode (`syncMode`), update strategy (`updateStrategy`), error handling (`skipInvalid...`), and canvas positioning (`flowPosition`). This resource requests the deployment and management of a specific dataflow version from a NiFi Registry into a running NiFi cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/5_references/5_nifi_dataflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  flowPosition:\n    posX: 0\n    posY: 0\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: squidflow\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: nifikop\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Example Data Persistence Storage Configuration in NiFiKop (YAML)\nDESCRIPTION: This YAML snippet demonstrates how to define multiple storage configurations for persistent data in NiFi clusters, by specifying mount paths, PVC specifications, resource requests, labels, and annotations. These ensure data durability across pod restarts and deletions, which is crucial for stateful applications like NiFi.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nstorageConfigs:\n  - mountPath: /opt/nifi/nifi-current/logs\n    name: logs\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/data\n    name: data\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 50Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/extensions\n    name: extensions-repository\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 5Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/flowfile_repository\n    name: flowfile-repository\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/nifi-current/conf\n    name: conf\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 5Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/content_repository\n    name: content-repository\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 500Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/provenance_repository\n    name: provenance-repository\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 500Gi\n```\n\n----------------------------------------\n\nTITLE: Setting Docker Registry Environment Variable (Bash)\nDESCRIPTION: Sets the 'DOCKER_REGISTRY_BASE' environment variable to specify the target Docker registry prefix for building and pushing the operator image. Replace '{your-docker-repo}' with the actual registry path (e.g., 'docker.io/yourusername').\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/6_contributing/1_developer_guide.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nexport DOCKER_REGISTRY_BASE={your-docker-repo}\n```\n\n----------------------------------------\n\nTITLE: Retrieving Helm Release Status\nDESCRIPTION: Command to get detailed status information about a specific Helm release, useful for troubleshooting and verifying deployment health.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nhelm status nifikop\n```\n\n----------------------------------------\n\nTITLE: Running the Operator Using Make\nDESCRIPTION: Starts the operator locally outside of the Kubernetes cluster by executing the make run command, which uses default kubeconfig and namespace settings.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/6_contributing/1_developer_guide.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmake run\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Dataflow Custom Resource with YAML\nDESCRIPTION: This YAML snippet demonstrates the creation of a NiFiDataflow resource for the konpyutaika/nifikop Kubernetes operator. The specification includes references to the process group, bucket, flow, and registry clients, as well as update strategies and error handling controls via various boolean fields. Required fields such as bucketId, flowId, flowVersion, clusterRef, registryClientRef, and updateStrategy are included, alongside optional fields for parameter context and GUI placement. The resource synchronizes a NiFi flow instance and manages its lifecycle using the specified cluster and registry.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/5_references/5_nifi_dataflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  flowPosition:\n    posX: 0\n    posY: 0\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: squidflow\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: nifikop\n  updateStrategy: drain\n\n```\n\n----------------------------------------\n\nTITLE: Defining Inherited NifiParameterContext with Secret Reference in Kubernetes YAML\nDESCRIPTION: This YAML manifest demonstrates how to create a child NifiParameterContext in Kubernetes that inherits from a parent context and includes secret references. The resource, named \"dataflow-lifecycle-child\", specifies its own description, cluster reference, secrets, and parameters while referring to an existing parent parameter context through inheritedParameterContexts. Prerequisites are a deployed nifikop operator and an existing referenced parent context. Important fields are metadata.name, spec.inheritedParameterContexts (for inheritance), and spec.parameters, which allow for parameter overrides. The manifest enables hierarchical parameter management in NiFi with support for secure value injection; ensure referenced resources exist before applying.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/5_references/4_nifi_parameter_context.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle-child\nspec:\n  description: \"It is a child test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  inheritedParameterContexts:\n    - name: dataflow-lifecycle\n  parameters:\n    - name: test\n      value: toto-child\n      description: tutu (child)\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi UserGroup Schema\nDESCRIPTION: This YAML snippet defines a `NifiUserGroup` object within a Kubernetes cluster, specifying its API version, kind, metadata (name), and specification. The `spec` section includes references to a NiFi cluster and users, along with access policies. It's used to manage and configure user groups within a NiFi deployment by establishing links to existing NiFi users and to grant them specific access control based on the specified policies and resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/5_references/6_nifi_usergroup.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n    - name: nc-controller.nifikop.mgt.cluster.local\n  accessPolicies:\n    - type: global\n      action: read\n      resource: /counters\n```\n```\n\n----------------------------------------\n\nTITLE: Configuring Nifi Cluster Read-Only Settings in YAML\nDESCRIPTION: This YAML snippet demonstrates the structure of the `readOnlyConfig` object used within NifiKop custom resources. It allows specifying cluster-wide read-only settings for Apache NiFi, such as maximum thread counts, logback configurations, authorizer settings, and overrides for nifi.properties, zookeeper.properties, and bootstrap.properties. Configurations can be sourced from external ConfigMaps/Secrets or defined inline.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/5_references/1_nifi_cluster/2_read_only_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreadOnlyConfig:\n  # MaximumTimerDrivenThreadCount define the maximum number of threads for timer driven processors available to the system.\n  maximumTimerDrivenThreadCount: 30\n  # MaximumEventDrivenThreadCount define the maximum number of threads for event driven processors available to the system.\n  maximumEventDrivenThreadCount: 10\n  # Logback configuration that will be applied to the node\n  logbackConfig:\n    # logback.xml configuration that will replace the one produced based on template\n    replaceConfigMap:\n      # The key of the value,in data content, that we want use.\n      data: logback.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # logback.xml configuration that will replace the one produced based on template and overrideConfigMap\n    replaceSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: logback.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n  # Authorizer configuration that will be applied to the node\n  authorizerConfig:\n    # An authorizers.xml configuration template that will replace the default template seen in authorizers.go\n    replaceTemplateConfigMap:\n      # The key of the value, in data content, that we want use.\n      data: authorizers.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # An authorizers.xml configuration template that will replace the default template seen in authorizers.go and the replaceTemplateConfigMap\n    replaceTemplateSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: authorizers.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n  # NifiProperties configuration that will be applied to the node.\n  nifiProperties:\n    # Additionnal nifi.properties configuration that will override the one produced based on template and\n    # configuration\n    overrideConfigMap:\n      # The key of the value,in data content, that we want use.\n      data: nifi.properties\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop.\n    # Additionnal nifi.properties configuration that will override the one produced based\n    #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    overrideSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: nifi.properties\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # Additionnal nifi.properties configuration that will override the one produced based\n    #\ton template, configurations and overrideConfigMap\n    overrideConfigs: |\n      nifi.ui.banner.text=NiFiKop\n    # A comma separated list of allowed HTTP Host header values to consider when NiFi\n    # is running securely and will be receiving requests to a different host[:port] than it is bound to.\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#web-properties\n    #      webProxyHosts:\n    # Nifi security client auth\n    needClientAuth: false\n    # Indicates which of the configured authorizers in the authorizers.xml file to use\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#authorizer-configuration\n  #      authorizer:\n  # ZookeeperProperties configuration that will be applied to the node.\n  zookeeperProperties:\n    #      # Additionnal zookeeeper.properties configuration that will override the one produced based on template and\n    #      # configuration\n    #      overrideConfigMap:\n    #        # The key of the value,in data content, that we want use.\n    #        data: zookeeeper.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop.\n    #      # Additionnal zookeeeper.properties configuration that will override the one produced based\n    #      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    #      overrideSecretConfig:\n    #        # The key of the value,in data content, that we want use.\n    #        data: zookeeeper.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop\n    # Additionnal zookeeper.properties configuration that will override the one produced based\n    # on template and configurations.\n    overrideConfigs: |\n      initLimit=15\n      autopurge.purgeInterval=24\n      syncLimit=5\n      tickTime=2000\n      dataDir=./state/zookeeper\n      autopurge.snapRetainCount=30\n  # BootstrapProperties configuration that will be applied to the node.\n  bootstrapProperties:\n    #      # Additionnal bootstrap.properties configuration that will override the one produced based on template and\n    #      # configuration\n    #      overrideConfigMap:\n    #        # The key of the value,in data content, that we want use.\n    #        data: bootstrap.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop.\n    #      # Additionnal bootstrap.properties configuration that will override the one produced based\n    #      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    #      overrideSecretConfig:\n    #        # The key of the value,in data content, that we want use.\n    #        data: bootstrap.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop\n    # JVM memory settings\n    nifiJvmMemory: \"512m\"\n    # Additionnal bootstrap.properties configuration that will override the one produced based\n    # on template and configurations.\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#bootstrap_properties\n    overrideConfigs: |\n      # java.arg.4=-Djava.net.preferIPv4Stack=true\n```\n\n----------------------------------------\n\nTITLE: Define Storage Configurations for NiFi Nodes - YAML\nDESCRIPTION: This YAML snippet defines storage configurations for various NiFi components. Each configuration specifies the mount path, name, metadata (labels and annotations), PVC spec (access modes, resources, storage class name).  It includes storage for logs, data, extensions, flowfile repository, configurations, content repository and provenance repository.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n...\nstorageConfigs:\n  - mountPath: /opt/nifi/nifi-current/logs\n    name: logs\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/data\n    name: data\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 50Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/extensions\n    name: extensions-repository\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 5Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/flowfile_repository\n    name: flowfile-repository\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/nifi-current/conf\n    name: conf\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 5Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/content_repository\n    name: content-repository\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 500Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/provenance_repository\n    name: provenance-repository\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 500Gi\n      storageClassName: ssd-wait\n```\n\n----------------------------------------\n\nTITLE: Installing NifiKop Operator using Helm Chart\nDESCRIPTION: Deploys the NifiKop operator to Kubernetes via Helm, specifying the version, resource allocations, and namespace. It optionally allows disabling CRD installation if they have been deployed manually or by other means. The Helm command manages the operator's deployment lifecycle.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/2_setup/1_getting_started.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop \\\n    oci://ghcr.io/konpyutaika/helm-charts/nifikop \\\n    --namespace=nifi \\\n    --version 0.11.0 \\\n    --set image.tag=v0.11.0-release \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set namespaces={\"nifi\"}\n```\n\n----------------------------------------\n\nTITLE: Setting Operator Name Environment Variable Bash\nDESCRIPTION: This command sets the OPERATOR_NAME environment variable to 'nifi-operator', which is necessary for running the operator directly using Go. It's typically required in local development or debugging sessions. Must be executed in a bash-compatible shell prior to process launch.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/6_contributing/1_developer_guide.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport OPERATOR_NAME=nifi-operator\n```\n\n----------------------------------------\n\nTITLE: Getting the Status of a Specific Helm Release\nDESCRIPTION: Command to retrieve detailed status information about the Helm release named 'nifikop', including deployment state and resource status.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nhelm status nifikop\n```\n\n----------------------------------------\n\nTITLE: Configuring Production Certificate Issuer with Cert-Manager in YAML\nDESCRIPTION: This YAML manifest configures a production cluster issuer using cert-manager to provision TLS certificates automatically for ingress resources, essential for securing HTTP endpoints with HTTPS in the Kubernetes cluster. It assumes cert-manager is deployed in the 'cert-manager' namespace and requires access to a valid ACME provider such as Let's Encrypt.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/config/samples/keycloak-example/README.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\nlets-encrypt-issuer.yaml\n```\n\n----------------------------------------\n\nTITLE: Creating RoleBinding for Kubernetes State Management\nDESCRIPTION: This YAML snippet creates a RoleBinding, associating the Role with a `ServiceAccount` within the specified namespace. It allows the `ServiceAccount` to use the permissions defined in the `Role`. This requires the `Role` defined in the previous step and a service account named `default` in the `nifi` namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: simplenifi\n  namespace: nifi\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: nifi\nroleRef:\n  kind: Role\n  name: simplenifi\n  apiGroup: rbac.authorization.k8s.io\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFi cluster on OpenShift using kubectl in Bash\nDESCRIPTION: Creates Kubernetes resources from the updated 'config/samples/openshift.yaml' file in the 'nifi' namespace, deploying a NiFi cluster configured with correct security context for OpenShift. This deployment follows prior steps for setting UID and fsGroup values.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create -n nifi -f config/samples/openshift.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing Kubectl Plugin using Make and Sudo (Console)\nDESCRIPTION: This console command sequence first builds the `kubectl-nifikop` executable using the provided `Makefile`. Upon successful compilation, it uses `sudo` to copy the generated binary from the `./bin` directory to `/usr/local/bin`. This directory is typically included in the system's PATH, making the `kubectl nifikop` command available globally. Requires `make` to be installed and appropriate user privileges for `sudo`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/2_deploy_nifikop/3_kubectl_plugin.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nmake kubectl-nifikop && sudo cp ./bin/kubectl-nifikop /usr/local/bin\n```\n\n----------------------------------------\n\nTITLE: Node.js Script for Migrating NiFiKop CRDs (JavaScript)\nDESCRIPTION: This Node.js script uses the `@kubernetes/client-node` library to migrate NiFiKop custom resources between API groups. It connects to the Kubernetes cluster using default credentials, lists resources of a specified type (passed via command-line argument `--type`) in a given namespace (passed via `--namespace`, defaults to 'default') from the old API group (`nifi.orange.com/v1alpha1`). For each resource not managed by an owner reference, it creates a new resource under the target API group (`nifi.konpyutaika.com/v1alpha1`), copying metadata (name, annotations, labels) and the spec. Finally, it copies the status from the old resource to the newly created one. Includes error handling and disables TLS verification (`NODE_TLS_REJECT_UNAUTHORIZED=0`).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nprocess.env['NODE_TLS_REJECT_UNAUTHORIZED'] = 0;\nconst k8s = require('@kubernetes/client-node');\n\nconst kc = new k8s.KubeConfig();\nkc.loadFromDefault();\n\nconst k8sApi = kc.makeApiClient(k8s.CustomObjectsApi);\n\nconst KONPYUTAIKA_GROUP = 'nifi.konpyutaika.com';\nconst KONPYUTAIKA_GROUP_VERSION = 'v1alpha1';\nconst ORANGE_GROUP = 'nifi.orange.com';\nconst ORANGE_GROUP_VERSION = 'v1alpha1';\n\nconst call = async (SRC_GRP, SRC_GRP_VER, DST_GRP, DST_GRP_VER, KIND_PLURAL, NAMESPACE) => {\n\tconsole.log(`Listing ${KIND_PLURAL} of ${SRC_GRP}/${SRC_GRP_VER} in ${NAMESPACE}...`);\n\tconst listResources = (await k8sApi.listNamespacedCustomObject(SRC_GRP, SRC_GRP_VER, NAMESPACE, KIND_PLURAL)).body.items;\n\treturn Promise.all(listResources.map(async (resource) => {\n\t\ttry {\n\t\t\tconsole.log(`Found ${resource.kind} \"${resource.metadata.name}\" of ${resource.apiVersion} in ${NAMESPACE}`);\n\n\t\t\tif (resource.metadata.ownerReferences) {\n\t\t\t\tconsole.log(`${resource.kind} ${resource.metadata.name} mananged by something else (ownerRefereces is set).`);\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tconst bodyResource = {\n\t\t\t\tapiVersion: `${DST_GRP}/${DST_GRP_VER}`,\n\t\t\t\tkind: resource.kind,\n\t\t\t\tmetadata: {\n\t\t\t\t\tname: resource.metadata.name,\n\t\t\t\t\tannotations: resource.metadata.annotations,\n\t\t\t\t\tlabels: resource.metadata.labels\n\t\t\t\t},\n\t\t\t\tspec: resource.spec\n\t\t\t};\n\n\t\t\tconsole.log(`Creating ${bodyResource.kind} \"${bodyResource.metadata.name}\" of ${bodyResource.apiVersion} in ${NAMESPACE}...`);\n\t\t\tconst newResource = (await k8sApi.createNamespacedCustomObject(DST_GRP, DST_GRP_VER, NAMESPACE, KIND_PLURAL, bodyResource)).body;\n\t\t\tconsole.log('...done creating.');\n\n\t\t\tconst bodyStatus = {\n\t\t\t\tapiVersion: newResource.apiVersion,\n\t\t\t\tkind: newResource.kind,\n\t\t\t\tmetadata: {\n\t\t\t\t\tname: newResource.metadata.name,\n\t\t\t\t\tresourceVersion: newResource.metadata.resourceVersion\n\t\t\t\t},\n\t\t\t\tstatus: resource.status\n\t\t\t};\n\n\t\t\tconsole.log(`Copying status from ${resource.kind} \"${resource.metadata.name}\" of ${newResource.apiVersion} to ${newResource.kind} \"${newResource.metadata.name}\" of ${newResource.apiVersion} in ${NAMESPACE}...`);\n\t\t\tconst newResourceWithStatus = (await k8sApi.replaceNamespacedCustomObjectStatus(DST_GRP, DST_GRP_VER, NAMESPACE, KIND_PLURAL, bodyStatus.metadata.name, bodyStatus)).body;\n\t\t\tconsole.log('...done copying.');\n\t\t\treturn newResourceWithStatus;\n\t\t}\n\t\tcatch (e) {\n\t\t\tconsole.error(e.body ? e.body.message ? e.body.message : e.body : e);\n\t\t}\n\t}));\n};\n\nconst argv = require('minimist')(process.argv.slice(2));\n\nlet NAMESPACE = argv.namespace ? argv.namespace.length > 0 ? argv.namespace : 'default' : 'default';\nlet KIND_PLURAL = {\n\tcluster: 'nificlusters',\n\tdataflow: 'nifidataflows',\n\tparametercontext: 'nifiparametercontexts',\n\tregistryclient: 'nifiregistryclients',\n\tuser: 'nifiusers',\n\tusergroup: 'nifiusergroups',\n};\n\nif (!argv.type) {\n\tconsole.error('Type not provided');\n\tprocess.exit(1);\n}\n\nif (!KIND_PLURAL[argv.type]) {\n\tconsole.error(`Type ${argv.type} is not one of the following types: ${Object.keys(KIND_PLURAL)}`);\n\tprocess.exit(1);\n}\n\nconsole.log(`########### START: ${KIND_PLURAL[argv.type]} ###########`);\ncall( ORANGE_GROUP, ORANGE_GROUP_VERSION, KONPYUTAIKA_GROUP, KONPYUTAIKA_GROUP_VERSION, KIND_PLURAL[argv.type], NAMESPACE)\n\t.then(r => console.log('############ END ############'))\n\t.catch(e => console.error(e));\n```\n\n----------------------------------------\n\nTITLE: Complete package.json Configuration\nDESCRIPTION: Shows the expected structure of the `package.json` file after project initialization and dependency installation. It includes project metadata, defined scripts (including the `start` script for the migration), keywords, license, and the installed dependencies with their versions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"nifikop_crd_migration\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Script to migrate from the old CRDs to the new CRDs.\",\n  \"main\": \"index.js\",\n  \"scripts\": {\n    \"start\": \"node --no-warnings index.js\",\n    \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\"\n  },\n  \"keywords\": [\n    \"K8S\",\n    \"NiFiKop\",\n    \"CRDs\"\n  ],\n  \"license\": \"ISC\",\n  \"dependencies\": {\n    \"@kubernetes/client-node\": \"^0.16.3\",\n    \"minimist\": \"^1.2.6\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Install cert-manager with kubectl\nDESCRIPTION: This snippet installs cert-manager using kubectl, which is required by NiFiKop for issuing certificates in secured clusters. It applies the cert-manager.yaml file directly from the jetstack GitHub releases.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/2_deploy_nifikop/1_quick_start.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f \\\n    https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.yaml\n```\n\n----------------------------------------\n\nTITLE: Adding KEDA Helm Repository\nDESCRIPTION: Adds the official KEDA Helm chart repository to your local Helm configuration. This allows you to install KEDA using Helm.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nhelm repo add kedacore https://kedacore.github.io/charts\n```\n\n----------------------------------------\n\nTITLE: Monitoring NiFi Node Decommissioning Status (Console)\nDESCRIPTION: This command uses `kubectl describe` to inspect the `NifiCluster` resource named `simplenifi`. The output includes the `Status.Nodes State` section, which shows the current state of each node, including the `Graceful Action State` (e.g., `GracefulDownscaleRequired`) for nodes undergoing decommissioning (like node ID 2 in the example).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_5\n\nLANGUAGE: console\nCODE:\n```\nkubectl describe nificluster simplenifi\n\n...\nStatus:\n  Nodes State:\n    ...\n    2:\n      Configuration State:  ConfigInSync\n      Graceful Action State:\n        Action State:   GracefulDownscaleRequired\n        Error Message:\n    ...\n...\n```\n\n----------------------------------------\n\nTITLE: Building and Installing Kubectl NiFiKop Plugin (Console)\nDESCRIPTION: This command sequence first builds the kubectl-nifikop executable using `make` within the project directory. It then uses `sudo` to copy the compiled executable from the local `bin` directory into a standard system directory (`/usr/local/bin`) which is typically included in the user's PATH, making the plugin globally accessible. This requires `make` to be installed and root privileges for copying to the target directory.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/2_deploy_nifikop/3_kubectl_plugin.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nmake kubectl-nifikop && sudo cp ./bin/kubectl-nifikop /usr/local/bin\n```\n\n----------------------------------------\n\nTITLE: Define Custom NiFi Authorizer Configuration Template\nDESCRIPTION: Provides a YAML template, incorporating Go templating, for the NiFi authorizers.xml file. This template includes configurations for both the default file-based authorizer and a hypothetical custom database-backed authorizer, allowing NiFiKOp to dynamically inject node identities and the controller user. This template is used to replace the default authorizers.xml via NiFiKOp's configuration options.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/3_tasks/2_security/2_authorization/1_custom_authorizer.md#_snippet_0\n\nLANGUAGE: YAML + Go Templating\nCODE:\n```\n{{- $nodeList := .NodeList }}\n{{- $clusterName := .ClusterName }}\n{{- $namespace := .Namespace }}<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>\n<authorizers>\n    <userGroupProvider>\n        <identifier>file-user-group-provider</identifier>\n        <class>org.apache.nifi.authorization.FileUserGroupProvider</class>\n        <property name=\"Users File\">../data/users.xml</property>\n        <property name=\"Legacy Authorized Users File\"></property>\n        <property name=\"Initial User Identity admin\">{{ .ControllerUser }}</property>\n{{- range $i, $host := .NodeList }}\n        <property name=\"Initial User Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n    </userGroupProvider>\n    <userGroupProvider>\n        <identifier>database-user-group-provider</identifier>\n        <class>my.custom.DatabaseUserGroupProvider</class>\n        <!-- Any extra configuration for this provider goes here -->\n{{- range $i, $host := .NodeList }}\n        <property name=\"Initial User Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n    </userGroupProvider>\n    <accessPolicyProvider>\n        <identifier>file-access-policy-provider</identifier>\n        <class>org.apache.nifi.authorization.FileAccessPolicyProvider</class>\n        <property name=\"User Group Provider\">file-user-group-provider</property>\n        <property name=\"Authorizations File\">../data/authorizations.xml</property>\n        <property name=\"Initial Admin Identity\">{{ .ControllerUser }}</property>\n        <property name=\"Legacy Authorized Users File\"></property>\n{{- range $i, $host := .NodeList }}\n        <property name=\"Node Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n\t\t<property name=\"Node Group\"></property>\n    </accessPolicyProvider>\n    <accessPolicyProvider>\n        <identifier>database-access-policy-provider</identifier>\n        <class>my.custom.DatabaseAccessPolicyProvider</class>\n        <!-- Any extra configuration for this provider goes here -->\n{{- range $i, $host := .NodeList }}\n        <property name=\"Node Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n\t\t<property name=\"Node Group\"></property>\n    </accessPolicyProvider>\n    <authorizer>\n        <identifier>managed-authorizer</identifier>\n        <class>org.apache.nifi.authorization.StandardManagedAuthorizer</class>\n        <property name=\"Access Policy Provider\">file-access-policy-provider</property>\n    </authorizer>\n    <authorizer>\n        <identifier>custom-database-authorizer</identifier>\n        <class>my.custom.DatabaseAuthorizer</class>\n        <property name=\"Access Policy Provider\">database-access-policy-provider</property>\n    </authorizer>\n</authorizers>\n```\n\n----------------------------------------\n\nTITLE: Setting the Authorizer Property in NiFi\nDESCRIPTION: This shell snippet demonstrates how to set the `nifi.security.user.authorizer` property to use the custom database authorizer. This property tells NiFi which authorizer to use from the `authorizers.xml` configuration file. Setting it to `custom-database-authorizer` will enable the custom authorizer that reads rules from a database.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/1_manage_clusters/1_deploy_cluster/6_users_authorization/1_custom_user_authorizer.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nnifi.security.user.authorizer=custom-database-authorizer\n```\n\n----------------------------------------\n\nTITLE: Selecting the Active NiFi Authorizer (Shell/Properties)\nDESCRIPTION: This shell snippet shows how to set the `nifi.security.user.authorizer` property within NiFi's `nifi.properties` file. This property directs NiFi to use the authorizer definition matching the specified identifier (in this case, `custom-database-authorizer`) from the configured `authorizers.xml` file. This activates the custom authorizer defined in the previous YAML example.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/6_users_authorization/1_custom_user_authorizer.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nnifi.security.user.authorizer=custom-database-authorizer\n```\n\n----------------------------------------\n\nTITLE: Creating a custom Kubernetes StorageClass with WaitForFirstConsumer volume binding mode in YAML\nDESCRIPTION: Defines a custom StorageClass resource in Kubernetes to specify storage parameters optimized for Apache NiFi deployment, leveraging the volumeBindingMode 'WaitForFirstConsumer' to delay volume binding until a pod consumer is scheduled. This manifest requires Kubernetes cluster with storage provisioner (here gce-pd) and specifies reclaimPolicy to 'Delete'. The YAML manifest includes the apiVersion, kind, metadata, parameters, provisioner, reclaimPolicy, and volumeBindingMode fields.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: exampleStorageclass\nparameters:\n  type: pd-standard\nprovisioner: kubernetes.io/gce-pd\nreclaimPolicy: Delete\nvolumeBindingMode: WaitForFirstConsumer\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiUserGroup Resource in YAML\nDESCRIPTION: Example YAML manifest for creating a NifiUserGroup custom resource. It defines a group named 'group-test', associates it with the 'nc' NifiCluster in the 'nifikop' namespace, includes specific user identities via 'usersRef', and grants a global read access policy to the '/counters' resource within NiFi using 'accessPolicies'. This resource requires the Nifikop operator and the NifiUserGroup CRD to be installed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/5_references/6_nifi_usergroup.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n    - name: nc-controller.nifikop.mgt.cluster.local\n  accessPolicies:\n    - type: global\n      action: read\n      resource: /counters\n```\n\n----------------------------------------\n\nTITLE: Pushing Docker Image for NiFiKop Operator Bash\nDESCRIPTION: This command pushes the locally built NiFiKop Docker image to the configured registry. Requires that 'export DOCKER_REGISTRY_BASE' has already been set and make docker-build invoked. Needs Docker credentials for target registry. Results in the operator image becoming available for Helm or Kubernetes deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/6_contributing/1_developer_guide.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nmake docker-push\n```\n\n----------------------------------------\n\nTITLE: Overriding nifi.properties for OIDC in NiFiKop Kubernetes Manifest (YAML)\nDESCRIPTION: This YAML snippet demonstrates how to configure OIDC authentication by overriding nifi.properties in a NifiCluster custom resource for NiFiKop. It uses the 'overrideConfigs' field under 'spec.readOnlyConfig.nifiProperties' to set required OIDC and identity mapping properties. Prerequisites include an existing OIDC provider and proper permissions to edit the NiFiKop NifiCluster manifests. The snippet expects the user to substitute placeholders for the OIDC discovery URL, client ID, and client secret.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/5_users_authentication/1_oidc.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\n...\nspec:\n  ...\n  readOnlyConfig:\n    # NifiProperties configuration that will be applied to the node.\n    nifiProperties:\n      webProxyHosts:\n        - nifistandard2.trycatchlearn.fr:8443\n      # Additionnal nifi.properties configuration that will override the one produced based\n      # on template and configurations.\n      overrideConfigs: |\n        nifi.security.user.oidc.discovery.url=<oidc server discovery url>\n        nifi.security.user.oidc.client.id=<oidc client's id>\n        nifi.security.user.oidc.client.secret=<oidc client's secret>\n        nifi.security.identity.mapping.pattern.dn=CN=([^,]*)(?:, (?:O|OU)=.*)?\n        nifi.security.identity.mapping.value.dn=$1\n        nifi.security.identity.mapping.transform.dn=NONE\n      ...\n   ...\n...\n```\n\n----------------------------------------\n\nTITLE: Manually Applying NiFiKop CRDs Using kubectl - Bash\nDESCRIPTION: This snippet sequentially installs all the necessary CustomResourceDefinitions (CRDs) for the NiFiKop operator via kubectl from public URLs. Manual installation of these CRDs is required if you choose to skip CRD installation when deploying the Helm chart (i.e., with --skip-crds). Each URL references a YAML manifest necessary for the NiFiKop operator to manage clusters, users, user groups, data flows, parameter contexts, and registry clients. Successful application enables the operator to manage all NiFi resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/2_deploy_nifikop/1_quick_start.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining NifiUser Custom Resource in Kubernetes with YAML\nDESCRIPTION: This YAML manifest demonstrates how to define a NifiUser custom resource for use with the nifikop operator in Kubernetes. It sets the user's identity, associates it with a specified NiFi cluster, and configures certificate creation options. Required dependencies include a running Kubernetes cluster, the nifikop custom resource definitions (CRDs) installed, and an existing NifiCluster resource. The YAML expects fields for user identity, cluster references, and certificate toggles, and will create a NiFi user resource when applied with kubectl. Inputs are the specification fields shown; outputs are the corresponding Kubernetes objects. Limitations include schema field requirements and correct operator setup.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/5_references/2_nifi_user.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiUser\nmetadata:\n  name: aguitton\nspec:\n  identity: alexandre.guitton@konpyutaika.com\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  createCert: false\n\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi User Resource in YAML\nDESCRIPTION: This YAML snippet defines a `NifiUser` custom resource for managing a user within a NiFi cluster. It specifies the user's identity, cluster reference, whether to include a Java keystore, and whether to create a certificate. The `accessPolicies` section configures the user's permissions within the NiFi environment. It defines the `type`, `action`, and `resource` for access control, and can specify `componentType` and `componentId` for component-level access.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/3_manage_nifi/2_manage_users_and_accesses/1_users_management.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: aguitton\nspec:\n  # identity field is use to define the user identity on NiFi cluster side,\n  #\t it use full when the user's name doesn't suite with Kubernetes resource name.\n  identity: alexandre.guitton@konpyutaika.com\n  # Contains the reference to the NifiCluster with the one the registry client is linked.\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  # Whether or not the the operator also include a Java keystore format (JKS) with you secret\n  includeJKS: false\n  # Whether or not a certificate will be created for this user.\n  createCert: false\n  # defines the list of access policies that will be granted to the group.\n  accessPolicies:\n      # defines the kind of access policy, could be \"global\" or \"component\".\n    - type: component\n      # defines the kind of action that will be granted, could be \"read\" or \"write\"\n      action: read\n      # resource defines the kind of resource targeted by this access policies, please refer to the following page:\n      #\thttps://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#access-policies\n      resource: /data\n      # componentType is used if the type is \"component\", it's allow to define the kind of component on which is the\n      # access policy\n      componentType: \"process-groups\"\n      # componentId is used if the type is \"component\", it's allow to define the id of the component on which is the\n      # access policy\n      componentId: \"\"\n```\n\n----------------------------------------\n\nTITLE: Applying NiFiKop CRDs (Bash/kubectl)\nDESCRIPTION: Uses `kubectl apply` to install the necessary NiFiKop Custom Resource Definitions (CRDs) into the target Kubernetes cluster. This step is required before running the operator.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/6_contributing/1_developer_guide.md#_snippet_4\n\nLANGUAGE: Bash\nCODE:\n```\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing cert-manager via Helm 3 - Bash\nDESCRIPTION: This sequence of commands installs cert-manager using Helm 3, including manually applying CRDs, adding the Jetstack Helm repository, updating it, and installing the chart in a user-created namespace. Helm and kubectl are required, and the namespace must be created in advance. The commands target cert-manager chart version v1.7.2 for compatibility with NiFiKop.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/2_deploy_nifikop/1_quick_start.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Install CustomResourceDefinitions first\nkubectl apply --validate=false -f \\\n   https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.crds.yaml\n\n# Add the jetstack helm repo\nhelm repo add jetstack https://charts.jetstack.io\nhelm repo update\n\n# You have to create the namespace before executing following command\nhelm install cert-manager \\\n    --namespace cert-manager \\\n    --version v1.7.2 jetstack/cert-manager\n```\n\n----------------------------------------\n\nTITLE: Defining Storage Configurations for NiFi Nodes in YAML\nDESCRIPTION: This YAML snippet presents an example of storage configuration within a `NodeConfigGroup`. It defines persistent volume claims (PVCs) for various NiFi data directories. Each PVC specifies access modes, resource requests for storage, and storage class name, ensuring data persistence in a Kubernetes environment. The `mountPath` specifies where to persist the volume into the NiFi container.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n...\nstorageConfigs:\n  - mountPath: /opt/nifi/nifi-current/logs\n    name: logs\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/data\n    name: data\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 50Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/extensions\n    name: extensions-repository\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 5Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/flowfile_repository\n    name: flowfile-repository\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/nifi-current/conf\n    name: conf\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 5Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/content_repository\n    name: content-repository\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 500Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/provenance_repository\n    name: provenance-repository\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 500Gi\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Registry Client in YAML\nDESCRIPTION: This YAML snippet defines a `NifiRegistryClient` custom resource. It specifies the API version, kind, and metadata.  The `spec` section configures the client, referencing a NiFi cluster (`clusterRef`) and setting a description and the URI of the NiFi registry.  This is how a NiFi registry client is declared in Kubernetes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/5_references/3_nifi_registry_client.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiRegistryClient\nmetadata:\n  name: squidflow\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Squidflow demo\"\n  uri: \"http://nifi-registry:18080\"\n```\n```\n\n----------------------------------------\n\nTITLE: Defining a NiFi User Group and Its Access Policies in Kubernetes (YAML)\nDESCRIPTION: Specifies a 'NifiUserGroup' Kubernetes CR for grouping multiple NiFi users and managing their collective access policies with the nifikop operator. This YAML format references an existing NiFi cluster and lists users via 'usersRef' (requiring user resource names and, optionally, namespaces). The 'accessPolicies' field allows the group to be granted specific resource permissions. Dependencies: All referenced users must exist as NifiUser resources. Inputs: YAML document linking users and policies. Outputs: Synchronized user group and group policies in NiFi. Commented portions are optional, showing how to further specify component-related parameters.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/3_tasks/4_nifi_user_group.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n    - name: nc-controller.nifikop.mgt.cluster.local\n  accessPolicies:\n    - type: global\n      action: read\n      resource: /counters\n#      componentType: \"process-groups\"\n#      componentId: \"\"\n\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiConnection Resource in YAML\nDESCRIPTION: Defines a `NifiConnection` custom resource named `connection` in the `nifikop` namespace using YAML. This resource establishes a connection from the `output` port of the `input` NifiDataflow to the `input` port of the `output` NifiDataflow. It includes configuration for back pressure thresholds, flow file expiration, label index (`labelIndex`), connection bend points (`bends`), and update strategy (`drain`).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/4_manage_connections/1_deploy_connection.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiConnection\nmetadata:\n  name: connection\n  namespace: nifikop\nspec:\n  source:\n    name: input\n    namespace: nifikop\n    subName: output\n    type: dataflow\n  destination:\n    name: output\n    namespace: nifikop\n    subName: input\n    type: dataflow\n  configuration:\n    backPressureDataSizeThreshold: 100 GB\n    backPressureObjectThreshold: 10000\n    flowFileExpiration: 1 hour\n    labelIndex: 0\n    bends:\n      - posX: 550\n        posY: 550\n      - posX: 550\n        posY: 440\n      - posX: 550\n        posY: 88\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Applying NiFiKop CustomResourceDefinitions Manually - Bash\nDESCRIPTION: This snippet applies the NiFiKop operator's CustomResourceDefinitions (CRDs) manually using kubectl. It is useful when deploying NiFiKop without Helm CRD management (e.g., using the '--skip-crds' flag). The CRDs contain definitions for resources like NiFiClusters, NiFiUsers, and NiFiDataFlows. Prerequisites include kubectl configured to target the desired cluster. The commands ensure Kubernetes is aware of required CRDs before operator components are applied.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/2_deploy_nifikop/1_quick_start.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifinodegroupautoscalers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificonnections.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop - Dry Run\nDESCRIPTION: This command performs a dry run installation of the NiFiKop chart using Helm. It simulates the installation process without actually deploying the chart. This allows you to validate the configuration before applying it to the cluster.  It also sets the log level to Debug and the namespace to \"nifikop\".\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop \\\n    --dry-run \\\n    --set logLevel=Debug \\\n    --set namespaces={\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Defining Istio Gateway for HTTP\nDESCRIPTION: This YAML snippet defines an Istio Gateway resource. The purpose of this Gateway is to intercept HTTP requests for a specific domain (nifi.my-domain.com) on port 80. The 'selector' field specifies the Istio ingress gateway, and the 'servers' section details the port, name (http), protocol (HTTP), and the hosts the gateway should handle traffic for. This Gateway will then be used in a VirtualService to route traffic.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: nifi-gateway\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - nifi.my-domain.com\n```\n```\n\n----------------------------------------\n\nTITLE: Setting Custom Authorizer Property in NiFi Configuration\nDESCRIPTION: A shell command that shows how to set the NiFi property to use the custom database authorizer instead of the default one. This property needs to be configured along with the authorizers.xml template.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/6_users_authorization/1_custom_user_authorizer.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nnifi.security.user.authorizer=custom-database-authorizer\n```\n\n----------------------------------------\n\nTITLE: Defining a Basic NiFi Parameter Context in Kubernetes (YAML)\nDESCRIPTION: This YAML manifest defines a `NifiParameterContext` custom resource named `dataflow-lifecycle`. It links to a NiFi cluster (`nc` in the `nifikop` namespace), references a Kubernetes secret (`secret-params`) for sensitive parameters, and defines both non-sensitive (`test`) and sensitive (`test2`) parameters directly within the spec.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/5_references/4_nifi_parameter_context.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      description: toto\n      sensistive: true\n```\n\n----------------------------------------\n\nTITLE: Defining a NiFi UserGroup Resource in Kubernetes YAML\nDESCRIPTION: This YAML snippet demonstrates how to define a NiFi user group using the 'NifiUserGroup' Custom Resource in Kubernetes for management by the nifikop operator. It specifies metadata such as resource name, the associated cluster reference, user references array, and access policies to be granted. Key fields include 'clusterRef' for identifying the target NiFi cluster, 'usersRef' for listing users in the group, and 'accessPolicies' for configuring allowed operations; dependencies include an initialized Kubernetes cluster with the nifikop CRDs installed. The snippet expects that referenced users and clusters already exist, and outputs a configured user group resource upon application.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/5_references/6_nifi_usergroup.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n    - name: nc-controller.nifikop.mgt.cluster.local\n  accessPolicies:\n    - type: global\n      action: read\n      resource: /counters\n\n```\n\n----------------------------------------\n\nTITLE: Deploying Prometheus Operator with Helm\nDESCRIPTION: Helm command to install the Prometheus Operator with custom configurations, disabling unnecessary components for a minimal deployment focused on monitoring NiFi.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_5\n\nLANGUAGE: console\nCODE:\n```\nhelm install prometheus prometheus/kube-prometheus-stack --namespace monitoring-system \\\n    --set prometheusOperator.createCustomResource=false \\\n    --set prometheusOperator.logLevel=debug \\\n    --set prometheusOperator.alertmanagerInstanceNamespaces=monitoring-system \\\n    --set prometheusOperator.namespaces.additional[0]=monitoring-system \\\n    --set prometheusOperator.prometheusInstanceNamespaces=monitoring-system \\\n    --set prometheusOperator.thanosRulerInstanceNamespaces=monitoring-system \\\n    --set defaultRules.enable=false \\\n    --set alertmanager.enabled=false \\\n    --set grafana.enabled=false \\\n    --set kubeApiServer.enabled=false \\\n    --set kubelet.enabled=false \\\n    --set kubeControllerManager.enabled=false \\\n    --set coreDNS.enabled=false \\\n    --set kubeEtcd.enabled=false \\\n    --set kubeScheduler.enabled=false \\\n    --set kubeProxy.enabled=false \\\n    --set kubeStateMetrics.enabled=false \\\n    --set prometheus.enabled=false\n```\n\n----------------------------------------\n\nTITLE: Declaring External NiFi Cluster with Nifikop Operator - YAML Example\nDESCRIPTION: This YAML manifest defines a 'NifiCluster' custom resource configured as an external cluster for the Nifikop operator. It requires specifying fields such as 'rootProcessGroupId', a hostname URI template for cluster nodes, an array of node IDs (type int32), the cluster type set to 'external', the authentication method (clientType: 'basic' or 'tls'), and a reference to a Kubernetes secret containing credentials. All referenced node IDs must be integers, and the operator requires access to a preconfigured secret for authentication.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/1_manage_clusters/3_external_cluster.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: externalcluster\nspec:\n  # rootProcessGroupId contains the uuid of the root process group for this cluster.\n  rootProcessGroupId: 'd37bee03-017a-1000-cff7-4eaaa82266b7'\n  # nodeURITemplate used to dynamically compute node uri.\n  nodeURITemplate: 'nifi0%d.integ.mapreduce.m0.p.fti.net:9090'\n  # all node requiresunique id\n  nodes:\n    - id: 1\n    - id: 2\n    - id: 3\n  # type defines if the cluster is internal (i.e manager by the operator) or external.\n  # :Enum={\"external\",\"internal\"}\n  type: 'external'\n  # clientType defines if the operator will use basic or tls authentication to query the NiFi cluster.\n  # Enum={\"tls\",\"basic\"}\n  clientType: 'basic'\n  # secretRef reference the secret containing the informations required to authenticate to the cluster.\n  secretRef:\n    name: nifikop-credentials\n    namespace: nifikop-nifi\n```\n\n----------------------------------------\n\nTITLE: Retrieving Kubernetes Services (Console)\nDESCRIPTION: This command retrieves a list of Kubernetes services.  The output shows the service name, type, cluster IP, external IP, ports, and age.  In this case, it shows the `cluster-access` service, which is a LoadBalancer exposing ports 443 and 80.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_3\n\nLANGUAGE: console\nCODE:\n```\nkubectl get services\nNAME                TYPE           CLUSTER-IP    EXTERNAL-IP      PORT(S)                                AGE\ncluster-access      LoadBalancer   10.88.21.98   35.180.241.132   443:30421/TCP,80:30521/TCP             20d\n```\n\n----------------------------------------\n\nTITLE: Applying Kubernetes Custom Resource Definitions (CRDs) for NiFiKop - Bash\nDESCRIPTION: This series of 'kubectl apply' commands registers the Custom Resource Definitions required by NiFiKop. It includes all NiFiKop CRD types such as nificlusters, nifidataflows, nifiparametercontexts, nifiregistryclients, nifiusergroups, and nifiusers. Kubernetes CLI and cluster access with proper permissions are required.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/6_contributing/1_developer_guide.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring External Kubernetes Services for NiFi Cluster Exposure in YAML\nDESCRIPTION: Shows how to expose NiFi cluster internal listeners externally by configuring Kubernetes externalServices with names, service types, and port configurations that map external ports to internal listeners. This snippet demonstrates linking internal listener names to external ports using a LoadBalancer service to expose NiFi UI and custom HTTP endpoints to users or monitoring tools.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    - type: \"https\"\n      name: \"https\"\n      containerPort: 8443\n    - type: \"cluster\"\n      name: \"cluster\"\n      containerPort: 6007\n    - type: \"s2s\"\n      name: \"s2s\"\n      containerPort: 10000\n    - type: \"prometheus\"\n      name: \"prometheus\"\n      containerPort: 9090\n    - type: \"load-balance\"\n      name: \"load-balance\"\n      containerPort: 6342\n    - name: \"http-tracking\"\n      containerPort: 8081\nexternalServices:\n  - name: cluster-access\n    spec:\n      portConfigs:\n        - internalListenerName: https\n          port: 443\n        - internalListenerName: http-tracking\n          port: 80\n      type: LoadBalancer\n```\n\n----------------------------------------\n\nTITLE: Configuring External Kubernetes Services (YAML)\nDESCRIPTION: Example configuration using the `externalServices` field in the NiFiCluster CRD to expose internal listeners to the outside world. This snippet defines a Kubernetes service named `cluster-access` of type `LoadBalancer`. It maps the internal `https` listener (port 8443) to external port 443 and the custom `http-tracking` listener (port 8081) to external port 80. Requires prior definition of the corresponding `internalListeners`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    - type: \"https\"\n      name: \"https\"\n      containerPort: 8443\n    - type: \"cluster\"\n      name: \"cluster\"\n      containerPort: 6007\n    - type: \"s2s\"\n      name: \"s2s\"\n      containerPort: 10000\n    - type: \"prometheus\"\n      name: \"prometheus\"\n      containerPort: 9090\n    - type: \"load-balance\"\n      name: \"load-balance\"\n      containerPort: 6342\n    - name: \"http-tracking\"\n      containerPort: 8081\nexternalServices:\n  - name: cluster-access\n    spec:\n      portConfigs:\n        - internalListenerName: https\n          port: 443\n        - internalListenerName: http-tracking\n          port: 80\n      type: LoadBalancer\n```\n\n----------------------------------------\n\nTITLE: NifiCluster: Update InitContainerImage to bash (YAML)\nDESCRIPTION: This snippet provides the corrected configuration for the NifiCluster custom resource, showing how to update the `initContainerImage` field. For Nifikop v0.15.0 and later, the Zookeeper init container image should be an image containing a bash shell, such as the official `bash` image. This is required if you previously overrode the default.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/7_upgrade_guides/2_v0.14.1_to_v0.15.0.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  initContainerImage:\n    repository: bash\n    tag: \"5.2.2\"\n```\n\n----------------------------------------\n\nTITLE: Adding Helm Repository (console)\nDESCRIPTION: This command adds the official prometheus-community Helm chart repository to your local Helm configuration, allowing you to access and install charts like `kube-prometheus-stack`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_6\n\nLANGUAGE: console\nCODE:\n```\nhelm repo add prometheus https://prometheus-community.github.io/helm-charts\n```\n\n----------------------------------------\n\nTITLE: Applying NiFiKop Custom Resource Definitions (CRDs)\nDESCRIPTION: Manually applies all necessary NiFiKop CustomResourceDefinitions (CRDs) to the Kubernetes cluster using `kubectl apply`. This is required if installing NiFiKop via Helm with the `--skip-crds` flag.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/2_deploy_nifikop/1_quick_start.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing Prometheus Operator with Custom Options - console\nDESCRIPTION: Installs the Prometheus operator Helm chart into the 'monitoring-system' namespace with specific settings tailored for NiFi monitoring. Requires helm, configured repository, and access to the Kubernetes cluster. Adjusts several configuration parameters to disable or customize Prometheus components as required. Parameters like namespaces, logLevel, resource disables, and feature flags can all be customized.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_8\n\nLANGUAGE: console\nCODE:\n```\nhelm install prometheus prometheus/kube-prometheus-stack --namespace monitoring-system \\\n    --set prometheusOperator.createCustomResource=false \\\n    --set prometheusOperator.logLevel=debug \\\n    --set prometheusOperator.alertmanagerInstanceNamespaces=monitoring-system \\\n    --set prometheusOperator.namespaces.additional[0]=monitoring-system \\\n    --set prometheusOperator.prometheusInstanceNamespaces=monitoring-system \\\n    --set prometheusOperator.thanosRulerInstanceNamespaces=monitoring-system \\\n    --set defaultRules.enable=false \\\n    --set alertmanager.enabled=false \\\n    --set grafana.enabled=false \\\n    --set kubeApiServer.enabled=false \\\n    --set kubelet.enabled=false \\\n    --set kubeControllerManager.enabled=false \\\n    --set coreDNS.enabled=false \\\n    --set kubeEtcd.enabled=false \\\n    --set kubeScheduler.enabled=false \\\n    --set kubeProxy.enabled=false \\\n    --set kubeStateMetrics.enabled=false \\\n    --set prometheus.enabled=false\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio DestinationRule for HTTPS Sticky Sessions in NiFi\nDESCRIPTION: Defines an Istio DestinationRule that re-encrypts traffic to HTTPS and configures sticky sessions based on the authorization cookie. This is crucial for NiFi's authentication to work properly through the Istio service mesh.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: DestinationRule\nmetadata:\n  name: nifi-dr\nspec:\n  host: <service-name>.<namespace>.svc.cluster.local\n  trafficPolicy:\n    tls:\n      mode: SIMPLE\n    loadBalancer:\n      consistentHash:\n        httpCookie:\n          name: __Secure-Authorization-Bearer\n          ttl: 0s\n```\n\n----------------------------------------\n\nTITLE: Updating Helm Repository Configuration Console\nDESCRIPTION: This command updates the list of available charts from all configured Helm repositories. This ensures that the latest version of the 'kube-prometheus-stack' chart is available for installation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_7\n\nLANGUAGE: console\nCODE:\n```\nhelm repo update\n```\n\n----------------------------------------\n\nTITLE: Configuring Additional Custom Internal Listeners in YAML\nDESCRIPTION: Example of adding a custom internal listener without a specific NiFi type. This allows exposing custom NiFi processor endpoints, such as HTTP endpoints for receiving requests directly in NiFi.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    ...\n    - name: \"http-tracking\"\n      containerPort: 8081\n```\n\n----------------------------------------\n\nTITLE: Configuring OIDC Identity Mapping in nifi.properties using shell syntax\nDESCRIPTION: This snippet provides the required nifi.properties settings in shell format to customize the Distinguished Name (DN) identity mapping for OpenId Connect support. These settings add or override the mapping pattern, mapping value, and the transform type in nifi.properties. No additional dependencies are needed. Parameters such as 'pattern.dn' and 'value.dn' control how user identities are interpreted and mapped; these must match the expected identity formats in your environment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/5_users_authentication/1_oidc.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nnifi.security.identity.mapping.pattern.dn=CN=([^,]*)(?:, (?:O|OU)=.*)?\nnifi.security.identity.mapping.value.dn=$1\nnifi.security.identity.mapping.transform.dn=NONE\n```\n\n----------------------------------------\n\nTITLE: Templated `authorizers.xml` for Custom Authorizer Configuration (YAML/XML)\nDESCRIPTION: This YAML snippet represents a templated `authorizers.xml` file used by NiFiKOp to configure NiFi authorizers. It includes both the default file-based providers (`FileUserGroupProvider`, `FileAccessPolicyProvider`) and placeholders for a custom `DatabaseAuthorizer`, `DatabaseUserGroupProvider`, and `DatabaseAccessPolicyProvider`. The template uses Go templating syntax (`{{ }}`) to dynamically insert node identities (`.NodeList`), cluster name (`.ClusterName`), namespace (`.Namespace`), and controller user (`.ControllerUser`) provided by NiFiKOp at deployment time. This template must be placed within a Kubernetes ConfigMap or Secret referenced by `Spec.readOnlyConfig.authorizerConfig.replaceTemplateConfigMap` or `Spec.readOnlyConfig.authorizerConfig.replaceTemplateSecretConfig`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/1_manage_clusters/1_deploy_cluster/6_users_authorization/1_custom_user_authorizer.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n{{- $nodeList := .NodeList }}\n{{- $clusterName := .ClusterName }}\n{{- $namespace := .Namespace }}<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>\n<authorizers>\n    <userGroupProvider>\n        <identifier>file-user-group-provider</identifier>\n        <class>org.apache.nifi.authorization.FileUserGroupProvider</class>\n        <property name=\"Users File\">../data/users.xml</property>\n        <property name=\"Legacy Authorized Users File\"></property>\n        <property name=\"Initial User Identity admin\">{{ .ControllerUser }}</property>\n{{- range $i, $host := .NodeList }}\n        <property name=\"Initial User Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n    </userGroupProvider>\n    <userGroupProvider>\n        <identifier>database-user-group-provider</identifier>\n        <class>my.custom.DatabaseUserGroupProvider</class>\n        <!-- Any extra configuration for this provider goes here -->\n{{- range $i, $host := .NodeList }}\n        <property name=\"Initial User Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n    </userGroupProvider>\n    <accessPolicyProvider>\n        <identifier>file-access-policy-provider</identifier>\n        <class>org.apache.nifi.authorization.FileAccessPolicyProvider</class>\n        <property name=\"User Group Provider\">file-user-group-provider</property>\n        <property name=\"Authorizations File\">../data/authorizations.xml</property>\n        <property name=\"Initial Admin Identity\">{{ .ControllerUser }}</property>\n        <property name=\"Legacy Authorized Users File\"></property>\n{{- range $i, $host := .NodeList }}\n        <property name=\"Node Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n\t\t<property name=\"Node Group\"></property>\n    </accessPolicyProvider>\n    <accessPolicyProvider>\n        <identifier>database-access-policy-provider</identifier>\n        <class>my.custom.DatabaseAccessPolicyProvider</class>\n        <!-- Any extra configuration for this provider goes here -->\n{{- range $i, $host := .NodeList }}\n        <property name=\"Node Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n\t\t<property name=\"Node Group\"></property>\n    </accessPolicyProvider>\n    <authorizer>\n        <identifier>managed-authorizer</identifier>\n        <class>org.apache.nifi.authorization.StandardManagedAuthorizer</class>\n        <property name=\"Access Policy Provider\">file-access-policy-provider</property>\n    </authorizer>\n    <authorizer>\n        <identifier>custom-database-authorizer</identifier>\n        <class>my.custom.DatabaseAuthorizer</class>\n        <property name=\"Access Policy Provider\">database-access-policy-provider</property>\n    </authorizer>\n</authorizers>\n```\n\n----------------------------------------\n\nTITLE: Templating NiFi Authorizers Configuration with Go Template\nDESCRIPTION: This code snippet provides an example of a templated `authorizers.xml` configuration suitable for use with NiFiKOp. It includes definitions for both the default file-based authorizer components and custom database-backed components (DatabaseUserGroupProvider, DatabaseAccessPolicyProvider, DatabaseAuthorizer), using Go templating syntax (e.g., `{{- range }}`) to dynamically include node identities.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/3_tasks/2_security/2_authorization/1_custom_authorizer.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n{{- $nodeList := .NodeList }}\n{{- $clusterName := .ClusterName }}\n{{- $namespace := .Namespace }}<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>\n<authorizers>\n    <userGroupProvider>\n        <identifier>file-user-group-provider</identifier>\n        <class>org.apache.nifi.authorization.FileUserGroupProvider</class>\n        <property name=\"Users File\">../data/users.xml</property>\n        <property name=\"Legacy Authorized Users File\"></property>\n        <property name=\"Initial User Identity admin\">{{ .ControllerUser }}</property>\n{{- range $i, $host := .NodeList }}\n        <property name=\"Initial User Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n    </userGroupProvider>\n    <userGroupProvider>\n        <identifier>database-user-group-provider</identifier>\n        <class>my.custom.DatabaseUserGroupProvider</class>\n        <!-- Any extra configuration for this provider goes here -->\n{{- range $i, $host := .NodeList }}\n        <property name=\"Initial User Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n    </userGroupProvider>\n    <accessPolicyProvider>\n        <identifier>file-access-policy-provider</identifier>\n        <class>org.apache.nifi.authorization.FileAccessPolicyProvider</class>\n        <property name=\"User Group Provider\">file-user-group-provider</property>\n        <property name=\"Authorizations File\">../data/authorizations.xml</property>\n        <property name=\"Initial Admin Identity\">{{ .ControllerUser }}</property>\n        <property name=\"Legacy Authorized Users File\"></property>\n{{- range $i, $host := .NodeList }}\n        <property name=\"Node Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n\t\t<property name=\"Node Group\"></property>\n    </accessPolicyProvider>\n    <accessPolicyProvider>\n        <identifier>database-access-policy-provider</identifier>\n        <class>my.custom.DatabaseAccessPolicyProvider</class>\n        <!-- Any extra configuration for this provider goes here -->\n{{- range $i, $host := .NodeList }}\n        <property name=\"Node Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n\t\t<property name=\"Node Group\"></property>\n    </accessPolicyProvider>\n    <authorizer>\n        <identifier>managed-authorizer</identifier>\n        <class>org.apache.nifi.authorization.StandardManagedAuthorizer</class>\n        <property name=\"Access Policy Provider\">file-access-policy-provider</property>\n    </authorizer>\n    <authorizer>\n        <identifier>custom-database-authorizer</identifier>\n        <class>my.custom.DatabaseAuthorizer</class>\n        <property name=\"Access Policy Provider\">database-access-policy-provider</property>\n    </authorizer>\n</authorizers>\n```\n\n----------------------------------------\n\nTITLE: Declaring Nodes and Node Configuration in YAML\nDESCRIPTION: This YAML snippet illustrates how to declare NiFi cluster nodes and associate them with previously defined node configuration groups. It shows how to assign nodes to different groups ('default_group' and 'high_mem_group') based on their resource needs. It also demonstrates how to directly configure a node without using predefined groups.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n- id: 0\n  nodeConfigGroup: \"default_group\"\n- id: 2\n  nodeConfigGroup: \"high_mem_group\"\n- id: 3\n  nodeConfigGroup: \"high_mem_group\"\n- id: 5\n  nodeConfig:\n    resourcesRequirements:\n      limits:\n        cpu: \"2\"\n        memory: 3Gi\n      requests:\n        cpu: \"1\"\n        memory: 1Gi\n```\n\n----------------------------------------\n\nTITLE: AccessPolicyResource Enum Definitions for Resource Targets\nDESCRIPTION: This schema enumerates the various resources within NiFi that can have access policies applied, such as '/flow', '/controller', '/policies', etc., allowing precise permission assignment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/5_references/2_nifi_user.md#_snippet_8\n\nLANGUAGE: YAML\nCODE:\n```\n`AccessPolicyResource:\n  FlowAccessPolicyResource: /flow\n  ControllerAccessPolicyResource: /controller\n  ParameterContextAccessPolicyResource: /parameter-context\n  ProvenanceAccessPolicyResource: /provenance\n  RestrictedComponentsAccessPolicyResource: /restricted-components\n  PoliciesAccessPolicyResource: /policies\n  TenantsAccessPolicyResource: /tenants\n  SiteToSiteAccessPolicyResource: /site-to-site\n  SystemAccessPolicyResource: /system\n  ProxyAccessPolicyResource: /proxy\n  CountersAccessPolicyResource: /counters\n  ComponentsAccessPolicyResource: /\n  OperationAccessPolicyResource: /operation\n  ProvenanceDataAccessPolicyResource: /provenance-data\n  DataAccessPolicyResource: /data\n  PoliciesComponentAccessPolicyResource: /policies\n  DataTransferAccessPolicyResource: /data-transfer`\n```\n\n----------------------------------------\n\nTITLE: Setting NiFi Authorizer Property using Shell Script\nDESCRIPTION: This shell script snippet shows how to set the `nifi.security.user.authorizer` property to use a custom authorizer. The property indicates which of the configured authorizers in the `authorizers.xml` file NiFi should use. Setting it to `custom-database-authorizer` tells NiFi to use the custom database authorizer.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/1_manage_clusters/1_deploy_cluster/6_users_authorization/1_custom_user_authorizer.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nnifi.security.user.authorizer=custom-database-authorizer\n```\n\n----------------------------------------\n\nTITLE: Implementing OIDC in NiFiKop Custom Resource\nDESCRIPTION: Example of a NiFiCluster custom resource configuration that enables OpenId Connect authentication. This snippet shows how to use the overrideConfigs field to specify OIDC-related properties including discovery URL, client ID, client secret, and identity mapping rules.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/5_users_authentication/1_oidc.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\\nkind: NifiCluster\\n...\\nspec:\\n  ...\\n  readOnlyConfig:\\n    # NifiProperties configuration that will be applied to the node.\\n    nifiProperties:\\n      webProxyHosts:\\n        - nifistandard2.trycatchlearn.fr:8443\\n      # Additionnal nifi.properties configuration that will override the one produced based\\n      # on template and configurations.\\n      overrideConfigs: |\\n        nifi.security.user.oidc.discovery.url=<oidc server discovery url>\\n        nifi.security.user.oidc.client.id=<oidc client's id>\\n        nifi.security.user.oidc.client.secret=<oidc client's secret>\\n        nifi.security.identity.mapping.pattern.dn=CN=([^,]*)(?:, (?:O|OU)=.*)?\\n        nifi.security.identity.mapping.value.dn=$1\\n        nifi.security.identity.mapping.transform.dn=NONE\\n      ...\\n   ...\\n...\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFi Cluster on OpenShift with Security Context\nDESCRIPTION: Retrieves OpenShift UID/GID for 'nifi' namespace and modifies the NiFi deployment YAML to include security context using 'sed'. Then deploys the updated configuration to OpenShift. Dependencies include kubectl access and proper namespace annotations.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nuid=$(kubectl get namespace nifi -o=jsonpath='{.metadata.annotations.openshift\\.io/sa\\.scc\\.supplemental-groups}' | sed 's/\\/10000$//' | tr -d '[:space:]')\nsed -i \"s/1000690000/$uid/g\" config/samples/openshift.yaml\nkubectl create -n nifi -f config/samples/openshift.yaml\n```\n\n----------------------------------------\n\nTITLE: PortConfig Schema for Service Ports\nDESCRIPTION: This schema defines individual port configurations for services, including port number, internal listener name, optional node port for external exposure, and network protocol (defaulting to TCP). It supports precise network configuration for service exposure.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/5_references/1_nifi_cluster/7_external_service_config.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n|Field|Type|Description|Required|Default|\n|-----|----|-----------|--------|--------|\n|port|int32|Exposed port number for the service| Yes | - |\n|internalListenerName|string|Name of the internal listener targeted by this port| Yes | - |\n|nodePort|int32|External node port, applicable if service type is NodePort| No | - |\n|protocol|[Protocol](https://pkg.go.dev/k8s.io/api/core/v1#Protocol)|Network protocol, defaults to TCP| No | `TCP` |\n```\n\n----------------------------------------\n\nTITLE: Local Error Handling in Go\nDESCRIPTION: This snippet demonstrates the standard Go pattern for handling errors immediately after they occur.  It checks if an error occurred after calling `doSomething()` and executes error handling logic if `err` is not `nil`. The error handling logic is represented by a comment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/docs/error-handling-guide.md#_snippet_0\n\nLANGUAGE: go\nCODE:\n```\nerr := doSomething()\nif err != nil {\n\t// some handling\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Managed Users and Groups in NiFiCluster Spec (YAML)\nDESCRIPTION: This YAML snippet demonstrates how to define managed admin and reader users within the `NifiCluster.Spec` field. It shows the structure for specifying user identities and names for inclusion in the managed groups. The operator will create NifiUsers based on this configuration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/3_manage_nifi/2_manage_users_and_accesses/3_managed_groups.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  ...\n  oneNifiNodePerNode: false\n  propagateLabels: true\n  managedAdminUsers:\n    -  identity: \"alexandre.guitton@konpyutaika.com\"\n       name: \"aguitton\"\n    -  identity: \"nifiuser@konpyutaika.com\"\n       name: \"nifiuser\"\n  managedReaderUsers:\n    -  identity: \"toto@konpyutaika.com\"\n       name: \"toto\"\n    ...\n```\n\n----------------------------------------\n\nTITLE: Updating Helm Repository\nDESCRIPTION: This command updates the Helm repository to fetch the latest chart versions and dependencies.  It ensures that the user has access to the most recent KEDA chart.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nhelm repo update\n```\n\n----------------------------------------\n\nTITLE: Installing cert-manager with Helm\nDESCRIPTION: Sets up cert-manager using Helm by first applying CRDs, then adding the Jetstack Helm repository, updating it, and installing cert-manager in the 'cert-manager' namespace at version v1.7.2.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/2_setup/1_getting_started.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Install CustomResourceDefinitions first\\nkubectl apply --validate=false -f \\n   https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.crds.yaml\\n\\n# Add the jetstack helm repo\\nhelm repo add jetstack https://charts.jetstack.io\\nhelm repo update\\n\\n# Install cert-manager via Helm\\nhelm install cert-manager \\n    --namespace cert-manager \\n    --version v1.7.2 jetstack/cert-manager\n```\n\n----------------------------------------\n\nTITLE: Configuring DestinationRule for NiFi HTTPS in Istio\nDESCRIPTION: Defines an Istio DestinationRule that redirects HTTP traffic to HTTPS with encryption and manages sticky sessions using cookies.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: DestinationRule\nmetadata:\n  name: nifi-dr\nspec:\n  host: <service-name>.<namespace>.svc.cluster.local\n  trafficPolicy:\n    tls:\n      mode: SIMPLE\n    loadBalancer:\n      consistentHash:\n        httpCookie:\n          name: __Secure-Authorization-Bearer\n          ttl: 0s\n```\n\n----------------------------------------\n\nTITLE: Defining an External NifiCluster Resource in YAML\nDESCRIPTION: YAML configuration for a `NifiCluster` custom resource to manage an external NiFi cluster. It specifies essential fields like `rootProcessGroupId`, `nodeURITemplate` for node discovery, node IDs, `type` set to 'external', `clientType` for authentication ('basic' or 'tls'), and a `secretRef` pointing to the Kubernetes secret containing authentication credentials.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/3_manage_nifi/1_manage_clusters/3_external_cluster.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: externalcluster\nspec:\n  # rootProcessGroupId contains the uuid of the root process group for this cluster.\n  rootProcessGroupId: 'd37bee03-017a-1000-cff7-4eaaa82266b7'\n  # nodeURITemplate used to dynamically compute node uri.\n  nodeURITemplate: 'nifi0%d.integ.mapreduce.m0.p.fti.net:9090'\n  # all node requiresunique id\n  nodes:\n    - id: 1\n    - id: 2\n    - id: 3\n  # type defines if the cluster is internal (i.e manager by the operator) or external.\n  # :Enum={\"external\",\"internal\"}\n  type: 'external'\n  # clientType defines if the operator will use basic or tls authentication to query the NiFi cluster.\n  # Enum={\"tls\",\"basic\"}\n  clientType: 'basic'\n  # secretRef reference the secret containing the informations required to authenticate to the cluster.\n  secretRef:\n    name: nifikop-credentials\n    namespace: nifikop-nifi\n```\n\n----------------------------------------\n\nTITLE: Deploying Prometheus Operator with Helm\nDESCRIPTION: Installs the Prometheus operator with custom configurations using Helm. The installation disables several default components to provide a minimal setup focused on monitoring the NiFi cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_7\n\nLANGUAGE: console\nCODE:\n```\nhelm install prometheus prometheus/kube-prometheus-stack --namespace monitoring-system \\\n    --set prometheusOperator.createCustomResource=false \\\n    --set prometheusOperator.logLevel=debug \\\n    --set prometheusOperator.alertmanagerInstanceNamespaces=monitoring-system \\\n    --set prometheusOperator.namespaces.additional[0]=monitoring-system \\\n    --set prometheusOperator.prometheusInstanceNamespaces=monitoring-system \\\n    --set prometheusOperator.thanosRulerInstanceNamespaces=monitoring-system \\\n    --set defaultRules.enable=false \\\n    --set alertmanager.enabled=false \\\n    --set grafana.enabled=false \\\n    --set kubeApiServer.enabled=false \\\n    --set kubelet.enabled=false \\\n    --set kubeControllerManager.enabled=false \\\n    --set coreDNS.enabled=false \\\n    --set kubeEtcd.enabled=false \\\n    --set kubeScheduler.enabled=false \\\n    --set kubeProxy.enabled=false \\\n    --set kubeStateMetrics.enabled=false \\\n    --set prometheus.enabled=false\n```\n\n----------------------------------------\n\nTITLE: Configuring Data Persistence Storage for NiFi Nodes\nDESCRIPTION: This example provides a comprehensive setup for persistent storage of NiFi node data, defining storage volumes, access modes, storage size, labels, annotations, and mount paths. It ensures data durability across pod restarts and deletions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nstorageConfigs:\n  - mountPath: /opt/nifi/nifi-current/logs\n    name: logs\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/data\n    name: data\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 50Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/extensions\n    name: extensions-repository\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 5Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/flowfile_repository\n    name: flowfile-repository\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/nifi-current/conf\n    name: conf\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 5Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/content_repository\n    name: content-repository\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 500Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/provenance_repository\n    name: provenance-repository\n    reclaimPolicy: Delete\n    metadata:\n      labels:\n        my-label: my-value\n      annotations:\n        my-annotation: my-value\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 500Gi\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Secret for Basic Authentication (kubectl)\nDESCRIPTION: A `kubectl` command to create a Kubernetes secret named `nifikop-credentials` in the `nifikop-nifi` namespace. This secret is referenced by the `NifiCluster` resource when using 'basic' `clientType`. It must contain the `username` and `password` for NiFi API authentication, and optionally a `ca.crt` for trusting the server certificate, loaded from local files.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/3_manage_nifi/1_manage_clusters/3_external_cluster.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nkubectl create secret generic nifikop-credentials \\\n  --from-file=username=./secrets/username\\\n  --from-file=password=./secrets/password\\\n  --from-file=ca.crt=./secrets/ca.crt\\\n  -n nifikop-nifi\n```\n\n----------------------------------------\n\nTITLE: Verifying the NiFiKop Kubectl Plugin Installation\nDESCRIPTION: Command to test the installed NiFiKop kubectl plugin and view the available subcommands. Shows that the plugin provides access to various NiFi resources through kubectl.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/2_deploy_nifikop/3_kubectl_plugin.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\n$ kubectl nifikop\nUsage:\n  nifikop [command]\n\nAvailable Commands:\n  completion          Generate the autocompletion script for the specified shell\n  help                Help about any command\n  nificluster         \n  nificonnection      \n  nifidataflow        \n  nifigroupautoscaler \n  nifiregistryclient  \n  nifiuser            \n  nifiusergroup \n```\n\n----------------------------------------\n\nTITLE: Example package.json for NiFiKop CRD Migration Project - JSON\nDESCRIPTION: A sample package.json file for the migration tool project defines project metadata, keywords, license, dependencies (@kubernetes/client-node and minimist), and scripts including start and test. It serves as a boilerplate ensuring correct dependency versions and script setup.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"nifikop_crd_migration\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Script to migrate from the old CRDs to the new CRDs.\",\n  \"main\": \"index.js\",\n  \"scripts\": {\n    \"start\": \"node --no-warnings index.js\",\n    \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\"\n  },\n  \"keywords\": [\n    \"K8S\",\n    \"NiFiKop\",\n    \"CRDs\"\n  ],\n  \"license\": \"ISC\",\n  \"dependencies\": {\n    \"@kubernetes/client-node\": \"^0.16.3\",\n    \"minimist\": \"^1.2.6\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Using initContainer to update configuration\nDESCRIPTION: This YAML configuration defines an initContainer to automatically update the encryption algorithm during NiFi cluster upgrades. The container uses the `apache/nifi-toolkit:latest` image. It first extracts the sensitive properties key from the nifi.properties file then executes the `encrypt-config.sh` script for the flow.json.gz and flow.xml.gz files. The volumeMounts and mountPath configurations are crucial for accessing the NiFi configuration and data directories within the container. This initContainer should run before the operator and the NiFi cluster are upgraded, and then removed afterward.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/7_upgrade_guides/4_v1.3.1_to_v1.4.0.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ninitContainers:\n  - image: \"apache/nifi-toolkit:latest\"\n    name: nifi-toolkit\n    imagePullPolicy: Always\n    command:\n      - \"sh\"\n      - \"-c\"\n      - \"NIFI_SENSITIVE_PROPS_KEY=$(grep 'nifi.sensitive.props.key' /opt/nifi/nifi-current/conf/nifi.properties | cut -d'=' -f2) && bin/encrypt-config.sh -n /opt/nifi/nifi-current/conf/nifi.properties -f /opt/nifi/data/flow.json.gz -x -A NIFI_PBKDF2_AES_GCM_256 -s $NIFI_SENSITIVE_PROPS_KEY; bin/encrypt-config.sh -n /opt/nifi/nifi-current/conf/nifi.properties -f /opt/nifi/data/flow.xml.gz -x -A NIFI_PBKDF2_AES_GCM_256 -s $NIFI_SENSITIVE_PROPS_KEY\"\n    volumeMounts:\n      - name: data\n        mountPath: /opt/nifi/data\n      - name: conf\n        mountPath: /opt/nifi/nifi-current/conf\n```\n\n----------------------------------------\n\nTITLE: Migrating Operator Custom Resources Using Kubernetes API - Node.js\nDESCRIPTION: This script in Node.js performs automated migration of NiFiKop resources from the old CRD group (nifi.orange.com/v1alpha1) to the new group (nifi.konpyutaika/v1alpha1) in a specified Kubernetes namespace. It establishes a secure API client, identifies resource kinds to migrate, reconstructs resources with only relevant fields, and transfers their statuses. Dependencies include @kubernetes/client-node and minimist. Run with npm start, specifying --type for resource kind (e.g., cluster, dataflow) and --namespace for the Kubernetes namespace. Script expects sufficient permissions on the target cluster and requires operator downtime to prevent concurrent mutations.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nprocess.env['NODE_TLS_REJECT_UNAUTHORIZED'] = 0;\nconst k8s = require('@kubernetes/client-node');\n\nconst kc = new k8s.KubeConfig();\nkc.loadFromDefault();\n\nconst k8sApi = kc.makeApiClient(k8s.CustomObjectsApi);\n\nconst KONPYUTAIKA_GROUP = 'nifi.konpyutaika.com';\nconst KONPYUTAIKA_GROUP_VERSION = 'v1alpha1';\nconst ORANGE_GROUP = 'nifi.orange.com';\nconst ORANGE_GROUP_VERSION = 'v1alpha1';\n\nconst call = async (SRC_GRP, SRC_GRP_VER, DST_GRP, DST_GRP_VER, KIND_PLURAL, NAMESPACE) => {\n\tconsole.log(`Listing ${KIND_PLURAL} of ${SRC_GRP}/${SRC_GRP_VER} in ${NAMESPACE}...`);\n\tconst listResources = (await k8sApi.listNamespacedCustomObject(SRC_GRP, SRC_GRP_VER, NAMESPACE, KIND_PLURAL)).body.items;\n\treturn Promise.all(listResources.map(async (resource) => {\n\t\ttry {\n\t\t\tconsole.log(`Found ${resource.kind} \\\"${resource.metadata.name}\\\" of ${resource.apiVersion} in ${NAMESPACE}`);\n\n\t\t\tif (resource.metadata.ownerReferences) {\n\t\t\t\tconsole.log(`${resource.kind} ${resource.metadata.name} mananged by something else (ownerRefereces is set).`);\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tconst bodyResource = {\n\t\t\t\tapiVersion: `${DST_GRP}/${DST_GRP_VER}`,\n\t\t\t\tkind: resource.kind,\n\t\t\t\tmetadata: {\n\t\t\t\t\tname: resource.metadata.name,\n\t\t\t\t\tannotations: resource.metadata.annotations,\n\t\t\t\t\tlabels: resource.metadata.labels\n\t\t\t\t},\n\t\t\t\tspec: resource.spec\n\t\t\t};\n\n\t\t\tconsole.log(`Creating ${bodyResource.kind} \\\"${bodyResource.metadata.name}\\\" of ${bodyResource.apiVersion} in ${NAMESPACE}...`);\n\t\t\tconst newResource = (await k8sApi.createNamespacedCustomObject(DST_GRP, DST_GRP_VER, NAMESPACE, KIND_PLURAL, bodyResource)).body;\n\t\t\tconsole.log('...done creating.');\n\n\t\t\tconst bodyStatus = {\n\t\t\t\tapiVersion: newResource.apiVersion,\n\t\t\t\tkind: newResource.kind,\n\t\t\t\tmetadata: {\n\t\t\t\t\tname: newResource.metadata.name,\n\t\t\t\t\tresourceVersion: newResource.metadata.resourceVersion\n\t\t\t\t},\n\t\t\t\tstatus: resource.status\n\t\t\t};\n\n\t\t\tconsole.log(`Copying status from ${resource.kind} \\\"${resource.metadata.name}\\\" of ${newResource.apiVersion} to ${newResource.kind} \\\"${newResource.metadata.name}\\\" of ${newResource.apiVersion} in ${NAMESPACE}...`);\n\t\t\tconst newResourceWithStatus = (await k8sApi.replaceNamespacedCustomObjectStatus(DST_GRP, DST_GRP_VER, NAMESPACE, KIND_PLURAL, bodyStatus.metadata.name, bodyStatus)).body;\n\t\t\tconsole.log('...done copying.');\n\t\t\treturn newResourceWithStatus;\n\t\t}\n\t\tcatch (e) {\n\t\t\tconsole.error(e.body ? e.body.message ? e.body.message : e.body : e);\n\t\t}\n\t}));\n};\n\nconst argv = require('minimist')(process.argv.slice(2));\n\nlet NAMESPACE = argv.namespace ? argv.namespace.length > 0 ? argv.namespace : 'default' : 'default';\nlet KIND_PLURAL = {\n\tcluster: 'nificlusters',\n\tdataflow: 'nifidataflows',\n\tparametercontext: 'nifiparametercontexts',\n\tregistryclient: 'nifiregistryclients',\n\tuser: 'nifiusers',\n\tusergroup: 'nifiusergroups',\n};\n\nif (!argv.type) {\n\tconsole.error('Type not provided');\n\tprocess.exit(1);\n}\n\nif (!KIND_PLURAL[argv.type]) {\n\tconsole.error(`Type ${argv.type} is not one of the following types: ${Object.keys(KIND_PLURAL)}`);\n\tprocess.exit(1);\n}\n\nconsole.log(`########### START: ${KIND_PLURAL[argv.type]} ###########`);\ncall( ORANGE_GROUP, ORANGE_GROUP_VERSION, KONPYUTAIKA_GROUP, KONPYUTAIKA_GROUP_VERSION, KIND_PLURAL[argv.type], NAMESPACE)\n\t.then(r => console.log('############ END ############'))\n\t.catch(e => console.error(e));\n```\n\n----------------------------------------\n\nTITLE: Defining a NiFi Dataflow Resource ('input') in YAML\nDESCRIPTION: Defines a `NifiDataflow` custom resource named 'input' in the 'nifikop' namespace using YAML for Kubernetes. This dataflow references a NiFi cluster (`nc`), a specific flow version (1) from a NiFi Registry bucket, and sets synchronization/update strategies. It serves as the source dataflow for the connection example and requires an output port.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/4_manage_connections/1_deploy_connection.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiDataflow\nmetadata:\n  name: input\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  bucketId: deedb9f6-65a4-44e9-a1c9-722008fcd0ba\n  flowId: ab95431d-980d-41bd-904a-fac4bd864ba6\n  flowVersion: 1\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  skipInvalidComponent: true\n  skipInvalidControllerService: true\n  syncMode: always\n  updateStrategy: drain\n  flowPosition:\n    posX: 0\n    posY: 0\n```\n\n----------------------------------------\n\nTITLE: Creating a NifiUser Resource using kubectl (Shell)\nDESCRIPTION: Shell command using `kubectl apply` to create a `NifiUser` custom resource named `example-client` in the `nifi` namespace. This resource instructs the NiFi operator to generate SSL client certificates signed by the cluster's CA (referenced by `clusterRef`) and store them in a Kubernetes secret named `example-client-secret`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_3\n\nLANGUAGE: console\nCODE:\n```\ncat << EOF | kubectl apply -n nifi -f -\napiVersion:  nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: example-client\n  namespace: nifi\nspec:\n  clusterRef:\n    name: nifi\n  secretName: example-client-secret\nEOF\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiConnection Resource in YAML\nDESCRIPTION: This YAML manifest defines a `NifiConnection` Kubernetes custom resource named 'connection' within the 'instances' namespace. It establishes a connection between a source component ('input', subName 'output_1') and a destination component ('output', subName 'input_1'), both of type 'dataflow' within the same namespace. The configuration section specifies details like flow file expiration, back pressure thresholds, load balancing strategy (partition by attribute 'partition_attribute'), flow file prioritizers, and visual layout properties (bends, label index).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/5_references/8_nifi_connection.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiConnection\nmetadata:\n  name: connection\n  namespace: instances\nspec:\n  source:\n    name: input\n    namespace: instances\n    subName: output_1\n    type: dataflow\n  destination:\n    name: output\n    namespace: instances\n    subName: input_1\n    type: dataflow\n  configuration:\n    flowFileExpiration: 1 hour\n    backPressureDataSizeThreshold: 100 GB\n    backPressureObjectThreshold: 10000\n    loadBalanceStrategy: PARTITION_BY_ATTRIBUTE\n    loadBalancePartitionAttribute: partition_attribute\n    loadBalanceCompression: DO_NOT_COMPRESS\n    prioritizers: \n      - NewestFlowFileFirstPrioritizer\n      - FirstInFirstOutPrioritizer\n    labelIndex: 0\n    bends:\n      - posX: 550\n        posY: 550\n      - posX: 550\n        posY: 440\n      - posX: 550\n        posY: 88\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Install cert-manager CRDs Separately (bash)\nDESCRIPTION: Applies only the cert-manager CustomResourceDefinitions (CRDs) using a manifest file from the official Jetstack GitHub repository. This step is necessary when installing cert-manager via Helm with --skip-crds or when installing CRDs manually beforehand. Requires kubectl.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/2_deploy_nifikop/1_quick_start.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply --validate=false -f \\\n   https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.crds.yaml\n```\n\n----------------------------------------\n\nTITLE: Applying NiFiKop Operator CustomResourceDefinitions (CRDs) Manually Using Bash\nDESCRIPTION: This bash snippet manually applies multiple custom resource definition (CRD) manifests to the Kubernetes cluster to enable NiFiKop operator functionalities. Each kubectl apply command references raw YAML resources for different NiFiKop custom resource types such as nificlusters, nifiusers, and parameter contexts hosted on the nifikop GitHub repository. kubectl must be configured and connected to the target cluster. This method is recommended when opting not to deploy CRDs via Helm (`--skip-crds` flag).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/2_deploy_nifikop/1_quick_start.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\n```\n\n----------------------------------------\n\nTITLE: Re-encrypting NiFi Flow Configuration using encrypt-config.sh\nDESCRIPTION: Demonstrates using the NiFi Encrypt-Config Tool (`encrypt-config.sh`) via shell commands to update the encryption algorithm for sensitive properties in `flow.xml.gz` and `flow.json.gz` files to `NIFI_PBKDF2_AES_GCM_256`. Requires the NiFi Toolkit, the `nifi.properties` file, the flow configuration files, and the sensitive properties key (`PROPERTIES_KEY`).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/7_upgrade_guides/4_v1.3.1_to_v1.4.0.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nencrypt-config.sh -n nifi.properties -f flow.xml.gz -x -s PROPERTIES_KEY -A NIFI_PBKDF2_AES_GCM_256\nencrypt-config.sh -n nifi.properties -f flow.json.gz -x -s PROPERTIES_KEY -A NIFI_PBKDF2_AES_GCM_256\n```\n\n----------------------------------------\n\nTITLE: Purging Helm Release\nDESCRIPTION: Purge a specific Helm release. This command deletes the release and removes its history from Helm's storage, making the release name available for reuse. Unlike simple deletion (`helm del`), purging completely removes the release record.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nhelm delete --purge nifikop\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio HTTPS VirtualService to Service YAML\nDESCRIPTION: This YAML snippet defines an Istio VirtualService complementing the HTTPS Gateway. It routes the traffic, which is now HTTP after gateway termination, for the specified host and path prefix (`/`) to the target Kubernetes service host and its HTTPS port (8443), allowing the destination service (NiFi) to handle TLS re-encryption.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: nifi-vs\nspec:\n  gateways:\n  - nifi-gateway\n  hosts:\n  - nifi.my-domain.com\n  http:\n  - match:\n    - uri:\n        prefix: /\n    route:\n    - destination:\n        host: <service-name>.<namespace>.svc.cluster.local\n        port:\n          number: 8443\n```\n\n----------------------------------------\n\nTITLE: Extracting NiFi User Credentials from Secret (Console)\nDESCRIPTION: These console commands use `kubectl get secret` with `jsonpath` to extract the base64-encoded CA certificate, user certificate, and user private key from a Kubernetes Secret created by the NifiUser resource. The output is then piped to `base64 -d` to decode the content and save it to local files.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['ca\\.crt']}\" | base64 -d > ca.crt\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['tls\\.crt']}\" | base64 -d > tls.crt\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['tls\\.key']}\" | base64 -d > tls.key\n```\n\n----------------------------------------\n\nTITLE: Defining Node Configuration in YAML for Nifi Kubernetes Deployment\nDESCRIPTION: This YAML snippet illustrates the structure of a node configuration object in Kubernetes for Nifi, including parameters such as provenance storage, user IDs, node roles, metadata, image pull policies, external volumes, and storage configurations. It specifies how nodes are configured to operate within a cluster, including storage paths, volume reclaim policies, and node affinity settings.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/5_references/1_nifi_cluster/3_node_config.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n   default_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      # Set this to true if the instance is a node in a cluster.\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#basic-cluster-setup\n      isNode: true\n      # Additionnal metadata to merge to the pod associated\n      podMetadata:\n        annotations:\n          node-annotation: \"node-annotation-value\"\n        labels:\n          node-label: \"node-label-value\"\n      # Docker image used by the operator to create the node associated\n      # https://hub.docker.com/r/apache/nifi/\n#      image: \"apache/nifi:1.11.2\"\n      # nodeAffinity can be specified, operator populates this value if new pvc added later to node\n      # https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity\n#      nodeAffinity:\n      # imagePullPolicy define the pull policy for NiFi cluster docker image\n      imagePullPolicy: IfNotPresent\n      # priorityClassName define the name of the priority class to be applied to these nodes\n      priorityClassName: \"example-priority-class-name\"\n      # externalVolumeConfigs specifies a list of volume to mount into the main container.\n      externalVolumeConfigs:\n        - name: example-volume\n          mountPath: \"/opt/nifi/example\"\n          secret:\n            secretName: \"raw-controller\"\n      # storageConfigs specifies the node related configs\n      storageConfigs:\n        # Name of the storage config, used to name PV to reuse into sidecars for example.\n        - name: provenance-repository\n          # Retain this PVC throughout NifiCluster deletions.\n          reclaimPolicy: Retain\n          # Path where the volume will be mount into the main nifi container inside the pod.\n          mountPath: \"/opt/nifi/provenance_repository\"\n          # Metadata to attach to the PVC that gets created\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          # Kubernetes PVC spec\n          # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          reclaimPolicy: Delete\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n```\n\n----------------------------------------\n\nTITLE: NiFiCluster SSL Configuration (YAML)\nDESCRIPTION: This YAML snippet demonstrates how to configure SSL for a NiFi cluster using the NiFi operator. It shows how to define listeners, specify SSL secrets, and configure web proxy hosts. `tlsSecretName` specifies the name of the Kubernetes secret containing the SSL certificates, and `create: true` instructs the operator to create these certificates.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  readOnlyConfig:\n    # NifiProperties configuration that will be applied to the node.\n    nifiProperties:\n      webProxyHosts:\n        - nifistandard2.trycatchlearn.fr:8443\n        ...\n  ...\n  listenersConfig:\n    internalListeners:\n      - type: \"https\"\n        name: \"https\"\n        containerPort: 8443\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n```\n\n----------------------------------------\n\nTITLE: Secret Content Specification for TLS Authentication in NiFi\nDESCRIPTION: No code snippet provided; the documentation specifies the secrets required for TLS authentication, including TLS key, certificate, CA certificate, password, and key/trust stores. These secrets are referenced by the NifiCluster resource to enable secure TLS communication with the external NiFi cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/3_tasks/1_nifi_cluster/4_external_cluster.md#_snippet_2\n\n\n\n----------------------------------------\n\nTITLE: Installing cert-manager via Helm 3\nDESCRIPTION: This snippet provides commands to install cert-manager using Helm 3, including adding the Jetstack repository, applying CRDs separately, creating the namespace, and deploying cert-manager helm chart. It requires Helm 3 setup and cluster access.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/2_deploy_nifikop/1_quick_start.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Install CustomResourceDefinitions first\nkubectl apply --validate=false -f \\ \n   https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.crds.yaml\n\n# Add the jetstack helm repo\nhelm repo add jetstack https://charts.jetstack.io\nhelm repo update\n\n# You have to create the namespace before executing following command\nhelm install cert-manager \\\n    --namespace cert-manager \\\n    --version v1.7.2 jetstack/cert-manager\n```\n\n----------------------------------------\n\nTITLE: Configuring NifiCluster InitContainerImage in YAML\nDESCRIPTION: This YAML snippet shows an example of how a `NifiCluster` resource might be configured, including specifying the `initContainerImage`.  The example sets the image repository to `busybox` and tag to `1.34.0`.  If this configuration is in use and a version upgrade is done, this needs to be changed to bash or an image that provides bash.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/7_upgrade_guides/2_v0.14.1_to_v0.15.0.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  initContainerImage:\n    repository: busybox\n    tag: \"1.34.0\"\n```\n\n----------------------------------------\n\nTITLE: Updating Helm Repo\nDESCRIPTION: This command updates the Helm repository.  It fetches the latest chart information from all configured Helm repositories, including the one added for KEDA. This keeps your local Helm charts up-to-date before installation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nhelm repo update\n```\n\n----------------------------------------\n\nTITLE: Kubectl Get Nodes\nDESCRIPTION: This command retrieves a list of nodes in the Kubernetes cluster, providing information about their status, roles, and version. It's used to verify that the GKE cluster has been successfully deployed and that nodes are in a ready state. This is a diagnostic check to ensure the cluster nodes are functional.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/blog/2020-06-30-secured_nifi_cluster_on_gcp_with_external_dns.md#_snippet_3\n\nLANGUAGE: console\nCODE:\n```\nkubectl get nodes\n```\n\n----------------------------------------\n\nTITLE: Creating a Kubernetes StorageClass for Dynamic Volume Provisioning\nDESCRIPTION: Defines a custom StorageClass named 'exampleStorageclass' with 'WaitForFirstConsumer' volume binding mode, suitable for dynamic volume provisioning tailored to specific storage backend requirements. Dependencies include a Kubernetes cluster with storage provisioner support.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: exampleStorageclass\nparameters:\n  type: pd-standard\nprovisioner: kubernetes.io/gce-pd\nreclaimPolicy: Delete\nvolumeBindingMode: WaitForFirstConsumer\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop Operator via Helm Chart\nDESCRIPTION: Command to install NiFiKop Helm chart with specified image tag and namespace, deploying the operator onto Kubernetes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/6_contributing/1_developer_guide.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nhelm install skeleton ./helm/nifikop \\\n    --set image.tag=v0.5.1-release \\\n    --namespace-{\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Automatic Creation of NifiUser Resources Based on Managed Users\nDESCRIPTION: Upon applying the YAML configuration, the operator creates NifiUser resources for each listed user, assigning the specified identities and names. For example, users 'aguitton', 'nifiuser', and 'toto' are instantiated with their corresponding identities, facilitating managed access control in the NiFi cluster. These resource creations are crucial for role-based access management and are handled automatically by the operator.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/3_manage_nifi/2_manage_users_and_accesses/3_managed_groups.md#_snippet_1\n\n\n\n----------------------------------------\n\nTITLE: YAML Configuration for Securing NiFi Cluster with SSL\nDESCRIPTION: Defines a NiFiCluster resource with annotations for SSL, including listener configurations, SSL secrets, and proxy host settings. Dependencies include the Nifikop operator and Kubernetes secrets. Key parameters include sslSecrets.create (to generate or use existing secrets), tlsSecretName, and listener ports. The snippet facilitates setting up HTTPS and secure communication pathways.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  readOnlyConfig:\n    # NifiProperties configuration that will be applied to the node.\n    nifiProperties:\n      webProxyHosts:\n        - nifistandard2.trycatchlearn.fr:8443\n      ...\n  ...\n  listenersConfig:\n    internalListeners:\n      - type: \"https\"\n        name: \"https\"\n        containerPort: 8443\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n```\n\n----------------------------------------\n\nTITLE: Configure Recommended OIDC Identity Mapping Properties | Shell\nDESCRIPTION: Provides recommended `nifi.properties` configuration properties for OpenID Connect to enhance support for multiple identity providers. These settings define a pattern and value transformation for mapping distinguished names (DNs) to NiFi user identities, typically extracting the CN (Common Name).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/1_manage_clusters/1_deploy_cluster/5_users_authentication/1_oidc.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nnifi.security.identity.mapping.pattern.dn=CN=([^,]*)(?:, (?:O|OU)=.*)?\nnifi.security.identity.mapping.value.dn=$1\nnifi.security.identity.mapping.transform.dn=NONE\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom NiFi Authorizers Template (YAML/Go Template)\nDESCRIPTION: This YAML snippet, processed as a Go template, defines the `authorizers.xml` configuration for NiFi managed by NiFiKOp. It includes both the standard file-based authorizers and a custom `DatabaseAuthorizer` example, dynamically populating initial user/node identities based on the NiFiKOp deployment context (`.NodeList`, `.ControllerUser`). This template is intended to replace the default configuration via `Spec.readOnlyConfig.authorizerConfig.replaceTemplateConfigMap` or `Spec.readOnlyConfig.authorizerConfig.replaceTemplateSecretConfig`, requiring the custom Java classes (e.g., `my.custom.DatabaseUserGroupProvider`) to be on NiFi's classpath.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/6_users_authorization/1_custom_user_authorizer.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n{{- $nodeList := .NodeList }}\n{{- $clusterName := .ClusterName }}\n{{- $namespace := .Namespace }}<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>\n<authorizers>\n    <userGroupProvider>\n        <identifier>file-user-group-provider</identifier>\n        <class>org.apache.nifi.authorization.FileUserGroupProvider</class>\n        <property name=\"Users File\">../data/users.xml</property>\n        <property name=\"Legacy Authorized Users File\"></property>\n        <property name=\"Initial User Identity admin\">{{ .ControllerUser }}</property>\n{{- range $i, $host := .NodeList }}\n        <property name=\"Initial User Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n    </userGroupProvider>\n    <userGroupProvider>\n        <identifier>database-user-group-provider</identifier>\n        <class>my.custom.DatabaseUserGroupProvider</class>\n        <!-- Any extra configuration for this provider goes here -->\n{{- range $i, $host := .NodeList }}\n        <property name=\"Initial User Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n    </userGroupProvider>\n    <accessPolicyProvider>\n        <identifier>file-access-policy-provider</identifier>\n        <class>org.apache.nifi.authorization.FileAccessPolicyProvider</class>\n        <property name=\"User Group Provider\">file-user-group-provider</property>\n        <property name=\"Authorizations File\">../data/authorizations.xml</property>\n        <property name=\"Initial Admin Identity\">{{ .ControllerUser }}</property>\n        <property name=\"Legacy Authorized Users File\"></property>\n{{- range $i, $host := .NodeList }}\n        <property name=\"Node Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n\t\t<property name=\"Node Group\"></property>\n    </accessPolicyProvider>\n    <accessPolicyProvider>\n        <identifier>database-access-policy-provider</identifier>\n        <class>my.custom.DatabaseAccessPolicyProvider</class>\n        <!-- Any extra configuration for this provider goes here -->\n{{- range $i, $host := .NodeList }}\n        <property name=\"Node Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n\t\t<property name=\"Node Group\"></property>\n    </accessPolicyProvider>\n    <authorizer>\n        <identifier>managed-authorizer</identifier>\n        <class>org.apache.nifi.authorization.StandardManagedAuthorizer</class>\n        <property name=\"Access Policy Provider\">file-access-policy-provider</property>\n    </authorizer>\n    <authorizer>\n        <identifier>custom-database-authorizer</identifier>\n        <class>my.custom.DatabaseAuthorizer</class>\n        <property name=\"Access Policy Provider\">database-access-policy-provider</property>\n    </authorizer>\n</authorizers>\n```\n\n----------------------------------------\n\nTITLE: Verify New Node Resources - Shell\nDESCRIPTION: This shell command retrieves the pods, configmaps, and PVCs associated with the newly added NiFi node (nodeId=25) in the Kubernetes cluster. It uses `kubectl get` with label selectors to filter the resources. The output shows the status and configuration of the new node's resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/3_tasks/1_nifi_cluster/2_cluster_scaling.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl get pods,configmap,pvc -l nodeId=25\nNAME                          READY   STATUS    RESTARTS   AGE\npod/simplenifi-25-nodem5jh4   1/1     Running   0          11m\n\nNAME                             DATA   AGE\nconfigmap/simplenifi-config-25   7      11m\n\nNAME                                               STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\npersistentvolumeclaim/simplenifi-25-storagehwn24   Bound    pvc-7da86076-728e-11ea-846d-42010a8400f2   10Gi       RWO            standard       11m\n```\n\n----------------------------------------\n\nTITLE: Configuring Authorizers XML Template\nDESCRIPTION: This YAML snippet shows a templated authorizers.xml file that can be used to configure a custom authorizer in NiFiKop. It includes configurations for both file-based and database-based user group and access policy providers. The template utilizes Go templating to dynamically inject node identities and controller user during NiFiKop deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/3_tasks/2_security/2_authorization/1_custom_authorizer.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n{{- $nodeList := .NodeList }}\n{{- $clusterName := .ClusterName }}\n{{- $namespace := .Namespace }}<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>\n<authorizers>\n    <userGroupProvider>\n        <identifier>file-user-group-provider</identifier>\n        <class>org.apache.nifi.authorization.FileUserGroupProvider</class>\n        <property name=\"Users File\">../data/users.xml</property>\n        <property name=\"Legacy Authorized Users File\"></property>\n        <property name=\"Initial User Identity admin\">{{ .ControllerUser }}</property>\n{{- range $i, $host := .NodeList }}\n        <property name=\"Initial User Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n    </userGroupProvider>\n    <userGroupProvider>\n        <identifier>database-user-group-provider</identifier>\n        <class>my.custom.DatabaseUserGroupProvider</class>\n        <!-- Any extra configuration for this provider goes here -->\n{{- range $i, $host := .NodeList }}\n        <property name=\"Initial User Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n    </userGroupProvider>\n    <accessPolicyProvider>\n        <identifier>file-access-policy-provider</identifier>\n        <class>org.apache.nifi.authorization.FileAccessPolicyProvider</class>\n        <property name=\"User Group Provider\">file-user-group-provider</property>\n        <property name=\"Authorizations File\">../data/authorizations.xml</property>\n        <property name=\"Initial Admin Identity\">{{ .ControllerUser }}</property>\n        <property name=\"Legacy Authorized Users File\"></property>\n{{- range $i, $host := .NodeList }}\n        <property name=\"Node Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n\t\t<property name=\"Node Group\"></property>\n    </accessPolicyProvider>\n    <accessPolicyProvider>\n        <identifier>database-access-policy-provider</identifier>\n        <class>my.custom.DatabaseAccessPolicyProvider</class>\n        <!-- Any extra configuration for this provider goes here -->\n{{- range $i, $host := .NodeList }}\n        <property name=\"Node Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n\t\t<property name=\"Node Group\"></property>\n    </accessPolicyProvider>\n    <authorizer>\n        <identifier>managed-authorizer</identifier>\n        <class>org.apache.nifi.authorization.StandardManagedAuthorizer</class>\n        <property name=\"Access Policy Provider\">file-access-policy-provider</property>\n    </authorizer>\n    <authorizer>\n        <identifier>custom-database-authorizer</identifier>\n        <class>my.custom.DatabaseAuthorizer</class>\n        <property name=\"Access Policy Provider\">database-access-policy-provider</property>\n    </authorizer>\n</authorizers>\n\n```\n\n----------------------------------------\n\nTITLE: Defining ReadOnlyConfig in YAML - NiFiKop\nDESCRIPTION: This YAML snippet defines the structure and fields of the ReadOnlyConfig object in NiFiKop, which is used to specify read-only NiFi configurations at the cluster level. It includes configurations for thread counts, logback, authorizer, nifi properties, zookeeper, and bootstrap properties. It demonstrates how to override default configurations using configmaps, secrets, and inline configurations.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/5_references/1_nifi_cluster/2_read_only_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreadOnlyConfig:\n  # MaximumTimerDrivenThreadCount define the maximum number of threads for timer driven processors available to the system.\n  maximumTimerDrivenThreadCount: 30\n  # MaximumEventDrivenThreadCount define the maximum number of threads for event driven processors available to the system (@DEPRECATED. This has no effect from NiFiKOp v1.9.0 or later).\n  maximumEventDrivenThreadCount: 10\n  # Logback configuration that will be applied to the node\n  logbackConfig:\n    # logback.xml configuration that will replace the one produced based on template\n    replaceConfigMap:\n      # The key of the value,in data content, that we want use.\n      data: logback.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # logback.xml configuration that will replace the one produced based on template and overrideConfigMap\n    replaceSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: logback.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n  # Authorizer configuration that will be applied to the node\n  authorizerConfig:\n    # An authorizers.xml configuration template that will replace the default template seen in authorizers.go\n    replaceTemplateConfigMap:\n      # The key of the value, in data content, that we want use.\n      data: authorizers.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # An authorizers.xml configuration template that will replace the default template seen in authorizers.go and the replaceTemplateConfigMap\n    replaceTemplateSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: authorizers.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n  # NifiProperties configuration that will be applied to the node.\n  nifiProperties:\n    # Additionnal nifi.properties configuration that will override the one produced based on template and\n    # configuration\n    overrideConfigMap:\n      # The key of the value,in data content, that we want use.\n      data: nifi.properties\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop.\n    # Additionnal nifi.properties configuration that will override the one produced based\n    #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    overrideSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: nifi.properties\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # Additionnal nifi.properties configuration that will override the one produced based\n    #\ton template, configurations and overrideConfigMap\n    overrideConfigs: |\n      nifi.ui.banner.text=NiFiKop\n    # A comma separated list of allowed HTTP Host header values to consider when NiFi\n    # is running securely and will be receiving requests to a different host[:port] than it is bound to.\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#web-properties\n    #      webProxyHosts:\n    # Nifi security client auth\n    needClientAuth: false\n    # Indicates which of the configured authorizers in the authorizers.xml file to use\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#authorizer-configuration\n  #      authorizer:\n  # ZookeeperProperties configuration that will be applied to the node.\n  zookeeperProperties:\n    #      # Additionnal zookeeeper.properties configuration that will override the one produced based on template and\n    #      # configuration\n    #      overrideConfigMap:\n    #        # The key of the value,in data content, that we want use.\n    #        data: zookeeeper.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop.\n    #      # Additionnal zookeeeper.properties configuration that will override the one produced based\n    #      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    #      overrideSecretConfig:\n    #        # The key of the value,in data content, that we want use.\n    #        data: zookeeeper.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop\n    # Additionnal zookeeper.properties configuration that will override the one produced based\n    # on template and configurations.\n    overrideConfigs: |\n      initLimit=15\n      autopurge.purgeInterval=24\n      syncLimit=5\n      tickTime=2000\n      dataDir=./state/zookeeper\n      autopurge.snapRetainCount=30\n  # BootstrapProperties configuration that will be applied to the node.\n  bootstrapProperties:\n    #      # Additionnal bootstrap.conf configuration that will override the one produced based on template and\n    #      # configuration\n    #      overrideConfigMap:\n    #        # The key of the value,in data content, that we want use.\n    #        data: bootstrap.conf\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop.\n    #      # Additionnal bootstrap.conf configuration that will override the one produced based\n    #      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    #      overrideSecretConfig:\n    #        # The key of the value,in data content, that we want use.\n    #        data: bootstrap.conf\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop\n    # JVM memory settings\n    nifiJvmMemory: \"512m\"\n    # Additionnal bootstrap.conf configuration that will override the one produced based\n    # on template and configurations.\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#bootstrap_properties\n    overrideConfigs: |\n      # java.arg.4=-Djava.net.preferIPv4Stack=true\n```\n\n----------------------------------------\n\nTITLE: Cloning NiFiKop Repository (Bash)\nDESCRIPTION: Clones the NiFiKop project source code from the specified GitHub repository and navigates into the project directory. This is the first step in setting up the development environment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/6_contributing/1_developer_guide.md#_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\ngit clone https://github.com/konpyutaika/nifikop.git\ncd nifikop\n```\n\n----------------------------------------\n\nTITLE: Defining NifiDataflow Resource with YAML\nDESCRIPTION: This YAML configures a NifiDataflow resource, the central object for deploying and managing a versioned NiFi dataflow with NiFiKop. Required fields include apiVersion, kind, metadata (name), and spec with parentProcessGroupID, bucketId, flowId, flowVersion, syncMode, updateStrategy, and references to cluster, registryClient, and parameterContext. Correct IDs are necessary; refer to documentation for acquiring versioned flow info. Flags like skipInvalidControllerService and skipInvalidComponent assist in handling problematic components during rollout.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: demo\n  updateStrategy: drain\n\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop Helm Chart without CRDs\nDESCRIPTION: This command installs the NiFiKop Helm chart while skipping the installation of CRDs. This is useful if the CRDs are already deployed or if you want to avoid modifying existing CRDs. The `--set namespaces` parameter specifies the namespaces for the operator.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/helm/nifikop/README.md#_snippet_10\n\nLANGUAGE: Shell\nCODE:\n```\n$ helm install --name nifikop ./helm/nifikop --set namespaces={\"nifikop\"} --skip-crds\n```\n\n----------------------------------------\n\nTITLE: Updating NifiCluster InitContainerImage in YAML\nDESCRIPTION: This YAML snippet shows the updated configuration for `NifiCluster` to use `bash` or another image with a bash shell, if one has overridden the default `initContainerImage` in the existing YAML configuration.  The `repository` is changed to `bash` and tag to `5.2.2`. This is a required change when upgrading from 0.14.1 to 0.15.0 of the nifikop project if a custom `initContainerImage` was configured previously.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/7_upgrade_guides/2_v0.14.1_to_v0.15.0.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  initContainerImage:\n    repository: bash\n    tag: \"5.2.2\"\n```\n\n----------------------------------------\n\nTITLE: Deploying a Simple NiFi Cluster with kubectl\nDESCRIPTION: This command deploys a sample NiFi cluster by applying a YAML configuration that references the Zookeeper service, within the 'nifi' namespace. It facilitates quick setup of a functional NiFi environment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/2_setup/1_getting_started.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create -n nifi -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring a ClusterIP Service in Nifikop (YAML)\nDESCRIPTION: This snippet configures a ClusterIP service for Nifikop, exposing two ports: 8080 for the 'http' listener and 7182 for 'my-custom-listener'.  It defines the service type as ClusterIP and includes metadata such as annotations and labels. The 'protocol' field defaults to TCP when not explicitly specified.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/5_references/1_nifi_cluster/7_external_service_config.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\nexternalServices:\n    - name: \"clusterip\"\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8080\n            internalListenerName: \"http\"\n          - port: 7182\n            internalListenerName: \"my-custom-listener\"\n            protocol: TCP\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n\n----------------------------------------\n\nTITLE: Installing Nifikop Helm Chart (Set Parameters)\nDESCRIPTION: Install the Nifikop Helm chart with specific parameters set using the `--set` flag. This example installs the chart named 'nifikop' and sets the `namespaces` value to `{\"nifikop\"}`, indicating the namespaces where the operator should watch for resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop --set namespaces={\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Building a Docker Image for the Operator\nDESCRIPTION: Builds a Docker image from the current branch of the NiFiKop project with a specified Docker registry prefix, enabling containerized deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/6_contributing/1_developer_guide.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nexport DOCKER_REGISTRY_BASE={your-docker-repo}\nmake docker-build\n```\n\n----------------------------------------\n\nTITLE: Applying NiFi Cluster Configuration (Shell)\nDESCRIPTION: Applies the updated `NifiCluster` custom resource definition to the Kubernetes cluster using `kubectl`. This command instructs the NiFiKop operator to reconcile the cluster state based on the provided YAML file. It is run in the `nifi` namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nkubectl -n nifi apply -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Authorizer Template Replacement using YAML\nDESCRIPTION: This YAML snippet demonstrates a template replacement for NiFi's authorizers.xml configuration, which is used to define user group providers and access policy providers. It uses Go templating to dynamically configure the authorizers based on the NiFi cluster's node list and other deployment-specific parameters. This is useful when using NiFiKop to manage NiFi deployments, as it allows for dynamic node identities.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/1_manage_clusters/1_deploy_cluster/6_users_authorization/1_custom_user_authorizer.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n{{- $nodeList := .NodeList }}\n{{- $clusterName := .ClusterName }}\n{{- $namespace := .Namespace }}<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>\n<authorizers>\n    <userGroupProvider>\n        <identifier>file-user-group-provider</identifier>\n        <class>org.apache.nifi.authorization.FileUserGroupProvider</class>\n        <property name=\"Users File\">../data/users.xml</property>\n        <property name=\"Legacy Authorized Users File\"></property>\n        <property name=\"Initial User Identity admin\">{{ .ControllerUser }}</property>\n{{- range $i, $host := .NodeList }}\n        <property name=\"Initial User Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n    </userGroupProvider>\n    <userGroupProvider>\n        <identifier>database-user-group-provider</identifier>\n        <class>my.custom.DatabaseUserGroupProvider</class>\n        <!-- Any extra configuration for this provider goes here -->\n{{- range $i, $host := .NodeList }}\n        <property name=\"Initial User Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n    </userGroupProvider>\n    <accessPolicyProvider>\n        <identifier>file-access-policy-provider</identifier>\n        <class>org.apache.nifi.authorization.FileAccessPolicyProvider</class>\n        <property name=\"User Group Provider\">file-user-group-provider</property>\n        <property name=\"Authorizations File\">../data/authorizations.xml</property>\n        <property name=\"Initial Admin Identity\">{{ .ControllerUser }}</property>\n        <property name=\"Legacy Authorized Users File\"></property>\n{{- range $i, $host := .NodeList }}\n        <property name=\"Node Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n\t\t<property name=\"Node Group\"></property>\n    </accessPolicyProvider>\n    <accessPolicyProvider>\n        <identifier>database-access-policy-provider</identifier>\n        <class>my.custom.DatabaseAccessPolicyProvider</class>\n        <!-- Any extra configuration for this provider goes here -->\n{{- range $i, $host := .NodeList }}\n        <property name=\"Node Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n\t\t<property name=\"Node Group\"></property>\n    </accessPolicyProvider>\n    <authorizer>\n        <identifier>managed-authorizer</identifier>\n        <class>org.apache.nifi.authorization.StandardManagedAuthorizer</class>\n        <property name=\"Access Policy Provider\">file-access-policy-provider</property>\n    </authorizer>\n    <authorizer>\n        <identifier>custom-database-authorizer</identifier>\n        <class>my.custom.DatabaseAuthorizer</class>\n        <property name=\"Access Policy Provider\">database-access-policy-provider</property>\n    </authorizer>\n</authorizers>\n\n```\n\n----------------------------------------\n\nTITLE: Defining Node Configuration Groups - YAML\nDESCRIPTION: This YAML snippet demonstrates the configuration of node groups within a NiFi cluster using `NodeConfigGroups`.  It defines resource requirements, storage configurations and other parameters, providing different node configurations for various needs (e.g., high memory). The output is a configuration for a Kubernetes custom resource definition (CRD) that specifies the node configuration groups.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  # nodeConfigGroups specifies multiple node configs with unique name\n  nodeConfigGroups:\n    default_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      serviceAccountName: \"default\"\n      # resourceRequirements works exactly like Container resources, the user can specify the limit and the requests\n      # through this property\n      # https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 3Gi\n    high_mem_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      serviceAccountName: \"default\"\n      # resourceRequirements works exactly like Container resources, the user can specify the limit and the requests\n      # through this property\n      # https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 30Gi\n        requests:\n          cpu: \"1\"\n          memory: 30Gi\n```\n\n----------------------------------------\n\nTITLE: Extracting SSL Credentials from Secret with kubectl\nDESCRIPTION: Provides command-line instructions to decode and save individual certificate and key files from the `example-client-secret` Kubernetes secret. Base64 decoding is applied to retrieve usable files for client authentication.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/3_tasks/2_security/1_ssl.md#_snippet_5\n\nLANGUAGE: Console\nCODE:\n```\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['ca\\.crt']}\" | base64 -d > ca.crt\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['tls\\.crt']}\" | base64 -d > tls.crt\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['tls\\.key']}\" | base64 -d > tls.key\n```\n\n----------------------------------------\n\nTITLE: Defining Simple NiFi Cluster using YAML\nDESCRIPTION: This YAML snippet provides a complete example of a `NifiCluster` custom resource definition used by the Nifikop operator. It specifies configuration for a basic NiFi cluster, including service and pod metadata, Zookeeper connectivity, node groups with storage and resource requests, internal listeners, and an external service configuration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/5_references/1_nifi_cluster/1_nifi_cluster.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n    annotations:\n      tyty: ytyt\n    labels:\n      tete: titi  \n  pod:\n    annotations:\n      toto: tata\n    labels:\n      titi: tutu\n  zkAddress: 'zookeepercluster-client.zookeeper:2181'\n  zkPath: '/simplenifi'\n  clusterImage: 'apache/nifi:1.11.3'\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      podMetadata:\n        annotations:\n          node-annotation: \"node-annotation-value\"\n        labels:\n          node-label: \"node-label-value\"\n      externalVolumeConfigs:\n        - name: example-volume\n          mountPath: \"/opt/nifi/example\"\n          secret:\n            secretName: \"raw-controller\"\n      storageConfigs:\n        - mountPath: '/opt/nifi/nifi-current/logs'\n          name: logs\n          # Metadata to attach to the PVC that gets created\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: 'standard'\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: 'default'\n      resourcesRequirements:\n        limits:\n          cpu: '2'\n          memory: 3Gi\n        requests:\n          cpu: '1'\n          memory: 1Gi\n  nodes:\n    - id: 1\n      nodeConfigGroup: 'default_group'\n    - id: 2\n      nodeConfigGroup: 'default_group'\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: 'http'\n        name: 'http'\n        containerPort: 8080\n      - type: 'cluster'\n        name: 'cluster'\n        containerPort: 6007\n      - type: 's2s'\n        name: 's2s'\n        containerPort: 10000\n  externalServices:\n    - name: 'clusterip'\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8080\n            internalListenerName: 'http'\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n\n----------------------------------------\n\nTITLE: Deploying ServiceMonitor for NiFi Metrics\nDESCRIPTION: Creates a ServiceMonitor to scrape NiFi metrics at specified endpoints, labels, and namespaces. Uses relabeling rules to extract pod IP, node ID, and NiFi CR labels for Prometheus. Requires Prometheus Operator CRDs.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_10\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: cluster\n  namespace: monitoring-system\n  labels:\n    app: nifi-cluster\n    nifi_cr: cluster\nspec:\n  selector:\n    matchLabels:\n      app: nifi\n      nifi_cr: cluster\n  namespaceSelector:\n    matchNames:\n      - clusters\n  endpoints:\n    - interval: 10s\n      port: prometheus\n      path: /metrics\n      honorLabels: true\n      relabelings:\n        - sourceLabels: [__meta_kubernetes_pod_ip]\n          separator: \";\"\n          regex: (.*)\n          targetLabel: pod_ip\n          replacement: $1\n          action: replace\n        - sourceLabels: [__meta_kubernetes_pod_label_nodeId]\n          separator: \";\"\n          regex: (.*)\n          targetLabel: nodeId\n          replacement: $1\n          action: replace\n        - sourceLabels: [__meta_kubernetes_pod_label_nifi_cr]\n          separator: \";\"\n          regex: (.*)\n          targetLabel: nifi_cr\n          replacement: $1\n          action: replace\n```\n\n----------------------------------------\n\nTITLE: Extracting GKE Cluster Credentials Using gcloud Console Commands\nDESCRIPTION: This console command snippet is used on Google Cloud SDK to retrieve access credentials for a Google Kubernetes Engine (GKE) cluster. The command uses `gcloud container clusters get-credentials` with placeholders for cluster name, zone, and project name. This ensures kubectl is configured to communicate with the specified GKE cluster context.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/2_deploy_nifikop/1_quick_start.md#_snippet_7\n\nLANGUAGE: console\nCODE:\n```\ngcloud container clusters get-credentials CLUSTER_NAME --zone ZONE_NAME --project PROJECT_NAME.\n```\n\n----------------------------------------\n\nTITLE: Adding Non-NiFi Internal Listeners in YAML\nDESCRIPTION: This snippet shows how to add internal listeners that are not related to NiFi's internal behavior. The `name` and `containerPort` are specified. This is particularly useful for exposing a NiFi processor through a specific port, such as an HTTP endpoint.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    ...\n    - name: \"http-tracking\"\n      containerPort: 8081\n```\n\n----------------------------------------\n\nTITLE: Deploying a NifiDataflow CRD in YAML\nDESCRIPTION: This YAML manifest defines a `NifiDataflow` custom resource, which instructs NiFiKop to deploy a specific versioned dataflow from NiFi Registry. It requires references to the target cluster (`clusterRef`), the `NifiRegistryClient` (`registryClientRef`), and the `NifiParameterContext` (`parameterContextRef`). It also specifies the flow details (`bucketId`, `flowId`, `flowVersion`), synchronization behavior (`syncMode`), and update strategy (`updateStrategy`).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: demo\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Setting Operator Name Environment Variable (Bash)\nDESCRIPTION: Sets and exports the `OPERATOR_NAME` environment variable, typically required before running the operator binary directly from the command line.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/6_contributing/1_developer_guide.md#_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\nexport OPERATOR_NAME=nifi-operator\n```\n\n----------------------------------------\n\nTITLE: NiFiKop Resource Migration Script (JavaScript)\nDESCRIPTION: A Node.js script using the `@kubernetes/client-node` library to migrate NiFiKop custom resources from the `nifi.orange.com/v1alpha1` API group to `nifi.konpyutaika.com/v1alpha1`. It connects to the Kubernetes cluster using the default KubeConfig, lists resources of a specific type in a given namespace, creates new resources in the target API group copying relevant metadata and spec, and then updates the status of the new resources. The script uses `minimist` to parse command-line arguments for resource type and namespace, and disables Node.js TLS certificate verification (`NODE_TLS_REJECT_UNAUTHORIZED=0`).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nprocess.env['NODE_TLS_REJECT_UNAUTHORIZED'] = 0;\nconst k8s = require('@kubernetes/client-node');\n\nconst kc = new k8s.KubeConfig();\nkc.loadFromDefault();\n\nconst k8sApi = kc.makeApiClient(k8s.CustomObjectsApi);\n\nconst KONPYUTAIKA_GROUP = 'nifi.konpyutaika.com';\nconst KONPYUTAIKA_GROUP_VERSION = 'v1alpha1';\nconst ORANGE_GROUP = 'nifi.orange.com';\nconst ORANGE_GROUP_VERSION = 'v1alpha1';\n\nconst call = async (SRC_GRP, SRC_GRP_VER, DST_GRP, DST_GRP_VER, KIND_PLURAL, NAMESPACE) => {\n\tconsole.log(`Listing ${KIND_PLURAL} of ${SRC_GRP}/${SRC_GRP_VER} in ${NAMESPACE}...`);\n\tconst listResources = (await k8sApi.listNamespacedCustomObject(SRC_GRP, SRC_GRP_VER, NAMESPACE, KIND_PLURAL)).body.items;\n\treturn Promise.all(listResources.map(async (resource) => {\n\t\ttry {\n\t\t\tconsole.log(`Found ${resource.kind} \"${resource.metadata.name}\" of ${resource.apiVersion} in ${NAMESPACE}`);\n\n\t\t\tif (resource.metadata.ownerReferences) {\n\t\t\t\tconsole.log(`${resource.kind} ${resource.metadata.name} mananged by something else (ownerRefereces is set).`);\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tconst bodyResource = {\n\t\t\t\tapiVersion: `${DST_GRP}/${DST_GRP_VER}`,\n\t\t\t\tkind: resource.kind,\n\t\t\t\tmetadata: {\n\t\t\t\t\tname: resource.metadata.name,\n\t\t\t\t\tannotations: resource.metadata.annotations,\n\t\t\t\t\tlabels: resource.metadata.labels\n\t\t\t\t},\n\t\t\t\tspec: resource.spec\n\t\t\t};\n\n\t\t\tconsole.log(`Creating ${bodyResource.kind} \"${bodyResource.metadata.name}\" of ${bodyResource.apiVersion} in ${NAMESPACE}...`);\n\t\t\tconst newResource = (await k8sApi.createNamespacedCustomObject(DST_GRP, DST_GRP_VER, NAMESPACE, KIND_PLURAL, bodyResource)).body;\n\t\t\tconsole.log('...done creating.');\n\n\t\t\tconst bodyStatus = {\n\t\t\t\tapiVersion: newResource.apiVersion,\n\t\t\t\tkind: newResource.kind,\n\t\t\t\tmetadata: {\n\t\t\t\t\tname: newResource.metadata.name,\n\t\t\t\t\tresourceVersion: newResource.metadata.resourceVersion\n\t\t\t\t},\n\t\t\t\tstatus: resource.status\n\t\t\t};\n\n\t\t\tconsole.log(`Copying status from ${resource.kind} \"${resource.metadata.name}\" of ${newResource.apiVersion} to ${newResource.kind} \"${newResource.metadata.name}\" of ${newResource.apiVersion} in ${NAMESPACE}...`);\n\t\t\tconst newResourceWithStatus = (await k8sApi.replaceNamespacedCustomObjectStatus(DST_GRP, DST_GRP_VER, NAMESPACE, KIND_PLURAL, bodyStatus.metadata.name, bodyStatus)).body;\n\t\t\tconsole.log('...done copying.');\n\t\t\treturn newResourceWithStatus;\n\t\t}\n\t\tcatch (e) {\n\t\t\tconsole.error(e.body ? e.body.message ? e.body.message : e.body : e);\n\t\t}\n\t}));\n};\n\nconst argv = require('minimist')(process.argv.slice(2));\n\nlet NAMESPACE = argv.namespace ? argv.namespace.length > 0 ? argv.namespace : 'default' : 'default';\nlet KIND_PLURAL = {\n\tcluster: 'nificlusters',\n\tdataflow: 'nifidataflows',\n\tparametercontext: 'nifiparametercontexts',\n\tregistryclient: 'nifiregistryclients',\n\tuser: 'nifiusers',\n\tusergroup: 'nifiusergroups',\n};\n\nif (!argv.type) {\n\tconsole.error('Type not provided');\n\tprocess.exit(1);\n}\n\nif (!KIND_PLURAL[argv.type]) {\n\tconsole.error(`Type ${argv.type} is not one of the following types: ${Object.keys(KIND_PLURAL)}`);\n\tprocess.exit(1);\n}\n\nconsole.log(`########### START: ${KIND_PLURAL[argv.type]} ###########`);\ncall( ORANGE_GROUP, ORANGE_GROUP_VERSION, KONPYUTAIKA_GROUP, KONPYUTAIKA_GROUP_VERSION, KIND_PLURAL[argv.type], NAMESPACE)\n\t.then(r => console.log('############ END ############'))\n\t.catch(e => console.error(e));\n```\n\n----------------------------------------\n\nTITLE: Uninstalling Helm Release for Nifikop Bash\nDESCRIPTION: This bash command removes the Helm release named 'nifikop', which deletes all Kubernetes resources associated with the Nifikop chart. It assumes you have the Helm CLI installed and configured to access your Kubernetes cluster. No additional parameters are required, and the only input is the release name. Output is the removal of the release and its managed resources, except for CRDs that must be deleted separately.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/2_setup/3_install/1_customizable_install_with_helm.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nhelm del nifikop\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiUserGroup resource in YAML\nDESCRIPTION: This YAML snippet defines a NifiUserGroup resource named \"group-test\". It specifies the NiFi cluster the group belongs to, a list of NiFi users that are members of the group, and a list of access policies granted to the group. The access policy in this example grants read access to the /counters resource.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/2_manage_users_and_accesses/2_groups_management.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  # Contains the reference to the NifiCluster with the one the registry client is linked.\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  # contains the list of reference to NifiUsers that are part to the group.\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n#     namespace: nifikop\n    - name: nc-controller.nifikop.mgt.cluster.local\n  # defines the list of access policies that will be granted to the group.\n  accessPolicies:\n      # defines the kind of access policy, could be \"global\" or \"component\".\n    - type: global\n      # defines the kind of action that will be granted, could be \"read\" or \"write\"\n      action: read\n      # resource defines the kind of resource targeted by this access policies, please refer to the following page:\n      #\thttps://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#access-policies\n      resource: /counters\n#      # componentType is used if the type is \"component\", it's allow to define the kind of component on which is the\n#      # access policy\n#      componentType: \"process-groups\"\n#      # componentId is used if the type is \"component\", it's allow to define the id of the component on which is the\n#      # access policy\n#      componentId: \"\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio Gateway for HTTPS Termination for NiFi (YAML)\nDESCRIPTION: Defines an Istio Gateway named `nifi-gateway` to listen on port 443 for HTTPS traffic destined for `nifi.my-domain.com`. It uses `SIMPLE` TLS mode with credentials from the specified Kubernetes secret (`my-secret`) to terminate TLS encryption at the gateway, forwarding traffic as plain HTTP internally.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: nifi-gateway\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 443\n      name: https\n      protocol: HTTPS\n    tls:\n      mode: SIMPLE\n      credentialName: my-secret\n    hosts:\n    - nifi.my-domain.com\n```\n\n----------------------------------------\n\nTITLE: NiFiCluster Node Addition - YAML\nDESCRIPTION: This YAML snippet demonstrates how to add a new node to a NiFi cluster using the NifiCluster resource definition. It shows the `nodes` section with a new node added, including the node ID and nodeConfigGroup.  It assumes that the `default_group` nodeConfigGroup is already defined.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/3_tasks/1_nifi_cluster/2_cluster_scaling.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.12.1\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n    - id: 2\n      nodeConfigGroup: \"default_group\"\n# >>>> START: The new node\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n# <<<< END\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: Installing Helm Chart\nDESCRIPTION: This bash command installs the NiFiKop operator using Helm.  It specifies the image tag and the namespace. The command assumes the Helm chart has been packaged and available, that a Kubernetes cluster is running, and that helm is configured.  The image tag should correspond to a previously built and pushed Docker image.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/6_contributing/1_developer_guide.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nhelm install skeleton ./helm/nifikop \\\n    --set image.tag=v0.5.1-release \\\n    --namespace-{\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Cluster to Use External Issuer for SSL\nDESCRIPTION: This snippet demonstrates modifying the NiFi cluster configuration YAML to reference an external certificate issuer in `issuerRef`, enabling the cluster to obtain SSL certificates from an external CA like Let's Encrypt.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/3_tasks/2_security/1_ssl.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  clusterDomain: <DNS zone name>\n  useExternalDNS: true\n  ...\n  sslSecrets:\n    tlsSecretName: \"test-nifikop\"\n    create: true\n    issuerRef:\n      name: letsencrypt-staging\n      kind: Issuer\n```\n\n----------------------------------------\n\nTITLE: Getting Helm Release Status (Bash)\nDESCRIPTION: Uses `helm status` to retrieve the current status and deployment details of a specific Helm release, identified by its release name (e.g., `nifikop`).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nhelm status nifikop\n```\n\n----------------------------------------\n\nTITLE: Building and Installing Kubectl Plugin for NiFiKop Using Console Commands\nDESCRIPTION: This snippet shows the command to build the kubectl-nifikop binary using 'make' and then copies it to '/usr/local/bin' with elevated permissions to make the plugin executable globally. The dependencies include having a Makefile configured to build the plugin and sudo access for copying into system PATH directories. The input is the source code environment capable of building the plugin, and the output is the installed kubectl plugin executable.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/2_deploy_nifikop/3_kubectl_plugin.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nmake kubectl-nifikop && sudo cp ./bin/kubectl-nifikop /usr/local/bin\n```\n\n----------------------------------------\n\nTITLE: Overriding NiFi Properties for OIDC Configuration in NiFiKop YAML\nDESCRIPTION: This YAML snippet demonstrates how to override NiFi properties related to OpenId Connect authentication in the NiFiKop custom resource definition for a Kubernetes-managed NiFi cluster. It replaces or supplements existing configurations by setting the OIDC discovery URL, client ID, client secret, and identity mapping parameters. Key parameters include 'nifi.security.user.oidc.discovery.url', 'nifi.security.user.oidc.client.id', and associated identity mapping settings. The snippet assumes the NiFiKop operator is installed and configured in the Kubernetes cluster to accept this configuration under 'spec.readOnlyConfig.nifiProperties.overrideConfigs'.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/5_users_authentication/1_oidc.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  readOnlyConfig:\n    # NifiProperties configuration that will be applied to the node.\n    nifiProperties:\n      webProxyHosts:\n        - nifistandard2.trycatchlearn.fr:8443\n      # Additionnal nifi.properties configuration that will override the one produced based\n      # on template and configurations.\n      overrideConfigs: |\n        nifi.security.user.oidc.discovery.url=<oidc server discovery url>\n        nifi.security.user.oidc.client.id=<oidc client\\'s id>\n        nifi.security.user.oidc.client.secret=<oidc client\\'s secret>\n        nifi.security.identity.mapping.pattern.dn=CN=([^,]*)(?:, (?:O|OU)=.*)?\n        nifi.security.identity.mapping.value.dn=$1\n        nifi.security.identity.mapping.transform.dn=NONE\n      ...\n   ...\n...\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Cluster with SSL Configuration in YAML\nDESCRIPTION: This snippet defines the YAML configuration for deploying a NiFi cluster with SSL enabled, including listener settings, SSL secret management, admin users, and web proxy hosts. It supports creating self-signed certificates or referencing existing secrets if `create` is set to false.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/3_tasks/2_security/1_ssl.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\n...\nspec:\n  ...\n  managedAdminUsers:\n    - identity: \"alexandre.guitton@konpyutaika.com\"\n      name: \"aguitton\"\n  ...\n  readOnlyConfig:\n    nifiProperties:\n      webProxyHosts:\n        - nifistandard2.trycatchlearn.fr:8443\n        ...\n  ...\n  listenersConfig:\n    internalListeners:\n      - type: \"https\"\n        name: \"https\"\n        containerPort: 8443\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n```\n\n----------------------------------------\n\nTITLE: Displaying Kubernetes Services\nDESCRIPTION: This console snippet displays a Kubernetes service named `cluster-access` of type `LoadBalancer`. It shows the cluster IP, external IP, and the ports it exposes (443 and 80), along with their corresponding node ports (30421 and 30521). This command helps verify the successful creation and configuration of the external service.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nkubectl get services\nNAME                TYPE           CLUSTER-IP    EXTERNAL-IP      PORT(S)                                AGE\ncluster-access      LoadBalancer   10.88.21.98   35.180.241.132   443:30421/TCP,80:30521/TCP             20d\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio HTTPS DestinationRule (SSL Re-encryption, Sticky Session) in YAML\nDESCRIPTION: Defines an Istio `DestinationRule` resource for the internal NiFi service (`<service-name>.<namespace>.svc.cluster.local`). It sets the traffic policy to re-encrypt connections using TLS (`mode: SIMPLE`) before reaching the backend service. Additionally, it configures a consistent hash load balancer based on the `__Secure-Authorization-Bearer` HTTP cookie to ensure sticky sessions for user authentication.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_4\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: DestinationRule\nmetadata:\n  name: nifi-dr\nspec:\n  host: <service-name>.<namespace>.svc.cluster.local\n  trafficPolicy:\n    tls:\n      mode: SIMPLE\n    loadBalancer:\n      consistentHash:\n        httpCookie:\n          name: __Secure-Authorization-Bearer\n          ttl: 0s\n```\n\n----------------------------------------\n\nTITLE: Installing Nifikop Helm Chart and Setting Parameters\nDESCRIPTION: Illustrates how to install the Nifikop Helm chart while overriding default configuration values using the `--set` flag. This example specifically sets the `namespaces` parameter to target the 'nifikop' namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop --set namespaces={\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring SSL Secret referencing existing secret\nDESCRIPTION: This snippet indicates how to configure the operator to use an existing TLS secret for SSL by setting 'create' to false and specifying the secret name. The secret must contain CA certificates, client certificates, and keys required for secure communication. This is suitable when SSL certificates are managed externally or by a separate CA authority.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\nlistenersConfig:\n  sslSecrets:\n    tlsSecretName: \"your-existing-secret\"\n    create: false\n```\n\n----------------------------------------\n\nTITLE: Adding Bitnami Helm Repository for Zookeeper\nDESCRIPTION: This bash command adds Bitnami's Helm chart repository for Zookeeper to facilitate Helm-based deployment. It ensures the cluster has access to the latest Helm charts from Bitnami for deploying Zookeeper with consistent configurations. Prerequisites include Helm installed and configured access to the Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhelm repo add bitnami https://charts.bitnami.com/bitnami\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTPS VirtualService for NiFi in Istio\nDESCRIPTION: Defines an Istio VirtualService that redirects all HTTP traffic to the NiFi ClusterIP Service on port 8443.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: nifi-vs\nspec:\n  gateways:\n  - nifi-gateway\n  hosts:\n  - nifi.my-domain.com\n  http:\n  - match:\n    - uri:\n        prefix: /\n    route:\n    - destination:\n        host: <service-name>.<namespace>.svc.cluster.local\n        port:\n          number: 8443\n```\n\n----------------------------------------\n\nTITLE: Defining Istio VirtualService for HTTPS Routing - YAML\nDESCRIPTION: Defines a VirtualService to route HTTP (after HTTPS termination at Gateway) traffic to an internal NiFi ClusterIP service on port 8443. Replace <service-name>.<namespace> with actual service and namespace values. Requires a running NiFi ClusterIP service and a Gateway configured for HTTPS. Routes all URIs using prefix matching.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: nifi-vs\nspec:\n  gateways:\n  - nifi-gateway\n  hosts:\n  - nifi.my-domain.com\n  http:\n  - match:\n    - uri:\n        prefix: /\n    route:\n    - destination:\n        host: <service-name>.<namespace>.svc.cluster.local\n        port:\n          number: 8443\n\n```\n\n----------------------------------------\n\nTITLE: Adding Additional Custom Internal Listeners in NiFi YAML\nDESCRIPTION: Demonstrates how to append an extra, non-standard internal listener by specifying a name and containerPort without the type property. This example adds an http-tracking endpoint to the NiFi deployment, suitable for use cases like HTTP processor endpoints. Prerequisites: modification of listenersConfig.internalListeners. This approach is appropriate when exposing custom application ports beyond those used by NiFi's core cluster functions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    ...\n    - name: \"http-tracking\"\n      containerPort: 8081\n```\n\n----------------------------------------\n\nTITLE: Defining NiFiKop NodeConfigGroups in YAML\nDESCRIPTION: Demonstrates how to define multiple node configuration groups (`default_group`, `high_mem_group`) using `Spec.NodeConfigGroups` in NiFiKop. Each group specifies settings like `provenanceStorage`, `runAsUser`, `serviceAccountName`, and Kubernetes resource requirements (`resourcesRequirements`) including CPU/memory limits and requests for NiFi nodes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  # nodeConfigGroups specifies multiple node configs with unique name\n  nodeConfigGroups:\n    default_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      serviceAccountName: \"default\"\n      # resourceRequirements works exactly like Container resources, the user can specify the limit and the requests\n      # through this property\n      # https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 3Gi\n    high_mem_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      serviceAccountName: \"default\"\n      # resourceRequirements works exactly like Container resources, the user can specify the limit and the requests\n      # through this property\n      # https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 30Gi\n        requests:\n          cpu: \"1\"\n          memory: 30Gi\n```\n\n----------------------------------------\n\nTITLE: Overriding NiFiKop OIDC Properties in Cluster Specification - YAML\nDESCRIPTION: This YAML snippet demonstrates how to configure OpenID Connect settings and identity mapping directly in a NiFiKop-managed NifiCluster custom resource. The overrideConfigs field under spec.readOnlyConfig.nifiProperties allows the injection of custom OIDC discovery URL, client credentials, and identity mapping settings. It requires NiFiKop (Kubernetes operator for Apache NiFi), a running NiFi cluster, and suitable secrets for OIDC client authentication. Replace the placeholders with actual OIDC provider information to enable OIDC login; improper configuration may prevent authentication.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/5_users_authentication/1_oidc.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  readOnlyConfig:\n    # NifiProperties configuration that will be applied to the node.\n    nifiProperties:\n      webProxyHosts:\n        - nifistandard2.trycatchlearn.fr:8443\n      # Additionnal nifi.properties configuration that will override the one produced based\n      # on template and configurations.\n      overrideConfigs: |\n        nifi.security.user.oidc.discovery.url=<oidc server discovery url>\n        nifi.security.user.oidc.client.id=<oidc client's id>\n        nifi.security.user.oidc.client.secret=<oidc client's secret>\n        nifi.security.identity.mapping.pattern.dn=CN=([^,]*)(?:, (?:O|OU)=.*)?\n        nifi.security.identity.mapping.value.dn=$1\n        nifi.security.identity.mapping.transform.dn=NONE\n      ...\n   ...\n...\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Cluster Configuration YAML\nDESCRIPTION: This YAML configuration defines a NiFi cluster named `simplenifi` with various specifications. It includes settings for service configuration (headless service), pod configurations (annotations, labels), Zookeeper connection details (address, path), cluster image, node configurations (storage, resources, service account), node definitions, listener configurations (internal listeners), and external services.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/5_references/1_nifi_cluster/1_nifi_cluster.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n    annotations:\n      tyty: ytyt\n    labels:\n      tete: titi  \n  pod:\n    annotations:\n      toto: tata\n    labels:\n      titi: tutu\n  zkAddress: 'zookeepercluster-client.zookeeper:2181'\n  zkPath: '/simplenifi'\n  clusterImage: 'apache/nifi:1.11.3'\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      podMetadata:\n        annotations:\n          node-annotation: \"node-annotation-value\"\n        labels:\n          node-label: \"node-label-value\"\n      externalVolumeConfigs:\n        - name: example-volume\n          mountPath: \"/opt/nifi/example\"\n          secret:\n            secretName: \"raw-controller\"\n      storageConfigs:\n        - mountPath: '/opt/nifi/nifi-current/logs'\n          name: logs\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: 'standard'\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: 'default'\n      resourcesRequirements:\n        limits:\n          cpu: '2'\n          memory: 3Gi\n        requests:\n          cpu: '1'\n          memory: 1Gi\n  nodes:\n    - id: 1\n      nodeConfigGroup: 'default_group'\n    - id: 2\n      nodeConfigGroup: 'default_group'\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: 'http'\n        name: 'http'\n        containerPort: 8080\n      - type: 'cluster'\n        name: 'cluster'\n        containerPort: 6007\n      - type: 's2s'\n        name: 's2s'\n        containerPort: 10000\n  externalServices:\n    - name: 'clusterip'\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8080\n            internalListenerName: 'http'\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Node Configurations (YAML)\nDESCRIPTION: This YAML snippet demonstrates how to configure individual NiFi nodes in a NiFiKop deployment. It shows examples for node ID 0 using a `nodeConfigGroup` and `readOnlyConfig` to override the UI banner text, and node ID 2 specifying custom `readOnlyConfig`, resource requests/limits (`resourcesRequirements`), and persistent volume configuration (`storageConfigs`) including PVC specs.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/5_references/1_nifi_cluster/4_node.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n    - id: 0\n      # nodeConfigGroup can be used to ease the node configuration, if set only the id is required\n      nodeConfigGroup: \"default_group\"\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        nifiProperties:\n          overrideConfigs: |\n            nifi.ui.banner.text=NiFiKop - Node 0\n      # node configuration\n#       nodeConfig:\n    - id: 2\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        overrideConfigs: |\n          nifi.ui.banner.text=NiFiKop - Node 2\n      # node configuration\n      nodeConfig:\n        resourcesRequirements:\n          limits:\n            cpu: \"2\"\n            memory: 3Gi\n          requests:\n            cpu: \"1\"\n            memory: 1Gi\n        storageConfigs:\n          # Name of the storage config, used to name PV to reuse into sidecars for example.\n          - name: provenance-repository\n            # Path where the volume will be mount into the main nifi container inside the pod.\n            mountPath: \"/opt/nifi/provenance_repository\"\n            # Metadata to attach to the PVC that gets created\n            metadata:\n              labels:\n                my-label: my-value\n              annotations:\n                my-annotation: my-value\n            # Kubernetes PVC spec\n            # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n            pvcSpec:\n              accessModes:\n                - ReadWriteOnce\n              storageClassName: \"standard\"\n              resources:\n                requests:\n                  storage: 8Gi\n```\n\n----------------------------------------\n\nTITLE: Advanced NiFiCluster Multi-Directory Content and Provenance Storage - YAML\nDESCRIPTION: This YAML snippet illustrates advanced NiFiCluster configuration for specifying multiple directories for content and provenance repositories to optimize high-performance deployments. Dependencies include a running NiFiKop operator and properly configured PersistentVolumeClaims matching the specified storage requirements. The 'readOnlyConfig.nifiProperties.overrideConfigs' field declares repository directory mappings, and 'nodeConfigGroups.default_group.storageConfigs' defines the underlying persistent volume mounts with required access modes and storage classes. Users must map mount paths to logical repository directories; outputs are NiFi nodes with multiple, isolated paths for content and provenance data, subject to correct volume and path alignment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n...\n  readOnlyConfig:\n    nifiProperties:\n      overrideConfigs: |\n        nifi.content.repository.directory.dir1=../content-additional/dir1\n        nifi.content.repository.directory.dir2=../content-additional/dir2\n        nifi.content.repository.directory.dir3=../content-additional/dir3\n        nifi.provenance.repository.directory.dir1=../provenance-additional/dir1\n        nifi.provenance.repository.directory.dir2=../provenance-additional/dir2\n...\n  nodeConfigGroups:\n    default_group:\n      ...\n      storageConfigs:\n      - mountPath: \"/opt/nifi/content-additional/dir1\"\n        name: content-repository-dir1\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/content-additional/dir2\"\n        name: content-repository-dir2\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/content-additional/dir3\"\n        name: content-repository-dir3\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/provenance-additional/dir1\"\n        name: provenance-repository-dir1\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/provenance-additional/dir2\"\n        name: provenance-repository-dir2\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      ...\n```\n\n----------------------------------------\n\nTITLE: Set NiFi Custom Authorizer Property (Shell)\nDESCRIPTION: This snippet shows the NiFi property setting required to activate the custom authorizer defined in the `authorizers.xml` configuration. The `nifi.security.user.authorizer` property must be set to the identifier of the desired authorizer, which is `custom-database-authorizer` in this example. This property is typically set in the NiFi configuration file (`nifi.properties`).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/6_users_authorization/1_custom_user_authorizer.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\nnifi.security.user.authorizer=custom-database-authorizer\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFiKop with Helm on Kubernetes\nDESCRIPTION: Installs the NiFiKop operator using Helm chart from GitHub Container Registry. This command configures the operator with specific resource limits and targets it to monitor the 'nifi' namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/2_deploy_nifikop/1_quick_start.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# You have to create the namespace before executing following command\nhelm install nifikop \\\n    oci://ghcr.io/konpyutaika/helm-charts/nifikop \\\n    --namespace=nifi \\\n    --version 1.12.0 \\\n    --set image.tag=v1.12.0-release \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set namespaces={\"nifi\"}\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop Helm Chart\nDESCRIPTION: Installs the NiFiKop Helm chart into a Kubernetes cluster. It sets the image tag and namespace for the deployment.  The image repository must be configured to point to a pushed image.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/6_contributing/1_developer_guide.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nhelm install skeleton ./helm/nifikop \\\n    --set image.tag=v0.5.1-release \\\n    --namespace-{\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring Identity Mapping in nifi.properties\nDESCRIPTION: Recommended identity mapping configuration to add to nifi.properties for supporting multiple identity providers with OpenId Connect.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/5_users_authentication/1_oidc.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nnifi.security.identity.mapping.pattern.dn=CN=([^,]*)(?:, (?:O|OU)=.*)?\\nnifi.security.identity.mapping.value.dn=$1\\nnifi.security.identity.mapping.transform.dn=NONE\n```\n\n----------------------------------------\n\nTITLE: Creating a Kubernetes Secret for Sensitive Parameters using kubectl\nDESCRIPTION: This shell command uses `kubectl` to create a Kubernetes Secret named `secret-params` in the `nifikop` namespace. The secret contains key-value pairs (`secret1`, `secret2`) that will be treated as sensitive parameters when referenced by a `NifiParameterContext`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl create secret generic secret-params \\\n    --from-literal=secret1=yop \\\n    --from-literal=secret2=yep \\\n    -n nifikop\n```\n\n----------------------------------------\n\nTITLE: Manually Applying Nifikop CRDs using Kubectl\nDESCRIPTION: Applies the necessary Nifikop Custom Resource Definitions (CRDs) directly to the Kubernetes cluster using `kubectl`. This step is required before installing the Helm chart if the `--skip-crds` flag will be used.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifinodegroupautoscalers.yaml\n```\n\n----------------------------------------\n\nTITLE: Describing NiFiCluster (Bash)\nDESCRIPTION: This command retrieves detailed information about a NiFiCluster resource using `kubectl describe`. The output includes the current status, node states, and any error messages related to the cluster. It requires `kubectl` access to the Kubernetes cluster and the name of the NiFiCluster (e.g., `simplenifi`). This information can be used to monitor the progress of scaling operations.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n```console \nkubectl describe nificluster simplenifi\n```\n```\n\n----------------------------------------\n\nTITLE: Purging a NiFiKop Helm Release\nDESCRIPTION: Command to completely purge a NiFiKop release from the Helm history, removing all records of the release. This allows the release name to be reused for future installations.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nhelm delete --purge nifikop\n```\n\n----------------------------------------\n\nTITLE: Configuring Scale Up for NiFiKop Cluster in YAML\nDESCRIPTION: This YAML manifest demonstrates how to add a new node to an existing NiFi cluster managed by NiFiKop by expanding the 'NifiCluster.Spec.Nodes' list. The snippet contains detailed configuration for operator and cluster set up, such as Zookeeper connectivity, node groups, storage configuration, resource requests, service accounts, and listener settings. A unique 'id' is assigned to the new node, and all node IDs in the list must remain unique. Expected input is a Kubernetes manifest in YAML, which when applied will trigger NiFiKop to create the additional node and its associated resources according to the spec.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.12.1\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n    - id: 2\n      nodeConfigGroup: \"default_group\"\n# >>>> START: The new node\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n# <<<< END\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: Defining Istio DestinationRule for HTTPS\nDESCRIPTION: This YAML defines an Istio DestinationRule, which is crucial for handling HTTPS traffic and managing sticky sessions. It directs traffic to the internal service '<service-name>.<namespace>.svc.cluster.local' and configures the traffic policy to use TLS in SIMPLE mode. Furthermore, it defines a consistent hash load balancer using the `httpCookie` property to create a sticky session,  based on the `__Secure-Authorization-Bearer` cookie. This setup ensures that all traffic for a given session is directed to the same instance of the service, enhancing the user experience and maintaining state.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\napiVersion: networking.istio.io/v1beta1\nkind: DestinationRule\nmetadata:\n  name: nifi-dr\nspec:\n  host: <service-name>.<namespace>.svc.cluster.local\n  trafficPolicy:\n    tls:\n      mode: SIMPLE\n    loadBalancer:\n      consistentHash:\n        httpCookie:\n          name: __Secure-Authorization-Bearer\n          ttl: 0s\n```\n```\n\n----------------------------------------\n\nTITLE: Get the user ID\nDESCRIPTION: This command retrieves the user ID (UID) from the OpenShift namespace annotations. It uses `kubectl get namespace` with `jsonpath` to extract the `openshift.io/sa.scc.supplemental-groups` annotation, removes the trailing `/10000` and whitespace. Requires kubectl configured for OpenShift. The output is the UID of the namespace, stored in the `uid` variable.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/2_deploy_nifikop/1_quick_start.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nuid=$(kubectl get namespace nifi -o=jsonpath='{.metadata.annotations.openshift\\.io/sa\\.scc\\.supplemental-groups}' | sed 's///10000$//' | tr -d '[:space:]')\n```\n\n----------------------------------------\n\nTITLE: Creating Let's Encrypt Issuer for NiFi SSL in YAML\nDESCRIPTION: YAML configuration for creating a Let's Encrypt issuer that can be used to generate SSL certificates for your NiFi cluster. This example uses the Let's Encrypt staging environment with HTTP01 challenge.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: cert-manager.io/v1alpha2\nkind: Issuer\nmetadata:\n  name: letsencrypt-staging\nspec:\n  acme:\n    # You must replace this email address with your own.\n    # Let's Encrypt will use this to contact you about expiring\n    # certificates, and issues related to your account.\n    email: <your email address>\n    server: https://acme-staging-v02.api.letsencrypt.org/directory\n    privateKeySecretRef:\n      # Secret resource used to store the account's private key.\n      name: example-issuer-account-key\n    # Add a single challenge solver, HTTP01 using nginx\n    solvers:\n      - http01:\n          ingress:\n            ingressTemplate:\n              metadata:\n                annotations:\n                  \"external-dns.alpha.kubernetes.io/ttl\": \"5\"\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenID Connect with NiFiKop - YAML\nDESCRIPTION: This YAML snippet demonstrates how to configure OpenID Connect settings using the `NifiCluster` custom resource within the NiFiKop operator.  It uses `overrideConfigs` to add/override NiFi's properties including OIDC settings like `discovery.url`, `client.id`, and `client.secret` alongside the DN mapping settings shown earlier. This approach ensures that the correct OIDC settings are applied to the NiFi cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/5_users_authentication/1_oidc.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  readOnlyConfig:\n    # NifiProperties configuration that will be applied to the node.\n    nifiProperties:\n      webProxyHosts:\n        - nifistandard2.trycatchlearn.fr:8443\n      # Additionnal nifi.properties configuration that will override the one produced based\n      # on template and configurations.\n      overrideConfigs: |\n        nifi.security.user.oidc.discovery.url=<oidc server discovery url>\n        nifi.security.user.oidc.client.id=<oidc client's id>\n        nifi.security.user.oidc.client.secret=<oidc client's secret>\n        nifi.security.identity.mapping.pattern.dn=CN=([^,]*)(?:, (?:O|OU)=.*)?\n        nifi.security.identity.mapping.value.dn=$1\n        nifi.security.identity.mapping.transform.dn=NONE\n      ...\n   ...\n...\n```\n\n----------------------------------------\n\nTITLE: Installing the NiFiKop Helm Chart using Bash\nDESCRIPTION: Uses `helm install` to deploy the NiFiKop operator using its Helm chart located at `./helm/nifikop`. It sets the Helm release name to `skeleton`, specifies the image tag to use (`v0.5.1-release` in this example), and installs it into the `nifikop` namespace. Requires Helm v3.4.2+. Ensure the `image.tag` matches the tag of the pushed Docker image.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/6_contributing/1_developer_guide.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nhelm install skeleton ./helm/nifikop \\\n    --set image.tag=v0.5.1-release \\\n    --namespace-{\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Verifying Operator Pods - Bash\nDESCRIPTION: This command verifies that the operator is running in the `nifikop` namespace using `kubectl get pods`. This checks the status of the deployed pods, and confirms that the operator is in a `Running` state. It provides a way to confirm that the deployment was successful.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/6_contributing/1_developer_guide.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n$ kubectl get pods -n nifikop\n```\n\n----------------------------------------\n\nTITLE: Listing All Helm Releases\nDESCRIPTION: List all Helm releases, regardless of their status (deployed, deleted, failed, etc.). This provides a comprehensive view of all Helm activity in the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --all\n```\n\n----------------------------------------\n\nTITLE: Defining Managed User Groups in NiFi Cluster Spec YAML\nDESCRIPTION: This YAML snippet defines the specification for a NiFiCluster, including managed user groups for admin and reader roles. The 'managedAdminUsers' and 'managedReaderUsers' fields list users by their identities (e.g., email) and associated names. The operator uses this specification to create corresponding NiFiUsers and groups with appropriate access rights. This snippet requires the NiFi operator to be deployed and properly configured to interpret and manage these specifications. Inputs include user identities and names; outputs are managed NiFiUser and NiFiUserGroup resources within the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/2_manage_users_and_accesses/3_managed_groups.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  ...\n  oneNifiNodePerNode: false\n  propagateLabels: true\n  managedAdminUsers:\n    -  identity: \"alexandre.guitton@konpyutaika.com\"\n       name: \"aguitton\"\n    -  identity: \"nifiuser@konpyutaika.com\"\n       name: \"nifiuser\"\n  managedReaderUsers:\n    -  identity: \"toto@konpyutaika.com\"\n       name: \"toto\"\n    ...\n```\n\n----------------------------------------\n\nTITLE: Configuring SSL in NifiCluster Resource\nDESCRIPTION: A YAML configuration example showing how to secure a NiFi cluster with SSL. It demonstrates setting up managed admin users, web proxy hosts, internal listeners for HTTPS, cluster, and S2S communications, and SSL secret configuration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/3_tasks/2_security/1_ssl.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\n...\nspec:\n  ...\n  managedAdminUsers:\n    - identity: \"alexandre.guitton@konpyutaika.com\"\n      name: \"aguitton\"\n  ...\n  readOnlyConfig:\n    # NifiProperties configuration that will be applied to the node.\n    nifiProperties:\n      webProxyHosts:\n        - nifistandard2.trycatchlearn.fr:8443\n        ...\n  ...\n  listenersConfig:\n    internalListeners:\n      - type: \"https\"\n        name: \"https\"\n        containerPort: 8443\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n```\n\n----------------------------------------\n\nTITLE: BootstrapProperties Configuration Structure in Markdown\nDESCRIPTION: This table defines the configuration fields for Bootstrap properties in NiFiKop, including override options and JVM memory settings for NiFi nodes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/5_references/1_nifi_cluster/2_read_only_config.md#_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n## BootstrapProperties\n\n|Field|Type|Description|Required|Default|\n|-----|----|-----------|--------|--------|\n|overrideConfigMap|[ConfigmapReference](#configmapreference)|Additionnal bootstrap.properties configuration that will override the one produced based on template and configuration.|No|nil|\n|overrideConfigs|string|Additionnal bootstrap.properties configuration that will override the one produced based on template, configurations and overrideConfigMap.|No|\"\"|\n|overrideSecretConfig|[SecretConfigReference](#secretconfigreference)|Additionnal bootstrap.properties configuration that will override the one produced based on template, configurations, overrideConfigMap and overrideConfigs.|No|nil|\n|NifiJvmMemory|string|JVM memory settings.|No|\"512m\"|\n```\n\n----------------------------------------\n\nTITLE: Configuring Persistent Storage for NiFi Nodes in NiFiKop YAML\nDESCRIPTION: This snippet demonstrates how to define persistent storage volumes for various NiFi data repositories within a `NodeConfigGroup`. It specifies the mount path within the container, a unique name for the storage configuration, and the Kubernetes Persistent Volume Claim (PVC) specification, including access modes, storage requests, and storage class name.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\n...\nstorageConfigs:\n  - mountPath: /opt/nifi/nifi-current/logs\n    name: logs\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/data\n    name: data\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 50Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/extensions\n    name: extensions-repository\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 5Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/flowfile_repository\n    name: flowfile-repository\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/nifi-current/conf\n    name: conf\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 5Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/content_repository\n    name: content-repository\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 500Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/provenance_repository\n    name: provenance-repository\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 500Gi\n      storageClassName: ssd-wait\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Node Read-Only Settings - YAML\nDESCRIPTION: This YAML snippet defines read-only configurations for a NiFi node. It sets an override for the UI banner text.  The `readOnlyConfig` section specifies settings that trigger rolling upgrades when changed. This includes the `overrideConfigs` section, where `nifi.ui.banner.text` is set to customize the NiFi UI.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/5_references/1_nifi_cluster/4_node.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n    - id: 0\n      # nodeConfigGroup can be used to ease the node configuration, if set only the id is required\n      nodeConfigGroup: \"default_group\"\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        nifiProperties:\n          overrideConfigs: |\n            nifi.ui.banner.text=NiFiKop - Node 0\n      # node configuration\n#       nodeConfig:\n\n```\n\n----------------------------------------\n\nTITLE: Granting Cluster Admin Permissions to Current User\nDESCRIPTION: This snippet creates a cluster role binding that grants cluster administrator permissions to the currently configured gcloud user. It is necessary for managing RBAC rules to allow NiFiKop components to operate with administrative rights within the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/2_setup/2_platform_setup/1_gke.md#_snippet_3\n\nLANGUAGE: Shell script\nCODE:\n```\nkubectl create clusterrolebinding cluster-admin-binding \\\n    --clusterrole=cluster-admin \\\n    --user=$(gcloud config get-value core/account)\n```\n\n----------------------------------------\n\nTITLE: Listing All Helm Releases in Bash\nDESCRIPTION: This Bash command displays all Helm releases—including active, failed, and deleted—in the cluster. Requires Helm CLI. Output: comprehensive release state list.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --all\n```\n\n----------------------------------------\n\nTITLE: NiFi Cluster Configuration for Auto-scaling - YAML\nDESCRIPTION: This YAML configuration snippet defines part of a NiFi cluster deployment. It configures a Prometheus listener and defines a `NodeConfigGroup` named `auto_scaling` which includes resource limits, requests, and storage configurations for nodes.  This section is crucial for the integration with KEDA and for the cluster's ability to scale.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n...\nspec:\n  ...\n  listenersConfig:\n    internalListeners:\n    ...\n    - containerPort: 9090\n      name: prometheus\n      type: prometheus \n    ...\n  ...\n  nodeConfigGroups:\n    auto_scaling:\n      isNode: true\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 2Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n      serviceAccountName: external-dns\n      storageConfigs:\n        - mountPath: /opt/nifi/nifi-current/logs\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/data\n          name: data\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/extensions\n          name: extensions-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/flowfile_repository\n          name: flowfile-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/nifi-current/conf\n          name: conf\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/content_repository\n          name: content-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/provenance_repository\n          name: provenance-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n  ...\n```\n\n----------------------------------------\n\nTITLE: Defining Node Configuration YAML\nDESCRIPTION: This YAML snippet defines the configuration for a NiFi node, including settings for storage, image, and cluster behavior. It specifies parameters like `provenanceStorage`, `runAsUser`, `isNode`, and `podMetadata`.  The `storageConfigs` section defines persistent volume claims for storage. It also includes `externalVolumeConfigs` to mount external volumes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/5_references/1_nifi_cluster/3_node_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n   default_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      # Set this to true if the instance is a node in a cluster.\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#basic-cluster-setup\n      isNode: true\n      # Additionnal metadata to merge to the pod associated\n      podMetadata:\n        annotations:\n          node-annotation: \"node-annotation-value\"\n        labels:\n          node-label: \"node-label-value\"\n      # Docker image used by the operator to create the node associated\n      # https://hub.docker.com/r/apache/nifi/\n#      image: \"apache/nifi:1.11.2\"\n      # nodeAffinity can be specified, operator populates this value if new pvc added later to node\n      # https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity\n#      nodeAffinity:\n      # imagePullPolicy define the pull policy for NiFi cluster docker image\n      imagePullPolicy: IfNotPresent\n      # priorityClassName define the name of the priority class to be applied to these nodes\n      priorityClassName: \"example-priority-class-name\"\n      # externalVolumeConfigs specifies a list of volume to mount into the main container.\n      externalVolumeConfigs:\n        - name: example-volume\n          mountPath: \"/opt/nifi/example\"\n          secret:\n            secretName: \"raw-controller\"\n      # storageConfigs specifies the node related configs\n      storageConfigs:\n        # Name of the storage config, used to name PV to reuse into sidecars for example.\n        - name: provenance-repository\n          # Path where the volume will be mount into the main nifi container inside the pod.\n          mountPath: \"/opt/nifi/provenance_repository\"\n          # Kubernetes PVC spec\n          # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n```\n\n----------------------------------------\n\nTITLE: Scaling Up a NiFi Cluster by Adding a Node (YAML)\nDESCRIPTION: This YAML snippet demonstrates how to scale up a NiFi cluster managed by NiFiKop by adding a new node definition (with id 25) to the `spec.nodes` list within the `NifiCluster` custom resource. The new node uses the `default_group` configuration. It's crucial that the `Node.Id` is unique within the list.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.12.1\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n    - id: 2\n      nodeConfigGroup: \"default_group\"\n# >>>> START: The new node\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n# <<<< END\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: Schema Documentation for ListenersConfig\nDESCRIPTION: The schema details for the ListenersConfig object, including fields for internalListeners, sslSecrets, clusterDomain, and useExternalDNS. It specifies types, descriptions, and default values for configuration validation, facilitating structured deployment of Nifi listeners with security and cluster considerations.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/5_references/1_nifi_cluster/6_listeners_config.md#_snippet_1\n\n\n\n----------------------------------------\n\nTITLE: Configuring NiFiCluster for Autoscaling/Prometheus - YAML\nDESCRIPTION: Snippet showing relevant parts of a `NifiCluster` YAML definition. It configures a Prometheus listener on port 9090 and defines a `nodeConfigGroup` named `auto_scaling` with specific resource requests/limits and storage volumes, intended for nodes managed by the autoscaler. Requires the NiFiKop operator.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n...\nspec:\n  ...\n  listenersConfig:\n    internalListeners:\n    ...\n    - containerPort: 9090\n      name: prometheus\n      type: prometheus \n    ...\n  ...\n  nodeConfigGroups:\n    auto_scaling:\n      isNode: true\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 2Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n      serviceAccountName: external-dns\n      storageConfigs:\n        - mountPath: /opt/nifi/nifi-current/logs\n          name: logs\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/data\n          name: data\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/extensions\n          name: extensions-repository\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/flowfile_repository\n          name: flowfile-repository\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/nifi-current/conf\n          name: conf\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/content_repository\n          name: content-repository\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/provenance_repository\n          name: provenance-repository\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n  ...\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Cluster for Autoscaling\nDESCRIPTION: This YAML snippet defines the specification of a NiFi cluster, including the configuration for Prometheus and the setup of a specific `NodeConfigGroup` named `auto_scaling` designed for autoscaling. It configures resource limits, storage, and service accounts.  It's a part of the NiFi cluster definition and is a prerequisite to use the autoscaler.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\n...\nspec:\n  ...\n  listenersConfig:\n    internalListeners:\n    ...\n    - containerPort: 9090\n      name: prometheus\n      type: prometheus \n    ...\n  ...\n  nodeConfigGroups:\n    auto_scaling:\n      isNode: true\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 2Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n      serviceAccountName: external-dns\n      storageConfigs:\n        - mountPath: /opt/nifi/nifi-current/logs\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/data\n          name: data\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/extensions\n          name: extensions-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/flowfile_repository\n          name: flowfile-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/nifi-current/conf\n          name: conf\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/content_repository\n          name: content-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/provenance_repository\n          name: provenance-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n  ...\n```\n\n----------------------------------------\n\nTITLE: Adding Node to NiFiCluster Spec - NiFiKop - YAML\nDESCRIPTION: This YAML snippet shows the `NifiCluster` custom resource definition with a new node added for scaling up. It includes `id: 25` in the `spec.nodes` list, referencing the `default_group` configuration. Applying this manifest tells the NiFiKop operator to provision and add node 25 to the NiFi cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.12.1\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n    - id: 2\n      nodeConfigGroup: \"default_group\"\n# >>>> START: The new node\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n# <<<< END\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: Port Forwarding to Prometheus\nDESCRIPTION: This command sets up port forwarding to the Prometheus service.  It forwards local port 9090 to the Prometheus service, allowing you to access the Prometheus web interface. You will then be able to check if the metrics are being scraped correctly.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_11\n\nLANGUAGE: console\nCODE:\n```\nkubectl port-forward -n monitoring-system service/prometheus-operated 9090:9090\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFi CustomResourceDefinitions with kubectl Bash\nDESCRIPTION: This snippet applies several NiFiKop CRDs to the Kubernetes cluster using kubectl. Each command references a YAML manifest for a unique custom resource. Requires kubectl and a configured kubeconfig file, and the user must have cluster-level permissions to apply CRDs. Expected result is the registration of six NiFi CustomResourceDefinitions in the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/6_contributing/1_developer_guide.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\n```\n\n----------------------------------------\n\nTITLE: Listing All Helm Releases\nDESCRIPTION: Command to show all Helm releases, including currently deployed, deleted, and failed deployments. Provides a complete view of Helm activity in the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --all\n```\n\n----------------------------------------\n\nTITLE: Installing Nifikop with Release Name\nDESCRIPTION: This is the base command for installing the Nifikop Helm chart. The `<release name>` is a placeholder for the desired release name. This command installs the chart from the specified repository.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nhelm install <release name> konpyutaika/nifikop\n```\n\n----------------------------------------\n\nTITLE: Viewing Kubernetes Services Exposing NiFi Listeners (Console Command)\nDESCRIPTION: This console command output demonstrates how to verify which Kubernetes services have been created to expose NiFi listeners. The command 'kubectl get services' will show service names, types (e.g., LoadBalancer), cluster IPs, external IPs, ports mapped from internal listeners, and uptime. No dependencies except kubectl and access to the target Kubernetes cluster. The displayed table helps confirm that expected listeners (e.g., https and http-tracking) are available via the assigned ports and external IPs, with each port mapping shown clearly for troubleshooting and validation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nkubectl get services\nNAME                TYPE           CLUSTER-IP    EXTERNAL-IP      PORT(S)                                AGE\ncluster-access      LoadBalancer   10.88.21.98   35.180.241.132   443:30421/TCP,80:30521/TCP             20d\n```\n\n----------------------------------------\n\nTITLE: Purging a Helm Release\nDESCRIPTION: This command purges a Helm release, which permanently removes all release history. The `nifikop` is the release name.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nhelm delete --purge nifikop\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Listeners Configuration in YAML\nDESCRIPTION: This YAML snippet defines the listeners configuration for a NiFi instance. It specifies internal listeners for https, cluster communication, site-to-site (s2s) communication, and Prometheus metrics. It also configures SSL secrets, including the secret name and whether to create the secrets using a certificate manager. This configuration is crucial for defining how NiFi is accessed internally and secured with SSL/TLS.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/5_references/1_nifi_cluster/6_listeners_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    - type: \"https\"\n      name: \"https\"\n      containerPort: 8443\n    - type: \"cluster\"\n      name: \"cluster\"\n      containerPort: 6007\n    - type: \"s2s\"\n      name: \"s2s\"\n      containerPort: 10000\n    - type: \"prometheus\"\n      name: \"prometheus\"\n      containerPort: 9090\n  sslSecrets:\n    tlsSecretName: \"test-nifikop\"\n    create: true\n```\n\n----------------------------------------\n\nTITLE: NiFiKop CRD Conversion Webhook Configuration\nDESCRIPTION: This YAML snippet shows the configuration needed in the CRD definition to enable the conversion webhook. This is required to handle resource conversions from `v1alpha1` to `v1`. It defines annotations and the webhook strategy including the service details.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/2_deploy_nifikop/1_quick_start.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio HTTP Gateway YAML\nDESCRIPTION: This YAML snippet defines an Istio Gateway resource. It configures the Istio ingress gateway selector to listen for HTTP traffic on port 80 for the specified domain, allowing external access to the service mesh. It serves as a prerequisite for routing HTTP traffic using a VirtualService.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: nifi-gateway\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - nifi.my-domain.com\n```\n\n----------------------------------------\n\nTITLE: Creating a Let's Encrypt Issuer for NiFi SSL\nDESCRIPTION: YAML configuration for creating a Let's Encrypt issuer using cert-manager to provide SSL certificates for NiFi.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/3_tasks/2_security/1_ssl.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: cert-manager.io/v1alpha2\nkind: Issuer\nmetadata:\n  name: letsencrypt-staging\nspec:\n  acme:\n    # You must replace this email address with your own.\n    # Let's Encrypt will use this to contact you about expiring\n    # certificates, and issues related to your account.\n    email: <your email address>\n    server: https://acme-staging-v02.api.letsencrypt.org/directory\n    privateKeySecretRef:\n      # Secret resource used to store the account's private key.\n      name: example-issuer-account-key\n    # Add a single challenge solver, HTTP01 using nginx\n    solvers:\n      - http01:\n          ingress:\n            ingressTemplate:\n              metadata:\n                annotations:\n                  \"external-dns.alpha.kubernetes.io/ttl\": \"5\"\n```\n\n----------------------------------------\n\nTITLE: Installing Node.js Dependencies for NiFiKop Migration Script - Bash\nDESCRIPTION: Commands to initialize a new Node.js project and install the required dependencies @kubernetes/client-node (version 0.16.3) and minimist (version 1.2.6). These packages are essential for interacting with Kubernetes APIs and parsing CLI arguments respectively.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm init -y\nnpm install @kubernetes/client-node@0.16.3 minimist@1.2.6\n```\n\n----------------------------------------\n\nTITLE: Defining Istio VirtualService for HTTP\nDESCRIPTION: This YAML snippet defines an Istio VirtualService, which is responsible for routing traffic intercepted by the previously defined Gateway. It specifies that requests to 'nifi.my-domain.com' should be routed to the 'nifi' service on port 8080 when the URI prefix is '/'. This configuration directs traffic from the external world (through the Gateway) to the NiFi service internally. The 'gateways' field links this VirtualService to the 'nifi-gateway'.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: nifi-vs\nspec:\n  gateways:\n  - nifi-gateway\n  hosts:\n  - nifi.my-domain.com\n  http:\n  - match:\n    - uri:\n        prefix: /\n    route:\n    - destination:\n        host: nifi\n        port:\n          number: 8080\n```\n```\n\n----------------------------------------\n\nTITLE: Listing Managed NiFi User Groups Using kubectl in Console\nDESCRIPTION: This console snippet demonstrates how to retrieve the list of managed NiFi user groups created and reconciled by the nifikop operator within the Kubernetes cluster namespace. Executing `kubectl get` on the NifiUserGroup custom resource reveals groups like managed-admins, managed-nodes, and managed-readers, which the operator automatically manages with appropriate user memberships. This command requires access to the Kubernetes cluster and the correct namespace to view the resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/2_manage_users_and_accesses/3_managed_groups.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nkubectl get -n nifikop nifiusergroups.nifi.konpyutaika.com \nNAME              AGE\nmanaged-admins    6d7h\nmanaged-nodes     6d7h\nmanaged-readers   6d7h\n```\n\n----------------------------------------\n\nTITLE: Creating a cert-manager Issuer for Let's Encrypt in Kubernetes YAML\nDESCRIPTION: Provides a Kubernetes YAML manifest to create a cert-manager Issuer resource that configures Let's Encrypt ACME server for automatic TLS certificate issuance. It specifies the email for LetsEncrypt account contact, server URL for staging environment, and the private key secret. It also configures HTTP01 challenge solver with nginx ingress and includes annotations for external DNS TTL management. Prerequisites include cert-manager installed and properly configured cluster DNS. Outputs a usable issuer for obtaining SSL certificates from Let's Encrypt.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: cert-manager.io/v1alpha2\nkind: Issuer\nmetadata:\n  name: letsencrypt-staging\nspec:\n  acme:\n    # You must replace this email address with your own.\n    # Let's Encrypt will use this to contact you about expiring\n    # certificates, and issues related to your account.\n    email: <your email address>\n    server: https://acme-staging-v02.api.letsencrypt.org/directory\n    privateKeySecretRef:\n      # Secret resource used to store the account's private key.\n      name: example-issuer-account-key\n    # Add a single challenge solver, HTTP01 using nginx\n    solvers:\n      - http01:\n          ingress:\n            ingressTemplate:\n              metadata:\n                annotations:\n                  \"external-dns.alpha.kubernetes.io/ttl\": \"5\"\n```\n\n----------------------------------------\n\nTITLE: Basic NifiNodeGroupAutoscaler Configuration in YAML\nDESCRIPTION: A complete example of a NifiNodeGroupAutoscaler resource that configures automatic scaling for a NiFi cluster node group. It references a NiFi cluster, identifies which node group to manage via a nodeConfigGroupId and label selector, and defines scaling strategies.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/5_references/7_nifi_nodegroup_autoscaler.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiNodeGroupAutoscaler\nmetadata:\n  name: nifinodegroupautoscaler-sample\nspec:\n  # contains the reference to the NifiCluster with the one the node group autoscaler is linked.\n  clusterRef:\n    name: nificluster-name\n    namespace: nifikop\n  # defines the id of the NodeConfig contained in NifiCluster.Spec.NodeConfigGroups\n  nodeConfigGroupId: default-node-group\n  # The selector used to identify nodes in NifiCluster.Spec.Nodes this autoscaler will manage\n  # Use Node.Labels in combination with this selector to clearly define which nodes will be managed by this autoscaler \n  nodeLabelsSelector: \n    matchLabels:\n      nifi_cr: nificluster-name\n      nifi_node_group: default-node-group\n  # the strategy used to decide how to add nodes to a nifi cluster\n  upscaleStrategy: simple\n  # the strategy used to decide how to remove nodes from an existing cluster\n  downscaleStrategy: lifo\n```\n\n----------------------------------------\n\nTITLE: Listing Managed Groups with kubectl\nDESCRIPTION: Command to list all NifiUserGroups created by the operator, including the managed-admins, managed-nodes, and managed-readers groups.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/2_manage_users_and_accesses/3_managed_groups.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nkubectl get -n nifikop nifiusergroups.nifi.konpyutaika.com \nNAME              AGE\nmanaged-admins    6d7h\nmanaged-nodes     6d7h\nmanaged-readers   6d7h\n```\n\n----------------------------------------\n\nTITLE: Applying OIDC and Identity Mapping Configs via NiFiKop (YAML)\nDESCRIPTION: Demonstrates how to configure OIDC and the recommended identity mapping properties within the `NifiCluster` custom resource used by NiFiKop. The `spec.readOnlyConfig.nifiProperties.overrideConfigs` field allows specifying custom `nifi.properties` content which will be appended or used to override existing configuration generated by the operator.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/3_tasks/2_security/2_authentication/1_oidc.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\n...\nspec:\n  ...\n  readOnlyConfig:\n    # NifiProperties configuration that will be applied to the node.\n    nifiProperties:\n      webProxyHosts:\n        - nifistandard2.trycatchlearn.fr:8443\n      # Additionnal nifi.properties configuration that will override the one produced based\n      # on template and configurations.\n      overrideConfigs: |\n        nifi.security.user.oidc.discovery.url=<oidc server discovery url>\n        nifi.security.user.oidc.client.id=<oidc client's id>\n        nifi.security.user.oidc.client.secret=<oidc client's secret>\n        nifi.security.identity.mapping.pattern.dn=CN=([^,]*)(?:, (?:O|OU)=.*)?\n        nifi.security.identity.mapping.value.dn=$1\n        nifi.security.identity.mapping.transform.dn=NONE\n      ...\n   ...\n...\n```\n\n----------------------------------------\n\nTITLE: NiFiCluster YAML configuration for scale down\nDESCRIPTION: This YAML configuration shows how to remove a node from the NiFi cluster by commenting out or deleting the corresponding entry in the `NifiCluster.Spec.Nodes` list. The example shows how to remove the node with ID 2.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  headlessServiceEnabled: true\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.11.3\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n# >>>> START: node removed\n#    - id: 2\n#      nodeConfigGroup: \"default_group\"\n# <<<< END\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: Helm Install with Parameter Override\nDESCRIPTION: This command demonstrates how to install the NiFiKop Helm chart and override a default parameter. It uses the `--set` flag to change the `image.tag` parameter to `asyncronous` during the installation. The `--replace` flag allows re-use of a charts release name.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/helm/nifikop/README.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\n$ helm install nifikop konpyutaika-incubator/nifikop --replace --set image.tag=asyncronous\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop with Custom Parameters\nDESCRIPTION: Command for installing the NiFiKop operator with custom parameters set via the --set flag. In this example, the operator is configured to watch only the 'nifikop' namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop --set namespaces={\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Listing Managed User Groups Kubectl Console\nDESCRIPTION: Shows how to use the `kubectl` command-line tool to list the `NifiUserGroup` resources that are managed by the NifiKop operator. This command specifically filters for resources in the 'nifikop' namespace and lists the names and ages of the managed groups, which typically include 'managed-admins', 'managed-nodes', and 'managed-readers'.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/3_manage_nifi/2_manage_users_and_accesses/3_managed_groups.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nkubectl get -n nifikop nifiusergroups.nifi.konpyutaika.com \n```\n\n----------------------------------------\n\nTITLE: Listing Deleted Helm Releases\nDESCRIPTION: Command to list all deleted Helm releases in the Kubernetes cluster. Helm keeps records of deleted releases to enable rollback functionality if needed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --deleted\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop with Helm in Dry Run Mode\nDESCRIPTION: Command for installing the NiFiKop operator in dry run mode with debug logging level and specific namespace configuration. Useful for testing the installation before actually deploying.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop \\\n    --dry-run \\\n    --set logLevel=Debug \\\n    --set namespaces={\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Defining Istio HTTPS Gateway with TLS Termination (YAML)\nDESCRIPTION: This YAML snippet defines an Istio Gateway for HTTPS traffic. It listens on port 443 using the HTTPS protocol and configures TLS termination in `SIMPLE` mode using the secret `my-secret`. It handles traffic for the host `nifi.my-domain.com`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: nifi-gateway\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 443\n      name: https\n      protocol: HTTPS\n    tls:\n      mode: SIMPLE\n      credentialName: my-secret\n    hosts:\n    - nifi.my-domain.com\n```\n\n----------------------------------------\n\nTITLE: Creating a NifiUser Resource for Client Certificate Authentication\nDESCRIPTION: Creates a client certificate signed by the cluster's CA for applications needing to authenticate to NiFi. The secret contains CA cert, user cert, and private key, and can be mounted into pods or exported for external use.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_5\n\nLANGUAGE: Console Commands\nCODE:\n```\ncat << EOF | kubectl apply -n nifi -f -\napiVersion:  nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: example-client\n  namespace: nifi\nspec:\n  clusterRef:\n    name: nifi\n  secretName: example-client-secret\nEOF\n```\n\n----------------------------------------\n\nTITLE: Pod Ports Configuration in Kubernetes\nDESCRIPTION: Generated pod YAML showing how internal listeners are translated into container ports in the Kubernetes pod specification. Each port is defined with a name, container port, and protocol.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n    ports:\n    - containerPort: 8443\n      name: https\n      protocol: TCP\n    - containerPort: 6007\n      name: cluster\n      protocol: TCP\n    - containerPort: 6342\n      name: load-balance\n      protocol: TCP\n    - containerPort: 10000\n      name: s2s\n      protocol: TCP\n    - containerPort: 9090\n      name: prometheus\n      protocol: TCP\n```\n\n----------------------------------------\n\nTITLE: Applying Nifikop CRDs Manually\nDESCRIPTION: Use these `kubectl apply` commands to manually install the Custom Resource Definitions (CRDs) required by Nifikop. This is necessary if you opt to skip CRD installation via the Helm chart (`--skip-crds`). These commands fetch the latest CRD definitions directly from the project's GitHub repository.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifinodegroupautoscalers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificonnections.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing cert-manager via Helm (Bash/helm)\nDESCRIPTION: Adds the Jetstack Helm repository, updates the local cache, and installs the cert-manager Helm chart (version v1.7.2) into the 'cert-manager' namespace. Assumes the namespace has already been created.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/2_deploy_nifikop/1_quick_start.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Add the jetstack helm repo\nhelm repo add jetstack https://charts.jetstack.io\nhelm repo update\n\n# You have to create the namespace before executing following command\nhelm install cert-manager \\\n    --namespace cert-manager \\\n    --version v1.7.2 jetstack/cert-manager\n```\n\n----------------------------------------\n\nTITLE: Retrieving Credentials for Kubectl (Shell)\nDESCRIPTION: This command retrieves the necessary credentials for `kubectl` to interact with the created GKE cluster. It uses the `gcloud container clusters get-credentials` command, which configures `kubectl` to use the specified cluster based on the `$CLUSTER_NAME`, `$GCP_ZONE`, and `$GCP_PROJECT` environment variables. This step is crucial for accessing and managing the Kubernetes cluster from the local machine using `kubectl`. Prerequisites: gcloud CLI and Kubernetes client installed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/2_setup/2_platform_setup/1_gke.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ncloud container clusters get-credentials $CLUSTER_NAME \\\n    --zone $GCP_ZONE \\\n    --project $GCP_PROJECT\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFi Cluster on OpenShift\nDESCRIPTION: Kubectl command to deploy a NiFi cluster on OpenShift using the updated configuration file with proper security context settings.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create -n nifi -f config/samples/openshift.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining Child NifiParameterContext\nDESCRIPTION: This YAML snippet demonstrates how to define a child `NifiParameterContext` that inherits from another. It specifies the API version, kind, and metadata like the name of the child context. It also references the parent using `inheritedParameterContexts` along with its configuration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/5_references/4_nifi_parameter_context.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n--- \napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle-child\nspec:\n  description: \"It is a child test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  inheritedParameterContexts:\n    - name: dataflow-lifecycle\n  parameters:\n    - name: test\n      value: toto-child\n      description: tutu (child)\n```\n\n----------------------------------------\n\nTITLE: Defining Nifi Listeners Configuration Using YAML\nDESCRIPTION: This YAML snippet configures multiple internal Nifi listeners, detailing their type, name, and container ports. It also includes an SSL secrets section specifying TLS secret details to enable secure communication. Required keys such as 'internalListeners' allow the definition of various listener port types like https, cluster, s2s, prometheus, and load-balance, along with custom listeners that specify a protocol if needed. This snippet depends on understanding Kubernetes networking protocols and expected port assignments for Nifi services.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/5_references/1_nifi_cluster/6_listeners_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  listenersConfig:\n    internalListeners:\n      - type: \"https\"\n        name: \"https\"\n        containerPort: 8443\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n      - type: \"prometheus\"\n        name: \"prometheus\"\n        containerPort: 9090\n      - type: \"load-balance\"\n        name: \"load-balance\"\n        containerPort: 6342\n      - name: \"my-custom-listener-port\"\n        containerPort: 1234\n        protocol: \"TCP\"\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n```\n\n----------------------------------------\n\nTITLE: ClusterReference YAML Schema for Cluster Linking\nDESCRIPTION: This schema models the ClusterReference object, including name and namespace, which link the NiFi user to a specific NiFi cluster instance. It ensures correct association between user resources and cluster components.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/5_references/2_nifi_user.md#_snippet_4\n\nLANGUAGE: YAML\nCODE:\n```\n`ClusterReference:\n  name: string\n  namespace: string`\n```\n\n----------------------------------------\n\nTITLE: Deploying Prometheus RBAC Resources\nDESCRIPTION: Creates ServiceAccount, ClusterRole, and ClusterRoleBinding resources to grant Prometheus permissions to monitor Kubernetes resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: prometheus\n  namespace: monitoring-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: prometheus\nrules:\n- apiGroups: [\"\"]\n  resources:\n  - nodes\n  - nodes/metrics\n  - services\n  - endpoints\n  - pods\n  verbs: [\"get\", \"list\", \"watch\"]\n- apiGroups: [\"\"]\n  resources:\n  - configmaps\n  verbs: [\"get\"]\n- apiGroups:\n  - networking.k8s.io\n  resources:\n  - ingresses\n  verbs: [\"get\", \"list\", \"watch\"]\n- nonResourceURLs: [\"/metrics\"]\n  verbs: [\"get\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: prometheus\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: prometheus\nsubjects:\n- kind: ServiceAccount\n  name: prometheus\n  namespace: monitoring-system\n```\n\n----------------------------------------\n\nTITLE: Verify Kubectl NiFiKop Plugin Installation\nDESCRIPTION: This snippet shows how to verify the installation of the kubectl-nifikop plugin. By running `kubectl nifikop`, it displays the available commands for the plugin. This output confirms that the plugin is correctly installed and recognized by kubectl.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/2_deploy_nifikop/3_kubectl_plugin.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\n$ kubectl nifikop\nUsage:\n  nifikop [command]\n\nAvailable Commands:\n  completion          Generate the autocompletion script for the specified shell\n  help                Help about any command\n  nificluster         \n  nificonnection      \n  nifidataflow        \n  nifigroupautoscaler \n  nifiregistryclient  \n  nifiuser            \n  nifiusergroup \n```\n\n----------------------------------------\n\nTITLE: Node Configuration YAML for NiFi K8s Operator\nDESCRIPTION: This YAML snippet defines the configuration for a NiFi node within a Kubernetes cluster, including provenance storage, user ID, node labels, volume mounts, and resource specifications. It serves as the primary configuration template to deploy NiFi nodes with specific settings and storage options.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/5_references/1_nifi_cluster/3_node_config.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n   default_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      # Set this to true if the instance is a node in a cluster.\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#basic-cluster-setup\n      isNode: true\n      # Additionnal metadata to merge to the pod associated\n      podMetadata:\n        annotations:\n          node-annotation: \"node-annotation-value\"\n        labels:\n          node-label: \"node-label-value\"\n      # Docker image used by the operator to create the node associated\n      # https://hub.docker.com/r/apache/nifi/\n#      image: \"apache/nifi:1.11.2\"\n      # nodeAffinity can be specified, operator populates this value if new pvc added later to node\n      # https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity\n#      nodeAffinity:\n      # imagePullPolicy define the pull policy for NiFi cluster docker image\n      imagePullPolicy: IfNotPresent\n      # priorityClassName define the name of the priority class to be applied to these nodes\n      priorityClassName: \"example-priority-class-name\"\n      # externalVolumeConfigs specifies a list of volume to mount into the main container.\n      externalVolumeConfigs:\n        - name: example-volume\n          mountPath: \"/opt/nifi/example\"\n          secret:\n            secretName: \"raw-controller\"\n      # storageConfigs specifies the node related configs\n      storageConfigs:\n        # Name of the storage config, used to name PV to reuse into sidecars for example.\n        - name: provenance-repository\n          # Path where the volume will be mount into the main nifi container inside the pod.\n          mountPath: \"/opt/nifi/provenance_repository\"\n          # Kubernetes PVC spec\n          # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n```\n\n----------------------------------------\n\nTITLE: Defining an External NifiCluster Resource in YAML\nDESCRIPTION: This YAML snippet demonstrates how to define a NifiCluster resource to represent an external NiFi cluster. It includes essential fields like `rootProcessGroupId`, `nodeURITemplate`, `nodes`, `type` (set to 'external'), `clientType` (basic or tls) and `secretRef` to reference authentication credentials.  The `nodeURITemplate` and `nodes` fields are used to dynamically generate node hostnames.  The `type` field specifies that the cluster is external, and the `clientType` specifies the authentication method.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/3_external_cluster.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: externalcluster\nspec:\n  # rootProcessGroupId contains the uuid of the root process group for this cluster.\n  rootProcessGroupId: 'd37bee03-017a-1000-cff7-4eaaa82266b7'\n  # nodeURITemplate used to dynamically compute node uri.\n  nodeURITemplate: 'nifi0%d.integ.mapreduce.m0.p.fti.net:9090'\n  # all node requiresunique id\n  nodes:\n    - id: 1\n    - id: 2\n    - id: 3\n  # type defines if the cluster is internal (i.e manager by the operator) or external.\n  # :Enum={\"external\",\"internal\"}\n  type: 'external'\n  # clientType defines if the operator will use basic or tls authentication to query the NiFi cluster.\n  # Enum={\"tls\",\"basic\"}\n  clientType: 'basic'\n  # secretRef reference the secret containing the informations required to authenticate to the cluster.\n  secretRef:\n    name: nifikop-credentials\n    namespace: nifikop-nifi\n```\n\n----------------------------------------\n\nTITLE: Deleting Nifikop Resources with kubectl\nDESCRIPTION: This set of kubectl commands is used to delete the Nifi clusters and Custom Resource Definitions (CRDs) created by Nifikop. It removes the securednificluster and both nificlusters.nifi.orange.com and nifiusers.nifi.orange.com CRDs from the specified namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/blog/2020-05-20-secured_nifi_cluster_on_gcp.md#_snippet_12\n\nLANGUAGE: consol\nCODE:\n```\nkubectl delete nificlusters.nifi.orange.com securednificluster -n nifi\nkubectl delete crds nificlusters.nifi.orange.com\nkubectl delete crds nifiusers.nifi.orange.com\n./destroy.sh <service account key's path>\n```\n\n----------------------------------------\n\nTITLE: Defining an External NiFi Cluster Resource in Kubernetes (YAML)\nDESCRIPTION: This YAML manifest defines a `NifiCluster` Kubernetes custom resource (CR) for an externally managed NiFi cluster. It specifies essential details for the NiFi operator to connect and interact with the cluster, including the root process group ID, a template for node URIs, node identifiers, setting the type explicitly to 'external', defining the client authentication method ('basic' or 'tls'), and referencing a Kubernetes secret containing the necessary credentials.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/1_manage_clusters/3_external_cluster.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: externalcluster\nspec:\n  # rootProcessGroupId contains the uuid of the root process group for this cluster.\n  rootProcessGroupId: 'd37bee03-017a-1000-cff7-4eaaa82266b7'\n  # nodeURITemplate used to dynamically compute node uri.\n  nodeURITemplate: 'nifi0%d.integ.mapreduce.m0.p.fti.net:9090'\n  # all node requiresunique id\n  nodes:\n    - id: 1\n    - id: 2\n    - id: 3\n  # type defines if the cluster is internal (i.e manager by the operator) or external.\n  # :Enum={\"external\",\"internal\"}\n  type: 'external'\n  # clientType defines if the operator will use basic or tls authentication to query the NiFi cluster.\n  # Enum={\"tls\",\"basic\"}\n  clientType: 'basic'\n  # secretRef reference the secret containing the informations required to authenticate to the cluster.\n  secretRef:\n    name: nifikop-credentials\n    namespace: nifikop-nifi\n```\n\n----------------------------------------\n\nTITLE: Adding KedaCore Helm Repository in Console\nDESCRIPTION: This code snippet demonstrates how to add the KedaCore Helm chart repository using a console command. There are no prerequisites other than having Helm installed and set up on your system. This command updates your local Helm repositories to include the KEDA charts, which is required before installing KEDA.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nhelm repo add kedacore https://kedacore.github.io/charts\n```\n\n----------------------------------------\n\nTITLE: Configuring kubectl for EKS\nDESCRIPTION: Command to write the EKS cluster's kubeconfig to the local configuration file, making the context available to kubectl. Requires AWS CLI and eksctl to be installed and configured.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/2_deploy_nifikop/1_quick_start.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\neksctl utils write-kubeconfig --cluster=${CLUSTER NAME}\n```\n\n----------------------------------------\n\nTITLE: Configuring SSL in NiFiCluster YAML\nDESCRIPTION: Basic configuration example for securing a NiFi cluster with SSL. The configuration specifies web proxy hosts, internal listeners for different connection types, and SSL secret settings.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  readOnlyConfig:\n    # NifiProperties configuration that will be applied to the node.\n    nifiProperties:\n      webProxyHosts:\n        - nifistandard2.trycatchlearn.fr:8443\n        ...\n  ...\n  listenersConfig:\n    internalListeners:\n      - type: \"https\"\n        name: \"https\"\n        containerPort: 8443\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n```\n\n----------------------------------------\n\nTITLE: Configuring Conversion Webhook for Nifikop CRDs - YAML\nDESCRIPTION: This YAML snippet shows how to configure webhook conversion for Nifikop CRDs, enabling automatic resource version upgrades. It injects CA certificates via cert-manager annotations and configures webhook client service details. Prerequisites include cert-manager installed, a valid certificate and webhook service. Parameters are namespace, certificate_name, and webhook_service_name, passed as Helm values or environment variables. The CRD YAML must be updated before applying.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n```\n\n----------------------------------------\n\nTITLE: Configuring GKE credentials for kubectl access\nDESCRIPTION: Command to configure kubectl to connect to a Google Kubernetes Engine (GKE) cluster. This uses gcloud to authenticate and set up the proper kubectl context for accessing the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/2_deploy_nifikop/1_quick_start.md#_snippet_6\n\nLANGUAGE: console\nCODE:\n```\ngcloud container clusters get-credentials CLUSTER_NAME --zone ZONE_NAME --project PROJECT_NAME.\n```\n\n----------------------------------------\n\nTITLE: Deploying Operator Using Helm\nDESCRIPTION: Installs the NiFiKop operator Helm chart with a specified image tag into a Kubernetes namespace, based on a locally built container image.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/6_contributing/1_developer_guide.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nhelm install skeleton ./helm/nifikop \\ --set image.tag=v0.5.1-release \\ --namespace-{\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Updating NiFi OpenShift Sample Configuration UID with sed - Bash\nDESCRIPTION: Updates the sample OpenShift manifest file for NiFi by replacing a placeholder UID value ('1000690000') with the actual UID obtained from the namespace. This ensures Pod security context is correctly set to run with the proper user permissions on OpenShift.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nsed -i \"s/1000690000/$uid/g\" config/samples/openshift.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Kubernetes StorageClass for NiFi (YAML)\nDESCRIPTION: Defines a Kubernetes StorageClass named 'exampleStorageclass' using the GCE Persistent Disk provisioner ('kubernetes.io/gce-pd'). It specifies 'pd-standard' type, 'Delete' reclaim policy, and crucially sets 'volumeBindingMode' to 'WaitForFirstConsumer' as recommended for NiFi deployments to ensure volumes are provisioned in the correct availability zone.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: exampleStorageclass\nparameters:\n  type: pd-standard\nprovisioner: kubernetes.io/gce-pd\nreclaimPolicy: Delete\nvolumeBindingMode: WaitForFirstConsumer\n```\n\n----------------------------------------\n\nTITLE: Installing Nifikop with Helm Dry Run Option - Bash\nDESCRIPTION: Executes a dry run of the Helm installation for Nifikop using the --dry-run flag, setting the log level to Debug and targeting the nifikop namespace. This prevents any changes to the cluster and outputs a preview of the installation. Dependencies: Helm CLI, access to the konpyutaika/nifikop chart repository, and permissions to list or install resources in the specified namespace. Takes logLevel and namespaces as main configurable parameters.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop \\\n    --dry-run \\\n    --set logLevel=Debug \\\n    --set namespaces={\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop Operator with runAsUser UID on OpenShift Using Helm in Bash\nDESCRIPTION: Helm install command for deploying the NiFiKop operator on OpenShift specifying the UID retrieved from the namespace annotation to comply with the restricted SCC requirements. It also specifies CPU, memory resource requests and limits, target namespaces, and the operator image tag.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/2_deploy_nifikop/1_quick_start.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop \\\n    nifikop \\\n    --namespace=nifi \\\n    --version 1.1.1 \\\n    --set image.tag=v1.1.1-release \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set namespaces={\"nifi\"} \\\n    --set runAsUser=$uid\n```\n\n----------------------------------------\n\nTITLE: Creating a Let's Encrypt Issuer for NiFi SSL Certificates\nDESCRIPTION: YAML configuration for setting up a Let's Encrypt certificate issuer for NiFi SSL certificates. This uses cert-manager's HTTP01 challenge solver and external-dns for DNS management.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: cert-manager.io/v1alpha2\nkind: Issuer\nmetadata:\n  name: letsencrypt-staging\nspec:\n  acme:\n    # You must replace this email address with your own.\n    # Let's Encrypt will use this to contact you about expiring\n    # certificates, and issues related to your account.\n    email: <your email address>\n    server: https://acme-staging-v02.api.letsencrypt.org/directory\n    privateKeySecretRef:\n      # Secret resource used to store the account's private key.\n      name: example-issuer-account-key\n    # Add a single challenge solver, HTTP01 using nginx\n    solvers:\n      - http01:\n          ingress:\n            ingressTemplate:\n              metadata:\n                annotations:\n                  \"external-dns.alpha.kubernetes.io/ttl\": \"5\"\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Identity Mapping Properties in Shell\nDESCRIPTION: Provides recommended `nifi.properties` settings for identity mapping patterns (DN) to support multiple OIDC identity providers in Apache NiFi. These properties define how user identities extracted from certificates or tokens are mapped to NiFi user identities.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/5_users_authentication/1_oidc.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nnifi.security.identity.mapping.pattern.dn=CN=([^,]*)(?:, (?:O|OU)=.*)?\nnifi.security.identity.mapping.value.dn=$1\nnifi.security.identity.mapping.transform.dn=NONE\n```\n\n----------------------------------------\n\nTITLE: Configuring CRD Webhook Annotations\nDESCRIPTION: This snippet demonstrates the necessary annotations needed within the CRD definition to enable the conversion webhook functionality. This is specifically for handling resource conversion from `v1alpha1` to `v1`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n```\n\n----------------------------------------\n\nTITLE: Defining an External NifiCluster Resource - YAML\nDESCRIPTION: This YAML snippet defines a NifiCluster resource configured to represent an external NiFi cluster. It specifies the root process group ID, node URI template, node IDs, cluster type (external), client type (basic), and a reference to a secret containing authentication credentials. The operator uses this information to interact with the external NiFi cluster's API.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/1_manage_clusters/3_external_cluster.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: externalcluster\nspec:\n  # rootProcessGroupId contains the uuid of the root process group for this cluster.\n  rootProcessGroupId: 'd37bee03-017a-1000-cff7-4eaaa82266b7'\n  # nodeURITemplate used to dynamically compute node uri.\n  nodeURITemplate: 'nifi0%d.integ.mapreduce.m0.p.fti.net:9090'\n  # all node requiresunique id\n  nodes:\n    - id: 1\n    - id: 2\n    - id: 3\n  # type defines if the cluster is internal (i.e manager by the operator) or external.\n  # :Enum={\"external\",\"internal\"}\n  type: 'external'\n  # clientType defines if the operator will use basic or tls authentication to query the NiFi cluster.\n  # Enum={\"tls\",\"basic\"}\n  clientType: 'basic'\n  # secretRef reference the secret containing the informations required to authenticate to the cluster.\n  secretRef:\n    name: nifikop-credentials\n    namespace: nifikop-nifi\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi UserGroup in YAML\nDESCRIPTION: This YAML snippet defines a `NifiUserGroup` resource.  It specifies the API version, kind, metadata (name), and the specification, which includes references to the NifiCluster (`clusterRef`), a list of NifiUsers (`usersRef`), and access policies (`accessPolicies`).  The `clusterRef` specifies the name and namespace of the NiFi cluster. The `usersRef` specifies list of the users part of the group. The `accessPolicies` defines the access level given to the group.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/5_references/6_nifi_usergroup.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n    - name: nc-controller.nifikop.mgt.cluster.local\n  accessPolicies:\n    - type: global\n      action: read\n      resource: /counters\n```\n\n----------------------------------------\n\nTITLE: Deleting Nifikop CRDs Manually in Bash\nDESCRIPTION: This Bash snippet shows how to manually delete Nifikop-related CRDs via kubectl. Dependencies: kubectl CLI, appropriate permissions. Inputs: CRD names; outputs: CRDs removed from the cluster. WARNING: Deleting a CRD will remove all custom resources of that type.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete crd nificlusters.nifi.konpyutaika.com\nkubectl delete crd nifiusers.nifi.konpyutaika.com\nkubectl delete crd nifiusergroups.nifi.konpyutaika.com\nkubectl delete crd nifiregistryclients.nifi.konpyutaika.com\nkubectl delete crd nifiparametercontexts.nifi.konpyutaika.com\nkubectl delete crd nifidataflows.nifi.konpyutaika.com\n```\n\n----------------------------------------\n\nTITLE: Creating a NiFiUser for Client Certificate Generation (Bash/kubectl)\nDESCRIPTION: Uses `kubectl apply` with a here-document (`cat << EOF ... EOF`) to create a `NifiUser` custom resource named `example-client` in the `nifi` namespace. This resource references the NiFi cluster named `nifi` and specifies that the generated client credentials (certificates signed by the cluster's CA) should be stored in a Kubernetes secret named `example-client-secret`. This automates the process of creating client identities for interacting with the secure NiFi cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncat << EOF | kubectl apply -n nifi -f -\napiVersion:  nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: example-client\n  namespace: nifi\nspec:\n  clusterRef:\n    name: nifi\n  secretName: example-client-secret\nEOF\n```\n\n----------------------------------------\n\nTITLE: Manually Applying Nifikop CRDs using kubectl\nDESCRIPTION: Shows how to manually apply the necessary Nifikop Custom Resource Definitions (CRDs) to a Kubernetes cluster using `kubectl apply`. This is required if installing the Helm chart with the `--skip-crds` flag. It lists commands for applying CRDs like `nificlusters`, `nifiusers`, `nifiusergroups`, etc.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifinodegroupautoscalers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificonnections.yaml\n```\n\n----------------------------------------\n\nTITLE: Checking New Node Resources (kubectl)\nDESCRIPTION: Uses kubectl to list Kubernetes resources (pods, configmaps, and persistent volume claims) filtered by the label 'nodeId=25', demonstrating that the operator has created the necessary resources for the new NiFi node.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl get pods,configmap,pvc -l nodeId=25\nNAME                          READY   STATUS    RESTARTS   AGE\npod/simplenifi-25-nodem5jh4   1/1     Running   0          11m\n\nNAME                             DATA   AGE\nconfigmap/simplenifi-config-25   7      11m\n\nNAME                                               STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\npersistentvolumeclaim/simplenifi-25-storagehwn24   Bound    pvc-7da86076-728e-11ea-846d-42010a8400f2   10Gi       RWO            standard       11m\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFiKop on OpenShift with proper security context\nDESCRIPTION: Installs NiFiKop v1.1.1 on OpenShift, setting the runAsUser parameter to match the namespace UID range to comply with OpenShift's restricted Security Context Constraints (SCC) requirements.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/2_deploy_nifikop/1_quick_start.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop \\\n    nifikop \\\n    --namespace=nifi \\\n    --version 1.1.1 \\\n    --set image.tag=v1.1.1-release \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set namespaces={\"nifi\"} \\\n    --set runAsUser=$uid\n```\n\n----------------------------------------\n\nTITLE: Defining NifiCluster for Node Removal (YAML)\nDESCRIPTION: Defines the `NifiCluster` custom resource manifest modified to trigger a graceful scale-down. This snippet shows the `spec.nodes` array with node ID 2 commented out or removed. The NiFiKop operator detects this change upon application and initiates the documented graceful decommission process for the specified node.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  headlessServiceEnabled: true\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.11.3\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n# >>>> START: node removed\n#    - id: 2\n#      nodeConfigGroup: \"default_group\"\n# <<<< END\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: Adding Prometheus Helm Repository\nDESCRIPTION: Commands to add and update the Prometheus community Helm chart repository.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nhelm repo add prometheus https://prometheus-community.github.io/helm-charts\n```\n\nLANGUAGE: console\nCODE:\n```\nhelm repo update\n```\n\n----------------------------------------\n\nTITLE: Listing Deleted Helm Releases in Kubernetes\nDESCRIPTION: Helm command to view previously deleted releases in the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --deleted\n```\n\n----------------------------------------\n\nTITLE: Defining an External NifiCluster in YAML\nDESCRIPTION: This snippet provides a YAML example to declare an external NiFi cluster resource, including essential parameters such as root process group ID, node URI template, nodes, cluster type, client authentication type, and secret reference for credentials. It enables automation and centralized management of external NiFi clusters in different environments.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/3_external_cluster.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: externalcluster\nspec:\n  rootProcessGroupId: 'd37bee03-017a-1000-cff7-4eaaa82266b7'\n  nodeURITemplate: 'nifi0%d.integ.mapreduce.m0.p.fti.net:9090'\n  nodes:\n    - id: 1\n    - id: 2\n    - id: 3\n  type: 'external'\n  clientType: 'basic'\n  secretRef:\n    name: nifikop-credentials\n    namespace: nifikop-nifi\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi NodeGroup Autoscaler - YAML\nDESCRIPTION: This YAML configuration defines a `NifiNodeGroupAutoscaler` resource, which is a Kubernetes custom resource designed to manage the automatic scaling of a NiFi cluster's node group. It specifies the API version, kind, metadata (name), and the autoscaler's `spec`. The `spec` includes references to the NiFi cluster, node group ID, node labels for node selection, and strategies for scaling up and down. This configuration is essential for enabling automated scaling of NiFi nodes based on defined criteria. The `clusterRef` is a required field.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/5_references/7_nifi_nodegroup_autoscaler.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiNodeGroupAutoscaler\nmetadata:\n  name: nifinodegroupautoscaler-sample\nspec:\n  # contains the reference to the NifiCluster with the one the node group autoscaler is linked.\n  clusterRef:\n    name: nificluster-name\n    namespace: nifikop\n  # defines the id of the NodeConfig contained in NifiCluster.Spec.NodeConfigGroups\n  nodeConfigGroupId: default-node-group\n  # The selector used to identify nodes in NifiCluster.Spec.Nodes this autoscaler will manage\n  # Use Node.Labels in combination with this selector to clearly define which nodes will be managed by this autoscaler \n  nodeLabelsSelector: \n    matchLabels:\n      nifi_cr: nificluster-name\n      nifi_node_group: default-node-group\n  # the strategy used to decide how to add nodes to a nifi cluster\n  upscaleStrategy: simple\n  # the strategy used to decide how to remove nodes from an existing cluster\n  downscaleStrategy: lifo\n```\n```\n\n----------------------------------------\n\nTITLE: Declaring External NiFi Cluster Resource (YAML)\nDESCRIPTION: This YAML snippet defines a `NifiCluster` custom resource configured to represent an external NiFi cluster. It provides the necessary details for the NiFiKop operator to connect to the cluster's API, including the root process group ID, a template for constructing node URIs, a list of node IDs, the 'external' type declaration, the authentication method ('basic' or 'tls'), and a reference to a Kubernetes Secret containing authentication credentials.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/3_tasks/1_nifi_cluster/4_external_cluster.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: externalcluster\nspec:\n  # rootProcessGroupId contains the uuid of the root process group for this cluster.\n  rootProcessGroupId: 'd37bee03-017a-1000-cff7-4eaaa82266b7'\n  # nodeURITemplate used to dynamically compute node uri.\n  nodeURITemplate: 'nifi0%d.integ.mapreduce.m0.p.fti.net:9090'\n  # all node requiresunique id\n  nodes:\n    - id: 1\n    - id: 2\n    - id: 3\n  # type defines if the cluster is internal (i.e manager by the operator) or external.\n  # :Enum={\"external\",\"internal\"}\n  type: 'external'\n  # clientType defines if the operator will use basic or tls authentication to query the NiFi cluster.\n  # Enum={\"tls\",\"basic\"}\n  clientType: 'basic'\n  # secretRef reference the secret containing the informations required to authenticate to the cluster.\n  secretRef:\n    name: nifikop-credentials\n    namespace: nifikop-nifi\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Secret for NiFi Sensitive Parameters\nDESCRIPTION: Command to create a Kubernetes secret that holds sensitive parameters for NiFi. These parameters will be converted into sensitive parameters in the NiFi Parameter Context.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create secret generic secret-params \\\n    --from-literal=secret1=yop \\\n    --from-literal=secret2=yep \\\n    -n nifikop\n```\n\n----------------------------------------\n\nTITLE: Kubernetes Pod Port Configuration for NiFi Listeners in YAML\nDESCRIPTION: Shows the corresponding Kubernetes pod ports configuration reflecting the internalListeners declaration. Each entry specifies the containerPort number, port name, and TCP protocol. This mapping confirms the ports opened in the pod's container, used by Kubernetes to route traffic inside the cluster for the NiFi listeners.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n    ports:\n    - containerPort: 8443\n      name: https\n      protocol: TCP\n    - containerPort: 6007\n      name: cluster\n      protocol: TCP\n    - containerPort: 6342\n      name: load-balance\n      protocol: TCP\n    - containerPort: 10000\n      name: s2s\n      protocol: TCP\n    - containerPort: 9090\n      name: prometheus\n      protocol: TCP\n```\n\n----------------------------------------\n\nTITLE: Listing Deployed Helm Charts Using Bash\nDESCRIPTION: This simple Bash command lists all Helm chart releases currently deployed in the Kubernetes cluster, providing an overview of active charts for management and verification.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nhelm list\n```\n\n----------------------------------------\n\nTITLE: Kubernetes InitContainer for NiFi Sensitive Property Encryption Upgrade - YAML\nDESCRIPTION: This YAML snippet defines a Kubernetes initContainer that automatically runs the NiFi Encrypt-Config Tool inside an apache/nifi-toolkit container before starting the NiFi operator. It fetches the sensitive properties key from the nifi.properties file and applies the new encryption algorithm to both flow.json.gz and flow.xml.gz files. Dependencies include properly mounted volumes for NiFi configuration and data paths and the availability of the nifi-toolkit image. This container must be temporarily added to the NifiCluster spec during upgrade and removed afterward. Paths in volumeMounts should be adapted to the deployment specifics.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/7_upgrade_guides/4_v1.3.1_to_v1.4.0.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ninitContainers:\n  - image: \"apache/nifi-toolkit:latest\"\n    name: nifi-toolkit\n    imagePullPolicy: Always\n    command:\n      - \"sh\"\n      - \"-c\"\n      - \"NIFI_SENSITIVE_PROPS_KEY=$(grep 'nifi.sensitive.props.key' /opt/nifi/nifi-current/conf/nifi.properties | cut -d'=' -f2) && bin/encrypt-config.sh -n /opt/nifi/nifi-current/conf/nifi.properties -f /opt/nifi/data/flow.json.gz -x -A NIFI_PBKDF2_AES_GCM_256 -s $NIFI_SENSITIVE_PROPS_KEY; bin/encrypt-config.sh -n /opt/nifi/nifi-current/conf/nifi.properties -f /opt/nifi/data/flow.xml.gz -x -A NIFI_PBKDF2_AES_GCM_256 -s $NIFI_SENSITIVE_PROPS_KEY\"\n    volumeMounts:\n      - name: data\n        mountPath: /opt/nifi/data\n      - name: conf\n        mountPath: /opt/nifi/nifi-current/conf\n```\n\n----------------------------------------\n\nTITLE: Defining NifiCluster Resource for Scale Up (YAML)\nDESCRIPTION: This YAML snippet defines the desired state for a NiFi cluster managed by NiFiKop. It demonstrates how to add a new node with ID 25 to the existing list of nodes by modifying the `spec.nodes` field. This configuration is applied to trigger a scale-up operation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n  clusterManager: zookeeper\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.12.1\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n    - id: 2\n      nodeConfigGroup: \"default_group\"\n# >>>> START: The new node\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n# <<<< END\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: Mapping Internal Listeners to Pod Ports (YAML)\nDESCRIPTION: Represents the YAML configuration inside the Kubernetes pod manifest, defining the container ports, their names, and protocols for each internal listener, aligning with the listener setup. This configuration ensures that each service port is correctly exposed inside the pods for internal communication.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\nports:\n- containerPort: 8443\n  name: https\n  protocol: TCP\n- containerPort: 6007\n  name: cluster\n  protocol: TCP\n- containerPort: 6342\n  name: load-balance\n  protocol: TCP\n- containerPort: 10000\n  name: s2s\n  protocol: TCP\n- containerPort: 9090\n  name: prometheus\n  protocol: TCP\n```\n\n----------------------------------------\n\nTITLE: Go Struct Definition for NodeConfig in Kubernetes NiFi Deployment\nDESCRIPTION: This Go code defines the NodeConfig struct with associated fields representing configuration options for deploying NiFi nodes on Kubernetes. The struct includes various properties such as provenance storage limits, user IDs, node labels, resource requirements, and volume configurations, providing a programmatic way to generate or validate deployment configurations.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/5_references/1_nifi_cluster/3_node_config.md#_snippet_1\n\nLANGUAGE: Go\nCODE:\n```\ntype NodeConfig struct {\n    ProvenanceStorage     string                     `json:\"provenanceStorage\"` // Max amount of provenance data stored\n    RunAsUser             int64                      `json:\"runAsUser\"` // User ID to run NiFi\n    FsGroup               int64                      `json:\"fsGroup\"` // Group ID for volumes\n    IsNode                bool                       `json:\"isNode\"` // Indicates if node is part of a cluster\n    Image                 string                     `json:\"image\"` // Container image for NiFi\n    ImagePullPolicy       string                     `json:\"imagePullPolicy\"` // Policy for pulling images\n    NodeAffinity          *string                    `json:\"nodeAffinity\"` // Node affinity rules\n    StorageConfigs        []StorageConfig            `json:\"storageConfigs\"` // Storage configurations\n    ExternalVolumeConfigs []ExternalVolumeConfig     `json:\"externalVolumeConfigs\"` // Volume mounts\n    ServiceAccountName    string                     `json:\"serviceAccountName\"` // Service account name\n    ResourcesRequirements *ResourceRequirements        `json:\"resourcesRequirements\"` // Resource limits and requests\n    ImagePullSecrets      []LocalObjectReference       `json:\"imagePullSecrets\"` // Secrets for private registry\n    NodeSelector          map[string]string          `json:\"nodeSelector\"` // Node selector labels\n    Tolerations           []Toleration               `json:\"tolerations\"` // Tolerations for taints\n    PodMetadata           *Metadata                  `json:\"podMetadata\"` // Additional pod metadata\n    HostAliases           []HostAlias                `json:\"hostAliases\"` // Hosts file substitutions\n    PriorityClassName     string                     `json:\"priorityClassName\"` // Priority class\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Prometheus Community Helm Repository\nDESCRIPTION: Adds the official Prometheus community Helm chart repository to the local Helm configuration using the `helm repo add` command. This step is necessary to allow Helm to locate and install the Prometheus charts.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_6\n\nLANGUAGE: console\nCODE:\n```\nhelm repo add prometheus https://prometheus-community.github.io/helm-charts\n```\n\n----------------------------------------\n\nTITLE: Defining a NiFi Registry Client Resource in Kubernetes using YAML\nDESCRIPTION: This YAML snippet demonstrates how to define a NifiRegistryClient custom resource for integration with the nifikop operator. Dependencies include a running Kubernetes cluster, nifikop CRDs installed, and the relevant NiFi registry service reachable at the specified URI. The specification includes a cluster reference (clusterRef), a description, and a URI to the remote NiFi registry. 'name', 'namespace', and 'description' are customizable; 'clusterRef' and 'uri' are essential for connecting to the correct NiFi cluster and registry. Applying this resource creates or updates the NiFi registry client within the designated namespace. Limitations: the external NiFi registry must be accessible at the given URI and the nifikop operator must support the specified CRD version.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/5_references/3_nifi_registry_client.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiRegistryClient\nmetadata:\n  name: squidflow\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Squidflow demo\"\n  uri: \"http://nifi-registry:18080\"\n```\n\n----------------------------------------\n\nTITLE: Running NiFi Encrypt-Config Tool\nDESCRIPTION: This shell script uses the `encrypt-config.sh` tool from the NiFi toolkit to update the encryption algorithm in both `nifi.properties` and the flow configuration files (`flow.xml.gz`, `flow.json.gz`).  It requires the NiFi properties key, input files, and the new algorithm.  The `-x` flag likely indicates that the tool will overwrite the original configuration files. The `-A` flag specifies the new encryption algorithm, and `-s` specifies the sensitive property key.  This is a one-time execution.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/7_upgrade_guides/4_v1.3.1_to_v1.4.0.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nencrypt-config.sh -n nifi.properties -f flow.xml.gz -x -s PROPERTIES_KEY -A NIFI_PBKDF2_AES_GCM_256\nencrypt-config.sh -n nifi.properties -f flow.json.gz -x -s PROPERTIES_KEY -A NIFI_PBKDF2_AES_GCM_256\n```\n\n----------------------------------------\n\nTITLE: NiFiCluster Node Removal - YAML\nDESCRIPTION: This YAML snippet demonstrates how to remove a node from a NiFi cluster by commenting it out from the NifiCluster resource definition. It shows the `nodes` section with node id 2 commented out. After applying this configuration, the NiFiKop operator will initiate the graceful decommissioning process for that node.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/3_tasks/1_nifi_cluster/2_cluster_scaling.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  headlessServiceEnabled: true\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.11.3\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n# >>>> START: node removed\n#    - id: 2\n#      nodeConfigGroup: \"default_group\"\n# <<<< END\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: Creating Basic Authentication Secret (Console)\nDESCRIPTION: This console command uses `kubectl` to create a generic Kubernetes Secret named `nifikop-credentials`. This secret is intended to store files containing the username, password, and optional CA certificate required for the Nifikop operator to authenticate using basic authentication against an external NiFi cluster's REST API, as referenced by the `NifiCluster` resource's `spec.secretRef` field.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/1_manage_clusters/3_external_cluster.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nkubectl create secret generic nifikop-credentials \\\n  --from-file=username=./secrets/username\\\n  --from-file=password=./secrets/password\\\n  --from-file=ca.crt=./secrets/ca.crt\\\n  -n nifikop-nifi\n```\n\n----------------------------------------\n\nTITLE: CRD Conversion Webhook Configuration Patch YAML\nDESCRIPTION: This YAML snippet describes the patch needed for CRDs to enable version conversion via a webhook during the upgrade process. It specifies the webhook client configuration, strategy, and review versions, and relies on environment-specific variables for namespace and service name.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/7_upgrade_guides/3_v0.16.0_to_v1.0.0.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n\n```\n\n----------------------------------------\n\nTITLE: Install NiFiKop on OpenShift\nDESCRIPTION: This command installs NiFiKop using Helm on OpenShift, incorporating the `runAsUser=$uid` parameter to set the security context. It uses the same helm install command as previously, but includes the `runAsUser` parameter for OpenShift compatibility, which requires the `uid` variable to be defined.  Requires Helm and OpenShift-configured `kubectl`. The UID should be obtained from the namespace prior to running this command.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/2_deploy_nifikop/1_quick_start.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop \\\n    nifikop \\\n    --namespace=nifi \\\n    --version 1.1.1 \\\n    --set image.tag=v1.1.1-release \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set namespaces={\"nifi\"} \\\n    --set runAsUser=$uid\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi to use Custom Authorizer - Shell\nDESCRIPTION: This snippet shows how to configure NiFi to use the custom database authorizer. The `nifi.security.user.authorizer` property in nifi.properties is set to the identifier of the custom authorizer defined in the authorizers.xml file. This setting ensures that NiFi uses the specified custom authorizer for all authorization decisions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/6_users_authorization/1_custom_user_authorizer.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nnifi.security.user.authorizer=custom-database-authorizer\n```\n\n----------------------------------------\n\nTITLE: LogbackConfig Configuration Structure in Markdown\nDESCRIPTION: This table defines how to configure custom Logback settings in NiFiKop through either ConfigMap or Secret references, replacing the default template-based configuration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/5_references/1_nifi_cluster/2_read_only_config.md#_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n## LogbackConfig\n\n|Field|Type|Description|Required|Default|\n|-----|----|-----------|--------|--------|\n|replaceConfigMap|[ConfigmapReference](#configmapreference)|logback.xml configuration that will replace the one produced based on template.|No|nil|\n|replaceSecretConfig|[SecretConfigReference](#secretconfigreference)|logback.xml configuration that will replace the one produced based on template and overrideConfigMap.|No|nil|\n```\n\n----------------------------------------\n\nTITLE: Getting Status for the Nifikop Helm Deployment in Bash\nDESCRIPTION: This Bash command retrieves the status and health details for the 'nifikop' Helm release. Requires Helm CLI. Parameter: release name. Output: verbose deployment status.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nhelm status nifikop\n```\n\n----------------------------------------\n\nTITLE: Verifying Kubernetes Nodes in Deployed GKE Cluster Using kubectl Console Command\nDESCRIPTION: This kubectl command lists all nodes in the Kubernetes cluster, showing their name, status, roles, age, and Kubernetes version. It confirms the successful provisioning and readiness of GKE nodes after deployment. No additional parameters are required.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/blog/2020-05-20-secured_nifi_cluster_on_gcp.md#_snippet_3\n\nLANGUAGE: console\nCODE:\n```\nkubectl get nodes\nNAME                                                  STATUS   ROLES    AGE    VERSION\ngke-nifi-cluster-tracking-ptf20200520-a1aec8fe-2dl3   Ready    <none>   110m   v1.15.9-gke.24\ngke-nifi-cluster-tracking-ptf20200520-a1aec8fe-5bzb   Ready    <none>   110m   v1.15.9-gke.24\ngke-nifi-cluster-tracking-ptf20200520-a1aec8fe-5t3l   Ready    <none>   110m   v1.15.9-gke.24\ngke-nifi-cluster-tracking-ptf20200520-a1aec8fe-w86j   Ready    <none>   110m   v1.15.9-gke.24\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi External Services with Internal Listeners in YAML\nDESCRIPTION: Complete configuration example showing how to expose NiFi internal listeners to external users through Kubernetes services, including both standard listeners and custom endpoints.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    - type: \"https\"\n      name: \"https\"\n      containerPort: 8443\n    - type: \"cluster\"\n      name: \"cluster\"\n      containerPort: 6007\n    - type: \"s2s\"\n      name: \"s2s\"\n      containerPort: 10000\n    - type: \"prometheus\"\n      name: \"prometheus\"\n      containerPort: 9090\n    - type: \"load-balance\"\n      name: \"load-balance\"\n      containerPort: 6342\n    - name: \"http-tracking\"\n      containerPort: 8081\nexternalServices:\n  - name: cluster-access\n    spec:\n      portConfigs:\n        - internalListenerName: https\n          port: 443\n        - internalListenerName: http-tracking\n          port: 80\n      type: LoadBalancer\n```\n\n----------------------------------------\n\nTITLE: Creating ServiceMonitor for NiFi Metrics\nDESCRIPTION: YAML definition for a ServiceMonitor resource that configures Prometheus to scrape metrics from NiFi pods, including label handling and relabeling to preserve metadata from the Kubernetes pods.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: cluster\n  namespace: monitoring-system\n  labels:\n    app: nifi-cluster\n    nifi_cr: cluster\nspec:\n  selector:\n    matchLabels:\n      app: nifi\n      nifi_cr: cluster\n  namespaceSelector:\n    matchNames:\n      - clusters\n  endpoints:\n    - interval: 10s\n      port: prometheus\n      path: /metrics\n      honorLabels: true\n      relabelings:\n        - sourceLabels: [__meta_kubernetes_pod_ip]\n          separator: ;\n          regex: (.*)\n          targetLabel: pod_ip\n          replacement: $1\n          action: replace\n        - sourceLabels: [__meta_kubernetes_pod_label_nodeId]\n          separator: ;\n          regex: (.*)\n          targetLabel: nodeId\n          replacement: $1\n          action: replace\n        - sourceLabels: [__meta_kubernetes_pod_label_nifi_cr]\n          separator: ;\n          regex: (.*)\n          targetLabel: nifi_cr\n          replacement: $1\n          action: replace\n```\n\n----------------------------------------\n\nTITLE: Executing kubectl apply to update NiFi Cluster after node removal\nDESCRIPTION: This shell command applies the modified cluster configuration YAML that excludes a node, initiating the decommission process. Dependencies include kubectl and cluster access. The command triggers the removal of the specified node, following the decommissioning procedure as per the updated configuration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_4\n\nLANGUAGE: Shell\nCODE:\n```\nkubectl -n nifi apply -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Declaring Nodes with Different Configuration Groups\nDESCRIPTION: YAML configuration showing how to declare NiFi cluster nodes using predefined nodeConfigGroups or custom configurations. Demonstrates assigning nodes to different configuration groups.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n- id: 0\n  nodeConfigGroup: \"default_group\"\n- id: 2\n  nodeConfigGroup: \"high_mem_group\"\n- id: 3\n  nodeConfigGroup: \"high_mem_group\"\n- id: 5\n  nodeConfig:\n    resourcesRequirements:\n      limits:\n        cpu: \"2\"\n        memory: 3Gi\n      requests:\n        cpu: \"1\"\n        memory: 1Gi\n```\n\n----------------------------------------\n\nTITLE: Getting Kubernetes Services Output\nDESCRIPTION: This console command and its output demonstrate how to view the Kubernetes services created by the Nifikop operator based on the `externalServices` configuration. It shows a service named 'cluster-access' of type LoadBalancer, along with its cluster IP, external IP, and the port mappings (external:nodePort/protocol) for the exposed internal listeners.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nkubectl get services\nNAME                TYPE           CLUSTER-IP    EXTERNAL-IP      PORT(S)                                AGE\ncluster-access      LoadBalancer   10.88.21.98   35.180.241.132   443:30421/TCP,80:30521/TCP             20d\n```\n\n----------------------------------------\n\nTITLE: Dry Running Helm Install for NiFiKop (Bash)\nDESCRIPTION: This snippet executes a Helm install command in dry-run mode to simulate the deployment of the NiFiKop chart with custom values (e.g., `logLevel=Debug` and targeting namespaces `{\"nifikop\"}`). This allows users to preview what resources Helm would install without actually applying changes to the cluster. Requires Helm to be installed and does not alter the live cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/2_setup/3_install/1_customizable_install_with_helm.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop \\\n    --dry-run \\\n    --set logLevel=Debug \\\n    --set namespaces={\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Recommended NiFi Identity Mapping for OIDC (Shell)\nDESCRIPTION: Shell example of `nifi.properties` configurations recommended for enabling multiple identity provider support with OIDC by defining identity mapping patterns and transformations. These specific settings extract the Common Name (CN) from the Distinguished Name (DN) provided by the identity provider.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/1_manage_clusters/1_deploy_cluster/5_users_authentication/1_oidc.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnifi.security.identity.mapping.pattern.dn=CN=([^,]*)(?:, (?:O|OU)=.*)?\nnifi.security.identity.mapping.value.dn=$1\nnifi.security.identity.mapping.transform.dn=NONE\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFiKop on OpenShift (Bash)\nDESCRIPTION: This code snippet shows how to deploy NiFiKop on OpenShift. It retrieves the OpenShift supplemental group ID, and then installs the operator using Helm, using the retrieved ID. Requires OpenShift, a configured Kubernetes cluster, Helm installed and configured, and the `nifi` namespace created. It also requires the OpenShift CLI to retrieve the uid.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/2_deploy_nifikop/1_quick_start.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nuid=$(kubectl get namespace nifi -o=jsonpath='{.metadata.annotations.openshift\\.io/sa\\.scc\\.supplemental-groups}' | sed 's/\\/10000$//' | tr -d '[:space:]')\n\n```\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop \\\n    nifikop \\\n    --namespace=nifi \\\n    --version 1.1.1 \\\n    --set image.tag=v1.1.1-release \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set namespaces={\"{nifi}\"} \\\n    --set runAsUser=$uid\n```\n\n----------------------------------------\n\nTITLE: Installing Nifikop Chart (Set Parameters)\nDESCRIPTION: This snippet demonstrates installing the Nifikop chart using Helm, setting specific parameters such as the namespace.  It requires Helm and a valid Helm repository.  It takes `--set` parameters for configuring the installation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop --set namespaces={\\\"nifikop\\\"}\n```\n\n----------------------------------------\n\nTITLE: Defining NiFiRegistryClient Custom Resource\nDESCRIPTION: This snippet introduces the CRD for NiFiRegistryClient, enabling users to declare and manage NiFi registry clients within Kubernetes. It allows integration with external NiFi registries, facilitating deployment and lifecycle management through the controller's reconcile loop.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/3_manage_dataflows/0_design_principles.md#_snippet_0\n\nLANGUAGE: Markdown\nCODE:\n```\n- **NiFiRegistryClient:** Allowing you to declare a [NiFi registry client](https://nifi.apache.org/docs/nifi-registry-docs/html/getting-started.html#connect-nifi-to-the-registry).\n```\n\n----------------------------------------\n\nTITLE: Applying NiFiKop Custom Resource Definitions (CRDs) Manually Using Kubectl in Bash\nDESCRIPTION: Commands to manually apply NiFiKop CRDs using kubectl. These must be run if Helm is instructed to skip CRD installation. Each command applies a specific custom resource definition necessary for the NiFi Kubernetes operator to function correctly.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/2_setup/3_install/1_customizable_install_with_helm.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop Helm Chart\nDESCRIPTION: This command installs the NiFiKop Helm chart. It sets the image tag and namespace during installation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/6_contributing/1_developer_guide.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nhelm install skeleton ./helm/nifikop \\\n    --set image.tag=v0.5.1-release \\\n    --namespace-{\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Adding Bitnami Helm Repository\nDESCRIPTION: Adds the Bitnami Helm chart repository to your local Helm configuration. This repository contains the recommended Zookeeper chart used as a prerequisite for NiFi.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/2_setup/1_getting_started.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhelm repo add bitnami https://charts.bitnami.com/bitnami\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Internal Listeners (YAML)\nDESCRIPTION: Demonstrates how to add an additional `internalListener` without a predefined NiFi `type`. This example adds a listener named `http-tracking` on port 8081, suitable for exposing custom endpoints created by NiFi processors, such as an HTTP listener.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    ...\n    - name: \"http-tracking\"\n      containerPort: 8081\n```\n\n----------------------------------------\n\nTITLE: Configuring kubectl for GKE Cluster Access\nDESCRIPTION: Command to fetch credentials for a Google Kubernetes Engine (GKE) cluster and configure `kubectl` to use them. Requires `gcloud` CLI tool to be installed and authenticated. Replace CLUSTER_NAME, ZONE_NAME, and PROJECT_NAME with actual values.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/2_deploy_nifikop/1_quick_start.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\ngcloud container clusters get-credentials CLUSTER_NAME --zone ZONE_NAME --project PROJECT_NAME.\n```\n\n----------------------------------------\n\nTITLE: Defining Container Ports in a NiFi Kubernetes Pod - YAML\nDESCRIPTION: This YAML excerpt demonstrates how the defined internalListeners are mapped to actual container ports within the NiFi Kubernetes pod specification. Each port entry includes the containerPort number, a name for reference, and the communication protocol (TCP). This correspondence is automatically managed by the NiFi Kubernetes operator based on the listenersConfig settings. Inputs: port numbers and names as specified in configuration; Outputs: Kubernetes Pod spec with matching ports. Dependencies: Kubernetes manifest conventions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\n    ports:\n    - containerPort: 8443\n      name: https\n      protocol: TCP\n    - containerPort: 6007\n      name: cluster\n      protocol: TCP\n    - containerPort: 6342\n      name: load-balance\n      protocol: TCP\n    - containerPort: 10000\n      name: s2s\n      protocol: TCP\n    - containerPort: 9090\n      name: prometheus\n      protocol: TCP\n```\n\n----------------------------------------\n\nTITLE: Updating Helm Repositories (console)\nDESCRIPTION: This command updates the list of available charts from all configured Helm repositories, ensuring you have access to the latest versions before installing.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_7\n\nLANGUAGE: console\nCODE:\n```\nhelm repo update\n```\n\n----------------------------------------\n\nTITLE: Importing Theme Components in Markdown\nDESCRIPTION: Imports 'Tabs' and 'TabItem' components from the theme to structure content into tabbed sections, aiding readability and organization in documentation pages.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n```\n\n----------------------------------------\n\nTITLE: Installing cert-manager for Kubernetes cluster security using Bash\nDESCRIPTION: This snippet demonstrates two methods to install cert-manager, which manages TLS certificates in the Kubernetes cluster: directly applying manifest files using kubectl and using Helm 3 to install cert-manager along with its CustomResourceDefinitions (CRDs). Required dependencies include kubectl or Helm 3 CLI. Installing cert-manager v1.7.2 or later is necessary to issue certificates for secure NiFiKop cluster deployments.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/2_deploy_nifikop/1_quick_start.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f \\\n    https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.yaml\n```\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply --validate=false -f \\\n   https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.crds.yaml\n\nhelm repo add jetstack https://charts.jetstack.io\nhelm repo update\n\nhelm install cert-manager \\\n    --namespace cert-manager \\\n    --version v1.7.2 jetstack/cert-manager\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop with CRD Skip Option\nDESCRIPTION: Helm command to install NiFiKop without installing the CRDs, useful when the CRDs are already deployed in the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nhelm install --name nifikop ./helm/nifikop --set namespaces={\"nifikop\"} --skip-crds\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Non-NiFi Internal Listener in Kubernetes YAML\nDESCRIPTION: Illustrates adding an additional internal listener without specifying a 'type', indicating it is not related to NiFi’s internal behavior but exposes a custom port inside the pod. Useful for exposing custom NiFi processors or HTTP endpoints directly inside the container.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    ...\n    - name: \"http-tracking\"\n      containerPort: 8081\n```\n\n----------------------------------------\n\nTITLE: Deploying KEDA with Helm on Kubernetes - console\nDESCRIPTION: These console commands demonstrate how to add the KEDA Helm repository to your local Helm setup, update it to get the latest charts, and install KEDA into a dedicated Kubernetes namespace named 'keda'. The commands should be executed in order and require Helm and kubectl CLI tools configured for the target Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nhelm repo add kedacore https://kedacore.github.io/charts\nhelm repo update\nkubectl create namespace keda\nhelm install keda kedacore/keda --namespace keda\n```\n\n----------------------------------------\n\nTITLE: NifiNodeGroupAutoscaler Resource Definition Schema\nDESCRIPTION: This section describes the schema of the NifiNodeGroupAutoscaler resource, including its main fields, their types, optionality, and default values. It helps in understanding how to instantiate and manage the auto-scaling resource in Kubernetes, ensuring correct field usage and validation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/5_references/7_nifi_nodegroup_autoscaler.md#_snippet_1\n\n\n\n----------------------------------------\n\nTITLE: Configuration for TLS Authentication Secret in Kubernetes\nDESCRIPTION: Defines the required secret components for TLS client authentication, including private key, user certificate, CA certificate, and password. These elements facilitate secure communication with the NiFi cluster via TLS.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/3_external_cluster.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ntls.key\n(tls user private key)\ntls.crt\n(tls user certificate)\npassword\n(password for API access)\nca.crt\n(CA certificate for server trust)\ntruststore.jks\n(Truststore Java keystore)\nkeystore.jks\n(Keypair Java keystore)\n```\n\n----------------------------------------\n\nTITLE: Installing Nifikop Helm Chart (Dry Run)\nDESCRIPTION: Perform a dry run installation of the Nifikop Helm chart to see the Kubernetes manifests that would be generated without actually installing them. This command installs the chart named 'nifikop' from the 'konpyutaika/nifikop' repository, enables dry run, sets the log level to Debug, and specifies the target namespace(s).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop \\\n    --dry-run \\\n    --set logLevel=Debug \\\n    --set namespaces={\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Cluster with External Certificate Issuer\nDESCRIPTION: YAML configuration for setting up a NiFi cluster with an external certificate issuer like Let's Encrypt. This configuration specifies cluster security settings, DNS configuration, and the external issuer reference.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\n...\nspec:\n  ...\n  clusterSecure: true\n  siteToSiteSecure: true\n  ...\n  listenersConfig:\n    clusterDomain: <DNS zone name>\n    useExternalDNS: true\n    ...\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n      issuerRef:\n        name: letsencrypt-staging\n        kind: Issuer\n```\n\n----------------------------------------\n\nTITLE: Templated authorizers.xml Configuration\nDESCRIPTION: This YAML code snippet provides a template for NiFi's `authorizers.xml` file, designed to be used with NiFiKop. It includes configurations for both file-based and database-based user group providers and access policy providers. The template utilizes Go templating to dynamically inject node identities and other cluster-specific information at deployment time. It defines two authorizers: `StandardManagedAuthorizer` and a custom `DatabaseAuthorizer`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/6_users_authorization/1_custom_user_authorizer.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n{{- $nodeList := .NodeList }}\n{{- $clusterName := .ClusterName }}\n{{- $namespace := .Namespace }}<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>\n<authorizers>\n    <userGroupProvider>\n        <identifier>file-user-group-provider</identifier>\n        <class>org.apache.nifi.authorization.FileUserGroupProvider</class>\n        <property name=\"Users File\">../data/users.xml</property>\n        <property name=\"Legacy Authorized Users File\"></property>\n        <property name=\"Initial User Identity admin\">{{ .ControllerUser }}</property>\n{{- range $i, $host := .NodeList }}\n        <property name=\"Initial User Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n    </userGroupProvider>\n    <userGroupProvider>\n        <identifier>database-user-group-provider</identifier>\n        <class>my.custom.DatabaseUserGroupProvider</class>\n        <!-- Any extra configuration for this provider goes here -->\n{{- range $i, $host := .NodeList }}\n        <property name=\"Initial User Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n    </userGroupProvider>\n    <accessPolicyProvider>\n        <identifier>file-access-policy-provider</identifier>\n        <class>org.apache.nifi.authorization.FileAccessPolicyProvider</class>\n        <property name=\"User Group Provider\">file-user-group-provider</property>\n        <property name=\"Authorizations File\">../data/authorizations.xml</property>\n        <property name=\"Initial Admin Identity\">{{ .ControllerUser }}</property>\n        <property name=\"Legacy Authorized Users File\"></property>\n{{- range $i, $host := .NodeList }}\n        <property name=\"Node Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n\t\t<property name=\"Node Group\"></property>\n    </accessPolicyProvider>\n    <accessPolicyProvider>\n        <identifier>database-access-policy-provider</identifier>\n        <class>my.custom.DatabaseAccessPolicyProvider</class>\n        <!-- Any extra configuration for this provider goes here -->\n{{- range $i, $host := .NodeList }}\n        <property name=\"Node Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n\t\t<property name=\"Node Group\"></property>\n    </accessPolicyProvider>\n    <authorizer>\n        <identifier>managed-authorizer</identifier>\n        <class>org.apache.nifi.authorization.StandardManagedAuthorizer</class>\n        <property name=\"Access Policy Provider\">file-access-policy-provider</property>\n    </authorizer>\n    <authorizer>\n        <identifier>custom-database-authorizer</identifier>\n        <class>my.custom.DatabaseAuthorizer</class>\n        <property name=\"Access Policy Provider\">database-access-policy-provider</property>\n    </authorizer>\n</authorizers>\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFi Cluster on OpenShift (kubectl YAML)\nDESCRIPTION: Deploys a NiFi cluster in the 'nifi' namespace with configuration adjusted for OpenShift, notably updating the configuration YAML file with the UID/GID. Assumes Zookeeper connection is configured and accessible.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create -n nifi -f config/samples/openshift.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing Bitnami Zookeeper Helm chart with OpenShift security context in Bash\nDESCRIPTION: Installs Bitnami's Zookeeper Helm chart specifying OpenShift security context parameters RunAsUser and fsGroup to the retrieved UID value stored in 'zookeper_uid'. This ensures the Zookeeper pods run with the correct user permissions complying with OpenShift's security constraints, alongside resource limits, storage, network policy and replica configurations.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nhelm install zookeeper oci://registry-1.docker.io/bitnamicharts/zookeeper \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set global.storageClass=standard \\\n    --set networkPolicy.enabled=true \\\n    --set replicaCount=3 \\\n    --set containerSecurityContext.runAsUser=$zookeper_uid \\\n    --set podSecurityContext.fsGroup=$zookeper_uid\n```\n\n----------------------------------------\n\nTITLE: Sample NiFi authorizers.xml template for custom DatabaseAuthorizer\nDESCRIPTION: This YAML-based template demonstrates how to configure a custom DatabaseAuthorizer within NiFi's authorizers.xml, replacing default providers and incorporating node-specific properties through templated variables, ensuring dynamic deployment across clustered environments.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/6_users_authorization/1_custom_user_authorizer.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n{{- $nodeList := .NodeList }}\n{{- $clusterName := .ClusterName }}\n{{- $namespace := .Namespace }}<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>\n<authorizers>\n    <userGroupProvider>\n        <identifier>file-user-group-provider</identifier>\n        <class>org.apache.nifi.authorization.FileUserGroupProvider</class>\n        <property name=\"Users File\">../data/users.xml</property>\n        <property name=\"Legacy Authorized Users File\"></property>\n        <property name=\"Initial User Identity admin\">{{ .ControllerUser }}</property>\n{{- range $i, $host := .NodeList }}\n        <property name=\"Initial User Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n    </userGroupProvider>\n    <userGroupProvider>\n        <identifier>database-user-group-provider</identifier>\n        <class>my.custom.DatabaseUserGroupProvider</class>\n        <!-- Any extra configuration for this provider goes here -->\n{{- range $i, $host := .NodeList }}\n        <property name=\"Initial User Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n    </userGroupProvider>\n    <accessPolicyProvider>\n        <identifier>file-access-policy-provider</identifier>\n        <class>org.apache.nifi.authorization.FileAccessPolicyProvider</class>\n        <property name=\"User Group Provider\">file-user-group-provider</property>\n        <property name=\"Authorizations File\">../data/authorizations.xml</property>\n        <property name=\"Initial Admin Identity\">{{ .ControllerUser }}</property>\n        <property name=\"Legacy Authorized Users File\"></property>\n{{- range $i, $host := .NodeList }}\n        <property name=\"Node Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n\t\t<property name=\"Node Group\"></property>\n    </accessPolicyProvider>\n    <accessPolicyProvider>\n        <identifier>database-access-policy-provider</identifier>\n        <class>my.custom.DatabaseAccessPolicyProvider</class>\n        <!-- Any extra configuration for this provider goes here -->\n{{- range $i, $host := .NodeList }}\n        <property name=\"Node Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n\t\t<property name=\"Node Group\"></property>\n    </accessPolicyProvider>\n    <authorizer>\n        <identifier>managed-authorizer</identifier>\n        <class>org.apache.nifi.authorization.StandardManagedAuthorizer</class>\n        <property name=\"Access Policy Provider\">file-access-policy-provider</property>\n    </authorizer>\n    <authorizer>\n        <identifier>custom-database-authorizer</identifier>\n        <class>my.custom.DatabaseAuthorizer</class>\n        <property name=\"Access Policy Provider\">database-access-policy-provider</property>\n    </authorizer>\n</authorizers>\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Nodes with YAML in NiFiKop\nDESCRIPTION: This YAML configuration demonstrates how to define a node configuration for Apache NiFi instances in Kubernetes using NiFiKop. It shows settings for provenance storage, user permissions, cluster mode, metadata, storage volumes, and external volume mounting.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/5_references/1_nifi_cluster/3_node_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n   default_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      # Set this to true if the instance is a node in a cluster.\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#basic-cluster-setup\n      isNode: true\n      # Additionnal metadata to merge to the pod associated\n      podMetadata:\n        annotations:\n          node-annotation: \"node-annotation-value\"\n        labels:\n          node-label: \"node-label-value\"\n      # Docker image used by the operator to create the node associated\n      # https://hub.docker.com/r/apache/nifi/\n#      image: \"apache/nifi:1.11.2\"\n      # nodeAffinity can be specified, operator populates this value if new pvc added later to node\n      # https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity\n#      nodeAffinity:\n      # imagePullPolicy define the pull policy for NiFi cluster docker image\n      imagePullPolicy: IfNotPresent\n      # priorityClassName define the name of the priority class to be applied to these nodes\n      priorityClassName: \"example-priority-class-name\"\n      # externalVolumeConfigs specifies a list of volume to mount into the main container.\n      externalVolumeConfigs:\n        - name: example-volume\n          mountPath: \"/opt/nifi/example\"\n          secret:\n            secretName: \"raw-controller\"\n      # storageConfigs specifies the node related configs\n      storageConfigs:\n        # Name of the storage config, used to name PV to reuse into sidecars for example.\n        - name: provenance-repository\n          # Retain this PVC throughout NifiCluster deletions.\n          reclaimPolicy: Retain\n          # Path where the volume will be mount into the main nifi container inside the pod.\n          mountPath: \"/opt/nifi/provenance_repository\"\n          # Metadata to attach to the PVC that gets created\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          # Kubernetes PVC spec\n          # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          reclaimPolicy: Delete\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n```\n\n----------------------------------------\n\nTITLE: Pushing NiFiKop Docker Image to Repository - Bash\nDESCRIPTION: This command pushes the locally built NiFiKop Docker image to the configured docker repository. This step requires authentication and network access to the target Docker registry.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/6_contributing/1_developer_guide.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nmake docker-push\n```\n\n----------------------------------------\n\nTITLE: Configuring ReadOnlyConfig in YAML for NiFiKop\nDESCRIPTION: Complete example of a ReadOnlyConfig YAML configuration for NiFiKop, showing how to set thread counts, logging, authorizations, and various NiFi properties. This configuration affects all nodes in the cluster but can be overridden at the node level.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/5_references/1_nifi_cluster/2_read_only_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreadOnlyConfig:\n  # MaximumTimerDrivenThreadCount define the maximum number of threads for timer driven processors available to the system.\n  maximumTimerDrivenThreadCount: 30\n  # MaximumEventDrivenThreadCount define the maximum number of threads for event driven processors available to the system.\n  maximumEventDrivenThreadCount: 10\n  # Logback configuration that will be applied to the node\n  logbackConfig:\n    # logback.xml configuration that will replace the one produced based on template\n    replaceConfigMap:\n      # The key of the value,in data content, that we want use.\n      data: logback.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # logback.xml configuration that will replace the one produced based on template and overrideConfigMap\n    replaceSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: logback.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n  # Authorizer configuration that will be applied to the node\n  authorizerConfig:\n    # An authorizers.xml configuration template that will replace the default template seen in authorizers.go\n    replaceTemplateConfigMap:\n      # The key of the value, in data content, that we want use.\n      data: authorizers.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # An authorizers.xml configuration template that will replace the default template seen in authorizers.go and the replaceTemplateConfigMap\n    replaceTemplateSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: authorizers.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n  # NifiProperties configuration that will be applied to the node.\n  nifiProperties:\n    # Additionnal nifi.properties configuration that will override the one produced based on template and\n    # configuration\n    overrideConfigMap:\n      # The key of the value,in data content, that we want use.\n      data: nifi.properties\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop.\n    # Additionnal nifi.properties configuration that will override the one produced based\n    #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    overrideSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: nifi.properties\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # Additionnal nifi.properties configuration that will override the one produced based\n    #\ton template, configurations and overrideConfigMap\n    overrideConfigs: |\n      nifi.ui.banner.text=NiFiKop\n    # A comma separated list of allowed HTTP Host header values to consider when NiFi\n    # is running securely and will be receiving requests to a different host[:port] than it is bound to.\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#web-properties\n    #      webProxyHosts:\n    # Nifi security client auth\n    needClientAuth: false\n    # Indicates which of the configured authorizers in the authorizers.xml file to use\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#authorizer-configuration\n  #      authorizer:\n  # ZookeeperProperties configuration that will be applied to the node.\n  zookeeperProperties:\n    #      # Additionnal zookeeeper.properties configuration that will override the one produced based on template and\n    #      # configuration\n    #      overrideConfigMap:\n    #        # The key of the value,in data content, that we want use.\n    #        data: zookeeeper.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop.\n    #      # Additionnal zookeeeper.properties configuration that will override the one produced based\n    #      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    #      overrideSecretConfig:\n    #        # The key of the value,in data content, that we want use.\n    #        data: zookeeeper.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop\n    # Additionnal zookeeper.properties configuration that will override the one produced based\n    # on template and configurations.\n    overrideConfigs: |\n      initLimit=15\n      autopurge.purgeInterval=24\n      syncLimit=5\n      tickTime=2000\n      dataDir=./state/zookeeper\n      autopurge.snapRetainCount=30\n  # BootstrapProperties configuration that will be applied to the node.\n  bootstrapProperties:\n    #      # Additionnal bootstrap.properties configuration that will override the one produced based on template and\n    #      # configuration\n    #      overrideConfigMap:\n    #        # The key of the value,in data content, that we want use.\n    #        data: bootstrap.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop.\n    #      # Additionnal bootstrap.properties configuration that will override the one produced based\n    #      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    #      overrideSecretConfig:\n    #        # The key of the value,in data content, that we want use.\n    #        data: bootstrap.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop\n    # JVM memory settings\n    nifiJvmMemory: \"512m\"\n    # Additionnal bootstrap.properties configuration that will override the one produced based\n    # on template and configurations.\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#bootstrap_properties\n    overrideConfigs: |\n      # java.arg.4=-Djava.net.preferIPv4Stack=true\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFiKop Operator via Helm (Bash/helm)\nDESCRIPTION: Installs the NiFiKop operator using its Helm chart (version 1.2.0) from the OCI registry into the 'nifi' namespace. It specifies the operator image tag, resource requests/limits, and the namespaces the operator should watch. Assumes the namespace has already been created.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/2_deploy_nifikop/1_quick_start.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n# You have to create the namespace before executing following command\nhelm install nifikop \\\n    oci://ghcr.io/konpyutaika/helm-charts/nifikop \\\n    --namespace=nifi \\\n    --version 1.2.0 \\\n    --set image.tag=v1.2.0-release \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set namespaces={\"nifi\"}\n```\n\n----------------------------------------\n\nTITLE: NiFi NodeGroup Autoscaler Configuration Example\nDESCRIPTION: This YAML snippet demonstrates the configuration of a `NifiNodeGroupAutoscaler` resource. It includes the reference to the `NifiCluster`, the ID of the `NodeConfig`, node labels selector, upscale strategy and downscale strategy. The `clusterRef` refers to the NiFi cluster that this autoscaler is managing. The `nodeConfigGroupId` refers to a pre-defined NodeConfig in NifiCluster spec. The `nodeLabelsSelector` is used to identify nodes managed by this autoscaler. The `upscaleStrategy` and `downscaleStrategy` specify how nodes are added to and removed from the NiFi cluster respectively.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/5_references/7_nifi_nodegroup_autoscaler.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiNodeGroupAutoscaler\nmetadata:\n  name: nifinodegroupautoscaler-sample\nspec:\n  # contains the reference to the NifiCluster with the one the node group autoscaler is linked.\n  clusterRef:\n    name: nificluster-name\n    namespace: nifikop\n  # defines the id of the NodeConfig contained in NifiCluster.Spec.NodeConfigGroups\n  nodeConfigGroupId: default-node-group\n  # The selector used to identify nodes in NifiCluster.Spec.Nodes this autoscaler will manage\n  # Use Node.Labels in combination with this selector to clearly define which nodes will be managed by this autoscaler \n  nodeLabelsSelector: \n    matchLabels:\n      nifi_cr: nificluster-name\n      nifi_node_group: default-node-group\n  # the strategy used to decide how to add nodes to a nifi cluster\n  upscaleStrategy: simple\n  # the strategy used to decide how to remove nodes from an existing cluster\n  downscaleStrategy: lifo\n```\n\n----------------------------------------\n\nTITLE: Removing a node from the NiFiCluster via YAML configuration\nDESCRIPTION: This YAML snippet updates the NiFiCluster spec by removing a node with 'id: 2' from the 'nodes' list, initiating a graceful decommission process for that node. The change reflects the intention to scale down the cluster by removing the specified node.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  headlessServiceEnabled: true\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.11.3\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n# >>>> START: node removed\n#    - id: 2\n#      nodeConfigGroup: \"default_group\"\n# <<<< END\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: Applying CRDs manually for NiFiKop deployment\nDESCRIPTION: This snippet includes multiple kubectl commands to manually apply each NiFiKop custom resource definition (CRD) required for deployment when skipping helm CRD installation. It is necessary to deploy these before deploying NiFiKop if not using the Helm chart's CRD setup. Requires kubectl access.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/2_deploy_nifikop/1_quick_start.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Apply each NiFiKop CRD\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifinodegroupautoscalers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificonnections.yaml\n```\n\n----------------------------------------\n\nTITLE: Updated NifiCluster Configuration with bash InitContainerImage\nDESCRIPTION: Example YAML configuration showing how to update the NifiCluster to use bash as the init container image after upgrading to v0.15.0.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/7_upgrade_guides/2_v0.14.1_to_v0.15.0.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  initContainerImage:\n    repository: bash\n    tag: \"5.2.2\"\n```\n\n----------------------------------------\n\nTITLE: NifiCluster configuration with bash init container (post-upgrade)\nDESCRIPTION: Updated YAML configuration for a NifiCluster that uses the bash image as the init container, which is required after upgrading to v0.15.0.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/7_upgrade_guides/2_v0.14.1_to_v0.15.0.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  initContainerImage:\n    repository: bash\n    tag: \"5.2.2\"\n```\n\n----------------------------------------\n\nTITLE: Configuring SSL for NiFi Cluster in YAML\nDESCRIPTION: Example YAML configuration for securing a NiFi cluster with SSL. This configuration enables HTTPS listeners and instructs the operator to create the necessary SSL certificates.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  readOnlyConfig:\n    # NifiProperties configuration that will be applied to the node.\n    nifiProperties:\n      webProxyHosts:\n        - nifistandard2.trycatchlearn.fr:8443\n        ...\n  ...\n  listenersConfig:\n    internalListeners:\n      - type: \"https\"\n        name: \"https\"\n        containerPort: 8443\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n```\n\n----------------------------------------\n\nTITLE: Deploying KEDA ScaledObject for NiFi Autoscaling\nDESCRIPTION: Defines a KEDA ScaledObject to automatically scale NiFi nodes based on a Prometheus metric. Contains parameters for scale target, polling interval, and fallback. Requires KEDA installed and a relevant HorizontalPodAutoscaler managed alongside.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_12\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: cluster\n  namespace: clusters\nspec:\n  scaleTargetRef:\n    apiVersion:    nifi.konpyutaika.com/v1alpha1     # Optional. Default: apps/v1\n    kind:          NifiNodeGroupAutoscaler           # Optional. Default: Deployment\n    name:          nifinodegroupautoscaler-sample    # Mandatory. Must be in the same namespace as the ScaledObject\n    envSourceContainerName: nifi                     # Optional. Default: .spec.template.spec.containers[0]\n  pollingInterval:  30                               # Optional. Default: 30 seconds\n  cooldownPeriod:   300                              # Optional. Default: 300 seconds\n  idleReplicaCount: 0                                # Optional. Default: ignored, must be less than minReplicaCount \n  minReplicaCount:  1                                # Optional. Default: 0\n  maxReplicaCount:  3                                # Optional. Default: 100\n  fallback:                                          # Optional. Section to specify fallback options\n    failureThreshold: 5                              # Mandatory if fallback section is included\n    replicas: 1                                      # Mandatory if fallback section is included\n  triggers:\n    - type: prometheus\n      metadata:\n        serverAddress: http://prometheus-operated.monitoring-system.svc:9090\n        metricName: http_requests_total\n        threshold: <threshold value>\n        query: <prometheus query>\n```\n\n----------------------------------------\n\nTITLE: Installing Zookeeper on OpenShift with Helm\nDESCRIPTION: This bash script uses Helm to install the Zookeeper chart, setting parameters for resource requests/limits and security context configurations in an OpenShift environment. It uses the pre-acquired UID and sets runAsUser and fsGroup to the retrieved value. This requires Helm installed and configured, a Kubernetes cluster, and access to the cluster. It needs the Zookeeper chart repository and namespace to be correctly configured.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhelm install zookeeper oci://registry-1.docker.io/bitnamicharts/zookeeper \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set global.storageClass=standard \\\n    --set networkPolicy.enabled=true \\\n    --set replicaCount=3 \\\n    --set containerSecurityContext.runAsUser=$zookeper_uid \\\n    --set podSecurityContext.fsGroup=$zookeper_uid\n```\n\n----------------------------------------\n\nTITLE: Defining Istio VirtualService for HTTPS\nDESCRIPTION: This YAML defines a VirtualService that routes HTTP traffic (after decryption by the gateway) to a specific internal service.  It links to the 'nifi-gateway' and hosts 'nifi.my-domain.com'. The `http` section defines a route where traffic with any URI prefix ('/') is routed to the service with the address  '<service-name>.<namespace>.svc.cluster.local' on port 8443. This setup directs HTTP traffic from the gateway to the internal service, which is then encrypted again by the destination rule.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: nifi-vs\nspec:\n  gateways:\n  - nifi-gateway\n  hosts:\n  - nifi.my-domain.com\n  http:\n  - match:\n    - uri:\n        prefix: /\n    route:\n    - destination:\n        host: <service-name>.<namespace>.svc.cluster.local\n        port:\n          number: 8443\n```\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Authorizer XML Template in NiFiKOp with YAML\nDESCRIPTION: A templated YAML configuration for authorizers.xml that defines both the default file-based authorizer and a custom database authorizer. The template includes dynamic node identity configuration and supports both user group providers and access policy providers.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/6_users_authorization/1_custom_user_authorizer.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n{{- $nodeList := .NodeList }}\n{{- $clusterName := .ClusterName }}\n{{- $namespace := .Namespace }}<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>\n<authorizers>\n    <userGroupProvider>\n        <identifier>file-user-group-provider</identifier>\n        <class>org.apache.nifi.authorization.FileUserGroupProvider</class>\n        <property name=\"Users File\">../data/users.xml</property>\n        <property name=\"Legacy Authorized Users File\"></property>\n        <property name=\"Initial User Identity admin\">{{ .ControllerUser }}</property>\n{{- range $i, $host := .NodeList }}\n        <property name=\"Initial User Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n    </userGroupProvider>\n    <userGroupProvider>\n        <identifier>database-user-group-provider</identifier>\n        <class>my.custom.DatabaseUserGroupProvider</class>\n        <!-- Any extra configuration for this provider goes here -->\n{{- range $i, $host := .NodeList }}\n        <property name=\"Initial User Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n    </userGroupProvider>\n    <accessPolicyProvider>\n        <identifier>file-access-policy-provider</identifier>\n        <class>org.apache.nifi.authorization.FileAccessPolicyProvider</class>\n        <property name=\"User Group Provider\">file-user-group-provider</property>\n        <property name=\"Authorizations File\">../data/authorizations.xml</property>\n        <property name=\"Initial Admin Identity\">{{ .ControllerUser }}</property>\n        <property name=\"Legacy Authorized Users File\"></property>\n{{- range $i, $host := .NodeList }}\n        <property name=\"Node Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n\t\t<property name=\"Node Group\"></property>\n    </accessPolicyProvider>\n    <accessPolicyProvider>\n        <identifier>database-access-policy-provider</identifier>\n        <class>my.custom.DatabaseAccessPolicyProvider</class>\n        <!-- Any extra configuration for this provider goes here -->\n{{- range $i, $host := .NodeList }}\n        <property name=\"Node Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n\t\t<property name=\"Node Group\"></property>\n    </accessPolicyProvider>\n    <authorizer>\n        <identifier>managed-authorizer</identifier>\n        <class>org.apache.nifi.authorization.StandardManagedAuthorizer</class>\n        <property name=\"Access Policy Provider\">file-access-policy-provider</property>\n    </authorizer>\n    <authorizer>\n        <identifier>custom-database-authorizer</identifier>\n        <class>my.custom.DatabaseAuthorizer</class>\n        <property name=\"Access Policy Provider\">database-access-policy-provider</property>\n    </authorizer>\n</authorizers>\n```\n\n----------------------------------------\n\nTITLE: Defining Istio DestinationRule for TLS Origination and Sticky Sessions in YAML\nDESCRIPTION: This Istio DestinationRule targets traffic destined for the NiFi ClusterIP service '<service-name>.<namespace>.svc.cluster.local'. It re-encrypts the traffic using TLS ('mode: SIMPLE') before it reaches the NiFi pods. It also configures load balancing with consistent hashing based on the '__Secure-Authorization-Bearer' HTTP cookie for sticky sessions. Replace placeholders.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: DestinationRule\nmetadata:\n  name: nifi-dr\nspec:\n  host: <service-name>.<namespace>.svc.cluster.local\n  trafficPolicy:\n    tls:\n      mode: SIMPLE\n    loadBalancer:\n      consistentHash:\n        httpCookie:\n          name: __Secure-Authorization-Bearer\n          ttl: 0s\n```\n\n----------------------------------------\n\nTITLE: Running the NiFiKop Operator Locally using Make in Bash\nDESCRIPTION: Executes the operator in development mode by invoking 'make run', which runs against the configured Kubernetes cluster using the default kubeconfig. Requires GNU Make, Golang environment, and that the CRDs are already deployed. No parameters; output is a running operator process in the local environment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/6_contributing/1_developer_guide.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmake run\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for IDE\nDESCRIPTION: Sets environment variables required to run NiFiKop in a development IDE. These variables configure the connection to the Kubernetes cluster, the namespace to watch, the name for the operator pod and the log level.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/6_contributing/1_developer_guide.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nKUBECONFIG={path/to/your/kubeconfig}\nWATCH_NAMESPACE={namespace_to_watch}\nPOD_NAME={name for operator pod}\nLOG_LEVEL=Debug\nOPERATOR_NAME=ide\n```\n\n----------------------------------------\n\nTITLE: Packaging NiFiKop Helm Chart with Make Bash\nDESCRIPTION: This snippet calls the Makefile target to package the NiFiKop Helm chart for distribution. Relies on GNU Make and a correctly structured Helm chart directory. No arguments are needed; produces a Helm-compatible package file in the output directory.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/6_contributing/1_developer_guide.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nmake helm-package\n```\n\n----------------------------------------\n\nTITLE: Encrypting NiFi Configuration with encrypt-config.sh - Shell\nDESCRIPTION: This shell script uses the `encrypt-config.sh` tool to encrypt the sensitive properties in `nifi.properties`, `flow.xml.gz` and `flow.json.gz` files. It updates the algorithm and encrypts sensitive properties to use the new `NIFI_PBKDF2_AES_GCM_256` algorithm.  It takes the key from the environment variable `PROPERTIES_KEY`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/7_upgrade_guides/4_v1.3.1_to_v1.4.0.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nencrypt-config.sh -n nifi.properties -f flow.xml.gz -x -s PROPERTIES_KEY -A NIFI_PBKDF2_AES_GCM_256\nencrypt-config.sh -n nifi.properties -f flow.json.gz -x -s PROPERTIES_KEY -A NIFI_PBKDF2_AES_GCM_256\n```\n\n----------------------------------------\n\nTITLE: Specifying NifiCluster Init Container Image Using busybox in YAML\nDESCRIPTION: This YAML snippet defines a NifiCluster custom resource specifying the initContainerImage using the 'busybox' repository at tag '1.34.0'. It is used as an example of the old configuration prior to version 0.15.0, which requires updating because the default init container image changed. The snippet requires the Kubernetes custom resource definitions and is intended to configure the initialization environment of Nifi cluster pods.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/7_upgrade_guides/2_v0.14.1_to_v0.15.0.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  initContainerImage:\n    repository: busybox\n    tag: \"1.34.0\"\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiParameterContext Resource in YAML\nDESCRIPTION: Example YAML manifest for creating a `NifiParameterContext` custom resource in Kubernetes using NiFiKop. This defines a set of parameters, potentially including sensitive ones sourced from Kubernetes Secrets (`secretRefs`), to be used by NiFi dataflows. It references the target NiFi cluster (`clusterRef`).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/3_tasks/3_nifi_dataflow.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\n  namespace: demo\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      value: toto\n      description: toto\n```\n\n----------------------------------------\n\nTITLE: Configuring npm Start Script for Migration - JSON\nDESCRIPTION: This snippet adds a start script to 'package.json' to launch the migration script with Node.js and suppress warnings. Ensure 'index.js' exists at the project root before starting. The configuration allows users to run the migration with 'npm start'.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"start\": \"node --no-warnings index.js\"\n```\n\n----------------------------------------\n\nTITLE: Installing Nifikop Helm Chart Skipping CRDs\nDESCRIPTION: Installs the Nifikop Helm chart using `helm install` while explicitly preventing the automatic installation of its Custom Resource Definitions (CRDs) via the `--skip-crds` flag. This requires the CRDs to have been applied manually beforehand. The example also sets the target `namespaces`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nhelm install --name nifikop ./helm/nifikop --set namespaces={\"nifikop\"} --skip-crds\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Cluster with NodeConfigGroup for Autoscaling\nDESCRIPTION: YAML configuration for a NiFi cluster with Prometheus metrics enabled and a specific NodeConfigGroup that will be used for autoscaling. It defines resource requirements and storage configurations for the autoscaling node group.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n...\nspec:\n  ...\n  listenersConfig:\n    internalListeners:\n    ...\n    - containerPort: 9090\n      name: prometheus\n      type: prometheus \n    ...\n  ...\n  nodeConfigGroups:\n    auto_scaling:\n      isNode: true\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 2Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n      serviceAccountName: external-dns\n      storageConfigs:\n        - mountPath: /opt/nifi/nifi-current/logs\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/data\n          name: data\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/extensions\n          name: extensions-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/flowfile_repository\n          name: flowfile-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/nifi-current/conf\n          name: conf\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/content_repository\n          name: content-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/provenance_repository\n          name: provenance-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n  ...\n```\n\n----------------------------------------\n\nTITLE: Defining Prometheus Custom Resource\nDESCRIPTION: YAML manifest for creating a Prometheus custom resource (apiVersion `monitoring.coreos.com/v1`) named 'prometheus' in the 'monitoring-system' namespace. It configures the Prometheus instance with specific scrape/evaluation intervals, log level, resource requests, and defines selectors (`podMonitorSelector`, `serviceMonitorSelector`) to automatically discover `PodMonitor` and `ServiceMonitor` resources within the cluster for scraping targets.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: Prometheus\nmetadata:\n  name: prometheus\n  namespace: monitoring-system\nspec:\n  enableAdminAPI: false\n  evaluationInterval: 30s\n  logLevel: debug\n  podMonitorSelector:\n    matchExpressions:\n    - key: release\n      operator: In\n      values:\n      - prometheus\n  resources:\n    requests:\n      memory: 400Mi\n  scrapeInterval: 30s\n  serviceAccountName: prometheus\n  serviceMonitorSelector:\n    matchExpressions:\n    - key: app\n      operator: In\n      values:\n      - nifi-cluster\n```\n\n----------------------------------------\n\nTITLE: Data model schema for NifiRegistryClient resource in Go\nDESCRIPTION: The `NifiRegistryClient` schema defines the data structure for representing a NiFi registry client, including metadata, specification, and status. It forms the core model used by controllers/operators to manage the resource's desired and observed state within a Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/5_references/3_nifi_registry_client.md#_snippet_1\n\n\n\n----------------------------------------\n\nTITLE: Configuring Internal Listeners for NiFi Cluster in YAML\nDESCRIPTION: Defines six types of internal listeners for a NiFi Kubernetes cluster using the Spec.ListernersConfig.InternalListeners field. Each listener is specified with a 'type', 'name', and 'containerPort', designating the protocol port inside the pod for cluster communication, HTTP/HTTPS UI access, Site-to-Site communication, load balancing, and Prometheus monitoring. The snippet helps configure ports that NiFi will use internally in the container environment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    - type: \"https\"\n      name: \"https\"\n      containerPort: 8443\n    - type: \"cluster\"\n      name: \"cluster\"\n      containerPort: 6007\n    - type: \"s2s\"\n      name: \"s2s\"\n      containerPort: 10000\n    - type: \"prometheus\"\n      name: \"prometheus\"\n      containerPort: 9090\n    - type: \"load-balance\"\n      name: \"load-balance\"\n      containerPort: 6342\n```\n\n----------------------------------------\n\nTITLE: Manually Deleting CustomResourceDefinitions (CRDs)\nDESCRIPTION: Commands to delete CRD instances manually, which is necessary if automatic cleanup is not configured. Removing CRDs will delete all clusters derived from these definitions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete crd nificlusters.nifi.konpyutaika.com\n```\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete crd nifiusers.nifi.konpyutaika.com\n```\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete crd nifiusergroups.nifi.konpyutaika.com\n```\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete crd nifiregistryclients.nifi.konpyutaika.com\n```\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete crd nifiparametercontexts.nifi.konpyutaika.com\n```\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete crd nifidataflows.nifi.konpyutaika.com\n```\n\n----------------------------------------\n\nTITLE: Set Up Prometheus Operator for Kubernetes Monitoring\nDESCRIPTION: This sequence of commands deploys the Prometheus Operator in a dedicated 'monitoring-system' namespace via Helm. It configures the operator with specific settings like disabling creation of custom resources and enabling debugging logs. Prometheus is also set up as a custom resource with specific evaluation intervals and resource requests, and a ServiceMonitor is created to scrape metrics from NiFi clusters. Dependencies include Helm and kubectl; inputs include repository URLs and deployment parameters; outputs enable Prometheus metrics collection for autoscaling triggers.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_3\n\nLANGUAGE: Console\nCODE:\n```\nkubectl create namespace monitoring-system\nhelm repo add prometheus https://prometheus-community.github.io/helm-charts\nhelm repo update\nhelm install prometheus prometheus/kube-prometheus-stack --namespace monitoring-system \\ \n    --set prometheusOperator.createCustomResource=false \\ \n    --set prometheusOperator.logLevel=debug \\ \n    --set prometheusOperator.alertmanagerInstanceNamespaces=monitoring-system \\ \n    --set prometheusOperator.namespaces.additional[0]=monitoring-system \\ \n    --set prometheusOperator.prometheusInstanceNamespaces=monitoring-system \\ \n    --set prometheusOperator.thanosRulerInstanceNamespaces=monitoring-system \\ \n    --set defaultRules.enable=false \\ \n    --set alertmanager.enabled=false \\ \n    --set grafana.enabled=false \\ \n    --set kubeApiServer.enabled=false \\ \n    --set kubelet.enabled=false \\ \n    --set kubeControllerManager.enabled=false \\ \n    --set coreDNS.enabled=false \\ \n    --set kubeEtcd.enabled=false \\ \n    --set kubeScheduler.enabled=false \\ \n    --set kubeProxy.enabled=false \\ \n    --set kubeStateMetrics.enabled=false \\ \n    --set prometheus.enabled=false\n```\n\n----------------------------------------\n\nTITLE: Listing All Installed Helm Charts (Bash)\nDESCRIPTION: Executes the basic Helm list command to display all installed Helm releases in the current namespace (or across all if using the `-A` flag). No arguments are required and the output is a tabulated list of releases. Assumes a configured and accessible Kubernetes context.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/2_setup/3_install/1_customizable_install_with_helm.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nhelm list\n```\n\n----------------------------------------\n\nTITLE: Cloning the NiFiKop Repository\nDESCRIPTION: Command to clone the NiFiKop repository from GitHub and navigate into the project directory, preparing for development or deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/6_contributing/1_developer_guide.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/konpyutaika/nifikop.git\ncd nifikop\n```\n\n----------------------------------------\n\nTITLE: Creating an Issuer for Let's Encrypt ACME Certificates\nDESCRIPTION: Provides a Kubernetes manifest to create an ACME Issuer resource pointing to Let's Encrypt staging environment, including contact email, private key secret, and HTTP challenge solver configuration. Essential for automating SSL certificate issuance with external DNS.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: cert-manager.io/v1alpha2\nkind: Issuer\nmetadata:\n  name: letsencrypt-staging\nspec:\n  acme:\n    email: <your email address>\n    server: https://acme-staging-v02.api.letsencrypt.org/directory\n    privateKeySecretRef:\n      name: example-issuer-account-key\n    solvers:\n      - http01:\n          ingress:\n            ingressTemplate:\n              metadata:\n                annotations:\n                  \"external-dns.alpha.kubernetes.io/ttl\": \"5\"\n```\n\n----------------------------------------\n\nTITLE: Accessing Prometheus Dashboard\nDESCRIPTION: Command to set up port forwarding to access the Prometheus web interface, allowing visualization and querying of NiFi metrics for testing the monitoring setup.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_8\n\nLANGUAGE: console\nCODE:\n```\nkubectl port-forward -n monitoring-system service/prometheus-operated 9090:9090\n```\n\n----------------------------------------\n\nTITLE: Install Nifikop Chart and Skip CRDs\nDESCRIPTION: This command installs the Nifikop chart while skipping the installation of the CRDs using the `--skip-crds` flag. This is useful if you want to manage CRDs separately. The command sets the namespaces via `--set`. Dependencies: Helm, Kubernetes cluster, and the Nifikop chart repository.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\n$ helm install --name nifikop ./helm/nifikop --set namespaces={\\\"nifikop\\\"} --skip-crds\n```\n\n----------------------------------------\n\nTITLE: Testing Installed Kubectl Plugin for NiFiKop via Console\nDESCRIPTION: This snippet demonstrates running the 'kubectl nifikop' command to verify the plugin is installed and accessible. It outputs usage information and lists available commands provided by the NiFiKop kubectl plugin. It assumes the plugin executable is properly installed in the PATH and executable. The input is the command typed in console, and the expected output is the usage message along with the command list.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/2_deploy_nifikop/3_kubectl_plugin.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\n$ kubectl nifikop\nUsage:\n  nifikop [command]\n\nAvailable Commands:\n  completion          Generate the autocompletion script for the specified shell\n  help                Help about any command\n  nificluster         \n  nificonnection      \n  nifidataflow        \n  nifigroupautoscaler \n  nifiregistryclient  \n  nifiuser            \n  nifiusergroup \n```\n\n----------------------------------------\n\nTITLE: Define NiFi Cluster Node Configuration (Scale-down) YAML\nDESCRIPTION: This YAML snippet defines the configuration for a NiFi cluster, specifically to remove a node. It's similar to the scale-up configuration, but the `nodes` section is modified to remove a node. The `- id: 2` node is commented out to remove it from the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  headlessServiceEnabled: true\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.11.3\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n# >>>> START: node removed\n#    - id: 2\n#      nodeConfigGroup: \"default_group\"\n# <<<< END\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom DatabaseAuthorizer in authorizers.xml Template for NiFiKOp\nDESCRIPTION: A template for authorizers.xml that sets up both the default file-based authorizer and a custom database authorizer. The template includes placeholders that will be filled by NiFiKOp at deployment time to handle node identities dynamically.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/6_users_authorization/1_custom_user_authorizer.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n{{- $nodeList := .NodeList }}\n{{- $clusterName := .ClusterName }}\n{{- $namespace := .Namespace }}<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>\n<authorizers>\n    <userGroupProvider>\n        <identifier>file-user-group-provider</identifier>\n        <class>org.apache.nifi.authorization.FileUserGroupProvider</class>\n        <property name=\"Users File\">../data/users.xml</property>\n        <property name=\"Legacy Authorized Users File\"></property>\n        <property name=\"Initial User Identity admin\">{{ .ControllerUser }}</property>\n{{- range $i, $host := .NodeList }}\n        <property name=\"Initial User Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n    </userGroupProvider>\n    <userGroupProvider>\n        <identifier>database-user-group-provider</identifier>\n        <class>my.custom.DatabaseUserGroupProvider</class>\n        <!-- Any extra configuration for this provider goes here -->\n{{- range $i, $host := .NodeList }}\n        <property name=\"Initial User Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n    </userGroupProvider>\n    <accessPolicyProvider>\n        <identifier>file-access-policy-provider</identifier>\n        <class>org.apache.nifi.authorization.FileAccessPolicyProvider</class>\n        <property name=\"User Group Provider\">file-user-group-provider</property>\n        <property name=\"Authorizations File\">../data/authorizations.xml</property>\n        <property name=\"Initial Admin Identity\">{{ .ControllerUser }}</property>\n        <property name=\"Legacy Authorized Users File\"></property>\n{{- range $i, $host := .NodeList }}\n        <property name=\"Node Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n\t\t<property name=\"Node Group\"></property>\n    </accessPolicyProvider>\n    <accessPolicyProvider>\n        <identifier>database-access-policy-provider</identifier>\n        <class>my.custom.DatabaseAccessPolicyProvider</class>\n        <!-- Any extra configuration for this provider goes here -->\n{{- range $i, $host := .NodeList }}\n        <property name=\"Node Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n\t\t<property name=\"Node Group\"></property>\n    </accessPolicyProvider>\n    <authorizer>\n        <identifier>managed-authorizer</identifier>\n        <class>org.apache.nifi.authorization.StandardManagedAuthorizer</class>\n        <property name=\"Access Policy Provider\">file-access-policy-provider</property>\n    </authorizer>\n    <authorizer>\n        <identifier>custom-database-authorizer</identifier>\n        <class>my.custom.DatabaseAuthorizer</class>\n        <property name=\"Access Policy Provider\">database-access-policy-provider</property>\n    </authorizer>\n</authorizers>\n```\n\n----------------------------------------\n\nTITLE: Installing Prometheus Operator with Helm in Kubernetes Using Console\nDESCRIPTION: Deploys the Prometheus Operator via Helm into the 'monitoring-system' namespace with extensive customization to disable all default components except core operator features. The command disables the creation of CustomResourceDefinitions, Alertmanager, Grafana, core Kubernetes metrics exporters, and Prometheus instances itself, preparing an operator-only deployment. This tailored installation is a prerequisite for manual Prometheus resource definitions defined later. Helm 3 and Kubernetes cluster access are required.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_7\n\nLANGUAGE: console\nCODE:\n```\nhelm install prometheus prometheus/kube-prometheus-stack --namespace monitoring-system \\\n    --set prometheusOperator.createCustomResource=false \\\n    --set prometheusOperator.logLevel=debug \\\n    --set prometheusOperator.alertmanagerInstanceNamespaces=monitoring-system \\\n    --set prometheusOperator.namespaces.additional[0]=monitoring-system \\\n    --set prometheusOperator.prometheusInstanceNamespaces=monitoring-system \\\n    --set prometheusOperator.thanosRulerInstanceNamespaces=monitoring-system \\\n    --set defaultRules.enable=false \\\n    --set alertmanager.enabled=false \\\n    --set grafana.enabled=false \\\n    --set kubeApiServer.enabled=false \\\n    --set kubelet.enabled=false \\\n    --set kubeControllerManager.enabled=false \\\n    --set coreDNS.enabled=false \\\n    --set kubeEtcd.enabled=false \\\n    --set kubeScheduler.enabled=false \\\n    --set kubeProxy.enabled=false \\\n    --set kubeStateMetrics.enabled=false \\\n    --set prometheus.enabled=false\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Database Authorizer XML Template in NiFiKOp\nDESCRIPTION: This YAML snippet contains a templated authorizers.xml configuration for NiFiKOp that implements both the default file-based authorizer and a custom database authorizer. The template dynamically includes node identities and controller user settings.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/6_users_authorization/1_custom_user_authorizer.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n{{- $nodeList := .NodeList }}\n{{- $clusterName := .ClusterName }}\n{{- $namespace := .Namespace }}<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>\n<authorizers>\n    <userGroupProvider>\n        <identifier>file-user-group-provider</identifier>\n        <class>org.apache.nifi.authorization.FileUserGroupProvider</class>\n        <property name=\"Users File\">../data/users.xml</property>\n        <property name=\"Legacy Authorized Users File\"></property>\n        <property name=\"Initial User Identity admin\">{{ .ControllerUser }}</property>\n{{- range $i, $host := .NodeList }}\n        <property name=\"Initial User Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n    </userGroupProvider>\n    <userGroupProvider>\n        <identifier>database-user-group-provider</identifier>\n        <class>my.custom.DatabaseUserGroupProvider</class>\n        <!-- Any extra configuration for this provider goes here -->\n{{- range $i, $host := .NodeList }}\n        <property name=\"Initial User Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n    </userGroupProvider>\n    <accessPolicyProvider>\n        <identifier>file-access-policy-provider</identifier>\n        <class>org.apache.nifi.authorization.FileAccessPolicyProvider</class>\n        <property name=\"User Group Provider\">file-user-group-provider</property>\n        <property name=\"Authorizations File\">../data/authorizations.xml</property>\n        <property name=\"Initial Admin Identity\">{{ .ControllerUser }}</property>\n        <property name=\"Legacy Authorized Users File\"></property>\n{{- range $i, $host := .NodeList }}\n        <property name=\"Node Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n\t\t<property name=\"Node Group\"></property>\n    </accessPolicyProvider>\n    <accessPolicyProvider>\n        <identifier>database-access-policy-provider</identifier>\n        <class>my.custom.DatabaseAccessPolicyProvider</class>\n        <!-- Any extra configuration for this provider goes here -->\n{{- range $i, $host := .NodeList }}\n        <property name=\"Node Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n\t\t<property name=\"Node Group\"></property>\n    </accessPolicyProvider>\n    <authorizer>\n        <identifier>managed-authorizer</identifier>\n        <class>org.apache.nifi.authorization.StandardManagedAuthorizer</class>\n        <property name=\"Access Policy Provider\">file-access-policy-provider</property>\n    </authorizer>\n    <authorizer>\n        <identifier>custom-database-authorizer</identifier>\n        <class>my.custom.DatabaseAuthorizer</class>\n        <property name=\"Access Policy Provider\">database-access-policy-provider</property>\n    </authorizer>\n</authorizers>\n```\n\n----------------------------------------\n\nTITLE: Applying Nifikop CRDs with kubectl - Bash\nDESCRIPTION: This snippet demonstrates manual installation of all required Nifikop CustomResourceDefinitions (CRDs) using kubectl. Each command applies a remote YAML CRD manifest from the project's GitHub repository. Dependencies include kubectl with cluster admin privileges and network access. No inputs are required except cluster context; output is the registered CRDs within the Kubernetes API. Ensure no pre-existing conflicting CRDs before applying.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifinodegroupautoscalers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificonnections.yaml\n```\n\n----------------------------------------\n\nTITLE: Get Status of Helm Deployment\nDESCRIPTION: This command retrieves the status of a specific Helm release named 'nifikop'. It displays the current state of the release, providing information on deployed resources, and any issues.  Dependencies: Helm.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nhelm status nifikop\n```\n\n----------------------------------------\n\nTITLE: NiFi Operator Cluster Integration with External Issuer - YAML\nDESCRIPTION: This YAML manifest configures a NifiCluster resource to leverage an external DNS and a pre-existing cert-manager Issuer for SSL certificates. It references an external DNS zone, enables the use of external DNS services, and specifies the usage of a named Issuer (such as Let's Encrypt) for certificate issuance. Prerequisites include a functioning cert-manager installation and a matching Issuer in the same namespace. Ensure that DNS zone names and issuer references are valid and accessible. Limitation: All referenced resources must exist prior to applying this manifest, and DNS propagation must be properly set up.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  clusterSecure: true\n  siteToSiteSecure: true\n  ...\n  listenersConfig:\n    clusterDomain: <DNS zone name>\n    useExternalDNS: true\n    ...\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n      issuerRef:\n        name: letsencrypt-staging\n        kind: Issuer\n```\n\n----------------------------------------\n\nTITLE: Building and Installing NiFiKop Kubectl Plugin on UNIX - Console\nDESCRIPTION: This command builds the NiFiKop kubectl plugin using 'make kubectl-nifikop' and copies the resulting executable to '/usr/local/bin' to make it accessible from anywhere in the terminal. Prerequisites include having 'make' and the necessary build environment set up for NiFiKop. It expects write permissions for '/usr/local/bin' and assumes the source code is present in the working directory.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/2_deploy_nifikop/3_kubectl_plugin.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nmake kubectl-nifikop && sudo cp ./bin/kubectl-nifikop /usr/local/bin\n```\n\n----------------------------------------\n\nTITLE: Removing a Node from NifiCluster Spec - YAML\nDESCRIPTION: This YAML snippet illustrates how to remove a node from the `NifiCluster.Spec.Nodes` configuration to initiate a scaledown. The node with id '2' is commented out, effectively removing it from the cluster definition. Note that `headlessServiceEnabled` is used instead of `headlessEnabled`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  headlessServiceEnabled: true\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.11.3\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n# >>>> START: node removed\n#    - id: 2\n#      nodeConfigGroup: \"default_group\"\n# <<<< END\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: Defining Nifikop NifiUser Resource YAML\nDESCRIPTION: Explains how to create a Kubernetes resource for a NiFi user using the Nifikop operator. Details essential fields like the user's identity on the NiFi cluster, the target cluster reference, certificate creation options, and how to specify fine-grained access policies using `accessPolicies`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/3_tasks/4_nifi_user_group.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiUser\nmetadata:\n  name: aguitton\nspec:\n  # identity field is use to define the user identity on NiFi cluster side,\n  #\tit use full when the user's name doesn't suite with Kubernetes resource name.\n  identity: alexandre.guitton@konpyutaika.com\n  # Contains the reference to the NifiCluster with the one the registry client is linked.\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  # Whether or not the the operator also include a Java keystore format (JKS) with you secret\n  includeJKS: false\n  # Whether or not a certificate will be created for this user.\n  createCert: false\n  # defines the list of access policies that will be granted to the group.\n  accessPolicies:\n      # defines the kind of access policy, could be \"global\" or \"component\".\n    - type: component\n      # defines the kind of action that will be granted, could be \"read\" or \"write\"\n      action: read\n      # resource defines the kind of resource targeted by this access policies, please refer to the following page:\n      #\thttps://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#access-policies\n      resource: /data\n      # componentType is used if the type is \"component\", it's allow to define the kind of component on which is the\n      # access policy\n      componentType: \"process-groups\"\n      # componentId is used if the type is \"component\", it's allow to define the id of the component on which is the\n      # access policy\n      componentId: \"\"\n```\n\n----------------------------------------\n\nTITLE: Defining ServiceMonitor Resource for Prometheus - yaml\nDESCRIPTION: This ServiceMonitor resource specifies how Prometheus should scrape metrics from NiFi pods labeled with specific app and cluster names. Dependencies include the Prometheus Operator and ServiceMonitor CRDs. Key configuration includes endpoint scraping paths, relabelings for pod metadata, and namespace targeting. Inputs are selectors and endpoint configuration; results are Prometheus scraping additional cluster metrics from 'clusters' namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: cluster\n  namespace: monitoring-system\n  labels:\n    app: nifi-cluster\n    nifi_cr: cluster\nspec:\n  selector:\n    matchLabels:\n      app: nifi\n      nifi_cr: cluster\n  namespaceSelector:\n    matchNames:\n      - clusters\n  endpoints:\n    - interval: 10s\n      port: prometheus\n      path: /metrics\n      honorLabels: true\n      relabelings:\n        - sourceLabels: [__meta_kubernetes_pod_ip]\n          separator: ;\n          regex: (.*)\n          targetLabel: pod_ip\n          replacement: $1\n          action: replace\n        - sourceLabels: [__meta_kubernetes_pod_label_nodeId]\n          separator: ;\n          regex: (.*)\n          targetLabel: nodeId\n          replacement: $1\n          action: replace\n        - sourceLabels: [__meta_kubernetes_pod_label_nifi_cr]\n          separator: ;\n          regex: (.*)\n          targetLabel: nifi_cr\n          replacement: $1\n          action: replace\n\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Cluster SSL with Operator-Managed Certs (YAML)\nDESCRIPTION: This YAML snippet shows how to configure a NifiCluster custom resource to enable SSL using certificates automatically generated by the NiFi operator. It defines the required listeners (https, cluster, s2s) and specifies sslSecrets with `create: true` and a `tlsSecretName` for storing the generated certificates.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  readOnlyConfig:\n    # NifiProperties configuration that will be applied to the node.\n    nifiProperties:\n      webProxyHosts:\n        - nifistandard2.trycatchlearn.fr:8443\n        ...\n  ...\n  listenersConfig:\n    internalListeners:\n      - type: \"https\"\n        name: \"https\"\n        containerPort: 8443\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n```\n\n----------------------------------------\n\nTITLE: Applying NiFi OIDC Configuration via NiFiKop CRD - YAML\nDESCRIPTION: This YAML snippet demonstrates how to apply OpenID Connect and identity mapping configurations to a NiFi cluster managed by NiFiKop. It shows how to use the `spec.readOnlyConfig.nifiProperties.overrideConfigs` field within the `NifiCluster` CRD to inject custom `nifi.properties` settings, including the OIDC discovery URL, client ID, client secret, and the recommended identity mapping properties.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/5_users_authentication/1_oidc.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\n...\nspec:\n  ...\n  readOnlyConfig:\n    # NifiProperties configuration that will be applied to the node.\n    nifiProperties:\n      webProxyHosts:\n        - nifistandard2.trycatchlearn.fr:8443\n      # Additionnal nifi.properties configuration that will override the one produced based\n      # on template and configurations.\n      overrideConfigs: |\n        nifi.security.user.oidc.discovery.url=<oidc server discovery url>\n        nifi.security.user.oidc.client.id=<oidc client's id>\n        nifi.security.user.oidc.client.secret=<oidc client's secret>\n        nifi.security.identity.mapping.pattern.dn=CN=([^,]*)(?:, (?:O|OU)=.*)?\n        nifi.security.identity.mapping.value.dn=$1\n        nifi.security.identity.mapping.transform.dn=NONE\n      ...\n   ...\n...\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Nodes NiFiKop YAML\nDESCRIPTION: This YAML snippet demonstrates how to define and configure individual NiFi nodes using the NiFiKop operator's custom resource. It shows configurations for two nodes, illustrating the use of `nodeConfigGroup`, `readOnlyConfig` for overriding NiFi properties like banner text, and `nodeConfig` for defining resource requirements and storage configurations with PVC specifications. The `id` field is required for each node.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/5_references/1_nifi_cluster/4_node.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n    - id: 0\n      # nodeConfigGroup can be used to ease the node configuration, if set only the id is required\n      nodeConfigGroup: \"default_group\"\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        nifiProperties:\n          overrideConfigs: |\n            nifi.ui.banner.text=NiFiKop - Node 0\n      # node configuration\n#       nodeConfig:\n    - id: 2\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        overrideConfigs: |\n          nifi.ui.banner.text=NiFiKop - Node 2\n      # node configuration\n      nodeConfig:\n        resourcesRequirements:\n          limits:\n            cpu: \"2\"\n            memory: 3Gi\n          requests:\n            cpu: \"1\"\n            memory: 1Gi\n        storageConfigs:\n          # Name of the storage config, used to name PV to reuse into sidecars for example.\n          - name: provenance-repository\n            # Path where the volume will be mount into the main nifi container inside the pod.\n            mountPath: \"/opt/nifi/provenance_repository\"\n            # Metadata to attach to the PVC that gets created\n            metadata:\n              labels:\n                my-label: my-value\n              annotations:\n                my-annotation: my-value\n            # Kubernetes PVC spec\n            # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n            pvcSpec:\n              accessModes:\n                - ReadWriteOnce\n              storageClassName: \"standard\"\n              resources:\n                requests:\n                  storage: 8Gi\n```\n\n----------------------------------------\n\nTITLE: Applying NiFiCluster Configuration - Kubectl - Shell\nDESCRIPTION: This shell command uses `kubectl` to apply the updated `NifiCluster` custom resource definition from the specified YAML file. It targets the `nifi` namespace. Applying this configuration triggers the NiFiKop operator to reconcile the cluster state, initiating either a scale-up or scale-down process based on the changes made to the `NifiCluster.Spec.Nodes` list.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nkubectl -n nifi apply -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Node Configuration Example 2 YAML\nDESCRIPTION: This YAML snippet provides a more detailed configuration for a NiFi node, including resource requirements (CPU and memory limits and requests) and storage configurations. It demonstrates how to define a PersistentVolumeClaim (PVC) for the provenance repository, including access modes, storage class, and storage size.  `overrideConfigs` property allows modification of `nifi.properties`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/5_references/1_nifi_cluster/4_node.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n    - id: 2\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        overrideConfigs: |\n          nifi.ui.banner.text=NiFiKop - Node 2\n      # node configuration\n      nodeConfig:\n        resourcesRequirements:\n          limits:\n            cpu: \"2\"\n            memory: 3Gi\n          requests:\n            cpu: \"1\"\n            memory: 1Gi\n        storageConfigs:\n          # Name of the storage config, used to name PV to reuse into sidecars for example.\n          - name: provenance-repository\n            # Path where the volume will be mount into the main nifi container inside the pod.\n            mountPath: \"/opt/nifi/provenance_repository\"\n            # Metadata to attach to the PVC that gets created\n            metadata:\n              labels:\n                my-label: my-value\n              annotations:\n                my-annotation: my-value\n            # Kubernetes PVC spec\n            # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n            pvcSpec:\n              accessModes:\n                - ReadWriteOnce\n              storageClassName: \"standard\"\n              resources:\n                requests:\n                  storage: 8Gi\n```\n\n----------------------------------------\n\nTITLE: Configuring Conversion Webhook for NiFiKop CRDs\nDESCRIPTION: YAML configuration required for enabling the conversion webhook to handle resource version conversions from v1alpha1 to v1 in NiFiKop CRDs.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n```\n\n----------------------------------------\n\nTITLE: Listing All Helm Releases (Deployed, Deleted, Failed)\nDESCRIPTION: Demonstrates the `helm list --all` command to retrieve a comprehensive list of all Helm releases known to the cluster, including currently deployed, deleted, and failed installation attempts.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --all\n```\n\n----------------------------------------\n\nTITLE: Verifying Horizontal Pod Autoscaler (HPA) deployment with kubectl (console command)\nDESCRIPTION: Displays the current status of the HPA managing the NiFi node autoscaler, including related target metrics, pod targets, and replica counts. Dependencies include a Kubernetes cluster with HPA deployment and kubectl configured to access the 'clusters' namespace. The output helps verify that autoscaling is functioning correctly based on metrics.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_13\n\nLANGUAGE: console\nCODE:\n```\nkubectl get -n clusters hpa\nNAME                REFERENCE                                                TARGETS         MINPODS   MAXPODS   REPLICAS   AGE\nkeda-hpa-cluster    NifiNodeGroupAutoscaler/nifinodegroupautoscaler-sample     1833m/2 (avg)   1         3         2          25d\n```\n\n----------------------------------------\n\nTITLE: Defining Additional Internal Listener in NiFi on Kubernetes (YAML)\nDESCRIPTION: This snippet shows how to add an additional internal listener for a NiFi processor exposed through a port, such as an HTTP endpoint for receiving HTTP requests within NiFi. Unlike the standard internal listeners, this one doesn't have a specific type, indicating it's not related to core NiFi internal behavior.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    ...\n    - name: \"http-tracking\"\n      containerPort: 8081\n```\n\n----------------------------------------\n\nTITLE: Installing Zookeeper Using Helm Chart\nDESCRIPTION: This command installs Zookeeper using the Bitnami Helm chart with specified resource requests and limits, resource class, replica count, and namespace. It requires Helm 3 and appropriately configured Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/2_setup/1_getting_started.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhelm install zookeeper bitnami/zookeeper \\\n    --namespace=zookeeper \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set global.storageClass=standard \\\n    --set networkPolicy.enabled=true \\\n    --set replicaCount=3 \\\n    --create-namespace\n```\n\n----------------------------------------\n\nTITLE: NifiUserStatus YAML Schema for User State\nDESCRIPTION: This YAML schema defines the status structure for NifiUser, including fields like id and version. It captures the current node ID and revision version of the NiFi user for synchronization and state monitoring.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/5_references/2_nifi_user.md#_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\n`status:\n  id: string\n  version: string`\n```\n\n----------------------------------------\n\nTITLE: Listing All Helm Releases (bash)\nDESCRIPTION: Lists all Helm releases in the cluster, including those that are currently deployed, deleted, or failed. This command provides a complete overview of all Helm release history in the cluster. Requires Helm installed and configured.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --all\n```\n\n----------------------------------------\n\nTITLE: Packaging NiFiKop Helm Chart\nDESCRIPTION: Packages the NiFiKop Helm chart into a deployable archive. This allows the chart to be easily distributed and installed in Kubernetes clusters.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/6_contributing/1_developer_guide.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nmake helm-package\n```\n\n----------------------------------------\n\nTITLE: Installing cert-manager with Helm 3 in Bash\nDESCRIPTION: This snippet provides an alternative method to install cert-manager using Helm 3. It begins by applying the CustomResourceDefinitions with validation disabled, adds and updates the jetstack Helm repository, and then installs cert-manager into the cert-manager namespace. This approach allows for better management of cert-manager via Helm but requires Helm 3 installed and access to the internet.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/2_deploy_nifikop/1_quick_start.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Install CustomResourceDefinitions first\nkubectl apply --validate=false -f \\\n   https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.crds.yaml\n\n# Add the jetstack helm repo\nhelm repo add jetstack https://charts.jetstack.io\nhelm repo update\n\n# You have to create the namespace before executing following command\nhelm install cert-manager \\\n    --namespace cert-manager \\\n    --version v1.7.2 jetstack/cert-manager\n```\n\n----------------------------------------\n\nTITLE: Adding Helm Repo for Prometheus\nDESCRIPTION: This snippet adds the Prometheus Helm repository to your Helm configuration. It retrieves the necessary chart information to install Prometheus using Helm. This is necessary to then install the Prometheus stack in the next steps.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_6\n\nLANGUAGE: console\nCODE:\n```\nhelm repo add prometheus https://prometheus-community.github.io/helm-charts\n```\n\n----------------------------------------\n\nTITLE: Installing Nifikop Helm Chart and Skipping CRD Installation - Bash\nDESCRIPTION: This snippet shows how to install the Nifikop chart using Helm while skipping installation of CRDs by setting --skip-crds. This is beneficial if CRDs already exist or are managed externally. Key parameters are --name for release name, the chart path, --set for namespaces, and --skip-crds. Ensure CRDs are present before use.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nhelm install --name nifikop ./helm/nifikop --set namespaces={\"nifikop\"} --skip-crds\n```\n\n----------------------------------------\n\nTITLE: Apply NiFiCluster configuration via Kubectl\nDESCRIPTION: This command applies the `NifiCluster` configuration to the Kubernetes cluster in the `nifi` namespace. It uses the configuration defined in `config/samples/simplenificluster.yaml`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nkubectl -n nifi apply -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Deploying RBAC Resources for Prometheus\nDESCRIPTION: Creates ServiceAccount, ClusterRole, and ClusterRoleBinding for Prometheus in the 'monitoring-system' namespace, granting necessary permissions for metrics and API access. Requires YAML syntax and Kubernetes cluster access.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_8\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: prometheus\n  namespace: monitoring-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: prometheus\nrules:\n- apiGroups: [\"\"]\n  resources:\n  - nodes\n  - nodes/metrics\n  - services\n  - endpoints\n  - pods\n  verbs: [\"get\", \"list\", \"watch\"]\n- apiGroups: [\"\"]\n  resources:\n  - configmaps\n  verbs: [\"get\"]\n- apiGroups:\n  - networking.k8s.io\n  resources:\n  - ingresses\n  verbs: [\"get\", \"list\", \"watch\"]\n- nonResourceURLs: [\"/metrics\"]\n  verbs: [\"get\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: prometheus\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: prometheus\nsubjects:\n- kind: ServiceAccount\n  name: prometheus\n  namespace: monitoring-system\n```\n\n----------------------------------------\n\nTITLE: Node Configuration Example 1 YAML\nDESCRIPTION: This YAML snippet demonstrates the basic configuration of a NiFi node, specifying an ID and a node configuration group. It also includes a read-only configuration to set a custom banner text in the NiFi UI. The `overrideConfigs` property allows modification of `nifi.properties`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/5_references/1_nifi_cluster/4_node.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n    - id: 0\n      # nodeConfigGroup can be used to ease the node configuration, if set only the id is required\n      nodeConfigGroup: \"default_group\"\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        nifiProperties:\n          overrideConfigs: |\n            nifi.ui.banner.text=NiFiKop - Node 0\n      # node configuration\n```\n\n----------------------------------------\n\nTITLE: Verifying deployment of the new node\nDESCRIPTION: This CLI command retrieves Kubernetes pods, configmaps, and persistent volume claims with label nodeId=25 to verify that the new node resource is properly created and running. It helps confirm that the scale-up process completed successfully and that the new node is operational, with its associated storage and configuration resources bound.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\nkubectl get pods,configmap,pvc -l nodeId=25\n```\n\n----------------------------------------\n\nTITLE: Deleting Nifi Operator CRDs with kubectl (Bash)\nDESCRIPTION: These commands use kubectl to manually delete the specific Custom Resource Definitions (CRDs) installed by the Nifi operator chart. This step is necessary if you want to remove the CRDs after deleting the Helm release, as Helm does not remove them by default. Be extremely cautious, as deleting CRDs will also delete ALL custom resources (like NifiClusters, NifiUsers, etc.) created from those CRDs.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/2_setup/3_install/1_customizable_install_with_helm.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete crd nificlusters.nifi.konpyutaika.com\nkubectl delete crd nifiusers.nifi.konpyutaika.com\nkubectl delete crd nifiusergroups.nifi.konpyutaika.com\nkubectl delete crd nifiregistryclients.nifi.konpyutaika.com\nkubectl delete crd nifiparametercontexts.nifi.konpyutaika.com\nkubectl delete crd nifidataflows.nifi.konpyutaika.com\n```\n\n----------------------------------------\n\nTITLE: Port-Forwarding to Prometheus (console)\nDESCRIPTION: This command sets up a local port-forwarding tunnel, allowing you to access the Prometheus web UI running inside the Kubernetes cluster from your local machine. It maps local port 9090 to port 9090 of the `prometheus-operated` service in the `monitoring-system` namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_12\n\nLANGUAGE: console\nCODE:\n```\nkubectl port-forward -n monitoring-system service/prometheus-operated 9090:9090\n```\n\n----------------------------------------\n\nTITLE: Building NiFiKop Project - Bash\nDESCRIPTION: This snippet builds the NiFiKop project using the `make build` command, assuming the developer is using a local Go environment. This command compiles the source code into an executable binary. The output is a runnable operator that can be executed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/6_contributing/1_developer_guide.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmake build\n```\n\n----------------------------------------\n\nTITLE: Adding Node to NiFi Cluster Custom Resource (YAML)\nDESCRIPTION: Defines a NifiCluster Custom Resource configuration. It includes adding a new node with `id: 25` to the `spec.nodes` list, inheriting configuration from the `default_group`. This definition is used by the NiFiKop operator to scale up the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n  clusterManager: zookeeper\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.12.1\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n    - id: 2\n      nodeConfigGroup: \"default_group\"\n# >>>> START: The new node\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n# <<<< END\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: Obtaining OpenShift Namespace UID for NiFi Security Context\nDESCRIPTION: Retrieves the supplemental group ID associated with the 'nifi' namespace in OpenShift to configure security context parameters for running NiFi pods. This command parses the namespace annotations to extract the UID/GID needed to align with OpenShift's Security Context Constraints.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nuid=$(kubectl get namespace nifi -o=jsonpath='{.metadata.annotations.openshift\\.io/sa\\.scc\\.supplemental-groups}' | sed 's/\\/10000$//' | tr -d '[:space:]')\n```\n\n----------------------------------------\n\nTITLE: Checking Helm Release Status Using Bash\nDESCRIPTION: This Bash command retrieves and displays the status of a specific Helm release named `nifikop`, providing insight into the deployment state, health, and any issues.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nhelm status nifikop\n```\n\n----------------------------------------\n\nTITLE: Installing Helm Chart without CRDs - Helm (bash)\nDESCRIPTION: This command installs a Helm chart named \"nifikop\" from a local directory while skipping the installation of CRDs. It utilizes the `--skip-crds` flag and sets the namespace using the `--set` parameter. This command requires Helm CLI installed and configured.  The output is the deployment of the chart's resources without modifying existing CRDs.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\n$ helm install --name nifikop ./helm/nifikop --set namespaces={\"nifikop\"} --skip-crds\n```\n\n----------------------------------------\n\nTITLE: Installing KEDA Helm Chart and Namespace Creation - kubectl & Helm CLI - console\nDESCRIPTION: This set of commands creates a Kubernetes namespace ('keda') and installs the KEDA Helm chart in that namespace. Requires prior initialization of the Helm repository for KEDA. The commands ensure the KEDA components run isolated in their own namespace, following best practice for cluster add-ons.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl create namespace keda\nhelm install keda kedacore/keda --namespace keda\n```\n\n----------------------------------------\n\nTITLE: Updating NiFi OpenShift Sample Manifest with Correct UID\nDESCRIPTION: Modifies the OpenShift NiFi cluster sample manifest file by replacing the placeholder UID value (1000690000) with the actual UID obtained for the 'nifi' namespace. This Bash sed command performs an in-place substitution ensuring the manifest uses the correct security context to comply with OpenShift requirements.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nsed -i \"s/1000690000/$uid/g\" config/samples/openshift.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing Nifikop Helm Chart Skipping CRDs (Bash)\nDESCRIPTION: This command installs the Nifikop Helm chart located at './helm/nifikop' with the release name 'nifikop' and sets the 'namespaces' parameter. The crucial flag `--skip-crds` instructs Helm not to install the Custom Resource Definitions bundled with the chart. This is useful in scenarios where CRDs are managed separately or already exist in the cluster and should not be overwritten.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/2_setup/3_install/1_customizable_install_with_helm.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\n$ helm install --name nifikop ./helm/nifikop --set namespaces={\"nifikop\"} --skip-crds\n```\n\n----------------------------------------\n\nTITLE: Setting Active NiFi Authorizer Property\nDESCRIPTION: This snippet shows the NiFi property `nifi.security.user.authorizer` used to specify which authorizer, defined in `authorizers.xml`, should be the active one. Setting this property to `custom-database-authorizer` activates the custom authorizer defined in the provided XML template.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/3_tasks/2_security/2_authorization/1_custom_authorizer.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nnifi.security.user.authorizer=custom-database-authorizer\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFiCluster for Scale Down (YAML)\nDESCRIPTION: Modifies the NifiCluster custom resource definition (CRD) to remove a node definition from the 'nodes' list. This instructs the NiFiKop operator to initiate the graceful decommissioning process for the specified NiFi node.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  headlessServiceEnabled: true\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.11.3\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n# >>>> START: node removed\n#    - id: 2\n#      nodeConfigGroup: \"default_group\"\n# <<<< END\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Internal Listeners for NiFi Processors (YAML)\nDESCRIPTION: This YAML example shows how to add a custom internal listener (without a type) to the internalListeners array for exposing a NiFi processor via an additional port (e.g., to receive HTTP requests). Dependencies include enabling additional ports in both the NiFi configuration and Kubernetes YAML, and the list is appended to the existing internalListeners. Inputs required are the name and containerPort of the new listener. Outputs are the new port made available inside the NiFi pod. No type key is specified, indicating it is not related to core NiFi internal behavior, and arbitrary application ports can be exposed this way.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    ...\n    - name: \"http-tracking\"\n      containerPort: 8081\n```\n\n----------------------------------------\n\nTITLE: Configuring Internal NiFi Listeners with YAML\nDESCRIPTION: This YAML snippet demonstrates how to define internal listener ports for a NiFi cluster within Kubernetes using the listenersConfig.internalListeners field. Each entry specifies the type, name, and the containerPort, which corresponds to NiFi's internal functional endpoints (such as https, cluster, s2s, prometheus, and load-balance). Prerequisites: a functioning Kubernetes cluster and the NiFi operator installed. All fields are required except for additional listener types, which can be omitted from the type property if not directly tied to core NiFi behavior.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    - type: \"https\"\n      name: \"https\"\n      containerPort: 8443\n    - type: \"cluster\"\n      name: \"cluster\"\n      containerPort: 6007\n    - type: \"s2s\"\n      name: \"s2s\"\n      containerPort: 10000\n    - type: \"prometheus\"\n      name: \"prometheus\"\n      containerPort: 9090\n    - type: \"load-balance\"\n      name: \"load-balance\"\n      containerPort: 6342\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop Operator with RunAsUser on OpenShift Using Helm and Bash\nDESCRIPTION: This Helm install command deploys the NiFiKop operator on an OpenShift cluster, explicitly setting the `runAsUser` parameter to comply with OpenShift restricted SCC policies. It assumes the namespace exists and uses version 1.1.1 of the operator image. The resources requests and limits are specified for CPU and memory, and the UID obtained from the earlier snippet is passed to the runAsUser parameter to ensure security compliance of pod execution.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/2_deploy_nifikop/1_quick_start.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop \\\n    nifikop \\\n    --namespace=nifi \\\n    --version 1.1.1 \\\n    --set image.tag=v1.1.1-release \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set namespaces={\"nifi\"} \\\n    --set runAsUser=$uid\n```\n\n----------------------------------------\n\nTITLE: Templating authorizers.xml with Custom Providers in NiFi (YAML)\nDESCRIPTION: This YAML template is intended for dynamically generating NiFi's authorizers.xml configuration file within a NiFiKOp deployment. It injects cluster-specific values using Go templating expressions (e.g., .NodeList, .ClusterName, .Namespace), supporting both standard (file-based) and custom (database-backed) user group and access policy providers. The output file configures user group providers, access policy providers, and references custom authorizer classes. Prerequisites include a running NiFiKOp environment and the corresponding provider classes on the NiFi classpath. Limitations include ensuring all injected template properties are defined and the required custom classes are available. The template expects to output a valid XML configuration for NiFi authorization.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/6_users_authorization/1_custom_user_authorizer.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n{{- $nodeList := .NodeList }}\n{{- $clusterName := .ClusterName }}\n{{- $namespace := .Namespace }}<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>\n<authorizers>\n    <userGroupProvider>\n        <identifier>file-user-group-provider</identifier>\n        <class>org.apache.nifi.authorization.FileUserGroupProvider</class>\n        <property name=\"Users File\">../data/users.xml</property>\n        <property name=\"Legacy Authorized Users File\"></property>\n        <property name=\"Initial User Identity admin\">{{ .ControllerUser }}</property>\n{{- range $i, $host := .NodeList }}\n        <property name=\"Initial User Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n    </userGroupProvider>\n    <userGroupProvider>\n        <identifier>database-user-group-provider</identifier>\n        <class>my.custom.DatabaseUserGroupProvider</class>\n        <!-- Any extra configuration for this provider goes here -->\n{{- range $i, $host := .NodeList }}\n        <property name=\"Initial User Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n    </userGroupProvider>\n    <accessPolicyProvider>\n        <identifier>file-access-policy-provider</identifier>\n        <class>org.apache.nifi.authorization.FileAccessPolicyProvider</class>\n        <property name=\"User Group Provider\">file-user-group-provider</property>\n        <property name=\"Authorizations File\">../data/authorizations.xml</property>\n        <property name=\"Initial Admin Identity\">{{ .ControllerUser }}</property>\n        <property name=\"Legacy Authorized Users File\"></property>\n{{- range $i, $host := .NodeList }}\n        <property name=\"Node Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n\t\t<property name=\"Node Group\"></property>\n    </accessPolicyProvider>\n    <accessPolicyProvider>\n        <identifier>database-access-policy-provider</identifier>\n        <class>my.custom.DatabaseAccessPolicyProvider</class>\n        <!-- Any extra configuration for this provider goes here -->\n{{- range $i, $host := .NodeList }}\n        <property name=\"Node Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n\t\t<property name=\"Node Group\"></property>\n    </accessPolicyProvider>\n    <authorizer>\n        <identifier>managed-authorizer</identifier>\n        <class>org.apache.nifi.authorization.StandardManagedAuthorizer</class>\n        <property name=\"Access Policy Provider\">file-access-policy-provider</property>\n    </authorizer>\n    <authorizer>\n        <identifier>custom-database-authorizer</identifier>\n        <class>my.custom.DatabaseAuthorizer</class>\n        <property name=\"Access Policy Provider\">database-access-policy-provider</property>\n    </authorizer>\n</authorizers>\n\n```\n\n----------------------------------------\n\nTITLE: Defining Basic NiFi Node with Group and Read-Only Configurations in YAML\nDESCRIPTION: This snippet initializes NiFi nodes with optional grouping and read-only configurations. It includes detailed settings for node identifiers, configuration groups, and read-only parameters such as UI banner text. The configuration facilitates rolling upgrades when the read-only config changes and helps manage multiple nodes with shared or distinct settings.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/5_references/1_nifi_cluster/4_node.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n    - id: 0\n      # nodeConfigGroup can be used to ease the node configuration, if set only the id is required\n      nodeConfigGroup: \"default_group\"\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        nifiProperties:\n          overrideConfigs: |\n            nifi.ui.banner.text=NiFiKop - Node 0\n      # node configuration\n```\n\n----------------------------------------\n\nTITLE: Defining Prerequisite NifiDataflow Resources in YAML\nDESCRIPTION: Defines two example NifiDataflow custom resources ('input' and 'output') using YAML within the 'nifikop' namespace. These resources represent the source and destination dataflows required before creating a NifiConnection between them. The 'input' dataflow must have an output port and the 'output' dataflow must have an input port.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/4_manage_connections/1_deploy_connection.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiDataflow\nmetadata:\n  name: input\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  bucketId: deedb9f6-65a4-44e9-a1c9-722008fcd0ba\n  flowId: ab95431d-980d-41bd-904a-fac4bd864ba6\n  flowVersion: 1\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  skipInvalidComponent: true\n  skipInvalidControllerService: true\n  syncMode: always\n  updateStrategy: drain\n  flowPosition:\n    posX: 0\n    posY: 0\n---\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiDataflow\nmetadata:\n  name: output\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  bucketId: deedb9f6-65a4-44e9-a1c9-722008fcd0ba\n  flowId: fc5363eb-801e-432f-aa94-488838674d07\n  flowVersion: 2\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  skipInvalidComponent: true\n  skipInvalidControllerService: true\n  syncMode: always\n  updateStrategy: drain\n  flowPosition:\n    posX: 750\n    posY: 0\n```\n\n----------------------------------------\n\nTITLE: NifiProperties overrideConfigs Example\nDESCRIPTION: This YAML snippet shows an example of using `overrideConfigs` within the `nifiProperties` section of `ReadOnlyConfig` to set a custom banner text. This setting will override any default banner text or text configured via ConfigMaps and Secrets.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/5_references/1_nifi_cluster/2_read_only_config.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\n    overrideConfigs: |\n      nifi.ui.banner.text=NiFiKop\n```\n\n----------------------------------------\n\nTITLE: Configuring DestinationRule for TLS and Sticky Session Management in Istio\nDESCRIPTION: This YAML snippet defines an Istio DestinationRule to enforce TLS connection mode as SIMPLE for traffic directed to the specified service, and to manage session stickiness via the 'httpCookie' with the name '__Secure-Authorization-Bearer'.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_4\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: DestinationRule\nmetadata:\n  name: nifi-dr\nspec:\n  host: <service-name>.<namespace>.svc.cluster.local\n  trafficPolicy:\n    tls:\n      mode: SIMPLE\n    loadBalancer:\n      consistentHash:\n        httpCookie:\n          name: __Secure-Authorization-Bearer\n          ttl: 0s\n```\n\n----------------------------------------\n\nTITLE: Configuring NifiCluster InitContainerImage with Busybox - YAML\nDESCRIPTION: This YAML manifest defines a NifiCluster resource using the older approach where the initContainerImage is set to 'busybox' with tag '1.34.0'. This configuration is no longer compatible with nifikop v0.15.0 due to the dependency on the presence of Bash in the init container image. Required parameters include the apiVersion, kind, metadata.name, and spec.initContainerImage fields. Inputs must conform to the expected NifiCluster CRD schema. Limitation: 'busybox' does not provide a full Bash shell, leading to upgrade issues.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/7_upgrade_guides/2_v0.14.1_to_v0.15.0.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  initContainerImage:\n    repository: busybox\n    tag: \"1.34.0\"\n```\n\n----------------------------------------\n\nTITLE: Verifying External Service Creation (kubectl)\nDESCRIPTION: Shows the expected output of the `kubectl get services` command after applying the `externalServices` configuration. It confirms the creation of the `cluster-access` service, displaying its type (`LoadBalancer`), internal cluster IP, assigned external IP (if available from the cloud provider), and the port mappings (external port:node port/protocol).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nkubectl get services\nNAME                TYPE           CLUSTER-IP    EXTERNAL-IP      PORT(S)                                AGE\ncluster-access      LoadBalancer   10.88.21.98   35.180.241.132   443:30421/TCP,80:30521/TCP             20d\n```\n\n----------------------------------------\n\nTITLE: Installing nifikop Helm Chart with Custom Release Name - Bash\nDESCRIPTION: Installs the nifikop Helm chart using a user-specified release name. No custom parameters or options are set in this example. Helm must have the konpyutaika repository configured and access to the desired namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nhelm install <release name> konpyutaika/nifikop\n```\n\n----------------------------------------\n\nTITLE: Adding the Bitnami Helm Repository (Bash)\nDESCRIPTION: Adds the Bitnami Helm chart repository to your local Helm configuration. This repository hosts the Zookeeper chart, which is a prerequisite for installing NiFi. Requires the Helm CLI to be installed and configured.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhelm repo add bitnami https://charts.bitnami.com/bitnami\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi UserGroup Resource (YAML)\nDESCRIPTION: This YAML snippet provides an example definition of a Kubernetes custom resource of kind `NifiUserGroup`. It demonstrates how to specify the API version, resource kind, metadata (name), and the desired state (`spec`) including the group's identity, references to the associated NiFi cluster and users, and a list of access policies.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/5_references/6_nifi_usergroup.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  identity: \"My Special Group\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n    - name: nc-controller.nifikop.mgt.cluster.local\n  accessPolicies:\n    - type: global\n      action: read\n      resource: /counters\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFi cluster using kubectl in Bash\nDESCRIPTION: Deploys a simple NiFi cluster by creating Kubernetes resources defined in the local configuration file 'config/samples/simplenificluster.yaml' within the 'nifi' namespace using kubectl. This requires prior installation of Zookeeper and the existence of the namespace 'nifi'.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create -n nifi -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring Conversion Webhook in CRDs for NiFiKop\nDESCRIPTION: YAML configuration for enabling the conversion webhook in NiFiKop CRDs. This setup allows handling resource conversions from v1alpha1 to v1 versions using cert-manager for certificate management.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Versioned Dataflows (JSON)\nDESCRIPTION: Provides an example structure for configuring versioned NiFi dataflows deployed by the operator via the Helm chart. It defines parameters like bucket/flow IDs, name, version, position, parameter context, registry client, synchronization mode, and update strategy. Requires pre-configured registry clients and parameter contexts to be enabled and present.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/helm/nifi-cluster/README.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[{\"bucketId\":\"\",\"enabled\":false,\"flowId\":\"\",\"flowPosition\":{\"posX\":0,\"posY\":0},\"flowVersion\":1,\"name\":\"My Special Dataflow\",\"parameterContextRef\":{\"name\":\"default\",\"namespace\":\"nifi\"},\"registryClientRef\":{\"name\":\"default\",\"namespace\":\"nifi\"},\"skipInvalidComponent\":true,\"skipInvalidControllerService\":true,\"syncMode\":\"always\",\"updateStrategy\":\"drain\"}]\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Namespace for Monitoring\nDESCRIPTION: Creates a dedicated namespace 'monitoring-system' in Kubernetes for deploying Prometheus and related resources. Requires kubectl access to the cluster and appropriate permissions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create namespace monitoring-system\n```\n\n----------------------------------------\n\nTITLE: Creating Prometheus Namespace\nDESCRIPTION: This command creates a dedicated namespace, `monitoring-system`, for the Prometheus deployment. This is used for organizational purposes, isolating monitoring components from other resources in the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_5\n\nLANGUAGE: console\nCODE:\n```\nkubectl create namespace monitoring-system\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFiKop CRD Conversion Webhook\nDESCRIPTION: YAML snippet illustrating the annotations and spec configuration required within NiFiKop CRD definitions to enable the conversion webhook. This allows seamless conversion between API versions (e.g., v1alpha1 to v1) and requires cert-manager for CA injection.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/2_deploy_nifikop/1_quick_start.md#_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n```\n\n----------------------------------------\n\nTITLE: Performing a Dry Run Helm Install of Nifikop\nDESCRIPTION: Executes a dry run installation of the Nifikop Helm chart using `helm install --dry-run`. This command simulates the installation process, outputting the generated Kubernetes manifests without applying them, allowing for review. It also demonstrates setting the `logLevel` and target `namespaces` parameters.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop \\\n    --dry-run \\\n    --set logLevel=Debug \\\n    --set namespaces={\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: NiFiKop Migration Script Implementation in Node.js\nDESCRIPTION: The main migration script that copies NiFiKop resources from the old API group (nifi.orange.com) to the new API group (nifi.konpyutaika.com) while preserving their specifications and status.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/7_upgrade/1_v0.7.x_to_v0.8.0.md#_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nprocess.env['NODE_TLS_REJECT_UNAUTHORIZED'] = 0;\nconst k8s = require('@kubernetes/client-node');\n\nconst kc = new k8s.KubeConfig();\nkc.loadFromDefault();\n\nconst k8sApi = kc.makeApiClient(k8s.CustomObjectsApi);\n\nconst KONPYUTAIKA_GROUP = 'nifi.konpyutaika.com';\nconst KONPYUTAIKA_GROUP_VERSION = 'v1alpha1';\nconst ORANGE_GROUP = 'nifi.orange.com';\nconst ORANGE_GROUP_VERSION = 'v1alpha1';\n\nconst call = async (SRC_GRP, SRC_GRP_VER, DST_GRP, DST_GRP_VER, KIND_PLURAL, NAMESPACE) => {\n\tconsole.log(`Listing ${KIND_PLURAL} of ${SRC_GRP}/${SRC_GRP_VER} in ${NAMESPACE}...`);\n\tconst listResources = (await k8sApi.listNamespacedCustomObject(SRC_GRP, SRC_GRP_VER, NAMESPACE, KIND_PLURAL)).body.items;\n\treturn Promise.all(listResources.map(async (resource) => {\n\t\ttry {\n\t\t\tconsole.log(`Found ${resource.kind} \"${resource.metadata.name}\" of ${resource.apiVersion} in ${NAMESPACE}`);\n\n\t\t\tif (resource.metadata.ownerReferences) {\n\t\t\t\tconsole.log(`${resource.kind} ${resource.metadata.name} mananged by something else (ownerRefereces is set).`);\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tconst bodyResource = {\n\t\t\t\tapiVersion: `${DST_GRP}/${DST_GRP_VER}`,\n\t\t\t\tkind: resource.kind,\n\t\t\t\tmetadata: {\n\t\t\t\t\tname: resource.metadata.name,\n\t\t\t\t\tannotations: resource.metadata.annotations,\n\t\t\t\t\tlabels: resource.metadata.labels\n\t\t\t\t},\n\t\t\t\tspec: resource.spec\n\t\t\t};\n\n\t\t\tconsole.log(`Creating ${bodyResource.kind} \"${bodyResource.metadata.name}\" of ${bodyResource.apiVersion} in ${NAMESPACE}...`);\n\t\t\tconst newResource = (await k8sApi.createNamespacedCustomObject(DST_GRP, DST_GRP_VER, NAMESPACE, KIND_PLURAL, bodyResource)).body;\n\t\t\tconsole.log('...done creating.');\n\n\t\t\tconst bodyStatus = {\n\t\t\t\tapiVersion: newResource.apiVersion,\n\t\t\t\tkind: newResource.kind,\n\t\t\t\tmetadata: {\n\t\t\t\t\tname: newResource.metadata.name,\n\t\t\t\t\tresourceVersion: newResource.metadata.resourceVersion\n\t\t\t\t},\n\t\t\t\tstatus: resource.status\n\t\t\t};\n\n\t\t\tconsole.log(`Copying status from ${resource.kind} \"${resource.metadata.name}\" of ${newResource.apiVersion} to ${newResource.kind} \"${newResource.metadata.name}\" of ${newResource.apiVersion} in ${NAMESPACE}...`);\n\t\t\tconst newResourceWithStatus = (await k8sApi.replaceNamespacedCustomObjectStatus(DST_GRP, DST_GRP_VER, NAMESPACE, KIND_PLURAL, bodyStatus.metadata.name, bodyStatus)).body;\n\t\t\tconsole.log('...done copying.');\n\t\t\treturn newResourceWithStatus;\n\t\t}\n\t\tcatch (e) {\n\t\t\tconsole.error(e.body ? e.body.message ? e.body.message : e.body : e);\n\t\t}\n\t}));\n};\n\nconst argv = require('minimist')(process.argv.slice(2));\n\nlet NAMESPACE = argv.namespace ? argv.namespace.length > 0 ? argv.namespace : 'default' : 'default';\nlet KIND_PLURAL = {\n\tcluster: 'nificlusters',\n\tdataflow: 'nifidataflows',\n\tparametercontext: 'nifiparametercontexts',\n\tregistryclient: 'nifiregistryclients',\n\tuser: 'nifiusers',\n\tusergroup: 'nifiusergroups',\n};\n\nif (!argv.type) {\n\tconsole.error('Type not provided');\n\tprocess.exit(1);\n}\n\nif (!KIND_PLURAL[argv.type]) {\n\tconsole.error(`Type ${argv.type} is not one of the following types: ${Object.keys(KIND_PLURAL)}`);\n\tprocess.exit(1);\n}\n\nconsole.log(`########### START: ${KIND_PLURAL[argv.type]} ###########`);\ncall( ORANGE_GROUP, ORANGE_GROUP_VERSION, KONPYUTAIKA_GROUP, KONPYUTAIKA_GROUP_VERSION, KIND_PLURAL[argv.type], NAMESPACE)\n\t.then(r => console.log('############ END ############'))\n\t.catch(e => console.error(e));\n```\n\n----------------------------------------\n\nTITLE: Configuring KEDA ScaledObject for NiFi Autoscaling\nDESCRIPTION: Defines a KEDA ScaledObject that targets a NifiNodeGroupAutoscaler resource. Configures scaling parameters and uses a Prometheus query trigger to determine when to scale the NiFi cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: cluster\n  namespace: clusters\nspec:\n  scaleTargetRef:\n    apiVersion:    nifi.konpyutaika.com/v1alpha1     # Optional. Default: apps/v1\n    kind:          NifiNodeGroupAutoscaler           # Optional. Default: Deployment\n    name:          nifinodegroupautoscaler-sample    # Mandatory. Must be in the same namespace as the ScaledObject\n    envSourceContainerName: nifi                     # Optional. Default: .spec.template.spec.containers[0]\n  pollingInterval:  30                               # Optional. Default: 30 seconds\n  cooldownPeriod:   300                              # Optional. Default: 300 seconds\n  idleReplicaCount: 0                                # Optional. Default: ignored, must be less than minReplicaCount \n  minReplicaCount:  1                                # Optional. Default: 0\n  maxReplicaCount:  3                                # Optional. Default: 100\n  fallback:                                          # Optional. Section to specify fallback options\n    failureThreshold: 5                              # Mandatory if fallback section is included\n    replicas: 1                                      # Mandatory if fallback section is included\n  # advanced:                                          # Optional. Section to specify advanced options\n  #   restoreToOriginalReplicaCount: true              # Optional. Whether the target resource should be scaled back to original replicas count, after the ScaledObject is deleted\n  #   horizontalPodAutoscalerConfig:                   # Optional. Section to specify HPA related options\n  #     name: {name-of-hpa-resource}                   # Optional. Default: keda-hpa-{scaled-object-name}\n  #     behavior:                                      # Optional. Use to modify HPA's scaling behavior\n  #       scaleDown:\n  #         stabilizationWindowSeconds: 300 <--- \n  #         policies:\n  #         - type: Percent\n  #           value: 100\n  #           periodSeconds: 15\n  triggers:\n    - type: prometheus\n      metadata:\n        serverAddress: http://prometheus-operated.monitoring-system.svc:9090\n        metricName: http_requests_total\n        threshold: <threshold value>\n        query: <prometheus query>\n```\n\n----------------------------------------\n\nTITLE: Adding a new NiFi node to NifiCluster Spec - YAML\nDESCRIPTION: This YAML snippet demonstrates how to add a new NiFi node configuration to the `NifiCluster.Spec.Nodes` section.  The `id` field must be unique within the nodes list. This configuration defines the node's properties, including the node config group.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/3_tasks/1_nifi_cluster/2_cluster_scaling.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.12.1\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n    - id: 2\n      nodeConfigGroup: \"default_group\"\n# >>>> START: The new node\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n# <<<< END\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: Running NiFiKop Operator Locally via Makefile (Bash)\nDESCRIPTION: Executes the 'make run' command to start the NiFiKop operator locally. This command typically compiles and runs the operator binary, connecting to the Kubernetes cluster specified by the KUBECONFIG environment variable and watching the namespace defined by WATCH_NAMESPACE. It is a convenient way to run the operator for development and testing outside of a containerized deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/6_contributing/1_developer_guide.md#_snippet_5\n\nLANGUAGE: Bash\nCODE:\n```\nmake run\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop using Helm\nDESCRIPTION: Command to install the NiFiKop operator in a Kubernetes cluster using Helm with a custom image tag.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/6_contributing/1_developer_guide.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nhelm install skeleton ./helm/nifikop \\\n    --set image.tag=v0.5.1-release \\\n    --namespace-{\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Override NiFi Properties using ConfigMap, Secret, and Inline YAML\nDESCRIPTION: This YAML snippet illustrates how to override default or templated NiFi configuration properties within a `NiFiCluster` custom resource definition. It demonstrates referencing configurations stored in a Kubernetes ConfigMap (`overrideConfigMap`) or Secret (`overrideSecretConfig`), and defining inline overrides directly in the `overrideConfigs` field, showcasing the priority order (Secret > ConfigMap > Override).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n nifiProperties:\n      # Additionnal nifi.properties configuration that will override the one produced based on template and\n      # configuration\n      overrideConfigMap:\n        # The key of the value,in data content, that we want use.\n        data: nifi.properties\n        # Name of the configmap that we want to refer.\n        name: raw\n        # Namespace where is located the secret that we want to refer.\n        namespace: nifikop\n      # Additionnal nifi.properties configuration that will override the one produced based\n      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n      overrideSecretConfig:\n        # The key of the value,in data content, that we want use.\n        data: nifi.properties\n        # Name of the configmap that we want to refer.\n        name: raw\n        # Namespace where is located the secret that we want to refer.\n        namespace: nifikop\n      # Additionnal nifi.properties configuration that will override the one produced based\n      #\ton template, configurations and overrideConfigMap\n      overrideConfigs: |\n        nifi.ui.banner.text=NiFiKop\n        nifi.sensitive.props.key=thisIsABadSensitiveKeyPassword\n```\n\n----------------------------------------\n\nTITLE: Install NiFiKop Helm Chart with Set Parameters - Bash\nDESCRIPTION: Installs the NiFiKop Helm chart using a specific release name and overrides one or more default configuration parameters directly on the command line using the `--set` flag. Multiple parameters can be set by repeating the flag or separating with commas.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/2_setup/3_install/1_customizable_install_with_helm.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop --set namespaces={\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Applying NifiCluster Configuration Changes (Shell)\nDESCRIPTION: This shell command uses `kubectl apply` to update the NiFi cluster configuration in the specified namespace ('nifi') using the provided YAML file. This triggers the NiFiKop operator to reconcile the cluster state, initiating either a scale-up or scale-down based on the changes in the file.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nkubectl -n nifi apply -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Deleting Nifikop CRDs Manually (Bash)\nDESCRIPTION: Uses `kubectl delete crd` to manually remove the Nifikop Custom Resource Definitions (CRDs) from the cluster. This is typically done after uninstalling the Helm chart if the CRDs were not automatically removed or were installed manually. Warning: This action will delete all custom resources created using these CRDs.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete crd nificlusters.nifi.konpyutaika.com\nkubectl delete crd nifiusers.nifi.konpyutaika.com\nkubectl delete crd nifiusergroups.nifi.konpyutaika.com\nkubectl delete crd nifiregistryclients.nifi.konpyutaika.com\nkubectl delete crd nifiparametercontexts.nifi.konpyutaika.com\nkubectl delete crd nifidataflows.nifi.konpyutaika.com\n```\n\n----------------------------------------\n\nTITLE: Configuring OIDC in NiFiCluster Resource\nDESCRIPTION: Example YAML configuration for setting up OpenId Connect authentication in a NiFiCluster resource using the overrideConfigs field within readOnlyConfig.nifiProperties.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/5_users_authentication/1_oidc.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\\nkind: NifiCluster\\n...\\nspec:\\n  ...\\n  readOnlyConfig:\\n    # NifiProperties configuration that will be applied to the node.\\n    nifiProperties:\\n      webProxyHosts:\\n        - nifistandard2.trycatchlearn.fr:8443\\n      # Additionnal nifi.properties configuration that will override the one produced based\\n      # on template and configurations.\\n      overrideConfigs: |\\n        nifi.security.user.oidc.discovery.url=<oidc server discovery url>\\n        nifi.security.user.oidc.client.id=<oidc client's id>\\n        nifi.security.user.oidc.client.secret=<oidc client's secret>\\n        nifi.security.identity.mapping.pattern.dn=CN=([^,]*)(?:, (?:O|OU)=.*)?\\n        nifi.security.identity.mapping.value.dn=$1\\n        nifi.security.identity.mapping.transform.dn=NONE\\n      ...\\n   ...\\n...\n```\n\n----------------------------------------\n\nTITLE: Initializing Node.js Project and Installing Dependencies (bash)\nDESCRIPTION: Initializes a new Node.js project and installs the required dependencies '@kubernetes/client-node' and 'minimist' with specific versions needed for the migration script. This setup is prerequisite for running the migration script. Execute this in your project root to enable JavaScript access to Kubernetes resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm init -y\nnpm install @kubernetes/client-node@0.16.3 minimist@1.2.6\n```\n\n----------------------------------------\n\nTITLE: Defining KEDA ScaledObject for NiFi (YAML)\nDESCRIPTION: This YAML manifest defines a KEDA `ScaledObject` custom resource to enable autoscaling for the `NifiNodeGroupAutoscaler`. It targets the specific autoscaler instance (`nifinodegroupautoscaler-sample`) and configures a Prometheus trigger, specifying the Prometheus service address, the metric name, a query, and a threshold. The `minReplicaCount` and `maxReplicaCount` define the scaling boundaries.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: cluster\n  namespace: clusters\nspec:\n  scaleTargetRef:\n    apiVersion:    nifi.konpyutaika.com/v1alpha1     # Optional. Default: apps/v1\n    kind:          NifiNodeGroupAutoscaler           # Optional. Default: Deployment\n    name:          nifinodegroupautoscaler-sample    # Mandatory. Must be in the same namespace as the ScaledObject\n    envSourceContainerName: nifi                     # Optional. Default: .spec.template.spec.containers[0]\n  pollingInterval:  30                               # Optional. Default: 30 seconds\n  cooldownPeriod:   300                              # Optional. Default: 300 seconds\n  idleReplicaCount: 0                                # Optional. Default: ignored, must be less than minReplicaCount \n  minReplicaCount:  1                                # Optional. Default: 0\n  maxReplicaCount:  3                                # Optional. Default: 100\n  fallback:                                          # Optional. Section to specify fallback options\n    failureThreshold: 5                              # Mandatory if fallback section is included\n    replicas: 1                                      # Mandatory if fallback section is included\n  # advanced:                                          # Optional. Section to specify advanced options\n  #   restoreToOriginalReplicaCount: true              # Optional. Whether the target resource should be scaled back to original replicas count, after the ScaledObject is deleted\n  #   horizontalPodAutoscalerConfig:                   # Optional. Section to specify HPA related options\n  #     name: {name-of-hpa-resource}                   # Optional. Default: keda-hpa-{scaled-object-name}\n  #     behavior:                                      # Optional. Use to modify HPA's scaling behavior\n  #       scaleDown:\n  #         stabilizationWindowSeconds: 300 <--- \n  #         policies:\n  #         - type: Percent\n  #           value: 100\n  #           periodSeconds: 15\n  triggers:\n    - type: prometheus\n      metadata:\n        serverAddress: http://prometheus-operated.monitoring-system.svc:9090\n        metricName: http_requests_total\n        threshold: <threshold value>\n        query: <prometheus query>\n```\n\n----------------------------------------\n\nTITLE: Specifying Service Configuration for HTTPS Traffic in YAML\nDESCRIPTION: This is an extract from a cluster Deployment YAML, and it shows how to configure the NiFi service to handle HTTPS traffic. The `externalServices` section defines a `ClusterIP` service named `nifi-cluster` with a port configuration for 8443, which is the port used for HTTPS traffic. This configuration is necessary for the `VirtualService` to redirect traffic appropriately.  This requires the service name to be consistent with the destination in the VirtualService. The expected output is the availability of the NiFi cluster through HTTPS.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\nspec:  \n  externalServices:  \n    - name: \"nifi-cluster\"\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8443\n            internalListenerName: \"https\"\n```\n```\n\n----------------------------------------\n\nTITLE: Installing nifikop Helm Chart with Specific Namespace - Bash\nDESCRIPTION: Installs the nifikop chart and explicitly sets the namespaces argument to 'nifikop'. The command attaches no extra parameters except for namespace selection. Helm must be installed and initialized with konpyutaika/nifikop.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop --set namespaces={\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Dry Run Install Nifikop Chart\nDESCRIPTION: This snippet shows a dry-run installation of the Nifikop chart using Helm. It tests the installation without making changes to the cluster. It accepts parameters for log level and namespace. It requires Helm and a valid Helm repository.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop \\\n    --dry-run \\\n    --set logLevel=Debug \\\n    --set namespaces={\\\"nifikop\\\"}\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop Helm Chart using Dry Run in Bash\nDESCRIPTION: Demonstrates a dry-run installation of the NiFiKop Helm chart using the Helm CLI in a Bash environment. The example sets the log level to Debug and restricts the operator's watch namespaces to \"nifikop\". This command simulates deployment without applying changes, useful for validation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/2_setup/3_install/1_customizable_install_with_helm.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop \\\n    --dry-run \\\n    --set logLevel=Debug \\\n    --set namespaces={\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Cluster to Use External DNS and Let’s Encrypt Issuer with TLS Secrets - YAML\nDESCRIPTION: Demonstrates how to configure a NifiCluster resource to secure communication using TLS certificates issued by an external ACME issuer like Let's Encrypt using external DNS. Key fields include enabling cluster and site-to-site security, setting clusterDomain and useExternalDNS flags, and specifying sslSecrets with the issuerRef referencing an existing cert-manager Issuer resource. This setup automates the TLS management lifecycle via the NiFi operator. The snippet depends on cert-manager and external DNS being configured in the Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  clusterSecure: true\n  siteToSiteSecure: true\n  ...\n  listenersConfig:\n    clusterDomain: <DNS zone name>\n    useExternalDNS: true\n    ...\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n      issuerRef:\n        name: letsencrypt-staging\n        kind: Issuer\n```\n\n----------------------------------------\n\nTITLE: Setting operator name environment variable\nDESCRIPTION: Command to set the operator name environment variable for local execution of the NiFiKop operator.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/6_contributing/1_developer_guide.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport OPERATOR_NAME=nifi-operator\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop Operator Helm Chart with Specified Image Tag and Namespace - Bash\nDESCRIPTION: This helm command installs the NiFiKop operator Helm chart named 'skeleton' into the 'nifikop' namespace. The 'image.tag' is set explicitly to match the pushed Docker image tag to ensure the deployed operator container is consistent with the build. Proper matching is critical to avoid deployment mismatches.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/6_contributing/1_developer_guide.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nhelm install skeleton ./helm/nifikop \\\n    --set image.tag=v0.5.1-release \\\n    --namespace-{\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Adding a New Node to NifiCluster Spec - YAML\nDESCRIPTION: This snippet demonstrates how to add a new node to the `NifiCluster.Spec.Nodes` field in the NifiCluster YAML configuration. The new node is configured with the 'default_group'. The Node.Id field must be unique within the NifiCluster.Spec.Nodes list.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n  clusterManager: zookeeper\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.12.1\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n    - id: 2\n      nodeConfigGroup: \"default_group\"\n# >>>> START: The new node\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n# <<<< END\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: Deleting Helm Release\nDESCRIPTION: Delete a specific Helm release. This command removes all Kubernetes components associated with the specified release ('nifikop') and marks the release as deleted. Note that CRDs installed by the chart are not removed by default.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nhelm del nifikop\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Secret for NiFi Sensitive Parameters\nDESCRIPTION: This snippet shows the kubectl command to create a Kubernetes secret containing sensitive parameters that will be used by the NifiParameterContext for sensitive NiFi parameters.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl create secret generic secret-params \\\n    --from-literal=secret1=yop \\\n    --from-literal=secret2=yep \\\n    -n nifikop\n```\n\n----------------------------------------\n\nTITLE: Creating NiFi User Client Credentials (Console)\nDESCRIPTION: This console command uses `kubectl apply` with an inline YAML definition to create a NifiUser custom resource. This instructs the NiFi operator to generate client SSL credentials signed by the cluster's CA and store them in a specified Kubernetes Secret (`example-client-secret`).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_3\n\nLANGUAGE: console\nCODE:\n```\ncat << EOF | kubectl apply -n nifi -f -\napiVersion:  nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: example-client\n  namespace: nifi\nspec:\n  clusterRef:\n    name: nifi\n  secretName: example-client-secret\nEOF\n```\n\n----------------------------------------\n\nTITLE: Creating Secret for Sensitive Parameters in NiFi\nDESCRIPTION: This snippet shows how to create a Kubernetes secret that contains sensitive parameters to be used in a NifiParameterContext. The secret values will be converted into sensitive parameters in NiFi.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create secret generic secret-params \\\n    --from-literal=secret1=yop \\\n    --from-literal=secret2=yep \\\n    -n nifikop\n```\n\n----------------------------------------\n\nTITLE: Get Kubernetes resources for new node\nDESCRIPTION: This command retrieves the pods, configmaps, and PVCs associated with the newly added NiFi node (nodeId=25).  It verifies that the resources have been successfully created in Kubernetes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl get pods,configmap,pvc -l nodeId=25\nNAME                          READY   STATUS    RESTARTS   AGE\npod/simplenifi-25-nodem5jh4   1/1     Running   0          11m\n\nNAME                             DATA   AGE\nconfigmap/simplenifi-config-25   7      11m\n\nNAME                                               STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\npersistentvolumeclaim/simplenifi-25-storagehwn24   Bound    pvc-7da86076-728e-11ea-846d-42010a8400f2   10Gi       RWO            standard       11m\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiUser resource in YAML\nDESCRIPTION: This YAML snippet defines a NifiUser resource named 'aguitton'. It specifies the user's identity, the cluster to which it belongs, and access policies. The 'createCert' and 'includeJKS' fields are set to false, indicating that no certificate or JKS file should be generated. Access policies define what actions the user can perform on specific resources within NiFi.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/2_manage_users_and_accesses/1_users_management.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: aguitton\nspec:\n  # identity field is use to define the user identity on NiFi cluster side,\n  #\tit use full when the user's name doesn't suite with Kubernetes resource name.\n  identity: alexandre.guitton@konpyutaika.com\n  # Contains the reference to the NifiCluster with the one the registry client is linked.\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  # Whether or not the the operator also include a Java keystore format (JKS) with you secret\n  includeJKS: false\n  # Whether or not a certificate will be created for this user.\n  createCert: false\n  # defines the list of access policies that will be granted to the group.\n  accessPolicies:\n      # defines the kind of access policy, could be \"global\" or \"component\".\n    - type: component\n      # defines the kind of action that will be granted, could be \"read\" or \"write\"\n      action: read\n      # resource defines the kind of resource targeted by this access policies, please refer to the following page:\n      #\thttps://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#access-policies\n      resource: /data\n      # componentType is used if the type is \"component\", it's allow to define the kind of component on which is the\n      # access policy\n      componentType: \"process-groups\"\n      # componentId is used if the type is \"component\", it's allow to define the id of the component on which is the\n      # access policy\n      componentId: \"\"\n```\n\n----------------------------------------\n\nTITLE: Defining Managed Users in NiFiCluster Spec YAML\nDESCRIPTION: This YAML snippet shows how to configure the `NifiCluster` resource to leverage NiFiKop's managed groups. By listing user identities and names under `managedAdminUsers` and `managedReaderUsers`, the operator automatically creates and manages `NifiUser` resources and assigns them to the built-in 'managed-admins' and 'managed-readers' groups respectively, simplifying access control setup.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/2_manage_users_and_accesses/3_managed_groups.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  ...\n  oneNifiNodePerNode: false\n  propagateLabels: true\n  managedAdminUsers:\n    -  identity: \"alexandre.guitton@konpyutaika.com\"\n       name: \"aguitton\"\n    -  identity: \"nifiuser@konpyutaika.com\"\n       name: \"nifiuser\"\n  managedReaderUsers:\n    -  identity: \"toto@konpyutaika.com\"\n       name: \"toto\"\n    ...\n```\n\n----------------------------------------\n\nTITLE: Verifying HPA Deployment for NiFi Autoscaling\nDESCRIPTION: Command to check the status of the Horizontal Pod Autoscaler (HPA) created by KEDA to manage the NiFi node group scaling.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_14\n\nLANGUAGE: console\nCODE:\n```\nkubectl get -n clusters hpa\nNAME                REFERENCE                                                TARGETS         MINPODS   MAXPODS   REPLICAS   AGE\nkeda-hpa-cluster    NifiNodeGroupAutoscaler/nifinodegroupautoscaler-sample     1833m/2 (avg)   1         3         2          25d\n```\n\n----------------------------------------\n\nTITLE: Installing KEDA Helm Chart\nDESCRIPTION: This snippet installs the KEDA Helm chart into the `keda` namespace. It creates the namespace if it doesn't exist.  The command installs KEDA using its Helm chart, making it available for use in the Kubernetes cluster. The chart is specified as `kedacore/keda`. KEDA will then run in the specified namespace, and it is a prerequisite to using event-driven autoscaling.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl create namespace keda\nhelm install keda kedacore/keda --namespace keda\n```\n\n----------------------------------------\n\nTITLE: ZookeeperProperties overrideConfigs Example\nDESCRIPTION: This YAML snippet shows an example of using `overrideConfigs` within the `zookeeperProperties` section of `ReadOnlyConfig` to override zookeeper properties like `initLimit`, `syncLimit` and `dataDir`. These settings will override any default configurations or those configured via ConfigMaps and Secrets.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/5_references/1_nifi_cluster/2_read_only_config.md#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\n    overrideConfigs: |\n      initLimit=15\n      autopurge.purgeInterval=24\n      syncLimit=5\n      tickTime=2000\n      dataDir=./state/zookeeper\n      autopurge.snapRetainCount=30\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Node Configuration\nDESCRIPTION: This YAML snippet defines the configuration for a NiFi node, including its ID, node configuration group, and read-only configurations. It shows how to override NiFi properties such as the banner text. The `nodeConfigGroup` simplifies node setup by referencing pre-defined configurations.  `readOnlyConfig` allows for read-only changes which trigger a rolling upgrade. `nodeConfig` specifies resource limits and storage configurations.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/5_references/1_nifi_cluster/4_node.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n    - id: 0\n      # nodeConfigGroup can be used to ease the node configuration, if set only the id is required\n      nodeConfigGroup: \"default_group\"\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        nifiProperties:\n          overrideConfigs: |\n            nifi.ui.banner.text=NiFiKop - Node 0\n      # node configuration\n#       nodeConfig:\n    - id: 2\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        overrideConfigs: |\n          nifi.ui.banner.text=NiFiKop - Node 2\n      # node configuration\n      nodeConfig:\n        resourcesRequirements:\n          limits:\n            cpu: \"2\"\n            memory: 3Gi\n          requests:\n            cpu: \"1\"\n            memory: 1Gi\n        storageConfigs:\n          # Name of the storage config, used to name PV to reuse into sidecars for example.\n          - name: provenance-repository\n            # Path where the volume will be mount into the main nifi container inside the pod.\n            mountPath: \"/opt/nifi/provenance_repository\"\n            # Kubernetes PVC spec\n            # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n            pvcSpec:\n              accessModes:\n                - ReadWriteOnce\n              storageClassName: \"standard\"\n              resources:\n                requests:\n                  storage: 8Gi\n```\n\n----------------------------------------\n\nTITLE: Adding Helm Repository for Prometheus Charts\nDESCRIPTION: Adds the Helm chart repository from 'prometheus-community' to Helm to fetch Prometheus operator charts. Use Helm CLI commands to manage Helm repositories.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nhelm repo add prometheus https://prometheus-community.github.io/helm-charts\n```\n\n----------------------------------------\n\nTITLE: Configure CRD conversion webhook\nDESCRIPTION: This snippet shows how to configure the conversion webhook within the CRD YAML definition. It defines annotations, a conversion strategy using a webhook, the service details (namespace, name, path), and supported conversion review versions. This configuration is necessary to handle the conversion of resources from `v1alpha1` to `v1` within the CRD.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/2_deploy_nifikop/1_quick_start.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n... \nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n```\n\n----------------------------------------\n\nTITLE: NifiCluster Configuration with Busybox Init Container (Pre-v0.15.0)\nDESCRIPTION: Example YAML configuration for a NifiCluster custom resource using `busybox` as the `initContainerImage`. This configuration was valid for Nifikop v0.14.1 but requires modification for v0.15.0 due to changes in the default init container requirements.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/7_upgrade_guides/2_v0.14.1_to_v0.15.0.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  initContainerImage:\n    repository: busybox\n    tag: \"1.34.0\"\n```\n\n----------------------------------------\n\nTITLE: Deploying CRDs with Kubectl\nDESCRIPTION: This bash command sequence deploys custom resource definitions (CRDs) to a Kubernetes cluster using `kubectl`.  It applies YAML configuration files located in `config/crd/bases`.  Requires `kubectl` to be correctly configured and connected to a Kubernetes cluster, and the necessary CRD YAML files in the specified locations.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/6_contributing/1_developer_guide.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\n```\n\n----------------------------------------\n\nTITLE: Retrieve Client Credentials from Secret\nDESCRIPTION: These console commands retrieve the client certificate, key and CA certificate from the Kubernetes secret created by the `NifiUser` resource. They use `kubectl get secret` and `base64 -d` to decode and save the credentials to local files.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['ca\\.crt']}\" | base64 -d > ca.crt\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['tls\\.crt']}\" | base64 -d > tls.crt\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['tls\\.key']}\" | base64 -d > tls.key\n```\n\n----------------------------------------\n\nTITLE: Retrieving Certificates from Secret (Console)\nDESCRIPTION: This set of console commands retrieves and decodes the base64 encoded data from the Kubernetes secret named `example-client-secret`, which stores the CA certificate, user certificate and user private key. The output is then written to local files: `ca.crt`, `tls.crt`, and `tls.key` respectively, allowing access to the certificates and keys.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['ca\\.crt']}\" | base64 -d > ca.crt\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['tls\\.crt']}\" | base64 -d > tls.crt\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['tls\\.key']}\" | base64 -d > tls.key\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio VirtualService for HTTPS in NiFi\nDESCRIPTION: This YAML configuration defines an Istio VirtualService that routes HTTPS traffic to a specific ClusterIP Service targeting port 8443 in the NiFi cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: nifi-vs\nspec:\n  gateways:\n  - nifi-gateway\n  hosts:\n  - nifi.my-domain.com\n  http:\n  - match:\n    - uri:\n        prefix: /\n    route:\n    - destination:\n        host: <service-name>.<namespace>.svc.cluster.local\n        port:\n          number: 8443\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio Gateway for HTTPS Access to NiFi\nDESCRIPTION: Defines an Istio Gateway resource that handles HTTPS traffic on port 443 for a specific domain host. It uses TLS in SIMPLE mode with a credential secret to decrypt the traffic before passing it to the VirtualService.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: nifi-gateway\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 443\n      name: https\n      protocol: HTTPS\n    tls:\n      mode: SIMPLE\n      credentialName: my-secret\n    hosts:\n    - nifi.my-domain.com\n```\n\n----------------------------------------\n\nTITLE: Defining Istio Gateway for HTTP\nDESCRIPTION: This snippet defines a `Gateway` resource for Istio, configured to intercept HTTP traffic on port 80 for the domain `nifi.my-domain.com`. This gateway acts as the entry point for external HTTP requests to the NiFi cluster.  It requires Istio to be installed and configured in the Kubernetes cluster. The `selector` field targets the Istio ingress gateway, while the `hosts` field specifies the domain name.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n```yaml\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: nifi-gateway\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - nifi.my-domain.com\n```\n```\n\n----------------------------------------\n\nTITLE: Listing Deleted Helm Releases\nDESCRIPTION: Shows the `helm list --deleted` command which displays Helm releases that have been previously deleted but whose records are retained by Helm, useful for history tracking or rollback.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --deleted\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenId Connect in nifi.properties - Shell\nDESCRIPTION: This shell snippet shows the configuration settings to be added to the `nifi.properties` file for OIDC authentication. These properties define patterns and transforms for mapping identities.  It requires the NiFi instance to be running with appropriate security settings and the OIDC server details to be configured, as shown in the following YAML example. The output modifies the NiFi cluster's configuration, enabling OIDC.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/5_users_authentication/1_oidc.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nnifi.security.identity.mapping.pattern.dn=CN=([^,]*)(?:, (?:O|OU)=.*)?\nnifi.security.identity.mapping.value.dn=$1\nnifi.security.identity.mapping.transform.dn=NONE\n```\n\n----------------------------------------\n\nTITLE: Templating authorizers.xml for Custom Authorizer - YAML\nDESCRIPTION: This YAML template demonstrates how to replace the default authorizers.xml file with a custom configuration. It includes definitions for both file-based and database-based UserGroupProviders and AccessPolicyProviders, along with authorizers. The template uses Go templating to dynamically configure node identities based on the NiFiKOp deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/6_users_authorization/1_custom_user_authorizer.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n{{- $nodeList := .NodeList }}\n{{- $clusterName := .ClusterName }}\n{{- $namespace := .Namespace }}<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>\n<authorizers>\n    <userGroupProvider>\n        <identifier>file-user-group-provider</identifier>\n        <class>org.apache.nifi.authorization.FileUserGroupProvider</class>\n        <property name=\"Users File\">../data/users.xml</property>\n        <property name=\"Legacy Authorized Users File\"></property>\n        <property name=\"Initial User Identity admin\">{{ .ControllerUser }}</property>\n{{- range $i, $host := .NodeList }}\n        <property name=\"Initial User Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n    </userGroupProvider>\n    <userGroupProvider>\n        <identifier>database-user-group-provider</identifier>\n        <class>my.custom.DatabaseUserGroupProvider</class>\n        <!-- Any extra configuration for this provider goes here -->\n{{- range $i, $host := .NodeList }}\n        <property name=\"Initial User Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n    </userGroupProvider>\n    <accessPolicyProvider>\n        <identifier>file-access-policy-provider</identifier>\n        <class>org.apache.nifi.authorization.FileAccessPolicyProvider</class>\n        <property name=\"User Group Provider\">file-user-group-provider</property>\n        <property name=\"Authorizations File\">../data/authorizations.xml</property>\n        <property name=\"Initial Admin Identity\">{{ .ControllerUser }}</property>\n        <property name=\"Legacy Authorized Users File\"></property>\n{{- range $i, $host := .NodeList }}\n        <property name=\"Node Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n\t\t<property name=\"Node Group\"></property>\n    </accessPolicyProvider>\n    <accessPolicyProvider>\n        <identifier>database-access-policy-provider</identifier>\n        <class>my.custom.DatabaseAccessPolicyProvider</class>\n        <!-- Any extra configuration for this provider goes here -->\n{{- range $i, $host := .NodeList }}\n        <property name=\"Node Identity {{ $i }}\">{{ $host }}</property>\n{{- end }}\n\t\t<property name=\"Node Group\"></property>\n    </accessPolicyProvider>\n    <authorizer>\n        <identifier>managed-authorizer</identifier>\n        <class>org.apache.nifi.authorization.StandardManagedAuthorizer</class>\n        <property name=\"Access Policy Provider\">file-access-policy-provider</property>\n    </authorizer>\n    <authorizer>\n        <identifier>custom-database-authorizer</identifier>\n        <class>my.custom.DatabaseAuthorizer</class>\n        <property name=\"Access Policy Provider\">database-access-policy-provider</property>\n    </authorizer>\n</authorizers>\n\n```\n\n----------------------------------------\n\nTITLE: Defining Istio DestinationRule for HTTPS\nDESCRIPTION: This YAML defines a `DestinationRule` that directs HTTP traffic to the service and configures a consistent hash-based load balancing for sticky sessions based on the `__Secure-Authorization-Bearer` cookie, ensuring that requests from the same client go to the same NiFi instance, critical for stateful applications and user authentication.  It operates after the traffic has been decrypted and re-encrypts the traffic before reaching the NiFi service.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_4\n\nLANGUAGE: YAML\nCODE:\n```\n```yaml\napiVersion: networking.istio.io/v1beta1\nkind: DestinationRule\nmetadata:\n  name: nifi-dr\nspec:\n  host: <service-name>.<namespace>.svc.cluster.local\n  trafficPolicy:\n    tls:\n      mode: SIMPLE\n    loadBalancer:\n      consistentHash:\n        httpCookie:\n          name: __Secure-Authorization-Bearer\n          ttl: 0s\n```\n```\n\n----------------------------------------\n\nTITLE: Extract user credentials from Kubernetes secret\nDESCRIPTION: These console commands extract the CA certificate, TLS certificate, and TLS key from the Kubernetes secret named `example-client-secret`. The commands use `kubectl` with `jsonpath` to retrieve the base64-encoded values and then decode them using `base64 -d` before writing them to local files.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_5\n\nLANGUAGE: console\nCODE:\n```\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['ca\\.crt']}\" | base64 -d > ca.crt\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['tls\\.crt']}\" | base64 -d > tls.crt\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['tls\\.key']}\" | base64 -d > tls.key\n```\n\n----------------------------------------\n\nTITLE: Installing Zookeeper using Helm in Kubernetes\nDESCRIPTION: Helm command to install Bitnami's Zookeeper chart with specific resource limits, storage class, network policy, and replica count. Zookeeper is a prerequisite for NiFi.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhelm install zookeeper oci://registry-1.docker.io/bitnamicharts/zookeeper \\\n    --namespace=zookeeper \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set global.storageClass=standard \\\n    --set networkPolicy.enabled=true \\\n    --set replicaCount=3 \\\n    --create-namespace\n```\n\n----------------------------------------\n\nTITLE: Extracting Secrets using kubectl (Bash)\nDESCRIPTION: These bash commands extract the CA certificate, user certificate, and user private key from a Kubernetes secret named `example-client-secret`. It uses `kubectl` to retrieve the secret data and `base64` to decode the encoded values, writing each to a respective file.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['ca\\.crt']}\" | base64 -d > ca.crt\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['tls\\.crt']}\" | base64 -d > tls.crt\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['tls\\.key']}\" | base64 -d > tls.key\n```\n\n----------------------------------------\n\nTITLE: Creating a GKE Cluster - Shell\nDESCRIPTION: This shell script creates a new GKE cluster.  It uses the `gcloud container clusters create` command, specifying the cluster name, Kubernetes version, machine type, the number of nodes, the zone, and the GCP project.  The user needs `gcloud` and the appropriate Google Cloud permissions.  The output is the newly created cluster, which is ready for further configuration.  The cluster version is set to latest, and the machine type is n1-standard-1.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/2_setup/2_platform_setup/1_gke.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ngcloud container clusters create $CLUSTER_NAME \\\n  --cluster-version latest \\\n  --machine-type=n1-standard-1 \\\n  --num-nodes 4 \\\n  --zone $GCP_ZONE \\\n  --project $GCP_PROJECT\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFiKop CRD Conversion Webhook with cert-manager (YAML)\nDESCRIPTION: Example YAML snippet showing annotations and spec configuration for NiFiKop CRDs to enable the conversion webhook. It uses cert-manager for CA injection and defines the webhook service details and supported conversion versions (v1, v1alpha1), essential for handling resource version upgrades.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/2_deploy_nifikop/1_quick_start.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n```\n\n----------------------------------------\n\nTITLE: Adding Bitnami Helm Repository in Bash\nDESCRIPTION: This command adds the Bitnami Helm chart repository to your Helm configuration. This is a prerequisite step to installing Zookeeper using the Bitnami chart. It adds the Helm repository for the Bitnami charts, which includes the Zookeeper chart, using its URL. Before this is run you need helm installed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhelm repo add bitnami https://charts.bitnami.com/bitnami\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio VirtualService for HTTP (YAML)\nDESCRIPTION: This YAML snippet configures an Istio VirtualService to redirect HTTP traffic intercepted by the gateway. The VirtualService targets traffic for `nifi.my-domain.com`, and routes all requests (prefix `/`) to the `nifi` service on port 8080.  This ensures that traffic reaching the gateway is directed to the NiFi service within the cluster.  It requires the Istio Gateway to be properly configured and deployed, as well as the NiFi service. Key parameters are `gateways`, `hosts`, `http`, and the destination `host` and `port`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: nifi-vs\nspec:\n  gateways:\n  - nifi-gateway\n  hosts:\n  - nifi.my-domain.com\n  http:\n  - match:\n    - uri:\n        prefix: /\n    route:\n    - destination:\n        host: nifi\n        port:\n          number: 8080\n```\n```\n\n----------------------------------------\n\nTITLE: Verifying NiFiKop Operator Pod Status In Kubernetes Namespace (Bash)\nDESCRIPTION: This snippet uses 'kubectl get pods' to list the pods in the 'nifikop' namespace, confirming that the NiFiKop operator pod is up and running. Output shows pod name, readiness, status, restarts, and age, which helps verify successful deployment via Helm or other methods.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/6_contributing/1_developer_guide.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get pods -n nifikop\n# Expected output example:\n# NAME                                                READY   STATUS    RESTARTS   AGE\n# skeleton-nifikop-8946b89dc-4cfs9   1/1     Running   0          7m45s\n```\n\n----------------------------------------\n\nTITLE: NifiUserGroup Resource Definition\nDESCRIPTION: This YAML defines a NifiUserGroup resource named 'group-test'. It specifies the associated NiFi cluster, references to the NifiUsers that are part of the group and defines access policies for the group to have read access to the /counters resource at the global level.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/3_tasks/4_nifi_user_group.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  # Contains the reference to the NifiCluster with the one the registry client is linked.\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  # contains the list of reference to NifiUsers that are part to the group.\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n#     namespace: nifikop\n    - name: nc-controller.nifikop.mgt.cluster.local\n  # defines the list of access policies that will be granted to the group.\n  accessPolicies:\n      # defines the kind of access policy, could be \"global\" or \"component\".\n    - type: global\n      # defines the kind of action that will be granted, could be \"read\" or \"write\"\n      action: read\n      # resource defines the kind of resource targeted by this access policies, please refer to the following page:\n      #\thttps://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#access-policies\n      resource: /counters\n#      # componentType is used if the type is \"component\", it's allow to define the kind of component on which is the\n#      # access policy\n#      componentType: \"process-groups\"\n#      # componentId is used if the type is \"component\", it's allow to define the id of the component on which is the\n#      # access policy\n#      componentId: \"\"\n```\n\n----------------------------------------\n\nTITLE: Creating a Secret for Basic Authentication using kubectl\nDESCRIPTION: This `kubectl` command creates a generic secret named `nifikop-credentials` in the `nifikop-nifi` namespace. The secret contains the username, password, and optional CA certificate required for basic authentication to the external NiFi cluster.  The username, password and ca.crt values are read from the specified files.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/3_external_cluster.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create secret generic nifikop-credentials \\\n  --from-file=username=./secrets/username\\\n  --from-file=password=./secrets/password\\\n  --from-file=ca.crt=./secrets/ca.crt\\\n  -n nifikop-nifi\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi ClusterIP Service\nDESCRIPTION: This YAML snippet defines the ClusterIP service configuration used for internal routing within the Kubernetes cluster. It defines a service of type ClusterIP listening on port 8443 and maps it to the `https` internal listener name within NiFi. This is referenced by the VirtualService.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nspec:  \n  externalServices:  \n    - name: \"nifi-cluster\"\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8443\n            internalListenerName: \"https\"\n```\n\n----------------------------------------\n\nTITLE: Defining an Inherited NiFi Parameter Context with Kubernetes YAML\nDESCRIPTION: This YAML snippet demonstrates defining a NiFi parameter context that inherits parameters from another context, enabling hierarchical configuration. It uses the NifiParameterContext kind, references a parent context, defines its own parameters, and specifies secret references for sensitive data. Required elements include the nifikop operator, a correctly configured parent parameter context, and valid cluster and secret references. The manifest should be applied to a Kubernetes cluster; upon success, the operator will create a new parameter context in NiFi that extends the specified parent.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/5_references/4_nifi_parameter_context.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle-child\nspec:\n  description: \"It is a child test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  inheritedParameterContexts:\n    - name: dataflow-lifecycle\n  parameters:\n    - name: test\n      value: toto-child\n      description: tutu (child)\n```\n\n----------------------------------------\n\nTITLE: Creating a Cert-Manager Issuer Resource for Let's Encrypt - Kubernetes YAML\nDESCRIPTION: Presents a cert-manager Issuer resource sample that uses ACME protocol to request SSL certificates from Let's Encrypt staging environment. The issuer is designed for use in clusters where external-dns manages DNS records, and specifies challenge solvers via HTTP01 using ingress with custom annotations. Dependencies include cert-manager installed in the cluster and proper DNS configuration. Replace the email address and secret name as needed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: cert-manager.io/v1alpha2\nkind: Issuer\nmetadata:\n  name: letsencrypt-staging\nspec:\n  acme:\n    # You must replace this email address with your own.\n    # Let's Encrypt will use this to contact you about expiring\n    # certificates, and issues related to your account.\n    email: <your email address>\n    server: https://acme-staging-v02.api.letsencrypt.org/directory\n    privateKeySecretRef:\n      # Secret resource used to store the account's private key.\n      name: example-issuer-account-key\n    # Add a single challenge solver, HTTP01 using nginx\n    solvers:\n      - http01:\n          ingress:\n            ingressTemplate:\n              metadata:\n                annotations:\n                  \"external-dns.alpha.kubernetes.io/ttl\": \"5\"\n\n```\n\n----------------------------------------\n\nTITLE: Deploying NifiDataflow CRD Referencing Registry Client and Parameter Context in YAML\nDESCRIPTION: Defines a NifiDataflow custom resource that deploys a NiFi dataflow version to the cluster. It references the parent process group, bucket, flow IDs, and flow version in the NiFi Registry. This CRD includes synchronization and update strategy options controlling the lifecycle management of the dataflow, as well as references to the NifiRegistryClient and NifiParameterContext for integrating flow versioning and parameters.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/3_tasks/3_nifi_dataflow.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: demo\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Manually Deploying CRDs (Skip CRDs)\nDESCRIPTION: This snippet demonstrates how to manually deploy Custom Resource Definitions (CRDs) when the helm chart is configured to skip their installation.  It's crucial for cases where the CRDs are already present or when you want control over their lifecycle. It requires `kubectl` and access to the Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifinodegroupautoscalers.yaml\n```\n\n----------------------------------------\n\nTITLE: Assigning Node Configurations in NiFiKop YAML\nDESCRIPTION: This YAML snippet demonstrates assigning configurations to individual NiFi nodes within a `NifiCluster` specification's `nodes` array. Nodes 0, 2, and 3 reference predefined `NodeConfigGroups` (`default_group` and `high_mem_group`) using the `nodeConfigGroup` field. Node 5 showcases defining resource requirements (`resourcesRequirements`) directly inline using `nodeConfig`, overriding or bypassing group definitions for specific node needs.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n- id: 0\n  nodeConfigGroup: \"default_group\"\n- id: 2\n  nodeConfigGroup: \"high_mem_group\"\n- id: 3\n  nodeConfigGroup: \"high_mem_group\"\n- id: 5\n  nodeConfig:\n    resourcesRequirements:\n      limits:\n        cpu: \"2\"\n        memory: 3Gi\n      requests:\n        cpu: \"1\"\n        memory: 1Gi\n```\n\n----------------------------------------\n\nTITLE: Manually Deleting Nifikop CRDs - Bash\nDESCRIPTION: This command sequence deletes specific Nifikop CustomResourceDefinitions (CRDs) from Kubernetes. Cluster admin access is required. Deleting CRDs permanently removes all instances of the CR type in the cluster; proceed with caution. The output is permanent removal of both the definition and any custom resources associated.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete crd nificlusters.nifi.konpyutaika.com\nkubectl delete crd nifiusers.nifi.konpyutaika.com\nkubectl delete crd nifiusergroups.nifi.konpyutaika.com\nkubectl delete crd nifiregistryclients.nifi.konpyutaika.com\nkubectl delete crd nifiparametercontexts.nifi.konpyutaika.com\nkubectl delete crd nifidataflows.nifi.konpyutaika.com\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for GKE Setup (Shell)\nDESCRIPTION: This snippet defines several environment variables necessary for the GKE cluster setup. It sets the `GCP_PROJECT`, `GCP_ZONE`, and `CLUSTER_NAME` variables, which are used in subsequent commands.  The user must replace the placeholder values (`<project_id>`, `<zone>`, and `<cluster-name>`) with their actual project ID, zone, and desired cluster name. These variables are essential for configuring the GKE cluster within a specified project and zone.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/2_setup/2_platform_setup/1_gke.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nexport GCP_PROJECT=<project_id>\nexport GCP_ZONE=<zone>\nexport CLUSTER_NAME=<cluster-name>\n```\n\n----------------------------------------\n\nTITLE: Manually Applying NiFiKop CRDs with Kubectl\nDESCRIPTION: Applies the Custom Resource Definitions (CRDs) required by the NiFiKop operator directly using kubectl. This is necessary if you choose to skip CRD installation when deploying the operator Helm chart.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/2_setup/1_getting_started.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining Nifi NodeConfig in YAML for Kubernetes Deployment\nDESCRIPTION: This YAML snippet specifies the configuration schema for a Nifi node within a Kubernetes cluster, including default settings like provenance storage, user ID, node role, container image, affinity, storage bindings, and pod metadata. It delineates how to set up node properties, storage, external volumes, and metadata annotations for deployment, aimed at orchestrating scalable, cluster-aware Nifi nodes. Dependencies include Kubernetes API objects and specific volume/pvc specifications.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/5_references/1_nifi_cluster/3_node_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n   default_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      # Set this to true if the instance is a node in a cluster.\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#basic-cluster-setup\n      isNode: true\n      # Additionnal metadata to merge to the pod associated\n      podMetadata:\n        annotations:\n          node-annotation: \"node-annotation-value\"\n        labels:\n          node-label: \"node-label-value\"\n      # Docker image used by the operator to create the node associated\n      # https://hub.docker.com/r/apache/nifi/\n#      image: \"apache/nifi:1.11.2\"\n      # nodeAffinity can be specified, operator populates this value if new pvc added later to node\n      # https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity\n#      nodeAffinity:\n      # imagePullPolicy define the pull policy for NiFi cluster docker image\n      imagePullPolicy: IfNotPresent\n      # priorityClassName define the name of the priority class to be applied to these nodes\n      priorityClassName: \"example-priority-class-name\"\n      # externalVolumeConfigs specifies a list of volume to mount into the main container.\n      externalVolumeConfigs:\n        - name: example-volume\n          mountPath: \"/opt/nifi/example\"\n          secret:\n            secretName: \"raw-controller\"\n      # storageConfigs specifies the node related configs\n      storageConfigs:\n        # Name of the storage config, used to name PV to reuse into sidecars for example.\n        - name: provenance-repository\n          # Path where the volume will be mount into the main nifi container inside the pod.\n          mountPath: \"/opt/nifi/provenance_repository\"\n          # Kubernetes PVC spec\n          # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n```\n\n----------------------------------------\n\nTITLE: Declaring an External NiFi Cluster - NifiKop YAML\nDESCRIPTION: This YAML snippet defines a NifiCluster resource for referencing an external NiFi cluster within Kubernetes. Required dependencies include the NifiKop operator and an existing Kubernetes cluster. Key parameters are 'rootProcessGroupId' (UUID for the root process group), 'nodeURITemplate' (hostname template for cluster nodes), 'nodes' (list of node IDs), 'type' (set as 'external'), 'clientType' (authentication mode, either 'basic' or 'tls'), and 'secretRef' (points to a Kubernetes secret with user credentials). The configuration must match the actual node hostnames and use int32 IDs. Expected input is Kubernetes resource YAML as shown, with no programmatic output but enables the operator to manage external resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/3_external_cluster.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: externalcluster\nspec:\n  # rootProcessGroupId contains the uuid of the root process group for this cluster.\n  rootProcessGroupId: 'd37bee03-017a-1000-cff7-4eaaa82266b7'\n  # nodeURITemplate used to dynamically compute node uri.\n  nodeURITemplate: 'nifi0%d.integ.mapreduce.m0.p.fti.net:9090'\n  # all node requiresunique id\n  nodes:\n    - id: 1\n    - id: 2\n    - id: 3\n  # type defines if the cluster is internal (i.e manager by the operator) or external.\n  # :Enum={\"external\",\"internal\"}\n  type: 'external'\n  # clientType defines if the operator will use basic or tls authentication to query the NiFi cluster.\n  # Enum={\"tls\",\"basic\"}\n  clientType: 'basic'\n  # secretRef reference the secret containing the informations required to authenticate to the cluster.\n  secretRef:\n    name: nifikop-credentials\n    namespace: nifikop-nifi\n```\n\n----------------------------------------\n\nTITLE: Applying NifiCluster CR Update | Shell\nDESCRIPTION: This command uses `kubectl apply` to send the modified `NifiCluster` Custom Resource definition to the Kubernetes API server. The NiFiKop operator watches for these changes and initiates the scale-up process based on the updated configuration. Assumes the cluster is in the `nifi` namespace and the file is `config/samples/simplenificluster.yaml`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nkubectl -n nifi apply -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio Gateway for HTTP to Expose NiFi Cluster\nDESCRIPTION: Defines an Istio Gateway resource named 'nifi-gateway' that intercepts incoming HTTP requests on port 80 for the domain 'nifi.my-domain.com'. This setup allows external access to the NiFi cluster via Istio ingress gateway.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: nifi-gateway\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - nifi.my-domain.com\n```\n\n----------------------------------------\n\nTITLE: Verifying New NiFi Node Resources - kubectl / Console\nDESCRIPTION: These shell/console commands display the pods, configmaps, and persistent volume claims associated with the newly added NiFi node (id 25). They rely on kubectl and label selectors (nodeId=25) to confirm operational readiness. Output includes resource names, statuses, ages, and storage bindings. Prerequisites: an active Kubernetes context, sufficient RBAC, and a recently applied cluster configuration including the new node.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl get pods,configmap,pvc -l nodeId=25\nNAME                          READY   STATUS    RESTARTS   AGE\npod/simplenifi-25-nodem5jh4   1/1     Running   0          11m\n\nNAME                             DATA   AGE\nconfigmap/simplenifi-config-25   7      11m\n\nNAME                                               STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\npersistentvolumeclaim/simplenifi-25-storagehwn24   Bound    pvc-7da86076-728e-11ea-846d-42010a8400f2   10Gi       RWO            standard       11m\n```\n\n----------------------------------------\n\nTITLE: Helm Get Status\nDESCRIPTION: This command retrieves the status of a specific Helm deployment, identified by its release name (`nifikop`). It provides information about the current state and resources associated with the deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/helm/nifikop/README.md#_snippet_6\n\nLANGUAGE: console\nCODE:\n```\nhelm status nifikop\n```\n\n----------------------------------------\n\nTITLE: Installing Zookeeper with Helm in Kubernetes\nDESCRIPTION: Creates a Zookeeper deployment in Kubernetes using Bitnami's Helm chart, specifying resources, storage, network policies, and replica count. It requires Helm to add the Bitnami repository and run the install command with custom values, enabling Zookeeper for NiFi cluster coordination.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/2_setup/1_getting_started.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhelm repo add bitnami https://charts.bitnami.com/bitnami\nhelm install zookeeper bitnami/zookeeper \\\n    --namespace=zookeeper \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set global.storageClass=standard \\\n    --set networkPolicy.enabled=true \\\n    --set replicaCount=3 \\\n    --create-namespace\n```\n\n----------------------------------------\n\nTITLE: Applying the updated NiFiCluster configuration - SH\nDESCRIPTION: This shell command applies the updated `NifiCluster` configuration to the Kubernetes cluster using `kubectl`. It specifies the namespace (`nifi`) and the path to the YAML configuration file (`config/samples/simplenificluster.yaml`). This triggers the node removal.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/3_tasks/1_nifi_cluster/2_cluster_scaling.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\nkubectl -n nifi apply -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Getting Status of NiFiKop Helm Deployment in Bash\nDESCRIPTION: Command to retrieve detailed status information about the NiFiKop Helm deployment, including release revision, updated time, namespace, resource status, and hooks. This is helpful for troubleshooting and verifying deployment health.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/2_setup/3_install/1_customizable_install_with_helm.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nhelm status nifikop\n```\n\n----------------------------------------\n\nTITLE: Defining Node Configuration in YAML\nDESCRIPTION: This YAML snippet demonstrates how to define NiFi node configurations using NiFiKop. It includes examples for specifying node IDs, node config groups, read-only configurations (overrideConfigs), resource requirements (CPU and memory limits/requests), and storage configurations with PVC specifications.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/5_references/1_nifi_cluster/4_node.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n    - id: 0\n      # nodeConfigGroup can be used to ease the node configuration, if set only the id is required\n      nodeConfigGroup: \"default_group\"\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        nifiProperties:\n          overrideConfigs: |\n            nifi.ui.banner.text=NiFiKop - Node 0\n      # node configuration\n#       nodeConfig:\n    - id: 2\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        overrideConfigs: |\n          nifi.ui.banner.text=NiFiKop - Node 2\n      # node configuration\n      nodeConfig:\n        resourcesRequirements:\n          limits:\n            cpu: \"2\"\n            memory: 3Gi\n          requests:\n            cpu: \"1\"\n            memory: 1Gi\n        storageConfigs:\n          # Name of the storage config, used to name PV to reuse into sidecars for example.\n          - name: provenance-repository\n            # Path where the volume will be mount into the main nifi container inside the pod.\n            mountPath: \"/opt/nifi/provenance_repository\"\n            # Kubernetes PVC spec\n            # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n            pvcSpec:\n              accessModes:\n                - ReadWriteOnce\n              storageClassName: \"standard\"\n              resources:\n                requests:\n                  storage: 8Gi\n```\n\n----------------------------------------\n\nTITLE: Verifying HPA Deployment Kubernetes/kubectl Console\nDESCRIPTION: This command lists the HorizontalPodAutoscaler (HPA) resources in the 'clusters' namespace. KEDA creates and manages an HPA based on the ScaledObject definition. This command is used to verify that the HPA has been successfully deployed and is targeting the specified NiFiNodeGroupAutoscaler resource.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_14\n\nLANGUAGE: console\nCODE:\n```\nkubectl get -n clusters hpa\n```\n\n----------------------------------------\n\nTITLE: Listing NiFi User Groups with kubectl\nDESCRIPTION: Command to retrieve all NifiUserGroup resources in the nifikop namespace, which includes the automatically created managed groups.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/2_manage_users_and_accesses/3_managed_groups.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get -n nifikop nifiusergroups.nifi.konpyutaika.com\n```\n\n----------------------------------------\n\nTITLE: Updating Helm Repositories\nDESCRIPTION: Updates the list of available charts from all configured Helm repositories, ensuring you can access the latest version of the KEDA chart.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nhelm repo update\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop Helm Chart with values.yaml (console)\nDESCRIPTION: This command installs the NiFiKop Helm chart from the specified repository. It utilizes the `-f` flag to provide a YAML file (`values.yaml`) containing custom configuration parameters that override the chart's default settings. This approach allows for managing complex configurations externally.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nhelm install nifikop \\\n      konpyutaika/nifikop \\\n      -f values.yaml\n```\n\n----------------------------------------\n\nTITLE: Specifying NifiCluster Init Container Image Using bash in YAML\nDESCRIPTION: This YAML snippet defines a NifiCluster custom resource specifying the initContainerImage using the 'bash' repository at tag '5.2.2'. It represents the updated recommended configuration after the upgrade to version 0.15.0 of nifikop, reflecting the change in the default init container image from busybox to bash. The bash image is required because the init container expects a bash shell for initialization scripts and tasks.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/7_upgrade_guides/2_v0.14.1_to_v0.15.0.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  initContainerImage:\n    repository: bash\n    tag: \"5.2.2\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Conversion Webhook for CRD YAML Definitions in Kubernetes\nDESCRIPTION: This YAML snippet configures the conversion webhook section within a Kubernetes CRD manifest to enable resource conversion between versions (`v1alpha1` to `v1`). It requires setting proper annotations to reference a TLS certificate managed by cert-manager. The webhook clientConfig defines the service namespace, name, and path to route conversion requests correctly. This setup is essential if the Helm deployment keeps the conversion webhook enabled for smooth CRD version migrations.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n```\n\n----------------------------------------\n\nTITLE: Create ServiceMonitor Resource for NiFi Metrics Scraping\nDESCRIPTION: This YAML creates a ServiceMonitor to instruct Prometheus to scrape metrics from NiFi cluster pods. It matches labels like 'app: nifi' and 'nifi_cr: cluster', specifies namespace access, and sets the scraping interval and port. It also includes relabeling rules for node identification. This resource enables Prometheus to collect NiFi metrics necessary for autoscaling triggers.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_5\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: cluster\n  namespace: monitoring-system\n  labels:\n    app: nifi-cluster\n    nifi_cr: cluster\nspec:\n  selector:\n    matchLabels:\n      app: nifi\n      nifi_cr: cluster\n  namespaceSelector:\n    matchNames:\n      - clusters\n  endpoints:\n    - interval: 10s\n      port: prometheus\n      path: /metrics\n      honorLabels: true\n      relabelings:\n        - sourceLabels: [__meta_kubernetes_pod_ip]\n          separator: ;\n          regex: (.*)\n          targetLabel: pod_ip\n          replacement: $1\n          action: replace\n        - sourceLabels: [__meta_kubernetes_pod_label_nodeId]\n          separator: ;\n          regex: (.*)\n          targetLabel: nodeId\n          replacement: $1\n          action: replace\n        - sourceLabels: [__meta_kubernetes_pod_label_nifi_cr]\n          separator: ;\n          regex: (.*)\n          targetLabel: nifi_cr\n          replacement: $1\n          action: replace\n```\n\n----------------------------------------\n\nTITLE: Configure NiFi Cluster with Prometheus Metrics Endpoint\nDESCRIPTION: This YAML snippet configures the NiFi cluster specification, specifically adding an internal listener on port 9090 for Prometheus metrics collection. It also defines a NodeConfigGroup named 'auto_scaling' with resource limits and storage configurations, enabling NiFi to expose metrics suitable for autoscaling triggers. Dependencies include correct NiFi deployment and volume storage classes; inputs include resource requests and storage limits; outputs are the configured NiFi cluster ready for monitoring and autoscaling.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\n...spec:\n  ...\n  listenersConfig:\n    internalListeners:\n    ...\n    - containerPort: 9090\n      name: prometheus\n      type: prometheus \n    ...\n  ...\n  nodeConfigGroups:\n    auto_scaling:\n      isNode: true\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 2Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n      serviceAccountName: external-dns\n      storageConfigs:\n        - mountPath: /opt/nifi/nifi-current/logs\n          name: logs\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        ... (additional storage configurations follow) ...\n```\n\n----------------------------------------\n\nTITLE: Defining NifiParameterContext Child in YAML\nDESCRIPTION: This YAML snippet defines another `NifiParameterContext` resource, similar to the previous one, but with different metadata and a specification that includes an `inheritedParameterContexts` field.  This indicates that this parameter context inherits parameters from the `dataflow-lifecycle` context. This setup enables parameter inheritance and redefinition within NiFi parameter contexts managed by the operator.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/5_references/4_nifi_parameter_context.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n--- \napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle-child\nspec:\n  description: \"It is a child test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  inheritedParameterContexts:\n    - name: dataflow-lifecycle\n  parameters:\n    - name: test\n      value: toto-child\n      description: tutu (child)\n```\n\n----------------------------------------\n\nTITLE: Defining Additional Custom Internal Listener Without Type - YAML\nDESCRIPTION: This minimal YAML example shows how to add an unnamed or custom internal listener under listenersConfig.internalListeners without the 'type' parameter, typically for exposing NiFi processors directly (e.g., an HTTP endpoint for webhooks into NiFi). No additional requirements beyond Kubernetes and the NiFi CRD. The name key labels the listener, and containerPort specifies its port. The port can then be externally exposed as needed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    ...\n    - name: \"http-tracking\"\n      containerPort: 8081\n```\n\n----------------------------------------\n\nTITLE: Cloning and Navigating to the NiFiKop Repository with Git in Bash\nDESCRIPTION: This snippet clones the NiFiKop repository from GitHub and navigates into its directory. No dependencies are required aside from Git itself. Accepts no parameters; input is the GitHub repository URL, and output is a local copy of the project directory. The snippet expects a Unix-like shell and assumes network access to GitHub.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/6_contributing/1_developer_guide.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/konpyutaika/nifikop.git\ncd nifikop\n```\n\n----------------------------------------\n\nTITLE: Schema Definition for NifiRegistryClient Resource\nDESCRIPTION: This markdown documents the structure of the NifiRegistryClient resource, including its fields: metadata, spec, and status. It explains each field's purpose, data type, whether it is required, and default values. The schema guides users in correctly defining and managing NiFi registry clients within Kubernetes, ensuring schema adherence and information clarity.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/5_references/3_nifi_registry_client.md#_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n## NifiRegistryClient\n|Field|Type|Description|Required|Default|\n|-----|----|-----------|--------|--------|\n|metadata|[ObjectMetadata](https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1#ObjectMeta)|is metadata that all persisted resources must have, which includes all objects registry clients must create.|No|nil|\n|spec|[NifiRegistryClientSpec](#nifiregistryclientspec)|defines the desired state of NifiRegistryClient.|No|nil|\n|status|[NifiRegistryClientStatus](#nifiregistryclientstatus)|defines the observed state of NifiRegistryClient.|No|nil|\n\n## NifiRegistryClientsSpec\n\n|Field|Type|Description|Required|Default|\n|-----|----|-----------|--------|--------|\n|description|string| describes the Registry client. |No| - |\n|uri|string| URI of the NiFi registry that should be used for pulling the flow. |Yes| - |\n|clusterRef|[ClusterReference](./2_nifi_user.md#clusterreference)|  contains the reference to the NifiCluster with the one the user is linked. |Yes| - |\n```\n\n----------------------------------------\n\nTITLE: Manually Deleting Nifikop CRDs\nDESCRIPTION: Removes the Nifikop Custom Resource Definitions (CRDs) from the Kubernetes cluster using `kubectl`. This is typically done after uninstalling the Helm chart if CRDs were not automatically removed. Warning: Deleting CRDs will also delete all custom resources created using them.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete crd nificlusters.nifi.konpyutaika.com\nkubectl delete crd nifiusers.nifi.konpyutaika.com\nkubectl delete crd nifiusergroups.nifi.konpyutaika.com\nkubectl delete crd nifiregistryclients.nifi.konpyutaika.com\nkubectl delete crd nifiparametercontexts.nifi.konpyutaika.com\nkubectl delete crd nifidataflows.nifi.konpyutaika.com\n```\n\n----------------------------------------\n\nTITLE: Secured NiFi Cluster Custom Resource Definition Example in YAML\nDESCRIPTION: This YAML snippet defines a NifiCluster custom resource for deploying a secured NiFi cluster using NiFiKop. Key configurable fields include the initial admin user email for authentication, web proxy hosts used by NiFi's OIDC configuration, and override configurations specifying OAuth client ID and secret for OpenID Connect authentication integration. This resource drives the instantiation and management of NiFi pods within Kubernetes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/blog/2020-05-20-secured_nifi_cluster_on_gcp.md#_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.orange.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: securednificluster\n  namespace: nifi\nspec:\n  ...\n  initialAdminUser: <your google account email>\n  readOnlyConfig:\n    # NifiProperties configuration that will be applied to the node.\n    nifiProperties:\n      webProxyHosts:\n        - <nifi's hostname>:8443\n      # Additionnal nifi.properties configuration that will override the one produced based\n      # on template and configurations.\n      overrideConfigs: |\n        ...\n        nifi.security.user.oidc.client.id=<oidc client's id>\n        nifi.security.user.oidc.client.secret=<oidc client's secret>\n        ...\n```\n\n----------------------------------------\n\nTITLE: Installing Prometheus Operator via Helm\nDESCRIPTION: Installs the `kube-prometheus-stack` Helm chart (renamed to 'prometheus' upon installation) into the 'monitoring-system' namespace using `helm install`. Several default components (Alertmanager, Grafana, default rules, etc.) are disabled via `--set` flags, and the operator's scope is restricted to specific namespaces, resulting in a minimal Prometheus Operator setup focused on collecting metrics.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_7\n\nLANGUAGE: console\nCODE:\n```\nhelm install prometheus prometheus/kube-prometheus-stack --namespace monitoring-system \\\n    --set prometheusOperator.createCustomResource=false \\\n    --set prometheusOperator.logLevel=debug \\\n    --set prometheusOperator.alertmanagerInstanceNamespaces=monitoring-system \\\n    --set prometheusOperator.namespaces.additional[0]=monitoring-system \\\n    --set prometheusOperator.prometheusInstanceNamespaces=monitoring-system \\\n    --set prometheusOperator.thanosRulerInstanceNamespaces=monitoring-system \\\n    --set defaultRules.enable=false \\\n    --set alertmanager.enabled=false \\\n    --set grafana.enabled=false \\\n    --set kubeApiServer.enabled=false \\\n    --set kubelet.enabled=false \\\n    --set kubeControllerManager.enabled=false \\\n    --set coreDNS.enabled=false \\\n    --set kubeEtcd.enabled=false \\\n    --set kubeScheduler.enabled=false \\\n    --set kubeProxy.enabled=false \\\n    --set kubeStateMetrics.enabled=false \\\n    --set prometheus.enabled=false\n```\n\n----------------------------------------\n\nTITLE: Building and Installing the NiFiKop kubectl plugin on UNIX\nDESCRIPTION: Commands to build the NiFiKop kubectl plugin using make and install it by copying the executable to /usr/local/bin in the system PATH.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/2_deploy_nifikop/3_kubectl_plugin.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nmake kubectl-nifikop && sudo cp ./bin/kubectl-nifikop /usr/local/bin\n```\n\n----------------------------------------\n\nTITLE: Connecting to GKE Cluster with gcloud\nDESCRIPTION: Command to authenticate with Google Kubernetes Engine (GKE) and configure kubectl to connect to a specific GKE cluster. Requires the Google Cloud SDK to be installed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/2_deploy_nifikop/1_quick_start.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\ngcloud container clusters get-credentials CLUSTER_NAME --zone ZONE_NAME --project PROJECT_NAME.\n```\n\n----------------------------------------\n\nTITLE: Connecting to GKE cluster with gcloud\nDESCRIPTION: Uses gcloud CLI to authenticate and connect to a Google Kubernetes Engine (GKE) cluster, then configure kubectl to use the GKE cluster context for further operations.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/2_deploy_nifikop/1_quick_start.md#_snippet_6\n\nLANGUAGE: console\nCODE:\n```\ngcloud container clusters get-credentials CLUSTER_NAME --zone ZONE_NAME --project PROJECT_NAME.\n```\n\n----------------------------------------\n\nTITLE: Listing Currently Deployed Helm Charts\nDESCRIPTION: Command to list all Helm releases currently deployed in the cluster, displaying their status, namespace, and other details.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nhelm list\n```\n\n----------------------------------------\n\nTITLE: Declaring Cluster Nodes with Config Groups in NiFiKop (YAML)\nDESCRIPTION: This YAML configuration fragment shows how to assign specific NodeConfigGroups to individual NiFi cluster nodes or declare ad-hoc node config sections. The id property uniquely identifies the node, nodeConfigGroup references a defined NodeConfigGroup, and nodeConfig allows specifying non-reusable node settings inline. This structure is typically included under the nodes section of a NiFiKop CRD. Users must ensure all referenced NodeConfigGroups are previously defined and resource requests are valid per the Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n- id: 0\n  nodeConfigGroup: \"default_group\"\n- id: 2\n  nodeConfigGroup: \"high_mem_group\"\n- id: 3\n  nodeConfigGroup: \"high_mem_group\"\n- id: 5\n  nodeConfig:\n    resourcesRequirements:\n      limits:\n        cpu: \"2\"\n        memory: 3Gi\n      requests:\n        cpu: \"1\"\n        memory: 1Gi\n\n```\n\n----------------------------------------\n\nTITLE: Setting NiFi Authorizer Property\nDESCRIPTION: This shell command demonstrates how to set the `nifi.security.user.authorizer` property in NiFi to use the custom database authorizer. This configuration property instructs NiFi to use the `custom-database-authorizer` defined in the `authorizers.xml` file.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/6_users_authorization/1_custom_user_authorizer.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nnifi.security.user.authorizer=custom-database-authorizer\n```\n\n----------------------------------------\n\nTITLE: Updating NifiCluster InitContainerImage (YAML)\nDESCRIPTION: This snippet demonstrates the updated YAML configuration for a `NifiCluster`. It changes the `initContainerImage` repository to `bash` and updates the tag to the bash version. This update is crucial for users who have overridden the default init container image to ensure compatibility with the upgraded nifikop version. It ensures that the init container uses a bash shell. The output is a running NifiCluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/7_upgrade_guides/2_v0.14.1_to_v0.15.0.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  initContainerImage:\n    repository: bash\n    tag: \"5.2.2\"\n```\n\n----------------------------------------\n\nTITLE: Installing Zookeeper on OpenShift with Security Context Settings\nDESCRIPTION: Installs Zookeeper on OpenShift using Helm with resource limits, network policy, replica count, and custom runAsUser and fsGroup security context parameters. The command leverages the previously obtained UID/GID to comply with OpenShift's user ID constraints, ensuring the Zookeeper pods operate correctly within OpenShift's security model.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhelm install zookeeper oci://registry-1.docker.io/bitnamicharts/zookeeper \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set global.storageClass=standard \\\n    --set networkPolicy.enabled=true \\\n    --set replicaCount=3 \\\n    --set containerSecurityContext.runAsUser=$zookeper_uid \\\n    --set podSecurityContext.fsGroup=$zookeper_uid\n```\n\n----------------------------------------\n\nTITLE: Get OpenShift Namespace UID Range (bash)\nDESCRIPTION: Uses kubectl to retrieve the openshift.io/sa.scc.supplemental-groups annotation from the nifi namespace, pipes the output to sed to remove /10000 from the end, and uses tr to remove whitespace. This extracts the base UID allocated for service accounts in the namespace, required for setting runAsUser on OpenShift. Requires kubectl and assumes the nifi namespace exists.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/2_deploy_nifikop/1_quick_start.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nuid=$(kubectl get namespace nifi -o=jsonpath='{.metadata.annotations.openshift\\.io/sa\\.scc\\.supplemental-groups}' | sed 's/\\/10000$//' | tr -d '[:space:]')\n```\n\n----------------------------------------\n\nTITLE: Purging a NiFiKop Release\nDESCRIPTION: Command to completely remove a Helm release including its release history. This prevents potential reuse of the release name or rollback operations.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nhelm delete --purge nifikop\n```\n\n----------------------------------------\n\nTITLE: Applying NiFiKop CRDs to Kubernetes Cluster using kubectl in Bash\nDESCRIPTION: This snippet applies multiple CustomResourceDefinition (CRD) YAML files for NiFiKop to the active Kubernetes cluster. Dependencies include kubectl and appropriate cluster access. Inputs are the YAML file paths; outputs are the active CRDs in the Kubernetes API server. Sequential commands are required for each resource.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/6_contributing/1_developer_guide.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\n```\n\n----------------------------------------\n\nTITLE: Removing a Node from NiFi Cluster Configuration in YAML\nDESCRIPTION: YAML configuration showing how to remove a node (id: 2) from an existing NiFi cluster by updating the NifiCluster custom resource definition.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/3_tasks/1_nifi_cluster/2_cluster_scaling.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  headlessServiceEnabled: true\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.11.3\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n# >>>> START: node removed\n#    - id: 2\n#      nodeConfigGroup: \"default_group\"\n# <<<< END\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: Defining a NiFi Cluster with NifiCluster CRD in YAML\nDESCRIPTION: This YAML snippet defines a NiFi cluster using the `NifiCluster` custom resource definition. It specifies the desired state of the NiFi cluster, including details such as the ZooKeeper address, cluster image, node configurations, storage configurations, listeners, and external services. The `spec` section configures the NiFi cluster, while `metadata` provides identifying information.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/5_references/1_nifi_cluster/1_nifi_cluster.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n    annotations:\n      tyty: ytyt\n    labels:\n      tete: titi  \n  pod:\n    annotations:\n      toto: tata\n    labels:\n      titi: tutu\n  zkAddress: 'zookeepercluster-client.zookeeper:2181'\n  zkPath: '/simplenifi'\n  clusterImage: 'apache/nifi:1.11.3'\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      podMetadata:\n        annotations:\n          node-annotation: \"node-annotation-value\"\n        labels:\n          node-label: \"node-label-value\"\n      externalVolumeConfigs:\n        - name: example-volume\n          mountPath: \"/opt/nifi/example\"\n          secret:\n            secretName: \"raw-controller\"\n      storageConfigs:\n        - mountPath: '/opt/nifi/nifi-current/logs'\n          name: logs\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: 'standard'\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: 'default'\n      resourcesRequirements:\n        limits:\n          cpu: '2'\n          memory: 3Gi\n        requests:\n          cpu: '1'\n          memory: 1Gi\n  nodes:\n    - id: 1\n      nodeConfigGroup: 'default_group'\n    - id: 2\n      nodeConfigGroup: 'default_group'\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: 'http'\n        name: 'http'\n        containerPort: 8080\n      - type: 'cluster'\n        name: 'cluster'\n        containerPort: 6007\n      - type: 's2s'\n        name: 's2s'\n        containerPort: 10000\n  externalServices:\n    - name: 'clusterip'\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8080\n            internalListenerName: 'http'\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n\n----------------------------------------\n\nTITLE: Setting environment variables for local development\nDESCRIPTION: Environment variables needed to configure your IDE for local development and debugging of the NiFiKop operator.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/6_contributing/1_developer_guide.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nKUBECONFIG={path/to/your/kubeconfig}\nWATCH_NAMESPACE={namespace_to_watch}\nPOD_NAME={name for operator pod}\nLOG_LEVEL=Debug\nOPERATOR_NAME=ide\n```\n\n----------------------------------------\n\nTITLE: Configuring Conversion Webhook for NiFiKop CRDs\nDESCRIPTION: YAML configuration needed for enabling the conversion webhook on NiFiKop CRDs. This handles version conversion between v1alpha1 and v1 resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n```\n\n----------------------------------------\n\nTITLE: Installing cert-manager directly in Kubernetes\nDESCRIPTION: Deploys cert-manager via kubectl applying the YAML manifest to set up CustomResourceDefinitions and the cert-manager components, enabling certificate issuance for secured NiFi clusters.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/2_setup/1_getting_started.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Install the CustomResourceDefinitions and cert-manager itself\nkubectl apply -f \\\n    https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.yaml\n```\n\n----------------------------------------\n\nTITLE: Manually Deleting Nifikop-Related Kubernetes CRDs in Bash\nDESCRIPTION: These Bash commands delete all Custom Resource Definitions (CRDs) related to nifikop from the Kubernetes cluster. This action is irreversible and will remove all custom resources managed by the CRDs, including all nifi clusters, users, groups, and related entities. Prerequisites include kubectl access with cluster-admin privileges, and extreme caution is advised as deleting CRDs will delete all associated cluster resources. Each command targets a specific CRD object by name.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete crd nificlusters.nifi.konpyutaika.com\nkubectl delete crd nifiusers.nifi.konpyutaika.com\nkubectl delete crd nifiusergroups.nifi.konpyutaika.com\nkubectl delete crd nifiregistryclients.nifi.konpyutaika.com\nkubectl delete crd nifiparametercontexts.nifi.konpyutaika.com\nkubectl delete crd nifidataflows.nifi.konpyutaika.com\n```\n\n----------------------------------------\n\nTITLE: Installing Zookeeper Using Helm\nDESCRIPTION: Helm command to install Bitnami's Zookeeper chart with specific resource constraints, storage class configuration, and network policy settings. This creates a 3-node Zookeeper cluster for NiFi's state management.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhelm install zookeeper oci://registry-1.docker.io/bitnamicharts/zookeeper \\\n    --namespace=zookeeper \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set global.storageClass=standard \\\n    --set networkPolicy.enabled=true \\\n    --set replicaCount=3 \\\n    --create-namespace\n```\n\n----------------------------------------\n\nTITLE: Defining Prometheus Resource - YAML\nDESCRIPTION: YAML definition for a `Prometheus` custom resource managed by the Prometheus Operator. It configures the Prometheus server instance itself, specifying scrape intervals, resource limits, and selecting which `PodMonitor` and `ServiceMonitor` resources to use for discovering targets. Requires the Prometheus Operator CRDs.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: Prometheus\nmetadata:\n  name: prometheus\n  namespace: monitoring-system\nspec:\n  enableAdminAPI: false\n  evaluationInterval: 30s\n  logLevel: debug\n  podMonitorSelector:\n    matchExpressions:\n    - key: release\n      operator: In\n      values:\n      - prometheus\n  resources:\n    requests:\n      memory: 400Mi\n  scrapeInterval: 30s\n  serviceAccountName: prometheus\n  serviceMonitorSelector:\n    matchExpressions:\n    - key: app\n      operator: In\n      values:\n      - nifi-cluster\n```\n\n----------------------------------------\n\nTITLE: Listing Managed Groups with kubectl\nDESCRIPTION: This console command retrieves a list of NifiUserGroup resources, showing the managed groups created by the operator.  It is used to verify that the managed groups (managed-admins, managed-nodes, and managed-readers) have been successfully created and managed.  The output displays the names and ages of the groups.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/3_manage_nifi/2_manage_users_and_accesses/3_managed_groups.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nkubectl get -n nifikop nifiusergroups.nifi.konpyutaika.com \nNAME              AGE\nmanaged-admins    6d7h\nmanaged-nodes     6d7h\nmanaged-readers   6d7h\n```\n\n----------------------------------------\n\nTITLE: Deleting CRDs Manually (After Uninstall)\nDESCRIPTION: This snippet provides the commands to manually delete the CRDs after uninstalling the chart.  This should only be done if the CRDs are no longer needed. Requires `kubectl` and proper cluster access.  Deleting these CRDs will delete all clusters/resources created by them.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete crd nificlusters.nifi.konpyutaika.com\nkubectl delete crd nifiusers.nifi.konpyutaika.com\nkubectl delete crd nifiusergroups.nifi.konpyutaika.com\nkubectl delete crd nifiregistryclients.nifi.konpyutaika.com\nkubectl delete crd nifiparametercontexts.nifi.konpyutaika.com\nkubectl delete crd nifidataflows.nifi.konpyutaika.com\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFiKop on OpenShift using Helm (Bash)\nDESCRIPTION: Installs the NiFiKop operator on OpenShift using Helm, similar to the standard deployment, but includes the '--set runAsUser=$uid' parameter. This sets the security context for the operator pods using the UID obtained from the target namespace's annotations, ensuring compatibility with OpenShift's security constraints (SCC).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/2_deploy_nifikop/1_quick_start.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop \\\n    nifikop \\\n    --namespace=nifi \\\n    --version 1.1.1 \\\n    --set image.tag=v1.1.1-release \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set namespaces={\\\"nifi\\\"} \\\n    --set runAsUser=$uid\n```\n\n----------------------------------------\n\nTITLE: Installing Nifikop without CRDs\nDESCRIPTION: This command installs the Nifikop chart while skipping the installation of the CRDs using the `--skip-crds` flag. This allows for a manual CRD deployment strategy, and sets `namespaces` parameter.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\n$ helm install --name nifikop ./helm/nifikop --set namespaces={\\\"nifikop\\\"} --skip-crds\n```\n\n----------------------------------------\n\nTITLE: Manually deploying NiFiKop CRDs on Kubernetes\nDESCRIPTION: Applies the Custom Resource Definitions required by NiFiKop when skipping CRD installation via Helm. This includes CRDs for NiFi clusters, users, user groups, dataflows, parameter contexts, registry clients, node group autoscalers, and connections.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/2_deploy_nifikop/1_quick_start.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifinodegroupautoscalers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificonnections.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring CRD Conversion Webhook Settings\nDESCRIPTION: Specifies the necessary YAML configuration within a CRD definition to enable the conversion webhook. This allows seamless conversion between resource versions (e.g., v1alpha1 to v1) and requires cert-manager for CA injection. Replace placeholders like `${namespace}`, `${certificate_name}`, and `${webhook_service_name}` with actual values.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n```\n\n----------------------------------------\n\nTITLE: Purging a NiFiKop Helm Release\nDESCRIPTION: This command purges the specified NiFiKop Helm release.  It completely removes all traces of the release from Helm's history.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/helm/nifikop/README.md#_snippet_9\n\nLANGUAGE: Console\nCODE:\n```\nhelm del nifikop\n```\n\n----------------------------------------\n\nTITLE: Checking Node Decommission Status (kubectl)\nDESCRIPTION: Uses kubectl to describe the NifiCluster resource and examine the 'Status.Nodes State' section. This allows monitoring the progress of the graceful node decommissioning process triggered by the operator, indicated by the 'Graceful Action State' and 'Action State' fields.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_5\n\nLANGUAGE: console\nCODE:\n```\nkubectl describe nificluster simplenifi\n\n...\nStatus:\n  Nodes State:\n    ...\n    2:\n      Configuration State:  ConfigInSync\n      Graceful Action State:\n        Action State:   GracefulDownscaleRequired\n        Error Message:\n    ...\n...\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes StorageClass with WaitForFirstConsumer Volume Binding in YAML\nDESCRIPTION: Defines a custom Kubernetes StorageClass resource leveraging the volume binding mode 'WaitForFirstConsumer' to delay volume binding until a pod consumes it, improving scheduling flexibility. The StorageClass uses the GCE PD provisioner with a 'Delete' reclaim policy and sets the parameter type to 'pd-standard'. This snippet requires Kubernetes cluster access with permission to create StorageClass objects.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: exampleStorageclass\nparameters:\n  type: pd-standard\nprovisioner: kubernetes.io/gce-pd\nreclaimPolicy: Delete\nvolumeBindingMode: WaitForFirstConsumer\n```\n\n----------------------------------------\n\nTITLE: State Enumeration for Upscale and Downscale Operations in NiFiKop\nDESCRIPTION: Enumerations that track the state of upscale and downscale operations on NiFi nodes, indicating whether an operation is required, in progress, or completed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/5_references/1_nifi_cluster/5_node_state.md#_snippet_4\n\nLANGUAGE: go\nCODE:\n```\ntype State string\n\nconst (\n\t// Upscale states\n\tGracefulUpscaleRequired State = \"GracefulUpscaleRequired\"\n\tGracefulUpscaleRunning State = \"GracefulUpscaleRunning\"\n\tGracefulUpscaleSucceeded State = \"GracefulUpscaleSucceeded\"\n\n\t// Downscale states\n\tGracefulDownscaleRequired State = \"GracefulDownscaleRequired\"\n\tGracefulDownscaleRunning State = \"GracefulDownscaleRunning\"\n\t// Note: The documentation shows this as GracefulUpscaleSucceeded which might be a typo\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Prometheus Operator\nDESCRIPTION: This command installs the Prometheus operator and associated components using Helm.  It specifies configurations for various Prometheus components, disabling some components like Grafana and alertmanager to simplify the deployment.  This command deploys a Prometheus operator which will then be responsible for deploying the Prometheus resource.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_8\n\nLANGUAGE: console\nCODE:\n```\nhelm install prometheus prometheus/kube-prometheus-stack --namespace monitoring-system \\\n    --set prometheusOperator.createCustomResource=false \\\n    --set prometheusOperator.logLevel=debug \\\n    --set prometheusOperator.alertmanagerInstanceNamespaces=monitoring-system \\\n    --set prometheusOperator.namespaces.additional[0]=monitoring-system \\\n    --set prometheusOperator.prometheusInstanceNamespaces=monitoring-system \\\n    --set prometheusOperator.thanosRulerInstanceNamespaces=monitoring-system \\\n    --set defaultRules.enable=false \\\n    --set alertmanager.enabled=false \\\n    --set grafana.enabled=false \\\n    --set kubeApiServer.enabled=false \\\n    --set kubelet.enabled=false \\\n    --set kubeControllerManager.enabled=false \\\n    --set coreDNS.enabled=false \\\n    --set kubeEtcd.enabled=false \\\n    --set kubeScheduler.enabled=false \\\n    --set kubeProxy.enabled=false \\\n    --set kubeStateMetrics.enabled=false \\\n    --set prometheus.enabled=false\n```\n\n----------------------------------------\n\nTITLE: Defining NifiDataflow Kubernetes Resource\nDESCRIPTION: This snippet provides an example YAML definition for a `NifiDataflow` resource. It specifies essential fields like the parent process group, bucket ID, flow ID, flow version, flow position, synchronization mode, skip flags for invalid components/services, update strategy, and references to the associated NiFi cluster, registry client, and parameter context resources managed by the Nifikop operator.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/5_references/5_nifi_dataflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  flowPosition:\n    posX: 0\n    posY: 0\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: squidflow\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: nifikop\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Configuring Internal Listeners for NiFi Cluster in Kubernetes\nDESCRIPTION: YAML configuration for internal listeners in a NiFi cluster, including HTTPS, cluster communication, site-to-site, Prometheus, and load-balancing ports. These listeners define the ports used for internal communication within the NiFi cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    - type: \"https\"\n      name: \"https\"\n      containerPort: 8443\n    - type: \"cluster\"\n      name: \"cluster\"\n      containerPort: 6007\n    - type: \"s2s\"\n      name: \"s2s\"\n      containerPort: 10000\n    - type: \"prometheus\"\n      name: \"prometheus\"\n      containerPort: 9090\n    - type: \"load-balance\"\n      name: \"load-balance\"\n      containerPort: 6342\n```\n\n----------------------------------------\n\nTITLE: Removing a NiFi node from NifiCluster Spec - YAML\nDESCRIPTION: This YAML snippet shows how to remove a NiFi node from the `NifiCluster.Spec.Nodes` configuration.  The removal triggers the graceful decommission process. The node with id 2 is commented out to remove it.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/3_tasks/1_nifi_cluster/2_cluster_scaling.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  headlessServiceEnabled: true\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.11.3\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n# >>>> START: node removed\n#    - id: 2\n#      nodeConfigGroup: \"default_group\"\n# <<<< END\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: Assigning NodeConfigGroups and Inline Configs to NiFi Nodes - YAML\nDESCRIPTION: This YAML example illustrates how to assign previously defined NodeConfigGroups to specific NiFi cluster nodes, as well as how to define resources inline for a node. Node IDs reference nodeConfigGroups by name, while custom configurations can be set per node under nodeConfig. NiFiKop and a valid cluster definition are prerequisites. Each node must have a unique id and, if using inline nodeConfig, specify required fields such as resourcesRequirements; missing mandatory fields will prevent node creation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n- id: 0\n  nodeConfigGroup: \"default_group\"\n- id: 2\n  nodeConfigGroup: \"high_mem_group\"\n- id: 3\n  nodeConfigGroup: \"high_mem_group\"\n- id: 5\n  nodeConfig:\n    resourcesRequirements:\n      limits:\n        cpu: \"2\"\n        memory: 3Gi\n      requests:\n        cpu: \"1\"\n        memory: 1Gi\n\n```\n\n----------------------------------------\n\nTITLE: Configuring initContainer to encrypt - YAML\nDESCRIPTION: This YAML snippet defines an `initContainer` for a Kubernetes deployment to automatically update NiFi's sensitive properties during the upgrade.  It leverages the `apache/nifi-toolkit:latest` image and executes a shell command that extracts the sensitive properties key, and uses encrypt-config.sh to encrypt relevant configuration files, adapting volume mounts according to requirements.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/7_upgrade_guides/4_v1.3.1_to_v1.4.0.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ninitContainers:\n  - image: \"apache/nifi-toolkit:latest\"\n    name: nifi-toolkit\n    imagePullPolicy: Always\n    command:\n      - \"sh\"\n      - \"-c\"\n      - \"NIFI_SENSITIVE_PROPS_KEY=$(grep 'nifi.sensitive.props.key' /opt/nifi/nifi-current/conf/nifi.properties | cut -d'=' -f2) && bin/encrypt-config.sh -n /opt/nifi/nifi-current/conf/nifi.properties -f /opt/nifi/data/flow.json.gz -x -A NIFI_PBKDF2_AES_GCM_256 -s $NIFI_SENSITIVE_PROPS_KEY; bin/encrypt-config.sh -n /opt/nifi/nifi-current/conf/nifi.properties -f /opt/nifi/data/flow.xml.gz -x -A NIFI_PBKDF2_AES_GCM_256 -s $NIFI_SENSITIVE_PROPS_KEY\"\n    volumeMounts:\n      - name: data\n        mountPath: /opt/nifi/data\n      - name: conf\n        mountPath: /opt/nifi/nifi-current/conf\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for IDE Execution (Bash)\nDESCRIPTION: Defines essential environment variables required for running the NiFiKop operator directly from an IDE. These variables configure the Kubernetes connection (`KUBECONFIG`), the target namespace (`WATCH_NAMESPACE`), pod identification (`POD_NAME`, `OPERATOR_NAME`), and logging verbosity (`LOG_LEVEL`).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/6_contributing/1_developer_guide.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nKUBECONFIG={path/to/your/kubeconfig}\nWATCH_NAMESPACE={namespace_to_watch}\nPOD_NAME={name for operator pod}\nLOG_LEVEL=Debug\nOPERATOR_NAME=ide\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Secret for Basic Authentication - Console\nDESCRIPTION: This console command uses 'kubectl' to create a generic Kubernetes secret named 'nifikop-credentials' containing files for username, password, and (optionally) a CA certificate, intended for basic authentication of the Nifikop operator to an external NiFi cluster. Dependencies include pre-existing files in a './secrets/' directory for each credential, and an appropriate Kubernetes namespace. Inputs are file paths and the target namespace; output is a secret stored in Kubernetes. Limitation: The secret must match the reference in the NifiCluster resource and the command should be run with sufficient permissions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/1_manage_clusters/3_external_cluster.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nkubectl create secret generic nifikop-credentials \\\n  --from-file=username=./secrets/username\\\n  --from-file=password=./secrets/password\\\n  --from-file=ca.crt=./secrets/ca.crt\\\n  -n nifikop-nifi\n```\n\n----------------------------------------\n\nTITLE: ServiceMonitor Resource to Scrape NiFi Metrics - yaml\nDESCRIPTION: YAML manifest defining a ServiceMonitor resource for Prometheus to scrape metrics from NiFi cluster pods. It includes label selectors for targeted pods, namespace selectors, endpoints configuration for scraping every 10 seconds on the Prometheus port and path, and relabeling rules to properly label scraped metrics with pod IP, nodeId, and nifi_cr values.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: cluster\n  namespace: monitoring-system\n  labels:\n    app: nifi-cluster\n    nifi_cr: cluster\nspec:\n  selector:\n    matchLabels:\n      app: nifi\n      nifi_cr: cluster\n  namespaceSelector:\n    matchNames:\n      - clusters\n  endpoints:\n    - interval: 10s\n      port: prometheus\n      path: /metrics\n      honorLabels: true\n      relabelings:\n        - sourceLabels: [__meta_kubernetes_pod_ip]\n          separator: ;\n          regex: (.*)\n          targetLabel: pod_ip\n          replacement: $1\n          action: replace\n        - sourceLabels: [__meta_kubernetes_pod_label_nodeId]\n          separator: ;\n          regex: (.*)\n          targetLabel: nodeId\n          replacement: $1\n          action: replace\n        - sourceLabels: [__meta_kubernetes_pod_label_nifi_cr]\n          separator: ;\n          regex: (.*)\n          targetLabel: nifi_cr\n          replacement: $1\n          action: replace\n```\n\n----------------------------------------\n\nTITLE: Purging a Deleted Helm Release Record (Nifikop)\nDESCRIPTION: Shows the `helm delete --purge nifikop` command (primarily relevant for Helm 2) for the `nifikop` release. This command removes the release resources and its record from Helm's history, allowing the name reuse. In Helm 3, `helm uninstall nifikop` achieves a similar outcome.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nhelm delete --purge nifikop\n```\n\n----------------------------------------\n\nTITLE: Recommended NiFi Identity Mapping Configuration (Shell)\nDESCRIPTION: Shows the recommended `nifi.security.identity.mapping` properties to add to `nifi.properties` for enabling support for multiple identity providers. These properties define how user identities are extracted or transformed, specifically focusing on parsing Common Name (CN) from a Distinguished Name (DN).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/3_tasks/2_security/2_authentication/1_oidc.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nnifi.security.identity.mapping.pattern.dn=CN=([^,]*)(?:, (?:O|OU)=.*)?\nnifi.security.identity.mapping.value.dn=$1\nnifi.security.identity.mapping.transform.dn=NONE\n```\n\n----------------------------------------\n\nTITLE: Adding Bitnami Helm Repository - Bash\nDESCRIPTION: Adds the official Bitnami chart repository to your local Helm configuration. This repository hosts a wide variety of applications, including the recommended Zookeeper chart necessary for NiFi.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhelm repo add bitnami https://charts.bitnami.com/bitnami\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiCluster Resource with a New Node (YAML)\nDESCRIPTION: This YAML manifest defines a `NifiCluster` custom resource for Kubernetes. It adds a new node with `id: 25` to an existing cluster configuration by modifying the `spec.nodes` list, demonstrating how to scale up the cluster. The new node uses the `default_group` configuration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/3_tasks/1_nifi_cluster/2_cluster_scaling.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.12.1\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n    - id: 2\n      nodeConfigGroup: \"default_group\"\n# >>>> START: The new node\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n# <<<< END\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: Pushing Docker Image to Repository\nDESCRIPTION: Pushes the locally built Docker image to a remote container registry (e.g., Docker Hub) for use in Helm deployments.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/6_contributing/1_developer_guide.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmake docker-push\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop Helm Chart using Bash and Helm\nDESCRIPTION: Installs the NiFiKop Helm chart located in `./helm/nifikop` into the `nifikop` namespace using Helm v3. It sets the chart release name to `skeleton` and specifies the image tag to use (`--set image.tag=...`), which must match the tag of the image pushed in the previous step. The `image.repository` must also match the `DOCKER_REPO_BASE` used during the build.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/6_contributing/1_developer_guide.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nhelm install skeleton ./helm/nifikop \\\n    --set image.tag=v0.5.1-release \\\n    --namespace-{\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Installing Nifikop with Custom Release Name - Bash\nDESCRIPTION: Performs a standard installation of the Nifikop operator via Helm, allowing the user to define a custom release name. No extra parameters are set; only release name and chart location are required. Dependencies: Helm CLI and access to konpyutaika/nifikop chart repository. Inputs: Must replace <release name> with the desired Helm release identifier.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nhelm install <release name> konpyutaika/nifikop\n```\n\n----------------------------------------\n\nTITLE: Installing KEDA via Helm (Console)\nDESCRIPTION: Creates a dedicated Kubernetes namespace 'keda' and then installs the KEDA Helm chart from the 'kedacore' repository into this namespace. This deploys the necessary KEDA components to the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl create namespace keda\nhelm install keda kedacore/keda --namespace keda\n```\n\n----------------------------------------\n\nTITLE: Adding Start Script to package.json (JSON)\nDESCRIPTION: Defines an npm `start` script within the `scripts` section of `package.json`. This script allows running the migration logic contained in `index.js` using the command `npm start`, executing it with Node.js while suppressing potential warnings (`--no-warnings`).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"start\": \"node --no-warnings index.js\"\n```\n\n----------------------------------------\n\nTITLE: Installing Nifikop Helm Chart with a Specific Release Name\nDESCRIPTION: Shows the basic command structure to install the Nifikop Helm chart using `helm install`. Replace `<release name>` with the desired name for this specific deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nhelm install <release name> konpyutaika/nifikop\n```\n\n----------------------------------------\n\nTITLE: Deploying KEDA ScaledObject for NiFi Node Group Autoscaler in YAML\nDESCRIPTION: Defines a ScaledObject resource to enable autoscaling for a NiFi node group autoscaler deployment using KEDA. The snippet specifies metadata, target resource references, polling intervals, cooldown periods, replica counts, fallback options, and a Prometheus trigger configured to scale based on custom metrics. Dependencies include a Kubernetes cluster with KEDA installed and Prometheus available as a metrics server. Key parameters include scaleTargetRef to link the target resource, minReplicaCount and maxReplicaCount to control scale boundaries, and triggers with Prometheus queries to determine scaling thresholds. The snippet assumes a NifiNodeGroupAutoscaler resource exists in the clusters namespace and requires customization of threshold and query values for effective scaling.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: cluster\n  namespace: clusters\nspec:\n  scaleTargetRef:\n    apiVersion:    nifi.konpyutaika.com/v1alpha1     # Optional. Default: apps/v1\n    kind:          NifiNodeGroupAutoscaler           # Optional. Default: Deployment\n    name:          nifinodegroupautoscaler-sample    # Mandatory. Must be in the same namespace as the ScaledObject\n    envSourceContainerName: nifi                     # Optional. Default: .spec.template.spec.containers[0]\n  pollingInterval:  30                               # Optional. Default: 30 seconds\n  cooldownPeriod:   300                              # Optional. Default: 300 seconds\n  idleReplicaCount: 0                                # Optional. Default: ignored, must be less than minReplicaCount \n  minReplicaCount:  1                                # Optional. Default: 0\n  maxReplicaCount:  3                                # Optional. Default: 100\n  fallback:                                          # Optional. Section to specify fallback options\n    failureThreshold: 5                              # Mandatory if fallback section is included\n    replicas: 1                                      # Mandatory if fallback section is included\n  # advanced:                                          # Optional. Section to specify advanced options\n  #   restoreToOriginalReplicaCount: true              # Optional. Whether the target resource should be scaled back to original replicas count, after the ScaledObject is deleted\n  #   horizontalPodAutoscalerConfig:                   # Optional. Section to specify HPA related options\n  #     name: {name-of-hpa-resource}                   # Optional. Default: keda-hpa-{scaled-object-name}\n  #     behavior:                                      # Optional. Use to modify HPA's scaling behavior\n  #       scaleDown:\n  #         stabilizationWindowSeconds: 300 <--- \n  #         policies:\n  #         - type: Percent\n  #           value: 100\n  #           periodSeconds: 15\n  triggers:\n    - type: prometheus\n      metadata:\n        serverAddress: http://prometheus-operated.monitoring-system.svc:9090\n        metricName: http_requests_total\n        threshold: <threshold value>\n        query: <prometheus query>\n```\n\n----------------------------------------\n\nTITLE: Manually Applying NiFiKop Custom Resource Definitions (Bash)\nDESCRIPTION: Applies all necessary NiFiKop Custom Resource Definitions (CRDs) (NifiCluster, NifiUser, NifiUserGroup, etc.) to the Kubernetes cluster using `kubectl apply`. This step is required if deploying the NiFiKop Helm chart with the `--skip-crds` flag.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/2_deploy_nifikop/1_quick_start.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing Zookeeper with Security Context for OpenShift\nDESCRIPTION: Command to install Zookeeper on OpenShift with the required security context settings. Configures runAsUser and fsGroup parameters to match OpenShift's security requirements.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhelm install zookeeper oci://registry-1.docker.io/bitnamicharts/zookeeper \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set global.storageClass=standard \\\n    --set networkPolicy.enabled=true \\\n    --set replicaCount=3 \\\n    --set containerSecurityContext.runAsUser=$zookeper_uid \\\n    --set podSecurityContext.fsGroup=$zookeper_uid\n```\n\n----------------------------------------\n\nTITLE: Applying CRDs Manually Using kubectl\nDESCRIPTION: This snippet provides commands to manually deploy Custom Resource Definitions (CRDs) for Nifikop if the '--skip-crds' option is used during Helm installation. It ensures all necessary CRDs are present before deploying the Helm chart.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifinodegroupautoscalers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificonnections.yaml\n```\n\n----------------------------------------\n\nTITLE: Creating a K3D Kubernetes Cluster (Shell)\nDESCRIPTION: Creates a K3D cluster using a specific K3s image version (v1.21.10-k3s1). The --wait flag ensures the command waits until the cluster is ready. This command requires k3d version 5.3.0 or later to be installed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/2_setup/2_platform_setup/2_k3d.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nk3d cluster create --image rancher/k3s:v1.21.10-k3s1 --wait\n```\n\n----------------------------------------\n\nTITLE: Setting the Operator Name Environment Variable in Bash\nDESCRIPTION: Exports the `OPERATOR_NAME` environment variable with the value `nifi-operator`. This variable is used when running the operator locally via the Go binary method to identify the operator instance.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/6_contributing/1_developer_guide.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport OPERATOR_NAME=nifi-operator\n```\n\n----------------------------------------\n\nTITLE: Creating a NiFi Parameter Context with Secret References in YAML\nDESCRIPTION: This YAML snippet shows how to define a NifiParameterContext custom resource, which manages configuration parameters for NiFi dataflows. It specifies metadata, a description, cluster references, and parameters including references to Kubernetes secrets for sensitive values. Sensitive parameters must be handled carefully because their values are obscured from the NiFi REST API. The secret references enable secure parameter injection into NiFi flows managed by NiFiKop.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\n  namespace: demo\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      value: toto\n      description: toto\n```\n\n----------------------------------------\n\nTITLE: Defining NifiUserGroup Kubernetes Resource (YAML)\nDESCRIPTION: This YAML snippet defines a NifiUserGroup custom resource for the Nifikop operator. It specifies the target NiFi cluster, lists references to existing NifiUser resources to include in the group, and defines a global read access policy for '/counters' resource.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/3_manage_nifi/2_manage_users_and_accesses/2_groups_management.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  # Contains the reference to the NifiCluster with the one the registry client is linked.\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  # contains the list of reference to NifiUsers that are part to the group.\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n#     namespace: nifikop\n    - name: nc-controller.nifikop.mgt.cluster.local\n  # defines the list of access policies that will be granted to the group.\n  accessPolicies:\n      # defines the kind of access policy, could be \"global\" or \"component\".\n    - type: global\n      # defines the kind of action that will be granted, could be \"read\" or \"write\"\n      action: read\n      # resource defines the kind of resource targeted by this access policies, please refer to the following page:\n      #\thttps://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#access-policies\n      resource: /counters\n#      # componentType is used if the type is \"component\", it's allow to define the kind of component on which is the\n#      # access policy\n#      componentType: \"process-groups\"\n#      # componentId is used if the type is \"component\", it's allow to define the id of the component on which is the\n#      # access policy\n#      componentId: \"\"\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop on OpenShift with RunAsUser Security Context - Bash\nDESCRIPTION: This snippet shows how to comply with OpenShift's restricted Security Context Constraints (SCC) by setting the RunAsUser parameter to the namespace UID derived via the prior snippet. It installs NiFiKop using Helm with image version, resource requests/limits, target namespace, and runAsUser configured. This enables NiFiKop pods to run with permissions aligned to namespace restrictions. Prerequisites include running on OpenShift with SCC enabled and having the UID available from the namespace annotation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/2_deploy_nifikop/1_quick_start.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop \\\n    nifikop \\\n    --namespace=nifi \\\n    --version 1.1.1 \\\n    --set image.tag=v1.1.1-release \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set namespaces={\"nifi\"} \\\n    --set runAsUser=$uid\n```\n\n----------------------------------------\n\nTITLE: Adding a Node to NiFi Cluster Configuration in YAML\nDESCRIPTION: YAML configuration for adding a new node (ID: 25) to an existing NiFi cluster. This shows the complete NifiCluster resource definition with the additional node configuration using the default_group node configuration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.12.1\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n    - id: 2\n      nodeConfigGroup: \"default_group\"\n# >>>> START: The new node\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n# <<<< END\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Node with Resources and Storage Settings in YAML\nDESCRIPTION: This snippet details advanced configuration for a NiFi node, including resource requests and limits for CPU and memory, along with persistent storage setup for provenance repository data. It specifies storage class, access mode, and volume mount path, enabling efficient resource management and data persistence in Kubernetes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/5_references/1_nifi_cluster/4_node.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\n    - id: 2\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        overrideConfigs: |\n          nifi.ui.banner.text=NiFiKop - Node 2\n      # node configuration\n      nodeConfig:\n        resourcesRequirements:\n          limits:\n            cpu: \"2\"\n            memory: 3Gi\n          requests:\n            cpu: \"1\"\n            memory: 1Gi\n        storageConfigs:\n          # Name of the storage config, used to name PV to reuse into sidecars for example.\n          - name: provenance-repository\n            # Path where the volume will be mount into the main nifi container inside the pod.\n            mountPath: \"/opt/nifi/provenance_repository\"\n            # Kubernetes PVC spec\n            # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n            pvcSpec:\n              accessModes:\n                - ReadWriteOnce\n              storageClassName: \"standard\"\n              resources:\n                requests:\n                  storage: 8Gi\n```\n\n----------------------------------------\n\nTITLE: Running the NiFiKop migration script\nDESCRIPTION: Command to execute the migration script, specifying the NiFiKop resource type and Kubernetes namespace to migrate resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nnpm start -- --type=<NIFIKOP_RESOURCE> --namespace=<K8S_NAMESPACE>\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Nodes with YAML in NiFiKop\nDESCRIPTION: This YAML example demonstrates how to configure NiFi nodes with different settings. It shows both a simple node using a nodeConfigGroup and a more complex node with custom resource requirements and storage configuration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/5_references/1_nifi_cluster/4_node.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n    - id: 0\n      # nodeConfigGroup can be used to ease the node configuration, if set only the id is required\n      nodeConfigGroup: \"default_group\"\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        nifiProperties:\n          overrideConfigs: |\n            nifi.ui.banner.text=NiFiKop - Node 0\n      # node configuration\n#       nodeConfig:\n    - id: 2\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        overrideConfigs: |\n          nifi.ui.banner.text=NiFiKop - Node 2\n      # node configuration\n      nodeConfig:\n        resourcesRequirements:\n          limits:\n            cpu: \"2\"\n            memory: 3Gi\n          requests:\n            cpu: \"1\"\n            memory: 1Gi\n        storageConfigs:\n          # Name of the storage config, used to name PV to reuse into sidecars for example.\n          - name: provenance-repository\n            # Path where the volume will be mount into the main nifi container inside the pod.\n            mountPath: \"/opt/nifi/provenance_repository\"\n            # Metadata to attach to the PVC that gets created\n            metadata:\n              labels:\n                my-label: my-value\n              annotations:\n                my-annotation: my-value\n            # Kubernetes PVC spec\n            # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n            pvcSpec:\n              accessModes:\n                - ReadWriteOnce\n              storageClassName: \"standard\"\n              resources:\n                requests:\n                  storage: 8Gi\n```\n\n----------------------------------------\n\nTITLE: Define NifiDataflow Resource (YAML)\nDESCRIPTION: Demonstrates the YAML structure for defining a `NifiDataflow` custom resource. It configures a specific dataflow by referencing its source in a NiFi registry and links it to a NiFi cluster, registry client, and parameter context, specifying deployment details like the parent group, position, sync mode, and update strategy. Requires a running Nifikop operator, NiFi cluster, and NiFi registry client.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/5_references/5_nifi_dataflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  flowPosition:\n    posX: 0\n    posY: 0\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: squidflow\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: nifikop\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Installing cert-manager Helm Chart with Helm 3\nDESCRIPTION: Sets up cert-manager in Kubernetes using Helm 3, including CRD installation, adding the Jetstack Helm repo, updating repositories, creating namespace, and deploying cert-manager with the specified version. It ensures certificate management capabilities for secured NiFi deployments.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/2_setup/1_getting_started.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Install CustomResourceDefinitions first\nkubectl apply --validate=false -f \\\n   https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.crds.yaml\n\n# Add the jetstack helm repo\nhelm repo add jetstack https://charts.jetstack.io\nhelm repo update\n\n# You have to create the namespace before executing following command\nhelm install cert-manager \\\n    --namespace cert-manager \\\n    --version v1.7.2 jetstack/cert-manager\n```\n\n----------------------------------------\n\nTITLE: Formal Commit Message Layout - Text\nDESCRIPTION: This snippet formally expresses the expected formatting template for commit messages, ensuring that all essential information (subsystem, change summary, rationale, and an optional footer) is included in a clear and consistent manner. This aids in project maintainability and collaboration by making messages easy to review and reference. Ensure the first line is the subject and is no longer than 70 characters, with body and footer lines wrapped at 80 characters.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/CONTRIBUTING.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n<subsystem>: <what changed>\n<BLANK LINE>\n<why this change was made>\n<BLANK LINE>\n<footer>\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFiKop Operator with Helm - Bash\nDESCRIPTION: This snippet deploys the NiFiKop operator to the 'nifi' namespace using Helm by referencing the OCI chart URI and setting specific resource and image parameters. Before running this command, the target namespace must exist and any necessary CRDs must already be installed (if --skip-crds was used). Helm must be installed and configured. The set parameters control the operator image version as well as its resource requests and limits to ensure predictable deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/2_deploy_nifikop/1_quick_start.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# You have to create the namespace before executing following command\nhelm install nifikop \\\n    oci://ghcr.io/konpyutaika/helm-charts/nifikop \\\n    --namespace=nifi \\\n    --version 0.16.0 \\\n    --set image.tag=v0.16.0-release \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set namespaces={\"nifi\"}\n```\n\n----------------------------------------\n\nTITLE: Installing cert-manager directly via kubectl\nDESCRIPTION: This snippet demonstrates how to deploy cert-manager by applying the official YAML manifest directly to your Kubernetes cluster, ensuring the necessary CustomResourceDefinitions and cert-manager components are installed. It requires kubectl access and the appropriate permissions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/2_deploy_nifikop/1_quick_start.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install the CustomResourceDefinitions and cert-manager itself\nkubectl apply -f \\\n    https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.yaml\n```\n\n----------------------------------------\n\nTITLE: Applying NiFiKop CRDs Manually in Kubernetes\nDESCRIPTION: Commands to manually deploy the NiFiKop Custom Resource Definitions when using the --skip-crds option with Helm. This includes all necessary CRDs for NiFi clusters, users, groups, dataflows, parameter contexts, registry clients, node group autoscalers, and connections.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifinodegroupautoscalers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificonnections.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing Zookeeper Cluster via Helm with Custom Resources\nDESCRIPTION: This Helm install command deploys a 3-replica Zookeeper cluster in the 'zookeeper' namespace with specified resource requests and limits, setting the storage class to 'standard' and enabling network policies. Replace the 'storageClass' value with your own to match your storage configuration. Dependencies include Helm and a Kubernetes cluster with RBAC permissions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nhelm install zookeeper bitnami/zookeeper \\\n    --namespace=zookeeper \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set global.storageClass=standard \\\n    --set networkPolicy.enabled=true \\\n    --set replicaCount=3 \\\n    --create-namespace\n```\n\n----------------------------------------\n\nTITLE: Running NiFiKop locally\nDESCRIPTION: Command to run the NiFiKop operator locally during development after deploying the CRDs.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/6_contributing/1_developer_guide.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmake run\n```\n\n----------------------------------------\n\nTITLE: Defining NifiCluster with Busybox Init Container (v0.14.1)\nDESCRIPTION: Example YAML configuration for a `NifiCluster` resource using `busybox:1.34.0` as the `initContainerImage`. This configuration was valid for Nifikop v0.14.1 but requires modification when upgrading to v0.15.0 or later, as the init container now requires a bash shell.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/7_upgrade_guides/2_v0.14.1_to_v0.15.0.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  initContainerImage:\n    repository: busybox\n    tag: \"1.34.0\"\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop Helm Chart with Release Name in Bash\nDESCRIPTION: Example of a basic Helm install command specifying a custom release name. This command uses bash syntax to deploy the NiFiKop chart from the konpyutaika repository without additional parameters, suitable for default installation scenarios.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/2_setup/3_install/1_customizable_install_with_helm.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhelm install <release name> konpyutaika/nifikop\n```\n\n----------------------------------------\n\nTITLE: Verifying Resource Creation After Scale Up (Shell)\nDESCRIPTION: This shell command and its output show how to verify that the NiFiKop operator has created the corresponding Kubernetes resources (Pod, ConfigMap, PVC) for the newly added node (with nodeId=25) after applying the updated configuration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\nkubectl get pods,configmap,pvc -l nodeId=25\nNAME                          READY   STATUS    RESTARTS   AGE\npod/simplenifi-25-nodem5jh4   1/1     Running   0          11m\n\nNAME                             DATA   AGE\nconfigmap/simplenifi-config-25   7      11m\n\nNAME                                               STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\npersistentvolumeclaim/simplenifi-25-storagehwn24   Bound    pvc-7da86076-728e-11ea-846d-42010a8400f2   10Gi       RWO            standard       11m\n```\n\n----------------------------------------\n\nTITLE: Packaging Helm Chart\nDESCRIPTION: Creates a packaged Helm chart archive for distribution or repository hosting, using the Makefile target. Necessary for sharing or deploying via Helm repositories.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/6_contributing/1_developer_guide.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nmake helm-package\n```\n\n----------------------------------------\n\nTITLE: Cloning the NiFiKop Repository in Bash\nDESCRIPTION: Commands to clone the NiFiKop repository from GitHub and navigate to the project directory.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/6_contributing/1_developer_guide.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/konpyutaika/nifikop.git\ncd nifikop\n```\n\n----------------------------------------\n\nTITLE: Updating NifiUser certificates on resource updates\nDESCRIPTION: This code handles the re-issuance or update of certificates when the associated NifiUser resource undergoes modification, maintaining security standards. It depends on certificate management libraries and triggers on resource change events.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/CHANGELOG.md#_snippet_4\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.k8s.io/v1alpha1\nkind: NifiUser\nmetadata:\n  name: user-cert\nspec:\n  certificate: |\n    -----BEGIN CERTIFICATE-----\n    ...\n    -----END CERTIFICATE-----\n```\n\n----------------------------------------\n\nTITLE: Installing Zookeeper on OpenShift with Security Context\nDESCRIPTION: Helm command for installing Zookeeper on OpenShift with the required security context settings (runAsUser and fsGroup) based on the extracted OpenShift UID.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nhelm install zookeeper oci://registry-1.docker.io/bitnamicharts/zookeeper \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set global.storageClass=standard \\\n    --set networkPolicy.enabled=true \\\n    --set replicaCount=3 \\\n    --set containerSecurityContext.runAsUser=$zookeper_uid \\\n    --set podSecurityContext.fsGroup=$zookeper_uid\n```\n\n----------------------------------------\n\nTITLE: NiFiRegistryClient Controller Reconcile Loop\nDESCRIPTION: This snippet illustrates the reconcile loop of the NiFiRegistryClient controller. It manages the lifecycle of NiFi registry clients by continuously reconciling the desired state with the actual state, ensuring proper deployment, updates, and cleanup within Kubernetes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/3_manage_dataflows/0_design_principles.md#_snippet_3\n\nLANGUAGE: Markdown\nCODE:\n```\n![NiFi registry client's reconcile loop](/img/1_concepts/2_design_principes/registry_client_reconcile_loop.jpeg)\n```\n\n----------------------------------------\n\nTITLE: Initializing Node.js Project and Installing Dependencies - bash\nDESCRIPTION: Initializes a new Node.js project and installs the required dependencies '@kubernetes/client-node@0.16.3' and 'minimist@1.2.6'. Requires the user to have Node.js version 15.3.0+ and npm version 7.0.14+ installed on the environment. On completion, the project will include all libraries necessary for interacting with the Kubernetes API and parsing command-line arguments.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm init -y\nnpm install @kubernetes/client-node@0.16.3 minimist@1.2.6\n```\n\n----------------------------------------\n\nTITLE: Automating NiFi Flow Re-encryption with InitContainer (YAML)\nDESCRIPTION: Provides a Kubernetes initContainer configuration to automate the re-encryption of NiFi flow configurations during the upgrade. It uses the apache/nifi-toolkit image to execute the encrypt-config.sh tool, extracting the sensitive properties key and applying the new algorithm to both flow.json.gz and flow.xml.gz. Requires mounting the NiFi configuration and data volumes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/7_upgrade_guides/4_v1.3.1_to_v1.4.0.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ninitContainers:\n  - image: \"apache/nifi-toolkit:latest\"\n    name: nifi-toolkit\n    imagePullPolicy: Always\n    command:\n      - \"sh\"\n      - \"-c\"\n      - \"NIFI_SENSITIVE_PROPS_KEY=$(grep 'nifi.sensitive.props.key' /opt/nifi/nifi-current/conf/nifi.properties | cut -d'=' -f2) && bin/encrypt-config.sh -n /opt/nifi/nifi-current/conf/nifi.properties -f /opt/nifi/data/flow.json.gz -x -A NIFI_PBKDF2_AES_GCM_256 -s $NIFI_SENSITIVE_PROPS_KEY; bin/encrypt-config.sh -n /opt/nifi/nifi-current/conf/nifi.properties -f /opt/nifi/data/flow.xml.gz -x -A NIFI_PBKDF2_AES_GCM_256 -s $NIFI_SENSITIVE_PROPS_KEY\"\n    volumeMounts:\n      - name: data\n        mountPath: /opt/nifi/data\n      - name: conf\n        mountPath: /opt/nifi/nifi-current/conf\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFi Cluster on OpenShift (after config)\nDESCRIPTION: This command deploys a NiFi cluster on OpenShift, using the modified configuration file (config/samples/openshift.yaml). It assumes that the file has been correctly adjusted to account for user ID specific to the OpenShift environment and then creates the NiFi resources within the 'nifi' namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create -n nifi -f config/samples/openshift.yaml\n```\n\n----------------------------------------\n\nTITLE: Install NiFiKop Helm Chart on OpenShift (bash)\nDESCRIPTION: Installs the NiFiKop operator Helm chart into the nifi namespace, setting the runAsUser to the previously extracted UID. This is necessary for deploying NiFiKop on OpenShift clusters with restrictive Security Context Constraints (SCCs). Requires Helm 3+, the extracted $uid, and the nifi namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/2_deploy_nifikop/1_quick_start.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop \\\n    nifikop \\\n    --namespace=nifi \\\n    --version 1.1.1 \\\n    --set image.tag=v1.1.1-release \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set namespaces={\"nifi\"} \\\n    --set runAsUser=$uid\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop with Helm in Dry Run Mode\nDESCRIPTION: Command to simulate the installation of NiFiKop with custom logging level and namespace parameters. Useful for validating configuration before actual deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop \\\n    --dry-run \\\n    --set logLevel=Debug \\\n    --set namespaces={\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiDataflow Resource in YAML\nDESCRIPTION: Example YAML configuration for deploying a NiFi dataflow with specified parent process group, bucket, flow details, synchronization mode, and references to cluster, registry client, and parameter context.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/5_references/5_nifi_dataflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  flowPosition:\n    posX: 0\n    posY: 0\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: squidflow\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: nifikop\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio VirtualService for HTTP in NiFi\nDESCRIPTION: This YAML configuration defines an Istio VirtualService that routes all requests intercepted by the Gateway to the NiFi service on port 8080.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: nifi-vs\nspec:\n  gateways:\n  - nifi-gateway\n  hosts:\n  - nifi.my-domain.com\n  http:\n  - match:\n    - uri:\n        prefix: /\n    route:\n    - destination:\n        host: nifi\n        port:\n          number: 8080\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiParameterContext CRD in YAML\nDESCRIPTION: This YAML manifest defines a `NifiParameterContext` Custom Resource for NiFiKop. It links to a NiFi cluster (`clusterRef`), optionally references Kubernetes Secrets containing sensitive parameters (`secretRefs`), and lists non-sensitive parameters (`parameters`) with their names, values, and descriptions. This resource defines configuration parameters for associated `NifiDataflow` resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/3_tasks/3_nifi_dataflow.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\n  namespace: demo\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      value: toto\n      description: toto\n```\n\n----------------------------------------\n\nTITLE: Installing KEDA with Helm\nDESCRIPTION: Commands for adding the KEDA Helm repository, updating it, and installing KEDA in a dedicated namespace using Helm.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nhelm repo add kedacore https://kedacore.github.io/charts\n```\n\nLANGUAGE: console\nCODE:\n```\nhelm repo update\n```\n\nLANGUAGE: console\nCODE:\n```\nkubectl create namespace keda\nhelm install keda kedacore/keda --namespace keda\n```\n\n----------------------------------------\n\nTITLE: Getting the Status of a Nifikop Helm Deployment\nDESCRIPTION: Shows how to use the `helm status` command to check the deployment status and details of a specific Helm release, using 'nifikop' as the example release name.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nhelm status nifikop\n```\n\n----------------------------------------\n\nTITLE: Install NiFiKop Operator Helm Chart (bash)\nDESCRIPTION: Installs the NiFiKop operator Helm chart from the specified OCI registry URL into the nifi namespace. It sets the chart version, image tag, resource requests/limits, and target namespaces for the operator. Requires Helm 3+ and the ability to pull charts from the OCI registry.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/2_deploy_nifikop/1_quick_start.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop \\\n    oci://ghcr.io/konpyutaika/helm-charts/nifikop \\\n    --namespace=nifi \\\n    --version 1.13.0 \\\n    --set image.tag=v1.13.0-release \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set namespaces={\"nifi\"}\n```\n\n----------------------------------------\n\nTITLE: Listing Managed NiFi User Groups with kubectl (Console)\nDESCRIPTION: This console command retrieves the list of managed NiFi user groups in the configured namespace using kubectl for the NifiKop operator. It displays group names and their ages, validating that groups like 'managed-admins', 'managed-nodes', and 'managed-readers' have been correctly created by the operator. Prerequisites include kubectl installed and set up with access to the correct Kubernetes cluster and namespace. Input is the standard Kubernetes API for custom resource definitions; output is a tabular listing of the NiFi user groups. Ensure that the correct namespace ('nifikop' in this example) is specified.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/3_manage_nifi/2_manage_users_and_accesses/3_managed_groups.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nkubectl get -n nifikop nifiusergroups.nifi.konpyutaika.com \nNAME              AGE\nmanaged-admins    6d7h\nmanaged-nodes     6d7h\nmanaged-readers   6d7h\n\n```\n\n----------------------------------------\n\nTITLE: Getting Zookeeper UID/GID on OpenShift\nDESCRIPTION: This bash command retrieves the Zookeeper's allowed UID/GID from the OpenShift namespace using `kubectl get namespace` command with `jsonpath` formatting. This UID/GID is later used to configure Zookeeper deployment correctly within OpenShift.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nzookeper_uid=$(kubectl get namespace zookeeper -o=jsonpath='{.metadata.annotations.openshift\\.io/sa\\.scc\\.supplemental-groups}' | sed 's///10000$//' | tr -d '[:space:]')\n```\n\n----------------------------------------\n\nTITLE: Uninstalling Helm Chart Deployment\nDESCRIPTION: Command to delete the specific Helm release from Kubernetes, cleaning up all associated components and Helm records.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nhelm del nifikop\n```\n\n----------------------------------------\n\nTITLE: Get Pods, ConfigMaps, and PVCs with kubectl\nDESCRIPTION: This command retrieves information about pods, configmaps, and persistent volume claims (PVCs) that have a specific label. The `-l nodeId=25` filter selects resources with the label `nodeId` equal to `25`. This command is run after scaling up to check if the resources for the new node have been created correctly.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nkubectl get pods,configmap,pvc -l nodeId=25\n```\n\n----------------------------------------\n\nTITLE: Creating a NifiUser resource in YAML\nDESCRIPTION: YAML configuration for creating a NifiUser resource that specifies user identity, cluster reference, certificate requirements, and access policies. This configuration creates a user named 'aguitton' with read access to process groups data.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/3_tasks/4_nifi_user_group.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiUser\nmetadata:\n  name: aguitton\nspec:\n  # identity field is use to define the user identity on NiFi cluster side,\n  #\tit use full when the user's name doesn't suite with Kubernetes resource name.\n  identity: alexandre.guitton@konpyutaika.com\n  # Contains the reference to the NifiCluster with the one the registry client is linked.\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  # Whether or not the the operator also include a Java keystore format (JKS) with you secret\n  includeJKS: false\n  # Whether or not a certificate will be created for this user.\n  createCert: false\n  # defines the list of access policies that will be granted to the group.\n  accessPolicies:\n      # defines the kind of access policy, could be \"global\" or \"component\".\n    - type: component\n      # defines the kind of action that will be granted, could be \"read\" or \"write\"\n      action: read\n      # resource defines the kind of resource targeted by this access policies, please refer to the following page:\n      #\thttps://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#access-policies\n      resource: /data\n      # componentType is used if the type is \"component\", it's allow to define the kind of component on which is the\n      # access policy\n      componentType: \"process-groups\"\n      # componentId is used if the type is \"component\", it's allow to define the id of the component on which is the\n      # access policy\n      componentId: \"\"\n```\n\n----------------------------------------\n\nTITLE: Storage Configuration for NiFi Data Persistence\nDESCRIPTION: YAML configuration for data persistence in a NiFi cluster, defining various storage configurations for logs, data, extensions, flowfile repository, configurations, content repository, and provenance repository with different storage requirements.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n...\nstorageConfigs:\n  - mountPath: /opt/nifi/nifi-current/logs\n    name: logs\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/data\n    name: data\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 50Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/extensions\n    name: extensions-repository\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 5Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/flowfile_repository\n    name: flowfile-repository\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/nifi-current/conf\n    name: conf\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 5Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/content_repository\n    name: content-repository\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 500Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/provenance_repository\n    name: provenance-repository\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 500Gi\n```\n\n----------------------------------------\n\nTITLE: Creating a NiFi Connection Between Dataflows with YAML\nDESCRIPTION: This snippet demonstrates how to define a NiFi connection between two dataflows ('input' and 'output') as a Kubernetes CRD. It specifies source and destination dataflows, connection parameters like backpressure thresholds, flow file expiration, bends for visualization, label placement via labelIndex, and the update strategy. This establishes the flow of data between the two dataflows.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/4_manage_connections/1_deploy_connection.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiConnection\nmetadata:\n  name: connection\n  namespace: nifikop\nspec:\n  source:\n    name: input\n    namespace: nifikop\n    subName: output\n    type: dataflow\n  destination:\n    name: output\n    namespace: nifikop\n    subName: input\n    type: dataflow\n  configuration:\n    backPressureDataSizeThreshold: 100 GB\n    backPressureObjectThreshold: 10000\n    flowFileExpiration: 1 hour\n    labelIndex: 0\n    bends:\n      - posX: 550\n        posY: 550\n      - posX: 550\n        posY: 440\n      - posX: 550\n        posY: 88\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Deploying ServiceMonitor for NiFi Cluster - YAML\nDESCRIPTION: Configures a ServiceMonitor for a NiFi cluster, enabling Prometheus to scrape /metrics from all NiFi endpoints in the 'clusters' namespace. Relabeling is used to add key Kubernetes pod labels as Prometheus labels. Modify 'selector' and 'namespaceSelector' as needed to fit your deployment. Apply using 'kubectl apply -f'.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: cluster\n  namespace: monitoring-system\n  labels:\n    app: nifi-cluster\n    nifi_cr: cluster\nspec:\n  selector:\n    matchLabels:\n      app: nifi\n      nifi_cr: cluster\n  namespaceSelector:\n    matchNames:\n      - clusters\n  endpoints:\n    - interval: 10s\n      port: prometheus\n      path: /metrics\n      honorLabels: true\n      relabelings:\n        - sourceLabels: [__meta_kubernetes_pod_ip]\n          separator: ;\n          regex: (.*)\n          targetLabel: pod_ip\n          replacement: $1\n          action: replace\n        - sourceLabels: [__meta_kubernetes_pod_label_nodeId]\n          separator: ;\n          regex: (.*)\n          targetLabel: nodeId\n          replacement: $1\n          action: replace\n        - sourceLabels: [__meta_kubernetes_pod_label_nifi_cr]\n          separator: ;\n          regex: (.*)\n          targetLabel: nifi_cr\n          replacement: $1\n          action: replace\n```\n\n----------------------------------------\n\nTITLE: Applying Nifikop CRDs Manually using kubectl\nDESCRIPTION: Applies Nifikop Custom Resource Definitions (CRDs) directly to a Kubernetes cluster using `kubectl apply` from raw GitHub URLs. This is necessary if the Helm chart installation is performed with the `--skip-crds` flag. Requires `kubectl` and internet access to fetch the CRD manifests.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifinodegroupautoscalers.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Parameter Contexts in YAML for NifiKop\nDESCRIPTION: Example YAML definitions for NifiParameterContext resources, showing both a parent context and a child context that inherits from the parent. The examples demonstrate how to specify parameters, reference secrets, and link to NiFi clusters.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/5_references/4_nifi_parameter_context.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      description: toto\n      sensistive: true\n---\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle-child\nspec:\n  description: \"It is a child test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  inheritedParameterContexts:\n    - name: dataflow-lifecycle\n  parameters:\n    - name: test\n      value: toto-child\n      description: tutu (child)\n```\n\n----------------------------------------\n\nTITLE: Defining a NiFi Connection Resource in YAML\nDESCRIPTION: Defines a `NifiConnection` custom resource named 'connection' in the 'nifikop' namespace using YAML for Kubernetes. It establishes a connection between the 'input' dataflow's 'output' port and the 'output' dataflow's 'input' port, requiring these dataflows to exist first. Includes configuration for back pressure, flow file expiration, visual label index, connection bends, and update strategy.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/4_manage_connections/1_deploy_connection.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiConnection\nmetadata:\n  name: connection\n  namespace: nifikop\nspec:\n  source:\n    name: input\n    namespace: nifikop\n    subName: output\n    type: dataflow\n  destination:\n    name: output\n    namespace: nifikop\n    subName: input\n    type: dataflow\n  configuration:\n    backPressureDataSizeThreshold: 100 GB\n    backPressureObjectThreshold: 10000\n    flowFileExpiration: 1 hour\n    labelIndex: 0\n    bends:\n      - posX: 550\n        posY: 550\n      - posX: 550\n        posY: 440\n      - posX: 550\n        posY: 88\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Initializing Node.js Project for NiFiKop Migration (Bash)\nDESCRIPTION: Initializes a new Node.js project using `npm init -y` and installs the required dependencies (`@kubernetes/client-node` version 0.16.3 and `minimist` version 1.2.6) using `npm install`. This setup is necessary to run the migration script.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/7_upgrade/1_v0.7.x_to_v0.8.0.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm init -y\nnpm install @kubernetes/client-node@0.16.3 minimist@1.2.6\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Nodes with YAML in NiFiKop\nDESCRIPTION: This YAML snippet demonstrates how to configure NiFi nodes within a NiFiKop deployment. It shows two node configurations: a simple one using nodeConfigGroup and a more complex one with custom resource requirements and storage configurations.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/5_references/1_nifi_cluster/4_node.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n    - id: 0\n      # nodeConfigGroup can be used to ease the node configuration, if set only the id is required\n      nodeConfigGroup: \"default_group\"\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        nifiProperties:\n          overrideConfigs: |\n            nifi.ui.banner.text=NiFiKop - Node 0\n      # node configuration\n#       nodeConfig:\n    - id: 2\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        overrideConfigs: |\n          nifi.ui.banner.text=NiFiKop - Node 2\n      # node configuration\n      nodeConfig:\n        resourcesRequirements:\n          limits:\n            cpu: \"2\"\n            memory: 3Gi\n          requests:\n            cpu: \"1\"\n            memory: 1Gi\n        storageConfigs:\n          # Name of the storage config, used to name PV to reuse into sidecars for example.\n          - name: provenance-repository\n            # Path where the volume will be mount into the main nifi container inside the pod.\n            mountPath: \"/opt/nifi/provenance_repository\"\n            # Kubernetes PVC spec\n            # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n            pvcSpec:\n              accessModes:\n                - ReadWriteOnce\n              storageClassName: \"standard\"\n              resources:\n                requests:\n                  storage: 8Gi\n```\n\n----------------------------------------\n\nTITLE: Let's Encrypt Issuer Configuration\nDESCRIPTION: This YAML configuration defines a Kubernetes Issuer resource to obtain TLS certificates from Let's Encrypt using the ACME protocol. It specifies the email for notifications, the Let's Encrypt staging server for testing (or the production server), and a reference to a secret where the account's private key is stored. It also defines an HTTP01 solver using ingress for certificate validation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/blog/2020-06-30-secured_nifi_cluster_on_gcp_with_external_dns.md#_snippet_14\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: cert-manager.io/v1alpha2\nkind: Issuer\nmetadata:\n  name: letsencrypt-staging\nspec:\n  acme:\n    # You must replace this email address with your own.\n    # Let's Encrypt will use this to contact you about expiring\n    # certificates, and issues related to your account.\n    email: <your email>\n    server: https://acme-staging-v02.api.letsencrypt.org/directory\n    privateKeySecretRef:\n      # Secret resource used to store the account's private key.\n      name: example-issuer-account-key\n    # Add a single challenge solver, HTTP01 using nginx\n    solvers:\n      - http01:\n          ingress:\n            ingressTemplate:\n              metadata:\n                annotations:\n                  \"external-dns.alpha.kubernetes.io/ttl\": \"5\"\n```\n\n----------------------------------------\n\nTITLE: Defining Nifi ListenersConfig in YAML\nDESCRIPTION: This YAML snippet demonstrates the configuration structure for `listenersConfig` in a Nifikop-managed Nifi cluster. It specifies various internal listeners (https, cluster, s2s, prometheus, load-balance, and a custom one) with their types, names, and container ports. It also includes the configuration for SSL secrets, referencing a secret name and indicating that the secrets should be created automatically, likely via cert-manager.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/5_references/1_nifi_cluster/6_listeners_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  listenersConfig:\n    internalListeners:\n      - type: \"https\"\n        name: \"https\"\n        containerPort: 8443\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n      - type: \"prometheus\"\n        name: \"prometheus\"\n        containerPort: 9090\n      - type: \"load-balance\"\n        name: \"load-balance\"\n        containerPort: 6342\n      - name: \"my-custom-listener-port\"\n        containerPort: 1234\n        protocol: \"TCP\"\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFiKop Operator using Helm (Bash)\nDESCRIPTION: Installs the NiFiKop operator using its Helm chart from the OCI registry. This command specifies the release name, target namespace ('nifi', which must exist), chart version, operator image tag, resource requests/limits, and the namespaces the operator should watch.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/2_deploy_nifikop/1_quick_start.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n# You have to create the namespace before executing following command\nhelm install nifikop \\\n    oci://ghcr.io/konpyutaika/helm-charts/nifikop \\\n    --namespace=nifi \\\n    --version 1.11.2 \\\n    --set image.tag=v1.11.2-release \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set namespaces={\\\"nifi\\\"}\n```\n\n----------------------------------------\n\nTITLE: Setting Operator Name Environment Variable (Bash)\nDESCRIPTION: Sets the 'OPERATOR_NAME' environment variable to 'nifi-operator'. This variable is used when running the operator locally via the Go binary to identify the operator instance.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/6_contributing/1_developer_guide.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport OPERATOR_NAME=nifi-operator\n```\n\n----------------------------------------\n\nTITLE: Defining Istio Gateway for HTTPS Access to NiFi Cluster\nDESCRIPTION: This YAML configures an Istio Gateway that accepts HTTPS traffic on port 443 and transforms it to HTTP. It uses TLS in SIMPLE mode with a specified credential secret for the domain.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: nifi-gateway\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 443\n      name: https\n      protocol: HTTPS\n    tls:\n      mode: SIMPLE\n      credentialName: my-secret\n    hosts:\n    - nifi.my-domain.com\n```\n\n----------------------------------------\n\nTITLE: Checking Pods Status in NiFi Namespace to Confirm Secured NiFi Cluster Deployment\nDESCRIPTION: This kubectl command lists all pods running in the nifi namespace including the NiFiKop operator and NiFi cluster nodes. Pod readiness and running status indicates successful deployment and startup of the secured NiFi cluster components.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/blog/2020-05-20-secured_nifi_cluster_on_gcp.md#_snippet_10\n\nLANGUAGE: console\nCODE:\n```\nkubectl get pods -n nifi\nNAME                             READY   STATUS    RESTARTS   AGE\nnifikop-849fc8548f-ss6w4         1/1     Running   0          28h\nsecurednificluster-0-node9tqff   1/1     Running   0          5m52s\nsecurednificluster-1-nodew9tsd   1/1     Running   0          6m30s\nsecurednificluster-2-nodemlxs8   1/1     Running   0          6m28s\nsecurednificluster-3-nodeckw8p   1/1     Running   0          6m26s\nsecurednificluster-4-nodewzjt7   1/1     Running   0          6m24s\n```\n\n----------------------------------------\n\nTITLE: Installing Nifikop Helm Chart with Set Parameters\nDESCRIPTION: Installs the Nifikop Helm chart named `nifikop` from the `konpyutaika` repository, overriding default values by setting the `namespaces` parameter.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop --set namespaces={\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio Gateway for HTTPS (YAML)\nDESCRIPTION: This YAML snippet defines an Istio Gateway that accepts HTTPS traffic. It configures the gateway to listen on port 443 with HTTPS protocol, uses `SIMPLE` TLS mode and specifies the credential name (`my-secret`) and host (nifi.my-domain.com). This part handles the HTTPS termination at the gateway level and forwards to HTTP inside the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: nifi-gateway\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 443\n      name: https\n      protocol: HTTPS\n    tls:\n      mode: SIMPLE\n      credentialName: my-secret\n    hosts:\n    - nifi.my-domain.com\n```\n```\n\n----------------------------------------\n\nTITLE: Verifying NiFiKop Deployment\nDESCRIPTION: Command to check if the NiFiKop operator pods are running correctly in the specified namespace after deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/6_contributing/1_developer_guide.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get pods -n nifikop\n```\n\n----------------------------------------\n\nTITLE: Purging a Deleted Helm Release\nDESCRIPTION: Permanently removes the record of a deleted Helm release (e.g., `nifikop`), allowing the release name to be reused for future deployments. Use with caution.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nhelm delete --purge nifikop\n```\n\n----------------------------------------\n\nTITLE: Overriding default naming for NifiUserGroup\nDESCRIPTION: This code enables users to set a custom 'Identity' for NifiUserGroup, overriding default naming conventions. It requires access to the 'NifiUserGroup' resource specifications and influences how user groups are identified within NiFi environments.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/CHANGELOG.md#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.k8s.io/v1alpha1\nkind: NifiUserGroup\nmetadata:\n  name: custom-usergroup\nspec:\n  identity: custom-identity-name\n```\n\n----------------------------------------\n\nTITLE: Cloning NiFiKop Repository (Bash)\nDESCRIPTION: Clones the NiFiKop project from GitHub and navigates into the project directory. This is the initial step for setting up the development environment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/6_contributing/1_developer_guide.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/konpyutaika/nifikop.git\ncd nifikop\n```\n\n----------------------------------------\n\nTITLE: Verifying NiFiKop Operator Pod Status Using Kubectl - Bash\nDESCRIPTION: This command lists the pods running in the 'nifikop' namespace and shows their status to verify the NiFiKop operator deployment has started successfully. It requires 'kubectl' configured with access to the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/6_contributing/1_developer_guide.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get pods -n nifikop\nNAME                                                READY   STATUS    RESTARTS   AGE\nskeleton-nifikop-8946b89dc-4cfs9   1/1     Running   0          7m45s\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Nodes with YAML in NiFiKop\nDESCRIPTION: This snippet demonstrates how to configure multiple NiFi nodes with different settings, including node IDs, configuration groups, custom properties, resource requirements, and persistent storage configurations.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/5_references/1_nifi_cluster/4_node.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n    - id: 0\n      # nodeConfigGroup can be used to ease the node configuration, if set only the id is required\n      nodeConfigGroup: \"default_group\"\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        nifiProperties:\n          overrideConfigs: |\n            nifi.ui.banner.text=NiFiKop - Node 0\n      # node configuration\n#       nodeConfig:\n    - id: 2\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        overrideConfigs: |\n          nifi.ui.banner.text=NiFiKop - Node 2\n      # node configuration\n      nodeConfig:\n        resourcesRequirements:\n          limits:\n            cpu: \"2\"\n            memory: 3Gi\n          requests:\n            cpu: \"1\"\n            memory: 1Gi\n        storageConfigs:\n          # Name of the storage config, used to name PV to reuse into sidecars for example.\n          - name: provenance-repository\n            # Path where the volume will be mount into the main nifi container inside the pod.\n            mountPath: \"/opt/nifi/provenance_repository\"\n            # Metadata to attach to the PVC that gets created\n            metadata:\n              labels:\n                my-label: my-value\n              annotations:\n                my-annotation: my-value\n            # Kubernetes PVC spec\n            # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n            pvcSpec:\n              accessModes:\n                - ReadWriteOnce\n              storageClassName: \"standard\"\n              resources:\n                requests:\n                  storage: 8Gi\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Identity Mapping Using Shell for OIDC\nDESCRIPTION: This shell snippet configures the identity mapping properties in the nifi.properties file to enable multiple identity provider support for OpenId Connect authentication in Apache NiFi. It defines a regex pattern to extract the common name (CN) from distinguished names, sets the value to be extracted accordingly, and specifies that no transformation is applied. Dependencies include Apache NiFi with OIDC enabled and access to the nifi.properties configuration file. These properties influence how user identities are mapped from OIDC tokens to NiFi user identities.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/5_users_authentication/1_oidc.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nnifi.security.identity.mapping.pattern.dn=CN=([^,]*)(?:, (?:O|OU)=.*)?\nnifi.security.identity.mapping.value.dn=$1\nnifi.security.identity.mapping.transform.dn=NONE\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Identity Mapping Properties (Shell)\nDESCRIPTION: Specifies recommended identity mapping properties for NiFi's `nifi.properties` file to handle multiple identity providers, extracting the Common Name (CN) from a distinguished name (DN). These properties define a regular expression pattern for matching user identities, the capture group value to extract, and the transformation method (none in this case). These settings are crucial for correctly mapping OIDC identities to NiFi users.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/5_users_authentication/1_oidc.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nnifi.security.identity.mapping.pattern.dn=CN=([^,]*)(?:, (?:O|OU)=.*)?\nnifi.security.identity.mapping.value.dn=$1\nnifi.security.identity.mapping.transform.dn=NONE\n```\n\n----------------------------------------\n\nTITLE: Defining Internal Listeners for NiFi Cluster in Kubernetes YAML\nDESCRIPTION: Configures the internal listeners of the NiFi cluster by specifying the listener type, name, and container port in a Kubernetes CustomResource YAML. These listeners manage different NiFi protocols such as HTTPS for UI access, cluster communication, Site-to-Site transfers, load balancing, and Prometheus metrics. Additional user-defined ports without a type can be added for custom use cases. This snippet assumes knowledge of the NiFi listener concepts and Kubernetes CRD structure.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    - type: \"https\"\n      name: \"https\"\n      containerPort: 8443\n    - type: \"cluster\"\n      name: \"cluster\"\n      containerPort: 6007\n    - type: \"s2s\"\n      name: \"s2s\"\n      containerPort: 10000\n    - type: \"prometheus\"\n      name: \"prometheus\"\n      containerPort: 9090\n    - type: \"load-balance\"\n      name: \"load-balance\"\n      containerPort: 6342\n```\n\n----------------------------------------\n\nTITLE: Installing Helm Chart with Custom Release Name (bash)\nDESCRIPTION: Installs the `konpyutaika/nifikop` Helm chart using `helm install`, allowing the user to specify a custom `<release name>` for the deployment. This is the standard command for deploying a chart under a unique name. Requires Helm installed and configured for the target Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nhelm install <release name> konpyutaika/nifikop\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFiKop CRDs to Kubernetes\nDESCRIPTION: Commands to apply the Custom Resource Definitions required by NiFiKop to a Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/6_contributing/1_developer_guide.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\n```\n\n----------------------------------------\n\nTITLE: Creating a NiFiUser Resource for Certificates Sharing\nDESCRIPTION: This console snippet applies a NiFiUser custom resource to generate a user certificate signed by the CA, specifying a secret to store credentials securely. The secret includes CA cert, user cert, and user key, which can be mounted into pods or exported locally.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/3_tasks/2_security/1_ssl.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\ncat << EOF | kubectl apply -n nifi -f -\napiVersion:  nifi.konpyutaika.com/v1alpha1\nkind: NifiUser\nmetadata:\n  name: example-client\n  namespace: nifi\nspec:\n  clusterRef:\n    name: nifi\n  secretName: example-client-secret\nEOF\n```\n\n----------------------------------------\n\nTITLE: Installing cert-manager with Helm 3 for NiFiKop on Kubernetes\nDESCRIPTION: Installs cert-manager v1.7.2 using Helm 3, first by applying the CRDs and then deploying the Helm chart from the jetstack repository. This is an alternative method to the direct installation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/2_deploy_nifikop/1_quick_start.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Install CustomResourceDefinitions first\nkubectl apply --validate=false -f \\\n   https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.crds.yaml\n\n# Add the jetstack helm repo\nhelm repo add jetstack https://charts.jetstack.io\nhelm repo update\n\n# You have to create the namespace before executing following command\nhelm install cert-manager \\\n    --namespace cert-manager \\\n    --version v1.7.2 jetstack/cert-manager\n```\n\n----------------------------------------\n\nTITLE: Manually Applying NiFiKop CRDs using kubectl (Bash)\nDESCRIPTION: Applies all necessary NiFiKop CustomResourceDefinitions (CRDs) directly from the master branch of the NiFiKop GitHub repository using kubectl. This is required if installing the NiFiKop Helm chart with the '--skip-crds' flag.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/2_deploy_nifikop/1_quick_start.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifinodegroupautoscalers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificonnections.yaml\n```\n\n----------------------------------------\n\nTITLE: Applying cert-manager Kubernetes Resources Directly Using Bash\nDESCRIPTION: This snippet installs cert-manager in the Kubernetes cluster by applying CustomResourceDefinitions (CRDs) and the cert-manager resources directly via kubectl commands. It requires kubectl to be installed and configured to access the target cluster. The kubectl apply commands fetch resources from the jetstack GitHub repository for version 1.7.2. The output registers cert-manager custom resources required for certificate management within the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/2_deploy_nifikop/1_quick_start.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f \\\n    https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.yaml\n```\n\n----------------------------------------\n\nTITLE: Patching NifiKop CRDs with Conversion Webhook in YAML\nDESCRIPTION: This YAML snippet shows how to patch NifiKop CRDs to enable webhook-based conversion for migrating resource versions from v1alpha1 to v1. It configures the CRD's conversion strategy to Webhook and sets the clientConfig, specifying the service namespace, name, and the conversion webhook path. It also defines supported conversionReviewVersions to allow conversions between v1 and v1alpha1. Required parameters include namespace, certificate_name, and webhook_service_name, which are linked to the Helm release and deployment context.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/7_upgrade_guides/3_v0.16.0_to_v1.0.0.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n```\n\n----------------------------------------\n\nTITLE: Deploying KEDA with Helm\nDESCRIPTION: Commands for adding the KEDA Helm repository, updating it, and installing KEDA in a dedicated namespace. These commands set up KEDA as the autoscaling component that will work with NiFi.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nhelm repo add kedacore https://kedacore.github.io/charts\n```\n\nLANGUAGE: console\nCODE:\n```\nhelm repo update\n```\n\nLANGUAGE: console\nCODE:\n```\nkubectl create namespace keda\nhelm install keda kedacore/keda --namespace keda\n```\n\n----------------------------------------\n\nTITLE: Applying Nifikop CRDs Manually with kubectl\nDESCRIPTION: Demonstrates how to manually apply the necessary Nifikop Custom Resource Definitions (CRDs) to a Kubernetes cluster using `kubectl apply`. This step is required if installing the Helm chart with the `--skip-crds` flag, ensuring the cluster recognizes Nifikop custom resource types.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifinodegroupautoscalers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificonnections.yaml\n```\n\n----------------------------------------\n\nTITLE: Explanation of NiFi Configuration Override Mechanisms\nDESCRIPTION: This section describes the four methods available for overriding NiFi configuration files in Kubernetes: default, Secret, ConfigMap, and override field, with a priority order for conflicts. It provides an example YAML snippet illustrating how to specify overrideConfigMap, overrideSecretConfig, and overrideConfigs for nifi.properties.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n nifiProperties:\n      # Additionnal nifi.properties configuration that will override the one produced based on template and\n      # configuration\n      overrideConfigMap:\n        # The key of the value,in data content, that we want use.\n        data: nifi.properties\n        # Name of the configmap that we want to refer.\n        name: raw\n        # Namespace where is located the secret that we want to refer.\n        namespace: nifikop\n      # Additionnal nifi.properties configuration that will override the one produced based\n      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n      overrideSecretConfig:\n        # The key of the value,in data content, that we want use.\n        data: nifi.properties\n        # Name of the configmap that we want to refer.\n        name: raw\n        # Namespace where is located the secret that we want to refer.\n        namespace: nifikop\n      # Additionnal nifi.properties configuration that will override the one produced based\n      #\ton template, configurations and overrideConfigMap\n      overrideConfigs: |\n        nifi.ui.banner.text=NiFiKop\n        nifi.sensitive.props.key=thisIsABadSensitiveKeyPassword\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop Operator with a Custom Release Name Using Helm in Bash\nDESCRIPTION: This snippet shows how to install the NiFiKop operator Helm chart by specifying a custom release name placeholder. Replace \"<release name>\" with the desired release identifier. This requires Helm version 3 or above. The snippet assumes CRDs are managed either manually or by Helm depending on flags used.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhelm install <release name> konpyutaika/nifikop\n```\n\n----------------------------------------\n\nTITLE: Defining Internal Listeners for NiFi Cluster (YAML)\nDESCRIPTION: Configuration example for `listenersConfig.internalListeners` within the NiFiCluster Custom Resource Definition (CRD). This section specifies the essential ports used by NiFi pods for internal operations, including HTTPS UI access (8443), inter-node communication (6007), Site-to-Site (10000), Prometheus metrics (9090), and load balancing (6342). Each listener defines a type, a name, and the container port.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    - type: \"https\"\n      name: \"https\"\n      containerPort: 8443\n    - type: \"cluster\"\n      name: \"cluster\"\n      containerPort: 6007\n    - type: \"s2s\"\n      name: \"s2s\"\n      containerPort: 10000\n    - type: \"prometheus\"\n      name: \"prometheus\"\n      containerPort: 9090\n    - type: \"load-balance\"\n      name: \"load-balance\"\n      containerPort: 6342\n```\n\n----------------------------------------\n\nTITLE: Configuring Conversion Webhook for Nifikop CRDs Using YAML\nDESCRIPTION: This YAML snippet configures the conversion webhook for Nifikop CRDs to support resource version conversions between v1alpha1 and v1. This enables smooth upgrades and backward compatibility within Kubernetes. The configuration includes annotations for the certificate manager to inject webhook CA certificates and a spec section defining the webhook strategy, client service details, and conversion review versions. Key parameters such as namespace, certificate_name, and webhook_service_name need to be replaced with the actual deployment values.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n```\n\n----------------------------------------\n\nTITLE: Purging a Deleted Helm Release\nDESCRIPTION: Shows the `helm delete --purge nifikop` command (Helm 2 syntax) used to permanently remove the record of a deleted Helm release named 'nifikop'. In Helm 3, `helm uninstall nifikop` inherently performs the purge.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nhelm delete --purge nifikop\n```\n\n----------------------------------------\n\nTITLE: Deploying NifiDataflow (YAML)\nDESCRIPTION: This YAML snippet defines a NifiDataflow resource named `input`. It specifies the cluster to which the dataflow belongs, along with its bucket ID, flow ID, version, and registry client. It also sets configuration options like skipping invalid components or services, the update strategy, and flow position. This resource is essential for setting up the initial components needed for the connection.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/4_manage_connections/1_deploy_connection.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiDataflow\nmetadata:\n  name: input\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  bucketId: deedb9f6-65a4-44e9-a1c9-722008fcd0ba\n  flowId: ab95431d-980d-41bd-904a-fac4bd864ba6\n  flowVersion: 1\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  skipInvalidComponent: true\n  skipInvalidControllerService: true\n  syncMode: always\n  updateStrategy: drain\n  flowPosition:\n    posX: 0\n    posY: 0\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiNodeGroupAutoscaler Resource in YAML\nDESCRIPTION: This YAML snippet demonstrates how to define a `NifiNodeGroupAutoscaler` resource in Kubernetes. It specifies the target `NifiCluster` via `clusterRef`, identifies the node group using `nodeConfigGroupId` and `nodeLabelsSelector`, and sets the scaling strategies (`upscaleStrategy` and `downscaleStrategy`) for managing the nodes within that group automatically.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/5_references/7_nifi_nodegroup_autoscaler.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiNodeGroupAutoscaler\nmetadata:\n  name: nifinodegroupautoscaler-sample\nspec:\n  # contains the reference to the NifiCluster with the one the node group autoscaler is linked.\n  clusterRef:\n    name: nificluster-name\n    namespace: nifikop\n  # defines the id of the NodeConfig contained in NifiCluster.Spec.NodeConfigGroups\n  nodeConfigGroupId: default-node-group\n  # The selector used to identify nodes in NifiCluster.Spec.Nodes this autoscaler will manage\n  # Use Node.Labels in combination with this selector to clearly define which nodes will be managed by this autoscaler \n  nodeLabelsSelector: \n    matchLabels:\n      nifi_cr: nificluster-name\n      nifi_node_group: default-node-group\n  # the strategy used to decide how to add nodes to a nifi cluster\n  upscaleStrategy: simple\n  # the strategy used to decide how to remove nodes from an existing cluster\n  downscaleStrategy: lifo\n```\n\n----------------------------------------\n\nTITLE: Creating a NifiUser Resource for Certificate Generation - Kubernetes YAML (kubectl apply)\nDESCRIPTION: Demonstrates how to create a NifiUser resource and immediately apply it to a namespace using \"kubectl apply\" with a heredoc. This resource instructs the NiFi Operator to generate a new client certificate, storing credentials in a Kubernetes secret named \"example-client-secret\". Dependencies include the NiFi Operator installed, the target cluster existing, and kubectl access. Parameters include clusterRef.name and secretName. Optionally, add the \"includeJKS\" flag to include Java keystore files in the secret.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_3\n\nLANGUAGE: console\nCODE:\n```\ncat << EOF | kubectl apply -n nifi -f -\napiVersion:  nifi.konpyutaika.com/v1alpha1\nkind: NifiUser\nmetadata:\n  name: example-client\n  namespace: nifi\nspec:\n  clusterRef:\n    name: nifi\n  secretName: example-client-secret\nEOF\n\n```\n\n----------------------------------------\n\nTITLE: Setting nifi.properties Identity Mapping for OpenId Connect in Shell\nDESCRIPTION: This snippet configures nifi.properties identity mapping rules to support OpenId Connect authentication in NiFi, ensuring proper DN pattern extraction and transformation. It defines regex patterns to extract the CN field from the distinguished name for mapping user identities. These configurations are intended to be added directly into the nifi.properties file to facilitate multiple identity providers. Required dependencies include a correctly set up NiFi environment and access to nifi.properties for modification. The values control user identity resolution during authentication.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/5_users_authentication/1_oidc.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnifi.security.identity.mapping.pattern.dn=CN=([^,]*)(?:, (?:O|OU)=.*)?\nnifi.security.identity.mapping.value.dn=$1\nnifi.security.identity.mapping.transform.dn=NONE\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiDataflow Resource in YAML\nDESCRIPTION: Defines a `NifiDataflow` custom resource to deploy and manage a versioned flow from NiFi Registry. It specifies the target flow (bucketId, flowId, flowVersion), references the required `NifiRegistryClient` and `NifiParameterContext`, defines the target NiFi cluster (`clusterRef`) and process group (`parentProcessGroupID`), and controls synchronization (`syncMode`) and update behavior (`updateStrategy`).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: demo\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Verifying the NiFi node resources in Kubernetes - Console\nDESCRIPTION: This console output shows the Kubernetes resources created for the new NiFi node (nodeId=25), including the pod, configmap, and persistent volume claim.  It validates the successful creation of the node's supporting infrastructure.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/3_tasks/1_nifi_cluster/2_cluster_scaling.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl get pods,configmap,pvc -l nodeId=25\nNAME                          READY   STATUS    RESTARTS   AGE\npod/simplenifi-25-nodem5jh4   1/1     Running   0          11m\n\nNAME                             DATA   AGE\nconfigmap/simplenifi-config-25   7      11m\n\nNAME                                               STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\npersistentvolumeclaim/simplenifi-25-storagehwn24   Bound    pvc-7da86076-728e-11ea-846d-42010a8400f2   10Gi       RWO            standard       11m\n```\n\n----------------------------------------\n\nTITLE: Adding Prometheus Helm Repository - Console\nDESCRIPTION: Adds the official Prometheus Community Helm chart repository to your local Helm configuration. This repository contains charts for deploying Prometheus and its ecosystem components using Helm. Requires Helm to be installed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_6\n\nLANGUAGE: console\nCODE:\n```\nhelm repo add prometheus https://prometheus-community.github.io/helm-charts\n```\n\n----------------------------------------\n\nTITLE: Creating a NifiUserGroup resource in YAML\nDESCRIPTION: YAML configuration for creating a NifiUserGroup resource that groups multiple NifiUsers together and assigns them collective access policies. This example creates a group with read access to counters for two specified users.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/3_tasks/4_nifi_user_group.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  # Contains the reference to the NifiCluster with the one the registry client is linked.\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  # contains the list of reference to NifiUsers that are part to the group.\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n#     namespace: nifikop\n    - name: nc-controller.nifikop.mgt.cluster.local\n  # defines the list of access policies that will be granted to the group.\n  accessPolicies:\n      # defines the kind of access policy, could be \"global\" or \"component\".\n    - type: global\n      # defines the kind of action that will be granted, could be \"read\" or \"write\"\n      action: read\n      # resource defines the kind of resource targeted by this access policies, please refer to the following page:\n      #\thttps://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#access-policies\n      resource: /counters\n#      # componentType is used if the type is \"component\", it's allow to define the kind of component on which is the\n#      # access policy\n#      componentType: \"process-groups\"\n#      # componentId is used if the type is \"component\", it's allow to define the id of the component on which is the\n#      # access policy\n#      componentId: \"\"\n```\n\n----------------------------------------\n\nTITLE: Getting UID/GID for NiFi Deployment on OpenShift\nDESCRIPTION: Bash command to extract the appropriate UID/GID from an OpenShift namespace for NiFi deployment, ensuring proper permissions are set.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nuid=$(kubectl get namespace nifi -o=jsonpath='{.metadata.annotations.openshift\\.io/sa\\.scc\\.supplemental-groups}' | sed 's/\\/10000$//' | tr -d '[:space:]')\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Parameter Context Using Kubernetes YAML\nDESCRIPTION: This YAML snippet defines a custom Kubernetes resource of kind `NifiParameterContext` representing a NiFi parameter context. It sets metadata such as name and namespace, specifies descriptive information, references to the NiFi cluster, and secret references for sensitive parameters. It additionally lists parameters with their names, values, and sensitivity flags. The snippet serves as a template to configure NiFi parameter contexts declaratively in Kubernetes environments. Dependencies include Kubernetes CustomResourceDefinitions (CRDs) and NiFi operator components recognizing this schema.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/5_references/4_nifi_parameter_context.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      description: toto\n      sensistive: true\n```\n\n----------------------------------------\n\nTITLE: Querying KEDA-Managed Horizontal Pod Autoscalers - console\nDESCRIPTION: Lists Horizontal Pod Autoscalers (HPA) managed by KEDA in the 'clusters' namespace, displaying their targets, current and min/max pods, and age. Requires kubectl and KEDA deployed in the cluster. Outputs HPA table. No arguments are required.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_14\n\nLANGUAGE: console\nCODE:\n```\nkubectl get -n clusters hpa\n```\n\n----------------------------------------\n\nTITLE: Exporting Operator Name Environment Variable in Bash\nDESCRIPTION: This bash command sets the OPERATOR_NAME environment variable to 'nifi-operator', which is required for running the operator locally outside of the cluster. Ensures the operator process has a defined name for management and logging purposes. Requires standard shell access and must be run before executing further deployment commands.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/6_contributing/1_developer_guide.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport OPERATOR_NAME=nifi-operator\n```\n\n----------------------------------------\n\nTITLE: Extracting OpenShift Namespace UID Annotation Using Bash\nDESCRIPTION: This snippet retrieves the supplemental group UID range annotation from a given OpenShift namespace (here `nifi`). It uses kubectl with jsonpath to fetch the annotation for SCC UID ranges, trims the last 10000 characters from the range, and removes whitespace using sed and tr commands. This UID is necessary for configuring the RunAsUser parameter to satisfy OpenShift security constraints when deploying NiFiKop.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/2_deploy_nifikop/1_quick_start.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nuid=$(kubectl get namespace nifi -o=jsonpath='{.metadata.annotations.openshift\\.io/sa\\.scc\\.supplemental-groups}' | sed 's/\\/10000$//' | tr -d '[:space:]')\n```\n\n----------------------------------------\n\nTITLE: Configuration of SSL Secrets when Not Creating New Certificates\nDESCRIPTION: Specifies the expectations for existing SSL secret keys when `sslSecrets.create` is set to false. The secret must contain CA certificate, CA key, client certificate, and client key for secure communication with the NiFi cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\nsslSecrets:\n  tlsSecretName: \"test-nifikop\"\n  create: false\n```\n\n----------------------------------------\n\nTITLE: Deleting Nifikop Helm Release Using Helm CLI - Bash\nDESCRIPTION: This bash snippet removes the 'nifikop' Helm release from the Kubernetes cluster, deleting all Kubernetes resources that were created by the chart. Prerequisite: Helm must be installed and configured to access your cluster. The 'nifikop' release name should match the installed release. This does not remove any CRDs created by the chart, which must be deleted separately.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nhelm del nifikop\n```\n\n----------------------------------------\n\nTITLE: Initializing Node.js Project for Kubernetes Resource Migration Script\nDESCRIPTION: This snippet demonstrates the bash commands to initialize a new Node.js project and install required dependencies '@kubernetes/client-node' and 'minimist' needed for Kubernetes API interactions and command-line argument parsing. It ensures Node.js environment setup with versions 15.3.0+ and npm 7.0.14+.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm init -y\nnpm install @kubernetes/client-node@0.16.3 minimist@1.2.6\n```\n\n----------------------------------------\n\nTITLE: Listing Deleted Helm Releases - Bash\nDESCRIPTION: This bash command shows all Helm releases in the 'deleted' state within the current Kubernetes context. It requires that Helm is installed and configured. The command is useful for auditing or recovering releases that have been deleted, as Helm retains metadata after deletions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --deleted\n```\n\n----------------------------------------\n\nTITLE: Apply NiFiCluster configuration via Kubectl (scaledown)\nDESCRIPTION: This command applies the updated `NifiCluster` configuration (after removing a node) to the Kubernetes cluster in the `nifi` namespace. It triggers the decommissioning process for the removed node.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\nkubectl -n nifi apply -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing Bitnami Zookeeper Helm chart with resource limits and storage class in Bash\nDESCRIPTION: Uses Helm to install a Zookeeper cluster via Bitnami's Helm chart repository specifying namespace 'zookeeper', resource requests and limits, storage class, network policy, replica count, and namespace creation. This command assumes Helm and kubectl are configured and the Kubernetes cluster is accessible. The user must replace the storageClass parameter with a valid value if different from 'standard'.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhelm install zookeeper oci://registry-1.docker.io/bitnamicharts/zookeeper \\\n    --namespace=zookeeper \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set global.storageClass=standard \\\n    --set networkPolicy.enabled=true \\\n    --set replicaCount=3 \\\n    --create-namespace\n```\n\n----------------------------------------\n\nTITLE: Apply OIDC Configuration via NiFiKop NifiCluster Spec | YAML\nDESCRIPTION: Demonstrates how to apply OpenID Connect configuration, including client details and the recommended identity mapping properties, using the NiFiKop operator. This is achieved by setting the `overrideConfigs` field within the `spec.readOnlyConfig.nifiProperties` section of the `NifiCluster` YAML definition.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/1_manage_clusters/1_deploy_cluster/5_users_authentication/1_oidc.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  readOnlyConfig:\n    # NifiProperties configuration that will be applied to the node.\n    nifiProperties:\n      webProxyHosts:\n        - nifistandard2.trycatchlearn.fr:8443\n      # Additionnal nifi.properties configuration that will override the one produced based\n      # on template and configurations.\n      overrideConfigs: |\n        nifi.security.user.oidc.discovery.url=<oidc server discovery url>\n        nifi.security.user.oidc.client.id=<oidc client's id>\n        nifi.security.user.oidc.client.secret=<oidc client's secret>\n        nifi.security.identity.mapping.pattern.dn=CN=([^,]*)(?:, (?:O|OU)=.*)?\n        nifi.security.identity.mapping.value.dn=$1\n        nifi.security.identity.mapping.transform.dn=NONE\n      ...\n   ...\n...\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFiKop Custom Resource Definitions (CRDs) Using Kubectl - Bash\nDESCRIPTION: These commands apply multiple Kubernetes Custom Resource Definitions (CRDs) required by NiFiKop operator. They create the custom resources in the cluster to enable the operator to manage NiFi-specific objects. It expects 'kubectl' configured to communicate with the target Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/6_contributing/1_developer_guide.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\n```\n\n----------------------------------------\n\nTITLE: Install cert-manager Directly with Manifest (bash)\nDESCRIPTION: Applies the cert-manager manifest file from the official Jetstack GitHub repository to install the cert-manager components and CustomResourceDefinitions (CRDs) into the cluster. This is a direct installation method. Requires kubectl configured to connect to the target cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/2_deploy_nifikop/1_quick_start.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f \\\n    https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop Helm Chart with Custom Release Name (Bash)\nDESCRIPTION: This snippet details how to deploy the NiFiKop Helm chart with a user-specified release name placeholder (`<release name>`). Provide your own release name when using this command. Prerequisites include Helm v3+ and optionally pre-installed CRDs. No configuration overrides are used here. The chart will be installed to the current Kube context/namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/2_setup/3_install/1_customizable_install_with_helm.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhelm install <release name> konpyutaika/nifikop\n```\n\n----------------------------------------\n\nTITLE: Packaging NiFiKop Helm Chart (Bash)\nDESCRIPTION: Executes the 'make helm-package' command to create a packaged archive (.tgz file) of the NiFiKop Helm chart. This step is necessary before publishing or distributing the Helm chart. It requires 'make' and Helm to be installed and configured.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/6_contributing/1_developer_guide.md#_snippet_10\n\nLANGUAGE: Bash\nCODE:\n```\nmake helm-package\n```\n\n----------------------------------------\n\nTITLE: Building NiFiKop Docker Image (Bash)\nDESCRIPTION: Uses the 'make docker-build' command to build the NiFiKop operator Docker image based on the current branch. Requires Docker (v18.09+) to be installed and running. The image will be tagged using the version from 'version/version.go' and the branch name, prefixed by 'DOCKER_REGISTRY_BASE' if set.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/6_contributing/1_developer_guide.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nmake docker-build\n```\n\n----------------------------------------\n\nTITLE: Listing Managed Groups with kubectl\nDESCRIPTION: Command to list all NifiUserGroups in the nifikop namespace, which includes the managed groups created by the operator (managed-admins, managed-nodes, and managed-readers).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/2_manage_users_and_accesses/3_managed_groups.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nkubectl get -n nifikop nifiusergroups.nifi.konpyutaika.com \nNAME              AGE\nmanaged-admins    6d7h\nmanaged-nodes     6d7h\nmanaged-readers   6d7h\n```\n\n----------------------------------------\n\nTITLE: Updating Helm Repository\nDESCRIPTION: Updates local Helm repository index to ensure the latest charts are available for installation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_7\n\nLANGUAGE: console\nCODE:\n```\nhelm repo update\n```\n\n----------------------------------------\n\nTITLE: Retrieving GKE Cluster Credentials (Console/gcloud)\nDESCRIPTION: Command template for retrieving GKE cluster credentials using the gcloud CLI tool. Replace placeholders with actual cluster name, zone, and project ID. This configures `kubectl` to connect to the specified GKE cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/2_deploy_nifikop/1_quick_start.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\ngcloud container clusters get-credentials CLUSTER_NAME --zone ZONE_NAME --project PROJECT_NAME.\n```\n\n----------------------------------------\n\nTITLE: Configuring kubectl for GKE Cluster Access (Console)\nDESCRIPTION: Connects kubectl to a Google Kubernetes Engine (GKE) cluster. This command retrieves cluster credentials using gcloud based on the cluster name, zone, and project ID, making the cluster context available to kubectl.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/2_deploy_nifikop/1_quick_start.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\ngcloud container clusters get-credentials CLUSTER_NAME --zone ZONE_NAME --project PROJECT_NAME.\n```\n\n----------------------------------------\n\nTITLE: Retrieving Kubernetes Credentials - Shell\nDESCRIPTION: This snippet retrieves the credentials for `kubectl` to interact with the newly created GKE cluster. It uses the `gcloud container clusters get-credentials` command along with the cluster name, zone, and project.  It sets up the user's local `kubectl` configuration to target the specified cluster.  The user needs the Google Cloud SDK installed and authorized to access the project and cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/2_setup/2_platform_setup/1_gke.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ncloud container clusters get-credentials $CLUSTER_NAME \\\n    --zone $GCP_ZONE \\\n    --project $GCP_PROJECT\n```\n\n----------------------------------------\n\nTITLE: Installing cert-manager Directly Using kubectl Bash Commands\nDESCRIPTION: This snippet demonstrates how to install cert-manager by applying the official CustomResourceDefinitions and cert-manager manifests directly via kubectl. This approach is suitable for Kubernetes clusters with direct internet access. The commands download and install cert-manager version v1.7.2 including CRD resources required for certificate management.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/2_deploy_nifikop/1_quick_start.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Install the CustomResourceDefinitions and cert-manager itself\nkubectl apply -f \\\n    https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.yaml\n```\n\n----------------------------------------\n\nTITLE: Updating OpenShift Configuration File with UID\nDESCRIPTION: Command to update the OpenShift sample configuration file with the correct UID value obtained from the namespace, which ensures proper security context for the NiFi deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nsed -i \"s/1000690000/$uid/g\" config/samples/openshift.yaml\n```\n\n----------------------------------------\n\nTITLE: Secret Creation for Basic Authentication in Kubernetes\nDESCRIPTION: This console command creates a Kubernetes secret containing username, password, and optional CA certificate required for basic authentication with the external NiFi cluster. The secret is stored in the specified namespace and is referenced in the NifiCluster resource for secure API access.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/3_external_cluster.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nkubectl create secret generic nifikop-credentials \\\n  --from-file=username=./secrets/username\\\n  --from-file=password=./secrets/password\\\n  --from-file=ca.crt=./secrets/ca.crt\\\n  -n nifikop-nifi\n```\n\n----------------------------------------\n\nTITLE: Listing Deployed Helm Charts\nDESCRIPTION: Provides the `helm list` command, which displays all currently deployed Helm releases within the active Kubernetes context.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nhelm list\n```\n\n----------------------------------------\n\nTITLE: Running Operator Locally - Bash\nDESCRIPTION: This command starts the NiFiKop operator locally. It uses the `make run` command, which is typically associated with a Makefile that handles the build and deployment process. The operator will run in the default namespace using the default kubeconfig file ( `~/.kube/config`).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/6_contributing/1_developer_guide.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmake run\n```\n\n----------------------------------------\n\nTITLE: Testing Kubectl Plugin Installation (Console)\nDESCRIPTION: This console command executes the `kubectl nifikop` plugin after installation. A successful execution results in the plugin displaying its usage information and a list of available subcommands for managing various NiFiKop resources (e.g., `nificluster`, `nificonnection`, `nifidataflow`). This output confirms that the plugin was installed correctly and is accessible in the system's PATH.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/2_deploy_nifikop/3_kubectl_plugin.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\n$ kubectl nifikop\nUsage:\n  nifikop [command]\n\nAvailable Commands:\n  completion          Generate the autocompletion script for the specified shell\n  help                Help about any command\n  nificluster         \n  nificonnection      \n  nifidataflow        \n  nifigroupautoscaler \n  nifiregistryclient  \n  nifiuser            \n  nifiusergroup \n\n```\n\n----------------------------------------\n\nTITLE: Installing Prometheus Operator Helm Chart - Console\nDESCRIPTION: Installs the `kube-prometheus-stack` Helm chart into the `monitoring-system` namespace with specific `--set` flags to disable most components except the core Prometheus operator and resource definitions. This sets up the foundation for deploying a customized Prometheus instance. Requires `kubectl` and `helm` installed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_8\n\nLANGUAGE: console\nCODE:\n```\nhelm install prometheus prometheus/kube-prometheus-stack --namespace monitoring-system \\\n    --set prometheusOperator.createCustomResource=false \\\n    --set prometheusOperator.logLevel=debug \\\n    --set prometheusOperator.alertmanagerInstanceNamespaces=monitoring-system \\\n    --set prometheusOperator.namespaces.additional[0]=monitoring-system \\\n    --set prometheusOperator.prometheusInstanceNamespaces=monitoring-system \\\n    --set prometheusOperator.thanosRulerInstanceNamespaces=monitoring-system \\\n    --set defaultRules.enable=false \\\n    --set alertmanager.enabled=false \\\n    --set grafana.enabled=false \\\n    --set kubeApiServer.enabled=false \\\n    --set kubelet.enabled=false \\\n    --set kubeControllerManager.enabled=false \\\n    --set coreDNS.enabled=false \\\n    --set kubeEtcd.enabled=false \\\n    --set kubeScheduler.enabled=false \\\n    --set kubeProxy.enabled=false \\\n    --set kubeStateMetrics.enabled=false \\\n    --set prometheus.enabled=false\n```\n\n----------------------------------------\n\nTITLE: Kubernetes Pod Ports Configuration for NiFi Listeners\nDESCRIPTION: This YAML snippet shows the configuration of ports for a Kubernetes pod corresponding to the NiFi internal listeners. It defines the `containerPort`, `name`, and `protocol` (TCP) for each listener. These port definitions are essential for Kubernetes to route traffic to the correct NiFi services running within the pod.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\nports:\n    - containerPort: 8443\n      name: https\n      protocol: TCP\n    - containerPort: 6007\n      name: cluster\n      protocol: TCP\n    - containerPort: 6342\n      name: load-balance\n      protocol: TCP\n    - containerPort: 10000\n      name: s2s\n      protocol: TCP\n    - containerPort: 9090\n      name: prometheus\n      protocol: TCP\n```\n\n----------------------------------------\n\nTITLE: Installing cert-manager Directly in Kubernetes\nDESCRIPTION: Command to install cert-manager v1.7.2 directly in Kubernetes. This deploys both the CustomResourceDefinitions and cert-manager itself in a single step.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/2_deploy_nifikop/1_quick_start.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Install the CustomResourceDefinitions and cert-manager itself\nkubectl apply -f \\\n    https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing Helm Chart and Setting Parameters (bash)\nDESCRIPTION: Installs the `konpyutaika/nifikop` Helm chart using `helm install`, setting chart values via the `--set` flag. Specifically sets the `namespaces` parameter to `{\"nifikop\"}`. This is a common way to customize chart behavior during installation. Requires Helm installed and configured for the target Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop --set namespaces={\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Creating Secret for Basic Authentication - kubectl Command\nDESCRIPTION: This command demonstrates how to create a Kubernetes secret named 'nifikop-credentials' for basic authentication with the NiFi cluster. Dependencies include kubectl CLI and Kubernetes API access. The parameters include 'username', 'password', and an optional 'ca.crt' for SSL CA trust, all sourced from local files. The expected input is valid file paths for the credentials, and the output is a secret resource in the designated namespace. Files must exist before running the command and secret data must be properly secured.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/3_external_cluster.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nkubectl create secret generic nifikop-credentials \\\n  --from-file=username=./secrets/username\\\n  --from-file=password=./secrets/password\\\n  --from-file=ca.crt=./secrets/ca.crt\\\n  -n nifikop-nifi\n```\n\n----------------------------------------\n\nTITLE: Creating Secret for Basic Authentication - Console\nDESCRIPTION: This console command creates a Kubernetes secret named `nifikop-credentials` using files containing the username, password, and optionally, the CA certificate for basic authentication to the NiFi cluster. This secret is referenced by the `NifiCluster` resource. The command uses `kubectl` to create the secret within the specified namespace.  It requires `kubectl` configured and access to the Kubernetes cluster and the relevant files. The result is a Kubernetes secret containing the necessary credentials.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/3_external_cluster.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\n```console\nkubectl create secret generic nifikop-credentials \\\n  --from-file=username=./secrets/username\\\n  --from-file=password=./secrets/password\\\n  --from-file=ca.crt=./secrets/ca.crt\\\n  -n nifikop-nifi\n```\n```\n\n----------------------------------------\n\nTITLE: Patching NiFiKop CRDs for Webhook Conversion (YAML)\nDESCRIPTION: YAML configuration snippet demonstrating how to patch NiFiKop CRDs to enable API version conversion using a webhook. This patch is required when upgrading from v0.16.0 (using v1alpha1 CRDs) to v1.0.0 (using v1 CRDs) to handle the API version change seamlessly. It adds annotations for cert-manager CA injection and configures the CRD's conversion strategy to use a webhook, specifying the service details (`namespace`, `name`, `path`) and the supported `conversionReviewVersions` (v1 and v1alpha1). Replace placeholders like `${namespace}`, `${certificate_name}`, and `${webhook_service_name}` with actual values from your Helm deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/7_upgrade_guides/3_v0.16.0_to_v1.0.0.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom StorageClass for NiFi in Kubernetes\nDESCRIPTION: YAML configuration for creating a custom StorageClass with the volume binding mode set to WaitForFirstConsumer, which is recommended for NiFi deployments. This StorageClass uses GCE persistent disk as the provisioner.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: exampleStorageclass\nparameters:\n  type: pd-standard\nprovisioner: kubernetes.io/gce-pd\nreclaimPolicy: Delete\nvolumeBindingMode: WaitForFirstConsumer\n```\n\n----------------------------------------\n\nTITLE: Verifying Ingress Resources with kubectl get ingresses (Bash)\nDESCRIPTION: This bash snippet checks the created ingress resources in the nifikop namespace, validating the exposure and DNS mapping for NiFi nodes and controllers. Prerequisites: ingress controllers and external DNS must be reconciled. Parameters: none, namespace is hard-coded. Outputs the hostnames, addresses, ports, and age for each ingress, confirming DNS and certificate assignment for cluster endpoints.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/blog/2020-06-30-secured_nifi_cluster_on_gcp_with_external_dns.md#_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get ingresses -n nifikop\nNAME                        HOSTS                                                 ADDRESS          PORTS   AGE\ncm-acme-http-solver-4pff9   nifi-2-node.nifi-headless.orange.trycatchlearn.fr                      80      2m27s\ncm-acme-http-solver-cfsf4   nifi-0-node.nifi-headless.orange.trycatchlearn.fr     34.120.24.109    80      2m30s\ncm-acme-http-solver-hn8jj   nifi-controller.nifikop.mgt.orange.trycatchlearn.fr   34.120.90.24     80      2m29s\ncm-acme-http-solver-llhsp   nifi-1-node.nifi-headless.orange.trycatchlearn.fr                      80      2m27s\ncm-acme-http-solver-v8dmm   nifi-headless.orange.trycatchlearn.fr                 34.120.201.215   80      2m28s\ncm-acme-http-solver-xvs9f   nifi.orange.trycatchlearn.fr                          35.244.202.176   80      2m27s\n```\n\n----------------------------------------\n\nTITLE: Uninstalling NiFiKop Helm Chart\nDESCRIPTION: Helm command to delete the NiFiKop operator deployment from Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nhelm del nifikop\n```\n\n----------------------------------------\n\nTITLE: Checking HPA Status for NiFi Autoscaling\nDESCRIPTION: Command to check the status of the Horizontal Pod Autoscaler (HPA) created by KEDA for managing the NiFi node group scaling.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_13\n\nLANGUAGE: console\nCODE:\n```\nkubectl get -n clusters hpa\n```\n\n----------------------------------------\n\nTITLE: Installing cert-manager using Helm 3\nDESCRIPTION: Installs cert-manager v1.7.2 using Helm 3. First, it applies the CRDs separately, then adds the Jetstack Helm repository, updates it, and finally installs the cert-manager chart into the 'cert-manager' namespace. Requires Helm 3 and kubectl.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/2_deploy_nifikop/1_quick_start.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Install CustomResourceDefinitions first\nkubectl apply --validate=false -f \\\n   https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.crds.yaml\n\n# Add the jetstack helm repo\nhelm repo add jetstack https://charts.jetstack.io\nhelm repo update\n\n# You have to create the namespace before executing following command\nhelm install cert-manager \\\n    --namespace cert-manager \\\n    --version v1.7.2 jetstack/cert-manager\n```\n\n----------------------------------------\n\nTITLE: Manually Applying NiFiKop CRDs in Kubernetes\nDESCRIPTION: Commands to manually apply Custom Resource Definitions (CRDs) for NiFiKop when using the --skip-crds option during Helm installation. These commands deploy all necessary CRDs from the project's GitHub repository.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifinodegroupautoscalers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificonnections.yaml\n```\n\n----------------------------------------\n\nTITLE: Removing a Node from NiFiCluster Configuration in YAML\nDESCRIPTION: A YAML configuration example showing how to remove a node (id: 2) from an existing NiFi cluster by modifying the NifiCluster resource definition.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  headlessServiceEnabled: true\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.11.3\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n# >>>> START: node removed\n#    - id: 2\n#      nodeConfigGroup: \"default_group\"\n# <<<< END\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: Manually Deleting CRDs - Bash\nDESCRIPTION: These commands manually delete Custom Resource Definitions (CRDs) associated with the chart.  This is needed because Helm does not remove CRDs by default. Deleting CRDs will remove all associated clusters. It's extremely important to take care and understand the consequences before running these commands.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/2_setup/3_install/1_customizable_install_with_helm.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete crd nificlusters.nifi.konpyutaika.com\nkubectl delete crd nifiusers.nifi.konpyutaika.com\nkubectl delete crd nifiusergroups.nifi.konpyutaika.com\nkubectl delete crd nifiregistryclients.nifi.konpyutaika.com\nkubectl delete crd nifiparametercontexts.nifi.konpyutaika.com\nkubectl delete crd nifidataflows.nifi.konpyutaika.com\n```\n\n----------------------------------------\n\nTITLE: Initializing Node.js Project for NiFiKop Migration (Bash)\nDESCRIPTION: Sets up a new Node.js project using `npm init -y` and installs the necessary `@kubernetes/client-node` (v0.16.3) and `minimist` (v1.2.6) libraries using `npm install`. This prepares the environment for running the migration script.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm init -y\nnpm install @kubernetes/client-node@0.16.3 minimist@1.2.6\n```\n\n----------------------------------------\n\nTITLE: Applying Nifikop CRDs Manually (Bash)\nDESCRIPTION: Uses `kubectl apply` to manually install all necessary Nifikop Custom Resource Definitions (CRDs) directly from the project's GitHub repository. This step is required if installing the Helm chart with the `--skip-crds` flag.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifinodegroupautoscalers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificonnections.yaml\n```\n\n----------------------------------------\n\nTITLE: Getting Status of Helm Deployment (bash)\nDESCRIPTION: Retrieves the detailed status of a specific Helm release using the `helm status` command, typically providing information about resources deployed, hooks, and notes. The example uses `nifikop` as the release name. Requires Helm installed and configured for the target Kubernetes cluster, and the specified release must exist.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nhelm status nifikop\n```\n\n----------------------------------------\n\nTITLE: Configuring CRD Conversion Webhook - YAML\nDESCRIPTION: This YAML snippet configures a CRD to enable webhook-based conversion between v1alpha1 and v1 API versions, which is essential when using conversion webhooks with cert-manager for automatic CA injection. Dependencies: cert-manager must be installed; certificate_name and webhook_service_name should match your Helm release. Key parameters include the annotations for CA injection, the namespace, conversion.strategy, service path, and supported review versions. Inputs: Customize placeholders for namespace, certificate, and service name as appropriate.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFiKop CRDs (Bash)\nDESCRIPTION: This command applies the Custom Resource Definitions (CRDs) for NiFiKop manually. This is necessary when the CRDs are not installed via Helm, or if `--skip-crds` is used during Helm install. It requires `kubectl` and a configured Kubernetes cluster. The CRDs define the custom resources used by the operator.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/2_deploy_nifikop/1_quick_start.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring Managed Users in NifiCluster YAML\nDESCRIPTION: This YAML snippet demonstrates how to specify users to be automatically managed by the NiFiKop operator within the `NifiCluster` custom resource. Users listed under `managedAdminUsers` are added to the 'managed-admins' group with full access, while users under `managedReaderUsers` are added to the 'managed-readers' group with view-only access. The operator creates corresponding `NifiUser` resources and manages their membership in the respective `NifiUserGroup` resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/2_manage_users_and_accesses/3_managed_groups.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  ...\n  oneNifiNodePerNode: false\n  propagateLabels: true\n  managedAdminUsers:\n    -  identity: \"alexandre.guitton@konpyutaika.com\"\n       name: \"aguitton\"\n    -  identity: \"nifiuser@konpyutaika.com\"\n       name: \"nifiuser\"\n  managedReaderUsers:\n    -  identity: \"toto@konpyutaika.com\"\n       name: \"toto\"\n    ...\n```\n\n----------------------------------------\n\nTITLE: Example NiFi Listeners Configuration (YAML)\nDESCRIPTION: This YAML snippet demonstrates how to configure the `listenersConfig` section for a NiFi cluster managed by the nifikop operator. It defines several internal listeners (https, cluster, s2s, prometheus) each with a type, name, and container port. It also specifies the configuration for SSL secrets, including the name of the Kubernetes secret (`tlsSecretName`) and a flag (`create`) indicating whether the operator should create it.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/5_references/1_nifi_cluster/6_listeners_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  listenersConfig:\n    internalListeners:\n      - type: \"https\"\n        name: \"https\"\n        containerPort: 8443\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n      - type: \"prometheus\"\n        name: \"prometheus\"\n        containerPort: 9090\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n```\n\n----------------------------------------\n\nTITLE: Uninstalling NiFiKop Chart\nDESCRIPTION: Command to uninstall the NiFiKop chart from the Kubernetes cluster. This removes the operator deployment but preserves the CRDs by default.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nhelm del nifikop\n```\n\n----------------------------------------\n\nTITLE: Configuring NifiCluster for Adding a Node (YAML)\nDESCRIPTION: This YAML snippet demonstrates the configuration for adding a new node (ID 25) to an existing NiFi cluster. It defines the node's configuration group, which specifies resources, storage, and service account. The `NifiCluster` resource is a custom resource used in the NiFiKop operator. Prerequisites include the NiFiKop operator and a running NiFi cluster. The key parameter is `id`, which uniquely identifies the new node in the cluster. The expected output is a new NiFi pod, configmap and persistent volume claim.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.12.1\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n    - id: 2\n      nodeConfigGroup: \"default_group\"\n# >>>> START: The new node\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n# <<<< END\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: Declaring Cluster Nodes with Node Configuration Groups - YAML\nDESCRIPTION: This YAML snippet shows how to declare individual NiFi cluster nodes and associate them with predefined nodeConfigGroups or provide custom resource requirements directly at the node level. The configuration allows for both reusable (shared across multiple nodes) and inlined (per-node unique) specifications. Useful for tailoring different nodes with separate performance profiles within a single cluster. Requires NiFiKop operator and relevant nodeConfigGroups definitions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\n- id: 0\n  nodeConfigGroup: \"default_group\"\n- id: 2\n  nodeConfigGroup: \"high_mem_group\"\n- id: 3\n  nodeConfigGroup: \"high_mem_group\"\n- id: 5\n  nodeConfig:\n    resourcesRequirements:\n      limits:\n        cpu: \"2\"\n        memory: 3Gi\n      requests:\n        cpu: \"1\"\n        memory: 1Gi\n```\n\n----------------------------------------\n\nTITLE: Complete Node.js Package Configuration - JSON\nDESCRIPTION: Shows the complete `package.json` file configuration for the migration script project after initialization and dependency installation. It includes basic project metadata, the defined \"start\" script, and the required dependencies (`@kubernetes/client-node`, `minimist`) with their installed versions. Requires a Node.js project initialized with npm and dependencies installed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"nifikop_crd_migration\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Script to migrate from the old CRDs to the new CRDs.\",\n  \"main\": \"index.js\",\n  \"scripts\": {\n    \"start\": \"node --no-warnings index.js\",\n    \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\"\n  },\n  \"keywords\": [\n    \"K8S\",\n    \"NiFiKop\",\n    \"CRDs\"\n  ],\n  \"license\": \"ISC\",\n  \"dependencies\": {\n    \"@kubernetes/client-node\": \"^0.16.3\",\n    \"minimist\": \"^1.2.6\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Granting Cluster Admin Permissions - Shell\nDESCRIPTION: This command grants cluster administrator permissions to the current user. It uses `kubectl create clusterrolebinding` to create a binding that associates the current user (determined via `gcloud config get-value core/account`) with the `cluster-admin` role.  This is required so the user can create necessary RBAC rules for NiFiKop. The current user needs the necessary permissions to execute this command; otherwise, they may not be able to grant this access to themselves.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/2_setup/2_platform_setup/1_gke.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nkubectl create clusterrolebinding cluster-admin-binding \\\n    --clusterrole=cluster-admin \\\n    --user=$(gcloud config get-value core/account)\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio Gateway for HTTP\nDESCRIPTION: This YAML snippet defines an Istio Gateway to intercept HTTP requests for a specific domain, enabling external access to the NiFi cluster. It listens on port 80 for HTTP traffic on the specified host. The `istio: ingressgateway` selector specifies the Istio ingress gateway to use.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: nifi-gateway\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - nifi.my-domain.com\n```\n\n----------------------------------------\n\nTITLE: Adding and Updating Prometheus Helm Repository Using Helm Console\nDESCRIPTION: These snippets add the Prometheus community Helm charts repository and update the local Helm repository cache. Helm is required for managing Kubernetes packages, and having the latest chart index enables deployment of Prometheus operator and stack components efficiently. Helm CLI must be installed and configured for these commands to work.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_6\n\nLANGUAGE: console\nCODE:\n```\nhelm repo add prometheus https://prometheus-community.github.io/helm-charts\n```\n\nLANGUAGE: console\nCODE:\n```\nhelm repo update\n```\n\n----------------------------------------\n\nTITLE: Adding Start Script to package.json for Node.js Application - JSON\nDESCRIPTION: This JSON snippet demonstrates how to add a custom 'start' script to the scripts section of package.json, specifying the command to execute the core migration logic with Node.js. The 'node --no-warnings index.js' directive disables Node.js warnings during execution. This setup is required for easily running the main migration script via 'npm start'.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"start\": \"node --no-warnings index.js\"\n```\n\n----------------------------------------\n\nTITLE: Adding a new node to the NiFiCluster via YAML configuration\nDESCRIPTION: This YAML snippet defines the specifications for adding a new node with a unique 'id' (25) to an existing NiFi cluster. It includes node configuration, resource limits, storage settings, and labels, ensuring the new node is properly integrated and managed within the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  service:\n    headlessEnabled: true\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.12.1\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n    - id: 2\n      nodeConfigGroup: \"default_group\"\n# >>>> START: The new node\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n# <<<< END\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: Applying NiFiKop CRDs to Kubernetes - Bash\nDESCRIPTION: This series of kubectl commands applies all required Custom Resource Definition (CRD) YAML files for the NiFiKop operator to the Kubernetes cluster. The commands require kubectl to be configured and have permissions to modify cluster resources. It registers multiple NiFiKop custom resource types needed for operation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/6_contributing/1_developer_guide.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\n```\n\n----------------------------------------\n\nTITLE: Listing Deployed Helm Charts in Bash\nDESCRIPTION: A simple Helm CLI command to list all Helm charts deployed in the Kubernetes cluster, showing their release names, namespaces, and statuses. Useful for managing and auditing Helm deployments.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/2_setup/3_install/1_customizable_install_with_helm.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nhelm list\n```\n\n----------------------------------------\n\nTITLE: Helm List Deployments\nDESCRIPTION: This command demonstrates listing the deployed Helm charts in the cluster.  It uses the `helm list` command.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/helm/nifikop/README.md#_snippet_5\n\nLANGUAGE: console\nCODE:\n```\nhelm list\n```\n\n----------------------------------------\n\nTITLE: Example YAML Snippet for NiFiKop CRD Conversion Webhook Configuration\nDESCRIPTION: Illustrates the YAML configuration needed to enable the conversion webhook on NiFiKop CRDs. This webhook handles the conversion of custom resources between API versions v1alpha1 and v1. It requires setting annotations for cert-manager injected CA certificates, defining webhook client configuration, and specifying supported conversion review versions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/2_deploy_nifikop/1_quick_start.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n```\n\n----------------------------------------\n\nTITLE: Installing cert-manager directly via kubectl\nDESCRIPTION: Deploys cert-manager by applying the official YAML manifest, which installs CustomResourceDefinitions and the cert-manager components. Ensure you have the necessary permissions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/2_setup/1_getting_started.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Install the CustomResourceDefinitions and cert-manager itself\\nkubectl apply -f \\n    https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.yaml\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for NiFiKop Operator in Bash\nDESCRIPTION: This set of commands defines environment variables required for local development and operator interaction with a Kubernetes cluster. Required variables include KUBECONFIG (specifying the kubeconfig path), WATCH_NAMESPACE, POD_NAME, LOG_LEVEL, and OPERATOR_NAME. These must be set before running or debugging the operator through an IDE or CLI, and their values determine target cluster and logging behavior.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/6_contributing/1_developer_guide.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nKUBECONFIG={path/to/your/kubeconfig}\nWATCH_NAMESPACE={namespace_to_watch}\nPOD_NAME={name for operator pod}\nLOG_LEVEL=Debug\nOPERATOR_NAME=ide\n```\n\n----------------------------------------\n\nTITLE: Installing Nifikop Helm Chart with a Release Name\nDESCRIPTION: Provides the basic command structure for installing the Nifikop Helm chart from the `konpyutaika` repository using `helm install`. A unique `<release name>` must be specified for the deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nhelm install <release name> konpyutaika/nifikop\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio VirtualService for HTTPS\nDESCRIPTION: This YAML snippet defines an Istio VirtualService that routes HTTP traffic from the gateway to the ClusterIP service for HTTPS re-encryption. It directs traffic to the specified service and namespace on port 8443. Replace `<service-name>.<namespace>.svc.cluster.local` with the actual service name and namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: nifi-vs\nspec:\n  gateways:\n  - nifi-gateway\n  hosts:\n  - nifi.my-domain.com\n  http:\n  - match:\n    - uri:\n        prefix: /\n    route:\n    - destination:\n        host: <service-name>.<namespace>.svc.cluster.local\n        port:\n          number: 8443\n```\n\n----------------------------------------\n\nTITLE: Creating a NifiUser Resource to Generate Client SSL Credentials in Kubernetes CLI\nDESCRIPTION: This CLI snippet demonstrates how to create a NifiUser custom resource in a Kubernetes cluster to generate new client certificates signed by the cluster's CA. This resource references the NiFi cluster and specifies a secret name to store the client credentials. The command uses a HEREDOC piped into kubectl apply, automating resource creation. The expected output is a Kubernetes secret containing CA and client certificates and keys, used for authenticating client applications against NiFi.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_3\n\nLANGUAGE: console\nCODE:\n```\ncat << EOF | kubectl apply -n nifi -f -\napiVersion:  nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: example-client\n  namespace: nifi\nspec:\n  clusterRef:\n    name: nifi\n  secretName: example-client-secret\nEOF\n```\n\n----------------------------------------\n\nTITLE: Defining External Services to Expose NiFi Internal Listeners in Kubernetes YAML\nDESCRIPTION: Configures Kubernetes external services to expose selected NiFi internal listeners externally. The service maps internal listeners by name to external ports and specifies the service type (e.g., LoadBalancer). This enables access to the NiFi UI, custom HTTP endpoints, and other services from outside the Kubernetes cluster, with ports directed accordingly through the specified service name.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    - type: \"https\"\n      name: \"https\"\n      containerPort: 8443\n    - type: \"cluster\"\n      name: \"cluster\"\n      containerPort: 6007\n    - type: \"s2s\"\n      name: \"s2s\"\n      containerPort: 10000\n    - type: \"prometheus\"\n      name: \"prometheus\"\n      containerPort: 9090\n    - type: \"load-balance\"\n      name: \"load-balance\"\n      containerPort: 6342\n    - name: \"http-tracking\"\n      containerPort: 8081\nexternalServices:\n  - name: cluster-access\n    spec:\n      portConfigs:\n        - internalListenerName: https\n          port: 443\n        - internalListenerName: http-tracking\n          port: 80\n      type: LoadBalancer\n```\n\n----------------------------------------\n\nTITLE: Defining NifiParameterContext Resource with YAML\nDESCRIPTION: This YAML defines a NifiParameterContext resource specifying parameters and secret references for use in a NiFi dataflow. It requires apiVersion, kind, metadata (name, namespace), and spec, which includes a reference to the NiFi cluster, a description, secrets containing sensitive values, and a list of parameters (name, value, description). Ensure referenced secrets exist and have necessary key-value pairs for sensitive parameters.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\n  namespace: demo\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      value: toto\n      description: toto\n\n```\n\n----------------------------------------\n\nTITLE: Verifying Kubernetes Services With kubectl - Console\nDESCRIPTION: This command-line output example shows the result of running kubectl get services after deploying externalServices as specified in the YAML configuration. It provides a list of Kubernetes services, including their names, types (e.g. LoadBalancer), internal cluster IPs, external IPs, ports, and uptime. This output helps confirm successful exposure of the NiFi cluster endpoints. Prerequisites: a running Kubernetes cluster and access to kubectl. Inputs: none; Outputs: tabular service info.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nkubectl get services\nNAME                TYPE           CLUSTER-IP    EXTERNAL-IP      PORT(S)                                AGE\ncluster-access      LoadBalancer   10.88.21.98   35.180.241.132   443:30421/TCP,80:30521/TCP             20d\n```\n\n----------------------------------------\n\nTITLE: Pod Port Configuration Reflecting Internal Listeners - YAML\nDESCRIPTION: This snippet shows an example of Kubernetes pod port configuration corresponding to the internal listeners defined for the NiFi cluster. Each port mapping specifies the `containerPort` matching the internal listener port, the `name` which associates the port with the appropriate listener, and the `protocol` set to TCP. This configuration allows Kubernetes to route traffic appropriately to the NiFi container ports exposed by the listeners. It serves as verification that the pod ports align with the internal listener definitions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\n    ports:\n    - containerPort: 8443\n      name: https\n      protocol: TCP\n    - containerPort: 6007\n      name: cluster\n      protocol: TCP\n    - containerPort: 6342\n      name: load-balance\n      protocol: TCP\n    - containerPort: 10000\n      name: s2s\n      protocol: TCP\n    - containerPort: 9090\n      name: prometheus\n      protocol: TCP\n```\n\n----------------------------------------\n\nTITLE: Defining Listener Types in Nifi (YAML)\nDESCRIPTION: This YAML snippet defines various internal listener configurations for a Nifi deployment. It specifies the listener type, name, and container port for different protocols such as https, cluster, s2s, prometheus and load-balance. It also includes an example of a custom listener with protocol TCP and port 1234. The sslSecrets section configures the settings related to ssl secrets.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/5_references/1_nifi_cluster/6_listeners_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  listenersConfig:\n    internalListeners:\n      - type: \"https\"\n        name: \"https\"\n        containerPort: 8443\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n      - type: \"prometheus\"\n        name: \"prometheus\"\n        containerPort: 9090\n      - type: \"load-balance\"\n        name: \"load-balance\"\n        containerPort: 6342\n      - name: \"my-custom-listener-port\"\n        containerPort: 1234\n        protocol: \"TCP\"\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n```\n\n----------------------------------------\n\nTITLE: Deleting Helm Release - Bash\nDESCRIPTION: This command deletes the Helm release named 'nifikop'. It removes all Kubernetes components associated with the chart and deletes the Helm release.  It is a crucial step in completely removing the operator from the Kubernetes cluster. Ensure you understand potential CRD removal consequences.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/2_setup/3_install/1_customizable_install_with_helm.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nhelm del nifikop\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio VirtualService for HTTP\nDESCRIPTION: This YAML snippet defines an Istio VirtualService that routes traffic intercepted by the Gateway to a specific service (NiFi in this case) on port 8080. It matches all URI prefixes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: nifi-vs\nspec:\n  gateways:\n  - nifi-gateway\n  hosts:\n  - nifi.my-domain.com\n  http:\n  - match:\n    - uri:\n        prefix: /\n    route:\n    - destination:\n        host: nifi\n        port:\n          number: 8080\n```\n\n----------------------------------------\n\nTITLE: Configure NiFiCluster with SSL\nDESCRIPTION: This YAML snippet configures a NifiCluster resource to use SSL. It defines internal listeners (https, cluster, s2s), specifies the SSL secret name, and indicates whether the secret should be created. The `readOnlyConfig` section configures `webProxyHosts` for secure operation. `create: true` indicates that the operator should create the TLS secret; otherwise it will look for the secret.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  readOnlyConfig:\n    # NifiProperties configuration that will be applied to the node.\n    nifiProperties:\n      webProxyHosts:\n        - nifistandard2.trycatchlearn.fr:8443\n        ...\n  ...\n  listenersConfig:\n    internalListeners:\n      - type: \"https\"\n        name: \"https\"\n        containerPort: 8443\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n```\n\n----------------------------------------\n\nTITLE: Deploying NifiDataflow Example (output)\nDESCRIPTION: This snippet is another example of a `NifiDataflow` resource definition in YAML. It defines metadata like name and namespace and specifies details such as cluster reference, bucket ID, flow ID, flow version, registry client, and update strategy. The `flowPosition` is different compared to the input dataflow to define a different position in the NiFi UI.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/4_manage_connections/1_deploy_connection.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\n--- \napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiDataflow\nmetadata:\n  name: output\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  bucketId: deedb9f6-65a4-44e9-a1c9-722008fcd0ba\n  flowId: fc5363eb-801e-432f-aa94-488838674d07\n  flowVersion: 2\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  skipInvalidComponent: true\n  skipInvalidControllerService: true\n  syncMode: always\n  updateStrategy: drain\n  flowPosition:\n    posX: 750\n    posY: 0\n```\n\n----------------------------------------\n\nTITLE: Configure NiFiCluster for Operator-Managed SSL (YAML)\nDESCRIPTION: This YAML snippet configures a `NifiCluster` resource to enable SSL managed by the operator. It specifies the HTTPS port (8443), defines internal listeners, and sets `sslSecrets.create` to `true` for automatic certificate generation, storing them in the secret named `test-nifikop`. It also includes an example for `webProxyHosts`. Requires the NifiCluster CRD to be installed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  readOnlyConfig:\n    # NifiProperties configuration that will be applied to the node.\n    nifiProperties:\n      webProxyHosts:\n        - nifistandard2.trycatchlearn.fr:8443\n        ...\n  ...\n  listenersConfig:\n    internalListeners:\n      - type: \"https\"\n        name: \"https\"\n        containerPort: 8443\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n```\n\n----------------------------------------\n\nTITLE: Cloning NiFiKop Repository (Bash)\nDESCRIPTION: Clones the NiFiKop source code repository from GitHub using Git and changes the current directory into the cloned project folder. This is the initial step for setting up the development environment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/6_contributing/1_developer_guide.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/konpyutaika/nifikop.git\ncd nifikop\n```\n\n----------------------------------------\n\nTITLE: Configuring VirtualService to Redirect HTTPS Traffic to Internal Service\nDESCRIPTION: This YAML snippet creates an Istio VirtualService to handle HTTPS traffic routed through the Gateway, forwarding requests to a cluster-local service on port 8443. The destination service name and namespace are placeholders for actual deployment details.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: nifi-vs\nspec:\n  gateways:\n  - nifi-gateway\n  hosts:\n  - nifi.my-domain.com\n  http:\n  - match:\n    - uri:\n        prefix: /\n    route:\n    - destination:\n        host: <service-name>.<namespace>.svc.cluster.local\n        port:\n          number: 8443\n```\n\n----------------------------------------\n\nTITLE: Pushing Docker Image - Bash\nDESCRIPTION: Pushes the Docker image to a specified Docker repository. It requires a Docker registry configured and accessible from the current machine. The image is tagged with the version defined in `verion/version.go` and the branch name. This allows the images to be deployed using helm charts later on.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/6_contributing/1_developer_guide.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n$ make docker-push\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi External Services using Nifikop YAML\nDESCRIPTION: Configures how to expose internal NiFi listeners externally using Kubernetes Services defined within the `externalServices` array. This example creates a Service named `cluster-access` of type `LoadBalancer`. It maps the `internalListenerName` 'https' to external port 443 and `http-tracking` to external port 80. Requires corresponding `internalListeners` to be defined in `listenersConfig`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    - type: \"https\"\n      name: \"https\"\n      containerPort: 8443\n    - type: \"cluster\"\n      name: \"cluster\"\n      containerPort: 6007\n    - type: \"s2s\"\n      name: \"s2s\"\n      containerPort: 10000\n    - type: \"prometheus\"\n      name: \"prometheus\"\n      containerPort: 9090\n    - type: \"load-balance\"\n      name: \"load-balance\"\n      containerPort: 6342\n    - name: \"http-tracking\"\n      containerPort: 8081\nexternalServices:\n  - name: cluster-access\n    spec:\n      portConfigs:\n        - internalListenerName: https\n          port: 443\n        - internalListenerName: http-tracking\n          port: 80\n      type: LoadBalancer\n```\n\n----------------------------------------\n\nTITLE: Uninstalling a Helm Chart\nDESCRIPTION: Deletes a specified Helm release (e.g., `nifikop`) and removes all associated Kubernetes components managed by that chart, except potentially CRDs.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nhelm del nifikop\n```\n\n----------------------------------------\n\nTITLE: Manually Applying NiFiKop CustomResourceDefinitions Using kubectl Bash Commands\nDESCRIPTION: Demonstrates manual deployment of NiFiKop operator's CustomResourceDefinitions (CRDs) by applying multiple YAML manifests using kubectl. These CRDs define the Kubernetes custom resource schema for managing NiFi clusters, users, user groups, data flows, parameter contexts, registry clients, node group autoscalers, and connections.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/2_deploy_nifikop/1_quick_start.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifinodegroupautoscalers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificonnections.yaml\n```\n\n----------------------------------------\n\nTITLE: Cloning NiFiKop Project - Bash\nDESCRIPTION: This command clones the NiFiKop project from the specified GitHub repository. It downloads the source code and sets up the local environment for further development or modification. There are no dependencies required other than having git installed and a network connection.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/6_contributing/1_developer_guide.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/konpyutaika/nifikop.git\ncd nifikop\n```\n\n----------------------------------------\n\nTITLE: Configuring External Services for NiFi Cluster Access\nDESCRIPTION: This comprehensive YAML snippet shows how to configure both internal listeners and external Kubernetes services within the NifiCluster spec. It defines standard internal listeners and then uses the `externalServices` section to create a LoadBalancer service named 'cluster-access' that exposes the 'https' and 'http-tracking' internal listeners on external ports 443 and 80 respectively.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    - type: \"https\"\n      name: \"https\"\n      containerPort: 8443\n    - type: \"cluster\"\n      name: \"cluster\"\n      containerPort: 6007\n    - type: \"s2s\"\n      name: \"s2s\"\n      containerPort: 10000\n    - type: \"prometheus\"\n      name: \"prometheus\"\n      containerPort: 9090\n    - type: \"load-balance\"\n      name: \"load-balance\"\n      containerPort: 6342\n    - name: \"http-tracking\"\n      containerPort: 8081\nexternalServices:\n  - name: cluster-access\n    spec:\n      portConfigs:\n        - internalListenerName: https\n          port: 443\n        - internalListenerName: http-tracking\n          port: 80\n      type: LoadBalancer\n```\n\n----------------------------------------\n\nTITLE: Example NifiUserGroup Manifest (YAML)\nDESCRIPTION: This YAML manifest demonstrates how to define a `NifiUserGroup` resource in Kubernetes. It specifies the group's identity within NiFi, links it to a specific NiFi cluster (`nc`), assigns users (referenced by their `NifiUser` resource names), and defines an access policy granting read access to `/counters` globally.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/5_references/6_nifi_usergroup.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  identity: \"My Special Group\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n    - name: nc-controller.nifikop.mgt.cluster.local\n  accessPolicies:\n    - type: global\n      action: read\n      resource: /counters\n```\n\n----------------------------------------\n\nTITLE: Cloning NiFiKop Repository (bash)\nDESCRIPTION: This snippet demonstrates how to clone the NiFiKop source code repository from GitHub and navigate into the project directory using bash commands. This is typically the initial step when starting development.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/6_contributing/1_developer_guide.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/konpyutaika/nifikop.git\ncd nifikop\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Namespace for Prometheus Monitoring - Console\nDESCRIPTION: This snippet creates a dedicated Kubernetes namespace called 'monitoring-system' where Prometheus and related resources will be deployed. It uses the 'kubectl create namespace' command to isolate monitoring components from other workloads and aid in organization and security within the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_5\n\nLANGUAGE: console\nCODE:\n```\nkubectl create namespace monitoring-system\n```\n\n----------------------------------------\n\nTITLE: Updating Helm Repositories (Console)\nDESCRIPTION: Command to update the list of available charts from all configured Helm repositories. Running this after adding the KEDA repository ensures that the charts are indexed and available for installation. Requires Helm to be installed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nhelm repo update\n```\n\n----------------------------------------\n\nTITLE: Creating NiFi User Client Certificate via NifiUser CRD - YAML\nDESCRIPTION: Applies a NifiUser custom resource manifest using kubectl and standard input to request user-specific credentials for the NiFi cluster. The operation generates a new secret containing a client certificate, key, and CA certificate, which can be mounted in pods or accessed locally. Requires Nifikop operator and a running NiFi cluster. Parameters: user and cluster names, secret name. The output is a Kubernetes secret storing the generated credentials.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_3\n\nLANGUAGE: console\nCODE:\n```\ncat << EOF | kubectl apply -n nifi -f -\napiVersion:  nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: example-client\n  namespace: nifi\nspec:\n  clusterRef:\n    name: nifi\n  secretName: example-client-secret\nEOF\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Node Configuration in YAML\nDESCRIPTION: This YAML snippet demonstrates the structure and configurable options for defining a NiFi node's configuration.  It showcases default group settings, including storage configurations (provenance and logs), external volume configurations, pod metadata, image pull policy, and other node-specific settings.  Key aspects include provenance storage limits, user ID for running the NiFi image, and Kubernetes PVC specifications for persistent storage.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/5_references/1_nifi_cluster/3_node_config.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n   default_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      # Set this to true if the instance is a node in a cluster.\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#basic-cluster-setup\n      isNode: true\n      # Additionnal metadata to merge to the pod associated\n      podMetadata:\n        annotations:\n          node-annotation: \"node-annotation-value\"\n        labels:\n          node-label: \"node-label-value\"\n      # Docker image used by the operator to create the node associated\n      # https://hub.docker.com/r/apache/nifi/\n#      image: \"apache/nifi:1.11.2\"\n      # nodeAffinity can be specified, operator populates this value if new pvc added later to node\n      # https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity\n#      nodeAffinity:\n      # imagePullPolicy define the pull policy for NiFi cluster docker image\n      imagePullPolicy: IfNotPresent\n      # priorityClassName define the name of the priority class to be applied to these nodes\n      priorityClassName: \"example-priority-class-name\"\n      # externalVolumeConfigs specifies a list of volume to mount into the main container.\n      externalVolumeConfigs:\n        - name: example-volume\n          mountPath: \"/opt/nifi/example\"\n          secret:\n            secretName: \"raw-controller\"\n      # storageConfigs specifies the node related configs\n      storageConfigs:\n        # Name of the storage config, used to name PV to reuse into sidecars for example.\n        - name: provenance-repository\n          # Path where the volume will be mount into the main nifi container inside the pod.\n          mountPath: \"/opt/nifi/provenance_repository\"\n          # Kubernetes PVC spec\n          # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n```\n\n----------------------------------------\n\nTITLE: Configuring External NiFi Cluster Resource in YAML\nDESCRIPTION: Example YAML configuration for declaring an external NiFi cluster to be referenced by other NiFi resources. It defines the root process group ID, node URI template, node IDs, and authentication details.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/3_tasks/1_nifi_cluster/4_external_cluster.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: externalcluster\nspec:\n  # rootProcessGroupId contains the uuid of the root process group for this cluster.\n  rootProcessGroupId: 'd37bee03-017a-1000-cff7-4eaaa82266b7'\n  # nodeURITemplate used to dynamically compute node uri.\n  nodeURITemplate: 'nifi0%d.integ.mapreduce.m0.p.fti.net:9090'\n  # all node requiresunique id\n  nodes:\n    - id: 1\n    - id: 2\n    - id: 3\n  # type defines if the cluster is internal (i.e manager by the operator) or external.\n  # :Enum={\"external\",\"internal\"}\n  type: 'external'\n  # clientType defines if the operator will use basic or tls authentication to query the NiFi cluster.\n  # Enum={\"tls\",\"basic\"}\n  clientType: 'basic'\n  # secretRef reference the secret containing the informations required to authenticate to the cluster.\n  secretRef:\n    name: nifikop-credentials\n    namespace: nifikop-nifi\n```\n\n----------------------------------------\n\nTITLE: Querying Kubernetes HPA Status with kubectl (Console)\nDESCRIPTION: This console snippet demonstrates how to inspect the status of the Kubernetes Horizontal Pod Autoscaler (HPA) for the NiFiNodeGroupAutoscaler resource using kubectl. It shows the output of kubectl get hpa scoped to the clusters namespace, listing HPA names, targets, current pod counts, and related metrics. Prerequisites include access to a Kubernetes cluster where the described ScaledObject and autoscaler have been deployed, and sufficient permission to run kubectl commands.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_10\n\nLANGUAGE: console\nCODE:\n```\nkubectl get -n clusters hpa\nNAME                REFERENCE                                                TARGETS         MINPODS   MAXPODS   REPLICAS   AGE\nkeda-hpa-cluster    NifiNodeGroupAutoscaler/nifinodegroupautoscaler-sample     1833m/2 (avg)   1         3         2          25d\n\n```\n\n----------------------------------------\n\nTITLE: Getting Status for Helm Deployment\nDESCRIPTION: This snippet uses Helm to get the status of a specific Helm deployment. It requires Helm and the release name. The command retrieves detailed information about the deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nhelm status nifikop\n```\n\n----------------------------------------\n\nTITLE: Manually Updating NiFi Flow Encryption Algorithm using Shell\nDESCRIPTION: These shell commands utilize the NiFi Encrypt-Config Tool (`encrypt-config.sh`) to re-encrypt sensitive properties within NiFi flow configuration files (`flow.xml.gz` and `flow.json.gz`). This is necessary when upgrading to Nifikop v1.4.0 if the default encryption algorithm was used previously. It requires the `nifi.properties` file, the flow files, and the sensitive properties key (`PROPERTIES_KEY`) to execute.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/7_upgrade_guides/4_v1.3.1_to_v1.4.0.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nencrypt-config.sh -n nifi.properties -f flow.xml.gz -x -s PROPERTIES_KEY -A NIFI_PBKDF2_AES_GCM_256\n```\n\nLANGUAGE: sh\nCODE:\n```\nencrypt-config.sh -n nifi.properties -f flow.json.gz -x -s PROPERTIES_KEY -A NIFI_PBKDF2_AES_GCM_256\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio Gateway for HTTPS\nDESCRIPTION: This YAML snippet defines an Istio Gateway configured to handle HTTPS traffic, decrypting it before forwarding it to the NiFi service. It listens on port 443 and utilizes a secret ('my-secret') for TLS termination. The `mode: SIMPLE` setting indicates basic TLS termination.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: nifi-gateway\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 443\n      name: https\n      protocol: HTTPS\n    tls:\n      mode: SIMPLE\n      credentialName: my-secret\n    hosts:\n    - nifi.my-domain.com\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Local Operator Development in Bash\nDESCRIPTION: This snippet illustrates setting up environment variables required to run the NiFiKop operator locally. Variables include KUBECONFIG for Kubernetes credentials, WATCH_NAMESPACE to specify the namespace scope, POD_NAME for naming the operator pod, LOG_LEVEL for logging verbosity, and OPERATOR_NAME as the operator identifier. Properly setting these variables is key to connecting and monitoring the operator during development.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/6_contributing/1_developer_guide.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nKUBECONFIG={path/to/your/kubeconfig}\nWATCH_NAMESPACE={namespace_to_watch}\nPOD_NAME={name for operator pod}\nLOG_LEVEL=Debug\nOPERATOR_NAME=ide\n```\n\n----------------------------------------\n\nTITLE: Installing cert-manager using kubectl\nDESCRIPTION: This snippet applies the cert-manager CustomResourceDefinitions and manifests directly from the Jetstack GitHub repository to install cert-manager v1.7.2 using kubectl. It sets up the necessary resources for managing certificates within Kubernetes, which is required for NiFiKop's security features.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/2_deploy_nifikop/1_quick_start.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install the CustomResourceDefinitions and cert-manager itself\nkubectl apply -f \\\n    https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.yaml\n\n```\n\n----------------------------------------\n\nTITLE: Configuring conversion webhook for NiFiKop CRDs using YAML\nDESCRIPTION: This YAML snippet provides an example of the annotations and spec configuration required to enable a conversion webhook for CRDs. The webhook facilitates resource version conversion between `v1alpha1` and `v1`. The user must specify the namespace, certificate name, and webhook service name that correspond to their Helm deployment context. This is a prerequisite for advanced CRD version management and requires cert-manager integration for TLS certificate injection.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/2_deploy_nifikop/1_quick_start.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n```\n\n----------------------------------------\n\nTITLE: Apply NiFiKop CRDs manually - kubectl\nDESCRIPTION: This set of commands applies the NiFiKop Custom Resource Definitions (CRDs) to the Kubernetes cluster using kubectl.  This is necessary when installing the chart with `--skip-crds`. It ensures that the Kubernetes API server recognizes the NiFiKop custom resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifinodegroupautoscalers.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring SSL for NiFi Cluster in YAML\nDESCRIPTION: This snippet illustrates how to enable SSL on NiFi cluster nodes by configuring the 'listenersConfig' with 'https' type, specifying container ports, and creating SSL secrets. It also shows how to set web proxy hosts for secure request handling. The configuration requires setting 'sslSecrets.create' to true to generate certificates or false to reference existing ones.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  readOnlyConfig:\n    # NifiProperties configuration that will be applied to the node.\n    nifiProperties:\n      webProxyHosts:\n        - nifistandard2.trycatchlearn.fr:8443\n        ...\n  ...\n  listenersConfig:\n    internalListeners:\n      - type: \"https\"\n        name: \"https\"\n        containerPort: 8443\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n```\n\n----------------------------------------\n\nTITLE: Configuring Managed NiFi Users and Groups in NifiCluster YAML\nDESCRIPTION: This YAML snippet illustrates how to define users for the pre-configured 'managed-admins' and 'managed-readers' groups directly within the `NifiCluster` CRD spec. By listing users under `managedAdminUsers` and `managedReaderUsers` with `identity` and `name`, the NiFiKop operator automatically creates the corresponding `NifiUser` resources and adds them to the respective managed groups, simplifying access control configuration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/2_manage_users_and_accesses/3_managed_groups.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  ...\n  oneNifiNodePerNode: false\n  propagateLabels: true\n  managedAdminUsers:\n    -  identity: \"alexandre.guitton@konpyutaika.com\"\n       name: \"aguitton\"\n    -  identity: \"nifiuser@konpyutaika.com\"\n       name: \"nifiuser\"\n  managedReaderUsers:\n    -  identity: \"toto@konpyutaika.com\"\n       name: \"toto\"\n    ...\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFiKop with OIDC in YAML\nDESCRIPTION: This YAML snippet provides an example configuration for deploying NiFi with OpenId Connect support using NiFiKop. It sets the discovery URL, client ID, secret, and includes the DN mapping configuration in the overrideConfigs field. The snippet is intended for Kubernetes deployment, requiring prior setup of the NifiCluster custom resource.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/5_users_authentication/1_oidc.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  readOnlyConfig:\n    # NifiProperties configuration that will be applied to the node.\n    nifiProperties:\n      webProxyHosts:\n        - nifistandard2.trycatchlearn.fr:8443\n      # Additionnal nifi.properties configuration that will override the one produced based\n      # on template and configurations.\n      overrideConfigs: |\n        nifi.security.user.oidc.discovery.url=<oidc server discovery url>\n        nifi.security.user.oidc.client.id=<oidc client's id>\n        nifi.security.user.oidc.client.secret=<oidc client's secret>\n        nifi.security.identity.mapping.pattern.dn=CN=([^,]*)(?:, (?:O|OU)=.*)?\n        nifi.security.identity.mapping.value.dn=$1\n        nifi.security.identity.mapping.transform.dn=NONE\n      ...\n   ...\n```\n\n----------------------------------------\n\nTITLE: Deploy Prometheus Custom Resource for NiFi Metrics\nDESCRIPTION: This YAML defines a Prometheus custom resource which configures Prometheus server parameters, resource limits, and evaluation intervals for monitoring NiFi clusters. It specifies access to the admin API, disablement of certain features, and resource requests. Dependencies include the Prometheus Operator CRDs; inputs are resource specifications; outputs are a running Prometheus server configured to scrape metrics from NiFi nodes for autoscaling.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_4\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: Prometheus\nmetadata:\n  name: prometheus\n  namespace: monitoring-system\nspec:\n  enableAdminAPI: false\n  evaluationInterval: 30s\n  logLevel: debug\n  podMonitorSelector:\n    matchExpressions:\n    - key: release\n      operator: In\n      values:\n      - prometheus\n  resources:\n    requests:\n      memory: 400Mi\n  scrapeInterval: 30s\n  serviceAccountName: prometheus\n  serviceMonitorSelector:\n    matchExpressions:\n    - key: app\n      operator: In\n      values:\n      - nifi-cluster\n```\n\n----------------------------------------\n\nTITLE: Configuring NifiNodeGroupAutoscaler CRD\nDESCRIPTION: YAML definition for a NifiNodeGroupAutoscaler custom resource that defines autoscaling parameters for a NiFi cluster, including node group selection, scaling strategies, and custom configurations.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiNodeGroupAutoscaler\nmetadata:\n  name: nifinodegroupautoscaler-sample\n  namespace: clusters\nspec:\n  # contains the reference to the NifiCluster with the one the node group autoscaler is linked.\n  clusterRef:\n    name: cluster\n    namespace: clusters\n  # defines the id of the NodeConfig contained in NifiCluster.Spec.NodeConfigGroups\n  nodeConfigGroupId: auto_scaling\n  # readOnlyConfig can be used to pass Nifi node config\n  # which has type read-only these config changes will trigger rolling upgrade\n  readOnlyConfig:\n    nifiProperties:\n      overrideConfigs: |\n        nifi.ui.banner.text=NiFiKop - Scale Group\n  # This is an example of a node config you can apply to each replica in this node group.\n  # Any settings here will override those in the configured nodeConfigGroupId\n#  nodeConfig:\n#    nodeSelector:\n#      node_type: high-mem\n  # The selector used to identify nodes in NifiCluster.Spec.Nodes this autoscaler will manage\n  # Use Node.Labels in combination with this selector to clearly define which nodes will be managed by this autoscaler \n  nodeLabelsSelector: \n    matchLabels:\n      nifi_cr: cluster\n      nifi_node_group: auto-scaling\n  # the strategy used to decide how to add nodes to a nifi cluster\n  upscaleStrategy: simple\n  # the strategy used to decide how to remove nodes from an existing cluster\n  downscaleStrategy: lifo\n```\n\n----------------------------------------\n\nTITLE: Installing cert-manager Using Helm 3\nDESCRIPTION: Series of commands to install cert-manager v1.7.2 using Helm 3. Includes applying CustomResourceDefinitions, adding the Jetstack Helm repository, and installing cert-manager in its own namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/2_deploy_nifikop/1_quick_start.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Install CustomResourceDefinitions first\nkubectl apply --validate=false -f \\\n   https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.crds.yaml\n\n# Add the jetstack helm repo\nhelm repo add jetstack https://charts.jetstack.io\nhelm repo update\n\n# You have to create the namespace before executing following command\nhelm install cert-manager \\\n    --namespace cert-manager \\\n    --version v1.7.2 jetstack/cert-manager\n```\n\n----------------------------------------\n\nTITLE: Defining NiFiUserGroup Custom Resource in YAML\nDESCRIPTION: This YAML snippet defines an instance of the NifiUserGroup custom resource for the Konpyutaika Nifikop operator. It specifies metadata such as the resource's name and the spec section that includes references to the target NiFi cluster, the users within the group, and their access policies. The snippet demonstrates how to declaratively manage user group permissions and memberships by linking to existing NiFiUser resources and cluster references.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/5_references/6_nifi_usergroup.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n    - name: nc-controller.nifikop.mgt.cluster.local\n  accessPolicies:\n    - type: global\n      action: read\n      resource: /counters\n```\n\n----------------------------------------\n\nTITLE: Configuring kubectl for GKE Cluster Access (Bash)\nDESCRIPTION: Retrieves GKE cluster credentials and configures kubectl to use the context for the specified cluster, zone, and project. Requires gcloud CLI to be authenticated.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/2_deploy_nifikop/1_quick_start.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngcloud container clusters get-credentials CLUSTER_NAME --zone ZONE_NAME --project PROJECT_NAME.\n```\n\n----------------------------------------\n\nTITLE: Listing Managed NiFi User Groups via Kubernetes CLI\nDESCRIPTION: This console snippet demonstrates how to retrieve and display the list of managed NiFi user groups in the 'nifikop' namespace using the Kubernetes command-line tool. The output lists three groups: 'managed-admins', 'managed-nodes', and 'managed-readers', which are automatically created and managed by the operator based on the configuration in the NifiCluster resource. This command helps cluster administrators verify group existence and age, facilitating cluster user access management.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/2_manage_users_and_accesses/3_managed_groups.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nkubectl get -n nifikop nifiusergroups.nifi.konpyutaika.com \nNAME              AGE\nmanaged-admins    6d7h\nmanaged-nodes     6d7h\nmanaged-readers   6d7h\n```\n\n----------------------------------------\n\nTITLE: Updating OpenShift NiFi Deployment Configuration with UID (sed command)\nDESCRIPTION: Modifies the existing OpenShift YAML configuration file to replace placeholder UID with the actual UID retrieved from OpenShift, ensuring correct security context setup for NiFi deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nsed -i \"s/1000690000/$uid/g\" config/samples/openshift.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for React Native Website\nDESCRIPTION: These commands are used to install the dependencies required to run the React Native website locally. The `cd` commands navigate the user through the directory structure and `yarn` installs the necessary packages.  This setup is powered by Docusaurus.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/README.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\n1.  `cd react-native-website` to go into the project root.\n1.  `yarn` to install the website's workspace dependencies.\n1.  `cd website` to go into the website portion of the project.\n```\n\n----------------------------------------\n\nTITLE: Migrating NiFiKop Custom Resources (JavaScript)\nDESCRIPTION: This Node.js script connects to the Kubernetes API using the `@kubernetes/client-node` library to migrate specified NiFiKop custom resources. It loads kubeconfig, disables TLS verification, lists resources from the old API group and namespace, creates new resources in the target API group by copying spec and metadata (excluding owner references), and then copies the status from the old resource to the newly created one. It uses `minimist` to parse `--type` and `--namespace` command-line arguments.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nprocess.env['NODE_TLS_REJECT_UNAUTHORIZED'] = 0;\nconst k8s = require('@kubernetes/client-node');\n\nconst kc = new k8s.KubeConfig();\nkc.loadFromDefault();\n\nconst k8sApi = kc.makeApiClient(k8s.CustomObjectsApi);\n\nconst KONPYUTAIKA_GROUP = 'nifi.konpyutaika.com';\nconst KONPYUTAIKA_GROUP_VERSION = 'v1alpha1';\nconst ORANGE_GROUP = 'nifi.orange.com';\nconst ORANGE_GROUP_VERSION = 'v1alpha1';\n\nconst call = async (SRC_GRP, SRC_GRP_VER, DST_GRP, DST_GRP_VER, KIND_PLURAL, NAMESPACE) => {\n\tconsole.log(`Listing ${KIND_PLURAL} of ${SRC_GRP}/${SRC_GRP_VER} in ${NAMESPACE}...`);\n\tconst listResources = (await k8sApi.listNamespacedCustomObject(SRC_GRP, SRC_GRP_VER, NAMESPACE, KIND_PLURAL)).body.items;\n\treturn Promise.all(listResources.map(async (resource) => {\n\t\ttry {\n\t\t\tconsole.log(`Found ${resource.kind} \"${resource.metadata.name}\" of ${resource.apiVersion} in ${NAMESPACE}`);\n\n\t\t\tif (resource.metadata.ownerReferences) {\n\t\t\t\tconsole.log(`${resource.kind} ${resource.metadata.name} mananged by something else (ownerRefereces is set).`);\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tconst bodyResource = {\n\t\t\t\tapiVersion: `${DST_GRP}/${DST_GRP_VER}`,\n\t\t\t\tkind: resource.kind,\n\t\t\t\tmetadata: {\n\t\t\t\t\tname: resource.metadata.name,\n\t\t\t\t\tannotations: resource.metadata.annotations,\n\t\t\t\t\tlabels: resource.metadata.labels\n\t\t\t\t},\n\t\t\t\tspec: resource.spec\n\t\t\t};\n\n\t\t\tconsole.log(`Creating ${bodyResource.kind} \"${bodyResource.metadata.name}\" of ${bodyResource.apiVersion} in ${NAMESPACE}...`);\n\t\t\tconst newResource = (await k8sApi.createNamespacedCustomObject(DST_GRP, DST_GRP_VER, NAMESPACE, KIND_PLURAL, bodyResource)).body;\n\t\t\tconsole.log('...done creating.');\n\n\t\t\tconst bodyStatus = {\n\t\t\t\tapiVersion: newResource.apiVersion,\n\t\t\t\tkind: newResource.kind,\n\t\t\t\tmetadata: {\n\t\t\t\t\tname: newResource.metadata.name,\n\t\t\t\t\tresourceVersion: newResource.metadata.resourceVersion\n\t\t\t\t},\n\t\t\t\tstatus: resource.status\n\t\t\t};\n\n\t\t\tconsole.log(`Copying status from ${resource.kind} \"${resource.metadata.name}\" of ${newResource.apiVersion} to ${newResource.kind} \"${newResource.metadata.name}\" of ${newResource.apiVersion} in ${NAMESPACE}...`);\n\t\t\tconst newResourceWithStatus = (await k8sApi.replaceNamespacedCustomObjectStatus(DST_GRP, DST_GRP_VER, NAMESPACE, KIND_PLURAL, bodyStatus.metadata.name, bodyStatus)).body;\n\t\t\tconsole.log('...done copying.');\n\t\t\treturn newResourceWithStatus;\n\t\t}\n\t\tcatch (e) {\n\t\t\tconsole.error(e.body ? e.body.message ? e.body.message : e.body : e);\n\t\t}\n\t}));\n};\n\nconst argv = require('minimist')(process.argv.slice(2));\n\nlet NAMESPACE = argv.namespace ? argv.namespace.length > 0 ? argv.namespace : 'default' : 'default';\nlet KIND_PLURAL = {\n\tcluster: 'nificlusters',\n\tdataflow: 'nifidataflows',\n\tparametercontext: 'nifiparametercontexts',\n\tregistryclient: 'nifiregistryclients',\n\tuser: 'nifiusers',\n\tusergroup: 'nifiusergroups',\n};\n\nif (!argv.type) {\n\tconsole.error('Type not provided');\n\tprocess.exit(1);\n}\n\nif (!KIND_PLURAL[argv.type]) {\n\tconsole.error(`Type ${argv.type} is not one of the following types: ${Object.keys(KIND_PLURAL)}`);\n\tprocess.exit(1);\n}\n\nconsole.log(`########### START: ${KIND_PLURAL[argv.type]} ###########`);\ncall( ORANGE_GROUP, ORANGE_GROUP_VERSION, KONPYUTAIKA_GROUP, KONPYUTAIKA_GROUP_VERSION, KIND_PLURAL[argv.type], NAMESPACE)\n\t.then(r => console.log('############ END ############'))\n\t.catch(e => console.error(e));\n```\n\n----------------------------------------\n\nTITLE: Declaring an External NiFiCluster Resource in YAML\nDESCRIPTION: Defines the YAML configuration to declare an external NiFi cluster for management via the NifiKop operator. The configuration includes cluster identification, node URL templating, authentication type, secret reference, and cluster type. It enables the operator to interact with the external cluster using specified parameters, facilitating automation and uniform management across environments.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/3_tasks/1_nifi_cluster/4_external_cluster.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: externalcluster\nspec:\n  rootProcessGroupId: 'd37bee03-017a-1000-cff7-4eaaa82266b7'\n  nodeURITemplate: 'nifi0%d.integ.mapreduce.m0.p.fti.net:9090'\n  nodes:\n    - id: 1\n    - id: 2\n    - id: 3\n  type: 'external'\n  clientType: 'basic'\n  secretRef:\n    name: nifikop-credentials\n    namespace: nifikop-nifi\n```\n\n----------------------------------------\n\nTITLE: Pushing Docker Image with Make\nDESCRIPTION: This bash command pushes the Docker image to a Docker registry using `make docker-push`. It assumes the Docker image has been built and the user is authenticated with a Docker registry. The specific registry is configured in `DOCKER_REPO_BASE`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/6_contributing/1_developer_guide.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n$ make docker-push\n```\n\n----------------------------------------\n\nTITLE: Checking NiFiKop Helm Deployment Status\nDESCRIPTION: Helm command to retrieve the status of the NiFiKop deployment in Kubernetes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nhelm status nifikop\n```\n\n----------------------------------------\n\nTITLE: NifiParameterContextStatus Structure\nDESCRIPTION: Represents the observed state of a NiFi Parameter Context, including its ID, revision version, latest update request, and secrets' resource version. Used for monitoring and synchronization purposes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/5_references/4_nifi_parameter_context.md#_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\nNifiParameterContextStatus:\n  id: string\n  version: int64\n  latestUpdateRequest: [ParameterContextUpdateRequest]\n  latestSecretsResourceVersion: [SecretResourceVersion]\n```\n\n----------------------------------------\n\nTITLE: Manually Deleting Nifikop CRDs in Kubernetes Using Bash\nDESCRIPTION: This snippet provides bash commands to manually delete Nifikop Custom Resource Definitions (CRDs) from the Kubernetes cluster. Since Helm does not remove CRDs by default on deletion, manual cleanup is required if CRDs are no longer needed. It is critical to avoid deleting CRDs without caution because it will remove all custom resources created by those definitions, potentially causing data loss or service disruption.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete crd nificlusters.nifi.konpyutaika.com\nkubectl delete crd nifiusers.nifi.konpyutaika.com\nkubectl delete crd nifiusergroups.nifi.konpyutaika.com\nkubectl delete crd nifiregistryclients.nifi.konpyutaika.com\nkubectl delete crd nifiparametercontexts.nifi.konpyutaika.com\nkubectl delete crd nifidataflows.nifi.konpyutaika.com\n```\n\n----------------------------------------\n\nTITLE: Applying NiFiKop CRDs with Kubectl\nDESCRIPTION: These commands apply the Custom Resource Definitions (CRDs) for NiFiKop using `kubectl`. These CRDs define the custom resources that NiFiKop manages in the Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/6_contributing/1_developer_guide.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing Nifikop Helm Chart with Parameters (Bash)\nDESCRIPTION: Installs the Nifikop Helm chart using `helm install`, providing a release name (`nifikop`) and setting the `namespaces` parameter using the `--set` flag.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop --set namespaces={\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Encrypt NiFi Configurations using encrypt-config.sh\nDESCRIPTION: This shell script uses the NiFi encrypt-config.sh tool to update the flow configuration files (flow.xml.gz and flow.json.gz) with the new `NIFI_PBKDF2_AES_GCM_256` algorithm. It requires access to the `nifi.properties` file and the `PROPERTIES_KEY` to decrypt and re-encrypt sensitive properties. Ensure the `encrypt-config.sh` tool is available in your environment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/7_upgrade_guides/4_v1.3.1_to_v1.4.0.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nencrypt-config.sh -n nifi.properties -f flow.xml.gz -x -s PROPERTIES_KEY -A NIFI_PBKDF2_AES_GCM_256\nencrypt-config.sh -n nifi.properties -f flow.json.gz -x -s PROPERTIES_KEY -A NIFI_PBKDF2_AES_GCM_256\n```\n\n----------------------------------------\n\nTITLE: Manually applying NiFiKop CustomResourceDefinitions using kubectl in Bash\nDESCRIPTION: This snippet includes commands to manually apply the NiFiKop operator CRDs to the cluster using kubectl. This is required if the user opts out of CRD deployment via Helm by using `--skip-crds`. The CRDs manage custom resources like NiFi clusters, users, user groups, data flows, parameter contexts, and registry clients. The commands depend on kubectl access to the Kubernetes cluster and the raw files hosted on the NiFiKop GitHub repository.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/2_deploy_nifikop/1_quick_start.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\n```\n\n----------------------------------------\n\nTITLE: Example package.json Node.js Migration Project JSON\nDESCRIPTION: This is a complete example of the `package.json` file after setting up the Node.js project for the migration script. It includes metadata like name, version, description, and defines the 'start' script. It also lists the required `@kubernetes/client-node` and `minimist` dependencies with their versions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"nifikop_crd_migration\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Script to migrate from the old CRDs to the new CRDs.\",\n  \"main\": \"index.js\",\n  \"scripts\": {\n    \"start\": \"node --no-warnings index.js\",\n    \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\"\n  },\n  \"keywords\": [\n    \"K8S\",\n    \"NiFiKop\",\n    \"CRDs\"\n  ],\n  \"license\": \"ISC\",\n  \"dependencies\": {\n    \"@kubernetes/client-node\": \"^0.16.3\",\n    \"minimist\": \"^1.2.6\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Cloning the NiFiKop repository using Git\nDESCRIPTION: Clones the NiFiKop project repository from GitHub and navigates into the project directory, setting up the local environment for development and deployment tasks.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/6_contributing/1_developer_guide.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/konpyutaika/nifikop.git\ncd nifikop\n```\n\n----------------------------------------\n\nTITLE: Configuring Webhook Conversion for CRDs in YAML\nDESCRIPTION: This YAML block demonstrates the annotation and conversion configuration required for enabling a webhook conversion strategy on Nifikop CRDs. Dependencies include cert-manager and a running webhook service. Key parameters are: cert-manager.io/inject-ca-from for automatic CA injection, namespace, certificate_name, and webhook_service_name. Inputs: CRD YAML specification with placeholders. Outputs: Upgraded CRDs supporting v1alpha1 and v1 conversions via custom webhook. Limitation: Proper certificate and service setup must exist.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n```\n\n----------------------------------------\n\nTITLE: Defining Managed NiFi User Groups in NifiCluster Spec YAML\nDESCRIPTION: This YAML snippet shows how to define managed admin and reader user groups within the NifiCluster custom resource specification. It specifies lists of users with their identities and names under the fields 'managedAdminUsers' and 'managedReaderUsers'. The operator uses these definitions to create and manage corresponding NifiUsers and NifiUserGroups automatically. Key parameters include identity (user's identity string) and name (username). This approach avoids manually creating multiple YAMLs for each user and simplifies group access management for NiFi clusters in Kubernetes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/2_manage_users_and_accesses/3_managed_groups.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  ...\n  oneNifiNodePerNode: false\n  propagateLabels: true\n  managedAdminUsers:\n    -  identity: \"alexandre.guitton@konpyutaika.com\"\n       name: \"aguitton\"\n    -  identity: \"nifiuser@konpyutaika.com\"\n       name: \"nifiuser\"\n  managedReaderUsers:\n    -  identity: \"toto@konpyutaika.com\"\n       name: \"toto\"\n    ...\n```\n\n----------------------------------------\n\nTITLE: Specifying OIDC configuration in NiFiKop custom resource YAML\nDESCRIPTION: This YAML snippet demonstrates how to override nifi.properties settings within a NiFiCluster custom resource managed by NiFiKop, including the OIDC discovery URL, client ID, secret, and identity mapping pattern. It allows automated deployment and configuration of NiFi with OIDC support.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/3_tasks/2_security/2_authentication/1_oidc.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\\nkind: NifiCluster\\nspec:\\n  ...\\n  readOnlyConfig:\\n    # NifiProperties configuration that will be applied to the node.\\n    nifiProperties:\\n      webProxyHosts:\\n        - nifistandard2.trycatchlearn.fr:8443\\n      # Additionnal nifi.properties configuration that will override the one produced based\\n      # on template and configurations.\\n      overrideConfigs: |\\n        nifi.security.user.oidc.discovery.url=<oidc server discovery url>\\n        nifi.security.user.oidc.client.id=<oidc client's id>\\n        nifi.security.user.oidc.client.secret=<oidc client's secret>\\n        nifi.security.identity.mapping.pattern.dn=CN=([^,]*)(?:, (?:O|OU)=.*)?\\n        nifi.security.identity.mapping.value.dn=$1\\n        nifi.security.identity.mapping.transform.dn=NONE\n```\n\n----------------------------------------\n\nTITLE: Define External NiFiCluster Resource - YAML\nDESCRIPTION: This YAML snippet demonstrates how to define a NifiCluster resource for an external NiFi cluster. It includes essential fields like rootProcessGroupId, nodeURITemplate, node IDs, cluster type (external), client type (basic), and a reference to a secret containing authentication credentials.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/1_manage_clusters/3_external_cluster.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: externalcluster\nspec:\n  # rootProcessGroupId contains the uuid of the root process group for this cluster.\n  rootProcessGroupId: 'd37bee03-017a-1000-cff7-4eaaa82266b7'\n  # nodeURITemplate used to dynamically compute node uri.\n  nodeURITemplate: 'nifi0%d.integ.mapreduce.m0.p.fti.net:9090'\n  # all node requiresunique id\n  nodes:\n    - id: 1\n    - id: 2\n    - id: 3\n  # type defines if the cluster is internal (i.e manager by the operator) or external.\n  # :Enum={\\\"external\\\",\\\"internal\\\"}\n  type: 'external'\n  # clientType defines if the operator will use basic or tls authentication to query the NiFi cluster.\n  # Enum={\\\"tls\\\",\\\"basic\\\"}\n  clientType: 'basic'\n  # secretRef reference the secret containing the informations required to authenticate to the cluster.\n  secretRef:\n    name: nifikop-credentials\n    namespace: nifikop-nifi\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFiKop operator Helm chart with resource and namespace configuration using Bash\nDESCRIPTION: This snippet shows the Helm command to deploy the NiFiKop Kubernetes operator chart from an OCI registry. It includes parameters for setting the operator image tag, resource requests and limits for CPU and memory, namespace assignment, and the Helm chart version. Creating the target namespace before executing the command is a prerequisite. An additional flag is noted to disable cert-manager if deploying unsecured NiFi clusters.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/2_deploy_nifikop/1_quick_start.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop \\\n    oci://ghcr.io/konpyutaika/helm-charts/nifikop \\\n    --namespace=nifi \\\n    --version 1.1.1 \\\n    --set image.tag=v1.1.1-release \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set namespaces={\"nifi\"}\n```\n\n----------------------------------------\n\nTITLE: Extracting NiFi User Certificates from Kubernetes Secret (Bash/kubectl)\nDESCRIPTION: Provides three `kubectl get secret` commands to retrieve specific data fields from the `example-client-secret` Kubernetes secret. It uses `jsonpath` to select the base64 encoded certificate and key data (`ca.crt`, `tls.crt`, `tls.key`), decodes it using `base64 -d`, and redirects the output to create local files (`ca.crt`, `tls.crt`, `tls.key`) containing the PEM-encoded credentials.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['ca\\.crt']}\" | base64 -d > ca.crt\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['tls\\.crt']}\" | base64 -d > tls.crt\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['tls\\.key']}\" | base64 -d > tls.key\n```\n\n----------------------------------------\n\nTITLE: Deploy NiFiKop CRDs\nDESCRIPTION: These commands apply the Custom Resource Definitions (CRDs) for NiFiKop, enabling the management of NiFi clusters and users within the Kubernetes cluster. The first snippet applies the NiFiCluster CRD, and the second applies the NiFiUser CRD. These resources are necessary to define and manage NiFi clusters using the NiFiKop operator. Ensure that you have the correct Kubernetes version for the provided yaml files.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/blog/2020-06-30-secured_nifi_cluster_on_gcp_with_external_dns.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/Orange-OpenSource/nifikop/master/deploy/crds/nifi.orange.com_nificlusters_crd.yaml\nkubectl apply -f https://raw.githubusercontent.com/Orange-OpenSource/nifikop/master/deploy/crds/nifi.orange.com_nifiusers_crd.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Parameter Context Resource in YAML\nDESCRIPTION: This YAML snippet declares a NifiParameterContext custom resource instance for use with the nifikop operator in Kubernetes. It includes metadata such as resource name and namespace references. The spec defines a description, references to the NiFi cluster and related secrets, and a list of parameters where some are marked as sensitive. This snippet is essential for configuring parameter contexts that provide parameter values to NiFi dataflows.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/5_references/4_nifi_parameter_context.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      description: toto\n      sensistive: true\n```\n\n----------------------------------------\n\nTITLE: Purging a Helm Release\nDESCRIPTION: Command to fully delete a Helm release from the cluster history and resources, using the '--purge' option. Use with caution as it removes all records and associated resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nhelm delete --purge nifikop\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Listeners and Autoscaling\nDESCRIPTION: This YAML configuration snippet defines the NiFi cluster's listener configuration, including a Prometheus listener, and the configuration for an autoscaling node group. It sets up internal listeners for NiFi and exposes a Prometheus endpoint on port 9090.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n...\nspec:\n  ...\n  listenersConfig:\n    internalListeners:\n    ...\n    - containerPort: 9090\n      name: prometheus\n      type: prometheus \n    ...\n  ...\n  nodeConfigGroups:\n    auto_scaling:\n      isNode: true\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 2Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n      serviceAccountName: external-dns\n      storageConfigs:\n        - mountPath: /opt/nifi/nifi-current/logs\n          name: logs\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/data\n          name: data\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/extensions\n          name: extensions-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/flowfile_repository\n          name: flowfile-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/nifi-current/conf\n          name: conf\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/content_repository\n          name: content-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n        - mountPath: /opt/nifi/provenance_repository\n          name: provenance-repository\n          metadata:\n            labels:\n              my-label: my-value\n            annotations:\n              my-annotation: my-value\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n            storageClassName: ssd-wait\n  ...\n```\n\n----------------------------------------\n\nTITLE: Listing Deployed Helm Charts\nDESCRIPTION: This command lists all Helm releases deployed in the current Kubernetes cluster.  It shows information about deployed charts including release name, namespace, revision, and status.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/2_setup/3_install/1_customizable_install_with_helm.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhelm list\n```\n\n----------------------------------------\n\nTITLE: Creating a NifiParameterContext in YAML\nDESCRIPTION: Example of defining a NifiParameterContext custom resource which defines parameters that can be used by dataflows. It references a secret for sensitive parameters and includes regular parameters with values and descriptions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\n  namespace: demo\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      value: toto\n      description: toto\n```\n\n----------------------------------------\n\nTITLE: Building NiFiKop Locally (Bash)\nDESCRIPTION: Compiles the NiFiKop operator source code using the 'make build' command. Assumes the Go development environment (v1.22+) is set up correctly. Produces a binary executable in the project directory.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/6_contributing/1_developer_guide.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmake build\n```\n\n----------------------------------------\n\nTITLE: Verifying NiFiKop Deployment Status using kubectl in Console\nDESCRIPTION: Checks the status of pods in the 'nifikop' namespace to verify the running state of the NiFiKop operator. Requires kubectl and cluster access with appropriate namespace permissions. Output lists pod status and names; input is the namespace selector.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/6_contributing/1_developer_guide.md#_snippet_9\n\nLANGUAGE: console\nCODE:\n```\nkubectl get pods -n nifikop\nNAME                                                READY   STATUS    RESTARTS   AGE\nskeleton-nifikop-8946b89dc-4cfs9   1/1     Running   0          7m45s\n```\n\n----------------------------------------\n\nTITLE: Installing cert-manager Directly with Kubectl\nDESCRIPTION: Installs the cert-manager Custom Resource Definitions (CRDs) and the cert-manager controller itself by applying a YAML manifest directly from the jetstack GitHub repository using kubectl. This is one method to install cert-manager as a prerequisite for secured NiFi clusters.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/2_setup/1_getting_started.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Install the CustomResourceDefinitions and cert-manager itself\nkubectl apply -f \\\n    https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop Operator with Helm Using console\nDESCRIPTION: This snippet shows the Helm CLI command to install the NiFiKop operator. It uses the `helm install` command with the release name `nifikop` and the chart `konpyutaika/nifikop`, specifying a custom values file `values.yaml` that can define configuration parameters for the deployment. No additional dependencies are required beyond Helm client version 3 or higher and Kubernetes cluster access. This command expects a properly formatted Helm chart and configuration values file, and outputs the installed resources and status.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n$ helm install nifikop \\\n      konpyutaika/nifikop \\\n      -f values.yaml\n```\n\n----------------------------------------\n\nTITLE: Verifying NiFiKop Pod Status\nDESCRIPTION: Command to check if the NiFiKop operator pod is running correctly in the specified namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/6_contributing/1_developer_guide.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get pods -n nifikop\n```\n\n----------------------------------------\n\nTITLE: Installing cert-manager using Helm 3 (Bash)\nDESCRIPTION: Installs cert-manager using its official Helm chart. It involves adding the Jetstack Helm repository, updating the local repository cache, and then installing the chart into the 'cert-manager' namespace (which must be created beforehand). Version v1.7.2 is specified.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/2_deploy_nifikop/1_quick_start.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# Add the jetstack helm repo\nhelm repo add jetstack https://charts.jetstack.io\nhelm repo update\n\n# You have to create the namespace before executing following command\nhelm install cert-manager \\\n    --namespace cert-manager \\\n    --version v1.7.2 jetstack/cert-manager\n```\n\n----------------------------------------\n\nTITLE: Performing Helm Chart Dry Run Installation (bash)\nDESCRIPTION: Executes a dry run installation of the `konpyutaika/nifikop` Helm chart using the `helm install` command with the `--dry-run` flag. It simulates the installation and outputs the generated Kubernetes manifests without deploying actual resources. Includes setting specific chart values like `logLevel` and `namespaces`. Requires Helm installed and configured for the target Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop \\\n    --dry-run \\\n    --set logLevel=Debug \\\n    --set namespaces={\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Installing KEDA Helm Chart - Console\nDESCRIPTION: Creates a dedicated Kubernetes namespace for KEDA and installs the KEDA Helm chart into it. This deploys the core KEDA components necessary for event-driven autoscaling within the Kubernetes cluster. Requires `kubectl` and `helm` installed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl create namespace keda\nhelm install keda kedacore/keda --namespace keda\n```\n\n----------------------------------------\n\nTITLE: Getting UID for NiFi Deployment on OpenShift\nDESCRIPTION: Command to extract the required UID/GID values from OpenShift namespace annotations for NiFi deployment. This ensures proper security context configuration for the NiFi cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nuid=$(kubectl get namespace nifi -o=jsonpath='{.metadata.annotations.openshift\\.io/sa\\.scc\\.supplemental-groups}' | sed 's/\\/10000$//' | tr -d '[:space:]')\n```\n\n----------------------------------------\n\nTITLE: Listing Deployed Helm Releases\nDESCRIPTION: Shows the `helm list` command, used to display all currently active Helm releases managed by Helm within the current Kubernetes context or specified namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nhelm list\n```\n\n----------------------------------------\n\nTITLE: Migrating NiFiKop Custom Resources using Kubernetes API (javascript)\nDESCRIPTION: This script connects to a Kubernetes cluster using '@kubernetes/client-node', migrates NiFiKop CRDs from one group/version to another, and updates their status. It supports different resource types, skips resources with ownerReferences, and preserves core metadata and spec fields. Prerequisites include nodejs v15.3.0+, npm v7.0.14+, and the required dependencies; run this as 'index.js' in the workspace root after preparing 'package.json'.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nprocess.env['NODE_TLS_REJECT_UNAUTHORIZED'] = 0;\nconst k8s = require('@kubernetes/client-node');\n\nconst kc = new k8s.KubeConfig();\nkc.loadFromDefault();\n\nconst k8sApi = kc.makeApiClient(k8s.CustomObjectsApi);\n\nconst KONPYUTAIKA_GROUP = 'nifi.konpyutaika.com';\nconst KONPYUTAIKA_GROUP_VERSION = 'v1alpha1';\nconst ORANGE_GROUP = 'nifi.orange.com';\nconst ORANGE_GROUP_VERSION = 'v1alpha1';\n\nconst call = async (SRC_GRP, SRC_GRP_VER, DST_GRP, DST_GRP_VER, KIND_PLURAL, NAMESPACE) => {\n\tconsole.log(`Listing ${KIND_PLURAL} of ${SRC_GRP}/${SRC_GRP_VER} in ${NAMESPACE}...`);\n\tconst listResources = (await k8sApi.listNamespacedCustomObject(SRC_GRP, SRC_GRP_VER, NAMESPACE, KIND_PLURAL)).body.items;\n\treturn Promise.all(listResources.map(async (resource) => {\n\t\ttry {\n\t\t\tconsole.log(`Found ${resource.kind} \\\"${resource.metadata.name}\\\" of ${resource.apiVersion} in ${NAMESPACE}`);\n\n\t\t\tif (resource.metadata.ownerReferences) {\n\t\t\t\tconsole.log(`${resource.kind} ${resource.metadata.name} mananged by something else (ownerRefereces is set).`);\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tconst bodyResource = {\n\t\t\t\tapiVersion: `${DST_GRP}/${DST_GRP_VER}`,\n\t\t\t\tkind: resource.kind,\n\t\t\t\tmetadata: {\n\t\t\t\t\tname: resource.metadata.name,\n\t\t\t\t\tannotations: resource.metadata.annotations,\n\t\t\t\t\tlabels: resource.metadata.labels\n\t\t\t\t},\n\t\t\t\tspec: resource.spec\n\t\t\t};\n\n\t\t\tconsole.log(`Creating ${bodyResource.kind} \\\"${bodyResource.metadata.name}\\\" of ${bodyResource.apiVersion} in ${NAMESPACE}...`);\n\t\t\tconst newResource = (await k8sApi.createNamespacedCustomObject(DST_GRP, DST_GRP_VER, NAMESPACE, KIND_PLURAL, bodyResource)).body;\n\t\t\tconsole.log('...done creating.');\n\n\t\t\tconst bodyStatus = {\n\t\t\t\tapiVersion: newResource.apiVersion,\n\t\t\t\tkind: newResource.kind,\n\t\t\t\tmetadata: {\n\t\t\t\t\tname: newResource.metadata.name,\n\t\t\t\t\tresourceVersion: newResource.metadata.resourceVersion\n\t\t\t\t},\n\t\t\t\tstatus: resource.status\n\t\t\t};\n\n\t\t\tconsole.log(`Copying status from ${resource.kind} \\\"${resource.metadata.name}\\\" of ${newResource.apiVersion} to ${newResource.kind} \\\"${newResource.metadata.name}\\\" of ${newResource.apiVersion} in ${NAMESPACE}...`);\n\t\t\tconst newResourceWithStatus = (await k8sApi.replaceNamespacedCustomObjectStatus(DST_GRP, DST_GRP_VER, NAMESPACE, KIND_PLURAL, bodyStatus.metadata.name, bodyStatus)).body;\n\t\t\tconsole.log('...done copying.');\n\t\t\treturn newResourceWithStatus;\n\t\t}\n\t\tcatch (e) {\n\t\t\tconsole.error(e.body ? e.body.message ? e.body.message : e.body : e);\n\t\t}\n\t}));\n};\n\nconst argv = require('minimist')(process.argv.slice(2));\n\nlet NAMESPACE = argv.namespace ? argv.namespace.length > 0 ? argv.namespace : 'default' : 'default';\nlet KIND_PLURAL = {\n\tcluster: 'nificlusters',\n\tdataflow: 'nifidataflows',\n\tparametercontext: 'nifiparametercontexts',\n\tregistryclient: 'nifiregistryclients',\n\tuser: 'nifiusers',\n\tusergroup: 'nifiusergroups',\n};\n\nif (!argv.type) {\n\tconsole.error('Type not provided');\n\tprocess.exit(1);\n}\n\nif (!KIND_PLURAL[argv.type]) {\n\tconsole.error(`Type ${argv.type} is not one of the following types: ${Object.keys(KIND_PLURAL)}`);\n\tprocess.exit(1);\n}\n\nconsole.log(`########### START: ${KIND_PLURAL[argv.type]} ###########`);\ncall( ORANGE_GROUP, ORANGE_GROUP_VERSION, KONPYUTAIKA_GROUP, KONPYUTAIKA_GROUP_VERSION, KIND_PLURAL[argv.type], NAMESPACE)\n\t.then(r => console.log('############ END ############'))\n\t.catch(e => console.error(e));\n```\n\n----------------------------------------\n\nTITLE: Listing Kubernetes Services with kubectl in Console\nDESCRIPTION: This console output shows the result of the `kubectl get services` command after applying the external service configuration. It displays the `cluster-access` service configured as a `LoadBalancer`, listing its internal `CLUSTER-IP`, external `EXTERNAL-IP`, and the mapped `PORT(S)` (external:internal, e.g., 443:30421/TCP), confirming the successful creation and configuration of the external access point.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nkubectl get services\nNAME                TYPE           CLUSTER-IP    EXTERNAL-IP      PORT(S)                                AGE\ncluster-access      LoadBalancer   10.88.21.98   35.180.241.132   443:30421/TCP,80:30521/TCP             20d\n```\n\n----------------------------------------\n\nTITLE: Adding Helm Repo for KEDA\nDESCRIPTION: This snippet adds the KEDA Helm repository to your Helm configuration.  It fetches the necessary chart information to install KEDA later. This action is a prerequisite for installing KEDA via Helm, allowing you to specify the KEDA chart by name.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nhelm repo add kedacore https://kedacore.github.io/charts\n```\n\n----------------------------------------\n\nTITLE: Install Nifikop Chart - Release Name\nDESCRIPTION: This command installs the Nifikop Helm chart using a specified release name. The user should replace `<release name>` with their chosen name for the Helm release. Dependencies: Helm, Kubernetes cluster, and the Nifikop chart repository. The command deploys the chart based on the default configuration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhelm install <release name> konpyutaika/nifikop\n```\n\n----------------------------------------\n\nTITLE: Viewing Pod Port Configuration via Kubernetes YAML\nDESCRIPTION: Shows an example of the `ports` section within a deployed NiFi pod's YAML definition, reflecting the internal listeners configured via the Nifikop operator. Each entry maps the listener's `name` to its `containerPort` and specifies the `protocol` (typically TCP).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n    ports:\n    - containerPort: 8443\n      name: https\n      protocol: TCP\n    - containerPort: 6007\n      name: cluster\n      protocol: TCP\n    - containerPort: 6342\n      name: load-balance\n      protocol: TCP\n    - containerPort: 10000\n      name: s2s\n      protocol: TCP\n    - containerPort: 9090\n      name: prometheus\n      protocol: TCP\n```\n\n----------------------------------------\n\nTITLE: Installing KEDA Helm Chart into Kubernetes Namespace\nDESCRIPTION: This sequence creates the 'keda' namespace and installs the KEDA Helm chart in that namespace, setting up KEDA autoscaling capabilities within Kubernetes. Dependencies include Helm CLI and Kubernetes cluster configuration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl create namespace keda\nhelm install keda kedacore/keda --namespace keda\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiRegistryClient using Kubernetes YAML\nDESCRIPTION: This YAML snippet defines a NifiRegistryClient custom resource for NiFiKop, which represents a connection to a NiFi Registry server. It is required before deploying NiFi dataflows since NiFiKop manages dataflows through the NiFi Registry feature. The snippet specifies metadata like name and namespace and the specification includes the cluster reference and the registry URI. Dependencies include having NiFiKop installed with access to a running NiFi Registry instance at the given URI.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiRegistryClient\nmetadata:\n  name: registry-client-example\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Registry client managed by NiFiKop\"\n  uri: \"http://nifi.hostname.com:18080\"\n```\n\n----------------------------------------\n\nTITLE: Defining a NiFi Dataflow with YAML\nDESCRIPTION: This YAML snippet demonstrates how to define an instance of the NifiDataflow Custom Resource. It specifies the dataflow to deploy using bucket, flow, and version IDs from a NiFi Registry, targets a specific NiFi Cluster, sets the deployment position, and defines synchronization and update strategies.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/5_references/5_nifi_dataflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  flowPosition:\n    posX: 0\n    posY: 0\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: squidflow\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: nifikop\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Creating NifiParameterContext YAML Example\nDESCRIPTION: This YAML snippet defines a `NifiParameterContext` resource, used for managing parameters within a dataflow.  It includes the API version, kind, metadata, and specification which involves a description, a cluster reference, secret references (for sensitive parameters), and a list of parameters with their names, values, and descriptions.  This allows the usage of secrets within the NiFi dataflow.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\n  namespace: demo\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      value: toto\n      description: toto\n```\n\n----------------------------------------\n\nTITLE: Configuring npm Start Script in package.json (JSON)\nDESCRIPTION: Defines the `start` script within the `scripts` section of `package.json`. This script executes the `index.js` file using Node.js, suppressing warnings (`--no-warnings`), allowing the migration script to be run easily via `npm start`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"start\": \"node --no-warnings index.js\"\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Properties with Multiple Override Methods in YAML\nDESCRIPTION: An example showing how to configure NiFi properties using different override methods: ConfigMap, Secret, and inline configurations. This demonstrates the priority hierarchy where Secret > ConfigMap > Override > Default.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\n nifiProperties:\n      # Additionnal nifi.properties configuration that will override the one produced based on template and\n      # configuration\n      overrideConfigMap:\n        # The key of the value,in data content, that we want use.\n        data: nifi.properties\n        # Name of the configmap that we want to refer.\n        name: raw\n        # Namespace where is located the secret that we want to refer.\n        namespace: nifikop\n      # Additionnal nifi.properties configuration that will override the one produced based\n      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n      overrideSecretConfig:\n        # The key of the value,in data content, that we want use.\n        data: nifi.properties\n        # Name of the configmap that we want to refer.\n        name: raw\n        # Namespace where is located the secret that we want to refer.\n        namespace: nifikop\n      # Additionnal nifi.properties configuration that will override the one produced based\n      #\ton template, configurations and overrideConfigMap\n      overrideConfigs: |\n        nifi.ui.banner.text=NiFiKop\n        nifi.sensitive.props.key=thisIsABadSensitiveKeyPassword\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop Operator using Helm CLI (console)\nDESCRIPTION: This snippet demonstrates how to install the NiFiKop operator Helm chart by running the helm install command using the CLI. It references the chart repository 'konpyutaika/nifikop' and enables the user to provide custom configuration values via a YAML file or inline set parameters. The example uses a values.yaml file to specify the chart configuration. Prerequisites include having Helm v3+ installed and Kubernetes cluster access. The output is the deployment of NiFiKop operator with user-defined customizations on the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n$ helm install nifikop \\\n      konpyutaika/nifikop \\\n      -f values.yaml\n```\n\n----------------------------------------\n\nTITLE: Example package.json for NiFiKop CRD Migration Project - JSON\nDESCRIPTION: This package.json example configures essential project metadata, scripts, and dependencies for running the NiFiKop CRD migration tool. It specifies the entrypoint, testing placeholder, keyword metadata, and lists @kubernetes/client-node and minimist as required dependencies. Copying and adapting this structure is necessary to build and manage the migration environment correctly.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"nifikop_crd_migration\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Script to migrate from the old CRDs to the new CRDs.\",\n  \"main\": \"index.js\",\n  \"scripts\": {\n    \"start\": \"node --no-warnings index.js\",\n    \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\"\n  },\n  \"keywords\": [\n    \"K8S\",\n    \"NiFiKop\",\n    \"CRDs\"\n  ],\n  \"license\": \"ISC\",\n  \"dependencies\": {\n    \"@kubernetes/client-node\": \"^0.16.3\",\n    \"minimist\": \"^1.2.6\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Extracting TLS Credentials from NiFi User Secret - Console\nDESCRIPTION: Uses kubectl and base64 on the command line to retrieve and decode the CA certificate, user certificate, and private key from the Kubernetes secret created by NifiUser. Inputs are secret name and field names; outputs are local PEM files containing credentials. No additional dependencies beyond kubectl and base64. Suitable for provisioning local or application-side authentication artifacts.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['ca\\.crt']}\" | base64 -d > ca.crt\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['tls\\.crt']}\" | base64 -d > tls.crt\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['tls\\.key']}\" | base64 -d > tls.key\n```\n\n----------------------------------------\n\nTITLE: Packaging NiFiKop Helm Chart using Bash and Make\nDESCRIPTION: Executes the `make helm-package` command. This command packages the NiFiKop Helm chart located in the project, typically creating a `.tgz` archive in a `dist` or similar directory, preparing it for distribution or inclusion in a Helm repository.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/6_contributing/1_developer_guide.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nmake helm-package\n```\n\n----------------------------------------\n\nTITLE: Checking NiFi Cluster Pod Status Post-Deployment (Console)\nDESCRIPTION: This console snippet verifies the status of the main NiFi pods and operator post-deployment in the nifikop namespace. Required: an existing and deployed NiFi cluster. Shows node pod names, operational state, and runtime info. Use this output to confirm the cluster is running as expected. Input: none except for namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/blog/2020-06-30-secured_nifi_cluster_on_gcp_with_external_dns.md#_snippet_19\n\nLANGUAGE: console\nCODE:\n```\nkubectl get pods -n nifikop\nNAME                            READY   STATUS    RESTARTS   AGE\nexternal-dns-569bf79b57-hjmtt   1/1     Running   0          9h\nnifi-0-nodekmhgz                1/1     Running   0          27m\nnifi-1-node4465q                1/1     Running   0          27m\nnifi-2-node5jwwx                1/1     Running   0          27m\nnifikop-56cb587d96-p8vdf        1/1     Running   0          40m\n```\n\n----------------------------------------\n\nTITLE: Verifying the Helm Deployment\nDESCRIPTION: Checks the status of NiFiKop operator pods in the specified namespace to confirm successful deployment and operation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/6_contributing/1_developer_guide.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n$ kubectl get pods -n nifikop\nNAME                                                READY   STATUS    RESTARTS   AGE\nskeleton-nifikop-8946b89dc-4cfs9   1/1     Running   0          7m45s\n```\n\n----------------------------------------\n\nTITLE: Packaging NiFiKop Helm Chart (Bash)\nDESCRIPTION: Executes the `helm-package` target in the Makefile to package the NiFiKop Helm chart located in the `helm/nifikop` directory into a versioned chart archive (`.tgz` file).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/6_contributing/1_developer_guide.md#_snippet_10\n\nLANGUAGE: Bash\nCODE:\n```\nmake helm-package\n```\n\n----------------------------------------\n\nTITLE: Building NiFiKop Locally (Bash)\nDESCRIPTION: Executes the `build` target in the project's Makefile. This command compiles the Go source code to create the NiFiKop operator binary within the local Go development environment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/6_contributing/1_developer_guide.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmake build\n```\n\n----------------------------------------\n\nTITLE: ExternalServiceConfig Schema Definition\nDESCRIPTION: This schema defines the structure for external service configurations, including mandatory fields such as 'name' and 'spec', optional 'metadata', and attributes like 'clusterIP', 'type', and load balancer settings. It supports various service exposure types and detailed port configurations to facilitate flexible service deployment in Kubernetes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/5_references/1_nifi_cluster/7_external_service_config.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n|Field|Type|Description|Required|Default|\n|-----|----|-----------|--------|--------|\n|name|string|Must be unique within a namespace for identification and configuration| Yes | - |\n|metadata|[Metadata](#metadata)|Additional metadata; optional| No | - |\n|spec|[ExternalServiceSpec](#externalservicespec)|Defines behavior of the service| Yes | - |\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiCluster Resource with a Removed Node (YAML)\nDESCRIPTION: This YAML manifest defines a `NifiCluster` custom resource for Kubernetes. It removes node `id: 2` from the cluster configuration by commenting it out in the `spec.nodes` list, demonstrating how to initiate a graceful scale-down of the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/3_tasks/1_nifi_cluster/2_cluster_scaling.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: simplenifi\nspec:\n  headlessServiceEnabled: true\n  zkAddress: \"zookeepercluster-client.zookeeper:2181\"\n  zkPath: \"/simplenifi\"\n  clusterImage: \"apache/nifi:1.11.3\"\n  oneNifiNodePerNode: false\n  nodeConfigGroups:\n    default_group:\n      isNode: true\n      storageConfigs:\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 1Gi\n  nodes:\n    - id: 0\n      nodeConfigGroup: \"default_group\"\n    - id: 1\n      nodeConfigGroup: \"default_group\"\n# >>>> START: node removed\n#    - id: 2\n#      nodeConfigGroup: \"default_group\"\n# <<<< END\n    - id: 25\n      nodeConfigGroup: \"default_group\"\n  propagateLabels: true\n  nifiClusterTaskSpec:\n    retryDurationMinutes: 10\n  listenersConfig:\n    internalListeners:\n      - type: \"http\"\n        name: \"http\"\n        containerPort: 8080\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Cluster SSL Listeners with NiFi Operator in YAML\nDESCRIPTION: This YAML snippet shows how to configure SSL listeners for a NiFi cluster using the NiFi operator. It specifies internal listeners for HTTPS, cluster communication, and Site-to-Site communication with defined container ports and SSL secrets management. The example uses the 'tlsSecretName' for storing certificates, and the operator can create certificates automatically if 'create' is true. Dependencies include NiFi operator and cluster context with readiness for SSL configuration, with inputs being listener types and ports, and outputs applied SSL context management.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  readOnlyConfig:\n    # NifiProperties configuration that will be applied to the node.\n    nifiProperties:\n      webProxyHosts:\n        - nifistandard2.trycatchlearn.fr:8443\n        ...\n  ...\n  listenersConfig:\n    internalListeners:\n      - type: \"https\"\n        name: \"https\"\n        containerPort: 8443\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n```\n\n----------------------------------------\n\nTITLE: Generating Helm Chart Documentation (Docker) - Shell\nDESCRIPTION: Runs a Docker command to generate `README.md` documentation for Helm charts using the `jnorwood/helm-docs` image. It mounts the current repository root into the container and sets the user ID to ensure correct file permissions. This command requires Docker installed and access to the specified `helm-docs` Docker image.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/helm/README.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ndocker run --rm --volume \"$(pwd):/helm-docs\" -u $(id -u) jnorwood/helm-docs:latest\n```\n\n----------------------------------------\n\nTITLE: Apply NiFi Cluster Configuration with kubectl\nDESCRIPTION: This command applies the NiFi cluster configuration defined in a YAML file to the Kubernetes cluster. It uses the `kubectl apply` command to create or update the NiFiCluster resource. The `-n nifi` flag specifies the namespace where the NiFi resources are deployed. The `-f config/samples/simplenificluster.yaml` specifies the path to the configuration file.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nkubectl -n nifi apply -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Create Role for Kubernetes State Management - YAML\nDESCRIPTION: This YAML defines a Kubernetes Role and RoleBinding to grant the NiFi ServiceAccount the necessary permissions to manage `Leases` and `ConfigMaps` resources. The Role grants all verbs (`*`) on `leases` in the `coordination.k8s.io` API group and all verbs on `configmaps` in the core API group. The RoleBinding then binds this Role to the `default` ServiceAccount in the `nifi` namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: simplenifi\n  namespace: nifi\nrules:\n- apiGroups: [\"coordination.k8s.io\"]\n  resources: [\"leases\"]\n  verbs: [\"*\"]\n- apiGroups: [\"\"]\n  resources: [\"configmaps\"]\n  verbs: [\"*\"]\n---\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: simplenifi\n  namespace: nifi\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: nifi\nroleRef:\n  kind: Role\n  name: simplenifi\n  apiGroup: rbac.authorization.k8s.io\n```\n\n----------------------------------------\n\nTITLE: Pushing NiFiKop Docker Image (Bash)\nDESCRIPTION: Executes the `docker-push` Makefile target to push the built NiFiKop operator Docker image to the repository specified by the `DOCKER_REGISTRY_BASE` environment variable. Requires prior successful execution of `make docker-build` and Docker login.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/6_contributing/1_developer_guide.md#_snippet_7\n\nLANGUAGE: Bash\nCODE:\n```\nmake docker-push\n```\n\n----------------------------------------\n\nTITLE: Install Cert-Manager Directly via Kubectl\nDESCRIPTION: Installs Cert-Manager, a required dependency for NiFiKop's security features, directly onto the Kubernetes cluster by applying a manifest from a remote URL using `kubectl apply`. This method installs both the Cert-Manager CRDs and the controller.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/2_deploy_nifikop/1_quick_start.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f     https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.yaml\n```\n\n----------------------------------------\n\nTITLE: Retrieving zookeeper namespace RunAsUser UID in OpenShift using Bash\nDESCRIPTION: Extracts the UID used for 'runAsUser' and the 'fsGroup' from OpenShift namespace annotations for the 'zookeeper' namespace. This UID is necessary to set security context parameters to run containers with correct permissions on OpenShift. The command uses kubectl with jsonpath and sed to parse and format the UID string for Helm chart installation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nzookeper_uid=$(kubectl get namespace zookeeper -o=jsonpath='{.metadata.annotations.openshift\\.io/sa\\.scc\\.supplemental-groups}' | sed 's/\\/10000$//' | tr -d '[:space:]')\n```\n\n----------------------------------------\n\nTITLE: Defining a NiFi Registry Client using YAML\nDESCRIPTION: This snippet provides a sample YAML definition for a `NifiRegistryClient` resource. It demonstrates how to specify the API version, kind, metadata (name), and the core specification fields including the associated NiFi Cluster reference, a description, and the URI of the NiFi Registry.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/5_references/3_nifi_registry_client.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiRegistryClient\nmetadata:\n  name: squidflow\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Squidflow demo\"\n  uri: \"http://nifi-registry:18080\"\n```\n\n----------------------------------------\n\nTITLE: Applying NiFiCluster Configuration (kubectl)\nDESCRIPTION: Applies the updated NifiCluster YAML configuration file to the Kubernetes cluster using kubectl. This command triggers the NiFiKop operator to reconcile the desired state (with the added node) with the current state.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nkubectl -n nifi apply -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Kubernetes Pod Port Mappings for NiFi Internal Listeners YAML\nDESCRIPTION: Shows a portion of the Kubernetes pod specification YAML illustrating how container ports correspond to the configured internal NiFi listeners. Each port is named and exposed using TCP protocol inside the pod, matching the earlier internal listener configurations to ensure proper routing and accessibility. This example helps verify the applied port settings during pod deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n    ports:\n    - containerPort: 8443\n      name: https\n      protocol: TCP\n    - containerPort: 6007\n      name: cluster\n      protocol: TCP\n    - containerPort: 6342\n      name: load-balance\n      protocol: TCP\n    - containerPort: 10000\n      name: s2s\n      protocol: TCP\n    - containerPort: 9090\n      name: prometheus\n      protocol: TCP\n```\n\n----------------------------------------\n\nTITLE: Configuring External Services with ClusterIP in Kubernetes YAML\nDESCRIPTION: This YAML snippet defines an external service of type ClusterIP with multiple port configurations and metadata annotations/labels. It outlines how to specify service name, ports, and associated internal listener names to facilitate cluster-internal communication. Dependencies include Kubernetes service schemas and network protocols. Inputs include service name, port configurations, and optional metadata, producing a service resource for deployment within a Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/5_references/1_nifi_cluster/7_external_service_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  externalServices:\n    - name: \"clusterip\"\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8080\n            internalListenerName: \"http\"\n          - port: 7182\n            internalListenerName: \"my-custom-listener\"\n            protocol: TCP\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n\n----------------------------------------\n\nTITLE: Configuring Internal Listeners for NiFi Cluster in YAML\nDESCRIPTION: Defines internal listener ports for NiFi components within the Kubernetes deployment, specifying listener type, name, and container port. It is essential for enabling proper intra-cluster and UI access. Dependencies include the NiFi cluster configuration schema; key parameters are 'type', 'name', and 'containerPort'. Inputs are YAML-formatted listener definitions; outputs influence NiFi's internal network setup.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    - type: \"https\"\n      name: \"https\"\n      containerPort: 8443\n    - type: \"cluster\"\n      name: \"cluster\"\n      containerPort: 6007\n    - type: \"s2s\"\n      name: \"s2s\"\n      containerPort: 10000\n    - type: \"prometheus\"\n      name: \"prometheus\"\n      containerPort: 9090\n    - type: \"load-balance\"\n      name: \"load-balance\"\n      containerPort: 6342\n```\n\n----------------------------------------\n\nTITLE: Creating an Issuer resource with Let's Encrypt\nDESCRIPTION: This snippet demonstrates how to define a Kubernetes Issuer resource for obtaining certificates from Let's Encrypt staging environment using ACME protocol. It includes contact email, server URL, private key secret reference, and challenge solver configuration with ingress annotations for DNS validation. Usage of such issuer facilitates automatic SSL certificate provisioning.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: cert-manager.io/v1alpha2\nkind: Issuer\nmetadata:\n  name: letsencrypt-staging\nspec:\n  acme:\n    email: <your email address>\n    server: https://acme-staging-v02.api.letsencrypt.org/directory\n    privateKeySecretRef:\n      name: example-issuer-account-key\n    solvers:\n      - http01:\n          ingress:\n            ingressTemplate:\n              metadata:\n                annotations:\n                  \"external-dns.alpha.kubernetes.io/ttl\": \"5\"\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiRegistryClient CRD with NiFiKop in YAML\nDESCRIPTION: Defines a NifiRegistryClient custom resource as required by the NiFiKop Operator to integrate NiFi with a NiFi Registry instance. The resource specifies cluster reference details, a description, and the target registry URI. Dependencies include a running NiFiKop Operator and an accessible NiFi Registry endpoint. Inputs are the fields in the YAML spec; the output is a custom resource managed by NiFiKop. The namespace and resource name must match your environment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiRegistryClient\nmetadata:\n  name: registry-client-example\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Registry client managed by NiFiKop\"\n  uri: \"http://nifi.hostname.com:18080\"\n```\n\n----------------------------------------\n\nTITLE: Defining Istio VirtualService for HTTP\nDESCRIPTION: This YAML defines a `VirtualService` that redirects traffic intercepted by the `nifi-gateway` to the `nifi` service on port 8080. It is used in conjunction with the `Gateway` configuration.  The `hosts` field specifies the domain, the `match` field defines the URI prefix, and the `route` field directs traffic to the NiFi service.  This configuration is essential to make the NiFi cluster accessible via HTTP.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\n```yaml\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: nifi-vs\nspec:\n  gateways:\n  - nifi-gateway\n  hosts:\n  - nifi.my-domain.com\n  http:\n  - match:\n    - uri:\n        prefix: /\n    route:\n    - destination:\n        host: nifi\n        port:\n          number: 8080\n```\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFiKop Custom Resource Definitions (CRDs) Using kubectl (Bash)\nDESCRIPTION: This multi-line snippet applies NiFiKop operator Custom Resource Definitions to the Kubernetes cluster using 'kubectl apply'. CRDs define custom Kubernetes resources like NiFiClusters and NiFiUsers, enabling the operator to manage these resource types. The commands must be run before the operator itself is deployed to allow proper recognition of custom resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/6_contributing/1_developer_guide.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\n```\n\n----------------------------------------\n\nTITLE: Snippet: CRD YAML Example\nDESCRIPTION: This YAML snippet shows an example configuration, demonstrating how to set annotations for cert-manager when using a conversion webhook. This configuration tells cert-manager to inject CA certificates.  It also shows how to configure a Webhook strategy for conversion in the CRD. This snippet is a template, variables (namespace, certificate_name, webhook_service_name) need to be replaced before use. It's essential for enabling the conversion webhook. The expected input is a complete CRD definition.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/2_deploy_nifikop/1_quick_start.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n```\n\n----------------------------------------\n\nTITLE: Defining Listeners Configuration in YAML Format\nDESCRIPTION: This snippet provides an example YAML configuration for setting up various internal listeners for Nifi, specifying the type, name, and container port for each listener. It also includes SSL secret settings. Dependencies include Kubernetes secrets management for SSL. Inputs include listener types and port numbers, producing a structured YAML config for deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/5_references/1_nifi_cluster/6_listeners_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  listenersConfig:\n    internalListeners:\n      - type: \"https\"\n        name: \"https\"\n        containerPort: 8443\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n      - type: \"prometheus\"\n        name: \"prometheus\"\n        containerPort: 9090\n      - type: \"load-balance\"\n        name: \"load-balance\"\n        containerPort: 6342\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n```\n\n----------------------------------------\n\nTITLE: Configuring Internal Listeners in NifiCluster Spec\nDESCRIPTION: This YAML snippet shows how to define essential internal listeners for a NiFi cluster within the `listenersConfig.internalListeners` section of the NifiCluster custom resource spec. It configures standard NiFi ports for HTTPS UI access, cluster communication, Site-to-Site, load balancing, and Prometheus metrics, specifying the type, a logical name, and the container port for each.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    - type: \"https\"\n      name: \"https\"\n      containerPort: 8443\n    - type: \"cluster\"\n      name: \"cluster\"\n      containerPort: 6007\n    - type: \"s2s\"\n      name: \"s2s\"\n      containerPort: 10000\n    - type: \"prometheus\"\n      name: \"prometheus\"\n      containerPort: 9090\n    - type: \"load-balance\"\n      name: \"load-balance\"\n      containerPort: 6342\n```\n\n----------------------------------------\n\nTITLE: Deploying Prometheus Operator with Helm (console)\nDESCRIPTION: This command installs the `kube-prometheus-stack` Helm chart into the `monitoring-system` namespace, specifically configured to *only* deploy the Prometheus Operator component by disabling other parts of the stack. This sets up the controller necessary to manage Prometheus custom resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_8\n\nLANGUAGE: console\nCODE:\n```\nhelm install prometheus prometheus/kube-prometheus-stack --namespace monitoring-system \\\n    --set prometheusOperator.createCustomResource=false \\\n    --set prometheusOperator.logLevel=debug \\\n    --set prometheusOperator.alertmanagerInstanceNamespaces=monitoring-system \\\n    --set prometheusOperator.namespaces.additional[0]=monitoring-system \\\n    --set prometheusOperator.prometheusInstanceNamespaces=monitoring-system \\\n    --set prometheusOperator.thanosRulerInstanceNamespaces=monitoring-system \\\n    --set defaultRules.enable=false \\\n    --set alertmanager.enabled=false \\\n    --set grafana.enabled=false \\\n    --set kubeApiServer.enabled=false \\\n    --set kubelet.enabled=false \\\n    --set kubeControllerManager.enabled=false \\\n    --set coreDNS.enabled=false \\\n    --set kubeEtcd.enabled=false \\\n    --set kubeScheduler.enabled=false \\\n    --set kubeProxy.enabled=false \\\n    --set kubeStateMetrics.enabled=false \\\n    --set prometheus.enabled=false\n```\n\n----------------------------------------\n\nTITLE: Pushing NiFiKop Docker Image in Bash\nDESCRIPTION: Command to push the built NiFiKop operator Docker image to a repository.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/6_contributing/1_developer_guide.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nmake docker-push\n```\n\n----------------------------------------\n\nTITLE: Defining Start Script in package.json (JSON)\nDESCRIPTION: Adds a 'start' script definition to the `scripts` section of the `package.json` file. This script uses `node` with the `--no-warnings` flag to execute the main migration logic located in `index.js`. This simplifies running the script via `npm start`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_1\n\nLANGUAGE: JSON\nCODE:\n```\n\"start\": \"node --no-warnings index.js\"\n```\n\n----------------------------------------\n\nTITLE: Setting up Kubernetes cluster and kubectl CLI using Bash and Console commands\nDESCRIPTION: This snippet provides bash and console commands for setting up different Kubernetes cluster environments including Docker Desktop, Minikube, Kind, GKE, and EKS along with instructions to install and configure the kubectl CLI tool. It includes commands for fetching cluster credentials and switching kubeconfig contexts relevant to the targeted environment. Dependencies include a Kubernetes cluster setup along with a functional installation of kubectl and optional cloud provider CLIs like gcloud and eksctl.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/2_deploy_nifikop/1_quick_start.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\ngcloud container clusters get-credentials CLUSTER_NAME --zone ZONE_NAME --project PROJECT_NAME.\n```\n\nLANGUAGE: bash\nCODE:\n```\nkubectl config use-context my-cluster-name\n```\n\nLANGUAGE: bash\nCODE:\n```\neksctl utils write-kubeconfig --cluster=${CLUSTER NAME}\n```\n\n----------------------------------------\n\nTITLE: Applying Updated NifiCluster Configuration for Scaledown (Shell)\nDESCRIPTION: This command applies the NifiCluster YAML configuration with the removed node definition. The NiFiKop operator detects the change and initiates the graceful decommissioning process for the specified node.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_4\n\nLANGUAGE: Shell\nCODE:\n```\nkubectl -n nifi apply -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Starting Minikube with Minimum Memory\nDESCRIPTION: Command to start a Minikube Kubernetes cluster, ensuring it has at least 4GB of RAM allocated, which is recommended for running NiFiKop.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/2_deploy_nifikop/1_quick_start.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nminikube start --memory=4000\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop Helm Chart with Custom Values File\nDESCRIPTION: Demonstrates how to install the NiFiKop Helm chart from the 'konpyutaika' repository, naming the release 'nifikop', and applying custom configuration settings defined in a 'values.yaml' file using the '-f' flag. This method allows for providing configuration parameters listed in the preceding table via a YAML file instead of individual '--set' arguments.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n$ helm install nifikop \\\n      konpyutaika/nifikop \\\n      -f values.yaml\n```\n\n----------------------------------------\n\nTITLE: Node.js Script to Migrate NiFiKop Custom Resources on Kubernetes\nDESCRIPTION: This Node.js script automates the migration of NiFiKop custom resources from the legacy group 'nifi.orange.com/v1alpha1' to the newer group 'nifi.konpyutaika.com/v1alpha1'. It utilizes the Kubernetes client library to list resources, create new resources with relevant fields, and copy statuses, all within specified namespaces. The script requires dependencies such as '@kubernetes/client-node' and 'minimist', and it relies on specific environment setup to authenticate with the Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/7_upgrade/1_v0.7.x_to_v0.8.0.md#_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nprocess.env['NODE_TLS_REJECT_UNAUTHORIZED'] = 0;\nconst k8s = require('@kubernetes/client-node');\n\nconst kc = new k8s.KubeConfig();\nkc.loadFromDefault();\n\nconst k8sApi = kc.makeApiClient(k8s.CustomObjectsApi);\n\nconst KONPYUTAIKA_GROUP = 'nifi.konpyutaika.com';\nconst KONPYUTAIKA_GROUP_VERSION = 'v1alpha1';\nconst ORANGE_GROUP = 'nifi.orange.com';\nconst ORANGE_GROUP_VERSION = 'v1alpha1';\n\nconst call = async (SRC_GRP, SRC_GRP_VER, DST_GRP, DST_GRP_VER, KIND_PLURAL, NAMESPACE) => {\n\tconsole.log(`Listing ${KIND_PLURAL} of ${SRC_GRP}/${SRC_GRP_VER} in ${NAMESPACE}...`);\n\tconst listResources = (await k8sApi.listNamespacedCustomObject(SRC_GRP, SRC_GRP_VER, NAMESPACE, KIND_PLURAL)).body.items;\n\treturn Promise.all(listResources.map(async (resource) => {\n\t\ttry {\n\t\t\tconsole.log(`Found ${resource.kind} \"${resource.metadata.name}\" of ${resource.apiVersion} in ${NAMESPACE}`);\n\n\t\t\tif (resource.metadata.ownerReferences) {\n\t\t\t\tconsole.log(`${resource.kind} ${resource.metadata.name} mananged by something else (ownerRefereces is set).`);\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tconst bodyResource = {\n\t\t\t\tapiVersion: `${DST_GRP}/${DST_GRP_VER}`,\n\t\t\t\tkind: resource.kind,\n\t\t\t\tmetadata: {\n\t\t\t\t\tname: resource.metadata.name,\n\t\t\t\t\tannotations: resource.metadata.annotations,\n\t\t\t\t\tlabels: resource.metadata.labels\n\t\t\t\t},\n\t\t\t\tspec: resource.spec\n\t\t\t};\n\n\t\t\tconsole.log(`Creating ${bodyResource.kind} \"${bodyResource.metadata.name}\" of ${bodyResource.apiVersion} in ${NAMESPACE}...`);\n\t\t\tconst newResource = (await k8sApi.createNamespacedCustomObject(DST_GRP, DST_GRP_VER, NAMESPACE, KIND_PLURAL, bodyResource)).body;\n\t\t\tconsole.log('...done creating.');\n\n\t\t\tconst bodyStatus = {\n\t\t\t\tapiVersion: newResource.apiVersion,\n\t\t\t\tkind: newResource.kind,\n\t\t\t\tmetadata: {\n\t\t\t\t\tname: newResource.metadata.name,\n\t\t\t\t\tresourceVersion: newResource.metadata.resourceVersion\n\t\t\t\t},\n\t\t\t\tstatus: resource.status\n\t\t\t};\n\n\t\t\tconsole.log(`Copying status from ${resource.kind} \"${resource.metadata.name}\" of ${newResource.apiVersion} to ${newResource.kind} \"${newResource.metadata.name}\" of ${newResource.apiVersion} in ${NAMESPACE}...`);\n\t\t\tconst newResourceWithStatus = (await k8sApi.replaceNamespacedCustomObjectStatus(DST_GRP, DST_GRP_VER, NAMESPACE, KIND_PLURAL, bodyStatus.metadata.name, bodyStatus)).body;\n\t\t\tconsole.log('...done copying.');\n\t\t\treturn newResourceWithStatus;\n\t\t}\n\t\tcatch (e) {\n\t\t\tconsole.error(e.body ? e.body.message ? e.body.message : e.body : e);\n\t\t}\n\t}));\n};\n\nconst argv = require('minimist')(process.argv.slice(2));\n\nlet NAMESPACE = argv.namespace ? (argv.namespace.length > 0 ? argv.namespace : 'default') : 'default';\nlet KIND_PLURAL = {\n\tcluster: 'nificlusters',\n\tdataflow: 'nifidataflows',\n\tparametercontext: 'nifiparametercontexts',\n\tregistryclient: 'nifiregistryclients',\n\tuser: 'nifiusers',\n\tusergroup: 'nifiusergroups'\n};\n\nif (!argv.type) {\n\tconsole.error('Type not provided');\n\tprocess.exit(1);\n}\n\nif (!KIND_PLURAL[argv.type]) {\n\tconsole.error(`Type ${argv.type} is not one of the following types: ${Object.keys(KIND_PLURAL)}`);\n\tprocess.exit(1);\n}\n\nconsole.log(`########### START: ${KIND_PLURAL[argv.type]} ###########`);\ncall( ORANGE_GROUP, ORANGE_GROUP_VERSION, KONPYUTAIKA_GROUP, KONPYUTAIKA_GROUP_VERSION, KIND_PLURAL[argv.type], NAMESPACE)\n\t.then(r => console.log('############ END ############'))\n\t.catch(e => console.error(e));\n```\n\n----------------------------------------\n\nTITLE: Accessing Prometheus via Port Forwarding\nDESCRIPTION: Uses the `kubectl port-forward` command to establish a connection between the local machine's port 9090 and port 9090 of the `prometheus-operated` service running in the 'monitoring-system' namespace. This allows accessing the Prometheus web UI by navigating to `http://localhost:9090` in a web browser.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_10\n\nLANGUAGE: console\nCODE:\n```\nkubectl port-forward -n monitoring-system service/prometheus-operated 9090:9090\n```\n\n----------------------------------------\n\nTITLE: Terraform Destroy Resource Script (Consol)\nDESCRIPTION: Executes the destroy.sh script to teardown all infrastructure provisioned with Terraform. Dependency: working Terraform setup and valid service account key file. Parameter required is the file system path to the service account key. This removes cloud resources not deleted by Kubernetes removal. Limitation: irreversibly deletes infrastructure, use with caution.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/blog/2020-06-30-secured_nifi_cluster_on_gcp_with_external_dns.md#_snippet_21\n\nLANGUAGE: consol\nCODE:\n```\ncd terraform\n./destroy.sh <service account key's path>\n```\n\n----------------------------------------\n\nTITLE: Defining Istio VirtualService for HTTPS in YAML\nDESCRIPTION: This YAML snippet defines an Istio `VirtualService` for HTTPS traffic.  It redirects the decrypted HTTP traffic from the `Gateway` to a specific `ClusterIP` service. The `gateways` field links to the `nifi-gateway`. The `hosts` specifies the domain.  The `http` section defines the routing rules, directing traffic to the specified service and port. The input is HTTP traffic from Gateway; the output is routing to the NiFi service.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: nifi-vs\nspec:\n  gateways:\n  - nifi-gateway\n  hosts:\n  - nifi.my-domain.com\n  http:\n  - match:\n    - uri:\n        prefix: /\n    route:\n    - destination:\n        host: <service-name>.<namespace>.svc.cluster.local\n        port:\n          number: 8443\n```\n```\n\n----------------------------------------\n\nTITLE: NiFiCluster SSL Configuration\nDESCRIPTION: This YAML snippet demonstrates the configuration required to secure a NiFi cluster with SSL using the NiFi operator. It defines internal listeners for HTTPS, cluster communication, and site-to-site communication. It also specifies the SSL secrets to be used, including the option to create them.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  readOnlyConfig:\n    # NifiProperties configuration that will be applied to the node.\n    nifiProperties:\n      webProxyHosts:\n        - nifistandard2.trycatchlearn.fr:8443\n        ...\n  ...\n  listenersConfig:\n    internalListeners:\n      - type: \"https\"\n        name: \"https\"\n        containerPort: 8443\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n```\n\n----------------------------------------\n\nTITLE: Kubectl Get Pods in nifikop\nDESCRIPTION: This command lists the pods within the nifikop namespace. The command is used to verify that the NiFiKop operator, which manages NiFi clusters, is running correctly. This is a key step for ensuring that the operator is ready to create and manage NiFi instances within the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/blog/2020-06-30-secured_nifi_cluster_on_gcp_with_external_dns.md#_snippet_7\n\nLANGUAGE: console\nCODE:\n```\nkubectl -n nifikop get pods\n```\n\n----------------------------------------\n\nTITLE: Pushing NiFiKop Docker Image\nDESCRIPTION: Pushes the NiFiKop Docker image to the configured Docker repository. This makes the image available for deployment in a Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/6_contributing/1_developer_guide.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n$ make docker-push\n```\n\n----------------------------------------\n\nTITLE: Listing Services in Kubernetes to Verify External Service Exposure - Console\nDESCRIPTION: This console command output shows how to verify the Kubernetes services deployed for NiFi cluster exposure. The `kubectl get services` command lists the service named `cluster-access` along with its type (`LoadBalancer`), cluster IP, external IP, ports exposed (mapping external to internal ports), and age. This information confirms that the external service configuration has been applied correctly and shows where the cluster is accessible externally.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_4\n\nLANGUAGE: Console\nCODE:\n```\nkubectl get services\nNAME                TYPE           CLUSTER-IP    EXTERNAL-IP      PORT(S)                                AGE\ncluster-access      LoadBalancer   10.88.21.98   35.180.241.132   443:30421/TCP,80:30521/TCP             20d\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Internal Listener in YAML\nDESCRIPTION: Example of adding a custom internal listener without a specific type for exposing a NiFi processor endpoint, such as an HTTP tracking endpoint on port 8081.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    ...\n    - name: \"http-tracking\"\n      containerPort: 8081\n```\n\n----------------------------------------\n\nTITLE: Listing All Helm Releases Including Deleted and Failed\nDESCRIPTION: Command to list all Helm releases in the Kubernetes cluster, including currently deployed, deleted, and failed releases. This provides a complete history of all release attempts.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --all\n```\n\n----------------------------------------\n\nTITLE: Verifying Operator Pod Status using Console\nDESCRIPTION: Uses `kubectl get pods` to list pods in the `nifikop` namespace, verifying that the operator pod (e.g., `skeleton-nifikop-...`) is running (`STATUS` should be `Running`, `READY` should be `1/1`) after Helm installation. Requires `kubectl` v1.16+.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/6_contributing/1_developer_guide.md#_snippet_9\n\nLANGUAGE: console\nCODE:\n```\n$ kubectl get pods -n nifikop\nNAME                                                READY   STATUS    RESTARTS   AGE\nskeleton-nifikop-8946b89dc-4cfs9   1/1     Running   0          7m45s\n```\n\n----------------------------------------\n\nTITLE: Building NiFiKop Locally with Make\nDESCRIPTION: Builds the NiFiKop operator binary locally using the Makefile. This is suitable for development environments where direct binaries are preferred over containerized deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/6_contributing/1_developer_guide.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmake build\n```\n\n----------------------------------------\n\nTITLE: Kubernetes Pod Port Definitions for NiFi Listeners (YAML)\nDESCRIPTION: This YAML snippet presents the ports section of a Kubernetes Pod spec, listing internal ports exposed by the NiFi container. Each port entry corresponds to an internal listener (such as https, cluster, load-balance, s2s, prometheus) by containerPort, name, and TCP protocol. This configuration is generated automatically based on the internalListeners configuration and is necessary for correct pod-to-pod and service communication in the NiFi cluster. No external dependencies except standard Kubernetes pod definition. All ports must align with the listener definitions, and protocol is typically TCP.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\n    ports:\n    - containerPort: 8443\n      name: https\n      protocol: TCP\n    - containerPort: 6007\n      name: cluster\n      protocol: TCP\n    - containerPort: 6342\n      name: load-balance\n      protocol: TCP\n    - containerPort: 10000\n      name: s2s\n      protocol: TCP\n    - containerPort: 9090\n      name: prometheus\n      protocol: TCP\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop Without Modifying Existing CRDs\nDESCRIPTION: Command to install NiFiKop while skipping CRD installation, useful when you want to keep existing CRD definitions untouched.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nhelm install --name nifikop ./helm/nifikop --set namespaces={\"nifikop\"} --skip-crds\n```\n\n----------------------------------------\n\nTITLE: BootstrapProperties overrideConfigs Example\nDESCRIPTION: This YAML snippet shows an example of using `overrideConfigs` within the `bootstrapProperties` section of `ReadOnlyConfig`. This allows for overwriting default `bootstrap.properties` configurations.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/5_references/1_nifi_cluster/2_read_only_config.md#_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\n    overrideConfigs: |\n      # java.arg.4=-Djava.net.preferIPv4Stack=true\n```\n\n----------------------------------------\n\nTITLE: Testing the NiFiKop Kubectl Plugin Installation (Console)\nDESCRIPTION: Verifies the successful installation and accessibility of the NiFiKop kubectl plugin by running the base command 'kubectl nifikop'. The expected output is the plugin's usage help text, listing available subcommands.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/2_deploy_nifikop/3_kubectl_plugin.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\n$ kubectl nifikop\nUsage:\n  nifikop [command]\n\nAvailable Commands:\n  completion          Generate the autocompletion script for the specified shell\n  help                Help about any command\n  nificluster         \n  nificonnection      \n  nifidataflow        \n  nifigroupautoscaler \n  nifiregistryclient  \n  nifiuser            \n  nifiusergroup \n```\n\n----------------------------------------\n\nTITLE: Port-Forwarding to Access Prometheus UI\nDESCRIPTION: Command to set up port forwarding to access the Prometheus web interface for querying and visualizing metrics.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_10\n\nLANGUAGE: console\nCODE:\n```\nkubectl port-forward -n monitoring-system service/prometheus-operated 9090:9090\n```\n\n----------------------------------------\n\nTITLE: Applying NiFi Cluster Configuration for Scale Down in Kubernetes\nDESCRIPTION: Shell command to apply the updated NiFi cluster configuration with a node removed, which will trigger the scale-down operation and graceful decommissioning process.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nkubectl -n nifi apply -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring externalTrafficPolicy and internalTrafficPolicy in NifiCluster\nDESCRIPTION: This snippet involves setting network policies for external services in the NifiCluster operator, allowing users to control traffic routing policies for better network management. Dependencies include Kubernetes service configurations, and key parameters are 'internalTrafficPolicy' and 'externalTrafficPolicy'.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/CHANGELOG.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: v1\nkind: Service\nmetadata:\n  name: nifi-external\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 80\n    targetPort: 8080\n  selector:\n    app: nifi\n  externalTrafficPolicy: ExternalTrafficPolicyType\n  internalTrafficPolicy: InternalTrafficPolicyType\n```\n\n----------------------------------------\n\nTITLE: Deploy CRDs Manually\nDESCRIPTION: This set of commands uses `kubectl apply -f` to install the Custom Resource Definitions (CRDs) required by the NiFiKop operator when `--skip-crds` is used with Helm. These CRDs define the custom resources that the operator manages. Requires `kubectl` and a configured Kubernetes cluster. The commands sequentially apply YAML files from a remote source, one CRD per command. These are typically pre-requisites.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/2_deploy_nifikop/1_quick_start.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifinodegroupautoscalers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificonnections.yaml\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFi Cluster on OpenShift\nDESCRIPTION: Command to create a NiFi cluster in OpenShift by applying the OpenShift-specific configuration file with the proper security context settings.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create -n nifi -f config/samples/openshift.yaml\n```\n\n----------------------------------------\n\nTITLE: Building and Installing Kubectl NiFiKop Plugin\nDESCRIPTION: This snippet demonstrates how to build the kubectl-nifikop plugin and copy the executable to a directory in your PATH, specifically /usr/local/bin, making it accessible system-wide. This allows kubectl to find and execute the plugin. The command utilizes make to build the plugin and sudo to grant the necessary permissions for copying to the /usr/local/bin directory. It assumes a UNIX-like operating system.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/2_deploy_nifikop/3_kubectl_plugin.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nmake kubectl-nifikop && sudo cp ./bin/kubectl-nifikop /usr/local/bin\n```\n\n----------------------------------------\n\nTITLE: Get Helm Deployment Status - Helm\nDESCRIPTION: This command retrieves the status of a specific Helm deployment, identified by its release name.  In this case, it retrieves the status of the `nifikop` deployment. It requires Helm to be installed and configured.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nhelm status nifikop\n```\n\n----------------------------------------\n\nTITLE: Extracting SSL Credentials from Kubernetes Secret\nDESCRIPTION: Commands for extracting SSL credentials (CA certificate, client certificate, and private key) from a Kubernetes secret and saving them as local files.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['ca\\.crt']}\" | base64 -d > ca.crt\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['tls\\.crt']}\" | base64 -d > tls.crt\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['tls\\.key']}\" | base64 -d > tls.key\n```\n\n----------------------------------------\n\nTITLE: Setting Development Environment Variables for NiFiKop in Bash\nDESCRIPTION: This section sets required environment variables for running NiFiKop in development mode. The variables define the kubeconfig path, watcher namespace, pod name, log level, and operator name. Dependencies include a shell that supports environment variables and access to the referenced Kubernetes context. Inputs are the variable values for connecting to desired clusters; no direct outputs but they affect subsequent processes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/6_contributing/1_developer_guide.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nKUBECONFIG={path/to/your/kubeconfig}\nWATCH_NAMESPACE={namespace_to_watch}\nPOD_NAME={name for operator pod}\nLOG_LEVEL=Debug\nOPERATOR_NAME=ide\n```\n\n----------------------------------------\n\nTITLE: Defining Istio Gateway for HTTP Ingress - YAML\nDESCRIPTION: Defines an Istio Gateway resource to expose NiFi via HTTP on port 80, intercepting external requests for a specified domain. Requires Istio installed with an ingress gateway, the domain properly routed to the gateway, and Kubernetes access. Set the desired host(s) in the 'hosts' array, and ensure that the NiFi service is accessible inside the cluster. Outputs a Gateway resource that mediates external traffic to the service mesh.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: nifi-gateway\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - nifi.my-domain.com\n\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Internal NiFi Listener in YAML\nDESCRIPTION: This YAML snippet demonstrates how to define additional, non-standard internal listeners within the `listenersConfig.internalListeners` array. By omitting the `type` field, you can specify a custom port and name (`containerPort` and `name`) for purposes not directly related to NiFi's core internal functions, such as exposing a custom processor endpoint.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    ...\n    - name: \"http-tracking\"\n      containerPort: 8081\n```\n\n----------------------------------------\n\nTITLE: Manually Deleting Nifikop CRDs with kubectl - Bash\nDESCRIPTION: Manually removes all Nifikop-related CustomResourceDefinitions (CRDs) from the Kubernetes cluster. Should only be used when a full teardown is required, as this action deletes all resources of the removed CRD kind. Dependencies: kubectl CLI and permission to delete cluster-scoped resources. No parameters required; each line is a standalone deletion command.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete crd nificlusters.nifi.konpyutaika.com\nkubectl delete crd nifiusers.nifi.konpyutaika.com\nkubectl delete crd nifiusergroups.nifi.konpyutaika.com\nkubectl delete crd nifiregistryclients.nifi.konpyutaika.com\nkubectl delete crd nifiparametercontexts.nifi.konpyutaika.com\nkubectl delete crd nifidataflows.nifi.konpyutaika.com\n```\n\n----------------------------------------\n\nTITLE: Creating a Kubernetes Secret for NiFi Basic Authentication (Shell)\nDESCRIPTION: This `kubectl` command creates a generic Kubernetes secret named `nifikop-credentials` within the `nifikop-nifi` namespace. It populates the secret using data from local files specified by `--from-file`, providing the username, password, and optionally a CA certificate (`ca.crt`). This secret is referenced by the `NifiCluster` CR when using 'basic' authentication (`clientType: 'basic'`) for the NiFi operator to connect to the external cluster's API.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/1_manage_clusters/3_external_cluster.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nkubectl create secret generic nifikop-credentials \\\n  --from-file=username=./secrets/username\\\n  --from-file=password=./secrets/password\\\n  --from-file=ca.crt=./secrets/ca.crt\\\n  -n nifikop-nifi\n```\n\n----------------------------------------\n\nTITLE: Deploying Prometheus Operator via Helm\nDESCRIPTION: Installs 'kube-prometheus-stack' Helm chart into the 'monitoring-system' namespace with specific configurations disabled and logging level set to debug. Dependencies include Helm CLI and a Kubernetes cluster reachable for deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nhelm install prometheus prometheus/kube-prometheus-stack --namespace monitoring-system \\n    --set prometheusOperator.createCustomResource=false \\n    --set prometheusOperator.logLevel=debug \\n    --set prometheusOperator.alertmanagerInstanceNamespaces=monitoring-system \\n    --set prometheusOperator.namespaces.additional[0]=monitoring-system \\n    --set prometheusOperator.prometheusInstanceNamespaces=monitoring-system \\n    --set prometheusOperator.thanosRulerInstanceNamespaces=monitoring-system \\n    --set defaultRules.enable=false \\n    --set alertmanager.enabled=false \\n    --set grafana.enabled=false \\n    --set kubeApiServer.enabled=false \\n    --set kubelet.enabled=false \\n    --set kubeControllerManager.enabled=false \\n    --set coreDNS.enabled=false \\n    --set kubeEtcd.enabled=false \\n    --set kubeScheduler.enabled=false \\n    --set kubeProxy.enabled=false \\n    --set kubeStateMetrics.enabled=false \\n    --set prometheus.enabled=false\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFiKop with Helm\nDESCRIPTION: Command to deploy the NiFiKop operator using Helm. Configures resource limits, namespaces, and specifies the operator version to use.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/2_deploy_nifikop/1_quick_start.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# You have to create the namespace before executing following command\nhelm install nifikop \\\n    oci://ghcr.io/konpyutaika/helm-charts/nifikop \\\n    --namespace=nifi \\\n    --version 1.0.0 \\\n    --set image.tag=v1.0.0-release \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set namespaces={\"nifi\"}\n```\n\n----------------------------------------\n\nTITLE: UserReference ObjectSchema\nDESCRIPTION: This schema defines the reference model for individual NiFi users, specifying the user name and namespace location. Both fields are mandatory, facilitating identification and linkage within user group configurations.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/5_references/6_nifi_usergroup.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n# Reference to a NiFi User, including name and namespace.\n\n```\n\n----------------------------------------\n\nTITLE: Installing cert-manager directly with kubectl for NiFiKop\nDESCRIPTION: Installs cert-manager v1.7.2 directly using kubectl by applying the manifest from the official GitHub repository. This is required for NiFiKop to issue certificates for users and nodes in secured clusters.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/2_deploy_nifikop/1_quick_start.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install the CustomResourceDefinitions and cert-manager itself\nkubectl apply -f \\\n    https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.yaml\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for NiFiKop IDE Development using Bash\nDESCRIPTION: Defines essential environment variables required for running the NiFiKop operator directly from an Integrated Development Environment (IDE). Variables include the Kubernetes config path (`KUBECONFIG`), namespace to watch (`WATCH_NAMESPACE`), operator pod name (`POD_NAME`), log level (`LOG_LEVEL`), and a name for the operator instance (`OPERATOR_NAME`).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/6_contributing/1_developer_guide.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nKUBECONFIG={path/to/your/kubeconfig}\nWATCH_NAMESPACE={namespace_to_watch}\nPOD_NAME={name for operator pod}\nLOG_LEVEL=Debug\nOPERATOR_NAME=ide\n```\n\n----------------------------------------\n\nTITLE: Cert-Manager Issuer Configuration\nDESCRIPTION: This YAML configuration defines a cert-manager Issuer to be used with Let's Encrypt.  It includes the ACME server, email address, private key secret reference, and HTTP01 challenge solver with ingress template for external-dns integration. This issuer is then referenced in the NiFiCluster spec to provision certificates.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: cert-manager.io/v1alpha2\nkind: Issuer\nmetadata:\n  name: letsencrypt-staging\nspec:\n  acme:\n    # You must replace this email address with your own.\n    # Let's Encrypt will use this to contact you about expiring\n    # certificates, and issues related to your account.\n    email: <your email address>\n    server: https://acme-staging-v02.api.letsencrypt.org/directory\n    privateKeySecretRef:\n      # Secret resource used to store the account's private key.\n      name: example-issuer-account-key\n    # Add a single challenge solver, HTTP01 using nginx\n    solvers:\n      - http01:\n          ingress:\n            ingressTemplate:\n              metadata:\n                annotations:\n                  \"external-dns.alpha.kubernetes.io/ttl\": \"5\"\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop with Helm\nDESCRIPTION: This snippet demonstrates the command-line usage of Helm to install the NiFiKop operator. It shows how to install the chart and provides an example using a values file to configure the installation. The command takes the release name, chart name, and optional arguments for customization.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n$ helm install nifikop \\\n      konpyutaika/nifikop \\\n      -f values.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop Using Helm\nDESCRIPTION: Command to install NiFiKop using Helm chart with a custom image tag and namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/6_contributing/1_developer_guide.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nhelm install skeleton ./helm/nifikop \\\n    --set image.tag=v0.5.1-release \\\n    --namespace-{\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Deploy NiFiKop CRDs (v1beta1)\nDESCRIPTION: These commands apply the Custom Resource Definitions (CRDs) for NiFiKop, enabling the management of NiFi clusters and users within the Kubernetes cluster. The first snippet applies the NiFiCluster CRD, and the second applies the NiFiUser CRD. These resources are necessary to define and manage NiFi clusters using the NiFiKop operator. These resources are meant to be used for older versions of Kubernetes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/blog/2020-06-30-secured_nifi_cluster_on_gcp_with_external_dns.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/Orange-OpenSource/nifikop/master/deploy/crds/v1beta1/nifi.orange.com_nificlusters_crd.yaml\nkubectl apply -f https://raw.githubusercontent.com/Orange-OpenSource/nifikop/master/deploy/crds/v1beta1/nifi.orange.com_nifiusers_crd.yaml\n```\n\n----------------------------------------\n\nTITLE: Building NiFiKop Operator with Make Bash\nDESCRIPTION: This command builds the NiFiKop operator using the provided Makefile. Requires GNU Make and all project build dependencies (e.g., Go) to be installed. No extra parameters are necessary for a local build. Produces operator binaries in the appropriate build directory. Intended for local development environments.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/6_contributing/1_developer_guide.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmake build\n```\n\n----------------------------------------\n\nTITLE: Cloning NiFiKop Repository using Bash\nDESCRIPTION: Clones the NiFiKop source code from GitHub and changes the current directory to the cloned project folder. This is the first step in setting up the development environment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/6_contributing/1_developer_guide.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/konpyutaika/nifikop.git\ncd nifikop\n```\n\n----------------------------------------\n\nTITLE: Formatting Commit Messages with Subsystem and Description - Text\nDESCRIPTION: This snippet demonstrates the correct structure for writing commit messages in the NiFiKop project, aligning with conventions that prioritize clarity and context. No dependencies are required, but contributors must follow the specific format: beginning with the subsystem and concise change summary, an empty line, then detailed explanation and an optional footer. The subject must be within 70 characters, and body lines should wrap at 80 characters for optimal readability across tools.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/CONTRIBUTING.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nscripts: add the test-cluster command\n\nthis uses tmux to setup a test cluster that can easily be killed and started for debugging.\n\nFixes #38\n```\n\n----------------------------------------\n\nTITLE: Updating Helm Repositories\nDESCRIPTION: Updates all Helm repositories to ensure access to the latest chart versions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_5\n\nLANGUAGE: console\nCODE:\n```\nhelm repo update\n```\n\n----------------------------------------\n\nTITLE: Running NiFiKop Operator Locally Using Makefile (Bash)\nDESCRIPTION: This snippet runs the NiFiKop operator binary locally via the 'make run' Makefile target. It uses the default Kubernetes configuration (usually $HOME/.kube/config) and runs the operator in the 'default' namespace. This method supports local development and testing without deploying inside the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/6_contributing/1_developer_guide.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmake run\n```\n\n----------------------------------------\n\nTITLE: Routing HTTP Traffic with Istio VirtualService for NiFi (YAML)\nDESCRIPTION: Configures an Istio VirtualService named `nifi-vs` associated with the `nifi-gateway`. It routes all incoming HTTP requests for `nifi.my-domain.com` matching the root URI prefix (`/`) to the internal NiFi service (assumed to be named `nifi`) on port 8080.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: nifi-vs\nspec:\n  gateways:\n  - nifi-gateway\n  hosts:\n  - nifi.my-domain.com\n  http:\n  - match:\n    - uri:\n        prefix: /\n    route:\n    - destination:\n        host: nifi\n        port:\n          number: 8080\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Cluster SSL with YAML using Nifi Operator\nDESCRIPTION: This YAML snippet demonstrates how to configure SSL for a NiFi cluster by specifying HTTPS listeners, SSL secret creation, and Web Proxy Hosts. It is intended for users deploying managed NiFi clusters with SSL in Kubernetes, ensuring encrypted communication within the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  readOnlyConfig:\n    # NifiProperties configuration that will be applied to the node.\n    nifiProperties:\n      webProxyHosts:\n        - nifistandard2.trycatchlearn.fr:8443\n        ...\n  ...\n  listenersConfig:\n    internalListeners:\n      - type: \"https\"\n        name: \"https\"\n        containerPort: 8443\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n```\n\n----------------------------------------\n\nTITLE: Sample package.json for NiFiKop CRD Migration - JSON\nDESCRIPTION: This snippet shows a complete 'package.json' configured for the CRD migration project with all scripts and dependencies. It prescribes versioned dependencies, provides a placeholder test script, and includes metadata for clarity. This is required for npm-based workflow and proper dependency resolution.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"nifikop_crd_migration\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Script to migrate from the old CRDs to the new CRDs.\",\n  \"main\": \"index.js\",\n  \"scripts\": {\n    \"start\": \"node --no-warnings index.js\",\n    \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\"\n  },\n  \"keywords\": [\n    \"K8S\",\n    \"NiFiKop\",\n    \"CRDs\"\n  ],\n  \"license\": \"ISC\",\n  \"dependencies\": {\n    \"@kubernetes/client-node\": \"^0.16.3\",\n    \"minimist\": \"^1.2.6\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Deleting CRDs - Kubernetes (bash)\nDESCRIPTION: This series of commands manually deletes Custom Resource Definitions (CRDs) associated with the NiFi operator in Kubernetes.  It's essential to understand that deleting CRDs will remove all clusters created using these CRDs. Requires kubectl installed and configured to connect to the cluster. The output is the removal of specified CRDs.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete crd nificlusters.nifi.konpyutaika.com\nkubectl delete crd nifiusers.nifi.konpyutaika.com\nkubectl delete crd nifiusergroups.nifi.konpyutaika.com\nkubectl delete crd nifiregistryclients.nifi.konpyutaika.com\nkubectl delete crd nifiparametercontexts.nifi.konpyutaika.com\nkubectl delete crd nifidataflows.nifi.konpyutaika.com\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFiCluster SSL with External DNS and Issuer (YAML)\nDESCRIPTION: Configures a `NifiCluster` to use SSL with an existing cert-manager `Issuer` (named `letsencrypt-staging`) and external DNS. It enables cluster and site-to-site security (`clusterSecure: true`, `siteToSiteSecure: true`), specifies the DNS zone (`clusterDomain`), enables external DNS integration (`useExternalDNS: true`), and references the pre-configured `Issuer` via `listenersConfig.sslSecrets.issuerRef`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  clusterSecure: true\n  siteToSiteSecure: true\n  ...\n  listenersConfig:\n    clusterDomain: <DNS zone name>\n    useExternalDNS: true\n    ...\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n      issuerRef:\n        name: letsencrypt-staging\n        kind: Issuer\n```\n\n----------------------------------------\n\nTITLE: Configuring Nifi External Service with ClusterIP in YAML\nDESCRIPTION: This YAML snippet defines an external service configuration for Nifi using the `ClusterIP` service type. It creates a service named \"clusterip\" that maps external port 8080 to the internal Nifi listener named \"http\" (TCP implied) and port 7182 to the internal listener \"my-custom-listener\" using TCP. Custom metadata including annotations and labels are also applied to the resulting Kubernetes Service object.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/5_references/1_nifi_cluster/7_external_service_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  externalServices:\n    - name: \"clusterip\"\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8080\n            internalListenerName: \"http\"\n          - port: 7182\n            internalListenerName: \"my-custom-listener\"\n            protocol: TCP\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n\n----------------------------------------\n\nTITLE: Building NiFiKop Locally Using Make\nDESCRIPTION: Executes the make command to build the NiFiKop project in the local environment, facilitating development workflows without deploying to a Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/6_contributing/1_developer_guide.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmake build\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiNodeGroupAutoscaler resource in YAML\nDESCRIPTION: A complete example of a NifiNodeGroupAutoscaler configuration that defines how to automatically scale a specific node group in a NiFi cluster. It includes references to the target cluster, node group identification, scaling strategies, and label selectors.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/5_references/7_nifi_nodegroup_autoscaler.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiNodeGroupAutoscaler\nmetadata:\n  name: nifinodegroupautoscaler-sample\nspec:\n  # contains the reference to the NifiCluster with the one the node group autoscaler is linked.\n  clusterRef:\n    name: nificluster-name\n    namespace: nifikop\n  # defines the id of the NodeConfig contained in NifiCluster.Spec.NodeConfigGroups\n  nodeConfigGroupId: default-node-group\n  # The selector used to identify nodes in NifiCluster.Spec.Nodes this autoscaler will manage\n  # Use Node.Labels in combination with this selector to clearly define which nodes will be managed by this autoscaler \n  nodeLabelsSelector: \n    matchLabels:\n      nifi_cr: nificluster-name\n      nifi_node_group: default-node-group\n  # the strategy used to decide how to add nodes to a nifi cluster\n  upscaleStrategy: simple\n  # the strategy used to decide how to remove nodes from an existing cluster\n  downscaleStrategy: lifo\n```\n\n----------------------------------------\n\nTITLE: Defining NifiUserGroup Resource - YAML\nDESCRIPTION: This YAML snippet defines a Kubernetes custom resource of kind `NifiUserGroup`. It demonstrates how to specify the group's identity, reference the target NiFi cluster and namespace, list member users by referencing `NifiUser` resources, and define initial access policies for the group. This resource is intended to be applied to a Kubernetes cluster running the `nifikop` operator.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/5_references/6_nifi_usergroup.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  identity: \"My Special Group\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n    - name: nc-controller.nifikop.mgt.cluster.local\n  accessPolicies:\n    - type: global\n      action: read\n      resource: /counters\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Secret for Sensitive NiFi Parameters Using kubectl Console\nDESCRIPTION: This console command creates a generic Kubernetes secret named secret-params in the nifikop namespace. It includes two key-value pairs representing sensitive parameter values that can be referenced in the NifiParameterContext. Since NiFi treats sensitive parameters securely and does not expose them via REST API, secrets must be updated or replaced properly to manage these sensitive values.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl create secret generic secret-params \\\n    --from-literal=secret1=yop \\\n    --from-literal=secret2=yep \\\n    -n nifikop\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Parameter Contexts with YAML Examples\nDESCRIPTION: Examples of NiFi parameter context definitions showing basic usage and inheritance. The first example creates a basic parameter context with regular and sensitive parameters, while the second demonstrates a child parameter context that inherits from the first one.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/5_references/4_nifi_parameter_context.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      description: toto\n      sensistive: true\n---\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle-child\nspec:\n  description: \"It is a child test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  inheritedParameterContexts:\n    - name: dataflow-lifecycle\n  parameters:\n    - name: test\n      value: toto-child\n      description: tutu (child)\n```\n\n----------------------------------------\n\nTITLE: Monitoring NiFi Node Scale-Down Status using kubectl describe\nDESCRIPTION: Uses `kubectl describe nificluster` to inspect the status of the `NifiCluster` custom resource named 'simplenifi'. The output shows the `Status.Nodes State` section, allowing monitoring of the graceful action state (e.g., `GracefulDownscaleRequired`) for the node being decommissioned (node ID 2).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_5\n\nLANGUAGE: console\nCODE:\n```\nkubectl describe nificluster simplenifi\n\n...\nStatus:\n  Nodes State:\n    ...\n    2:\n      Configuration State:  ConfigInSync\n      Graceful Action State:\n        Action State:   GracefulDownscaleRequired\n        Error Message:\n    ...\n...\n```\n\n----------------------------------------\n\nTITLE: Adding KEDA Helm Repository and Installing KEDA on Kubernetes Cluster using Console\nDESCRIPTION: This snippet demonstrates the commands to add the KEDA Helm repository to your local Helm configuration, update Helm repos to get the latest charts, create a Kubernetes namespace named 'keda', and install the KEDA Helm chart into that namespace. It is a prerequisite to deploying KEDA for event-driven autoscaling within Kubernetes clusters. Requires Helm installed and configured, and kubectl access to the target cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nhelm repo add kedacore https://kedacore.github.io/charts\nhelm repo update\nkubectl create namespace keda\nhelm install keda kedacore/keda --namespace keda\n```\n\n----------------------------------------\n\nTITLE: Patching NifiKop CRDs for Webhook Conversion (YAML)\nDESCRIPTION: This YAML snippet shows the necessary configuration changes to patch NifiKop CRDs (like NifiCluster, NifiDataflow, etc.) to enable webhook-based conversion between v1alpha1 and v1 API versions. It involves adding annotations for cert-manager CA injection and specifying the conversion strategy as 'Webhook' with details about the webhook service client configuration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/7_upgrade_guides/3_v0.16.0_to_v1.0.0.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n```\n\n----------------------------------------\n\nTITLE: Installing Nifikop Helm Chart (Dry Run) in Bash\nDESCRIPTION: This Bash command illustrates performing a dry run of the Helm chart installation for Nifikop, specifying a custom log level and namespace. Dependencies: Helm CLI, an accessible Helm repo (konpyutaika/nifikop), and a configured Kubernetes context. Parameters: --dry-run (test install), --set logLevel=Debug, --set namespaces. Inputs: none, outputs: simulated install manifest. Limitation: no real resources created.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop \\\n    --dry-run \\\n    --set logLevel=Debug \\\n    --set namespaces={\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Defining Prometheus RBAC Resources with Kubernetes YAML\nDESCRIPTION: This YAML manifest defines the Role-Based Access Control resources including a ServiceAccount named 'prometheus', ClusterRole with permissions to access Kubernetes cluster resources like nodes, metrics, services, configmaps, ingresses, and a ClusterRoleBinding linking the ClusterRole to the ServiceAccount. This configuration authorizes Prometheus to scrape required metrics from the cluster. It must be applied within the 'monitoring-system' namespace and requires Kubernetes cluster admin rights to deploy.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: prometheus\n  namespace: monitoring-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: prometheus\nrules:\n- apiGroups: [\"\"]\n  resources:\n  - nodes\n  - nodes/metrics\n  - services\n  - endpoints\n  - pods\n  verbs: [\"get\", \"list\", \"watch\"]\n- apiGroups: [\"\"]\n  resources:\n  - configmaps\n  verbs: [\"get\"]\n- apiGroups:\n  - networking.k8s.io\n  resources:\n  - ingresses\n  verbs: [\"get\", \"list\", \"watch\"]\n- nonResourceURLs: [\"/metrics\"]\n  verbs: [\"get\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: prometheus\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: prometheus\nsubjects:\n- kind: ServiceAccount\n  name: prometheus\n  namespace: monitoring-system\n```\n\n----------------------------------------\n\nTITLE: Creating NiFi User Certificate (YAML)\nDESCRIPTION: This YAML snippet defines a `NifiUser` custom resource, which is used to create and manage client certificates.  It specifies the cluster to which the user belongs and the desired secret name where the credentials will be stored. It uses the NiFi operator to generate the necessary certificates. The output is a secret containing the CA certificate, the client certificate, and the client private key.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\ncat << EOF | kubectl apply -n nifi -f -\napiVersion:  nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: example-client\n  namespace: nifi\nspec:\n  clusterRef:\n    name: nifi\n  secretName: example-client-secret\nEOF\n```\n\n----------------------------------------\n\nTITLE: Extracting NiFi User TLS Credentials from Kubernetes Secret - kubectl console commands\nDESCRIPTION: Provides commands to extract and decode the CA certificate, user certificate, and private key stored in a Kubernetes secret generated by the NifiUser resource. It uses kubectl with jsonpath to select the base64-encoded secret data keys, then decodes and writes them to local files. This allows users to obtain client TLS credentials for local use or pod mounting. It is dependent on existing secret presence created by the NiFi operator. The keys decoded include ca.crt, tls.crt, and tls.key.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['ca\\.crt']}\" | base64 -d > ca.crt\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['tls\\.crt']}\" | base64 -d > tls.crt\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['tls\\.key']}\" | base64 -d > tls.key\n```\n\n----------------------------------------\n\nTITLE: Exposing NiFi Cluster Ports (Shell)\nDESCRIPTION: This command exposes the NiFi cluster ports through a load balancer. The placeholder `<nifi_cluster_port>` should be replaced with the actual port number that the NiFi cluster will be accessible on. This step allows external access to the NiFi cluster after it's deployed within the K3D environment. The `@loadbalancer` part indicates that it should be handled by the load balancer.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/2_setup/2_platform_setup/2_k3d.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nk3d cluster edit k3s-default --port-add \"<nifi_cluster_port>:<nifi_cluster_port>@loadbalancer\"\n```\n\n----------------------------------------\n\nTITLE: Installing cert-manager via Helm with validation disabled\nDESCRIPTION: This snippet demonstrates the installation of cert-manager using Helm 3 by first applying the CRDs with validation turned off, adding the Jetstack Helm repository, updating it, and then installing cert-manager into the 'cert-manager' namespace at version v1.7.2. Helm simplifies deploying and managing cert-manager within Kubernetes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/2_deploy_nifikop/1_quick_start.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Install CustomResourceDefinitions first\nkubectl apply --validate=false -f \\\n   https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.crds.yaml\n\n# Add the jetstack helm repo\nhelm repo add jetstack https://charts.jetstack.io\nhelm repo update\n\n# You have to create the namespace before executing following command\nhelm install cert-manager \\\n    --namespace cert-manager \\\n    --version v1.7.2 jetstack/cert-manager\n\n```\n\n----------------------------------------\n\nTITLE: Verifying Kubernetes Horizontal Pod Autoscaler in Cluster\nDESCRIPTION: This console command retrieves the current HPA resources within the 'clusters' namespace, showing details like reference target, current metrics, minimum and maximum pod counts, replicas, and age. It is used to confirm that the HPA managing the NiFiNodeGroupAutoscaler is properly deployed and functioning within the Kubernetes environment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_11\n\nLANGUAGE: console\nCODE:\n```\nkubectl get -n clusters hpa\n```\n\n----------------------------------------\n\nTITLE: Getting Zookeeper UID/GID for OpenShift Security Context (Kubectl command)\nDESCRIPTION: Retrieves the 'supplemental-groups' annotation from the 'zookeeper' namespace in JSON format, processes it to extract the UID/GID value for configuring security settings 'RunAsUser' and 'fsGroup' in OpenShift environments.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nzookeper_uid=$(kubectl get namespace zookeeper -o=jsonpath='{.metadata.annotations.openshift\\.io/sa\\.scc\\.supplemental-groups}' | sed 's/\\/10000$//' | tr -d '[:space:]')\n```\n\n----------------------------------------\n\nTITLE: Applying NiFi Cluster Scale-Up Configuration (Shell)\nDESCRIPTION: This shell command uses `kubectl` to apply the updated `NifiCluster` configuration from a YAML file (`config/samples/simplenificluster.yaml`) to the `nifi` namespace in Kubernetes. This action triggers the NiFiKop operator to reconcile the cluster state and add the new node defined in the YAML.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nkubectl -n nifi apply -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Declaring Cluster Nodes with Group Configurations\nDESCRIPTION: This YAML snippet demonstrates how to declare NiFi cluster nodes and associate them with previously defined node configuration groups. It shows the use of `nodeConfigGroup` to assign nodes to specific configurations and `nodeConfig` for individual node overrides. Prerequisites: Defined node config groups. Input: YAML. Output: Configured NiFi cluster nodes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n- id: 0\n  nodeConfigGroup: \"default_group\"\n- id: 2\n  nodeConfigGroup: \"high_mem_group\"\n- id: 3\n  nodeConfigGroup: \"high_mem_group\"\n- id: 5\n  nodeConfig:\n    resourcesRequirements:\n      limits:\n        cpu: \"2\"\n        memory: 3Gi\n      requests:\n        cpu: \"1\"\n        memory: 1Gi\n```\n\n----------------------------------------\n\nTITLE: Creating Secrets for Basic Authentication with kubectl\nDESCRIPTION: This console command creates a secret named 'nifikop-credentials' in Kubernetes storing the username, password, and CA certificate necessary for basic authentication with the external NiFi cluster. It ensures secure handling of sensitive credentials.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/3_manage_nifi/1_manage_clusters/3_external_cluster.md#_snippet_1\n\nLANGUAGE: Console Commands\nCODE:\n```\nkubectl create secret generic nifikop-credentials \\\n  --from-file=username=./secrets/username\\\n  --from-file=password=./secrets/password\\\n  --from-file=ca.crt=./secrets/ca.crt\\\n  -n nifikop-nifi\n```\n\n----------------------------------------\n\nTITLE: Yaml configuration for defining managed admin and reader users in NifiCluster\nDESCRIPTION: This YAML snippet demonstrates how to specify lists of admin and reader users within the NifiCluster Custom Resource, including user identities and names. It enables automated creation and management of NifiUsers and NifiUserGroups based on these specifications, reducing manual configuration efforts.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/2_manage_users_and_accesses/3_managed_groups.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  ...\n  oneNifiNodePerNode: false\n  propagateLabels: true\n  managedAdminUsers:\n    -  identity: \"alexandre.guitton@konpyutaika.com\"\n       name: \"aguitton\"\n    -  identity: \"nifiuser@konpyutaika.com\"\n       name: \"nifiuser\"\n  managedReaderUsers:\n    -  identity: \"toto@konpyutaika.com\"\n       name: \"toto\"\n```\n\n----------------------------------------\n\nTITLE: Installing Helm Chart with CRD Skip - Bash\nDESCRIPTION: This command installs a Helm chart, in this case 'nifikop', and instructs Helm to skip the installation of CRDs. This is useful when the CRDs are already deployed or you do not want to modify the existing CRDs. It uses parameters to set the namespace and skip CRDs.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/2_setup/3_install/1_customizable_install_with_helm.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n$ helm install --name nifikop ./helm/nifikop --set namespaces={\\\"nifikop\\\"} --skip-crds\n```\n\n----------------------------------------\n\nTITLE: Updating Helm Repo for Prometheus\nDESCRIPTION: This command updates the Helm repository, similar to the earlier `helm repo update`. This pulls the latest chart information from the specified Prometheus repository. Keeping the helm repo up to date is required to install the newest versions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_7\n\nLANGUAGE: console\nCODE:\n```\nhelm repo update\n```\n\n----------------------------------------\n\nTITLE: Configuring Node.js start script (JSON)\nDESCRIPTION: Adds a custom script named \"start\" to the `package.json` file. This script executes the `index.js` file using Node.js, specifically disabling warnings. It provides a convenient alias for running the main migration script.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"start\": \"node --no-warnings index.js\"\n```\n\n----------------------------------------\n\nTITLE: Add Bitnami Helm repository (Bash)\nDESCRIPTION: This command adds the Bitnami Helm repository to your Helm configuration.  This allows you to install charts from the Bitnami repository, such as the Zookeeper chart used in this guide.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhelm repo add bitnami https://charts.bitnami.com/bitnami\n```\n\n----------------------------------------\n\nTITLE: Verifying New NiFi Node Resources in Kubernetes (Console)\nDESCRIPTION: This command uses `kubectl get` to query Kubernetes for pods, configmaps, and persistent volume claims (PVCs) associated with the newly added NiFi node (ID 25) using the label selector `nodeId=25`. The output confirms the creation and running status of the node's components.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl get pods,configmap,pvc -l nodeId=25\nNAME                          READY   STATUS    RESTARTS   AGE\npod/simplenifi-25-nodem5jh4   1/1     Running   0          11m\n\nNAME                             DATA   AGE\nconfigmap/simplenifi-config-25   7      11m\n\nNAME                                               STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\npersistentvolumeclaim/simplenifi-25-storagehwn24   Bound    pvc-7da86076-728e-11ea-846d-42010a8400f2   10Gi       RWO            standard       11m\n```\n\n----------------------------------------\n\nTITLE: Checking NiFiKop Deployment Status with Helm\nDESCRIPTION: Command to check the status of the NiFiKop operator deployment using Helm. This provides detailed information about the deployment including pods, services, and other Kubernetes resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nhelm status nifikop\n```\n\n----------------------------------------\n\nTITLE: Configuring Identity Mapping for OpenID Connect in NiFi - Shell\nDESCRIPTION: This shell snippet provides the required nifi.properties configuration lines for mapping distinguished names (DN) to user identities in NiFi when enabling OpenID Connect. Add these lines to nifi.properties to extract and transform CN attributes from user DNs. No additional dependencies are required, but correct placement in an existing nifi.properties file is necessary. The configuration expects certificate subject DNs and will map them, with no transformation applied, to facilitate multi-identity-provider authentication.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/5_users_authentication/1_oidc.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nnifi.security.identity.mapping.pattern.dn=CN=([^,]*)(?:, (?:O|OU)=.*)?\nnifi.security.identity.mapping.value.dn=$1\nnifi.security.identity.mapping.transform.dn=NONE\n```\n\n----------------------------------------\n\nTITLE: Configuring Internal Listeners in NiFi Kubernetes Deployment (YAML)\nDESCRIPTION: Defines the internal listener ports for NiFi cluster components, specifying type, name, and container port to facilitate internal communication and expose NiFi functionalities such as HTTPS, clustering, S2S, load balancing, and Prometheus metrics. These settings are essential for setting up port configurations in the Kubernetes pod spec.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    - type: \"https\"\n      name: \"https\"\n      containerPort: 8443\n    - type: \"cluster\"\n      name: \"cluster\"\n      containerPort: 6007\n    - type: \"s2s\"\n      name: \"s2s\"\n      containerPort: 10000\n    - type: \"prometheus\"\n      name: \"prometheus\"\n      containerPort: 9090\n    - type: \"load-balance\"\n      name: \"load-balance\"\n      containerPort: 6342\n```\n\n----------------------------------------\n\nTITLE: Manually Delete NiFiKop CRDs - Bash\nDESCRIPTION: Deletes the Custom Resource Definitions (CRDs) manually from the Kubernetes cluster using `kubectl delete crd`. This is a manual cleanup step after uninstalling the Helm chart, as CRDs are not deleted by default. Caution is advised as deleting CRDs will remove all custom resources (like NiFiClusters) defined by them.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/2_setup/3_install/1_customizable_install_with_helm.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete crd nificlusters.nifi.konpyutaika.com\nkubectl delete crd nifiusers.nifi.konpyutaika.com\nkubectl delete crd nifiusergroups.nifi.konpyutaika.com\nkubectl delete crd nifiregistryclients.nifi.konpyutaika.com\nkubectl delete crd nifiparametercontexts.nifi.konpyutaika.com\nkubectl delete crd nifidataflows.nifi.konpyutaika.com\n```\n\n----------------------------------------\n\nTITLE: Configuring External Exposure of NiFi Cluster via Kubernetes Service in YAML\nDESCRIPTION: Sets up external access to NiFi internal listeners via Kubernetes Service definitions, specifying 'name', service 'type', and port mappings to internal listeners. It enables external clients to reach NiFi endpoints using defined ports and service type like LoadBalancer. Dependencies include Kubernetes Service specifications; key parameters include 'port', 'internalListenerName', and 'type'. Inputs are YAML service configurations; outputs are Kubernetes services exposing NiFi externally.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nexternalServices:\n  - name: cluster-access\n    spec:\n      portConfigs:\n        - internalListenerName: https\n          port: 443\n        - internalListenerName: http-tracking\n          port: 80\n      type: LoadBalancer\n```\n\n----------------------------------------\n\nTITLE: Installing KEDA and Namespace - Helm and Kubectl Console Commands - console\nDESCRIPTION: This snippet first creates a dedicated Kubernetes namespace for KEDA using kubectl, then installs the KEDA Helm chart into that namespace using \"helm install\". Prerequisites are a functional Kubernetes cluster, kubectl, and Helm installed. The \"kubectl create namespace keda\" command creates the keda namespace, and the \"helm install keda kedacore/keda --namespace keda\" command deploys KEDA into that namespace. Inputs are fixed; output is success messages for namespace and Helm chart deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl create namespace keda\nhelm install keda kedacore/keda --namespace keda\n```\n\n----------------------------------------\n\nTITLE: Installing cert-manager directly using kubectl in Bash\nDESCRIPTION: This snippet shows how to install cert-manager using kubectl by applying a single manifest file containing both the CustomResourceDefinitions and cert-manager resources. This method requires network connectivity to GitHub and assumes kubectl is configured to interact with the target cluster. The cert-manager version installed is v1.7.2, which is compatible with NiFiKop's requirement of at least v1.0.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/2_deploy_nifikop/1_quick_start.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install the CustomResourceDefinitions and cert-manager itself\nkubectl apply -f \\\n    https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.yaml\n```\n\n----------------------------------------\n\nTITLE: Executing NiFiKop Migration Script - Bash\nDESCRIPTION: Runs the Node.js migration script defined in `package.json` using `npm start`. The double dash (`--`) separates npm arguments from arguments passed to the script (`index.js`). It takes required arguments `--type` specifying the NiFiKop resource type (e.g., `cluster`, `dataflow`) and optional `--namespace` specifying the Kubernetes namespace (defaults to 'default'). Requires Node.js, npm, the configured `package.json`, and `index.js` to be present.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nnpm start -- --type=<NIFIKOP_RESOURCE> --namespace=<K8S_NAMESPACE>\n```\n\n----------------------------------------\n\nTITLE: Manually Deleting Nifikop CRDs using kubectl\nDESCRIPTION: Provides `kubectl delete crd` commands to manually remove Nifikop Custom Resource Definitions from the cluster. This step might be necessary after Helm uninstall if CRDs were not automatically removed. **Warning:** Deleting CRDs permanently removes all custom resources managed by them (e.g., all NiFi clusters).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete crd nificlusters.nifi.konpyutaika.com\nkubectl delete crd nifiusers.nifi.konpyutaika.com\nkubectl delete crd nifiusergroups.nifi.konpyutaika.com\nkubectl delete crd nifiregistryclients.nifi.konpyutaika.com\nkubectl delete crd nifiparametercontexts.nifi.konpyutaika.com\nkubectl delete crd nifidataflows.nifi.konpyutaika.com\n```\n\n----------------------------------------\n\nTITLE: Configuring Internal Listeners in NiFi on Kubernetes\nDESCRIPTION: This YAML snippet demonstrates how to configure internal listeners for a NiFi cluster within Kubernetes using the `Spec.ListenersConfig.InternalListeners` field.  It defines the `type`, `name`, and `containerPort` for various listeners like `https`, `cluster`, `s2s`, `prometheus`, and `load-balance`.  These listeners are used for internal NiFi communication and exposing services like the UI or metrics.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    - type: \"https\"\n      name: \"https\"\n      containerPort: 8443\n    - type: \"cluster\"\n      name: \"cluster\"\n      containerPort: 6007\n    - type: \"s2s\"\n      name: \"s2s\"\n      containerPort: 10000\n    - type: \"prometheus\"\n      name: \"prometheus\"\n      containerPort: 9090\n    - type: \"load-balance\"\n      name: \"load-balance\"\n      containerPort: 6342\n```\n\n----------------------------------------\n\nTITLE: Exposing Listener Ports in a Pod's Kubernetes YAML Definition\nDESCRIPTION: This YAML fragment illustrates how the defined internal listeners appear in the ports section of a Kubernetes Pod or Deployment manifest. Each port is described by its containerPort, name, and protocol (TCP), mirroring the listener configuration for the NiFi application. This structure is used by Kubernetes to map container ports for internal and external networking.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\n    ports:\n    - containerPort: 8443\n      name: https\n      protocol: TCP\n    - containerPort: 6007\n      name: cluster\n      protocol: TCP\n    - containerPort: 6342\n      name: load-balance\n      protocol: TCP\n    - containerPort: 10000\n      name: s2s\n      protocol: TCP\n    - containerPort: 9090\n      name: prometheus\n      protocol: TCP\n```\n\n----------------------------------------\n\nTITLE: Defining Parent NifiParameterContext Resource - YAML\nDESCRIPTION: This YAML snippet defines a NifiParameterContext custom resource named 'dataflow-lifecycle'. It specifies the target NiFi cluster ('nc' in namespace 'nifikop'), references a Kubernetes secret ('secret-params') for sensitive values, and includes inline defined parameters like 'test'. This resource represents a parameter context within the NiFi cluster managed by the operator.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/5_references/4_nifi_parameter_context.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      description: toto\n      sensistive: true\n```\n\n----------------------------------------\n\nTITLE: Configuring NifiCluster InitContainerImage (YAML)\nDESCRIPTION: This snippet shows an example YAML configuration for a `NifiCluster` with a custom `initContainerImage`. It specifies the image repository and tag. This configuration is needed when you have overridden the default setting. It shows the structure for specifying the image. It is important to update this to `bash` or an image containing bash to avoid issues during the upgrade. Expected input is a YAML file and it outputs a running NifiCluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/7_upgrade_guides/2_v0.14.1_to_v0.15.0.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  initContainerImage:\n    repository: busybox\n    tag: \"1.34.0\"\n```\n\n----------------------------------------\n\nTITLE: Patching CRDs for Conversion Webhook - YAML\nDESCRIPTION: This YAML snippet demonstrates how to patch the Custom Resource Definitions (CRDs) to configure a conversion webhook for migrating between `v1alpha1` and `v1` versions. The patch includes annotations for cert-manager and the webhook configuration in the `spec.conversion` section. The `${namespace}`, `${certificate_name}`, and `${webhook_service_name}` placeholders need to be replaced with the appropriate values.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/7_upgrade_guides/3_v0.16.0_to_v1.0.0.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n```\n\n----------------------------------------\n\nTITLE: Defining a NiFi User with Access Policies using NifiUser YAML\nDESCRIPTION: This YAML manifest defines a `NifiUser` Kubernetes resource named `aguitton`. It specifies the NiFi user identity as `alexandre.guitton@konpyutaika.com` via the `identity` field, overriding the resource name. It links the user to the `nc` NiFi cluster within the `nifikop` namespace using `clusterRef`. The configuration disables JKS keystore inclusion (`includeJKS: false`) and prevents automatic certificate creation (`createCert: false`). An access policy is defined under `accessPolicies` to grant read access (`action: read`) to the `/data` resource (`resource: /data`) specifically for process groups (`componentType: \"process-groups\"`).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/2_manage_users_and_accesses/1_users_management.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: aguitton\nspec:\n  # identity field is use to define the user identity on NiFi cluster side,\n  #\tit use full when the user's name doesn't suite with Kubernetes resource name.\n  identity: alexandre.guitton@konpyutaika.com\n  # Contains the reference to the NifiCluster with the one the registry client is linked.\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  # Whether or not the the operator also include a Java keystore format (JKS) with you secret\n  includeJKS: false\n  # Whether or not a certificate will be created for this user.\n  createCert: false\n  # defines the list of access policies that will be granted to the group.\n  accessPolicies:\n      # defines the kind of access policy, could be \"global\" or \"component\".\n    - type: component\n      # defines the kind of action that will be granted, could be \"read\" or \"write\"\n      action: read\n      # resource defines the kind of resource targeted by this access policies, please refer to the following page:\n      #\thttps://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#access-policies\n      resource: /data\n      # componentType is used if the type is \"component\", it's allow to define the kind of component on which is the\n      # access policy\n      componentType: \"process-groups\"\n      # componentId is used if the type is \"component\", it's allow to define the id of the component on which is the\n      # access policy\n      componentId: \"\"\n```\n\n----------------------------------------\n\nTITLE: Patching CRDs for Version Migration - YAML\nDESCRIPTION: This YAML snippet demonstrates how to patch the CRDs for Nifikop resources to enable version migration from v1alpha1 to v1. It sets up the conversion webhook strategy and specifies the service details for the webhook, along with the accepted conversion review versions.  The `namespace`, `certificate_name`, and `webhook_service_name` need to be configured based on the Helm release.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/7_upgrade_guides/3_v0.16.0_to_v1.0.0.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n```\n\n----------------------------------------\n\nTITLE: Creating Cert-Manager Let's Encrypt Staging Issuer (YAML)\nDESCRIPTION: This YAML snippet defines a cert-manager Issuer resource configured to obtain certificates from the Let's Encrypt staging environment using the ACME protocol. It requires an email address, a secret to store the account key, and uses an HTTP01 solver configured via ingress annotations.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: cert-manager.io/v1alpha2\nkind: Issuer\nmetadata:\n  name: letsencrypt-staging\nspec:\n  acme:\n    # You must replace this email address with your own.\n    # Let's Encrypt will use this to contact you about expiring\n    # certificates, and issues related to your account.\n    email: <your email address>\n    server: https://acme-staging-v02.api.letsencrypt.org/directory\n    privateKeySecretRef:\n      # Secret resource used to store the account's private key.\n      name: example-issuer-account-key\n    # Add a single challenge solver, HTTP01 using nginx\n    solvers:\n      - http01:\n          ingress:\n            ingressTemplate:\n              metadata:\n                annotations:\n                  \"external-dns.alpha.kubernetes.io/ttl\": \"5\"\n```\n\n----------------------------------------\n\nTITLE: ReadOnlyConfig YAML Configuration\nDESCRIPTION: Defines a ReadOnlyConfig object with configurations for various NiFi components. It shows how to set thread counts, configure logback, authorizers, NiFi properties, ZooKeeper properties, and bootstrap properties using ConfigMaps, Secrets, and direct overrides.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/5_references/1_nifi_cluster/2_read_only_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreadOnlyConfig:\n  # MaximumTimerDrivenThreadCount define the maximum number of threads for timer driven processors available to the system.\n  maximumTimerDrivenThreadCount: 30\n  # MaximumEventDrivenThreadCount define the maximum number of threads for event driven processors available to the system (@DEPRECATED. This has no effect from NiFiKOp v1.9.0 or later).\n  maximumEventDrivenThreadCount: 10\n  # Logback configuration that will be applied to the node\n  logbackConfig:\n    # logback.xml configuration that will replace the one produced based on template\n    replaceConfigMap:\n      # The key of the value,in data content, that we want use.\n      data: logback.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # logback.xml configuration that will replace the one produced based on template and overrideConfigMap\n    replaceSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: logback.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n  # Authorizer configuration that will be applied to the node\n  authorizerConfig:\n    # An authorizers.xml configuration template that will replace the default template seen in authorizers.go\n    replaceTemplateConfigMap:\n      # The key of the value, in data content, that we want use.\n      data: authorizers.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # An authorizers.xml configuration template that will replace the default template seen in authorizers.go and the replaceTemplateConfigMap\n    replaceTemplateSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: authorizers.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n  # NifiProperties configuration that will be applied to the node.\n  nifiProperties:\n    # Additionnal nifi.properties configuration that will override the one produced based on template and\n    # configuration\n    overrideConfigMap:\n      # The key of the value,in data content, that we want use.\n      data: nifi.properties\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop.\n    # Additionnal nifi.properties configuration that will override the one produced based\n    #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    overrideSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: nifi.properties\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # Additionnal nifi.properties configuration that will override the one produced based\n    #\ton template, configurations and overrideConfigMap\n    overrideConfigs: |\n      nifi.ui.banner.text=NiFiKop\n    # A comma separated list of allowed HTTP Host header values to consider when NiFi\n    # is running securely and will be receiving requests to a different host[:port] than it is bound to.\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#web-properties\n    #      webProxyHosts:\n    # Nifi security client auth\n    needClientAuth: false\n    # Indicates which of the configured authorizers in the authorizers.xml file to use\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#authorizer-configuration\n  #      authorizer:\n  # ZookeeperProperties configuration that will be applied to the node.\n  zookeeperProperties:\n    #      # Additionnal zookeeeper.properties configuration that will override the one produced based on template and\n    #      # configuration\n    #      overrideConfigMap:\n    #        # The key of the value,in data content, that we want use.\n    #        data: zookeeeper.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop.\n    #      # Additionnal zookeeeper.properties configuration that will override the one produced based\n    #      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    #      overrideSecretConfig:\n    #        # The key of the value,in data content, that we want use.\n    #        data: zookeeeper.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop\n    # Additionnal zookeeper.properties configuration that will override the one produced based\n    # on template and configurations.\n    overrideConfigs: |\n      initLimit=15\n      autopurge.purgeInterval=24\n      syncLimit=5\n      tickTime=2000\n      dataDir=./state/zookeeper\n      autopurge.snapRetainCount=30\n  # BootstrapProperties configuration that will be applied to the node.\n  bootstrapProperties:\n    #      # Additionnal bootstrap.conf configuration that will override the one produced based on template and\n    #      # configuration\n    #      overrideConfigMap:\n    #        # The key of the value,in data content, that we want use.\n    #        data: bootstrap.conf\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop.\n    #      # Additionnal bootstrap.conf configuration that will override the one produced based\n    #      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    #      overrideSecretConfig:\n    #        # The key of the value,in data content, that we want use.\n    #        data: bootstrap.conf\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop\n    # JVM memory settings\n    nifiJvmMemory: \"512m\"\n    # Additionnal bootstrap.conf configuration that will override the one produced based\n    # on template and configurations.\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#bootstrap_properties\n    overrideConfigs: |\n      # java.arg.4=-Djava.net.preferIPv4Stack=true\n```\n\n----------------------------------------\n\nTITLE: Running NiFiKop Locally in Bash\nDESCRIPTION: Command to run the NiFiKop operator locally using the Makefile, connecting to the Kubernetes cluster using default configurations.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/6_contributing/1_developer_guide.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmake run\n```\n\n----------------------------------------\n\nTITLE: Determining Zookeeper UID/GID for OpenShift Security Context\nDESCRIPTION: Extracts the supplemental group ID (UID/GID) assigned to the 'zookeeper' namespace in OpenShift to configure container and pod security context settings appropriately during Helm installation. This Bash command uses kubectl and sed to parse annotations and transform the output. It ensures that Zookeeper pods run under permitted user/group IDs according to OpenShift Security Context Constraints.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nzookeper_uid=$(kubectl get namespace zookeeper -o=jsonpath='{.metadata.annotations.openshift\\.io/sa\\.scc\\.supplemental-groups}' | sed 's/\\/10000$//' | tr -d '[:space:]')\n```\n\n----------------------------------------\n\nTITLE: Install Zookeeper with UID/GID - OpenShift Helm\nDESCRIPTION: This command installs Zookeeper on OpenShift using the Bitnami Helm chart, specifying the `runAsUser` and `fsGroup` parameters using the UID/GID obtained in the previous step.  This ensures that Zookeeper runs with the correct security context in the OpenShift environment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhelm install zookeeper oci://registry-1.docker.io/bitnamicharts/zookeeper \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set global.storageClass=standard \\\n    --set networkPolicy.enabled=true \\\n    --set replicaCount=3 \\\n    --set containerSecurityContext.runAsUser=$zookeper_uid \\\n    --set podSecurityContext.fsGroup=$zookeper_uid\n```\n\n----------------------------------------\n\nTITLE: Configuring identity mapping pattern in nifi.properties for OIDC\nDESCRIPTION: This shell snippet shows the configuration lines to add to the nifi.properties file to enable proper DN pattern matching for multiple identity providers in NiFi's security setup. It sets up regex patterns for DN mapping, ensuring that user identities are correctly recognized based on their distinguished names.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/3_tasks/2_security/2_authentication/1_oidc.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\nnifi.security.identity.mapping.pattern.dn=CN=([^,]*)(?:, (?:O|OU)=.*)?\\nnifi.security.identity.mapping.value.dn=$1\\nnifi.security.identity.mapping.transform.dn=NONE\n```\n\n----------------------------------------\n\nTITLE: Deploying NifiRegistryClient CRD in YAML\nDESCRIPTION: This YAML snippet defines a NifiRegistryClient resource that NiFiKop uses to manage NiFi dataflows via the NiFi Registry feature. It specifies metadata such as name and namespace, and references the target NiFi cluster with a clusterRef containing the cluster's name and namespace. The uri field points to the NiFi Registry REST endpoint, which NiFiKop uses to communicate and synchronize flows.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiRegistryClient\nmetadata:\n  name: registry-client-example\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Registry client managed by NiFiKop\"\n  uri: \"http://nifi.hostname.com:18080\"\n```\n\n----------------------------------------\n\nTITLE: Complete package.json for NiFiKop Migration (JSON)\nDESCRIPTION: This is the complete `package.json` configuration file for the NiFiKop CRD migration script. It defines the project's name, version, description, main entry point, scripts (including the start command), keywords, license, and dependencies (`@kubernetes/client-node` and `minimist`).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"nifikop_crd_migration\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Script to migrate from the old CRDs to the new CRDs.\",\n  \"main\": \"index.js\",\n  \"scripts\": {\n    \"start\": \"node --no-warnings index.js\",\n    \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\"\n  },\n  \"keywords\": [\n    \"K8S\",\n    \"NiFiKop\",\n    \"CRDs\"\n  ],\n  \"license\": \"ISC\",\n  \"dependencies\": {\n    \"@kubernetes/client-node\": \"^0.16.3\",\n    \"minimist\": \"^1.2.6\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Example NifiUserGroup Kubernetes Resource (YAML)\nDESCRIPTION: This YAML manifest demonstrates how to define a `NifiUserGroup` resource named 'group-test'. It specifies the target NiFi cluster ('nc' in 'nifikop' namespace) via `clusterRef`, lists associated users (`usersRef`), and defines access policies, granting read access to '/counters' globally.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/5_references/6_nifi_usergroup.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n    - name: nc-controller.nifikop.mgt.cluster.local\n  accessPolicies:\n    - type: global\n      action: read\n      resource: /counters\n```\n\n----------------------------------------\n\nTITLE: Configuring NifiDataflow Resource in YAML\nDESCRIPTION: Example of a NifiDataflow custom resource that deploys a flow from NiFi Registry. It references a registry client and parameter context, and specifies the sync mode and update strategy for managing the flow's lifecycle.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: demo\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Applying Updated NifiCluster Configuration (Shell)\nDESCRIPTION: This shell command uses kubectl to apply the updated NifiCluster YAML configuration to the Kubernetes cluster. Executing this command instructs the NiFiKop operator to reconcile the changes and initiate the scale-up process by creating the necessary resources for the new node.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\nkubectl -n nifi apply -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Deploying NifiConnection Example\nDESCRIPTION: This YAML snippet showcases the definition of a `NifiConnection` resource, which establishes a connection between two previously deployed `NifiDataflow` resources ('input' and 'output'). The `spec` field defines the source and destination dataflows by name and namespace.  It also includes `configuration` to set connection parameters such as backpressure thresholds, flow file expiration and bends to control the look of the connection in the UI.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/4_manage_connections/1_deploy_connection.md#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiConnection\nmetadata:\n  name: connection\n  namespace: nifikop\nspec:\n  source:\n    name: input\n    namespace: nifikop\n    subName: output\n    type: dataflow\n  destination:\n    name: output\n    namespace: nifikop\n    subName: input\n    type: dataflow\n  configuration:\n    backPressureDataSizeThreshold: 100 GB\n    backPressureObjectThreshold: 10000\n    flowFileExpiration: 1 hour\n    labelIndex: 0\n    bends:\n      - posX: 550\n        posY: 550\n      - posX: 550\n        posY: 440\n      - posX: 550\n        posY: 88\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Node with YAML and Resource Requirements\nDESCRIPTION: This YAML snippet configures a NiFi node and specifies its resource requirements.  It defines read-only configurations and node configurations, including CPU and memory limits and requests. The node configuration includes storage configurations with mount paths, metadata, and PVC specifications. These configurations are critical for resource management and persistent storage within a Kubernetes environment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/5_references/1_nifi_cluster/4_node.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n    - id: 2\n      # readOnlyConfig can be used to pass Nifi node config\n      # which has type read-only these config changes will trigger rolling upgrade\n      readOnlyConfig:\n        overrideConfigs: |\n          nifi.ui.banner.text=NiFiKop - Node 2\n      # node configuration\n      nodeConfig:\n        resourcesRequirements:\n          limits:\n            cpu: \"2\"\n            memory: 3Gi\n          requests:\n            cpu: \"1\"\n            memory: 1Gi\n        storageConfigs:\n          # Name of the storage config, used to name PV to reuse into sidecars for example.\n          - name: provenance-repository\n            # Path where the volume will be mount into the main nifi container inside the pod.\n            mountPath: \"/opt/nifi/provenance_repository\"\n            # Metadata to attach to the PVC that gets created\n            metadata:\n              labels:\n                my-label: my-value\n              annotations:\n                my-annotation: my-value\n            # Kubernetes PVC spec\n            # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n            pvcSpec:\n              accessModes:\n                - ReadWriteOnce\n              storageClassName: \"standard\"\n              resources:\n                requests:\n                  storage: 8Gi\n```\n\n----------------------------------------\n\nTITLE: Installing Zookeeper on OpenShift with UID/GID\nDESCRIPTION: This snippet installs the Bitnami Zookeeper chart on OpenShift, specifying the `runAsUser` and `fsGroup` parameters using the UID/GID obtained in the previous step. This ensures that the Zookeeper pods have the correct permissions to access storage.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nhelm install zookeeper oci://registry-1.docker.io/bitnamicharts/zookeeper \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set global.storageClass=standard \\\n    --set networkPolicy.enabled=true \\\n    --set replicaCount=3 \\\n    --set containerSecurityContext.runAsUser=$zookeper_uid \\\n    --set podSecurityContext.fsGroup=$zookeper_uid\n```\n\n----------------------------------------\n\nTITLE: Creating SSL Credentials with NifiUser Resource\nDESCRIPTION: Example of creating a NifiUser resource to generate client certificates for applications to securely connect to the NiFi cluster. The certificates are stored in a Kubernetes secret that can be mounted into pods.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/3_tasks/2_security/1_ssl.md#_snippet_3\n\nLANGUAGE: console\nCODE:\n```\ncat << EOF | kubectl apply -n nifi -f -\napiVersion:  nifi.konpyutaika.com/v1alpha1\nkind: NifiUser\nmetadata:\n  name: example-client\n  namespace: nifi\nspec:\n  clusterRef:\n    name: nifi\n  secretName: example-client-secret\nEOF\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi ClusterIP Service (YAML)\nDESCRIPTION: This YAML snippet illustrates how to define a ClusterIP service. The `spec` section defines the service type as `ClusterIP` and sets the port configuration for HTTPS (port 8443). This service is referenced as the destination service in the VirtualService. This is a standard Kubernetes service definition. This definition has to be part of the Kubernetes cluster deployment YAML file.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\nspec:  \n  externalServices:  \n    - name: \"nifi-cluster\"\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8443\n            internalListenerName: \"https\"\n```\n```\n\n----------------------------------------\n\nTITLE: Installing Zookeeper with Helm on Kubernetes - Bash\nDESCRIPTION: Installs a Zookeeper cluster using Bitnami's Helm chart with resource requests and limits configured for CPU and memory, replica count set to 3, network policy enabled, and specifying a storage class. The storageClass parameter must be replaced with a user-specific value. This snippet assumes Helm is installed and configured alongside Kubernetes cluster access.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhelm install zookeeper oci://registry-1.docker.io/bitnamicharts/zookeeper \\\n    --namespace=zookeeper \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set global.storageClass=standard \\\n    --set networkPolicy.enabled=true \\\n    --set replicaCount=3 \\\n    --create-namespace\n```\n\n----------------------------------------\n\nTITLE: Running NiFi Encrypt-Config Tool to Update Encryption Algorithm - Shell\nDESCRIPTION: This shell snippet demonstrates the usage of the NiFi Encrypt-Config Tool to recalculate sensitive property values after changing the encryption algorithm to NIFI_PBKDF2_AES_GCM_256. It updates the flow configuration files flow.xml.gz and flow.json.gz using the sensitive properties key. It requires the NiFi toolkit installed and access to flow files and nifi.properties. The output updates encrypted sensitive values to the new algorithm format.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/7_upgrade_guides/4_v1.3.1_to_v1.4.0.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nencrypt-config.sh -n nifi.properties -f flow.xml.gz -x -s PROPERTIES_KEY -A NIFI_PBKDF2_AES_GCM_256\nencrypt-config.sh -n nifi.properties -f flow.json.gz -x -s PROPERTIES_KEY -A NIFI_PBKDF2_AES_GCM_256\n```\n\n----------------------------------------\n\nTITLE: Manually Deploying CRDs for NiFiKop\nDESCRIPTION: Commands to manually apply the Custom Resource Definitions needed by NiFiKop. This is required when using the --skip-crds flag with Helm.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifinodegroupautoscalers.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring Conversion Webhook for Nifikop CRDs in YAML\nDESCRIPTION: Provides a YAML snippet demonstrating how to configure annotations and conversion settings for Nifikop CRDs to enable the conversion webhook. This handles resource conversions between API versions (e.g., `v1alpha1` to `v1`) and requires cert-manager for CA injection via annotations. Placeholders like `${namespace}`, `${certificate_name}`, and `${webhook_service_name}` must be replaced.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n```\n\n----------------------------------------\n\nTITLE: Installing Zookeeper with Helm - Bash\nDESCRIPTION: Installs a Zookeeper cluster using the Bitnami Helm chart, specifying resource requests, limits, storage class, network policy, and replica count. Requires Helm CLI, cluster-admin permissions, and a Kubernetes environment. Users must replace the storageClass parameter value to match their provisioned StorageClass.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nhelm install zookeeper bitnami/zookeeper \\\n    --namespace=zookeeper \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set global.storageClass=standard \\\n    --set networkPolicy.enabled=true \\\n    --set replicaCount=3 \\\n    --create-namespace\n\n```\n\n----------------------------------------\n\nTITLE: Configuring CRD Conversion Webhook in YAML\nDESCRIPTION: Shows the YAML configuration snippet required within a Nifikop Custom Resource Definition (CRD) to enable the conversion webhook, which handles resource conversion between API versions (e.g., `v1alpha1` to `v1`). Requires `cert-manager` for CA injection and replacing placeholders (`${namespace}`, `${certificate_name}`, `${webhook_service_name}`) with values derived from the Helm release.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables - Shell\nDESCRIPTION: This snippet sets environment variables required for subsequent commands. It defines the GCP project ID, the zone where the cluster will be created, and the desired cluster name. These variables are used throughout the setup process, allowing for easy modification of key parameters without changing the core command structures. These variables should be replaced with the user's specific values before execution.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/2_setup/2_platform_setup/1_gke.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nexport GCP_PROJECT=<project_id>\nexport GCP_ZONE=<zone>\nexport CLUSTER_NAME=<cluster-name>\n```\n\n----------------------------------------\n\nTITLE: Configuring ServiceMonitor for NiFi Metrics\nDESCRIPTION: Creates a ServiceMonitor resource for Prometheus that targets the NiFi cluster services. Includes relabeling configurations to capture pod IP and NiFi node ID as metric labels.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: cluster\n  namespace: monitoring-system\n  labels:\n    app: nifi-cluster\n    nifi_cr: cluster\nspec:\n  selector:\n    matchLabels:\n      app: nifi\n      nifi_cr: cluster\n  namespaceSelector:\n    matchNames:\n      - clusters\n  endpoints:\n    - interval: 10s\n      port: prometheus\n      path: /metrics\n      honorLabels: true\n      relabelings:\n        - sourceLabels: [__meta_kubernetes_pod_ip]\n          separator: ;\n          regex: (.*)\n          targetLabel: pod_ip\n          replacement: $1\n          action: replace\n        - sourceLabels: [__meta_kubernetes_pod_label_nodeId]\n          separator: ;\n          regex: (.*)\n          targetLabel: nodeId\n          replacement: $1\n          action: replace\n        - sourceLabels: [__meta_kubernetes_pod_label_nifi_cr]\n          separator: ;\n          regex: (.*)\n          targetLabel: nifi_cr\n          replacement: $1\n          action: replace\n```\n\n----------------------------------------\n\nTITLE: Defining ClusterIP External Service Listener in YAML\nDESCRIPTION: This YAML snippet illustrates configuring an external service of type ClusterIP to define Nifi listener ports and protocols. It includes specifying multiple port configurations with listener names and optionally protocols, alongside metadata annotations and labels to augment Kubernetes service definitions. This setup is used to expose internal listeners inside a Kubernetes cluster via a stable ClusterIP address.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/5_references/1_nifi_cluster/7_external_service_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  externalServices:\n    - name: \"clusterip\"\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8080\n            internalListenerName: \"http\"\n          - port: 7182\n            internalListenerName: \"my-custom-listener\"\n            protocol: TCP\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n\n----------------------------------------\n\nTITLE: Getting LoadBalancer Service IP for Secured NiFi Cluster Using kubectl Console Command\nDESCRIPTION: This command lists Kubernetes services in the nifi namespace, revealing the external IP address assigned to the load balancer service for the Secured NiFi cluster. This external IP must be mapped to a DNS A record for accessing the NiFi web UI securely via HTTPS.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/blog/2020-05-20-secured_nifi_cluster_on_gcp.md#_snippet_11\n\nLANGUAGE: console\nCODE:\n```\nkubectl -n nifi get svc\nNAME                          TYPE           CLUSTER-IP      EXTERNAL-IP     PORT(S)                                         AGE\nsecurednificluster            LoadBalancer   10.15.248.159   34.78.140.135   8443:30248/TCP,6007:30517/TCP,10000:31985/TCP   27m\nsecurednificluster-headless   ClusterIP      None            <none>          8443/TCP,6007/TCP,10000/TCP                     27m\n```\n\n----------------------------------------\n\nTITLE: Installing Nifikop Helm Chart (Dry Run) (Bash)\nDESCRIPTION: Demonstrates how to perform a dry run installation of the Nifikop Helm chart using `helm install --dry-run`. This command simulates the installation, outputs the generated Kubernetes manifests, and sets parameters like `logLevel` and `namespaces` without actually deploying anything to the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop \\\n    --dry-run \\\n    --set logLevel=Debug \\\n    --set namespaces={\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Applying CRDs Manually\nDESCRIPTION: This command deploys the CRDs for NiFiKop manually.  This is needed when skipping the automatic CRD deployment via Helm. It applies the CRDs from a remote repository.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/2_setup/3_install/1_customizable_install_with_helm.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\n```\n\n----------------------------------------\n\nTITLE: Listing Managed Groups (kubectl)\nDESCRIPTION: This console command demonstrates how to retrieve a list of managed groups by querying the Kubernetes API for `NifiUserGroup` resources within the `nifikop` namespace.  The output shows the names and ages of the managed groups created by the operator, including managed-admins, managed-nodes, and managed-readers. It relies on the kubectl CLI and assumes the NiFi operator is running and the relevant CRDs are installed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/2_manage_users_and_accesses/3_managed_groups.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nkubectl get -n nifikop nifiusergroups.nifi.konpyutaika.com \nNAME              AGE\nmanaged-admins    6d7h\nmanaged-nodes     6d7h\nmanaged-readers   6d7h\n```\n\n----------------------------------------\n\nTITLE: Viewing Kubernetes Pod Ports Definition\nDESCRIPTION: This YAML snippet illustrates how the internal listeners defined in the NifiCluster spec are translated into the `ports` definition within the corresponding Kubernetes pod template. It shows the container ports, names, and TCP protocol for each configured internal listener, demonstrating how the NiFi configuration translates into pod-level network configuration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n    ports:\n    - containerPort: 8443\n      name: https\n      protocol: TCP\n    - containerPort: 6007\n      name: cluster\n      protocol: TCP\n    - containerPort: 6342\n      name: load-balance\n      protocol: TCP\n    - containerPort: 10000\n      name: s2s\n      protocol: TCP\n    - containerPort: 9090\n      name: prometheus\n      protocol: TCP\n```\n\n----------------------------------------\n\nTITLE: Deploying an Input NifiDataflow YAML\nDESCRIPTION: This YAML configuration defines an input NifiDataflow within a NiFiKop managed NiFi cluster. It specifies the cluster reference, bucket ID, flow ID, flow version, registry client reference, and synchronization mode. The `input` dataflow must have an `output port`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/4_manage_connections/1_deploy_connection.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiDataflow\nmetadata:\n  name: input\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  bucketId: deedb9f6-65a4-44e9-a1c9-722008fcd0ba\n  flowId: ab95431d-980d-41bd-904a-fac4bd864ba6\n  flowVersion: 1\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  skipInvalidComponent: true\n  skipInvalidControllerService: true\n  syncMode: always\n  updateStrategy: drain\n  flowPosition:\n    posX: 0\n    posY: 0\n```\n\n----------------------------------------\n\nTITLE: Verifying New Node Resources (Console)\nDESCRIPTION: This command uses `kubectl get` to list the Kubernetes resources (pods, configmaps, persistent volume claims) associated with the newly added NiFi node (nodeId=25). It helps verify that the scale-up operation successfully created the necessary components.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl get pods,configmap,pvc -l nodeId=25\nNAME                          READY   STATUS    RESTARTS   AGE\npod/simplenifi-25-nodem5jh4   1/1     Running   0          11m\n\nNAME                             DATA   AGE\nconfigmap/simplenifi-config-25   7      11m\n\nNAME                                               STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\npersistentvolumeclaim/simplenifi-25-storagehwn24   Bound    pvc-7da86076-728e-11ea-846d-42010a8400f2   10Gi       RWO            standard       11m\n```\n\n----------------------------------------\n\nTITLE: Defining Prometheus RBAC Kubernetes/YAML\nDESCRIPTION: This YAML manifest defines the necessary Kubernetes RBAC resources for the Prometheus instance. It includes a ServiceAccount for the Prometheus pods, a ClusterRole granting permissions to monitor Kubernetes resources (nodes, services, endpoints, pods, configmaps, ingresses, metrics endpoint), and a ClusterRoleBinding linking the ServiceAccount to the ClusterRole.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: prometheus\n  namespace: monitoring-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: prometheus\nrules:\n- apiGroups: [\"\"]\n  resources:\n  - nodes\n  - nodes/metrics\n  - services\n  - endpoints\n  - pods\n  verbs: [\"get\", \"list\", \"watch\"]\n- apiGroups: [\"\"]\n  resources:\n  - configmaps\n  verbs: [\"get\"]\n- apiGroups:\n  - networking.k8s.io\n  resources:\n  - ingresses\n  verbs: [\"get\", \"list\", \"watch\"]\n- nonResourceURLs: [\"/metrics\"]\n  verbs: [\"get\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: prometheus\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: prometheus\nsubjects:\n- kind: ServiceAccount\n  name: prometheus\n  namespace: monitoring-system\n```\n\n----------------------------------------\n\nTITLE: Example package.json for NiFiKop Migration Project (JSON)\nDESCRIPTION: Provides a complete example of the `package.json` file for the Node.js migration project. It includes project metadata (name, version, description), the main script file (`index.js`), the defined `start` script, keywords, license information, and lists the required dependencies (`@kubernetes/client-node` and `minimist`).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"nifikop_crd_migration\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Script to migrate from the old CRDs to the new CRDs.\",\n  \"main\": \"index.js\",\n  \"scripts\": {\n    \"start\": \"node --no-warnings index.js\",\n    \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\"\n  },\n  \"keywords\": [\n    \"K8S\",\n    \"NiFiKop\",\n    \"CRDs\"\n  ],\n  \"license\": \"ISC\",\n  \"dependencies\": {\n    \"@kubernetes/client-node\": \"^0.16.3\",\n    \"minimist\": \"^1.2.6\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Bitnami Helm Repository for Zookeeper Installation\nDESCRIPTION: Command to add the Bitnami Helm repository, which contains the Zookeeper chart needed as a prerequisite for NiFi deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhelm repo add bitnami https://charts.bitnami.com/bitnami\n```\n\n----------------------------------------\n\nTITLE: Deploying CRDs - Bash\nDESCRIPTION: Deploys the Custom Resource Definitions (CRDs) for NiFiKop. This command utilizes `kubectl apply` to install the necessary CRDs. The CRDs are defined in YAML files located within the `config/crd/bases/` directory. These CRDs are required for the operator to function correctly. Requires `kubectl` and access to the Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/6_contributing/1_developer_guide.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining NodeConfigGroups for NiFiKop in YAML\nDESCRIPTION: Defines multiple NodeConfigGroups within a NiFiKop Custom Resource using YAML. This example provides reusable node configurations named 'default_group' and 'high_mem_group', each specifying resource requirements, provenance storage, user ID, and service account. Dependencies: NiFiKop Operator installed; this must be placed under 'spec.nodeConfigGroups' of a NiFiCluster CR. Parameters specify limits and requests for CPU and memory, provenance storage, and user behavior. Outputs reusable groups referenced from node declarations. All values are subject to limits of the underlying Kubernetes platform.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  nodeConfigGroups:\n    default_group:\n      provenanceStorage: \"10 GB\"\n      runAsUser: 1000\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 3Gi\n    high_mem_group:\n      provenanceStorage: \"10 GB\"\n      runAsUser: 1000\n      serviceAccountName: \"default\"\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 30Gi\n        requests:\n          cpu: \"1\"\n          memory: 30Gi\n```\n\n----------------------------------------\n\nTITLE: Defining Persistent Storage in YAML\nDESCRIPTION: This YAML code snippet defines persistent storage configurations for a NiFi cluster. It uses `storageConfigs` to specify the `mountPath`, `name`, and `pvcSpec` (PersistentVolumeClaim specification) for different data directories.  The `pvcSpec` defines `accessModes`, `resources`, and `storageClassName`. Dependencies: Kubernetes, NiFiKop. Input: YAML. Output: Persistent storage volumes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n...\nstorageConfigs:\n  - mountPath: /opt/nifi/nifi-current/logs\n    name: logs\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/data\n    name: data\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 50Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/extensions\n    name: extensions-repository\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 5Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/flowfile_repository\n    name: flowfile-repository\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/nifi-current/conf\n    name: conf\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 5Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/content_repository\n    name: content-repository\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 500Gi\n      storageClassName: ssd-wait\n  - mountPath: /opt/nifi/provenance_repository\n    name: provenance-repository\n    pvcSpec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          storage: 500Gi\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiParameterContext with basic and sensitive parameters - YAML\nDESCRIPTION: This YAML snippet demonstrates how to define a basic `NifiParameterContext` custom resource. It specifies a description, references a target NiFi cluster by name and namespace, includes a reference to a Kubernetes Secret for sensitive parameters, and defines a list of non-sensitive parameters with names, values, and descriptions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/5_references/4_nifi_parameter_context.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      description: toto\n      sensistive: true\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiUserGroup via YAML Configuration\nDESCRIPTION: This YAML snippet defines the schema for creating a NiFi UserGroup resource in Kubernetes, including API version, kind, metadata, specifications such as cluster reference, user references, and access policies. It serves as a template for deploying user groups with specified access controls and related user references.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/5_references/6_nifi_usergroup.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n    - name: nc-controller.nifikop.mgt.cluster.local\n  accessPolicies:\n    - type: global\n      action: read\n      resource: /counters\n```\n\n----------------------------------------\n\nTITLE: Install Zookeeper using Helm (Bash)\nDESCRIPTION: This command installs the Zookeeper chart from the Bitnami repository into the `zookeeper` namespace. It sets resource requests and limits for memory and CPU, specifies the storage class to use, enables network policies, and sets the replica count to 3. It also creates the namespace if it does not exist.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nhelm install zookeeper bitnami/zookeeper \\\n    --namespace=zookeeper \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set global.storageClass=standard \\\n    --set networkPolicy.enabled=true \\\n    --set replicaCount=3 \\\n    --create-namespace\n```\n\n----------------------------------------\n\nTITLE: Creating Custom StorageClass in YAML\nDESCRIPTION: This snippet defines a custom Kubernetes StorageClass named `exampleStorageclass`. It configures the storage class to use `pd-standard` as the storage type, leverages the Google Compute Engine Persistent Disk provisioner, sets the reclaim policy to `Delete`, and enables `WaitForFirstConsumer` volume binding mode.  This storage class needs to be created before the NiFiCluster is deployed.  The `exampleStorageclass` name is used later when creating a NiFiCluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: exampleStorageclass\nparameters:\n  type: pd-standard\nprovisioner: kubernetes.io/gce-pd\nreclaimPolicy: Delete\nvolumeBindingMode: WaitForFirstConsumer\n```\n\n----------------------------------------\n\nTITLE: Integrating NifiCluster with Cert-Manager Issuer\nDESCRIPTION: Configures the NifiCluster custom resource to use an existing `cert-manager` Issuer for obtaining SSL certificates instead of generating self-signed ones. It enables cluster and site-to-site security, specifies listener settings including `clusterDomain` and `useExternalDNS`, and references the `letsencrypt-staging` Issuer created previously via `sslSecrets.issuerRef`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  clusterSecure: true\n  siteToSiteSecure: true\n  ...\n  listenersConfig:\n    clusterDomain: <DNS zone name>\n    useExternalDNS: true\n    ...\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n      issuerRef:\n        name: letsencrypt-staging\n        kind: Issuer\n```\n\n----------------------------------------\n\nTITLE: Listing Helm Releases (Bash)\nDESCRIPTION: Provides commands to list Helm releases using `helm list`. The basic command lists currently deployed releases. The `--deleted` flag lists only deleted releases, and the `--all` flag lists all releases regardless of status (deployed, deleted, failed).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nhelm list\n```\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --deleted\n```\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --all\n```\n\n----------------------------------------\n\nTITLE: Adding KEDA Helm Repository\nDESCRIPTION: Adds the official KEDA Helm chart repository ('kedacore') to the local Helm configuration using the `helm repo add` command. This step is necessary to make the KEDA chart available for installation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nhelm repo add kedacore https://kedacore.github.io/charts\n```\n\n----------------------------------------\n\nTITLE: Detailed NodeConfig Type Definition for Kubernetes Deployment\nDESCRIPTION: This section defines the TypeScript interface for NodeConfig, outlining all available properties such as provenance storage, user IDs, security context, affinity, resource requirements, and associated metadata. It serves as a blueprint for setting up node parameters within a Kubernetes operator for Nifi, facilitating validation and automated configuration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/5_references/1_nifi_cluster/3_node_config.md#_snippet_1\n\nLANGUAGE: TypeScript\nCODE:\n```\ninterface NodeConfig {\n  provenanceStorage?: string;\n  runAsUser?: number;\n  fsGroup?: number;\n  isNode?: boolean;\n  image?: string;\n  imagePullPolicy?: string;\n  nodeAffinity?: string | null;\n  seccompProfile?: SeccompProfile;\n  securityContext?: SecurityContext;\n  storageConfigs?: StorageConfig[];\n  externalVolumeConfigs?: ExternalVolumeConfig[];\n  serviceAccountName?: string;\n  resourcesRequirements?: ResourceRequirements;\n  imagePullSecrets?: LocalObjectReference[];\n  nodeSelector?: { [key: string]: string };\n  tolerations?: Toleration[];\n  podMetadata?: Metadata;\n  hostAliases?: HostAlias[];\n  priorityClassName?: string;\n}\n\ninterface StorageConfig {\n  name: string;\n  mountPath: string;\n  reclaimPolicy?: \"Delete\" | \"Retain\";\n  metadata?: Metadata;\n  pvcSpec: PersistentVolumeClaimSpec;\n}\n\ninterface ExternalVolumeConfig {\n  volumeMount: VolumeMount;\n  volumeSource: VolumeSource;\n}\n\n// Additional interfaces like SeccompProfile, SecurityContext, Metadata, HostAlias, Toleration, PersistentVolumeClaimSpec, VolumeMount, and VolumeSource are assumed to be imported or defined elsewhere.\n```\n\n----------------------------------------\n\nTITLE: Updating Helm Repositories - Console\nDESCRIPTION: Updates local Helm chart information to ensure latest versions are available before installation. Run this after adding new repositories to avoid installing outdated charts.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_7\n\nLANGUAGE: console\nCODE:\n```\nhelm repo update\n```\n\n----------------------------------------\n\nTITLE: Updating Helm Repositories\nDESCRIPTION: Updates the local Helm chart repository cache using `helm repo update`. This ensures that Helm has the latest information about available charts and versions, including the recently added Prometheus repository.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_7\n\nLANGUAGE: console\nCODE:\n```\nhelm repo update\n```\n\n----------------------------------------\n\nTITLE: Creating a GKE Cluster with gcloud CLI\nDESCRIPTION: This snippet creates a new GKE cluster with the latest version, using a specified machine type and node count within a defined zone and project. It configures the cluster for NiFiKop deployment by defining key parameters such as cluster version, machine type, and node count.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/2_setup/2_platform_setup/1_gke.md#_snippet_1\n\nLANGUAGE: Shell script\nCODE:\n```\ngcloud container clusters create $CLUSTER_NAME \\\n  --cluster-version latest \\\n  --machine-type=n1-standard-1 \\\n  --num-nodes 4 \\\n  --zone $GCP_ZONE \\\n  --project $GCP_PROJECT\n```\n\n----------------------------------------\n\nTITLE: Configuring NPM Start Script (json)\nDESCRIPTION: Provides a snippet for adding a custom 'start' command in 'package.json'. This configuration is used to execute the migration script entry point 'index.js' with suppressed node warnings. It requires 'package.json' to exist and expects the migration script file to be named 'index.js'.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"start\": \"node --no-warnings index.js\"\n```\n\n----------------------------------------\n\nTITLE: Configuring ReadOnlyConfig in YAML for NiFiKop\nDESCRIPTION: This YAML configuration demonstrates how to set up various read-only parameters for a NiFi cluster. It includes thread count settings, logback configuration, authorizer settings, and property overrides for NiFi, ZooKeeper, and Bootstrap components.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/5_references/1_nifi_cluster/2_read_only_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreadOnlyConfig:\n  # MaximumTimerDrivenThreadCount define the maximum number of threads for timer driven processors available to the system.\n  maximumTimerDrivenThreadCount: 30\n  # MaximumEventDrivenThreadCount define the maximum number of threads for event driven processors available to the system.\n  maximumEventDrivenThreadCount: 10\n  # Logback configuration that will be applied to the node\n  logbackConfig:\n    # logback.xml configuration that will replace the one produced based on template\n    replaceConfigMap:\n      # The key of the value,in data content, that we want use.\n      data: logback.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # logback.xml configuration that will replace the one produced based on template and overrideConfigMap\n    replaceSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: logback.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n  # Authorizer configuration that will be applied to the node\n  authorizerConfig:\n    # An authorizers.xml configuration template that will replace the default template seen in authorizers.go\n    replaceTemplateConfigMap:\n      # The key of the value, in data content, that we want use.\n      data: authorizers.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # An authorizers.xml configuration template that will replace the default template seen in authorizers.go and the replaceTemplateConfigMap\n    replaceTemplateSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: authorizers.xml\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n  # NifiProperties configuration that will be applied to the node.\n  nifiProperties:\n    # Additionnal nifi.properties configuration that will override the one produced based on template and\n    # configuration\n    overrideConfigMap:\n      # The key of the value,in data content, that we want use.\n      data: nifi.properties\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop.\n    # Additionnal nifi.properties configuration that will override the one produced based\n    #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    overrideSecretConfig:\n      # The key of the value,in data content, that we want use.\n      data: nifi.properties\n      # Name of the configmap that we want to refer.\n      name: raw\n      # Namespace where is located the secret that we want to refer.\n      namespace: nifikop\n    # Additionnal nifi.properties configuration that will override the one produced based\n    #\ton template, configurations and overrideConfigMap\n    overrideConfigs: |\n      nifi.ui.banner.text=NiFiKop\n    # A comma separated list of allowed HTTP Host header values to consider when NiFi\n    # is running securely and will be receiving requests to a different host[:port] than it is bound to.\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#web-properties\n    #      webProxyHosts:\n    # Nifi security client auth\n    needClientAuth: false\n    # Indicates which of the configured authorizers in the authorizers.xml file to use\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#authorizer-configuration\n  #      authorizer:\n  # ZookeeperProperties configuration that will be applied to the node.\n  zookeeperProperties:\n    #      # Additionnal zookeeeper.properties configuration that will override the one produced based on template and\n    #      # configuration\n    #      overrideConfigMap:\n    #        # The key of the value,in data content, that we want use.\n    #        data: zookeeeper.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop.\n    #      # Additionnal zookeeeper.properties configuration that will override the one produced based\n    #      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    #      overrideSecretConfig:\n    #        # The key of the value,in data content, that we want use.\n    #        data: zookeeeper.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop\n    # Additionnal zookeeper.properties configuration that will override the one produced based\n    # on template and configurations.\n    overrideConfigs: |\n      initLimit=15\n      autopurge.purgeInterval=24\n      syncLimit=5\n      tickTime=2000\n      dataDir=./state/zookeeper\n      autopurge.snapRetainCount=30\n  # BootstrapProperties configuration that will be applied to the node.\n  bootstrapProperties:\n    #      # Additionnal bootstrap.properties configuration that will override the one produced based on template and\n    #      # configuration\n    #      overrideConfigMap:\n    #        # The key of the value,in data content, that we want use.\n    #        data: bootstrap.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop.\n    #      # Additionnal bootstrap.properties configuration that will override the one produced based\n    #      #\ton template, configurations, overrideConfigMap and overrideConfigs.\n    #      overrideSecretConfig:\n    #        # The key of the value,in data content, that we want use.\n    #        data: bootstrap.properties\n    #        # Name of the configmap that we want to refer.\n    #        name: raw\n    #        # Namespace where is located the secret that we want to refer.\n    #        namespace: nifikop\n    # JVM memory settings\n    nifiJvmMemory: \"512m\"\n    # Additionnal bootstrap.properties configuration that will override the one produced based\n    # on template and configurations.\n    # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#bootstrap_properties\n    overrideConfigs: |\n      # java.arg.4=-Djava.net.preferIPv4Stack=true\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFi Cluster on OpenShift\nDESCRIPTION: This bash snippet outlines the steps for deploying a NiFi cluster on OpenShift, by retrieving and modifying user id, the command then uses `kubectl create` to deploy the NiFi cluster, utilizing an `openshift.yaml` configuration file located in `config/samples`.  It assumes zookeeper and the necessary OpenShift permissions have been set up.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create -n nifi -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Accessing Prometheus via Port Forwarding\nDESCRIPTION: Command to create port forwarding to access the Prometheus web interface for querying NiFi metrics.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_12\n\nLANGUAGE: console\nCODE:\n```\nkubectl port-forward -n monitoring-system service/prometheus-operated 9090:9090\n```\n\n----------------------------------------\n\nTITLE: Deploy NiFi Cluster - Kubernetes on OpenShift\nDESCRIPTION: This command deploys a NiFi cluster in OpenShift using the updated `config/samples/openshift.yaml` file, which now contains the correct UID/GID for the OpenShift environment. The cluster is created in the `nifi` namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create -n nifi -f config/samples/openshift.yaml\n```\n\n----------------------------------------\n\nTITLE: Schema Definition for NifiUserGroup in YAML\nDESCRIPTION: Defines the 'NifiUserGroup' resource schema including metadata, specification, and status fields along with associated subfields such as references to clusters, users, and access policies. The schema is used for validating and enforcing resource structure within a Kubernetes operator managing NiFi user groups.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/5_references/6_nifi_usergroup.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n    - name: nc-controller.nifikop.mgt.cluster.local\n  accessPolicies:\n    - type: global\n      action: read\n      resource: /counters\n```\n\n----------------------------------------\n\nTITLE: Listing NifiUserGroups using kubectl (Console)\nDESCRIPTION: Command to list all `NifiUserGroup` custom resources within the 'nifikop' namespace using `kubectl`. This allows verification of the managed groups ('managed-admins', 'managed-nodes', 'managed-readers') created and managed by the Nifikop operator based on the `NifiCluster` specification and node identities.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/2_manage_users_and_accesses/3_managed_groups.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nkubectl get -n nifikop nifiusergroups.nifi.konpyutaika.com \nNAME              AGE\nmanaged-admins    6d7h\nmanaged-nodes     6d7h\nmanaged-readers   6d7h\n```\n\n----------------------------------------\n\nTITLE: Configuring NifiCluster InitContainerImage in YAML\nDESCRIPTION: This YAML snippet demonstrates the configuration of the `NifiCluster.Spec.InitContainerImage`.  It specifies the image repository and tag for the init container.  The example shows the configuration for `busybox` and instructs users to change the image if they have overridden the default.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/7_upgrade_guides/2_v0.14.1_to_v0.15.0.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  initContainerImage:\n    repository: busybox\n    tag: \"1.34.0\"\n```\n```\n\n----------------------------------------\n\nTITLE: Inspecting NiFiKop Cluster Node Status - kubectl / Console\nDESCRIPTION: This console command sequence uses kubectl describe to view the status and action steps for nodes in the NiFiKop cluster resource. It allows tracking of in-progress decommissioning actions, showing NodeState and GracefulDownscaleRequired indicators. Prerequisites: kubectl installed and configured for the correct namespace. Inputs: cluster name (simplenifi). Outputs: Detailed status section with action and error messages for debugging or progress validation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nkubectl describe nificluster simplenifi\n\n...\nStatus:\n  Nodes State:\n    ...\n    2:\n      Configuration State:  ConfigInSync\n      Graceful Action State:\n        Action State:   GracefulDownscaleRequired\n        Error Message:\n    ...\n...\n```\n\n----------------------------------------\n\nTITLE: Setting Custom Authorizer Property in NiFi Configuration (Shell)\nDESCRIPTION: This shell command shows how to specify which authorizer implementation NiFi should use by setting the nifi.security.user.authorizer property. It is necessary to match this value with the identifier defined in the authorizers.xml file (for example, 'custom-database-authorizer' for a custom database-backed authorizer). This command is commonly used in a properties file or with a configuration-management tool. The output is to set the chosen authorizer for user access control within NiFi.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/6_users_authorization/1_custom_user_authorizer.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nnifi.security.user.authorizer=custom-database-authorizer\n\n```\n\n----------------------------------------\n\nTITLE: Cloning NiFiKop Repository Using Git in Bash\nDESCRIPTION: This snippet demonstrates how to clone the NiFiKop repository from Github and navigate into the project directory using basic shell commands. No dependencies besides a working git installation are required. The only parameter is the repository URL, and the output will be a new local directory named 'nifikop'.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/6_contributing/1_developer_guide.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/konpyutaika/nifikop.git\ncd nifikop\n```\n\n----------------------------------------\n\nTITLE: NodeState Structure in NiFiKop\nDESCRIPTION: The main structure that holds information about a NiFi node's state including action status, configuration, cluster membership status, and readiness.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/5_references/1_nifi_cluster/5_node_state.md#_snippet_0\n\nLANGUAGE: go\nCODE:\n```\ntype NodeState struct {\n\tgracefulActionState GracefulActionState\n\tconfigurationState ConfigurationState\n\tinitClusterNode InitClusterNode\n\tpodIsReady bool\n\tcreationTime v1.Time\n}\n```\n\n----------------------------------------\n\nTITLE: Verifying New Node Resources (Console)\nDESCRIPTION: Displays the Kubernetes resources (pods, configmaps, PVCs) associated with a specific node ID (25) using `kubectl get`. This command verifies that the NiFiKop operator has successfully created the necessary Kubernetes components for the added node after a scale-up operation is applied. Requires `kubectl` access to the cluster and assumes resources are labeled with the node ID.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl get pods,configmap,pvc -l nodeId=25\nNAME                          READY   STATUS    RESTARTS   AGE\npod/simplenifi-25-nodem5jh4   1/1     Running   0          11m\n\nNAME                             DATA   AGE\nconfigmap/simplenifi-config-25   7      11m\n\nNAME                                               STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\npersistentvolumeclaim/simplenifi-25-storagehwn24   Bound    pvc-7da86076-728e-11ea-846d-42010a8400f2   10Gi       RWO            standard       11m\n```\n\n----------------------------------------\n\nTITLE: Verifying Zookeeper Cluster Pods Status Using kubectl Console Command\nDESCRIPTION: This kubectl command lists running pods in the zookeeper namespace to verify that Zookeeper instances have been correctly deployed and are operational. The output typically shows three pod instances in the Running state supporting NiFi cluster state management.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/blog/2020-05-20-secured_nifi_cluster_on_gcp.md#_snippet_6\n\nLANGUAGE: console\nCODE:\n```\nkubectl -n zookeeper get pods\nNAME          READY   STATUS    RESTARTS   AGE\nzookeeper-0   1/1     Running   0          74m\nzookeeper-1   1/1     Running   0          74m\nzookeeper-2   1/1     Running   0          74m\n```\n\n----------------------------------------\n\nTITLE: Forwarding Prometheus Service Port Locally for Access - console\nDESCRIPTION: This console command forwards the \"prometheus-operated\" service port 9090 from the monitoring-system namespace to the local machine. It enables local access to the Prometheus web UI via http://localhost:9090 for querying NiFi cluster metrics without exposing the service externally. This step is necessary for debugging and verifying Prometheus metric collection.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_11\n\nLANGUAGE: console\nCODE:\n```\nkubectl port-forward -n monitoring-system service/prometheus-operated 9090:9090\n```\n\n----------------------------------------\n\nTITLE: Defining Istio Gateway for HTTP\nDESCRIPTION: This YAML snippet defines an Istio Gateway that intercepts HTTP requests on port 80 for a specific domain. It selects the `ingressgateway` and forwards traffic to the specified hosts.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: nifi-gateway\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - nifi.my-domain.com\n```\n\n----------------------------------------\n\nTITLE: Installing Zookeeper Cluster Using Bitnami Helm Chart in Bash\nDESCRIPTION: Deploys a Zookeeper cluster in the 'zookeeper' namespace with specific CPU and memory resource requests and limits, using the Bitnami Zookeeper Helm chart. The snippet sets replica count to 3, enables network policies, and specifies the storage class. Users must replace the storageClass parameter to match their environment. Requires existing Helm installation, Kubernetes cluster access, and network support for policies.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nhelm install zookeeper bitnami/zookeeper \\\n    --namespace=zookeeper \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set global.storageClass=standard \\\n    --set networkPolicy.enabled=true \\\n    --set replicaCount=3 \\\n    --create-namespace\n```\n\n----------------------------------------\n\nTITLE: Listing Managed NiFi User Groups with kubectl\nDESCRIPTION: This console command uses `kubectl` to list the `NifiUserGroup` custom resources within a specified namespace (`nifikop`). This allows administrators to verify the existence and status of the managed groups automatically created by the NiFiKop operator, such as 'managed-admins', 'managed-nodes', and 'managed-readers'. Requires `kubectl` access to the Kubernetes cluster where NiFiKop is deployed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/2_manage_users_and_accesses/3_managed_groups.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nkubectl get -n nifikop nifiusergroups.nifi.konpyutaika.com \nNAME              AGE\nmanaged-admins    6d7h\nmanaged-nodes     6d7h\nmanaged-readers   6d7h\n```\n\n----------------------------------------\n\nTITLE: Applying NiFiKop CRDs manually\nDESCRIPTION: Commands for manually applying the Custom Resource Definitions (CRDs) required by NiFiKop when not using Helm's built-in CRD installation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/2_deploy_nifikop/1_quick_start.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\n```\n\n----------------------------------------\n\nTITLE: Getting UID/GID for NiFi Security Context (OpenShift, kubectl command)\nDESCRIPTION: Retrieves the 'supplemental-groups' annotation from the 'nifi' namespace to determine the UID/GID for setting security contexts in OpenShift, enabling proper permissions and security policies for NiFi deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nuid=$(kubectl get namespace nifi -o=jsonpath='{.metadata.annotations.openshift\\.io/sa\\.scc\\.supplemental-groups}' | sed 's/\\/10000$//' | tr -d '[:space:]')\n```\n\n----------------------------------------\n\nTITLE: YAML Example Configuration for NifiRegistryClient Resource\nDESCRIPTION: This YAML snippet demonstrates how to define a NiFi Registry Client resource in Kubernetes, specifying its metadata, cluster reference, description, and URI. It provides a concrete example of configuring the resource for deployment, including key parameters such as name, namespace, description, and the registry URI.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/5_references/3_nifi_registry_client.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiRegistryClient\nmetadata:\n  name: squidflow\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Squidflow demo\"\n  uri: \"http://nifi-registry:18080\"\n```\n\n----------------------------------------\n\nTITLE: Listing Node Resources by ID - Kubectl - Shell\nDESCRIPTION: This shell command uses `kubectl` to list Kubernetes resources (pods, configmaps, and persistent volume claims) that are labeled with `nodeId=25` within the `nifi` namespace. This command is used after scaling up to verify that the NiFiKop operator has successfully created the necessary components for the newly added node.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nkubectl get pods,configmap,pvc -l nodeId=25\n```\n\n----------------------------------------\n\nTITLE: Listing All Helm Releases Bash\nDESCRIPTION: This bash command retrieves all Helm release records, including deployed, failed, and deleted releases. Helm must be installed and configured. No other parameters are required. The command outputs a complete list of release statuses for cluster management.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/2_setup/3_install/1_customizable_install_with_helm.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --all\n```\n\n----------------------------------------\n\nTITLE: Manually Apply NiFiKop CRDs - Bash\nDESCRIPTION: Applies the necessary Custom Resource Definitions (CRDs) for NiFiKop resources (clusters, users, dataflows, etc.) directly to Kubernetes using `kubectl`. This is required if the Helm chart installation is performed with the `--skip-crds` flag. Requires `kubectl` access to the cluster and internet connectivity to fetch the CRD definitions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/2_setup/3_install/1_customizable_install_with_helm.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\n```\n\n----------------------------------------\n\nTITLE: Adding LDAP connection settings in NifiUserGroup\nDESCRIPTION: This snippet modifies the 'login_identity_providers.xml' to include LDAP connection configurations, enabling secure LDAP authentication for user groups. Dependencies involve correct XML schema and LDAP server details; responsible for securing user access.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/CHANGELOG.md#_snippet_1\n\nLANGUAGE: XML\nCODE:\n```\n<loginIdentityProviders>\n  <provider>\n    <identifier>ldap-provider</identifier>\n    <type>ldap-provider</type>\n    <property name=\"Connect URL\">ldap://ldap.example.com:636</property>\n    <property name=\"User DN Template\">uid={0},dc=example,dc=com</property>\n    <!-- Additional LDAP settings -->\n  </provider>\n</loginIdentityProviders>\n```\n\n----------------------------------------\n\nTITLE: AccessPolicyType Enum Definitions for Policy Scope\nDESCRIPTION: This schema provides constants for access policy types, differentiating between system-wide ('global') and component-level ('component') policies, guiding how permissions are managed within NiFi.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/5_references/2_nifi_user.md#_snippet_6\n\nLANGUAGE: YAML\nCODE:\n```\n`AccessPolicyType:\n  GlobalAccessPolicyType: global\n  ComponentAccessPolicyType: component`\n```\n\n----------------------------------------\n\nTITLE: Applying Kubernetes CRDs Manually Using Bash\nDESCRIPTION: This Bash snippet applies multiple CustomResourceDefinitions (CRDs) manually on a Kubernetes cluster with kubectl. It is used when the Helm install command is run with the `--skip-crds` flag to avoid automatic CRD deployment. The commands fetch CRDs from the nifikop GitHub repository and apply them to the running cluster. Each CRD corresponds to a specific custom resource handled by the nifikop operator such as nificlusters, nifiusers, and more.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifinodegroupautoscalers.yaml\n```\n\n----------------------------------------\n\nTITLE: Verifying NiFiKop Pods with kubectl Console\nDESCRIPTION: This snippet demonstrates how to list NiFiKop operator pods in the 'nifikop' namespace using kubectl. The command outputs pod names, status, restart counts, and age. Requires kubectl access to the cluster. Useful for troubleshooting and confirming correct operator deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/6_contributing/1_developer_guide.md#_snippet_9\n\nLANGUAGE: console\nCODE:\n```\nkubectl get pods -n nifikop\nNAME                                                READY   STATUS    RESTARTS   AGE\nskeleton-nifikop-8946b89dc-4cfs9   1/1     Running   0          7m45s\n```\n\n----------------------------------------\n\nTITLE: Updating OpenShift NiFi Cluster Manifest with UID - Bash\nDESCRIPTION: Performs in-place substitution of the hardcoded user ID in the OpenShift NiFi YAML manifest with the correct UID for cluster compliance. Utilizes sed for text replacement. Users must provide the correct path to config/samples/openshift.yaml and ensure permissions allow modification.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nsed -i \"s/1000690000/$uid/g\" config/samples/openshift.yaml\n\n```\n\n----------------------------------------\n\nTITLE: Verifying the Horizontal Pod Autoscaler (HPA) for NiFi Nodes (console command)\nDESCRIPTION: This command retrieves the current status of Horizontal Pod Autoscaler (HPA) resources in the 'clusters' namespace, specifically targeting the HPA managing the NiFi node autoscaler. It displays scaling targets, current utilization, replica counts, and age, allowing administrators to verify autoscaling behavior.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_7\n\nLANGUAGE: console\nCODE:\n```\nkubectl get -n clusters hpa\n```\n\n----------------------------------------\n\nTITLE: Querying Horizontal Pod Autoscalers in Namespace using kubectl - Console\nDESCRIPTION: This console command lists all Horizontal Pod Autoscaler (HPA) resources in the 'clusters' namespace using kubectl. It can be used to verify that the HPA managed by KEDA is present and associated with your NifiNodeGroupAutoscaler. This command requires access to a running Kubernetes cluster and kubectl configured with appropriate access credentials. The output is a table of HPA objects and their corresponding scaling targets. Parameters of interest include the HPA NAME, reference resource, scaling targets, replica counts, and age.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_13\n\nLANGUAGE: console\nCODE:\n```\nkubectl get -n clusters hpa\nNAME                REFERENCE                                                TARGETS         MINPODS   MAXPODS   REPLICAS   AGE\nkeda-hpa-cluster    NifiNodeGroupAutoscaler/nifinodegroupautoscaler-sample     1833m/2 (avg)   1         3         2          25d\n\n```\n\n----------------------------------------\n\nTITLE: Configuring OIDC identity mapping patterns in nifi.properties (Shell Script)\nDESCRIPTION: This shell code snippet shows the configuration of NiFi's properties to support multiple identity providers by setting specific patterns and transformation rules for distinguished names (DNs). The variables include the pattern, value, and transform for DN mapping, which help in extracting user identity information during authentication.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/5_users_authentication/1_oidc.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nnifi.security.identity.mapping.pattern.dn=CN=([^,]*)(?:, (?:O|OU)=.*)?\n nifi.security.identity.mapping.value.dn=$1\n nifi.security.identity.mapping.transform.dn=NONE\n```\n\n----------------------------------------\n\nTITLE: Manually Applying NiFiKop CRDs (Bash/kubectl)\nDESCRIPTION: Applies all necessary NiFiKop Custom Resource Definitions (CRDs) manually using `kubectl apply`. This is required if deploying the NiFiKop Helm chart with the `--skip-crds` flag.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/2_deploy_nifikop/1_quick_start.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring kubectl for EKS Cluster Access\nDESCRIPTION: Commands to configure `kubectl` for accessing an Amazon Elastic Kubernetes Service (EKS) cluster. It uses `eksctl` to write the kubeconfig entry and `kubectl` to switch to the appropriate context. Requires `eksctl` and AWS CLI to be installed and configured. Replace ${CLUSTER NAME} and ${eks context} with actual values.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/2_deploy_nifikop/1_quick_start.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Run eksctl to make the context available to kubectl\neksctl utils write-kubeconfig --cluster=${CLUSTER NAME}\n\n# Use kubectl config get-contexts to show available contexts\nkubectl config get-contexts\n\n# Run kubectl config use-context to access the cluster\nkubectl config use-context ${eks context}\n```\n\n----------------------------------------\n\nTITLE: Applying NiFiKop CRDs (Bash/kubectl)\nDESCRIPTION: Applies the Custom Resource Definitions (CRDs) required by the NiFiKop operator to the target Kubernetes cluster using 'kubectl apply'. These definitions enable Kubernetes to manage NiFi-specific resources like NifiClusters, NifiDataflows, etc. Requires kubectl v1.16+ configured to target the desired cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/6_contributing/1_developer_guide.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\n```\n\n----------------------------------------\n\nTITLE: Extracting NifiUser Credentials from Secret using kubectl (Shell)\nDESCRIPTION: Sequence of shell commands using `kubectl get secret` with `jsonpath` to retrieve TLS credentials from a Kubernetes secret (`example-client-secret`). It extracts the CA certificate (`ca.crt`), user certificate (`tls.crt`), and user private key (`tls.key`), decodes them from base64, and saves them to local files.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['ca\\.crt']}\" | base64 -d > ca.crt\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['tls\\.crt']}\" | base64 -d > tls.crt\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['tls\\.key']}\" | base64 -d > tls.key\n```\n\n----------------------------------------\n\nTITLE: Defining NiFiKop External ClusterIP Service (YAML)\nDESCRIPTION: Shows the `externalServices` section within a NiFiKop (NiFi Kubernetes Operator) custom resource definition or deployment YAML. This defines a Kubernetes ClusterIP service, referenced as `<service-name>` in other configurations (e.g., `nifi-cluster`), exposing NiFi's internal HTTPS listener (`https`) on port 8443. This service is the target for the HTTPS VirtualService.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nspec:  \n  externalServices:  \n    - name: \"nifi-cluster\"\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8443\n            internalListenerName: \"https\"\n```\n\n----------------------------------------\n\nTITLE: Resulting Pod Port Configuration (YAML)\nDESCRIPTION: Illustrates the `ports` section within a deployed NiFi pod's YAML manifest, generated based on the `internalListeners` configuration. It shows how each defined internal listener translates into a `containerPort` entry with its associated name and protocol (TCP) within the pod specification.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n    ports:\n    - containerPort: 8443\n      name: https\n      protocol: TCP\n    - containerPort: 6007\n      name: cluster\n      protocol: TCP\n    - containerPort: 6342\n      name: load-balance\n      protocol: TCP\n    - containerPort: 10000\n      name: s2s\n      protocol: TCP\n    - containerPort: 9090\n      name: prometheus\n      protocol: TCP\n```\n\n----------------------------------------\n\nTITLE: Complete Project Manifest for Migration Script (json)\nDESCRIPTION: Demonstrates a complete 'package.json' configuration needed for the migration. Specifies metadata, script commands, and dependencies for handling Kubernetes CRD migration tasks. This file ensures 'npm start' works per instructions and all required modules are available.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"nifikop_crd_migration\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Script to migrate from the old CRDs to the new CRDs.\",\n  \"main\": \"index.js\",\n  \"scripts\": {\n    \"start\": \"node --no-warnings index.js\",\n    \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\"\n  },\n  \"keywords\": [\n    \"K8S\",\n    \"NiFiKop\",\n    \"CRDs\"\n  ],\n  \"license\": \"ISC\",\n  \"dependencies\": {\n    \"@kubernetes/client-node\": \"^0.16.3\",\n    \"minimist\": \"^1.2.6\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Istio Gateway for HTTP in YAML\nDESCRIPTION: This YAML snippet defines an Istio `Gateway` resource. It configures an ingress gateway to intercept HTTP traffic on port 80 for a specific domain.  The `selector` field specifies the Istio ingress gateway. The `hosts` field lists the domain name the gateway will handle. This gateway is a prerequisite for routing traffic to the NiFi cluster. Input is an HTTP request; output is the routing of the traffic.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: nifi-gateway\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - nifi.my-domain.com\n```\n```\n\n----------------------------------------\n\nTITLE: Configuring Nifikop LoadBalancer Service (YAML)\nDESCRIPTION: This YAML snippet provides an example configuration for an `externalService` of type `LoadBalancer` in Nifikop. It maps service ports (8080 for 'http' via TCP, 7890 for 'my-custom-udp-listener' via UDP) to internal NiFi listeners. It also specifies a `loadBalancerClass` and includes additional Kubernetes metadata (annotations and labels).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/5_references/1_nifi_cluster/7_external_service_config.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\nexternalServices:\n    - name: \"nlb\"\n      spec:\n        type: LoadBalancer\n        loadBalancerClass: \"service.k8s.aws/nlb\"\n        portConfigs:\n          - port: 8080\n            internalListenerName: \"http\"\n          - port: 7890\n            internalListenerName: \"my-custom-udp-listener\"\n            protocol: UDP\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n\n----------------------------------------\n\nTITLE: Configuring npm start Script for CRD Migration Tool - JSON\nDESCRIPTION: This JSON code adds a custom 'start' script under the 'scripts' section in package.json, enabling invocation of index.js with suppressed warnings. To use this setup, include the line in your existing package.json 'scripts' object. The script runs the migration logic with Node.js. Dependencies and entry point file ('index.js') must exist.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"start\": \"node --no-warnings index.js\"\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Listeners and SSL in YAML\nDESCRIPTION: This example shows how to configure internal listeners for different connection types (https, cluster, s2s, prometheus, load-balance) along with SSL secrets in NiFiKop. The configuration includes container ports for each listener type and basic SSL certificate settings.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/5_references/1_nifi_cluster/6_listeners_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  listenersConfig:\n    internalListeners:\n      - type: \"https\"\n        name: \"https\"\n        containerPort: 8443\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n      - type: \"prometheus\"\n        name: \"prometheus\"\n        containerPort: 9090\n      - type: \"load-balance\"\n        name: \"load-balance\"\n        containerPort: 6342\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n```\n\n----------------------------------------\n\nTITLE: Defining NifiParameterContext in YAML\nDESCRIPTION: This YAML snippet defines a `NifiParameterContext` resource, specifying its API version, kind, metadata, and specification. It sets a description, references a NiFi cluster, references secrets for sensitive parameters, and defines a set of parameters. The snippet is a configuration for managing NiFi parameters within a Kubernetes environment using the NiFi Operator.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/5_references/4_nifi_parameter_context.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      description: toto\n      sensistive: true\n```\n\n----------------------------------------\n\nTITLE: Defining NifiDataflow in YAML for NiFiKop\nDESCRIPTION: This snippet demonstrates how to create a NifiDataflow resource that references a versioned flow from the NiFi Registry, a parameter context for configuration, and specifies synchronization and update strategies.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: demo\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Listing All Helm Releases Including Deleted and Failed Ones Using Bash\nDESCRIPTION: This Helm command lists all Helm releases, including those currently deployed, deleted, or failed. It is useful for comprehensive release management and troubleshooting.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --all\n```\n\n----------------------------------------\n\nTITLE: Getting the Status of a Helm Deployment (Bash)\nDESCRIPTION: This snippet queries Helm for the status of the NiFiKop deployment, providing details on the deployed resources, readiness, and any potential failures for the specified release name. The release name should match the installation. Dependencies are the Helm CLI and cluster access.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/2_setup/3_install/1_customizable_install_with_helm.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nhelm status nifikop\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus Monitoring (JSON)\nDESCRIPTION: Shows the basic structure for enabling monitoring via the Prometheus operator integration. Setting `enabled` to `true` will create a `ServiceMonitor` resource, requiring the Prometheus operator or Rancher Monitoring to be installed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/helm/nifi-cluster/README.md#_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\"enabled\":false}\n```\n\n----------------------------------------\n\nTITLE: Installing Helm Chart - Bash\nDESCRIPTION: Installs the NiFiKop operator using Helm. This command uses the `helm install` command to deploy the operator based on a local chart package. It specifies the image tag for the operator, the namespace, and possibly other configurations via `--set` parameters. Requires Helm installed and configured to access the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/6_contributing/1_developer_guide.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nhelm install skeleton ./helm/nifikop \\\n    --set image.tag=v0.5.1-release \\\n    --namespace-{\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: NiFiKop CRD Migration Script (JavaScript)\nDESCRIPTION: This JavaScript code defines the NiFiKop CRD migration script (`index.js`). It uses `@kubernetes/client-node` to interact with the Kubernetes API and `minimist` to parse command-line arguments. The script lists resources from the old CRDs and creates corresponding resources in the new CRDs, copying metadata, spec, and status.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nprocess.env['NODE_TLS_REJECT_UNAUTHORIZED'] = 0;\nconst k8s = require('@kubernetes/client-node');\n\nconst kc = new k8s.KubeConfig();\nkc.loadFromDefault();\n\nconst k8sApi = kc.makeApiClient(k8s.CustomObjectsApi);\n\nconst KONPYUTAIKA_GROUP = 'nifi.konpyutaika.com';\nconst KONPYUTAIKA_GROUP_VERSION = 'v1alpha1';\nconst ORANGE_GROUP = 'nifi.orange.com';\nconst ORANGE_GROUP_VERSION = 'v1alpha1';\n\nconst call = async (SRC_GRP, SRC_GRP_VER, DST_GRP, DST_GRP_VER, KIND_PLURAL, NAMESPACE) => {\n\tconsole.log(`Listing ${KIND_PLURAL} of ${SRC_GRP}/${SRC_GRP_VER} in ${NAMESPACE}...`);\n\tconst listResources = (await k8sApi.listNamespacedCustomObject(SRC_GRP, SRC_GRP_VER, NAMESPACE, KIND_PLURAL)).body.items;\n\treturn Promise.all(listResources.map(async (resource) => {\n\t\ttry {\n\t\t\tconsole.log(`Found ${resource.kind} \\\"${resource.metadata.name}\\\" of ${resource.apiVersion} in ${NAMESPACE}`);\n\n\t\t\tif (resource.metadata.ownerReferences) {\n\t\t\t\tconsole.log(`${resource.kind} ${resource.metadata.name} mananged by something else (ownerRefereces is set).`);\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tconst bodyResource = {\n\t\t\t\tapiVersion: `${DST_GRP}/${DST_GRP_VER}`,\n\t\t\t\tkind: resource.kind,\n\t\t\t\tmetadata: {\n\t\t\t\t\tname: resource.metadata.name,\n\t\t\t\t\tannotations: resource.metadata.annotations,\n\t\t\t\t\tlabels: resource.metadata.labels\n\t\t\t\t},\n\t\t\t\tspec: resource.spec\n\t\t\t};\n\n\t\t\tconsole.log(`Creating ${bodyResource.kind} \\\"${bodyResource.metadata.name}\\\" of ${bodyResource.apiVersion} in ${NAMESPACE}...`);\n\t\t\tconst newResource = (await k8sApi.createNamespacedCustomObject(DST_GRP, DST_GRP_VER, NAMESPACE, KIND_PLURAL, bodyResource)).body;\n\t\t\tconsole.log('...done creating.');\n\n\t\t\tconst bodyStatus = {\n\t\t\t\tapiVersion: newResource.apiVersion,\n\t\t\t\tkind: newResource.kind,\n\t\t\t\tmetadata: {\n\t\t\t\t\tname: newResource.metadata.name,\n\t\t\t\t\tresourceVersion: newResource.metadata.resourceVersion\n\t\t\t\t},\n\t\t\t\tstatus: resource.status\n\t\t\t};\n\n\t\t\tconsole.log(`Copying status from ${resource.kind} \\\"${resource.metadata.name}\\\" of ${newResource.apiVersion} to ${newResource.kind} \\\"${newResource.metadata.name}\\\" of ${newResource.apiVersion} in ${NAMESPACE}...`);\n\t\t\tconst newResourceWithStatus = (await k8sApi.replaceNamespacedCustomObjectStatus(DST_GRP, DST_GRP_VER, NAMESPACE, KIND_PLURAL, bodyStatus.metadata.name, bodyStatus)).body;\n\t\t\tconsole.log('...done copying.');\n\t\t\treturn newResourceWithStatus;\n\t\t}\n\t\tcatch (e) {\n\t\t\tconsole.error(e.body ? e.body.message ? e.body.message : e.body : e);\n\t\t}\n\t}));\n};\n\nconst argv = require('minimist')(process.argv.slice(2));\n\nlet NAMESPACE = argv.namespace ? argv.namespace.length > 0 ? argv.namespace : 'default' : 'default';\nlet KIND_PLURAL = {\n\tcluster: 'nificlusters',\n\tdataflow: 'nifidataflows',\n\tparametercontext: 'nifiparametercontexts',\n\tregistryclient: 'nifiregistryclients',\n\tuser: 'nifiusers',\n\tusergroup: 'nifiusergroups',\n};\n\nif (!argv.type) {\n\tconsole.error('Type not provided');\n\tprocess.exit(1);\n}\n\nif (!KIND_PLURAL[argv.type]) {\n\tconsole.error(`Type ${argv.type} is not one of the following types: ${Object.keys(KIND_PLURAL)}`);\n\tprocess.exit(1);\n}\n\nconsole.log(`########### START: ${KIND_PLURAL[argv.type]} ###########`);\ncall( ORANGE_GROUP, ORANGE_GROUP_VERSION, KONPYUTAIKA_GROUP, KONPYUTAIKA_GROUP_VERSION, KIND_PLURAL[argv.type], NAMESPACE)\n\t.then(r => console.log('############ END ############'))\n\t.catch(e => console.error(e));\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio VirtualService for HTTPS (YAML)\nDESCRIPTION: This YAML snippet defines a VirtualService that redirects HTTP traffic to the specific service (ClusterIP service).  It specifies the gateway, the host, the matching prefix ('/'), and the destination service (service-name.namespace.svc.cluster.local) on port 8443. This VirtualService is responsible for routing the traffic from the gateway to the cluster service.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: nifi-vs\nspec:\n  gateways:\n  - nifi-gateway\n  hosts:\n  - nifi.my-domain.com\n  http:\n  - match:\n    - uri:\n        prefix: /\n    route:\n    - destination:\n        host: <service-name>.<namespace>.svc.cluster.local\n        port:\n          number: 8443\n```\n```\n\n----------------------------------------\n\nTITLE: Adding Bitnami Helm Repository\nDESCRIPTION: Command to add the Bitnami Helm repository which contains the Zookeeper chart needed as a prerequisite for NiFi.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhelm repo add bitnami https://charts.bitnami.com/bitnami\n```\n\n----------------------------------------\n\nTITLE: Creating a Dedicated Namespace for Monitoring\nDESCRIPTION: Creates a Kubernetes namespace for Prometheus and monitoring resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_5\n\nLANGUAGE: console\nCODE:\n```\nkubectl create namespace monitoring-system\n```\n\n----------------------------------------\n\nTITLE: Cloning Repository\nDESCRIPTION: This code snippet clones the NiFiKop repository from GitHub, providing access to the necessary configuration files and deployment scripts for setting up the secured NiFi cluster. It is a prerequisite step for accessing the deployment resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/blog/2020-06-30-secured_nifi_cluster_on_gcp_with_external_dns.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/Okonpyutaika/nifikop.git\ncd nifikop/docs/tutorials/secured_nifi_cluster_on_gcp_with_external_dns\n```\n\n----------------------------------------\n\nTITLE: Listing Current Helm Releases\nDESCRIPTION: List all currently deployed Helm releases in your Kubernetes cluster. This command provides a summary of active chart installations, including their names, revisions, statuses, and namespaces.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nhelm list\n```\n\n----------------------------------------\n\nTITLE: Configuring kubectl for GKE\nDESCRIPTION: Command to retrieve GKE cluster credentials and configure kubectl to use the cluster's context. Requires gcloud CLI to be installed and authenticated.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/2_deploy_nifikop/1_quick_start.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\ngcloud container clusters get-credentials CLUSTER_NAME --zone ZONE_NAME --project PROJECT_NAME.\n```\n\n----------------------------------------\n\nTITLE: Configuring Nifikop ClusterIP Service (YAML)\nDESCRIPTION: This YAML snippet demonstrates how to configure an `externalService` of type `ClusterIP` for Nifikop. It maps a service port (8080) to an internal NiFi listener ('http') and another port (7182) to a custom listener ('my-custom-listener') using TCP. Additional Kubernetes metadata like annotations and labels are also included.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/5_references/1_nifi_cluster/7_external_service_config.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n  externalServices:\n    - name: \"clusterip\"\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8080\n            internalListenerName: \"http\"\n          - port: 7182\n            internalListenerName: \"my-custom-listener\"\n            protocol: TCP\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n\n----------------------------------------\n\nTITLE: Deploying Prometheus Operator with Helm\nDESCRIPTION: Installs the Prometheus Operator with specific configuration options to monitor the NiFi cluster. Disables unnecessary components to keep the deployment minimal and focused on monitoring NiFi.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_8\n\nLANGUAGE: console\nCODE:\n```\nhelm install prometheus prometheus/kube-prometheus-stack --namespace monitoring-system \\\n    --set prometheusOperator.createCustomResource=false \\\n    --set prometheusOperator.logLevel=debug \\\n    --set prometheusOperator.alertmanagerInstanceNamespaces=monitoring-system \\\n    --set prometheusOperator.namespaces.additional[0]=monitoring-system \\\n    --set prometheusOperator.prometheusInstanceNamespaces=monitoring-system \\\n    --set prometheusOperator.thanosRulerInstanceNamespaces=monitoring-system \\\n    --set defaultRules.enable=false \\\n    --set alertmanager.enabled=false \\\n    --set grafana.enabled=false \\\n    --set kubeApiServer.enabled=false \\\n    --set kubelet.enabled=false \\\n    --set kubeControllerManager.enabled=false \\\n    --set coreDNS.enabled=false \\\n    --set kubeEtcd.enabled=false \\\n    --set kubeScheduler.enabled=false \\\n    --set kubeProxy.enabled=false \\\n    --set kubeStateMetrics.enabled=false \\\n    --set prometheus.enabled=false\n```\n\n----------------------------------------\n\nTITLE: Retrieving Client Certificates from Kubernetes Secret\nDESCRIPTION: Uses `kubectl` to retrieve the CA certificate, user certificate, and user private key from the `example-client-secret` created by the `NifiUser` resource. It pipes the base64-encoded data from the JSON output into `base64 -d` to decode it and save the results into local files (`ca.crt`, `tls.crt`, `tls.key`).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['ca\\.crt']}\" | base64 -d > ca.crt\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['tls\\.crt']}\" | base64 -d > tls.crt\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['tls\\.key']}\" | base64 -d > tls.key\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiParameterContext CRD in YAML\nDESCRIPTION: This YAML snippet defines a NifiParameterContext custom resource used to create parameter contexts in NiFi via NiFiKop. It includes a description, references a NiFi cluster, lists regular parameters with names, values, and descriptions, and importantly, refers to a Kubernetes secret for sensitive parameters.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\n  namespace: demo\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      value: toto\n      description: toto\n```\n\n----------------------------------------\n\nTITLE: Helm Install NiFiKop\nDESCRIPTION: This command installs the NiFiKop operator using Helm, specifying the namespace and image tag. The `--namespace` flag specifies the namespace where the operator will be deployed.  The `--set` flag customizes the deployment by setting the image tag.  The image tag specified determines the version of the NiFiKop operator that will be deployed. The operator version should be set according to the user's needs.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/blog/2020-06-30-secured_nifi_cluster_on_gcp_with_external_dns.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n# You have to create the namespace before executing following command\nhelm install nifikop \\\n    --namespace=nifikop \\\n    --set image.tag=v0.2.1-release \\\n    orange-incubator/nifikop\n```\n\n----------------------------------------\n\nTITLE: Verifying New Node Resources (Console)\nDESCRIPTION: Displays the Kubernetes resources (pod, configmap, persistent volume claim) associated with the newly added NiFi node (identified by `nodeId=25`) using `kubectl`. This output confirms that the operator has created the necessary components for the node.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl get pods,configmap,pvc -l nodeId=25\nNAME                          READY   STATUS    RESTARTS   AGE\npod/simplenifi-25-nodem5jh4   1/1     Running   0          11m\n\nNAME                             DATA   AGE\nconfigmap/simplenifi-config-25   7      11m\n\nNAME                                               STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\npersistentvolumeclaim/simplenifi-25-storagehwn24   Bound    pvc-7da86076-728e-11ea-846d-42010a8400f2   10Gi       RWO            standard       11m\n```\n\n----------------------------------------\n\nTITLE: Configuring Local Operator Environment Variables Bash\nDESCRIPTION: This snippet sets environment variables required by development IDEs or local execution to configure NiFiKop's connection to a Kubernetes cluster. Variables include KUBECONFIG (cluster credentials), WATCH_NAMESPACE (namespace to watch), POD_NAME (operator pod name), LOG_LEVEL (debugging verbosity), and OPERATOR_NAME (logical name). Requires a bash shell and proper environment values.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/6_contributing/1_developer_guide.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nKUBECONFIG={path/to/your/kubeconfig}\nWATCH_NAMESPACE={namespace_to_watch}\nPOD_NAME={name for operator pod}\nLOG_LEVEL=Debug\nOPERATOR_NAME=ide\n```\n\n----------------------------------------\n\nTITLE: Executing the NiFiKop CRD Migration Script (Bash)\nDESCRIPTION: Demonstrates the command to execute the Node.js migration script using the npm start script defined in `package.json`. It requires specifying the NiFiKop resource type to migrate (e.g., 'cluster', 'dataflow') using the `--type` argument and the Kubernetes namespace containing the resources using the `--namespace` argument. The double dash (`--`) separates npm arguments from the script's arguments.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nnpm start -- --type=<NIFIKOP_RESOURCE> --namespace=<K8S_NAMESPACE>\n```\n\n----------------------------------------\n\nTITLE: Creating NiFiUser for Client Certificate (YAML)\nDESCRIPTION: This YAML creates a `NifiUser` resource within the NiFi operator, specifying the related `nifi` cluster by name.  It requests the generation of a new user and its associated credentials, which will be stored in a Kubernetes secret named `example-client-secret`. The operator generates and manages the client certificates signed by the CA.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion:  nifi.konpyutaika.com/v1alpha1\nkind: NifiUser\nmetadata:\n  name: example-client\n  namespace: nifi\nspec:\n  clusterRef:\n    name: nifi\n  secretName: example-client-secret\n```\n\n----------------------------------------\n\nTITLE: Defining Ingress Rules for NiFi Cluster with HTTPS Backend in YAML\nDESCRIPTION: This ingress manifest configures the routing of external HTTPS traffic to the internal NiFi cluster service. It uses annotations such as 'nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\"' to enforce secure backend communication and references the NiFi cluster service named 'nifi-cluster'. It supports exposing multiple NiFi clusters through unique URLs, with an emphasis on Kubernetes ingress standards and HTTPS enforcement.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/config/samples/keycloak-example/README.md#_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\ningress.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring RBAC Resources for Prometheus\nDESCRIPTION: Creates a ServiceAccount, ClusterRole, and ClusterRoleBinding to grant Prometheus the necessary permissions to access metrics endpoints in the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: prometheus\n  namespace: monitoring-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: prometheus\nrules:\n- apiGroups: [\"\"]\n  resources:\n  - nodes\n  - nodes/metrics\n  - services\n  - endpoints\n  - pods\n  verbs: [\"get\", \"list\", \"watch\"]\n- apiGroups: [\"\"]\n  resources:\n  - configmaps\n  verbs: [\"get\"]\n- apiGroups:\n  - networking.k8s.io\n  resources:\n  - ingresses\n  verbs: [\"get\", \"list\", \"watch\"]\n- nonResourceURLs: [\"/metrics\"]\n  verbs: [\"get\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: prometheus\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: prometheus\nsubjects:\n- kind: ServiceAccount\n  name: prometheus\n  namespace: monitoring-system\n```\n\n----------------------------------------\n\nTITLE: Defining Managed Users in NifiCluster Spec YAML\nDESCRIPTION: Demonstrates how to configure the list of users belonging to the predefined 'managed-admins' and 'managed-readers' groups directly within the `NifiCluster` resource specification. The operator uses the `managedAdminUsers` and `managedReaderUsers` fields to automatically create and manage the corresponding `NifiUser` and `NifiUserGroup` resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/3_manage_nifi/2_manage_users_and_accesses/3_managed_groups.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  ...\n  oneNifiNodePerNode: false\n  propagateLabels: true\n  managedAdminUsers:\n    -  identity: \"alexandre.guitton@konpyutaika.com\"\n       name: \"aguitton\"\n    -  identity: \"nifiuser@konpyutaika.com\"\n       name: \"nifiuser\"\n  managedReaderUsers:\n    -  identity: \"toto@konpyutaika.com\"\n       name: \"toto\"\n    ...\n```\n\n----------------------------------------\n\nTITLE: Combining Internal Listeners and Exposing via External Kubernetes Service - YAML\nDESCRIPTION: This comprehensive YAML sample demonstrates both the definition of multiple internal listeners and configuring an external service to expose those listeners to users outside the cluster. The externalServices field specifies a LoadBalancer service exposing two internal listeners ('https' and a custom 'http-tracking') on ports 443 and 80. The mapping between internalListenerName and the external service port is explicitly declared. This requires the cluster to be running with nifikop, and cloud infrastructure must support the LoadBalancer type for external access. Inputs: names of internal listeners, port numbers, and service configuration. Expected outcome is an externally accessible service mapped to correct NiFi endpoints.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    - type: \"https\"\n      name: \"https\"\n      containerPort: 8443\n    - type: \"cluster\"\n      name: \"cluster\"\n      containerPort: 6007\n    - type: \"s2s\"\n      name: \"s2s\"\n      containerPort: 10000\n    - type: \"prometheus\"\n      name: \"prometheus\"\n      containerPort: 9090\n    - type: \"load-balance\"\n      name: \"load-balance\"\n      containerPort: 6342\n    - name: \"http-tracking\"\n      containerPort: 8081\nexternalServices:\n  - name: cluster-access\n    spec:\n      portConfigs:\n        - internalListenerName: https\n          port: 443\n        - internalListenerName: http-tracking\n          port: 80\n      type: LoadBalancer\n```\n\n----------------------------------------\n\nTITLE: Installing Helm Chart with Dry Run\nDESCRIPTION: Helm command to simulate the deployment of the Nifikop chart with debug log level and custom namespace, without making actual changes. Useful for testing configuration and deployment plans.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop \\\n    --dry-run \\\n    --set logLevel=Debug \\\n    --set namespaces={\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Helm Install NiFiKop (previous)\nDESCRIPTION: This command installs the NiFiKop operator using Helm, specifying the release name, namespace, and image tag. The `--name` flag defines the release name, `--namespace` specifies the namespace where the operator will be deployed. The `--set` flag customizes the deployment by setting the image tag, determining the version of the operator to be deployed. This is meant to be used for previous helm version.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/blog/2020-06-30-secured_nifi_cluster_on_gcp_with_external_dns.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nhelm install --name=nifikop \\\n    --namespace=nifikop \\\n    --set image.tag=v0.2.1-release \\\n    orange-incubator/nifikop\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies Node.js Migration Bash\nDESCRIPTION: This command initializes a new Node.js project using default settings and installs the necessary dependencies for the migration script. It adds the Kubernetes client library and a command-line argument parser to the project's dependencies. This prepares the environment to run the migration script.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm init -y\nnpm install @kubernetes/client-node@0.16.3 minimist@1.2.6\n```\n\n----------------------------------------\n\nTITLE: Installing Nifikop Chart without CRDs Bash\nDESCRIPTION: This bash command installs the Nifikop Helm chart into a Kubernetes cluster but skips the CRD installation step. It requires Helm installed, access to the Nifikop chart directory, and the user may specify namespaces. Key parameters include the release name, chart path, target namespaces array, and the '--skip-crds' flag to preserve existing CRDs. Output is a deployed operator without altering cluster-wide CRDs.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/2_setup/3_install/1_customizable_install_with_helm.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nhelm install --name nifikop ./helm/nifikop --set namespaces={\"nifikop\"} --skip-crds\n```\n\n----------------------------------------\n\nTITLE: Checking NifiCluster Status During Scale Down (Console)\nDESCRIPTION: This command uses `kubectl describe` to show the detailed status of the `simplenifi` NifiCluster resource. The output includes the `Status.Nodes State` section, which reflects the current state of the node being decommissioned (e.g., `GracefulDownscaleRequired`), allowing monitoring of the scale-down process.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/3_tasks/1_nifi_cluster/2_cluster_scaling.md#_snippet_5\n\nLANGUAGE: console\nCODE:\n```\nkubectl describe nificluster simplenifi\n\n...\nStatus:\n  Nodes State:\n    ...\n    2:\n      Configuration State:  ConfigInSync\n      Graceful Action State:\n        Action State:   GracefulDownscaleRequired\n        Error Message:\n    ...\n...\n```\n\n----------------------------------------\n\nTITLE: Configuring package.json Script Entry for Migration\nDESCRIPTION: This JSON snippet outlines the script entry for 'package.json' to run the migration script 'index.js' with Node.js while suppressing warnings. It shows the essential metadata, dependencies, and start script for ease of running the migration utility.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"nifikop_crd_migration\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Script to migrate from the old CRDs to the new CRDs.\",\n  \"main\": \"index.js\",\n  \"scripts\": {\n    \"start\": \"node --no-warnings index.js\",\n    \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\"\n  },\n  \"keywords\": [\n    \"K8S\",\n    \"NiFiKop\",\n    \"CRDs\"\n  ],\n  \"license\": \"ISC\",\n  \"dependencies\": {\n    \"@kubernetes/client-node\": \"^0.16.3\",\n    \"minimist\": \"^1.2.6\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating ServiceMonitor for NiFi Metrics\nDESCRIPTION: YAML definition of a ServiceMonitor resource that configures Prometheus to scrape NiFi metrics from the prometheus endpoint and adds metadata labels for better metric organization.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: cluster\n  namespace: monitoring-system\n  labels:\n    app: nifi-cluster\n    nifi_cr: cluster\nspec:\n  selector:\n    matchLabels:\n      app: nifi\n      nifi_cr: cluster\n  namespaceSelector:\n    matchNames:\n      - clusters\n  endpoints:\n    - interval: 10s\n      port: prometheus\n      path: /metrics\n      honorLabels: true\n      relabelings:\n        - sourceLabels: [__meta_kubernetes_pod_ip]\n          separator: ;\n          regex: (.*)\n          targetLabel: pod_ip\n          replacement: $1\n          action: replace\n        - sourceLabels: [__meta_kubernetes_pod_label_nodeId]\n          separator: ;\n          regex: (.*)\n          targetLabel: nodeId\n          replacement: $1\n          action: replace\n        - sourceLabels: [__meta_kubernetes_pod_label_nifi_cr]\n          separator: ;\n          regex: (.*)\n          targetLabel: nifi_cr\n          replacement: $1\n          action: replace\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Parameter Context Resource (YAML)\nDESCRIPTION: Define a `NifiParameterContext` custom resource for NiFiKop to create and manage NiFi Parameter Contexts. It allows specifying parameters directly or referencing Kubernetes secrets for sensitive values, and associates the context with a NiFi cluster. This resource is used to provide configuration to NiFi dataflows.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\n  namespace: demo\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      value: toto\n      description: toto\n```\n\n----------------------------------------\n\nTITLE: Example package.json for NiFiKop Migration Script\nDESCRIPTION: Provides a complete example `package.json` file for the Node.js migration project. It includes project metadata (name, version, description, keywords), the main entry point (`index.js`), the 'start' script, licensing information, and lists the required dependencies (`@kubernetes/client-node` and `minimist`).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"nifikop_crd_migration\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Script to migrate from the old CRDs to the new CRDs.\",\n  \"main\": \"index.js\",\n  \"scripts\": {\n    \"start\": \"node --no-warnings index.js\",\n    \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\"\n  },\n  \"keywords\": [\n    \"K8S\",\n    \"NiFiKop\",\n    \"CRDs\"\n  ],\n  \"license\": \"ISC\",\n  \"dependencies\": {\n    \"@kubernetes/client-node\": \"^0.16.3\",\n    \"minimist\": \"^1.2.6\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Updating Helm Repositories\nDESCRIPTION: Updates the local Helm chart repositories to ensure the latest charts are available.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_6\n\nLANGUAGE: console\nCODE:\n```\nhelm repo update\n```\n\n----------------------------------------\n\nTITLE: Verifying NiFiKop Operator Pod Status - Bash\nDESCRIPTION: Checks the status of the NiFiKop operator pod running in the specified Kubernetes namespace by listing pods with 'kubectl get pods'. This command helps confirm that the operator is running successfully after deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/6_contributing/1_developer_guide.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get pods -n nifikop\nNAME                                                READY   STATUS    RESTARTS   AGE\nskeleton-nifikop-8946b89dc-4cfs9   1/1     Running   0          7m45s\n```\n\n----------------------------------------\n\nTITLE: Defining a NiFi UserGroup in YAML\nDESCRIPTION: This YAML snippet defines a NifiUserGroup resource. It specifies the API version, kind, metadata (name), and the specification including cluster reference, user references, and access policies. The cluster reference links this group to a NiFi cluster named 'nc' in the 'nifikop' namespace, and the user references list two users.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/5_references/6_nifi_usergroup.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n    - name: nc-controller.nifikop.mgt.cluster.local\n  accessPolicies:\n    - type: global\n      action: read\n      resource: /counters\n```\n\n----------------------------------------\n\nTITLE: Monitoring Node Decommission Status (Console)\nDESCRIPTION: This command uses `kubectl describe` to inspect the status of the `NifiCluster` resource named 'simplenifi'. It allows monitoring the state of individual nodes during the scale-down process, showing fields like `Graceful Action State` which indicates the current step in the decommissioning process (e.g., `GracefulDownscaleRequired`).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nkubectl describe nificluster simplenifi\n\n...\nStatus:\n  Nodes State:\n    ...\n    2:\n      Configuration State:  ConfigInSync\n      Graceful Action State:\n        Action State:   GracefulDownscaleRequired\n        Error Message:\n    ...\n...\n```\n\n----------------------------------------\n\nTITLE: Configuring Complete Internal Listeners with External Services in YAML\nDESCRIPTION: A complete example showing how to configure both internal listeners and external services. The external service exposes the HTTPS and custom HTTP tracking ports through a LoadBalancer service with specified ports.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    - type: \"https\"\n      name: \"https\"\n      containerPort: 8443\n    - type: \"cluster\"\n      name: \"cluster\"\n      containerPort: 6007\n    - type: \"s2s\"\n      name: \"s2s\"\n      containerPort: 10000\n    - type: \"prometheus\"\n      name: \"prometheus\"\n      containerPort: 9090\n    - type: \"load-balance\"\n      name: \"load-balance\"\n      containerPort: 6342\n    - name: \"http-tracking\"\n      containerPort: 8081\nexternalServices:\n  - name: cluster-access\n    spec:\n      portConfigs:\n        - internalListenerName: https\n          port: 443\n        - internalListenerName: http-tracking\n          port: 80\n      type: LoadBalancer\n```\n\n----------------------------------------\n\nTITLE: Deploy Let's Encrypt Issuer\nDESCRIPTION: This command creates a Let's Encrypt issuer within the Kubernetes cluster. The issuer will request certificates from Let's Encrypt for the NiFi cluster. The provided yaml file specifies an email address for certificate notifications. Requires the user to change the `spec.acme.email` field. It's necessary to have cert-manager installed and configured before running this command.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/blog/2020-06-30-secured_nifi_cluster_on_gcp_with_external_dns.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\ncd ..\nkubectl create -f kubernetes/nifi/letsencryptissuer.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring a LoadBalancer Service in Nifikop (YAML)\nDESCRIPTION: This snippet configures a LoadBalancer service for Nifikop, exposing ports 8080 and 7890 for 'http' and 'my-custom-udp-listener' respectively. It sets the 'loadBalancerClass' to 'service.k8s.aws/nlb' and specifies the UDP protocol for one of the ports. Metadata like annotations and labels are also included.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/5_references/1_nifi_cluster/7_external_service_config.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\nexternalServices:\n    - name: \"nlb\"\n      spec:\n        type: LoadBalancer\n        loadBalancerClass: \"service.k8s.aws/nlb\"\n        portConfigs:\n          - port: 8080\n            internalListenerName: \"http\"\n          - port: 7890\n            internalListenerName: \"my-custom-udp-listener\"\n            protocol: UDP\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n\n----------------------------------------\n\nTITLE: Defining Additional Custom Listeners in NiFi Deployment YAML\nDESCRIPTION: Allows adding custom internal listeners without predefined types, such as custom HTTP endpoints, by specifying 'name' and 'containerPort'. Useful for exposing additional NiFi functionalities internally or externally. Dependencies involve NiFi's listener configuration; key parameters include 'name' and 'containerPort'. Inputs are YAML snippets; outputs modify the listener setup for extended connectivity.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    ...\n    - name: \"http-tracking\"\n      containerPort: 8081\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiParameterContext YAML\nDESCRIPTION: This YAML defines a NifiParameterContext custom resource. It specifies the API version, kind, metadata (name and namespace), and the specification, including a description, a reference to the NiFi cluster, references to secrets containing parameters, and the parameter definitions themselves.  The cluster must exist.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\n  namespace: demo\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      value: toto\n      description: toto\n```\n\n----------------------------------------\n\nTITLE: Installing Zookeeper with UID/GID security parameters on OpenShift (Helm Bash)\nDESCRIPTION: Installs Zookeeper via Helm with specified resource requests and limits, sets the storageClass, enables network policy, and applies security context with the UID/GID obtained from OpenShift. Users should replace 'zookeper_uid' with the actual value.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nhelm install zookeeper bitnami/zookeeper \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set global.storageClass=standard \\\n    --set networkPolicy.enabled=true \\\n    --set replicaCount=3 \\\n    --set containerSecurityContext.runAsUser=$zookeper_uid \\\n    --set podSecurityContext.fsGroup=$zookeper_uid\n```\n\n----------------------------------------\n\nTITLE: Defining NifiConnection Resource YAML\nDESCRIPTION: This YAML defines a NifiConnection resource named 'connection' within the 'instances' namespace. It specifies the source and destination dataflows, along with configuration details such as flow file expiration, back pressure thresholds, load balancing strategy, and prioritizers. The update strategy is set to 'drain'.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/5_references/8_nifi_connection.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiConnection\nmetadata:\n  name: connection\n  namespace: instances\nspec:\n  source:\n    name: input\n    namespace: instances\n    subName: output_1\n    type: dataflow\n  destination:\n    name: output\n    namespace: instances\n    subName: input_1\n    type: dataflow\n  configuration:\n    flowFileExpiration: 1 hour\n    backPressureDataSizeThreshold: 100 GB\n    backPressureObjectThreshold: 10000\n    loadBalanceStrategy: PARTITION_BY_ATTRIBUTE\n    loadBalancePartitionAttribute: partition_attribute\n    loadBalanceCompression: DO_NOT_COMPRESS\n    prioritizers: \n      - NewestFlowFileFirstPrioritizer\n      - FirstInFirstOutPrioritizer\n    labelIndex: 0\n    bends:\n      - posX: 550\n        posY: 550\n      - posX: 550\n        posY: 440\n      - posX: 550\n        posY: 88\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Console Command for Creating a NifiUser Resource with Certificates\nDESCRIPTION: Automates creation of client certificates by applying a NifiUser CRD that generates a user secret containing CA cert, user cert, and private key. Dependencies include Kubernetes and the Nifikop operator. The key output is a secret named 'example-client-secret' that stores user credentials for secure NiFi communication.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_3\n\nLANGUAGE: console\nCODE:\n```\ncat << EOF | kubectl apply -n nifi -f -\napiVersion:  nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: example-client\n  namespace: nifi\nspec:\n  clusterRef:\n    name: nifi\n  secretName: example-client-secret\nEOF\n```\n\n----------------------------------------\n\nTITLE: Managing Nodes with Managed-Nodes Group\nDESCRIPTION: An additional 'managed-nodes' group is automatically created, containing individual NifiUser resources for each node. This group provides access controls for node-level operations and is managed alongside 'managed-admins' and 'managed-readers'. The list of these groups can be retrieved via Kubernetes CLI command, enabling operators to monitor and manage node-specific user permissions efficiently.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/3_manage_nifi/2_manage_users_and_accesses/3_managed_groups.md#_snippet_3\n\n\n\n----------------------------------------\n\nTITLE: Manually Deleting Nifikop CRDs with kubectl\nDESCRIPTION: Demonstrates how to manually delete specific Nifikop Custom Resource Definitions (CRDs) from the Kubernetes cluster using `kubectl delete crd`. This action is permanent and will also remove all custom resources created using these definitions (e.g., all NifiCluster instances).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete crd nificlusters.nifi.konpyutaika.com\nkubectl delete crd nifiusers.nifi.konpyutaika.com\nkubectl delete crd nifiusergroups.nifi.konpyutaika.com\nkubectl delete crd nifiregistryclients.nifi.konpyutaika.com\nkubectl delete crd nifiparametercontexts.nifi.konpyutaika.com\nkubectl delete crd nifidataflows.nifi.konpyutaika.com\n```\n\n----------------------------------------\n\nTITLE: Adding Additional Custom Internal Listeners in NiFi Clusters with YAML\nDESCRIPTION: Extends the internal listeners configuration by adding a custom port (here, 'http-tracking') without specifying a type. Useful for exposing NiFi processor endpoints directly (such as HTTP receivers). Requires the NiFi operator to accept unnamed listeners and generates a corresponding container port within the pod. The custom listener can be externally mapped through an additional service if needed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    ...\n    - name: \"http-tracking\"\n      containerPort: 8081\n```\n\n----------------------------------------\n\nTITLE: Installing Cert-Manager using Helm 3\nDESCRIPTION: Installs cert-manager v1.7.2 using Helm 3. First, it applies the CRDs separately, then adds the Jetstack Helm repository, updates the cache, and finally installs the chart into the `cert-manager` namespace. Requires Helm 3 installed and the `cert-manager` namespace must be created beforehand.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/2_deploy_nifikop/1_quick_start.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Install CustomResourceDefinitions first\nkubectl apply --validate=false -f \\\n   https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.crds.yaml\n\n# Add the jetstack helm repo\nhelm repo add jetstack https://charts.jetstack.io\nhelm repo update\n\n# You have to create the namespace before executing following command\nhelm install cert-manager \\\n    --namespace cert-manager \\\n    --version v1.7.2 jetstack/cert-manager\n```\n\n----------------------------------------\n\nTITLE: Deleting Nifikop CRDs Manually\nDESCRIPTION: Manually delete the Custom Resource Definitions (CRDs) created by Nifikop. This is typically done after uninstalling the Helm chart if you need to clean up CRDs. Be cautious, as deleting CRDs will delete ALL custom resources (like NifiClusters) managed by those definitions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete crd nificlusters.nifi.konpyutaika.com\nkubectl delete crd nifiusers.nifi.konpyutaika.com\nkubectl delete crd nifiusergroups.nifi.konpyutaika.com\nkubectl delete crd nifiregistryclients.nifi.konpyutaika.com\nkubectl delete crd nifiparametercontexts.nifi.konpyutaika.com\nkubectl delete crd nifidataflows.nifi.konpyutaika.com\n```\n\n----------------------------------------\n\nTITLE: Checking Node Graceful Scaledown Status | Console\nDESCRIPTION: This `kubectl describe` command displays the detailed status of the `simplenifi` NifiCluster Custom Resource. It shows the `Status.NodesState` section, where the NiFiKop operator reports the progress of the graceful decommissioning steps for each node, including the `Graceful Action State`. Useful for observing the scale-down workflow.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_5\n\nLANGUAGE: console\nCODE:\n```\nkubectl describe nificluster simplenifi\n\n...\nStatus:\n  Nodes State:\n    ...\n    2:\n      Configuration State:  ConfigInSync\n      Graceful Action State:\n        Action State:   GracefulDownscaleRequired\n        Error Message:\n    ...\n...\n```\n\n----------------------------------------\n\nTITLE: Creating NifiParameterContext YAML\nDESCRIPTION: This YAML snippet defines a NifiParameterContext CRD, used to configure parameters for the dataflow.  It includes a description, a reference to a NiFi cluster, and references to secrets which will be converted to sensitive parameters within NiFi.  This allows for storing sensitive information safely.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\n  namespace: demo\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      value: toto\n      description: toto\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFiDataflow Kubernetes CRD in YAML with Sync and Update Strategies\nDESCRIPTION: Defines a NiFiDataflow custom resource describing a NiFi versioned flow deployment, including parent process group ID, flow bucket and version information, sync mode, and update strategy. It references the cluster, the NiFiRegistryClient, and the NiFiParameterContext resources. The spec.SyncMode controls operator behavior to manage dataflow lifecycle, with options to never sync, sync once, or always synchronize.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiDataflow\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  parentProcessGroupID: \"16cfd2ec-0174-1000-0000-00004b9b35cc\"\n  bucketId: \"01ced6cc-0378-4893-9403-f6c70d080d4f\"\n  flowId: \"9b2fb465-fb45-49e7-94fe-45b16b642ac9\"\n  flowVersion: 2\n  syncMode: always\n  skipInvalidControllerService: true\n  skipInvalidComponent: true\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  parameterContextRef:\n    name: dataflow-lifecycle\n    namespace: demo\n  updateStrategy: drain\n```\n\n----------------------------------------\n\nTITLE: Defining Node Configuration Groups in YAML\nDESCRIPTION: This YAML snippet demonstrates how to define node configuration groups within a NiFiKop configuration. It showcases configurations for different resource requirements, including memory and CPU limits and requests, provenance storage, and service account names. The configurations are designed to specify technical requirements for the pods.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  # nodeConfigGroups specifies multiple node configs with unique name\n  nodeConfigGroups:\n    default_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      serviceAccountName: \"default\"\n      # resourceRequirements works exactly like Container resources, the user can specify the limit and the requests\n      # through this property\n      # https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 3Gi\n        requests:\n          cpu: \"1\"\n          memory: 3Gi\n    high_mem_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      serviceAccountName: \"default\"\n      # resourceRequirements works exactly like Container resources, the user can specify the limit and the requests\n      # through this property\n      # https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/\n      resourcesRequirements:\n        limits:\n          cpu: \"2\"\n          memory: 30Gi\n        requests:\n          cpu: \"1\"\n          memory: 30Gi\n```\n\n----------------------------------------\n\nTITLE: Creating Custom StorageClass for Persistent Volumes\nDESCRIPTION: This YAML manifest defines a custom StorageClass named 'exampleStorageclass' with parameters suitable for a cloud environment, using the 'WaitForFirstConsumer' volume binding mode. It enables tailored volume provisioning aligned with specific storage types.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/2_setup/1_getting_started.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: exampleStorageclass\nparameters:\n  type: pd-standard\nprovisioner: kubernetes.io/gce-pd\nreclaimPolicy: Delete\nvolumeBindingMode: WaitForFirstConsumer\n```\n\n----------------------------------------\n\nTITLE: Build and Install Kubectl NiFiKop Plugin\nDESCRIPTION: This command builds the kubectl-nifikop plugin and copies the executable to /usr/local/bin, making it accessible from the command line.  It requires make to be installed.  It assumes the current working directory is the root directory of the nifikop project.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/2_deploy_nifikop/3_kubectl_plugin.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nmake kubectl-nifikop && sudo cp ./bin/kubectl-nifikop /usr/local/bin\n```\n\n----------------------------------------\n\nTITLE: Create cert-manager Issuer for Let's Encrypt\nDESCRIPTION: This YAML snippet defines a cert-manager Issuer resource to use Let's Encrypt for generating certificates. It specifies the ACME server, email address, private key secret, and a solver for HTTP01 challenges using nginx ingress. External DNS annotations are used to manage DNS records.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: cert-manager.io/v1alpha2\nkind: Issuer\nmetadata:\n  name: letsencrypt-staging\nspec:\n  acme:\n    # You must replace this email address with your own.\n    # Let's Encrypt will use this to contact you about expiring\n    # certificates, and issues related to your account.\n    email: <your email address>\n    server: https://acme-staging-v02.api.letsencrypt.org/directory\n    privateKeySecretRef:\n      # Secret resource used to store the account's private key.\n      name: example-issuer-account-key\n    # Add a single challenge solver, HTTP01 using nginx\n    solvers:\n      - http01:\n          ingress:\n            ingressTemplate:\n              metadata:\n                annotations:\n                  \"external-dns.alpha.kubernetes.io/ttl\": \"5\"\n```\n\n----------------------------------------\n\nTITLE: Inspecting Externally Exposed Services with kubectl - Console\nDESCRIPTION: This console snippet demonstrates how to use the kubectl CLI tool to list Kubernetes services, verifying proper external exposure of NiFi endpoints via LoadBalancer services. It outputs NAME, TYPE, CLUSTER-IP, EXTERNAL-IP, PORT(S), and AGE for each service. No dependencies other than kubectl installed and configured to point to the appropriate cluster. Output includes the expected availability and public IP addresses of exposed NiFi services.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nkubectl get services\nNAME                TYPE           CLUSTER-IP    EXTERNAL-IP      PORT(S)                                AGE\ncluster-access      LoadBalancer   10.88.21.98   35.180.241.132   443:30421/TCP,80:30521/TCP             20d\n```\n\n----------------------------------------\n\nTITLE: Configuring package.json Script for Running the NiFiKop CRD Migration Node.js Script\nDESCRIPTION: Modification of the package.json file to include a script entry allowing convenient execution of the migration script using 'npm start'. The start script runs 'index.js' using node with warnings disabled. This configuration enables easy invocation of the migration logic.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"start\": \"node --no-warnings index.js\"\n```\n\n----------------------------------------\n\nTITLE: Initializing Node.js Project and Installing Dependencies - Bash\nDESCRIPTION: This snippet initializes a new Node.js project with default settings using npm and installs the @kubernetes/client-node and minimist packages as dependencies. These libraries enable Kubernetes API interaction and command-line parsing in the migration script. Ensure Node.js (v15.3.0+) and npm (v7.0.14+) are installed before running these commands.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm init -y\nnpm install @kubernetes/client-node@0.16.3 minimist@1.2.6\n```\n\n----------------------------------------\n\nTITLE: ConfigmapReference Structure in Markdown\nDESCRIPTION: This table defines the structure for referencing ConfigMap resources in Kubernetes, specifying the name, namespace, and specific data key to use within the ConfigMap.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/5_references/1_nifi_cluster/2_read_only_config.md#_snippet_6\n\nLANGUAGE: markdown\nCODE:\n```\n## ConfigmapReference\n\n|Field|Type|Description|Required|Default|\n|-----|----|-----------|--------|--------|\n|name|string|Name of the configmap that we want to refer.|Yes|\"\"|\n|namespace|string|Namespace where is located the configmap that we want to refer.|No|\"\"|\n|data|string|The key of the value,in data content, that we want use.|Yes|\"\"|\n```\n\n----------------------------------------\n\nTITLE: Creating Istio DestinationRule for HTTPS Traffic with Sticky Session\nDESCRIPTION: Defines a DestinationRule named 'nifi-dr' that enforces TLS mode 'SIMPLE' for secure communication and enables sticky sessions via an HTTP cookie named '__Secure-Authorization-Bearer'. This ensures session affinity is maintained based on the specified cookie.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: DestinationRule\nmetadata:\n  name: nifi-dr\nspec:\n  host: <service-name>.<namespace>.svc.cluster.local\n  trafficPolicy:\n    tls:\n      mode: SIMPLE\n    loadBalancer:\n      consistentHash:\n        httpCookie:\n          name: __Secure-Authorization-Bearer\n          ttl: 0s\n```\n\n----------------------------------------\n\nTITLE: Starting Terraform-based GKE Cluster Deployment Script Using Shell\nDESCRIPTION: This shell command executes the start.sh script with a required service account key file path as an argument to initiate the Terraform deployment of the GKE cluster. The script automates the creation and configuration of GKE nodes and associated resources. Users must configure deployment variables beforehand in the provided TFVars file for customization.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/blog/2020-05-20-secured_nifi_cluster_on_gcp.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n./start.sh <service account key's path>\n```\n\n----------------------------------------\n\nTITLE: Defining Nifi External Services Configuration in YAML\nDESCRIPTION: This snippet provides an example YAML configuration for defining a Nifi external service named \"clusterip\" with a ClusterIP service type. The service exposes port 8080 mapped to an internal listener named \"http\" and includes metadata specifying annotations and labels. This example demonstrates how to declare service types and port configurations used by Nifi listeners in a Kubernetes context.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/5_references/1_nifi_cluster/7_external_service_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  externalServices:\n    - name: \"clusterip\"\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8080\n            internalListenerName: \"http\"\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n\n----------------------------------------\n\nTITLE: Implementing OIDC Configuration in NiFiCluster Custom Resource\nDESCRIPTION: Example of configuring OIDC authentication in a NiFiCluster custom resource using the overrideConfigs field to set the required properties for OpenId Connect.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/5_users_authentication/1_oidc.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\\nkind: NifiCluster\\n...\\nspec:\\n  ...\\n  readOnlyConfig:\\n    # NifiProperties configuration that will be applied to the node.\\n    nifiProperties:\\n      webProxyHosts:\\n        - nifistandard2.trycatchlearn.fr:8443\\n      # Additionnal nifi.properties configuration that will override the one produced based\\n      # on template and configurations.\\n      overrideConfigs: |\\n        nifi.security.user.oidc.discovery.url=<oidc server discovery url>\\n        nifi.security.user.oidc.client.id=<oidc client's id>\\n        nifi.security.user.oidc.client.secret=<oidc client's secret>\\n        nifi.security.identity.mapping.pattern.dn=CN=([^,]*)(?:, (?:O|OU)=.*)?\\n        nifi.security.identity.mapping.value.dn=$1\\n        nifi.security.identity.mapping.transform.dn=NONE\\n      ...\\n   ...\\n...\n```\n\n----------------------------------------\n\nTITLE: Creating NifiUser for Client Certificates via Kubectl\nDESCRIPTION: Applies a `NifiUser` custom resource definition (CRD) to a Kubernetes cluster using `kubectl`. This resource instructs the NiFi operator to create a new client certificate signed by the NiFi cluster's CA and store the credentials in a specified Kubernetes secret (`example-client-secret`) within the `nifi` namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_3\n\nLANGUAGE: console\nCODE:\n```\ncat << EOF | kubectl apply -n nifi -f -\napiVersion:  nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: example-client\n  namespace: nifi\nspec:\n  clusterRef:\n    name: nifi\n  secretName: example-client-secret\nEOF\n```\n\n----------------------------------------\n\nTITLE: Advanced Storage Directory Configuration for High-Performance NiFi Installations\nDESCRIPTION: Demonstrates how to customize content and provenance repositories by setting multiple directories via overrideConfigs within the NiFiCluster configuration YAML. It also shows how to define corresponding PersistentVolumeClaims in node storage groups, specifying mount paths, labels, annotations, and storage requests for scalable high-performance setups.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_5\n\nLANGUAGE: YAML\nCODE:\n```\n... \n  readOnlyConfig:\n    nifiProperties:\n      overrideConfigs: |\n        nifi.content.repository.directory.dir1=../content-additional/dir1\n        nifi.content.repository.directory.dir2=../content-additional/dir2\n        nifi.content.repository.directory.dir3=../content-additional/dir3\n        nifi.provenance.repository.directory.dir1=../provenance-additional/dir1\n        nifi.provenance.repository.directory.dir2=../provenance-additional/dir2\n...\n  nodeConfigGroups:\n    default_group:\n      ...\n      storageConfigs:\n      - mountPath: \"/opt/nifi/content-additional/dir1\"\n        name: content-repository-dir1\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/content-additional/dir2\"\n        name: content-repository-dir2\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/content-additional/dir3\"\n        name: content-repository-dir3\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/provenance-additional/dir1\"\n        name: provenance-repository-dir1\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n      - mountPath: \"/opt/nifi/provenance-additional/dir2\"\n        name: provenance-repository-dir2\n        metadata:\n          labels:\n            my-label: my-value\n          annotations:\n            my-annotation: my-value\n        pvcSpec:\n          accessModes:\n            - ReadWriteOnce\n          storageClassName: {{ storageClassName }}\n          resources:\n            requests:\n              storage: 100G\n...\n```\n\n----------------------------------------\n\nTITLE: Configuring CRD Conversion Webhook (YAML)\nDESCRIPTION: Provides a YAML snippet demonstrating how to configure a Nifikop CRD to use a conversion webhook. This involves adding annotations for `cert-manager` CA injection and defining the `conversion` strategy and webhook client configuration, referencing the correct namespace, certificate name, and webhook service name.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n```\n\n----------------------------------------\n\nTITLE: Defining External Services with ClusterIP in YAML\nDESCRIPTION: This YAML snippet defines an external service named \"clusterip\" with ClusterIP type, specifying multiple port configurations and associated metadata such as annotations and labels. It is used to configure internal service access within a Kubernetes cluster, focusing on TCP protocols and internal listener mappings.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/5_references/1_nifi_cluster/7_external_service_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  externalServices:\n    - name: \"clusterip\"\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8080\n            internalListenerName: \"http\"\n          - port: 7182\n            internalListenerName: \"my-custom-listener\"\n            protocol: TCP\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n\n----------------------------------------\n\nTITLE: NifiNodeGroupAutoscaler Resource Schema\nDESCRIPTION: This section describes the schema for the NifiNodeGroupAutoscaler resource, including its metadata, specification, and status. It defines the necessary fields such as cluster reference, node configuration details, scaling strategies, and current state indicators, providing comprehensive configuration options for autoscaling NiFi node groups.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/5_references/7_nifi_nodegroup_autoscaler.md#_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n{\n  \"NifiNodeGroupAutoscaler\": {\n    \"metadata\": \"ObjectMetadata\",\n    \"spec\": \"NifiNodeGroupAutoscalerSpec\",\n    \"status\": \"NifiNodeGroupAutoscalerStatus\"\n  }\n}\n\n// Fields include:\n// - metadata: resource metadata\n// - spec: desired autoscaling settings\n// - status: current autoscaling state and metrics\n```\n\n----------------------------------------\n\nTITLE: GKE Cluster Configuration\nDESCRIPTION: This command retrieves the configuration for the GKE cluster and sets it as the current context for kubectl. The provided parameters include the cluster name, zone, and project ID. This allows subsequent kubectl commands to interact with the newly deployed Kubernetes cluster. Before running, replace the placeholder values with the actual GKE cluster configuration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/blog/2020-06-30-secured_nifi_cluster_on_gcp_with_external_dns.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\ngcloud container clusters get-credentials nifi-cluster --zone <configured gcp zone> --project <GCP project's id>\n```\n\n----------------------------------------\n\nTITLE: Listing Deployed HPAs for NiFi Autoscaling - console\nDESCRIPTION: Lists all deployed HorizontalPodAutoscaler (HPA) resources in the 'clusters' namespace, showing their targets, replica counts, and current status. Requires kubectl command-line access to the cluster. Input is the target namespace; output is a summary table of HPA configurations for NiFi autoscaler.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_14\n\nLANGUAGE: console\nCODE:\n```\nkubectl get -n clusters hpa\nNAME                REFERENCE                                                TARGETS         MINPODS   MAXPODS   REPLICAS   AGE\nkeda-hpa-cluster    NifiNodeGroupAutoscaler/nifinodegroupautoscaler-sample     1833m/2 (avg)   1         3         2          25d\n\n```\n\n----------------------------------------\n\nTITLE: NifiUserSpec YAML Schema for User Configuration\nDESCRIPTION: This YAML snippet details the schema for the NifiUserSpec, including fields like identity, secretName, clusterRef, DNSNames, includeJKS, createCert, and accessPolicies. It specifies user identity, security options, cluster linkage, DNS configurations, and access policies.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/5_references/2_nifi_user.md#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\n`spec:\n  identity: string\n  secretName: string\n  clusterRef:\n    name: string\n    namespace: string\n  DNSNames:\n    - string\n  includeJKS: boolean\n  createCert: boolean\n  accessPolicies:\n    - \n      type: [AccessPolicyType]\n      action: [AccessPolicyAction]\n      resource: [AccessPolicyResource]\n      componentType: string\n      componentId: string`\n```\n\n----------------------------------------\n\nTITLE: Building NiFiKop Locally\nDESCRIPTION: This command builds the NiFiKop operator in your local Go environment. It compiles the Go code and creates an executable binary.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/6_contributing/1_developer_guide.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmake build\n```\n\n----------------------------------------\n\nTITLE: Apply NiFiKop CRDs using kubectl\nDESCRIPTION: This snippet applies the NiFiKop Custom Resource Definitions (CRDs) using kubectl. This is necessary when deploying NiFiKop without using Helm's CRD management feature (`--skip-crds`). It applies each CRD YAML file from the NiFiKop GitHub repository.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/2_deploy_nifikop/1_quick_start.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\n```\n\n----------------------------------------\n\nTITLE: Pushing Changes to Remote Repository\nDESCRIPTION: This code details the steps needed to push local changes to a remote repository. It includes formatting the code, staging the changes, committing with a message, and pushing the branch to a fork. This is essential for contributing changes back to the React Native website project.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/README.md#_snippet_4\n\nLANGUAGE: Shell\nCODE:\n```\n1.  Run `yarn prettier` to ensure your changes are consistent with other files in the repo.\n1.  `git add -A && git commit -m \"My message\"` to stage and commit your changes.\n    > replace `My message` with a commit message, such as `Fixed header logo on Android`\n1.  `git push my-fork-name the-name-of-my-branch`\n```\n\n----------------------------------------\n\nTITLE: Building NiFiKop locally\nDESCRIPTION: Builds the NiFiKop application using the make command. This is typically done in a local development environment where the Go toolchain is available.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/6_contributing/1_developer_guide.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmake build\n```\n\n----------------------------------------\n\nTITLE: Verifying NiFiKop Operator Pods\nDESCRIPTION: This command retrieves the pods in the 'nifikop' namespace, allowing verification that the operator is running correctly after deploying with Helm.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/6_contributing/1_developer_guide.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n$ kubectl get pods -n nifikop\nNAME                                                READY   STATUS    RESTARTS   AGE\nskeleton-nifikop-8946b89dc-4cfs9   1/1     Running   0          7m45s\n```\n\n----------------------------------------\n\nTITLE: Updating Helm Chart Dependencies (Directory) - Shell\nDESCRIPTION: Executes the `helm dependency update` command within a specific Helm chart directory to download and update the chart's required dependencies. This command fetches charts from configured repositories and stores them locally. It requires the Helm CLI to be installed and run from inside the target chart's root directory.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/helm/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nhelm dependency update\n```\n\n----------------------------------------\n\nTITLE: Getting Zookeeper UID/GID for OpenShift Deployment\nDESCRIPTION: Extracts the required UID/GID values from the OpenShift namespace annotations for configuring Zookeeper security context, ensuring proper permissions in the OpenShift environment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nzookeper_uid=$(kubectl get namespace zookeeper -o=jsonpath='{.metadata.annotations.openshift\\.io/sa\\.scc\\.supplemental-groups}' | sed 's/\\/10000$//' | tr -d '[:space:]')\n```\n\n----------------------------------------\n\nTITLE: Purging Helm Release - Helm (bash)\nDESCRIPTION: This command purges the Helm release named \"nifikop\".  Purging permanently removes the release and its associated history from Helm's records. This operation requires Helm CLI. The input is the release name. The output is the complete removal of the specified chart release.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nhelm delete --purge nifikop\n```\n\n----------------------------------------\n\nTITLE: Purging Helm Release - Bash\nDESCRIPTION: This command purges the Helm release 'nifikop', permanently removing it and its associated records from Helm's history.  This makes the release name available for reuse. It's important to understand this action is not reversible and any rollback ability is lost.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/2_setup/3_install/1_customizable_install_with_helm.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nhelm delete --purge nifikop\n```\n\n----------------------------------------\n\nTITLE: Uninstalling Nifikop Helm Chart - Bash\nDESCRIPTION: Deletes the Nifikop Helm release including all associated Kubernetes resources, except for CRDs that must be removed manually. Dependencies: Helm CLI and delete permission on the namespace. Takes the release name as its argument.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nhelm del nifikop\n```\n\n----------------------------------------\n\nTITLE: Manually Deleting Kubernetes CRDs Using Bash\nDESCRIPTION: This Bash snippet instructs on removing all nifikop-related CRDs from the Kubernetes cluster manually using kubectl delete commands. Caution is advised as deleting CRDs will delete all custom resources created under those CRDs permanently.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete crd nificlusters.nifi.konpyutaika.com\nkubectl delete crd nifiusers.nifi.konpyutaika.com\nkubectl delete crd nifiusergroups.nifi.konpyutaika.com\nkubectl delete crd nifiregistryclients.nifi.konpyutaika.com\nkubectl delete crd nifiparametercontexts.nifi.konpyutaika.com\nkubectl delete crd nifidataflows.nifi.konpyutaika.com\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFiKop CRD Conversion Webhook Annotations (YAML)\nDESCRIPTION: Example YAML snippet showing the required annotations and spec conversion configuration for NiFiKop CRDs when the conversion webhook (for converting v1alpha1 to v1 resources) is enabled. Requires cert-manager for CA injection.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/2_deploy_nifikop/1_quick_start.md#_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n```\n\n----------------------------------------\n\nTITLE: Installing Prometheus Operator with Custom Settings via Helm - console\nDESCRIPTION: Helm command to install the Prometheus operator and kube-prometheus-stack components in the 'monitoring-system' namespace, disabling most components except core operator and setting custom resource handling and logging options. This setup provides a tailored Prometheus operator deployment focused on required features.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_6\n\nLANGUAGE: console\nCODE:\n```\nhelm install prometheus prometheus/kube-prometheus-stack --namespace monitoring-system \\\n    --set prometheusOperator.createCustomResource=false \\\n    --set prometheusOperator.logLevel=debug \\\n    --set prometheusOperator.alertmanagerInstanceNamespaces=monitoring-system \\\n    --set prometheusOperator.namespaces.additional[0]=monitoring-system \\\n    --set prometheusOperator.prometheusInstanceNamespaces=monitoring-system \\\n    --set prometheusOperator.thanosRulerInstanceNamespaces=monitoring-system \\\n    --set defaultRules.enable=false \\\n    --set alertmanager.enabled=false \\\n    --set grafana.enabled=false \\\n    --set kubeApiServer.enabled=false \\\n    --set kubelet.enabled=false \\\n    --set kubeControllerManager.enabled=false \\\n    --set coreDNS.enabled=false \\\n    --set kubeEtcd.enabled=false \\\n    --set kubeScheduler.enabled=false \\\n    --set kubeProxy.enabled=false \\\n    --set kubeStateMetrics.enabled=false \\\n    --set prometheus.enabled=false\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFi Cluster Resource via kubectl Command (Console)\nDESCRIPTION: This command deploys the NiFiCluster resource defined in YAML to the Kubernetes cluster using kubectl. Prerequisites: a valid and configured NiFi cluster YAML, access to a working Kubernetes context, and sufficient permissions. The primary parameter is the file path to the YAML resource. It creates the NiFi cluster custom resource; input is the path to the resource file. Errors will result if dependencies or configurations are incomplete.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/blog/2020-06-30-secured_nifi_cluster_on_gcp_with_external_dns.md#_snippet_16\n\nLANGUAGE: console\nCODE:\n```\nkubectl create -f kubernetes/nifi/secured_nifi_cluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Applying NiFiKop Custom Resource Definitions (bash)\nDESCRIPTION: This series of kubectl commands applies the YAML definition files for all NiFiKop Custom Resources (CRDs) to the target Kubernetes cluster. These CRDs define the custom API objects that the NiFiKop operator will manage, such as NiFiClusters and NiFiDataflows. This step must be completed before deploying the operator itself.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/6_contributing/1_developer_guide.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\n```\n\n----------------------------------------\n\nTITLE: Checking Node Decommissioning Status in Kubernetes\nDESCRIPTION: A command example showing how to check the status of a node being decommissioned, displaying the GracefulActionState from the NiFiCluster resource.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nkubectl describe nificluster simplenifi\n\n...\nStatus:\n  Nodes State:\n    ...\n    2:\n      Configuration State:  ConfigInSync\n      Graceful Action State:\n        Action State:   GracefulDownscaleRequired\n        Error Message:\n    ...\n...\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop Helm Chart (Dry Run)\nDESCRIPTION: This command performs a dry run installation of the NiFiKop Helm chart. It sets the log level to Debug and specifies the namespace.  The `--dry-run` flag simulates the installation without actually deploying the resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/2_setup/3_install/1_customizable_install_with_helm.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop \\\n    --dry-run \\\n    --set logLevel=Debug \\\n    --set namespaces={\\\"nifikop\\\"}\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for IDE - Bash\nDESCRIPTION: This snippet defines environment variables required for the development IDE to connect to a Kubernetes cluster. It sets the `KUBECONFIG` path, `WATCH_NAMESPACE`, `POD_NAME`, `LOG_LEVEL`, and `OPERATOR_NAME` variables. These variables configure the operator's behavior and connection to the cluster. The `KUBECONFIG` variable specifies the path to your kubeconfig file.  `WATCH_NAMESPACE` configures the namespace to watch and `POD_NAME` is used for identifying the operator pod.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/6_contributing/1_developer_guide.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nKUBECONFIG={path/to/your/kubeconfig}\nWATCH_NAMESPACE={namespace_to_watch}\nPOD_NAME={name for operator pod}\nLOG_LEVEL=Debug\nOPERATOR_NAME=ide\n```\n\n----------------------------------------\n\nTITLE: Manually Deleting CRDs\nDESCRIPTION: Commands to delete CRDs associated with Nifikop manually from the cluster, necessary if CRDs were not removed during Helm uninstall. Be cautious as this deletes all clusters created via these CRDs.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete crd nificlusters.nifi.konpyutaika.com\nkubectl delete crd nifiusers.nifi.konpyutaika.com\nkubectl delete crd nifiusergroups.nifi.konpyutaika.com\nkubectl delete crd nifiregistryclients.nifi.konpyutaika.com\nkubectl delete crd nifiparametercontexts.nifi.konpyutaika.com\nkubectl delete crd nifidataflows.nifi.konpyutaika.com\n```\n\n----------------------------------------\n\nTITLE: Manually Deleting Nifikop CRDs with kubectl\nDESCRIPTION: Provides the `kubectl delete crd` commands to manually remove specific Nifikop Custom Resource Definitions from the Kubernetes cluster. This action is performed with caution after uninstalling the operator, as it will delete all associated custom resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete crd nificlusters.nifi.konpyutaika.com\nkubectl delete crd nifiusers.nifi.konpyutaika.com\nkubectl delete crd nifiusergroups.nifi.konpyutaika.com\nkubectl delete crd nifiregistryclients.nifi.konpyutaika.com\nkubectl delete crd nifiparametercontexts.nifi.konpyutaika.com\nkubectl delete crd nifidataflows.nifi.konpyutaika.com\n```\n\n----------------------------------------\n\nTITLE: Getting Status of Nifikop Helm Deployment - Bash\nDESCRIPTION: Retrieves the deployment status of the Nifikop Helm release, including manifest, resources, and pending hooks. Helm CLI must be configured and nifikop release must exist. Accepts the release name as its main parameter and outputs relevant status details.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nhelm status nifikop\n```\n\n----------------------------------------\n\nTITLE: Secret Creation for Basic Authentication in NiFi Cluster\nDESCRIPTION: This bash command creates a Kubernetes secret named 'nifikop-credentials' containing username, password, and optional CA certificate files for basic auth. These secrets are used by the operator to authenticate API requests to the external NiFi cluster, ensuring secure credential management.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/3_tasks/1_nifi_cluster/4_external_cluster.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create secret generic nifikop-credentials \\\n  --from-file=username=./secrets/username\\\n  --from-file=password=./secrets/password\\\n  --from-file=ca.crt=./secrets/ca.crt\\\n  -n nifikop-nifi\n```\n\n----------------------------------------\n\nTITLE: Port Forwarding Prometheus Service to Localhost Using kubectl Console Command\nDESCRIPTION: This console command sets up a port-forward from the Prometheus service 'prometheus-operated' running in the 'monitoring-system' namespace to the local machine, exposing port 9090. It enables local access to the Prometheus UI at http://localhost:9090 for querying NiFi metrics and validating the monitoring setup. Requires kubectl configured with access to the cluster and port 9090 available locally.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_6\n\nLANGUAGE: console\nCODE:\n```\nkubectl port-forward -n monitoring-system service/prometheus-operated 9090:9090\n```\n\n----------------------------------------\n\nTITLE: NiFi Operator External Service Definition YAML\nDESCRIPTION: This YAML snippet is part of a NiFi cluster deployment specification (`spec`). It defines an `externalService` of type `ClusterIP` named \"nifi-cluster\", exposing port 8443 and mapping it to the internal \"https\" listener of the NiFi nodes. This service definition is the intended target for the Istio VirtualService configuration when using HTTPS.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_4\n\nLANGUAGE: YAML\nCODE:\n```\nspec:  \n  externalServices:  \n    - name: \"nifi-cluster\"\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8443\n            internalListenerName: \"https\"\n```\n\n----------------------------------------\n\nTITLE: Adding KEDA Helm Repository with Console Commands\nDESCRIPTION: Adds the official KEDA Helm chart repository and updates the local Helm repositories index. Required dependencies are Helm (v3+) installed and configured. These commands download chart metadata, facilitating the installation or upgrade of KEDA in the next steps. Inputs are the repository command strings; outputs are the updated Helm repo list on the user's system.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nhelm repo add kedacore https://kedacore.github.io/charts\nhelm repo update\n```\n\n----------------------------------------\n\nTITLE: Authenticating kubectl with GKE Cluster - Console\nDESCRIPTION: This command retrieves cluster credentials for a GKE-managed Kubernetes cluster so kubectl can authenticate and manage the cluster. It requires Google Cloud SDK ('gcloud') to be installed and authenticated, with appropriate project, cluster, and zone parameters provided. The command configures local kubectl access by updating its kubeconfig context.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/2_deploy_nifikop/1_quick_start.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\ngcloud container clusters get-credentials CLUSTER_NAME --zone ZONE_NAME --project PROJECT_NAME.\n```\n\n----------------------------------------\n\nTITLE: NifiCluster Init Container Configuration (busybox)\nDESCRIPTION: This YAML snippet shows the configuration of a NifiCluster resource with the `initContainerImage` specified as `busybox`. This configuration needs to be updated when upgrading to Nifikop v0.15.0 due to the change in the default init container image. The snippet defines the API version, kind, metadata (name), and the initContainerImage repository and tag.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/7_upgrade_guides/2_v0.14.1_to_v0.15.0.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  initContainerImage:\n    repository: busybox\n    tag: \"1.34.0\"\n```\n\n----------------------------------------\n\nTITLE: Applying NifiCluster Configuration (Shell)\nDESCRIPTION: Uses the `kubectl apply` command to submit the updated `NifiCluster` YAML manifest to the Kubernetes API server. This instructs the NiFiKop operator to reconcile the cluster state based on the new definition, initiating a scale-up or scale-down action as defined in the YAML file. This command assumes `kubectl` is configured and points to the target Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nkubectl -n nifi apply -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio Gateway for HTTPS Traffic Termination\nDESCRIPTION: Defines an Istio Gateway resource named 'nifi-gateway' configured for HTTPS. It listens on port 443 for traffic to 'nifi.my-domain.com', terminates TLS using SIMPLE mode with credentials from the Kubernetes secret 'my-secret', and forwards traffic as HTTP internally.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: nifi-gateway\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 443\n      name: https\n      protocol: HTTPS\n    tls:\n      mode: SIMPLE\n      credentialName: my-secret\n    hosts:\n    - nifi.my-domain.com\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop Helm Chart with Parameter Override (Bash)\nDESCRIPTION: This command installs the NiFiKop Helm chart and overrides the `namespaces` parameter via `--set namespaces={\"nifikop\"}`. It allows quick custom configuration on install without editing a YAML values file. Helm v3+ must be available, and an appropriate Kubernetes context selected. The command does not validate the parameter at runtime.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/2_setup/3_install/1_customizable_install_with_helm.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop --set namespaces={\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Namespace and Operator YAML Definitions for NiFi Deployment\nDESCRIPTION: The 'namespace.yaml' specifies the Kubernetes namespace dedicated to the NiFi deployment, facilitating resource organization and isolation. The 'operator.yaml' installs the NiFi operator and all related CustomResourceDefinitions (CRDs), enabling management of NiFi clusters via Kubernetes manifests. These configurations must be applied separately from Helm Kustomize deployments for proper resource initialization.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/config/samples/keycloak-example/README.md#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\nnamespace.yaml\n```\n\nLANGUAGE: YAML\nCODE:\n```\noperator.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing cert-manager with Helm 3 (Bash)\nDESCRIPTION: This code snippet installs cert-manager using Helm 3. It first installs the CustomResourceDefinitions, then adds the jetstack helm repository, updates the repository, and finally installs cert-manager.  It requires Helm 3, a configured Kubernetes cluster, and a namespace named \"cert-manager\".\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/2_deploy_nifikop/1_quick_start.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Install CustomResourceDefinitions first\nkubectl apply --validate=false -f \\\n   https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.crds.yaml\n\n# Add the jetstack helm repo\nhelm repo add jetstack https://charts.jetstack.io\nhelm repo update\n\n# You have to create the namespace before executing following command\nhelm install cert-manager \\\n    --namespace cert-manager \\\n    --version v1.7.2 jetstack/cert-manager\n```\n\n----------------------------------------\n\nTITLE: CRD Conversion Webhook Configuration Patch in YAML\nDESCRIPTION: This YAML snippet provides the patch required for CRDs to enable conversion webhooks for resource version migration. It sets the conversion strategy to Webhook and specifies the webhook client configuration including services and review versions, necessary for seamless resource conversion during upgrades.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/7_upgrade_guides/3_v0.16.0_to_v1.0.0.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n```\n\n----------------------------------------\n\nTITLE: Installing KEDA Helm Chart in Kubernetes Namespace with Console\nDESCRIPTION: This snippet first creates a separate 'keda' namespace in your Kubernetes cluster and then installs the KEDA Helm chart into that namespace. Helm and kubectl must be installed and configured with appropriate cluster credentials. The inputs are the chosen namespace and chart name, and the output is the deployed KEDA controller under the specified Kubernetes namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl create namespace keda\nhelm install keda kedacore/keda --namespace keda\n```\n\n----------------------------------------\n\nTITLE: Purging Nifikop Helm Release History (Bash)\nDESCRIPTION: This command deletes the specified Helm release 'nifikop' and also removes its history from Helm's storage. Using the `--purge` flag permanently removes the release record, making the release name available for reuse immediately. This differs from `helm delete`, which only marks the release as deleted but preserves history.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/2_setup/3_install/1_customizable_install_with_helm.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nhelm delete --purge nifikop\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiParameterContext with secrets in YAML\nDESCRIPTION: This YAML configuration creates a NifiParameterContext resource with parameters and references to secrets for sensitive data, including description, clusterRef, secretRefs, and parameters. Secrets are stored separately and referenced for sensitive parameters, enhancing security.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/3_tasks/3_nifi_dataflow.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\n  namespace: demo\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      value: toto\n      description: toto\n```\n\n----------------------------------------\n\nTITLE: Retrieving OpenShift Namespace UID Annotation Using kubectl Bash Command\nDESCRIPTION: Extracts the OpenShift namespace annotation for supplemental group IDs (openshift.io/sa.scc.supplemental-groups), removes the trailing \"/10000\" suffix, and strips whitespace to obtain the UID required for Pod Security Context Constraints when deploying NiFiKop on OpenShift.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/2_deploy_nifikop/1_quick_start.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nuid=$(kubectl get namespace nifi -o=jsonpath='{.metadata.annotations.openshift\\.io/sa\\.scc\\.supplemental-groups}' | sed 's/\\/10000$//' | tr -d '[:space:]')\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiRegistryClient CRD in YAML\nDESCRIPTION: This YAML snippet defines a NifiRegistryClient custom resource, which is a prerequisite for managing dataflows with NiFiKop. It specifies the name, namespace, cluster reference, a description, and the URI of the NiFi Registry instance. This resource allows NiFiKop to interact with the specified NiFi Registry.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiRegistryClient\nmetadata:\n  name: registry-client-example\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Registry client managed by NiFiKop\"\n  uri: \"http://nifi.hostname.com:18080\"\n```\n\n----------------------------------------\n\nTITLE: Listing Deleted Helm Releases Bash\nDESCRIPTION: This bash command lists all Helm releases that have been deleted in the current Kubernetes context. Requires the Helm CLI tool. No parameters are required, and the output is a list of deleted release entries for audit or troubleshooting purposes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/2_setup/3_install/1_customizable_install_with_helm.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --deleted\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFiKop using Helm chart\nDESCRIPTION: Command for deploying the NiFiKop operator version 0.14.1 using Helm chart with specified resource limits and namespaces configuration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/2_deploy_nifikop/1_quick_start.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# You have to create the namespace before executing following command\nhelm install nifikop \\\n    oci://ghcr.io/konpyutaika/helm-charts/nifikop \\\n    --namespace=nifi \\\n    --version 0.14.1 \\\n    --set image.tag=v0.14.1-release \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set namespaces={\"nifi\"}\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment Variables for GKE Cluster\nDESCRIPTION: This snippet exports essential environment variables such as project ID, zone, and cluster name to configure the context for subsequent GKE operations. It prepares the shell environment for cluster creation and management, requiring user-supplied values.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/2_setup/2_platform_setup/1_gke.md#_snippet_0\n\nLANGUAGE: Shell script\nCODE:\n```\nexport GCP_PROJECT=<project_id>\nexport GCP_ZONE=<zone>\nexport CLUSTER_NAME=<cluster-name>\n```\n\n----------------------------------------\n\nTITLE: Combined Internal and External Listener Configuration with Services (YAML)\nDESCRIPTION: This YAML snippet defines both internal NiFi listeners and external services in a single configuration. Internal listeners (with or without type) are listed, and the externalServices section maps these listeners to publicly accessible Kubernetes services. Required dependencies include the NiFi K8s operator and proper permissions to create LoadBalancer-type services. Key parameters are port, internalListenerName, and type under each external service definition. Expected outputs are publicly available endpoints for the specified listeners, allowing secure external access to the NiFi cluster. Constraints include matching internalListenerNames to previously declared listeners and ensuring appropriate ServiceType selection based on deployment environment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    - type: \"https\"\n      name: \"https\"\n      containerPort: 8443\n    - type: \"cluster\"\n      name: \"cluster\"\n      containerPort: 6007\n    - type: \"s2s\"\n      name: \"s2s\"\n      containerPort: 10000\n    - type: \"prometheus\"\n      name: \"prometheus\"\n      containerPort: 9090\n    - type: \"load-balance\"\n      name: \"load-balance\"\n      containerPort: 6342\n    - name: \"http-tracking\"\n      containerPort: 8081\nexternalServices:\n  - name: cluster-access\n    spec:\n      portConfigs:\n        - internalListenerName: https\n          port: 443\n        - internalListenerName: http-tracking\n          port: 80\n      type: LoadBalancer\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiUserGroup Custom Resource in Kubernetes\nDESCRIPTION: This YAML snippet defines a custom resource `NifiUserGroup` for Kubernetes that enables the management of NiFi user groups and their access policies. It specifies the target NiFi cluster, the list of users in the group, and associated access policies such as 'global' read permissions for specific resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/2_manage_users_and_accesses/2_groups_management.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  # Refers to the NiFi cluster this group belongs to\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  # List of NifiUser references included in this group\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n    - name: nc-controller.nifikop.mgt.cluster.local\n  # Access policies assigned to the group\n  accessPolicies:\n    - type: global\n      action: read\n      resource: /counters\n```\n\n----------------------------------------\n\nTITLE: Defining a Secret Reference Structure (Markdown)\nDESCRIPTION: Describes the structure used to reference a specific key within a Kubernetes Secret. Requires the Secret name (`name`) and the data key (`data`), optionally specifying the namespace (`namespace`). Used for overriding or replacing configurations, often for sensitive data.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/5_references/1_nifi_cluster/2_read_only_config.md#_snippet_7\n\nLANGUAGE: Markdown\nCODE:\n```\n## SecretConfigReference\n\n|Field|Type|Description|Required|Default|\n|-----|----|-----------|--------|--------|\n|name|string|Name of the secret that we want to refer.|Yes|\"\"|\n|namespace|string|Namespace where is located the secret that we want to refer.|No|\"\"|\n|data|string|The key of the value,in data content, that we want use.|Yes|\"\"|\n```\n\n----------------------------------------\n\nTITLE: Setting Operator Name Environment Variable\nDESCRIPTION: This command sets the OPERATOR_NAME environment variable to 'nifi-operator'. This variable is likely used by the operator during runtime.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/6_contributing/1_developer_guide.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport OPERATOR_NAME=nifi-operator\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiRegistryClient in YAML\nDESCRIPTION: This YAML snippet creates a NifiRegistryClient resource that connects NiFi to its registry, specifying the API version, kind, metadata, and spec fields like clusterRef, description, and URI. It allows NiFi to manage versioned flows via the registry.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/3_tasks/3_nifi_dataflow.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiRegistryClient\nmetadata:\n  name: registry-client-example\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Registry client managed by NiFiKop\"\n  uri: \"http://nifi.hostname.com:18080\"\n```\n\n----------------------------------------\n\nTITLE: Retrieving OpenShift Namespace UID Range\nDESCRIPTION: Command using kubectl to query the 'nifi' namespace in OpenShift and extract the allocated UID range annotation ('openshift.io/sa.scc.supplemental-groups'). This UID is needed to set the 'runAsUser' security context when deploying NiFiKop on OpenShift's restricted SCC.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/2_deploy_nifikop/1_quick_start.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nuid=$(kubectl get namespace nifi -o=jsonpath='{.metadata.annotations.openshift\\.io/sa\\.scc\\.supplemental-groups}' | sed 's/\\/10000$//' | tr -d '[:space:]')\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio HTTP VirtualService YAML\nDESCRIPTION: This YAML snippet defines an Istio VirtualService resource that binds to the `nifi-gateway`. It routes all HTTP requests (prefix `/`) for the specified host (`nifi.my-domain.com`) to the Kubernetes service named `nifi` on port 8080, directing external traffic to the NiFi service.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: nifi-vs\nspec:\n  gateways:\n  - nifi-gateway\n  hosts:\n  - nifi.my-domain.com\n  http:\n  - match:\n    - uri:\n        prefix: /\n    route:\n    - destination:\n        host: nifi\n        port:\n          number: 8080\n```\n\n----------------------------------------\n\nTITLE: Viewing Kubernetes Services via kubectl Console\nDESCRIPTION: Shows the expected output from the `kubectl get services` command after applying the external service configuration defined in the NiFiCluster resource. It demonstrates the creation of a Kubernetes service (e.g., 'cluster-access') with its type, IPs, and exposed ports mapped to node ports.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nkubectl get services\nNAME                TYPE           CLUSTER-IP    EXTERNAL-IP      PORT(S)                                AGE\ncluster-access      LoadBalancer   10.88.21.98   35.180.241.132   443:30421/TCP,80:30521/TCP             20d\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio DestinationRule for HTTPS\nDESCRIPTION: This YAML snippet configures an Istio DestinationRule for enabling HTTPS encryption and managing sticky sessions. It defines how traffic destined for the ClusterIP service is encrypted and load balanced. It sets TLS mode to SIMPLE for encryption and uses an HTTP cookie (__Secure-Authorization-Bearer) for sticky sessions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: DestinationRule\nmetadata:\n  name: nifi-dr\nspec:\n  host: <service-name>.<namespace>.svc.cluster.local\n  trafficPolicy:\n    tls:\n      mode: SIMPLE\n    loadBalancer:\n      consistentHash:\n        httpCookie:\n          name: __Secure-Authorization-Bearer\n          ttl: 0s\n```\n\n----------------------------------------\n\nTITLE: NiFi ClusterIP Service Specification for Istio HTTPS Routing in YAML\nDESCRIPTION: This snippet shows the relevant 'externalServices' section within a NiFi cluster deployment YAML. It defines a Kubernetes Service of type ClusterIP named 'nifi-cluster' (used as `<service-name>` in the VirtualService/DestinationRule) exposing port 8443, which maps to NiFi's internal 'https' listener.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nspec:  \n  externalServices:  \n    - name: \"nifi-cluster\"\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8443\n            internalListenerName: \"https\"\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom StorageClass\nDESCRIPTION: This snippet defines a custom StorageClass resource in Kubernetes. It sets the volume binding mode to `WaitForFirstConsumer`, which delays volume provisioning until a pod is scheduled on a node. This is recommended for dynamic provisioning in environments where node affinity is important. The `provisioner` parameter specifies the volume plugin to use.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: exampleStorageclass\nparameters:\n  type: pd-standard\nprovisioner: kubernetes.io/gce-pd\nreclaimPolicy: Delete\nvolumeBindingMode: WaitForFirstConsumer\n```\n\n----------------------------------------\n\nTITLE: Creating a Kubernetes Secret for Basic Authentication\nDESCRIPTION: This command creates a Kubernetes secret named `nifikop-credentials` in the `nifikop-nifi` namespace. The secret contains the username, password, and optionally the CA certificate required for basic authentication against the NiFi cluster API. The `--from-file` option reads the values from the specified files.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/1_manage_clusters/3_external_cluster.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create secret generic nifikop-credentials \\\n  --from-file=username=./secrets/username\\\n  --from-file=password=./secrets/password\\\n  --from-file=ca.crt=./secrets/ca.crt\\\n  -n nifikop-nifi\n```\n\n----------------------------------------\n\nTITLE: Listing Deployed Horizontal Pod Autoscaler - Console\nDESCRIPTION: Lists all Horizontal Pod Autoscalers (HPA) within the 'clusters' namespace, including the current scale status for NifiNodeGroupAutoscaler. Useful for verifying that HPA is active and properly managing NiFi node scaling. No arguments required.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_14\n\nLANGUAGE: console\nCODE:\n```\nkubectl get -n clusters hpa\nNAME                REFERENCE                                                TARGETS         MINPODS   MAXPODS   REPLICAS   AGE\nkeda-hpa-cluster    NifiNodeGroupAutoscaler/nifinodegroupautoscaler-sample     1833m/2 (avg)   1         3         2          25d\n```\n\n----------------------------------------\n\nTITLE: Updating config for OpenShift Deployment\nDESCRIPTION: This script uses the `sed` command to find and replace a hardcoded value (1000690000) with the dynamically retrieved UID obtained using a prior command. This modification is performed on the `config/samples/openshift.yaml` file, allowing the NiFi cluster configuration file to run within the context of the identified user ID on OpenShift.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nsed -i \"s/1000690000/$uid/g\" config/samples/openshift.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring Managed Groups in NifiCluster Specification\nDESCRIPTION: This YAML example shows how to configure managed users in a NifiCluster specification. It demonstrates defining managedAdminUsers and managedReaderUsers, which the operator will use to create and manage corresponding NifiUsers and NifiUserGroups.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/2_manage_users_and_accesses/3_managed_groups.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  ...\n  oneNifiNodePerNode: false\n  propagateLabels: true\n  managedAdminUsers:\n    -  identity: \"alexandre.guitton@konpyutaika.com\"\n       name: \"aguitton\"\n    -  identity: \"nifiuser@konpyutaika.com\"\n       name: \"nifiuser\"\n  managedReaderUsers:\n    -  identity: \"toto@konpyutaika.com\"\n       name: \"toto\"\n    ...\n```\n\n----------------------------------------\n\nTITLE: Port-Forwarding Prometheus Service Locally Using kubectl Console\nDESCRIPTION: Establishes a local port-forward from port 9090 on localhost to the Prometheus service 'prometheus-operated' in the 'monitoring-system' namespace. This enables local access to the Prometheus UI and API at 'http://localhost:9090' for querying NiFi cluster metrics without exposing the service externally. Requires kubectl configured access to the Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_11\n\nLANGUAGE: console\nCODE:\n```\nkubectl port-forward -n monitoring-system service/prometheus-operated 9090:9090\n```\n\n----------------------------------------\n\nTITLE: Initializing Node.js Project for NiFiKop Migration (Bash)\nDESCRIPTION: Creates a basic Node.js project directory with a default `package.json` file using `npm init -y` and installs the necessary dependencies: `@kubernetes/client-node` (version 0.16.3) for Kubernetes API interaction and `minimist` (version 1.2.6) for parsing command-line arguments.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm init -y\nnpm install @kubernetes/client-node@0.16.3 minimist@1.2.6\n```\n\n----------------------------------------\n\nTITLE: Configuring an Istio Gateway for HTTPS Traffic with TLS Termination\nDESCRIPTION: Defines an Istio Gateway for HTTPS traffic on port 443, enabling TLS termination by specifying credential name 'my-secret' for certificates. The gateway listens for requests destined for nifi.my-domain.com over HTTPS, facilitating secure external access to NiFi.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: nifi-gateway\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 443\n      name: https\n      protocol: HTTPS\n    tls:\n      mode: SIMPLE\n      credentialName: my-secret\n    hosts:\n    - nifi.my-domain.com\n```\n\n----------------------------------------\n\nTITLE: Exporting Certificate Secrets to Local Files\nDESCRIPTION: This snippet shows commands to extract CA cert, user cert, and user key from the Kubernetes secret, decode the base64-encoded data, and save them as local files for use in client applications or configuration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/3_tasks/2_security/1_ssl.md#_snippet_5\n\nLANGUAGE: console\nCODE:\n```\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['ca\\.crt']}\" | base64 -d > ca.crt\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['tls\\.crt']}\" | base64 -d > tls.crt\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['tls\\.key']}\" | base64 -d > tls.key\n```\n\n----------------------------------------\n\nTITLE: Setting up Node.js Project and Dependencies - Bash\nDESCRIPTION: Initializes a Node.js project with default settings and installs the required dependencies (`@kubernetes/client-node` for Kubernetes API interaction and `minimist` for command-line argument parsing) at specific versions. This prepares the environment for the migration script. Requires Node.js and npm installed. Outputs a `package.json` file and creates a `node_modules` directory.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm init -y\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @kubernetes/client-node@0.16.3 minimist@1.2.6\n```\n\n----------------------------------------\n\nTITLE: Purging a Helm Release Completely - Bash\nDESCRIPTION: This bash command completely removes the Helm release named 'nifikop' from the cluster and deletes all associated records, freeing the release name for reuse. Prerequisite: Helm 2.x must be used as '--purge' is not available in Helm 3; for Helm 3 use 'helm uninstall'. Use this when you want to fully clean up a release, including metadata.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nhelm delete --purge nifikop\n```\n\n----------------------------------------\n\nTITLE: Deploying Zookeeper with Custom Resources and Security Contexts on OpenShift\nDESCRIPTION: Configures and installs Zookeeper with specified UID/GID for security using Helm parameters. Retrieves the appropriate UID/GID from the OpenShift namespace annotations, then passes these values as 'runAsUser' and 'fsGroup' during installation. Ensures security compliance in OpenShift environments.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nzookeper_uid=$(kubectl get namespace zookeeper -o=jsonpath=''{.metadata.annotations.openshift\\.io/sa\\.scc\\.supplemental-groups}' | sed 's/\\/10000$//' | tr -d '[:space:]')\n```\n\nLANGUAGE: bash\nCODE:\n```\nhelm install zookeeper bitnami/zookeeper \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set global.storageClass=standard \\\n    --set networkPolicy.enabled=true \\\n    --set replicaCount=3 \\\n    --set containerSecurityContext.runAsUser=$zookeper_uid \\\n    --set podSecurityContext.fsGroup=$zookeper_uid\n```\n\n----------------------------------------\n\nTITLE: Describe NiFiCluster Status - Shell\nDESCRIPTION: This command retrieves and displays the status of the specified NifiCluster resource using `kubectl describe`. It allows monitoring of the graceful action state of the nodes during the scale-down process, including the action state and any error messages.  It's crucial to ensure the scale-down process is proceeding as expected.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/3_tasks/1_nifi_cluster/2_cluster_scaling.md#_snippet_5\n\nLANGUAGE: console\nCODE:\n```\nkubectl describe nificluster simplenifi\n\n...\nStatus:\n  Nodes State:\n    ...\n    2:\n      Configuration State:  ConfigInSync\n      Graceful Action State:\n        Action State:   GracefulDownscaleRequired\n        Error Message:\n    ...\n...\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFiCluster to use external issuer with SSL\nDESCRIPTION: This example shows how to configure the NiFi cluster to utilize an external CA issuer, such as Let's Encrypt, by setting 'issuerRef' with the issuer's name and kind in the 'sslSecrets' block within 'listenersConfig'. This enables automatic retrieval and renewal of SSL certificates from the specified issuer.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\nlistenersConfig:\n  issuerRef:\n    name: letsencrypt-staging\n    kind: Issuer\n```\n\n----------------------------------------\n\nTITLE: Verifying NiFiKop Operator Deployment with kubectl - Console\nDESCRIPTION: This snippet checks the status of NiFiKop operator pods in the 'nifikop' namespace using kubectl. It requires proper roles for namespace and pod listing. It prints a tabular output listing pod readiness, status, restarts, and age, confirming successful deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/6_contributing/1_developer_guide.md#_snippet_9\n\nLANGUAGE: console\nCODE:\n```\nkubectl get pods -n nifikop\nNAME                                                READY   STATUS    RESTARTS   AGE\nskeleton-nifikop-8946b89dc-4cfs9   1/1     Running   0          7m45s\n```\n\n----------------------------------------\n\nTITLE: Installing Zookeeper with Security Contexts on OpenShift\nDESCRIPTION: Helm command to install Zookeeper with specific runAsUser and fsGroup based on retrieved UID, ensuring proper security context on OpenShift. Dependencies include the previously obtained UID and Helm CLI.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhelm install zookeeper oci://registry-1.docker.io/bitnamicharts/zookeeper \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set global.storageClass=standard \\\n    --set networkPolicy.enabled=true \\\n    --set containerSecurityContext.runAsUser=$zookeper_uid \\\n    --set podSecurityContext.fsGroup=$zookeper_uid\n```\n\n----------------------------------------\n\nTITLE: Create a custom StorageClass in Kubernetes (YAML)\nDESCRIPTION: This YAML defines a custom StorageClass in Kubernetes named `exampleStorageclass`. It uses the `pd-standard` type, GCE persistent disk provisioner, sets the reclaim policy to `Delete`, and specifies `WaitForFirstConsumer` for volume binding mode, which delays volume binding until a pod is scheduled.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: exampleStorageclass\nparameters:\n  type: pd-standard\nprovisioner: kubernetes.io/gce-pd\nreclaimPolicy: Delete\nvolumeBindingMode: WaitForFirstConsumer\n```\n\n----------------------------------------\n\nTITLE: Patching CRDs with Conversion Webhook Configuration in YAML\nDESCRIPTION: YAML configuration to patch Custom Resource Definitions (CRDs) to enable the conversion webhook. This configuration adds necessary annotations for cert-manager integration and sets up the webhook conversion strategy to support both v1 and v1alpha1 versions of the resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/7_upgrade_guides/3_v0.16.0_to_v1.0.0.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n```\n\n----------------------------------------\n\nTITLE: Secret Structure for TLS Authentication in NiFi Cluster\nDESCRIPTION: This snippet outlines the required secret key-value pairs for TLS authentication, including private key, certificate, CA certificate, password, and keystore/truststore files. These are used by the operator to authenticate securely when connecting to the external NiFi cluster via TLS.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/3_manage_nifi/1_manage_clusters/3_external_cluster.md#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\ntls.key: <private-key>\ntls.crt: <certificate>\npassword: <user-password>\nca.crt: <CA-certificate>\ntruststore.jks: <truststore-file>\nkeystore.jks: <keystore-file>\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Development - Bash\nDESCRIPTION: This snippet shows essential environment variables to configure when running the operator via an IDE or locally. These include KUBECONFIG for Kubernetes context, WATCH_NAMESPACE for namespace scope, POD_NAME for operator identification, LOG_LEVEL for log verbosity, and OPERATOR_NAME. No direct output; ensures correct operator configuration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/6_contributing/1_developer_guide.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nKUBECONFIG={path/to/your/kubeconfig}\nWATCH_NAMESPACE={namespace_to_watch}\nPOD_NAME={name for operator pod}\nLOG_LEVEL=Debug\nOPERATOR_NAME=ide\n```\n\n----------------------------------------\n\nTITLE: Purging a Helm Release for Nifikop in Bash\nDESCRIPTION: This Bash command permanently deletes the nifikop Helm release and all associated Helm records, allowing the release name to be reused. The '--purge' flag ensures that Helm cleans up all stored history of the release. Helm v2 syntax is used; this command may be deprecated in Helm v3 where 'uninstall' is the equivalent. Prerequisites are the same as other Helm commands.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nhelm delete --purge nifikop\n```\n\n----------------------------------------\n\nTITLE: Checking HorizontalPodAutoscaler Status for NiFi\nDESCRIPTION: Retrieves the HorizontalPodAutoscaler (HPA) resources within the `clusters` namespace using `kubectl get hpa`. This command is used to verify that KEDA has successfully created and configured an HPA (e.g., `keda-hpa-cluster`) to manage the scaling of the target `NifiNodeGroupAutoscaler` based on the `ScaledObject` definition.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_14\n\nLANGUAGE: console\nCODE:\n```\nkubectl get -n clusters hpa\n```\n\n----------------------------------------\n\nTITLE: YAML Example of External Services Configuration for Nifi Listeners\nDESCRIPTION: This YAML snippet demonstrates how to define an external service named 'clusterip' with a specific port and listener name, including metadata annotations and labels. It serves as a template for creating external service configurations for Nifi.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/5_references/1_nifi_cluster/7_external_service_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nexternalServices:\n  - name: \"clusterip\"\n    spec:\n      type: ClusterIP\n      portConfigs:\n        - port: 8080\n          internalListenerName: \"http\"\n    metadata:\n      annotations:\n        toto: tata\n      labels:\n        titi: tutu\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Internal Listener in NifiCluster Spec\nDESCRIPTION: This snippet demonstrates how to add an additional internal listener to the `internalListeners` list that is not tied to a specific NiFi internal type. This is useful for exposing custom endpoints implemented by NiFi processors (e.g., an HTTP endpoint for receiving data), requiring only a name and container port.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    ...\n    - name: \"http-tracking\"\n      containerPort: 8081\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop Migration Dependencies with NPM - Bash\nDESCRIPTION: This snippet demonstrates initialization of a new Node.js project and the installation of necessary dependencies (@kubernetes/client-node and minimist) via npm. The 'npm init -y' command sets up package.json, and the subsequent install adds required libraries for Kubernetes API access and CLI argument parsing. No parameters are needed. The project must have Node.js and npm already installed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm init -y\nnpm install @kubernetes/client-node@0.16.3 minimist@1.2.6\n```\n\n----------------------------------------\n\nTITLE: Installing Nifikop Helm Chart with Custom Parameters\nDESCRIPTION: Illustrates using `helm install` with the `--set` flag to override default chart values during installation of the `konpyutaika/nifikop` chart. This example specifies the target `namespaces` for the `nifikop` release.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop --set namespaces={\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Kubectl Apply CRDs\nDESCRIPTION: These commands demonstrate how to manually apply the Custom Resource Definitions (CRDs) required by NiFiKop. This is necessary when deploying NiFiKop without using Helm's CRD management or when using Kubernetes versions older than 1.16.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/helm/nifikop/README.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/deploy/crds/v1beta1/nifi.konpyutaika.com_nificlusters_crd.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/deploy/crds/v1beta1/nifi.konpyutaika.com_nifiusers_crd.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaikanifikop/master/deploy/crds/v1beta1/nifi.konpyutaika.com_nifiusergroups_crd.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/deploy/crds/v1beta1/nifi.konpyutaika.com_nifidataflows_crd.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/deploy/crds/v1beta1/nifi.konpyutaika.com_nifiparametercontexts_crd.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/deploy/crds/v1beta1/nifi.konpyutaika.com_nifiregistryclients_crd.yaml\n```\n\n----------------------------------------\n\nTITLE: Define Inherited NiFi Parameter Context YAML\nDESCRIPTION: This YAML snippet illustrates how to define a `NifiParameterContext` that inherits parameters from one or more parent parameter contexts. It utilizes the `inheritedParameterContexts` field to reference the parent context and shows how to define or override parameters locally within the child context.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/5_references/4_nifi_parameter_context.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle-child\nspec:\n  description: \"It is a child test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  inheritedParameterContexts:\n    - name: dataflow-lifecycle\n  parameters:\n    - name: test\n      value: toto-child\n      description: tutu (child)\n\n```\n\n----------------------------------------\n\nTITLE: Updating NifiCluster InitContainerImage - YAML\nDESCRIPTION: This YAML snippet shows an updated configuration for the `NifiCluster` resource.  It demonstrates how to change the `initContainerImage` to align with the new version. The `repository` is changed to `bash` and the tag is set to \"5.2.2\" to ensure compatibility with the upgrade. This is a necessary change if the default image was overridden.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/7_upgrade_guides/2_v0.14.1_to_v0.15.0.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  initContainerImage:\n    repository: bash\n    tag: \"5.2.2\"\n```\n\n----------------------------------------\n\nTITLE: Perform NiFiKop Helm Chart Dry Run - Bash\nDESCRIPTION: Executes a dry run installation of the NiFiKop Helm chart using `helm install` with the `--dry-run` flag. This renders the Kubernetes manifests that would be created without actually deploying resources. Useful for debugging templates and verifying configuration, and can include setting specific parameters using `--set`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/2_setup/3_install/1_customizable_install_with_helm.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop \\\n    --dry-run \\\n    --set logLevel=Debug \\\n    --set namespaces={\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring Cluster Nodes with NodeConfigGroups (YAML)\nDESCRIPTION: This YAML example shows how to assign nodes to specific configuration groups or define unique configurations directly at node level, allowing flexible deployment of cluster nodes with different specifications, including resource allocations and custom configurations.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n- id: 0\n  nodeConfigGroup: \"default_group\"\n- id: 2\n  nodeConfigGroup: \"high_mem_group\"\n- id: 3\n  nodeConfigGroup: \"high_mem_group\"\n- id: 5\n  nodeConfig:\n    resourcesRequirements:\n      limits:\n        cpu: \"2\"\n        memory: 3Gi\n      requests:\n        cpu: \"1\"\n        memory: 1Gi\n```\n\n----------------------------------------\n\nTITLE: Defining a NiFi UserGroup Custom Resource in YAML\nDESCRIPTION: This YAML snippet demonstrates how to define a `NifiUserGroup` custom resource. It specifies the group's identity within NiFi, references the target NiFi cluster, lists associated users, and defines access policies granted to the group.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/5_references/6_nifi_usergroup.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUserGroup\nmetadata:\n  name: group-test\nspec:\n  identity: \"My Special Group\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  usersRef:\n    - name: nc-0-node.nc-headless.nifikop.svc.cluster.local\n    - name: nc-controller.nifikop.mgt.cluster.local\n  accessPolicies:\n    - type: global\n      action: read\n      resource: /counters\n```\n\n----------------------------------------\n\nTITLE: Configuring conversion webhook annotations in CRDs\nDESCRIPTION: This snippet provides YAML configuration snippets to modify CRDs for webhook-based conversion from v1alpha1 to v1. It requires setting annotations and conversion strategy fields to enable the webhook for resource conversion. Essential for maintaining compatibility between resource versions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/2_deploy_nifikop/1_quick_start.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n...\n```\n\n----------------------------------------\n\nTITLE: Example NifiUser Resource Definition - YAML\nDESCRIPTION: This YAML snippet provides an example of how to define an `NifiUser` resource in Kubernetes. It specifies the API version and kind, sets metadata like the name, defines the user's identity, references the target NiFi cluster, and indicates whether a certificate should be created for this user.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/5_references/2_nifi_user.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: aguitton\nspec:\n  identity: alexandre.guitton@konpyutaika.com\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  createCert: false\n```\n\n----------------------------------------\n\nTITLE: Installing the NiFiKop Helm Chart with a Custom Image Tag in Bash\nDESCRIPTION: Installs the NiFiKop operator to a Kubernetes cluster using a locally referenced Helm chart and custom image tag and namespace settings. Requires the Helm CLI, access to the chart files, and functional Kubernetes credentials. Inputs are chart path, image tag, and namespace; output is deployment of NiFiKop via Helm.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/6_contributing/1_developer_guide.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nhelm install skeleton ./helm/nifikop \\\n    --set image.tag=v0.5.1-release \\\n    --namespace-{\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Updating DNSNames in NifiUsers when webProxyHosts change\nDESCRIPTION: This snippet updates the DNSNames attribute for NifiUser resources in response to changes in webProxyHosts, ensuring correct network routing and proxy configuration. It depends on the operator logic that watches for webProxyHosts changes and modifies user resource annotations accordingly.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/CHANGELOG.md#_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.k8s.io/v1alpha1\nkind: NifiUser\nmetadata:\n  name: example-user\nspec:\n  DNSNames:\n    - proxy1.example.com\n    - proxy2.example.com\n```\n\n----------------------------------------\n\nTITLE: Creating an Issuer for Let's Encrypt SSL Certificates\nDESCRIPTION: YAML configuration for creating a Let's Encrypt Issuer in Kubernetes. This issuer will be used to generate valid SSL certificates for the NiFi cluster using the ACME protocol with HTTP-01 challenge.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: cert-manager.io/v1alpha2\nkind: Issuer\nmetadata:\n  name: letsencrypt-staging\nspec:\n  acme:\n    # You must replace this email address with your own.\n    # Let's Encrypt will use this to contact you about expiring\n    # certificates, and issues related to your account.\n    email: <your email address>\n    server: https://acme-staging-v02.api.letsencrypt.org/directory\n    privateKeySecretRef:\n      # Secret resource used to store the account's private key.\n      name: example-issuer-account-key\n    # Add a single challenge solver, HTTP01 using nginx\n    solvers:\n      - http01:\n          ingress:\n            ingressTemplate:\n              metadata:\n                annotations:\n                  \"external-dns.alpha.kubernetes.io/ttl\": \"5\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio VirtualService for HTTP\nDESCRIPTION: This YAML snippet defines an Istio VirtualService to route incoming HTTP requests intercepted by the Gateway to the NiFi service. It directs traffic from the specified host and path prefix to the 'nifi' service on port 8080. This configuration requires a corresponding Gateway configuration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: nifi-vs\nspec:\n  gateways:\n  - nifi-gateway\n  hosts:\n  - nifi.my-domain.com\n  http:\n  - match:\n    - uri:\n        prefix: /\n    route:\n    - destination:\n        host: nifi\n        port:\n          number: 8080\n```\n\n----------------------------------------\n\nTITLE: Setting environment variables for GKE\nDESCRIPTION: Sets the environment variables GCP_PROJECT, GCP_ZONE, and CLUSTER_NAME, which are used in subsequent commands to interact with the Google Cloud Platform. This snippet requires the user to replace the placeholders with their specific project ID, zone, and cluster name. It defines the configuration for creating and managing the GKE cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/2_setup/2_platform_setup/1_gke.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nexport GCP_PROJECT=<project_id>\nexport GCP_ZONE=<zone>\nexport CLUSTER_NAME=<cluster-name>\n```\n\n----------------------------------------\n\nTITLE: Configuring Internal Listeners for NiFi Cluster in YAML\nDESCRIPTION: This snippet demonstrates how to configure the internal listeners for a NiFi cluster, including HTTPS, cluster, Site-to-Site, Prometheus, and load balancing ports. These ports are used for internal NiFi cluster communications and services.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    - type: \"https\"\n      name: \"https\"\n      containerPort: 8443\n    - type: \"cluster\"\n      name: \"cluster\"\n      containerPort: 6007\n    - type: \"s2s\"\n      name: \"s2s\"\n      containerPort: 10000\n    - type: \"prometheus\"\n      name: \"prometheus\"\n      containerPort: 9090\n    - type: \"load-balance\"\n      name: \"load-balance\"\n      containerPort: 6342\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Registry Client Resource (YAML)\nDESCRIPTION: Define a `NifiRegistryClient` custom resource for NiFiKop to manage versioned dataflows. It specifies the connection details (URI) to the NiFi Registry and associates it with a NiFi cluster. This resource is a prerequisite for deploying `NifiDataflow` resources that reference versioned flows.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiRegistryClient\nmetadata:\n  name: registry-client-example\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Registry client managed by NiFiKop\"\n  uri: \"http://nifi.hostname.com:18080\"\n```\n\n----------------------------------------\n\nTITLE: Listing Nifikop Managed User Groups with Kubectl Shell\nDESCRIPTION: Provides a `kubectl` command to list the `NifiUserGroup` resources managed by the Nifikop operator within a specific namespace. This helps verify that the 'managed-admins', 'managed-nodes', and 'managed-readers' groups have been created and are visible as Kubernetes resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/3_tasks/4_nifi_user_group.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nkubectl get -n nifikop nifiusergroups.nifi.konpyutaika.com \nNAME              AGE\nmanaged-admins    6d7h\nmanaged-nodes     6d7h\nmanaged-readers   6d7h\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom StorageClass in Kubernetes\nDESCRIPTION: Defines a StorageClass resource for Kubernetes, specifying parameters like storage type, provisioner, reclaim policy, and volume binding mode. This setup facilitates volume provisioning optimized for NiFi datasets with proper volume binding behaviors.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/2_setup/1_getting_started.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: exampleStorageclass\nparameters:\n  type: pd-standard\nprovisioner: kubernetes.io/gce-pd\nreclaimPolicy: Delete\nvolumeBindingMode: WaitForFirstConsumer\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop Helm Chart (Bash/Helm)\nDESCRIPTION: Installs the NiFiKop operator using its Helm chart located in './helm/nifikop'. It names the Helm release 'skeleton', specifies the image tag to use (e.g., 'v0.5.1-release', which must match the pushed image tag), and deploys it into the 'nifikop' namespace. Requires Helm v3.4.2+ installed and configured.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/6_contributing/1_developer_guide.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nhelm install skeleton ./helm/nifikop \\\n    --set image.tag=v0.5.1-release \\\n    --namespace={\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi User Configuration\nDESCRIPTION: This snippet demonstrates the basic structure of a NiFi user configuration using YAML. It specifies the API version, kind, metadata, and some basic user details. It references a NiFi cluster and indicates whether a certificate should be created. The `metadata` section contains information about the user resource.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/5_references/2_nifi_user.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: aguitton\nspec:\n  identity: alexandre.guitton@konpyutaika.com\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  createCert: false\n```\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Parameter Contexts in Kubernetes using YAML\nDESCRIPTION: Examples of defining NifiParameterContext resources in Kubernetes, including a base parameter context and a child parameter context that inherits from the parent. Demonstrates parameter definition, secret references, and inheritance.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/5_references/4_nifi_parameter_context.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      description: toto\n      sensistive: true\n---\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle-child\nspec:\n  description: \"It is a child test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  inheritedParameterContexts:\n    - name: dataflow-lifecycle\n  parameters:\n    - name: test\n      value: toto-child\n      description: tutu (child)\n```\n\n----------------------------------------\n\nTITLE: Verifying NiFiKop Operator Pods with Kubectl in Console\nDESCRIPTION: This console command lists pods in the 'nifikop' namespace, allowing developers to confirm the running status of the deployed operator. Output includes pod names, readiness, status, restarts, and age. Prerequisite is cluster access and kubectl configuration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/6_contributing/1_developer_guide.md#_snippet_9\n\nLANGUAGE: console\nCODE:\n```\nkubectl get pods -n nifikop\nNAME                                                READY   STATUS    RESTARTS   AGE\nskeleton-nifikop-8946b89dc-4cfs9   1/1     Running   0          7m45s\n```\n\n----------------------------------------\n\nTITLE: Verifying NiFiKop Operator Pod Status using Console and kubectl\nDESCRIPTION: Uses `kubectl get pods` to list pods in the `nifikop` namespace, allowing verification that the operator pod (e.g., `skeleton-nifikop-...`) is running correctly after Helm installation. The output shows the pod name, readiness status, running status, restarts, and age.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/6_contributing/1_developer_guide.md#_snippet_9\n\nLANGUAGE: console\nCODE:\n```\n$ kubectl get pods -n nifikop\nNAME                                                READY   STATUS    RESTARTS   AGE\nskeleton-nifikop-8946b89dc-4cfs9   1/1     Running   0          7m45s\n```\n\n----------------------------------------\n\nTITLE: Starting Minikube with Minimum Memory (Bash)\nDESCRIPTION: Starts a Minikube cluster with at least 4GB of RAM allocated. This is a recommended minimum for running NiFiKop locally.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/2_deploy_nifikop/1_quick_start.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nminikube start --memory=4000\n```\n\n----------------------------------------\n\nTITLE: Cloning NiFiKop Project Repository using Git in Bash\nDESCRIPTION: This snippet clones the NiFiKop GitHub repository and changes the working directory to the project root. The prerequisite is having Git installed. This prepares the local environment to build and run the operator.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/6_contributing/1_developer_guide.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/konpyutaika/nifikop.git\ncd nifikop\n```\n\n----------------------------------------\n\nTITLE: Verifying Horizontal Pod Autoscaler Status for NiFi Autoscaling - Console\nDESCRIPTION: This kubectl command lists the Horizontal Pod Autoscaler (HPA) resources in the 'clusters' namespace. It verifies the HPA created by KEDA is monitoring the designated NifiNodeGroupAutoscaler and shows current metrics, minimum and maximum pod counts, and current replica count, confirming the autoscaling setup is active.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_13\n\nLANGUAGE: console\nCODE:\n```\nkubectl get -n clusters hpa\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio VirtualService for HTTP Traffic Routing\nDESCRIPTION: Defines an Istio VirtualService named 'nifi-vs' associated with the 'nifi-gateway'. It routes all HTTP requests for 'nifi.my-domain.com' with any URI prefix ('/') to the internal Kubernetes service named 'nifi' on port 8080.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: nifi-vs\nspec:\n  gateways:\n  - nifi-gateway\n  hosts:\n  - nifi.my-domain.com\n  http:\n  - match:\n    - uri:\n        prefix: /\n    route:\n    - destination:\n        host: nifi\n        port:\n          number: 8080\n```\n\n----------------------------------------\n\nTITLE: Configuring Internal Listeners for NiFi Cluster in YAML\nDESCRIPTION: Example of configuring internal listeners for a NiFi cluster, including ports for HTTPS, cluster communication, Site-to-Site, Prometheus metrics, and load balancing. Each listener defines its type, name, and container port.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    - type: \"https\"\n      name: \"https\"\n      containerPort: 8443\n    - type: \"cluster\"\n      name: \"cluster\"\n      containerPort: 6007\n    - type: \"s2s\"\n      name: \"s2s\"\n      containerPort: 10000\n    - type: \"prometheus\"\n      name: \"prometheus\"\n      containerPort: 9090\n    - type: \"load-balance\"\n      name: \"load-balance\"\n      containerPort: 6342\n```\n\n----------------------------------------\n\nTITLE: Defining npm Start Script in package.json (JSON)\nDESCRIPTION: Adds a \"start\" script to the `scripts` section of a `package.json` file. This script allows running the `index.js` file using `node`, including the `--no-warnings` flag to suppress Node.js warnings during execution. Use `npm start` to invoke this script.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"start\": \"node --no-warnings index.js\"\n```\n\n----------------------------------------\n\nTITLE: Exposing NiFi Cluster Port on k3d Load Balancer - Shell\nDESCRIPTION: This command modifies the default k3d cluster named `k3s-default` to add a port mapping. It exposes the specified `<nifi_cluster_port>` on the cluster's load balancer, routing traffic to the same port internally within the cluster. This allows external access to the NiFi cluster running inside the k3d environment. Replace `<nifi_cluster_port>` with the desired port number.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/2_setup/2_platform_setup/2_k3d.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nk3d cluster edit k3s-default --port-add \"<nifi_cluster_port>:<nifi_cluster_port>@loadbalancer\"\n```\n\n----------------------------------------\n\nTITLE: Checking NifiCluster Status During Scaledown (Shell)\nDESCRIPTION: This shell command and partial output demonstrate how to inspect the status of the NiFiCluster resource using `kubectl describe`. The output shows the `Status.Nodes State` field, indicating the current state of each node, including the graceful action being performed during the scale-down process.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_5\n\nLANGUAGE: Shell\nCODE:\n```\nkubectl describe nificluster simplenifi\n\n...\nStatus:\n  Nodes State:\n    ...\n    2:\n      Configuration State:  ConfigInSync\n      Graceful Action State:\n        Action State:   GracefulDownscaleRequired\n        Error Message:\n    ...\n...\n```\n\n----------------------------------------\n\nTITLE: Replacing Bootstrap Notification Services Configuration (Markdown)\nDESCRIPTION: Defines options for replacing the default bootstrap notification services configuration file (bootstrap_notifications_services.xml). Allows replacement using an external Kubernetes ConfigMap (`replaceConfigMap`) or a Kubernetes Secret (`replaceSecretConfig`), with the Secret taking precedence.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/5_references/1_nifi_cluster/2_read_only_config.md#_snippet_5\n\nLANGUAGE: Markdown\nCODE:\n```\n## BootstrapNotificationServicesConfig\n\n|Field|Type|Description|Required|Default|\n|-----|----|-----------|--------|--------|\n|replaceConfigMap|[ConfigmapReference](#configmapreference)|bootstrap_notifications_services.xml configuration that will replace the one produced based on template.|No|nil|\n|replaceSecretConfig|[SecretConfigReference](#secretconfigreference)|bootstrap_notifications_services.xml configuration that will replace the one produced based on template and overrideConfigMap.|No|nil|\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Registry Client resource in Kubernetes (YAML)\nDESCRIPTION: This YAML snippet defines a NifiRegistryClient custom resource named squidflow. It specifies the apiVersion, kind, metadata, and spec fields required by the nifikop operator to create or manage a NiFi Registry client configuration, including a reference to the NiFi cluster, a description, and the registry URI.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/5_references/3_nifi_registry_client.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiRegistryClient\nmetadata:\n  name: squidflow\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Squidflow demo\"\n  uri: \"http://nifi-registry:18080\"\n```\n\n----------------------------------------\n\nTITLE: Updating KEDA Helm Repository - Helm Console Commands - console\nDESCRIPTION: This snippet updates the local Helm chart repository index to ensure access to the latest KEDA chart versions. It uses the \"helm repo update\" console command, which must be run after adding or making changes to repositories. This has no input parameters and outputs the updated list or index confirmation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nhelm repo update\n```\n\n----------------------------------------\n\nTITLE: Creating NifiRegistryClient YAML\nDESCRIPTION: This YAML defines a NifiRegistryClient resource, which is a prerequisite for managing dataflows with NiFiKop. It specifies the cluster reference, a description, and the URI of the NiFi registry. It serves as a client for interacting with the NiFi registry to retrieve and manage dataflow definitions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiRegistryClient\nmetadata:\n  name: registry-client-example\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Registry client managed by NiFiKop\"\n  uri: \"http://nifi.hostname.com:18080\"\n```\n\n----------------------------------------\n\nTITLE: Installing cert-manager with Helm 3\nDESCRIPTION: This script installs cert-manager via Helm 3 by first deploying CustomResourceDefinitions, then adding the Jetstack Helm repository, updating it, and installing cert-manager in the designated namespace at version v1.7.2.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/2_setup/1_getting_started.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Install CustomResourceDefinitions first\nkubectl apply --validate=false -f \\\n   https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.crds.yaml\n\n# Add the jetstack helm repo\nhelm repo add jetstack https://charts.jetstack.io\nhelm repo update\n\n# You have to create the namespace before executing following command\nhelm install cert-manager \\\n    --namespace cert-manager \\\n    --version v1.7.2 jetstack/cert-manager\n```\n\n----------------------------------------\n\nTITLE: Installing Zookeeper with Helm\nDESCRIPTION: Installs Zookeeper using Helm with specified resource requests, limits, storage class, network policy, and replica count in the \"zookeeper\" namespace. Replace the 'storageClass' value as needed for your environment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/2_setup/1_getting_started.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhelm install zookeeper bitnami/zookeeper \\n    --namespace=zookeeper \\n    --set resources.requests.memory=256Mi \\n    --set resources.requests.cpu=250m \\n    --set resources.limits.memory=256Mi \\n    --set resources.limits.cpu=250m \\n    --set global.storageClass=standard \\n    --set networkPolicy.enabled=true \\n    --set replicaCount=3 \\n    --create-namespace\n```\n\n----------------------------------------\n\nTITLE: Run NiFiKop Migration Script (npm)\nDESCRIPTION: This command executes the NiFiKop CRD migration script. It takes two arguments: `--type` specifies the NiFiKop resource type to migrate (e.g., cluster, dataflow), and `--namespace` specifies the Kubernetes namespace where the resources are located.  If namespace is not given, it defaults to 'default'.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nnpm start -- --type=<NIFIKOP_RESOURCE> --namespace=<K8S_NAMESPACE>\n```\n\n----------------------------------------\n\nTITLE: Performing a Dry Run Helm Install for Nifikop\nDESCRIPTION: Demonstrates how to use `helm install` with the `--dry-run` flag to simulate the installation of the Nifikop chart from the `konpyutaika/nifikop` repository. This allows verification of templates and parameters (like `logLevel` and `namespaces`) without applying changes to the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop \\\n    --dry-run \\\n    --set logLevel=Debug \\\n    --set namespaces={\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Loading GKE Cluster Credentials Using gcloud Console Command\nDESCRIPTION: This gcloud console command fetches cluster credentials and configures kubectl to interact with the specified GKE cluster. It requires the GKE cluster name, zone, and GCP project ID as inputs. This step is necessary to manage Kubernetes resources on the newly created GKE cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/blog/2020-05-20-secured_nifi_cluster_on_gcp.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\ngcloud container clusters get-credentials nifi-cluster --zone <configured gcp zone> --project <GCP project's id>\n```\n\n----------------------------------------\n\nTITLE: Routing Istio HTTP Traffic with VirtualService (YAML)\nDESCRIPTION: This YAML snippet defines an Istio VirtualService. It links to the `nifi-gateway` and specifies that all HTTP requests (`/` prefix match) for `nifi.my-domain.com` should be routed to the service named `nifi` on port 8080.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: nifi-vs\nspec:\n  gateways:\n  - nifi-gateway\n  hosts:\n  - nifi.my-domain.com\n  http:\n  - match:\n    - uri:\n        prefix: /\n    route:\n    - destination:\n        host: nifi\n        port:\n          number: 8080\n```\n\n----------------------------------------\n\nTITLE: Defining NifiParameterContext in YAML for NiFiKop\nDESCRIPTION: This snippet demonstrates how to create a NifiParameterContext resource that defines parameters and their values for a dataflow. It allows referencing both regular parameters and sensitive parameters stored in Kubernetes secrets.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\n  namespace: demo\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      value: toto\n      description: toto\n```\n\n----------------------------------------\n\nTITLE: NifiCluster Configuration with busybox initContainerImage YAML\nDESCRIPTION: This snippet shows an example of a NifiCluster configuration in YAML where the `initContainerImage` is set to `busybox`. This configuration needs to be updated when upgrading to NifiKop v0.15.0. The configuration defines the `apiVersion`, `kind`, `metadata`, and `spec` for the NifiCluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/7_upgrade_guides/2_v0.14.1_to_v0.15.0.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  initContainerImage:\n    repository: busybox\n    tag: \"1.34.0\"\n```\n\n----------------------------------------\n\nTITLE: YAML Configuration for Updated initContainerImage with bash\nDESCRIPTION: This snippet demonstrates the necessary change in configuration where the initContainerImage repository is updated from 'busybox' to 'bash', ensuring the init container includes a bash shell. This update is required for compatibility with the new default image after the PR change.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/7_upgrade_guides/2_v0.14.1_to_v0.15.0.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  initContainerImage:\n    repository: bash\n    tag: \"5.2.2\"\n```\n\n----------------------------------------\n\nTITLE: Configuring SSL for NiFi Cluster\nDESCRIPTION: YAML configuration example for securing a NiFi cluster with SSL, including managed admin users, web proxy hosts, and listener configuration with SSL secrets.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/3_tasks/2_security/1_ssl.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\n...\nspec:\n  ...\n  managedAdminUsers:\n    - identity: \"alexandre.guitton@konpyutaika.com\"\n      name: \"aguitton\"\n  ...\n  readOnlyConfig:\n    # NifiProperties configuration that will be applied to the node.\n    nifiProperties:\n      webProxyHosts:\n        - nifistandard2.trycatchlearn.fr:8443\n        ...\n  ...\n  listenersConfig:\n    internalListeners:\n      - type: \"https\"\n        name: \"https\"\n        containerPort: 8443\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n```\n\n----------------------------------------\n\nTITLE: Creating a new GKE cluster\nDESCRIPTION: Creates a new GKE cluster using the gcloud command-line tool. It specifies the cluster version, machine type, number of nodes, zone, and project. This command requires the GCP_PROJECT and GCP_ZONE environment variables to be set. The created cluster will have four n1-standard-1 nodes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/2_setup/2_platform_setup/1_gke.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ngcloud container clusters create $CLUSTER_NAME \\\n  --cluster-version latest \\\n  --machine-type=n1-standard-1 \\\n  --num-nodes 4 \\\n  --zone $GCP_ZONE \\\n  --project $GCP_PROJECT\n```\n\n----------------------------------------\n\nTITLE: Checking Graceful Scaledown Status (Console)\nDESCRIPTION: Displays the status of the `simplenifi` NifiCluster resource using `kubectl describe`. This output includes the `Nodes State` section, where the graceful action state (`GracefulDownscaleRequired`) for the node being removed (id: 2) can be monitored during the decommission process.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_5\n\nLANGUAGE: console\nCODE:\n```\nkubectl describe nificluster simplenifi\n\n...\nStatus:\n  Nodes State:\n    ...\n    2:\n      Configuration State:  ConfigInSync\n      Graceful Action State:\n        Action State:   GracefulDownscaleRequired\n        Error Message:\n    ...\n...\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFi Cluster with kubectl - Bash\nDESCRIPTION: Applies a sample NiFiCluster YAML manifest to a specified Kubernetes namespace using kubectl. Assumes a configured Zookeeper service and a preexisting sample configuration file. Users need to customize config/samples/simplenificluster.yaml as per their cluster's Zookeeper connectivity settings.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Add your zookeeper svc name to the configuration\nkubectl create -n nifi -f config/samples/simplenificluster.yaml\n\n```\n\n----------------------------------------\n\nTITLE: YAML Configuration for NiFi Registry Client Resource\nDESCRIPTION: This YAML snippet defines a NiFi Registry Client custom resource, including metadata, specification, and description fields. It specifies the cluster reference, description, and URI for connecting to the NiFi registry. Dependencies include Kubernetes API schemas and custom resource definitions for NiFi Registry clients.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/5_references/3_nifi_registry_client.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiRegistryClient\nmetadata:\n  name: squidflow\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Squidflow demo\"\n  uri: \"http://nifi-registry:18080\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio VirtualService for HTTP Routing to NiFi\nDESCRIPTION: Configures an Istio VirtualService named 'nifi-vs' that links the 'nifi-gateway' to route traffic to the NiFi service on port 8080 for requests matching the root URI prefix, enabling external requests to reach the NiFi cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: nifi-vs\nspec:\n  gateways:\n  - nifi-gateway\n  hosts:\n  - nifi.my-domain.com\n  http:\n  - match:\n    - uri:\n        prefix: /\n    route:\n    - destination:\n        host: nifi\n        port:\n          number: 8080\n```\n\n----------------------------------------\n\nTITLE: Getting KEDA HPA Status (console)\nDESCRIPTION: This command lists the Horizontal Pod Autoscaler (HPA) resources in the `clusters` namespace. After deploying the KEDA ScaledObject, KEDA automatically creates an HPA that manages the scaling of the target resource (`NifiNodeGroupAutoscaler`), and this command allows you to verify its existence and current status.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_14\n\nLANGUAGE: console\nCODE:\n```\nkubectl get -n clusters hpa\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop Operator Helm Chart - Bash\nDESCRIPTION: Deploys the NiFiKop operator in a Kubernetes cluster using Helm. The command installs the Helm chart located in the local 'helm/nifikop' directory with a specific image tag and namespace value. This requires the previously pushed Docker image to be correctly referenced by image.repository and image.tag values.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/6_contributing/1_developer_guide.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nhelm install skeleton ./helm/nifikop \\\n    --set image.tag=v0.5.1-release \\\n    --namespace-{\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Get NiFi UID/GID - OpenShift\nDESCRIPTION: This command retrieves the UID/GID allowed for the NiFi namespace in OpenShift. It extracts the `openshift.io/sa.scc.supplemental-groups` annotation from the `nifi` namespace, removes the `/10000` suffix, and removes any whitespace. The result is stored in the `uid` variable for later use.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nuid=$(kubectl get namespace nifi -o=jsonpath='{.metadata.annotations.openshift\\.io/sa\\.scc\\.supplemental-groups}' | sed 's/\\/10000$//' | tr -d '[:space:]')\n```\n\n----------------------------------------\n\nTITLE: Defining Node Configuration YAML\nDESCRIPTION: This YAML snippet configures a NiFi node within a Kubernetes environment. It sets parameters like provenance storage size, user ID, cluster membership, pod metadata (annotations and labels), Docker image details, image pull policy, and external volume configurations. It also defines configurations for storage including mount paths and Persistent Volume Claim (PVC) specs. The configuration is designed for use with a Kubernetes operator.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/5_references/1_nifi_cluster/3_node_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n   default_group:\n      # provenanceStorage allow to specify the maximum amount of data provenance information to store at a time\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#write-ahead-provenance-repository-properties\n      provenanceStorage: \"10 GB\"\n      #RunAsUser define the id of the user to run in the Nifi image\n      # +kubebuilder:validation:Minimum=1\n      runAsUser: 1000\n      # Set this to true if the instance is a node in a cluster.\n      # https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#basic-cluster-setup\n      isNode: true\n      # Additionnal metadata to merge to the pod associated\n      podMetadata:\n        annotations:\n          node-annotation: \"node-annotation-value\"\n        labels:\n          node-label: \"node-label-value\"\n      # Docker image used by the operator to create the node associated\n      # https://hub.docker.com/r/apache/nifi/\n#      image: \"apache/nifi:1.11.2\"\n      # nodeAffinity can be specified, operator populates this value if new pvc added later to node\n      # https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity\n#      nodeAffinity:\n      # imagePullPolicy define the pull policy for NiFi cluster docker image\n      imagePullPolicy: IfNotPresent\n      # priorityClassName define the name of the priority class to be applied to these nodes\n      priorityClassName: \"example-priority-class-name\"\n      # externalVolumeConfigs specifies a list of volume to mount into the main container.\n      externalVolumeConfigs:\n        - name: example-volume\n          mountPath: \"/opt/nifi/example\"\n          secret:\n            secretName: \"raw-controller\"\n      # storageConfigs specifies the node related configs\n      storageConfigs:\n        # Name of the storage config, used to name PV to reuse into sidecars for example.\n        - name: provenance-repository\n          # Path where the volume will be mount into the main nifi container inside the pod.\n          mountPath: \"/opt/nifi/provenance_repository\"\n          # Kubernetes PVC spec\n          # https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n        - mountPath: \"/opt/nifi/nifi-current/logs\"\n          name: logs\n          pvcSpec:\n            accessModes:\n              - ReadWriteOnce\n            storageClassName: \"standard\"\n            resources:\n              requests:\n                storage: 10Gi\n```\n\n----------------------------------------\n\nTITLE: Terraform Deployment Initialization\nDESCRIPTION: This script initializes the Terraform deployment. It takes the path to a service account key as input. This key is necessary for Terraform to authenticate with Google Cloud and manage resources such as GKE clusters and related infrastructure. It will deploy the GKE cluster defined in terraform files.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/blog/2020-06-30-secured_nifi_cluster_on_gcp_with_external_dns.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd terraform\n./start.sh <service account key's path>\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFi Cluster on Kubernetes\nDESCRIPTION: This command deploys a NiFi cluster by applying a pre-defined configuration YAML in the 'nifi' namespace, referencing Zookeeper hostname as part of the configuration. Ensures NiFi has access to Zookeeper for coordination and cluster management. Prior setup of Zookeeper and the sample configuration file is required.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create -n nifi -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Updating NifiCluster Configuration to Bash Init Container (v0.15.0+)\nDESCRIPTION: Example YAML configuration for a NifiCluster custom resource updated for Nifikop v0.15.0+, specifying `bash` as the `initContainerImage`. This change is necessary because the default init container image was updated from `busybox` and requires a bash shell.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/7_upgrade_guides/2_v0.14.1_to_v0.15.0.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  initContainerImage:\n    repository: bash\n    tag: \"5.2.2\"\n```\n\n----------------------------------------\n\nTITLE: Adding and Updating Prometheus Helm Repository - Console\nDESCRIPTION: These commands add the official Prometheus community Helm chart repository to the local Helm client and then update it to fetch the latest available charts. This setup is necessary before installing the Prometheus operator via Helm to ensure the latest supported versions are used.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_6\n\nLANGUAGE: console\nCODE:\n```\nhelm repo add prometheus https://prometheus-community.github.io/helm-charts\n```\n\nLANGUAGE: console\nCODE:\n```\nhelm repo update\n```\n\n----------------------------------------\n\nTITLE: Configuring External Services with ClusterIP in NiFiKop\nDESCRIPTION: A YAML configuration example that demonstrates how to set up external services for NiFi using ClusterIP service type. It defines a service named 'clusterip' that exposes port 8080 and links to the 'http' internal listener. The example also includes custom annotations and labels.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/5_references/1_nifi_cluster/7_external_service_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  externalServices:\n    - name: \"clusterip\"\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8080\n            internalListenerName: \"http\"\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n\n----------------------------------------\n\nTITLE: Accessing Prometheus UI via Port Forwarding\nDESCRIPTION: Uses the `kubectl port-forward` command to establish a connection between the local machine's port 9090 and the `prometheus-operated` service's port 9090 within the `monitoring-system` namespace. This allows users to access the Prometheus web interface through their browser at `http://localhost:9090`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_12\n\nLANGUAGE: console\nCODE:\n```\nkubectl port-forward -n monitoring-system service/prometheus-operated 9090:9090\n```\n\n----------------------------------------\n\nTITLE: Managing SSL Secrets for NiFi Cluster\nDESCRIPTION: This snippet explains the process for specifying SSL secret details, including CA certificate, CA key, client certificate, and client key, especially when the `create` flag is set to false. It guides how to reference existing secret content for SSL configuration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/3_tasks/2_security/1_ssl.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nsslSecrets:\n  tlsSecretName: \"test-nifikop\"\n  create: false\n  # When create is false, these keys should be present in the secret:\n  # caCert, caKey, clientCert, clientKey\n```\n\n----------------------------------------\n\nTITLE: Installing NiFi Operator with Helm\nDESCRIPTION: Deploys the NiFi operator using Helm from the specified OCI registry, with configurable version, resource requests, limits, and namespace. Use '--skip-crds' to avoid re-installing CRDs if already present.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/2_setup/1_getting_started.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop \\n    oci://ghcr.io/konpyutaika/helm-charts/nifikop \\n    --namespace=nifi \\n    --version 0.12.0 \\n    --set image.tag=v0.12.0-release \\n    --set resources.requests.memory=256Mi \\n    --set resources.requests.cpu=250m \\n    --set resources.limits.memory=256Mi \\n    --set resources.limits.cpu=250m \\n    --set namespaces={\"nifi\"}\n```\n\n----------------------------------------\n\nTITLE: Apply NiFiKop CRDs Manually (bash)\nDESCRIPTION: Applies the NiFiKop CustomResourceDefinitions (CRDs) directly from their raw GitHub URLs. This step is necessary if the Helm chart installation is configured to skip CRD deployment (--skip-crds) and you need to define the custom resources the operator manages. Requires kubectl.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/2_deploy_nifikop/1_quick_start.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\n```\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\n```\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\n```\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\n```\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\n```\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\n```\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifinodegroupautoscalers.yaml\n```\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificonnections.yaml\n```\n\n----------------------------------------\n\nTITLE: Packaging Helm Chart\nDESCRIPTION: Command to create a distributable Helm package for the NiFiKop operator, facilitating deployment and sharing.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/6_contributing/1_developer_guide.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nmake helm-package\n```\n\n----------------------------------------\n\nTITLE: Patching NifiKop CRDs for Webhook Conversion in Kubernetes (YAML)\nDESCRIPTION: YAML configuration snippet used to patch NifiKop CRDs (NifiCluster, NifiDataflow, NifiParameterContext, NifiRegistryClient, NifiUser, NifiUserGroup) to enable conversion between v1alpha1 and v1 API versions using a webhook. It specifies the webhook strategy, the service details (requiring placeholders like `${namespace}`, `${certificate_name}`, `${webhook_service_name}` to be replaced), and the supported conversion versions. The `cert-manager.io/inject-ca-from` annotation integrates with cert-manager to inject the CA certificate for secure communication with the webhook.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/7_upgrade_guides/3_v0.16.0_to_v1.0.0.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n```\n\n----------------------------------------\n\nTITLE: Creating k3d Cluster (Kubernetes 1.21) - Shell\nDESCRIPTION: This command creates a new k3d Kubernetes cluster using the `rancher/k3s:v1.21.10-k3s1` image, which corresponds to Kubernetes version 1.21.10. The `--wait` flag ensures the command waits for the cluster to be fully ready before exiting. This step is essential for establishing the foundational environment where NiFiKop will be installed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/2_setup/2_platform_setup/2_k3d.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nk3d cluster create --image rancher/k3s:v1.21.10-k3s1 --wait\n```\n\n----------------------------------------\n\nTITLE: Installing Helm Chart with Skipping CRDs Deployment Using Bash\nDESCRIPTION: This Helm install command demonstrates how to deploy the nifikop chart while skipping the CRD installations using the `--skip-crds` flag. This mode is useful if CRDs are managed externally or need to be preserved as-is, avoiding cluster-wide modifications during the Helm install.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nhelm install --name nifikop ./helm/nifikop --set namespaces={\"nifikop\"} --skip-crds\n```\n\n----------------------------------------\n\nTITLE: Kubectl Get Namespaces\nDESCRIPTION: This command lists the namespaces in the Kubernetes cluster, displaying their status and age. This is used to confirm the namespaces required by the deployment (cert-manager, default, kube-system, nifikop, zookeeper) have been successfully created. This verifies the cluster setup and readiness of different components.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/blog/2020-06-30-secured_nifi_cluster_on_gcp_with_external_dns.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nkubectl get namespaces\n```\n\n----------------------------------------\n\nTITLE: Uninstalling Helm Chart and Kubernetes Operator Using Bash\nDESCRIPTION: This Helm command uninstalls the `nifikop` Helm chart, effectively deleting all Kubernetes resources managed by the chart but leaving the associated CustomResourceDefinitions (CRDs) intact. Users should manually delete CRDs if desired, as they are not automatically removed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nhelm del nifikop\n```\n\n----------------------------------------\n\nTITLE: Building NiFiKop Kubectl Plugin Executable - Shell\nDESCRIPTION: Builds the NiFiKop kubectl plugin executable binary using the Makefile and copies it to the system's PATH directory for global access. This snippet requires a UNIX-based system with make and sudo privileges. The output is a compiled kubectl plugin executable placed in /usr/local/bin, enabling direct invocation of the nifikop plugin command.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/2_deploy_nifikop/3_kubectl_plugin.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nmake kubectl-nifikop && sudo cp ./bin/kubectl-nifikop /usr/local/bin\n```\n\n----------------------------------------\n\nTITLE: Install Cert-Manager via Helm 3\nDESCRIPTION: Installs Cert-Manager using Helm 3. This method involves applying the Cert-Manager Custom Resource Definitions (CRDs) separately, adding the Jetstack Helm repository, updating repositories, and then installing the Cert-Manager chart into the `cert-manager` namespace. Requires Helm 3 installed and configured.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/2_deploy_nifikop/1_quick_start.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply --validate=false -f    https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.crds.yaml\n```\n\nLANGUAGE: bash\nCODE:\n```\nhelm repo add jetstack https://charts.jetstack.io\n```\n\nLANGUAGE: bash\nCODE:\n```\nhelm repo update\n```\n\nLANGUAGE: bash\nCODE:\n```\nhelm install cert-manager     --namespace cert-manager     --version v1.7.2 jetstack/cert-manager\n```\n\n----------------------------------------\n\nTITLE: Installing Helm Chart with Specified Release Name Using Bash\nDESCRIPTION: This Bash snippet installs the nifikop Helm chart using a user-specified release name placeholder. It represents the basic Helm install syntax without additional parameter settings, allowing users to replace `<release name>` with their desired release identifier for easier management and upgrades.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nhelm install <release name> konpyutaika/nifikop\n```\n\n----------------------------------------\n\nTITLE: Defining a NiFi Registry Client\nDESCRIPTION: This YAML snippet defines a NiFi Registry Client custom resource. It specifies the API version, kind, metadata (name), and the specification, including the cluster reference, description, and URI of the NiFi Registry. The `clusterRef` field is a reference to the NifiCluster the registry client is linked to.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/5_references/3_nifi_registry_client.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiRegistryClient\nmetadata:\n  name: squidflow\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Squidflow demo\"\n  uri: \"http://nifi-registry:18080\"\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop with Helm using a Values File\nDESCRIPTION: This command demonstrates how to install the NiFiKop Helm chart from the 'konpyutaika' repository, naming the release 'nifikop'. It uses the '-f' flag to specify a YAML file named 'values.yaml' which contains custom configuration parameters to override the chart's default settings.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n$ helm install nifikop \\\n      konpyutaika/nifikop \\\n      -f values.yaml\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Secret for Sensitive Parameters (kubectl)\nDESCRIPTION: Use the `kubectl` command to create a generic Kubernetes secret containing key-value pairs. This secret holds sensitive parameter values that can be referenced by a `NifiParameterContext`. NiFiKop will then use these values when creating or updating the corresponding NiFi Parameter Context.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl create secret generic secret-params \\\n    --from-literal=secret1=yop \\\n    --from-literal=secret2=yep \\\n    -n nifikop\n```\n\n----------------------------------------\n\nTITLE: Checking Nifikop Pod Status with kubectl\nDESCRIPTION: This command uses kubectl to list the pods belonging to the deployed Nifikop Helm release. It filters pods by the release namespace ({{ .Release.Namespace }}) and the release name label (release={{ .Release.Name }}). Requires kubectl access to the target Kubernetes cluster and expects Helm templating variables to be rendered.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/helm/nifikop/templates/NOTES.txt#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkubectl --namespace {{ .Release.Namespace }} get pods -l \"release={{ .Release.Name }}\"\n```\n\n----------------------------------------\n\nTITLE: Go Struct Definition for NifiRegistryClient\nDESCRIPTION: This Go struct models the 'NifiRegistryClient' Kubernetes custom resource, including metadata, spec, and status fields. It references other nested structs for detailed specifications and status info, providing the schema for managing NiFi registry clients in Go-based controllers or APIs.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/5_references/3_nifi_registry_client.md#_snippet_1\n\nLANGUAGE: Go\nCODE:\n```\ntype NifiRegistryClient struct {\n    Metadata ObjectMeta `json:\"metadata\"`\n    Spec     NifiRegistryClientSpec `json:\"spec\"`\n    Status   NifiRegistryClientStatus `json:\"status\"`\n}\n```\n\n----------------------------------------\n\nTITLE: Testing the NiFiKop kubectl plugin\nDESCRIPTION: This snippet demonstrates how to verify the plugin installation by running the 'kubectl nifikop' command and observing the output, which lists available subcommands and usage instructions. It assumes the plugin executable is in the system PATH.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/2_deploy_nifikop/3_kubectl_plugin.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\n$ kubectl nifikop\nUsage:\n  nifikop [command]\n\nAvailable Commands:\n  completion          Generate the autocompletion script for the specified shell\n  help                Help about any command\n  nificluster         \n  nificonnection      \n  nifidataflow        \n  nifigroupautoscaler \n  nifiregistryclient  \n  nifiuser            \n  nifiusergroup \n```\n\n----------------------------------------\n\nTITLE: Configuring Cluster Nodes with Node and NodeConfigGroup References\nDESCRIPTION: This YAML example demonstrates how to assign nodes to specific configuration groups or define custom resource requirements directly at the node level, facilitating flexible cluster topology and resource management.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\n- id: 0\n  nodeConfigGroup: \"default_group\"\n- id: 2\n  nodeConfigGroup: \"high_mem_group\"\n- id: 3\n  nodeConfigGroup: \"high_mem_group\"\n- id: 5\n  nodeConfig:\n    resourcesRequirements:\n      limits:\n        cpu: \"2\"\n        memory: 3Gi\n      requests:\n        cpu: \"1\"\n        memory: 1Gi\n```\n\n----------------------------------------\n\nTITLE: Defining Istio Gateway for HTTP Traffic to Expose NiFi\nDESCRIPTION: This YAML snippet defines an Istio Gateway to intercept incoming HTTP requests on port 80 for a specific domain. It configures the ingress gateway with a selector and binds the request handling to the specified host.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: nifi-gateway\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - nifi.my-domain.com\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Secret for Sensitive Parameters using kubectl\nDESCRIPTION: This `kubectl` command creates a generic Kubernetes secret named `secret-params` in the `nifikop` namespace. The secret contains key-value pairs (`secret1=yop`, `secret2=yep`) intended to be used as sensitive parameters referenced within a `NifiParameterContext` resource.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl create secret generic secret-params \\\n    --from-literal=secret1=yop \\\n    --from-literal=secret2=yep \\\n    -n nifikop\n```\n\n----------------------------------------\n\nTITLE: Verifying New Node Kubernetes Resources | Console\nDESCRIPTION: This `kubectl get` command filters for resources (pods, configmaps, pvcs) labeled with the new node ID (25). It confirms that Kubernetes has created the necessary components for the new NiFi node managed by NiFiKop. Shows the pod, configmap, and persistent volume claim status.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl get pods,configmap,pvc -l nodeId=25\n```\n\n----------------------------------------\n\nTITLE: Setting Active NiFi Authorizer Property (Shell)\nDESCRIPTION: This shell command illustrates setting the `nifi.security.user.authorizer` property within NiFi's configuration (typically `nifi.properties`). This property determines which authorizer, defined in `authorizers.xml`, NiFi will use for authorization decisions. In this example, it selects the `custom-database-authorizer` defined in the previous template.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/3_tasks/2_security/2_authorization/1_custom_authorizer.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nnifi.security.user.authorizer=custom-database-authorizer\n```\n\n----------------------------------------\n\nTITLE: Configuring readiness and liveness probes in Helm chart\nDESCRIPTION: This configuration allows setting custom readiness and liveness probes for the NiFi pod within the Helm chart, improving deployment health checks and resilience. It involves setting probe parameters such as 'initialDelaySeconds' and 'periodSeconds'.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/CHANGELOG.md#_snippet_10\n\nLANGUAGE: YAML\nCODE:\n```\nreadinessProbe:\n  httpGet:\n    path: /\n    port: 8080\n  initialDelaySeconds: 30\n  periodSeconds: 10\n\nlivenessProbe:\n  httpGet:\n    path: /\n    port: 8080\n  initialDelaySeconds: 60\n  periodSeconds: 15\n```\n\n----------------------------------------\n\nTITLE: Configuring Conversion Webhook for NiFiKop CRDs\nDESCRIPTION: YAML configuration to enable the conversion webhook for NiFiKop CRDs, allowing version conversion between v1alpha1 and v1. This snippet shows the required CRD annotations and spec settings needed for the webhook to function properly.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n```\n\n----------------------------------------\n\nTITLE: Defining Custom NiFi Internal Listeners using Nifikop YAML\nDESCRIPTION: Demonstrates adding a custom internal listener without a predefined NiFi `type`. This is useful for exposing specific NiFi processors, like an HTTP endpoint, on a dedicated port within the pod. The example defines a listener named `http-tracking` on `containerPort` 8081.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    ...\n    - name: \"http-tracking\"\n      containerPort: 8081\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Listeners for Non-Internal Use in NiFi Kubernetes Deployment (YAML)\nDESCRIPTION: Allows addition of custom or extra listeners (e.g., HTTP endpoints) without linking them to internal NiFi functionality by specifying a name and container port under internalListeners, providing flexibility for exposing custom interfaces such as HTTP endpoints for external communication.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    ...\n    - name: \"http-tracking\"\n      containerPort: 8081\n```\n\n----------------------------------------\n\nTITLE: Graceful handling of pod IP resolution in NiFi startup script\nDESCRIPTION: This snippet modifies the startup script to more gracefully resolve the pod's IP address, reducing startup errors and improving reliability in dynamic network environments. Dependencies include bash scripting and network commands like 'hostname' and 'ip'.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/CHANGELOG.md#_snippet_5\n\nLANGUAGE: Shell\nCODE:\n```\n#!/bin/sh\n\n# Improved IP resolution\nIP_ADDRESS=$(hostname -i || echo '127.0.0.1')\necho \"Pod IP: $IP_ADDRESS\"\n# Additional startup commands...\n```\n\n----------------------------------------\n\nTITLE: Running the Migration Script\nDESCRIPTION: Executes the Node.js migration script using the `npm start` command defined in `package.json`. It takes two command-line arguments: `--type` to specify the NiFiKop resource type to migrate (e.g., `cluster`, `dataflow`) and `--namespace` to specify the Kubernetes namespace where the resources reside.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nnpm start -- --type=<NIFIKOP_RESOURCE> --namespace=<K8S_NAMESPACE>\n```\n\n----------------------------------------\n\nTITLE: Example Kubernetes StorageClass Definition\nDESCRIPTION: Defines a Kubernetes StorageClass resource using YAML. This example uses the 'standard' GCE persistent disk provisioner, sets a Delete reclaim policy, and configures the `volumeBindingMode` to `WaitForFirstConsumer` which is recommended for NiFi statefulsets.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/2_setup/1_getting_started.md#_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: exampleStorageclass\nparameters:\n  type: pd-standard\nprovisioner: kubernetes.io/gce-pd\nreclaimPolicy: Delete\nvolumeBindingMode: WaitForFirstConsumer\n```\n\n----------------------------------------\n\nTITLE: Cloning NiFiKop Repository and Navigating to Tutorial Directory Using Shell\nDESCRIPTION: These shell commands clone the NiFiKop GitHub repository and change the current working directory to the tutorial directory for secured NiFi cluster deployment. This sets up the environment necessary to run deployment scripts and access configuration files.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/blog/2020-05-20-secured_nifi_cluster_on_gcp.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/konpyutaika/nifikop/nifikop.git\ncd nifikop/docs/tutorials/secured_nifi_cluster_on_gcp\n```\n\n----------------------------------------\n\nTITLE: Get Zookeeper UID/GID - OpenShift\nDESCRIPTION: This command retrieves the UID/GID allowed for the Zookeeper namespace in OpenShift. It extracts the `openshift.io/sa.scc.supplemental-groups` annotation from the `zookeeper` namespace, removes the `/10000` suffix, and removes any whitespace. This UID/GID is then used to configure the Zookeeper deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nzookeper_uid=$(kubectl get namespace zookeeper -o=jsonpath='{.metadata.annotations.openshift\\.io/sa\\.scc\\.supplemental-groups}' | sed 's/\\/10000$//' | tr -d '[:space:]')\n```\n\n----------------------------------------\n\nTITLE: Defining a NifiRegistryClient CRD in YAML\nDESCRIPTION: This YAML manifest defines a `NifiRegistryClient` Custom Resource for NiFiKop. It specifies the target NiFi cluster (`clusterRef`), provides a description, and sets the URI for the NiFi Registry instance (`uri`). This resource is a prerequisite for deploying `NifiDataflow` resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/3_tasks/3_nifi_dataflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiRegistryClient\nmetadata:\n  name: registry-client-example\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Registry client managed by NiFiKop\"\n  uri: \"http://nifi.hostname.com:18080\"\n```\n\n----------------------------------------\n\nTITLE: Deploying Prometheus Instance\nDESCRIPTION: Defines a Prometheus server instance configured to scrape metrics from NiFi components. Specifies service monitor selection criteria to target NiFi-related resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: Prometheus\nmetadata:\n  name: prometheus\n  namespace: monitoring-system\nspec:\n  enableAdminAPI: false\n  evaluationInterval: 30s\n  logLevel: debug\n  podMonitorSelector:\n    matchExpressions:\n    - key: release\n      operator: In\n      values:\n      - prometheus\n  resources:\n    requests:\n      memory: 400Mi\n  scrapeInterval: 30s\n  serviceAccountName: prometheus\n  serviceMonitorSelector:\n    matchExpressions:\n    - key: app\n      operator: In\n      values:\n      - nifi-cluster\n```\n\n----------------------------------------\n\nTITLE: Creating K3D Cluster with Kubernetes (Shell)\nDESCRIPTION: This snippet creates a K3D Kubernetes cluster. It specifies the Kubernetes image version to be used (v1.21.10-k3s1 in this example) and waits for the cluster to be ready. The user can modify the Kubernetes version by changing the `--kubernetes-version` argument. This sets up the foundational infrastructure for NiFiKop deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/2_setup/2_platform_setup/2_k3d.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nk3d cluster create --image rancher/k3s:v1.21.10-k3s1 --wait\n```\n\n----------------------------------------\n\nTITLE: Installing Nifikop Helm Chart (Skip CRDs)\nDESCRIPTION: Install the Nifikop Helm chart while explicitly skipping the installation of Custom Resource Definitions (CRDs). Use this option if CRDs are already installed cluster-wide or managed separately. The command includes setting the target namespace(s) and the `--skip-crds` flag.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\n$ helm install --name nifikop ./helm/nifikop --set namespaces={\"nifikop\"} --skip-crds\n```\n\n----------------------------------------\n\nTITLE: Listing All Helm Releases Including Deleted and Failed in Bash\nDESCRIPTION: This Bash command displays a complete list of Helm releases in all states, including active, deleted, and those that failed to deploy. It is useful for troubleshooting release history and diagnosing issues with the nifikop operator or other charts. Prerequisites include Helm installation and cluster access.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --all\n```\n\n----------------------------------------\n\nTITLE: Checking Helm Chart Versions (Make) - Make Shell\nDESCRIPTION: Executes the `make helm-chart-version-match` target from the repository root. This command verifies that the version specified in all individual chart `Chart.yaml` files matches the version in the central `helm/nifikop/Chart.yaml`. It is used in pipelines to enforce version consistency and exits with a non-zero status on mismatch.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/helm/README.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nmake helm-chart-version-match\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFiKop operator with Helm\nDESCRIPTION: Installs the NiFiKop operator v1.4.1 using Helm OCI registry, setting resource constraints, and specifying the managed namespace. Creates a deployment to manage Apache NiFi clusters on Kubernetes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/2_deploy_nifikop/1_quick_start.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# You have to create the namespace before executing following command\nhelm install nifikop \\\n    oci://ghcr.io/konpyutaika/helm-charts/nifikop \\\n    --namespace=nifi \\\n    --version 1.4.1 \\\n    --set image.tag=v1.4.1-release \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set namespaces={\"nifi\"}\n```\n\n----------------------------------------\n\nTITLE: Defining NifiParameterContext in YAML for NiFiKop\nDESCRIPTION: This snippet demonstrates how to define a NifiParameterContext resource which provides configuration for dataflows. It includes both regular parameters and references to secrets for sensitive parameters.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\n  namespace: demo\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      value: toto\n      description: toto\n```\n\n----------------------------------------\n\nTITLE: Metadata Schema for Annotations and Labels\nDESCRIPTION: This schema describes metadata for services, allowing additional annotations and labels to be merged with existing Kubernetes objects. Both maps are optional and default to empty if not specified, enabling enhanced customization and identification of services.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/5_references/1_nifi_cluster/7_external_service_config.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\n|Field|Type|Description|Required|Default|\n|-----|----|-----------|--------|--------|\n| annotations | map[string]string | Additional annotations for the service | No | `nil` |\n| labels  | map[string]string | Additional labels for the service | No | `nil` |\n```\n\n----------------------------------------\n\nTITLE: Defining Istio DestinationRule for HTTPS\nDESCRIPTION: This YAML snippet defines a DestinationRule to manage traffic to the NiFi ClusterIP service, enabling TLS and configuring sticky sessions using an HTTP cookie. The traffic is encrypted here before reaching the NiFi nodes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: DestinationRule\nmetadata:\n  name: nifi-dr\nspec:\n  host: <service-name>.<namespace>.svc.cluster.local\n  trafficPolicy:\n    tls:\n      mode: SIMPLE\n    loadBalancer:\n      consistentHash:\n        httpCookie:\n          name: __Secure-Authorization-Bearer\n          ttl: 0s\n```\n\n----------------------------------------\n\nTITLE: Purging Deleted Helm Release (bash)\nDESCRIPTION: Permanently removes the history of a specified Helm release, `nifikop`, from Helm's storage. This action makes the release name available for re-use without the `--replace` flag. Use with caution, as it removes the ability to rollback the release. Requires Helm installed and the release to be deleted.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nhelm delete --purge nifikop\n```\n\n----------------------------------------\n\nTITLE: Installing cert-manager v1.7.2 (Bash)\nDESCRIPTION: Installs cert-manager v1.7.2, a prerequisite for NiFiKop when deploying secured clusters. Provides two methods: directly applying the manifest using kubectl or using Helm 3 after applying CRDs separately and adding the Jetstack repository.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/2_deploy_nifikop/1_quick_start.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Install the CustomResourceDefinitions and cert-manager itself\nkubectl apply -f \\\n    https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.yaml\n```\n\nLANGUAGE: bash\nCODE:\n```\n# Install CustomResourceDefinitions first\nkubectl apply --validate=false -f \\\n   https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.crds.yaml\n\n# Add the jetstack helm repo\nhelm repo add jetstack https://charts.jetstack.io\nhelm repo update\n\n# You have to create the namespace before executing following command\nhelm install cert-manager \\\n    --namespace cert-manager \\\n    --version v1.7.2 jetstack/cert-manager\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Global and Node-Specific Settings\nDESCRIPTION: This example highlights how to set NiFi configurations at the cluster level or per node via the ReadOnlyConfig field, including thread count limits and other operational parameters. These settings influence NiFi's runtime behavior.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n/* Reference: Spec.ReadOnlyConfig for NiFi global or node-specific configurations */\n```\n\n----------------------------------------\n\nTITLE: Deploying CRDs for Local Operator Run\nDESCRIPTION: Applies Kubernetes Custom Resource Definitions (CRDs) required for the operator, preparing the environment for local deployment outside the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/6_contributing/1_developer_guide.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\n```\n\n----------------------------------------\n\nTITLE: Example Pod Port Configuration Generated from Internal Listeners\nDESCRIPTION: This YAML snippet shows the resulting port configuration in a pod specification after applying the internal listeners configuration. It specifies each containerPort with its corresponding name and protocol.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n    ports:\n    - containerPort: 8443\n      name: https\n      protocol: TCP\n    - containerPort: 6007\n      name: cluster\n      protocol: TCP\n    - containerPort: 6342\n      name: load-balance\n      protocol: TCP\n    - containerPort: 10000\n      name: s2s\n      protocol: TCP\n    - containerPort: 9090\n      name: prometheus\n      protocol: TCP\n```\n\n----------------------------------------\n\nTITLE: Building NiFiKop Operator Using Makefile in Local Go Environment (Bash)\nDESCRIPTION: This snippet triggers the build of the NiFiKop operator binary via a Makefile target. It assumes a local Go environment setup with the required Go version installed as specified. The 'make build' command compiles the source code into an executable. It is designed for local development and recompilation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/6_contributing/1_developer_guide.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmake build\n```\n\n----------------------------------------\n\nTITLE: Setting Parameters While Installing NiFiKop Helm Chart in Bash\nDESCRIPTION: Shows how to install the NiFiKop Helm chart with explicit configuration parameters passed via the --set flag, here restricting the operator's watch namespaces to \"nifikop\". This facilitates a customized operator deployment without the need for a values.yaml file.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/2_setup/3_install/1_customizable_install_with_helm.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop --set namespaces={\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Deploying KEDA with Helm (Console)\nDESCRIPTION: Commands to create a dedicated namespace for KEDA and then install the KEDA core components using its Helm chart. This deploys the necessary controllers into the Kubernetes cluster to enable event-driven autoscaling. Requires Kubectl and Helm configured to interact with the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl create namespace keda\nhelm install keda kedacore/keda --namespace keda\n```\n\n----------------------------------------\n\nTITLE: Deploying a Simple NiFi Cluster with kubectl\nDESCRIPTION: Creates a NiFi cluster in Kubernetes by applying a YAML manifest, which includes configuration for the NiFi and Zookeeper services. It serves as a minimal example to deploy NiFi for testing or initial setup, with the Zookeeper service specified in the configuration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/2_setup/1_getting_started.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# Add your zookeeper svc name to the configuration\nkubectl create -n nifi -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Creating a Secret for Basic Authentication Credentials in Kubernetes\nDESCRIPTION: Creates a Kubernetes secret containing username, password, and optional CA certificate to authenticate to the NiFi cluster using basic authentication. The secret is referenced in the NiFiCluster resource to enable secure API access.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/3_external_cluster.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nkubectl create secret generic nifikop-credentials \\\n  --from-file=username=./secrets/username\\\n  --from-file=password=./secrets/password\\\n  --from-file=ca.crt=./secrets/ca.crt\\\n  -n nifikop-nifi\n```\n\n----------------------------------------\n\nTITLE: Adding KEDA Helm Repository with Console\nDESCRIPTION: This snippet shows how to add the 'kedacore' Helm charts repository to your local Helm configuration using the console. It is required before installing KEDA into your Kubernetes cluster. No input parameters are necessary. The command must be run with Helm v3 installed on your system.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nhelm repo add kedacore https://kedacore.github.io/charts\n```\n\n----------------------------------------\n\nTITLE: NifiUser YAML Schema for Resource Definition\nDESCRIPTION: This YAML schema defines the core structure of the NifiUser custom resource, including apiVersion, kind, metadata, spec, and status. It models the desired and observed states, facilitating the management of NiFi users through Kubernetes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/5_references/2_nifi_user.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\n`apiVersion: nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: aguitton\nspec:\n  identity: alexandre.guitton@konpyutaika.com\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  createCert: false`\n```\n\n----------------------------------------\n\nTITLE: Installing Zookeeper with UID/GID Configuration on OpenShift\nDESCRIPTION: Retrieves the appropriate UID/GID for the 'zookeeper' namespace's security context, then installs Zookeeper with these values specified for 'runAsUser' and 'fsGroup' in Helm. Ensures proper security permissions when deploying in OpenShift.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nuid=$(kubectl get namespace nifi -o=jsonpath=''{.metadata.annotations.openshift\\.io/sa\\.scc\\.supplemental-groups}' | sed 's/\\/10000$//' | tr -d '[:space:]')\n```\n\n----------------------------------------\n\nTITLE: Listing Deleted Helm Releases in Bash\nDESCRIPTION: This Bash command lists all Helm releases marked as deleted in the current Kubernetes context. It helps administrators audit past releases and confirm that nifikop or other resources have been uninstalled. Requires Helm and access to the appropriate cluster context; no additional parameters are needed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --deleted\n```\n\n----------------------------------------\n\nTITLE: Install NiFiKop Migration Dependencies (npm)\nDESCRIPTION: This command initializes a Node.js project and installs the necessary dependencies: `@kubernetes/client-node` for interacting with the Kubernetes API and `minimist` for parsing command-line arguments. These dependencies are crucial for the migration script to function correctly.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm init -y\nnpm install @kubernetes/client-node@0.16.3 minimist@1.2.6\n```\n\n----------------------------------------\n\nTITLE: Patching CRDs with Conversion Webhook Configuration for NifiKop v1.0.0 Migration\nDESCRIPTION: YAML configuration for patching Custom Resource Definitions to enable conversion webhooks between v1alpha1 and v1 versions. This configuration specifies the webhook service and certificate needed for the conversion process.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/7_upgrade_guides/3_v0.16.0_to_v1.0.0.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio Gateway for HTTP Access to NiFi\nDESCRIPTION: Defines an Istio Gateway resource that intercepts HTTP requests on port 80 for a specific domain host and directs them to the NiFi cluster. This is the entry point for external traffic to the NiFi cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: nifi-gateway\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - nifi.my-domain.com\n```\n\n----------------------------------------\n\nTITLE: NifiUserGroupSpec Configuration Model\nDESCRIPTION: This schema details the configuration options within the NifiUserGroupSpec, including cluster reference, user references, and access policies. It specifies dependencies on other schemas, such as ClusterReference, UserReference, and AccessPolicy, establishing the structure for group setup and permissions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/5_references/6_nifi_usergroup.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n# Specification for the desired state of NifiUserGroup\n# Includes references to cluster, users, and access policies.\n\n```\n\n----------------------------------------\n\nTITLE: Installing Helm Chart with Namespace Parameter Using Bash\nDESCRIPTION: This Helm install command installs the nifikop chart while explicitly setting the Kubernetes namespace as `nifikop`. It demonstrates customizing the chart deployment scope using the `--set namespaces` parameter, useful for targeting resource creation within designated namespaces.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop --set namespaces={\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFiKop CRDs manually with kubectl\nDESCRIPTION: This code applies multiple CRD YAML manifests from the NiFiKop GitHub repository directly into the Kubernetes cluster to define custom resources such as nificlusters, nifiusers, and related entities. This is necessary if Helm's `--skip-crds` option is used or manual CRD deployment is preferred.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/2_deploy_nifikop/1_quick_start.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\n\n```\n\n----------------------------------------\n\nTITLE: Pushing Docker Image for NiFiKop to Registry with Make in Bash\nDESCRIPTION: This Makefile command pushes the previously built NiFiKop operator Docker image to the specified registry. Assumes that the image was already built and that Docker credentials are configured to access the target repository. The exact image tag depends on project versioning conventions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/6_contributing/1_developer_guide.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nmake docker-push\n```\n\n----------------------------------------\n\nTITLE: Configuring NifiCluster InitContainerImage with Bash - YAML\nDESCRIPTION: This YAML manifest demonstrates the recommended configuration for nifikop v0.15.0, setting the initContainerImage to 'bash' with tag '5.2.2'. It should be used to replace custom configurations where a different image was previously specified. Dependencies involve ensuring the specified container image includes a Bash shell. Parameters include apiVersion, kind, metadata.name, and spec.initContainerImage. Inputs must match the NifiCluster CRD schema. This configuration satisfies the new dependency introduced with the operator update.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/7_upgrade_guides/2_v0.14.1_to_v0.15.0.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  initContainerImage:\n    repository: bash\n    tag: \"5.2.2\"\n```\n\n----------------------------------------\n\nTITLE: Defining Istio VirtualService for HTTPS\nDESCRIPTION: This `VirtualService` configuration redirects HTTP traffic to a service within the cluster, after decryption.  It uses the `nifi-gateway` for routing and targets a service specified by `<service-name>.<namespace>.svc.cluster.local` on port 8443. It is crucial for forwarding traffic after the initial HTTPS decryption performed by the Gateway. It requires the service to be reachable within the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\n```yaml\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: nifi-vs\nspec:\n  gateways:\n  - nifi-gateway\n  hosts:\n  - nifi.my-domain.com\n  http:\n  - match:\n    - uri:\n        prefix: /\n    route:\n    - destination:\n        host: <service-name>.<namespace>.svc.cluster.local\n        port:\n          number: 8443\n```\n```\n\n----------------------------------------\n\nTITLE: Creating a Kubernetes Secret for Sensitive Parameters (kubectl)\nDESCRIPTION: This `kubectl` command creates a generic Kubernetes secret named `secret-params` in the `nifikop` namespace. The secret contains key-value pairs (`secret1=yop`, `secret2=yep`) intended to be used as sensitive parameters within a `NifiParameterContext`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl create secret generic secret-params \\\n    --from-literal=secret1=yop \\\n    --from-literal=secret2=yep \\\n    -n nifikop\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus Custom Resource\nDESCRIPTION: YAML definition for the Prometheus custom resource that specifies scraping settings, resource limits, and ServiceMonitor selection criteria for monitoring NiFi clusters.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: monitoring.coreos.com/v1\nkind: Prometheus\nmetadata:\n  name: prometheus\n  namespace: monitoring-system\nspec:\n  enableAdminAPI: false\n  evaluationInterval: 30s\n  logLevel: debug\n  podMonitorSelector:\n    matchExpressions:\n    - key: release\n      operator: In\n      values:\n      - prometheus\n  resources:\n    requests:\n      memory: 400Mi\n  scrapeInterval: 30s\n  serviceAccountName: prometheus\n  serviceMonitorSelector:\n    matchExpressions:\n    - key: app\n      operator: In\n      values:\n      - nifi-cluster\n```\n\n----------------------------------------\n\nTITLE: Port-Forwarding Prometheus Operated Service - console\nDESCRIPTION: Provides kubectl command for port-forwarding the Prometheus service 'prometheus-operated' to localhost for web UI or Prometheus API access. Requires a running Prometheus pod and appropriate kubectl access. Forwards remote 9090 port to local 9090. Local port can be changed if already used.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_12\n\nLANGUAGE: console\nCODE:\n```\nkubectl port-forward -n monitoring-system service/prometheus-operated 9090:9090\n```\n\n----------------------------------------\n\nTITLE: Initializing Kubernetes Namespace for Prometheus Deployment - console\nDESCRIPTION: This snippet creates a dedicated Kubernetes namespace called \"monitoring-system\" for deploying Prometheus stack components isolated from other workloads. It uses the kubectl CLI to provision the namespace in the Kubernetes cluster, which is required as a prerequisite for all subsequent Prometheus operator resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_5\n\nLANGUAGE: console\nCODE:\n```\nkubectl create namespace monitoring-system\n```\n\n----------------------------------------\n\nTITLE: List of Overrideable NiFi Configuration Files\nDESCRIPTION: This snippet lists the specific NiFi configuration files that can be overridden within the NifiKop project, including references to their GitHub locations. It emphasizes the importance of caution when modifying defaults to prevent operational issues.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n- nifi.properties\n- zookeeper.properties\n- bootstrap.properties\n- logback.xml\n- authorizers.xml\n- bootstrap_notification_services.xml\n```\n\n----------------------------------------\n\nTITLE: Building NiFiKop Operator Docker Image (Bash)\nDESCRIPTION: Sets the DOCKER_REPO_BASE environment variable to specify the target Docker registry and repository, then executes 'make docker-build'. This process builds a Docker image containing the NiFiKop operator code, ready to be pushed to a registry for deployment in a Kubernetes cluster. It requires Docker installed and configured, and the Makefile to be present.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/6_contributing/1_developer_guide.md#_snippet_6\n\nLANGUAGE: Bash\nCODE:\n```\nexport DOCKER_REPO_BASE={your-docker-repo}\nmake docker-build\n```\n\n----------------------------------------\n\nTITLE: Pushing NiFiKop Docker Image\nDESCRIPTION: This command pushes the built Docker image to the specified Docker repository. It requires the image to be built first.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/6_contributing/1_developer_guide.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n$ make docker-push\n```\n\n----------------------------------------\n\nTITLE: Specifying NifiCluster InitContainerImage with busybox - YAML\nDESCRIPTION: Defines a NifiCluster resource with a custom initContainerImage repository set to 'busybox' version '1.34.0'. This configuration is used prior to upgrade version 0.15.0 and serves as an example of the default behavior being overridden. The snippet requires Kubernetes CRD support for 'nifi.konpyutaika.com/v1alpha1' and assumes the existence of a NifiCluster operator that recognizes the 'initContainerImage' field under spec.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/7_upgrade_guides/2_v0.14.1_to_v0.15.0.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  initContainerImage:\n    repository: busybox\n    tag: \"1.34.0\"\n```\n\n----------------------------------------\n\nTITLE: Building NiFiKop Operator Locally - Bash\nDESCRIPTION: This snippet builds the NiFiKop operator using the local Go environment. It assumes that Go and Make are installed and that dependencies are already resolved. This command compiles the operator binary for development and testing.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/6_contributing/1_developer_guide.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmake build\n```\n\n----------------------------------------\n\nTITLE: Attaching Context to Errors in Go\nDESCRIPTION: This snippet shows how to enrich errors with context information and stack traces using the `emperror.dev/errors` package. It demonstrates different ways to wrap errors with messages, stack traces, and key-value details. These methods include `errors.Wrap`, `errors.WrapIf`, `errors.WithStack`, `errors.WithMessage`, `errors.WithDetails`, and `errors.WrapIfWithDetails`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/docs/error-handling-guide.md#_snippet_2\n\nLANGUAGE: go\nCODE:\n```\nimport (\n\t\"emperror.dev/errors\"\n)\n\n// ...\n\nerr := doSomething()\n\n// Attach a message and stack trace to the error\n// Note: this overwrites any previous stack trace\nerr = errors.Wrap(err, \"some additional message\")\n\n// Attach a message and stack trace to the error\n// Same as above with the exception that it does not override already existing stack trace\n// Use this if a stack trace is already available\nerr = errors.WrapIf(err, \"some additional message\")\n\n// Attach stack trace to the error without attaching a message\n// Note: this overwrites any previous stack trace\nerr = errors.WithStack(err)\n\n// Attach message to the error without attaching stack trace\nerr = errors.WithMessage(err, \"some additional message\")\n\n// Attach arbitrary context (mostly key-value pairs) to the error\nerr = errors.WithDetails(err, \"key1\", \"value1\", \"key2\", \"value2\" /*,...*/)\n\n// Combination of errors.WrapIf and errors.WithDetails\nerr = errors.WrapIfWithDetails(err, \"key1\", \"value1\", \"key2\", \"value2\" /*,...*/)\n```\n\n----------------------------------------\n\nTITLE: Get NiFi UID/GID on OpenShift\nDESCRIPTION: This snippet retrieves the allowed UID/GID for the NiFi namespace on OpenShift. It's similar to the Zookeeper UID retrieval but targets the NiFi namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nuid=$(kubectl get namespace nifi -o=jsonpath='{.metadata.annotations.openshift\\.io/sa\\.scc\\.supplemental-groups}' | sed 's/\\/10000$//' | tr -d '[:space:]')\n```\n\n----------------------------------------\n\nTITLE: Adding Prometheus Helm Repository\nDESCRIPTION: Adds the Prometheus community Helm chart repository to fetch the chart definitions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_6\n\nLANGUAGE: console\nCODE:\n```\nhelm repo add prometheus https://prometheus-community.github.io/helm-charts\n```\n\n----------------------------------------\n\nTITLE: Viewing Kubernetes Services Exposing NiFi Cluster in Console\nDESCRIPTION: Shows the output of the `kubectl get services` command listing the Kubernetes services exposing the NiFi cluster externally. It includes service names, types, cluster IPs, external IP addresses, and ports mapping, which illustrate how the NiFi cluster services are accessible via LoadBalancer IPs and ports configured in the external service YAML.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nkubectl get services\nNAME                TYPE           CLUSTER-IP    EXTERNAL-IP      PORT(S)                                AGE\ncluster-access      LoadBalancer   10.88.21.98   35.180.241.132   443:30421/TCP,80:30521/TCP             20d\n```\n\n----------------------------------------\n\nTITLE: Adding KEDA Helm Repository - Helm CLI - console\nDESCRIPTION: This shell command adds the official KEDA Helm chart repository to your Helm configuration. Helm must be installed on your client machine. The command makes it possible to later install or update KEDA via Helm by referencing the 'kedacore' repository URL.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nhelm repo add kedacore https://kedacore.github.io/charts\n```\n\n----------------------------------------\n\nTITLE: YAML Configuration for Custom initContainerImage with busybox\nDESCRIPTION: This snippet shows an example Kubernetes YAML manifest defining a NifiCluster resource with a custom initContainerImage set to 'busybox'. It illustrates how users have overridden the default image and need to update it during upgrade.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/7_upgrade_guides/2_v0.14.1_to_v0.15.0.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  initContainerImage:\n    repository: busybox\n    tag: \"1.34.0\"\n```\n\n----------------------------------------\n\nTITLE: Packaging NiFiKop Helm Chart - Bash\nDESCRIPTION: Executes 'make helm-package' to package the NiFiKop Helm chart for release or distribution. This facilitates Helm chart management and deployment. Requires make and Helm configured.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/6_contributing/1_developer_guide.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nmake helm-package\n```\n\n----------------------------------------\n\nTITLE: Updated YAML configuration for initContainerImage to use bash\nDESCRIPTION: This snippet shows the corrected YAML configuration where the initContainerImage repository is changed to 'bash' with an updated tag. It is intended for users modifying their configuration to comply with the new default image requirement after the upgrade.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/7_upgrade_guides/2_v0.14.1_to_v0.15.0.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  initContainerImage:\n    repository: bash\n    tag: \"5.2.2\"\n```\n\n----------------------------------------\n\nTITLE: Get OpenShift Security Context Constraint UID Range\nDESCRIPTION: Retrieves the supplemental group range annotation from the target `nifi` namespace on OpenShift using `kubectl`. This value is needed to configure the `runAsUser` setting when deploying the NiFiKop operator Helm chart in a restricted environment like OpenShift.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/2_deploy_nifikop/1_quick_start.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nuid=$(kubectl get namespace nifi -o=jsonpath='{.metadata.annotations.openshift\\.io/sa\\.scc\\.supplemental-groups}' | sed 's/\\/10000$//' | tr -d '[:space:]')\n```\n\n----------------------------------------\n\nTITLE: Deploying Prometheus Operator with Helm\nDESCRIPTION: Helm command to deploy the kube-prometheus-stack with customized configuration to focus on just the prometheus operator components needed for NiFi monitoring.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_5\n\nLANGUAGE: console\nCODE:\n```\nhelm install prometheus prometheus/kube-prometheus-stack --namespace monitoring-system \\\n    --set prometheusOperator.createCustomResource=false \\\n    --set prometheusOperator.logLevel=debug \\\n    --set prometheusOperator.alertmanagerInstanceNamespaces=monitoring-system \\\n    --set prometheusOperator.namespaces.additional[0]=monitoring-system \\\n    --set prometheusOperator.prometheusInstanceNamespaces=monitoring-system \\\n    --set prometheusOperator.thanosRulerInstanceNamespaces=monitoring-system \\\n    --set defaultRules.enable=false \\\n    --set alertmanager.enabled=false \\\n    --set grafana.enabled=false \\\n    --set kubeApiServer.enabled=false \\\n    --set kubelet.enabled=false \\\n    --set kubeControllerManager.enabled=false \\\n    --set coreDNS.enabled=false \\\n    --set kubeEtcd.enabled=false \\\n    --set kubeScheduler.enabled=false \\\n    --set kubeProxy.enabled=false \\\n    --set kubeStateMetrics.enabled=false \\\n    --set prometheus.enabled=false\n```\n\n----------------------------------------\n\nTITLE: Configuring Identity Mapping for OIDC in nifi.properties\nDESCRIPTION: Recommended identity mapping configuration for nifi.properties to ensure multiple identity provider support with OpenId Connect authentication.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/5_users_authentication/1_oidc.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nnifi.security.identity.mapping.pattern.dn=CN=([^,]*)(?:, (?:O|OU)=.*)?\\nnifi.security.identity.mapping.value.dn=$1\\nnifi.security.identity.mapping.transform.dn=NONE\n```\n\n----------------------------------------\n\nTITLE: Configuring CRD Conversion Webhook\nDESCRIPTION: This snippet demonstrates how to configure the CRD conversion webhook if you're keeping it enabled. This is necessary to handle the conversion of resources from `v1alpha1` to `v1`. This requires `kubectl`, an existing cert-manager setup, and knowledge of the release name and namespace.  The output is a YAML configuration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Nifikop CRD Conversion Webhook\nDESCRIPTION: This YAML snippet illustrates how to configure the `conversion` section within the CRD definition when keeping the conversion webhook enabled. It requires specifying annotations for certificate injection and the service details for the webhook client configuration to handle resource version conversions (v1alpha1 to v1). Place this configuration under the `spec` of your CRD definition.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Conversion Webhook (YAML)\nDESCRIPTION: This YAML snippet shows the configuration needed within your custom resource definition for enabling a conversion webhook. It is related to handling the conversion of resources from v1alpha1 to v1. You need to add these annotations to your yaml definition of CRDs.  Requires you have the `cert-manager` installed and configured in a Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/2_deploy_nifikop/1_quick_start.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n```\n\n----------------------------------------\n\nTITLE: Applying NifiCluster Scale Up Configuration (Shell)\nDESCRIPTION: Uses the `kubectl apply` command to apply the updated `NifiCluster` configuration (from `config/samples/simplenificluster.yaml`) to the 'nifi' namespace, triggering the cluster scale-up process by adding the new node.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nkubectl -n nifi apply -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop Operator via Helm Chart (Bash)\nDESCRIPTION: Uses the 'helm install' command to deploy the NiFiKop operator into a Kubernetes cluster using its Helm chart. The command installs the chart located at './helm/nifikop', names the release 'skeleton', specifies the Docker image tag to use, and sets the target namespace to 'nifikop'. This is a standard method for deploying applications packaged as Helm charts.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/6_contributing/1_developer_guide.md#_snippet_8\n\nLANGUAGE: Bash\nCODE:\n```\nhelm install skeleton ./helm/nifikop \\\n    --set image.tag=v0.5.1-release \\\n    --namespace-\"{\"nifikop\"}\"\n```\n\n----------------------------------------\n\nTITLE: Cloning Project Repository with Git Bash\nDESCRIPTION: This snippet demonstrates how to clone the NiFiKop operator project from GitHub using Git and change into the project directory. Dependencies include git. No parameters are required beyond a working git installation and internet connection. The output is a new local directory named 'nifikop' containing the source code.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/6_contributing/1_developer_guide.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/konpyutaika/nifikop.git\ncd nifikop\n```\n\n----------------------------------------\n\nTITLE: Installing Nifikop with Dry Run\nDESCRIPTION: This command performs a dry run installation of the Nifikop Helm chart. It simulates the installation process without actually deploying the chart to the cluster.  Key parameters are: `--dry-run`, `--set logLevel=Debug`, and `--set namespaces={\\\"nifikop\\\"}`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop \\\n    --dry-run \\\n    --set logLevel=Debug \\\n    --set namespaces={\\\"nifikop\\\"}\n```\n\n----------------------------------------\n\nTITLE: Verify NiFiKop Kubectl Plugin Installation\nDESCRIPTION: This command executes the `kubectl nifikop` plugin to confirm that it was successfully installed and is recognized by the `kubectl` command-line tool. The output displayed shows the plugin's basic usage syntax and lists the available subcommands for interacting with NiFiKop resources within Kubernetes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/2_deploy_nifikop/3_kubectl_plugin.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\n$ kubectl nifikop\nUsage:\n  nifikop [command]\n\nAvailable Commands:\n  completion          Generate the autocompletion script for the specified shell\n  help                Help about any command\n  nificluster         \n  nificonnection      \n  nifidataflow        \n  nifigroupautoscaler \n  nifiregistryclient  \n  nifiuser            \n  nifiusergroup \n```\n\n----------------------------------------\n\nTITLE: NifiNodeGroupAutoscalerSpec Schema Explanation\nDESCRIPTION: This detailed schema provides definitions for each field within the spec of the autoscaler, such as clusterRef, nodeConfigGroupId, label selectors, and scaling strategies. Each field’s required status, data type, and default values are explained to facilitate accurate configuration and deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/5_references/7_nifi_nodegroup_autoscaler.md#_snippet_2\n\n\n\n----------------------------------------\n\nTITLE: Updating NiFi OpenShift configuration with correct UID in Bash\nDESCRIPTION: Modifies the 'config/samples/openshift.yaml' configuration file to replace a placeholder UID value (1000690000) with the actual UID retrieved for the 'nifi' namespace. This ensures the NiFi pods run with correct security permissions on OpenShift using sed stream editor for in-place substitution.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nsed -i \"s/1000690000/$uid/g\" config/samples/openshift.yaml\n```\n\n----------------------------------------\n\nTITLE: Cloning NiFiKop Repository and Navigating to Directory - Bash\nDESCRIPTION: This snippet demonstrates how to clone the NiFiKop repository from GitHub and change the working directory to the repository root. It requires git to be installed on the system. Expected input is the standard command line, and the output is the cloned project ready for local development.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/6_contributing/1_developer_guide.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/konpyutaika/nifikop.git\ncd nifikop\n```\n\n----------------------------------------\n\nTITLE: Cloning the NiFiKop Repository using Bash\nDESCRIPTION: Clones the NiFiKop source code from its GitHub repository using `git clone` and changes the current directory to the newly cloned project folder (`nifikop`) using `cd`. This is the first step for setting up the development environment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/6_contributing/1_developer_guide.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/konpyutaika/nifikop.git\ncd nifikop\n```\n\n----------------------------------------\n\nTITLE: Defining Start Script package.json Node.js JSON\nDESCRIPTION: This JSON snippet adds a 'start' script definition to the project's `package.json` file. It configures `npm start` to execute the `index.js` file using Node.js with the `--no-warnings` flag. This provides a convenient way to run the migration script.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"start\": \"node --no-warnings index.js\"\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop via Helm Chart (Bash)\nDESCRIPTION: Deploys the NiFiKop operator using the local Helm chart (`./helm/nifikop`). It specifies the release name, target namespace, and overrides the image tag to use a custom-built image (e.g., `v0.5.1-release`). Ensure the specified image exists in the repository.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/6_contributing/1_developer_guide.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nhelm install skeleton ./helm/nifikop \\\n    --set image.tag=v0.5.1-release \\\n    --namespace-{\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio Gateway for HTTPS in NiFi\nDESCRIPTION: This YAML configuration defines an Istio Gateway that intercepts HTTPS traffic on port 443, handling TLS termination with a simple mode and credential reference for a specific domain host.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: nifi-gateway\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 443\n      name: https\n      protocol: HTTPS\n    tls:\n      mode: SIMPLE\n      credentialName: my-secret\n    hosts:\n    - nifi.my-domain.com\n```\n\n----------------------------------------\n\nTITLE: Defining a NiFi Parameter Context using YAML in Kubernetes\nDESCRIPTION: Example YAML definition for creating a NifiParameterContext resource. It includes basic configuration with parameter definitions, cluster reference, and secret references for sensitive parameters.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/5_references/4_nifi_parameter_context.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiParameterContext\nmetadata:\n  name: dataflow-lifecycle\nspec:\n  description: \"It is a test\"\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  secretRefs:\n    - name: secret-params\n      namespace: nifikop\n  parameters:\n    - name: test\n      value: toto\n      description: tutu\n    - name: test2\n      description: toto\n      sensistive: true\n```\n\n----------------------------------------\n\nTITLE: Installing Cert-Manager Directly via kubectl\nDESCRIPTION: Installs cert-manager v1.7.2 and its CustomResourceDefinitions (CRDs) directly into the Kubernetes cluster using `kubectl apply` with the official YAML manifest URL. Requires `kubectl` configured for the target cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/2_deploy_nifikop/1_quick_start.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Install the CustomResourceDefinitions and cert-manager itself\nkubectl apply -f \\\n    https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.yaml\n```\n\n----------------------------------------\n\nTITLE: Dry Run Helm Installation for Nifikop\nDESCRIPTION: Simulates the installation of the Nifikop Helm chart using the `--dry-run` flag. This command checks the generated Kubernetes manifests without actually applying them to the cluster. It also sets the log level to Debug and specifies the target namespaces.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop \\\n    --dry-run \\\n    --set logLevel=Debug \\\n    --set namespaces={\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Manually Applying CRDs for Nifikop on Kubernetes\nDESCRIPTION: Commands to manually deploy CustomResourceDefinitions (CRDs) for Nifikop if '--skip-crds' was used during Helm installation. These commands fetch and apply CRD YAML files from the repository to the cluster, establishing custom resource schemas.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\n```\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\n```\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\n```\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\n```\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\n```\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\n```\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifinodegroupautoscalers.yaml\n```\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificonnections.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Identity Mapping Patterns (Shell)\nDESCRIPTION: Sets recommended properties in `nifi.properties` to handle user identity mapping from Distinguished Names (DNs), specifically extracting the Common Name (CN) part. This configuration aids in supporting multiple identity providers.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/5_users_authentication/1_oidc.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nnifi.security.identity.mapping.pattern.dn=CN=([^,]*)(?:, (?:O|OU)=.*)?\nnifi.security.identity.mapping.value.dn=$1\nnifi.security.identity.mapping.transform.dn=NONE\n```\n\n----------------------------------------\n\nTITLE: Configuring kubectl for EKS (Bash)\nDESCRIPTION: This command configures `kubectl` to connect to your EKS cluster. Requires the AWS CLI installed and configured, `eksctl` installed, and the cluster name. It uses `eksctl` to write the kubeconfig and then selects the appropriate context for `kubectl`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/2_deploy_nifikop/1_quick_start.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\neksctl utils write-kubeconfig --cluster=${CLUSTER NAME}\n```\n\n----------------------------------------\n\nTITLE: Listing Managed NifiUserGroup Resources with Kubectl - Console\nDESCRIPTION: This console command demonstrates how to use kubectl to list the NifiUserGroup resources managed by the NifiKop operator within a specific namespace ('nifikop'). This allows verification that the operator has created the predefined managed groups, including 'managed-admins', 'managed-nodes', and 'managed-readers'.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/3_manage_nifi/2_manage_users_and_accesses/3_managed_groups.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nkubectl get -n nifikop nifiusergroups.nifi.konpyutaika.com\n```\n\n----------------------------------------\n\nTITLE: NifiUser Creation\nDESCRIPTION: This YAML snippet demonstrates how to create a `NifiUser` resource to generate client certificates for applications. It specifies the cluster reference (`clusterRef`) and the secret name (`secretName`) where the generated credentials will be stored.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion:  nifi.konpyutaika.com/v1\nkind: NifiUser\nmetadata:\n  name: example-client\n  namespace: nifi\nspec:\n  clusterRef:\n    name: nifi\n  secretName: example-client-secret\nEOF\n```\n\n----------------------------------------\n\nTITLE: Defining Node.js Start Script - JSON\nDESCRIPTION: Adds a \"start\" script entry to the `scripts` section of the `package.json` file. This script specifies how to run the main migration logic contained in `index.js` using Node.js, suppressing warnings. Requires a `package.json` file and the `index.js` script to be present.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"start\": \"node --no-warnings index.js\"\n```\n\n----------------------------------------\n\nTITLE: Installing cert-manager Directly (Bash)\nDESCRIPTION: This command installs the CustomResourceDefinitions and cert-manager directly from a URL.  It downloads and applies the necessary YAML configuration to set up cert-manager in the Kubernetes cluster.  Dependencies include a running Kubernetes cluster and `kubectl` installed and configured.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/2_deploy_nifikop/1_quick_start.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install the CustomResourceDefinitions and cert-manager itself\nkubectl apply -f \\\n    https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.yaml\n```\n\n----------------------------------------\n\nTITLE: ActionStep Enumeration in NiFiKop\nDESCRIPTION: Enumeration of the different steps and statuses that occur during node lifecycle operations such as connecting, disconnecting, offloading, and removing nodes from a NiFi cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/5_references/1_nifi_cluster/5_node_state.md#_snippet_5\n\nLANGUAGE: go\nCODE:\n```\ntype ActionStep string\n\nconst (\n\tDisconnectNodeAction ActionStep = \"DISCONNECTING\"\n\tDisconnectStatus ActionStep = \"DISCONNECTED\"\n\tOffloadNodeAction ActionStep = \"OFFLOADING\"\n\tOffloadStatus ActionStep = \"OFFLOADED\"\n\tRemovePodAction ActionStep = \"POD_REMOVING\"\n\tRemovePodStatus ActionStep = \"POD_REMOVED\"\n\tRemoveNodeAction ActionStep = \"REMOVING\"\n\tRemoveStatus ActionStep = \"REMOVED\"\n\tConnectNodeAction ActionStep = \"CONNECTING\"\n\tConnectStatus ActionStep = \"CONNECTED\"\n)\n```\n\n----------------------------------------\n\nTITLE: Packaging the NiFiKop Helm Chart using Bash\nDESCRIPTION: Executes the `helm-package` target from the project's Makefile. This command packages the Helm chart source files located in the Helm directory (e.g., `./helm/nifikop`) into a versioned chart archive file (`.tgz`) suitable for distribution or repository upload. Requires Helm v3.4.2+.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/6_contributing/1_developer_guide.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nmake helm-package\n```\n\n----------------------------------------\n\nTITLE: Installing Nifikop Helm Chart with Custom Namespaces - Bash\nDESCRIPTION: This Helm install command deploys the Nifikop chart with an explicit namespaces value, overriding the default. It enables targeting specific namespaces for Nifikop resources. Dependencies are Helm and a configured kubeconfig. The main parameter is namespaces; output is the chart deployed with that configuration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop --set namespaces={\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Updating Helm Repository in Console\nDESCRIPTION: This snippet updates the locally cached index of Helm repositories to fetch the latest charts, including the KedaCore repository. Running this command ensures you have access to the newest versions of KEDA and its dependencies before installation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nhelm repo update\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio VirtualService for HTTP NiFi Routing\nDESCRIPTION: This YAML configuration defines an Istio VirtualService that routes traffic intercepted by the Gateway to the NiFi service. It directs all requests for the specified host to the NiFi service on port 8080.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: nifi-vs\nspec:\n  gateways:\n  - nifi-gateway\n  hosts:\n  - nifi.my-domain.com\n  http:\n  - match:\n    - uri:\n        prefix: /\n    route:\n    - destination:\n        host: nifi\n        port:\n          number: 8080\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFiCluster with External DNS and Let's Encrypt\nDESCRIPTION: YAML configuration for a NiFi cluster that uses External DNS and Let's Encrypt for SSL certificate issuance. This setup automates the process of obtaining and renewing valid SSL certificates.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  clusterSecure: true\n  siteToSiteSecure: true\n  ...\n  listenersConfig:\n    clusterDomain: <DNS zone name>\n    useExternalDNS: true\n    ...\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n      issuerRef:\n        name: letsencrypt-staging\n        kind: Issuer\n```\n\n----------------------------------------\n\nTITLE: Installing KEDA via Helm\nDESCRIPTION: Creates a dedicated Kubernetes namespace 'keda' using `kubectl create namespace` and then installs the KEDA Helm chart from the 'kedacore' repository into that namespace using `helm install`. This deploys the KEDA components necessary for event-driven autoscaling.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl create namespace keda\nhelm install keda kedacore/keda --namespace keda\n```\n\n----------------------------------------\n\nTITLE: Exposing NiFi Cluster Port in K3D (Shell)\nDESCRIPTION: Modifies the default K3D cluster (k3s-default) to expose a specific NiFi cluster port through the cluster's load balancer. Replace `<nifi_cluster_port>` with the actual port number required for external NiFi access. This requires an existing K3D cluster named 'k3s-default'.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/2_setup/2_platform_setup/2_k3d.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nk3d cluster edit k3s-default --port-add \"<nifi_cluster_port>:<nifi_cluster_port>@loadbalancer\"\n```\n\n----------------------------------------\n\nTITLE: Uninstalling Nifikop Helm Release - Bash\nDESCRIPTION: This command deletes the Nifikop Helm release, removing associated Kubernetes resources except CRDs. Requires Helm and permissions to delete cluster resources. The main parameter is the release name. Output is successful removal of the deployment resources; CRDs may need manual deletion separately.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nhelm del nifikop\n```\n\n----------------------------------------\n\nTITLE: Installing cert-manager CRDs for Helm Installation (Bash)\nDESCRIPTION: Applies only the cert-manager CustomResourceDefinitions (CRDs) from the specified release URL. This step is necessary before installing the cert-manager Helm chart itself. Version v1.7.2 CRDs are used here.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/2_deploy_nifikop/1_quick_start.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Install CustomResourceDefinitions first\nkubectl apply --validate=false -f \\\n   https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.crds.yaml\n```\n\n----------------------------------------\n\nTITLE: Complete package.json Configuration for NiFiKop Migration Tool\nDESCRIPTION: The complete package.json configuration for the NiFiKop CRD migration tool, including dependencies, metadata, and scripts.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/7_upgrade/1_v0.7.x_to_v0.8.0.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"nifikop_crd_migration\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Script to migrate from the old CRDs to the new CRDs.\",\n  \"main\": \"index.js\",\n  \"scripts\": {\n    \"start\": \"node --no-warnings index.js\",\n    \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\"\n  },\n  \"keywords\": [\n    \"K8S\",\n    \"NiFiKop\",\n    \"CRDs\"\n  ],\n  \"license\": \"ISC\",\n  \"dependencies\": {\n    \"@kubernetes/client-node\": \"^0.16.3\",\n    \"minimist\": \"^1.2.6\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Helm Chart Documentation (Make) - Make Shell\nDESCRIPTION: Executes the `make helm-gen-docs` target from the repository root. This command provides a simplified way to trigger the Helm chart documentation generation process, likely wrapping the direct `docker run` command or similar logic within a Makefile target. It requires `make` to be installed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/helm/README.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nmake helm-gen-docs\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop via Helm Chart (bash)\nDESCRIPTION: This Helm command installs the NiFiKop operator using the provided Helm chart. It creates a release named 'skeleton', specifies the tag for the operator image, and targets the 'nifikop' namespace. This is the standard method for deploying the operator to a Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/6_contributing/1_developer_guide.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nhelm install skeleton ./helm/nifikop \\\n    --set image.tag=v0.5.1-release \\\n    --namespace-{\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Pushing the NiFiKop Docker Image using Bash\nDESCRIPTION: Executes the `docker-push` target from the project's Makefile. This command pushes the previously built Docker image (tagged automatically with version and branch name) to the container registry specified by the `DOCKER_REPO_BASE` environment variable. Requires prior login to the target Docker registry.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/6_contributing/1_developer_guide.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nmake docker-push\n```\n\n----------------------------------------\n\nTITLE: Kubernetes ExternalService Schema Definition\nDESCRIPTION: This markdown table describes the 'ExternalService' object used in Kubernetes YAML configurations, detailing each field, its data type, purpose, and whether it is required. It covers metadata, specifications, and optional parameters like load balancer settings.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/5_references/1_nifi_cluster/7_external_service_config.md#_snippet_1\n\n\n\n----------------------------------------\n\nTITLE: Building Docker Image - Bash\nDESCRIPTION: This snippet builds a Docker image for the NiFiKop operator using `make docker-build`.  It uses the `DOCKER_REPO_BASE` environment variable. This command compiles the operator's code, creates a Docker image, and tags it. The output is a Docker image that is ready to be pushed to a container registry.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/6_contributing/1_developer_guide.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nexport DOCKER_REPO_BASE={your-docker-repo}\nmake docker-build\n```\n\n----------------------------------------\n\nTITLE: Defining NifiCluster initContainerImage with bash in YAML\nDESCRIPTION: This YAML snippet defines a NifiCluster resource where the initContainerImage is set to use the bash repository tagged '5.2.2'. This reflects the required change after upgrading to version 0.15.0 to ensure the init container contains a bash shell as mandated by the updated default. Dependencies include having an image containing bash available. The input is the updated image specification, and the output is correct init container behavior post-upgrade.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/7_upgrade_guides/2_v0.14.1_to_v0.15.0.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  initContainerImage:\n    repository: bash\n    tag: \"5.2.2\"\n```\n\n----------------------------------------\n\nTITLE: Cloning NiFiKop Git Repository (Bash)\nDESCRIPTION: This snippet provides the standard Git commands to clone the NiFiKop project repository from GitHub and navigate into the project directory. This is the initial step required to obtain the source code for local development or building. It assumes Git is installed and configured on the system.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/6_contributing/1_developer_guide.md#_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\ngit clone https://github.com/konpyutaika/nifikop.git\ncd nifikop\n```\n\n----------------------------------------\n\nTITLE: Describing NifiCluster Status - Shell\nDESCRIPTION: This `kubectl` command retrieves the status of the `simplenifi` NifiCluster resource. The status includes information about the nodes' states and the progress of the graceful downscale action, useful for monitoring the decommissioning process.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nkubectl describe nificluster simplenifi\n\n...\nStatus:\n  Nodes State:\n    ...\n    2:\n      Configuration State:  ConfigInSync\n      Graceful Action State:\n        Action State:   GracefulDownscaleRequired\n        Error Message:\n    ...\n...\n```\n\n----------------------------------------\n\nTITLE: Anti-pattern: Logging and Returning Error\nDESCRIPTION: This snippet illustrates an anti-pattern where an error is both logged with context and then returned. This practice can lead to multiple log events and complicate debugging. The `log.WithSomeContext().Error(err)` call logs the error, and `return err` propagates it up the call stack.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/docs/error-handling-guide.md#_snippet_1\n\nLANGUAGE: go\nCODE:\n```\nerr := doSomething()\nif err != nil {\n\tlog.WithSomeContext().Error(err)\n\treturn err\n}\n```\n\n----------------------------------------\n\nTITLE: Install cert-manager with kubectl\nDESCRIPTION: This snippet installs cert-manager directly using `kubectl`. It applies the CustomResourceDefinitions and the cert-manager itself from a specified URL. The command downloads and applies the necessary YAML files for cert-manager to function within the Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/2_deploy_nifikop/1_quick_start.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install the CustomResourceDefinitions and cert-manager itself\nkubectl apply -f \\\n    https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.yaml\n```\n\n----------------------------------------\n\nTITLE: Testing Kubectl NiFiKop Plugin Installation (Console)\nDESCRIPTION: This command tests the successful installation of the kubectl-nifikop plugin by executing the command `kubectl nifikop`. The expected output is the plugin's usage message, listing the available subcommands for managing NiFi resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/2_deploy_nifikop/3_kubectl_plugin.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\n$ kubectl nifikop\nUsage:\n  nifikop [command]\n\nAvailable Commands:\n  completion          Generate the autocompletion script for the specified shell\n  help                Help about any command\n  nificluster         \n  nificonnection      \n  nifidataflow        \n  nifigroupautoscaler \n  nifiregistryclient  \n  nifiuser            \n  nifiusergroup \n```\n\n----------------------------------------\n\nTITLE: Listing Deleted Helm Releases (Bash)\nDESCRIPTION: This Helm command lists releases that have been previously deleted. By default, Helm keeps a record of deleted releases, which allows for potential rollback or auditing. This command helps view only those releases marked with the 'deleted' status.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/2_setup/3_install/1_customizable_install_with_helm.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --deleted\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom StorageClass for Persistent Volumes\nDESCRIPTION: Defines a StorageClass with specific parameters for volume provisioning, suitable for use with NiFi clusters utilizing volume binding mode 'WaitForFirstConsumer'. Remember to associate it properly with your NiFiCluster CR.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/2_setup/1_getting_started.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: storage.k8s.io/v1\\nkind: StorageClass\\nmetadata:\\n  name: exampleStorageclass\\nparameters:\\n  type: pd-standard\\nprovisioner: kubernetes.io/gce-pd\\nreclaimPolicy: Delete\\nvolumeBindingMode: WaitForFirstConsumer\n```\n\n----------------------------------------\n\nTITLE: Listing of Overridable NiFi Configuration Files\nDESCRIPTION: Lists the specific configuration files that can be overridden in NiFi, including links to their templates within the NifiKop repository. Modifying these files allows customization of NiFi behavior; however, caution is advised as improper changes may cause operator failures.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_4\n\n\n\n----------------------------------------\n\nTITLE: AccessPolicy YAML Schema for Defining Permissions\nDESCRIPTION: This schema defines the structure for access policies assigned to NiFi users. It includes fields like type, action, resource, componentType, and componentId, supporting both global and component-specific policies for granular permission control.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/5_references/2_nifi_user.md#_snippet_5\n\nLANGUAGE: YAML\nCODE:\n```\n`AccessPolicy:\n  type: [AccessPolicyType]\n  action: [AccessPolicyAction]\n  resource: [AccessPolicyResource]\n  componentType: string\n  componentId: string`\n```\n\n----------------------------------------\n\nTITLE: Define NiFi Identity Mapping Properties (sh)\nDESCRIPTION: Illustrates recommended `nifi.security.identity.mapping` properties for NiFi's `nifi.properties` file. These settings help configure how user identities are extracted and mapped, particularly useful for supporting multiple identity providers.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/1_deploy_cluster/5_users_authentication/1_oidc.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nnifi.security.identity.mapping.pattern.dn=CN=([^,]*)(?:, (?:O|OU)=.*)?\nnifi.security.identity.mapping.value.dn=$1\nnifi.security.identity.mapping.transform.dn=NONE\n```\n\n----------------------------------------\n\nTITLE: SecretConfigReference Structure in Markdown\nDESCRIPTION: This table defines the structure for referencing Secret resources in Kubernetes, specifying the name, namespace, and specific data key to use within the Secret.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/5_references/1_nifi_cluster/2_read_only_config.md#_snippet_7\n\nLANGUAGE: markdown\nCODE:\n```\n## SecretConfigReference\n\n|Field|Type|Description|Required|Default|\n|-----|----|-----------|--------|--------|\n|name|string|Name of the secret that we want to refer.|Yes|\"\"|\n|namespace|string|Namespace where is located the secret that we want to refer.|No|\"\"|\n|data|string|The key of the value,in data content, that we want use.|Yes|\"\"|\n```\n\n----------------------------------------\n\nTITLE: Installing KEDA using Helm (Console)\nDESCRIPTION: Creates a dedicated Kubernetes namespace 'keda' using `kubectl` and then installs the KEDA chart from the 'kedacore' repository into that namespace using Helm. This deploys the KEDA components required for event-driven autoscaling.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl create namespace keda\nhelm install keda kedacore/keda --namespace keda\n```\n\n----------------------------------------\n\nTITLE: Creating Secret for NifiParameterContext\nDESCRIPTION: This console command creates a Kubernetes secret named `secret-params` in the `nifikop` namespace. The secret stores key-value pairs that can be referenced by the `NifiParameterContext`.  This secret contains two literals, `secret1` and `secret2`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl create secret generic secret-params \\\n    --from-literal=secret1=yop \\\n    --from-literal=secret2=yep \\\n    -n nifikop\n```\n\n----------------------------------------\n\nTITLE: Installing Zookeeper with Helm for NiFi State Management\nDESCRIPTION: Command to install Zookeeper using Bitnami's Helm chart with recommended configuration for NiFi. Zookeeper is required for NiFi cluster state management unless using Kubernetes native state management.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhelm install zookeeper oci://registry-1.docker.io/bitnamicharts/zookeeper \\\n    --namespace=zookeeper \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set global.storageClass=standard \\\n    --set networkPolicy.enabled=true \\\n    --set replicaCount=3 \\\n    --create-namespace\n```\n\n----------------------------------------\n\nTITLE: Example Pod Port Configuration in YAML\nDESCRIPTION: Shows how the internal listeners configuration translates to container port definitions in the Kubernetes pod specification. This includes all the necessary ports for NiFi's internal communication mechanisms.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n    ports:\n    - containerPort: 8443\n      name: https\n      protocol: TCP\n    - containerPort: 6007\n      name: cluster\n      protocol: TCP\n    - containerPort: 6342\n      name: load-balance\n      protocol: TCP\n    - containerPort: 10000\n      name: s2s\n      protocol: TCP\n    - containerPort: 9090\n      name: prometheus\n      protocol: TCP\n```\n\n----------------------------------------\n\nTITLE: Setting Operator Name - Bash\nDESCRIPTION: This snippet sets the `OPERATOR_NAME` environment variable to `nifi-operator`.  This variable is used within the operator's configuration. This step is part of running the operator using the Go binary outside of the cluster. This will affect how the operator is built and deployed, which will affect its behavior in the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/6_contributing/1_developer_guide.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport OPERATOR_NAME=nifi-operator\n```\n\n----------------------------------------\n\nTITLE: Installing cert-manager directly for NiFiKop on Kubernetes\nDESCRIPTION: Installs cert-manager v1.7.2 directly using kubectl by applying the manifest from the official GitHub repository. This is required for NiFiKop to issue certificates for users and nodes when deploying secured clusters.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/2_deploy_nifikop/1_quick_start.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install the CustomResourceDefinitions and cert-manager itself\nkubectl apply -f \\\n    https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.yaml\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFi Cluster on Kubernetes\nDESCRIPTION: Creates a NiFi cluster based on a sample YAML configuration, integrating with Zookeeper. Needs updating with the actual Zookeeper service name. When deploying on OpenShift, retrieve UID/GID for security context and modify the deployment YAML accordingly.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create -n nifi -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Verifying NiFiKop Operator Pod Status Using kubectl in Bash\nDESCRIPTION: This command lists pods in the 'nifikop' namespace using kubectl to verify the NiFiKop operator pod is running and ready. It outputs pod name, ready status, restarts, and uptime. Kubernetes CLI access and permissions are mandatory.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/6_contributing/1_developer_guide.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get pods -n nifikop\n```\n\n----------------------------------------\n\nTITLE: Configuring Headless vs All Service Mode for NiFi Nodes (Markdown)\nDESCRIPTION: Explains the concept of exposing NiFi nodes internally using either a headless service (recommended for most cases) that allows DNS-based per-node access or one service per node for direct access, controlled via the Spec.HeadlessEnabled boolean setting.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_3\n\n\n\n----------------------------------------\n\nTITLE: Overriding Zookeeper Helm Chart Values (JSON)\nDESCRIPTION: Allows overriding values for the bundled Zookeeper Helm chart dependency. It includes configuration for enabling Zookeeper deployment, persistence settings (size, storage class), replica count, and resource requests/limits for the Zookeeper pods.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/helm/nifi-cluster/README.md#_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\"enabled\":false,\"persistence\":{\"size\":\"10Gi\",\"storageClass\":\"standard\"},\"replicaCount\":1,\"resources\":{\"limits\":{\"cpu\":2,\"memory\":\"500Mi\"},\"requests\":{\"cpu\":\"0.5m\",\"memory\":\"250Mi\"}}}\n```\n\n----------------------------------------\n\nTITLE: Creating NiFi User Credential Secrets with CRD\nDESCRIPTION: Shows how to define a `NifiUser` custom resource to generate certificates and store credentials in a secret. The secret includes CA certificate, user certificate, and private key, enabling secure client authentication to NiFi.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/3_tasks/2_security/1_ssl.md#_snippet_4\n\nLANGUAGE: Console\nCODE:\n```\ncat << EOF | kubectl apply -n nifi -f -\napiVersion:  nifi.konpyutaika.com/v1alpha1\nkind: NifiUser\nmetadata:\n  name: example-client\n  namespace: nifi\nspec:\n  clusterRef:\n    name: nifi\n  secretName: example-client-secret\nEOF\n```\n\n----------------------------------------\n\nTITLE: Deploying NifiRegistryClient in Kubernetes\nDESCRIPTION: This YAML configuration defines a NifiRegistryClient resource in Kubernetes. It specifies the cluster reference, description, and URI of the NiFi Registry instance.  This is a prerequisite for deploying NifiDataflows using NiFiKop.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiRegistryClient\nmetadata:\n  name: registry-client-example\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Registry client managed by NiFiKop\"\n  uri: \"http://nifi.hostname.com:18080\"\n```\n\n----------------------------------------\n\nTITLE: Verifying Operator Pod Status (Console)\nDESCRIPTION: Uses `kubectl get pods` to check the status of the NiFiKop operator pod within the specified namespace (`nifikop`) after deployment, typically via Helm. Verifies if the pod is in a 'Running' state.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/6_contributing/1_developer_guide.md#_snippet_8\n\nLANGUAGE: console\nCODE:\n```\n$ kubectl get pods -n nifikop\nNAME                                                READY   STATUS    RESTARTS   AGE\nskeleton-nifikop-8946b89dc-4cfs9   1/1     Running   0          7m45s\n```\n\n----------------------------------------\n\nTITLE: Pushing Docker Image to Repository\nDESCRIPTION: Command to push the locally built Docker image to a remote repository such as Docker Hub, preparing it for deployment via Helm.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/6_contributing/1_developer_guide.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmake docker-push\n```\n\n----------------------------------------\n\nTITLE: Automated Algorithm Change Using Kubernetes initContainer\nDESCRIPTION: Kubernetes YAML configuration for an initContainer that automatically updates the flow configuration using the NiFi Encrypt-Config Tool. This approach requires stopping the operator, adding the initContainer, upgrading, and then removing it.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/7_upgrade_guides/4_v1.3.1_to_v1.4.0.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ninitContainers:\n  - image: \"apache/nifi-toolkit:latest\"\n    name: nifi-toolkit\n    imagePullPolicy: Always\n    command:\n      - \"sh\"\n      - \"-c\"\n      - \"NIFI_SENSITIVE_PROPS_KEY=$(grep 'nifi.sensitive.props.key' /opt/nifi/nifi-current/conf/nifi.properties | cut -d'=' -f2) && bin/encrypt-config.sh -n /opt/nifi/nifi-current/conf/nifi.properties -f /opt/nifi/data/flow.json.gz -x -A NIFI_PBKDF2_AES_GCM_256 -s $NIFI_SENSITIVE_PROPS_KEY; bin/encrypt-config.sh -n /opt/nifi/nifi-current/conf/nifi.properties -f /opt/nifi/data/flow.xml.gz -x -A NIFI_PBKDF2_AES_GCM_256 -s $NIFI_SENSITIVE_PROPS_KEY\"\n    volumeMounts:\n      - name: data\n        mountPath: /opt/nifi/data\n      - name: conf\n        mountPath: /opt/nifi/nifi-current/conf\n```\n\n----------------------------------------\n\nTITLE: Getting UID range for NiFiKop deployment on OpenShift\nDESCRIPTION: Retrieves the UID range from the OpenShift namespace annotation to properly configure NiFiKop to run with the restricted Security Context Constraint (SCC) on OpenShift.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/2_deploy_nifikop/1_quick_start.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nuid=$(kubectl get namespace nifi -o=jsonpath='{.metadata.annotations.openshift\\.io/sa\\.scc\\.supplemental-groups}' | sed 's/\\/10000$//' | tr -d '[:space:]')\n```\n\n----------------------------------------\n\nTITLE: Listing All Helm Releases (Including Deleted and Failed) - Bash\nDESCRIPTION: This command lists all Helm releases, including those that are deployed, deleted, or have failed, by using the --all flag. Prerequisites are Helm CLI and cluster access. Output is a comprehensive list of all releases and their states.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --all\n```\n\n----------------------------------------\n\nTITLE: Helm Repository Add and Update\nDESCRIPTION: These commands add the Orange Incubator Helm repository and then update the local Helm repository cache. This makes the NiFiKop chart available for installation. These commands are a pre-requisite for installing NiFiKop via Helm.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/blog/2020-06-30-secured_nifi_cluster_on_gcp_with_external_dns.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nhelm repo add orange-incubator https://orange-kubernetes-charts-incubator.storage.googleapis.com/\nhelm repo update\n```\n\n----------------------------------------\n\nTITLE: Purging Helm Release - Bash\nDESCRIPTION: Completely removes the specified Helm release and all related history records from the cluster using the --purge flag (for Helm v2 compatibility). After purging, the release name can be reused. Helm version must support the --purge flag (deprecated in Helm v3). Parameter: release name.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nhelm delete --purge nifikop\n```\n\n----------------------------------------\n\nTITLE: Mounting External Volumes for Additional Configuration or Data Persistence\nDESCRIPTION: This snippet shows how to mount existing external volumes not managed by the operator to add configuration files or persist data for NiFi nodes. It uses the StorageConfig field for external volume integration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n/* Reference: Spec.NodeConfigGroup.StorageConfig for external volume mounting */\n```\n\n----------------------------------------\n\nTITLE: Using external volumes for additional data or configurations\nDESCRIPTION: This YAML snippet shows how to mount pre-existing or external volumes by defining storage configurations, allowing integration of non-managed volumes for custom data or configuration storage outside operator control.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nstorageConfigs:\n  - mountPath: /path/inside/container\n    name: external-volume\n    reclaimPolicy: Retain\n    metadata:\n      labels:\n        env: production\n    pvcSpec:\n      accessModes:\n        - ReadWriteMany\n      resources:\n        requests:\n          storage: 10Gi\n      storageClassName: \"manual\"\n\n```\n\n----------------------------------------\n\nTITLE: Configuring package.json Start Script (JSON)\nDESCRIPTION: Shows how to add a `start` script to the `scripts` section of a `package.json` file. This script (`node --no-warnings index.js`) is used to execute the migration script (`index.js`) while suppressing Node.js warnings.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/7_upgrade/1_v0.7.x_to_v0.8.0.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"start\": \"node --no-warnings index.js\"\n```\n\n----------------------------------------\n\nTITLE: Listing NifiUserGroup Resources\nDESCRIPTION: This `kubectl` command is used to retrieve a list of NifiUserGroup resources from the 'nifikop' namespace. The output shows the names and ages of the managed groups (managed-admins, managed-nodes, managed-readers) created by the operator.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/3_tasks/4_nifi_user_group.md#_snippet_3\n\nLANGUAGE: console\nCODE:\n```\nkubectl get -n nifikop nifiusergroups.nifi.konpyutaika.com \nNAME              AGE\nmanaged-admins    6d7h\nmanaged-nodes     6d7h\nmanaged-readers   6d7h\n```\n\n----------------------------------------\n\nTITLE: Delete NiFiKop Helm Release - Bash\nDESCRIPTION: Deletes a specific Helm release (e.g., `nifikop`) using the `helm del` command. This action removes all associated Kubernetes resources that were created by the chart. The release record is retained by default, allowing for potential rollback.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/2_setup/3_install/1_customizable_install_with_helm.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nhelm del nifikop\n```\n\n----------------------------------------\n\nTITLE: Updating KEDA Helm Repository - Helm CLI - console\nDESCRIPTION: This shell command updates your local Helm chart repository indexes, ensuring that the latest versions of charts—including KEDA—are available for installation. Helm must be properly installed prior to running this command.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nhelm repo update\n```\n\n----------------------------------------\n\nTITLE: Installing Zookeeper on OpenShift with Security Context\nDESCRIPTION: Helm command to install Zookeeper on OpenShift with specific security context settings including runAsUser and fsGroup parameters based on OpenShift's constraints.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nhelm install zookeeper bitnami/zookeeper \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set global.storageClass=standard \\\n    --set networkPolicy.enabled=true \\\n    --set replicaCount=3 \\\n    --set containerSecurityContext.runAsUser=$zookeper_uid \\\n    --set podSecurityContext.fsGroup=$zookeper_uid\n```\n\n----------------------------------------\n\nTITLE: ExternalServiceSpec Schema Details\nDESCRIPTION: This table explains the fields within 'ExternalServiceSpec', including port configurations, cluster IP, service type, external IPs, load balancer settings, and external name, providing guidance on their use and default values.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/5_references/1_nifi_cluster/7_external_service_config.md#_snippet_2\n\n\n\n----------------------------------------\n\nTITLE: Port-forwarding Prometheus Service - Console\nDESCRIPTION: Establishes a local port-forward from your workstation's port 9090 to the `prometheus-operated` service running on port 9090 in the `monitoring-system` namespace. This allows you to access the Prometheus web UI via `http://localhost:9090`. Requires `kubectl` and network access to the Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_11\n\nLANGUAGE: console\nCODE:\n```\nkubectl port-forward -n monitoring-system service/prometheus-operated 9090:9090\n```\n\n----------------------------------------\n\nTITLE: Add Start Script to package.json (JSON)\nDESCRIPTION: This JSON snippet adds a `start` script to the `package.json` file.  This script allows the user to run the migration script using the command `npm start`. The `--no-warnings` flag suppresses any warnings during execution.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"start\": \"node --no-warnings index.js\"\n```\n\n----------------------------------------\n\nTITLE: Get Secrets via Kubectl\nDESCRIPTION: These kubectl commands retrieve the CA certificate, TLS certificate, and TLS key from the specified secret, decode them from base64, and write them to local files.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nkubectl get secret example-client-secret -o jsonpath=\"{[\\'data\\'][\\'ca\\.crt\\']}\" | base64 -d > ca.crt\nkubectl get secret example-client-secret -o jsonpath=\"{[\\'data\\'][\\'tls\\.crt\\']}\" | base64 -d > tls.crt\nkubectl get secret example-client-secret -o jsonpath=\"{[\\'data\\'][\\'tls\\.key\\']}\" | base64 -d > tls.key\n```\n\n----------------------------------------\n\nTITLE: Deploy NiFi Cluster - Kubernetes\nDESCRIPTION: This command deploys a NiFi cluster using a YAML file located at `config/samples/simplenificluster.yaml`. It assumes that the Zookeeper service name has been added to the configuration file. The cluster will be created in the `nifi` namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# Add your zookeeper svc name to the configuration\nkubectl create -n nifi -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio Gateway for HTTP NiFi Access\nDESCRIPTION: This YAML configuration defines an Istio Gateway that intercepts all HTTP requests on port 80 for a specific domain host. It's used to enable external access to a NiFi cluster deployed on Kubernetes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: nifi-gateway\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - nifi.my-domain.com\n```\n\n----------------------------------------\n\nTITLE: Retrieving UID/GID for OpenShift Zookeeper Deployment\nDESCRIPTION: Retrieves the allowed UID/GID for deploying Zookeeper on OpenShift. Requires kubectl access and appropriate annotations. Used to set security context parameters for Zookeeper deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nzookeper_uid=$(kubectl get namespace zookeeper -o=jsonpath='{.metadata.annotations.openshift\\.io/sa\\.scc\\.supplemental-groups}' | sed 's/\\/10000$//' | tr -d '[:space:]')\n```\n\n----------------------------------------\n\nTITLE: Secret Content for TLS Authentication in Kubernetes\nDESCRIPTION: This section details the required secret contents for TLS authentication against the external NiFi cluster. It includes private key, client certificate, user password, CA certificate, and keystore/truststore files, which are necessary for establishing secure TLS communication.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/3_external_cluster.md#_snippet_2\n\n\n\n----------------------------------------\n\nTITLE: Purging a Helm Release Completely Using Bash\nDESCRIPTION: This Helm command completely deletes the `nifikop` release and purges its record from Helm’s storage to allow clean removal and potential re-use of the release name. It overrides the default behavior of preserving release history.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nhelm delete --purge nifikop\n```\n\n----------------------------------------\n\nTITLE: Go Struct for NifiRegistryClientStatus\nDESCRIPTION: This Go struct captures the observed state of the NiFi Registry Client, including its unique identifier and last known revision version. It helps controllers or operators track changes and manage lifecycle events for NiFi registry clients.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/5_references/3_nifi_registry_client.md#_snippet_3\n\nLANGUAGE: Go\nCODE:\n```\ntype NifiRegistryClientStatus struct {\n    Id      string `json:\"id\"`\n    Version int64  `json:\"version\"`\n}\n```\n\n----------------------------------------\n\nTITLE: Viewing External Service in Kubernetes with kubectl\nDESCRIPTION: This command shows how to view the created external services using kubectl, displaying the service type, cluster IP, external IP, and exposed ports for accessing the NiFi cluster externally.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nkubectl get services\nNAME                TYPE           CLUSTER-IP    EXTERNAL-IP      PORT(S)                                AGE\ncluster-access      LoadBalancer   10.88.21.98   35.180.241.132   443:30421/TCP,80:30521/TCP             20d\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Identity Mapping Properties (Shell)\nDESCRIPTION: Recommended `nifi.properties` settings for configuring identity mapping within Apache NiFi, particularly useful for supporting multiple identity providers. These properties define patterns and transformations for mapping Distinguished Names (DNs) to user identities.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/3_tasks/2_security/2_authentication/1_oidc.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\nnifi.security.identity.mapping.pattern.dn=CN=([^,]*)(?:, (?:O|OU)=.*)?\nnifi.security.identity.mapping.value.dn=$1\nnifi.security.identity.mapping.transform.dn=NONE\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop Helm chart with optional parameters\nDESCRIPTION: This note indicates that additional parameters can be added to customize the NiFiKop Helm deployment, such as disabling cert-manager for unsecured clusters, providing flexibility in different deployment scenarios.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/2_deploy_nifikop/1_quick_start.md#_snippet_5\n\nLANGUAGE: console\nCODE:\n```\nAdd the following parameter if you are using this instance to only deploy unsecured clusters: --set certManager.enabled=false\n```\n\n----------------------------------------\n\nTITLE: Patching CRDs with Conversion Webhook Configuration in YAML\nDESCRIPTION: YAML configuration for patching CRDs to enable conversion webhooks required for migrating resources from v1alpha1 to v1. The patch adds the necessary annotations for cert-manager integration and configures the webhook for conversion between versions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/7_upgrade_guides/3_v0.16.0_to_v1.0.0.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n```\n\n----------------------------------------\n\nTITLE: LDAP Configuration Schema for Nifi\nDESCRIPTION: Specifies LDAP integration settings, including enabling flag, list of LDAP server URLs, search base DN, and search filter. Configures external LDAP authentication for Nifi clusters. This schema helps manage LDAP-related connection settings but does not include code operations.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/5_references/1_nifi_cluster/1_nifi_cluster.md#_snippet_2\n\n\n\n----------------------------------------\n\nTITLE: Applying NifiCluster Configuration Changes (Shell)\nDESCRIPTION: This command uses `kubectl` to apply the configuration changes defined in the `simplenificluster.yaml` file to the `nifi` namespace in Kubernetes. This triggers the NiFiKop operator to reconcile the state, including adding the new node specified in the updated manifest.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/3_tasks/1_nifi_cluster/2_cluster_scaling.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nkubectl -n nifi apply -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Namespace with kubectl - Console\nDESCRIPTION: Initializes a dedicated namespace named \"monitoring-system\" within the Kubernetes cluster. This command organizes Prometheus and related monitoring resources for isolation and management. No parameters other than the namespace name are required.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_3\n\nLANGUAGE: console\nCODE:\n```\nkubectl create namespace monitoring-system\n\n```\n\n----------------------------------------\n\nTITLE: Creating a New Version of Documentation\nDESCRIPTION: The following commands are used to cut a new version of the documentation. This involves navigating to the `website` directory and using the Docusaurus command `docs:version` to create a new version.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/README.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\n1.  `cd react-native-website` to go into the project root.\n1.  `cd website` to go into the website portion of the project.\n1.  Run `yarn run docusaurus docs:version <newVersion>` where `<newVersion>` is the new version being released.\n```\n\n----------------------------------------\n\nTITLE: Port Forwarding to Access Prometheus Web UI - console\nDESCRIPTION: kubectl command to forward port 9090 from the Prometheus service in the 'monitoring-system' namespace to the local machine, enabling local browser access to the Prometheus web UI on http://localhost:9090 for metrics querying and monitoring verification.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_9\n\nLANGUAGE: console\nCODE:\n```\nkubectl port-forward -n monitoring-system service/prometheus-operated 9090:9090\n```\n\n----------------------------------------\n\nTITLE: Adding Start Script to package.json\nDESCRIPTION: Defines an npm script named 'start' within the `scripts` section of `package.json`. This script executes the `index.js` file using Node.js, with the `--no-warnings` flag to suppress warnings.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"start\": \"node --no-warnings index.js\"\n```\n\n----------------------------------------\n\nTITLE: Listing Deleted Helm Releases - Bash\nDESCRIPTION: Runs 'helm list --deleted' to show all chart releases that have been deleted but are still stored in the Helm release history. Requires Helm CLI access. No parameters are needed. Output is a listing of deleted releases for auditing or potential rollback.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --deleted\n```\n\n----------------------------------------\n\nTITLE: Get Zookeeper UID/GID on OpenShift\nDESCRIPTION: This snippet retrieves the allowed UID/GID for the Zookeeper namespace on OpenShift using `kubectl` and `jsonpath`. It extracts the `openshift.io/sa.scc.supplemental-groups` annotation and removes the trailing `/10000` to obtain the UID.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nzookeper_uid=$(kubectl get namespace zookeeper -o=jsonpath='{.metadata.annotations.openshift\\.io/sa\\.scc\\.supplemental-groups}' | sed 's/\\/10000$//' | tr -d '[:space:]')\n```\n\n----------------------------------------\n\nTITLE: Configuring External Services for NiFi in Kubernetes\nDESCRIPTION: Example YAML configuration for defining external services in NiFiKop. This example creates a ClusterIP service exposing port 8080 and maps it to the \"http\" internal listener, with custom annotations and labels.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/5_references/1_nifi_cluster/7_external_service_config.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  externalServices:\n    - name: \"clusterip\"\n      spec:\n        type: ClusterIP\n        portConfigs:\n          - port: 8080\n            internalListenerName: \"http\"\n      metadata:\n        annotations:\n          toto: tata\n        labels:\n          titi: tutu\n```\n\n----------------------------------------\n\nTITLE: NifiParameterContext Object Definition\nDESCRIPTION: Defines the main object for representing a NiFi Parameter Context, including metadata, spec, and status. It establishes the overall resource schema used within the Kubernetes environment for managing NiFi configs.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/5_references/4_nifi_parameter_context.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\nNifiParameterContext:\n  metadata: [ObjectMetadata]\n  spec: [NifiParameterContextSpec]\n  status: [NifiParameterContextStatus]\n```\n\n----------------------------------------\n\nTITLE: Listing All Helm Releases (Deleted, Deployed, Failed) - Bash\nDESCRIPTION: This bash command lists all Helm releases, regardless of their status (deployed, deleted, or failed) in the current Kubernetes cluster. Prerequisite: Helm CLI access to the target Kubernetes cluster. Useful for comprehensive Helm release management and troubleshooting.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --all\n```\n\n----------------------------------------\n\nTITLE: Attaching labels and annotations to PVCs in NifiCluster\nDESCRIPTION: This snippet permits the addition of custom labels and annotations to PersistentVolumeClaims created by the NifiCluster, allowing for better resource management and organization in Kubernetes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/CHANGELOG.md#_snippet_13\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.k8s.io/v1alpha1\nkind: NifiCluster\nspec:\n  volumeClaimTemplates:\n    - metadata:\n        labels:\n          environment: production\n        annotations:\n          backup: true\n      spec:\n        accessModes: [\"ReadWriteOnce\"]\n        resources:\n          requests:\n            storage: 10Gi\n```\n\n----------------------------------------\n\nTITLE: Listing Deleted Helm Releases\nDESCRIPTION: List all Helm releases that have been previously deleted. Helm keeps a record of deleted releases, which is useful for historical tracking or potentially rolling back a deletion. This command displays releases in the 'deleted' status.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --deleted\n```\n\n----------------------------------------\n\nTITLE: Install NiFiKop Chart with values.yaml - Helm\nDESCRIPTION: This command installs the NiFiKop Helm chart using a YAML file to specify configuration values.  The values.yaml file should contain parameter overrides as described in the configuration section.  It requires Helm to be installed and configured to connect to a Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n$ helm install nifikop \\\n      konpyutaika/nifikop \\\n      -f values.yaml\n```\n\n----------------------------------------\n\nTITLE: Describe NiFiCluster status via Kubectl\nDESCRIPTION: This command retrieves the status of the `NifiCluster` resource named `simplenifi`.  It is used to monitor the progress of the node decommissioning process, specifically the `Graceful Action State`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_5\n\nLANGUAGE: console\nCODE:\n```\nkubectl describe nificluster simplenifi\n\n...\nStatus:\n  Nodes State:\n    ...\n    2:\n      Configuration State:  ConfigInSync\n      Graceful Action State:\n        Action State:   GracefulDownscaleRequired\n        Error Message:\n    ...\n...\n```\n\n----------------------------------------\n\nTITLE: Accessing Prometheus Service via Port Forwarding (Console)\nDESCRIPTION: Uses `kubectl port-forward` to map local port 9090 to port 9090 of the `prometheus-operated` service within the `monitoring-system` namespace. This allows accessing the Prometheus web UI via `http://localhost:9090` from the local machine.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_10\n\nLANGUAGE: console\nCODE:\n```\nkubectl port-forward -n monitoring-system service/prometheus-operated 9090:9090\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Cluster Settings at the Global or Node Level\nDESCRIPTION: This YAML snippet highlights how to specify NiFi-specific configuration parameters such as thread counts using the `ReadOnlyConfig` field at either the global or node level, allowing fine-tuning of NiFi processor behavior.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/2_nodes_configuration.md#_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\nspec:\n  readOnlyConfig:\n    maximumTimerDrivenThreadCount: 10\n    maximumEventDrivenThreadCount: 20\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Cluster with External DNS and Let's Encrypt in YAML\nDESCRIPTION: Example YAML configuration for a NiFi cluster that uses External DNS and Let's Encrypt for SSL certificates. This setup enables automatic DNS management and certificate issuance.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  clusterSecure: true\n  siteToSiteSecure: true\n  ...\n  listenersConfig:\n    clusterDomain: <DNS zone name>\n    useExternalDNS: true\n    ...\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n      issuerRef:\n        name: letsencrypt-staging\n        kind: Issuer\n```\n\n----------------------------------------\n\nTITLE: Applying NiFiCluster Configuration (kubectl)\nDESCRIPTION: Applies the updated NifiCluster YAML configuration file to the Kubernetes cluster using kubectl. This command triggers the NiFiKop operator to reconcile the desired state (with the node removed) with the current state, starting the decommissioning process.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\nkubectl -n nifi apply -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Packaging NiFiKop Helm Chart (Bash)\nDESCRIPTION: Executes the `helm-package` target in the Makefile. This command packages the NiFiKop Helm chart located in the `helm/nifikop` directory into a versioned chart archive (`.tgz` file) suitable for distribution or uploading to a Helm repository.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/6_contributing/1_developer_guide.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nmake helm-package\n```\n\n----------------------------------------\n\nTITLE: Applying NifiCluster Scale Down Configuration (Shell)\nDESCRIPTION: Executes the `kubectl apply` command to submit the modified `NifiCluster` configuration (with the node removed) to the 'nifi' namespace, starting the graceful decommissioning and scale-down process managed by NiFiKop.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\nkubectl -n nifi apply -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Install npm dependencies for K8s interaction\nDESCRIPTION: This command initializes a Node.js project and installs the required dependencies: `@kubernetes/client-node` for interacting with the Kubernetes API and `minimist` for parsing command-line arguments.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm init -y\nnpm install @kubernetes/client-node@0.16.3 minimist@1.2.6\n```\n\n----------------------------------------\n\nTITLE: Uninstall Nifikop Chart\nDESCRIPTION: This command uninstalls the Nifikop Helm chart, removing all associated Kubernetes resources. It deletes the helm release, but by default doesn't remove the CRDs, which must be handled separately if needed.  Dependencies: Helm.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nhelm del nifikop\n```\n\n----------------------------------------\n\nTITLE: Running NiFiKop Operator with Make\nDESCRIPTION: This bash command executes the `make run` command to launch the NiFiKop operator within a Kubernetes cluster. It implicitly relies on the `Makefile` to build and deploy the operator.  It requires the CRDs to be deployed before running and assumes `kubectl` is configured correctly.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/6_contributing/1_developer_guide.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmake run\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop Helm Chart Using a Values YAML File in Console\nDESCRIPTION: Illustrates how to install the NiFiKop Helm chart by referencing a custom YAML file that specifies configuration parameters. This approach enables complex and reusable configurations without inline parameter setting.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/2_setup/3_install/1_customizable_install_with_helm.md#_snippet_6\n\nLANGUAGE: console\nCODE:\n```\n$ helm install nifikop \\\n      konpyutaika/nifikop \\\n      -f values.yaml\n```\n\n----------------------------------------\n\nTITLE: Listing Helm Deployed Charts in Bash\nDESCRIPTION: This simple command lists all Helm releases deployed into the Kubernetes cluster, displaying useful information such as release names, namespaces, statuses, and chart versions. It requires Helm CLI version 3 or newer.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nhelm list\n```\n\n----------------------------------------\n\nTITLE: Installing Nifikop Chart with Dry Run using Helm\nDESCRIPTION: Helm command to perform a dry run installation of the Nifikop chart with log level set to Debug and deploying to specified namespaces. It simulates the deployment without making actual changes, useful for validation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop \\\n    --dry-run \\\n    --set logLevel=Debug \\\n    --set namespaces={\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Getting Helm Release Status\nDESCRIPTION: Retrieve the status of a specific Helm release. Replace 'nifikop' with the actual release name if different. This command shows detailed information about the release, including its current status, resources deployed, and notes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nhelm status nifikop\n```\n\n----------------------------------------\n\nTITLE: Listing Deleted Helm Releases (bash)\nDESCRIPTION: Displays a list of all Helm releases that have previously been deleted using `helm del`. Helm retains a history of deleted releases by default. This is useful for reviewing past deployments or identifying releases that can be purged. Requires Helm installed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --deleted\n```\n\n----------------------------------------\n\nTITLE: Building NiFiKop Docker Image (bash)\nDESCRIPTION: This snippet first sets the `DOCKER_REPO_BASE` environment variable to specify the base path for the Docker image repository. It then uses the Makefile's `docker-build` target to build the NiFiKop operator Docker image locally, preparing it for containerized deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/6_contributing/1_developer_guide.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nexport DOCKER_REPO_BASE={your-docker-repo}\nmake docker-build\n```\n\n----------------------------------------\n\nTITLE: Pushing NiFiKop Docker Image (bash)\nDESCRIPTION: This command executes the 'docker-push' target in the Makefile, which pushes the previously built Docker image to the repository specified by `DOCKER_REPO_BASE`. This makes the image available for use in containerized environments like Kubernetes deployments via Helm.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/6_contributing/1_developer_guide.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n$ make docker-push\n```\n\n----------------------------------------\n\nTITLE: Listing all Helm Releases - Bash\nDESCRIPTION: This command lists all Helm releases, including those deleted, currently deployed, and those that failed. This provides a comprehensive view of all Helm deployments, helping with debugging and understanding the overall state of a Helm-managed cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/2_setup/3_install/1_customizable_install_with_helm.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --all\n```\n\n----------------------------------------\n\nTITLE: Apply NiFiCluster Scaledown Configuration - Shell\nDESCRIPTION: This shell command applies the updated NifiCluster configuration, with a node removed, to the Kubernetes cluster using `kubectl`.  It triggers the scale-down process. The `simplenificluster.yaml` file is expected to reflect the desired state of the NiFi cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/3_tasks/1_nifi_cluster/2_cluster_scaling.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nkubectl -n nifi apply -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Retrieving NiFi namespace RunAsUser UID in OpenShift using Bash\nDESCRIPTION: Retrieves the UID for RunAsUser and fsGroup from the OpenShift 'nifi' namespace annotations, enabling configuration of security context for NiFi deployments. The value is extracted with kubectl jsonpath and refined with sed and tr for later use in configuration files.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nuid=$(kubectl get namespace nifi -o=jsonpath='{.metadata.annotations.openshift\\.io/sa\\.scc\\.supplemental-groups}' | sed 's/\\/10000$//' | tr -d '[:space:]')\n```\n\n----------------------------------------\n\nTITLE: Anti-pattern: Formatting Error Messages\nDESCRIPTION: This snippet shows an anti-pattern where errors are formatted as strings instead of attaching context. It uses `errors.WrapIff` to create a formatted error message. The preferred approach is to use `errors.WrapIfWithDetails` to add context information directly to the error.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/docs/error-handling-guide.md#_snippet_3\n\nLANGUAGE: go\nCODE:\n```\nreturn errors.WrapIff(err, \"something %s failed\", important)\n```\n\n----------------------------------------\n\nTITLE: Viewing Kubernetes Services for NiFi with kubectl (Console Example)\nDESCRIPTION: Shows the output of the 'kubectl get services' command after deploying external services for NiFi. It displays the mapping between service names, types, cluster IPs, external IPs, associated ports, and service age. This is a verification step requiring a functioning cluster, kubectl access, and a successful deployment of the external service configuration. Useful for validating external endpoint exposure for NiFi components.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nkubectl get services\nNAME                TYPE           CLUSTER-IP    EXTERNAL-IP      PORT(S)                                AGE\ncluster-access      LoadBalancer   10.88.21.98   35.180.241.132   443:30421/TCP,80:30521/TCP             20d\n```\n\n----------------------------------------\n\nTITLE: Building NiFiKop Locally (bash)\nDESCRIPTION: This command utilizes the project's Makefile to build the NiFiKop operator binary locally. It compiles the Go code and generates an executable that can be run for development or testing outside of a container.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/6_contributing/1_developer_guide.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmake build\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFi Cluster on Kubernetes\nDESCRIPTION: Creates a NiFi cluster by applying the 'simplenificluster.yaml' configuration within the 'nifi' namespace. Assumes the YAML file defines necessary NiFi and Zookeeper settings. Uses 'kubectl create' for deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Add your zookeeper svc name to the configuration\nkubectl create -n nifi -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Manually Installing NiFiKop CRDs in Kubernetes\nDESCRIPTION: Bash commands to manually install required Custom Resource Definitions (CRDs) for NiFiKop when using the --skip-crds flag during Helm installation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifinodegroupautoscalers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificonnections.yaml\n```\n\n----------------------------------------\n\nTITLE: Manually Applying NiFi CRDs with kubectl in Bash\nDESCRIPTION: This snippet demonstrates how to manually apply NiFiKop-related CustomResourceDefinitions (CRDs) to a Kubernetes cluster using kubectl if you want to skip CRD installation during the Helm chart deployment (`--skip-crds`). All CRD URLs should be applied in order to ensure the operator can manage all NiFi resource types. Requires working kubectl access and appropriate cluster privileges. No direct input/output values and no retries/rollback features are present.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/2_setup/3_install/1_customizable_install_with_helm.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\n```\n\n----------------------------------------\n\nTITLE: Uninstalling NiFiKop Helm Chart\nDESCRIPTION: This command removes the NiFiKop chart and associated Kubernetes components from the cluster. It also deletes the Helm release. The CRDs created by the chart are not removed by default and may need to be manually cleaned up.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/helm/nifikop/README.md#_snippet_7\n\nLANGUAGE: Shell\nCODE:\n```\n$ helm del nifikop\n```\n\n----------------------------------------\n\nTITLE: Kubectl Get Pods in zookeeper\nDESCRIPTION: This command retrieves a list of pods within the zookeeper namespace. It verifies the successful deployment and operation of the Zookeeper cluster. The output includes pod names, readiness status, and age.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/blog/2020-06-30-secured_nifi_cluster_on_gcp_with_external_dns.md#_snippet_6\n\nLANGUAGE: console\nCODE:\n```\nkubectl -n zookeeper get pods\n```\n\n----------------------------------------\n\nTITLE: Install NiFiKop Helm Chart with Values File - Console\nDESCRIPTION: Installs the NiFiKop Helm chart using configurations provided in a YAML file (`values.yaml`). This is an alternative to using `--set` for specifying multiple parameters. Requires a local `values.yaml` file containing the desired chart overrides.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/2_setup/3_install/1_customizable_install_with_helm.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n$ helm install nifikop \\\n      konpyutaika/nifikop \\\n      -f values.yaml\n```\n\n----------------------------------------\n\nTITLE: Previous NifiCluster Configuration with busybox InitContainerImage\nDESCRIPTION: Example YAML configuration for a NifiCluster that uses the busybox image as the init container, which needs to be updated after upgrading to v0.15.0.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/7_upgrade_guides/2_v0.14.1_to_v0.15.0.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  initContainerImage:\n    repository: busybox\n    tag: \"1.34.0\"\n```\n\n----------------------------------------\n\nTITLE: Listing Deployed Helm Charts - Bash\nDESCRIPTION: This command lists all currently deployed Helm chart releases in the Kubernetes cluster. Requires Helm and cluster access. No parameters needed; the output is a tabular list of current releases and statuses. Use for monitoring and management of deployments.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nhelm list\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Secret for Basic Authentication\nDESCRIPTION: This command creates a Kubernetes secret that contains the username, password, and optional CA certificate for basic authentication with an external NiFi cluster. The secret will be referenced in the NifiCluster resource.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/3_external_cluster.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create secret generic nifikop-credentials \\\n  --from-file=username=./secrets/username\\\n  --from-file=password=./secrets/password\\\n  --from-file=ca.crt=./secrets/ca.crt\\\n  -n nifikop-nifi\n```\n\n----------------------------------------\n\nTITLE: Adding KEDA Helm Repository (Console)\nDESCRIPTION: Command to add the official KEDA Helm chart repository to your local Helm configuration. This repository contains the necessary charts to deploy KEDA into your Kubernetes cluster. Requires Helm to be installed and configured.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nhelm repo add kedacore https://kedacore.github.io/charts\n```\n\n----------------------------------------\n\nTITLE: NifiCluster Configuration Example (bash)\nDESCRIPTION: Example NifiCluster configuration showing the `initContainerImage` updated to `bash`. This configuration is suitable for Nifikop v0.15.0. This snippet demonstrates the updated configuration, replacing `busybox` with `bash`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/7_upgrade_guides/2_v0.14.1_to_v0.15.0.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  initContainerImage:\n    repository: bash\n    tag: \"5.2.2\"\n```\n\n----------------------------------------\n\nTITLE: Upgrading Go language version in the operator\nDESCRIPTION: This code indicates the CI/CD pipeline configuration snippet for upgrading the Golang environment used in the operator development, ensuring compatibility with newer Go features and dependencies. It is essential for build environments.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/CHANGELOG.md#_snippet_7\n\nLANGUAGE: Shell\nCODE:\n```\nexport GO_VERSION=1.22.1\n# Commands to install or switch to the specified Go version\n```\n\n----------------------------------------\n\nTITLE: Using NiFi Encrypt-Config Tool for Algorithm Change\nDESCRIPTION: Commands for using the NiFi Encrypt-Config Tool to change the sensitive algorithm in the flow configuration files. This recalculates sensitive values to work with the new algorithm.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/7_upgrade_guides/4_v1.3.1_to_v1.4.0.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nencrypt-config.sh -n nifi.properties -f flow.xml.gz -x -s PROPERTIES_KEY -A NIFI_PBKDF2_AES_GCM_256\nencrypt-config.sh -n nifi.properties -f flow.json.gz -x -s PROPERTIES_KEY -A NIFI_PBKDF2_AES_GCM_256\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop with values.yaml using Helm\nDESCRIPTION: This command installs NiFiKop using Helm and a `values.yaml` file for configuration.  The `values.yaml` file specifies the values for the configurable parameters of the NiFi Operator Helm chart.  It assumes the Helm client is already installed and configured.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n$ helm install nifikop \\\n      konpyutaika/nifikop \\\n      -f values.yaml\n```\n\n----------------------------------------\n\nTITLE: Pushing NiFiKop Docker Image to Repository - Bash\nDESCRIPTION: Uploads the locally built NiFiKop Docker image to a specified container registry (e.g., Docker Hub) using the make utility. This step is required so that the operator can be deployed to Kubernetes clusters referencing this image.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/6_contributing/1_developer_guide.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n$ make docker-push\n```\n\n----------------------------------------\n\nTITLE: Creating K3D Kubernetes Cluster\nDESCRIPTION: Creates a K3D Kubernetes cluster using the specified Rancher K3s image. The --wait flag ensures the cluster is ready before proceeding. Kubernetes version 1.21.10 is used in this example. Change the version with the --kubernetes-version value.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/2_setup/2_platform_setup/2_k3d.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nk3d cluster create --image rancher/k3s:v1.21.10-k3s1 --wait\n```\n\n----------------------------------------\n\nTITLE: Deploying CRDs for NiFiKop\nDESCRIPTION: Commands to deploy the Custom Resource Definitions (CRDs) required by the NiFiKop operator in a Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/6_contributing/1_developer_guide.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\n```\n\n----------------------------------------\n\nTITLE: Re-encrypting NiFi Flow Configuration with `encrypt-config.sh` (Shell)\nDESCRIPTION: These shell commands use the NiFi Encrypt-Config Tool (`encrypt-config.sh`) to decrypt and re-encrypt sensitive properties within a NiFi flow configuration file.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/7_upgrade_guides/4_v1.3.1_to_v1.4.0.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nencrypt-config.sh -n nifi.properties -f flow.xml.gz -x -s PROPERTIES_KEY -A NIFI_PBKDF2_AES_GCM_256\n```\n\nLANGUAGE: sh\nCODE:\n```\nencrypt-config.sh -n nifi.properties -f flow.json.gz -x -s PROPERTIES_KEY -A NIFI_PBKDF2_AES_GCM_256\n```\n\n----------------------------------------\n\nTITLE: Defining Istio Gateway for HTTP Traffic in YAML\nDESCRIPTION: This Istio Gateway configuration listens for HTTP traffic on port 80 for the specified host 'nifi.my-domain.com'. It uses the default Istio ingress gateway selector.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: nifi-gateway\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - nifi.my-domain.com\n```\n\n----------------------------------------\n\nTITLE: Adding and Updating Helm Repositories for KEDA Deployment - console\nDESCRIPTION: Commands to add the KEDA Helm chart repository and update it, necessary for installing KEDA into the Kubernetes cluster using Helm. These commands set up Helm to access KEDA charts.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nhelm repo add kedacore https://kedacore.github.io/charts\n```\n\nLANGUAGE: console\nCODE:\n```\nhelm repo update\n```\n\n----------------------------------------\n\nTITLE: Installing cert-manager CRDs for Helm (Bash/kubectl)\nDESCRIPTION: Installs only the cert-manager CustomResourceDefinitions (CRDs) using `kubectl apply`. This step is required before installing cert-manager via Helm if CRD installation is skipped in the Helm chart.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/2_deploy_nifikop/1_quick_start.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Install CustomResourceDefinitions first\nkubectl apply --validate=false -f \\\n   https://github.com/jetstack/cert-manager/releases/download/v1.7.2/cert-manager.crds.yaml\n```\n\n----------------------------------------\n\nTITLE: Creating a Kubernetes Secret\nDESCRIPTION: This command creates a Kubernetes secret named `secret-params` in the `nifikop` namespace. It populates the secret with key-value pairs, which can be referenced by NifiParameterContext for sensitive parameters.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl create secret generic secret-params \\\n    --from-literal=secret1=yop \\\n    --from-literal=secret2=yep \\\n    -n nifikop\n```\n\n----------------------------------------\n\nTITLE: Creating Node.js Project with Dependencies for Kubernetes Migration\nDESCRIPTION: Sets up a Node.js project and installs required dependencies for interacting with the Kubernetes API. Uses @kubernetes/client-node for K8s access and minimist for command-line argument parsing.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm init -y\nnpm install @kubernetes/client-node@0.16.3 minimist@1.2.6\n```\n\n----------------------------------------\n\nTITLE: Data model schema for NifiRegistryClientsSpec in Go\nDESCRIPTION: This `NifiRegistryClientSpec` schema describes the desired configuration state for a NiFi registry client, including optional description, mandatory URI, and cluster reference. It informs the operator or controller how to initialize or update the resource.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/5_references/3_nifi_registry_client.md#_snippet_2\n\n\n\n----------------------------------------\n\nTITLE: Installing NiFiKop Helm Chart (Release Name)\nDESCRIPTION: This command installs the NiFiKop Helm chart, replacing `<release name>` with the desired name for the release.  It installs the chart from the specified repository.  The release name identifies the specific deployment of the chart.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/2_setup/3_install/1_customizable_install_with_helm.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhelm install <release name> konpyutaika/nifikop\n```\n\n----------------------------------------\n\nTITLE: Creating Secret for Basic Authentication with kubectl\nDESCRIPTION: A kubectl command to create a Kubernetes secret containing the credentials needed for basic authentication with an external NiFi cluster, including username, password, and optional CA certificate.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/3_tasks/1_nifi_cluster/4_external_cluster.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nkubectl create secret generic nifikop-credentials \\\n  --from-file=username=./secrets/username\\\n  --from-file=password=./secrets/password\\\n  --from-file=ca.crt=./secrets/ca.crt\\\n  -n nifikop-nifi\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables for Development IDE\nDESCRIPTION: Sets environment variables such as KUBECONFIG, WATCH_NAMESPACE, POD_NAME, LOG_LEVEL, and OPERATOR_NAME to enable IDE-based development and direct the operator to connect to a Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/6_contributing/1_developer_guide.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nKUBECONFIG={path/to/your/kubeconfig}\nWATCH_NAMESPACE={namespace_to_watch}\nPOD_NAME={name for operator pod}\nLOG_LEVEL=Debug\nOPERATOR_NAME=ide\n```\n\n----------------------------------------\n\nTITLE: Deploying NiFiKop CRDs in Bash\nDESCRIPTION: Commands to deploy the Custom Resource Definitions (CRDs) required by the NiFiKop operator to the Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/6_contributing/1_developer_guide.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\n```\n\n----------------------------------------\n\nTITLE: Getting Helm Deployment Status\nDESCRIPTION: This command retrieves the status of a specific Helm deployment.  It provides information about the release, including the deployed resources, conditions, and any errors. Replace `nifikop` with the release name you want to check.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nhelm status nifikop\n```\n\n----------------------------------------\n\nTITLE: Update OpenShift Config with UID - Bash\nDESCRIPTION: This command updates the `config/samples/openshift.yaml` file by replacing the placeholder UID `1000690000` with the actual UID obtained from the OpenShift namespace using `sed`. The `-i` option modifies the file in place.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nsed -i \"s/1000690000/$uid/g\" config/samples/openshift.yaml\n```\n\n----------------------------------------\n\nTITLE: Applying NiFi Cluster Scale-Up Configuration using kubectl\nDESCRIPTION: Uses the `kubectl apply` command to apply the updated `NifiCluster` configuration file (presumably `config/samples/simplenificluster.yaml` containing the added node) to the Kubernetes cluster within the 'nifi' namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nkubectl -n nifi apply -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: List Deleted Helm Releases\nDESCRIPTION: This command lists Helm releases that have been deleted. Helm keeps records of deleted releases allowing for potential rollbacks.  This command shows the names, versions, and statuses of deleted releases. Dependencies: Helm.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --deleted\n```\n\n----------------------------------------\n\nTITLE: Configuring Identity Mapping in NiFi Properties\nDESCRIPTION: Essential identity mapping configuration recommended for NiFi when using OpenId Connect authentication. These settings ensure proper handling of distinguished names and support for multiple identity providers.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/5_users_authentication/1_oidc.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nnifi.security.identity.mapping.pattern.dn=CN=([^,]*)(?:, (?:O|OU)=.*)?\\nnifi.security.identity.mapping.value.dn=$1\\nnifi.security.identity.mapping.transform.dn=NONE\n```\n\n----------------------------------------\n\nTITLE: Building NiFiKop Operator Locally Using Make - Bash\nDESCRIPTION: This command builds the NiFiKop operator locally by invoking the 'make build' target. It requires a working Go environment and make utility. The build process compiles the operator into a binary that can be run or deployed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/6_contributing/1_developer_guide.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmake build\n```\n\n----------------------------------------\n\nTITLE: Setting Operator Name Environment Variable for Local Execution - Bash\nDESCRIPTION: This command exports an environment variable 'OPERATOR_NAME' with a value to identify the operator during local execution. It prepares the runtime environment for running the operator outside the cluster, facilitating development and debugging.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/6_contributing/1_developer_guide.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport OPERATOR_NAME=nifi-operator\n```\n\n----------------------------------------\n\nTITLE: Adding Prometheus Helm Repository - Console\nDESCRIPTION: Adds the 'prometheus-community' charts repository to the current Helm installation. This is a prerequisite for installing Prometheus Operator via Helm and does not require arguments other than the repository URL.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_6\n\nLANGUAGE: console\nCODE:\n```\nhelm repo add prometheus https://prometheus-community.github.io/helm-charts\n```\n\n----------------------------------------\n\nTITLE: Purging a Helm Release from Records - Bash\nDESCRIPTION: Fully removes the nifikop Helm release from the cluster, including its revision history. The --purge flag removes all Helm release records, allowing the release name to be re-used. Use with caution as this operation is irreversible.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nhelm delete --purge nifikop\n```\n\n----------------------------------------\n\nTITLE: Packaging Helm Chart with Make\nDESCRIPTION: This bash command executes the `make helm-package` command to package the Helm chart for the NiFiKop operator. It generates a `.tgz` file containing the chart and its dependencies.  This requires the project structure and the Helm chart files to be in place.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/6_contributing/1_developer_guide.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nmake helm-package\n```\n\n----------------------------------------\n\nTITLE: Attaching Context with Formatted Error\nDESCRIPTION: This snippet illustrates the case when a formatted error message needs to be returned to user directly. Even then it attaches the context to the error using `errors.WithDetails`. In this case `errors.WrapIff` can be used to return formatted error message.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/docs/error-handling-guide.md#_snippet_5\n\nLANGUAGE: go\nCODE:\n```\nreturn errors.WithDetails(errors.WrapIff(err, \"something %s failed\", important), \"what\", important)\n```\n\n----------------------------------------\n\nTITLE: Building NiFiKop Docker Image using Bash and Make\nDESCRIPTION: Sets the `DOCKER_REPO_BASE` environment variable to specify the target Docker repository and then runs the `make docker-build` command to build the NiFiKop operator Docker image. The image tag will be based on the version in `version/version.go` and the current Git branch name. Requires Docker v18.09+.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/6_contributing/1_developer_guide.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nexport DOCKER_REPO_BASE={your-docker-repo}\nmake docker-build\n```\n\n----------------------------------------\n\nTITLE: NiFiCluster SSL Configuration\nDESCRIPTION: This YAML configuration defines a NiFiCluster with SSL enabled. It specifies the internal listeners, SSL secrets, and readOnlyConfig for web proxy hosts. The `create: true` setting instructs the operator to generate the required certificates.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/1_deploy_cluster/4_ssl_configuration.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  readOnlyConfig:\n    # NifiProperties configuration that will be applied to the node.\n    nifiProperties:\n      webProxyHosts:\n        - nifistandard2.trycatchlearn.fr:8443\n        ...\n  ...\n  listenersConfig:\n    internalListeners:\n      - type: \"https\"\n        name: \"https\"\n        containerPort: 8443\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n```\n\n----------------------------------------\n\nTITLE: NifiClusterTaskSpec for Retry Duration\nDESCRIPTION: Defines a specification for cluster tasks, specifically the retry duration in minutes, guiding how long the operator should wait before retrying a failed task. It establishes a default retry wait time but is not tied to any executable code snippet.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/5_references/1_nifi_cluster/1_nifi_cluster.md#_snippet_3\n\n\n\n----------------------------------------\n\nTITLE: Setting Operator Name Environment Variable (bash)\nDESCRIPTION: This command sets a shell environment variable `OPERATOR_NAME` to 'nifi-operator'. This variable may be used by the operator's entry point or build process to configure its identity or behavior when running locally.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/6_contributing/1_developer_guide.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport OPERATOR_NAME=nifi-operator\n```\n\n----------------------------------------\n\nTITLE: Helm Install Command for NiFiKop\nDESCRIPTION: This shell command illustrates how to install the NiFiKop Helm chart with custom values specified through a YAML file or --set parameters, deploying the NiFi Kubernetes operator with default or user-defined configurations.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\n$ helm install nifikop \\\n      konpyutaika/nifikop \\\n      -f values.yaml\n```\n\n----------------------------------------\n\nTITLE: Referencing NiFi Registry Client (JSON)\nDESCRIPTION: Illustrates how to reference a NiFi Registry Client custom resource used for fetching versioned dataflows. It requires specifying the `name` and `namespace` of the target `NiFiRegistryClient` object.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/helm/nifi-cluster/README.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\"name\":\"default\",\"namespace\":\"nifi\"}\n```\n\n----------------------------------------\n\nTITLE: Listing NiFiKop Managed User Groups using kubectl (Bash)\nDESCRIPTION: This command uses kubectl to retrieve and list all NifiUserGroup custom resources within the specified namespace ('nifikop' in this example). This allows verification of the managed groups (managed-admins, managed-nodes, managed-readers) created and managed by the NiFiKop operator.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/2_manage_users_and_accesses/3_managed_groups.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nkubectl get -n nifikop nifiusergroups.nifi.konpyutaika.com\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop Without CRDs\nDESCRIPTION: Command to install the NiFiKop chart without deploying the CRDs, useful when you don't want to modify existing CRDs or when you've manually installed them beforehand.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nhelm install --name nifikop ./helm/nifikop --set namespaces={\"nifikop\"} --skip-crds\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Internal Listeners for Processor-Specific Ports\nDESCRIPTION: Configuration example showing how to add additional internal listeners without a specific type for custom NiFi processor ports. This is useful for exposing processor-specific endpoints like HTTP listeners.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    ...\n    - name: \"http-tracking\"\n      containerPort: 8081\n```\n\n----------------------------------------\n\nTITLE: Adding Bitnami Helm Repository for Kubernetes in Bash\nDESCRIPTION: Adds the Bitnami Helm chart repository to the local Helm client, enabling access to Bitnami charts such as the Zookeeper chart. This command requires Helm to be installed and configured to access the Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhelm repo add bitnami https://charts.bitnami.com/bitnami\n```\n\n----------------------------------------\n\nTITLE: Deploying CRDs for NiFiKop\nDESCRIPTION: Commands to apply the Custom Resource Definitions (CRDs) required by NiFiKop to the Kubernetes cluster. These define the resources that NiFiKop will manage.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/6_contributing/1_developer_guide.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\n```\n\n----------------------------------------\n\nTITLE: Applying NiFiKop CRDs to Kubernetes using Bash\nDESCRIPTION: Uses `kubectl apply` to create or update the necessary NiFiKop Custom Resource Definitions (CRDs) in the target Kubernetes cluster. These CRDs define the custom resources managed by the operator (NifiCluster, NifiDataflow, etc.). Requires `kubectl` v1.16+ configured to access a cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/6_contributing/1_developer_guide.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing Helm Chart with Debug Logging and Namespace Configuration Using Bash\nDESCRIPTION: This Bash Helm command performs a dry-run installation of the nifikop Helm chart with detailed debug logging enabled and sets the target deployment namespace. It verifies the installation without actually deploying, allowing users to validate configurations and rendered templates. The namespaces parameter controls the Kubernetes namespaces where the chart's resources are deployed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop \\\n    --dry-run \\\n    --set logLevel=Debug \\\n    --set namespaces={\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for NiFiKop Operator - Bash\nDESCRIPTION: Prepares a Docker image for the NiFiKop operator based on the current local branch by setting the DOCKER_REPO_BASE environment variable and running 'make docker-build'. The build requires a Docker daemon and proper repository base naming.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/6_contributing/1_developer_guide.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nexport DOCKER_REPO_BASE={your-docker-repo}\nmake docker-build\n```\n\n----------------------------------------\n\nTITLE: Extracting SSL Credentials from Kubernetes Secret\nDESCRIPTION: Console commands to extract the CA certificate, client certificate, and client private key from a Kubernetes secret. These commands decode the base64-encoded secret values and save them as files.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/3_tasks/2_security/1_ssl.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['ca\\.crt']}\" | base64 -d > ca.crt\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['tls\\.crt']}\" | base64 -d > tls.crt\nkubectl get secret example-client-secret -o jsonpath=\"{['data']['tls\\.key']}\" | base64 -d > tls.key\n```\n\n----------------------------------------\n\nTITLE: Access Prometheus Dashboard via Port Forwarding\nDESCRIPTION: Executes kubectl command to port-forward the Prometheus service locally on port 9090 for dashboard access and metrics querying. Assumes kubectl context is correctly configured.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nkubectl port-forward -n monitoring-system service/prometheus-operated 9090:9090\n```\n\n----------------------------------------\n\nTITLE: Applying the NiFiCluster configuration using kubectl - SH\nDESCRIPTION: This shell command applies the updated `NifiCluster` configuration to the Kubernetes cluster using `kubectl`. It specifies the namespace (`nifi`) and the path to the YAML configuration file (`config/samples/simplenificluster.yaml`).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/3_tasks/1_nifi_cluster/2_cluster_scaling.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nkubectl -n nifi apply -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Setting Operator Name for Local Execution - Bash\nDESCRIPTION: This snippet exports the OPERATOR_NAME environment variable prior to running the NiFiKop operator binary locally. It is a single-line command and is a prerequisite for local development or testing with the Go binary.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/6_contributing/1_developer_guide.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport OPERATOR_NAME=nifi-operator\n```\n\n----------------------------------------\n\nTITLE: Uninstalling NiFiKop Helm Chart\nDESCRIPTION: Command to uninstall the NiFiKop operator from the Kubernetes cluster using Helm. This removes all Kubernetes components associated with the chart except for the CRDs, which must be removed separately if needed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nhelm del nifikop\n```\n\n----------------------------------------\n\nTITLE: Configuring Istio Gateway for HTTP in NiFi\nDESCRIPTION: This YAML configuration defines an Istio Gateway that intercepts HTTP requests on port 80 for a specific domain host to access the NiFi cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: nifi-gateway\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - nifi.my-domain.com\n```\n\n----------------------------------------\n\nTITLE: Creating a NiFi Registry Client in Kubernetes\nDESCRIPTION: YAML definition for creating a NifiRegistryClient resource in Kubernetes. It specifies a reference to a NiFi cluster, a description, and the URI of the NiFi Registry service to connect to.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/5_references/3_nifi_registry_client.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiRegistryClient\nmetadata:\n  name: squidflow\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  description: \"Squidflow demo\"\n  uri: \"http://nifi-registry:18080\"\n```\n\n----------------------------------------\n\nTITLE: Getting Helm Deployment Status for Nifikop\nDESCRIPTION: Retrieves the current status and details of a specific Helm release, named 'nifikop' in this example, using the `helm status` command. This shows deployed resources, notes, and overall state.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nhelm status nifikop\n```\n\n----------------------------------------\n\nTITLE: Packaging the NiFiKop Helm Chart - Bash\nDESCRIPTION: Creates a Helm chart package from the NiFiKop source using a Makefile target. This is required before distributing or uploading the chart to a Helm repository. Outputs a '.tgz' packaged chart file in the appropriate directory.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/6_contributing/1_developer_guide.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nmake helm-package\n```\n\n----------------------------------------\n\nTITLE: Install NiFiKop Chart with Parameters - Helm\nDESCRIPTION: This command installs the NiFiKop Helm chart, setting specific parameters during the installation.  In this example, it sets the namespaces. It requires Helm to be installed and configured to connect to a Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop --set namespaces={\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Checking Helm Release Status for Nifikop\nDESCRIPTION: Provides the `helm status` command followed by the release name (`nifikop`) to retrieve detailed status information about that specific Helm deployment, including deployed resources and notes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nhelm status nifikop\n```\n\n----------------------------------------\n\nTITLE: Packaging Helm Chart with Make in Bash\nDESCRIPTION: This command packages the NiFiKop Helm chart using a Makefile target. Requires the Helm command-line tool and an available Makefile in the working directory. The output is a packaged chart for deployment or publication.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/6_contributing/1_developer_guide.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nmake helm-package\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTP VirtualService for NiFi in Istio\nDESCRIPTION: Defines an Istio VirtualService that redirects all requests intercepted by the Gateway to the NiFi service on port 8080.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: nifi-vs\nspec:\n  gateways:\n  - nifi-gateway\n  hosts:\n  - nifi.my-domain.com\n  http:\n  - match:\n    - uri:\n        prefix: /\n    route:\n    - destination:\n        host: nifi\n        port:\n          number: 8080\n```\n\n----------------------------------------\n\nTITLE: List All Helm Releases - Bash\nDESCRIPTION: Lists all Helm releases present in the cluster regardless of their status (deployed, deleted, failed) using the `helm list --all` command. Provides a complete history of chart deployments in the current context.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/2_setup/3_install/1_customizable_install_with_helm.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --all\n```\n\n----------------------------------------\n\nTITLE: Updating Helm Repositories - console\nDESCRIPTION: Refreshes chart information from all configured Helm repositories, ensuring latest chart versions are available. Requires helm CLI. No arguments are needed; updates cached chart repo metadata.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_7\n\nLANGUAGE: console\nCODE:\n```\nhelm repo update\n```\n\n----------------------------------------\n\nTITLE: Installing KEDA with Helm\nDESCRIPTION: Creates the dedicated 'keda' namespace and installs the KEDA core components into it using the Helm chart previously added. This deploys the necessary KEDA controllers and resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl create namespace keda\nhelm install keda kedacore/keda --namespace keda\n```\n\n----------------------------------------\n\nTITLE: Add and Update Jetstack Helm Repo (bash)\nDESCRIPTION: Adds the official Jetstack Helm chart repository and updates the local Helm repository cache. This allows Helm to discover and install charts from Jetstack, specifically the cert-manager chart. Requires Helm 3+ installed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/2_deploy_nifikop/1_quick_start.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nhelm repo add jetstack https://charts.jetstack.io\n```\n\nLANGUAGE: bash\nCODE:\n```\nhelm repo update\n```\n\n----------------------------------------\n\nTITLE: Creating Secret for Basic Authentication in Kubernetes\nDESCRIPTION: Command to create a Kubernetes secret with the required credentials for basic authentication with an external NiFi cluster. Includes username, password, and optional CA certificate.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/3_manage_nifi/1_manage_clusters/3_external_cluster.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nkubectl create secret generic nifikop-credentials \\\n  --from-file=username=./secrets/username\\\n  --from-file=password=./secrets/password\\\n  --from-file=ca.crt=./secrets/ca.crt\\\n  -n nifikop-nifi\n```\n\n----------------------------------------\n\nTITLE: Importing Theme Components for Tabbed Content in Documentation\nDESCRIPTION: Imports theme-specific components 'Tabs' and 'TabItem' for structuring tabbed sections within the documentation, facilitating organized presentation of multiple configuration options or sections.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n```\n\n----------------------------------------\n\nTITLE: Setting Operator Name Environment Variable\nDESCRIPTION: This bash command sets an environment variable named `OPERATOR_NAME` to `nifi-operator`. This is used within the build process to configure various aspects of the operator.  It requires a bash-compatible environment and the `export` command.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/6_contributing/1_developer_guide.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport OPERATOR_NAME=nifi-operator\n```\n\n----------------------------------------\n\nTITLE: Updating Helm Repository Cache\nDESCRIPTION: This command updates Helm repositories to fetch the latest charts, including KEDA. It ensures the local Helm cache is current before installing or upgrading Helm charts.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nhelm repo update\n```\n\n----------------------------------------\n\nTITLE: Running NiFiKop Operator Locally Using Make - Bash\nDESCRIPTION: Initiates the NiFiKop operator process running locally against a Kubernetes cluster using the 'make run' command. This relies on a default kubeconfig file and runs the operator in the default namespace to enable development and testing.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/6_contributing/1_developer_guide.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmake run\n```\n\n----------------------------------------\n\nTITLE: Installing Zookeeper on OpenShift with Security Context\nDESCRIPTION: Helm command for installing Zookeeper on OpenShift with specific security context settings to match OpenShift's security requirements.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhelm install zookeeper oci://registry-1.docker.io/bitnamicharts/zookeeper \\\n    --set resources.requests.memory=256Mi \\\n    --set resources.requests.cpu=250m \\\n    --set resources.limits.memory=256Mi \\\n    --set resources.limits.cpu=250m \\\n    --set global.storageClass=standard \\\n    --set networkPolicy.enabled=true \\\n    --set replicaCount=3 \\\n    --set containerSecurityContext.runAsUser=$zookeper_uid \\\n    --set podSecurityContext.fsGroup=$zookeper_uid\n```\n\n----------------------------------------\n\nTITLE: Updating Helm Repositories - Console\nDESCRIPTION: Updates the local cache of Helm charts from all configured repositories. This step ensures you have access to the latest chart versions, including those from the Prometheus Community repository. Requires Helm to be installed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_7\n\nLANGUAGE: console\nCODE:\n```\nhelm repo update\n```\n\n----------------------------------------\n\nTITLE: Referencing Global Logging Outputs (JSON)\nDESCRIPTION: Configures logging to reference pre-existing global outputs defined elsewhere within the Logging Operator setup. The example shows referencing a `loki-cluster-output` to direct logs to a Loki instance.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/helm/nifi-cluster/README.md#_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\"globalOutputRefs\":[\"loki-cluster-output\"]}\n```\n\n----------------------------------------\n\nTITLE: Parameter Data Model\nDESCRIPTION: Defines a Parameter object used within a NiFi Parameter Context, including name, optional value, description, and sensitivity flag. Used to represent individual configuration parameters.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/5_references/4_nifi_parameter_context.md#_snippet_4\n\nLANGUAGE: YAML\nCODE:\n```\nParameter:\n  name: string\n  value: string\n  description: string\n  sensitive: string\n```\n\n----------------------------------------\n\nTITLE: Metadata Schema for External Services\nDESCRIPTION: This table describes the 'Metadata' object, including optional annotations and labels, which allow users to add additional metadata to Kubernetes services for organization, identification, and management purposes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/5_references/1_nifi_cluster/7_external_service_config.md#_snippet_4\n\n\n\n----------------------------------------\n\nTITLE: Applying the NiFiCluster configuration to Kubernetes\nDESCRIPTION: This bash command applies the updated NiFi cluster configuration YAML to Kubernetes, resulting in the creation or update of resources to incorporate the new node into the cluster deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/1_scaling_mechanism.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl -n nifi apply -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Building and Installing Kubectl NiFiKop Plugin\nDESCRIPTION: This snippet builds the kubectl-nifikop executable and copies it to /usr/local/bin, making it accessible system-wide. It requires make to be installed and sudo privileges for copying the executable.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/2_deploy_nifikop/3_kubectl_plugin.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nmake kubectl-nifikop && sudo cp ./bin/kubectl-nifikop /usr/local/bin\n```\n\n----------------------------------------\n\nTITLE: Installing Helm Chart with Custom Parameters\nDESCRIPTION: Helm command for deploying the Nifikop chart with a specified release name and custom namespace parameter. Useful for integrating into existing Kubernetes environments with specific configurations.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop --set namespaces={\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Getting NiFi Namespace UID for OpenShift Deployment\nDESCRIPTION: Bash command to extract the allowed user ID from OpenShift namespace annotations for the NiFi namespace, needed for configuring the NiFi cluster deployment on OpenShift.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nuid=$(kubectl get namespace nifi -o=jsonpath='{.metadata.annotations.openshift\\.io/sa\\.scc\\.supplemental-groups}' | sed 's/\\/10000$//' | tr -d '[:space:]')\n```\n\n----------------------------------------\n\nTITLE: Adding Custom HTTP Endpoint Listener\nDESCRIPTION: Example showing how to add an additional internal listener without a specific type for custom HTTP endpoints in NiFi processors. This allows exposing processor-specific ports.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nlistenersConfig:\n  internalListeners:\n    ...\n    - name: \"http-tracking\"\n      containerPort: 8081\n```\n\n----------------------------------------\n\nTITLE: Setting NiFi Authorizer Property\nDESCRIPTION: This shell command sets the `nifi.security.user.authorizer` property in NiFi's configuration to use the `custom-database-authorizer`. This property tells NiFi which authorizer, defined in `authorizers.xml`, to use for managing user authorization.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/3_tasks/2_security/2_authorization/1_custom_authorizer.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nnifi.security.user.authorizer=custom-database-authorizer\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTP Gateway for NiFi in Istio\nDESCRIPTION: Defines an Istio Gateway that intercepts all HTTP requests on port 80 for a specific domain host (nifi.my-domain.com).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/2_istio_service_mesh.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: nifi-gateway\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - nifi.my-domain.com\n```\n\n----------------------------------------\n\nTITLE: Listing Deleted Helm Releases\nDESCRIPTION: This snippet uses the Helm CLI to list all deleted Helm releases. It requires Helm to be installed and configured. This is useful for recovering or inspecting releases that have been deleted.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --deleted\n```\n\n----------------------------------------\n\nTITLE: Troubleshooting CRD Installations\nDESCRIPTION: Example Helm command to install the chart without CRDs, avoiding modification of existing CRDs in the cluster. Useful in environments where CRDs are managed separately.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nhelm install --name nifikop ./helm/nifikop --set namespaces={\"nifikop\"} --skip-crds\n```\n\n----------------------------------------\n\nTITLE: Testing Kubectl NiFiKop Plugin Installation\nDESCRIPTION: This snippet demonstrates how to verify the successful installation of the kubectl-nifikop plugin by running it without any arguments. The expected output is the usage instructions for the plugin.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/2_deploy_nifikop/3_kubectl_plugin.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\n$ kubectl nifikop\nUsage:\n  nifikop [command]\n\nAvailable Commands:\n  completion          Generate the autocompletion script for the specified shell\n  help                Help about any command\n  nificluster         \n  nificonnection      \n  nifidataflow        \n  nifigroupautoscaler \n  nifiregistryclient  \n  nifiuser            \n  nifiusergroup \n```\n\n----------------------------------------\n\nTITLE: Defining NifiDataflow Resources for Input and Output Dataflows\nDESCRIPTION: These YAML snippets define two NifiDataflow custom resources representing 'input' and 'output' dataflows, including their specifications such as cluster reference, bucket ID, flow ID, version, registry, and position. These are prerequisites for establishing a connection between dataflows. The examples illustrate the necessary configuration fields and their purpose, such as skipInvalidComponent, syncMode, and flowPosition.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/4_manage_connections/1_deploy_connection.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiDataflow\nmetadata:\n  name: input\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  bucketId: deedb9f6-65a4-44e9-a1c9-722008fcd0ba\n  flowId: ab95431d-980d-41bd-904a-fac4bd864ba6\n  flowVersion: 1\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  skipInvalidComponent: true\n  skipInvalidControllerService: true\n  syncMode: always\n  updateStrategy: drain\n  flowPosition:\n    posX: 0\n    posY: 0\n```\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiDataflow\nmetadata:\n  name: output\n  namespace: nifikop\nspec:\n  clusterRef:\n    name: nc\n    namespace: nifikop\n  bucketId: deedb9f6-65a4-44e9-a1c9-722008fcd0ba\n  flowId: fc5363eb-801e-432f-aa94-488838674d07\n  flowVersion: 2\n  registryClientRef:\n    name: registry-client-example\n    namespace: nifikop\n  skipInvalidComponent: true\n  skipInvalidControllerService: true\n  syncMode: always\n  updateStrategy: drain\n  flowPosition:\n    posX: 750\n    posY: 0\n```\n\n----------------------------------------\n\nTITLE: Setting NiFi Authorizer Property\nDESCRIPTION: This shell command demonstrates how to set the `nifi.security.user.authorizer` property to use the custom database authorizer. This property tells NiFi which authorizer to use from the `authorizers.xml` configuration file.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/6_users_authorization/1_custom_user_authorizer.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nnifi.security.user.authorizer=custom-database-authorizer\n```\n\n----------------------------------------\n\nTITLE: Cloning NiFiKop Repository With Git - Bash\nDESCRIPTION: Clones the NiFiKop git repository from GitHub and navigates into the project directory. This is a prerequisite step for working with the NiFiKop operator codebase locally.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/6_contributing/1_developer_guide.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/konpyutaika/nifikop.git\ncd nifikop\n```\n\n----------------------------------------\n\nTITLE: Adding Prometheus Helm Repository\nDESCRIPTION: Adds the Prometheus community Helm chart repository to the local Helm configuration using `helm repo add`. This makes the `kube-prometheus-stack` chart available for installation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_6\n\nLANGUAGE: console\nCODE:\n```\nhelm repo add prometheus https://prometheus-community.github.io/helm-charts\n```\n\n----------------------------------------\n\nTITLE: Installing Nifikop Helm Chart Without CRDs in Bash\nDESCRIPTION: This Bash command installs the nifikop Helm chart while instructing Helm to skip installing CRDs using the '--skip-crds' flag. The '--set namespaces={\"nifikop\"}' argument specifies the target namespace(s) for the deployment. Prerequisites include Helm v3 or compatible version, access to './helm/nifikop' directory, and necessary cluster permissions. This approach is used when you want to avoid overwriting existing CRDs that may already be deployed in the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nhelm install --name nifikop ./helm/nifikop --set namespaces={\"nifikop\"} --skip-crds\n```\n\n----------------------------------------\n\nTITLE: Building NiFiKop Operator Locally with Make - Bash\nDESCRIPTION: This command runs the 'make build' target to compile the NiFiKop operator locally using the configured Go environment. It requires 'make' and Go installed and configured. This builds the operator binary for development or deployment purposes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/6_contributing/1_developer_guide.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmake build\n```\n\n----------------------------------------\n\nTITLE: Verifying HPA Deployment for NiFi\nDESCRIPTION: Command to check the status of the Horizontal Pod Autoscaler created by KEDA for the NiFi cluster, showing current scaling metrics and replica counts.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_14\n\nLANGUAGE: console\nCODE:\n```\nkubectl get -n clusters hpa\n```\n\n----------------------------------------\n\nTITLE: Listing Deleted Helm Releases\nDESCRIPTION: Shows the `helm list --deleted` command, which displays Helm releases that have been uninstalled but whose records are still kept by Helm.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --deleted\n```\n\n----------------------------------------\n\nTITLE: Listing All Helm Releases - Helm (bash)\nDESCRIPTION: This command lists all Helm releases, including deployed, deleted, and failed releases. It requires Helm CLI installed and configured. The output is a comprehensive list of all Helm releases.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --all\n```\n\n----------------------------------------\n\nTITLE: Creating a Secret for Sensitive Parameters with kubectl\nDESCRIPTION: Command to create a Kubernetes secret that contains sensitive parameters to be used by NifiParameterContext. This secret will be converted into sensitive parameters in NiFi.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/3_manage_dataflows/1_deploy_dataflow.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl create secret generic secret-params \\\n    --from-literal=secret1=yop \\\n    --from-literal=secret2=yep \\\n    -n nifikop\n```\n\n----------------------------------------\n\nTITLE: Adding Prometheus Helm Repository (Console)\nDESCRIPTION: Adds the Prometheus Community Helm chart repository named 'prometheus' to the local Helm configuration. This allows installation of the Prometheus Operator and related charts.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_6\n\nLANGUAGE: console\nCODE:\n```\nhelm repo add prometheus https://prometheus-community.github.io/helm-charts\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop with Custom Release Name\nDESCRIPTION: Command to install the NiFiKop chart with a custom release name. This allows for multiple installations of the operator in the same cluster with different configurations.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nhelm install <release name> konpyutaika/nifikop\n```\n\n----------------------------------------\n\nTITLE: Configuring Identity Mapping in nifi.properties for OIDC\nDESCRIPTION: Essential configuration for nifi.properties to support multiple identity providers with proper distinguished name (DN) mapping pattern.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/5_users_authentication/1_oidc.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nnifi.security.identity.mapping.pattern.dn=CN=([^,]*)(?:, (?:O|OU)=.*)?\\nnifi.security.identity.mapping.value.dn=$1\\nnifi.security.identity.mapping.transform.dn=NONE\n```\n\n----------------------------------------\n\nTITLE: Checking HPA Status for NiFi Autoscaling\nDESCRIPTION: Command to verify the status of the Horizontal Pod Autoscaler created by KEDA for the NiFi cluster. Shows current metrics, target values, and scaling details.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_12\n\nLANGUAGE: console\nCODE:\n```\nkubectl get -n clusters hpa\n```\n\n----------------------------------------\n\nTITLE: Pushing NiFiKop Docker Image using Bash and Make\nDESCRIPTION: Executes the `make docker-push` command to push the previously built NiFiKop operator Docker image to the configured Docker repository (specified by `DOCKER_REPO_BASE`). Requires prior authentication to the Docker repository.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/6_contributing/1_developer_guide.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nmake docker-push\n```\n\n----------------------------------------\n\nTITLE: Updating Helm Repositories - Console\nDESCRIPTION: Updates the local cache of Helm charts from all configured repositories. This ensures that you have access to the latest versions of charts, including the KEDA chart. Requires Helm to be installed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nhelm repo update\n```\n\n----------------------------------------\n\nTITLE: Adding KEDA Helm Repository (Console)\nDESCRIPTION: Adds the official KEDA Helm chart repository named 'kedacore' to the local Helm configuration. This step is necessary to locate and install the KEDA chart.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nhelm repo add kedacore https://kedacore.github.io/charts\n```\n\n----------------------------------------\n\nTITLE: Installing Nifikop Chart with a Specific Release Name\nDESCRIPTION: Basic Helm install command for deploying the Nifikop chart with a custom release name. Users should replace `<release name>` with their desired identifier.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nhelm install <release name> konpyutaika/nifikop\n```\n\n----------------------------------------\n\nTITLE: NifiNodeGroupAutoscalerSpec Schema\nDESCRIPTION: This schema defines the desired state for the NifiNodeGroupAutoscaler, including references to the NiFi cluster, node configuration identifiers, label selectors for node management, operational strategies, and scalability parameters. All fields are required except for 'replicas', which defaults to zero and is managed automatically by Kubernetes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/5_references/7_nifi_nodegroup_autoscaler.md#_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n{\n  \"clusterRef\": \"ClusterReference\",\n  \"nodeConfigGroupId\": \"string\",\n  \"nodeLabelsSelector\": \"LabelSelector\",\n  \"readOnlyConfig\": \"ReadOnlyConfig\",\n  \"nodeConfig\": \"NodeConfig\",\n  \"upscaleStrategy\": \"string\",\n  \"downscaleStrategy\": \"string\",\n  \"replicas\": \"int (optional, default 0)\"\n}\n```\n\n----------------------------------------\n\nTITLE: Purging a Helm Release Completely\nDESCRIPTION: Command to completely purge a Helm release including its release history. This differs from a regular delete as it removes all record of the release, preventing rollback.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nhelm delete --purge nifikop\n```\n\n----------------------------------------\n\nTITLE: PortConfig Configuration Schema\nDESCRIPTION: This section defines the schema for 'PortConfig', specifying the exposed port number and the internal listener name used as a target container in Kubernetes service configurations.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/5_references/1_nifi_cluster/7_external_service_config.md#_snippet_3\n\n\n\n----------------------------------------\n\nTITLE: Updating Helm Repositories (Console)\nDESCRIPTION: Updates the local cache of all configured Helm repositories, ensuring that the latest chart information, including the newly added KEDA repository, is available.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nhelm repo update\n```\n\n----------------------------------------\n\nTITLE: List All Helm Releases\nDESCRIPTION: This command lists all Helm releases, including currently deployed, deleted, and failed releases. It provides a comprehensive view of all Helm releases managed. Dependencies: Helm.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --all\n```\n\n----------------------------------------\n\nTITLE: Building NiFiKop Operator Locally - Bash\nDESCRIPTION: Builds the NiFiKop operator binary using the make utility in a local Go environment. Assumes prerequisites such as Go and make are installed and configured. This compiles the operator from source.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/6_contributing/1_developer_guide.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmake build\n```\n\n----------------------------------------\n\nTITLE: Setting Development Environment Variables for IDE\nDESCRIPTION: Environment variables required to configure an IDE for NiFiKop development. These variables allow the IDE to connect to a Kubernetes cluster and set operation parameters.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/6_contributing/1_developer_guide.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nKUBECONFIG={path/to/your/kubeconfig}\nWATCH_NAMESPACE={namespace_to_watch}\nPOD_NAME={name for operator pod}\nLOG_LEVEL=Debug\nOPERATOR_NAME=ide\n```\n\n----------------------------------------\n\nTITLE: Listing Deployed Helm Charts\nDESCRIPTION: Command to list all the Helm charts currently deployed in the Kubernetes cluster. This helps to verify that the NiFiKop chart has been installed correctly.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nhelm list\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for NiFiKop Migration Script in Node.js\nDESCRIPTION: Commands to create a Node.js project and install the required dependencies (@kubernetes/client-node and minimist) for the migration script.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/7_upgrade/1_v0.7.x_to_v0.8.0.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm init -y\nnpm install @kubernetes/client-node@0.16.3 minimist@1.2.6\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop with Custom Parameters\nDESCRIPTION: Command to install the NiFiKop chart with custom parameters, specifically setting the namespace that the operator will monitor for custom resources.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop --set namespaces={\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom StorageClass in Kubernetes\nDESCRIPTION: YAML definition for a custom StorageClass with volumeBindingMode set to WaitForFirstConsumer, which is recommended for NiFi deployments to ensure proper volume allocation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: exampleStorageclass\nparameters:\n  type: pd-standard\nprovisioner: kubernetes.io/gce-pd\nreclaimPolicy: Delete\nvolumeBindingMode: WaitForFirstConsumer\n```\n\n----------------------------------------\n\nTITLE: Listing Deployed Helm Charts\nDESCRIPTION: This command lists all deployed Helm charts in the current Kubernetes namespace.  It's used to verify the successful deployment of the NiFiKop chart and to view other deployed charts in the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nhelm list\n```\n\n----------------------------------------\n\nTITLE: Listing All Helm Releases Including Deleted\nDESCRIPTION: Command to list all Helm releases that have been deleted but are still retained in Helm's record, useful for audit and management purposes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --deleted\n```\n\n----------------------------------------\n\nTITLE: Deploy CRDs using kubectl\nDESCRIPTION: This snippet provides the commands to apply the Custom Resource Definitions (CRDs) manually using `kubectl`.  It retrieves the CRD YAML files from a GitHub repository and applies them to the Kubernetes cluster. This is necessary when the user chooses to skip deploying the CRDs using Helm.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/2_deploy_nifikop/1_quick_start.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nifinodegroupautoscalers.yaml\nkubectl apply -f https://raw.githubusercontent.com/konpyutaika/nifikop/master/config/crd/bases/nifi.konpyutaika.com_nificonnections.yaml\n```\n\n----------------------------------------\n\nTITLE: Listing Deleted Helm Releases - Helm (bash)\nDESCRIPTION: This command lists Helm releases that have been deleted.  It requires Helm CLI installed and configured.  The output is a list of deleted Helm releases.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --deleted\n```\n\n----------------------------------------\n\nTITLE: NiFiKop InitContainer Configuration\nDESCRIPTION: This YAML snippet defines a Kubernetes initContainer to automatically update the NiFi flow configurations during the NiFiKop upgrade process. It retrieves the `nifi.sensitive.props.key` from the `nifi.properties` file and uses the `encrypt-config.sh` tool to re-encrypt sensitive properties in both `flow.json.gz` and `flow.xml.gz` files with the `NIFI_PBKDF2_AES_GCM_256` algorithm.  It is important to adapt the `volumeMounts` and `mountPath` definitions to match the specific NiFi deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/7_upgrade_guides/4_v1.3.1_to_v1.4.0.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ninitContainers:\n  - image: \"apache/nifi-toolkit:latest\"\n    name: nifi-toolkit\n    imagePullPolicy: Always\n    command:\n      - \"sh\"\n      - \"-c\"\n      - \"NIFI_SENSITIVE_PROPS_KEY=$(grep 'nifi.sensitive.props.key' /opt/nifi/nifi-current/conf/nifi.properties | cut -d'=' -f2) && bin/encrypt-config.sh -n /opt/nifi/nifi-current/conf/nifi.properties -f /opt/nifi/data/flow.json.gz -x -A NIFI_PBKDF2_AES_GCM_256 -s $NIFI_SENSITIVE_PROPS_KEY; bin/encrypt-config.sh -n /opt/nifi/nifi-current/conf/nifi.properties -f /opt/nifi/data/flow.xml.gz -x -A NIFI_PBKDF2_AES_GCM_256 -s $NIFI_SENSITIVE_PROPS_KEY\"\n    volumeMounts:\n      - name: data\n        mountPath: /opt/nifi/data\n      - name: conf\n        mountPath: /opt/nifi/nifi-current/conf\n```\n\n----------------------------------------\n\nTITLE: Starting Minikube with Minimum Memory\nDESCRIPTION: Command to start a Minikube Kubernetes cluster, ensuring it has at least 4GB of RAM allocated using the --memory flag. This is a recommended minimum for running NiFiKop locally.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/2_deploy_nifikop/1_quick_start.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nminikube start --memory=4000\n```\n\n----------------------------------------\n\nTITLE: Retrieving Zookeeper UID/GID for OpenShift Security Context - Bash\nDESCRIPTION: Retrieves the supplementary groups (UID/GID) allowed for the 'zookeeper' namespace in OpenShift by querying namespace annotations and processing them with sed and tr commands. This UID is used to set the runAsUser and fsGroup in Pod security context for OpenShift deployments, ensuring proper permissions for Zookeeper containers.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nzookeper_uid=$(kubectl get namespace zookeeper -o=jsonpath='{.metadata.annotations.openshift\\.io/sa\\.scc\\.supplemental-groups}' | sed 's/\\/10000$//' | tr -d '[:space:]')\n```\n\n----------------------------------------\n\nTITLE: Defining NifiCluster Resource External YAML\nDESCRIPTION: This YAML snippet defines a NifiCluster custom resource configured to represent and manage an external Apache NiFi cluster. It specifies the cluster's root process group ID, a template for constructing node URIs, a list of node IDs, the cluster type as 'external', the client authentication type ('basic'), and a reference to a Kubernetes secret containing the authentication credentials.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/3_manage_nifi/1_manage_clusters/3_external_cluster.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\nmetadata:\n  name: externalcluster\nspec:\n  # rootProcessGroupId contains the uuid of the root process group for this cluster.\n  rootProcessGroupId: 'd37bee03-017a-1000-cff7-4eaaa82266b7'\n  # nodeURITemplate used to dynamically compute node uri.\n  nodeURITemplate: 'nifi0%d.integ.mapreduce.m0.p.fti.net:9090'\n  # all node requiresunique id\n  nodes:\n    - id: 1\n    - id: 2\n    - id: 3\n  # type defines if the cluster is internal (i.e manager by the operator) or external.\n  # :Enum={\"external\",\"internal\"}\n  type: 'external'\n  # clientType defines if the operator will use basic or tls authentication to query the NiFi cluster.\n  # Enum={\"tls\",\"basic\"}\n  clientType: 'basic'\n  # secretRef reference the secret containing the informations required to authenticate to the cluster.\n  secretRef:\n    name: nifikop-credentials\n    namespace: nifikop-nifi\n```\n\n----------------------------------------\n\nTITLE: Setting NiFi Authorizer Property (Shell)\nDESCRIPTION: This shell command demonstrates how to configure NiFi to use the custom database authorizer defined in the `authorizers.xml` file. It sets the `nifi.security.user.authorizer` property to `custom-database-authorizer`, instructing NiFi to use the custom authorizer for authorization decisions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/6_users_authorization/1_custom_user_authorizer.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nnifi.security.user.authorizer=custom-database-authorizer\n```\n\n----------------------------------------\n\nTITLE: Creating Monitoring Namespace in Kubernetes Using kubectl Console\nDESCRIPTION: This snippet creates a dedicated Kubernetes namespace called 'monitoring-system' to isolate and organize resources related to Prometheus monitoring. It uses the 'kubectl create namespace' command, which requires correct Kubernetes cluster access permissions and kubectl configured context. The namespace serves as a scope for subsequent resource deployments.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_5\n\nLANGUAGE: console\nCODE:\n```\nkubectl create namespace monitoring-system\n```\n\n----------------------------------------\n\nTITLE: Testing NiFiKop Kubectl Plugin Installation - Shell\nDESCRIPTION: Runs the installed nifikop kubectl plugin to display its usage and available commands. This test confirms that the plugin is executable and accessible via the system PATH. The output includes a list of supported NiFi resource commands like nificluster, nificonnection, and nifidataflow, indicating successful installation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/2_deploy_nifikop/3_kubectl_plugin.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\n$ kubectl nifikop\nUsage:\n  nifikop [command]\n\nAvailable Commands:\n  completion          Generate the autocompletion script for the specified shell\n  help                Help about any command\n  nificluster         \n  nificonnection      \n  nifidataflow        \n  nifigroupautoscaler \n  nifiregistryclient  \n  nifiuser            \n  nifiusergroup \n```\n\n----------------------------------------\n\nTITLE: Commands to List Helm Releases Including Deleted and Failed\nDESCRIPTION: Commands to list Helm releases that are deleted or in other states (failed), providing insights into release history and status.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --deleted\n```\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --all\n```\n\n----------------------------------------\n\nTITLE: Testing the NiFiKop kubectl plugin\nDESCRIPTION: Command to verify the successful installation of the kubectl plugin by displaying its help menu, showing available commands and functionality.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/2_deploy_nifikop/3_kubectl_plugin.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\n$ kubectl nifikop\nUsage:\n  nifikop [command]\n\nAvailable Commands:\n  completion          Generate the autocompletion script for the specified shell\n  help                Help about any command\n  nificluster         \n  nificonnection      \n  nifidataflow        \n  nifigroupautoscaler \n  nifiregistryclient  \n  nifiuser            \n  nifiusergroup \n```\n\n----------------------------------------\n\nTITLE: Testing Kubectl NiFiKop Plugin Installation - console\nDESCRIPTION: This command invokes the installed `kubectl nifikop` plugin. A successful installation is indicated by the output showing the usage instructions and a list of available subcommands for managing different NiFiKop resource types (e.g., `nificluster`, `nificonnection`).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/2_deploy_nifikop/3_kubectl_plugin.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\n$ kubectl nifikop\nUsage:\n  nifikop [command]\n\nAvailable Commands:\n  completion          Generate the autocompletion script for the specified shell\n  help                Help about any command\n  nificluster         \n  nificonnection      \n  nifidataflow        \n  nifigroupautoscaler \n  nifiregistryclient  \n  nifiuser            \n  nifiusergroup \n\n```\n\n----------------------------------------\n\nTITLE: Attaching Context Instead of Formatting\nDESCRIPTION: This snippet demonstrates the recommended approach of attaching context information to errors instead of formatting error messages. It utilizes `errors.WrapIfWithDetails` to add key-value details to the error, providing more structured information for debugging and handling.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/docs/error-handling-guide.md#_snippet_4\n\nLANGUAGE: go\nCODE:\n```\nreturn errors.WrapIfWithDetails(err, \"something failed\", \"what\", important)\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop with Custom Release Name\nDESCRIPTION: Command for installing the NiFiKop operator with a custom release name. This allows you to identify and manage multiple installations of the operator within the same cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhelm install <release name> konpyutaika/nifikop\n```\n\n----------------------------------------\n\nTITLE: Purging a Helm Release Bash\nDESCRIPTION: This bash command permanently removes the Helm release named 'nifikop', erasing its record from the Helm release history alongside associated resources. It requires Helm 2.x or compatible versions (note: newer versions use a different command). The key parameter is the release name, and the output is total removal of both deployment and Helm tracking data for 'nifikop'.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/2_setup/3_install/1_customizable_install_with_helm.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nhelm delete --purge nifikop\n```\n\n----------------------------------------\n\nTITLE: Deleting NiFiKop CRDs\nDESCRIPTION: These commands manually delete the Custom Resource Definitions (CRDs) associated with NiFiKop.  Deleting the CRDs will delete ALL NiFi clusters created using these CRDs. Use caution when executing.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/helm/nifikop/README.md#_snippet_8\n\nLANGUAGE: Shell\nCODE:\n```\nkubectl delete crd nificlusters.nifi.konpyutaika.com\nkubectl delete crd nifiusers.nifi.konpyutaika.com\nkubectl delete crd nifiusergroups.nifi.konpyutaika.com\nkubectl delete crd nifiregistryclients.nifi.konpyutaika.com\nkubectl delete crd nifiparametercontexts.nifi.konpyutaika.com\nkubectl delete crd nifidataflows.nifi.konpyutaika.com\n```\n\n----------------------------------------\n\nTITLE: Configuring kubectl for GKE (Console)\nDESCRIPTION: This command configures `kubectl` to connect to your GKE cluster. This command retrieves credentials for your cluster, and then uses `kubectl config use-context` to switch to the correct context for interacting with the cluster. It requires the `gcloud` CLI installed, authorized, and the cluster name, zone and project name.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/2_deploy_nifikop/1_quick_start.md#_snippet_6\n\nLANGUAGE: console\nCODE:\n```\ngcloud container clusters get-credentials CLUSTER_NAME --zone ZONE_NAME --project PROJECT_NAME.\n```\n\n----------------------------------------\n\nTITLE: Configuring Logging Operator Flow (JSON)\nDESCRIPTION: Provides an example configuration for the Banzai Cloud Logging Operator's `Flow` resource used with NiFi logs. It defines filters and match rules, including a regex parser for structuring standard NiFi log lines.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/helm/nifi-cluster/README.md#_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\"filters\":[{\"parser\":{\"parse\":{\"expression\":\"/^(?<time>\\\\d{4}-\\\\d{2}-\\\\d{2} \\\\d{2}:\\\\d{2}:\\\\d{2},\\\\d{3}) (?<level>[^\\\\s]+) \\\\[(?<thread>.*)\\\\] (?<message>.*)$/im\",\"keep_time_key\":true,\"time_format\":\"%iso8601\",\"time_key\":\"time\",\"time_type\":\"string\",\"type\":\"regexp\"}}}]\n```\n\n----------------------------------------\n\nTITLE: Building and Installing Kubectl NiFiKop Plugin - console\nDESCRIPTION: This command sequence builds the `kubectl-nifikop` executable using the `make` command and then copies it to a standard directory in the system's PATH (`/usr/local/bin`) using `sudo` for administrative privileges. This makes the plugin accessible directly from the command line via `kubectl nifikop`.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/2_deploy_nifikop/3_kubectl_plugin.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nmake kubectl-nifikop && sudo cp ./bin/kubectl-nifikop /usr/local/bin\n```\n\n----------------------------------------\n\nTITLE: Defining NifiCluster Resource with Bash InitContainer (YAML)\nDESCRIPTION: This YAML snippet demonstrates the corrected NifiCluster resource definition with initContainerImage set to use the 'bash' image (version 5.2.2). This configuration is required for compatibility with nifikop v0.15.0 and later, ensuring the init container has a bash shell. The resource expects standard nifikop and Kubernetes dependencies and no specific configuration constraints outside the bash requirement.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/7_upgrade_guides/2_v0.14.1_to_v0.15.0.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  initContainerImage:\n    repository: bash\n    tag: \"5.2.2\"\n```\n\n----------------------------------------\n\nTITLE: Rendering Application URLs using Helm Ingress Values - YAML/GoTemplate\nDESCRIPTION: This Helm template snippet generates accessible application URLs by iterating through specified ingress hosts and their associated paths. It conditionally prefixes URLs with 'https' when TLS is enabled, otherwise defaults to 'http'. Requires the application to be deployed in a Kubernetes environment using Helm, with 'ingress' values appropriately set. Outputs all relevant URLs for user access after successful deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/helm/nifi-cluster/templates/NOTES.txt#_snippet_0\n\nLANGUAGE: gotemplate\nCODE:\n```\n{{- if .Values.ingress.enabled }}\n{{- range $host := .Values.ingress.hosts }}\n  {{- range .paths }}\n  http{{ if $.Values.ingress.tls }}s{{ end }}://{{ $host.host }}{{ .path }}\n  {{- end }}\n{{- end }}\n{{- end }}\n```\n\n----------------------------------------\n\nTITLE: Building NiFiKop with Make\nDESCRIPTION: This bash command utilizes the `make` utility to build the NiFiKop project within the local go environment.  The `make build` command is a shorthand for building the project based on the Makefile's specifications.  It assumes that the user has a properly configured Go environment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/6_contributing/1_developer_guide.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmake build\n```\n\n----------------------------------------\n\nTITLE: Querying Deployed HPA Resources for NiFi Scaling - Console\nDESCRIPTION: Lists Horizontal Pod Autoscalers (HPAs) in the clusters namespace, showing resource references, current replica counts, and targets. Used to verify that KEDA has created an HPA for the NiFi autoscaler resource. Requires correct namespace context and HPA permissions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_12\n\nLANGUAGE: console\nCODE:\n```\nkubectl get -n clusters hpa\nNAME                REFERENCE                                                TARGETS         MINPODS   MAXPODS   REPLICAS   AGE\nkeda-hpa-cluster    NifiNodeGroupAutoscaler/nifinodegroupautoscaler-sample     1833m/2 (avg)   1         3         2          25d\n\n```\n\n----------------------------------------\n\nTITLE: kubectl command to retrieve and use GKE cluster credentials in console format\nDESCRIPTION: This console snippet provides the exact command to retrieve Kubernetes cluster credentials for a GKE cluster using the Google Cloud CLI. It requires the user to substitute the placeholders for cluster name, zone, and project. The command allows kubectl to communicate securely with the GKE cluster using the downloaded credentials. This is a prerequisite for managing the cluster with kubectl.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/2_deploy_nifikop/1_quick_start.md#_snippet_5\n\nLANGUAGE: console\nCODE:\n```\ngcloud container clusters get-credentials CLUSTER_NAME --zone ZONE_NAME --project PROJECT_NAME.\n```\n\n----------------------------------------\n\nTITLE: Pod Port Configuration Example in YAML\nDESCRIPTION: This YAML snippet demonstrates the port configuration within a deployed Kubernetes pod. It shows how the `containerPort` and `name` defined in the `internalListeners` configuration are reflected in the pod's ports.  The `protocol` is TCP for each port.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n    ports:\n    - containerPort: 8443\n      name: https\n      protocol: TCP\n    - containerPort: 6007\n      name: cluster\n      protocol: TCP\n    - containerPort: 6342\n      name: load-balance\n      protocol: TCP\n    - containerPort: 10000\n      name: s2s\n      protocol: TCP\n    - containerPort: 9090\n      name: prometheus\n      protocol: TCP\n```\n\n----------------------------------------\n\nTITLE: Listing Deleted Helm Releases - Bash\nDESCRIPTION: This command lists all deleted Helm releases. This allows you to view releases that have been deleted but whose records are still kept by Helm.  It is useful for troubleshooting and understanding the history of Helm deployments in a cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/2_setup/3_install/1_customizable_install_with_helm.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --deleted\n```\n\n----------------------------------------\n\nTITLE: Listing All Helm Releases Including Deleted and Failed Ones\nDESCRIPTION: Command to list all Helm releases in the Kubernetes cluster, including those that are currently deployed, have been deleted, or have failed installation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --all\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop with Custom Release Name\nDESCRIPTION: Basic Helm command to install NiFiKop with a custom release name. This allows multiple installations in the same cluster with different configurations.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nhelm install <release name> konpyutaika/nifikop\n```\n\n----------------------------------------\n\nTITLE: Listing Deployed Helm Charts in Kubernetes\nDESCRIPTION: Command to list all the Helm charts currently deployed in the Kubernetes cluster. This helps to verify that the NiFiKop operator has been successfully installed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nhelm list\n```\n\n----------------------------------------\n\nTITLE: Defining ListenersConfig in YAML\nDESCRIPTION: This YAML snippet defines the `listenersConfig` section, specifying internal listeners with various types (https, cluster, s2s, prometheus, load-balance, and a custom TCP listener). It also configures SSL secrets including the secret name and whether to create the secret.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/5_references/1_nifi_cluster/6_listeners_config.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n  listenersConfig:\n    internalListeners:\n      - type: \"https\"\n        name: \"https\"\n        containerPort: 8443\n      - type: \"cluster\"\n        name: \"cluster\"\n        containerPort: 6007\n      - type: \"s2s\"\n        name: \"s2s\"\n        containerPort: 10000\n      - type: \"prometheus\"\n        name: \"prometheus\"\n        containerPort: 9090\n      - type: \"load-balance\"\n        name: \"load-balance\"\n        containerPort: 6342\n      - name: \"my-custom-listener-port\"\n        containerPort: 1234\n        protocol: \"TCP\"\n    sslSecrets:\n      tlsSecretName: \"test-nifikop\"\n      create: true\n```\n\n----------------------------------------\n\nTITLE: Running the Operator\nLocally with the Makefile\nDESCRIPTION: Starts the NiFiKop operator locally outside the Kubernetes cluster, connecting via the default kubeconfig. Used during development for faster iteration and testing.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/6_contributing/1_developer_guide.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmake run\n```\n\n----------------------------------------\n\nTITLE: Creating Monitoring Namespace\nDESCRIPTION: Creates a dedicated Kubernetes namespace 'monitoring-system' using `kubectl create namespace`. This namespace will be used to deploy Prometheus and related monitoring components.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_5\n\nLANGUAGE: console\nCODE:\n```\nkubectl create namespace monitoring-system\n```\n\n----------------------------------------\n\nTITLE: Setting Operator Name Environment Variable (Bash)\nDESCRIPTION: Sets the `OPERATOR_NAME` environment variable, commonly used when running the operator locally via the Go binary to identify the operator process.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/6_contributing/1_developer_guide.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport OPERATOR_NAME=nifi-operator\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop with Helm\nDESCRIPTION: This command installs the NiFiKop chart from a remote repository using Helm. It specifies the chart name and repository. The command installs the chart without any specific configurations.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop \\\n      -f values.yaml\n```\n\n----------------------------------------\n\nTITLE: Creating a NiFi User with SSL Credentials\nDESCRIPTION: YAML configuration for creating a NiFi user with SSL credentials that can be used by client applications to securely connect to the NiFi cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/3_tasks/2_security/1_ssl.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion:  nifi.konpyutaika.com/v1alpha1\nkind: NifiUser\nmetadata:\n  name: example-client\n  namespace: nifi\nspec:\n  clusterRef:\n    name: nifi\n  secretName: example-client-secret\n```\n\n----------------------------------------\n\nTITLE: Updating Helm Repositories\nDESCRIPTION: Updates local Helm chart repositories to ensure latest versions are available before installation. Essential step before deploying Helm charts.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nhelm repo update\n```\n\n----------------------------------------\n\nTITLE: Building NiFiKop Operator Locally using Bash\nDESCRIPTION: Executes the `build` target in the Makefile to compile the NiFiKop operator Go source code into a binary executable on the local machine. Requires Go v1.17+ to be installed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/6_contributing/1_developer_guide.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmake build\n```\n\n----------------------------------------\n\nTITLE: Starting Minikube with Specified Memory (Bash)\nDESCRIPTION: Starts a Minikube Kubernetes cluster, allocating 4000MB of RAM. This is recommended for local testing of NiFiKop.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/2_deploy_nifikop/1_quick_start.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nminikube start --memory=4000\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop with Dry Run in Kubernetes\nDESCRIPTION: Helm command to perform a dry run installation of NiFiKop with debug logging enabled and namespace configuration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhelm install nifikop konpyutaika/nifikop \\\n    --dry-run \\\n    --set logLevel=Debug \\\n    --set namespaces={\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Setting up NPM dependencies for NiFiKop CRD migration script\nDESCRIPTION: Commands to initialize a Node.js project and install required dependencies (@kubernetes/client-node and minimist) for the migration script.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm init -y\nnpm install @kubernetes/client-node@0.16.3 minimist@1.2.6\n```\n\n----------------------------------------\n\nTITLE: Cloning the NiFiKop Repository in Bash\nDESCRIPTION: Commands to clone the NiFiKop repository from GitHub and navigate to the project directory. This is the first step in setting up a local development environment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/6_contributing/1_developer_guide.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/konpyutaika/nifikop.git\ncd nifikop\n```\n\n----------------------------------------\n\nTITLE: Running NiFiKop locally\nDESCRIPTION: Runs the NiFiKop operator in the local environment. It uses the default Kubernetes configuration file to connect to the cluster. Assumes CRDs have been previously applied.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/6_contributing/1_developer_guide.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmake run\n```\n\n----------------------------------------\n\nTITLE: Setting Operator Name Environment Variable using Bash\nDESCRIPTION: Sets the `OPERATOR_NAME` environment variable to `nifi-operator`. This is required when running the operator locally outside the cluster using the Go binary method.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/6_contributing/1_developer_guide.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport OPERATOR_NAME=nifi-operator\n```\n\n----------------------------------------\n\nTITLE: Accessing Prometheus UI via Port Forwarding\nDESCRIPTION: Commands to port-forward the Prometheus service to localhost, allowing access to the Prometheus web UI on port 9090.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_11\n\nLANGUAGE: console\nCODE:\n```\nkubectl port-forward -n monitoring-system service/prometheus-operated 9090:9090\n```\n\n----------------------------------------\n\nTITLE: Adding Bitnami Helm Repository for Zookeeper\nDESCRIPTION: This script adds the Bitnami Helm repository to facilitate installing Zookeeper in the Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/2_setup/1_getting_started.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhelm repo add bitnami https://charts.bitnami.com/bitnami\n```\n\n----------------------------------------\n\nTITLE: Listing Deployed Charts\nDESCRIPTION: This command lists all deployed Helm charts in the current Kubernetes context. It's used to verify installations and check the status of deployed releases.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nhelm list\n```\n\n----------------------------------------\n\nTITLE: Adding KEDA Helm Repository - Console\nDESCRIPTION: This command adds the KEDA Helm repository to your Helm configuration, allowing you to install KEDA using Helm.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nhelm repo add kedacore https://kedacore.github.io/charts\n```\n\n----------------------------------------\n\nTITLE: Deploying Prometheus Operator via Helm - console\nDESCRIPTION: Installs the prometheus-operator and related resources into the 'monitoring-system' namespace using Helm with various custom configuration flags. Requires Helm and access to the Kubernetes cluster, as well as the prometheus community repository to be added and updated. Command parameters customize Prometheus and monitoring component behavior (e.g., disabling Grafana and setting log levels). Output is a set of Kubernetes resources representing the Prometheus stack in the designated namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_8\n\nLANGUAGE: console\nCODE:\n```\nhelm install prometheus prometheus/kube-prometheus-stack --namespace monitoring-system \\\n    --set prometheusOperator.createCustomResource=false \\\n    --set prometheusOperator.logLevel=debug \\\n    --set prometheusOperator.alertmanagerInstanceNamespaces=monitoring-system \\\n    --set prometheusOperator.namespaces.additional[0]=monitoring-system \\\n    --set prometheusOperator.prometheusInstanceNamespaces=monitoring-system \\\n    --set prometheusOperator.thanosRulerInstanceNamespaces=monitoring-system \\\n    --set defaultRules.enable=false \\\n    --set alertmanager.enabled=false \\\n    --set grafana.enabled=false \\\n    --set kubeApiServer.enabled=false \\\n    --set kubelet.enabled=false \\\n    --set kubeControllerManager.enabled=false \\\n    --set coreDNS.enabled=false \\\n    --set kubeEtcd.enabled=false \\\n    --set kubeScheduler.enabled=false \\\n    --set kubeProxy.enabled=false \\\n    --set kubeStateMetrics.enabled=false \\\n    --set prometheus.enabled=false\n\n```\n\n----------------------------------------\n\nTITLE: Installing Helm Chart with Release Name\nDESCRIPTION: Helm command to deploy the Nifikop chart with a specified release name. Allows customization of the deployment identifier in Kubernetes and Helm records.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nhelm install <release name> konpyutaika/nifikop\n```\n\n----------------------------------------\n\nTITLE: Verifying NiFiKop Pod Status after Helm Install (console)\nDESCRIPTION: This command uses kubectl to check the status of pods in the 'nifikop' namespace after the Helm installation. The expected output shows a running operator pod, confirming that the deployment was successful.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/6_contributing/1_developer_guide.md#_snippet_8\n\nLANGUAGE: console\nCODE:\n```\n$ kubectl get pods -n nifikop\nNAME                                                READY   STATUS    RESTARTS   AGE\nskeleton-nifikop-8946b89dc-4cfs9   1/1     Running   0          7m45s\n```\n\n----------------------------------------\n\nTITLE: Retrieving kubectl credentials for GKE\nDESCRIPTION: Retrieves the credentials for kubectl, allowing the user to interact with the GKE cluster. It uses the gcloud command-line tool and requires the CLUSTER_NAME, GCP_ZONE, and GCP_PROJECT environment variables to be set. After running this command, kubectl will be configured to manage the specified cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/2_setup/2_platform_setup/1_gke.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ncloud container clusters get-credentials $CLUSTER_NAME \\\n    --zone $GCP_ZONE \\\n    --project $GCP_PROJECT\n```\n\n----------------------------------------\n\nTITLE: Adding Start Script to package.json for NiFiKop Migration\nDESCRIPTION: JSON snippet to add a start script to package.json that runs the migration script with warnings disabled.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/7_upgrade/1_v0.7.x_to_v0.8.0.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"start\": \"node --no-warnings index.js\"\n```\n\n----------------------------------------\n\nTITLE: Installing Nifikop Helm Chart with a Specific Release Name\nDESCRIPTION: Installs the Nifikop Helm chart from the `konpyutaika` repository, assigning it a specific release name provided by the user.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nhelm install <release name> konpyutaika/nifikop\n```\n\n----------------------------------------\n\nTITLE: Connecting to GKE cluster using gcloud\nDESCRIPTION: Command for configuring kubectl to connect to a Google Kubernetes Engine (GKE) cluster using gcloud CLI authentication.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/2_deploy_nifikop/1_quick_start.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\ngcloud container clusters get-credentials CLUSTER_NAME --zone ZONE_NAME --project PROJECT_NAME.\n```\n\n----------------------------------------\n\nTITLE: InitClusterNode Enumeration in NiFiKop\nDESCRIPTION: Enumeration that indicates whether a node was part of the initial cluster setup or added later during scaling operations.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/5_references/1_nifi_cluster/5_node_state.md#_snippet_3\n\nLANGUAGE: go\nCODE:\n```\ntype InitClusterNode bool\n\nconst (\n\tIsInitClusterNode InitClusterNode = true\n\tNotInitClusterNode InitClusterNode = false\n)\n```\n\n----------------------------------------\n\nTITLE: Listing All Helm Releases\nDESCRIPTION: This snippet uses Helm to list all releases, including those deployed, deleted, and failed. Requires Helm installed and configured.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --all\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for NiFiKop\nDESCRIPTION: Commands to build a Docker image for NiFiKop from the current branch. Requires setting a Docker repository base name as an environment variable.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/6_contributing/1_developer_guide.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nexport DOCKER_REPO_BASE={your-docker-repo}\nmake docker-build\n```\n\n----------------------------------------\n\nTITLE: Building and Installing the NiFiKop Kubectl Plugin\nDESCRIPTION: Commands to build the NiFiKop kubectl plugin and install it to a system path. This makes the plugin accessible as a kubectl subcommand.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/2_deploy_nifikop/3_kubectl_plugin.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nmake kubectl-nifikop && sudo cp ./bin/kubectl-nifikop /usr/local/bin\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop with Custom Release Name in Kubernetes\nDESCRIPTION: Helm command to install NiFiKop with a custom release name from the konpyutaika repository.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nhelm install <release name> konpyutaika/nifikop\n```\n\n----------------------------------------\n\nTITLE: Deleting Helm Release\nDESCRIPTION: This command uninstalls a Helm chart and removes the associated resources from the Kubernetes cluster. The 'nifikop' is the release name.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nhelm del nifikop\n```\n\n----------------------------------------\n\nTITLE: Listing Deleted Helm Releases\nDESCRIPTION: Command to display all deleted Helm releases. Useful for tracking historical deployments and for potential rollback operations.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --deleted\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for NiFiKop in Bash\nDESCRIPTION: Commands to build a Docker image for the NiFiKop operator from the local branch. Requires setting the Docker registry base variable.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/6_contributing/1_developer_guide.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nexport DOCKER_REGISTRY_BASE={your-docker-repo}\nmake docker-build\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Namespace for Monitoring (Console)\nDESCRIPTION: Creates a dedicated Kubernetes namespace named 'monitoring-system'. This namespace is used to isolate the Prometheus monitoring components.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_5\n\nLANGUAGE: console\nCODE:\n```\nkubectl create namespace monitoring-system\n```\n\n----------------------------------------\n\nTITLE: Adding Prometheus Helm Repository\nDESCRIPTION: Adds the Prometheus community Helm repository to access the Prometheus Operator chart for installation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_6\n\nLANGUAGE: console\nCODE:\n```\nhelm repo add prometheus https://prometheus-community.github.io/helm-charts\n```\n\n----------------------------------------\n\nTITLE: Adding Prometheus Helm Repository\nDESCRIPTION: Adds the Prometheus community Helm repository which contains charts for deploying Prometheus components.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nhelm repo add prometheus https://prometheus-community.github.io/helm-charts\n```\n\n----------------------------------------\n\nTITLE: Updating OpenShift Configuration with Correct UID\nDESCRIPTION: Bash command to update the OpenShift sample configuration file with the correct UID value extracted from the namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nsed -i \"s/1000690000/$uid/g\" config/samples/openshift.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing NiFiKop via Helm\nDESCRIPTION: Command to install NiFiKop using Helm. This deploys the operator to a Kubernetes cluster with the specified image tag and namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/6_contributing/1_developer_guide.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nhelm install skeleton ./helm/nifikop \\\n    --set image.tag=v0.5.1-release \\\n    --namespace-{\"nifikop\"}\n```\n\n----------------------------------------\n\nTITLE: Migrating NiFiKop Operator Resources Using Node.js and Kubernetes Client - JavaScript\nDESCRIPTION: A Node.js script that migrates NiFiKop custom resources from the old CRD group nifi.orange.com/v1alpha1 to the new group nifi.konpyutaika.com/v1alpha1. The script loads Kubernetes config, initializes the CustomObjectsApi client, lists resources of a specified type and namespace, and creates corresponding resources in the new group while preserving metadata and spec fields. It then copies the resource status to ensure consistency. The script handles CLI arguments for resource type and namespace, checks for owner references to avoid migrating dependent resources, and performs error handling with verbose logging. Requires Node.js v15.3.0+, npm v7.0.14+, and installed dependencies @kubernetes/client-node and minimist.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nprocess.env['NODE_TLS_REJECT_UNAUTHORIZED'] = 0;\nconst k8s = require('@kubernetes/client-node');\n\nconst kc = new k8s.KubeConfig();\nkc.loadFromDefault();\n\nconst k8sApi = kc.makeApiClient(k8s.CustomObjectsApi);\n\nconst KONPYUTAIKA_GROUP = 'nifi.konpyutaika.com';\nconst KONPYUTAIKA_GROUP_VERSION = 'v1alpha1';\nconst ORANGE_GROUP = 'nifi.orange.com';\nconst ORANGE_GROUP_VERSION = 'v1alpha1';\n\nconst call = async (SRC_GRP, SRC_GRP_VER, DST_GRP, DST_GRP_VER, KIND_PLURAL, NAMESPACE) => {\n\tconsole.log(`Listing ${KIND_PLURAL} of ${SRC_GRP}/${SRC_GRP_VER} in ${NAMESPACE}...`);\n\tconst listResources = (await k8sApi.listNamespacedCustomObject(SRC_GRP, SRC_GRP_VER, NAMESPACE, KIND_PLURAL)).body.items;\n\treturn Promise.all(listResources.map(async (resource) => {\n\t\ttry {\n\t\t\tconsole.log(`Found ${resource.kind} \\\"${resource.metadata.name}\\\" of ${resource.apiVersion} in ${NAMESPACE}`);\n\n\t\t\tif (resource.metadata.ownerReferences) {\n\t\t\t\tconsole.log(`${resource.kind} ${resource.metadata.name} mananged by something else (ownerRefereces is set).`);\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tconst bodyResource = {\n\t\t\t\tapiVersion: `${DST_GRP}/${DST_GRP_VER}`,\n\t\t\t\tkind: resource.kind,\n\t\t\t\tmetadata: {\n\t\t\t\t\tname: resource.metadata.name,\n\t\t\t\t\tannotations: resource.metadata.annotations,\n\t\t\t\t\tlabels: resource.metadata.labels\n\t\t\t\t},\n\t\t\t\tspec: resource.spec\n\t\t\t};\n\n\t\t\tconsole.log(`Creating ${bodyResource.kind} \\\"${bodyResource.metadata.name}\\\" of ${bodyResource.apiVersion} in ${NAMESPACE}...`);\n\t\t\tconst newResource = (await k8sApi.createNamespacedCustomObject(DST_GRP, DST_GRP_VER, NAMESPACE, KIND_PLURAL, bodyResource)).body;\n\t\t\tconsole.log('...done creating.');\n\n\t\t\tconst bodyStatus = {\n\t\t\t\tapiVersion: newResource.apiVersion,\n\t\t\t\tkind: newResource.kind,\n\t\t\t\tmetadata: {\n\t\t\t\t\tname: newResource.metadata.name,\n\t\t\t\t\tresourceVersion: newResource.metadata.resourceVersion\n\t\t\t\t},\n\t\t\t\tstatus: resource.status\n\t\t\t};\n\n\t\t\tconsole.log(`Copying status from ${resource.kind} \\\"${resource.metadata.name}\\\" of ${newResource.apiVersion} to ${newResource.kind} \\\"${newResource.metadata.name}\\\" of ${newResource.apiVersion} in ${NAMESPACE}...`);\n\t\t\tconst newResourceWithStatus = (await k8sApi.replaceNamespacedCustomObjectStatus(DST_GRP, DST_GRP_VER, NAMESPACE, KIND_PLURAL, bodyStatus.metadata.name, bodyStatus)).body;\n\t\t\tconsole.log('...done copying.');\n\t\t\treturn newResourceWithStatus;\n\t\t}\n\t\tcatch (e) {\n\t\t\tconsole.error(e.body ? e.body.message ? e.body.message : e.body : e);\n\t\t}\n\t}));\n};\n\nconst argv = require('minimist')(process.argv.slice(2));\n\nlet NAMESPACE = argv.namespace ? argv.namespace.length > 0 ? argv.namespace : 'default' : 'default';\nlet KIND_PLURAL = {\n\tcluster: 'nificlusters',\n\tdataflow: 'nifidataflows',\n\tparametercontext: 'nifiparametercontexts',\n\tregistryclient: 'nifiregistryclients',\n\tuser: 'nifiusers',\n\tusergroup: 'nifiusergroups',\n};\n\nif (!argv.type) {\n\tconsole.error('Type not provided');\n\tprocess.exit(1);\n}\n\nif (!KIND_PLURAL[argv.type]) {\n\tconsole.error(`Type ${argv.type} is not one of the following types: ${Object.keys(KIND_PLURAL)}`);\n\tprocess.exit(1);\n}\n\nconsole.log(`########### START: ${KIND_PLURAL[argv.type]} ###########`);\ncall( ORANGE_GROUP, ORANGE_GROUP_VERSION, KONPYUTAIKA_GROUP, KONPYUTAIKA_GROUP_VERSION, KIND_PLURAL[argv.type], NAMESPACE)\n\t.then(r => console.log('############ END ############'))\n\t.catch(e => console.error(e));\n```\n\n----------------------------------------\n\nTITLE: Purging a Helm Release\nDESCRIPTION: Command to permanently delete a Helm release from Helm's records, allowing the release name to be reused or for complete cleanup.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nhelm delete --purge nifikop\n```\n\n----------------------------------------\n\nTITLE: Verifying NiFiKop Deployment in Bash\nDESCRIPTION: Command to verify that the NiFiKop operator pod is running in the specified namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/6_contributing/1_developer_guide.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get pods -n nifikop\n```\n\n----------------------------------------\n\nTITLE: Fetching Status of a Specific Helm Release in Bash\nDESCRIPTION: This command retrieves detailed status information about the specified Helm release \"nifikop\", including release metadata, deployed resources, hooks, and notes. It requires Helm version 3 or higher.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nhelm status nifikop\n```\n\n----------------------------------------\n\nTITLE: Getting Helm Deployment Status\nDESCRIPTION: This command retrieves the status of a specific Helm release, providing information about its deployment status, resources, and any related events. The `nifikop` is the release name.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nhelm status nifikop\n```\n\n----------------------------------------\n\nTITLE: Kubectl Get Pods in cert-manager\nDESCRIPTION: This command retrieves a list of pods within the cert-manager namespace. This is crucial for verifying the successful deployment of cert-manager and its components. It displays the pod names, readiness status, and age, confirming that the certificate management system is running.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/blog/2020-06-30-secured_nifi_cluster_on_gcp_with_external_dns.md#_snippet_5\n\nLANGUAGE: console\nCODE:\n```\nkubectl -n cert-manager get pods\n```\n\n----------------------------------------\n\nTITLE: Deleting and Purging a Helm Release (Legacy) - Bash\nDESCRIPTION: Runs 'helm delete --purge nifikop' to fully delete a Helm release and purge its history from the cluster. Note: with Helm 3, '--purge' is deprecated, but included for legacy compatibility. Input is the release name; output is complete removal from Helm's listing and history.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nhelm delete --purge nifikop\n```\n\n----------------------------------------\n\nTITLE: Getting OpenShift UID for Zookeeper\nDESCRIPTION: Command to retrieve the allowed UID/GID values for Zookeeper deployment in OpenShift, which is necessary for proper security context configuration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/1_quick_start.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nzookeper_uid=$(kubectl get namespace zookeeper -o=jsonpath='{.metadata.annotations.openshift\\.io/sa\\.scc\\.supplemental-groups}' | sed 's/\\/10000$//' | tr -d '[:space:]')\n```\n\n----------------------------------------\n\nTITLE: Verifying HPA Deployment for NiFi Autoscaler using Kubectl\nDESCRIPTION: This command uses kubectl to retrieve information about the Horizontal Pod Autoscaler (HPA) deployed in the 'clusters' namespace. It verifies that the HPA is managing the specified NifiNodeGroupAutoscaler and displays its current status, including target metrics, replica counts, and age. Requires kubectl to be configured and connected to the Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\nkubectl get -n clusters hpa\nNAME                REFERENCE                                                TARGETS         MINPODS   MAXPODS   REPLICAS   AGE\nkeda-hpa-cluster    NifiNodeGroupAutoscaler/nifinodegroupautoscaler-sample     1833m/2 (avg)   1         3         2          25d\n```\n\n----------------------------------------\n\nTITLE: List Deployed Helm Charts\nDESCRIPTION: This command lists all deployed Helm charts in the current namespace. It provides information about the charts, including their names, versions, and statuses. The command is useful for verifying chart deployments and identifying existing releases.  Dependencies: Helm.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nhelm list\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Namespace for Monitoring System - console\nDESCRIPTION: Creates a dedicated Kubernetes namespace called 'monitoring-system' to isolate all monitoring resources. No dependencies beyond a running Kubernetes cluster and kubectl access. Takes no parameters and produces a new namespace if successful. Namespace must not already exist.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_5\n\nLANGUAGE: console\nCODE:\n```\nkubectl create namespace monitoring-system\n```\n\n----------------------------------------\n\nTITLE: Packaging NiFiKop Helm Chart using Make in Bash\nDESCRIPTION: This command packages the NiFiKop Helm chart into a distributable archive. It requires Make configured with the appropriate rules to locate the Helm chart directory and package it accordingly for distribution or deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/6_contributing/1_developer_guide.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nmake helm-package\n```\n\n----------------------------------------\n\nTITLE: Building NiFiKop in Local Environment\nDESCRIPTION: Commands to build the NiFiKop operator in a local development environment using make. This compiles the Go code into an executable binary.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/6_contributing/1_developer_guide.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmake build\n```\n\n----------------------------------------\n\nTITLE: Listing Helm Deployments\nDESCRIPTION: Command to list all Helm chart deployments in the cluster. Useful for verifying installation status and tracking deployments.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nhelm list\n```\n\n----------------------------------------\n\nTITLE: Adding start script to package.json\nDESCRIPTION: JSON snippet showing the start script to add to package.json for running the migration script without Node.js warnings.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"start\": \"node --no-warnings index.js\"\n```\n\n----------------------------------------\n\nTITLE: Deploying KEDA with Helm - Installing KEDA\nDESCRIPTION: Commands to create a dedicated namespace for KEDA and install it using Helm.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl create namespace keda\nhelm install keda kedacore/keda --namespace keda\n```\n\n----------------------------------------\n\nTITLE: Building NiFiKop Locally (Bash)\nDESCRIPTION: Executes the `build` target in the Makefile to compile the NiFiKop operator source code in the local Go environment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/6_contributing/1_developer_guide.md#_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\nmake build\n```\n\n----------------------------------------\n\nTITLE: Creating Prometheus Monitoring Namespace\nDESCRIPTION: Command to create a dedicated Kubernetes namespace for the Prometheus monitoring system.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_3\n\nLANGUAGE: console\nCODE:\n```\nkubectl create namespace monitoring-system\n```\n\n----------------------------------------\n\nTITLE: Creating a Dedicated Monitoring Namespace in Kubernetes\nDESCRIPTION: Creates a dedicated Kubernetes namespace called 'monitoring-system' to isolate monitoring components.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_3\n\nLANGUAGE: console\nCODE:\n```\nkubectl create namespace monitoring-system\n```\n\n----------------------------------------\n\nTITLE: Cloning the NiFiKop repository\nDESCRIPTION: Commands to check out the NiFiKop project repository from GitHub and navigate to the project directory.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/6_contributing/1_developer_guide.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/konpyutaika/nifikop.git\ncd nifikop\n```\n\n----------------------------------------\n\nTITLE: Getting Status of a Helm Deployment - Bash\nDESCRIPTION: Fetches the status of the nifikop release using helm status. Returns a textual summary of the release, including resource statuses and any potential errors. Prerequisite: nifikop chart must be installed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nhelm status nifikop\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Local IDE - Bash\nDESCRIPTION: Sets environment variables required for connecting to a Kubernetes cluster from a local IDE. Requires kubectl and a valid kubeconfig. These variables specify the path to the kubeconfig file, the namespace to watch, the name of the operator pod, and the log level. Ensure that all the parameters are configured based on the targeted Kubernetes environment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/6_contributing/1_developer_guide.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nKUBECONFIG={path/to/your/kubeconfig}\nWATCH_NAMESPACE={namespace_to_watch}\nPOD_NAME={name for operator pod}\nLOG_LEVEL=Debug\nOPERATOR_NAME=ide\n```\n\n----------------------------------------\n\nTITLE: Pod Port Configuration Generated from Internal Listeners\nDESCRIPTION: Generated pod YAML showing the port configuration derived from internal listeners. This demonstrates how the internal listener configuration translates to container port definitions in the Kubernetes pod specification.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n    ports:\n    - containerPort: 8443\n      name: https\n      protocol: TCP\n    - containerPort: 6007\n      name: cluster\n      protocol: TCP\n    - containerPort: 6342\n      name: load-balance\n      protocol: TCP\n    - containerPort: 10000\n      name: s2s\n      protocol: TCP\n    - containerPort: 9090\n      name: prometheus\n      protocol: TCP\n```\n\n----------------------------------------\n\nTITLE: Setting Operator Name for Local Execution\nDESCRIPTION: Command to set the operator name as an environment variable before local deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/6_contributing/1_developer_guide.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport OPERATOR_NAME=nifi-operator\n```\n\n----------------------------------------\n\nTITLE: Applying NiFi Cluster Configuration Changes\nDESCRIPTION: Shell command to apply the updated NiFi cluster configuration to the Kubernetes environment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/3_tasks/1_nifi_cluster/2_cluster_scaling.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nkubectl -n nifi apply -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Listing Deployed Helm Charts - Bash\nDESCRIPTION: Lists all currently deployed Helm charts in the Kubernetes cluster. No parameters are passed, so all default releases for the current context are shown. Requires helm CLI access.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nhelm list\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi OIDC via NiFiKop OverrideConfigs (YAML)\nDESCRIPTION: YAML example demonstrating how to apply OIDC settings (discovery URL, client ID, client secret) and identity mapping configurations to a NiFi cluster managed by NiFiKop. This is achieved using the `spec.readOnlyConfig.nifiProperties.overrideConfigs` field within the `NifiCluster` custom resource, which allows overriding default or template-based `nifi.properties` values.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/3_manage_nifi/1_manage_clusters/1_deploy_cluster/5_users_authentication/1_oidc.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1\nkind: NifiCluster\n...\nspec:\n  ...\n  readOnlyConfig:\n    # NifiProperties configuration that will be applied to the node.\n    nifiProperties:\n      webProxyHosts:\n        - nifistandard2.trycatchlearn.fr:8443\n      # Additionnal nifi.properties configuration that will override the one produced based\n      # on template and configurations.\n      overrideConfigs: |\n        nifi.security.user.oidc.discovery.url=<oidc server discovery url>\n        nifi.security.user.oidc.client.id=<oidc client's id>\n        nifi.security.user.oidc.client.secret=<oidc client's secret>\n        nifi.security.identity.mapping.pattern.dn=CN=([^,]*)(?:, (?:O|OU)=.*)?\n        nifi.security.identity.mapping.value.dn=$1\n        nifi.security.identity.mapping.transform.dn=NONE\n      ...\n   ...\n...\n```\n\n----------------------------------------\n\nTITLE: Verifying NiFiKop deployment\nDESCRIPTION: Command to check if the NiFiKop operator pod is running successfully in the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/6_contributing/1_developer_guide.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get pods -n nifikop\n```\n\n----------------------------------------\n\nTITLE: Exposing NiFi Cluster Ports via K3D\nDESCRIPTION: Exposes the specified NiFi cluster port on the K3D load balancer. Replace <nifi_cluster_port> with the actual port number. This command edits the existing 'k3s-default' cluster configuration to add the port mapping.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/2_setup/2_platform_setup/2_k3d.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nk3d cluster edit k3s-default --port-add \"<nifi_cluster_port>:<nifi_cluster_port>@loadbalancer\"\n```\n\n----------------------------------------\n\nTITLE: Creating Prometheus Monitoring Namespace\nDESCRIPTION: Command to create a dedicated Kubernetes namespace for Prometheus monitoring components.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_3\n\nLANGUAGE: console\nCODE:\n```\nkubectl create namespace monitoring-system\n```\n\n----------------------------------------\n\nTITLE: Delete CRDs Manually\nDESCRIPTION: These commands manually delete the Custom Resource Definitions (CRDs) associated with the Nifikop operator using `kubectl`. This is usually done *after* uninstalling the helm chart. Deleting CRDs will also delete any resources created using them.  Dependencies: kubectl, Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete crd nificlusters.nifi.konpyutaika.com\nkubectl delete crd nifiusers.nifi.konpyutaika.com\nkubectl delete crd nifiusergroups.nifi.konpyutaika.com\nkubectl delete crd nifiregistryclients.nifi.konpyutaika.com\nkubectl delete crd nifiparametercontexts.nifi.konpyutaika.com\nkubectl delete crd nifidataflows.nifi.konpyutaika.com\n```\n\n----------------------------------------\n\nTITLE: Pushing NiFiKop Docker image\nDESCRIPTION: Command to push the built NiFiKop Docker image to a container registry.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/6_contributing/1_developer_guide.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nmake docker-push\n```\n\n----------------------------------------\n\nTITLE: Building NiFiKop locally\nDESCRIPTION: Command to build the NiFiKop operator using the local Go environment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/6_contributing/1_developer_guide.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmake build\n```\n\n----------------------------------------\n\nTITLE: Getting Helm Deployment Status\nDESCRIPTION: This command retrieves the status of a specific Helm release named `nifikop`.  It provides details like deployed resources, events, and any issues.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/2_setup/3_install/1_customizable_install_with_helm.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nhelm status nifikop\n```\n\n----------------------------------------\n\nTITLE: Running NiFiKop Resource Migration Script - Bash\nDESCRIPTION: This bash command demonstrates how to execute the Node.js migration script with npm, passing resource type and namespace as arguments. '--type' specifies the NiFiKop resource category (e.g., cluster or dataflow), '--namespace' defines the Kubernetes namespace. The migration only works if prerequisites (CRDs, Node.js, npm dependencies) are met. Outputs logs and migration results.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nnpm start -- --type=<NIFIKOP_RESOURCE> --namespace=<K8S_NAMESPACE>\n```\n\n----------------------------------------\n\nTITLE: Configuring NifiCluster InitContainerImage - YAML\nDESCRIPTION: This YAML snippet demonstrates an example configuration for the `NifiCluster` resource where the `initContainerImage` is overridden. The example sets the image repository to `busybox` and the tag to \"1.34.0\".  Users who have a similar configuration must change the `repository` or the image tag.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/7_upgrade_guides/2_v0.14.1_to_v0.15.0.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  initContainerImage:\n    repository: busybox\n    tag: \"1.34.0\"\n```\n\n----------------------------------------\n\nTITLE: Patching NifiKop CRDs for v1alpha1 to v1 Migration (YAML)\nDESCRIPTION: This YAML snippet demonstrates the required patch for NifiKop CRDs (e.g., NifiCluster, NifiDataflow) to enable the conversion webhook for migrating from `v1alpha1` to `v1`. It includes annotations for cert-manager CA injection and specifies the webhook conversion strategy, pointing to the webhook service within the deployment namespace. Placeholders like `${namespace}`, `${certificate_name}`, and `${webhook_service_name}` need to be replaced with actual values from the Helm release.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/7_upgrade_guides/3_v0.16.0_to_v1.0.0.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n```\n\n----------------------------------------\n\nTITLE: Updating NifiCluster to Use Bash Init Container (v0.15.0)\nDESCRIPTION: Updated YAML configuration for a `NifiCluster` resource, changing the `initContainerImage` to `bash:5.2.2` or another image containing a bash shell. This change is necessary for compatibility with Nifikop v0.15.0 and later due to the requirement for a bash shell in the Zookeeper init container.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/7_upgrade_guides/2_v0.14.1_to_v0.15.0.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  initContainerImage:\n    repository: bash\n    tag: \"5.2.2\"\n```\n\n----------------------------------------\n\nTITLE: NifiCluster Init Container Configuration (bash)\nDESCRIPTION: This YAML snippet shows the updated configuration of a NifiCluster resource with the `initContainerImage` specified as `bash`. This configuration is necessary when upgrading to Nifikop v0.15.0 if the `initContainerImage` was previously overridden. The snippet defines the API version, kind, metadata (name), and the initContainerImage repository and tag.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/7_upgrade_guides/2_v0.14.1_to_v0.15.0.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  initContainerImage:\n    repository: bash\n    tag: \"5.2.2\"\n```\n\n----------------------------------------\n\nTITLE: Checking NiFiKop Operator Pod Status in Kubernetes Using kubectl Console Command\nDESCRIPTION: This command verifies that the NiFiKop operator pod is running successfully inside the nifi namespace. The operator is responsible for managing NiFi cluster lifecycle events, deployments, and configurations within Kubernetes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/blog/2020-05-20-secured_nifi_cluster_on_gcp.md#_snippet_7\n\nLANGUAGE: console\nCODE:\n```\nkubectl -n nifi get pods\nNAME          READY   STATUS    RESTARTS   AGE\nnifikop-849fc8548f-ss6w4   1/1     Running   0          74m\n```\n\n----------------------------------------\n\nTITLE: Configuring Conversion Webhook Settings for CRDs\nDESCRIPTION: Instructions to add webhook configuration annotations within the CRD YAML to enable resource conversion from v1alpha1 to v1 using a webhook, specifying namespace, webhook client service, and conversion review versions. This setup is necessary when keeping the conversion webhook enabled.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n... \nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n\n```\n\n----------------------------------------\n\nTITLE: Updating KEDA Helm Repo\nDESCRIPTION: This command updates the local Helm repository with the latest information from the remote repository. This ensures that you have access to the most recent chart versions and dependencies.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nhelm repo update\n```\n\n----------------------------------------\n\nTITLE: Retrieving OpenShift Namespace UID Range (Bash)\nDESCRIPTION: Bash command using `kubectl` with JSONPath to extract the base UID from the `openshift.io/sa.scc.supplemental-groups` annotation of the `nifi` namespace on OpenShift. The extracted UID is stored in the `uid` variable, needed for setting `runAsUser` during Helm deployment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/2_deploy_nifikop/1_quick_start.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nuid=$(kubectl get namespace nifi -o=jsonpath='{.metadata.annotations.openshift\\.io/sa\\.scc\\.supplemental-groups}' | sed 's/\\/10000$//' | tr -d '[:space:]')\n```\n\n----------------------------------------\n\nTITLE: Automating NiFi Flow Re-encryption with Kubernetes initContainer (yaml)\nDESCRIPTION: Provides a Kubernetes `initContainer` definition designed to automate the re-encryption of NiFi flow configuration files (`flow.json.gz`, `flow.xml.gz`) during the startup phase of a NiFi pod. It utilizes the `apache/nifi-toolkit` image to run the `encrypt-config.sh` command, extracting the sensitive properties key from the mounted `nifi.properties` file and applying the new `NIFI_PBKDF2_AES_GCM_256` algorithm to the flow files. This approach is useful for automating the upgrade process.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/7_upgrade_guides/4_v1.3.1_to_v1.4.0.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ninitContainers:\n  - image: \"apache/nifi-toolkit:latest\"\n    name: nifi-toolkit\n    imagePullPolicy: Always\n    command:\n      - \"sh\"\n      - \"-c\"\n      - \"NIFI_SENSITIVE_PROPS_KEY=$(grep 'nifi.sensitive.props.key' /opt/nifi/nifi-current/conf/nifi.properties | cut -d'=' -f2) && bin/encrypt-config.sh -n /opt/nifi/nifi-current/conf/nifi.properties -f /opt/nifi/data/flow.json.gz -x -A NIFI_PBKDF2_AES_GCM_256 -s $NIFI_SENSITIVE_PROPS_KEY; bin/encrypt-config.sh -n /opt/nifi/nifi-current/conf/nifi.properties -f /opt/nifi/data/flow.xml.gz -x -A NIFI_PBKDF2_AES_GCM_256 -s $NIFI_SENSITIVE_PROPS_KEY\"\n    volumeMounts:\n      - name: data\n        mountPath: /opt/nifi/data\n      - name: conf\n        mountPath: /opt/nifi/nifi-current/conf\n```\n\n----------------------------------------\n\nTITLE: Verifying Resources for the New NiFi Node (Console)\nDESCRIPTION: This command uses `kubectl` to list the Pod, ConfigMap, and PersistentVolumeClaim resources associated with the newly added NiFi node (nodeId=25). The output confirms that the necessary Kubernetes resources have been successfully created for the new node.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/3_tasks/1_nifi_cluster/2_cluster_scaling.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nkubectl get pods,configmap,pvc -l nodeId=25\nNAME                          READY   STATUS    RESTARTS   AGE\npod/simplenifi-25-nodem5jh4   1/1     Running   0          11m\n\nNAME                             DATA   AGE\nconfigmap/simplenifi-config-25   7      11m\n\nNAME                                               STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\npersistentvolumeclaim/simplenifi-25-storagehwn24   Bound    pvc-7da86076-728e-11ea-846d-42010a8400f2   10Gi       RWO            standard       11m\n```\n\n----------------------------------------\n\nTITLE: Creating GKE Cluster (Shell)\nDESCRIPTION: This shell command creates a new GKE cluster with the specified configuration. It uses the `gcloud container clusters create` command with parameters to define cluster characteristics.  `--cluster-version latest` sets the Kubernetes version to the latest available, `--machine-type=n1-standard-1` defines the machine type, `--num-nodes 4` sets the number of nodes, `--zone` specifies the zone, and `--project` specifies the Google Cloud project. The created cluster will have the name specified by the `$CLUSTER_NAME` environment variable.  Dependencies: gcloud CLI installed and configured with the appropriate project.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/2_setup/2_platform_setup/1_gke.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ngcloud container clusters create $CLUSTER_NAME \\\n  --cluster-version latest \\\n  --machine-type=n1-standard-1 \\\n  --num-nodes 4 \\\n  --zone $GCP_ZONE \\\n  --project $GCP_PROJECT\n```\n\n----------------------------------------\n\nTITLE: Manually Deleting NiFiKop CRDs from Kubernetes\nDESCRIPTION: Kubectl commands to manually clean up Custom Resource Definitions after uninstalling NiFiKop.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete crd nificlusters.nifi.konpyutaika.com\nkubectl delete crd nifiusers.nifi.konpyutaika.com\nkubectl delete crd nifiusergroups.nifi.konpyutaika.com\nkubectl delete crd nifiregistryclients.nifi.konpyutaika.com\nkubectl delete crd nifiparametercontexts.nifi.konpyutaika.com\nkubectl delete crd nifidataflows.nifi.konpyutaika.com\n```\n\n----------------------------------------\n\nTITLE: Updating Helm Repositories (Console)\nDESCRIPTION: Updates the local cache of available charts from all configured Helm repositories. This ensures that Helm has the latest information about chart versions before installation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nhelm repo update\n```\n\n----------------------------------------\n\nTITLE: GracefulActionState Structure in NiFiKop\nDESCRIPTION: Structure that tracks the execution state of graceful operations performed on a NiFi node, including error information, current step, timing, and overall state.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/5_references/1_nifi_cluster/5_node_state.md#_snippet_1\n\nLANGUAGE: go\nCODE:\n```\ntype GracefulActionState struct {\n\terrorMessage string\n\tactionStep ActionStep\n\ttaskStarted string\n\tactionState State\n}\n```\n\n----------------------------------------\n\nTITLE: List Deleted Helm Releases - Bash\nDESCRIPTION: Lists Helm releases that have been previously deleted using the `helm list --deleted` command. This shows the history of removed installations, which Helm retains unless explicitly purged.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/2_setup/3_install/1_customizable_install_with_helm.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --deleted\n```\n\n----------------------------------------\n\nTITLE: Running the NiFiKop Migration Script (Bash)\nDESCRIPTION: Demonstrates how to execute the Node.js migration script using `npm start`. It passes the required `--type` argument (specifying the NiFiKop resource type like 'cluster', 'dataflow', etc.) and the `--namespace` argument (specifying the Kubernetes namespace) to the script via command-line flags.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/7_upgrade/1_v0.7.x_to_v0.8.0.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nnpm start -- --type=<NIFIKOP_RESOURCE> --namespace=<K8S_NAMESPACE>\n```\n\n----------------------------------------\n\nTITLE: Configuring CRD Conversion Webhook in YAML\nDESCRIPTION: Configures a Custom Resource Definition (CRD) to use a conversion webhook for handling different API versions (v1alpha1 to v1). It includes necessary annotations for cert-manager and webhook service client configuration within the CRD `spec`. Requires a running webhook service and cert-manager setup.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n```\n\n----------------------------------------\n\nTITLE: Listing All Helm Releases (Deployed, Deleted, Failed) - Bash\nDESCRIPTION: Returns all Helm releases, including those that are currently deployed, deleted, or failed, in the cluster. Uses the --all flag with Helm CLI to broaden the listing. This facilitates troubleshooting and lifecycle management of releases.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --all\n```\n\n----------------------------------------\n\nTITLE: Packaging NiFiKop Helm Chart\nDESCRIPTION: This command packages the NiFiKop Helm chart. It creates a packaged version of the chart that can be deployed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/6_contributing/1_developer_guide.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nmake helm-package\n```\n\n----------------------------------------\n\nTITLE: Creating Namespace Kubernetes/kubectl Console\nDESCRIPTION: This command creates a dedicated Kubernetes namespace named 'monitoring-system' where the monitoring components, such as Prometheus and its associated resources, will be deployed. Using a separate namespace helps organize resources and manage access control.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_5\n\nLANGUAGE: console\nCODE:\n```\nkubectl create namespace monitoring-system\n```\n\n----------------------------------------\n\nTITLE: Listing Deleted Helm Releases\nDESCRIPTION: Uses the `helm list --deleted` command to display Helm releases that have been previously uninstalled but whose records are still retained by Helm's history.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --deleted\n```\n\n----------------------------------------\n\nTITLE: Replacing Authorizer Configuration Template (Markdown)\nDESCRIPTION: Specifies options for replacing the default authorizers configuration template (authorizers.xml). Allows replacement using an external Kubernetes ConfigMap (`replaceTemplateConfigMap`) or a Kubernetes Secret (`replaceTemplateSecretConfig`), with the Secret taking precedence over the ConfigMap.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/5_references/1_nifi_cluster/2_read_only_config.md#_snippet_4\n\nLANGUAGE: Markdown\nCODE:\n```\n## AuthorizerConfig\n\n|Field|Type|Description|Required|Default|\n|-----|----|-----------|--------|--------|\n|replaceTemplateConfigMap|[ConfigmapReference](#configmapreference)|authorizers.xml configuration template that will replace the default template.|No|nil|\n|replaceTemplateSecretConfig|[SecretConfigReference](#secretconfigreference)|authorizers.xml configuration that will replace the default template and the replaceTemplateConfigMap.|No|nil|\n```\n\n----------------------------------------\n\nTITLE: Apply NiFiCluster Configuration - Shell\nDESCRIPTION: This shell command applies the new NifiCluster configuration to the Kubernetes cluster using `kubectl`. It assumes that the `simplenificluster.yaml` file is located in the `config/samples/` directory and that the `nifi` namespace exists. It updates the NiFi cluster based on the provided YAML definition.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/3_tasks/1_nifi_cluster/2_cluster_scaling.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nkubectl -n nifi apply -f config/samples/simplenificluster.yaml\n```\n\n----------------------------------------\n\nTITLE: Setting up Node.js Project Dependencies\nDESCRIPTION: Initializes a new Node.js project with default settings and installs required packages for interacting with the Kubernetes API (`@kubernetes/client-node`) and parsing command-line arguments (`minimist`). These dependencies are essential for the migration script.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm init -y\nnpm install @kubernetes/client-node@0.16.3 minimist@1.2.6\n```\n\n----------------------------------------\n\nTITLE: Kubernetes Custom Resource Migration Script (JavaScript)\nDESCRIPTION: A Node.js script using the Kubernetes client library to migrate NiFiKop custom resources. It connects to the K8s API, lists resources from the source API group/version (`nifi.orange.com/v1alpha1`), creates new resources in the destination group/version (`nifi.konpyutaika.com/v1alpha1`) copying metadata (name, annotations, labels) and spec, and then updates the status of the newly created resource by copying the status from the old one. It handles different NiFiKop resource types based on command-line arguments.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\nprocess.env['NODE_TLS_REJECT_UNAUTHORIZED'] = 0;\nconst k8s = require('@kubernetes/client-node');\n\nconst kc = new k8s.KubeConfig();\nkc.loadFromDefault();\n\nconst k8sApi = kc.makeApiClient(k8s.CustomObjectsApi);\n\nconst KONPYUTAIKA_GROUP = 'nifi.konpyutaika.com';\nconst KONPYUTAIKA_GROUP_VERSION = 'v1alpha1';\nconst ORANGE_GROUP = 'nifi.orange.com';\nconst ORANGE_GROUP_VERSION = 'v1alpha1';\n\nconst call = async (SRC_GRP, SRC_GRP_VER, DST_GRP, DST_GRP_VER, KIND_PLURAL, NAMESPACE) => {\n\tconsole.log(`Listing ${KIND_PLURAL} of ${SRC_GRP}/${SRC_GRP_VER} in ${NAMESPACE}...`);\n\tconst listResources = (await k8sApi.listNamespacedCustomObject(SRC_GRP, SRC_GRP_VER, NAMESPACE, KIND_PLURAL)).body.items;\n\treturn Promise.all(listResources.map(async (resource) => {\n\t\ttry {\n\t\t\tconsole.log(`Found ${resource.kind} \"${resource.metadata.name}\" of ${resource.apiVersion} in ${NAMESPACE}`);\n\n\t\t\tif (resource.metadata.ownerReferences) {\n\t\t\t\tconsole.log(`${resource.kind} ${resource.metadata.name} mananged by something else (ownerRefereces is set).`);\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tconst bodyResource = {\n\t\t\t\tapiVersion: `${DST_GRP}/${DST_GRP_VER}`,\n\t\t\t\tkind: resource.kind,\n\t\t\t\tmetadata: {\n\t\t\t\t\tname: resource.metadata.name,\n\t\t\t\t\tannotations: resource.metadata.annotations,\n\t\t\t\t\tlabels: resource.metadata.labels\n\t\t\t\t},\n\t\t\t\tspec: resource.spec\n\t\t\t};\n\n\t\t\tconsole.log(`Creating ${bodyResource.kind} \"${bodyResource.metadata.name}\" of ${bodyResource.apiVersion} in ${NAMESPACE}...`);\n\t\t\tconst newResource = (await k8sApi.createNamespacedCustomObject(DST_GRP, DST_GRP_VER, NAMESPACE, KIND_PLURAL, bodyResource)).body;\n\t\t\tconsole.log('...done creating.');\n\n\t\t\tconst bodyStatus = {\n\t\t\t\tapiVersion: newResource.apiVersion,\n\t\t\t\tkind: newResource.kind,\n\t\t\t\tmetadata: {\n\t\t\t\t\tname: newResource.metadata.name,\n\t\t\t\t\tresourceVersion: newResource.metadata.resourceVersion\n\t\t\t\t},\n\t\t\t\tstatus: resource.status\n\t\t\t};\n\n\t\t\tconsole.log(`Copying status from ${resource.kind} \"${resource.metadata.name}\" of ${newResource.apiVersion} to ${newResource.kind} \"${newResource.metadata.name}\" of ${newResource.apiVersion} in ${NAMESPACE}...`);\n\t\t\tconst newResourceWithStatus = (await k8sApi.replaceNamespacedCustomObjectStatus(DST_GRP, DST_GRP_VER, NAMESPACE, KIND_PLURAL, bodyStatus.metadata.name, bodyStatus)).body;\n\t\t\tconsole.log('...done copying.');\n\t\t\treturn newResourceWithStatus;\n\t\t}\n\t\tcatch (e) {\n\t\t\tconsole.error(e.body ? e.body.message ? e.body.message : e.body : e);\n\t\t}\n\t}));\n};\n\nconst argv = require('minimist')(process.argv.slice(2));\n\nlet NAMESPACE = argv.namespace ? argv.namespace.length > 0 ? argv.namespace : 'default' : 'default';\nlet KIND_PLURAL = {\n\tcluster: 'nificlusters',\n\tdataflow: 'nifidataflows',\n\tparametercontext: 'nifiparametercontexts',\n\tregistryclient: 'nifiregistryclients',\n\tuser: 'nifiusers',\n\tusergroup: 'nifiusergroups',\n};\n\nif (!argv.type) {\n\tconsole.error('Type not provided');\n\tprocess.exit(1);\n}\n\nif (!KIND_PLURAL[argv.type]) {\n\tconsole.error(`Type ${argv.type} is not one of the following types: ${Object.keys(KIND_PLURAL)}`);\n\tprocess.exit(1);\n}\n\nconsole.log(`########### START: ${KIND_PLURAL[argv.type]} ###########`);\ncall( ORANGE_GROUP, ORANGE_GROUP_VERSION, KONPYUTAIKA_GROUP, KONPYUTAIKA_GROUP_VERSION, KIND_PLURAL[argv.type], NAMESPACE)\n\t.then(r => console.log('############ END ############'))\n\t.catch(e => console.error(e));\n```\n\n----------------------------------------\n\nTITLE: Cloning NiFiKop Project Repository with Git - Bash\nDESCRIPTION: This snippet clones the NiFiKop GitHub repository and changes into the project directory. It requires having Git installed on the system. The commands are intended to set up the local development environment by retrieving the source code.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/6_contributing/1_developer_guide.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/konpyutaika/nifikop.git\ncd nifikop\n```\n\n----------------------------------------\n\nTITLE: Updating Helm Repository\nDESCRIPTION: Updates the Helm repositories to ensure the latest chart versions are available.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_7\n\nLANGUAGE: console\nCODE:\n```\nhelm repo update\n```\n\n----------------------------------------\n\nTITLE: Running the NiFiKop Operator Locally via Makefile using Bash\nDESCRIPTION: Executes the `run` target from the project's Makefile. This command builds (if needed) and runs the operator Go binary locally, connecting to the Kubernetes cluster specified in the default kubeconfig (`$HOME/.kube/config`) and watching the `default` namespace unless overridden by environment variables.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/6_contributing/1_developer_guide.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmake run\n```\n\n----------------------------------------\n\nTITLE: Defining NifiCluster Resource with Busybox InitContainer (YAML)\nDESCRIPTION: This YAML snippet defines a NifiCluster Kubernetes resource where the initContainerImage is explicitly set to use the 'busybox' image (version 1.34.0). This configuration was valid in nifikop v0.14.1 but is incompatible with v0.15.0, as the new version requires an init container with a bash shell. Users must update the image to maintain compatibility. No additional parameters are required beyond standard Kubernetes and nifikop operator prerequisites.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/7_upgrade_guides/2_v0.14.1_to_v0.15.0.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  initContainerImage:\n    repository: busybox\n    tag: \"1.34.0\"\n```\n\n----------------------------------------\n\nTITLE: Running NiFiKop Operator Locally via Makefile using Bash\nDESCRIPTION: Executes the `run` target in the Makefile. This command starts the NiFiKop operator process locally, connecting to the Kubernetes cluster specified in the default kubeconfig file (`$HOME/.kube/config`) and watching the `default` namespace. Assumes CRDs have been applied and `OPERATOR_NAME` is set.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/6_contributing/1_developer_guide.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmake run\n```\n\n----------------------------------------\n\nTITLE: Sample package.json for NiFiKop CRD Migration Project - JSON\nDESCRIPTION: This snippet illustrates a complete sample package.json configured for the migration project. It includes necessary dependencies (@kubernetes/client-node and minimist), metadata, and defines scripts for starting the migration or running a placeholder test. This structure ensures reproducibility and dependency management for the migration script to function correctly.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/7_upgrade/1_v0.7.x_to_v0.8.0.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"nifikop_crd_migration\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Script to migrate from the old CRDs to the new CRDs.\",\n  \"main\": \"index.js\",\n  \"scripts\": {\n    \"start\": \"node --no-warnings index.js\",\n    \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\"\n  },\n  \"keywords\": [\n    \"K8S\",\n    \"NiFiKop\",\n    \"CRDs\"\n  ],\n  \"license\": \"ISC\",\n  \"dependencies\": {\n    \"@kubernetes/client-node\": \"^0.16.3\",\n    \"minimist\": \"^1.2.6\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Retrieving GKE Cluster Credentials for kubectl Access\nDESCRIPTION: This snippet retrieves and configures credentials for the created GKE cluster using Google Cloud SDK, enabling kubectl commands to interact with the cluster. It is essential for managing cluster resources and deploying applications.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/2_setup/2_platform_setup/1_gke.md#_snippet_2\n\nLANGUAGE: Shell script\nCODE:\n```\ncloud container clusters get-credentials $CLUSTER_NAME \\\n    --zone $GCP_ZONE \\\n    --project $GCP_PROJECT\n```\n\n----------------------------------------\n\nTITLE: Running Operator Locally with Go Binary - Bash\nDESCRIPTION: This command starts the NiFiKop operator. It is executed using `make run`. Requires a correctly configured Go environment, dependencies, the CRDs, and a functional Kubernetes cluster. It uses the default Kubernetes config file at `$HOME/.kube/config` to connect to the cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/6_contributing/1_developer_guide.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmake run\n```\n\n----------------------------------------\n\nTITLE: Creating a Branch for Contributions\nDESCRIPTION: This snippet guides the user on creating a new branch for contributions. It checks out the master branch, pulls the latest changes, and then creates a new branch with a specified name. This allows for isolated development of changes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/README.md#_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\n1.  `git checkout master` from any folder in your local `react-native-website` repository.\n1.  `git pull origin master` to ensure you have the latest main code.\n1.  `git checkout -b the-name-of-my-branch` to create a branch.\n    > replace `the-name-of-my-branch` with a suitable name, such as `update-animations-page`\n```\n\n----------------------------------------\n\nTITLE: Adding Bitnami Zookeeper Helm Repository\nDESCRIPTION: Adds the Bitnami Helm chart repository for Zookeeper setup, enabling easy installation via Helm.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/2_setup/1_getting_started.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhelm repo add bitnami https://charts.bitnami.com/bitnami\n```\n\n----------------------------------------\n\nTITLE: Deleting CRDs Manually\nDESCRIPTION: This command deletes Custom Resource Definitions (CRDs) using kubectl. This is necessary to clean up CRDs that were created by a Helm chart when the chart itself is uninstalled, as they are not removed by default.  It will delete *ALL* clusters that used those CRDs.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete crd nificlusters.nifi.konpyutaika.com\nkubectl delete crd nifiusers.nifi.konpyutaika.com\nkubectl delete crd nifiusergroups.nifi.konpyutaika.com\nkubectl delete crd nifiregistryclients.nifi.konpyutaika.com\nkubectl delete crd nifiparametercontexts.nifi.konpyutaika.com\nkubectl delete crd nifidataflows.nifi.konpyutaika.com\n```\n\n----------------------------------------\n\nTITLE: Listing Kubernetes Services with kubectl Console Command\nDESCRIPTION: Uses the `kubectl get services` command to display the Kubernetes services currently deployed in the cluster. The output shows an example `cluster-access` service of type `LoadBalancer`, including its Cluster IP, External IP (if available), and mapped ports (e.g., 443 and 80), confirming the external service configuration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/3_manage_nifi/1_manage_clusters/1_deploy_cluster/3_expose_cluster/1_kubernetes_service.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nkubectl get services\nNAME                TYPE           CLUSTER-IP    EXTERNAL-IP      PORT(S)                                AGE\ncluster-access      LoadBalancer   10.88.21.98   35.180.241.132   443:30421/TCP,80:30521/TCP             20d\n```\n\n----------------------------------------\n\nTITLE: Running NiFiKop Operator Locally Using Make in Bash\nDESCRIPTION: This command executes the NiFiKop operator binary in the default namespace using the current Kubernetes configuration. Prerequisites include a successful prior build and configured environment variables. The operator connects to the cluster referenced in KUBECONFIG and starts its reconciliation loops.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/6_contributing/1_developer_guide.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmake run\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFiKop CRDs for Conversion Webhook (YAML)\nDESCRIPTION: YAML snippet illustrating the necessary annotations and spec configuration within a NiFiKop CustomResourceDefinition (CRD) file to enable the conversion webhook. This allows handling resource conversions between API versions (e.g., v1alpha1 to v1) and requires cert-manager for certificate injection.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/2_deploy_nifikop/1_quick_start.md#_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n```\n\n----------------------------------------\n\nTITLE: Cluster State Constants for NifiCluster Lifecycle\nDESCRIPTION: Provides a set of string constants representing different states of the NifiCluster lifecycle, such as initializing, initialized, reconciling, rolling upgrade, and running. These constants are used for managing and monitoring cluster status but do not include code logic.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/5_references/1_nifi_cluster/1_nifi_cluster.md#_snippet_4\n\n\n\n----------------------------------------\n\nTITLE: Configuring npm Start Script for NiFiKop Migration Project - JSON\nDESCRIPTION: Provides the configuration snippet to add a start script in package.json which runs the Node.js script (index.js) with the flag --no-warnings to suppress warnings. This allows users to execute the migration script easily via npm start.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"start\": \"node --no-warnings index.js\"\n```\n\n----------------------------------------\n\nTITLE: Deleting Installed Nifikop Custom Resource Definitions (CRDs) - Bash\nDESCRIPTION: This sequence of bash commands manually deletes the custom resource definitions associated with Nifikop from the Kubernetes cluster. These CRDs include definitions for clusters, users, user groups, registry clients, parameter contexts, and dataflows. Prerequisites: 'kubectl' must be configured to access your cluster; replacing or omitting a CRD can result in the loss of all corresponding resources. Use extreme caution as deleting CRDs will remove all custom resources (e.g., Nifi clusters) using these definitions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete crd nificlusters.nifi.konpyutaika.com\nkubectl delete crd nifiusers.nifi.konpyutaika.com\nkubectl delete crd nifiusergroups.nifi.konpyutaika.com\nkubectl delete crd nifiregistryclients.nifi.konpyutaika.com\nkubectl delete crd nifiparametercontexts.nifi.konpyutaika.com\nkubectl delete crd nifidataflows.nifi.konpyutaika.com\n```\n\n----------------------------------------\n\nTITLE: Example package.json for NiFiKop Migration Project (JSON)\nDESCRIPTION: Shows a complete example `package.json` file after initialization and dependency installation. It includes project metadata (name, version, description), the configured `start` script, keywords, license information, and lists the required dependencies (`@kubernetes/client-node`, `minimist`).\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"nifikop_crd_migration\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Script to migrate from the old CRDs to the new CRDs.\",\n  \"main\": \"index.js\",\n  \"scripts\": {\n    \"start\": \"node --no-warnings index.js\",\n    \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\"\n  },\n  \"keywords\": [\n    \"K8S\",\n    \"NiFiKop\",\n    \"CRDs\"\n  ],\n  \"license\": \"ISC\",\n  \"dependencies\": {\n    \"@kubernetes/client-node\": \"^0.16.3\",\n    \"minimist\": \"^1.2.6\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Running NiFiKop Operator with Make Bash\nDESCRIPTION: This snippet executes the operator in development mode via Make. Assumes CRDs are registered and environment variables are set. It runs the operator locally using the kubeconfig credentials (usually targeting the 'default' namespace). Requires GNU Make and a valid Kubernetes context.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/6_contributing/1_developer_guide.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmake run\n```\n\n----------------------------------------\n\nTITLE: AuthorizerConfig Configuration Structure in Markdown\nDESCRIPTION: This table outlines how to provide custom authorizers.xml configuration templates in NiFiKop using ConfigMap or Secret references to replace the default template.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/5_references/1_nifi_cluster/2_read_only_config.md#_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n## AuthorizerConfig\n\n|Field|Type|Description|Required|Default|\n|-----|----|-----------|--------|--------|\n|replaceTemplateConfigMap|[ConfigmapReference](#configmapreference)|authorizers.xml configuration template that will replace the default template.|No|nil|\n|replaceTemplateSecretConfig|[SecretConfigReference](#secretconfigreference)|authorizers.xml configuration that will replace the default template and the replaceTemplateConfigMap.|No|nil|\n```\n\n----------------------------------------\n\nTITLE: Running the NiFiKop Migration Script with Type and Namespace Parameters\nDESCRIPTION: Command to execute the migration script with type (NiFiKop resource type) and namespace parameters for targeted migration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/7_upgrade/1_v0.7.x_to_v0.8.0.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nnpm start -- --type=<NIFIKOP_RESOURCE> --namespace=<K8S_NAMESPACE>\n```\n\n----------------------------------------\n\nTITLE: Setting Operator Name Environment Variable - Bash\nDESCRIPTION: Sets the OPERATOR_NAME environment variable to `nifi-operator`. This is used to identify the operator within the system. Requires no dependencies, just an accessible shell environment. This is important when running the operator to ensure it is correctly identified.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/6_contributing/1_developer_guide.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport OPERATOR_NAME=nifi-operator\n```\n\n----------------------------------------\n\nTITLE: Building and Tagging a Docker Image for NiFiKop with Make and Exported Environment Variable in Bash\nDESCRIPTION: Sets the base Docker repository and builds a Docker image for NiFiKop from the current branch. Requires Docker, GNU Make, and access to the specified Docker repository. Inputs are the repository base and current branch context; output is a tagged Docker image ready for push.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/6_contributing/1_developer_guide.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nexport DOCKER_REPO_BASE={your-docker-repo}\nmake docker-build\n```\n\n----------------------------------------\n\nTITLE: Building Local Docker Image for NiFiKop - Bash\nDESCRIPTION: This set of commands exports the Docker registry base as an environment variable and then builds a Docker image for the NiFiKop operator using the Makefile. Docker must be installed and running, and environment variables should point to your preferred image repository.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/6_contributing/1_developer_guide.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nexport DOCKER_REGISTRY_BASE={your-docker-repo}\nmake docker-build\n```\n\n----------------------------------------\n\nTITLE: Listing Deleted Helm Releases\nDESCRIPTION: Command to view a list of deleted Helm releases in the Kubernetes cluster. Useful for verifying that a release has been properly removed or for potentially rolling back a deletion.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --deleted\n```\n\n----------------------------------------\n\nTITLE: Packaging NiFiKop Helm Chart Using Make - Bash\nDESCRIPTION: This command packages the Helm chart for the NiFiKop operator by invoking the 'make helm-package' target. The packaged chart can then be distributed or deployed to package repositories and Kubernetes clusters.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/6_contributing/1_developer_guide.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nmake helm-package\n```\n\n----------------------------------------\n\nTITLE: Get NiFiKop Helm Release Status - Bash\nDESCRIPTION: Retrieves and displays the detailed status of a specific Helm release (e.g., `nifikop`) using the `helm status` command. This includes information about deployed Kubernetes resources, hooks, and notes provided by the chart, useful for checking the operator's state.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/2_setup/3_install/1_customizable_install_with_helm.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nhelm status nifikop\n```\n\n----------------------------------------\n\nTITLE: Configuring NifiCluster with Bash Init Container Image in YAML\nDESCRIPTION: This YAML snippet shows how to define a NifiCluster custom resource for NifiKop >= v0.15.0, with the Zookeeper init container image set to 'bash' version '5.2.2'. It demonstrates the required configuration change to ensure the init container supports bash, as mandated by recent updates. The YAML structure specifies all necessary Kubernetes resource fields including API version, kind, metadata, and the properly updated initContainerImage.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/7_upgrade_guides/2_v0.14.1_to_v0.15.0.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  initContainerImage:\n    repository: bash\n    tag: \"5.2.2\"\n```\n\n----------------------------------------\n\nTITLE: Building NiFiKop Operator Binary Locally (Bash)\nDESCRIPTION: Executes the 'make build' command to compile the NiFiKop operator's Go source code into an executable binary for local development. This process leverages the project's Makefile to manage the build process. It requires a Go development environment and the 'make' utility installed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/6_contributing/1_developer_guide.md#_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\nmake build\n```\n\n----------------------------------------\n\nTITLE: Running NiFiKop Operator Using Make - Bash\nDESCRIPTION: This snippet runs the NiFiKop operator using the 'make run' target. This command is typically used after configuration and CRD installation, launching the operator in the default namespace with the default Kubernetes configuration. Dependencies include Make, Go, and an appropriately configured kubeconfig file.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/6_contributing/1_developer_guide.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmake run\n```\n\n----------------------------------------\n\nTITLE: Adding Prometheus Helm Repository\nDESCRIPTION: Adds the Prometheus community Helm chart repository to the local Helm configuration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_5\n\nLANGUAGE: console\nCODE:\n```\nhelm repo add prometheus https://prometheus-community.github.io/helm-charts\n```\n\n----------------------------------------\n\nTITLE: ExternalServiceSpec Schema Definition\nDESCRIPTION: This schema specifies the configuration options for the service's behavior, including port configurations, IP settings, service type, external IPs, load balancer details, and traffic policies. It enables comprehensive customization of the external service's network exposure and routing policies.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/5_references/1_nifi_cluster/7_external_service_config.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n|Field|Type|Description|Required|Default|\n|-----|----|-----------|--------|--------|\n|portConfigs||[ ; ][PortConfig](#portconfig)|list of ports and listeners| Yes | - |\n|clusterIP|string|Virtual IP for the service, if applicable| No | - |\n|type|[ServiceType](https://godoc.org/k8s.io/api/core/v1#ServiceType)|Service exposure method; defaults to ClusterIP| No | - |\n|externalIPs|[ ; ]string|Additional IPs accepting traffic, outside Kubernetes management| No | - |\n|loadBalancerIP|string|Specific IP for LoadBalancer type services| No | - |\n|loadBalancerSourceRanges|[ ; ]string|Restricts traffic to specified source ranges for cloud load balancer| No | - |\n|externalName|string|CNAME record for external reference; no proxying involved| No | - |\n|loadBalancerClass|string|Class of the load balancer, defining implementation details| No | - |\n|externalTrafficPolicy|string|Traffic routing policy for external traffic; documentation link provided| No | - |\n|internalTrafficPolicy|string|Traffic policy internal to cluster; documentation link provided| No | - |\n```\n\n----------------------------------------\n\nTITLE: Adding Prometheus Helm Repository - console\nDESCRIPTION: This command adds the prometheus-community Helm chart repository to your Helm configuration, enabling access to Prometheus-related charts. Requires Helm to be installed and initialized. Takes no parameters and outputs repository addition confirmation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_6\n\nLANGUAGE: console\nCODE:\n```\nhelm repo add prometheus https://prometheus-community.github.io/helm-charts\n\n```\n\n----------------------------------------\n\nTITLE: Packaging Helm Chart - Bash\nDESCRIPTION: Packages the Helm chart for the NiFiKop operator, creating a `.tgz` file. It uses `make helm-package`. Requires the Helm CLI tool and the project's Helm chart directory structure to be present. This produces a chart archive that can be deployed later.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/6_contributing/1_developer_guide.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nmake helm-package\n```\n\n----------------------------------------\n\nTITLE: Listing All Helm Releases (Including Failed and Deleted)\nDESCRIPTION: Command to list all Helm releases regardless of their current state, including failed, active, and deleted releases, providing comprehensive release management.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --all\n```\n\n----------------------------------------\n\nTITLE: Pushing a Docker Image for NiFiKop to a Repository using Make in Bash\nDESCRIPTION: Pushes the Docker image built locally to the specified Docker repository. Requires Docker login credentials and correct repository configuration. No input except prior built image; output is a remote image available for Helm installation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/6_contributing/1_developer_guide.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nmake docker-push\n```\n\n----------------------------------------\n\nTITLE: Cloning and Setting Up NiFiKop Repository\nDESCRIPTION: Initial setup commands to clone the NiFiKop repository and navigate to the project directory.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/6_contributing/1_developer_guide.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/konpyutaika/nifikop.git\ncd nifikop\n```\n\n----------------------------------------\n\nTITLE: Executing Migration Script with npm (Bash)\nDESCRIPTION: Provides the command to run the Node.js migration script using `npm start`. It includes examples of command-line arguments: `--type` specifies the NiFiKop resource type to migrate (e.g., `cluster`, `dataflow`), and `--namespace` specifies the Kubernetes namespace where the resources reside. These arguments are parsed by the script to target specific resources and namespaces.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_4\n\nLANGUAGE: Bash\nCODE:\n```\nnpm start -- --type=<NIFIKOP_RESOURCE> --namespace=<K8S_NAMESPACE>\n```\n\n----------------------------------------\n\nTITLE: NiFiKop CRD Migration Script in JavaScript\nDESCRIPTION: Node.js script that migrates NiFiKop resources from the old CRDs (nifi.orange.com) to the new CRDs (nifi.konpyutaika.com) while preserving metadata, spec, and status.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nprocess.env['NODE_TLS_REJECT_UNAUTHORIZED'] = 0;\nconst k8s = require('@kubernetes/client-node');\n\nconst kc = new k8s.KubeConfig();\nkc.loadFromDefault();\n\nconst k8sApi = kc.makeApiClient(k8s.CustomObjectsApi);\n\nconst KONPYUTAIKA_GROUP = 'nifi.konpyutaika.com';\nconst KONPYUTAIKA_GROUP_VERSION = 'v1alpha1';\nconst ORANGE_GROUP = 'nifi.orange.com';\nconst ORANGE_GROUP_VERSION = 'v1alpha1';\n\nconst call = async (SRC_GRP, SRC_GRP_VER, DST_GRP, DST_GRP_VER, KIND_PLURAL, NAMESPACE) => {\n\tconsole.log(`Listing ${KIND_PLURAL} of ${SRC_GRP}/${SRC_GRP_VER} in ${NAMESPACE}...`);\n\tconst listResources = (await k8sApi.listNamespacedCustomObject(SRC_GRP, SRC_GRP_VER, NAMESPACE, KIND_PLURAL)).body.items;\n\treturn Promise.all(listResources.map(async (resource) => {\n\t\ttry {\n\t\t\tconsole.log(`Found ${resource.kind} \"${resource.metadata.name}\" of ${resource.apiVersion} in ${NAMESPACE}`);\n\n\t\t\tif (resource.metadata.ownerReferences) {\n\t\t\t\tconsole.log(`${resource.kind} ${resource.metadata.name} mananged by something else (ownerRefereces is set).`);\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tconst bodyResource = {\n\t\t\t\tapiVersion: `${DST_GRP}/${DST_GRP_VER}`,\n\t\t\t\tkind: resource.kind,\n\t\t\t\tmetadata: {\n\t\t\t\t\tname: resource.metadata.name,\n\t\t\t\t\tannotations: resource.metadata.annotations,\n\t\t\t\t\tlabels: resource.metadata.labels\n\t\t\t\t},\n\t\t\t\tspec: resource.spec\n\t\t\t};\n\n\t\t\tconsole.log(`Creating ${bodyResource.kind} \"${bodyResource.metadata.name}\" of ${bodyResource.apiVersion} in ${NAMESPACE}...`);\n\t\t\tconst newResource = (await k8sApi.createNamespacedCustomObject(DST_GRP, DST_GRP_VER, NAMESPACE, KIND_PLURAL, bodyResource)).body;\n\t\t\tconsole.log('...done creating.');\n\n\t\t\tconst bodyStatus = {\n\t\t\t\tapiVersion: newResource.apiVersion,\n\t\t\t\tkind: newResource.kind,\n\t\t\t\tmetadata: {\n\t\t\t\t\tname: newResource.metadata.name,\n\t\t\t\t\tresourceVersion: newResource.metadata.resourceVersion\n\t\t\t\t},\n\t\t\t\tstatus: resource.status\n\t\t\t};\n\n\t\t\tconsole.log(`Copying status from ${resource.kind} \"${resource.metadata.name}\" of ${newResource.apiVersion} to ${newResource.kind} \"${newResource.metadata.name}\" of ${newResource.apiVersion} in ${NAMESPACE}...`);\n\t\t\tconst newResourceWithStatus = (await k8sApi.replaceNamespacedCustomObjectStatus(DST_GRP, DST_GRP_VER, NAMESPACE, KIND_PLURAL, bodyStatus.metadata.name, bodyStatus)).body;\n\t\t\tconsole.log('...done copying.');\n\t\t\treturn newResourceWithStatus;\n\t\t}\n\t\tcatch (e) {\n\t\t\tconsole.error(e.body ? e.body.message ? e.body.message : e.body : e);\n\t\t}\n\t}));\n};\n\nconst argv = require('minimist')(process.argv.slice(2));\n\nlet NAMESPACE = argv.namespace ? argv.namespace.length > 0 ? argv.namespace : 'default' : 'default';\nlet KIND_PLURAL = {\n\tcluster: 'nificlusters',\n\tdataflow: 'nifidataflows',\n\tparametercontext: 'nifiparametercontexts',\n\tregistryclient: 'nifiregistryclients',\n\tuser: 'nifiusers',\n\tusergroup: 'nifiusergroups',\n};\n\nif (!argv.type) {\n\tconsole.error('Type not provided');\n\tprocess.exit(1);\n}\n\nif (!KIND_PLURAL[argv.type]) {\n\tconsole.error(`Type ${argv.type} is not one of the following types: ${Object.keys(KIND_PLURAL)}`);\n\tprocess.exit(1);\n}\n\nconsole.log(`########### START: ${KIND_PLURAL[argv.type]} ###########`);\ncall( ORANGE_GROUP, ORANGE_GROUP_VERSION, KONPYUTAIKA_GROUP, KONPYUTAIKA_GROUP_VERSION, KIND_PLURAL[argv.type], NAMESPACE)\n\t.then(r => console.log('############ END ############'))\n\t.catch(e => console.error(e));\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Start Script in package.json - json\nDESCRIPTION: Adds a 'start' script to the 'scripts' section of package.json, enabling the project to execute the migration script with 'node --no-warnings index.js'. Ensures the project can be run via 'npm start'. Assumes package.json already exists; otherwise, users should merge this entry. No parameters are required. Output is a properly configured package.json for the migration script.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"start\": \"node --no-warnings index.js\"\n```\n\n----------------------------------------\n\nTITLE: Migrating NiFiKop Operator Kafka Custom Resources via Node.js Kubernetes Client\nDESCRIPTION: A Node.js script leveraging '@kubernetes/client-node' SDK to migrate NiFiKop custom resources from the legacy 'nifi.orange.com/v1alpha1' group to 'nifi.konpyutaika.com/v1alpha1'. The script loads Kubernetes config, lists resources of specified kinds and namespaces, filters out resources managed by other owners, copies relevant resource fields (metadata annotations, labels, name, spec), creates new resources in the destination API group, then copies the resource status. It parses command-line arguments using 'minimist' to specify resource type and namespace. Error handling logs API errors to the console. This script is designed to be run with both CRDs present and the old operator stopped, ensuring seamless migration without service interruption.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nprocess.env['NODE_TLS_REJECT_UNAUTHORIZED'] = 0;\nconst k8s = require('@kubernetes/client-node');\n\nconst kc = new k8s.KubeConfig();\nkc.loadFromDefault();\n\nconst k8sApi = kc.makeApiClient(k8s.CustomObjectsApi);\n\nconst KONPYUTAIKA_GROUP = 'nifi.konpyutaika.com';\nconst KONPYUTAIKA_GROUP_VERSION = 'v1alpha1';\nconst ORANGE_GROUP = 'nifi.orange.com';\nconst ORANGE_GROUP_VERSION = 'v1alpha1';\n\nconst call = async (SRC_GRP, SRC_GRP_VER, DST_GRP, DST_GRP_VER, KIND_PLURAL, NAMESPACE) => {\n\tconsole.log(`Listing ${KIND_PLURAL} of ${SRC_GRP}/${SRC_GRP_VER} in ${NAMESPACE}...`);\n\tconst listResources = (await k8sApi.listNamespacedCustomObject(SRC_GRP, SRC_GRP_VER, NAMESPACE, KIND_PLURAL)).body.items;\n\treturn Promise.all(listResources.map(async (resource) => {\n\t\ttry {\n\t\t\tconsole.log(`Found ${resource.kind} \\\"${resource.metadata.name}\\\" of ${resource.apiVersion} in ${NAMESPACE}`);\n\n\t\t\tif (resource.metadata.ownerReferences) {\n\t\t\t\tconsole.log(`${resource.kind} ${resource.metadata.name} mananged by something else (ownerRefereces is set).`);\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tconst bodyResource = {\n\t\t\t\tapiVersion: `${DST_GRP}/${DST_GRP_VER}`,\n\t\t\t\tkind: resource.kind,\n\t\t\t\tmetadata: {\n\t\t\t\t\tname: resource.metadata.name,\n\t\t\t\t\tannotations: resource.metadata.annotations,\n\t\t\t\t\tlabels: resource.metadata.labels\n\t\t\t\t},\n\t\t\t\tspec: resource.spec\n\t\t\t};\n\n\t\t\tconsole.log(`Creating ${bodyResource.kind} \\\"${bodyResource.metadata.name}\\\" of ${bodyResource.apiVersion} in ${NAMESPACE}...`);\n\t\t\tconst newResource = (await k8sApi.createNamespacedCustomObject(DST_GRP, DST_GRP_VER, NAMESPACE, KIND_PLURAL, bodyResource)).body;\n\t\t\tconsole.log('...done creating.');\n\n\t\t\tconst bodyStatus = {\n\t\t\t\tapiVersion: newResource.apiVersion,\n\t\t\t\tkind: newResource.kind,\n\t\t\t\tmetadata: {\n\t\t\t\t\tname: newResource.metadata.name,\n\t\t\t\t\tresourceVersion: newResource.metadata.resourceVersion\n\t\t\t\t},\n\t\t\t\tstatus: resource.status\n\t\t\t};\n\n\t\t\tconsole.log(`Copying status from ${resource.kind} \\\"${resource.metadata.name}\\\" of ${newResource.apiVersion} to ${newResource.kind} \\\"${newResource.metadata.name}\\\" of ${newResource.apiVersion} in ${NAMESPACE}...`);\n\t\t\tconst newResourceWithStatus = (await k8sApi.replaceNamespacedCustomObjectStatus(DST_GRP, DST_GRP_VER, NAMESPACE, KIND_PLURAL, bodyStatus.metadata.name, bodyStatus)).body;\n\t\t\tconsole.log('...done copying.');\n\t\t\treturn newResourceWithStatus;\n\t\t}\n\t\tcatch (e) {\n\t\t\tconsole.error(e.body ? e.body.message ? e.body.message : e.body : e);\n\t\t}\n\t}));\n};\n\nconst argv = require('minimist')(process.argv.slice(2));\n\nlet NAMESPACE = argv.namespace ? argv.namespace.length > 0 ? argv.namespace : 'default' : 'default';\nlet KIND_PLURAL = {\n\tcluster: 'nificlusters',\n\tdataflow: 'nifidataflows',\n\tparametercontext: 'nifiparametercontexts',\n\tregistryclient: 'nifiregistryclients',\n\tuser: 'nifiusers',\n\tusergroup: 'nifiusergroups',\n};\n\nif (!argv.type) {\n\tconsole.error('Type not provided');\n\tprocess.exit(1);\n}\n\nif (!KIND_PLURAL[argv.type]) {\n\tconsole.error(`Type ${argv.type} is not one of the following types: ${Object.keys(KIND_PLURAL)}`);\n\tprocess.exit(1);\n}\n\nconsole.log(`########### START: ${KIND_PLURAL[argv.type]} ###########`);\ncall( ORANGE_GROUP, ORANGE_GROUP_VERSION, KONPYUTAIKA_GROUP, KONPYUTAIKA_GROUP_VERSION, KIND_PLURAL[argv.type], NAMESPACE)\n\t.then(r => console.log('############ END ############'))\n\t.catch(e => console.error(e));\n```\n\n----------------------------------------\n\nTITLE: Exporting Operator Name Environment Variable (Bash)\nDESCRIPTION: Sets the OPERATOR_NAME environment variable using the 'export' command. This variable provides a name for the operator instance, which can be useful for logging or identification purposes when running the operator locally outside of a Kubernetes cluster. It should be executed in the shell where the operator will be run.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/6_contributing/1_developer_guide.md#_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\nexport OPERATOR_NAME=nifi-operator\n```\n\n----------------------------------------\n\nTITLE: Building NiFiKop - Bash\nDESCRIPTION: This command builds the NiFiKop operator. This uses the `make build` command, assuming the correct Makefile is available in the current directory. The output is a compiled binary ready for execution. Requires a local Go environment configured correctly with dependencies and make installed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/6_contributing/1_developer_guide.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmake build\n```\n\n----------------------------------------\n\nTITLE: Building the NiFiKop Operator Locally using Bash\nDESCRIPTION: Executes the `build` target defined in the project's Makefile. This command compiles the Go source code to create the NiFiKop operator binary in the local development environment. Requires Go v1.17+ to be installed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/6_contributing/1_developer_guide.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmake build\n```\n\n----------------------------------------\n\nTITLE: Executing NiFiKop Migration Script (Bash)\nDESCRIPTION: Demonstrates how to run the Node.js migration script (`index.js`) using `npm start`. It passes the required `--type` (specifying the NiFiKop resource type like 'cluster', 'dataflow', etc.) and `--namespace` (specifying the target Kubernetes namespace) arguments to the script using `minimist` syntax.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nnpm start -- --type=<NIFIKOP_RESOURCE> --namespace=<K8S_NAMESPACE>\n```\n\n----------------------------------------\n\nTITLE: Creating Monitoring Namespace in Kubernetes\nDESCRIPTION: Creates a dedicated Kubernetes namespace for monitoring resources where Prometheus will be deployed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\nkubectl create namespace monitoring-system\n```\n\n----------------------------------------\n\nTITLE: Listing All Helm Releases (Bash)\nDESCRIPTION: This Helm command provides a comprehensive list of all releases known to Helm in the current cluster, regardless of their status. This includes deployed, deleted, failed, and other states. It's useful for getting a complete overview of the release history.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/2_setup/3_install/1_customizable_install_with_helm.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --all\n```\n\n----------------------------------------\n\nTITLE: Listing Deleted Helm Releases\nDESCRIPTION: Lists Helm releases that have been previously deleted but whose history is still recorded by Helm.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nhelm list --deleted\n```\n\n----------------------------------------\n\nTITLE: Listing Deployed Helm Charts (bash)\nDESCRIPTION: Executes the `helm list` command to display all currently deployed Helm releases in the cluster. This provides a quick overview of installed charts, their release names, and status. Requires Helm installed and configured for the target Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nhelm list\n```\n\n----------------------------------------\n\nTITLE: Pushing NiFiKop Docker Image\nDESCRIPTION: Command to push the built NiFiKop Docker image to a repository. This makes the image available for deployment via Helm.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.13.0/6_contributing/1_developer_guide.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nmake docker-push\n```\n\n----------------------------------------\n\nTITLE: Building NiFiKop Locally using Make in Bash\nDESCRIPTION: This command builds the NiFiKop binary using the Makefile provided in the repository. Requires GNU Make and a properly configured Go environment as dependencies. No input parameters; the output is the compiled NiFiKop binary, typically placed in the bin directory. This command should be run from the project root.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/6_contributing/1_developer_guide.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmake build\n```\n\n----------------------------------------\n\nTITLE: Go Struct for NifiRegistryClientSpec\nDESCRIPTION: This Go struct defines the desired state of the NiFi Registry Client, including optional description, mandatory URI, and cluster reference. It is used for specifying configuration parameters during resource creation or updates, ensuring the client connects to the correct registry instance.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/5_references/3_nifi_registry_client.md#_snippet_2\n\nLANGUAGE: Go\nCODE:\n```\ntype NifiRegistryClientSpec struct {\n    Description string `json:\"description,omitempty\"`\n    Uri         string `json:\"uri\"`\n    ClusterRef  ClusterReference `json:\"clusterRef\"`\n}\n```\n\n----------------------------------------\n\nTITLE: Setting up Node.js project dependencies (Bash)\nDESCRIPTION: Initializes a new Node.js project with default settings and installs the required Kubernetes client (`@kubernetes/client-node`) and argument parsing (`minimist`) libraries. This prepares the environment for running the migration script. Requires Node.js and npm to be installed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm init -y\nnpm install @kubernetes/client-node@0.16.3 minimist@1.2.6\n```\n\n----------------------------------------\n\nTITLE: N/A\nDESCRIPTION: No individual code snippets are provided outside of setup instructions and the main migration script.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_0\n\n\n\n----------------------------------------\n\nTITLE: Migrating NiFiKop Custom Resources - JavaScript\nDESCRIPTION: Node.js script to migrate NiFiKop custom resources from the `nifi.orange.com/v1alpha1` API group to `nifi.konpyutaika.com/v1alpha1` in Kubernetes. It connects to the Kubernetes API, lists resources of a specified type and namespace from the source group, creates new resources in the destination group with copied metadata and spec, and then copies the resource status. Requires `@kubernetes/client-node` and `minimist` dependencies, and Kubernetes access configured via default kubeconfig. Takes `--type` (resource type) and optional `--namespace` (namespace) as command-line arguments.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nprocess.env['NODE_TLS_REJECT_UNAUTHORIZED'] = 0;\nconst k8s = require('@kubernetes/client-node');\n\nconst kc = new k8s.KubeConfig();\nkc.loadFromDefault();\n\nconst k8sApi = kc.makeApiClient(k8s.CustomObjectsApi);\n\nconst KONPYUTAIKA_GROUP = 'nifi.konpyutaika.com';\nconst KONPYUTAIKA_GROUP_VERSION = 'v1alpha1';\nconst ORANGE_GROUP = 'nifi.orange.com';\nconst ORANGE_GROUP_VERSION = 'v1alpha1';\n\nconst call = async (SRC_GRP, SRC_GRP_VER, DST_GRP, DST_GRP_VER, KIND_PLURAL, NAMESPACE) => {\n\tconsole.log(`Listing ${KIND_PLURAL} of ${SRC_GRP}/${SRC_GRP_VER} in ${NAMESPACE}...`);\n\tconst listResources = (await k8sApi.listNamespacedCustomObject(SRC_GRP, SRC_GRP_VER, NAMESPACE, KIND_PLURAL)).body.items;\n\treturn Promise.all(listResources.map(async (resource) => {\n\t\ttry {\n\t\t\tconsole.log(`Found ${resource.kind} \"${resource.metadata.name}\" of ${resource.apiVersion} in ${NAMESPACE}`);\n\n\t\t\tif (resource.metadata.ownerReferences) {\n\t\t\t\tconsole.log(`${resource.kind} ${resource.metadata.name} mananged by something else (ownerRefereces is set).`);\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tconst bodyResource = {\n\t\t\t\tapiVersion: `${DST_GRP}/${DST_GRP_VER}`,\n\t\t\t\tkind: resource.kind,\n\t\t\t\tmetadata: {\n\t\t\t\t\tname: resource.metadata.name,\n\t\t\t\t\tannotations: resource.metadata.annotations,\n\t\t\t\t\tlabels: resource.metadata.labels\n\t\t\t\t},\n\t\t\t\tspec: resource.spec\n\t\t\t};\n\n\t\t\tconsole.log(`Creating ${bodyResource.kind} \"${bodyResource.metadata.name}\" of ${bodyResource.apiVersion} in ${NAMESPACE}...`);\n\t\t\tconst newResource = (await k8sApi.createNamespacedCustomObject(DST_GRP, DST_GRP_VER, NAMESPACE, KIND_PLURAL, bodyResource)).body;\n\t\t\tconsole.log('...done creating.');\n\n\t\t\tconst bodyStatus = {\n\t\t\t\tapiVersion: newResource.apiVersion,\n\t\t\t\tkind: newResource.kind,\n\t\t\t\tmetadata: {\n\t\t\t\t\tname: newResource.metadata.name,\n\t\t\t\t\tresourceVersion: newResource.metadata.resourceVersion\n\t\t\t\t},\n\t\t\t\tstatus: resource.status\n\t\t\t};\n\n\t\t\tconsole.log(`Copying status from ${resource.kind} \"${resource.metadata.name}\" of ${newResource.apiVersion} to ${newResource.kind} \"${newResource.metadata.name}\" of ${newResource.apiVersion} in ${NAMESPACE}...`);\n\t\t\tconst newResourceWithStatus = (await k8sApi.replaceNamespacedCustomObjectStatus(DST_GRP, DST_GRP_VER, NAMESPACE, KIND_PLURAL, bodyStatus.metadata.name, bodyStatus)).body;\n\t\t\tconsole.log('...done copying.');\n\t\t\treturn newResourceWithStatus;\n\t\t}\n\t\tcatch (e) {\n\t\t\tconsole.error(e.body ? e.body.message ? e.body.message : e.body : e);\n\t\t}\n\t}));\n};\n\nconst argv = require('minimist')(process.argv.slice(2));\n\nlet NAMESPACE = argv.namespace ? argv.namespace.length > 0 ? argv.namespace : 'default' : 'default';\nlet KIND_PLURAL = {\n\tcluster: 'nificlusters',\n\tdataflow: 'nifidataflows',\n\tparametercontext: 'nifiparametercontexts',\n\tregistryclient: 'nifiregistryclients',\n\tuser: 'nifiusers',\n\tusergroup: 'nifiusergroups'\n};\n\nif (!argv.type) {\n\tconsole.error('Type not provided');\n\tprocess.exit(1);\n}\n\nif (!KIND_PLURAL[argv.type]) {\n\tconsole.error(`Type ${argv.type} is not one of the following types: ${Object.keys(KIND_PLURAL)}`);\n\tprocess.exit(1);\n}\n\nconsole.log(`########### START: ${KIND_PLURAL[argv.type]} ###########`);\ncall( ORANGE_GROUP, ORANGE_GROUP_VERSION, KONPYUTAIKA_GROUP, KONPYUTAIKA_GROUP_VERSION, KIND_PLURAL[argv.type], NAMESPACE)\n\t.then(r => console.log('############ END ############'))\n\t.catch(e => console.error(e));\n```\n\n----------------------------------------\n\nTITLE: Exporting Operator Name Environment Variable For Local Operator Run (Bash)\nDESCRIPTION: This snippet exports an environment variable 'OPERATOR_NAME' that specifies the running operator's name, used when running the operator binary locally outside the Kubernetes cluster. It allows the operator instances to identify themselves uniquely within Kubernetes. Required for local development execution.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/6_contributing/1_developer_guide.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport OPERATOR_NAME=nifi-operator\n```\n\n----------------------------------------\n\nTITLE: Running NiFiKop Operator Locally (Bash)\nDESCRIPTION: Executes the `run` target in the project's Makefile. This starts the NiFiKop operator process locally, connecting to the Kubernetes cluster specified by the default kubeconfig (`$HOME/.kube/config`) and managing resources in the `default` namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/6_contributing/1_developer_guide.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmake run\n```\n\n----------------------------------------\n\nTITLE: Example package.json File Setup for NiFiKop CRD Migration Script in Node.js\nDESCRIPTION: Complete example of a package.json file defining the project metadata, dependencies (including specific versions for '@kubernetes/client-node' and 'minimist'), and scripts for running the migration script and testing placeholder. This file provides the baseline project configuration necessary to run the migration tool.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"nifikop_crd_migration\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Script to migrate from the old CRDs to the new CRDs.\",\n  \"main\": \"index.js\",\n  \"scripts\": {\n    \"start\": \"node --no-warnings index.js\",\n    \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\"\n  },\n  \"keywords\": [\n    \"K8S\",\n    \"NiFiKop\",\n    \"CRDs\"\n  ],\n  \"license\": \"ISC\",\n  \"dependencies\": {\n    \"@kubernetes/client-node\": \"^0.16.3\",\n    \"minimist\": \"^1.2.6\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Local Development\nDESCRIPTION: Configuration of environment variables required for connecting to Kubernetes when running the operator from an IDE.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/6_contributing/1_developer_guide.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nKUBECONFIG={path/to/your/kubeconfig}\nWATCH_NAMESPACE={namespace_to_watch}\nPOD_NAME={name for operator pod}\nLOG_LEVEL=Debug\nOPERATOR_NAME=ide\n```\n\n----------------------------------------\n\nTITLE: Updating NifiCluster InitContainerImage in YAML\nDESCRIPTION: This YAML snippet illustrates how to update the `NifiCluster.Spec.InitContainerImage` from `busybox` to `bash` or an image that contains bash. It is required if the user had overridden the default init container image before.  The configuration includes the repository and tag for the updated image.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/docs/7_upgrade_guides/2_v0.14.1_to_v0.15.0.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n```yaml\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  initContainerImage:\n    repository: bash\n    tag: \"5.2.2\"\n```\n```\n\n----------------------------------------\n\nTITLE: Migrating CRDs with Node.js and Kubernetes Client - javascript\nDESCRIPTION: Implements resource migration between old and new NiFiKop custom resource definitions using Node.js and the '@kubernetes/client-node' library. The script reads both resource types, replicates metadata and spec, and copies status while ensuring no resources controlled by owners are migrated. It relies on valid Kubernetes context, requires '@kubernetes/client-node' and 'minimist', and expects type and namespace parameters via CLI. Inputs include resource type and namespace. Outputs migrated CRDs and logs process details. Limitations: resources with ownerReferences are skipped, and errors are logged but do not halt migration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nprocess.env['NODE_TLS_REJECT_UNAUTHORIZED'] = 0;\nconst k8s = require('@kubernetes/client-node');\n\nconst kc = new k8s.KubeConfig();\nkc.loadFromDefault();\n\nconst k8sApi = kc.makeApiClient(k8s.CustomObjectsApi);\n\nconst KONPYUTAIKA_GROUP = 'nifi.konpyutaika.com';\nconst KONPYUTAIKA_GROUP_VERSION = 'v1alpha1';\nconst ORANGE_GROUP = 'nifi.orange.com';\nconst ORANGE_GROUP_VERSION = 'v1alpha1';\n\nconst call = async (SRC_GRP, SRC_GRP_VER, DST_GRP, DST_GRP_VER, KIND_PLURAL, NAMESPACE) => {\n\tconsole.log(`Listing ${KIND_PLURAL} of ${SRC_GRP}/${SRC_GRP_VER} in ${NAMESPACE}...`);\n\tconst listResources = (await k8sApi.listNamespacedCustomObject(SRC_GRP, SRC_GRP_VER, NAMESPACE, KIND_PLURAL)).body.items;\n\treturn Promise.all(listResources.map(async (resource) => {\n\t\ttry {\n\t\t\tconsole.log(`Found ${resource.kind} \\\"${resource.metadata.name}\\\" of ${resource.apiVersion} in ${NAMESPACE}`);\n\n\t\t\tif (resource.metadata.ownerReferences) {\n\t\t\t\tconsole.log(`${resource.kind} ${resource.metadata.name} mananged by something else (ownerRefereces is set).`);\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tconst bodyResource = {\n\t\t\t\tapiVersion: `${DST_GRP}/${DST_GRP_VER}`,\n\t\t\t\tkind: resource.kind,\n\t\t\t\tmetadata: {\n\t\t\t\t\tname: resource.metadata.name,\n\t\t\t\t\tannotations: resource.metadata.annotations,\n\t\t\t\t\tlabels: resource.metadata.labels\n\t\t\t\t},\n\t\t\t\tspec: resource.spec\n\t\t\t};\n\n\t\t\tconsole.log(`Creating ${bodyResource.kind} \\\"${bodyResource.metadata.name}\\\" of ${bodyResource.apiVersion} in ${NAMESPACE}...`);\n\t\t\tconst newResource = (await k8sApi.createNamespacedCustomObject(DST_GRP, DST_GRP_VER, NAMESPACE, KIND_PLURAL, bodyResource)).body;\n\t\t\tconsole.log('...done creating.');\n\n\t\t\tconst bodyStatus = {\n\t\t\t\tapiVersion: newResource.apiVersion,\n\t\t\t\tkind: newResource.kind,\n\t\t\t\tmetadata: {\n\t\t\t\t\tname: newResource.metadata.name,\n\t\t\t\t\tresourceVersion: newResource.metadata.resourceVersion\n\t\t\t\t},\n\t\t\t\tstatus: resource.status\n\t\t\t};\n\n\t\t\tconsole.log(`Copying status from ${resource.kind} \\\"${resource.metadata.name}\\\" of ${newResource.apiVersion} to ${newResource.kind} \\\"${newResource.metadata.name}\\\" of ${newResource.apiVersion} in ${NAMESPACE}...`);\n\t\t\tconst newResourceWithStatus = (await k8sApi.replaceNamespacedCustomObjectStatus(DST_GRP, DST_GRP_VER, NAMESPACE, KIND_PLURAL, bodyStatus.metadata.name, bodyStatus)).body;\n\t\t\tconsole.log('...done copying.');\n\t\t\treturn newResourceWithStatus;\n\t\t}\n\t\tcatch (e) {\n\t\t\tconsole.error(e.body ? e.body.message ? e.body.message : e.body : e);\n\t\t}\n\t}));\n};\n\nconst argv = require('minimist')(process.argv.slice(2));\n\nlet NAMESPACE = argv.namespace ? argv.namespace.length > 0 ? argv.namespace : 'default' : 'default';\nlet KIND_PLURAL = {\n\tcluster: 'nificlusters',\n\tdataflow: 'nifidataflows',\n\tparametercontext: 'nifiparametercontexts',\n\tregistryclient: 'nifiregistryclients',\n\tuser: 'nifiusers',\n\tusergroup: 'nifiusergroups',\n};\n\nif (!argv.type) {\n\tconsole.error('Type not provided');\n\tprocess.exit(1);\n}\n\nif (!KIND_PLURAL[argv.type]) {\n\tconsole.error(`Type ${argv.type} is not one of the following types: ${Object.keys(KIND_PLURAL)}`);\n\tprocess.exit(1);\n}\n\nconsole.log(`########### START: ${KIND_PLURAL[argv.type]} ###########`);\ncall( ORANGE_GROUP, ORANGE_GROUP_VERSION, KONPYUTAIKA_GROUP, KONPYUTAIKA_GROUP_VERSION, KIND_PLURAL[argv.type], NAMESPACE)\n\t.then(r => console.log('############ END ############'))\n\t.catch(e => console.error(e));\n```\n\n----------------------------------------\n\nTITLE: Manually Deleting NiFiKop CRDs\nDESCRIPTION: Commands to manually delete all the NiFiKop Custom Resource Definitions from the Kubernetes cluster. This should be done with caution as it will also delete all NiFi resources managed by those CRDs.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete crd nificlusters.nifi.konpyutaika.com\nkubectl delete crd nifiusers.nifi.konpyutaika.com\nkubectl delete crd nifiusergroups.nifi.konpyutaika.com\nkubectl delete crd nifiregistryclients.nifi.konpyutaika.com\nkubectl delete crd nifiparametercontexts.nifi.konpyutaika.com\nkubectl delete crd nifidataflows.nifi.konpyutaika.com\n```\n\n----------------------------------------\n\nTITLE: Patching NifiKop CRDs for Webhook Conversion in YAML\nDESCRIPTION: This YAML snippet demonstrates the required patch to be applied to NifiKop CRDs (like NifiCluster, NifiDataflow, etc.) when upgrading from v0.16.0 to v1.0.0. It adds annotations for cert-manager CA injection and configures the CRD's conversion strategy to use a webhook, specifying the service details and supported conversion versions (v1, v1alpha1). Placeholders like `${namespace}`, `${certificate_name}`, and `${webhook_service_name}` must be replaced with values corresponding to the Helm release.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/7_upgrade_guides/3_v0.16.0_to_v1.0.0.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n```\n\n----------------------------------------\n\nTITLE: Exporting Operator Name Environment Variable in Bash\nDESCRIPTION: Exports the operator name to be used by NiFiKop processes, facilitating configuration and logging. Requires a shell that supports export statements. Expects a name as input; output is availability of OPERATOR_NAME variable for child processes. No additional dependencies.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.0/6_contributing/1_developer_guide.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport OPERATOR_NAME=nifi-operator\n```\n\n----------------------------------------\n\nTITLE: Purging NiFiKop Helm Release\nDESCRIPTION: Helm command to completely purge a NiFiKop release, removing all records of the release.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/2_deploy_nifikop/2_customizable_install_with_helm.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nhelm delete --purge nifikop\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Local Operator Run (Bash)\nDESCRIPTION: Defines environment variables necessary for configuring the NiFiKop operator when running locally, allowing it to connect to a specific Kubernetes cluster and watch a designated namespace. Key variables include KUBECONFIG (path to the cluster config), WATCH_NAMESPACE (namespace to monitor), POD_NAME (identifier for the local instance), LOG_LEVEL, and OPERATOR_NAME. These must be set in the shell or IDE environment before running the operator binary.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.12.0/6_contributing/1_developer_guide.md#_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\nKUBECONFIG={path/to/your/kubeconfig}\nWATCH_NAMESPACE={namespace_to_watch}\nPOD_NAME={name for operator pod}\nLOG_LEVEL=Debug\nOPERATOR_NAME=ide\n```\n\n----------------------------------------\n\nTITLE: Updating All Helm Chart Dependencies (Root) - Make Shell\nDESCRIPTION: Executes the `make helm-dep-update` target from the repository root. This command is designed to automate the process of updating dependencies for all Helm charts within the project, typically by iterating through chart directories and running `helm dependency update` for each. It requires `make` and the Helm CLI to be installed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/helm/README.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nmake helm-dep-update\n```\n\n----------------------------------------\n\nTITLE: NifiNodeGroupAutoscalerStatus Schema\nDESCRIPTION: This schema captures the current observed state of the autoscaler, including its operational state, current number of replicas, and label selector used for managing the node group. These fields are set and maintained automatically by the autoscaler system to reflect real-time status.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/5_references/7_nifi_nodegroup_autoscaler.md#_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n{\n  \"state\": \"string\",\n  \"replicas\": \"int\",\n  \"selector\": \"string\"\n}\n```\n\n----------------------------------------\n\nTITLE: Patching Kubernetes CRD for Webhook Conversion (YAML)\nDESCRIPTION: This YAML snippet shows how to patch a Custom Resource Definition (CRD) to enable a webhook conversion strategy. This is necessary during the NifiKop migration to allow the Kubernetes API server to convert resources between older (v1alpha1) and newer (v1) versions using the operator's webhook. It configures annotations for CA injection and defines the webhook service endpoint and supported conversion versions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/7_upgrade_guides/3_v0.16.0_to_v1.0.0.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n```\n\n----------------------------------------\n\nTITLE: Setting Operator Name Environment Variable\nDESCRIPTION: Sets the OPERATOR_NAME environment variable, which is used to identify the operator during deployment and runtime. This is specifically used when running the operator locally with the Go binary.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.0/6_contributing/1_developer_guide.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport OPERATOR_NAME=nifi-operator\n```\n\n----------------------------------------\n\nTITLE: NifiCluster Configuration with bash initContainerImage YAML\nDESCRIPTION: This snippet shows the updated NifiCluster configuration in YAML where the `initContainerImage` is changed to `bash`. This is necessary when upgrading to NifiKop v0.15.0 if the `initContainerImage` was previously overridden. The configuration defines the `apiVersion`, `kind`, `metadata`, and `spec` for the NifiCluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/7_upgrade_guides/2_v0.14.1_to_v0.15.0.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  initContainerImage:\n    repository: bash\n    tag: \"5.2.2\"\n```\n\n----------------------------------------\n\nTITLE: Running NiFiKop Operator Locally - Bash\nDESCRIPTION: Runs the NiFiKop operator using the development Makefile, which starts the operator binary on the local host. It takes no parameters but expects previously built binaries and environment variables set for Kubernetes connectivity. The operator will operate in the 'default' Kubernetes namespace unless otherwise configured.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/6_contributing/1_developer_guide.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmake run\n```\n\n----------------------------------------\n\nTITLE: Adding and Updating Prometheus Helm Repository - console\nDESCRIPTION: These commands add the Prometheus Helm chart repository and update the local Helm repository index. This is necessary to fetch the latest versions of the Prometheus kube-prometheus-stack Helm chart before installation.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_6\n\nLANGUAGE: console\nCODE:\n```\nhelm repo add prometheus https://prometheus-community.github.io/helm-charts\n```\n\nLANGUAGE: console\nCODE:\n```\nhelm repo update\n```\n\n----------------------------------------\n\nTITLE: Initializing Node.js Project and Installing Dependencies\nDESCRIPTION: Initializes a new Node.js project with default settings and installs specific versions of the Kubernetes client library and a minimal argument parser. This setup prepares the environment to run the migration script, and is a required prerequisite step before implementing the migration logic.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.2/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm init -y\nnpm install @kubernetes/client-node@0.16.3 minimist@1.2.6\n```\n\n----------------------------------------\n\nTITLE: Migrating NiFiKop CustomResources via Kubernetes Client in Node.js\nDESCRIPTION: This JavaScript snippet is the core migration script 'index.js' to migrate NiFiKop custom resources from the old CRD group (nifi.orange.com/v1alpha1) to the new CRD group (nifi.konpyutaika.com/v1alpha1). It utilizes '@kubernetes/client-node' to load Kubernetes config and use CustomObjectsApi. The script lists namespaced custom resources of a specified type, filters out resources managed by owners (based on ownerReferences), recreates resources in the new CRD group copying essential metadata and spec, and copies resource status subsequently. It uses 'minimist' for parsing CLI arguments --type (resource kind) and --namespace. Error handling logs API call failures. Expected to run with Node.js environment with requisite dependencies installed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nprocess.env['NODE_TLS_REJECT_UNAUTHORIZED'] = 0;\nconst k8s = require('@kubernetes/client-node');\n\nconst kc = new k8s.KubeConfig();\nkc.loadFromDefault();\n\nconst k8sApi = kc.makeApiClient(k8s.CustomObjectsApi);\n\nconst KONPYUTAIKA_GROUP = 'nifi.konpyutaika.com';\nconst KONPYUTAIKA_GROUP_VERSION = 'v1alpha1';\nconst ORANGE_GROUP = 'nifi.orange.com';\nconst ORANGE_GROUP_VERSION = 'v1alpha1';\n\nconst call = async (SRC_GRP, SRC_GRP_VER, DST_GRP, DST_GRP_VER, KIND_PLURAL, NAMESPACE) => {\n\tconsole.log(`Listing ${KIND_PLURAL} of ${SRC_GRP}/${SRC_GRP_VER} in ${NAMESPACE}...`);\n\tconst listResources = (await k8sApi.listNamespacedCustomObject(SRC_GRP, SRC_GRP_VER, NAMESPACE, KIND_PLURAL)).body.items;\n\treturn Promise.all(listResources.map(async (resource) => {\n\t\ttry {\n\t\t\tconsole.log(`Found ${resource.kind} \\\"${resource.metadata.name}\\\" of ${resource.apiVersion} in ${NAMESPACE}`);\n\n\t\t\tif (resource.metadata.ownerReferences) {\n\t\t\t\tconsole.log(`${resource.kind} ${resource.metadata.name} mananged by something else (ownerRefereces is set).`);\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tconst bodyResource = {\n\t\t\t\tapiVersion: `${DST_GRP}/${DST_GRP_VER}`,\n\t\t\t\tkind: resource.kind,\n\t\t\t\tmetadata: {\n\t\t\t\t\tname: resource.metadata.name,\n\t\t\t\t\tannotations: resource.metadata.annotations,\n\t\t\t\t\tlabels: resource.metadata.labels\n\t\t\t\t},\n\t\t\t\tspec: resource.spec\n\t\t\t};\n\n\t\t\tconsole.log(`Creating ${bodyResource.kind} \\\"${bodyResource.metadata.name}\\\" of ${bodyResource.apiVersion} in ${NAMESPACE}...`);\n\t\t\tconst newResource = (await k8sApi.createNamespacedCustomObject(DST_GRP, DST_GRP_VER, NAMESPACE, KIND_PLURAL, bodyResource)).body;\n\t\t\tconsole.log('...done creating.');\n\n\t\t\tconst bodyStatus = {\n\t\t\t\tapiVersion: newResource.apiVersion,\n\t\t\t\tkind: newResource.kind,\n\t\t\t\tmetadata: {\n\t\t\t\t\tname: newResource.metadata.name,\n\t\t\t\t\tresourceVersion: newResource.metadata.resourceVersion\n\t\t\t\t},\n\t\t\t\tstatus: resource.status\n\t\t\t};\n\n\t\t\tconsole.log(`Copying status from ${resource.kind} \\\"${resource.metadata.name}\\\" of ${newResource.apiVersion} to ${newResource.kind} \\\"${newResource.metadata.name}\\\" of ${newResource.apiVersion} in ${NAMESPACE}...`);\n\t\t\tconst newResourceWithStatus = (await k8sApi.replaceNamespacedCustomObjectStatus(DST_GRP, DST_GRP_VER, NAMESPACE, KIND_PLURAL, bodyStatus.metadata.name, bodyStatus)).body;\n\t\t\tconsole.log('...done copying.');\n\t\t\treturn newResourceWithStatus;\n\t\t}\n\t\tcatch (e) {\n\t\t\tconsole.error(e.body ? e.body.message ? e.body.message : e.body : e);\n\t\t}\n\t}));\n};\n\nconst argv = require('minimist')(process.argv.slice(2));\n\nlet NAMESPACE = argv.namespace ? argv.namespace.length > 0 ? argv.namespace : 'default' : 'default';\nlet KIND_PLURAL = {\n\tcluster: 'nificlusters',\n\tdataflow: 'nifidataflows',\n\tparametercontext: 'nifiparametercontexts',\n\tregistryclient: 'nifiregistryclients',\n\tuser: 'nifiusers',\n\tusergroup: 'nifiusergroups',\n};\n\nif (!argv.type) {\n\tconsole.error('Type not provided');\n\tprocess.exit(1);\n}\n\nif (!KIND_PLURAL[argv.type]) {\n\tconsole.error(`Type ${argv.type} is not one of the following types: ${Object.keys(KIND_PLURAL)}`);\n\tprocess.exit(1);\n}\n\nconsole.log(`########### START: ${KIND_PLURAL[argv.type]} ###########`);\ncall( ORANGE_GROUP, ORANGE_GROUP_VERSION, KONPYUTAIKA_GROUP, KONPYUTAIKA_GROUP_VERSION, KIND_PLURAL[argv.type], NAMESPACE)\n\t.then(r => console.log('############ END ############'))\n\t.catch(e => console.error(e));\n```\n\n----------------------------------------\n\nTITLE: Running NiFiKop Locally with Makefile (bash)\nDESCRIPTION: This command executes the 'run' target defined in the project's Makefile. This target is typically configured to run the locally built operator binary, connecting to the Kubernetes cluster using the default configuration file (`$HOME/.kube/config`) and usually watching the 'default' namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.1/6_contributing/1_developer_guide.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmake run\n```\n\n----------------------------------------\n\nTITLE: Setting up Project Dependencies with npm (Bash)\nDESCRIPTION: Initializes a new Node.js project and installs the required dependencies for interacting with the Kubernetes API (`@kubernetes/client-node`) and parsing command-line arguments (`minimist`). This command prepares the project environment to run the migration script.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\nnpm init -y\nnpm install @kubernetes/client-node@0.16.3 minimist@1.2.6\n```\n\n----------------------------------------\n\nTITLE: Packaging NiFiKop Helm Chart Using Makefile (Bash)\nDESCRIPTION: This snippet packages the NiFiKop Helm chart for release using a Makefile command. It assumes Helm chart structure and semantics in place according to Helm repository best practices. The resulting packaged chart can be pushed to the 'konpyutaika-incubator' Helm repository for distribution.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/6_contributing/1_developer_guide.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nmake helm-package\n```\n\n----------------------------------------\n\nTITLE: Go Struct for ClusterReference\nDESCRIPTION: This struct references the NiFi cluster associated with the registry client, including the cluster's name and namespace. It is used within the spec to establish connections and manage multiple clusters efficiently.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/5_references/3_nifi_registry_client.md#_snippet_4\n\nLANGUAGE: Go\nCODE:\n```\ntype ClusterReference struct {\n    Name      string `json:\"name\"`\n    Namespace string `json:\"namespace\"`\n}\n```\n\n----------------------------------------\n\nTITLE: ConfigurationState Enumeration in NiFiKop\nDESCRIPTION: Enumeration that indicates whether a node's configuration is synchronized with the expected state or requires an update.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/5_references/1_nifi_cluster/5_node_state.md#_snippet_2\n\nLANGUAGE: go\nCODE:\n```\ntype ConfigurationState string\n\nconst (\n\tConfigInSync ConfigurationState = \"ConfigInSync\"\n\tConfigOutOfSync ConfigurationState = \"ConfigOutOfSync\"\n)\n```\n\n----------------------------------------\n\nTITLE: Specifying NifiCluster InitContainerImage Using Busybox YAML\nDESCRIPTION: This YAML snippet shows how a user might have originally configured the NifiCluster Custom Resource Definition (CRD) to specify an init container image using the busybox image with tag 1.34.0. The snippet includes necessary fields such as apiVersion, kind, metadata, and the spec section with initContainerImage configuration. This configuration is used as a prerequisite context to understand the required changes in the upgrade.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/7_upgrade_guides/2_v0.14.1_to_v0.15.0.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  initContainerImage:\n    repository: busybox\n    tag: \"1.34.0\"\n```\n\n----------------------------------------\n\nTITLE: NifiCluster configuration with busybox init container (pre-upgrade)\nDESCRIPTION: Example YAML configuration for a NifiCluster that uses the busybox image as the init container, which needs to be changed during the upgrade process.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/7_upgrade_guides/2_v0.14.1_to_v0.15.0.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  initContainerImage:\n    repository: busybox\n    tag: \"1.34.0\"\n```\n\n----------------------------------------\n\nTITLE: Detecting secret updates in ParameterContext\nDESCRIPTION: This snippet introduces logic to detect updates to secret references within a ParameterContext resource, ensuring sensitive data changes are recognized and synchronized appropriately.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/CHANGELOG.md#_snippet_11\n\nLANGUAGE: Go\nCODE:\n```\nfunc detectSecretUpdate(ctx context.Context, oldParams, newParams *ParameterContext) bool {\n    // Logic to compare secret references\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Local Operator Development - Bash\nDESCRIPTION: Sets environment variables needed to configure the NiFiKop operator during local development. These include the Kubernetes config file path, the Kubernetes namespace to watch, pod name, log verbosity level, and the operator's name. These variables help connect the operator to the local Kubernetes cluster and control its behavior.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/6_contributing/1_developer_guide.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nKUBECONFIG={path/to/your/kubeconfig}\nWATCH_NAMESPACE={namespace_to_watch}\nPOD_NAME={name for operator pod}\nLOG_LEVEL=Debug\nOPERATOR_NAME=ide\n```\n\n----------------------------------------\n\nTITLE: Packaging NiFiKop Helm Chart in Bash\nDESCRIPTION: Command to package the NiFiKop Helm chart for distribution using the Makefile.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/6_contributing/1_developer_guide.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nmake helm-package\n```\n\n----------------------------------------\n\nTITLE: Updating Helm Chart Repositories - console\nDESCRIPTION: This command updates the local list of Helm chart repositories to ensure the latest versions are available. Helm must be installed beforehand. Command takes no parameters and outputs repository update status messages.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/3_manage_nifi/1_manage_clusters/2_cluster_scaling/2_auto_scaling/1_using_keda.md#_snippet_7\n\nLANGUAGE: console\nCODE:\n```\nhelm repo update\n\n```\n\n----------------------------------------\n\nTITLE: Run NiFiKop migration script\nDESCRIPTION: This command executes the NiFiKop CRD migration script (`index.js`) using `npm start`. It passes the resource type (`--type`) and Kubernetes namespace (`--namespace`) as command-line arguments to the script. The script migrates the specified NiFiKop resource type from the old CRDs to the new CRDs within the given namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nnpm start -- --type=<NIFIKOP_RESOURCE> --namespace=<K8S_NAMESPACE>\n```\n\n----------------------------------------\n\nTITLE: Example package.json Configuration (JSON)\nDESCRIPTION: Shows the complete `package.json` file structure after initializing the project, adding the dependencies, and defining the 'start' script. It includes metadata like name, version, description, main entry point, scripts, keywords, license, and the installed dependencies with their versions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_2\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"name\": \"nifikop_crd_migration\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Script to migrate from the old CRDs to the new CRDs.\",\n  \"main\": \"index.js\",\n  \"scripts\": {\n    \"start\": \"node --no-warnings index.js\",\n    \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\"\n  },\n  \"keywords\": [\n    \"K8S\",\n    \"NiFiKop\",\n    \"CRDs\"\n  ],\n  \"license\": \"ISC\",\n  \"dependencies\": {\n    \"@kubernetes/client-node\": \"^0.16.3\",\n    \"minimist\": \"^1.2.6\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Patching NifiKop CRDs for Webhook Conversion - YAML\nDESCRIPTION: This YAML snippet demonstrates how to patch NifiKop CustomResourceDefinitions (CRDs) to set up webhook-based version conversion. It adds an annotation for cert-manager to inject a CA certificate into the CRD and configures the CRD conversion strategy to use a webhook with the specified namespace, certificate name, and service name. Dependencies include cert-manager and a properly configured NifiKop Helm chart with webhooks enabled. Parameters include 'namespace', 'certificate_name', and 'webhook_service_name', all of which must be set to match your deployment. The YAML patch should be applied to each affected NifiKop CRD resource to enable seamless v1alpha1 to v1 migration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/7_upgrade_guides/3_v0.16.0_to_v1.0.0.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n...\nannotations:\n    cert-manager.io/inject-ca-from: ${namespace}/${certificate_name}\n...\nspec:\n  ...\n  conversion:\n    strategy: Webhook\n    webhook:\n      clientConfig:\n        service:\n          namespace: ${namespace}\n          name: ${webhook_service_name}\n          path: /convert\n      conversionReviewVersions:\n        - v1\n        - v1alpha1\n  ...\n\n```\n\n----------------------------------------\n\nTITLE: Migration Script for NiFiKop CRDs in JavaScript\nDESCRIPTION: Node.js script that migrates NiFiKop resources from 'nifi.orange.com/v1alpha1' to 'nifi.konpyutaika.com/v1alpha1'. The script lists resources of the old API version, creates new resources with the updated API version, and copies over the status information.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nprocess.env['NODE_TLS_REJECT_UNAUTHORIZED'] = 0;\nconst k8s = require('@kubernetes/client-node');\n\nconst kc = new k8s.KubeConfig();\nkc.loadFromDefault();\n\nconst k8sApi = kc.makeApiClient(k8s.CustomObjectsApi);\n\nconst KONPYUTAIKA_GROUP = 'nifi.konpyutaika.com';\nconst KONPYUTAIKA_GROUP_VERSION = 'v1alpha1';\nconst ORANGE_GROUP = 'nifi.orange.com';\nconst ORANGE_GROUP_VERSION = 'v1alpha1';\n\nconst call = async (SRC_GRP, SRC_GRP_VER, DST_GRP, DST_GRP_VER, KIND_PLURAL, NAMESPACE) => {\n\tconsole.log(`Listing ${KIND_PLURAL} of ${SRC_GRP}/${SRC_GRP_VER} in ${NAMESPACE}...`);\n\tconst listResources = (await k8sApi.listNamespacedCustomObject(SRC_GRP, SRC_GRP_VER, NAMESPACE, KIND_PLURAL)).body.items;\n\treturn Promise.all(listResources.map(async (resource) => {\n\t\ttry {\n\t\t\tconsole.log(`Found ${resource.kind} \"${resource.metadata.name}\" of ${resource.apiVersion} in ${NAMESPACE}`);\n\n\t\t\tif (resource.metadata.ownerReferences) {\n\t\t\t\tconsole.log(`${resource.kind} ${resource.metadata.name} mananged by something else (ownerRefereces is set).`);\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tconst bodyResource = {\n\t\t\t\tapiVersion: `${DST_GRP}/${DST_GRP_VER}`,\n\t\t\t\tkind: resource.kind,\n\t\t\t\tmetadata: {\n\t\t\t\t\tname: resource.metadata.name,\n\t\t\t\t\tannotations: resource.metadata.annotations,\n\t\t\t\t\tlabels: resource.metadata.labels\n\t\t\t\t},\n\t\t\t\tspec: resource.spec\n\t\t\t};\n\n\t\t\tconsole.log(`Creating ${bodyResource.kind} \"${bodyResource.metadata.name}\" of ${bodyResource.apiVersion} in ${NAMESPACE}...`);\n\t\t\tconst newResource = (await k8sApi.createNamespacedCustomObject(DST_GRP, DST_GRP_VER, NAMESPACE, KIND_PLURAL, bodyResource)).body;\n\t\t\tconsole.log('...done creating.');\n\n\t\t\tconst bodyStatus = {\n\t\t\t\tapiVersion: newResource.apiVersion,\n\t\t\t\tkind: newResource.kind,\n\t\t\t\tmetadata: {\n\t\t\t\t\tname: newResource.metadata.name,\n\t\t\t\t\tresourceVersion: newResource.metadata.resourceVersion\n\t\t\t\t},\n\t\t\t\tstatus: resource.status\n\t\t\t};\n\n\t\t\tconsole.log(`Copying status from ${resource.kind} \"${resource.metadata.name}\" of ${newResource.apiVersion} to ${newResource.kind} \"${newResource.metadata.name}\" of ${newResource.apiVersion} in ${NAMESPACE}...`);\n\t\t\tconst newResourceWithStatus = (await k8sApi.replaceNamespacedCustomObjectStatus(DST_GRP, DST_GRP_VER, NAMESPACE, KIND_PLURAL, bodyStatus.metadata.name, bodyStatus)).body;\n\t\t\tconsole.log('...done copying.');\n\t\t\treturn newResourceWithStatus;\n\t\t}\n\t\tcatch (e) {\n\t\t\tconsole.error(e.body ? e.body.message ? e.body.message : e.body : e);\n\t\t}\n\t}));\n};\n\nconst argv = require('minimist')(process.argv.slice(2));\n\nlet NAMESPACE = argv.namespace ? argv.namespace.length > 0 ? argv.namespace : 'default' : 'default';\nlet KIND_PLURAL = {\n\tcluster: 'nificlusters',\n\tdataflow: 'nifidataflows',\n\tparametercontext: 'nifiparametercontexts',\n\tregistryclient: 'nifiregistryclients',\n\tuser: 'nifiusers',\n\tusergroup: 'nifiusergroups',\n};\n\nif (!argv.type) {\n\tconsole.error('Type not provided');\n\tprocess.exit(1);\n}\n\nif (!KIND_PLURAL[argv.type]) {\n\tconsole.error(`Type ${argv.type} is not one of the following types: ${Object.keys(KIND_PLURAL)}`);\n\tprocess.exit(1);\n}\n\nconsole.log(`########### START: ${KIND_PLURAL[argv.type]} ###########`);\ncall( ORANGE_GROUP, ORANGE_GROUP_VERSION, KONPYUTAIKA_GROUP, KONPYUTAIKA_GROUP_VERSION, KIND_PLURAL[argv.type], NAMESPACE)\n\t.then(r => console.log('############ END ############'))\n\t.catch(e => console.error(e));\n```\n\n----------------------------------------\n\nTITLE: Add start script to package.json\nDESCRIPTION: This JSON snippet defines a script named `start` within the `package.json` file.  It uses `node` to execute `index.js` with the `--no-warnings` flag, which suppresses Node.js warnings during script execution. This script is intended to be used for running the main NiFiKop CRD migration logic.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"start\": \"node --no-warnings index.js\"\n```\n\n----------------------------------------\n\nTITLE: Packaging NiFiKop Helm chart\nDESCRIPTION: Command to package the NiFiKop Helm chart for release.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/6_contributing/1_developer_guide.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nmake helm-package\n```\n\n----------------------------------------\n\nTITLE: Complete package.json configuration for NiFiKop migration tool\nDESCRIPTION: Full package.json configuration for the migration tool, including project metadata, dependencies, and scripts.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.6.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"nifikop_crd_migration\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Script to migrate from the old CRDs to the new CRDs.\",\n  \"main\": \"index.js\",\n  \"scripts\": {\n    \"start\": \"node --no-warnings index.js\",\n    \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\"\n  },\n  \"keywords\": [\n    \"K8S\",\n    \"NiFiKop\",\n    \"CRDs\"\n  ],\n  \"license\": \"ISC\",\n  \"dependencies\": {\n    \"@kubernetes/client-node\": \"^0.16.3\",\n    \"minimist\": \"^1.2.6\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Node.js Project and Installing Dependencies\nDESCRIPTION: Creates a basic Node.js project using `npm init -y` and installs the necessary dependencies: `@kubernetes/client-node` for interacting with the Kubernetes API and `minimist` for parsing command-line arguments.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.7.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm init -y\nnpm install @kubernetes/client-node@0.16.3 minimist@1.2.6\n```\n\n----------------------------------------\n\nTITLE: Running Migration Script npm Bash\nDESCRIPTION: This command executes the Node.js migration script configured in the `package.json` file using `npm start`. It passes command-line arguments `--type` and optionally `--namespace` to specify which NiFiKop resource type to migrate and the Kubernetes namespace where the resources are located. The `--type` argument is mandatory.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nnpm start -- --type=<NIFIKOP_RESOURCE> --namespace=<K8S_NAMESPACE>\n```\n\n----------------------------------------\n\nTITLE: Running the NiFiKop Migration Script with Parameters\nDESCRIPTION: Command to execute the migration script by specifying the NiFiKop resource type and Kubernetes namespace. The script will migrate resources of the given type from old to new CRD format.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nnpm start -- --type=<NIFIKOP_RESOURCE> --namespace=<K8S_NAMESPACE>\n```\n\n----------------------------------------\n\nTITLE: Exporting Operator Name Environment Variable - Bash\nDESCRIPTION: Sets the OPERATOR_NAME environment variable to a specific value to identify the NiFiKop operator instance during local runs outside the cluster. This helps manage operator naming and logging during development.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/6_contributing/1_developer_guide.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport OPERATOR_NAME=nifi-operator\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for IDE\nDESCRIPTION: This snippet demonstrates setting several environment variables for an IDE to connect to a Kubernetes cluster.  It specifies the path to the kubeconfig file, the namespace to watch, the operator pod's name, and the log level. Requires a kubeconfig file available and a Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/6_contributing/1_developer_guide.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nKUBECONFIG={path/to/your/kubeconfig}\nWATCH_NAMESPACE={namespace_to_watch}\nPOD_NAME={name for operator pod}\nLOG_LEVEL=Debug\nOPERATOR_NAME=ide\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables for IDE Execution (Bash)\nDESCRIPTION: Defines essential environment variables required to run the NiFiKop operator directly from an Integrated Development Environment (IDE). These variables configure the Kubernetes connection, target namespace, operator identity, and logging verbosity. Replace placeholders with actual values.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/6_contributing/1_developer_guide.md#_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\nKUBECONFIG={path/to/your/kubeconfig}\nWATCH_NAMESPACE={namespace_to_watch}\nPOD_NAME={name for operator pod}\nLOG_LEVEL=Debug\nOPERATOR_NAME=ide\n```\n\n----------------------------------------\n\nTITLE: Initializing Node.js Project for NiFiKop Migration - Bash\nDESCRIPTION: This snippet initializes a new Node.js project and installs the required dependencies '@kubernetes/client-node' v0.16.3 and 'minimist' v1.2.6 via npm. Ensure you have Node.js (15.3.0+) and npm (7.0.14+) installed before running these commands. The commands set up the environment for the migration script by preparing 'package.json' and necessary modules.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm init -y\nnpm install @kubernetes/client-node@0.16.3 minimist@1.2.6\n```\n\n----------------------------------------\n\nTITLE: Migrating NiFiKop CRDs using Kubernetes API - JavaScript\nDESCRIPTION: This JavaScript file (index.js) is the core logic for migrating custom resources (CRDs) from nifi.orange.com/v1alpha1 to nifi.konpyutaika/v1alpha1. It leverages the @kubernetes/client-node library for API access and minimist for command-line argument parsing. The script lists resources in a namespace, checks ownership, copies metadata/spec/status, and creates new CRDs accordingly. Users must specify --type (cluster, dataflow, user, usergroup, parametercontext, or registryclient) and optionally --namespace. Requires access to a configured kubeconfig with the appropriate permissions. All errors and progress are logged to the console.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/7_upgrade/1_v0.7.x_to_v0.8.0.md#_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nprocess.env['NODE_TLS_REJECT_UNAUTHORIZED'] = 0;\nconst k8s = require('@kubernetes/client-node');\n\nconst kc = new k8s.KubeConfig();\nkc.loadFromDefault();\n\nconst k8sApi = kc.makeApiClient(k8s.CustomObjectsApi);\n\nconst KONPYUTAIKA_GROUP = 'nifi.konpyutaika.com';\nconst KONPYUTAIKA_GROUP_VERSION = 'v1alpha1';\nconst ORANGE_GROUP = 'nifi.orange.com';\nconst ORANGE_GROUP_VERSION = 'v1alpha1';\n\nconst call = async (SRC_GRP, SRC_GRP_VER, DST_GRP, DST_GRP_VER, KIND_PLURAL, NAMESPACE) => {\n\tconsole.log(`Listing ${KIND_PLURAL} of ${SRC_GRP}/${SRC_GRP_VER} in ${NAMESPACE}...`);\n\tconst listResources = (await k8sApi.listNamespacedCustomObject(SRC_GRP, SRC_GRP_VER, NAMESPACE, KIND_PLURAL)).body.items;\n\treturn Promise.all(listResources.map(async (resource) => {\n\t\ttry {\n\t\t\tconsole.log(`Found ${resource.kind} \\\"${resource.metadata.name}\\\" of ${resource.apiVersion} in ${NAMESPACE}`);\n\n\t\t\tif (resource.metadata.ownerReferences) {\n\t\t\t\tconsole.log(`${resource.kind} ${resource.metadata.name} mananged by something else (ownerRefereces is set).`);\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tconst bodyResource = {\n\t\t\t\tapiVersion: `${DST_GRP}/${DST_GRP_VER}`,\n\t\t\t\tkind: resource.kind,\n\t\t\t\tmetadata: {\n\t\t\t\t\tname: resource.metadata.name,\n\t\t\t\t\tannotations: resource.metadata.annotations,\n\t\t\t\t\tlabels: resource.metadata.labels\n\t\t\t\t},\n\t\t\t\tspec: resource.spec\n\t\t\t};\n\n\t\t\tconsole.log(`Creating ${bodyResource.kind} \\\"${bodyResource.metadata.name}\\\" of ${bodyResource.apiVersion} in ${NAMESPACE}...`);\n\t\t\tconst newResource = (await k8sApi.createNamespacedCustomObject(DST_GRP, DST_GRP_VER, NAMESPACE, KIND_PLURAL, bodyResource)).body;\n\t\t\tconsole.log('...done creating.');\n\n\t\t\tconst bodyStatus = {\n\t\t\t\tapiVersion: newResource.apiVersion,\n\t\t\t\tkind: newResource.kind,\n\t\t\t\tmetadata: {\n\t\t\t\t\tname: newResource.metadata.name,\n\t\t\t\t\tresourceVersion: newResource.metadata.resourceVersion\n\t\t\t\t},\n\t\t\t\tstatus: resource.status\n\t\t\t};\n\n\t\t\tconsole.log(`Copying status from ${resource.kind} \\\"${resource.metadata.name}\\\" of ${newResource.apiVersion} to ${newResource.kind} \\\"${newResource.metadata.name}\\\" of ${newResource.apiVersion} in ${NAMESPACE}...`);\n\t\t\tconst newResourceWithStatus = (await k8sApi.replaceNamespacedCustomObjectStatus(DST_GRP, DST_GRP_VER, NAMESPACE, KIND_PLURAL, bodyStatus.metadata.name, bodyStatus)).body;\n\t\t\tconsole.log('...done copying.');\n\t\t\treturn newResourceWithStatus;\n\t\t}\n\t\tcatch (e) {\n\t\t\tconsole.error(e.body ? e.body.message ? e.body.message : e.body : e);\n\t\t}\n\t}));\n};\n\nconst argv = require('minimist')(process.argv.slice(2));\n\nlet NAMESPACE = argv.namespace ? argv.namespace.length > 0 ? argv.namespace : 'default' : 'default';\nlet KIND_PLURAL = {\n\tcluster: 'nificlusters',\n\tdataflow: 'nifidataflows',\n\tparametercontext: 'nifiparametercontexts',\n\tregistryclient: 'nifiregistryclients',\n\tuser: 'nifiusers',\n\tusergroup: 'nifiusergroups',\n};\n\nif (!argv.type) {\n\tconsole.error('Type not provided');\n\tprocess.exit(1);\n}\n\nif (!KIND_PLURAL[argv.type]) {\n\tconsole.error(`Type ${argv.type} is not one of the following types: ${Object.keys(KIND_PLURAL)}`);\n\tprocess.exit(1);\n}\n\nconsole.log(`########### START: ${KIND_PLURAL[argv.type]} ###########`);\ncall( ORANGE_GROUP, ORANGE_GROUP_VERSION, KONPYUTAIKA_GROUP, KONPYUTAIKA_GROUP_VERSION, KIND_PLURAL[argv.type], NAMESPACE)\n\t.then(r => console.log('############ END ############'))\n\t.catch(e => console.error(e));\n```\n\n----------------------------------------\n\nTITLE: Defining NifiCluster initContainerImage with busybox in YAML\nDESCRIPTION: This YAML snippet defines a NifiCluster resource with a custom overridden initContainerImage using the busybox repository tagged '1.34.0'. It shows how the init container image was specified prior to the upgrade. Dependencies include Kubernetes CRD for NifiCluster. The snippet expects the cluster to use this image for initialization steps, but this configuration is now outdated due to the upgrade.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.8.0/7_upgrade_guides/2_v0.14.1_to_v0.15.0.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  initContainerImage:\n    repository: busybox\n    tag: \"1.34.0\"\n```\n\n----------------------------------------\n\nTITLE: Running the NiFiKop Kubernetes CRD Migration Script via npm\nDESCRIPTION: Command to execute the migration script using npm start with parameters specifying the NiFiKop resource type to migrate and the Kubernetes namespace context. Users should replace <NIFIKOP_RESOURCE> with one of the supported resource types (cluster, dataflow, user, usergroup, parametercontext, or registryclient) and <K8S_NAMESPACE> with the target namespace.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.15.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nnpm start -- --type=<NIFIKOP_RESOURCE> --namespace=<K8S_NAMESPACE>\n```\n\n----------------------------------------\n\nTITLE: Configuring NifiCluster with Busybox Init Container Image in YAML\nDESCRIPTION: This YAML snippet defines a NifiCluster custom resource using the v1alpha1 API, where the Zookeeper init container is set to use the 'busybox' image version '1.34.0'. The configuration is intended for older NifiKop versions (<= v0.14.1) and requires updating the image to support bash in version v0.15.0 and above. The file specifies the image repository and tag for the initContainerImage field.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.12.0/7_upgrade_guides/2_v0.14.1_to_v0.15.0.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  initContainerImage:\n    repository: busybox\n    tag: \"1.34.0\"\n```\n\n----------------------------------------\n\nTITLE: Migrating NiFiKop CRDs Across API Versions - Node.js (JavaScript)\nDESCRIPTION: This JavaScript code implements the main migration logic for copying NiFiKop resources (CRDs) between API groups using the Kubernetes API. It authenticates Kubernetes credentials, defines CRD group/version constants, processes CLI arguments, and encapsulates migration logic in an asynchronous 'call' function. The script lists source resources, checks owner references, copies metadata/spec, creates resources in the new CRD, and replicates status. Required dependencies: @kubernetes/client-node, minimist, and a running Kubernetes cluster context. Key parameters include type (resource kind) and namespace. Script must be executed via 'npm start -- --type=... --namespace=...'. Outputs are migration logs; limitations include requiring exclusive operator control and both CRD versions present.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nprocess.env['NODE_TLS_REJECT_UNAUTHORIZED'] = 0;\nconst k8s = require('@kubernetes/client-node');\n\nconst kc = new k8s.KubeConfig();\nkc.loadFromDefault();\n\nconst k8sApi = kc.makeApiClient(k8s.CustomObjectsApi);\n\nconst KONPYUTAIKA_GROUP = 'nifi.konpyutaika.com';\nconst KONPYUTAIKA_GROUP_VERSION = 'v1alpha1';\nconst ORANGE_GROUP = 'nifi.orange.com';\nconst ORANGE_GROUP_VERSION = 'v1alpha1';\n\nconst call = async (SRC_GRP, SRC_GRP_VER, DST_GRP, DST_GRP_VER, KIND_PLURAL, NAMESPACE) => {\n\tconsole.log(`Listing ${KIND_PLURAL} of ${SRC_GRP}/${SRC_GRP_VER} in ${NAMESPACE}...`);\n\tconst listResources = (await k8sApi.listNamespacedCustomObject(SRC_GRP, SRC_GRP_VER, NAMESPACE, KIND_PLURAL)).body.items;\n\treturn Promise.all(listResources.map(async (resource) => {\n\t\ttry {\n\t\t\tconsole.log(`Found ${resource.kind} \\\"${resource.metadata.name}\\\" of ${resource.apiVersion} in ${NAMESPACE}`);\n\n\t\t\tif (resource.metadata.ownerReferences) {\n\t\t\t\tconsole.log(`${resource.kind} ${resource.metadata.name} mananged by something else (ownerRefereces is set).`);\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tconst bodyResource = {\n\t\t\t\tapiVersion: `${DST_GRP}/${DST_GRP_VER}`,\n\t\t\t\tkind: resource.kind,\n\t\t\t\tmetadata: {\n\t\t\t\t\tname: resource.metadata.name,\n\t\t\t\t\tannotations: resource.metadata.annotations,\n\t\t\t\t\tlabels: resource.metadata.labels\n\t\t\t\t},\n\t\t\t\tspec: resource.spec\n\t\t\t};\n\n\t\t\tconsole.log(`Creating ${bodyResource.kind} \\\"${bodyResource.metadata.name}\\\" of ${bodyResource.apiVersion} in ${NAMESPACE}...`);\n\t\t\tconst newResource = (await k8sApi.createNamespacedCustomObject(DST_GRP, DST_GRP_VER, NAMESPACE, KIND_PLURAL, bodyResource)).body;\n\t\t\tconsole.log('...done creating.');\n\n\t\t\tconst bodyStatus = {\n\t\t\t\tapiVersion: newResource.apiVersion,\n\t\t\t\tkind: newResource.kind,\n\t\t\t\tmetadata: {\n\t\t\t\t\tname: newResource.metadata.name,\n\t\t\t\t\tresourceVersion: newResource.metadata.resourceVersion\n\t\t\t\t},\n\t\t\t\tstatus: resource.status\n\t\t\t};\n\n\t\t\tconsole.log(`Copying status from ${resource.kind} \\\"${resource.metadata.name}\\\" of ${newResource.apiVersion} to ${newResource.kind} \\\"${newResource.metadata.name}\\\" of ${newResource.apiVersion} in ${NAMESPACE}...`);\n\t\t\tconst newResourceWithStatus = (await k8sApi.replaceNamespacedCustomObjectStatus(DST_GRP, DST_GRP_VER, NAMESPACE, KIND_PLURAL, bodyStatus.metadata.name, bodyStatus)).body;\n\t\t\tconsole.log('...done copying.');\n\t\t\treturn newResourceWithStatus;\n\t\t}\n\t\tcatch (e) {\n\t\t\tconsole.error(e.body ? e.body.message ? e.body.message : e.body : e);\n\t\t}\n\t}));\n};\n\nconst argv = require('minimist')(process.argv.slice(2));\n\nlet NAMESPACE = argv.namespace ? argv.namespace.length > 0 ? argv.namespace : 'default' : 'default';\nlet KIND_PLURAL = {\n\tcluster: 'nificlusters',\n\tdataflow: 'nifidataflows',\n\tparametercontext: 'nifiparametercontexts',\n\tregistryclient: 'nifiregistryclients',\n\tuser: 'nifiusers',\n\tusergroup: 'nifiusergroups',\n};\n\nif (!argv.type) {\n\tconsole.error('Type not provided');\n\tprocess.exit(1);\n}\n\nif (!KIND_PLURAL[argv.type]) {\n\tconsole.error(`Type ${argv.type} is not one of the following types: ${Object.keys(KIND_PLURAL)}`);\n\tprocess.exit(1);\n}\n\nconsole.log(`########### START: ${KIND_PLURAL[argv.type]} ###########`);\ncall( ORANGE_GROUP, ORANGE_GROUP_VERSION, KONPYUTAIKA_GROUP, KONPYUTAIKA_GROUP_VERSION, KIND_PLURAL[argv.type], NAMESPACE)\n\t.then(r => console.log('############ END ############'))\n\t.catch(e => console.error(e));\n```\n\n----------------------------------------\n\nTITLE: NifiCluster: Old InitContainerImage Configuration (YAML)\nDESCRIPTION: This snippet shows an example of a NifiCluster custom resource definition with the `initContainerImage` field explicitly set to `busybox`. This configuration was common or the default prior to Nifikop v0.15.0. If you have overridden this field with `busybox`, you must change it during the upgrade.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/7_upgrade_guides/2_v0.14.1_to_v0.15.0.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: nifi.konpyutaika.com/v1alpha1\nkind: NifiCluster\nmetadata:\n  name: mynifi\nspec:\n  initContainerImage:\n    repository: busybox\n    tag: \"1.34.0\"\n```\n\n----------------------------------------\n\nTITLE: Building NiFiKop Docker Image (Bash)\nDESCRIPTION: Sets the `DOCKER_REGISTRY_BASE` environment variable to specify the target Docker repository and then uses the `docker-build` Makefile target to build the operator's Docker image. Replace `{your-docker-repo}` with your actual repository path.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.0/6_contributing/1_developer_guide.md#_snippet_6\n\nLANGUAGE: Bash\nCODE:\n```\nexport DOCKER_REGISTRY_BASE={your-docker-repo}\nmake docker-build\n```\n\n----------------------------------------\n\nTITLE: Running Resource Migration Script - bash\nDESCRIPTION: Executes the Node.js migration script using npm, specifying the resource type ('--type') and optionally the Kubernetes namespace ('--namespace'). The NIFIKOP_RESOURCE argument selects the type of NiFiKop resource (e.g., cluster, dataflow, user) to migrate. The K8S_NAMESPACE specifies the Kubernetes namespace (defaults to 'default' if omitted). Outputs logs to console during migration and errors if parameters are missing.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nnpm start -- --type=<NIFIKOP_RESOURCE> --namespace=<K8S_NAMESPACE>\n```\n\n----------------------------------------\n\nTITLE: Data model schema for NifiRegistryClientStatus in Go\nDESCRIPTION: The `NifiRegistryClientStatus` schema captures the current observed state of a NiFi registry client, including its ID and last revision version, enabling status monitoring and reconciliation by controllers.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/5_references/3_nifi_registry_client.md#_snippet_3\n\n\n\n----------------------------------------\n\nTITLE: Packaging Helm Chart for NiFiKop Operator - Bash\nDESCRIPTION: This snippet leverages the Makefile to package the NiFiKop Helm chart for distribution. Helm and Make must be installed. The output will be a packaged chart file that can then be published to a Helm repository.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/6_contributing/1_developer_guide.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nmake helm-package\n```\n\n----------------------------------------\n\nTITLE: Complete package.json Configuration for Migration Project - json\nDESCRIPTION: Shows a full sample package.json configured for NiFiKop CRD migration. Includes metadata, start and test scripts, keywords, license, and locked dependency versions. Assumes npm and required packages are installed as previously described. This configuration ensures repeatable project setup and execution. Output is an example of a ready-to-use package.json file.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"nifikop_crd_migration\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Script to migrate from the old CRDs to the new CRDs.\",\n  \"main\": \"index.js\",\n  \"scripts\": {\n    \"start\": \"node --no-warnings index.js\",\n    \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\"\n  },\n  \"keywords\": [\n    \"K8S\",\n    \"NiFiKop\",\n    \"CRDs\"\n  ],\n  \"license\": \"ISC\",\n  \"dependencies\": {\n    \"@kubernetes/client-node\": \"^0.16.3\",\n    \"minimist\": \"^1.2.6\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Running NiFiKop Migration Script - Bash\nDESCRIPTION: This command starts the migration script, passing the resource type and (optionally) a namespace as arguments. <NIFIKOP_RESOURCE> must be one of the supported types (cluster, dataflow, user, usergroup, parametercontext, registryclient) and <K8S_NAMESPACE> should be your target namespace. Requires configured package.json scripts and dependencies installed.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/7_upgrade/1_v0.7.x_to_v0.8.0.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nnpm start -- --type=<NIFIKOP_RESOURCE> --namespace=<K8S_NAMESPACE>\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables To Run NiFiKop Operator Locally (Bash)\nDESCRIPTION: This snippet sets environment variables needed for running the NiFiKop operator locally in development mode. Variables include the kubectl configuration path, the Kubernetes namespace to watch, operator pod name, log verbosity level, and an arbitrary operator name. These variables enable the operator to connect to the target Kubernetes cluster for management tasks.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.4/6_contributing/1_developer_guide.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nKUBECONFIG={path/to/your/kubeconfig}\nWATCH_NAMESPACE={namespace_to_watch}\nPOD_NAME={name for operator pod}\nLOG_LEVEL=Debug\nOPERATOR_NAME=ide\n```\n\n----------------------------------------\n\nTITLE: NifiUserGroupStatus Fields\nDESCRIPTION: This schema defines the status fields for NiFi UserGroup, including unique node identifier and revision version. These fields are populated by the system to reflect the current state and versioning of the resource, with constraints specifying their necessity.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.1.1/5_references/6_nifi_usergroup.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n# Status information for NifiUserGroup, including node ID and version.\n\n```\n\n----------------------------------------\n\nTITLE: Initializing Node.js Project for NiFiKop Migration (Bash)\nDESCRIPTION: Initializes a new Node.js project using `npm init -y` and installs the required npm packages: `@kubernetes/client-node` (version 0.16.3) for Kubernetes API interaction and `minimist` (version 1.2.6) for command-line argument parsing. These dependencies are essential for the migration script.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.5.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm init -y\nnpm install @kubernetes/client-node@0.16.3 minimist@1.2.6\n```\n\n----------------------------------------\n\nTITLE: Cloning NiFiKop Project\nDESCRIPTION: This bash command clones the NiFiKop project from a GitHub repository and navigates into the project directory.  It assumes git is installed and available in the system's PATH. The output is the project files downloaded to the local directory.  There are no specific parameters.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.11.0/6_contributing/1_developer_guide.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/konpyutaika/nifikop.git\ncd nifikop\n```\n\n----------------------------------------\n\nTITLE: Initializing Node.js Project for NiFiKop Migration - Bash\nDESCRIPTION: This snippet initializes a new Node.js project and installs the required dependencies (@kubernetes/client-node and minimist) via npm. It must be run in the root directory of your migration project. Requires Node.js v15.3.0+ and npm v7.0.14+ pre-installed. It sets up the environment needed to interact programmatically with the Kubernetes API for migration.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/7_upgrade/1_v0.7.x_to_v0.8.0.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm init -y\nnpm install @kubernetes/client-node@0.16.3 minimist@1.2.6\n```\n\n----------------------------------------\n\nTITLE: Building Docker image for NiFiKop\nDESCRIPTION: Commands to build a Docker image of the NiFiKop operator from the current branch.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.16.0/6_contributing/1_developer_guide.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nexport DOCKER_REPO_BASE={your-docker-repo}\nmake docker-build\n```\n\n----------------------------------------\n\nTITLE: Example package.json for NiFiKop Migration Script (JSON)\nDESCRIPTION: Provides a complete example `package.json` file for the NiFiKop migration project. It includes project metadata (name, version, description), the entry point (`index.js`), the defined `start` script, keywords for discoverability, license information, and lists the required dependencies (`@kubernetes/client-node` and `minimist`) with their specific versions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.3.1/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"nifikop_crd_migration\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Script to migrate from the old CRDs to the new CRDs.\",\n  \"main\": \"index.js\",\n  \"scripts\": {\n    \"start\": \"node --no-warnings index.js\",\n    \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\"\n  },\n  \"keywords\": [\n    \"K8S\",\n    \"NiFiKop\",\n    \"CRDs\"\n  ],\n  \"license\": \"ISC\",\n  \"dependencies\": {\n    \"@kubernetes/client-node\": \"^0.16.3\",\n    \"minimist\": \"^1.2.6\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Running NiFiKop Operator Locally with Go Binary in Bash\nDESCRIPTION: This set of commands guides running the NiFiKop operator locally outside a Kubernetes cluster during development. It involves exporting the operator name environment variable, applying multiple CRD manifests via kubectl to register custom resource definitions, and running the operator using a Makefile command. It requires kubectl configured with correct permissions and access to a Kubernetes cluster.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/6_contributing/1_developer_guide.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport OPERATOR_NAME=nifi-operator\n```\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nificlusters.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifidataflows.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiparametercontexts.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiregistryclients.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusergroups.yaml\nkubectl apply -f config/crd/bases/nifi.konpyutaika.com_nifiusers.yaml\n```\n\nLANGUAGE: bash\nCODE:\n```\nmake run\n```\n\n----------------------------------------\n\nTITLE: Building NiFiKop Operator with Make in Bash\nDESCRIPTION: This command uses Make to compile the NiFiKop operator binary in a local Go environment. It assumes Go and Make are installed and configured. The output is an executable for running or testing the operator locally.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/6_contributing/1_developer_guide.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmake build\n```\n\n----------------------------------------\n\nTITLE: Exporting Operator Name as Environment Variable - Bash\nDESCRIPTION: Exports the operator name as an environment variable, allowing the operator binary to recognize its instance identity. This is especially important for local execution and may impact naming and deployment behaviors. Ensure the name matches expected conventions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.3/6_contributing/1_developer_guide.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport OPERATOR_NAME=nifi-operator\n```\n\n----------------------------------------\n\nTITLE: Setting Operator Name Environment Variable in Bash\nDESCRIPTION: Command to set the operator name environment variable before running the operator locally.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.11.1/6_contributing/1_developer_guide.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport OPERATOR_NAME=nifi-operator\n```\n\n----------------------------------------\n\nTITLE: Adding Start Script for NiFiKop Migration - JSON\nDESCRIPTION: This JSON snippet should be added to the scripts section of your package.json file. It defines a 'start' script that runs index.js with Node.js, suppressing warnings. This enables consistent command-line execution of the migration tool. Ensure that your main script file is named index.js.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v0.14.0/7_upgrade/1_v0.7.x_to_v0.8.0.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n\"start\": \"node --no-warnings index.js\"\n```\n\n----------------------------------------\n\nTITLE: NiFiKop Configuration Structures in Markdown Tables\nDESCRIPTION: A set of Markdown tables defining the configuration structures for NiFiKop, including field specifications, types, descriptions, and default values for various NiFi components.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.2.0/5_references/1_nifi_cluster/2_read_only_config.md#_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n## ZookeeperProperties\n\n|Field|Type|Description|Required|Default|\n|-----|----|-----------|--------|--------|\n|overrideConfigMap|[ConfigmapReference](#configmapreference)|Additionnal zookeeper.properties configuration that will override the one produced based on template and configuration.|No|nil|\n|overrideConfigs|string|Additionnal zookeeper.properties configuration that will override the one produced based on template, configurations and overrideConfigMap.|No|\"\"|\n|overrideSecretConfig|[SecretConfigReference](#secretconfigreference)|Additionnal zookeeper.properties configuration that will override the one produced based on template, configurations, overrideConfigMap and overrideConfigs.|No|nil|\n```\n\nLANGUAGE: markdown\nCODE:\n```\n## BootstrapProperties\n\n|Field|Type|Description|Required|Default|\n|-----|----|-----------|--------|--------|\n|overrideConfigMap|[ConfigmapReference](#configmapreference)|Additionnal bootstrap.properties configuration that will override the one produced based on template and configuration.|No|nil|\n|overrideConfigs|string|Additionnal bootstrap.properties configuration that will override the one produced based on template, configurations and overrideConfigMap.|No|\"\"|\n|overrideSecretConfig|[SecretConfigReference](#secretconfigreference)|Additionnal bootstrap.properties configuration that will override the one produced based on template, configurations, overrideConfigMap and overrideConfigs.|No|nil|\n|NifiJvmMemory|string|JVM memory settings.|No|\"512m\"|\n```\n\nLANGUAGE: markdown\nCODE:\n```\n## LogbackConfig\n\n|Field|Type|Description|Required|Default|\n|-----|----|-----------|--------|--------|\n|replaceConfigMap|[ConfigmapReference](#configmapreference)|logback.xml configuration that will replace the one produced based on template.|No|nil|\n|replaceSecretConfig|[SecretConfigReference](#secretconfigreference)|logback.xml configuration that will replace the one produced based on template and overrideConfigMap.|No|nil|\n```\n\nLANGUAGE: markdown\nCODE:\n```\n## AuthorizerConfig\n\n|Field|Type|Description|Required|Default|\n|-----|----|-----------|--------|--------|\n|replaceTemplateConfigMap|[ConfigmapReference](#configmapreference)|authorizers.xml configuration template that will replace the default template.|No|nil|\n|replaceTemplateSecretConfig|[SecretConfigReference](#secretconfigreference)|authorizers.xml configuration that will replace the default template and the replaceTemplateConfigMap.|No|nil|\n```\n\nLANGUAGE: markdown\nCODE:\n```\n## BootstrapNotificationServicesConfig\n\n|Field|Type|Description|Required|Default|\n|-----|----|-----------|--------|--------|\n|replaceConfigMap|[ConfigmapReference](#configmapreference)|bootstrap_notifications_services.xml configuration that will replace the one produced based on template.|No|nil|\n|replaceSecretConfig|[SecretConfigReference](#secretconfigreference)|bootstrap_notifications_services.xml configuration that will replace the one produced based on template and overrideConfigMap.|No|nil|\n```\n\nLANGUAGE: markdown\nCODE:\n```\n## ConfigmapReference\n\n|Field|Type|Description|Required|Default|\n|-----|----|-----------|--------|--------|\n|name|string|Name of the configmap that we want to refer.|Yes|\"\"|\n|namespace|string|Namespace where is located the configmap that we want to refer.|No|\"\"|\n|data|string|The key of the value,in data content, that we want use.|Yes|\"\"|\n```\n\nLANGUAGE: markdown\nCODE:\n```\n## SecretConfigReference\n\n|Field|Type|Description|Required|Default|\n|-----|----|-----------|--------|--------|\n|name|string|Name of the secret that we want to refer.|Yes|\"\"|\n|namespace|string|Namespace where is located the secret that we want to refer.|No|\"\"|\n|data|string|The key of the value,in data content, that we want use.|Yes|\"\"|\n```\n\n----------------------------------------\n\nTITLE: Setting default value for nifi.content.viewer.url property\nDESCRIPTION: This snippet updates the default value of the 'nifi.content.viewer.url' property within the operator configuration, affecting how content viewer URLs are generated or accessed. Dependencies include operator configuration files and property schema definitions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/CHANGELOG.md#_snippet_6\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: operator.k8s.io/v1\nkind: NifiCluster\nspec:\n  properties:\n    nifi.content.viewer.url: \"https://default-viewer.example.com\"\n```\n\n----------------------------------------\n\nTITLE: Adding ability to include NodePort in Helm webProxyHosts\nDESCRIPTION: This snippet enhances the Helm chart configuration to support specifying NodePort alongside custom hostnames in 'webProxyHosts', enabling external access via specific ports. Dependencies include Helm templates and Kubernetes service definitions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/CHANGELOG.md#_snippet_9\n\nLANGUAGE: YAML\nCODE:\n```\nwebProxyHosts:\n  - host: \"example.com\"\n    nodePort: 30080\n```\n\n----------------------------------------\n\nTITLE: Building NiFiKop Operator Locally\nDESCRIPTION: Command to build the NiFiKop operator in a local Go environment.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/6_contributing/1_developer_guide.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmake build\n```\n\n----------------------------------------\n\nTITLE: Cloning NiFiKop Repository - Bash\nDESCRIPTION: This snippet clones the NiFiKop repository from GitHub and navigates into the project directory. It requires git to be installed and accessible from the command line. The output will be a local copy of the repository ready for further operations.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.13.0/6_contributing/1_developer_guide.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/konpyutaika/nifikop.git\ncd nifikop\n```\n\n----------------------------------------\n\nTITLE: Complete package.json for NiFiKop migration\nDESCRIPTION: This JSON represents the complete `package.json` file for the NiFiKop CRD migration script. It includes metadata like name, version, description, main entry point, scripts (including the `start` script), keywords, license, and dependencies. The dependencies include `@kubernetes/client-node` for Kubernetes API interaction and `minimist` for command-line argument parsing.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.0.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"nifikop_crd_migration\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Script to migrate from the old CRDs to the new CRDs.\",\n  \"main\": \"index.js\",\n  \"scripts\": {\n    \"start\": \"node --no-warnings index.js\",\n    \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\"\n  },\n  \"keywords\": [\n    \"K8S\",\n    \"NiFiKop\",\n    \"CRDs\"\n  ],\n  \"license\": \"ISC\",\n  \"dependencies\": {\n    \"@kubernetes/client-node\": \"^0.16.3\",\n    \"minimist\": \"^1.2.6\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Running React Native Website Locally\nDESCRIPTION: This snippet describes how to start the React Native website's development server using `yarn start`. It also provides instructions on how to access the website in a web browser.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/README.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\n1.  `yarn start` to start the development server _(powered by [Docusaurus](https://v2.docusaurus.io))_.\n1.  `open http://localhost:3000/` to open the site in your favorite browser.\n```\n\n----------------------------------------\n\nTITLE: Example Node.js package.json configuration (JSON)\nDESCRIPTION: Displays the complete expected content of the `package.json` file after initializing the project, installing dependencies, and adding the custom \"start\" script. It includes basic project metadata, the configured scripts, keywords, license, and the installed dependencies with their versions.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.10.0/7_upgrade_guides/1_v0.7.x_to_v0.8.0.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"nifikop_crd_migration\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Script to migrate from the old CRDs to the new CRDs.\",\n  \"main\": \"index.js\",\n  \"scripts\": {\n    \"start\": \"node --no-warnings index.js\",\n    \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\"\n  },\n  \"keywords\": [\n    \"K8S\",\n    \"NiFiKop\",\n    \"CRDs\"\n  ],\n  \"license\": \"ISC\",\n  \"dependencies\": {\n    \"@kubernetes/client-node\": \"^0.16.3\",\n    \"minimist\": \"^1.2.6\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Schema Documentation for SSLSecrets\nDESCRIPTION: This schema describes the SSLSecrets object, including fields for tlsSecretName, create, clusterScoped, issuerRef, and pkiBackend, with their types and descriptions. It manages SSL certificate secrets required for securing Nifi communication, with options for creating or referencing existing certificates within Kubernetes.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.4.1/5_references/1_nifi_cluster/6_listeners_config.md#_snippet_3\n\n\n\n----------------------------------------\n\nTITLE: Developer Certificate of Origin (DCO) - Text\nDESCRIPTION: This snippet contains the full text of the Developer Certificate of Origin (DCO) version 1.1, which must be referenced and acknowledged by all contributors through commit signing. Contributors are required to sign each commit with the '-s' switch in git, signifying agreement to the terms that ensure code provenance and compliance with open source licensing. The DCO is critical for legal clarity and project governance; no further dependencies are needed, but strict adherence to signing is mandatory.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/CONTRIBUTING.md#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nDeveloper Certificate of Origin\nVersion 1.1\n\nCopyright (C) 2004, 2006 The Linux Foundation and its contributors.\n1 Letterman Drive\nSuite D4700\nSan Francisco, CA, 94129\n\nEveryone is permitted to copy and distribute verbatim copies of this\nlicense document, but changing it is not allowed.\n\n\nDeveloper's Certificate of Origin 1.1\n\nBy making a contribution to this project, I certify that:\n\n(a) The contribution was created in whole or in part by me and I\n    have the right to submit it under the open source license\n    indicated in the file; or\n\n(b) The contribution is based upon previous work that, to the best\n    of my knowledge, is covered under an appropriate open source\n    license and I have the right under that license to submit that\n    work with modifications, whether created in whole or in part\n    by me, under the same open source license (unless I am\n    permitted to submit under a different license), as indicated\n    in the file; or\n\n(c) The contribution was provided directly to me by some other\n    person who certified (a), (b) or (c) and I have not modified\n    it.\n\n(d) I understand and agree that this project and the contribution\n    are public and that a record of the contribution (including all\n    personal information I submit with it, including my sign-off) is\n    maintained indefinitely and may be redistributed consistent with\n    this project or the open source license(s) involved.\n```\n\n----------------------------------------\n\nTITLE: NifiParameterContextsSpec Schema Definition\nDESCRIPTION: Specifies the desired state of a NiFi Parameter Context, including description, parameters, secrets, cluster reference, inheritance, and takeover behavior. It serves as the configuration template for deploying or updating parameter contexts.\nSOURCE: https://github.com/konpyutaika/nifikop/blob/master/site/website/versioned_docs/version-v1.9.0/5_references/4_nifi_parameter_context.md#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\nNifiParameterContextsSpec:\n  description: string\n  parameters: [Parameter]\n  secretRefs: [SecretReference]\n  clusterRef: [ClusterReference]\n  inheritedParameterContext: [ParameterContextReference]\n  disableTakeOver: bool\n```"
  }
]
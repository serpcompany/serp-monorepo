[
  {
    "owner": "kserve",
    "repo": "kserve",
    "content": "TITLE: Defining KServe InferenceService in YAML\nDESCRIPTION: YAML configuration for creating a KServe InferenceService that deploys an XGBoost model with v2 protocol support, pulling model artifacts from a GCS storage location.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/xgboost/README.md#2025-04-21_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"xgboost-iris\"\nspec:\n  predictor:\n    xgboost:\n      protocolVersion: \"v2\"\n      storageUri: \"gs://kfserving-examples/models/xgboost/iris\"\n```\n\n----------------------------------------\n\nTITLE: Defining ClusterServingRuntime for Triton with FasterTransformer backend in YAML\nDESCRIPTION: YAML configuration for creating a custom ClusterServingRuntime that uses a Triton Server image with the FasterTransformer backend. This resource needs to be applied to the Kubernetes cluster with the image field replaced by your custom image.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/triton/fastertransformer/README.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: serving.kserve.io/v1alpha1\nkind: ClusterServingRuntime\nmetadata:\n  name: tritonserver-ft\nspec:\n  annotations:\n    prometheus.kserve.io/path: /metrics\n    prometheus.kserve.io/port: \"8002\"\n  containers:\n  - args:\n    - tritonserver\n    - --model-store=/mnt/models\n    - --grpc-port=9000\n    - --http-port=8080\n    - --allow-grpc=true\n    - --allow-http=true\n    image: <my-custom-triton-image>\n    name: kserve-container\n    resources:\n      limits:\n        cpu: \"1\"\n        memory: 2Gi\n      requests:\n        cpu: \"1\"\n        memory: 2Gi\n  protocolVersions:\n  - v2\n  - grpc-v2\n  supportedModelFormats:\n  - name: triton\n    version: \"2\"\n```\n\n----------------------------------------\n\nTITLE: Creating InferenceService with TensorFlow Model\nDESCRIPTION: Define an InferenceService with a TensorFlow serving spec, specifying the model storage URI and metadata\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/client/kfserving_sdk_v1beta1_sample.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\napi_version = constants.KFSERVING_GROUP + \"/\" + kfserving_version\n\nisvc = V1beta1InferenceService(\n    api_version=api_version,\n    kind=constants.KFSERVING_KIND,\n    metadata=client.V1ObjectMeta(name=\"flower-sample\", namespace=namespace),\n    spec=V1beta1InferenceServiceSpec(\n        predictor=V1beta1PredictorSpec(\n            tensorflow=(\n                V1beta1TFServingSpec(\n                    storage_uri=\"gs://kfserving-examples/models/tensorflow/flowers\"\n                )\n            )\n        )\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Load Testing Text Classifier with hey\nDESCRIPTION: This bash script sets up and executes load testing on a text classifier using the `hey` load generator.  It first retrieves the service hostname, then it sends POST requests with text data to the specified prediction endpoint using `hey`. It assumes `INGRESS_HOST` and `INGRESS_PORT` are already set.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/docs/autoscaling.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n\"MODEL_NAME=torchserve-custom\"\n\"SERVICE_HOSTNAME=$(kubectl get inferenceservice ${MODEL_NAME} <namespace> -o jsonpath='{.status.url}' | cut -d \\\"/\\\" -f 3)\"\n\n\"### Sample load testing on Text classifier\"\n\n\"./hey -m POST -z 30s -T \\\"application/octet-stream\\\" -d \\\"$(cat Huggingface_Transformers/Seq_classification_artifacts/sample_text.txt)\\\" -host ${SERVICE_HOSTNAME} http://${INGRESS_HOST}:${INGRESS_PORT}/predictions/BERTSeqClassification\"\n```\n\n----------------------------------------\n\nTITLE: Configure Autoscaling Target in InferenceService YAML\nDESCRIPTION: This snippet demonstrates how to set the `autoscaling.knative.dev/target` annotation in the InferenceService's metadata. This annotation specifies the desired concurrency target for the autoscaler, indicating the average number of requests each replica should handle. It defines the desired concurrency (5) and the container image to be used.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/docs/autoscaling.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n\"apiVersion\": \"serving.kserve.io/v1beta1\"\n\"kind\": \"InferenceService\"\n\"metadata\":\n  \"name\": \"torchserve-custom\"\n  \"annotations\":\n    \"autoscaling.knative.dev/target\": \"5\"\nspec:\n  \"predictor\":\n    \"containers\":\n    - \"image\": {username}/torchserve:latest\n      \"name\": \"torchserve-container\"\n```\n\n----------------------------------------\n\nTITLE: Creating TensorFlow InferenceService with gRPC\nDESCRIPTION: YAML configuration for deploying a TensorFlow model as an InferenceService with gRPC endpoint on port 9000.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/tensorflow/README.md#2025-04-21_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"flower-grpc\"\nspec:\n  predictor:\n    tensorflow:\n      storageUri: \"gs://kfserving-examples/models/tensorflow/flowers\"\n      ports:\n        - containerPort: 9000\n          name: h2c\n          protocol: TCP\n```\n\n----------------------------------------\n\nTITLE: Configuring InferenceService Autoscaling with YAML\nDESCRIPTION: This YAML configuration defines an InferenceService that sets the container concurrency limit to 10, specifying how many simultaneous requests can be processed by each replica. It is essential for configuring performance under load. Requires KServe installed.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/autoscaling/README.md#2025-04-21_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"flowers-sample\"\nspec:\n  predictor:\n    containerConcurrency: 10\n    tensorflow:\n      storageUri: \"gs://kfserving-examples/models/tensorflow/flowers\"\n```\n\n----------------------------------------\n\nTITLE: Visualizing Explanation Anchors and Metrics for High Income Prediction with Python\nDESCRIPTION: This snippet visualizes the anchors and performance metrics from the explanation obtained for the high income prediction, providing insights into explanation effectiveness and feature importance.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/alibi/income/income_explanations.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nshow_anchors(exp[\"data\"][\"anchor\"])\n\nshow_bar([exp[\"data\"][\"precision\"]], [\"\"], \"Precision\")\nshow_bar([exp[\"data\"][\"coverage\"]], [\"\"], \"Coverage\")\n```\n\n----------------------------------------\n\nTITLE: Deploying TorchServe InferenceService with V2 Protocol\nDESCRIPTION: Applies the MNIST model configuration to create an InferenceService in the Kubernetes cluster using the v2 protocol.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v2/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f mnist.yaml\n```\n\n----------------------------------------\n\nTITLE: Deploying XGBoost Model with kubectl\nDESCRIPTION: Bash command to deploy the XGBoost model as defined in the xgboost.yaml file to a Kubernetes cluster using kubectl.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/xgboost/README.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f ./xgboost.yaml\n```\n\n----------------------------------------\n\nTITLE: Deploying Llama2 Model with vLLM Backend on KServe\nDESCRIPTION: YAML configuration for deploying a Llama2 model on KServe using the HuggingFace runtime with vLLM as the backend. vLLM is set as the default backend for supported models to optimize text generation performance.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/huggingfaceserver/README.md#2025-04-21_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: serving.kserve.io/v1beta1\nkind: InferenceService\nmetadata:\n  name: huggingface-llama2\nspec:\n  predictor:\n    model:\n      modelFormat:\n        name: huggingface\n      args:\n      - --model_name=llama2\n      - --model_id=meta-llama/Llama-2-7b-chat-hf\n      resources:\n        limits:\n          cpu: \"6\"\n          memory: 24Gi\n          nvidia.com/gpu: \"1\"\n        requests:\n          cpu: \"6\"\n          memory: 24Gi\n          nvidia.com/gpu: \"1\"\n```\n\n----------------------------------------\n\nTITLE: Enable tag-based routing - KServe\nDESCRIPTION: This YAML configuration enables tag-based routing for the InferenceService. It adds the `serving.kserve.io/enable-tag-routing: \"true\"` annotation and sets `canaryTrafficPercent` to 10.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/rollout/README.md#2025-04-21_snippet_15\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"my-model\"\n  annotations:\n    serving.kserve.io/enable-tag-routing: \"true\"\nspec:\n  predictor:\n    canaryTrafficPercent: 10\n    tensorflow:\n      storageUri: \"gs://kfserving-examples/models/tensorflow/flowers-2\"\n```\n\n----------------------------------------\n\nTITLE: Running a prediction request to the InferenceService using cURL\nDESCRIPTION: This snippet shows how to make a POST request to the /model/predict endpoint of the InferenceService. It demonstrates how to set the model name and service hostname dynamically, and how to send an image file for prediction along with an optional threshold parameter.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/prebuilt-image/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nMODEL_NAME=custom-prebuilt-image\nSERVICE_HOSTNAME=$(kubectl get inferenceservice ${MODEL_NAME} -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\ncurl -v -F \"image=@dog-human.jpg\" http://${INGRESS_HOST}:${INGRESS_PORT}/model/predict -H \"Host: ${SERVICE_HOSTNAME}\"\n```\n\n----------------------------------------\n\nTITLE: Defining JSON Structure for Inference Response in JSON\nDESCRIPTION: Defines the JSON structure for a successful inference response, including model metadata and output tensors. Key fields include model name, version, request ID, parameters, and outputs where outputs are detailed in a separate schema. It establishes the semantics for various components crucial for understanding inference outcomes.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md#2025-04-21_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"model_name\" : $string,\n  \"model_version\" : $string #optional,\n  \"id\" : $string,\n  \"parameters\" : $parameters #optional,\n  \"outputs\" : [ $response_output, ... ]\n}\n```\n\n----------------------------------------\n\nTITLE: Creating TensorFlow InferenceService with HTTP\nDESCRIPTION: YAML configuration for deploying a TensorFlow model as an InferenceService with HTTP/REST endpoint. Specifies the model storage location in GCS.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/tensorflow/README.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"flower-sample\"\nspec:\n  predictor:\n    tensorflow:\n      storageUri: \"gs://kfserving-examples/models/tensorflow/flowers\"\n```\n\n----------------------------------------\n\nTITLE: Deploying Llama2 Model with Speculative Model Loading on KServe\nDESCRIPTION: YAML configuration for deploying a Llama2 model with speculative model loading on KServe. This configuration uses a smaller model (OPT-125M) to accelerate generation by speculating the next tokens.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/huggingfaceserver/README.md#2025-04-21_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: serving.kserve.io/v1beta1\nkind: InferenceService\nmetadata:\n  name: huggingface-llama2\nspec:\n  predictor:\n    model:\n      modelFormat:\n        name: huggingface\n      args:\n      - --model_name=llama2\n      - --model_id=meta-llama/Llama-2-7b-chat-hf\n      - --speculative-model=\"facebook/opt-125m\"\n      - --num-speculative-tokens 5\n      resources:\n        limits:\n          cpu: \"6\"\n          memory: 24Gi\n          nvidia.com/gpu: \"1\"\n        requests:\n          cpu: \"6\"\n          memory: 24Gi\n          nvidia.com/gpu: \"1\"\n```\n\n----------------------------------------\n\nTITLE: Configuring InferenceService with Hard Concurrency Limit in YAML\nDESCRIPTION: YAML configuration for InferenceService using containerConcurrency field to set a hard concurrency limit of 10 requests. This enforces a strict upper bound where excess requests are buffered.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/autoscaling/README.md#2025-04-21_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"torchserve\"\nspec:\n  predictor:\n    containerConcurrency: 10\n    pytorch:\n      storageUri: \"gs://kfserving-examples/models/torchserve/image_classifier/v1\"\n```\n\n----------------------------------------\n\nTITLE: Canary InferenceService Deployment YAML\nDESCRIPTION: Defines the canary deployment for the KServe InferenceService. It sets the traffic split to 20% for the canary, uses a different image version (torchserve_v2), and maintains the original container name for consistency. This allows testing a new version with a subset of traffic.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/docs/canary.md#2025-04-21_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"torchserve-custom\"\nspec:\n  predictor:\n    canaryTrafficPercent: 20\n    containers:\n    - image: {username}/torchserve_v2:latest\n      name: torchserve-container\n```\n\n----------------------------------------\n\nTITLE: Deploying Llama2 Model with HuggingFace Backend on KServe\nDESCRIPTION: YAML configuration for deploying a Llama2 model on KServe using the HuggingFace backend explicitly instead of vLLM. This configuration forces the use of HuggingFace's native inference engine.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/huggingfaceserver/README.md#2025-04-21_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: serving.kserve.io/v1beta1\nkind: InferenceService\nmetadata:\n  name: huggingface-llama2\nspec:\n  predictor:\n    model:\n      modelFormat:\n        name: huggingface\n      args:\n      - --model_name=llama2\n      - --model_id=meta-llama/Llama-2-7b-chat-hf\n      - --backend=huggingface\n      resources:\n        limits:\n          cpu: \"6\"\n          memory: 24Gi\n          nvidia.com/gpu: \"1\"\n        requests:\n          cpu: \"6\"\n          memory: 24Gi\n          nvidia.com/gpu: \"1\"\n```\n\n----------------------------------------\n\nTITLE: Creating InferenceService Using KFServing Client\nDESCRIPTION: Instantiate the KFServingClient and create the InferenceService in the specified Kubernetes namespace\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/client/kfserving_sdk_v1beta1_sample.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nKFServing = KFServingClient()\nKFServing.create(isvc)\n```\n\n----------------------------------------\n\nTITLE: KServe InferenceService Deployment Configuration\nDESCRIPTION: YAML configuration for deploying the SKLearn model to KServe using v1beta1 API with V2 protocol support.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/sklearn/v2/README.md#2025-04-21_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"sklearn-irisv2\"\nspec:\n  predictor:\n    sklearn:\n      protocolVersion: \"v2\"\n      storageUri: \"gs://seldon-models/sklearn/mms/lr_model\"\n```\n\n----------------------------------------\n\nTITLE: Show Precision and Coverage\nDESCRIPTION: These snippets extract the precision and coverage from the explanation data and display them using the `show_bar` function.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/alibi/moviesentiment/movie_review_explanations.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n\"show_bar([exp[\\\"data\\\"][\\\"precision\\\"]], [\\\"\\\"], \\\"Precision\\\")\\nshow_bar([exp[\\\"data\\\"][\\\"coverage\\\"]], [\\\"\\\"], \\\"Coverage\\\")\"\n```\n\n----------------------------------------\n\nTITLE: Deploying llama3.1 with KServe HuggingFace vLLM Runtime on CPU\nDESCRIPTION: This YAML configuration defines an InferenceService for deploying the llama3.1 model using the KServe HuggingFace vLLM runtime on a CPU. It specifies the model format, model name, and model ID, as well as resource requirements (CPU and memory limits and requests). The vLLM is used as the default backend, if available for the model.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/huggingfaceserver/README.md#2025-04-21_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: serving.kserve.io/v1beta1\nkind: InferenceService\nmetadata:\n  name: huggingface-llama3.1\nspec:\n  predictor:\n    model:\n      modelFormat:\n        name: huggingface\n      args:\n      - --model_name=llama3.1\n      - --model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      resources:\n        limits:\n          cpu: \"16\"\n          memory: 32Gi\n        requests:\n          cpu: \"16\"\n          memory: 32Gi\n```\n\n----------------------------------------\n\nTITLE: Uploading model to S3 storage\nDESCRIPTION: Shell command to recursively upload the prepared model repository to an S3 bucket for access by KServe.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/triton/fastertransformer/README.md#2025-04-21_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\naws s3 cp --recursive model-repo s3://example-bucket/bloom-ft-example\n```\n\n----------------------------------------\n\nTITLE: Configuring Splitter Node YAML Router\nDESCRIPTION: Defines a router configuration that splits traffic between two inference services with specified weights\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/graph/README.md#2025-04-21_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nroot:\n  routerType: Splitter \n  routes:\n  - serviceName: sklearn-iris\n    weight: 20\n  - serviceName: xgboost-iris\n    weight: 80\n```\n\n----------------------------------------\n\nTITLE: Creating KServe InferenceService for Cifar10\nDESCRIPTION: This snippet defines a KServe InferenceService resource for the Cifar10 image classification model. It specifies the location of the TensorFlow model in Google Cloud Storage (`storageUri`) and configures a logger to send requests to the Knative Broker. The InferenceService exposes the Cifar10 model for prediction requests.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/drift-detection/alibi-detect/cifar10/cifar10_drift.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n\"%%writefile cifar10.yaml\napiVersion: \\\"serving.kserve.io/v1beta1\\\"\nkind: \\\"InferenceService\\\"\nmetadata:\n  name: \\\"tfserving-cifar10\\\"\n  namespace: cifar10\nspec:\n  predictor:\n    tensorflow:\n      storageUri: \\\"gs://seldon-models/tfserving/cifar10/resnet32\\\"\n    logger:\n      mode: all\n      url: http://broker-ingress.knative-eventing.svc.cluster.local/cifar10/default\"\n```\n\n----------------------------------------\n\nTITLE: MLServer Model Settings Configuration\nDESCRIPTION: JSON configuration for MLServer defining the model name, version, and implementation details.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/sklearn/v2/README.md#2025-04-21_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"sklearn-iris\",\n  \"version\": \"v1.0.0\",\n  \"implementation\": \"mlserver_sklearn.SKLearnModel\"\n}\n```\n\n----------------------------------------\n\nTITLE: Installing KServe SDK with storage support using pip\nDESCRIPTION: Command to install the KServe Python SDK with additional storage support dependencies using pip package manager.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/README.md#2025-04-21_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\npip install kserve[storage]\n```\n\n----------------------------------------\n\nTITLE: Sending Explanation Request to Model\nDESCRIPTION: Constructs a JSON payload for explanation from the model based on a test sample and sends an HTTP POST request to the model's explanation endpoint to retrieve interpretability outputs.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/alibi/cifar10/cifar10_explanations.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ntest_example = X_test[idx : idx + 1].tolist()\npayload = '{\"instances\":' + f\"{test_example}\" + \" }\"\ncmd = f\"\"\"curl -s -d '{payload}' \\\n   http://{CLUSTER_IP}/v1/models/cifar10:explain \\\n   -H \\\"Host: {SERVICE_HOSTNAME}\\\" \\\n   -H \\\"Content-Type: application/json\\\"\n\"\"\"\nret = Popen(cmd, shell=True, stdout=PIPE)\nraw = ret.stdout.read().decode(\"utf-8\")\nexplanation = json.loads(raw)\narr = np.array(explanation[\"data\"][\"anchor\"])\nplt.imshow(arr)\n```\n\n----------------------------------------\n\nTITLE: Predict with LightGBM using Python Requests\nDESCRIPTION: Uses Python requests to send a prediction request to the running LightGBM server. A JSON formatted input is submitted, and the resulting prediction is printed. Requires the requests library and assumes the server is running locally.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/lightgbm/README.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\nrequest = {'sepal_width_(cm)': {0: 3.5}, 'petal_length_(cm)': {0: 1.4}, 'petal_width_(cm)': {0: 0.2},'sepal_length_(cm)': {0: 5.1} }\nformData = {\n    'inputs': [request]\n}\nres = requests.post('http://localhost:8080/v1/models/lgb:predict', json=formData)\nprint(res)\nprint(res.text)\n\n```\n\n----------------------------------------\n\nTITLE: Promote canary model - KServe\nDESCRIPTION: This YAML configuration promotes the canary model by removing the `canaryTrafficPercent` field. This directs all traffic to the latest revision, effectively making the canary the primary model.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/rollout/README.md#2025-04-21_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"my-model\"\nspec:\n  predictor:\n    tensorflow:\n      storageUri: \"gs://kfserving-examples/models/tensorflow/flowers-2\"\n```\n\n----------------------------------------\n\nTITLE: Requesting Captum Explanations for BERT Model Predictions\nDESCRIPTION: Commands to request model explanations using Captum Insights, showing word importances and attributions for the predictions made by the BERT model.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v1/bert/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nMODEL_NAME=torchserve-bert\nSERVICE_HOSTNAME=$(kubectl get inferenceservice ${MODEL_NAME} -n <namespace> -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\n\ncurl -v -H \"Host: ${SERVICE_HOSTNAME}\" http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/BERTSeqClassification:explain -d ./sample_text.txt\n```\n\n----------------------------------------\n\nTITLE: Sample V2 Protocol Inference Request\nDESCRIPTION: Example JSON payload following the V2 Dataplane protocol for model inference.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/sklearn/v2/README.md#2025-04-21_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"inputs\": [\n    {\n      \"name\": \"input-0\",\n      \"shape\": [2, 4],\n      \"datatype\": \"FP32\",\n      \"data\": [\n        [6.8, 2.8, 4.8, 1.4],\n        [6.0, 3.4, 4.5, 1.6]\n      ]\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Running Prediction on Tensorflow Model with Curl\nDESCRIPTION: Uses curl to make a prediction request to the tensorflow model deployed with KServe. Requires the model name and input path.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/storage/uri/README.md#2025-04-21_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nMODEL_NAME=tensorflow-from-uri\nINPUT_PATH=@./input.json\ncurl -v -H \"Host: ${SERVICE_HOSTNAME}\" http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/$MODEL_NAME:predict -d $INPUT_PATH\n```\n\n----------------------------------------\n\nTITLE: Create InferenceService initial model - KServe\nDESCRIPTION: This YAML configuration defines an InferenceService resource in KServe. It specifies the initial model to be served using TensorFlow and its storage URI. This is the first step in setting up a canary deployment.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/rollout/README.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"my-model\"\nspec:\n  predictor:\n    tensorflow:\n      storageUri: \"gs://kfserving-examples/models/tensorflow/flowers\"\n```\n\n----------------------------------------\n\nTITLE: Defining and Running TensorFlow KServe Pipeline\nDESCRIPTION: This code defines a Kubeflow Pipeline for deploying a TensorFlow model using KServe. It uses the `kserve_op` component to define the KServe deployment and sets parameters such as the model name, model URI, namespace, and framework. It compiles and runs the pipeline, deploying the specified TensorFlow model.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/pipelines/kfs-pipeline.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n\"# kfserving_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/master/components/kubeflow/kfserving/component.yaml')\nkserve_op = components.load_component_from_url(\n    \\\"https://raw.githubusercontent.com/kubeflow/pipelines/master/components/kserve/component.yaml\\\"\n)\n\n\n@dsl.pipeline(name=\\\"KServe pipeline\\\", description=\\\"A pipeline for KServe.\\\")\ndef kservePipeline(\n    action=\\\"apply\\\",\n    model_name=\\\"tensorflow-sample\\\",\n    model_uri=\\\"gs://kfserving-examples/models/tensorflow/flowers\\\",\n    namespace=\\\"anonymous\\\",\n    framework=\\\"tensorflow\\\",\n):\n\n    kserve = kserve_op(\n        action=action,\n        model_name=model_name,\n        model_uri=model_uri,\n        namespace=namespace,\n        framework=framework,\n    ).set_image_pull_policy(\\\"Always\\\")\n\n\n# Compile pipeline\ncompiler.Compiler().compile(kservePipeline, \\\"tf-flower.tar.gz\\\")\n\n# Execute pipeline\nrun = client.run_pipeline(experiment.id, \\\"tf-flower\\\", \\\"tf-flower.tar.gz\\\")\"\n```\n\n----------------------------------------\n\nTITLE: Training SKLearn SVM Classifier with Iris Dataset\nDESCRIPTION: Trains a Support Vector Machine classifier using scikit-learn on the iris dataset and saves the model to disk using joblib.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/sklearn/v2/README.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn import svm\nfrom sklearn import datasets\nfrom joblib import dump\n\niris = datasets.load_iris()\nX, y = iris.data, iris.target\n\nclf = svm.SVC(gamma='scale')\nclf.fit(X, y)\n\ndump(clf, 'model.joblib')\n```\n\n----------------------------------------\n\nTITLE: Deploying Initial InferenceService with SKLearn Model\nDESCRIPTION: Creates a Kubernetes InferenceService using a pre-trained SKLearn Iris model from Google Cloud Storage, defining model format and storage URI\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/canary-testing/README.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"sklearn-iris\"\nspec:\n  predictor:\n    model:\n      modelFormat:\n        name: sklearn\n      storageUri: \"gs://kfserving-examples/models/sklearn/1.0/model\"\n```\n\n----------------------------------------\n\nTITLE: Defining and deploying KServe InferenceService for BLOOM\nDESCRIPTION: YAML configuration applied via kubectl to create an InferenceService that integrates the predictor using the Triton FasterTransformer backend and a custom transformer for handling input/output processing.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/triton/fastertransformer/README.md#2025-04-21_snippet_10\n\nLANGUAGE: sh\nCODE:\n```\nkubectl apply -f - << EOF\napiVersion: serving.kserve.io/v1beta1\nkind: InferenceService\nmetadata:\n  name: bloom-560m\nspec:\n  predictor:\n    model:\n      modelFormat:\n        name: triton\n        version: \"2\"\n      name: \"\"\n      resources:\n        limits:\n          cpu: \"1\"\n          memory: 8Gi\n          nvidia.com/gpu: \"1\"\n        requests:\n          cpu: \"1\"\n          memory: 4Gi\n          nvidia.com/gpu: \"1\"\n      runtime: tritonserver-ft\n      storageUri: s3://example-bucket/bloom-ft-example\n  transformer:\n    containers:\n    - args:\n      - --model_name\n      - fastertransformer\n      - --protocol\n      - v2\n      - --tokenizer_path\n      - /mnt/models/\n      command:\n      - python\n      - transformer.py\n      env:\n      - name: STORAGE_URI\n        value: s3://example-bucket/bloom-ft-example/fastertransformer/1/tokenizer\n      image: <my-transformer-image>\n      name: kserve-container\n      resources:\n        limits:\n          cpu: \"1\"\n          memory: 2Gi\n        requests:\n          cpu: \"1\"\n          memory: 2Gi\nEOF\n```\n\n----------------------------------------\n\nTITLE: Enable Prometheus Scraping in Deployment YAML\nDESCRIPTION: This YAML snippet demonstrates how to enable Prometheus scraping for a TorchServe deployment by adding annotations to the deployment YAML file. This allows Prometheus to collect metrics from the TorchServe instance.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/docs/metrics.md#2025-04-21_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\n\"apiVersion: \\\"serving.kserve.io/v1beta1\\\"\nkind: \\\"InferenceService\\\"\nmetadata:\n  name: torchserve-custom\n  annotations:\n    prometheus.io/scrape: 'true'\n    prometheus.io/port: '8082'\nspec:\n  predictor:\n    containers:\n    - image: {username}/torchserve:latest\n      name: torchserve-container\"\n```\n\n----------------------------------------\n\nTITLE: Update InferenceService canary model - KServe\nDESCRIPTION: This YAML configuration updates the InferenceService resource to introduce a canary model. It sets `canaryTrafficPercent` to 10, indicating that 10% of the traffic should be routed to the new model specified by the updated `storageUri`.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/rollout/README.md#2025-04-21_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"my-model\"\nspec:\n  predictor:\n    # 10% of traffic is sent to this model\n    canaryTrafficPercent: 10\n    tensorflow:\n      storageUri: \"gs://kfserving-examples/models/tensorflow/flowers-2\"\n```\n\n----------------------------------------\n\nTITLE: Defining HuggingFace Runtime Container Configuration\nDESCRIPTION: Specification for configuring container runtime parameters for HuggingFace model serving, including image, environment variables, resource allocation, and lifecycle management\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/docs/V1beta1HuggingFaceRuntimeSpec.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass V1beta1HuggingFaceRuntimeSpec:\n    def __init__(self, \n        image=None,\n        command=None,\n        args=None,\n        env=None,\n        resources=None\n    ):\n```\n\n----------------------------------------\n\nTITLE: Creating Tensorflow InferenceService in YAML\nDESCRIPTION: Demonstrates how to create an InferenceService resource in Kubernetes for a tensorflow model, specifying the storage URI for the model tarball.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/storage/uri/README.md#2025-04-21_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: serving.kserve.io/v1beta1\nkind: InferenceService\nmetadata:\n  name: tensorflow-from-uri-gzip\nspec:\n  predictor:\n    tensorflow:\n       storageUri: https://raw.githubusercontent.com/tduffy000/kfserving-uri-examples/master/tensorflow/frozen/model_artifacts.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Creating Request Message for Inference - Python\nDESCRIPTION: This snippet constructs the JSON message required to invoke the prediction service. It defines the input name, shape, data type, and the image data to be sent, compiling these elements into the final message data structure.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/onnx/mosaic-onnx.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\n# Create request message to be sent to the predictor\nmessage_data = {}\ninputs = {}\nmessage_data[\"inputs\"] = []\ninputs[\"name\"] = \"input1\"\ninputs[\"shape\"] = norm_img_data.shape\ninputs[\"datatype\"] = \"FP32\"  # as the given onnx model expects float32\ninputs[\"data\"] = norm_img_data.tolist()\nmessage_data[\"inputs\"].append(inputs)\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Creating an InferenceService for Multi-Model Hosting\nDESCRIPTION: This YAML configuration sets up an InferenceService for Triton in KFServing, allocating resources for hosting multiple models. It includes specifications for CPU and memory requirements.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/multimodelserving/triton/README.md#2025-04-21_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"triton-mms\"\nspec:\n  predictor:\n    triton:\n      args:\n      - --log-verbose=1\n      resources:\n        limits:\n          cpu: \"1\"\n          memory: 2Gi\n        requests:\n          cpu: \"1\"\n          memory: 2Gi\n```\n\n----------------------------------------\n\nTITLE: Testing Deployed Model with curl\nDESCRIPTION: Bash commands to extract the service hostname and make an inference request to the deployed XGBoost model using curl, sending the input data in the V2 protocol format.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/xgboost/README.md#2025-04-21_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nSERVICE_HOSTNAME=$(kubectl get inferenceservice xgboost-iris -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\n\ncurl -v \\\n  -H \"Host: ${SERVICE_HOSTNAME}\" \\\n  -d @./iris-input.json \\\n  http://${INGRESS_HOST}:${INGRESS_PORT}/v2/models/xgboost-iris/infer\n```\n\n----------------------------------------\n\nTITLE: Sending Model Explanation Request with Curl\nDESCRIPTION: Curl command to send an explanation request to a deployed machine learning model via KServe, using specified service hostname and input data\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v1/README.md#2025-04-21_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ncurl -v -H \"Host: ${SERVICE_HOSTNAME}\" http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/mnist:explain -d @./mnist.json\n```\n\n----------------------------------------\n\nTITLE: Training the Model (Python)\nDESCRIPTION: This snippet trains the model using the pipeline defined earlier and the selected features and target variable. The `fit` method of the pipeline is used to train the model.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/sklearn/v1/sklearn-mixedtype-model/sklearn-mixedtype-model.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmodel = p.fit(df, y)\n```\n\n----------------------------------------\n\nTITLE: Applying InferenceService Configuration using kubectl\nDESCRIPTION: This snippet demonstrates applying a PaddleServer InferenceService configuration using kubectl to set up an image classification service. It requires KServe to be installed on your cluster and the Istio Ingress gateway to be accessible. The input is a YAML configuration file (paddle.yaml) and the expected output is the creation of an InferenceService in the cluster.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/paddle/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f paddle.yaml\n```\n\n----------------------------------------\n\nTITLE: Apply KServe InferenceService YAML\nDESCRIPTION: Applies the pmml.yaml file to create a KServe InferenceService. This command deploys the PMML model within the KServe environment.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/pmml/README.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nkubectl apply -f pmml.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing Transformers Package\nDESCRIPTION: Install the Hugging Face transformers library required for BERT model processing and dependencies\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/bert-sample/hugging-face-bert-sample.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install transformers\n```\n\n----------------------------------------\n\nTITLE: Building a BentoML Prediction Service with Scikit-Learn\nDESCRIPTION: This Python code defines an IrisClassifier using BentoML, which serves as a prediction service that utilizes a Scikit-learn model. It specifies the required environment and artifacts for handling input data as a DataFrame.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/bentoml/README.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# iris_classifier.py\nfrom bentoml import env, artifacts, api, BentoService\nfrom bentoml.handlers import DataframeHandler\nfrom bentoml.artifact import SklearnModelArtifact\n\n@env(auto_pip_dependencies=True)\n@artifacts([SklearnModelArtifact('model')])\nclass IrisClassifier(BentoService):\n\n    @api(DataframeHandler)\n    def predict(self, df):\n        return self.artifacts.model.predict(df)\n```\n\n----------------------------------------\n\nTITLE: Running Prediction with Deployed BERT Model on KServe\nDESCRIPTION: Commands to set up environment variables and make an inference request to the deployed BERT model. Includes retrieving the service hostname and sending a prediction request with sample text.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v1/bert/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nMODEL_NAME=torchserve-bert\nSERVICE_HOSTNAME=$(kubectl get inferenceservice ${MODEL_NAME} -n <namespace> -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\n\ncurl -v -H \"Host: ${SERVICE_HOSTNAME}\" http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/BERTSeqClassification:predict -d ./sample_text.txt\n```\n\n----------------------------------------\n\nTITLE: Configuring Inference Batcher in KServe InferenceService\nDESCRIPTION: YAML configuration for setting up an InferenceService with batching enabled. Demonstrates how to configure batch size, latency thresholds, and timeout parameters for a PyTorch model deployment.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/batcher/README.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"pytorch-cifar10\"\nspec:\n  predictor:\n    timeout: 60\n    minReplicas: 1\n    batcher:\n      maxBatchSize: 32\n      maxLatency: 5000\n    pytorch:\n      storageUri: \"gs://kfserving-examples/models/torchserve/image-classifier\"\n```\n\n----------------------------------------\n\nTITLE: Creating Specific ConfigMap for InferenceService - YAML\nDESCRIPTION: This snippet demonstrates how to create a specific ConfigMap for a given inference service, which applies a CA bundle only for that service to enhance security.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/cabundle/README.md#2025-04-21_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nkubectl create configmap local-cabundle --from-file=/path/to/cabundle.crt\n\nkubectl get configmap cabundle -o yaml\napiVersion: v1\ndata:\n  cabundle.crt: XXXXX\nkind: ConfigMap\nmetadata:\n  name: local-cabundle\n  namespace: kserve-demo\n```\n\n----------------------------------------\n\nTITLE: Get Cluster IP\nDESCRIPTION: This snippet retrieves the cluster IP address using `kubectl` and stores it in the `CLUSTER_IP` variable. This IP address is likely used to access the inference service.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/alibi/moviesentiment/movie_review_explanations.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n\"CLUSTER_IPS = !(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\\nCLUSTER_IP = CLUSTER_IPS[0]\\nprint(CLUSTER_IP)\"\n```\n\n----------------------------------------\n\nTITLE: Canary Deployment with KServe - Promotion\nDESCRIPTION: Final step in a successful canary deployment, promoting the candidate model to receive 100% of traffic. This effectively replaces the original model with the new version.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/kfp/README.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nkserve_op(\n    action='apply',\n    model_name='tf-sample',\n    model_uri='gs://kfserving-examples/models/tensorflow/flowers-2',\n    framework='tensorflow'\n)\n```\n\n----------------------------------------\n\nTITLE: Loading KServe Component in Kubeflow Pipelines\nDESCRIPTION: Demonstrates how to import and load the KServe component from a URL to use in a Kubeflow Pipeline. This is the initial setup required before creating a model deployment.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/kfp/README.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport kfp.dsl as dsl\nimport kfp\nfrom kfp import components\n\nkserve_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/master/components/kserve/component.yaml')\n```\n\n----------------------------------------\n\nTITLE: Creating and Saving a SVM Model with Scikit-Learn in Python\nDESCRIPTION: This code snippet demonstrates how to create a Support Vector Machine (SVM) model using Scikit-Learn, train it with the Iris dataset, and save the trained model to a file using joblib. Requires the `scikit-learn` and `joblib` libraries.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/sklearn/v1/README.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn import svm\nfrom sklearn import datasets\nfrom joblib import dump\nclf = svm.SVC(gamma='scale')\niris = datasets.load_iris()\nX, y = iris.data, iris.target\nclf.fit(X, y)\ndump(clf, 'model.joblib')\n```\n\n----------------------------------------\n\nTITLE: Sample Input JSON for V2 Protocol Inference\nDESCRIPTION: Example JSON payload formatted according to the V2 Dataplane protocol for making inference requests to the deployed XGBoost model, containing Iris flower measurements as input data.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/xgboost/README.md#2025-04-21_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"inputs\": [\n    {\n      \"name\": \"input-0\",\n      \"shape\": [2, 4],\n      \"datatype\": \"FP32\",\n      \"data\": [\n        [6.8, 2.8, 4.8, 1.4],\n        [6.0, 3.4, 4.5, 1.6]\n      ]\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Sending Image to KFServing in Python\nDESCRIPTION: Submits the image to the KFServing inference service for explanation. Logs the time taken for the request using the requests library to send a POST request. Requires an endpoint and headers specified.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/aix/mnist/query_explain.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nprint(\"Sending Explain Query\")\n\nx = time.time()\n\nres = requests.post(endpoint, json=input_image, headers=headers)\n\nprint(\"TIME TAKEN: \", time.time() - x)\n```\n\n----------------------------------------\n\nTITLE: Configuring PyTorch InferenceService with Batcher - YAML Configuration\nDESCRIPTION: Defines a KServe InferenceService configuration for PyTorch model with batching parameters. Sets maxBatchSize to 32 and maxLatency to 5000ms for observable batching behavior.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/batcher/basic/README.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"pytorch-batcher\"\nspec:\n  predictor:\n    minReplicas: 1\n    timeout: 60\n    batcher:\n      maxBatchSize: 32\n      maxLatency: 5000\n    pytorch:\n      storageUri: \"gs://kfserving-examples/models/torchserve/image_classifier/v1\"\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries (Python)\nDESCRIPTION: This snippet imports the necessary libraries for data manipulation, preprocessing, model building, and model persistence. It includes pandas for data handling, scikit-learn for building pipelines and preprocessing data, and joblib for saving the model. Also includes custom transformer.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/sklearn/v1/sklearn-mixedtype-model/sklearn-mixedtype-model.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.linear_model import SGDRegressor\nfrom custom_transformer import DictToDFTransformer\nimport joblib\n```\n\n----------------------------------------\n\nTITLE: Downloading BLOOM model conversion script\nDESCRIPTION: Shell command to download the HuggingFace to FasterTransformer conversion script from NVIDIA's repository.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/triton/fastertransformer/README.md#2025-04-21_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\ncurl -LO https://raw.githubusercontent.com/NVIDIA/FasterTransformer/main/examples/pytorch/gpt/utils/huggingface_bloom_convert.py\n```\n\n----------------------------------------\n\nTITLE: Deploying Custom HuggingFace Serving Runtime\nDESCRIPTION: Deploy a modified HuggingFace serving runtime for sequential loading of safetensors weights.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/fluid/README.md#2025-04-21_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create -f kserve-huggingfaceserver.yaml\n```\n\n----------------------------------------\n\nTITLE: Apply InferenceService update - Kubernetes CLI\nDESCRIPTION: This command applies the Kubernetes resource defined in the `canary.yaml` file.  This applies the updated InferenceService with the canary configuration to the cluster, routing a portion of traffic to the canary model.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/rollout/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f canary.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining InferenceService with Pod Identity - YAML\nDESCRIPTION: This YAML configuration sets up an InferenceService in KServe that uses a service account and associates a Managed Identity through Pod Identity for secure access to Azure Blob Storage. The configuration includes the storage URI where the model is located.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/storage/azure/README.md#2025-04-21_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n---\napiVersion: serving.kserve.io/v1beta1\nkind: InferenceService\nmetadata:\n  name: kserve-simple-string\n  labels:\n    aadpodidbinding: piselector\nspec:\n  template:\n    metadata:\n  predictor:\n    serviceAccountName: sa\n    tensorflow:\n      storageUri: \"https://kserve.blob.core.windows.net/triton/simple_string/\"\n```\n\n----------------------------------------\n\nTITLE: Running HuggingFace Server Locally with Python\nDESCRIPTION: Command to start a local HuggingFace server using the bert-base-uncased model. The server loads the model and makes it available for inference requests.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/huggingfaceserver/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m huggingfaceserver --model_id=bert-base-uncased --model_name=bert\n```\n\n----------------------------------------\n\nTITLE: Model Metadata Response JSON Schema\nDESCRIPTION: Defines the structure for a successful model metadata response, including model name, versions, platform, input and output tensors\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md#2025-04-21_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\" : $string,\n  \"versions\" : [ $string, ... ] #optional,\n  \"platform\" : $string,\n  \"inputs\" : [ $metadata_tensor, ... ],\n  \"outputs\" : [ $metadata_tensor, ... ]\n}\n```\n\n----------------------------------------\n\nTITLE: Calling the Inference Model - Python\nDESCRIPTION: This snippet sends the prepared request message to the ONNX InferenceService. It gathers necessary environment variables for building the request URL, sets the required headers, and executes a POST request to the service, handling the response status code.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/onnx/mosaic-onnx.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\n# Call predictor\n\nservice_hostname = os.environ[\"SERVICE_HOSTNAME\"]\nmodel_name = os.environ[\"MODEL_NAME\"]\ningress_ip = \"localhost\"\ningress_port = os.environ[\"INGRESS_PORT\"]\npredictor_url = f\"http://{ingress_ip}:{ingress_port}/v2/models/{model_name}/infer\"\nrequest_headers = {\n    \"Content-Type\": \"application/json\",\n    \"Accept\": \"application/json\",\n    \"Host\": service_hostname,\n}\nresponse = requests.post(\n    predictor_url, headers=request_headers, data=json.dumps(message_data)\n)\nprint(response.status_code)\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Switch Node in KServe Inference Graph\nDESCRIPTION: A YAML configuration for a Switch Node that conditionally routes requests to different InferenceServices based on matching conditions using GJSON syntax for JSON path expressions.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/graph/README.md#2025-04-21_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n...\nroot:\n  routerType: Switch\n  steps:\n  - serviceUrl: http://single-1.default.{$your-domain}/switch\n    condition: \"[@this].#(source==client)\" #object matching\n  - serviceUrl: http://single-2.default.{$your-domain}/switch\n    condition: \"instances.#(intval>10)\" #array matching\n  - serviceUrl: http://single-3.default.{$your-domain}/switch \n    condition: \"instances.#(strval%*red-server*)\" #pattern matching\n...\n```\n\n----------------------------------------\n\nTITLE: Deploying BERT Model InferenceService on KServe\nDESCRIPTION: Command to apply the YAML configuration to create an InferenceService for the BERT model in a Kubernetes cluster.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v1/bert/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f bert.yaml\n```\n\n----------------------------------------\n\nTITLE: Deploying and Testing Switch Node in KServe\nDESCRIPTION: Shell commands for deploying and testing a Switch Node in KServe, which routes requests to different services based on specified conditions, with expected output showing conditional routing.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/graph/README.md#2025-04-21_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nkubectl apply -f switch.yaml\n```\n\nLANGUAGE: shell\nCODE:\n```\nkubectl get pods\nNAME                                                           READY   STATUS    RESTARTS   AGE\nmodel-switch-00001-deployment-856876dfc8-d66cn                 2/2     Running   0          13m\nsingle-1-predictor-default-00001-deployment-9fb5b49d4-9zx4h    2/2     Running   0          66m\nsingle-2-predictor-default-00001-deployment-f44d84d54-f58t8    2/2     Running   0          66m\nsingle-3-predictor-default-00001-deployment-6446f55849-tgvpq   2/2     Running   0          66m              \n\nkubectl get isvc\nNAME       URL                                             READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                AGE\nsingle-1   http://single-1.default.10.166.15.29.sslip.io   True           100                              single-1-predictor-default-00001   67m\nsingle-2   http://single-2.default.10.166.15.29.sslip.io   True           100                              single-2-predictor-default-00001   67m\nsingle-3   http://single-3.default.10.166.15.29.sslip.io   True           100                              single-3-predictor-default-00001   67m\n\nkubectl get ig\nNAME            URL                                                  READY   AGE\nmodel-switch     http://model-switch.default.10.166.15.29.sslip.io     True    35s\n```\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://model-switch.default.10.166.15.29.sslip.io -d '{\"source\":\"client1\",\"instances\":[{\"name\":\"blue\",\"intval\":0,\"strval\":\"kserve\"},{\"name\":\"green\",\"intval\":1,\"strval\":\"1red-server1\"}]}'\n```\n\n----------------------------------------\n\nTITLE: Deploying InferenceService using Kubernetes\nDESCRIPTION: This snippet creates an InferenceService for a Scikit-learn model on Kubernetes. It defines the necessary API version, kind, metadata, and model specifications, including the storage URI.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/performance-testing/README.md#2025-04-21_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncat <<EOF | kubectl apply -f -\napiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"sklearn-irisv2\"\nspec:\n  predictor:\n    model:\n      modelFormat:\n        name: sklearn\n      runtime: kserve-mlserver\n      storageUri: \"gs://seldon-models/sklearn/mms/lr_model\"\nEOF\n```\n\n----------------------------------------\n\nTITLE: Loading and Normalizing CIFAR10 Data\nDESCRIPTION: Loads the CIFAR10 dataset using TensorFlow and normalizes the image data by scaling pixel values to a range of 0-1. Prepares the data for use in model prediction.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/alibi/cifar10/cifar10_explanations.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\ntrain, test = tf.keras.datasets.cifar10.load_data()\nX_train, y_train = train\nX_test, y_test = test\n\nX_train = X_train.astype(\"float32\") / 255\nX_test = X_test.astype(\"float32\") / 255\nprint(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\nclass_names = [\n    \"airplane\",\n    \"automobile\",\n    \"bird\",\n    \"cat\",\n    \"deer\",\n    \"dog\",\n    \"frog\",\n    \"horse\",\n    \"ship\",\n    \"truck\",\n]\n```\n\n----------------------------------------\n\nTITLE: Rollback and pin model - KServe\nDESCRIPTION: This YAML configuration rolls back the model to the previous good model by setting `canaryTrafficPercent` to 0. This directs all traffic to the previously rolled-out revision.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/rollout/README.md#2025-04-21_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"my-model\"\nspec:\n  predictor:\n    canaryTrafficPercent: 0\n    tensorflow:\n      storageUri: \"gs://kfserving-examples/models/tensorflow/flowers-2\"\n```\n\n----------------------------------------\n\nTITLE: Creating InferenceService for SKLearn\nDESCRIPTION: This YAML snippet creates an InferenceService named 'sklearn-iris-example', which serves SKLearn models. It defines resource limits for CPU and memory. Ensure KServe is configured for multi-model serving.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/multimodelserving/sklearn/README.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n\"apiVersion: \\\"serving.kserve.io/v1beta1\\\"\\nkind: \\\"InferenceService\\\"\\nmetadata:\\n  name: \\\"sklearn-iris-example\\\"\\nspec:\\n  predictor:\\n    minReplicas: 1\\n    sklearn:\\n      protocolVersion: v1\\n      name: \\\"sklearn-iris-predictor\\\"\\n      resources:\\n        limits:\\n          cpu: 100m\\n          memory: 512Mi\\n        requests:\\n          cpu: 100m\\n          memory: 512Mi\"\n```\n\n----------------------------------------\n\nTITLE: Generating Torchserve Model Archive File\nDESCRIPTION: Command to create a model archive file for the BERT sequence classification model using torch-model-archiver\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/bert-sample/hugging-face-bert-sample.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ntorch-model-archiver --model-name BERTSeqClassification --version 1.0 --serialized-file Transformer_model/pytorch_model.bin --handler ./Transformer_handler_generalized.py --extra-files \"Transformer_model/config.json,./setup_config.json,./Seq_classification_artifacts/index_to_name.json\" -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Building Custom Docker Image for XGBoost Server\nDESCRIPTION: Command to build a custom Docker image for the XGBoost server. This should be run from the 'python' directory, one level up from the current directory.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/xgbserver/README.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t docker_user_name/xgbserver -f xgb.Dockerfile .\n```\n\n----------------------------------------\n\nTITLE: Making a Prediction (Python)\nDESCRIPTION: This snippet demonstrates how to make a prediction using the trained model. It defines a sample request as a list of dictionaries and then uses the model's `predict` method to generate a response.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/sklearn/v1/sklearn-mixedtype-model/sklearn-mixedtype-model.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nrequest = [\n    {\n        \"MSZoning\": \"RL\",\n        \"LotArea\": 8450,\n        \"LotShape\": \"Reg\",\n        \"Utilities\": \"AllPub\",\n        \"YrSold\": 2008,\n        \"Neighborhood\": \"CollgCr\",\n        \"OverallQual\": 7,\n        \"YearBuilt\": 2003,\n        \"SaleType\": \"WD\",\n        \"GarageArea\": 548,\n    }\n]\nresponse = model.predict(sample_request)\n```\n\n----------------------------------------\n\nTITLE: Creating Model Archive File for BERT Sequence Classification with TorchServe\nDESCRIPTION: Command to create a model archive file (MAR) for BERT sequence classification model using torch-model-archiver, packaging the serialized model with custom handler and configuration files.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v2/bert/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ntorch-model-archiver --model-name BERTSeqClassification --version 1.0 \\\n--serialized-file Transformer_model/pytorch_model.bin \\\n--handler ./Transformer_kserve_handler.py \\\n--extra-files \"Transformer_model/config.json,./setup_config.json,./Seq_classification_artifacts/index_to_name.json,./Transformer_handler_generalized.py\"\n```\n\n----------------------------------------\n\nTITLE: Checking InferenceService Status\nDESCRIPTION: Retrieve and watch the status of the created InferenceService with a timeout\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/client/kfserving_sdk_v1beta1_sample.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nKFServing.get(\"flower-sample\", namespace=namespace, watch=True, timeout_seconds=120)\n```\n\n----------------------------------------\n\nTITLE: Applying InferenceService CRD (Custom Resource Definition) using kubectl\nDESCRIPTION: This snippet demonstrates how to apply a custom YAML file to create the InferenceService in a Kubernetes cluster using kubectl. This is a prerequisite for running predictions on the model.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/prebuilt-image/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f custom.yaml\n```\n\n----------------------------------------\n\nTITLE: Making Predictions using a Trained Model via HTTP in Python\nDESCRIPTION: This Python snippet demonstrates how to make a prediction by sending an HTTP POST request to the SKLearn server with an example input taken from the Iris dataset. It uses the `requests` library for making the request.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/sklearn/v1/README.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn import datasets\nimport requests\niris = datasets.load_iris()\nX, y = iris.data, iris.target\nformData = {\n    'instances': X[0:1].tolist()\n}\nres = requests.post('http://localhost:8080/v1/models/svm:predict', json=formData)\nprint(res)\nprint(res.text)\n```\n\n----------------------------------------\n\nTITLE: Creating TorchServe Model Archive\nDESCRIPTION: Command to generate a Model Archive (MAR) file for MNIST model using torch-model-archiver utility, specifying model files, handlers, and versioning.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ntorch-model-archiver --model-name mnist --version 1.0 \\\n--model-file model-archiver/model-store/mnist/mnist.py \\\n--serialized-file model-archiver/model-store/mnist/mnist_cnn.pt \\\n--handler model-archiver/model-store/mnist/mnist_handler.py\n```\n\n----------------------------------------\n\nTITLE: Chaining Multiple Models in Sequence for Inference\nDESCRIPTION: This YAML snippet describes an InferenceGraph named 'model-chainer' that chains multiple models in sequence. It allows the output of one model to be passed as input to subsequent models, maintaining a structured flow of data through the inference pipeline.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/docs/V1alpha1InferenceRouter.md#2025-04-21_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nkind: InferenceGraph\nmetadata:\n  name: model-chainer\nspec:\n  nodes:\n    root:\n      routerType: Sequence\n      routes:\n      - service: mymodel-s1\n      - service: mymodel-s2\n        data: $response\n      - service: mymodel-s3\n        data: $response\n```\n\n----------------------------------------\n\nTITLE: Training and Saving the Iris Classifier Model\nDESCRIPTION: This Python code snippet trains a Support Vector Classifier on the iris dataset using Scikit-learn, packs the trained model into the IrisClassifier service, and saves it for serving. It demonstrates the process of preparing a model for prediction in BentoML.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/bentoml/README.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# main.py\nfrom sklearn import svm\nfrom sklearn import datasets\n\nfrom iris_classifier import IrisClassifier\n\nif __name__ == '__main__':\n    # Load training data\n    iris = datasets.load_iris()\n    X, y = iris.data, iris.target\n\n    # Model Training\n    clf = svm.SVC(gamma='scale')\n    clf.fit(X, y)\n\n    # Create a iris classifier service instance\n    iris_classifier_service = IrisClassifier()\n\n    # Pack the newly trained model artifact\n    iris_classifier_service.pack('model', clf)\n\n    # Save the prediction service to disk for model serving\n    saved_path = iris_classifier_service.save()\n```\n\n----------------------------------------\n\nTITLE: Training XGBoost Model with Iris Dataset in Python\nDESCRIPTION: Python script to train an XGBoost model using the Iris dataset and save it as 'model.bst'. The model is configured with multi-class classification parameters and trained on the full Iris dataset.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/xgboost/README.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport xgboost as xgb\nfrom sklearn.datasets import load_iris\nimport os\n\nmodel_dir = \".\"\nBST_FILE = \"model.bst\"\n\niris = load_iris()\ny = iris['target']\nX = iris['data']\ndtrain = xgb.DMatrix(X, label=y)\nparam = {'max_depth': 6,\n            'eta': 0.1,\n            'silent': 1,\n            'nthread': 4,\n            'num_class': 10,\n            'objective': 'multi:softmax'\n            }\nxgb_model = xgb.train(params=param, dtrain=dtrain)\nmodel_file = os.path.join((model_dir), BST_FILE)\nxgb_model.save_model(model_file)\n```\n\n----------------------------------------\n\nTITLE: Training and Packing Tensorflow Model in Python\nDESCRIPTION: Trains a tensorflow model and saves it in a required directory structure before packing it into a tarball. Requires tensorflow library.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/storage/uri/README.md#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn import datasets\nimport numpy as np\nimport tensorflow as tf\n\ndef _ohe(targets):\n    y = np.zeros((150, 3))\n    for i, label in enumerate(targets):\n        y[i, label] = 1.0\n    return y\n\ndef train(X, y, epochs, batch_size=16):\n    model = tf.keras.Sequential([\n        tf.keras.layers.InputLayer(input_shape=(4,)),\n        tf.keras.layers.Dense(16, activation=tf.nn.relu),\n        tf.keras.layers.Dense(16, activation=tf.nn.relu),\n        tf.keras.layers.Dense(3, activation='softmax')\n    ])\n    model.compile(tf.keras.optimizers.RMSprop(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n    model.fit(X, y, epochs=epochs)\n    return model\n\ndef freeze(model, path='../frozen'):\n    model.save(f'{path}/0001')\n    return True\n\nif __name__ == '__main__':\n    iris = datasets.load_iris()\n    X, targets = iris.data, iris.target\n    y = _ohe(targets)\n    model = train(X, y, epochs=50)\n    freeze(model)\n```\n\n----------------------------------------\n\nTITLE: Implementing Dex Authentication Session Function\nDESCRIPTION: Python function that obtains a session cookie by authenticating with Dex. This supports both staticPasswords and LDAP authentication methods, and returns authentication session details including cookies needed for authenticated requests.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/istio-dex/README.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport re\nfrom urllib.parse import urlsplit\nimport requests\n\ndef get_istio_auth_session(url: str, username: str, password: str) -> dict:\n    \"\"\"\n    Determine if the specified URL is secured by Dex and try to obtain a session cookie.\n    WARNING: only Dex `staticPasswords` and `LDAP` authentication are currently supported\n             (we default to using `staticPasswords` if both are enabled)\n\n    :param url: Kubeflow server URL, including protocol\n    :param username: Dex `staticPasswords` or `LDAP` username\n    :param password: Dex `staticPasswords` or `LDAP` password\n    :return: auth session information\n    \"\"\"\n    # define the default return object\n    auth_session = {\n        \"endpoint_url\": url,  # KF endpoint URL\n        \"redirect_url\": None,  # KF redirect URL, if applicable\n        \"dex_login_url\": None,  # Dex login URL (for POST of credentials)\n        \"is_secured\": None,  # True if KF endpoint is secured\n        \"session_cookie\": None,  # Resulting session cookies in the form \"key1=value1; key2=value2\"\n    }\n\n    # use a persistent session (for cookies)\n    with requests.Session() as s:\n        ################\n        # Determine if Endpoint is Secured\n        ################\n        resp = s.get(url, allow_redirects=True)\n        if resp.status_code != 200:\n            raise RuntimeError(\n                f\"HTTP status code '{resp.status_code}' for GET against: {url}\"\n            )\n\n        auth_session[\"redirect_url\"] = resp.url\n\n        # if we were NOT redirected, then the endpoint is UNSECURED\n        if len(resp.history) == 0:\n            auth_session[\"is_secured\"] = False\n            return auth_session\n        else:\n            auth_session[\"is_secured\"] = True\n\n        ################\n        # Get Dex Login URL\n        ################\n        redirect_url_obj = urlsplit(auth_session[\"redirect_url\"])\n\n        # if we are at `/auth?=xxxx` path, we need to select an auth type\n        if re.search(r\"/auth$\", redirect_url_obj.path):\n            #######\n            # TIP: choose the default auth type by including ONE of the following\n            #######\n\n            # OPTION 1: set \"staticPasswords\" as default auth type\n            redirect_url_obj = redirect_url_obj._replace(\n                path=re.sub(r\"/auth$\", \"/auth/local\", redirect_url_obj.path)\n            )\n            # OPTION 2: set \"ldap\" as default auth type\n            # redirect_url_obj = redirect_url_obj._replace(\n            #     path=re.sub(r\"/auth$\", \"/auth/ldap\", redirect_url_obj.path)\n            # )\n\n        # if we are at `/auth/xxxx/login` path, then no further action is needed (we can use it for login POST)\n        if re.search(r\"/auth/.*/login$\", redirect_url_obj.path):\n            auth_session[\"dex_login_url\"] = redirect_url_obj.geturl()\n\n        # else, we need to be redirected to the actual login page\n        else:\n            # this GET should redirect us to the `/auth/xxxx/login` path\n            resp = s.get(redirect_url_obj.geturl(), allow_redirects=True)\n            if resp.status_code != 200:\n                raise RuntimeError(\n                    f\"HTTP status code '{resp.status_code}' for GET against: {redirect_url_obj.geturl()}\"\n                )\n\n            # set the login url\n            auth_session[\"dex_login_url\"] = resp.url\n\n        ################\n        # Attempt Dex Login\n        ################\n        resp = s.post(\n            auth_session[\"dex_login_url\"],\n            data={\"login\": username, \"password\": password},\n            allow_redirects=True,\n        )\n        if len(resp.history) == 0:\n            raise RuntimeError(\n                f\"Login credentials were probably invalid - \"\n                f\"No redirect after POST to: {auth_session['dex_login_url']}\"\n            )\n\n        # store the session cookies in a \"key1=value1; key2=value2\" string\n        auth_session[\"session_cookie\"] = \"; \".join(\n            [f\"{c.name}={c.value}\" for c in s.cookies]\n        )\n        auth_session[\"authservice_session\"] = s.cookies.get(\"authservice_session\")\n\n    return auth_session\n```\n\n----------------------------------------\n\nTITLE: Importing KFServing SDK Dependencies\nDESCRIPTION: Import necessary Kubernetes and KFServing client libraries for creating and managing InferenceService resources\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/client/kfserving_sdk_v1beta1_sample.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom kubernetes import client\nfrom kfserving import KFServingClient\nfrom kfserving import constants\nfrom kfserving import utils\nfrom kfserving import V1beta1InferenceService\nfrom kfserving import V1beta1InferenceServiceSpec\nfrom kfserving import V1beta1PredictorSpec\nfrom kfserving import V1beta1TFServingSpec\n```\n\n----------------------------------------\n\nTITLE: Run prediction against InferenceService - Curl\nDESCRIPTION: This set of commands sets up the environment variables and then sends a prediction request to the InferenceService using curl. It first retrieves the service hostname, and then sends a POST request to the predict endpoint.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/rollout/README.md#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nMODEL_NAME=my-model\nINPUT_PATH=@./input.json\nSERVICE_HOSTNAME=$(kubectl get isvc ${MODEL_NAME} -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\n\ncurl -v -H \"Host: ${SERVICE_HOSTNAME}\" http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/$MODEL_NAME:predict -d $INPUT_PATH\n```\n\n----------------------------------------\n\nTITLE: Packing Tensorflow Model Artifacts with tar\nDESCRIPTION: Commands to package a trained tensorflow model into a tarball for deployment with KServe. Necessary for models with multiple files.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/storage/uri/README.md#2025-04-21_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ncd ../frozen\ntar -cvf artifacts.tar 0001/\ngzip < artifacts.tar > artifacts.tgz\n```\n\n----------------------------------------\n\nTITLE: Converting HuggingFace BLOOM model to FasterTransformer format\nDESCRIPTION: Shell commands to install required Python dependencies and convert the BLOOM model from HuggingFace format to FasterTransformer format with tensor parallelism set to 1.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/triton/fastertransformer/README.md#2025-04-21_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\npip3 install numpy transformers torch\npython3 huggingface_bloom_convert.py -o bloom -i ./bloom-560m/ -tp 1\n```\n\n----------------------------------------\n\nTITLE: Creating InferenceService with Target Concurrency - KServe - YAML\nDESCRIPTION: Defines an InferenceService using TensorFlow with a specific concurrency scaling target. It sets an annotation for autoscaling to manage requests efficiently. Requires a Kubernetes cluster with KServe installed and network access via Istio Ingress.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/autoscaling/README.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"flowers-sample\"\n  annotations:\n    autoscaling.knative.dev/target: \"1\"\nspec:\n  predictor:\n    tensorflow:\n      storageUri: \"gs://kfserving-examples/models/tensorflow/flowers\"\n```\n\n----------------------------------------\n\nTITLE: Creating JWT Token for ServiceAccount Authentication\nDESCRIPTION: Creates a JWT authentication token for the default-editor ServiceAccount with the appropriate audience and duration. This token will be used to authenticate prediction requests to the InferenceService.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/istio-dex/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nTOKEN=$(kubectl create token default-editor -n kubeflow-user-example-com --audience=istio-ingressgateway.istio-system.svc.cluster.local --duration=24h)\n```\n\n----------------------------------------\n\nTITLE: Creating Initial InferenceService with Default Model in YAML\nDESCRIPTION: Defines a KServe InferenceService resource for deploying a PyTorch model. The configuration specifies the model name and storage location.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/canary/README.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"torchserve\"\nspec:\n  predictor:\n    pytorch:\n      storageUri: \"gs://kfserving-examples/models/torchserve/image_classifier/v1\"\n```\n\n----------------------------------------\n\nTITLE: Installing Prometheus with Prometheus Operator\nDESCRIPTION: This snippet installs Prometheus using Prometheus Operator, including applying Kustomize configurations and waiting for the necessary Custom Resource Definitions (CRDs) to be established.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/metrics-and-monitoring/README.md#2025-04-21_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncd kfserving\nkubectl apply -k docs/samples/metrics-and-monitoring/prometheus-operator\nkubectl wait --for condition=established --timeout=120s crd/prometheuses.monitoring.coreos.com\nkubectl wait --for condition=established --timeout=120s crd/servicemonitors.monitoring.coreos.com\nkubectl apply -k docs/samples/metrics-and-monitoring/prometheus\n```\n\n----------------------------------------\n\nTITLE: Creating InferenceService with GPU Resource - KServe - YAML\nDESCRIPTION: Defines an InferenceService using TensorFlow leveraging GPU resources for model serving. Includes specifications for the runtime version and GPU limits. Requires a Kubernetes cluster with GPU support.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/autoscaling/README.md#2025-04-21_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"flowers-sample-gpu\"\nspec:\n  predictor:\n    tensorflow:\n      storageUri: \"gs://kfserving-examples/models/tensorflow/flowers\"\n      runtimeVersion: \"1.14.0-gpu\"\n      resources:\n        limits:\n          nvidia.com/gpu: 1\n```\n\n----------------------------------------\n\nTITLE: Apply InferenceService YAML with kubectl\nDESCRIPTION: This command deploys the InferenceService using `kubectl apply`. It reads the configuration from the `autoscale.yaml` file and creates the corresponding resources in the Kubernetes cluster, enabling autoscaling for the service.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/docs/autoscaling.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n\"kubectl apply -f autoscale.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Verifying Traffic Split Between Revisions\nDESCRIPTION: Command to check the actual traffic distribution between the default and canary model revisions.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/canary/README.md#2025-04-21_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get ksvc torchserve-predictor-default -oyaml\n  status:\n    address:\n      url: http://torchserve-predictor-default.default.svc.cluster.local\n    traffic:\n    - latestRevision: true\n      percent: 20\n      revisionName: torchserve-predictor-default-kxp96\n      tag: latest\n      url: http://latest-torchserve-predictor-default.default.example.com\n    - latestRevision: false\n      percent: 80\n      revisionName: torchserve-predictor-default-9lttm\n      tag: prev\n      url: http://prev-torchserve-predictor-default.default.example.com\n    url: http://torchserve-predictor-default.default.example.com\n```\n\n----------------------------------------\n\nTITLE: Launching Iter8 Experiment with Prometheus Metrics\nDESCRIPTION: Configures and launches a comprehensive Iter8 experiment to validate service-level objectives (SLOs) by collecting custom metrics from Prometheus, assessing model performance across stable and canary versions\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/canary-testing/README.md#2025-04-21_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\niter8 k launch \\\n--set \"tasks={ready,custommetrics,assess}\" \\\n--set ready.isvc=${ISVC} \\\n--set ready.timeout=600s \\\n--set custommetrics.templates.kserve-prometheus=\"https://gist.githubusercontent.com/kalantar/adc6c9b0efe483c00b8f0c20605ac36c/raw/fc0696233aa766ec4ba1d82c206c1b78f8f1f267/kserve-prometheus.tpl\" \\\n--set custommetrics.values.labels.service_name=${ISVC}-predictor-default \\\n--set \"custommetrics.versionValues[0].labels.revision_name=${CURRENT_REVISION}\" \\\n--set \"custommetrics.versionValues[1].labels.revision_name=${CANARY_REVISION}\" \\\n--set \"custommetrics.values.latencyPercentiles={50,75,90,95}\" \\\n--set assess.SLOs.upper.kserve-prometheus/latency-mean=50 \\\n--set assess.SLOs.upper.kserve-prometheus/latency-p90=75 \\\n--set assess.SLOs.upper.kserve-prometheus/error-count=0 \\\n--set runner=cronjob \\\n--set cronjobSchedule=\"*/1 * * * *\"\n```\n\n----------------------------------------\n\nTITLE: Deploying TorchServe InferenceService with GPU Support\nDESCRIPTION: Deploys the TorchServe InferenceService with GPU acceleration by applying a GPU-specific configuration file.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v2/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f mnist_gpu.yaml\n```\n\n----------------------------------------\n\nTITLE: Waiting for InferenceService to be Ready After Patching\nDESCRIPTION: Wait for the updated InferenceService to become ready in the cluster\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/client/kfserving_sdk_v1beta1_sample.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nKFServing.wait_isvc_ready(\"flower-sample\", namespace=namespace)\n```\n\n----------------------------------------\n\nTITLE: Prediction Request using Curl on LightGBM InferenceService\nDESCRIPTION: Sends a prediction request using curl to the LightGBM model deployed on KServe. The command assumes the necessary environment variables like INGRESS_HOST and INGRESS_PORT are set. The response will contain the model predictions.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/lightgbm/README.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nMODEL_NAME=lightgbm-iris\nINPUT_PATH=@./iris-input.json\nSERVICE_HOSTNAME=$(kubectl get inferenceservice lightgbm-iris -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\ncurl -v -H \"Host: ${SERVICE_HOSTNAME}\" http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/$MODEL_NAME:predict -d $INPUT_PATH\n\n```\n\n----------------------------------------\n\nTITLE: Building and Publishing BGTest Docker Image\nDESCRIPTION: Three-step process to vendor Go dependencies, build Docker image, and push to DockerHub. Requires Go installed, Docker configured, and DockerHub credentials set up.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/graph/bgtest/README.md#2025-04-21_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n1. cd bgtest;go mod vendor; cd -\n2. docker build -t {$your-dockerhub-name}/bgtest:latest .\n3. docker push {$your-dockerhub-name}/bgtest:latest\n```\n\n----------------------------------------\n\nTITLE: Setting model checkpoint path in Triton configuration\nDESCRIPTION: Example of the FasterTransformer backend configuration parameter needed in the config.pbtxt file to specify the absolute path where the model will be mounted in the container.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/triton/fastertransformer/README.md#2025-04-21_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\nparameters {\n  key: \"model_checkpoint_path\"\n  value: {\n    string_value: \"/mnt/models/fastertransformer/1\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring TorchServe Predictor with Logger Using YAML\nDESCRIPTION: Defines an InferenceService in KServe to deploy a TorchServe model predictor. It includes a logger configured to use the 'message-dumper' service for logging the cloud events. Required dependencies include KServe and TorchServe setup in the cluster.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/logger/README.md#2025-04-21_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"torchserve-logger\"\nspec:\n  predictor:\n    minReplicas: 1\n    logger:\n      url: http://message-dumper.default.svc.cluster.local\n      mode: all\n    containers:\n      - image: {username}/torchserve:latest\n        name: torchserve-container\n```\n\n----------------------------------------\n\nTITLE: Creating Outlier Detector Configuration - Python\nDESCRIPTION: This snippet writes a YAML configuration for a KServe service named 'vae-outlier' which serves as an outlier detector for the CIFAR10 dataset using a pre-trained model.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/outlier-detection/alibi-detect/cifar10/cifar10_outlier.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n%%writefile cifar10od.yaml\napiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: vae-outlier\n  namespace: cifar10\nspec:\n  template:\n    metadata:\n      annotations:\n        autoscaling.knative.dev/minScale: \"1\"\n    spec:\n      containers:\n      - image: seldonio/alibi-detect-server:1.12.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --model_name\n        - cifar10od\n        - --http_port\n        - '8080'\n        - --protocol\n        - tensorflow.http\n        - --storage_uri\n        - gs://seldon-models/alibi-detect/od/OutlierVAE/cifar10\n        - --reply_url\n        - http://hello-display.cifar10\n        - --event_type\n        - org.kubeflow.serving.inference.outlier\n        - --event_source\n        - org.kubeflow.serving.cifar10od\n        - OutlierDetector\n```\n\n----------------------------------------\n\nTITLE: Generating OpenAI API Client from OpenAPI Specification\nDESCRIPTION: Downloads OpenAI OpenAPI YAML specification and uses datamodel-codegen to generate a Pydantic v2 based Python client with strict typing and enum field handling\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/kserve/protocol/rest/openai/types/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl https://raw.githubusercontent.com/openai/openai-openapi/master/openapi.yaml -o openapi-2.0.0.yaml\ndatamodel-codegen --input openapi-2.0.0.yaml --input-file-type openapi --output openapi.py --output-model-type pydantic_v2.BaseModel --use-double-quotes --collapse-root-models  --enum-field-as-literal all --strict-nullable\n```\n\n----------------------------------------\n\nTITLE: Direct Outlier Detection Call - Python\nDESCRIPTION: This snippet demonstrates how to call the outlier detector directly with an image input, allowing for an immediate response on whether the image is detected as an outlier.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/outlier-detection/alibi-detect/cifar10/cifar10_outlier.ipynb#2025-04-21_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nod_preds = outlier(X)\n```\n\n----------------------------------------\n\nTITLE: Running Predictions via InferenceService\nDESCRIPTION: This Bash snippet outlines the steps for determining the ingress IP and ports, and then sends a prediction request to the deployed InferenceService endpoint within a Kubernetes environment.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/bentoml/README.md#2025-04-21_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nMODEL_NAME=iris-classifier\nSERVICE_HOSTNAME=$(kubectl get inferenceservice ${MODEL_NAME} -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\n\ncurl -v -H \"Host: ${SERVICE_HOSTNAME}\" \\\n  --header \"Content-Type: application/json\" \\\n  --request POST \\\n  --data '[[5.1, 3.5, 1.4, 0.2]]' \\\n  http://${INGRESS_HOST}:${INGRESS_PORT}/predict\n```\n\n----------------------------------------\n\nTITLE: Making Prediction Request to KFServing Endpoint with Authorization\nDESCRIPTION: This segment covers the steps to enable authorization for the inference service and make a prediction request using the modified iap_request_auth.py script. It details the necessary service account permissions and the expected process to handle possible errors from unauthorized access.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/gcp-iap/README.md#2025-04-21_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nkubectl apply -f sklearn-iap-with-authz.yaml\n\nbash make-prediction-auth.sh\n```\n\n----------------------------------------\n\nTITLE: Apply KServe InferenceService CRD\nDESCRIPTION: This command applies the Kubernetes resource definition (CRD) for the InferenceService, defined in `onnx.yaml`, to create the InferenceService within the cluster. This is a crucial step to instruct KServe to deploy the ONNX model for serving.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/onnx/README.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nkubectl apply -f onnx.yaml\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Inference Service\nDESCRIPTION: Sets up environment variables for the model name and service hostname required to make inference requests to the deployed service.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v2/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nMODEL_NAME=mnist\nSERVICE_HOSTNAME=$(kubectl get inferenceservice torchserve-mnist-v2 -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\n```\n\n----------------------------------------\n\nTITLE: Configuring sklearn Predictor with Logger in YAML\nDESCRIPTION: This YAML file sets up an InferenceService named 'sklearn-iris' with an sklearn predictor and a logger that sends CloudEvents to the message dumper. Ensure the sklearn model is accessible at the specified storage URI.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/logger/basic/README.md#2025-04-21_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: serving.kserve.io/v1beta1\nkind: InferenceService\nmetadata:\n  name: sklearn-iris\nspec:\n  predictor:\n    logger:\n      mode: all\n      url: http://message-dumper.default/\n    sklearn:\n      storageUri: gs://kfserving-examples/models/sklearn/1.0/model\n\n```\n\n----------------------------------------\n\nTITLE: Configuring InferenceService for KFServing Deployment\nDESCRIPTION: This YAML snippet specifies the configuration for deploying the IrisClassifier as an InferenceService in KFServing. It includes metadata and container image details, crucial for resource management in Kubernetes.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/bentoml/README.md#2025-04-21_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: serving.kserve.io/v1alpha2\nkind: InferenceService\nmetadata:\n  labels:\n    controller-tools.k8s.io: \"1.0\"\n  name: iris-classifier\nspec:\n  default:\n    predictor:\n      custom:\n        container:\n          image: {docker_username}/iris-classifier\n          ports:\n            - containerPort: 5000\n```\n\n----------------------------------------\n\nTITLE: Running a Prediction on an InferenceService using Shell\nDESCRIPTION: This shell snippet describes how to fetch the service hostname and execute a curl command to get predictions from the deployed inference service, passing input data for the prediction.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/sklearn/v1/README.md#2025-04-21_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nMODEL_NAME=sklearn-iris\nINPUT_PATH=@./iris-input.json\nSERVICE_HOSTNAME=$(kubectl get inferenceservice sklearn-iris -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\ncurl -v -H \"Host: ${SERVICE_HOSTNAME}\" http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/$MODEL_NAME:predict -d $INPUT_PATH\n```\n\n----------------------------------------\n\nTITLE: Querying Models Over Ingress Gateway\nDESCRIPTION: Example of using curl to send prediction requests to the deployed models via the ingress gateway. The commands show how to structure requests for both models.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/multimodelserving/sklearn/README.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n\"curl -v -H \\\"Host: ${SERVICE_HOSTNAME}\\\" http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/model1-sklearn:predict -d @./docs/samples/v1beta1/sklearn/v1/iris-input.json\\ncurl -v -H \\\"Host: ${SERVICE_HOSTNAME}\\\" http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/model2-sklearn:predict -d @./docs/samples/v1beta1/sklearn/v1/iris-input.json\"\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Inference\nDESCRIPTION: These shell commands set environment variables required for interacting with the deployed InferenceService. `MODEL_NAME` stores the name of the model, `ISVC_NAME` stores the name of the InferenceService, and `SERVICE_HOSTNAME` extracts the hostname of the deployed service from the InferenceService status.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/onnx/README.md#2025-04-21_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nexport MODEL_NAME=onnx-model\nexport ISVC_NAME=style-sample\nexport SERVICE_HOSTNAME=$(kubectl get inferenceservice ${ISVC_NAME} -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\n```\n\n----------------------------------------\n\nTITLE: Creating KServe Inference Service for CIFAR10 - Python\nDESCRIPTION: This snippet creates a YAML configuration for a KServe InferenceService for the CIFAR10 model using TensorFlow. This service will handle prediction requests and log them to a specified URL.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/outlier-detection/alibi-detect/cifar10/cifar10_outlier.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n%%writefile cifar10.yaml\napiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"tfserving-cifar10\"\n  namespace: cifar10\nspec:\n    predictor:\n      tensorflow:\n        storageUri: \"gs://seldon-models/tfserving/cifar10/resnet32\"\n      logger:\n        mode: all\n        url: http://broker-ingress.knative-eventing.svc.cluster.local/cifar10/default\n```\n\n----------------------------------------\n\nTITLE: Container Runtime Environment Configuration\nDESCRIPTION: Detailed configuration for setting up container runtime environment, including environment variables, image pull policies, and storage specifications\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/docs/V1beta1HuggingFaceRuntimeSpec.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n{\n    \"env\": [V1EnvVar],\n    \"env_from\": [V1EnvFromSource],\n    \"image_pull_policy\": \"str\",\n    \"storage_uri\": \"str\"\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Event Display Deployment and Service\nDESCRIPTION: This snippet defines a Kubernetes Deployment and Service for an event display application. The deployment runs a simple event display container, and the service exposes it for access. The event-display allows for checking the result of the drift detector. It displays events received by the Knative broker.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/drift-detection/alibi-detect/cifar10/cifar10_drift.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n\"%%writefile event-display.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-display\n  namespace: cifar10\nspec:\n  replicas: 1\n  selector:\n    matchLabels: &labels\n      app: hello-display\n  template:\n    metadata:\n      labels: *labels\n    spec:\n      containers:\n        - name: event-display\n          image: gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display\n\n---\n\nkind: Service\napiVersion: v1\nmetadata:\n  name: hello-display\n  namespace: cifar10\nspec:\n  selector:\n    app: hello-display\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 8080\"\n```\n\n----------------------------------------\n\nTITLE: Creating Knative Service for Drift Detector\nDESCRIPTION: This snippet defines a Knative Service for the drift detector.  It uses the `seldonio/alibi-detect-server` image and configures it to serve the pre-trained drift detection model. The `drift_batch_size` parameter configures the number of requests to wait for before making a drift prediction. The reply_url sends the drift prediction to the message-dumper.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/drift-detection/alibi-detect/cifar10/cifar10_drift.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n\"%%writefile cifar10cd.yaml\napiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: drift-detector\n  namespace: cifar10\nspec:\n  template:\n    metadata:\n      annotations:\n        autoscaling.knative.dev/minScale: \\\"1\\\"\n    spec:\n      containers:\n      - image: seldonio/alibi-detect-server:0.0.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - --model_name\n        - cifar10cd\n        - --http_port\n        - '8080'\n        - --protocol\n        - tensorflow.http\n        - --storage_uri\n        - gs://seldon-models/alibi-detect/cd/ks/cifar10\n        - --reply_url\n        - http://hello-display.cifar10\n        - --event_type\n        - org.kubeflow.serving.inference.outlier\n        - --event_source\n        - org.kubeflow.serving.cifar10cd\n        - DriftDetector\n        - --drift_batch_size\n        - '5000'\\n\"\n```\n\n----------------------------------------\n\nTITLE: Creating Dataset and Runtime with Fluid\nDESCRIPTION: Set up a Fluid Dataset and JindoFS Runtime for caching the model data.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/fluid/README.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# please update the `mountPoint` and `options`\nkubectl create -f jindo.yaml -n kserve-fluid-demo\n```\n\n----------------------------------------\n\nTITLE: Attach Secret to Service Account\nDESCRIPTION: Provides a configuration to attach a secret to a Kubernetes Service Account. This allows secrets to be used by the predictions in KServe.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/storage/uri/README.md#2025-04-21_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: sa\nsecrets:\n  - name: mysecret\n```\n\n----------------------------------------\n\nTITLE: Sending a Prediction Request to Paddle ResNet50 Model in KServe\nDESCRIPTION: This code snippet demonstrates how to make an inference request to a deployed Paddle ResNet50 model using curl. The script determines the model service hostname from the KServe InferenceService and sends a prediction request with image data from jay.json.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/paddle/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nMODEL_NAME=paddle-resnet50\nSERVICE_HOSTNAME=$(kubectl get inferenceservice ${MODEL_NAME} -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\ncurl -v -H \"Host: ${SERVICE_HOSTNAME}\" http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/${MODEL_NAME}:predict -d @./jay.json\n```\n\n----------------------------------------\n\nTITLE: Applying Canary Configuration to InferenceService\nDESCRIPTION: Command to apply the updated InferenceService with canary deployment configuration.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/canary/README.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f canary.yaml\n```\n\n----------------------------------------\n\nTITLE: Creating a Model Archive File for BERT Sequence Classification with TorchServe\nDESCRIPTION: Command to package a Huggingface BERT model into a TorchServe Model Archive (MAR) file with necessary handler and configuration files.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v1/bert/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ntorch-model-archiver --model-name BERTSeqClassification --version 1.0 \\\n--serialized-file Transformer_model/pytorch_model.bin \\\n--handler ./Transformer_handler_generalized.py \\\n--extra-files \"Transformer_model/config.json,./setup_config.json,./Seq_classification_artifacts/index_to_name.json\"\n```\n\n----------------------------------------\n\nTITLE: Creating an InferenceService in KServe - YAML\nDESCRIPTION: This YAML configuration defines an InferenceService in KServe that points to a specific model stored in the specified storage bucket, using the credentials defined in the storage secret. It sets parameters like the bucket name and model path.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/storage/storageSpec/README.md#2025-04-21_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: serving.kserve.io/v1beta1\nkind: InferenceService\nmetadata:\n  name: example-sklearn-isvc\nspec:\n  predictor:\n    sklearn:\n      storage:\n        key: localMinIO # Credential key for the destination storage in the common secret\n        path: sklearn # Model path inside the bucket\n        # schemaPath: null # Optional schema files for payload schema\n        parameters: # Parameters to override the default values inside the common secret.\n          bucket: example-models\n```\n\n----------------------------------------\n\nTITLE: Deploying InferenceService with Fluid\nDESCRIPTION: Create an InferenceService using the Fluid-cached model data.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/fluid/README.md#2025-04-21_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n# please update the `storageUri`.\nkubectl create -f fluid-isvc.yaml -n kserve-fluid-demo\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Ingress Resource for Custom Domain\nDESCRIPTION: This snippet outlines how to edit and apply a Kubernetes Ingress resource defined in 'kfserving-ingress.yaml'. It includes setting a custom wildcard domain in the 'spec.rules.host' section to direct traffic to the 'istio-ingressgateway'. This step is crucial when not directly configuring the domain to route traffic to 'istio-ingressgateway'.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/custom-domain/README.md#2025-04-21_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.k8s.io/v1beta1\nkind: Ingress\nmetadata:\n  name: kfserving-ingress\n  namespace: istio-system\nspec:\n  rules:\n    - host: \"<*.custom_domain>\"\n      http:\n        paths:\n          - backend:\n              serviceName: istio-ingressgateway\n              servicePort: 80\n```\n\n----------------------------------------\n\nTITLE: Creating Knative Trigger\nDESCRIPTION: This snippet defines a Knative Trigger to forward logging events to the drift detector. It filters events based on the `type` attribute (`org.kubeflow.serving.inference.request`) and routes matching events to the `drift-detector` Knative Service. This is the mechanism that causes the drift detector to receive requests.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/drift-detection/alibi-detect/cifar10/cifar10_drift.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n\"%%writefile trigger.yaml\napiVersion: eventing.knative.dev/v1\nkind: Trigger\nmetadata:\n  name: drift-trigger\n  namespace: cifar10\nspec:\n  broker: default\n  filter:\n    attributes:\n      type: org.kubeflow.serving.inference.request\n  subscriber:\n    ref:\n      apiVersion: serving.knative.dev/v1\n      kind: Service\n      name: drift-detector\n      namespace: cifar10\"\n```\n\n----------------------------------------\n\nTITLE: Getting Inference Service Hostname for CIFAR10 - Python\nDESCRIPTION: This snippet retrieves the hostname for the inference service for the CIFAR10 model deployed in the Kubernetes cluster, enabling subsequent prediction requests.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/outlier-detection/alibi-detect/cifar10/cifar10_outlier.ipynb#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nSERVICE_HOSTNAMES = !(kubectl get inferenceservice -n cifar10 tfserving-cifar10 -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\nSERVICE_HOSTNAME_CIFAR10 = SERVICE_HOSTNAMES[0]\nprint(SERVICE_HOSTNAME_CIFAR10)\n```\n\n----------------------------------------\n\nTITLE: Deploying and Testing Sequence Node in KServe\nDESCRIPTION: Shell commands for deploying an InferenceService and InferenceGraph for a Sequence Node, checking their status, and testing the graph with sample data.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/graph/README.md#2025-04-21_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nkubectl apply -f sequence.yaml\n```\n\nLANGUAGE: shell\nCODE:\n```\nkubectl get pods\nNAME                                                              READY   STATUS    RESTARTS   AGE\nmodel-chainer-00001-deployment-6bf7cf7776-zn5p4                   2/2     Running   0          32s\nsklearn-iris-predictor-default-00001-deployment-8495cbf8cbdqfjg   2/2     Running   0          52s\nxgboost-iris-predictor-default-00001-deployment-7b86bcdcf-7njrl   2/2     Running   0          50s\n\nkubectl get isvc\nNAME              URL                                                    READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                       AGE\nsklearn-iris      http://sklearn-iris.default.10.166.15.29.sslip.io      True           100                              sklearn-iris-predictor-default-00001      80m\nxgboost-iris      http://xgboost-iris.default.10.166.15.29.sslip.io      True           100                              xgboost-iris-predictor-default-00001      80m\n\nkubectl get ig\nNAME            URL                                                  READY   AGE\nmodel-chainer   http://model-chainer.default.10.166.15.29.sslip.io   True    5s\n```\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://model-chainer.default.10.166.15.29.sslip.io -d @./iris-input.json\n```\n\nLANGUAGE: shell\nCODE:\n```\n{\"predictions\":[1,1]}\n```\n\n----------------------------------------\n\nTITLE: Initializing KFP Client and Experiment\nDESCRIPTION: This code initializes the Kubeflow Pipelines (KFP) client and creates a new experiment. The KFP client is used to interact with the KFP cluster, and the experiment provides a namespace for organizing pipeline runs. If the client is not running on the same cluster, you must specify the KubeFlow Pipeline endpoint.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/pipelines/kfs-pipeline.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n\"import kfp.compiler as compiler\nimport kfp.dsl as dsl\nimport kfp\nfrom kfp import components\n\n# Create kfp client\n# Note: Add the KubeFlow Pipeline endpoint below if the client is not running on the same cluster.\n# Example: kfp.Client('http://192.168.1.27:31380/pipeline')\nclient = kfp.Client()\nEXPERIMENT_NAME = \\\"KServe Experiments\\\"\nexperiment = client.create_experiment(name=EXPERIMENT_NAME, namespace=\\\"anonymous\\\")\"\n```\n\n----------------------------------------\n\nTITLE: Canary Deployment Configuration\nDESCRIPTION: YAML configuration for implementing canary deployment with 20% traffic allocation to the new model version.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/tensorflow/README.md#2025-04-21_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"flower-example\"\nspec:\n  predictor:\n    canaryTrafficPercent: 20\n    tensorflow:\n      storageUri: \"gs://kfserving-examples/models/tensorflow/flowers-2\"\n```\n\n----------------------------------------\n\nTITLE: Running Model Prediction\nDESCRIPTION: Curl command to send a prediction request to the deployed Torchserve BERT model through KServe\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/bert-sample/hugging-face-bert-sample.md#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nMODEL_NAME=torchserve-bert\nSERVICE_HOSTNAME=$(kubectl get inferenceservice ${MODEL_NAME} -n <namespace> -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\n\ncurl -v -H \"Host: ${SERVICE_HOSTNAME}\" http://${INGRESS_HOST}:${INGRESS_PORT}/predictions/BERTSeqClassification -T serve/examples/Huggingface_Transformers/sample_text.txt\n```\n\n----------------------------------------\n\nTITLE: Directory Structure for TorchServe Model Files\nDESCRIPTION: Expected directory structure for TorchServe model files, showing the model-store directory containing the model archive file and the config directory with configuration properties.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v1/bloom/README.md#2025-04-21_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\n|_model-store\n  |_bloom-560m.mar\n|_config\n  |_config.properties\n```\n\n----------------------------------------\n\nTITLE: Sharding Huggingface BLOOM Model in Python\nDESCRIPTION: Python script to download and shard a Huggingface BLOOM model for compatibility with TorchServe. This process splits large models into smaller shards of 5GB each for better management and loads the model and tokenizer from the pretrained source.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v1/bloom/README.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name=\"bigscience/bloomz-7b1\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel.save_pretrained(\"model/\"+model_name, max_shard_size=\"5GB\")\ntokenizer.save_pretrained(\"model/\"+model_name)\n```\n\n----------------------------------------\n\nTITLE: Inference Request JSON Schema\nDESCRIPTION: Defines the structure for an inference request, including optional request ID, parameters, input tensors, and output tensor specifications\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md#2025-04-21_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\" : $string #optional,\n  \"parameters\" : $parameters #optional,\n  \"inputs\" : [ $request_input, ... ],\n  \"outputs\" : [ $request_output, ... ] #optional\n}\n```\n\n----------------------------------------\n\nTITLE: Editing ConfigMap for Custom Domain in Kubernetes\nDESCRIPTION: This snippet demonstrates how to edit the 'config-domain' ConfigMap in the 'knative-serving' namespace to set a custom domain for Knative services, replacing the default 'example.com'. This change is necessary to assign hostnames based on the custom domain.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/custom-domain/README.md#2025-04-21_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nkubectl edit configmap config-domain -n knative-serving\n```\n\n----------------------------------------\n\nTITLE: Deploying SKLearn Model with Logging\nDESCRIPTION: Applies the sklearn-logging.yaml configuration to create a KFServing inference service that will send events to the logger.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/logger/knative-eventing/logger_demo.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n!kubectl apply -f sklearn-logging.yaml\n```\n\n----------------------------------------\n\nTITLE: Creating Service Account with HDFS Secret\nDESCRIPTION: Kubernetes service account configuration with HDFS credentials secret attached\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/storage/hdfs/README.md#2025-04-21_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: sa\nsecrets:\n- name: hdfscreds\n```\n\n----------------------------------------\n\nTITLE: Configuring MLServer Model Settings in JSON\nDESCRIPTION: JSON configuration file for MLServer that specifies the model name, version, and implementation class to use for serving XGBoost models locally.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/xgboost/README.md#2025-04-21_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"xgboost-iris\",\n  \"version\": \"v1.0.0\",\n  \"implementation\": \"mlserver_xgboost.XGBoostModel\"\n}\n```\n\n----------------------------------------\n\nTITLE: Deploying Custom Runtime Model with KServe\nDESCRIPTION: Example of deploying a custom model runtime using the custom_model_spec parameter. This deploys the MAX Object Detector model from the codait container on port 5000.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/kfp/README.md#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ncontainer_spec = '{ \"image\": \"codait/max-object-detector\", \"port\":5000, \"name\": \"custom-container\"}'\nkserve_op(\n    action='apply',\n    model_name='custom-simple',\n    custom_model_spec=container_spec\n)\n```\n\n----------------------------------------\n\nTITLE: Patching InferenceService with Canary Traffic\nDESCRIPTION: Update the InferenceService to add a canary deployment with a specified traffic percentage and a new model URI\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/client/kfserving_sdk_v1beta1_sample.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nisvc = V1beta1InferenceService(\n    api_version=api_version,\n    kind=constants.KFSERVING_KIND,\n    metadata=client.V1ObjectMeta(name=\"flower-sample\", namespace=namespace),\n    spec=V1beta1InferenceServiceSpec(\n        predictor=V1beta1PredictorSpec(\n            canary_traffic_percent=20,\n            tensorflow=(\n                V1beta1TFServingSpec(\n                    storage_uri=\"gs://kfserving-examples/models/tensorflow/flowers-2\"\n                )\n            ),\n        )\n    ),\n)\n\nKFServing.patch(\"flower-sample\", isvc, namespace=namespace)\n```\n\n----------------------------------------\n\nTITLE: Example of Inference Response in JSON\nDESCRIPTION: Illustrates a typical inference response from a server when a valid request is processed, showing the output tensor's structure and datatype. The example assists developers in parsing and verifying the server's output data effectively.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md#2025-04-21_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\" : \"42\"\n  \"outputs\" : [\n    {\n      \"name\" : \"output0\",\n      \"shape\" : [ 3, 2 ],\n      \"datatype\"  : \"FP32\",\n      \"data\" : [ 1.0, 1.1, 2.0, 2.1, 3.0, 3.1 ]\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Up and Running Load Testing\nDESCRIPTION: Bash commands for installing the hey load generator and running concurrent inference requests to test autoscaling behavior.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/autoscaling/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngo get -u github.com/rakyll/hey\n```\n\nLANGUAGE: bash\nCODE:\n```\nMODEL_NAME=mnist\nSERVICE_HOSTNAME=$(kubectl get inferenceservice torchserve -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\n\n./hey -m POST -z 30s -D ./mnist.json -host ${SERVICE_HOSTNAME} http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/${MODEL_NAME}:predict\n```\n\n----------------------------------------\n\nTITLE: Installing KServe SDK using Make with Poetry\nDESCRIPTION: Command to install the KServe Python SDK for development purposes using Poetry dependency management through a Make command.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/README.md#2025-04-21_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nmake dev_install\n```\n\n----------------------------------------\n\nTITLE: Selecting Features and Target (Python)\nDESCRIPTION: This snippet selects the features and target variable from the training data. It creates a DataFrame `df` containing the selected features and a NumPy array `y` containing the log-transformed \"SalePrice\" (target variable).  It also identifies categorical and numerical features.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/sklearn/v1/sklearn-mixedtype-model/sklearn-mixedtype-model.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndf = train_data[features]\ny = np.log1p(train_data[\"SalePrice\"])\ncategorical_features = df.select_dtypes(object)\nnumerical_features = df.select_dtypes(exclude=object)\n```\n\n----------------------------------------\n\nTITLE: Creating Sklearn InferenceService in YAML\nDESCRIPTION: Demonstrates how to create an InferenceService resource in Kubernetes for a sklearn model, specifying the storage URI for the model.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/storage/uri/README.md#2025-04-21_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: serving.kserve.io/v1beta1\nkind: InferenceService\nmetadata:\n  name: sklearn-from-uri\nspec:\n  predictor:\n    sklearn:\n      storageUri: https://github.com/tduffy000/kfserving-uri-examples/blob/master/sklearn/frozen/model.joblib?raw=true\n```\n\n----------------------------------------\n\nTITLE: Configuring InferenceService with Prometheus Annotations\nDESCRIPTION: YAML configuration for enabling Prometheus metrics scraping in a TorchServe deployment with metrics port 8082\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/metrics/README.md#2025-04-21_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: serving.kserve.io/v1beta1\nkind: InferenceService\nmetadata:\n  name: \"torch-metrics\"\n  annotations:\n    prometheus.io/scrape: 'true'\n    prometheus.io/port: '8082'\nspec:\n  predictor:\n    pytorch:\n      storageUri: gs://kfserving-examples/models/torchserve/image_classifier/v1\n```\n\n----------------------------------------\n\nTITLE: Deploying Simple String Tensorflow Model with TrainedModel\nDESCRIPTION: This YAML configuration adds a Simple String model trained with TensorFlow to the existing Triton InferenceService, detailing framework and memory specifications.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/multimodelserving/triton/README.md#2025-04-21_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: \"serving.kserve.io/v1alpha1\"\nkind: \"TrainedModel\"\nmetadata:\n  name: \"simple-string\"\nspec:\n  inferenceService: triton-mms\n  model:\n    framework: tensorflow\n    storageUri: gs://kfserving-examples/models/tensorrt/simple_string\n    memory: 1Gi\n```\n\n----------------------------------------\n\nTITLE: Configuring BERT Model Setup JSON\nDESCRIPTION: Configuration file for setting up BERT model parameters, specifying model type, classification mode, and preprocessing options\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/bert-sample/hugging-face-bert-sample.md#2025-04-21_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n \"model_name\":\"bert-base-uncased\",\n \"mode\":\"sequence_classification\",\n \"do_lower_case\":\"True\",\n \"num_labels\":\"2\",\n \"save_mode\":\"pretrained\",\n \"max_length\":\"150\"\n}\n```\n\n----------------------------------------\n\nTITLE: Loading and Resizing an Image - Python\nDESCRIPTION: This snippet uses the PIL library to open an image file named 'image.jpg' and resize it to 224x224 pixels, which is the expected input size for the ONNX model. The resized image object is returned.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/onnx/mosaic-onnx.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\n# load & resize image\nimage = Image.open(\"image.jpg\")\nimage = image.resize((224,224), Image.LANCZOS)\nimage\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Installing KServe SDK with storage support using Poetry (Option 2)\nDESCRIPTION: Alternative command to install the KServe Python SDK with storage support using Poetry's --extras flag.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/README.md#2025-04-21_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\npoetry install --extras \"storage\"\n```\n\n----------------------------------------\n\nTITLE: Creating SKLearn Raw Deployment with HPA\nDESCRIPTION: YAML configuration for deploying SKLearn iris model using raw Kubernetes deployment with HPA, bypassing Knative components.\nSOURCE: https://github.com/kserve/kserve/blob/master/test/benchmark/README.md#2025-04-21_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"sklearn-iris\"\n  annotations:\n    serving.kserve.io/deploymentMode: RawDeployment\n    serving.kserve.io/autoscalerClass: hpa\n    serving.kserve.io/metric: cpu\n    serving.kserve.io/targetUtilizationPercentage: \"80\"\nspec:\n  predictor:\n    sklearn:\n      storageUri: \"gs://kfserving-examples/models/sklearn/1.0/model\"\n```\n\n----------------------------------------\n\nTITLE: Creating ConfigMap for CA Bundle - YAML\nDESCRIPTION: This snippet shows how to create a ConfigMap containing a CA bundle certificate and retrieve its YAML representation. This is required for configuring KServe to recognize the CA bundle for secure connections.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/cabundle/README.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nkubectl create configmap cabundle --from-file=/path/to/cabundle.crt\n\nkubectl get configmap cabundle -o yaml\napiVersion: v1\ndata:\n  cabundle.crt: XXXXX\nkind: ConfigMap\nmetadata:\n  name: cabundle\n  namespace: kserve\n```\n\n----------------------------------------\n\nTITLE: Creating a Storage Secret in KServe - YAML\nDESCRIPTION: This YAML snippet defines a Kubernetes Secret that contains storage credentials used by KServe for accessing models stored in an S3-compatible storage service. It includes the access key, secret key, endpoint URL, and bucket details necessary for retrieval.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/storage/storageSpec/README.md#2025-04-21_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: v1\nstringData:\n  localMinIO: |\n    {\n      \"type\": \"s3\",\n      \"access_key_id\": \"minio\",\n      \"secret_access_key\": \"minio123\",\n      \"endpoint_url\": \"http://minio-service.kubeflow:9000\",\n      \"bucket\": \"mlpipeline\",\n      \"region\": \"us-south\",\n      \"anonymous\": \"False\"\n    }\nkind: Secret\nmetadata:\n  name: storage-config\ntype: Opaque\n```\n\n----------------------------------------\n\nTITLE: Running Prediction on Sklearn Model with Curl\nDESCRIPTION: Uses curl to make a prediction request to the sklearn model deployed with KServe. Requires the model name and input path.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/storage/uri/README.md#2025-04-21_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nMODEL_NAME=sklearn-from-uri\nINPUT_PATH=@./input.json\ncurl -v -H \"Host: ${SERVICE_HOSTNAME}\" http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/$MODEL_NAME:predict -d $INPUT_PATH\n```\n\n----------------------------------------\n\nTITLE: Defining Inference Service Specification in V1beta1\nDESCRIPTION: Describes the core structure for configuring an inference service with optional components for prediction, explanation, and transformation of machine learning models\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/docs/V1beta1InferenceServiceSpec.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nV1beta1InferenceServiceSpec:\n  predictor: V1beta1PredictorSpec\n  explainer: V1beta1ExplainerSpec (optional)\n  transformer: V1beta1TransformerSpec (optional)\n```\n\n----------------------------------------\n\nTITLE: Running LightGBM Server using Shell\nDESCRIPTION: This shell command launches the LightGBM server with the previously generated model. It assumes the model is in the specified directory. The server supports models from various storage types, such as local, S3, Azure, or Google Cloud.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/lightgbm/README.md#2025-04-21_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npython -m lgbserver --model_dir /path/to/model_dir --model_name lgb\n```\n\n----------------------------------------\n\nTITLE: Specifying GPU Type in KServe InferenceService\nDESCRIPTION: YAML annotation to specify the GPU type for an InferenceService in GKE. This annotation allows selecting specific GPU models when multiple types are available in the cluster.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/accelerators/README.md#2025-04-21_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmetadata:\n  annotations:\n    \"serving.kubeflow.org/gke-accelerator\": \"nvidia-tesla-k80\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Ensemble Node in KServe Inference Graph\nDESCRIPTION: A YAML configuration for an Ensemble Node that combines results from multiple ML models. This allows for techniques like majority voting or averaging in classification or regression problems.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/graph/README.md#2025-04-21_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\n...\nroot:\n  routerType: Ensemble\n  routes:\n  - serviceName: sklearn-iris\n    name: sklearn-iris\n  - serviceName: xgboost-iris\n    name: xgboost-iris\n...\n```\n\n----------------------------------------\n\nTITLE: Deploying GPU-enabled TorchServe InferenceService\nDESCRIPTION: Command to deploy the TorchServe InferenceService on GPU using kubectl.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v1/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f gpu.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring InferenceService with Soft Concurrency Limit in YAML\nDESCRIPTION: YAML configuration for InferenceService using autoscaling.knative.dev/target annotation to set a soft concurrency limit of 10 requests. This configuration allows for exceeding the limit during sudden request bursts.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/autoscaling/README.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"torchserve\"\n  annotations:\n    autoscaling.knative.dev/target: \"10\"\nspec:\n  predictor:\n    pytorch:\n      storageUri: \"gs://kfserving-examples/models/torchserve/image_classifier/v1\"\n```\n\n----------------------------------------\n\nTITLE: Running a Prediction Using Curl\nDESCRIPTION: This snippet uses curl to send a PUT request for model prediction to a deployed Torchserve InferenceService. It requires the determination of ingress IP/ports and setting environment variables such as MODEL_NAME, SERVICE_HOSTNAME, INGRESS_HOST, and INGRESS_PORT. The input is an image file, and the output is the model's prediction.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nMODEL_NAME=torchserve-custom\nSERVICE_HOSTNAME=$(kubectl get inferenceservice ${MODEL_NAME} -n <namespace> -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\n\ncurl -v -H \"Host: ${SERVICE_HOSTNAME}\" http://${INGRESS_HOST}:${INGRESS_PORT}/predictions/mnist -T 0.png\n```\n\n----------------------------------------\n\nTITLE: Downloading Pretrained Tensorflow Model\nDESCRIPTION: Downloads a pretrained TensorFlow model from a Google Bucket and loads it for further use. The URL points to the CIFAR10 model stored in an accessible repository.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/alibi/cifar10/cifar10_explanations.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nurl = \"https://storage.googleapis.com/seldon-models/alibi-detect/classifier/\"\npath_model = os.path.join(url, \"cifar10\", \"resnet32\", \"model.h5\")\nsave_path = tf.keras.utils.get_file(\"resnet32\", path_model)\nmodel = tf.keras.models.load_model(save_path)\n```\n\n----------------------------------------\n\nTITLE: Port Forwarding to Istio Ingress Gateway - Shell\nDESCRIPTION: This snippet details the commands needed to port-forward the Istio Ingress Gateway service, allowing access to deployed services in a local Kubernetes environment, such as Kind or Minikube.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/outlier-detection/alibi-detect/cifar10/cifar10_outlier.ipynb#2025-04-21_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\n$ INGRESS_GATEWAY_SERVICE=$(kubectl get svc --namespace istio-system --selector=\"app=istio-ingressgateway\" --output jsonpath='{.items[0].metadata.name}')\n$ kubectl port-forward --namespace istio-system svc/${INGRESS_GATEWAY_SERVICE} 8080:80\n$ CLUSTER_IP=\"localhost:8080\"\n```\n\n----------------------------------------\n\nTITLE: Installing Iter8 CLI using Brew\nDESCRIPTION: This command installs the Iter8 CLI via the Homebrew package manager on macOS. It requires Brew to be installed beforehand. The input is the series of brew commands and the output is the Iter8 CLI installed on the system.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/performance-testing/grpc.md#2025-04-21_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nbrew tap iter8-tools/iter8\nbrew install iter8@0.13\n```\n\n----------------------------------------\n\nTITLE: Applying TrainedModel Configuration\nDESCRIPTION: A Bash command that deploys the model configuration to KFServing using kubectl. This loads the specified model into the system.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/multimodelserving/triton/README.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f trained_model.yaml\n```\n\n----------------------------------------\n\nTITLE: Making an Outlier Prediction - Python\nDESCRIPTION: This snippet defines a function to make predictions using the outlier detector. It formats the input data, sends an HTTP POST request to the outlier detector service, and processes the response to include the feature scores.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/outlier-detection/alibi-detect/cifar10/cifar10_outlier.ipynb#2025-04-21_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndef outlier(X):\n    formData = {\"instances\": X.tolist()}\n    headers = {\n        \"Alibi-Detect-Return-Feature-Score\": \"true\",\n        \"Alibi-Detect-Return-Instance-Score\": \"true\",\n        \"ce-namespace\": \"default\",\n        \"ce-modelid\": \"cifar10\",\n        \"ce-type\": \"io.seldon.serving.inference.request\",\n        \"ce-id\": \"1234\",\n        \"ce-source\": \"localhost\",\n        \"ce-specversion\": \"1.0\",\n    }\n    headers[\"Host\"] = SERVICE_HOSTNAME_VAEOD\n    res = requests.post(\"http://\" + CLUSTER_IP + \"/\", json=formData, headers=headers)\n    if res.status_code == 200:\n        od = res.json()\n        od[\"data\"][\"feature_score\"] = np.array(od[\"data\"][\"feature_score\"])\n        od[\"data\"][\"instance_score\"] = np.array(od[\"data\"][\"instance_score\"])\n        return od\n    else:\n        print(\"Failed with \", res.status_code)\n        return []\n```\n\n----------------------------------------\n\nTITLE: Starting BentoML API Server for Model Serving\nDESCRIPTION: This Bash command starts the BentoML API server to serve the IrisClassifier model, allowing for REST API predictions. It enables functionality to test the model's prediction endpoint locally.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/bentoml/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Start BentoML API server:\nbentoml serve IrisClassifier:latest\n```\n\n----------------------------------------\n\nTITLE: Apply InferenceService - Kubernetes CLI\nDESCRIPTION: This command applies the Kubernetes resource defined in the `default.yaml` file. It's used to create or update the InferenceService in the cluster, deploying the initial model.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/rollout/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f default.yaml\n```\n\n----------------------------------------\n\nTITLE: Querying mean latency in Prometheus\nDESCRIPTION: This Prometheus query calculates the mean latency for serving prediction requests for the `sklearn-iris-predictor-default` model over the last 60 seconds. It divides the sum of latencies by the total number of requests to obtain the average latency.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/metrics-and-monitoring/README.md#2025-04-21_snippet_6\n\nLANGUAGE: promql\nCODE:\n```\nsum(increase(revision_app_request_latencies_sum{service_name=~\"sklearn-iris-predictor-default\"}[60s]))/sum(increase(revision_app_request_latencies_count{service_name=~\"sklearn-iris-predictor-default\"}[60s]))\n```\n\n----------------------------------------\n\nTITLE: Running SKLearn Server to Serve a Model using Shell\nDESCRIPTION: This snippet shows the command to run the SKLearn Server to serve the trained model, specifying the model's directory and name. This requires that the SKLearn server is properly installed and accessible.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/sklearn/v1/README.md#2025-04-21_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n# we should indicate the directory containing the model file (model.joblib) by --model_dir\npython -m sklearnserver --model_dir ./  --model_name svm\n```\n\n----------------------------------------\n\nTITLE: Starting MLServer for Local Model Serving\nDESCRIPTION: Bash command to start MLServer in the current directory, which will use the model-settings.json file to serve the XGBoost model locally.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/xgboost/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmlserver start .\n```\n\n----------------------------------------\n\nTITLE: Get Explanation for Negative Prediction\nDESCRIPTION: This snippet calls the `explain` function to get an explanation for the negative prediction at `idxNeg`. It sends a request to the explainer service and retrieves the explanation.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/alibi/moviesentiment/movie_review_explanations.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n\"exp = explain(\\n    movies.data[idxNeg : idxNeg + 1], \\\"moviesentiment\\\", SERVICE_HOSTNAME, CLUSTER_IP\\n)\"\n```\n\n----------------------------------------\n\nTITLE: Deploying PyTorch Model with KServe\nDESCRIPTION: Example of deploying a PyTorch model using the KServe component. This snippet shows the operation to apply a PyTorch model from a Google Cloud Storage location.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/kfp/README.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nkserve_op(\n    action='apply',\n    model_name='pytorch-test',\n    model_uri='gs://kfserving-examples/models/torchserve/image_classifier',\n    framework='pytorch'\n)\n```\n\n----------------------------------------\n\nTITLE: Deploying InferenceService in Kubernetes\nDESCRIPTION: This Bash command applies the InferenceService configuration defined in the bentoml.yaml file to the Kubernetes cluster, enabling the deployment of the model API server.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/bentoml/README.md#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f bentoml.yaml\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Domain in ConfigMap\nDESCRIPTION: This snippet shows how to specify a custom domain in the 'data' section of the ConfigMap. The default domain should be removed to ensure the new domain is applied when routing traffic to Knative services. No specific output is generated directly from this action.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/custom-domain/README.md#2025-04-21_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: ConfigMap\ndata:\n  <custom_domain>: \"\"\nmetadata:\n...\n```\n\n----------------------------------------\n\nTITLE: Deploying Canary Version with Traffic Splitting\nDESCRIPTION: Updates the existing InferenceService to include a canary model version, configuring traffic distribution to route 10% of requests to the new model version\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/canary-testing/README.md#2025-04-21_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"sklearn-iris\"\nspec:\n  predictor:\n    canaryTrafficPercent: 10\n    model:\n      modelFormat:\n        name: sklearn\n      storageUri: \"gs://kfserving-examples/models/sklearn/1.0/model-2\"\n```\n\n----------------------------------------\n\nTITLE: Sending Prediction Request to InferenceService\nDESCRIPTION: Command to send a prediction request to the deployed model, after setting up the required environment variables for host and port.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/canary/README.md#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nMODEL_NAME=mnist\nSERVICE_HOSTNAME=$(kubectl get inferenceservice torchserve -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\n\ncurl -v -H \"Host: ${SERVICE_HOSTNAME}\" http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/${MODEL_NAME}:predict -d @./mnist.json\n```\n\n----------------------------------------\n\nTITLE: Installing Scikit-Learn Server for Development\nDESCRIPTION: Command to install the Scikit-Learn server for local development. This sets up the necessary dependencies for running the server.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/sklearnserver/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmake dev_install\n```\n\n----------------------------------------\n\nTITLE: Applying InferenceService Configuration\nDESCRIPTION: Command to apply the InferenceService custom resource to the Kubernetes cluster.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/spark/README.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f spark_pmml.yaml\n```\n\n----------------------------------------\n\nTITLE: Saving the Model (Python)\nDESCRIPTION: This snippet saves the trained model to a file named \"model.joblib\" using the `joblib.dump` function. This allows the model to be loaded and used later without retraining.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/sklearn/v1/sklearn-mixedtype-model/sklearn-mixedtype-model.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\njoblib.dump(model, \"model.joblib\")\n```\n\n----------------------------------------\n\nTITLE: Getting Outlier Detector Hostname - Python\nDESCRIPTION: This snippet retrieves the hostname for the outlier detection service deployed in the Kubernetes cluster, which will be used to send requests for outlier predictions.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/outlier-detection/alibi-detect/cifar10/cifar10_outlier.ipynb#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nSERVICE_HOSTNAMES = !(kubectl get ksvc -n cifar10 vae-outlier -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\nSERVICE_HOSTNAME_VAEOD = SERVICE_HOSTNAMES[0]\nprint(SERVICE_HOSTNAME_VAEOD)\n```\n\n----------------------------------------\n\nTITLE: Deploy TorchServe Predictor with Logger Using Bash\nDESCRIPTION: Deploys the TorchServe predictor YAML configuration to the Kubernetes cluster. This setup leverages a logger for inference events and assumes a running Kubernetes installation.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/logger/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f torchserve-logger.yaml -n kserve-test\n```\n\n----------------------------------------\n\nTITLE: Applying InferenceService Configuration to Kubernetes\nDESCRIPTION: This snippet demonstrates how to apply a Persistent Volume Claim (PVC) configuration file to an on-prem Kubernetes cluster using kubectl. The command updates and registers the InferenceService defined in the YAML file.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/storage/pvc/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f mnist-pvc.yaml\n```\n\n----------------------------------------\n\nTITLE: Checking InferenceService Status\nDESCRIPTION: Command to retrieve the status of the Triton InferenceService via kubectl, which provides details of readiness and revisions.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/multimodelserving/triton/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get isvc triton-mms\n```\n\n----------------------------------------\n\nTITLE: Postprocessing the Inference Output - Python\nDESCRIPTION: This snippet performs post-processing on the model's output. It clips the resulting values to the appropriate range, transposes the dimensions back for image format compatibility, and converts the numpy array back into an image format using the PIL library.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/onnx/mosaic-onnx.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\n# postprocess\nresult = np.clip(output1, 0, 255)\nresult = result.transpose(1, 2, 0).astype(\"uint8\")\nimg = Image.fromarray(result)\nimg\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Configuring InferenceService with Custom Prometheus Settings\nDESCRIPTION: YAML configuration for InferenceService with custom Prometheus port and path settings.\nSOURCE: https://github.com/kserve/kserve/blob/master/qpext/README.md#2025-04-21_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"sklearn-irisv2\"\n  annotations:\n    serving.kserve.io/enable-metric-aggregation: \"true\"\n    serving.kserve.io/enable-prometheus-scraping: \"true\"\n    prometheus.kserve.io/port: '8081'\n    prometheus.kserve.io/path: \"/other/metrics\"\nspec:\n  predictor:\n    sklearn:\n      protocolVersion: v2\n      storageUri: \"gs://kfserving-examples/models/sklearn/1.0/model\"\n```\n\n----------------------------------------\n\nTITLE: Sending Test Prediction Request to the API Server\nDESCRIPTION: This Bash command uses curl to send a test request to the model prediction endpoint of the BentoML API server. It is configured to send a sample iris flower measurement for classification.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/bentoml/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Send test request:\ncurl -i \\\n  --header \"Content-Type: application/json\" \\\n  --request POST \\\n  --data '[[5.1, 3.5, 1.4, 0.2]]' \\\n  http://localhost:5000/predict\n```\n\n----------------------------------------\n\nTITLE: Getting Drift Detector Service hostname\nDESCRIPTION: This snippet retrieves the hostname of the drift detector Knative Service using `kubectl`. It extracts the hostname from the service's URL. This is then used to construct the URL to call for predictions against the drift detector.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/drift-detection/alibi-detect/cifar10/cifar10_drift.ipynb#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n\"SERVICE_HOSTNAMES = !(kubectl get ksvc -n cifar10 drift-detector -o jsonpath='{.status.url}' | cut -d \\\"/\\\" -f 3)\nSERVICE_HOSTNAME_VAEOD = SERVICE_HOSTNAMES[0]\nprint(SERVICE_HOSTNAME_VAEOD)\"\n```\n\n----------------------------------------\n\nTITLE: Example of Inference Request in JSON\nDESCRIPTION: Provides an example JSON object representing a typical inference request to the server, complete with input and output specifications to guide server interaction. The example is crucial for developers to understand how to structure their requests for effective server processing.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md#2025-04-21_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\" : \"42\",\n  \"inputs\" : [\n    {\n      \"name\" : \"input0\",\n      \"shape\" : [ 2, 2 ],\n      \"datatype\" : \"UINT32\",\n      \"data\" : [ 1, 2, 3, 4 ]\n    },\n    {\n      \"name\" : \"input1\",\n      \"shape\" : [ 3 ],\n      \"datatype\" : \"BOOL\",\n      \"data\" : [ true ]\n    }\n  ],\n  \"outputs\" : [\n    {\n      \"name\" : \"output0\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Deploying KServe Model with YAML Configuration\nDESCRIPTION: Example of deploying a model using a raw InferenceService YAML configuration. This approach provides more fine-grained control over the deployment specifications than using the component's parameters.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/kfp/README.md#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nisvc_yaml = '''\napiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"sklearn-iris\"\n  namespace: \"anonymous\"\nspec:\n  predictor:\n    sklearn:\n      storageUri: \"gs://kfserving-examples/models/sklearn/iris\"\n'''\nkserve_op(\n    action='apply',\n    inferenceservice_yaml=isvc_yaml\n)\n```\n\n----------------------------------------\n\nTITLE: Referencing Secret in InferenceService\nDESCRIPTION: Describes how to reference a secret in an InferenceService using annotations to include credentials for accessing storage.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/storage/uri/README.md#2025-04-21_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: serving.kserve.io/v1beta1\nkind: InferenceService\nmetadata:\n  name: sklearn-from-uri\n  annotations:\n    serving.kserve.io/storageSecretName: mysecret\n\nspec:\n  predictor:\n    sklearn:\n      storageUri: https://github.com/tduffy000/kfserving-uri-examples/blob/master/sklearn/frozen/model.joblib?raw=true\n```\n\n----------------------------------------\n\nTITLE: Training Spark MLlib Model and Exporting to PMML\nDESCRIPTION: Python code to train a Decision Tree Classifier on Iris dataset using Spark MLlib and export the model to PMML format.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/spark/README.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.feature import RFormula\n\ndf = spark.read.csv(\"Iris.csv\", header = True, inferSchema = True)\n\nformula = RFormula(formula = \"Species ~ .\")\nclassifier = DecisionTreeClassifier()\npipeline = Pipeline(stages = [formula, classifier])\npipelineModel = pipeline.fit(df)\n\nfrom pyspark2pmml import PMMLBuilder\n\npmmlBuilder = PMMLBuilder(sc, df, pipelineModel)\n\npmmlBuilder.buildFile(\"DecisionTreeIris.pmml\")\n```\n\n----------------------------------------\n\nTITLE: Curl Cifar10 Model Metadata Endpoint\nDESCRIPTION: Curl command to access the metadata of the deployed Cifar10 model, verifying the model's availability and providing insights into its configuration.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/multimodelserving/triton/README.md#2025-04-21_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nMODEL_NAME=cifar10\nSERVICE_HOSTNAME=$(kubectl get inferenceservices triton-mms -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\ncurl -H \"Host: ${SERVICE_HOSTNAME}\" http://${INGRESS_HOST}:${INGRESS_PORT}/v2/models/$MODEL_NAME\n```\n\n----------------------------------------\n\nTITLE: Applying KServe Inference Service Configuration - Python\nDESCRIPTION: This snippet applies the cifar10.yaml to the Kubernetes cluster to deploy the inference service for the CIFAR10 model, enabling it to serve prediction requests.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/outlier-detection/alibi-detect/cifar10/cifar10_outlier.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n!kubectl apply -f cifar10.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining and Running Custom Model KServe Pipeline\nDESCRIPTION: This code defines a Kubeflow Pipeline for deploying a custom model using KServe. It uses the `kserve_op` component to define the KServe deployment and sets parameters such as the model name, namespace, and custom model specification. It compiles and runs the pipeline, deploying the specified custom model.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/pipelines/kfs-pipeline.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n\"# kfserving_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/master/components/kubeflow/kfserving/component.yaml')\nkserve_op = components.load_component_from_url(\n    \\\"https://raw.githubusercontent.com/kubeflow/pipelines/master/components/kserve/component.yaml\\\"\n)\n\n\n@dsl.pipeline(name=\\\"KServe pipeline\\\", description=\\\"A pipeline for KServe.\\\")\ndef kservePipeline(\n    action=\\\"apply\\\",\n    model_name=\\\"max-image-segmenter\\\",\n    namespace=\\\"anonymous\\\",\n    custom_model_spec='{\\\"name\\\": \\\"image-segmenter\\\", \\\"image\\\": \\\"codait/max-image-segmenter:latest\\\", \\\"port\\\": \\\"5000\\\"}',\n):\n\n    kserve = kserve_op(\n        action=action,\n        model_name=model_name,\n        namespace=namespace,\n        custom_model_spec=custom_model_spec,\n    ).set_image_pull_policy(\\\"Always\\\")\n\n\n# Compile pipeline\ncompiler.Compiler().compile(kservePipeline, \\\"custom.tar.gz\\\")\n\n# Execute pipeline\nrun = client.run_pipeline(experiment.id, \\\"custom-model\\\", \\\"custom.tar.gz\\\")\"\n```\n\n----------------------------------------\n\nTITLE: Verify Inference Service Health\nDESCRIPTION: This curl command checks the health status of the deployed model. It sends a request to the `/v2/models/${MODEL_NAME}` endpoint, which is a standard health check endpoint for KFServing models. The `Host` header is set to the service hostname to route the request correctly.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/onnx/README.md#2025-04-21_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ncurl -v -H \"Host:${SERVICE_HOSTNAME}\" http://localhost:8080/v2/models/${MODEL_NAME}\n```\n\n----------------------------------------\n\nTITLE: Creating LightGBM Model using Python\nDESCRIPTION: This Python script generates a LightGBM model using the Iris dataset and saves it to the local filesystem. It requires LightGBM and scikit-learn as dependencies. The model is saved in the specified directory for later use with a LightGBM server.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/lightgbm/README.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport lightgbm as lgb\nfrom sklearn.datasets import load_iris\nimport os\n\nmodel_dir = \".\"\nBST_FILE = \"model.bst\"\n\niris = load_iris()\ny = iris['target']\nX = iris['data']\ndtrain = lgb.Dataset(X, label=y)\n\nparams = {\n    'objective':'multiclass', \n    'metric':'softmax',\n    'num_class': 3\n}\nlgb_model = lgb.train(params=params, train_set=dtrain)\nmodel_file = os.path.join(model_dir, BST_FILE)\nlgb_model.save_model(model_file)\n\n```\n\n----------------------------------------\n\nTITLE: Server Metadata Error Response JSON Schema\nDESCRIPTION: Defines the structure for a server metadata error response with a descriptive error message\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md#2025-04-21_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"error\": $string\n}\n```\n\n----------------------------------------\n\nTITLE: Performing Inference Using OpenAI Completions API\nDESCRIPTION: cURL command to send a text completion request to the OpenAI-compatible completions endpoint. The request includes parameters like model name, prompt text, and maximum tokens to generate.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/huggingfaceserver/README.md#2025-04-21_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ncurl -H \"content-type:application/json\" -v localhost:8080/openai/v1/completions -d '{\"model\": \"gpt2\", \"prompt\": \"<prompt>\", \"stream\":false, \"max_tokens\": 30 }'\n```\n\n----------------------------------------\n\nTITLE: Making a Prediction - Python\nDESCRIPTION: This snippet defines a function to make predictions using the CIFAR10 model. It formats the input data, sends an HTTP POST request to the model endpoint, and processes the response to return the predicted class label.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/outlier-detection/alibi-detect/cifar10/cifar10_outlier.ipynb#2025-04-21_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndef predict(X):\n    formData = {\"instances\": X.tolist()}\n    headers = {}\n    headers[\"Host\"] = SERVICE_HOSTNAME_CIFAR10\n    res = requests.post(\n        \"http://\" + CLUSTER_IP + \"/v1/models/tfserving-cifar10:predict\",\n        json=formData,\n        headers=headers,\n    )\n    if res.status_code == 200:\n        return classes[np.array(res.json()[\"predictions\"])[0].argmax()]\n    else:\n        print(\"Failed with \", res.status_code)\n        return []\n```\n\n----------------------------------------\n\nTITLE: Retrieving Inference Service URL\nDESCRIPTION: Retrieves the URL of the deployed inference service using kubectl. This is necessary for accessing the service to make queries or for obtaining explanations from the model.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/aix/Fetch_20newsgroups/README.md#2025-04-21_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nkubectl get inferenceservice aix-explainer\n```\n\n----------------------------------------\n\nTITLE: Applying Kubernetes Configuration for Torchserve\nDESCRIPTION: This snippet demonstrates how to apply a Kubernetes configuration for an InferenceService using a custom Torchserve Docker image. The YAML file should have the container image pre-configured with the user's Docker Hub username. The primary function is to create an InferenceService resource in the Kubernetes cluster.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f torchserve-custom.yaml\n```\n\n----------------------------------------\n\nTITLE: Expected Server Response from Paddle ResNet50 Prediction\nDESCRIPTION: This shows the expected console output when making a prediction request to the Paddle ResNet50 model. It includes the HTTP connection details, request headers, and the beginning of the JSON response containing class probabilities for the image classification.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/paddle/README.md#2025-04-21_snippet_2\n\nLANGUAGE: console\nCODE:\n```\n*   Trying 127.0.0.1:80...\n* TCP_NODELAY set\n* Connected to localhost (127.0.0.1) port 80 (#0)\n> POST /v1/models/paddle-resnet50:predict HTTP/1.1\n> Host: paddle-resnet50.default.example.com\n> User-Agent: curl/7.68.0\n> Accept: */*\n> Content-Length: 3010209\n> Content-Type: application/x-www-form-urlencoded\n> Expect: 100-continue\n>\n* Mark bundle as not supporting multiuse\n< HTTP/1.1 100 Continue\n* We are completely uploaded and fine\n* Mark bundle as not supporting multiuse\n< HTTP/1.1 200 OK\n< content-length: 23399\n< content-type: application/json; charset=UTF-8\n< date: Mon, 17 May 2021 03:34:58 GMT\n< server: istio-envoy\n< x-envoy-upstream-service-time: 511\n<\n{\"predictions\": [[6.736678770380422e-09, 1.1535990829258935e-08, 5.142250714129659e-08, 6.647170636142619e-08, 4.094492567219277e-08, 1.3402451770616608e-07, 9.355561303436843e-08, 2.8935891904779965e-08, 6.845367295227334e-08, 7.680615965455218e-08, 2.0334689452283783e-06, 1.1085678579547675e-06, 2.3477592492326949e-07, 6.582037030966603e-07, 0.00012373103527352214, 4.2878804151769145e-07, 6.419959845516132e-06, 0.9993496537208557, 7.372002437477931e-05, 3.101135735050775e-05, 5.6028093240456656e-06, 2.1862508674530545e-06, 1.9544044604913324e-08, 3.728893887000595e-07, 4.2903633357127546e-07, 1.8251179767503345e-07, 7.159925985433802e-08, 9.231618136595898e-09, 6.469241498052725e-07, 7.031690341108288e-09, 4.451231561120039e-08, 1.2455971898361895e-07, 9.44632745358831e-08, 4.347704418705689e-08, 4.658220120745682e-07, 6.797721141538204e-08, 2.1060276367279585e-07, 2.2605123106700376e-08, 1.4311490303953178e-07, 7.951298641728499e-08, 1.2341783417468832e-07, 1.0921713737843675e-06, 1.5243892448779661e-05, 3.1173343018053856e-07, 2.4152058131221565e-07, 6.863762536113427e-08, 8.467682022228473e-08, 9.4246772164297e-08, 1.0219210366813058e-08, 3.3770753304906975e-08, 3.6928835100979995e-08, 1.3694031508748594e-07, 1.0674284567357972e-07, 2.599483650556067e-07, 3.4866405940192635e-07, 3.132053549848024e-08, 3.574873232992104e-07, 6.64843895492595e-08, 3.1638955988455564e-07, 1.2095878219042788e-06, 8.66409024524728e-08, 4.0144172430700564e-08, 1.2544761318622477e-07, 3.3201178695208e-08, 1.9731444922399533e-07, 3.806405572959193e-07, 1.3827865075199952e-07, 2.300225965257141e-08, 7.14422512260171e-08, 2.851114544455413e-08, 2.982567437470607e-08, 8.936032713791064e-08, 6.22388370175031e-07, 6.478838798784636e-08, 1.3663023423760023e-07, 9.973181391842445e-08, 2.5761554667269593e-08, 4.130220077058766e-08, 3.9384463690339544e-08, 1.2158079698565416e-07, 4.302821707824478e-06, 1.8179063090428826e-06, 1.8520155435908237e-06, 1.6246107179540559e-06, 1.6448313544970006e-05, 1.0544916221988387e-05, 3.993061909568496e-06, 2.646479799750523e-07, 1.9193475964129902e-05, 4.803242745765601e-07, 1.696285067964709e-07, 4.550505764200352e-06, 4.235929372953251e-05, 4.443338639248395e-06, 5.104009687784128e-06, 1.3506396498996764e-05, 4.1758724478313525e-07, 4.494491463447048e-07, 3.156698369366495e-07, 1.0557599807725637e-06, 1.336463917311903e-08, 1.3893659556174498e-08, 6.770379457066156e-08, 1.4129696523923485e-07, 7.170518756538513e-08, 7.934466594861078e-08, 2.639154317307657e-08, 2.6134321373660896e-08, 7.196725881897237e-09, 2.1752363466021052e-08, 6.684639686227456e-08, 3.417795824134373e-08, 1.6228275967478112e-07, 4.107114648377319e-07, 6.472135396506928e-07, 2.951379372007068e-07, 5.653474133282543e-09, 4.830144462175667e-08, 8.887481861563629e-09, 3.7306168820805397e-08, 1.7784264727538357e-08, 4.641905082536368e-09, 3.413118676576232e-08, 1.937393818707278e-07, 1.2980176506971475e-06, 3.5641004814124244e-08, 2.149332445355867e-08, 3.055293689158134e-07, 1.5532516783878236e-07, 1.4520978766086046e-06, 3.488464628276233e-08, 3.825438398052938e-05, 4.5088432898410247e-07, 4.1766969616219285e-07, 6.770622462681786e-07, 1.4142248971893423e-07, 1.4235997696232516e-05, 6.293820433711517e-07, 4.762866865348769e-06, 9.024900577969674e-07, 9.058987870957935e-07, 1.5713684433649178e-06, 1.5720647184025438e-07, 1.818536503606083e-07, 7.193188622522939e-08, 1.1952824934269302e-06, 8.874837362782273e-07, 2.0870831463071227e-07, 9.906239029078279e-08, 7.793621747964607e-09, 1.0058498389753368e-07, 4.2059440374941914e-07, 1.843624630737395e-07, 1.6437947181202617e-07, 7.025352743994517e-08, 2.570448600636155e-07, 7.586877615040066e-08, 7.841313731660193e-07, 2.495309274763713e-07, 5.157681925993529e-08, 4.0674127177453556e-08, 7.531796519799627e-09, 4.797485431140558e-08, 1.7419973019627832e-08, 1.7958679165985814e-07, 1.2566392371127222e-08, 8.975440124459055e-08, 3.26965476915575e-08, 1.1208359751435637e-07, 3.906746215420753e-08, 4.6769045525252295e-08, 1.8523553535487736e-07, 1.4833052830454108e-07, 1.2279349448363064e-07, 1.0729105497375713e-06, 3.6538490011395197e-09, 1.6198403329781286e-07, 1.6190719875908144e-08, 1.2004933580556099e-07, 1.4800277448046018e-08, 4.02294837442696e-08, 2.15060893538066e-07, 1.1925696696835075e-07, 4.8982514044837444e-08, 7.608920071788816e-08, 2.3137479487900237e-08, 8.521050176568679e-08, 9.586213423062873e-08, 1.3351650807180704e-07, 3.021699157557123e-08, 4.423876376336011e-08, 2.610667060309879e-08, 2.3977091245797055e-07, 1.3192564551900432e-07, 1.6734931662654162e-08, 1.588336999702733e-07, 4.0643516285854275e-07, 8.753454494581092e-08, 8.366999395548191e-07, 3.437598650180007e-08, 7.847892646850596e-08, 8.526394701391382e-09, 9.601382799928615e-08, 5.258924034023948e-07, 1.3557448141909845e-07, 1.0307226716577134e-07, 1.0429813457335513e-08, 5.187714435805901e-08, 2.187001335585137e-08, 1.1791439824548888e-08, 2.98065643278278e-08, 4.338393466696289e-08, 2.9991046091026874e-08, 2.8507610494443725e-08, 3.058665143385042e-08, 6.441099031917474e-08, 1.5364101102477434e-08, 1.5973883549236234e-08, 2.5736850872704053e-08, 1.0903765712555469e-07, 3.2118737891551064e-08, 6.819742992547617e-09, 1.9251311300649832e-07, 5.8258109447706374e-08, 1.8765761922168167e-07, 4.0070790419122204e-07, 1.5791577823165426e-08, 1.950158434738114e-07, 1.0142063189277906e-08, 2.744815041921811e-08, 1.2843531571604672e-08, 3.7297493094001766e-08, 7.407496838141014e-08, 4.20607833007125e-08, 1.6924804668860816e-08, 1.459203531339881e-07, 4.344977000414474e-08, 1.7191403856031684e-07, 3.5817443233554513e-08, 8.440249388286247e-09, 4.194829728021432e-08, 2.514032360068086e-08, 2.8340199520471288e-08, 8.747196034164517e-08, 8.277125651545703e-09, 1.1676293709683705e-08, 1.4548514570833504e-07, 7.200282148289716e-09, 2.623600948936655e-06, 5.675736929333652e-07, 1.9483527466945816e-06, 6.752595282932816e-08, 8.168475318370838e-08, 1.0933046468153407e-07, 1.670913718498923e-07, 3.1387276777650186e-08, 2.973524537708272e-08, 5.752163900751839e-08, 5.850877471402782e-08, 3.2544622285968217e-07, 3.330221431951941e-08, 4.186786668469722e-07, 1.5085906568401697e-07, 2.3346819943981245e-07, 2.86402780602657e-07, 2.2940319865938363e-07, 1.8537603807544656e-07, 3.151798182443599e-07, 1.1075967449869495e-06, 1.5369782602192572e-07, 1.9237509718550427e-07, 1.64044664074936e-07, 2.900835340824415e-07, 1.246654903752642e-07, 5.802622027317739e-08, 5.186220519703966e-08, 6.0094205167615655e-09, 1.2333241272699524e-07, 1.3798474185477971e-07, 1.7370231830682314e-07, 5.617761189569137e-07, 5.1604470030497396e-08, 4.813277598714194e-08, 8.032698417537176e-08, 2.0645263703045202e-06, 5.638597713186755e-07, 8.794199857220519e-07, 3.4785980460583232e-06, 2.972389268052211e-07, 3.3904532870110415e-07, 9.469074058188198e-08, 3.754845678827223e-08, 1.5679037801419327e-07, 8.203105039683578e-08, 6.847962641387539e-09, 1.8251624211984563e-08, 6.050240841659615e-08, 3.956342808919544e-08, 1.0699947949888156e-07, 3.2566634899922065e-07, 3.5369430406717584e-07, 7.326295303755614e-08, 4.85765610847011e-07, 7.717713401689252e-07, 3.4567779749750116e-08, 3.246204585138912e-07, 3.1608601602783892e-06, 5.33099466792919e-08, 3.645687343123427e-07, 5.48158936908294e-07, 4.62306957160763e-08, 1.3466177506415988e-07, 4.3529482240955986e-08, 1.6404105451783835e-07, 2.463695381038633e-08, 5.958712634424046e-08, 9.493651020875404e-08, 5.523462576206839e-08, 5.7412357534758485e-08, 1.1850350347231142e-05, 5.8263944993086625e-06, 7.4208674050169066e-06, 9.127966222877149e-07, 2.0019581370434025e-06, 1.033498961078294e-06, 3.5146850763112525e-08, 2.058995278275688e-06, 3.5655509122989315e-07, 6.873234070781109e-08, 2.1935298022413008e-09, 5.560363547374436e-08, 3.3266996979364194e-07, 1.307369217329324e-07, 2.718762992515167e-08, 1.0462929189714032e-08, 7.466680358447775e-07, 6.923166040451179e-08, 1.6145664361033596e-08, 8.568521003837759e-09, 4.76221018175238e-09, 1.233977116044116e-07, 8.340628632197422e-09, 3.2649041248333788e-09, 5.0632489312363305e-09, 4.0704994930251814e-09, 1.2043538610839732e-08, 5.105608380517879e-09, 7.267142887457112e-09, 1.184516307262129e-07, 7.53557927168913e-08, 6.386964201965384e-08, 1.6212936770898523e-08, 2.610429419291904e-07, 6.979425393183192e-07, 6.647513117741255e-08, 7.717492849224072e-07, 6.651206945207377e-07, 3.324495310152997e-07, 3.707282019149716e-07, 3.99564243025452e-07, 6.411632114122767e-08, 7.107352217872176e-08, 1.6380016631956096e-07, 6.876800995314625e-08, 3.462474467141874e-07, 2.0256503319160402e-07, 6.19610148078209e-07, 2.6841073363925716e-08, 6.720335363752383e-07, 1.1348340649419697e-06, 1.8397931853542104e-06, 6.397251581802266e-07, 7.257533241045167e-08, 4.2213909523525217e-07, 3.9657925299252383e-07, 1.4037439655112394e-07, 3.249856774800719e-07, 1.5857655455420172e-07,\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for ONNX Model Prediction - Python\nDESCRIPTION: This snippet sets up environment variables needed for connecting to the ONNX InferenceService. It retrieves the service URL using kubectl commands and sets the model name and ingress details. The variables configured include `MODEL_NAME`, `HOSTNAME`, `INGRESS_HOST`, `INGRESS_PORT`, and `SERVICE_HOSTNAME`.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/onnx/mosaic-onnx.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\n%env MODEL_NAME=style\nHOSTNAME=!(kubectl get inferenceservice \"style-sample\" -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\n%env INGRESS_HOST=localhost\n%env INGRESS_PORT=8080\n%env SERVICE_HOSTNAME={HOSTNAME[0]}\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Applying Persistent Volume Configuration with kubectl\nDESCRIPTION: This snippet applies the persistent volume configuration defined in the pv.yaml file using Kubernetes. Ensure that the volume ID in the pv.yaml file matches your setup before running the command. The expected output indicates successful creation of the volume.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/custom-server-with-external-storage.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f pv.yaml\n```\n\n----------------------------------------\n\nTITLE: Run PMML Prediction using curl\nDESCRIPTION: Executes a prediction request against the deployed PMML model using curl.  The script sets the MODEL_NAME, INPUT_PATH, and SERVICE_HOSTNAME variables before sending a POST request to the KServe endpoint.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/pmml/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nMODEL_NAME=pmml-demo\nINPUT_PATH=@./pmml-input.json\nSERVICE_HOSTNAME=$(kubectl get inferenceservice pmml-demo -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\ncurl -v -H \"Host: ${SERVICE_HOSTNAME}\" http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/$MODEL_NAME:predict -d $INPUT_PATH\n```\n\n----------------------------------------\n\nTITLE: Running Vegeta Performance Test on KServe\nDESCRIPTION: This Bash command initializes a Vegeta load test on the MultiModelInferenceService using 'cifar10' model in KServe. Essential for performance benchmarking, it requires a performance YAML file, 'perf.yaml', to be configured. Outputs include numeric metrics on request rates, durations, latencies, byte transfers, success ratios, and status codes.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/multimodelserving/triton/README.md#2025-04-21_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create -f perf.yaml\nRequests      [total, rate, throughput]         600, 10.02, 10.01\nDuration      [total, attack, wait]             59.912s, 59.9s, 11.755ms\nLatencies     [min, mean, 50, 90, 95, 99, max]  5.893ms, 11.262ms, 10.252ms, 16.077ms, 18.804ms, 26.745ms, 39.202ms\nBytes In      [total, mean]                     189000, 315.00\nBytes Out     [total, mean]                     66587400, 110979.00\nSuccess       [ratio]                           100.00%\nStatus Codes  [code:count]                      200:600  \nError Set:\n```\n\n----------------------------------------\n\nTITLE: Deploying Fluid with Helm\nDESCRIPTION: Add Fluid repository to Helm and install Fluid in the fluid-system namespace.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/fluid/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Add Fluid repository to Helm repos and keep it up-to-date\nhelm repo add fluid https://fluid-cloudnative.github.io/charts\nhelm repo update\n# Deploy Fluid with Helm\nhelm upgrade --install fluid --create-namespace --namespace=fluid-system fluid/fluid\n```\n\n----------------------------------------\n\nTITLE: JSON Input Example for Switch Node Testing\nDESCRIPTION: Sample JSON input for testing the Switch Node routing conditions, containing various fields that will be evaluated against the defined conditions in the graph configuration.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/graph/README.md#2025-04-21_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"source\": \"client1\",\n    \"instances\": [\n        {\n            \"name\": \"blue\",\n            \"intval\": 0,\n            \"strval\": \"kserve\"\n        },\n        {\n            \"name\": \"red\",\n            \"intval\":1,\n            \"strval\": \"1red-server1\"\n        }]\n}\n```\n\n----------------------------------------\n\nTITLE: Run Prediction against InferenceService\nDESCRIPTION: Executes a prediction request against the deployed InferenceService using curl.  It sets the Host header to the service hostname and sends a PUT request to the /predictions/mnist endpoint with the content of the 1.png file. Before this command, you must set `INGRESS_HOST` and `INGRESS_PORT`.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/docs/canary.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nMODEL_NAME=torchserve-custom\nSERVICE_HOSTNAME=$(kubectl get inferenceservice ${MODEL_NAME} -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\ncurl -v -H \"Host: ${SERVICE_HOSTNAME}\" http://${INGRESS_HOST}:${INGRESS_PORT}/predictions/mnist -T 1.png\n```\n\n----------------------------------------\n\nTITLE: Applying Outlier Detector Configuration - Python\nDESCRIPTION: This snippet applies the cifar10od.yaml to the Kubernetes cluster, effectively creating the outlier detection service which will respond to prediction requests.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/outlier-detection/alibi-detect/cifar10/cifar10_outlier.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n!kubectl apply -f cifar10od.yaml\n```\n\n----------------------------------------\n\nTITLE: Running a Prediction Request - Bash\nDESCRIPTION: This Bash snippet runs a prediction request against the deployed InferenceService using curl. It retrieves the service's host and port from the Kubernetes cluster and sends input data in JSON format for prediction.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/storage/storageSpec/README.md#2025-04-21_snippet_4\n\nLANGUAGE: Bash\nCODE:\n```\nMODEL_NAME=example-sklearn-isvc\nINPUT_PATH=@./iris-input.json\nSERVICE_HOSTNAME=$(kubectl get inferenceservice ${MODEL_NAME} -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\ncurl -v -H \"Host: ${SERVICE_HOSTNAME}\" http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/$MODEL_NAME:predict -d $INPUT_PATH\n```\n\n----------------------------------------\n\nTITLE: Applying Kubernetes CRD for LightGBM InferenceService\nDESCRIPTION: Applies a Kubernetes Custom Resource Definition (CRD) to create an InferenceService for LightGBM. This operation is part of setting up the KServe InferenceService and requires a Kubernetes cluster with KServe and Istio configured.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/lightgbm/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f lightgbm.yaml\n```\n\n----------------------------------------\n\nTITLE: Performing Inference Using OpenAI Chat Completions API\nDESCRIPTION: cURL command to send a chat completion request to the OpenAI-compatible chat completions endpoint. The request includes a message structure with role and content, targeting the GPT-2 model.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/huggingfaceserver/README.md#2025-04-21_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ncurl -H \"content-type:application/json\" -v localhost:8080/openai/v1/chat/completions -d '{\"model\": \"gpt2\", \"messages\": [{\"role\": \"user\",\"content\": \"<message>\"}], \"stream\":false }'\n```\n\n----------------------------------------\n\nTITLE: Load InferenceService with Concurrent Requests - KServe - Shell\nDESCRIPTION: Sends concurrent requests to an InferenceService using the 'hey' tool to simulate traffic. Parameters include model name and input JSON path. Requires determination of ingress IP and ports.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/autoscaling/README.md#2025-04-21_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nMODEL_NAME=flowers-sample\nINPUT_PATH=../tensorflow/input.json\nHOST=$(kubectl get inferenceservice $MODEL_NAME -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\nhey -z 30s -c 5 -m POST -host ${HOST} -D $INPUT_PATH http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/$MODEL_NAME:predict\n```\n\n----------------------------------------\n\nTITLE: Sending Prediction Request to Model\nDESCRIPTION: Constructs a JSON payload for prediction from the model based on a test sample and sends an HTTP POST request to the model's prediction endpoint.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/alibi/cifar10/cifar10_explanations.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom subprocess import PIPE, Popen, run\n\nimport numpy as np\n\nidx = 12\ntest_example = X_test[idx : idx + 1].tolist()\npayload = '{\"instances\":' + f\"{test_example}\" + \" }\"\ncmd = f\"\"\"curl -s -d '{payload}' \\\n   http://{CLUSTER_IP}/v1/models/cifar10:predict \\\n   -H \\\"Host: {SERVICE_HOSTNAME}\\\" \\\n   -H \\\"Content-Type: application/json\\\"\n\"\"\"\nret = Popen(cmd, shell=True, stdout=PIPE)\nraw = ret.stdout.read().decode(\"utf-8\")\nres = json.loads(raw)\nprint(res)\narr = np.array(res[\"predictions\"])\nX = X_test[idx].reshape(1, 32, 32, 3)\nplt.imshow(X.reshape(32, 32, 3))\nplt.axis(\"off\")\nplt.show()\nprint(\"class:\", class_names[y_test[idx][0]])\nprint(\"prediction:\", class_names[arr[0].argmax()])\n```\n\n----------------------------------------\n\nTITLE: Building a Docker Image for LightGBM Server\nDESCRIPTION: Build a custom Docker image for the LightGBM server. Run the docker build command from the python directory, specifying the Dockerfile and image name. Modify the LightGBM version in setup.py if needed.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/lgbserver/README.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t docker_user_name/lgbserver -f lgb.Dockerfile .\n```\n\n----------------------------------------\n\nTITLE: Creating working directory for BLOOM model conversion\nDESCRIPTION: Shell command to create and navigate to a working directory for the BLOOM model conversion process.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/triton/fastertransformer/README.md#2025-04-21_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nmkdir ~/triton-bloom\ncd ~/triton-bloom\n```\n\n----------------------------------------\n\nTITLE: Creating SKLearn Inference Service with Knative Queue Proxy\nDESCRIPTION: YAML configuration for deploying SKLearn iris model with ContainerConcurrency set to 8, utilizing Knative queue proxy and activator.\nSOURCE: https://github.com/kserve/kserve/blob/master/test/benchmark/README.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"sklearn-iris\"\nspec:\n  predictor:\n    containerConcurrency: 8 # CC=8\n    sklearn:\n      storageUri: \"gs://kfserving-examples/models/sklearn/1.0/model\"\n```\n\n----------------------------------------\n\nTITLE: Run prediction with tag URL - Curl\nDESCRIPTION: This shows how to route the requests to the canary or previous revisions by specifying the host using the tag URL.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/rollout/README.md#2025-04-21_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\ncurl -v -H \"Host: latest-my-model-predictor-default.default.example.com\" http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/$MODEL_NAME:predict -d $INPUT_PATH\nor\ncurl -v -H \"Host: prev-my-model-predictor-default.default.example.com\" http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/$MODEL_NAME:predict -d $INPUT_PATH\n```\n\n----------------------------------------\n\nTITLE: Running inference with the BLOOM model\nDESCRIPTION: Shell command demonstrating how to send a text generation request to the deployed BLOOM model via curl, providing prompt text and desired output length.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/triton/fastertransformer/README.md#2025-04-21_snippet_12\n\nLANGUAGE: sh\nCODE:\n```\ncurl -d '{\"inputs\":[{\"input\":\"Kubernetes is the best platform to serve your models because\",\"output_len\":\"18\"}]}' \\\n  http://bloom-560m-default.example.com/v1/models/fastertransformer:predict\n```\n\n----------------------------------------\n\nTITLE: Creating HTTP/HTTPS Header Secret in YAML\nDESCRIPTION: Defines a secret for HTTP/HTTPS service requests in Kubernetes. It requires headers and hosts to be base64 encoded. The secret must be in JSON format.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/storage/uri/README.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: Secret\nmetadata:\n  name: mysecret\ntype: Opaque\ndata:\n  https-host: ZXhhbXBsZS5jb20=\n  headers: |-\n    ewoiYWNjb3VudC1uYW1lIjogInNvbWVfYWNjb3VudF9uYW1lIiwKInNlY3JldC1rZXkiOiAic29tZV9zZWNyZXRfa2V5Igp9\n```\n\n----------------------------------------\n\nTITLE: Creating Knative Trigger for Event Routing\nDESCRIPTION: This YAML snippet creates a Knative trigger which routes events from the 'default' broker to the 'message-dumper' service, facilitating the logging of incoming events.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/logger/knative-eventing/README.md#2025-04-21_snippet_5\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: eventing.knative.dev/v1\nkind: Trigger\nmetadata:\n  name: message-dumper-trigger\nspec:\n  broker: default\n  subscriber:\n    ref:\n      apiVersion: serving.knative.dev/v1\n      kind: Service\n      name: message-dumper\n```\n\n----------------------------------------\n\nTITLE: Displaying SKLearn Logging YAML Configuration\nDESCRIPTION: Shows the sklearn-logging.yaml file that defines an SKLearn inference service with CloudEvent logging capabilities.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/logger/knative-eventing/logger_demo.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n!pygmentize sklearn-logging.yaml\n```\n\n----------------------------------------\n\nTITLE: Apply InferenceService YAML\nDESCRIPTION: Applies the InferenceService YAML file to create the InferenceService.  This deploys the model for serving.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/docs/metrics.md#2025-04-21_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n\"kubectl apply -f torchserve-custom.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Making Inference Request\nDESCRIPTION: Commands to set up and execute an inference request to the deployed model\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/metrics/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nMODEL_NAME=mnist\nSERVICE_HOSTNAME=$(kubectl get inferenceservice torch-metrics <namespace> -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\n\ncurl -v -H \"Host: ${SERVICE_HOSTNAME}\" http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/${MODEL_NAME}:predict -d @./mnist.json\n```\n\n----------------------------------------\n\nTITLE: Applying Inference Service YAML File\nDESCRIPTION: This command creates an Inference Service from a provided YAML file, deploying the model as a service under the specified namespace.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/OPENSHIFT_GUIDE.md#2025-04-21_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\noc apply -f ./samples/v1beta1/sklearn/v1/sklearn.yaml -n kserve-demo\n```\n\n----------------------------------------\n\nTITLE: Requesting Captum Explanations for BERT Model Predictions\nDESCRIPTION: Curl command to request explanations for BERT model predictions using Captum Insights, returning word importances and attributions for model interpretability.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v2/bert/README.md#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncurl -v -H \"Host: ${SERVICE_HOSTNAME}\" http://${INGRESS_HOST}:${INGRESS_PORT}/v2/models/BERTSeqClassification/explain -d ./sequence_classification/bytes/bert_v2.json\n```\n\n----------------------------------------\n\nTITLE: Creating a Sample Request (Python)\nDESCRIPTION: This snippet creates a sample request by extracting the first row from the DataFrame `df` (using the specified columns `cols`) and converting it to a dictionary. This can be used for testing the model.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/sklearn/v1/sklearn-mixedtype-model/sklearn-mixedtype-model.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nsample_request = df[cols].head(1).to_dict(\"records\")\nsample_request\n```\n\n----------------------------------------\n\nTITLE: Loading CIFAR10 Dataset and Preparing Data for Prediction - Python\nDESCRIPTION: This snippet loads the CIFAR10 dataset and normalizes the training and test data, preparing it for model predictions. It also prints the shape of the datasets for verification.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/outlier-detection/alibi-detect/cifar10/cifar10_outlier.ipynb#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport json\nimport tensorflow as tf\n\ntf.keras.backend.clear_session()\n\ntrain, test = tf.keras.datasets.cifar10.load_data()\nX_train, y_train = train\nX_test, y_test = test\n\nX_train = X_train.astype(\"float32\") / 255\nX_test = X_test.astype(\"float32\") / 255\nprint(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\nclasses = (\n    \"plane\",\n    \"car\",\n    \"bird\",\n    \"cat\",\n    \"deer\",\n    \"dog\",\n    \"frog\",\n    \"horse\",\n    \"ship\",\n    \"truck\",\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Secret with HDFS Credentials\nDESCRIPTION: Example of creating a Kubernetes secret for HDFS connection using kubectl with various configuration options\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/storage/hdfs/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ kubectl create secret generic hdfscreds \\\n    --from-literal=HDFS_NAMENODE=\"https://host1:port;https://host2:port\" \\\n    --from-file=TLS_CERT=./client.crt \\\n    --from-file=TLS_KEY=./client.key \\\n    --from-literal=TLS_SKIP_VERIFY=\"true\"\n    --from-literal=HDFS_ROOTPATH=\"/user/myuser\" \\\n    --from-literal=HEADERS='{\"x-my-container\": \"my-container\"}'\n```\n\n----------------------------------------\n\nTITLE: Getting Explanation for High Income Example Prediction with Python\nDESCRIPTION: This snippet retrieves explanations for the prediction made on a high income data point, similar to the low income explanation but applied to a different index.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/alibi/income/income_explanations.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nexp = explain(\n    adult.data[idxHigh : idxHigh + 1].tolist(), \"income\", SERVICE_HOSTNAME, CLUSTER_IP\n)\n```\n\n----------------------------------------\n\nTITLE: Preloading Data into Fluid Cache\nDESCRIPTION: Preload the model data into Fluid workers to improve performance.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/fluid/README.md#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# please update the `path` under `target`\nkubectl create -f dataload.yaml -n kserve-fluid-demo\n```\n\n----------------------------------------\n\nTITLE: Setting up Service Environment Variables\nDESCRIPTION: Commands to set up model name and service hostname variables for inference requests.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v1/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nMODEL_NAME=mnist\nSERVICE_HOSTNAME=$(kubectl get inferenceservice torchserve -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\n```\n\n----------------------------------------\n\nTITLE: Building Docker Images for CPU and GPU - TorchServe\nDESCRIPTION: This code snippet demonstrates how to build Docker images for TorchServe. It provides separate commands for building CPU and GPU versions of the image using Docker build. The CPU version uses the default Dockerfile, while the GPU version uses a base image with CUDA support.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/torchserve-image/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# For CPU:\nDOCKER_BUILDKIT=1 docker build --file Dockerfile -t torchserve:latest .\n\n# For GPU:\nDOCKER_BUILDKIT=1 docker build --file Dockerfile --build-arg BASE_IMAGE=nvidia/cuda:10.1-cudnn7-runtime-ubuntu18.04 -t torchserve-gpu:latest .\n\ndocker push {username}/torchserve:latest\n```\n\n----------------------------------------\n\nTITLE: Port Forward Prometheus Service\nDESCRIPTION: This command sets up port forwarding for the Prometheus service running in the `kfserving-monitoring` namespace, allowing access to the Prometheus UI through `localhost:9090`.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/metrics-and-monitoring/README.md#2025-04-21_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nkubectl port-forward service/prometheus-operated -n kfserving-monitoring 9090:9090\n```\n\n----------------------------------------\n\nTITLE: Deploying Inference Services and Graph with Kubectl\nDESCRIPTION: Shell commands to apply Kubernetes configuration for deploying inference services and checking their status\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/graph/README.md#2025-04-21_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nkubectl apply -f splitter.yaml\n```\n\nLANGUAGE: shell\nCODE:\n```\nkubectl get pods\n```\n\nLANGUAGE: shell\nCODE:\n```\nkubectl get isvc\n```\n\nLANGUAGE: shell\nCODE:\n```\nkubectl get ig\n```\n\n----------------------------------------\n\nTITLE: Testing Inference Graph Endpoint\nDESCRIPTION: Curl command to send test inference request to the splitter model endpoint\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/graph/README.md#2025-04-21_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://splitter-model.default.10.166.15.29.sslip.io -d @./iris-input.json\n```\n\n----------------------------------------\n\nTITLE: Setting variable for CA Bundle in storage-config Secret - YAML\nDESCRIPTION: This snippet illustrates how to set a variable in the storage-config Secret to associate the CA bundle with your storage configuration while using KServe.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/cabundle/README.md#2025-04-21_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nstringData:\n  localMinIO: |\n    {\n      \"type\": \"s3\",\n      \"access_key_id\": \"THEACCESSKEY\",\n      \"secret_access_key\": \"THEPASSWORD\",\n      \"endpoint_url\": \"https://minio.minio.svc:9000\",\n      \"bucket\": \"modelmesh-example-models\",\n      \"region\": \"us-south\",\n      \"cabundle_configmap\": \"local-cabundle\"\n    }\nkind: Secret\nmetadata:\n  name: storage-config\n  namespace: kserve-demo\ntype: Opaque\n```\n\n----------------------------------------\n\nTITLE: Custom Model Spec JSON Format\nDESCRIPTION: JSON specification format for a custom model serving runtime in KServe. This defines the container image, port, name, environment variables, and resources for a custom model deployment.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/kfp/README.md#2025-04-21_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"image\": \"some_image\",\n    \"port\": \"port_number\",\n    \"name\": \"custom-container\",\n    \"env\" : [{ \"name\": \"some_name\", \"value\": \"some_value\"}],\n    \"resources\": { \"requests\": {},  \"limits\": {}}\n}\n```\n\n----------------------------------------\n\nTITLE: Checking Running Pods in InferenceService\nDESCRIPTION: Command to check the pods running for both revisions of the model deployment.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/canary/README.md#2025-04-21_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get pods -l serving.kserve.io/inferenceservice=torchserve\nNAME                                                             READY   STATUS    RESTARTS   AGE\ntorchserve-predictor-default-9lttm-deployment-7dd5cff4cb-tmmlc   2/2     Running   0          21m\ntorchserve-predictor-default-kxp96-deployment-5d949864df-bmzfk   2/2     Running   0          20m\n```\n\n----------------------------------------\n\nTITLE: Defining Namespace for InferenceService Deployment\nDESCRIPTION: Set the Kubernetes namespace where the InferenceService will be deployed, with an option to use the default namespace or a specific one\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/client/kfserving_sdk_v1beta1_sample.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# namespace = utils.get_default_target_namespace()\nnamespace = \"kfserving-test\"\n```\n\n----------------------------------------\n\nTITLE: Defining prediction and drift functions\nDESCRIPTION: This snippet defines functions for making predictions with the Cifar10 model and for invoking the drift detector. The `predict` function sends a request to the Cifar10 InferenceService, and the `drift` function sends a request to the drift detector. The predict function sends requests to the `/v1/models/tfserving-cifar10:predict` endpoint, whereas the drift detector simply POSTs to `/`.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/drift-detection/alibi-detect/cifar10/cifar10_drift.ipynb#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n\"import matplotlib.pyplot as plt\nimport numpy as np\nimport requests\nimport json\nimport tensorflow as tf\n\ntf.keras.backend.clear_session()\n\ntrain, test = tf.keras.datasets.cifar10.load_data()\nX_train, y_train = train\nX_test, y_test = test\n\nX_train = X_train.astype(\\\"float32\\\") / 255\nX_test = X_test.astype(\\\"float32\\\") / 255\nprint(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\nclasses = (\n    \\\"plane\\\",\n    \\\"car\\\",\n    \\\"bird\\\",\n    \\\"cat\\\",\n    \\\"deer\\\",\n    \\\"dog\\\",\n    \\\"frog\\\",\n    \\\"horse\\\",\n    \\\"ship\\\",\n    \\\"truck\\\",\n)\n\n\ndef show(X):\n    plt.imshow(X.reshape(32, 32, 3))\n    plt.axis(\\\"off\\\")\n    plt.show()\n\n\ndef predict(X):\n    formData = {\\\"instances\\\": X.tolist()}\n    headers = {}\n    headers[\\\"Host\\\"] = SERVICE_HOSTNAME_CIFAR10\n    res = requests.post(\n        \\\"http://\\\" + CLUSTER_IP + \\\"/v1/models/tfserving-cifar10:predict\\\",\n        json=formData,\n        headers=headers,\n    )\n    if res.status_code == 200:\n        j = res.json()\n        if len(j[\\\"predictions\\\"]) == 1:\n            return classes[np.array(j[\\\"predictions\\\"])[0].argmax()]\n    else:\n        print(\\\"Failed with \\\", res.status_code)\n        return []\n\n\ndef drift(X):\n    formData = {\\\"instances\\\": X.tolist()}\n    headers = {\n        \\\"ce-namespace\\\": \\\"default\\\",\n        \\\"ce-modelid\\\": \\\"cifar10drift\\\",\n        \\\"ce-type\\\": \\\"io.seldon.serving.inference.request\\\",\n        \\\"ce-id\\\": \\\"1234\\\",\n        \\\"ce-source\\\": \\\"localhost\\\",\n        \\\"ce-specversion\\\": \\\"1.0\\\",\n    }\n    headers[\\\"Host\\\"] = SERVICE_HOSTNAME_VAEOD\n    res = requests.post(\\\"http://\\\" + CLUSTER_IP + \\\"/\\\", json=formData, headers=headers)\n    if res.status_code == 200:\n        od = res.json()\n        return od\n    else:\n        print(\\\"Failed with \\\", res.status_code)\n        return []\"\n```\n\n----------------------------------------\n\nTITLE: Uploading Model to S3 Bucket\nDESCRIPTION: Upload the downloaded model artifacts to an S3 bucket for use with KServe.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/fluid/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\naws s3 cp --recursive ${output_dir} s3://${bucket}/models/meta-llama--Meta-Llama-3.1-8B-Instruct\n```\n\n----------------------------------------\n\nTITLE: Loading Adult Dataset and Preparing Categorical Mapping with Python\nDESCRIPTION: This snippet imports necessary libraries, loads the adult dataset, and prepares a mapping for categorical features, allowing for structured handling of the dataset's features during explanation retrieval.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/alibi/income/income_explanations.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nimport sys\n\nsys.path.append(\"../\")\nfrom alibi_helper import *\nfrom alibi.datasets import fetch_adult\n\nadult = fetch_adult()\ncmap = dict.fromkeys(adult.category_map.keys())\nfor key, val in adult.category_map.items():\n    cmap[key] = {i: v for i, v in enumerate(val)}\n```\n\n----------------------------------------\n\nTITLE: Deploying and Testing Ensemble Node in KServe\nDESCRIPTION: Shell commands for deploying and testing an Ensemble Node that combines results from multiple inference services, with expected output showing predictions from all models.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/graph/README.md#2025-04-21_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nkubectl apply -f switch.yaml\n```\n\nLANGUAGE: shell\nCODE:\n```\nkubectl get pods\nNAME                                                              READY   STATUS    RESTARTS   AGE\nensemble-model-00001-deployment-7d48f984b6-qqqsh                  2/2     Running   0          32s\nsklearn-iris-predictor-default-00001-deployment-8495cbf8cbdqfjg   2/2     Running   0          52s\nxgboost-iris-predictor-default-00001-deployment-7b86bcdcf-7njrl   2/2     Running   0          50s\n\nkubectl get isvc\nNAME              URL                                                    READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                       AGE\nsklearn-iris      http://sklearn-iris.default.10.166.15.29.sslip.io      True           100                              sklearn-iris-predictor-default-00001      80m\nxgboost-iris      http://xgboost-iris.default.10.166.15.29.sslip.io      True           100                              xgboost-iris-predictor-default-00001      80m\n\nkubectl get ig\nNAME            URL                                                  READY   AGE\nensemble-model   http://ensemble-model.default.10.166.15.29.sslip.io   True    15m\n```\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://ensemble-model.default.10.166.15.29.sslip.io -d @./iris-input.json\n```\n\nLANGUAGE: shell\nCODE:\n```\n{\"sklearn-iris\":{\"predictions\":[1,1]},\"xgboost-iris\":{\"predictions\":[1,1]}}\n```\n\n----------------------------------------\n\nTITLE: Setting Ingress IP and Querying Explanations with Python\nDESCRIPTION: This Python command line code sets the necessary environment variables and performs a query to get explanations from the AIX model explainer. Dependencies include a configured Kubernetes cluster and the 'query_explain.py' script. Key parameters include INGRESS_HOST and INGRESS_PORT, which are set from the cluster's ingress information.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/aix/Fetch_20newsgroups/README.md#2025-04-21_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nMODEL_NAME=aix-explainer\nSERVICE_HOSTNAME=$(kubectl get inferenceservice ${MODEL_NAME} -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\npython query_explain.py http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/${MODEL_NAME}:explain ${SERVICE_HOSTNAME}\n```\n\n----------------------------------------\n\nTITLE: Defining Prediction Function in Python\nDESCRIPTION: This snippet defines a function to make predictions using the deployed sklearn model. It sends a POST request to the model's endpoint with the provided input data.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/logger/basic/logger_demo.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\n\ndef predict(X, name, svc_hostname, cluster_ip):\n    formData = {\"instances\": X}\n    headers = {}\n    headers[\"Host\"] = svc_hostname\n    res = requests.post(\n        \"http://\" + cluster_ip + \"/v1/models/\" + name + \":predict\",\n        json=formData,\n        headers=headers,\n    )\n    if res.status_code == 200:\n        return res.json()\n    else:\n        print(\"Failed with \", res.status_code)\n        return []\n```\n\n----------------------------------------\n\nTITLE: Verifying Model Access Through Custom Domain\nDESCRIPTION: This snippet provides a cURL command to verify accessing deployed models through a custom domain. It demonstrates how to construct a request to the model using the top-level domain and a specific subdomain, thereby confirming proper routing setup.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/custom-domain/README.md#2025-04-21_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ncurl -v \\\n    http://sklearn-iris.default.customdomain.com/v1/models/sklearn-iris:predict \\\n    -d @./input.json\n```\n\n----------------------------------------\n\nTITLE: Measuring Total Time in KServe Benchmark\nDESCRIPTION: This bash script command measures the total time taken for an inference request to complete within a specified timeout. It captures various components of processing time including pod initialization, model download, and inference time. Dependencies include a working KServe setup with defined environment variables such as SERVICE_HOSTNAME, INGRESS_HOST, and INGRESS_PORT.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/fluid/README.md#2025-04-21_snippet_10\n\nLANGUAGE: sh\nCODE:\n```\n# Total time includes: pod initialization and running + download model (storage initializer) + load model + inference + network\n$ curl --connect-timeout 3600 --max-time 3600 -o /dev/null -s -w 'Total: %{time_total}s\\n' -H \"Content-Type: application/json\" -H \"Host: ${SERVICE_HOSTNAME}\" \"http://${INGRESS_HOST}:${INGRESS_PORT}/openai/v1/completions\" -d '{\"model\": \"'\"$MODEL_NAME\"'\", \"prompt\": \"Write a poem about colors\", \"stream\":false, \"max_tokens\": 30}'\n# Total time: 53.037210s, status code: 200\n```\n\n----------------------------------------\n\nTITLE: Deploy Message Dumper Service to Cluster Using Bash\nDESCRIPTION: Utilizes kubectl to deploy the defined 'message-dumper' service YAML to the Kubernetes cluster within the 'kserve-test' namespace. Prerequisites include having a Kubernetes cluster and kubectl configured.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/logger/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f message-dumper.yaml -n kserve-test\n```\n\n----------------------------------------\n\nTITLE: Sending Traffic to InferenceService Using Shell\nDESCRIPTION: This snippet shows how to send concurrent requests to an InferenceService using the 'hey' tool. It demonstrates setting up the environment variables for model and host retrieval, and captures performance metrics for the requests sent to the service. Requires 'kubectl' and 'hey' as dependencies.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/autoscaling/README.md#2025-04-21_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nMODEL_NAME=flowers-sample-gpu\nINPUT_PATH=../tensorflow/input.json\nHOST=$(kubectl get inferenceservice $MODEL_NAME -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\nhey -z 30s -c 5 -m POST -host ${HOST} -D $INPUT_PATH http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/$MODEL_NAME:predict\n```\n\n----------------------------------------\n\nTITLE: Send Prediction Request (curl)\nDESCRIPTION: This snippet uses curl to send prediction requests to the `sklearn-iris` model. It constructs the curl command with headers and data to interact with the model's inference endpoint.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/metrics-and-monitoring/README.md#2025-04-21_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nwhile clear; do \\\n  curl -v \\\n  -H \"Host: ${SERVICE_HOSTNAME}\" \\\n  -d @./iris-input.json \\\n  http://${INGRESS_HOST}:${INGRESS_PORT}/v2/models/sklearn-iris/infer\n  sleep 0.3\ndone\n```\n\n----------------------------------------\n\nTITLE: Deploying InferenceService with Service Mesh Annotations\nDESCRIPTION: This bash script creates and applies an InferenceService resource with the necessary Service Mesh annotations. It deploys a sklearn model for iris classification and includes the required OpenShift-specific configurations.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/OPENSHIFT_GUIDE.md#2025-04-21_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\ncat <<EOF | oc apply -f -\napiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"sklearn-iris\"\n  namespace: kserve-demo\n  annotations:\n    sidecar.istio.io/inject: \"true\"\n    sidecar.istio.io/rewriteAppHTTPProbers: \"true\"\n    serving.knative.openshift.io/enablePassthrough: \"true\"\n    serving.kserve.io/storage-initializer-uid: \"1000860001\"\nspec:\n  predictor:\n    sklearn:\n      storageUri: \"gs://kfserving-examples/models/sklearn/1.0/model\"\nEOF\n```\n\n----------------------------------------\n\nTITLE: Configuring InferenceGraph with Service Mesh Annotations\nDESCRIPTION: This YAML snippet demonstrates the configuration for an InferenceGraph resource with the required annotations for Service Mesh integration on OpenShift. It includes the same annotations as the InferenceService for consistency.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/OPENSHIFT_GUIDE.md#2025-04-21_snippet_19\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: \"serving.kserve.io/v1alpha1\"\nkind: \"InferenceGraph\"\nmetadata:\n  ...\n  annotations:\n    sidecar.istio.io/inject: \"true\"\n    sidecar.istio.io/rewriteAppHTTPProbers: \"true\"\n    serving.knative.openshift.io/enablePassthrough: \"true\"\n    serving.kserve.io/storage-initializer-uid: \"1000860001\" # has to be changed to your namespaces value, see note above\nspec:\n...\n```\n\n----------------------------------------\n\nTITLE: Deploying PyTorch Batcher Service - Kubernetes Command\nDESCRIPTION: Kubernetes command to deploy the PyTorch inference service with batching configuration.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/batcher/basic/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create -f pytorch-batcher.yaml\n```\n\n----------------------------------------\n\nTITLE: Sending multiple prediction requests to trigger drift\nDESCRIPTION: This snippet sends 5000 prediction requests to the Cifar10 model in batches of 100. Since the `drift_batch_size` is set to 5000, the drift detector will run after all these requests have been processed.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/drift-detection/alibi-detect/cifar10/cifar10_drift.ipynb#2025-04-21_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n\"from tqdm.notebook import tqdm\n\nfor i in tqdm(range(0, 5000, 100)):\n    X = X_train[i : i + 100]\n    predict(X)\"\n```\n\n----------------------------------------\n\nTITLE: Apply rollback configuration - Kubernetes CLI\nDESCRIPTION: This command applies the Kubernetes resource defined in the `pinned.yaml` file.  This executes the rollback operation, routing all traffic to the previous stable revision of the model.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/rollout/README.md#2025-04-21_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f pinned.yaml\n```\n\n----------------------------------------\n\nTITLE: Getting ConfigMap for Loaded Models\nDESCRIPTION: This YAML snippet shows how to retrieve the ConfigMap that contains the configurations of the models with their storage URIs and other specifications.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/multimodelserving/sklearn/README.md#2025-04-21_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n\"apiVersion: v1\\ndata:\\n   models.json: '[{\\\\\"modelName\\\\\":\\\\\"model1-sklearn\\\\\",\\\\\"modelSpec\\\\\":{\\\\\"storageUri\\\\\":\\\\\"gs://kfserving-examples/models/sklearn/1.0/model\\\\\",\\\\\"framework\\\\\":\\\\\"sklearn\\\\\",\\\\\"memory\\\\\":\\\\\"256Mi\\\\\"}},{\\\\\"modelName\\\\\":\\\\\"model2-sklearn\\\\\",\\\\\"modelSpec\\\\\":{\\\\\"storageUri\\\\\":\\\\\"gs://kfserving-examples/models/sklearn/1.0/model\\\\\",\\\\\"framework\\\\\":\\\\\"sklearn\\\\\",\\\\\"memory\\\\\":\\\\\"256Mi\\\\\"}}]'\\nkind: ConfigMap\\nmetadata:\\n   creationTimestamp: \\\"2021-01-20T16:22:52Z\\\"\\n   name: modelconfig-sklearn-iris-example-0\\n   namespace: default\"\n```\n\n----------------------------------------\n\nTITLE: Checking TrainedModel Logs\nDESCRIPTION: Command to view logs of the TrainedModel deployment process, ensuring that download and load operations are completed successfully within the Triton Inference Server.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/multimodelserving/triton/README.md#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nkubectl logs triton-mms-predictor-default-2g8lg-deployment-69c9964bc4-mfg92 agent\n```\n\n----------------------------------------\n\nTITLE: Get Explanation for Positive Example\nDESCRIPTION: This snippet calls the `explain` function to get an explanation for the positive prediction at `idxPos`. It sends a request to the explainer service and retrieves the explanation.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/alibi/moviesentiment/movie_review_explanations.ipynb#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n\"exp = explain(\\n    movies.data[idxPos : idxPos + 1], \\\"moviesentiment\\\", SERVICE_HOSTNAME, CLUSTER_IP\\n)\"\n```\n\n----------------------------------------\n\nTITLE: Creating KServe InferenceService for TorchServe BERT Model\nDESCRIPTION: Command to apply the Kubernetes Custom Resource Definition (CRD) to create an InferenceService for the BERT model on KServe.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v2/bert/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f bert.yaml\n```\n\n----------------------------------------\n\nTITLE: Simulating User Requests for Canary Testing\nDESCRIPTION: Creates a continuous curl loop to simulate user traffic, sending prediction requests to the local Kubernetes service with predefined Iris dataset instances\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/canary-testing/README.md#2025-04-21_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nwhile true; do\n  curl -H 'Host: sklearn-iris.default.example.com' \\\n  http://localhost:8080/v1/models/sklearn-iris:predict \\\n  -d '{\"instances\": [[6.8,  2.8,  4.8,  1.4], [6.0,  3.4,  4.5,  1.6]]}';\n  echo \"\";\n  sleep 1\ndone\n```\n\n----------------------------------------\n\nTITLE: Deploying HuggingFace BERT Model on KServe\nDESCRIPTION: YAML configuration for deploying a BERT model on KServe using the HuggingFace runtime for both preprocessing/postprocessing and inference. Includes resource specifications and model loading parameters.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/huggingfaceserver/README.md#2025-04-21_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: serving.kserve.io/v1beta1\nkind: InferenceService\nmetadata:\n  name: huggingface-bert\nspec:\n  predictor:\n    model:\n      modelFormat:\n        name: huggingface\n      args:\n      - --model_name=bert\n      - --model_id=bert-base-uncased\n      - --tensor_input_names=input_ids\n      resources:\n        limits:\n          cpu: \"1\"\n          memory: 2Gi\n          nvidia.com/gpu: \"1\"\n        requests:\n          cpu: 100m\n          memory: 2Gi\n```\n\n----------------------------------------\n\nTITLE: Retrieving Service Hostname\nDESCRIPTION: Gets the hostname of the inference service deployed for the CIFAR10 model. This is used to route requests to the appropriate service in the Kubernetes cluster.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/alibi/cifar10/cifar10_explanations.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nSERVICE_HOSTNAMES = !(kubectl get inferenceservice cifar10 -n default -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\nSERVICE_HOSTNAME = SERVICE_HOSTNAMES[0]\nprint(SERVICE_HOSTNAME)\n```\n\n----------------------------------------\n\nTITLE: Deploying Default Broker\nDESCRIPTION: This shell command applies a YAML configuration for the default Knative event broker to the Kubernetes cluster using the 'kubectl' command-line tool.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/logger/knative-eventing/README.md#2025-04-21_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nkubectl create -f broker.yaml\n```\n\n----------------------------------------\n\nTITLE: Model Store Directory Structure for TorchServe\nDESCRIPTION: Shows the expected directory layout for TorchServe model store, including configuration and model archive files (MAR).\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n├── config\n│   ├── config.properties\n├── model-store\n│   ├── densenet_161.mar\n│   ├── mnist.mar\n```\n\n----------------------------------------\n\nTITLE: Storage Configuration Secret with HDFS and Kerberos\nDESCRIPTION: Kubernetes secret for storage configuration with HDFS and Kerberos authentication details\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/storage/hdfs/README.md#2025-04-21_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: Secret\nmetadata:\n  name: storage-config\ntype: Opaque\nstringData:\n  internalhdfs: |\n    {\n      \"type\": \"hdfs\",\n      \"HDFS_NAMENODE\": \"https://domain1:port;https://domain2:port\",\n      \"KERBEROS_PRINCIPAL\": \"myaccount@REALM\",\n      \"KERBEROS_KEYTAB\": \"<base64-encoded-data>\"\n    }\n```\n\n----------------------------------------\n\nTITLE: Deploying BERT Model with Triton Inference Server on KServe\nDESCRIPTION: YAML configuration for deploying a BERT model using Triton inference server for execution and KServe transformer with HuggingFace runtime for preprocessing/postprocessing. Shows a combined deployment with separate predictor and transformer.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/huggingfaceserver/README.md#2025-04-21_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: serving.kserve.io/v1beta1\nkind: InferenceService\nmetadata:\n  name: huggingface-triton\nspec:\n  predictor:\n    model:\n      args:\n      - --log-verbose=1\n      modelFormat:\n        name: triton\n      protocolVersion: v2\n      resources:\n        limits:\n          cpu: \"1\"\n          memory: 8Gi\n          nvidia.com/gpu: \"1\"\n        requests:\n          cpu: \"1\"\n          memory: 8Gi\n      runtimeVersion: 23.10-py3\n      storageUri: gs://kfserving-examples/models/triton/huggingface/model_repository\n  transformer:\n    containers:\n    - args:\n      - --model_name=bert\n      - --model_id=bert-base-uncased\n      - --predictor_protocol=v2\n      - --tensor_input_names=input_ids\n      image: kserve/huggingfaceserver:latest\n      name: kserve-container\n      resources:\n        limits:\n          cpu: \"1\"\n          memory: 2Gi\n        requests:\n          cpu: 100m\n          memory: 2Gi\n```\n\n----------------------------------------\n\nTITLE: Apply Canary Configuration\nDESCRIPTION: Applies the canary InferenceService configuration using kubectl. This command updates the existing InferenceService resource with the canary deployment settings.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/docs/canary.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f canary.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining ModelInfer Response Message in Protocol Buffers\nDESCRIPTION: Protocol buffer definition for model inference response message containing output tensor data, model information, and optional parameters. Supports both raw and structured response formats.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md#2025-04-21_snippet_16\n\nLANGUAGE: protobuf\nCODE:\n```\nmessage ModelInferResponse {\n  message InferOutputTensor {\n    string name = 1;\n    string datatype = 2;\n    repeated int64 shape = 3;\n    map<string, InferParameter> parameters = 4;\n    InferTensorContents contents = 5;\n  }\n\n  string model_name = 1;\n  string model_version = 2;\n  string id = 3;\n  map<string, InferParameter> parameters = 4;\n  repeated InferOutputTensor outputs = 5;\n  repeated bytes raw_output_contents = 6;\n}\n```\n\n----------------------------------------\n\nTITLE: Applying InferenceService Configuration - Bash\nDESCRIPTION: This command applies the InferenceService YAML configuration to the Kubernetes cluster, making the InferenceService available for serving predictions.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/storage/storageSpec/README.md#2025-04-21_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\nkubectl apply -f sklearn_storagespec.yaml\n```\n\n----------------------------------------\n\nTITLE: Generating gRPC Python Client Stub\nDESCRIPTION: Command to generate Python gRPC client stub from proto files.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v1/README.md#2025-04-21_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython -m grpc_tools.protoc --proto_path=serve/frontend/server/src/main/resources/proto/ --python_out=. --grpc_python_out=. serve/frontend/server/src/main/resources/proto/inference.proto serve/frontend/server/src/main/resources/proto/management.proto\n```\n\n----------------------------------------\n\nTITLE: Creating and Verifying InferenceService Deployment\nDESCRIPTION: Bash commands to deploy the InferenceService and verify its creation using kubectl.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/autoscaling/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f torchserve.yaml\n```\n\nLANGUAGE: bash\nCODE:\n```\n$inferenceservice.serving.kserve.io/torchserve created\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS ALB Annotations for Internal Load Balancer\nDESCRIPTION: This YAML snippet configures annotations for deploying an internal Application Load Balancer on AWS using the 'kfserving-ingress.yaml' file. Custom annotations are added to replace the default Classic Load Balancer configuration, facilitating advanced traffic handling in conjunction with services like 'external-dns'.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/custom-domain/README.md#2025-04-21_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nmetadata:\n  annotations:\n    kubernetes.io/ingress.class: alb\n    alb.ingress.kubernetes.io/scheme: internal\n```\n\n----------------------------------------\n\nTITLE: Making Bytes Input Inference Request to TorchServe\nDESCRIPTION: Sends a V2 inference request with bytes input format to the deployed TorchServe model using curl. Uses a pre-formatted JSON file with binary data for the request payload.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v2/README.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncurl -v -H \"Host: ${SERVICE_HOSTNAME}\" http://${INGRESS_HOST}:${INGRESS_PORT}/v2/models/${MODEL_NAME}/infer -d @./bytes_conv/mnist_v2_bytes.json\n```\n\n----------------------------------------\n\nTITLE: Training and Freezing Sklearn Model in Python\nDESCRIPTION: Details the process to train an iris dataset model using sklearn and freeze the model using joblib. Requires scikit-learn==1.0.2.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/storage/uri/README.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn import svm\nfrom sklearn import datasets\nimport joblib\n\ndef train(X, y):\n    clf = svm.SVC(gamma='auto')\n    clf.fit(X, y)\n    return clf\n\ndef freeze(clf, path='../frozen'):\n    joblib.dump(clf, f'{path}/model.joblib')\n    return True\n\nif __name__ == '__main__':\n    iris = datasets.load_iris()\n    X, y = iris.data, iris.target\n    clf = train(X, y)\n    freeze(clf)\n```\n\n----------------------------------------\n\nTITLE: Making Byte-Input Inference Request to TorchServe BERT Model\nDESCRIPTION: Curl command to send a byte-formatted inference request to the BERT model deployed on KServe, with the expected JSON response showing the prediction result.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v2/bert/README.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncurl -v -H \"Host: ${SERVICE_HOSTNAME}\" http://${INGRESS_HOST}:${INGRESS_PORT}/v2/models/BERTSeqClassification/infer -d @./sequence_classification/bytes/bert_v2.json\n```\n\n----------------------------------------\n\nTITLE: Defining Server Metadata Request and Response in Protobuf\nDESCRIPTION: Defines the ServerMetadata API's messages used to retrieve information about the server, including its name, version, and supported extensions. These messages help clients to adapt to different server capabilities effectively.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md#2025-04-21_snippet_13\n\nLANGUAGE: protobuf\nCODE:\n```\nmessage ServerMetadataRequest {}\n\nmessage ServerMetadataResponse\n{\n  string name = 1;\n  string version = 2;\n  repeated string extensions = 3;\n}\n```\n\n----------------------------------------\n\nTITLE: Sending Test Request to SKLearn Model\nDESCRIPTION: Uses curl to send a POST request with JSON input data to the locally deployed SKLearn Iris model for prediction.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/logger/local/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl -v -d @./input.json http://0.0.0.0:8081/v1/models/sklearn-iris:predict\n```\n\n----------------------------------------\n\nTITLE: Sending Prediction Request to Local InferenceService\nDESCRIPTION: This curl command sends a POST request to the local InferenceService for prediction, using data from input.json file.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/transformer/torchserve_image_transformer/README.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncurl localhost:8080/v1/models/mnist:predict --data @./input.json\n```\n\n----------------------------------------\n\nTITLE: Istio Gateway Configuration for Grafana\nDESCRIPTION: YAML configuration for exposing Grafana through Istio ingress with associated VirtualService and DestinationRule\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/metrics/README.md#2025-04-21_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: grafana-gateway\n  namespace: istio-system\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 80\n      name: http-grafana\n      protocol: HTTP\n    hosts:\n    - \"grafana.example.com\"\n---\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: grafana-vs\n  namespace: istio-system\nspec:\n  hosts:\n  - \"grafana.example.com\"\n  gateways:\n  - grafana-gateway\n  http:\n  - route:\n    - destination:\n        host: grafana\n        port:\n          number: 3000\n---\napiVersion: networking.istio.io/v1beta1\nkind: DestinationRule\nmetadata:\n  name: grafana\n  namespace: istio-system\nspec:\n  host: grafana\n  trafficPolicy:\n    tls:\n      mode: DISABLE\n---\n```\n\n----------------------------------------\n\nTITLE: Torchserve Config Properties for Metrics\nDESCRIPTION: Configuration properties for TorchServe to enable metrics. These properties configure the metrics address, enable the metrics API, and set the metrics format to Prometheus.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/docs/metrics.md#2025-04-21_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n\"metrics_address=http://0.0.0.0:8082\nenable_metrics_api=true\nmetrics_format=prometheus\"\n```\n\n----------------------------------------\n\nTITLE: Installing OpenShift Serverless with Bash\nDESCRIPTION: This snippet provides commands to install the OpenShift Serverless Operator, ensuring the necessary pods are running. It includes the step for installing the cert-manager operator as a prerequisite. Pods are checked for readiness as part of the process.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/OPENSHIFT_GUIDE.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install OpenShift Serverless operator\noc apply -f openshift/serverless/operator.yaml\n# This might take a moment until the pods appear in the following command\noc wait --for=condition=ready pod --all -n openshift-serverless --timeout=300s\npod/knative-openshift-5f598cf56b-hk2kb condition met                                                                                                                                                           ─╯\npod/knative-openshift-ingress-6546869495-tfg2b condition met\npod/knative-operator-webhook-86c5f88574-8vckn condition met\n```\n\nLANGUAGE: bash\nCODE:\n```\n# Install cert-manager operator\noc apply -f openshift/cert-manager/operator.yaml\noc wait --for=condition=ready pod --all -n cert-manager-operator --timeout=300s\n# This might take a moment until the pods appear in the following command\npod/cert-manager-operator-controller-manager-545ccc5977-lrcpz condition met\n```\n\n----------------------------------------\n\nTITLE: Deploying Inference Service in KFServing\nDESCRIPTION: This snippet applies the InferenceService Custom Resource Definition (CRD) for KFServing. It requires the namespace to be specified prior to applying the YAML configuration file. This command registers the service with the KFServing controller which manages its lifecycle.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/gcp-iap/README.md#2025-04-21_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nkubectl apply -f sklearn-iap-no-authz.yaml\n```\n\n----------------------------------------\n\nTITLE: Viewing CloudEvent Logs\nDESCRIPTION: Displays the logs from the message dumper pod to verify that CloudEvents were captured from the model prediction.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/logger/knative-eventing/logger_demo.ipynb#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n!kubectl logs  $(kubectl get pod -l serving.knative.dev/configuration=message-dumper -o jsonpath='{.items[0].metadata.name}') user-container\n```\n\n----------------------------------------\n\nTITLE: Defining Tensor Contents Message in Protocol Buffers\nDESCRIPTION: Protocol buffer definition for various tensor data types including boolean, integer, float, and bytes. Supports flattened, one-dimensional, row-major order tensor representations.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md#2025-04-21_snippet_18\n\nLANGUAGE: protobuf\nCODE:\n```\nmessage InferTensorContents {\n  repeated bool bool_contents = 1;\n  repeated int32 int_contents = 2;\n  repeated int64 int64_contents = 3;\n  repeated uint32 uint_contents = 4;\n  repeated uint64 uint64_contents = 5;\n  repeated float fp32_contents = 6;\n  repeated double fp64_contents = 7;\n  repeated bytes bytes_contents = 8;\n}\n```\n\n----------------------------------------\n\nTITLE: Expected Output from Prediction Request\nDESCRIPTION: Shows the expected response from the prediction request, including HTTP headers and the prediction result.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/canary/README.md#2025-04-21_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n*   Trying 52.89.19.61...\n* Connected to a881f5a8c676a41edbccdb0a394a80d6-2069247558.us-west-2.elb.amazonaws.com (52.89.19.61) port 80 (#0)\n> PUT /v1/models/mnist:predict HTTP/1.1\n> Host: torchserve.kserve-test.example.com\n> User-Agent: curl/7.47.0\n> Accept: */*\n> Content-Length: 167\n> Expect: 100-continue\n> \n< HTTP/1.1 100 Continue\n* We are completely uploaded and fine\n< HTTP/1.1 200 OK\n< cache-control: no-cache; no-store, must-revalidate, private\n< content-length: 1\n< date: Tue, 27 Oct 2020 08:26:19 GMT\n< expires: Thu, 01 Jan 1970 00:00:00 UTC\n< pragma: no-cache\n< x-request-id: b10cfc9f-cd0f-4cda-9c6c-194c2cdaa517\n< x-envoy-upstream-service-time: 6\n< server: istio-envoy\n< \n* Connection #0 to host a881f5a8c676a41edbccdb0a394a80d6-2069247558.us-west-2.elb.amazonaws.com left intact\n{\"predictions\": [\"2\"]}\n```\n\n----------------------------------------\n\nTITLE: Get Prometheus ServiceMonitor Configuration\nDESCRIPTION: This command retrieves the Prometheus ServiceMonitor object in YAML format from a specific namespace.  It allows inspection of which labels Prometheus expects on ServiceMonitor objects.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/metrics-and-monitoring/README.md#2025-04-21_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n$ kubectl get prometheus -n <name-of-the-namespace> -o yaml\n...\n  serviceMonitorNamespaceSelector: {}\n  serviceMonitorSelector:\n    matchLabels:\n      release: kube-prometheus-stack-1651295153\n```\n\n----------------------------------------\n\nTITLE: Checking TrainedModel Status\nDESCRIPTION: Commands to verify the status of the deployed TrainedModel in KFServing, ensuring the model is ready and available for inference requests.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/multimodelserving/triton/README.md#2025-04-21_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get tm cifar10\nkubectl get tm cifar10 -oyaml\n```\n\n----------------------------------------\n\nTITLE: Running Model Prediction and Logging - Python\nDESCRIPTION: Executes predictions using a deployed model and logs the payload for bias detection. The script 'simulate_predicts.py' sends a request to the InferenceService's endpoint, which requires the INGRESS_HOST and INGRESS_PORT to be set. Output is logged for analysis.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/aif/germancredit/README.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npython simulate_predicts.py http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/$MODEL_NAME:predict ${SERVICE_HOSTNAME}\n```\n\n----------------------------------------\n\nTITLE: Load Testing PyTorch Inference Service - Hey Command\nDESCRIPTION: Commands to set up and execute load testing using Hey tool against the deployed PyTorch inference service. Runs a 10-second test with 5 concurrent connections.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/batcher/basic/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nMODEL_NAME=pytorch-batcher\nINPUT_PATH=@./input.json\nSERVICE_HOSTNAME=$(kubectl get inferenceservice pytorch-batcher -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\nhey -z 10s -c 5 -m POST -host \"${SERVICE_HOSTNAME}\" -H \"Content-Type: application/json\" -D ./input.json \"http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/$MODEL_NAME:predict\"\n```\n\n----------------------------------------\n\nTITLE: Defining Feature Columns (Python)\nDESCRIPTION: This snippet defines the feature columns by combining the sorted numerical and categorical feature column names. This is useful for ensuring consistent feature order.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/sklearn/v1/sklearn-mixedtype-model/sklearn-mixedtype-model.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ncols = sorted(numerical_features.columns) + sorted(categorical_features.columns)\ncols\n```\n\n----------------------------------------\n\nTITLE: Making Tensor-Input Inference Request to TorchServe BERT Model\nDESCRIPTION: Curl command to send a tensor-formatted inference request to the BERT model deployed on KServe.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v2/bert/README.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncurl -v -H \"Host: ${SERVICE_HOSTNAME}\" http://${INGRESS_HOST}:${INGRESS_PORT}/v2/models/BERTSeqClassification/infer -d @./sequence_classification/tensor/bert_v2.json\n```\n\n----------------------------------------\n\nTITLE: Exporting Environment Variables for Ingress Configuration\nDESCRIPTION: Sets environment variables for accessing the ingress gateway. Adapt the commands based on the cluster setup to retrieve the necessary IPs and ports.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/multimodelserving/sklearn/README.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n\"export INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\\nexport INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name==\\\\\\\"http2\\\\\\\")].port}')\\nexport SERVICE_HOSTNAME=$(kubectl get inferenceservice sklearn-iris-example -n default -o jsonpath='{.status.url}' | cut -d / -f 3)\"\n```\n\n----------------------------------------\n\nTITLE: Deploying gRPC InferenceService\nDESCRIPTION: Command to deploy TorchServe InferenceService with gRPC protocol support.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v1/README.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f grpc.yaml\n```\n\n----------------------------------------\n\nTITLE: Deploying TorchServe InferenceService\nDESCRIPTION: Command to deploy the TorchServe InferenceService on CPU using kubectl.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v1/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f torchserve.yaml\n```\n\n----------------------------------------\n\nTITLE: Port Forwarding for Grafana and Prometheus\nDESCRIPTION: Sets up port forwarding to access Grafana and Prometheus instances locally. This allows users to access the Grafana dashboard and Prometheus UI from their local machine.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/docs/metrics.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n\n\"# Grafana\nkubectl port-forward --namespace knative-monitoring $(kubectl get pods --namespace knative-monitoring \\\n--selector=app=grafana --output=jsonpath=\\\"{.items..metadata.name}\\\") 3000\n\n# Prometheus\nkubectl port-forward -n knative-monitoring $(kubectl get pods -n knative-monitoring \\\n--selector=app=prometheus --output=jsonpath=\\\"{.items[0].metadata.name}\\\") 9090\"\n\n```\n\n----------------------------------------\n\nTITLE: Cloning HuggingFace BLOOM repository\nDESCRIPTION: Shell commands to install git-lfs and clone the HuggingFace BLOOM 560m model repository, which will be converted to FasterTransformer format.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/triton/fastertransformer/README.md#2025-04-21_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n# Make sure you have git-lfs installed (https://git-lfs.com)\ngit lfs install\ngit clone https://huggingface.co/bigscience/bloom-560m\n```\n\n----------------------------------------\n\nTITLE: Verifying Outlier Detection Result from Logs - Python\nDESCRIPTION: This snippet checks the logs to confirm if the masked image was detected as an outlier. It processes the log data similar to previous examples.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/outlier-detection/alibi-detect/cifar10/cifar10_outlier.ipynb#2025-04-21_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nres = !kubectl logs -n cifar10 $(kubectl get pod -n cifar10 -l app=hello-display -o jsonpath='{.items[0].metadata.name}')\ndata = []\nfor i in range(0, len(res)):\n    if res[i] == \"Data,\":\n        data.append(res[i + 1])\nj = json.loads(json.loads(data[-1]))\nprint(\"Outlier\", j[\"data\"][\"is_outlier\"] == [1])\n```\n\n----------------------------------------\n\nTITLE: Viewing Message Dumper Logs in Python\nDESCRIPTION: This snippet retrieves and displays the logs from the message-dumper pod, showing the CloudEvents received by the service.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/logger/basic/logger_demo.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n!kubectl logs  $(kubectl get pod -l serving.knative.dev/configuration=message-dumper -o jsonpath='{.items[0].metadata.name}') user-container\n```\n\n----------------------------------------\n\nTITLE: Sending Inference Request to sklearn Model with curl\nDESCRIPTION: These shell commands send a request to the deployed sklearn model, using curl to interact with the InferenceService endpoint. Set up the INGRESS_HOST and INGRESS_PORT following official guidance.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/logger/basic/README.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nMODEL_NAME=sklearn-iris\nINPUT_PATH=@./iris-input.json\nSERVICE_HOSTNAME=$(kubectl get inferenceservice sklearn-iris -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\ncurl -v -H \"Host: ${SERVICE_HOSTNAME}\" http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/$MODEL_NAME:predict -d $INPUT_PATH\n\n```\n\n----------------------------------------\n\nTITLE: Sample Prediction Response from BLOOM Model\nDESCRIPTION: Expected output from the curl request to the BLOOM model endpoint, showing the HTTP request/response flow and successful acceptance of the prediction request.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v1/bloom/README.md#2025-04-21_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\n*   Trying 44.239.20.204...\n* Connected to a881f5a8c676a41edbccdb0a394a80d6-2069247558.us-west-2.elb.amazonaws.com (44.239.20.204) port 80 (#0)\n> PUT /v1/models/BLOOMSeqClassification:predict HTTP/1.1\n> Host: torchserve-bloom-560m.kserve-test.example.com\n> User-Agent: curl/7.47.0\n> Accept: */*\n> Content-Length: 79\n> Expect: 100-continue\n>\n< HTTP/1.1 100 Continue\n* We are completely uploaded and fine\n< HTTP/1.1 200 OK\n< cache-control: no-cache; no-store, must-revalidate, private\n< content-length: 8\n< date: Wed, 04 Nov 2020 10:54:49 GMT\n< expires: Thu, 01 Jan 1970 00:00:00 UTC\n< pragma: no-cache\n< x-request-id: 4b54d3ac-185f-444c-b344-b8a785fdeb50\n< x-envoy-upstream-service-time: 2085\n< server: istio-envoy\n<\n* Connection #0 to host torchserve-bloom-560m.kserve-test.example.com left intact\nAccepted\n```\n\n----------------------------------------\n\nTITLE: Applying Knative Broker configuration\nDESCRIPTION: This snippet applies the Knative Broker configuration defined in the `broker.yaml` file. This command creates the Knative Broker in the `cifar10` namespace using the definition in the broker.yaml file.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/drift-detection/alibi-detect/cifar10/cifar10_drift.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n\"!kubectl create -f broker.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Running gRPC Inference\nDESCRIPTION: Command to run inference using the gRPC client.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v1/README.md#2025-04-21_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npython torchserve_gprc_client.py infer <model_name> <input json>\n```\n\n----------------------------------------\n\nTITLE: Defining KServe InferenceService Model Properties\nDESCRIPTION: Describes the core properties of an InferenceService in KServe, including API version, kind, metadata, specification, and status attributes. Used for defining machine learning service resources in Kubernetes.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/docs/V1beta1InferenceService.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass V1beta1InferenceService:\n    def __init__(self, \n        api_version=None, \n        kind=None, \n        metadata=None, \n        spec=None, \n        status=None\n    ):\n        self.api_version = api_version  # Versioned schema representation\n        self.kind = kind  # REST resource representation\n        self.metadata = metadata  # Kubernetes object metadata\n        self.spec = spec  # Inference service specification\n        self.status = status  # Current status of the inference service\n```\n\n----------------------------------------\n\nTITLE: Displaying corrupted CIFAR10 examples\nDESCRIPTION: This snippet displays the first three corrupted CIFAR10 examples.  It uses the `show` function defined earlier to visualize the images.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/drift-detection/alibi-detect/cifar10/cifar10_drift.ipynb#2025-04-21_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n\"show(X_corr[0])\nshow(X_corr[1])\nshow(X_corr[2])\"\n```\n\n----------------------------------------\n\nTITLE: Outlier Detection for Masked Image - Python\nDESCRIPTION: This snippet makes a direct call to the outlier detector with the masked image to check if it is classified as an outlier.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/outlier-detection/alibi-detect/cifar10/cifar10_outlier.ipynb#2025-04-21_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nod_preds = outlier(X_mask)\n```\n\n----------------------------------------\n\nTITLE: Making Tensor Input Inference Request to TorchServe\nDESCRIPTION: Sends a V2 inference request with tensor input format to the deployed TorchServe model using curl. Uses a pre-formatted JSON file for the request payload.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v2/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nMODEL_NAME=mnist\ncurl -v -H \"Host: ${SERVICE_HOSTNAME}\" http://${INGRESS_HOST}:${INGRESS_PORT}/v2/models/${MODEL_NAME}/infer -d @./tensor_conv/mnist_v2.json\n```\n\n----------------------------------------\n\nTITLE: Starting KFServing Logger\nDESCRIPTION: Launches the KFServing logger with specific configuration parameters, including log URL, component port, and inference service details.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/logger/local/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbin/logger --log-url http://0.0.0.0:8000 --component-port 8080 --log-mode all --inference-service=iris --namespace=default --endpoint=default\n```\n\n----------------------------------------\n\nTITLE: Deploying sklearn InferenceService with kubectl\nDESCRIPTION: This command applies the 'sklearn-logging.yaml' file, deploying an InferenceService in the Kubernetes cluster. Ensure 'kubectl' is properly configured for your cluster.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/logger/basic/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create -f sklearn-logging.yaml\n```\n\n----------------------------------------\n\nTITLE: Applying Income Prediction Model Deployment with Python\nDESCRIPTION: This command applies the income.yaml configuration to deploy the specified resources, including the SKLearn model and the necessary service in the Kubernetes cluster.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/alibi/income/income_explanations.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\n!kubectl apply -f income.yaml\n```\n\n----------------------------------------\n\nTITLE: Import Alibi Helper functions\nDESCRIPTION: This snippet imports the `alibi_helper` module, which likely contains helper functions for interacting with the Alibi explainer.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/alibi/moviesentiment/movie_review_explanations.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n\"import sys\\n\\nsys.path.append(\\\"../\\\")\\nfrom alibi_helper import *\"\n```\n\n----------------------------------------\n\nTITLE: Verifying Scikit-Learn Server Installation\nDESCRIPTION: Command to check if the Scikit-Learn server is installed correctly. It displays the usage information and required arguments for the server.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/sklearnserver/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m sklearnserver\n```\n\n----------------------------------------\n\nTITLE: Making Prediction with SKLearn Model in Python\nDESCRIPTION: This snippet calls the predict function to make a prediction using the deployed sklearn-iris model.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/logger/basic/logger_demo.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npredict([[6.8, 2.8, 4.8, 1.4]], \"sklearn-iris\", SERVICE_HOSTNAME, CLUSTER_IP)\n```\n\n----------------------------------------\n\nTITLE: Setting MinReplicas to 0 for Zero Downtime Scaling\nDESCRIPTION: This YAML configuration sets the 'minReplicas' of the InferenceService to 0, allowing automatic scaling down to zero when there is no traffic, which is useful for optimizing resource usage in GPU-based environments. Requires KServe installed.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/autoscaling/README.md#2025-04-21_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"flowers-sample\"\nspec:\n  predictor:\n    minReplicas: 0\n    tensorflow:\n      storageUri: \"gs://kfserving-examples/models/tensorflow/flowers\"\n```\n\n----------------------------------------\n\nTITLE: Deploying KServe InferenceService\nDESCRIPTION: Command to apply the KServe InferenceService configuration for the Torchserve BERT model\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/bert-sample/hugging-face-bert-sample.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f bert.yaml\n```\n\n----------------------------------------\n\nTITLE: Performing Inference on a HuggingFace Model\nDESCRIPTION: cURL command to send a prediction request to the deployed HuggingFace model. The example shows a masked language modeling request for BERT to predict the missing word in the sentence.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/huggingfaceserver/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl -H \"content-type:application/json\" -v localhost:8080/v1/models/bert:predict -d '{\"instances\": [\"The capital of france is [MASK].\"] }'\n```\n\n----------------------------------------\n\nTITLE: List pods - Kubernetes CLI\nDESCRIPTION: This command lists the pods associated with the InferenceService `my-model`. It helps to verify that both the original and canary model pods are running.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/rollout/README.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get pods -l serving.kserve.io/inferenceservice=my-model\n```\n\n----------------------------------------\n\nTITLE: Creating and Mounting NVMe Drive Directory\nDESCRIPTION: Bash commands to create a mount directory, set permissions, and mount the NVMe drive. This prepares the storage for use with Kubernetes persistent volumes.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v1/bloom/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ sudo mkdir -p /mnt/data/vol1\n$ sudo chmod -R 777 /mnt     \n$ sudo mount /dev/nvme1n1 /mnt/data/vol1\n```\n\n----------------------------------------\n\nTITLE: Getting Inference Service Hostname\nDESCRIPTION: Retrieves the hostname of the deployed SKLearn inference service to be used in request headers.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/logger/knative-eventing/logger_demo.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nSERVICE_HOSTNAMES = !(kubectl get inferenceservice sklearn-iris -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\nSERVICE_HOSTNAME = SERVICE_HOSTNAMES[0]\nprint(SERVICE_HOSTNAME)\n```\n\n----------------------------------------\n\nTITLE: Canary Deployment with KServe - Rollback\nDESCRIPTION: Alternative to promotion, rolling back a canary deployment by setting traffic to the candidate model to 0%. This reverts back to the original model if issues are found with the candidate.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/kfp/README.md#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nkserve_op(\n    action='apply',\n    model_name='tf-sample',\n    model_uri='gs://kfserving-examples/models/tensorflow/flowers-2',\n    framework='tensorflow',\n    canary_traffic_percent='0'\n)\n```\n\n----------------------------------------\n\nTITLE: Applying Kubernetes Secret - Bash\nDESCRIPTION: This Bash command applies the previously defined Secret YAML configuration to the Kubernetes cluster, creating the storage secret required for KServe operations.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/storage/storageSpec/README.md#2025-04-21_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\nkubectl apply -f common_secret.yaml\n```\n\n----------------------------------------\n\nTITLE: Processing the Inference Response - Python\nDESCRIPTION: This snippet extracts the output data from the inference response. It converts the received JSON response into a numpy array, reshaping it to the expected output format (3 channels by 224 by 224 pixels) for further post-processing.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/onnx/mosaic-onnx.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\nresponse_message = json.loads(response.text)\noutput1 = np.array(response_message[\"outputs\"][0][\"data\"], dtype=np.float32)\noutput1 = output1.reshape(3, 224, 224)\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Sending Request to Sklearn Inference Service\nDESCRIPTION: This shell script sends a request to the Sklearn inference service, using variables to determine the service host and sends a JSON input using 'curl'. It requires a pre-set INGRESS_HOST and INGRESS_PORT.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/logger/knative-eventing/README.md#2025-04-21_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nMODEL_NAME=sklearn-iris\nINPUT_PATH=@./iris-input.json\nSERVICE_HOSTNAME=$(kubectl get inferenceservice sklearn-iris -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\ncurl -v -H \"Host: ${SERVICE_HOSTNAME}\" \"http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/$MODEL_NAME:predict\" -d $INPUT_PATH\n```\n\n----------------------------------------\n\nTITLE: Showing Feature Coverage and Examples from Explanation with Python\nDESCRIPTION: This snippet utilizes functions to display feature coverage provided by the explanation data, along with specific examples from the adult dataset related to the prediction output.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/alibi/income/income_explanations.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nshow_feature_coverage(exp[\"data\"])\n\nshow_examples(exp[\"data\"], 0, adult)\nshow_examples(exp[\"data\"], 0, adult, False)\n```\n\n----------------------------------------\n\nTITLE: Enabling Node Selector in KServe InferenceService\nDESCRIPTION: Patch the KNative config to enable node selector and tolerations for KServe InferenceService.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/fluid/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkubectl patch configmap/config-features \\\n  --namespace knative-serving \\\n  --type merge \\\n  --patch '{\"data\":{\"kubernetes.podspec-nodeselector\":\"enabled\", \"kubernetes.podspec-tolerations\":\"enabled\"}}'\n```\n\n----------------------------------------\n\nTITLE: Applying Kubernetes YAML Configuration\nDESCRIPTION: Applies the cifar10.yaml configuration file to the Kubernetes cluster in the default namespace, which manages the service deployment.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/alibi/cifar10/cifar10_explanations.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n!kubectl apply -f cifar10.yaml -n default\n```\n\n----------------------------------------\n\nTITLE: Check KServe InferenceService Status\nDESCRIPTION: This is the expected output when the `onnx.yaml` file is applied successfully. It shows that the `style-sample` InferenceService is configured, indicating that the KServe controller has processed the CRD.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/onnx/README.md#2025-04-21_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n$ inferenceservice.serving.kserve.io/style-sample configured\n```\n\n----------------------------------------\n\nTITLE: Sending normal prediction request\nDESCRIPTION: This snippet demonstrates sending a normal prediction request to the Cifar10 model.  It selects an image from the training dataset, displays it, and then calls the `predict` function to classify the image.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/drift-detection/alibi-detect/cifar10/cifar10_drift.ipynb#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n\"idx = 1\nX = X_train[idx : idx + 1]\nshow(X)\npredict(X)\"\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Docker\nDESCRIPTION: This bash script builds a Docker image for a Logistic Regression model using the specified Dockerfile. Ensure you have Docker installed and replace 'dockeruser' with your actual Docker username. The key parameter includes '-t' for tagging the image.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/aif/germancredit/server/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t dockeruser/aifserver:latest -f aiffairness.Dockerfile .\n```\n\n----------------------------------------\n\nTITLE: Creating Trained Models for SKLearn\nDESCRIPTION: This YAML snippet defines two TrainedModels which are associated with the previously created InferenceService. It specifies the model's storage URI, framework, and requested memory.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/multimodelserving/sklearn/README.md#2025-04-21_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n\"apiVersion: \\\"serving.kserve.io/v1alpha1\\\"\\nkind: \\\"TrainedModel\\\"\\nmetadata:\\n  name: \\\"model1-sklearn\\\"\\nspec:\\n  inferenceService: \\\"sklearn-iris-example\\\"\\n  model:\\n    storageUri: \\\"gs://kfserving-examples/models/sklearn/1.0/model\\\"\\n    framework: \\\"sklearn\\\"\\n    memory: \\\"256Mi\\\"\\n---\\napiVersion: \\\"serving.kserve.io/v1alpha1\\\"\\nkind: \\\"TrainedModel\\\"\\nmetadata:\\n  name: \\\"model2-sklearn\\\"\\nspec:\\n  inferenceService: \\\"sklearn-iris-example\\\"\\n  model:\\n    storageUri: \\\"gs://kfserving-examples/models/sklearn/1.0/model\\\"\\n    framework: \\\"sklearn\\\"\\n    memory: \\\"256Mi\\\"\"\n```\n\n----------------------------------------\n\nTITLE: Deploying InferenceService with KServe v1beta1 API\nDESCRIPTION: Command to apply the YAML configuration for deploying the ART InferenceService using kubectl.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/art/mnist/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f art.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing KFP Library\nDESCRIPTION: This shell command installs or upgrades the Kubeflow Pipelines (KFP) library using pip3. It ensures that the KFP library is available for defining and running pipelines.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/pipelines/kfs-pipeline.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"!pip3 install kfp --upgrade\"\n```\n\n----------------------------------------\n\nTITLE: Curl Simple String Model Metadata Endpoint\nDESCRIPTION: Curl command to access the metadata endpoint for the Simple String model, checking its configuration in the Triton Inference Server.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/multimodelserving/triton/README.md#2025-04-21_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nMODEL_NAME=simple-string\nSERVICE_HOSTNAME=$(kubectl get inferenceservices triton-mms -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\ncurl -H \"Host: ${SERVICE_HOSTNAME}\" http://${INGRESS_HOST}:${INGRESS_PORT}/v2/models/$MODEL_NAME\n```\n\n----------------------------------------\n\nTITLE: Check Knative Dashboard - Knative - Shell\nDESCRIPTION: Forwards Grafana dashboard to local machine for viewing Knative Serving metrics. Assumes Grafana is installed in the knative-monitoring namespace.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/autoscaling/README.md#2025-04-21_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nkubectl port-forward --namespace knative-monitoring $(kubectl get pods --namespace knative-monitoring --selector=app=grafana  --output=jsonpath=\"{.items..metadata.name}\") 3000\n```\n\n----------------------------------------\n\nTITLE: Verify InferenceService Creation\nDESCRIPTION: This command shows the expected output after successfully applying the InferenceService YAML. It confirms that the `torchserve-custom` InferenceService resource has been created in the `serving.kserve.io` API group.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/docs/autoscaling.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n\"$inferenceservice.serving.kserve.io/torchserve-custom created\"\n```\n\n----------------------------------------\n\nTITLE: HTTP Prediction Response Example\nDESCRIPTION: Sample JSON response from the HTTP prediction endpoint showing model predictions with confidence scores.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/tensorflow/README.md#2025-04-21_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"predictions\": [\n        {\n            \"scores\": [0.999114931, 9.20987877e-05, 0.000136786213, 0.000337257545, 0.000300532585, 1.84813616e-05],\n            \"prediction\": 0,\n            \"key\": \"   1\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Making Cookie-Authenticated Prediction Request in Python\nDESCRIPTION: Python code that obtains a Dex authentication session cookie and uses it to send a prediction request to the InferenceService. It configures the appropriate headers and constructs the input data for the model.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/istio-dex/README.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport requests \n\nKUBEFLOW_ENDPOINT = \"http://localhost:8080\"   # Cluster IP and port\nKUBEFLOW_USERNAME = \"user@example.com\"\nKUBEFLOW_PASSWORD = \"12341234\"\nMODEL_NAME = \"sklearn-iris\"\nSERVICE_HOSTNAME = \"sklearn-iris.kubeflow-user-example-com.example.com\"\nPREDICT_ENDPOINT = f\"{KUBEFLOW_ENDPOINT}/v1/models/{MODEL_NAME}:predict\"\niris_input = {\"instances\": [[6.8, 2.8, 4.8, 1.4], [6.0, 3.4, 4.5, 1.6]]}\n\n_auth_session = get_istio_auth_session(\n    url=KUBEFLOW_ENDPOINT, username=KUBEFLOW_USERNAME, password=KUBEFLOW_PASSWORD\n)\n\ncookies = {\"authservice_session\": _auth_session['authservice_session']}\njar = requests.cookies.cookiejar_from_dict(cookies)\n\nres = requests.post(\n    url=PREDICT_ENDPOINT,\n    headers={\"Host\": SERVICE_HOSTNAME, \"Content-Type\": \"application/json\"},\n    cookies=jar,\n    json=iris_input,\n    timeout=200,\n)\nprint(\"Status Code: \", res.status_code)\nprint(\"Response: \", res.json())\n```\n\n----------------------------------------\n\nTITLE: Interpreting Inference Results in Python\nDESCRIPTION: Processes and displays the response from the model explanation. Extracts explanations such as masks and labels for visualization. Checks response validity and uses matplotlib for plotting. Requires a valid JSON response.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/aix/mnist/query_explain.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nprint(res)\nif not res.ok:\n    res.raise_for_status()\nres_json = res.json()\ntemp = np.array(res_json[\"explanations\"][\"temp\"])\nmasks = np.array(res_json[\"explanations\"][\"masks\"])\ntop_labels = np.array(res_json[\"explanations\"][\"top_labels\"])\n\nfig, m_axs = plt.subplots(2, 5, figsize=(12, 6))\nfor i, c_ax in enumerate(m_axs.flatten()):\n    mask = masks[i]\n    c_ax.imshow(label2rgb(mask, temp, bg_label=0), interpolation=\"nearest\")\n    c_ax.set_title(\"Positive for {}\\nActual {}\".format(top_labels[i], actual))\n    c_ax.axis(\"off\")\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Converting Image to Tensor Format\nDESCRIPTION: Command to convert a PNG image file into tensor data format, generating an input.json file\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v2/tensor_conv/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython totensor.py 0.png\n```\n\n----------------------------------------\n\nTITLE: Deploying Knative Trigger\nDESCRIPTION: Applies the trigger.yaml configuration to create a Knative trigger that routes events to the message logger service.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/logger/knative-eventing/logger_demo.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n!kubectl apply -f trigger.yaml\n```\n\n----------------------------------------\n\nTITLE: Run Jupyter Notebook\nDESCRIPTION: This command starts a Jupyter Notebook server, allowing you to open and run the `mosaic-onnx.ipynb` notebook. The notebook contains the code for running a sample inference using the deployed ONNX model.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/onnx/README.md#2025-04-21_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\njupyter notebook\n```\n\n----------------------------------------\n\nTITLE: Making Prediction Request to PMML InferenceService\nDESCRIPTION: Commands to set up and execute a prediction request against the deployed PMML model service using curl.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/spark/README.md#2025-04-21_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nMODEL_NAME=spark-pmml\nINPUT_PATH=@./pmml-input.json\nSERVICE_HOSTNAME=$(kubectl get inferenceservice spark-pmml -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\ncurl -v -H \"Host: ${SERVICE_HOSTNAME}\" http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/$MODEL_NAME:predict -d $INPUT_PATH\n```\n\n----------------------------------------\n\nTITLE: Retrieve Message Dumper Logs Using Bash\nDESCRIPTION: Uses kubectl to fetch logs from the 'message-dumper' Knative service. This command helps in verifying the cloud events being logged as expected when inference requests are made.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/logger/README.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nkubectl logs $(kubectl get pod -l serving.knative.dev/service=message-dumper -o jsonpath='{.items[0].metadata.name}') -c user-container\n```\n\n----------------------------------------\n\nTITLE: Get Service Hostname\nDESCRIPTION: This snippet retrieves the service hostname using `kubectl` and stores it in the `SERVICE_HOSTNAME` variable. This hostname is used to access the inference service.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/alibi/moviesentiment/movie_review_explanations.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n\"SERVICE_HOSTNAMES = !(kubectl get inferenceservice moviesentiment -o jsonpath='{.status.url}' | cut -d \\\"/\\\" -f 3)\\nSERVICE_HOSTNAME = SERVICE_HOSTNAMES[0]\\nprint(SERVICE_HOSTNAME)\"\n```\n\n----------------------------------------\n\nTITLE: Apply InferenceService Configuration\nDESCRIPTION: Applies the initial InferenceService configuration using kubectl. This command creates the InferenceService resource in the Kubernetes cluster.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/docs/canary.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f torchserve-custom.yaml\n```\n\n----------------------------------------\n\nTITLE: Show Movie Review Predictions\nDESCRIPTION: This code iterates through predefined negative and positive indices, prints the movie review at each index, and calls the `show_prediction` function to display the model's prediction for that review. It relies on a function `predict` that sends a request to the `moviesentiment` inference service running at `SERVICE_HOSTNAME` and `CLUSTER_IP`.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/alibi/moviesentiment/movie_review_explanations.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n\"idxNeg = 37\\nidxPos = 5227\\nfor idx in [idxNeg, idxPos]:\\n    print(movies.data[idx])\\n    show_prediction(\\n        predict(\\n            movies.data[idx : idx + 1],\\n            \\\"moviesentiment\\\",\\n            movies,\\n            SERVICE_HOSTNAME,\\n            CLUSTER_IP,\\n        )\\n    )\"\n```\n\n----------------------------------------\n\nTITLE: Setting Variables for KServe Inference Requests\nDESCRIPTION: Commands to set environment variables for the model name and service hostname to be used in inference requests.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v2/bert/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nMODEL_NAME=BERTSeqClassification\nSERVICE_HOSTNAME=$(kubectl get inferenceservice torchserve-bert-v2 -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\n```\n\n----------------------------------------\n\nTITLE: Get Prometheus CRD Configuration\nDESCRIPTION: This command retrieves the Prometheus Custom Resource Definition (CRD) configuration in YAML format, allowing inspection of its settings.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/metrics-and-monitoring/README.md#2025-04-21_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n$ kubectl get prometheus -n <name-of-the-namespace> -o yaml\n...\n  serviceMonitorNamespaceSelector: {}\n...\n```\n\n----------------------------------------\n\nTITLE: Launching Iter8 Experiment\nDESCRIPTION: This command initiates an Iter8 experiment within the Kubernetes cluster, setting various tasks and parameters to assess the performance of the InferenceService. It specifies the URL, payload URL, content type, and SLO constraints.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/performance-testing/README.md#2025-04-21_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\niter8 k launch \\\n--set \"tasks={ready,http,assess}\" \\\n--set ready.isvc=sklearn-irisv2 \\\n--set ready.timeout=600s \\\n--set http.url=http://sklearn-irisv2.default.svc.cluster.local/v2/models/sklearn-irisv2/infer \\\n--set http.payloadURL=https://gist.githubusercontent.com/kalantar/d2dd03e8ebff2c57c3cfa992b44a54ad/raw/97a0480d0dfb1deef56af73a0dd31c80dc9b71f4/sklearn-irisv2-input.json \\\n--set http.contentType=\"application/json\" \\\n--set http.warmupNumRequests=50 \\\n--set assess.SLOs.upper.http/latency-mean=500 \\\n--set assess.SLOs.upper.http/latency-p90=1000 \\\n--set assess.SLOs.upper.http/error-count=0 \\\n--set runner=job\n```\n\n----------------------------------------\n\nTITLE: Fetching Message Dumper Logs\nDESCRIPTION: This shell command retrieves logs from the message dumper service, allowing observation of CloudEvent logs generated by the inference service.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/logger/knative-eventing/README.md#2025-04-21_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\nkubectl logs $(kubectl get pod -l serving.knative.dev/service=message-dumper -o jsonpath='{.items[0].metadata.name}') user-container\n```\n\n----------------------------------------\n\nTITLE: Getting Istio Ingress Gateway IP Address - Python\nDESCRIPTION: This snippet retrieves the IP address of the Istio Ingress Gateway, which is necessary for accessing services from outside the Kubernetes cluster.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/outlier-detection/alibi-detect/cifar10/cifar10_outlier.ipynb#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nCLUSTER_IPS = !(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\nCLUSTER_IP = CLUSTER_IPS[0]\nprint(CLUSTER_IP)\n```\n\n----------------------------------------\n\nTITLE: Converting Image to Byte Array using Python Script\nDESCRIPTION: This command runs a Python script named 'img2bytearray.py' that converts an image file '0.png' to a byte array. The output is written to a file named 'input.json'. This process is useful for preparing image data for machine learning models in KServe.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v1/imgconv/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython img2bytearray.py 0.png\n```\n\n----------------------------------------\n\nTITLE: HDFS Credentials Kubernetes Secret YAML\nDESCRIPTION: YAML representation of a Kubernetes secret for storing HDFS connection details\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/storage/hdfs/README.md#2025-04-21_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: Secret\nmetadata:\n  name: hdfscreds\ntype: Opaque\nstringData:\n  HDFS_NAMENODE: xxxx\n  ...\n```\n\n----------------------------------------\n\nTITLE: Getting Explanation for Low Income Prediction with Python\nDESCRIPTION: This snippet calls the explain function to retrieve explanations for the prediction made on a low income data point. The result contains detailed interpretability metrics for the specific prediction.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/alibi/income/income_explanations.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nexp = explain(\n    adult.data[idxLow : idxLow + 1].tolist(), \"income\", SERVICE_HOSTNAME, CLUSTER_IP\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Knative Trigger for Outlier Detection - Python\nDESCRIPTION: This snippet writes a YAML configuration for a Knative Trigger that links incoming prediction requests to the outlier detector service. It filters events based on their type.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/outlier-detection/alibi-detect/cifar10/cifar10_outlier.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n%%writefile trigger.yaml\napiVersion: eventing.knative.dev/v1\nkind: Trigger\nmetadata:\n  name: vaeoutlier-trigger\n  namespace: cifar10\nspec:\n  broker: default\n  filter:\n    attributes:\n      type: org.kubeflow.serving.inference.request\n  subscriber:\n    ref:\n      apiVersion: serving.knative.dev/v1\n      kind: Service\n      name: vae-outlier\n      namespace: cifar10\n```\n\n----------------------------------------\n\nTITLE: Displaying Feature Coverage and Examples for High Income Prediction with Python\nDESCRIPTION: This snippet manages the display of feature coverage metrics and corresponding examples from the explanation of the high income prediction, drawing attention to how features contributed to the prediction.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/alibi/income/income_explanations.ipynb#2025-04-21_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nshow_feature_coverage(exp[\"data\"])\n\nshow_examples(exp[\"data\"], 0, adult)\nshow_examples(exp[\"data\"], 0, adult, False)\n```\n\n----------------------------------------\n\nTITLE: Using a Custom SKLearnServer Docker Image for InferenceService Configuration\nDESCRIPTION: This YAML snippet outlines how to configure a custom SKLearnServer image for an InferenceService in Kubernetes, allowing for a specific version of scikit-learn to be used with a pickled model.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/sklearn/v1/README.md#2025-04-21_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\n        \"sklearn\": {\n            \"image\": \"<your-dockerhub-id>/kfserving/sklearnserver\",\n        },\n```\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"sklearn-iris\"\nspec:\n  predictor:\n    sklearn:\n      storageUri: \"gs://kfserving-examples/models/sklearn/1.0/model\"\n      runtimeVersion: X.X.X\n```\n\n----------------------------------------\n\nTITLE: Defining Prediction Function\nDESCRIPTION: Creates a function that sends prediction requests to the inference service with the proper headers and formatting.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/logger/knative-eventing/logger_demo.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\n\ndef predict(X, name, svc_hostname, cluster_ip):\n    formData = {\"instances\": X}\n    headers = {}\n    headers[\"Host\"] = svc_hostname\n    res = requests.post(\n        \"http://\" + cluster_ip + \"/v1/models/\" + name + \":predict\",\n        json=formData,\n        headers=headers,\n    )\n    if res.status_code == 200:\n        return res.json()\n    else:\n        print(\"Failed with \", res.status_code)\n        return []\n```\n\n----------------------------------------\n\nTITLE: Starting MLServer Locally\nDESCRIPTION: Command to start MLServer in the current directory for local model serving.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/sklearn/v2/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmlserver start .\n```\n\n----------------------------------------\n\nTITLE: Obtaining Service Hostname for Income Prediction with Python\nDESCRIPTION: This snippet retrieves the hostname of the deployed income prediction service, extracting it from the inference service's URL through a kubectl command and string manipulation.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/alibi/income/income_explanations.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nSERVICE_HOSTNAMES = (\n    !(kubectl get inferenceservice income -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\n)\nSERVICE_HOSTNAME = SERVICE_HOSTNAMES[0]\nprint(SERVICE_HOSTNAME)\n```\n\n----------------------------------------\n\nTITLE: Deploying InferenceService using kubectl\nDESCRIPTION: This command applies the aix-explainer.yaml file to deploy an InferenceService to the Kubernetes cluster.  It creates the necessary resources for the AIX explainer service.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/aix/mnist/README.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n`kubectl apply -f aix-explainer.yaml`\n```\n\n----------------------------------------\n\nTITLE: Applying Cifar10 InferenceService configuration\nDESCRIPTION: This snippet applies the KServe InferenceService configuration defined in the `cifar10.yaml` file.  This will create the InferenceService which deploys the tfserving-cifar10 model within the cifar10 namespace.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/drift-detection/alibi-detect/cifar10/cifar10_drift.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n\"!kubectl apply -f cifar10.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Deploying InferenceService\nDESCRIPTION: Command to apply the InferenceService configuration and its expected output\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/metrics/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f metrics.yaml\n```\n\nLANGUAGE: bash\nCODE:\n```\n$inferenceservice.serving.kserve.io/torch-metrics created\n```\n\n----------------------------------------\n\nTITLE: Checking InferenceService Status on Kubernetes\nDESCRIPTION: This snippet shows how to retrieve the status of the InferenceService deployed on the Kubernetes cluster. It lists the service details including its name, URL, readiness status, and traffic distribution.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/storage/pvc/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ kubectl get inferenceservice\nNAME           URL                                                               READY     DEFAULT TRAFFIC   CANARY TRAFFIC   AGE\nmnist-sample   http://mnist-sample.kubeflow.example.com/v1/models/mnist-sample   True      100                                1m\n```\n\n----------------------------------------\n\nTITLE: Querying the Explanation Endpoint with Python\nDESCRIPTION: This Python command executes the `query_explain.py` script to send a request to the explanation endpoint of the deployed model. It uses the Ingress host, port, model name, and optional parameters like the MNIST image index to generate explanations.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/aix/mnist/README.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n`MODEL_NAME=aix-explainer\nSERVICE_HOSTNAME=$(kubectl get inferenceservice ${MODEL_NAME} -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\npython query_explain.py http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/${MODEL_NAME}:explain ${SERVICE_HOSTNAME}`\n```\n\n----------------------------------------\n\nTITLE: Creating Demo Namespace and S3 Credentials\nDESCRIPTION: Create a namespace for the demo and set up S3 credentials as a Kubernetes secret.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/fluid/README.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create ns kserve-fluid-demo\n\n# please update the s3 credentials to yours\nkubectl create -f s3creds.yaml -n kserve-fluid-demo\n```\n\n----------------------------------------\n\nTITLE: Checking Revisions after Canary Deployment\nDESCRIPTION: Command to verify that two revisions of the model have been created after applying the canary configuration.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/canary/README.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get revisions -l serving.kserve.io/inferenceservice=torchserve\nNAME                                 CONFIG NAME                    K8S SERVICE NAME                     GENERATION   READY   REASON\ntorchserve-predictor-default-9lttm   torchserve-predictor-default   torchserve-predictor-default-9lttm   1            True\ntorchserve-predictor-default-kxp96   torchserve-predictor-default   torchserve-predictor-default-kxp96   2            True\n```\n\n----------------------------------------\n\nTITLE: Creating a Pipeline (Python)\nDESCRIPTION: This snippet defines a pipeline for preprocessing the data and training the model. The pipeline includes steps for handling missing values (imputation), scaling numerical features, one-hot encoding categorical features, and training a linear regression model (SGDRegressor). It also includes a custom transformer to convert dict to dataframe.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/sklearn/v1/sklearn-mixedtype-model/sklearn-mixedtype-model.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\np = Pipeline(\n    [\n        (\"dicttodf\", DictToDFTransformer()),\n        (\n            \"preprocess\",\n            ColumnTransformer(\n                [\n                    (\n                        \"numerical\",\n                        make_pipeline(\n                            SimpleImputer(strategy=\"mean\"),\n                            StandardScaler(),\n                        ),\n                        sorted(numerical_features.columns),\n                    ),\n                    (\n                        \"categorical\",\n                        make_pipeline(\n                            SimpleImputer(strategy=\"most_frequent\"),\n                            OneHotEncoder(handle_unknown=\"ignore\", sparse=False),\n                        ),\n                        sorted(categorical_features.columns),\n                    ),\n                ]\n            ),\n        ),\n        (\"regressor\", SGDRegressor(random_state=666, learning_rate=\"adaptive\")),\n    ]\n)\n```\n\n----------------------------------------\n\nTITLE: Get Pods in Namespace\nDESCRIPTION: Retrieves a list of pods running in the kserve-test namespace using kubectl. This command helps to verify the status and number of pods deployed for the InferenceService.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/docs/canary.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nKubectl get pods -n kserve-test\n```\n\n----------------------------------------\n\nTITLE: Show Feature Coverage\nDESCRIPTION: This snippet extracts the feature coverage from the explanation data and displays it using the `show_feature_coverage` function.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/alibi/moviesentiment/movie_review_explanations.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n\"show_feature_coverage(exp[\\\"data\\\"])\"\n```\n\n----------------------------------------\n\nTITLE: Generating OpenAPI Spec and Swagger File in KServe\nDESCRIPTION: This command executes a script to generate OpenAPI specifications and swagger.json file from KServe types using openapi-gen. The generated files are stored in the pkg/apis/serving/v1beta1/ directory.\nSOURCE: https://github.com/kserve/kserve/blob/master/hack/python-sdk/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./hack/update-openapigen.sh\n```\n\n----------------------------------------\n\nTITLE: Defining GRPC Service Endpoints in Protobuf\nDESCRIPTION: Describes the GRPC service endpoints for an inference server including ServerLive, ServerReady, ModelReady, ServerMetadata, ModelMetadata, and ModelInfer. These endpoints facilitate checking server and model statuses and performing inference operations.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md#2025-04-21_snippet_9\n\nLANGUAGE: protobuf\nCODE:\n```\nservice GRPCInferenceService\n{\n  rpc ServerLive(ServerLiveRequest) returns (ServerLiveResponse) {}\n\n  rpc ServerReady(ServerReadyRequest) returns (ServerReadyResponse) {}\n\n  rpc ModelReady(ModelReadyRequest) returns (ModelReadyResponse) {}\n\n  rpc ServerMetadata(ServerMetadataRequest) returns (ServerMetadataResponse) {}\n\n  rpc ModelMetadata(ModelMetadataRequest) returns (ModelMetadataResponse) {}\n\n  rpc ModelInfer(ModelInferRequest) returns (ModelInferResponse) {}\n}\n```\n\n----------------------------------------\n\nTITLE: InferenceService with New Storage Specification\nDESCRIPTION: InferenceService configuration using the new storage specification with HDFS key\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/storage/hdfs/README.md#2025-04-21_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: serving.kserve.io/v1beta1\nkind: InferenceService\nmetadata:\n  name: my-model\nspec:\n  predictor:\n    model:\n      modelFormat:\n        name: tensorflow\n      storage:\n        key: internalhdfs\n        path: /user/myuser/path/to/model\n```\n\n----------------------------------------\n\nTITLE: Preparing Environment and Downloading Model\nDESCRIPTION: Set up a virtual environment, install dependencies, and download the Llama3.1-8B-Instruct model from HuggingFace.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/fluid/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython -m venv .venv\nsource .venv/bin/activate\npip3 install --upgrade pip\npip3 install \"huggingface_hub[hf_transfer]\"\n\n# create models folder\nmkdir -p models\n\n# to download Llama models, you need accept the Llama 3 community license agreement and set HF_TOKEN\nexport HF_TOKEN=\"xxxxxxxx\"\nHF_HUB_ENABLE_HF_TRANSFER=1 python3 download_model.py --model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\" --model_dir=\"./models\"\n```\n\n----------------------------------------\n\nTITLE: List InferenceService after rollback - Kubernetes CLI\nDESCRIPTION: This command retrieves information about the InferenceService named `my-model` after the rollback.  This confirms that traffic has shifted back to the previous stable version of the model.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/rollout/README.md#2025-04-21_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get isvc my-model\n```\n\n----------------------------------------\n\nTITLE: Launching PySpark with JPMML-SparkML JAR\nDESCRIPTION: Command to start PySpark with the required JPMML-SparkML JAR for PMML export support.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/spark/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npyspark --jars ./jpmml-sparkml-executable-1.6.3.jar\n```\n\n----------------------------------------\n\nTITLE: Testing the Deployed InferenceService\nDESCRIPTION: Set up environment variables and send a test request to the deployed model service.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/fluid/README.md#2025-04-21_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nexport MODEL_NAME=\"llama-31-8b-instruct\"\nexport SERVICE_HOSTNAME=$(kubectl get inferenceservice ${MODEL_NAME} -n kserve-fluid-demo -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\nexport INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')\nexport INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name==\"http2\")].port}')\n\ncurl \"http://${INGRESS_HOST}:${INGRESS_PORT}/openai/v1/completions\" \\\n-H \"content-type: application/json\" \\\n-H \"Host: ${SERVICE_HOSTNAME}\" \\\n-d '{\"model\": \"'\"$MODEL_NAME\"'\", \"prompt\": \"Write a poem about colors\", \"stream\":false, \"max_tokens\": 30}'\n```\n\n----------------------------------------\n\nTITLE: Generating Python SDK for KServe\nDESCRIPTION: This command runs a script that installs openapi-codegen and generates the Python SDK based on the previously created OpenAPI specifications. The generated SDK is stored in the python/kserve directory.\nSOURCE: https://github.com/kserve/kserve/blob/master/hack/python-sdk/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./hack/python-sdk/client-gen.sh\n```\n\n----------------------------------------\n\nTITLE: Deploying Message Dumper Trigger\nDESCRIPTION: This shell command applies a YAML configuration for the 'message-dumper-trigger' to the Kubernetes cluster, ensuring it listens to events from the broker using 'kubectl'.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/logger/knative-eventing/README.md#2025-04-21_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nkubectl create -f trigger.yaml\n```\n\n----------------------------------------\n\nTITLE: Deleting InferenceService\nDESCRIPTION: Remove the InferenceService from the specified Kubernetes namespace\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/client/kfserving_sdk_v1beta1_sample.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nKFServing.delete(\"flower-sample\", namespace=namespace)\n```\n\n----------------------------------------\n\nTITLE: Copying Model Archive and Config Properties to Storage\nDESCRIPTION: This snippet involves copying a model archive (mar file) and config properties to the storage mounted in the model-store pod. The first command creates directories as needed. Ensure the model-store pod is running before executing these commands.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/custom-server-with-external-storage.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl exec --tty pod/model-store-pod -- mkdir /pv/model-store/\nkubectl cp mnist.mar model-store-pod:/pv/model-store/mnist.mar\n\nkubectl exec --tty pod/model-store-pod -- mkdir /pv/config/\nkubectl cp config.properties model-store-pod:/pv/config/config.properties\n```\n\n----------------------------------------\n\nTITLE: Building AIF Bias Detector Docker Image in KServe\nDESCRIPTION: Command to build a Docker image for the AIF fairness server. This should be run from the kserve/python directory with the dockeruser replaced by your Docker username.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/aiffairness/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t dockeruser/aifserver:latest -f aiffairness.Dockerfile .\n```\n\n----------------------------------------\n\nTITLE: Applying Knative Broker Configuration - Python\nDESCRIPTION: This snippet applies the previously created broker.yaml to the Kubernetes cluster, effectively creating the Knative Broker resource defined in the file.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/outlier-detection/alibi-detect/cifar10/cifar10_outlier.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n!kubectl create -f broker.yaml\n```\n\n----------------------------------------\n\nTITLE: Example Model Explanation Response\nDESCRIPTION: Demonstration of a successful model explanation request response, including HTTP headers and a multi-dimensional array of explanation values\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v1/README.md#2025-04-21_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n*   Trying 52.89.19.61...\n* Connected to a881f5a8c676a41edbccdb0a394a80d6-2069247558.us-west-2.elb.amazonaws.com (52.89.19.61) port 80 (#0)\n> PUT /v1/models/mnist:explain HTTP/1.1\n> Host: torchserve.kserve-test.example.com\n> User-Agent: curl/7.47.0\n> Accept: */*\n> Content-Length: 167\n> Expect: 100-continue\n>\n< HTTP/1.1 100 Continue\n* We are completely uploaded and fine\n< HTTP/1.1 200 OK\n< cache-control: no-cache; no-store, must-revalidate, private\n< content-length: 1\n< date: Tue, 27 Oct 2020 08:26:19 GMT\n< expires: Thu, 01 Jan 1970 00:00:00 UTC\n< pragma: no-cache\n< x-request-id: b10cfc9f-cd0f-4cda-9c6c-194c2cdaa517\n< x-envoy-upstream-service-time: 6\n< server: istio-envoy\n<\n* Connection #0 to host a881f5a8c676a41edbccdb0a394a80d6-2069247558.us-west-2.elb.amazonaws.com left intact\n{\"explanations\": [[...multi-dimensional array of float values...]]}\n```\n\n----------------------------------------\n\nTITLE: Deleting a Model from KServe\nDESCRIPTION: Example showing how to delete a deployed model from KServe. This sets the action to 'delete' and requires only the model_name parameter to identify which model to remove.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/kfp/README.md#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nkserve_op(\n    action='delete',\n    model_name='tf-sample'\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Up and Running Prediction Request to BLOOM Model\nDESCRIPTION: Bash commands to set up environment variables and send a prediction request to the deployed BLOOM model. This includes retrieving the service hostname and using curl to submit a text file for inference.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v1/bloom/README.md#2025-04-21_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\nMODEL_NAME=BLOOMSeqClassification\nISVC_NAME=torchserve-bloom-560m\nSERVICE_HOSTNAME=$(kubectl get inferenceservice ${ISVC_NAME} -n <namespace> -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\n\ncurl -v -H \"Host: ${SERVICE_HOSTNAME}\" http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/${MODEL_NAME}:predict -d ./sample_text.txt\n```\n\n----------------------------------------\n\nTITLE: Creating Firewall Rules for Private K8s Clusters using gcloud\nDESCRIPTION: This code snippet creates firewall rules for a private GKE cluster to allow communication between the master and worker nodes for cert-manager and KFServing webhook functionalities. Ensure to adjust the source ranges, target tags, and network to suit your environment.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/gcp-iap/README.md#2025-04-21_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngcloud compute firewall-rules create kubeflow-webhook-probe --allow=tcp:8443 --target-tags kubeflow-worker --direction INGRESS --network default --priority 1000 --source-ranges 172.16.0.0/28\n\ngcloud compute firewall-rules create kubeflow-cert-manager --allow=tcp:6443 --target-tags kubeflow-worker --direction INGRESS --network default --priority 1000 --source-ranges 172.16.0.0/28\n```\n\n----------------------------------------\n\nTITLE: Preparing Endpoint and Input Data in Python\nDESCRIPTION: Prepares endpoint URLs, headers, and loads input data whether from a file or the MNIST dataset. Throws an exception if the endpoint is not specified. Requires `sys` module for argument handling.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/aix/mnist/query_explain.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nprint(\"************************************************************\")\nprint(\"************************************************************\")\nprint(\"************************************************************\")\nprint(\"starting query\")\n\nif len(sys.argv) < 3:\n    raise Exception(\"No endpoint specified. \")\n\nendpoint = sys.argv[1]\nheaders = {\"Host\": sys.argv[2]}\ntest_num = 1002\nis_file = False\nif len(sys.argv) > 3:\n    try:\n        test_num = int(sys.argv[2])\n    except:\n        is_file = True\n\nif is_file:\n    inputs = open(sys.argv[2])\n    inputs = json.load(inputs)\n    actual = \"unk\"\nelse:\n    data = MNISTDataset()\n    inputs = data.test_data[test_num]\n    labels = data.test_labels[test_num]\n    actual = 0\n    for x in range(1, len(labels)):\n        if labels[x] != 0:\n            actual = x\n    inputs = gray2rgb(inputs.reshape((-1, 28, 28)))\n    inputs = np.reshape(inputs, (28, 28, 3))\ninput_image = {\"instances\": [inputs.tolist()]}\n```\n\n----------------------------------------\n\nTITLE: Check Pods Status in Knative Monitoring Namespace\nDESCRIPTION: Retrieves the status of pods in the `knative-monitoring` namespace. This allows verification that the pods for Prometheus and Grafana have been successfully deployed and are running.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/docs/metrics.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n\"kubectl get pods --namespace knative-monitoring\"\n```\n\n----------------------------------------\n\nTITLE: Fetching corrupted CIFAR10 examples\nDESCRIPTION: This snippet fetches CIFAR10 examples with motion blur corruption using the `alibi_detect` library. It prepares corrupted data to simulate data drift.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/drift-detection/alibi-detect/cifar10/cifar10_drift.ipynb#2025-04-21_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n\"from alibi_detect.datasets import fetch_cifar10c, corruption_types_cifar10c\n\ncorruption = [\\\"motion_blur\\\"]\nX_corr, y_corr = fetch_cifar10c(corruption=corruption, severity=5, return_X_y=True)\nX_corr = X_corr.astype(\\\"float32\\\") / 255\"\n```\n\n----------------------------------------\n\nTITLE: Patching Knative Service Ingress Class\nDESCRIPTION: This command updates the Knative configuration to use Kourier as the ingress class for routing traffic, ensuring that all traffic is correctly managed by Kourier.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/OPENSHIFT_GUIDE.md#2025-04-21_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\noc patch configmap/config-network \\\n  --namespace knative-serving \\\n  --type merge \\\n  --patch '{\"data\":{\"ingress.class\":\"kourier.ingress.networking.knative.dev\"}}'\n```\n\n----------------------------------------\n\nTITLE: List pods after promotion - Kubernetes CLI\nDESCRIPTION: This command lists the pods associated with the InferenceService `my-model` after promotion. It verifies that the old revision pods are terminating and the new revision pods are running.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/rollout/README.md#2025-04-21_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get pods -l serving.kserve.io/inferenceservice=my-model\n```\n\n----------------------------------------\n\nTITLE: Expected output structure of a prediction request\nDESCRIPTION: This snippet illustrates the expected JSON output from the prediction request including status information and detected objects. It serves as a reference for understanding the response format returned by the InferenceService.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/prebuilt-image/README.md#2025-04-21_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\"status\": \"ok\", \"predictions\": [{\"label_id\": \"1\", \"label\": \"person\", \"probability\": 0.944034993648529, \"detection_box\": [0.1242099404335022, 0.12507186830043793, 0.8423267006874084, 0.5974075794219971]}, {\"label_id\": \"18\", \"label\": \"dog\", \"probability\": 0.8645511865615845, \"detection_box\": [0.10447660088539124, 0.1779915690422058, 0.8422801494598389, 0.7320017218589783]}]}\n```\n\n----------------------------------------\n\nTITLE: Deploying Sklearn Inference Service with Logging\nDESCRIPTION: This shell command applies the Sklearn InferenceService YAML configuration to the Kubernetes cluster using 'kubectl', enabling inference requests and responses logging.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/logger/knative-eventing/README.md#2025-04-21_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nkubectl create -f sklearn-logging.yaml\n```\n\n----------------------------------------\n\nTITLE: Checking Scaled Pods Status\nDESCRIPTION: Bash command to verify the autoscaling behavior by checking the number of pods and their status.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/autoscaling/README.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get pods -n kserve-test \n\nNAME                                                             READY   STATUS        RESTARTS   AGE\ntorchserve-predictor-default-cj2d8-deployment-69444c9c74-67qwb   2/2     Terminating   0          103s\ntorchserve-predictor-default-cj2d8-deployment-69444c9c74-nnxk8   2/2     Terminating   0          95s\ntorchserve-predictor-default-cj2d8-deployment-69444c9c74-rq8jq   2/2     Running       0          50m\ntorchserve-predictor-default-cj2d8-deployment-69444c9c74-tsrwr   2/2     Running       0          113s\ntorchserve-predictor-default-cj2d8-deployment-69444c9c74-vvpjl   2/2     Running       0          109s\ntorchserve-predictor-default-cj2d8-deployment-69444c9c74-xvn7t   2/2     Terminating   0          103s\n```\n\n----------------------------------------\n\nTITLE: Running Image Transformer with SSL (KServe 0.11+)\nDESCRIPTION: This command runs the image transformer Python module with SSL enabled, for KServe version 0.11 or later.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/transformer/torchserve_image_transformer/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m image_transformer --use_ssl --predictor_host={predictor-url}  --workers=1 --config_path=\"local_config.properties\"\n```\n\n----------------------------------------\n\nTITLE: Applying StorageClass to Kubernetes Cluster\nDESCRIPTION: Kubernetes command to apply the StorageClass configuration to the cluster, making it available for persistent volume claims.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v1/bloom/README.md#2025-04-21_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n$ kubectl apply -f storageclass.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies with Pip - Python\nDESCRIPTION: This snippet installs the required Python dependencies listed in the 'requirements_notebook.txt' file using pip. Ensure the environment has access to the internet for the installation to succeed.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/outlier-detection/alibi-detect/cifar10/cifar10_outlier.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -r requirements_notebook.txt\n```\n\n----------------------------------------\n\nTITLE: Get ServiceMonitor Configuration\nDESCRIPTION: This command retrieves a specific ServiceMonitor object in YAML format. It illustrates how the `release` label should match the `serviceMonitorSelector` from the Prometheus object.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/metrics-and-monitoring/README.md#2025-04-21_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n$ kubectl get servicemonitor activator -o yaml -n knative-serving\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  generation: 3\n  labels:\n    release: kube-prometheus-stack-1651295153    <<<<-------- Same as serviceMonitorSelector\n  name: activator\n  namespace: knative-serving\nspec:\n  endpoints:\n  - interval: 30s\n    port: http-metrics\n  namespaceSelector:\n    matchNames:\n    - knative-serving\n  selector:\n    matchLabels:\n      serving.knative.dev/release: v0.22.1 <<<-- This label should be on K8s service objects.\n```\n\n----------------------------------------\n\nTITLE: Get Kubernetes Services\nDESCRIPTION: This command lists Kubernetes Service objects within the `knative-serving` namespace along with their labels.  This is used to verify that the correct labels are present for Prometheus to scrape metrics.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/metrics-and-monitoring/README.md#2025-04-21_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n$ kubectl get svc -n knative-serving --show-labels\nNAME                         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                           AGE   LABELS\nactivator-service            ClusterIP   10.124.9.157    <none>        9090/TCP,8008/TCP,80/TCP,81/TCP   25d   app=activator,serving.knative.dev/release=v0.22.1\nautoscaler                   ClusterIP   10.124.8.155    <none>        9090/TCP,8008/TCP,8080/TCP        25d   app=autoscaler,serving.knative.dev/release=v0.22.1\nautoscaler-bucket-00-of-01   ClusterIP   10.124.14.250   <none>        8080/TCP                          25d   <none>\ncontroller                   ClusterIP   10.124.4.127    <none>        9090/TCP,8008/TCP                 25d   app=controller,serving.knative.dev/release=v0.22.1\nistio-webhook                ClusterIP   10.124.0.96     <none>        9090/TCP,8008/TCP,443/TCP         25d   networking.knative.dev/ingress-provider=istio,role=istio-webhook,serving.knative.dev/release=v0.22.1\nwebhook                      ClusterIP   10.124.15.22    <none>        9090/TCP,8008/TCP,443/TCP         25d   role=webhook,serving.knative.dev/release=v0.22.1\n```\n\n----------------------------------------\n\nTITLE: Defining Canary Route with Splitter Type InferenceGraph\nDESCRIPTION: This YAML snippet defines an InferenceGraph named 'canary-route' using the Splitter router type. It specifies the routing of requests to two services with defined weights to control the distribution. This allows for gradual deployment strategies such as canary releases.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/docs/V1alpha1InferenceRouter.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nkind: InferenceGraph\nmetadata:\n  name: canary-route\nspec:\n  nodes:\n    root:\n      routerType: Splitter\n      routes:\n      - service: mymodel1\n        weight: 20\n      - service: mymodel2\n        weight: 80\n```\n\n----------------------------------------\n\nTITLE: Cleanup KServe and Fluid Resources\nDESCRIPTION: This shell script removes KServe and Fluid configurations using kubectl and helm commands. It deletes namespaces, configurations, and custom resource definitions related to Fluid and KServe, ensuring a complete cleanup. Dependencies include kubectl and helm CLI tools with configured access to the Kubernetes cluster.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/fluid/README.md#2025-04-21_snippet_11\n\nLANGUAGE: sh\nCODE:\n```\nkubectl delete -f fluid-isvc.yaml -n kserve-fluid-demo\nkubectl delete -f kserve-isvc.yaml -n kserve-fluid-demo\n\nkubectl delete -f dataload.yaml -n kserve-fluid-demo\nkubectl delete -f jindo.yaml -n kserve-fluid-demo\n\nkubectl delete -f s3creds.yaml -n kserve-fluid-demo\n\n# uninstall Fluid\nhelm delete fluid -n fluid-system\nkubectl get crd | grep data.fluid.io | awk '{ print $1 }' | xargs kubectl delete crd\nkubectl delete ns fluid-system\n```\n\n----------------------------------------\n\nTITLE: Getting Cifar10 InferenceService hostname\nDESCRIPTION: This snippet retrieves the hostname of the deployed Cifar10 InferenceService using `kubectl`.  It is used to construct the URL for sending prediction requests.  This hostname is used in the Host header of requests to the InferenceService.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/drift-detection/alibi-detect/cifar10/cifar10_drift.ipynb#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n\"SERVICE_HOSTNAMES = !(kubectl get inferenceservice -n cifar10 tfserving-cifar10 -o jsonpath='{.status.url}' | cut -d \\\"/\\\" -f 3)\nSERVICE_HOSTNAME_CIFAR10 = SERVICE_HOSTNAMES[0]\nprint(SERVICE_HOSTNAME_CIFAR10)\"\n```\n\n----------------------------------------\n\nTITLE: Creating an Azure Kubernetes Secret for Authentication - YAML\nDESCRIPTION: This YAML configuration creates a Kubernetes secret to store Azure Service Principal credentials needed for KServe to authenticate with Azure Blob Storage. It defines the secret's data, which includes client ID, secret, subscription ID, and tenant ID.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/storage/azure/README.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: Secret\nmetadata:\n  name: azcreds\ntype: Opaque\ndata:\n  AZURE_CLIENT_ID: xxxxx\n  AZURE_CLIENT_SECRET: xxxxx\n  AZURE_SUBSCRIPTION_ID: xxxxx\n  AZURE_TENANT_ID: xxxxx\n```\n\n----------------------------------------\n\nTITLE: Send Inference Request to TorchServe Model Using Bash\nDESCRIPTION: Demonstrates sending an inference request to the TorchServe model using curl. An image is downloaded and sent to the model endpoint via HTTP PUT request. Assumes the cluster and InferenceService are configured correctly.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/logger/README.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nwget https://raw.githubusercontent.com/pytorch/serve/master/examples/image_classifier/mnist/test_data/1.png\n\nMODEL_NAME=torchserve-logger\nINGRESS_GATEWAY=istio-ingressgateway\nCLUSTER_IP=$(kubectl -n istio-system get service $INGRESS_GATEWAY -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')\nSERVICE_HOSTNAME=$(kubectl get inferenceservice torchserve-logger -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\ncurl -v -H \"Host: ${SERVICE_HOSTNAME}\" http://$CLUSTER_IP/predictions/mnist -T 1.png\n```\n\n----------------------------------------\n\nTITLE: Displaying Message Dumper YAML Configuration\nDESCRIPTION: Command to display the contents of the message dumper YAML file for KNative service\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/logger/logger_demo.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pygmentize message-dumper.yaml\n```\n\n----------------------------------------\n\nTITLE: Base64 Encoding Script in Text\nDESCRIPTION: Provides examples of how to base64 encode a host and headers using shell commands. This is necessary to create the secret.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/storage/uri/README.md#2025-04-21_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nexample.com\n# echo -n \"example.com\" | base64\nZXhhbXBsZS5jb20=\n---\n{\n  \"account-name\": \"some_account_name\",\n  \"secret-key\": \"some_secret_key\"\n}\n# echo -n '{\\n\"account-name\": \"some_account_name\",\\n\"secret-key\": \"some_secret_key\"\\n}' | base64\newoiYWNjb3VudC1uYW1lIjogInNvbWVfYWNjb3VudF9uYW1lIiwKInNlY3JldC1rZXkiOiAic29tZV9zZWNyZXRfa2V5Igp9\n```\n\n----------------------------------------\n\nTITLE: Uploading PMML Model to GCS Bucket\nDESCRIPTION: Command to upload the generated PMML model file to a Google Cloud Storage bucket for use with KServe.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/spark/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngsutil cp ./DecisionTreeIris.pmml gs://$BUCKET_NAME/sparkpmml/model.pmml\n```\n\n----------------------------------------\n\nTITLE: Docker Image Build Commands\nDESCRIPTION: Commands to build Docker images for Torchserve BERT model, with separate instructions for CPU and GPU environments\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/bert-sample/hugging-face-bert-sample.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# For CPU\nDOCKER_BUILDKIT=1 docker build --file Dockerfile -t torchserve-bert:latest .\n\n# For GPU\nDOCKER_BUILDKIT=1 docker build --file Dockerfile --build-arg BASE_IMAGE=nvidia/cuda:10.1-cudnn7-runtime-ubuntu18.04 -t torchserve-bert-gpu:latest .\n\n# Push the container to docker registry\ndocker tag torchserve-bert:latest {username}/torchserve-bert:latest\ndocker push {username}/torchserve-bert:latest\n```\n\n----------------------------------------\n\nTITLE: Applying SKLearn Logging YAML in Python\nDESCRIPTION: This snippet applies the sklearn-logging.yaml configuration to the Kubernetes cluster using kubectl.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/logger/basic/logger_demo.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n!kubectl apply -f sklearn-logging.yaml\n```\n\n----------------------------------------\n\nTITLE: Checking Loading of Simple String Model into Triton\nDESCRIPTION: Log snippet confirming the loading of a Simple String model into Triton Inference Server. It verifies that the model is operational alongside any existing models.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/multimodelserving/triton/README.md#2025-04-21_snippet_11\n\nLANGUAGE: \nCODE:\n```\nI0523 00:04:19.298966 1 model_repository_manager.cc:737] loading: simple-string:1\nI0523 00:04:20.367808 1 tensorflow.cc:1281] Creating instance simple-string on CPU using model.graphdef\nI0523 00:04:20.497748 1 model_repository_manager.cc:925] successfully loaded 'simple-string' version 1\n```\n\n----------------------------------------\n\nTITLE: Setting REST SSL Certificate Environment Variable\nDESCRIPTION: This command sets the environment variable for the REST SSL certificate file path.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/transformer/torchserve_image_transformer/README.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nexport SSL_CERT_FILE=tls-ca-bundle.pem\n```\n\n----------------------------------------\n\nTITLE: Showing YAML File\nDESCRIPTION: Displays the contents of the cifar10.yaml configuration file using Pygments for syntax highlighting.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/alibi/cifar10/cifar10_explanations.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n!pygmentize cifar10.yaml\n```\n\n----------------------------------------\n\nTITLE: Applying Event Display Deployment - Python\nDESCRIPTION: This snippet applies the event-display.yaml to the Kubernetes cluster, thereby deploying the event display application defined in the YAML configuration.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/outlier-detection/alibi-detect/cifar10/cifar10_outlier.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n!kubectl apply -f event-display.yaml\n```\n\n----------------------------------------\n\nTITLE: Building and pushing custom transformer image\nDESCRIPTION: Shell commands to build and push a Docker image for the custom transformer that handles tokenization and detokenization for the BLOOM model.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/triton/fastertransformer/README.md#2025-04-21_snippet_9\n\nLANGUAGE: sh\nCODE:\n```\ndocker build -t <my-transformer-image> -f transformer/Dockerfile transformer\ndocker push <my-transformer-image>\n```\n\n----------------------------------------\n\nTITLE: Creating Directories and Copying Model Files to PVC\nDESCRIPTION: Commands to create necessary directories in the persistent volume and copy the model archive file and configuration to the appropriate locations for TorchServe.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v1/bloom/README.md#2025-04-21_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nkubectl exec -it pv-pod -- mkdir /pv/config\nkubectl exec -it pv-pod -- mkdir /pv/model-store\n\nkubectl cp config.properties pv-pod:/pv/config\nkubectl cp bloom-560m.mar -it pv-pod:/pv/config\n```\n\n----------------------------------------\n\nTITLE: Defining A/B Test Route with Switch Type InferenceGraph\nDESCRIPTION: This YAML snippet illustrates an InferenceGraph named 'abtest' configured with a Switch router type, whereby the routing decision is made based on the user ID input. Different services are invoked depending on the specified conditions, enabling A/B testing scenarios.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/docs/V1alpha1InferenceRouter.md#2025-04-21_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nkind: InferenceGraph\nmetadata:\n  name: abtest\nspec:\n  nodes:\n    mymodel:\n      routerType: Switch\n      routes:\n      - service: mymodel1\n        condition: \"{ .input.userId == 1 }\"\n      - service: mymodel2\n        condition: \"{ .input.userId == 2 }\"\n```\n\n----------------------------------------\n\nTITLE: Applying Knative Trigger configuration\nDESCRIPTION: This snippet applies the Knative Trigger configuration defined in the `trigger.yaml` file.  This creates the drift-trigger which will forward events of type `org.kubeflow.serving.inference.request` to the drift-detector service.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/drift-detection/alibi-detect/cifar10/cifar10_drift.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n\"!kubectl apply -f trigger.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Applying Drift Detector Service configuration\nDESCRIPTION: This snippet applies the Knative Service configuration defined in the `cifar10cd.yaml` file. This creates the drift-detector Knative Service, deploying the Alibi Detect drift detection server within the cifar10 namespace.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/drift-detection/alibi-detect/cifar10/cifar10_drift.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n\"!kubectl apply -f cifar10cd.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Get Pod Status with kubectl\nDESCRIPTION: This command retrieves the status of the pods in the `kserve-test` namespace using `kubectl get pods`. The output provides information about the readiness, status, restarts, and age of each pod, which is helpful for monitoring the autoscaling behavior of the InferenceService. It allows for observing scaling events and diagnosing potential issues.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/docs/autoscaling.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n\"Kubectl get pods -n kserve-test\"\n\n```\n\n----------------------------------------\n\nTITLE: Querying Explanation with Image Index\nDESCRIPTION: This Python command sends a request to the explanation endpoint, specifying the index of the MNIST image to be explained.  It helps in exploring explanations for different samples in the MNIST dataset.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/aix/mnist/README.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n`python query_explain.py http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/${MODEL_NAME}:explain ${SERVICE_HOSTNAME} 100`\n```\n\n----------------------------------------\n\nTITLE: Getting Updated InferenceService Details\nDESCRIPTION: Retrieve and watch the details of the patched InferenceService\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/client/kfserving_sdk_v1beta1_sample.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nKFServing.get(\"flower-sample\", namespace=namespace, watch=True)\n```\n\n----------------------------------------\n\nTITLE: Running Type Checks for PMML Server\nDESCRIPTION: Command to perform static type checking on the PMML server codebase using mypy\nSOURCE: https://github.com/kserve/kserve/blob/master/python/pmmlserver/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmypy --ignore-missing-imports pmmlserver\n```\n\n----------------------------------------\n\nTITLE: Load Test Results - Hey Output\nDESCRIPTION: Sample output from the Hey load testing tool showing performance metrics including latency distribution, request rates, and status code distribution.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/batcher/basic/README.md#2025-04-21_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nSummary:\n  Total:        10.6268 secs\n  Slowest:      1.6477 secs\n  Fastest:      0.0050 secs\n  Average:      0.1006 secs\n  Requests/sec: 48.1800\n\n  Total data:   167424 bytes\n  Size/request: 327 bytes\n\nResponse time histogram:\n  0.005 [1]     |\n  0.169 [447]   |■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■\n  0.334 [30]    |■■■\n  0.498 [7]     |■\n  0.662 [10]    |■\n  0.826 [3]     |\n  0.991 [6]     |■\n  1.155 [5]     |\n  1.319 [1]     |\n  1.483 [1]     |\n  1.648 [1]     |\n\n\nLatency distribution:\n  10% in 0.0079 secs\n  25% in 0.0114 secs\n  50% in 0.0398 secs\n  75% in 0.0867 secs\n  90% in 0.2029 secs\n  95% in 0.5170 secs\n  99% in 1.1428 secs\n\nDetails (average, fastest, slowest):\n  DNS+dialup:   0.0000 secs, 0.0050 secs, 1.6477 secs\n  DNS-lookup:   0.0000 secs, 0.0000 secs, 0.0000 secs\n  req write:    0.0002 secs, 0.0001 secs, 0.0004 secs\n  resp wait:    0.1000 secs, 0.0046 secs, 1.6473 secs\n  resp read:    0.0003 secs, 0.0000 secs, 0.0620 secs\n\nStatus code distribution:\n  [200] 512 responses\n```\n\n----------------------------------------\n\nTITLE: Adding Annotation to storage-config Secret - YAML\nDESCRIPTION: This snippet shows how to add an annotation to the storage-config Secret to reference the specific ConfigMap containing the CA bundle for the inference service.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/cabundle/README.md#2025-04-21_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\ndata:\n  AWS_ACCESS_KEY_ID: VEhFQUNDRVNTS0VZ\n  AWS_SECRET_ACCESS_KEY: VEhFUEFTU1dPUkQ=\nkind: Secret\nmetadata:\n  annotations:\n    serving.kserve.io/s3-cabundle-configmap: local-cabundle\n    ...\n  name: storage-config\n  namespace: kserve-demo\ntype: Opaque\n```\n\n----------------------------------------\n\nTITLE: Server Metadata Response JSON Schema\nDESCRIPTION: Defines the structure for a successful server metadata response, including server name, version, and supported extensions\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md#2025-04-21_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\" : $string,\n  \"version\" : $string,\n  \"extensions\" : [ $string, ... ]\n}\n```\n\n----------------------------------------\n\nTITLE: Displaying Predictions for Low and High Income Examples with Python\nDESCRIPTION: This snippet shows the features and predictions for both the lowest and highest income cases by accessing specific indices in the adult dataset and calling prediction and display functions.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/alibi/income/income_explanations.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nidxLow = 0\nidxHigh = 32554\nfor idx in [idxLow, idxHigh]:\n    show_row([getFeatures([adult.data[idx]], cmap)], adult)\n    show_prediction(\n        predict(\n            adult.data[idx : idx + 1].tolist(),\n            \"income\",\n            adult,\n            SERVICE_HOSTNAME,\n            CLUSTER_IP,\n        )\n    )\n```\n\n----------------------------------------\n\nTITLE: Inference with curl\nDESCRIPTION: Performs an inference request using curl against the deployed model.  This is a test to ensure that the model is serving correctly.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/docs/metrics.md#2025-04-21_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n\"MODEL_NAME=torchserve-custom\nSERVICE_HOSTNAME=$(kubectl get inferenceservice ${MODEL_NAME} <namespace> -o jsonpath='{.status.url}' | cut -d \\\"/\\\" -f 3)\n\ncurl -v -H \\\"Host: ${SERVICE_HOSTNAME}\\\" http://${INGRESS_HOST}:${INGRESS_PORT}/predictions/mnist -T serve/examples/image_classifier/test_data/0.png\"\n```\n\n----------------------------------------\n\nTITLE: Applying Kubernetes Ingress Configuration\nDESCRIPTION: This snippet applies the Ingress configuration using 'kubectl apply'. The command reads the 'kfserving-ingress.yaml' file to create the ingress resource, enabling network traffic routing as specified in the YAML configuration.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/custom-domain/README.md#2025-04-21_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nkubectl apply -f kfserving-ingress.yaml\n```\n\n----------------------------------------\n\nTITLE: Getting Event Display Logs\nDESCRIPTION: This snippet retrieves the logs from the `hello-display` pod. The logs will contain the drift detection results after enough requests are sent.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/drift-detection/alibi-detect/cifar10/cifar10_drift.ipynb#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n\"!kubectl logs -n cifar10 $(kubectl get pod -n cifar10 -l app=hello-display -o jsonpath='{.items[0].metadata.name}')\"\n```\n\n----------------------------------------\n\nTITLE: Defining Transformer Specification in Kubernetes\nDESCRIPTION: Specifies the configuration for a transformer service in KServe, which allows pre and post-processing of machine learning model data\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/docs/V1beta1TransformerSpec.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n# V1beta1TransformerSpec defines the transformer service configuration\n```\n\n----------------------------------------\n\nTITLE: Fetching Message Dumper Logs to Verify Requests\nDESCRIPTION: This command retrieves and displays the logs of the 'message-dumper' service, allowing verification of CloudEvents related to inference requests. Make sure 'kubectl' is configured to access your cluster.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/logger/basic/README.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nkubectl logs $(kubectl get pod -l serving.knative.dev/service=message-dumper -o jsonpath='{.items[0].metadata.name}') user-container\n\n```\n\n----------------------------------------\n\nTITLE: Getting Cluster IP for Ingress Gateway\nDESCRIPTION: Fetches the external IP address of the Istio ingress gateway service in the istio-system namespace, which is needed to send requests to the deployed model.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/alibi/cifar10/cifar10_explanations.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nCLUSTER_IPS = !(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\nCLUSTER_IP = CLUSTER_IPS[0]\nprint(CLUSTER_IP)\n```\n\n----------------------------------------\n\nTITLE: Creating Knative Channel Broker\nDESCRIPTION: Creates a Knative channel broker that will handle event routing in the system.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/logger/knative-eventing/logger_demo.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n!kubectl create -f broker.yaml\n```\n\n----------------------------------------\n\nTITLE: Checking Simple String Model Status\nDESCRIPTION: Command to query the status of the deployed Simple String model in KFServing, confirming its readiness and accessibility for model inference.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/multimodelserving/triton/README.md#2025-04-21_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get tm simple-string\nkubectl get tm simple-string -oyaml\n```\n\n----------------------------------------\n\nTITLE: Viewing Iter8 Experiment Report\nDESCRIPTION: This command generates an HTML report of the Iter8 experiment and saves it to 'report.html', which can be opened in a web browser for review.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/performance-testing/README.md#2025-04-21_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\niter8 k report -o html > report.html # view in a browser\n```\n\n----------------------------------------\n\nTITLE: Getting InferenceService URL using kubectl\nDESCRIPTION: This command retrieves the URL of the deployed InferenceService. The URL is then used to access the model and retrieve explanations.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/aix/mnist/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n`kubectl get inferenceservice`\n```\n\n----------------------------------------\n\nTITLE: Installing MLServer Dependencies\nDESCRIPTION: Command to install MLServer and its SKLearn runtime for local testing.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/sklearn/v2/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install mlserver mlserver-sklearn\n```\n\n----------------------------------------\n\nTITLE: Installing Development Dependencies with Make\nDESCRIPTION: Run the make dev_install command to set up the development environment by installing necessary dependencies. This command should be executed under the project folder in a GitHub repository containing the LightGBM server files.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/lgbserver/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmake dev_install\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Image Data for ONNX - Python\nDESCRIPTION: This snippet preprocesses the loaded image into a format suitable for the ONNX model. It normalizes the image data into a float32 numpy array, transposes its dimensions to match the model's requirements, and expands its dimensions to create a batch of size one.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/onnx/mosaic-onnx.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\n# preprocess image data\nnorm_img_data = np.array(image).astype(\"float32\")\nnorm_img_data = np.transpose(norm_img_data, [2, 0, 1])\nnorm_img_data = np.expand_dims(norm_img_data, axis=0)\nnp.shape(norm_img_data)\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Listing Persistent Volumes in Kubernetes\nDESCRIPTION: Output of kubectl get pv showing a bound local persistent volume with 116Gi capacity, using the fast-disks storage class.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v1/bloom/README.md#2025-04-21_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n$ kubectl get pv\n\nNAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                     STORAGECLASS   REASON   AGE\nlocal-pv-2a85b8ac                          116Gi      RWO            Delete           Bound       kserve-test/model-cache   fast-disks              4d3h\n```\n\n----------------------------------------\n\nTITLE: Normal Prediction Example - Python\nDESCRIPTION: This snippet shows how to utilize the previously defined functions to display an image from the training set and make a prediction using the CIFAR10 model.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/outlier-detection/alibi-detect/cifar10/cifar10_outlier.ipynb#2025-04-21_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nidx = 1\nX = X_train[idx : idx + 1]\nshow(X)\npredict(X)\n```\n\n----------------------------------------\n\nTITLE: List InferenceService after promotion - Kubernetes CLI\nDESCRIPTION: This command retrieves information about the InferenceService named `my-model` after the canary model has been promoted. It verifies that all traffic is directed to the latest revision.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/rollout/README.md#2025-04-21_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get isvc my-model\n```\n\n----------------------------------------\n\nTITLE: Waiting for InferenceService Readiness\nDESCRIPTION: Command to wait until the InferenceService is ready to serve predictions.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/spark/README.md#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nkubectl wait --for=condition=Ready inferenceservice spark-pmml\n```\n\n----------------------------------------\n\nTITLE: Viewing Message Dumper Logs\nDESCRIPTION: Retrieves and displays logs from the message dumper service\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/logger/logger_demo.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n!kubectl logs $(kubectl get pod -l serving.knative.dev/configuration=message-dumper -n default -o jsonpath='{.items[0].metadata.name}') -c user-container\n```\n\n----------------------------------------\n\nTITLE: Creating Message Dumper Knative Service in YAML\nDESCRIPTION: This YAML file describes a Knative service named 'message-dumper' that uses an image for event display. It is essential for receiving and logging CloudEvents. To apply this configuration, ensure your Kubernetes cluster is set up with Knative Serving.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/logger/basic/README.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: message-dumper\nspec:\n  template:\n    spec:\n      containers:\n      - image: gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display\n\n```\n\n----------------------------------------\n\nTITLE: Fetching Cluster IP for Istio Ingress Gateway with Python\nDESCRIPTION: This snippet retrieves the external IP address of the Istio ingress gateway service in the istio-system namespace to facilitate communication with the deployed income prediction service.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/alibi/income/income_explanations.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nCLUSTER_IPS = !(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\nCLUSTER_IP = CLUSTER_IPS[0]\nprint(CLUSTER_IP)\n```\n\n----------------------------------------\n\nTITLE: Making Prediction Request to SKLearn Model\nDESCRIPTION: Sends a sample prediction request to the deployed SKLearn model, which should generate CloudEvents.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/logger/knative-eventing/logger_demo.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\npredict([[6.8, 2.8, 4.8, 1.4]], \"sklearn-iris\", SERVICE_HOSTNAME, CLUSTER_IP)\n```\n\n----------------------------------------\n\nTITLE: Installing KServe Helm Chart\nDESCRIPTION: Command to install the KServe Helm chart from a remote OCI registry. Uses specific version v0.15.0 and installs the chart with the release name 'kserve'.\nSOURCE: https://github.com/kserve/kserve/blob/master/charts/kserve-resources/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhelm install kserve oci://ghcr.io/kserve/charts/kserve --version v0.15.0\n```\n\n----------------------------------------\n\nTITLE: Making Prediction Request to KFServing Endpoint without Authorization\nDESCRIPTION: This code illustrates how to test the KFServing inference service's external predict endpoint using the iap_request.py Python script. It involves creating or using an existing service account, granting it the necessary IAP resource access, and executing a prediction request script.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/gcp-iap/README.md#2025-04-21_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ngcloud iam service-accounts create --project=$PROJECT $SERVICE_ACCOUNT\n\ngcloud projects add-iam-policy-binding $PROJECT \\\n --role roles/iap.httpsResourceAccessor \\\n --member serviceAccount:$SERVICE_ACCOUNT\n\nbash make-prediction.sh\n```\n\n----------------------------------------\n\nTITLE: Accessing Prometheus and Grafana Dashboards via Istio\nDESCRIPTION: Commands to access Prometheus and Grafana dashboards using istioctl\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/metrics/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Grafana\nistioctl dashboard grafana\n\n# Prometheus\nistioctl dashboard prometheus\n```\n\n----------------------------------------\n\nTITLE: Querying Bias Metrics - Python\nDESCRIPTION: Sends collected predictions and inputs to the AIF server for bias metrics computation. The 'query_bias.py' script calculates various metrics such as disparate impact to provide insights on possible model biases using the generated input.json file.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/aif/germancredit/README.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npython query_bias.py http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/$MODEL_NAME:explain ${SERVICE_HOSTNAME} input.json\n```\n\n----------------------------------------\n\nTITLE: Defining Parameter Message in Protocol Buffers\nDESCRIPTION: Protocol buffer definition for parameter values that can be boolean, integer, or string types. Used for both request and response parameters.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md#2025-04-21_snippet_17\n\nLANGUAGE: protobuf\nCODE:\n```\nmessage InferParameter {\n  oneof parameter_choice {\n    bool bool_param = 1;\n    int64 int64_param = 2;\n    string string_param = 3;\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Applying InferenceService Configuration - KServe - Shell\nDESCRIPTION: Applies the InferenceService configuration for autoscaling using kubectl. Assumes the configuration file is named autoscale.yaml and is available in the current working directory. The output confirms the service setup.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/autoscaling/README.md#2025-04-21_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nkubectl apply -f autoscale.yaml\n```\n\n----------------------------------------\n\nTITLE: Get predictor status - Kubernetes CLI\nDESCRIPTION: This command retrieves the predictor component status including the URLs of the revisions based on the tags provided.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/rollout/README.md#2025-04-21_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get isvc my-model -ojsonpath=\"{.status.components.predictor}\"  | jq\n```\n\n----------------------------------------\n\nTITLE: Applying Kubernetes Configuration for Secrets - Bash\nDESCRIPTION: This bash command applies the Kubernetes configuration defined in the 'azcreds.yaml' file, thereby creating the secret in the specified Kubernetes cluster context.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/storage/azure/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f azcreds.yaml\n```\n\n----------------------------------------\n\nTITLE: Install Python Dependencies\nDESCRIPTION: This command installs the necessary Python packages required to run the sample inference notebook. It uses `pip` and the `requirements.txt` file to install the listed dependencies.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/onnx/README.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Displaying Prediction (Python)\nDESCRIPTION: This snippet displays the prediction generated by the model. The output will be a NumPy array containing the predicted housing price.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/sklearn/v1/sklearn-mixedtype-model/sklearn-mixedtype-model.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nresponse\n```\n\n----------------------------------------\n\nTITLE: Checking Pod Status after Kourier Installation\nDESCRIPTION: This command checks the readiness status of all pods in the 'knative-serving' and 'kourier-system' namespaces, ensuring that all necessary components are running properly after installation.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/OPENSHIFT_GUIDE.md#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\noc get pods -n knative-serving && kubectl get pods -n kourier-system\n```\n\n----------------------------------------\n\nTITLE: Applying InferenceService CRD using kubectl - YAML\nDESCRIPTION: This command applies a custom resource definition for the InferenceService using KFServing. It's a required step to deploy the ML model for bias detection. The file 'bias.yaml' should be configured to define the InferenceService.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/aif/germancredit/README.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nkubectl apply -f bias.yaml\n```\n\n----------------------------------------\n\nTITLE: Sending Prediction Request\nDESCRIPTION: Sends a prediction request to the Torchserve MNIST model using the downloaded image\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/logger/logger_demo.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n!curl -v -H \"Host: $SERVICE_HOSTNAME\" http://$CLUSTER_IP/predictions/mnist -T 1.png\n```\n\n----------------------------------------\n\nTITLE: Creating Message Dumper Service for Payload Logs - YAML\nDESCRIPTION: Deploys a service called message-dumper that collects logs when predictions are made on the KFServing InferenceService. Kafka could be used in a production environment as an alternative. The 'message-dumper.yaml' file should detail the service configuration.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/aif/germancredit/README.md#2025-04-21_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nkubectl apply -f message-dumper.yaml\n```\n\n----------------------------------------\n\nTITLE: Dockerizing the BentoML Service for Deployment\nDESCRIPTION: This Bash code snippet demonstrates how to build a Docker image for the IrisClassifier prediction service using BentoML. It retrieves the saved model's path and constructs a Docker image for deployment on Docker Hub.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/bentoml/README.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmodel_path=$(bentoml get IrisClassifier:latest -q | jq -r \".uri.uri\")\n\n# Replace {docker_username} with your Docker Hub username\ndocker build -t {docker_username}/iris-classifier $model_path\ndocker push {docker_username}/iris-classifier\n```\n\n----------------------------------------\n\nTITLE: Getting Istio Ingress Gateway IP\nDESCRIPTION: Retrieves the cluster IP of the Istio ingress gateway to be used for sending requests to the service.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/logger/knative-eventing/logger_demo.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nCLUSTER_IPS = !(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\nCLUSTER_IP = CLUSTER_IPS[0]\nprint(CLUSTER_IP)\n```\n\n----------------------------------------\n\nTITLE: Running Helm Install Command\nDESCRIPTION: This console command installs the kserve-crd-minimal Helm chart from GitHub Container Registry using Helm. It requires Helm to be installed and configured in your environment. The command specifies the chart version v0.15.0 and assumes access to the OCI-based chart repository. The command must be run in a terminal with appropriate permissions.\nSOURCE: https://github.com/kserve/kserve/blob/master/charts/kserve-crd-minimal/README.md#2025-04-21_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n$ helm install kserve-crd-minimal oci://ghcr.io/kserve/charts/kserve-crd-minimal --version v0.15.0\n```\n\n----------------------------------------\n\nTITLE: Deleting Message Dumper Service\nDESCRIPTION: Removes the message dumper service from the kfserving-test namespace\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/logger/logger_demo.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n!kubectl delete -f message-dumper.yaml -n kfserving-test\n```\n\n----------------------------------------\n\nTITLE: Deleting Kubernetes Resources\nDESCRIPTION: Cleans up the resources created by deleting the cifar10.yaml configuration from the Kubernetes cluster in the default namespace.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/alibi/cifar10/cifar10_explanations.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n!kubectl delete -f cifar10.yaml -n default\n```\n\n----------------------------------------\n\nTITLE: Specifying Datatype for HuggingFace Model\nDESCRIPTION: Command to start a HuggingFace server with a specific datatype (float16) for the model. Available choices include float16, bfloat16, float32, auto, float, half.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/huggingfaceserver/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython -m huggingfaceserver --model_id=bert-base-uncased --model_name=bert --dtype=float16\n```\n\n----------------------------------------\n\nTITLE: Preparing model files for Triton Server\nDESCRIPTION: Shell commands to create a Triton-compatible model repository structure, copying the converted model and tokenizer files to the appropriate locations.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/triton/fastertransformer/README.md#2025-04-21_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\n# Create a folder for our model. Our model will have the name 'fastertransformer'\nmkdir -p model-repo/fastertransformer\n# Our model will be version 1\ncp -r bloom/1-gpu model-repo/fastertransformer/1\n# Add the tokenizer artifacts. These will be used by the KServe transformer\nmkdir model-repo/fastertransformer/1/tokenizer\ncp bloom-560m/tokenizer*.json model-repo/fastertransformer/1/tokenizer\n```\n\n----------------------------------------\n\nTITLE: Sample Response JSON from XGBoost Model\nDESCRIPTION: Example JSON response from the deployed XGBoost model following the V2 Dataplane protocol, showing the prediction results for the input data with model metadata.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/xgboost/README.md#2025-04-21_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\": \"4e546709-0887-490a-abd6-00cbc4c26cf4\",\n  \"model_name\": \"xgboost-iris\",\n  \"model_version\": \"v1.0.0\",\n  \"outputs\": [\n    {\n      \"data\": [1.0, 1.0],\n      \"datatype\": \"FP32\",\n      \"name\": \"predict\",\n      \"parameters\": null,\n      \"shape\": [2]\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Downloading Triton configuration for BLOOM model\nDESCRIPTION: Shell command to fetch the sample BLOOM model configuration for Triton from the FasterTransformer backend repository.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/triton/fastertransformer/README.md#2025-04-21_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\ncurl -L https://raw.githubusercontent.com/triton-inference-server/fastertransformer_backend/main/all_models/bloom/fastertransformer/config.pbtxt \\\n> model-repo/fastertransformer/config.pbtxt\n```\n\n----------------------------------------\n\nTITLE: Defining V1alpha1ClusterStorageContainer Properties\nDESCRIPTION: Defines the structure and attributes of a Kubernetes V1alpha1ClusterStorageContainer resource, including API version, kind, metadata, and optional specification\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/docs/V1alpha1ClusterStorageContainer.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass V1alpha1ClusterStorageContainer:\n    properties = {\n        \"api_version\": \"str\",\n        \"disabled\": \"bool\",\n        \"kind\": \"str\",\n        \"metadata\": \"V1ObjectMeta\",\n        \"spec\": \"V1alpha1StorageContainerSpec\"\n    }\n```\n\n----------------------------------------\n\nTITLE: Listing Available Disks on Worker Nodes with lsblk\nDESCRIPTION: Bash command to list available disk devices on worker nodes, showing the name, size, and mount points of available disks. Used to identify NVMe drives for Kubernetes persistent volumes.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v1/bloom/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nNAME          MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT\nnvme1n1       259:0    0 116.4G  0 disk \nnvme0n1       259:1    0    80G  0 disk \n├─nvme0n1p1   259:2    0    80G  0 part /\n└─nvme0n1p128 259:3    0     1M  0 part \n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries in Python\nDESCRIPTION: Imports necessary Python libraries for handling model explainability with KFServing. Includes libraries for requests, image processing, and data set handling. Assumes a Python environment with access to these libraries.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/aix/mnist/query_explain.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport sys\nimport requests\nimport json\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom aix360.datasets import MNISTDataset\nfrom keras.applications import inception_v3 as inc_net\nfrom keras.preprocessing import image\nfrom keras.applications.imagenet_utils import decode_predictions\nimport time\nfrom skimage.color import (\n    gray2rgb,\n    rgb2gray,\n    label2rgb,\n)  # since the code wants color images\n```\n\n----------------------------------------\n\nTITLE: Interpreting Bias Metrics - Bash\nDESCRIPTION: Demonstrates the output from querying bias metrics, highlighting key analysis points like disparate impact, which may indicate bias in the model. The output includes metrics such as base rate and consistency measures.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/aif/germancredit/README.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nbash-3.2$ python query_bias.py http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/$MODEL_NAME:explain ${SERVICE_HOSTNAME} input.json\\nSending bias query...\\nTIME TAKEN:  0.21137404441833496\\n<Response [200]>\\nbase_rate :  0.9329608938547486\\nconsistency :  [0.982122905027933]\\ndisparate_impact :  0.52\\nnum_instances :  179.0\\nnum_negatives :  12.0\\nnum_positives :  167.0\\nstatistical_parity_difference :  -0.48\n```\n\n----------------------------------------\n\nTITLE: Making REST Inference Request\nDESCRIPTION: curl command to make a prediction request to the TorchServe endpoint using REST.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v1/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl -v -H \"Host: ${SERVICE_HOSTNAME}\" http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/${MODEL_NAME}:predict -d @./mnist.json\n```\n\n----------------------------------------\n\nTITLE: Show Anchors\nDESCRIPTION: This snippet extracts the anchor from the explanation data and displays it using the `show_anchors` function.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/alibi/moviesentiment/movie_review_explanations.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n\"show_anchors(exp[\\\"data\\\"][\\\"anchor\\\"])\"\n```\n\n----------------------------------------\n\nTITLE: Load Testing Image Classifier with hey\nDESCRIPTION: This bash script performs load testing on an image classifier using the `hey` load generator. It sends POST requests with image data to the specified prediction endpoint, simulating user traffic to evaluate the service's performance.  It assumes `INGRESS_HOST` and `INGRESS_PORT` are already set.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/docs/autoscaling.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n\"### Sample load testing on image classifier\"\n\n\"./hey -m POST -z 30s -T \\\"image/*\\\" -D image_classifier/mnist/test_data -host ${SERVICE_HOSTNAME} http://${INGRESS_HOST}:${INGRESS_PORT}/predictions/mnist\"\n```\n\n----------------------------------------\n\nTITLE: Base64 Encoding Kerberos Keytab\nDESCRIPTION: Bash commands for base64 encoding a Kerberos keytab file for use in secret configuration\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/storage/hdfs/README.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ cat kerberos.keytab | base64 > kerberos.keytab.b64\n# Copy data to clipboard on osx using pbcopy\n$ cat kerberos.keytab.b64 | pbcopy\n```\n\n----------------------------------------\n\nTITLE: Curling the Inference Service\nDESCRIPTION: This block of commands sends a prediction request to the Inference Service using curl, demonstrating how to use the service after creation.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/OPENSHIFT_GUIDE.md#2025-04-21_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nMODEL_NAME=sklearn-iris\\\nINPUT_PATH=@samples/v1beta1/sklearn/v1/iris-input.json\\\nSERVICE_HOSTNAME=$(oc get inferenceservice sklearn-iris -o jsonpath='{.status.url}' -n kserve-demo | cut -d \"/\" -f 3)\\\necho \"Using payload from $INPUT_PATH\"\\\ncat $(echo $INPUT_PATH | cut -d@ -f 2)\\\ncurl -k -H \"Host: ${SERVICE_HOSTNAME}\" -H \"Content-Type: application/json\" \\\n  https://$SERVICE_HOSTNAME/v1/models/$MODEL_NAME:predict -d $INPUT_PATH\n```\n\n----------------------------------------\n\nTITLE: Configuring Sequence Node in KServe Inference Graph\nDESCRIPTION: A YAML configuration for a Sequence Node that connects multiple InferenceServices in a sequence. The example shows how to define steps that are executed in order, with options to pass request or response data between services.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/graph/README.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n...\nroot:\n  routerType: Sequence \n  steps:\n  - serviceName: isvc1\n  - serviceName: isvc2\n    data: $request\n...\n```\n\n----------------------------------------\n\nTITLE: Defining V1beta1 InferenceServices Configuration Properties\nDESCRIPTION: Represents a configuration schema for KServe InferenceServices with options to configure explainers, resource allocation, and service metadata restrictions\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/docs/V1beta1InferenceServicesConfig.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass V1beta1InferenceServicesConfig:\n    properties:\n        explainers: V1beta1ExplainersConfig\n        resource: V1beta1ResourceConfig (optional)\n        service_annotation_disallowed_list: list[str]\n        service_label_disallowed_list: list[str]\n```\n\n----------------------------------------\n\nTITLE: Patching Kubernetes Configurations for Triton Setup\nDESCRIPTION: These Bash commands modify Kubernetes ConfigMaps for deploying Triton efficiently on KFServing. They skip tag resolution for nvcr.io and increase the deadline for pulling large images.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/multimodelserving/triton/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkubectl patch cm config-deployment --patch '{\"data\":{\"registriesSkippingTagResolving\":\"nvcr.io\"}}' -n knative-serving\n```\n\nLANGUAGE: bash\nCODE:\n```\nkubectl patch cm config-deployment --patch '{\"data\":{\"progressDeadline\": \"600s\"}}' -n knative-serving\n```\n\n----------------------------------------\n\nTITLE: Processing Logs for Bias Metrics - Python\nDESCRIPTION: Processes the payload logs from message-dumper, creating an interpretable JSON file for AI Fairness 360. The 'json_from_logs.py' script matches inputs with outputs and generates a 'data.json' file for bias analysis.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/aif/germancredit/README.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npython json_from_logs.py\n```\n\n----------------------------------------\n\nTITLE: Starting Echo HTTP Server with Docker\nDESCRIPTION: Runs a Docker container that echoes HTTP requests on port 8000. This is used to capture and display logged requests.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/logger/local/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -it -p 8000:80 --rm -t mendhak/http-https-echo\n```\n\n----------------------------------------\n\nTITLE: Creating KServe InferenceService for BLOOM Model\nDESCRIPTION: Command to apply the InferenceService custom resource definition to Kubernetes, which deploys the TorchServe BLOOM model as a serving endpoint in KServe.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v1/bloom/README.md#2025-04-21_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f bloom-560m.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing Service Mesh Operator with Bash\nDESCRIPTION: This snippet installs the Service Mesh operator on OpenShift. It includes commands to ensure the necessary service mesh components are ready and guides on setting up namespaces for service mesh members.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/OPENSHIFT_GUIDE.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Install Service Mesh operator\noc apply -f openshift/service-mesh/operators.yaml\noc wait --for=condition=ready pod --all -n openshift-operators --timeout=300s\n\n\n# Create an istio instance\noc create ns istio-system\noc apply -f openshift/service-mesh/smcp.yaml\noc wait --for=condition=ready pod --all -n openshift-operators --timeout=300s\noc wait --for=condition=ready pod --all -n istio-system --timeout=300s\n\n# Make sure to add your namespaces to the ServiceMeshMemberRoll and create a\n# PeerAuthentication Policy for each of your namespaces\noc create ns kserve; oc create ns kserve-demo\noc apply -f openshift/service-mesh/smmr.yaml\noc apply -f openshift/service-mesh/peer-authentication.yaml\n\n# Create an Knative instance\noc apply -f openshift/serverless/knativeserving-istio.yaml\n```\n\nLANGUAGE: bash\nCODE:\n```\n# Wait for all pods in `knative-serving` to be ready\noc wait --for=condition=ready pod --all -n knative-serving --timeout=300s\n# the pods needs to be like:\n# oc get pods -n knative-serving\n#NAME                              READY   STATUS    RESTARTS   AGE\n#activator-748f464bd6-j6ptx        2/2     Running   0          165m\n#activator-748f464bd6-n9w8g        2/2     Running   0          166m\n#autoscaler-6fc46ddb97-h5x7n       2/2     Running   0          166m\n#autoscaler-6fc46ddb97-l28m9       2/2     Running   0          166m\n#autoscaler-hpa-67cb476ddc-h6m7g   2/2     Running   0          166m\n#autoscaler-hpa-67cb476ddc-rr4sg   2/2     Running   0          166m\n#controller-5bb67c45f7-5tz2h       2/2     Running   0          165m\n#controller-5bb67c45f7-mv68q       2/2     Running   0          166m\n#webhook-6cb5d8875d-964sc          2/2     Running   0          166m\n#webhook-6cb5d8875d-pg5gq          2/2     Running   0          165m\n```\n\n----------------------------------------\n\nTITLE: Expected Output After Creating InferenceService\nDESCRIPTION: Shows the expected console output after successfully creating the InferenceService in Kubernetes.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/canary/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$inferenceservice.serving.kserve.io/torchserve created\n```\n\n----------------------------------------\n\nTITLE: Deploying SKLearn Iris Model Locally\nDESCRIPTION: Downloads the SKLearn Iris model and starts the sklearnserver on port 8080. Requires the sklearnserver to be installed via pip.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/logger/local/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngsutil cp -r gs://kfserving-examples/models/sklearn/1.0/model .\nLOCAL_DIR=$(pwd)/iris\npython -m sklearnserver --model_dir $LOCAL_DIR --model_name sklearn-iris --http_port 8080\n```\n\n----------------------------------------\n\nTITLE: Performing Static Type Checks with Mypy\nDESCRIPTION: The mypy command checks for type errors in the lgbserver module. To ignore missing import statements, use the --ignore-missing-imports flag. An empty result indicates success.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/lgbserver/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmypy --ignore-missing-imports lgbserver\n```\n\n----------------------------------------\n\nTITLE: Running Tests for Scikit-Learn Server\nDESCRIPTION: Command to run the test suite for the Scikit-Learn server. Ensure that the test file points to your model.joblib file before running this command.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/sklearnserver/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmake test\n```\n\n----------------------------------------\n\nTITLE: Building PMML Server Docker Image\nDESCRIPTION: Command to build a Docker image for the PMML server using a custom Dockerfile\nSOURCE: https://github.com/kserve/kserve/blob/master/python/pmmlserver/README.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker build . -t docker_user_name/pmmlserver:latest -f pmml.Dockerfile\n```\n\n----------------------------------------\n\nTITLE: Fetch Movie Sentiment Data\nDESCRIPTION: This snippet fetches the movie sentiment dataset using the `fetch_movie_sentiment` function from the `alibi.datasets` module.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/alibi/moviesentiment/movie_review_explanations.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n\"from alibi.datasets import fetch_movie_sentiment\\n\\nmovies = fetch_movie_sentiment()\"\n```\n\n----------------------------------------\n\nTITLE: Display YAML configuration\nDESCRIPTION: This snippet uses the `pygmentize` command to display the contents of the `moviesentiment.yaml` file, which likely contains the configuration for the KServe inference service.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/alibi/moviesentiment/movie_review_explanations.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"!pygmentize moviesentiment.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Verifying Inference Service Readiness\nDESCRIPTION: This snippet provides a command to verify that the InferenceService is ready by waiting for the Ready condition. It assumes an existing Kubernetes setup with kubectl access. The input is the specific service name, and the output indicates the readiness of the service.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/performance-testing/grpc.md#2025-04-21_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nkubectl wait --for=condition=Ready --timeout=600s isvc/sklearn-irisv2\n```\n\n----------------------------------------\n\nTITLE: Applying Persistent Volume Claim with kubectl\nDESCRIPTION: This command applies the persistent volume claim configuration from the pvc.yaml file using Kubernetes. The expected output confirms the creation of the persistent volume claim.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/custom-server-with-external-storage.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f pvc.yaml\n```\n\n----------------------------------------\n\nTITLE: GKE GPU Node Taint Specification\nDESCRIPTION: The node taint that GKE automatically applies to GPU-enabled nodes. This taint prevents regular workloads from being scheduled on expensive GPU nodes unless they have a matching toleration.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/accelerators/README.md#2025-04-21_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nKey: nvidia.com/gpu\nEffect: NoSchedule\n```\n\n----------------------------------------\n\nTITLE: Installing Development Dependencies for Scikit-Learn Server\nDESCRIPTION: Command to install the development dependencies for the Scikit-Learn server. This is useful for contributors and developers working on the server code.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/sklearnserver/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmake dev_install\n```\n\n----------------------------------------\n\nTITLE: Get InferenceService status - Kubernetes CLI\nDESCRIPTION: This command retrieves information about the InferenceService named `my-model`. It shows the traffic split between the previous and latest revisions.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/rollout/README.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get isvc my-model\n```\n\n----------------------------------------\n\nTITLE: Getting Istio Ingress Gateway IP address\nDESCRIPTION: This snippet retrieves the IP address of the Istio Ingress Gateway using `kubectl`. This IP address is used to send requests to the deployed services.  The LoadBalancer is assumed to be set up correctly.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/drift-detection/alibi-detect/cifar10/cifar10_drift.ipynb#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n\"CLUSTER_IPS = !(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\nCLUSTER_IP = CLUSTER_IPS[0]\nprint(CLUSTER_IP)\"\n```\n\n----------------------------------------\n\nTITLE: InferenceService with HDFS Storage URI\nDESCRIPTION: Example InferenceService configuration specifying HDFS model storage location and service account\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/storage/hdfs/README.md#2025-04-21_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: serving.kserve.io/v1beta1\nkind: InferenceService\nmetadata:\n  name: model-test\nspec:\n  predictor:\n    serviceAccountName: sa\n    model:\n    \tstorageUri: \"hdfs://path/to/model\"\n```\n\n----------------------------------------\n\nTITLE: Retrieving OpenShift Namespace Annotation Range\nDESCRIPTION: This bash command retrieves the annotation range for a specific namespace in OpenShift, which is needed to set the correct storage-initializer-uid for KServe resources.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/OPENSHIFT_GUIDE.md#2025-04-21_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\noc describe namespace kserve-demo\n```\n\n----------------------------------------\n\nTITLE: Deploying Message Dumper Knative Service\nDESCRIPTION: Applies the message-dumper.yaml configuration to create a Knative service that will capture and print CloudEvents.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/logger/knative-eventing/logger_demo.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n!kubectl apply -f message-dumper.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Spark MLlib PMML\nDESCRIPTION: Commands to install the required Python packages for working with Spark MLlib and PMML export functionality.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/spark/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install pyspark~=3.0.0\npip install pyspark2pmml\n```\n\n----------------------------------------\n\nTITLE: Cleaning Up KServe Installation with Bash\nDESCRIPTION: This snippet provides commands for cleaning up the KServe installation from the OpenShift platform. It involves uninstalling the KServe and all operators with associated resources, making it ideal for resetting the setup to its initial state.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/OPENSHIFT_GUIDE.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Uninstall KServe\noc delete -f \"https://github.com/kserve/kserve/releases/download/${KSERVE_VERSION}/kserve.yaml\"\noc delete ns kserve-demo\n\n# Uninstall Serverless operator\noc delete KnativeServing knative-serving  -n knative-serving\noc delete all --all -n openshift-serverless --force --grace-period=0 \noc patch knativeservings.operator.knative.dev knative-serving -n knative-serving -p '{\"metadata\": {\"finalizers\": []}}' --type=merge\noc delete KnativeServing --all -n knative-serving --force --grace-period=0\noc delete pod,deployment,all --all -n knative-serving  --force --grace-period=0\noc patch ns knative-serving -p '{\"metadata\": {\"finalizers\": []}}' --type=merge\noc patch ns knative-eventing -p '{\"metadata\": {\"finalizers\": []}}' --type=merge\noc patch ns rhoai-serverless -p '{\"metadata\": {\"finalizers\": []}}' --type=merge\noc patch ns openshift-serverless -p '{\"metadata\": {\"finalizers\": []}}' --type=merge\n\noc delete validatingwebhookconfiguration $(oc get validatingwebhookconfiguration --no-headers|grep -E \"kserve|knative|istio\"|awk '{print $1}')\noc delete mutatingwebhookconfiguration $(oc get mutatingwebhookconfiguration --no-headers|grep -E \"kserve|knative|istio\"|awk '{print $1}')\n```\n\nLANGUAGE: bash\nCODE:\n```\noc delete ServiceMeshControlPlane,pod --all -n istio-system --force --grace-period=0  -n istio-system\noc patch smcp $(oc get smcp -n istio-system | awk 'NR>1 {print $1}') -n istio-system -p '{\"metadata\": {\"finalizers\": []}}' --type=merge\noc delete smcp -n istio-system $(oc get smcp -n istio-system | awk 'NR>1 {print $1}')\noc patch smmr $(oc get smmr -n istio-system | awk 'NR>1 {print $1}') -n istio-system -p '{\"metadata\": {\"finalizers\": []}}' --type=merge\noc delete smm --all --force --grace-period=0  -A\noc delete smmr,smm --all --force --grace-period=0  -n istio-system\noc patch ns istio-system -p '{\"metadata\": {\"finalizers\": []}}' --type=merge\noc delete ns istio-system\n\noc delete subscription serverless-operator -n openshift-serverless\noc delete csv $(oc get csv -n openshift-serverless -o jsonpath='{.items[?(@.spec.displayName==\"Red Hat OpenShift Serverless\")].metadata.name}') -n openshift-serverless\noc delete csv OperatorGroup serverless-operators -n openshift-serverless\n```\n\nLANGUAGE: bash\nCODE:\n```\noc delete sub servicemeshoperator --force --grace-period=0 -n openshift-operators\noc delete svc maistra-admission-controller -n openshift-operators\noc delete csv $(oc get csv -n openshift-operators | grep servicemeshoperator|awk '{print $1}') -n openshift-operators\n\n# Uninstall cert-manager operator\noc delete -f openshift/cert-manager/operator.yaml\n```\n\n----------------------------------------\n\nTITLE: Running Static Type Checks for Scikit-Learn Server\nDESCRIPTION: Command to perform static type checking on the Scikit-Learn server code using mypy. An empty result indicates successful type checking.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/sklearnserver/README.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmypy --ignore-missing-imports sklearnserver\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes namespace\nDESCRIPTION: This snippet creates a Kubernetes namespace named `cifar10`. Namespaces provide a way to logically isolate resources within a Kubernetes cluster. This command ensures all the resources created for this example will reside in their own namespace.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/drift-detection/alibi-detect/cifar10/cifar10_drift.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n\"!kubectl create namespace cifar10\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Pod Security Context\nDESCRIPTION: Defines security-related settings for a Kubernetes pod, controlling permissions, user contexts, and security capabilities at the pod level.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/docs/V1beta1CustomExplainer.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nsecurity_context: V1PodSecurityContext = None\n```\n\n----------------------------------------\n\nTITLE: Deploying Cifar10 PyTorch Model with TrainedModel\nDESCRIPTION: This YAML configuration registers a Cifar10 model trained with PyTorch onto the existing Triton InferenceService, specifying memory requirements and storage URI.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/multimodelserving/triton/README.md#2025-04-21_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: \"serving.kserve.io/v1alpha1\"\nkind: \"TrainedModel\"\nmetadata:\n  name: \"cifar10\"\nspec:\n  inferenceService: triton-mms\n  model:\n    framework: pytorch\n    storageUri: gs://kfserving-examples/models/torchscript/cifar10\n    memory: 1Gi\n```\n\n----------------------------------------\n\nTITLE: Teardown for Income Prediction Model Deployment with Python\nDESCRIPTION: This command is used to delete the income.yaml configuration from the Kubernetes cluster, effectively tearing down the deployed resources related to the income prediction model.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/alibi/income/income_explanations.ipynb#2025-04-21_snippet_13\n\nLANGUAGE: Shell\nCODE:\n```\n!kubectl delete -f income.yaml\n```\n\n----------------------------------------\n\nTITLE: Launching Iter8 gRPC Experiment\nDESCRIPTION: The snippet launches an Iter8 experiment in a Kubernetes environment to test the inference service's performance. It specifies tasks and settings for readiness, gRPC request generation, and performance assessments based on SLOs. Dependencies include an installed Iter8 CLI and a running kubectl setup. Inputs include configuration settings, and the output is an experiment execution within Kubernetes.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/performance-testing/grpc.md#2025-04-21_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\niter8 k launch \\\n--set \"tasks={ready,grpc,assess}\" \\\n--set ready.isvc=sklearn-irisv2 \\\n--set ready.timeout=600s \\\n--set grpc.protoURL=https://raw.githubusercontent.com/kserve/kserve/master/docs/predict-api/v2/grpc_predict_v2.proto \\\n--set grpc.host=sklearn-irisv2-predictor-default.default.svc.cluster.local:80 \\\n--set grpc.call=inference.GRPCInferenceService.ModelInfer \\\n--set grpc.dataURL=https://gist.githubusercontent.com/kalantar/6e9eaa03cad8f4e86b20eeb712efef45/raw/56496ed5fa9078b8c9cdad590d275ab93beaaee4/sklearn-irisv2-input-grpc.json \\\n--set grpc.warmupNumRequests=10 \\\n--set assess.SLOs.upper.grpc/latency/mean=500 \\\n--set assess.SLOs.upper.grpc/latency/p90=900 \\\n--set assess.SLOs.upper.grpc/error-count=0 \\\n--set runner=job\n```\n\n----------------------------------------\n\nTITLE: Updating inferenceservice-config ConfigMap - YAML\nDESCRIPTION: This snippet outlines how to update the inferenceservice-config ConfigMap to include the CA bundle configuration for all inference services. This is essential for KServe to function properly with self-signed certificates.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/cabundle/README.md#2025-04-21_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nstorageInitializer: |-\n{\n    ...\n    \"caBundleConfigMapName\": \"cabundle\",\n    ...\n}\n```\n\n----------------------------------------\n\nTITLE: Installing XGBoost Server for Development in KServe\nDESCRIPTION: Command to install the XGBoost server locally for development purposes. This should be run in the project's root directory after installing KServe.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/xgbserver/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmake dev_install\n```\n\n----------------------------------------\n\nTITLE: Validating LightGBM Server Installation\nDESCRIPTION: Execute the command to run the LightGBM server and check for successful installation. This requires the --model_dir parameter, indicating where the model is stored. The server can load models from different storage services.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/lgbserver/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m lgbserver\nusage: __main__.py [-h] [--http_port HTTP_PORT] [--grpc_port GRPC_PORT]\n                   --model_dir MODEL_DIR [--model_name MODEL_NAME]\n__main__.py: error: the following arguments are required: --model_dir\n```\n\n----------------------------------------\n\nTITLE: List pods after rollback - Kubernetes CLI\nDESCRIPTION: This command lists the pods associated with the InferenceService `my-model` after rollback. It verifies that the previous revision pods are running and the new revision pods are scaling down.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/rollout/README.md#2025-04-21_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get pods -l serving.kserve.io/inferenceservice=my-model\n```\n\n----------------------------------------\n\nTITLE: Deploying SKLearn InferenceService in Kubeflow Namespace\nDESCRIPTION: Command to apply the SKLearn InferenceService manifest in a Kubeflow user namespace. This creates a machine learning model endpoint that will be secured by Istio-Dex authentication.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/istio-dex/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f sklearn.yaml -n kubeflow-user-example-com\n```\n\n----------------------------------------\n\nTITLE: Checking Triton Inference Server Logs\nDESCRIPTION: Log snippet showing successful model loading into Triton Inference Server, confirming that models are active and ready for inference.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/multimodelserving/triton/README.md#2025-04-21_snippet_7\n\nLANGUAGE: \nCODE:\n```\nI0522 20:59:09.469834 1 model_repository_manager.cc:737] loading: cifar10:1\nI0522 20:59:09.471278 1 libtorch_backend.cc:217] Creating instance cifar10_0_0_cpu on CPU using model.pt\nI0522 20:59:09.638318 1 model_repository_manager.cc:925] successfully loaded 'cifar10' version 1\n```\n\n----------------------------------------\n\nTITLE: Deploying Message Dumper Service to Kubernetes\nDESCRIPTION: Applies the message dumper YAML configuration to the default namespace\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/logger/logger_demo.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n!kubectl apply -f message-dumper.yaml -n default\n```\n\n----------------------------------------\n\nTITLE: Defining Ensemble Model Route in InferenceGraph\nDESCRIPTION: This YAML snippet defines an InferenceGraph called 'ensemble' utilizing the Sequence router type. It depicts a processing flow where an ensemble of models combines results from multiple services for making predictions, illustrating a model ensemble strategy.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/docs/V1alpha1InferenceRouter.md#2025-04-21_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nkind: InferenceGraph\nmetadata:\n  name: ensemble\nspec:\n  nodes:\n    root:\n      routerType: Sequence\n      routes:\n      - service: feast\n      - nodeName: ensembleModel\n        data: $response\n    ensembleModel:\n      routerType: Ensemble\n      routes:\n      - service: sklearn-model\n      - service: xgboost-model\n```\n\n----------------------------------------\n\nTITLE: Installing KServe and Network Policies with Bash\nDESCRIPTION: This snippet details the installation of KServe on OpenShift, including the setup of network policies to allow corresponding traffic. It includes commands to download and apply necessary resources and configurations needed for KServe.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/OPENSHIFT_GUIDE.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Create the Knative gateways\n# This contains a self-signed TLS certificate, you can change this to your own\n# Please consider https://access.redhat.com/documentation/en-us/openshift_container_platform/4.12/html/serverless/integrations#serverlesss-ossm-external-certs_serverless-ossm-setup for more information\noc apply -f openshift/serverless/gateways.yaml\n\n# Install KServe\nKSERVE_VERSION=v0.14.1\noc apply --server-side -f \"https://github.com/kserve/kserve/releases/download/${KSERVE_VERSION}/kserve.yaml\"\noc wait --for=condition=ready pod -l control-plane=kserve-controller-manager -n kserve --timeout=300s\n\n# Add NetworkPolicies to allow traffic to kserve webhook\noc apply -f openshift/networkpolicies.yaml\n\n# Install KServe built-in serving runtimes and storagecontainers\noc apply -f \"https://github.com/kserve/kserve/releases/download/${KSERVE_VERSION}/kserve-cluster-resources.yaml\"\n```\n\nLANGUAGE: bash\nCODE:\n```\noc edit configmap/inferenceservice-config --namespace kserve\n# edit this section\ningress : |- {\n    \"disableIstioVirtualHost\": true\n}\nYou are now ready to [testing](#test-kserve-installation) the installation.\n```\n\n----------------------------------------\n\nTITLE: Cleaning Up Iter8 Experiment and InferenceService\nDESCRIPTION: This snippet deletes the created Iter8 experiment and the KServe inference service, aiding in resource cleanup post-testing. It depends on kubectl and Iter8 CLI to properly delete resources from Kubernetes. Inputs are the CLI commands, resulting in cleaned up resources in the Kubernetes environment.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/performance-testing/grpc.md#2025-04-21_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\niter8 k delete\nkubectl delete isvc sklearn-irisv2\n```\n\n----------------------------------------\n\nTITLE: Running Tests for XGBoost Server\nDESCRIPTION: Command to execute the test suite for the XGBoost server project.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/xgbserver/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmake test\n```\n\n----------------------------------------\n\nTITLE: Basic KServe InferenceService Pipeline Creation\nDESCRIPTION: Example of a basic Kubeflow Pipeline that uses the KServe component to deploy a TensorFlow model. The pipeline applies a model from a Google Cloud Storage location using the tensorflow framework.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/kfp/README.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@dsl.pipeline(\n  name='KServe Pipeline',\n  description='A pipeline for KServe.'\n)\ndef kserve_pipeline():\n    kserve_op(\n        action='apply',\n        model_name='tf-sample',\n        model_uri='gs://kfserving-examples/models/tensorflow/flowers',\n        framework='tensorflow',\n    )\nkfp.Client().create_run_from_pipeline_func(kserve_pipeline, arguments={})\n```\n\n----------------------------------------\n\nTITLE: Defining PyTorch Model Serving Container Specification\nDESCRIPTION: Specification for configuring a PyTorch model serving container with detailed Kubernetes-compatible properties. Includes options for container runtime, environment setup, image configuration, and resource management.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/docs/V1beta1TorchServeSpec.md#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nV1beta1TorchServeSpec(\n    args=[...],\n    command=[...],\n    env=[...],\n    image=\"pytorch-model-server\",\n    image_pull_policy=\"Always\",\n    runtime_version=\"latest\"\n)\n```\n\n----------------------------------------\n\nTITLE: Applying the InferenceService with Kubectl Commands\nDESCRIPTION: This shell snippet shows how to use kubectl to apply a custom resource definition (CRD) for an inference service based on a specified configuration YAML file. It expects that the Kubernetes cluster environment is set up properly.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/sklearn/v1/README.md#2025-04-21_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nkubectl apply -f sklearn.yaml\n```\n\n----------------------------------------\n\nTITLE: Teardown Kubernetes resources\nDESCRIPTION: This snippet deletes the Kubernetes resources defined in the `moviesentiment.yaml` file, effectively tearing down the deployed inference service.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/alibi/moviesentiment/movie_review_explanations.ipynb#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n\"!kubectl delete -f moviesentiment.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Plotting Feature Scores for Outlier Detection - Python\nDESCRIPTION: This snippet visualizes the feature scores returned by the outlier detector along with the original masked image, providing insight into how the decision was made regarding its classification.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/outlier-detection/alibi-detect/cifar10/cifar10_outlier.ipynb#2025-04-21_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nplot_feature_outlier_image(od_preds, X_mask, X_recon=None)\n```\n\n----------------------------------------\n\nTITLE: Applying PVC and Pod Configuration\nDESCRIPTION: Command to apply the PVC (Persistent Volume Claim) and pod configuration to the Kubernetes cluster, which will be used to store and access the model files.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v1/bloom/README.md#2025-04-21_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f pvc-pod.yaml\n```\n\n----------------------------------------\n\nTITLE: Create Knative Monitoring Namespace\nDESCRIPTION: Creates the `knative-monitoring` namespace where Prometheus and Grafana will be deployed. This namespace isolates the monitoring components from other services in the cluster.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/docs/metrics.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n\"Kubectl create namespace knative-monitoring\"\n```\n\n----------------------------------------\n\nTITLE: Defining Features (Python)\nDESCRIPTION: This snippet defines a list of feature names to be used for training the model. The features are selected from the columns of the training data.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/sklearn/v1/sklearn-mixedtype-model/sklearn-mixedtype-model.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfeatures = [\n    \"MSZoning\",\n    \"LotArea\",\n    \"LotShape\",\n    \"Utilities\",\n    \"YrSold\",\n    \"Neighborhood\",\n    \"OverallQual\",\n    \"YearBuilt\",\n    \"SaleType\",\n    \"GarageArea\",\n]\n```\n\n----------------------------------------\n\nTITLE: Extracting drift result from Event Display logs\nDESCRIPTION: This snippet extracts the first drift result from the `hello-display` logs.  It parses the logs, extracts the data section, and then parses the JSON data to determine if drift was detected. The drift result is stored in the message-dumper.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/drift-detection/alibi-detect/cifar10/cifar10_drift.ipynb#2025-04-21_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n\"res = !kubectl logs -n cifar10 $(kubectl get pod -n cifar10 -l app=hello-display -o jsonpath='{.items[0].metadata.name}')\ndata = []\nfor i in range(0, len(res)):\n    if res[i] == \\\"Data,\\\":\n        data.append(res[i + 1])\nj = json.loads(json.loads(data[0]))\nprint(\\\"Drift\\\", j[\\\"data\\\"][\\\"is_drift\\\"] == 1)\"\n```\n\n----------------------------------------\n\nTITLE: Applying Event Display configuration\nDESCRIPTION: This snippet applies the Kubernetes Deployment and Service configuration for the event display defined in the `event-display.yaml` file.  This creates the hello-display deployment and service within the cifar10 namespace.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/drift-detection/alibi-detect/cifar10/cifar10_drift.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n\"!kubectl apply -f event-display.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Sending corrupted examples to trigger drift detection\nDESCRIPTION: This snippet sends 5000 corrupted CIFAR10 examples to the Cifar10 model in batches of 100. This is designed to trigger the drift detector after enough requests have been sent.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/drift-detection/alibi-detect/cifar10/cifar10_drift.ipynb#2025-04-21_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n\"for i in tqdm(range(0, 5000, 100)):\n    X = X_corr[i : i + 100]\n    predict(X)\"\n```\n\n----------------------------------------\n\nTITLE: Deleting Message Dumper Configuration in Python\nDESCRIPTION: This snippet removes the message-dumper configuration from the Kubernetes cluster.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/logger/basic/logger_demo.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n!kubectl delete -f message-dumper.yaml\n```\n\n----------------------------------------\n\nTITLE: Viewing Income Prediction Service Configuration with Python\nDESCRIPTION: This snippet uses the pygmentize command to display the content of the income.yaml configuration file, which defines the resources for the income prediction model in KFServing.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/alibi/income/income_explanations.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\n!pygmentize income.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing Nvidia Device Drivers on Kubernetes\nDESCRIPTION: Command to install Nvidia drivers using Google's DaemonSet for Kubernetes. This automatically installs the drivers on all nodes to enable GPU scheduling and acceleration.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/accelerators/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/521205d4de2a4a4eaf7259b27bc2290c823d857c/nvidia-driver-installer/cos/daemonset-preloaded.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining V1beta1 ExplainerSpec Struct\nDESCRIPTION: Struct definition for specifying model explanation server configuration with mutually exclusive explanation options\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/docs/V1beta1ExplainerSpec.md#2025-04-21_snippet_0\n\nLANGUAGE: go\nCODE:\n```\ntype V1beta1ExplainerSpec struct {\n  // ExplainerSpec defines the container spec for a model explanation server\n  // The following fields follow a \"1-of\" semantic. Users must specify exactly one spec.\n}\n```\n\n----------------------------------------\n\nTITLE: Building Custom Scikit-Learn Server Docker Image\nDESCRIPTION: Command to build a custom Docker image for the Scikit-Learn server. This allows developers to create their own server image with specific configurations.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/sklearnserver/README.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t docker_user_name/sklearnserver -f sklearn.Dockerfile .\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements for Alibi Detect in KServe\nDESCRIPTION: Specifies three required Python packages with minimum version constraints: alibi-detect for outlier/drift detection, matplotlib for visualization capabilities, and tqdm for progress monitoring.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/drift-detection/alibi-detect/cifar10/requirements_notebook.txt#2025-04-21_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nalibi-detect>=0.4.0\nmatplotlib>=3.1.1\ntqdm>=4.45.0\n```\n\n----------------------------------------\n\nTITLE: Defining Model Metadata Request and Response in Protobuf\nDESCRIPTION: Details the ModelMetadata API's messages for acquiring model-specific metadata such as model name, versions, platform, and tensor metadata for inputs and outputs. These are critical for integrating models accurately with client applications.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md#2025-04-21_snippet_14\n\nLANGUAGE: protobuf\nCODE:\n```\nmessage ModelMetadataRequest\n{\n  string name = 1;\n  string version = 2;\n}\n\nmessage ModelMetadataResponse\n{\n  message TensorMetadata\n  {\n    string name = 1;\n    string datatype = 2;\n    repeated int64 shape = 3;\n  }\n\n  string name = 1;\n  repeated string versions = 2;\n  string platform = 3;\n  repeated TensorMetadata inputs = 4;\n  repeated TensorMetadata outputs = 5;\n}\n```\n\n----------------------------------------\n\nTITLE: Deploying InferenceService KServe YAML\nDESCRIPTION: This snippet depicts the deployment of an InferenceService in KServe using a YAML configuration that specifies a Scikit-learn model served via gRPC. Dependencies include a Kubernetes cluster with kubectl installed and configured. The primary input is the YAML configuration, and the expected output is a deployed service ready for inference requests.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/performance-testing/grpc.md#2025-04-21_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncat <<EOF | kubectl apply -f -\napiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"sklearn-irisv2\"\nspec:\n  predictor:\n    model:\n      modelFormat:\n        name: sklearn\n      runtime: kserve-mlserver\n      protocolVersion: v2\n      storageUri: \"gs://seldon-models/sklearn/mms/lr_model\"\n      ports:\n      - containerPort: 9000\n        name: h2c\n        protocol: TCP\nEOF\n```\n\n----------------------------------------\n\nTITLE: Deploying Message Dumper Service\nDESCRIPTION: This shell command applies a YAML configuration to create the 'message-dumper' service on the Kubernetes cluster using the 'kubectl' command-line tool.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/logger/knative-eventing/README.md#2025-04-21_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nkubectl create -f message-dumper.yaml\n```\n\n----------------------------------------\n\nTITLE: Displaying Masked Image - Python\nDESCRIPTION: This snippet shows the masked version of the image created in the previous step and uses the predict function to classify it as well.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/outlier-detection/alibi-detect/cifar10/cifar10_outlier.ipynb#2025-04-21_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nshow(X_mask)\npredict(X_mask)\n```\n\n----------------------------------------\n\nTITLE: Deleting Trained Models in KServe\nDESCRIPTION: This command removes specified inference services or trained models in a KServe deployment using kubectl. It requires the name of the service or model to be deleted. The command helps in managing model lifecycle by facilitating model cleanup.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/multimodelserving/triton/README.md#2025-04-21_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete inferenceservice triton-mms\n```\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete tm $MODEL_NAME\n```\n\n----------------------------------------\n\nTITLE: Querying ART Explainer with custom MNIST index or input file\nDESCRIPTION: Python commands to query the ART Explainer with a specific MNIST image index or a custom input file. The data format for custom input must be {\"instances\": [<image>, <label>]}.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/art/mnist/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython query_explain.py http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/${MODEL_NAME}:explain ${SERVICE_HOSTNAME} 100\npython query_explain.py http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/${MODEL_NAME}:explain ${SERVICE_HOSTNAME} ./input.json\n```\n\n----------------------------------------\n\nTITLE: Installing Paddle Server Locally - Make Command\nDESCRIPTION: This snippet shows how to install Paddle Server for local development using a Makefile. The command should be executed in the root folder of the GitHub repository.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/paddleserver/README.md#2025-04-21_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nmake dev_install\n```\n\n----------------------------------------\n\nTITLE: Defining Python Dependencies for KServe with Alibi\nDESCRIPTION: A requirements file that specifies the Python package dependencies needed for KServe with Alibi explainer integration. It pins Alibi to version 0.6.0 and scikit-learn to version 0.20.3, while setting minimum versions for dill and plotly packages.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/alibi/moviesentiment/requirements.txt#2025-04-21_snippet_0\n\nLANGUAGE: pip\nCODE:\n```\nalibi==0.6.0\nscikit-learn == 0.20.3\ndill>=0.3.0\nplotly>=4.2.1\n```\n\n----------------------------------------\n\nTITLE: Testing Docker Container with Python Script\nDESCRIPTION: This command executes a Python script to test the Docker container. Ensure you have Python installed and that 'query_bias.py' and 'input.json' are located in the root directory. It sends a test request to the locally running model service.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/aif/germancredit/server/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# make sure you are in the example's root directory where the `query_bias.py` script and the `input.json` file are located.\npython query_bias.py http://localhost:8080/v1/models/german-credit:predict input.json\n```\n\n----------------------------------------\n\nTITLE: Checking Inference Service Status\nDESCRIPTION: This command retrieves the status of the created Inference Service, allowing users to ensure it is ready to handle predictions.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/OPENSHIFT_GUIDE.md#2025-04-21_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\noc get inferenceservices sklearn-iris -n kserve-demo\n```\n\n----------------------------------------\n\nTITLE: Installing Iter8 CLI via Homebrew\nDESCRIPTION: Uses Homebrew package manager to install Iter8 command-line interface, specifically version 0.13, enabling advanced Kubernetes release optimization\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/canary-testing/README.md#2025-04-21_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nbrew tap iter8-tools/iter8\nbrrew install iter8@0.13\n```\n\n----------------------------------------\n\nTITLE: Defining JSON Structure for Inference Error Response in JSON\nDESCRIPTION: Defines the JSON structure for an error response in inference requests. The key field 'error' provides a descriptive message about the error, ensuring the caller can diagnose issues effectively.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md#2025-04-21_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"error\": <error message string>\n}\n```\n\n----------------------------------------\n\nTITLE: Logging and Monitoring Model Loading\nDESCRIPTION: This log output captures information during the model loading process, detailing the downloading of models from storage and confirming successful loading.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/multimodelserving/sklearn/README.md#2025-04-21_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n\"{\\\"level\\\":\\\"info\\\",\\\"ts\\\":\\\"2021-01-20T16:24:00.421Z\\\",\\\"caller\\\":\\\"agent/puller.go:129\\\",\\\"msg\\\":\\\"Downloading model from gs://kfserving-examples/models/sklearn/1.0/model\\\"}\\n{\\\"level\\\":\\\"info\\\",\\\"ts\\\":\\\"2021-01-20T16:24:00.421Z\\\",\\\"caller\\\":\\\"agent/downloader.go:47\\\",\\\"msg\\\":\\\"Downloading gs://kfserving-examples/models/sklearn/1.0/model to model dir /mnt/models\\\"}\\n{\\\"level\\\":\\\"info\\\",\\\"ts\\\":\\\"2021-01-20T16:24:09.255Z\\\",\\\"caller\\\":\\\"agent/puller.go:146\\\",\\\"msg\\\":\\\"Successfully loaded model model2-sklearn\\\"}\\n{\\\"level\\\":\\\"info\\\",\\\"ts\\\":\\\"2021-01-20T16:24:09.260Z\\\",\\\"caller\\\":\\\"agent/puller.go:146\\\",\\\"msg\\\":\\\"Successfully loaded model model1-sklearn\\\"}\"\n```\n\n----------------------------------------\n\nTITLE: Creating InferenceService with Canary Model in YAML\nDESCRIPTION: Configuration for updating the InferenceService to include a canary model that receives 20% of traffic while pointing to the new model version.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/canary/README.md#2025-04-21_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"torchserve\"\nspec:\n  predictor:\n    canaryTrafficPercent: 20\n    pytorch:\n      storageUri: \"gs://kfserving-examples/models/torchserve/image_classifier/v2\"\n```\n\n----------------------------------------\n\nTITLE: Displaying Model (Python)\nDESCRIPTION: This snippet displays the trained model. The exact output will depend on the specific model implementation and its parameters.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/sklearn/v1/sklearn-mixedtype-model/sklearn-mixedtype-model.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmodel\n```\n\n----------------------------------------\n\nTITLE: Creating an Istio Virtual Service for GCP IAP\nDESCRIPTION: This snippet demonstrates how to create an Istio virtual service that matches on a path-based route required by IAP. It modifies the virtual-service.yaml file to replace the namespace placeholder and applies the configuration using kubectl.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/gcp-iap/README.md#2025-04-21_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nkubectl apply -f virtual-service.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining Server Ready Request and Response in Protobuf\nDESCRIPTION: Defines the request and response messages for the ServerReady API, indicating whether the server is prepared to take inference requests. The response contains a boolean 'ready' that signals server readiness.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md#2025-04-21_snippet_11\n\nLANGUAGE: protobuf\nCODE:\n```\nmessage ServerReadyRequest {}\n\nmessage ServerReadyResponse\n{\n  bool ready = 1;\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Iter8 CLI\nDESCRIPTION: This snippet outlines the commands to install the Iter8 CLI using Homebrew. It provides a convenient method for macOS users to set up Iter8.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/performance-testing/README.md#2025-04-21_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nbrew tap iter8-tools/iter8\nbrew install iter8@0.13\n```\n\n----------------------------------------\n\nTITLE: Configuring InferenceService with Service Mesh Annotations\nDESCRIPTION: This YAML snippet shows the configuration for an InferenceService resource with the necessary annotations for Service Mesh integration on OpenShift. It includes annotations for Istio sidecar injection and KServe storage initializer UID.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/OPENSHIFT_GUIDE.md#2025-04-21_snippet_18\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  ...\n  annotations:\n    sidecar.istio.io/inject: \"true\"\n    sidecar.istio.io/rewriteAppHTTPProbers: \"true\"\n    serving.knative.openshift.io/enablePassthrough: \"true\"\n    serving.kserve.io/storage-initializer-uid: \"1000860001\" # has to be changed to your namespaces value, see note above\nspec:\n...\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container Locally with Custom CMD\nDESCRIPTION: This Bash command modifies the CMD argument in a Dockerfile to configure the local execution of the Logistic Regression model container. The dependencies include the Python script 'model.py'. Parameters specify predictor host, model name, feature names, label names, and privileged/unprivileged groups.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/aif/germancredit/server/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nCMD [\"python\", \"model.py\",\n     \"-m\", \"aifserver\",\n     \"--predictor_host\", \"german-credit-predictor-default.default.svc.cluster.local\",\n     \"--model_name\", \"german-credit\",\n     \"--feature_names\", \"age\", \"sex\", \"credit_history=Delay\", \"credit_history=None/Paid\", \"credit_history=Other\", \"savings=500+\", \"savings=<500\", \"savings=Unknown/None\", \"employment=1-4 years\", \"employment=4+ years\", \"employment=unemployed\",\n     \"--label_names\", \"credit\",\n     \"--favorable_label\", \"1\",\n     \"--unfavorable_label\", \"2\",\n     \"--privileged_groups\", \"{\\\"age\\\": 1}\",\n     \"--unprivileged_groups\", \"{\\\"age\\\": 0}\"]\n```\n\n----------------------------------------\n\nTITLE: Publishing Docker Image to Docker Hub\nDESCRIPTION: Push the built Docker image to DockerHub using the docker push command. This requires a docker_user_name and image tag, which can be 'latest' for the most recent build version.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/lgbserver/README.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndocker push docker_user_name/lgbserver:latest\n```\n\n----------------------------------------\n\nTITLE: Displaying Input Image in Python\nDESCRIPTION: Visualizes the input image using matplotlib. Displays the original image before sending it for inference. Assumes the use of the numpy library for image manipulation.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/aix/mnist/query_explain.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfig0 = (inputs[:, :, 0] + 0.5) * 255\nf, axarr = plt.subplots(1, 1, figsize=(10, 10))\naxarr.set_title(\"Original Image\")\naxarr.imshow(fig0, cmap=\"gray\")\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Displaying Anchors and Performance Metrics for Explanation with Python\nDESCRIPTION: This snippet visualizes the anchors and performance metrics such as precision and coverage from the explanation object returned for the previous low income prediction. It provides insight into the reliability of the anchors used for interpretation.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/alibi/income/income_explanations.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nshow_anchors(exp[\"data\"][\"anchor\"])\n\nshow_bar([exp[\"data\"][\"precision\"]], [\"\"], \"Precision\")\nshow_bar([exp[\"data\"][\"coverage\"]], [\"\"], \"Coverage\")\n```\n\n----------------------------------------\n\nTITLE: Edit Knative Serving ConfigMap for Prometheus Metrics\nDESCRIPTION: This command edits the knative-serving ConfigMap to enable Prometheus as the backend for request metrics. This is a prerequisite for collecting and visualizing metrics in Prometheus.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/docs/metrics.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n\"Kubectl edit cm -n knative-serving config-observability\"\n```\n\n----------------------------------------\n\nTITLE: Pushing Docker Image to Docker Hub\nDESCRIPTION: This command pushes the Docker image to a Docker Hub repository. Ensure authentication with Docker Hub and replace 'dockeruser' with your Docker username. The input is the local image name, and the output is the image in the Docker Hub repository.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/aif/germancredit/server/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker push dockeruser/aifserver:latest\n```\n\n----------------------------------------\n\nTITLE: Building Python SDK Distribution Packages\nDESCRIPTION: This command uses Poetry to build distribution packages for the KServe Python SDK, creating both wheel and source distribution files that can be uploaded to PyPI.\nSOURCE: https://github.com/kserve/kserve/blob/master/hack/python-sdk/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npoetry build\n```\n\n----------------------------------------\n\nTITLE: Paddle Server CLI Usage - Help Output\nDESCRIPTION: This snippet displays the help output of the Paddle Server command-line interface, detailing optional arguments for configuring the server, including ports and model settings.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/paddleserver/README.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nusage: __main__.py [-h] [--http_port HTTP_PORT] [--grpc_port GRPC_PORT] [--max_buffer_size MAX_BUFFER_SIZE] [--workers WORKERS] --model_dir MODEL_DIR [--model_name MODEL_NAME]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --http_port HTTP_PORT\n                        The HTTP Port listened to by the model server.\n  --grpc_port GRPC_PORT\n                        The GRPC Port listened to by the model server.\n  --max_buffer_size MAX_BUFFER_SIZE\n                        The max buffer size for tornado.\n  --workers WORKERS     The number of works to fork\n  --model_dir MODEL_DIR\n                        A URI pointer to the model directory\n  --model_name MODEL_NAME\n                        The name that the model is served under.\n```\n\n----------------------------------------\n\nTITLE: Defining ModelInfer Request Message in Protocol Buffers\nDESCRIPTION: Protocol buffer definition for model inference request message containing input tensor specifications, model details, and optional parameters. Supports both raw and structured tensor data formats.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md#2025-04-21_snippet_15\n\nLANGUAGE: protobuf\nCODE:\n```\nmessage ModelInferRequest {\n  message InferInputTensor {\n    string name = 1;\n    string datatype = 2;\n    repeated int64 shape = 3;\n    map<string, InferParameter> parameters = 4;\n    InferTensorContents contents = 5;\n  }\n\n  message InferRequestedOutputTensor {\n    string name = 1;\n    map<string, InferParameter> parameters = 2;\n  }\n\n  string model_name = 1;\n  string model_version = 2;\n  string id = 3;\n  map<string, InferParameter> parameters = 4;\n  repeated InferInputTensor inputs = 5;\n  repeated InferRequestedOutputTensor outputs = 6;\n  repeated bytes raw_input_contents = 7;\n}\n```\n\n----------------------------------------\n\nTITLE: Successful InferenceService Creation Output\nDESCRIPTION: Expected output after successfully creating the TorchServe BLOOM model InferenceService in KServe.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v1/bloom/README.md#2025-04-21_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\n$inferenceservice.serving.kserve.io/torchserve-bloom-560m created\n```\n\n----------------------------------------\n\nTITLE: Markdown Model Properties Table\nDESCRIPTION: Table describing the properties of the V1beta1TransformersConfig model, including property names, types, descriptions and optional status.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/docs/V1beta1TransformersConfig.md#2025-04-21_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Name | Type | Description | Notes |\n| ------------ | ------------- | ------------- | ------------- |\n| feast | V1beta1TransformerConfig | | [optional] |\n```\n\n----------------------------------------\n\nTITLE: Making Authenticated Prediction Request with Service Account Token\nDESCRIPTION: cURL command that sends a prediction request to the InferenceService using the JWT token for authentication. The request includes the Host header targeting the specific InferenceService and submits JSON input data.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/istio-dex/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl -v -H \"Host: sklearn-iris.kubeflow-user-example-com.example.com\" -H \"Authorization: Bearer $TOKEN\" -H \"Content-Type: application/json\" -d @./iris-input.json http://localhost:8080/v1/models/sklearn-iris:predict\n```\n\n----------------------------------------\n\nTITLE: Adding Required Dependencies in Gopkg.toml\nDESCRIPTION: Example of how to add required dependencies in Gopkg.toml configuration file, specifically for cases where dependencies need to persist without direct imports. This is particularly useful for projects like TensorFlow when only using the protos.\nSOURCE: https://github.com/kserve/kserve/blob/master/tools/tf2openapi/DEVELOPER_GUIDE.md#2025-04-21_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\nrequired = [\"github.com/foo/bar/..] # this must be a package/subpackage containing Go code\n```\n\n----------------------------------------\n\nTITLE: Show Examples\nDESCRIPTION: This snippet extracts examples from the explanation data and displays them using the `show_examples` function.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/alibi/moviesentiment/movie_review_explanations.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n\"show_examples(exp[\\\"data\\\"], 0, movies)\\nshow_examples(exp[\\\"data\\\"], 0, movies, False)\"\n```\n\n----------------------------------------\n\nTITLE: Pushing Custom XGBoost Server Docker Image\nDESCRIPTION: Command to push the custom-built XGBoost server Docker image to DockerHub repository.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/xgbserver/README.md#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ndocker push docker_user_name/xgbserver:latest\n```\n\n----------------------------------------\n\nTITLE: Loading Training Data (Python)\nDESCRIPTION: This snippet loads the training data from a CSV file named \"train.csv\" into a pandas DataFrame.  It assumes that the CSV file is in the same directory as the script.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/sklearn/v1/sklearn-mixedtype-model/sklearn-mixedtype-model.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntrain_data = pd.read_csv(\"train.csv\")\n```\n\n----------------------------------------\n\nTITLE: Defining Model Ready Request and Response in Protobuf\nDESCRIPTION: Specifies the ModelReady API request and response messages that determine if a certain model is ready to perform inference. Important fields include model name and an optional version for validating readiness of specific model versions.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md#2025-04-21_snippet_12\n\nLANGUAGE: protobuf\nCODE:\n```\nmessage ModelReadyRequest\n{\n  string name = 1;\n  string version = 2;\n}\n\nmessage ModelReadyResponse\n{\n  bool ready = 1;\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Pod Configuration Properties for KServe\nDESCRIPTION: Schema for specifying advanced Kubernetes pod configuration options in KServe, including container specifications, networking settings, and runtime parameters\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/docs/V1beta1PredictorSpec.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n{\n  \"active_deadline_seconds\": int,\n  \"affinity\": V1Affinity,\n  \"annotations\": {\"str\": \"str\"},\n  \"containers\": [V1Container],\n  \"host_network\": bool,\n  \"node_selector\": {\"str\": \"str\"},\n  \"priority\": int\n}\n```\n\n----------------------------------------\n\nTITLE: Publishing KServe SDK to Test PyPI\nDESCRIPTION: This command publishes the built KServe Python SDK packages to the Test PyPI repository for verification before official release.\nSOURCE: https://github.com/kserve/kserve/blob/master/hack/python-sdk/README.md#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npoetry publish -r test-pypi\n```\n\n----------------------------------------\n\nTITLE: Converting Images to Bytes with tobytes.py for KServe V2 Protocol\nDESCRIPTION: Command to run the tobytes.py script which converts an input image (0.png) to bytes format and saves it as a JSON file (0.json). This format is compatible with KServe V2 protocol and can be used with custom handlers without modifications.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v2/bytes_conv/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython tobytes.py 0.png\n```\n\n----------------------------------------\n\nTITLE: Creating TensorFlow Flowers Inference Service (CC=0)\nDESCRIPTION: YAML configuration for deploying TensorFlow flowers model with unlimited container concurrency.\nSOURCE: https://github.com/kserve/kserve/blob/master/test/benchmark/README.md#2025-04-21_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"flowers-sample\"\nspec:\n  predictor:\n    tensorflow:\n      storageUri: \"gs://kfserving-examples/models/tensorflow/flowers\"\n      resources:\n        requests:\n          cpu: \"4\"\n          memory: 2Gi\n        limits:\n          cpu: \"4\"\n          memory: 2Gi\n```\n\n----------------------------------------\n\nTITLE: Allowing Pods to Run as Knative User\nDESCRIPTION: This command allows the KServe deployment pods to run as a specific user, which may be required due to security context constraints in OpenShift environments.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/OPENSHIFT_GUIDE.md#2025-04-21_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\noc adm policy add-scc-to-user anyuid -z default -n kserve-demo\n```\n\n----------------------------------------\n\nTITLE: Pushing ART Model Explainer to DockerHub\nDESCRIPTION: Command to push the built Docker image to DockerHub repository for later use in KServe deployments.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/artexplainer/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker push dockeruser/artserver:latest\n```\n\n----------------------------------------\n\nTITLE: Defining V1alpha1ClusterServingRuntime Python Model\nDESCRIPTION: Defines the schema for a ClusterServingRuntime resource with key properties including API version, kind, metadata, specification, and status\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/docs/V1alpha1ClusterServingRuntime.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass V1alpha1ClusterServingRuntime:\n    def __init__(self,\n        api_version=None,\n        kind=None,\n        metadata=None,\n        spec=None,\n        status=None\n    ):\n```\n\n----------------------------------------\n\nTITLE: Applying Apache License Boilerplate Notice\nDESCRIPTION: Standard boilerplate notice to apply the Apache License to a work, with placeholders for copyright information and instructions for including the license in project files.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/third_party/library/license.txt#2025-04-21_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\nCopyright [yyyy] [name of copyright owner]\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Namespace - Python\nDESCRIPTION: This snippet creates a new Kubernetes namespace named 'cifar10' where all related resources for the CIFAR-10 deployment will be created. Proper permissions are required to execute this command.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/outlier-detection/alibi-detect/cifar10/cifar10_outlier.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n!kubectl create namespace cifar10\n```\n\n----------------------------------------\n\nTITLE: Creating InferenceService YAML for PMML Model\nDESCRIPTION: YAML configuration to define a KServe InferenceService that serves the PMML model from the GCS bucket.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/spark/README.md#2025-04-21_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"spark-pmml\"\nspec:\n  predictor:\n    pmml:\n      storageUri: gs://kfserving-examples/models/sparkpmml\n```\n\n----------------------------------------\n\nTITLE: Port-forwarding Predictor Pod in Kubernetes\nDESCRIPTION: This command port-forwards the predictor pod in Kubernetes, exposing the pod's 8080 port to localhost:8081.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/transformer/torchserve_image_transformer/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkubectl port-forward pods/{name-of-predictor-pod} 8081:8080\n```\n\n----------------------------------------\n\nTITLE: Applying Message Dumper YAML in Python\nDESCRIPTION: This snippet applies the message-dumper.yaml configuration to the Kubernetes cluster using kubectl.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/logger/basic/logger_demo.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n!kubectl apply -f message-dumper.yaml\n```\n\n----------------------------------------\n\nTITLE: Development and Testing Commands\nDESCRIPTION: Shell commands for building, testing, and deploying the qpext extension.\nSOURCE: https://github.com/kserve/kserve/blob/master/qpext/README.md#2025-04-21_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nkubectl patch configmaps -n knative-serving config-deployment --patch-file qpext_image_patch.yaml\nkubectl get configmaps inferenceservice-config -n kserve -oyaml\nkubectl patch configmaps -n knative-serving config-deployment --patch-file qpext_image_patch.yaml\nkubectl get configmaps -n knative-serving config-deployment -oyaml\nkubectl apply -f sklearn.yaml\nkubectl port-forward pods/{pod_name} 9088:9088\ncurl localhost:9088/metrics\n```\n\n----------------------------------------\n\nTITLE: Formatting JSON Input Payload for KServe Inference API\nDESCRIPTION: A JSON structure that follows the KServe inference API format, containing an array of instances where each instance has a data field. This example contains a single text instance for processing, typically used for NLP tasks.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v2/tensor_conv/sample.txt#2025-04-21_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"instances\": [\n    {\n      \"data\": \"Bloomberg has reported on the economy\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Defining JSON Structure for Response Output in JSON\nDESCRIPTION: Defines the JSON schema for individual model output tensors. This includes name, shape, datatype, optional parameters, and data of the tensor. The schema facilitates the consistent representation of output tensors across different models and inference requests.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md#2025-04-21_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\" : $string,\n  \"shape\" : [ $number, ... ],\n  \"datatype\"  : $string,\n  \"parameters\" : $parameters #optional,\n  \"data\" : $tensor_data\n}\n```\n\n----------------------------------------\n\nTITLE: Displaying Broker YAML Configuration\nDESCRIPTION: Uses pygmentize to display the broker.yaml file that defines a Knative channel broker for event routing.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/logger/knative-eventing/logger_demo.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n!pygmentize broker.yaml\n```\n\n----------------------------------------\n\nTITLE: Generating and Applying Local Provisioner Configuration\nDESCRIPTION: Commands to generate the local volume provisioner configuration using Helm and then apply it to the Kubernetes cluster. This enables automatic management of local persistent volumes.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v1/bloom/README.md#2025-04-21_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ncd sig-storage-local-static-provisioner\nhelm template ./helm/provisioner > ./provisioner/deployment/kubernetes/provisioner_generated.yaml\n\nkubectl apply -f ./deployment/kubernetes/provisioner_generated.yaml\n```\n\n----------------------------------------\n\nTITLE: Formatting JSON Request for KServe Inference\nDESCRIPTION: A sample JSON payload structure for sending text data to a KServe inference endpoint. The request wraps a text string within the 'instances' array, with each instance containing data in a 'data' field.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v1/bloom/sample_text.txt#2025-04-21_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"instances\": [\n    {\n      \"data\": \"My dog is cute\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Installing KServe SDK with storage support using Poetry (Option 1)\nDESCRIPTION: Command to install the KServe Python SDK with storage support using Poetry's -E flag for extras.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/README.md#2025-04-21_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\npoetry install -E storage\n```\n\n----------------------------------------\n\nTITLE: Installing Alibi Explainer Dev Dependencies\nDESCRIPTION: This command installs the development dependencies required for working with the Alibi Explainer project. It uses `make` to execute the `dev_install` target, which typically involves installing necessary packages and tools.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/alibi/alibiexplainer/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmake dev_install\n```\n\n----------------------------------------\n\nTITLE: Base64 Encoding and Conditional Classification in InferenceGraph\nDESCRIPTION: This YAML snippet illustrates the 'dog-breed-classification' InferenceGraph, which processes input through a pre-processing step that encodes the image in base64. It routes requests to two model nodes based on conditions determined from classification results, showcasing a layered inference approach.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/docs/V1alpha1InferenceRouter.md#2025-04-21_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nkind: InferenceGraph\nmetadata:\n  name: dog-breed-classification\nspec:\n  nodes:\n    root:\n      routerType: Sequence\n      routes:\n      - service: cat-dog-classifier\n      - nodeName: breed-classifier\n        data: $request\n    breed-classifier:\n      routerType: Switch\n      routes:\n      - service: dog-breed-classifier\n        condition: { .predictions.class == \"dog\" }\n      - service: cat-breed-classifier\n        condition: { .predictions.class == \"cat\" }\n```\n\n----------------------------------------\n\nTITLE: Applying InferenceService Configuration\nDESCRIPTION: A Bash command to apply the multi-model Triton configuration to the Kubernetes cluster using kubectl. This sets the service in motion.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/multimodelserving/triton/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f multi_model_triton.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining Model Revision States in KServe\nDESCRIPTION: Represents the current and target states of a machine learning model revision with two key state properties: active_model_state and target_model_state\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/docs/V1beta1ModelRevisionStates.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass V1beta1ModelRevisionStates:\n    active_model_state: str = ''\n    target_model_state: str = None\n```\n\n----------------------------------------\n\nTITLE: Configuring a Custom LightGBM Docker Image for KServe\nDESCRIPTION: Provides YAML configuration to specify a custom Docker image for LightGBM in KServe. This configuration is necessary if compatibility issues arise between pre-built image versions and custom trained models.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/lightgbm/README.md#2025-04-21_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\n\"lightgbm\": {\n    \"image\": \"<your-dockerhub-id>/kfserving/lgbserver\",\n}\n\n```\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"lightgbm-iris\"\nspec:\n  predictor:\n    lightgbm:\n      storageUri: \"gs://kfserving-examples/models/lightgbm/iris\"\n      runtimeVersion: X.X.X\n\n```\n\n----------------------------------------\n\nTITLE: Running Image Transformer with Local Config\nDESCRIPTION: This command runs the image transformer Python module, specifying the predictor host, number of workers, and local configuration file path.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/transformer/torchserve_image_transformer/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m image_transformer --predictor_host={predictor-url}  --workers=1 --config_path=\"local_config.properties\"\n```\n\n----------------------------------------\n\nTITLE: Deploying Message Dumper with kubectl\nDESCRIPTION: This command applies the 'message-dumper.yaml' file, creating a Knative service in the Kubernetes cluster. Ensure 'kubectl' is installed and configured to interact with your cluster.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/logger/basic/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create -f message-dumper.yaml\n```\n\n----------------------------------------\n\nTITLE: Setting variable to Skip SSL Verification in storage-config Secret - YAML\nDESCRIPTION: This snippet outlines how to set a variable in the storage-config Secret that disables SSL verification, allowing for easier access during testing.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/cabundle/README.md#2025-04-21_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nstringData:\n  localMinIO: |\n    {\n      \"type\": \"s3\",\n      \"access_key_id\": \"THEACCESSKEY\",\n      \"secret_access_key\": \"THEPASSWORD\",\n      \"endpoint_url\": \"https://minio.minio.svc:9000\",\n      \"bucket\": \"modelmesh-example-models\",\n      \"region\": \"us-south\",\n      \"verify_ssl\": \"0\"  # 1 is true, 0 is false  (You can set True/true/False/false too)\n    }\nkind: Secret\nmetadata:\n  name: storage-config\n  namespace: kserve-demo\ntype: Opaque\n```\n\n----------------------------------------\n\nTITLE: Applying KServe InferenceService Configuration\nDESCRIPTION: Command to apply the InferenceService YAML configuration to the Kubernetes cluster.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/canary/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f torchserve.yaml\n```\n\n----------------------------------------\n\nTITLE: KServe Model Deployment Command\nDESCRIPTION: kubectl command to deploy the InferenceService to the cluster.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/sklearn/v2/README.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f ./sklearn.yaml\n```\n\n----------------------------------------\n\nTITLE: Attaching Secret to Service Account - YAML\nDESCRIPTION: This YAML configuration associates the previously created Kubernetes secret 'azcreds' with a Kubernetes service account. The service account is used by KServe to access the Azure Blob Storage securely.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/storage/azure/README.md#2025-04-21_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: sa\nsecrets:\n- name: azcreds\n```\n\n----------------------------------------\n\nTITLE: Tearing Down the Kubernetes Namespace - Python\nDESCRIPTION: This snippet cleans up the resources by deleting the 'cifar10' namespace from the Kubernetes cluster. This is important for resource management and cost control after the experiments are completed.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/outlier-detection/alibi-detect/cifar10/cifar10_outlier.ipynb#2025-04-21_snippet_29\n\nLANGUAGE: python\nCODE:\n```\n!kubectl delete ns cifar10\n```\n\n----------------------------------------\n\nTITLE: Pushing AIF Bias Detector Docker Image to DockerHub\nDESCRIPTION: Command to push the built Docker image to DockerHub repository. The pushed image can then be referenced in KServe InferenceService deployment YAML files.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/aiffairness/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker push dockeruser/aifserver:latest\n```\n\n----------------------------------------\n\nTITLE: Checking Paddle Server Installation - Python Command\nDESCRIPTION: This snippet provides a Python command to check if Paddle Server is installed correctly. It displays help information for the Paddle Server command-line interface.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/paddleserver/README.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n$ python3 -m paddleserver --help\n```\n\n----------------------------------------\n\nTITLE: Converting Text to Tensor Format\nDESCRIPTION: Command to convert a text file into tensor data format, generating an input.json file\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v2/tensor_conv/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython totensor.py sample.txt\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Income Prediction Model with Python\nDESCRIPTION: This snippet installs the required Python packages specified in the requirements.txt file for running the income prediction model and its explanations.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/alibi/income/income_explanations.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\n!pip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Querying prediction requests count in Prometheus\nDESCRIPTION: This Prometheus query retrieves the sum of prediction requests to the sklearn model, specifically `sklearn-iris-predictor-default`, over the last 60 seconds. The query uses the `increase` function to calculate the increase in the counter metric over the specified time range.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/metrics-and-monitoring/README.md#2025-04-21_snippet_5\n\nLANGUAGE: promql\nCODE:\n```\nsum(increase(revision_app_request_latencies_count{service_name=~\"sklearn-iris-predictor-default\"}[60s]))\n```\n\n----------------------------------------\n\nTITLE: Setting GRPC SSL Certificate Environment Variable\nDESCRIPTION: This command sets the environment variable for the GRPC SSL certificate file path.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/transformer/torchserve_image_transformer/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport GRPC_DEFAULT_SSL_ROOTS_FILE_PATH=tls-ca-bundle.pem\n```\n\n----------------------------------------\n\nTITLE: Stopping the Inference Service with kubectl\nDESCRIPTION: This command removes the inference service by deleting its YAML configuration. This is useful for cleaning up resources or when re-deploying services, depending on the lifecycle of the deployment.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/aix/Fetch_20newsgroups/README.md#2025-04-21_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nkubectl delete -f aix-explainer.yaml\n```\n\n----------------------------------------\n\nTITLE: Creating Namespace for Testing\nDESCRIPTION: This command creates a namespace specifically for testing KServe deployments, which may include allowing certain security contexts for pods.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/OPENSHIFT_GUIDE.md#2025-04-21_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\noc create ns kserve-demo\n```\n\n----------------------------------------\n\nTITLE: Defining V1beta1DeployConfig in Python\nDESCRIPTION: Represents a configuration class for deployment settings with an optional default deployment mode parameter\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/docs/V1beta1DeployConfig.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndefault_deployment_mode: str = None\n```\n\n----------------------------------------\n\nTITLE: Fixing CI Failure in Verify Generated Code\nDESCRIPTION: When a new API is added for CRD in a PR, the CI step for verifying generated code may fail due to outdated openapi generated files. To resolve, run make commands to generate code and manifests. Ensure that Makefile and relevant build tools are available.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/COMMON_ISSUES_AND_SOLUTIONS.md#2025-04-21_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nkserve/kserve is out of date. Please run make generate\n```\n\nLANGUAGE: shell\nCODE:\n```\nmake generate\nmake manifests\n```\n\n----------------------------------------\n\nTITLE: Creating TensorFlow Flowers Inference Service (CC=1)\nDESCRIPTION: YAML configuration for deploying TensorFlow flowers model with container concurrency limited to 1.\nSOURCE: https://github.com/kserve/kserve/blob/master/test/benchmark/README.md#2025-04-21_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"flowers-sample\"\nspec:\n  predictor:\n    containerConcurrency: 1\n    tensorflow:\n      storageUri: \"gs://kfserving-examples/models/tensorflow/flowers\"\n      resources:\n        requests:\n          cpu: \"4\"\n          memory: 2Gi\n        limits:\n          cpu: \"4\"\n          memory: 2Gi\n```\n\n----------------------------------------\n\nTITLE: Checking Logs for Outlier Detection Result - Python\nDESCRIPTION: This snippet retrieves logs from the event display service to check for the latest outlier detection predictions. It processes the log data to yield confirmation of whether an outlier was detected.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/outlier-detection/alibi-detect/cifar10/cifar10_outlier.ipynb#2025-04-21_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nres = !kubectl logs -n cifar10 $(kubectl get pod -n cifar10 -l app=hello-display -o jsonpath='{.items[0].metadata.name}')\ndata = []\nfor i in range(0, len(res)):\n    if res[i] == \"Data,\":\n        data.append(res[i + 1])\nj = json.loads(json.loads(data[-1]))\nprint(\"Outlier\", j[\"data\"][\"is_outlier\"] == [1])\n```\n\n----------------------------------------\n\nTITLE: Verifying Alibi Explainer installation\nDESCRIPTION: This command runs the Alibi Explainer's main script, specifying the explainer type (AnchorTabular, AnchorText, or AnchorImages).  It allows you to verify if the installation was successful by checking for the log output.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/alibi/alibiexplainer/README.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n$ python alibiexplainer/__main__.py {AnchorTabular|AnchorText|AnchorImages}  \n...\n2024-10-17 15:48:59.751 51916 kserve INFO [explainer.py:__init__():54] Predict URL set to None\n```\n\n----------------------------------------\n\nTITLE: Installing Alibi Explainer Dev Dependencies using make\nDESCRIPTION: This command installs development dependencies required to work on alibiexplainer using make.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/alibi/alibiexplainer/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmake dev_install\n```\n\n----------------------------------------\n\nTITLE: Defining Python Dependencies for KServe Project\nDESCRIPTION: Lists required Python packages including KServe core library, scikit-learn for machine learning, AI Fairness 360 (aif360) for bias detection and mitigation, and numba for performance optimization.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/aif/germancredit/server/requirements.txt#2025-04-21_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nkserve>=0.13.0\nscikit-learn\naif360\nnumba\n```\n\n----------------------------------------\n\nTITLE: Deploying Torchserve Logger to Kubernetes\nDESCRIPTION: Applies the Torchserve logger YAML configuration to the kfserving-test namespace\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/logger/logger_demo.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n!kubectl apply -f torchserve-logger.yaml -n kfserving-test\n```\n\n----------------------------------------\n\nTITLE: Cleaning Up SKLearn Model Resources\nDESCRIPTION: Deletes the SKLearn inference service with logging configuration.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/logger/knative-eventing/logger_demo.ipynb#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n!kubectl delete -f sklearn-logging.yaml\n```\n\n----------------------------------------\n\nTITLE: Retrieving Service Hostname\nDESCRIPTION: Extracts the hostname for the Torchserve inference service\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/logger/logger_demo.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nSERVICE_HOSTNAMES = !(kubectl get inferenceservice torchserve-custom -n kfserving-test -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\nSERVICE_HOSTNAME = SERVICE_HOSTNAMES[0]\nprint(SERVICE_HOSTNAME)\n```\n\n----------------------------------------\n\nTITLE: Checking InferenceService status\nDESCRIPTION: Shell command to watch the status of the deployed InferenceService until it becomes ready.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/triton/fastertransformer/README.md#2025-04-21_snippet_11\n\nLANGUAGE: sh\nCODE:\n```\nkubectl get isvc -w\n```\n\n----------------------------------------\n\nTITLE: Displaying Trigger YAML Configuration\nDESCRIPTION: Shows the trigger.yaml file content that defines how events will be routed to the message logger service.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/logger/knative-eventing/logger_demo.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n!pygmentize trigger.yaml\n```\n\n----------------------------------------\n\nTITLE: Default Metrics Aggregator Configuration\nDESCRIPTION: YAML configuration showing default metrics aggregator settings in the configmap.\nSOURCE: https://github.com/kserve/kserve/blob/master/qpext/README.md#2025-04-21_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nmetricsAggregator: |-\n  {\n    \"enableMetricAggregation\": \"false\",\n    \"enablePrometheusScraping\" : \"false\"\n  }\n```\n\n----------------------------------------\n\nTITLE: Defining KnativeURL Properties in Markdown\nDESCRIPTION: This code snippet defines the properties of the KnativeURL model using a markdown table. It lists each property, its type, and a description of its purpose.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/docs/KnativeURL.md#2025-04-21_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n## Properties\nName | Type | Description | Notes\n------------ | ------------- | ------------- | -------------\n**force_query** | **bool** | encoded path hint (see EscapedPath method) | \n**fragment** | **str** | encoded query values, without &#39;?&#39; | \n**host** | **str** | username and password information | \n**opaque** | **str** |  | \n**path** | **str** | host or host:port | \n**raw_path** | **str** | path (relative paths may omit leading slash) | \n**raw_query** | **str** | append a query (&#39;?&#39;) even if RawQuery is empty | \n**scheme** | **str** |  | \n**user** | [**NetUrlUserinfo**](NetUrlUserinfo.md) | encoded opaque data | \n```\n\n----------------------------------------\n\nTITLE: Pod Configuration Properties for KServe\nDESCRIPTION: Comprehensive listing of Kubernetes pod configuration properties specific to KServe deployments. Includes settings for pod lifecycle, resource management, scaling, security, and framework-specific configurations for ML model serving.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/docs/V1beta1PredictorSpec.md#2025-04-21_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n**readiness_gates** | list[V1PodReadinessGate] | Pod readiness evaluation gates\\n**resource_claims** | list[V1PodResourceClaim] | Resource allocation and reservation settings\\n**resources** | V1ResourceRequirements | Resource requirements configuration\\n**restart_policy** | str | Container restart policy (Always/OnFailure/Never)\\n**runtime_class_name** | str | RuntimeClass object reference\\n**scale_metric** | str | Autoscaler metric type (concurrency/rps/cpu/memory)\\n**scale_metric_type** | str | Metric type (Utilization/AverageValue)\\n**scale_target** | int | Target value for autoscaler metric\\n**scheduler_name** | str | Pod scheduler specification\\n**scheduling_gates** | list[V1PodSchedulingGate] | Pod scheduling control gates\\n**security_context** | V1PodSecurityContext | Pod security context settings\\n**service_account** | str | Deprecated service account name\\n**service_account_name** | str | Service account for pod execution\\n**set_hostname_as_fqdn** | bool | FQDN hostname configuration\\n**share_process_namespace** | bool | Process namespace sharing settings\\n**sklearn** | V1beta1SKLearnSpec | SKLearn model serving configuration\\n**subdomain** | str | Pod hostname subdomain\\n**tensorflow** | V1beta1TFServingSpec | TensorFlow serving configuration\\n**termination_grace_period_seconds** | int | Pod termination grace period\\n**timeout** | int | Request timeout configuration\\n**tolerations** | list[V1Toleration] | Pod scheduling tolerations\\n**topology_spread_constraints** | list[V1TopologySpreadConstraint] | Pod topology spread settings\\n**triton** | V1beta1TritonSpec | Triton serving configuration\\n**volumes** | list[V1Volume] | Pod volume configurations\\n**worker_spec** | V1beta1WorkerSpec | Worker specification settings\\n**xgboost** | V1beta1XGBoostSpec | XGBoost model serving configuration\n```\n\n----------------------------------------\n\nTITLE: V1beta1MultiNodeConfig Properties\nDESCRIPTION: Defines the properties of the `V1beta1MultiNodeConfig` object, specifically the `custom_gpu_resource_type_list`. This list specifies the custom GPU resource types permitted in the ServingRuntime and InferenceService configurations. The property is optional.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/docs/V1beta1MultiNodeConfig.md#2025-04-21_snippet_0\n\n\n\n----------------------------------------\n\nTITLE: Cloning Local Storage Provisioner Repository\nDESCRIPTION: Git command to clone the Kubernetes SIG Storage local static provisioner repository, which is used to manage local persistent volumes in Kubernetes.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v1/bloom/README.md#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n$ git clone https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner.git\n```\n\n----------------------------------------\n\nTITLE: Creating a Masked Image for Outlier Detection - Python\nDESCRIPTION: This snippet generates a masked version of an input image, which is used to test if the outlier detector can identify modified images as anomalies.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/outlier-detection/alibi-detect/cifar10/cifar10_outlier.ipynb#2025-04-21_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nnp.random.seed(0)\nX_mask, mask = apply_mask(\n    X.reshape(1, 32, 32, 3),\n    mask_size=(10, 10),\n    n_masks=1,\n    channels=[0, 1, 2],\n    mask_type=\"normal\",\n    noise_distr=(0, 1),\n    clip_rng=(0, 1),\n)\n\n```\n\n----------------------------------------\n\nTITLE: Using Tf2OpenAPI CLI Tool in Bash\nDESCRIPTION: Command-line usage instructions for the Tf2OpenAPI tool. It shows required and optional flags for generating OpenAPI specifications from TensorFlow SavedModel files.\nSOURCE: https://github.com/kserve/kserve/blob/master/tools/tf2openapi/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nUsage:\n  tf2openapi [flags]\n\nRequired Flags:\n  -m, --model_base_path string           Absolute path of SavedModel file\n\nFlags:\n  -h, --help                     help for tf2openapi\n  -t, --metagraph_tags strings   All tags identifying desired MetaGraph\n  -m, --model_base_path string   Absolute path of SavedModel file\n  -n, --name string              Name of model (default \"model\")\n  -o, --output_file string       Absolute path of file to write OpenAPI spec to\n  -s, --signature_def string     Serving Signature Def Key\n  -v, --version string           Model version (default \"1\")\n```\n\n----------------------------------------\n\nTITLE: PMML Server Usage Command\nDESCRIPTION: Command line interface usage showing required and optional arguments for running the PMML server\nSOURCE: https://github.com/kserve/kserve/blob/master/python/pmmlserver/README.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npython3 -m pmmlserver\nusage: __main__.py [-h] [--http_port HTTP_PORT] [--grpc_port GRPC_PORT] [--max_buffer_size MAX_BUFFER_SIZE] [--workers WORKERS] --model_dir MODEL_DIR [--model_name MODEL_NAME]\n__main__.py: error: the following arguments are required: --model_dir\n```\n\n----------------------------------------\n\nTITLE: Deleting Torchserve Logger\nDESCRIPTION: Removes the Torchserve logger deployment from the kfserving-test namespace\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/logger/logger_demo.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n!kubectl delete -f torchserve-logger.yaml -n kfserving-test\n```\n\n----------------------------------------\n\nTITLE: Setting Localhost IP for Kind or Minikube - Python\nDESCRIPTION: This snippet sets the CLUSTER_IP variable to 'localhost:8080', which is used when accessing services within Kind or Minikube clusters. It is crucial to use port forwarding for connections.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/outlier-detection/alibi-detect/cifar10/cifar10_outlier.ipynb#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nCLUSTER_IP = \"localhost:8080\"\n```\n\n----------------------------------------\n\nTITLE: Removing Message Dumper Service\nDESCRIPTION: Deletes the message dumper Knative service that was capturing CloudEvents.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/logger/knative-eventing/logger_demo.ipynb#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n!kubectl delete -f message-dumper.yaml\n```\n\n----------------------------------------\n\nTITLE: Querying Explanation with Custom Parameters\nDESCRIPTION: This Python command sends a request to the explanation endpoint, including a JSON string to customize the explainer's parameters. Parameters such as `top_labels`, `segmentation_alg`, `num_samples`, `positive_only`, and `min_weight` can be modified to influence the explanation process.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/aix/mnist/README.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n`python query_explain.py http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/${MODEL_NAME}:explain ${SERVICE_HOSTNAME} 100 '{\"top_labels\":\"10\"}'`\n```\n\n----------------------------------------\n\nTITLE: Waiting for Pods to be Ready\nDESCRIPTION: Waits for all pods defined in the Kubernetes deployment to be in a ready state, ensuring that the deployment is successful before proceeding.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/alibi/cifar10/cifar10_explanations.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n!kubectl wait --for condition=ready --timeout=600s pods --all -n default\n```\n\n----------------------------------------\n\nTITLE: Displaying SKLearn Logging YAML in Python\nDESCRIPTION: This snippet uses the pygmentize command to display the contents of the sklearn-logging.yaml file.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/logger/basic/logger_demo.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n!pygmentize sklearn-logging.yaml\n```\n\n----------------------------------------\n\nTITLE: Properties Table for KServe Pod Specification\nDESCRIPTION: Markdown table defining the properties, types, and descriptions for KServe pod specification fields. Includes detailed information about pod configuration options, scheduling parameters, and container settings.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/docs/V1beta1TransformerSpec.md#2025-04-21_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\nName | Type | Description | Notes\n------------ | ------------- | ------------- | -------------\n**active_deadline_seconds** | **int** | Optional duration in seconds the pod may be active on the node relative to StartTime before the system will actively try to mark it failed and kill associated containers. Value must be a positive integer. | [optional] \n**affinity** | [**V1Affinity**](https://github.com/kubernetes-client/python/blob/master/kubernetes/docs/V1Affinity.md) |  | [optional] \n**annotations** | **dict(str, str)** | Annotations that will be added to the component pod. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/ | [optional] \n**auto_scaling** | [**V1beta1AutoScalingSpec**](V1beta1AutoScalingSpec.md) |  | [optional] \n**automount_service_account_token** | **bool** | AutomountServiceAccountToken indicates whether a service account token should be automatically mounted. | [optional]\n```\n\n----------------------------------------\n\nTITLE: Deleting SKLearn Logging Configuration in Python\nDESCRIPTION: This snippet removes the sklearn-logging configuration from the Kubernetes cluster.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/logger/basic/logger_demo.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n!kubectl delete -f sklearn-logging.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining V1beta1ResourceMetricSource in Python\nDESCRIPTION: Class property definition for a Kubernetes resource metric source, specifying a resource name and target metric configuration\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/docs/V1beta1ResourceMetricSource.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass V1beta1ResourceMetricSource:\n    def __init__(self, name='', target=None):\n        self.name = name\n        self.target = target\n```\n\n----------------------------------------\n\nTITLE: Defining TrainedModelSpec Properties in Python\nDESCRIPTION: Specifies the core properties for a trained model, including the parent inference service and model specification\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/docs/V1alpha1TrainedModelSpec.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ninference_service: str = ''\nmodel: V1alpha1ModelSpec\n```\n\n----------------------------------------\n\nTITLE: Installing MLServer and XGBoost Runtime\nDESCRIPTION: Bash command to install MLServer and its XGBoost runtime via pip for local testing of XGBoost models before deployment.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/xgboost/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install mlserver mlserver-xgboost\n```\n\n----------------------------------------\n\nTITLE: Configuring Message Dumper Service Using KNative and YAML\nDESCRIPTION: Defines a Knative service named 'message-dumper' to print received CloudEvents. This service utilizes the 'event_display' container image from the Knative releases. Intended to log cloud events for debugging purposes.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/logger/README.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: message-dumper\nspec:\n  template:\n    spec:\n      containers:\n      - image: gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display\n```\n\n----------------------------------------\n\nTITLE: List Knative Revisions\nDESCRIPTION: Lists Knative revisions in a specified namespace using the kn CLI tool. This command displays the traffic distribution, tags, and status of each revision, allowing verification of the canary rollout configuration.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/docs/canary.md#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nkn revision list -n <namespace>\n```\n\n----------------------------------------\n\nTITLE: Structuring Input Data for KServe Model Inference in JSON\nDESCRIPTION: This JSON structure defines the input format for making inference requests to a KServe model. It contains an 'instances' array with a single object, which has a 'data' field containing the text to be processed by the model.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v1/bert/sample_text.txt#2025-04-21_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"instances\": [\n    {\n      \"data\": \"Bloomberg has reported on the economy\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Pod Hostname and FQDN Configuration\nDESCRIPTION: Controls how the pod's hostname is configured, with option to set it as Fully Qualified Domain Name (FQDN) in Linux and Windows containers.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/docs/V1beta1CustomExplainer.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nset_hostname_as_fqdn: bool = False\n```\n\n----------------------------------------\n\nTITLE: Python Copyright Header Template\nDESCRIPTION: Standard copyright header template to be included at the top of Nucleic project source files. Indicates collective ownership by the Nucleic Development Team and specifies the Modified BSD License.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/third_party/library/license.txt#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n#------------------------------------------------------------------------------\n# Copyright (c) 2013, Nucleic Development Team.\n#\n# Distributed under the terms of the Modified BSD License.\n#\n```\n\n----------------------------------------\n\nTITLE: Generating KServe Install Manifest\nDESCRIPTION: This command generates the install manifest for a specific KServe version using the generate-install.sh script. It's part of the release process to create installation files for the new version.\nSOURCE: https://github.com/kserve/kserve/blob/master/release/RELEASE_PROCESS_v2.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./hack/generate-install.sh $VERSION\n```\n\n----------------------------------------\n\nTITLE: Applying GPU InferenceService Configuration - KServe - Shell\nDESCRIPTION: Applies the GPU-specific InferenceService configuration using kubectl. Assumes the configuration file is named autoscale_gpu.yaml and is present in the current directory.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/autoscaling/README.md#2025-04-21_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nkubectl apply -f autoscale_gpu.yaml\n```\n\n----------------------------------------\n\nTITLE: Expected Response from Switch Node\nDESCRIPTION: The expected response when testing the Switch Node, showing the result from the service 'single-3' that matched the pattern condition in the input data.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/graph/README.md#2025-04-21_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n{\"source\":\"single-3\",\"instances\":[{\"name\":\"blue\",\"intval\":0,\"strval\":\"kserve\"},{\"name\":\"green\",\"intval\":1,\"strval\":\"1red-server1\"}]}\n```\n\n----------------------------------------\n\nTITLE: Creating Event Display Deployment - Python\nDESCRIPTION: This snippet writes a YAML configuration for deploying an application named 'hello-display' that will display events as they are processed. This application is part of the eventing architecture.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/outlier-detection/alibi-detect/cifar10/cifar10_outlier.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n%%writefile event-display.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-display\n  namespace: cifar10\nspec:\n  replicas: 1\n  selector:\n    matchLabels: &labels\n      app: hello-display\n  template:\n    metadata:\n      labels: *labels\n    spec:\n      containers:\n        - name: event-display\n          image: gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display\n\n---\n\nkind: Service\napiVersion: v1\nmetadata:\n  name: hello-display\n  namespace: cifar10\nspec:\n  selector:\n    app: hello-display\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 8080\n```\n\n----------------------------------------\n\nTITLE: Running Static Type Checks for XGBoost Server\nDESCRIPTION: Command to perform static type checking on the XGBoost server code using mypy. An empty result indicates success.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/xgbserver/README.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmypy --ignore-missing-imports xgbserver\n```\n\n----------------------------------------\n\nTITLE: Applying Tensorflow CRD Using kubectl in Bash\nDESCRIPTION: Command to apply a Kubernetes Custom Resource Definition using kubectl for the tensorflow model deployment.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/storage/uri/README.md#2025-04-21_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f tensorflow_uri.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining V1beta1WorkerSpec Struct in Go\nDESCRIPTION: Defines the Go struct for V1beta1WorkerSpec which represents the specification for worker components in KServe. This likely contains configuration options for worker instances in a KServe deployment.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/docs/V1beta1WorkerSpec.md#2025-04-21_snippet_0\n\nLANGUAGE: go\nCODE:\n```\ntype V1beta1WorkerSpec struct\n```\n\n----------------------------------------\n\nTITLE: Cleaning Up KServe and Kourier\nDESCRIPTION: These commands uninstall KServe and Kourier from the OpenShift cluster along with cleaning up associated resources, ensuring no remnants are left behind after testing.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/OPENSHIFT_GUIDE.md#2025-04-21_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\noc delete -f \"https://github.com/kserve/kserve/releases/download/${KSERVE_VERSION}/kserve.yaml\"\\\noc delete -f \"https://github.com/kserve/kserve/releases/download/${KSERVE_VERSION}/kserve-cluster-resources.yaml\"\\\noc delete -f openshift/serverless/knativeserving-kourier.yaml\n```\n\n----------------------------------------\n\nTITLE: Publishing PMML Server Docker Image\nDESCRIPTION: Command to push the built Docker image to DockerHub repository\nSOURCE: https://github.com/kserve/kserve/blob/master/python/pmmlserver/README.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndocker push docker_user_name/pmmlserver:latest\n```\n\n----------------------------------------\n\nTITLE: Extracting drift result from Event Display logs (corrupted data)\nDESCRIPTION: This snippet extracts the last drift result from the `hello-display` logs after sending the corrupted data.  It parses the logs, extracts the data section, and then parses the JSON data to determine if drift was detected.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/drift-detection/alibi-detect/cifar10/cifar10_drift.ipynb#2025-04-21_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n\"res = !kubectl logs -n cifar10 $(kubectl get pod -n cifar10 -l app=hello-display -o jsonpath='{.items[0].metadata.name}')\ndata = []\nfor i in range(0, len(res)):\n    if res[i] == \\\"Data,\\\":\n        data.append(res[i + 1])\nj = json.loads(json.loads(data[-1]))\nprint(\\\"Drift\\\", j[\\\"data\\\"][\\\"is_drift\\\"] == 1)\"\n```\n\n----------------------------------------\n\nTITLE: Deleting InferenceService using kubectl\nDESCRIPTION: This command deletes the InferenceService deployed using the aix-explainer.yaml file, cleaning up the resources allocated to the AIX explainer service.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/aix/mnist/README.md#2025-04-21_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\n`kubectl delete -f aix-explainer.yaml`\n```\n\n----------------------------------------\n\nTITLE: Defining KServe Python Package Dependencies\nDESCRIPTION: Specifies the required Python package dependencies for a KServe project. It pins KServe to exact version 0.14.1 and specifies protobuf with compatible version ~=3.19.0, which allows patch version updates but not minor or major version changes.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/kfp/requirements.txt#2025-04-21_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nkserve==0.14.1\nprotobuf~=3.19.0\n```\n\n----------------------------------------\n\nTITLE: Displaying Message Dumper YAML Configuration\nDESCRIPTION: Uses the pygmentize command to display the content of the message-dumper.yaml file, which defines a Knative service for capturing CloudEvents.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/logger/knative-eventing/logger_demo.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pygmentize message-dumper.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing gRPC Python Dependencies\nDESCRIPTION: pip command to install required Python packages for gRPC client.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v1/README.md#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npip install -U grpcio protobuf grpcio-tools\n```\n\n----------------------------------------\n\nTITLE: Apache 2.0 License Header for KServe\nDESCRIPTION: Standard copyright and Apache 2.0 license header used in KServe project files. Establishes copyright ownership by KServe Authors and specifies the Apache 2.0 license terms.\nSOURCE: https://github.com/kserve/kserve/blob/master/hack/boilerplate.python.txt#2025-04-21_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\n# Copyright 2023 The KServe Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n```\n\n----------------------------------------\n\nTITLE: Running BERT Tokenizer Command\nDESCRIPTION: Command line invocation to tokenize text input using the BERT tokenizer script. Takes an input text string as a parameter and generates tokenized output.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v2/bert/sequence_classification/tensor/README.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npython bert_tokenizer.py --input_text \"this year business is good\"\n```\n\n----------------------------------------\n\nTITLE: Retrieving InferenceService URL\nDESCRIPTION: Command to get the URL of the deployed InferenceService using kubectl.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/art/mnist/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get inferenceservice\n```\n\n----------------------------------------\n\nTITLE: Creating Knative Broker Configuration - Python\nDESCRIPTION: This snippet writes a YAML configuration for a Knative Broker named 'default' in the 'cifar10' namespace. The configuration is essential for event handling in Knative.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/outlier-detection/alibi-detect/cifar10/cifar10_outlier.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n%%writefile broker.yaml\napiVersion: eventing.knative.dev/v1\nkind: Broker\nmetadata:\n name: default\n namespace: cifar10\n```\n\n----------------------------------------\n\nTITLE: Defining Inference Service List Model Properties\nDESCRIPTION: Defines the schema for a list of Inference Services with optional API version, items, kind, and metadata properties\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/docs/V1beta1InferenceServiceList.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass V1beta1InferenceServiceList:\n    def __init__(self, \n        api_version=None, \n        items=None, \n        kind=None, \n        metadata=None\n    ):\n```\n\n----------------------------------------\n\nTITLE: Building Paddle Server Docker Image - Docker Command\nDESCRIPTION: This snippet describes how to build a Docker image for Paddle Server. The command should be run from the python directory and points to a specific Dockerfile.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/paddleserver/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t ${docker_user_name}/paddleserver -f paddle.Dockerfile .\n```\n\n----------------------------------------\n\nTITLE: Defining KnativeCondition Properties in Python\nDESCRIPTION: Structured definition of properties for tracking Knative resource conditions with optional fields for transition details, messages, and status\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/docs/KnativeCondition.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n{\n    \"last_transition_time\": KnativeVolatileTime,\n    \"message\": str,\n    \"reason\": str,\n    \"severity\": str,\n    \"status\": str,\n    \"type\": str\n}\n```\n\n----------------------------------------\n\nTITLE: Tearing Down Resources\nDESCRIPTION: This snippet deletes the `cifar10` namespace, removing all resources created in this example. This cleans up the Kubernetes cluster after the demonstration.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/drift-detection/alibi-detect/cifar10/cifar10_drift.ipynb#2025-04-21_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n\"!kubectl delete ns cifar10\"\n```\n\n----------------------------------------\n\nTITLE: Running PMML Server Tests\nDESCRIPTION: Command to execute the test suite for the PMML server implementation\nSOURCE: https://github.com/kserve/kserve/blob/master/python/pmmlserver/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmake test\n```\n\n----------------------------------------\n\nTITLE: Installing Kourier Ingress Controller\nDESCRIPTION: This command installs the Kourier ingress controller in OpenShift, which is necessary for handling network routing for serverless model inference deployments. Ensure that the appropriate yaml file is provided for deployment.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/OPENSHIFT_GUIDE.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\noc apply -f openshift/serverless/knativeserving-kourier.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing Development Dependencies for XGBoost Server\nDESCRIPTION: Command to install the development dependencies for the XGBoost server project.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/xgbserver/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmake dev_install\n```\n\n----------------------------------------\n\nTITLE: Deleting the Kubernetes Deployment\nDESCRIPTION: This Bash command removes the deployment of the InferenceService defined in the bentoml.yaml file from the Kubernetes cluster, cleaning up resources.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/bentoml/README.md#2025-04-21_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete -f bentoml.yaml\n```\n\n----------------------------------------\n\nTITLE: Downloading Test Image\nDESCRIPTION: Downloads a sample MNIST test image for model prediction\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/logger/logger_demo.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n!wget https://raw.githubusercontent.com/pytorch/serve/master/examples/image_classifier/mnist/test_data/1.png\n```\n\n----------------------------------------\n\nTITLE: Configuring Queue Sidecar Image in Knative\nDESCRIPTION: YAML configuration to patch the queue-sidecar-image in the Knative deployment config.\nSOURCE: https://github.com/kserve/kserve/blob/master/qpext/README.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ndata:\n  queue-sidecar-image: kserve/qpext:latest\n```\n\n----------------------------------------\n\nTITLE: Pushing Custom Scikit-Learn Server Docker Image\nDESCRIPTION: Command to push the custom Scikit-Learn server Docker image to DockerHub. This makes the image available for deployment in KServe environments.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/sklearnserver/README.md#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ndocker push docker_user_name/sklearnserver:latest\n```\n\n----------------------------------------\n\nTITLE: Cleaning Up Iter8 Experiment and InferenceService\nDESCRIPTION: This snippet deletes the Iter8 experiment and the KServe InferenceService named 'sklearn-irisv2', ensuring that no resources are left running after testing.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/performance-testing/README.md#2025-04-21_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\niter8 k delete\nkubectl delete isvc sklearn-irisv2\n```\n\n----------------------------------------\n\nTITLE: Displaying Torchserve Logger YAML Configuration\nDESCRIPTION: Command to display the contents of the Torchserve logger YAML file\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/logger/logger_demo.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n!pygmentize torchserve-logger.yaml\n```\n\n----------------------------------------\n\nTITLE: Publishing KServe SDK to PyPI\nDESCRIPTION: This command publishes the built KServe Python SDK packages to the official PyPI repository, making them available for public installation via pip.\nSOURCE: https://github.com/kserve/kserve/blob/master/hack/python-sdk/README.md#2025-04-21_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npoetry publish\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for KServe\nDESCRIPTION: This snippet lists the required Python packages and their minimum versions for the KServe project. It includes Alibi for AI explainability, scikit-learn for machine learning algorithms, dill for object serialization, and Plotly for interactive visualizations.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/alibi/income/requirements.txt#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nalibi>=0.9.1\nscikit-learn>=1.0.0\ndill>=0.3.0\nplotly>=4.2.1\n```\n\n----------------------------------------\n\nTITLE: Data Chunk Example 1 - Initial Data\nDESCRIPTION: This JSON snippet represents the initial chunk in a chat completion stream. It provides the basic metadata about the completion, including the ID, object type, creation timestamp, model name, and system fingerprint, and begins the choices array which includes role of assistant with no content yet.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/test/fixtures/openai/chat_completion_chunk_stream.txt#2025-04-21_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\n\"data: {\\\"id\\\":\\\"chatcmpl-9GsTs1IiRnXYuM1CeKqWeiU1B7E4a\\\",\\\"object\\\":\\\"chat.completion.chunk\\\",\\\"created\\\":1713809700,\\\"model\\\":\\\"gpt-3.5-turbo-0125\\\",\\\"system_fingerprint\\\":\\\"fp_c2295e73ad\\\",\\\"choices\\\":[{\\\"index\\\":0,\\\"delta\\\":{\\\"role\\\":\\\"assistant\\\",\\\"content\\\":\\\"\\\"},\\\"logprobs\\\":{\\\"content\\\":[]},\\\"finish_reason\\\":null}]}\"\n```\n\n----------------------------------------\n\nTITLE: Service Account Configuration for Kubernetes Pods\nDESCRIPTION: Specifies the service account to be used for running a pod, providing authentication and authorization mechanism for pod interactions.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/docs/V1beta1CustomExplainer.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nservice_account_name: str = None\n```\n\n----------------------------------------\n\nTITLE: Installing Poetry for Python Package Management\nDESCRIPTION: This command installs Poetry, a tool for dependency management and packaging in Python, which is required for building and publishing the KServe Python SDK.\nSOURCE: https://github.com/kserve/kserve/blob/master/hack/python-sdk/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install poetry\n```\n\n----------------------------------------\n\nTITLE: Editing Inference Service Configmap\nDESCRIPTION: This command edits the inferenceservice-config ConfigMap to include a specific flag required for KServe to operate correctly under Kourier networking conditions.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/OPENSHIFT_GUIDE.md#2025-04-21_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\noc edit configmap/inferenceservice-config --namespace kserve\\\n# Add the flag \"disableIstioVirtualHost\": true under the ingress section\\\ningress : |- {\\\n    \"disableIstioVirtualHost\": true\\\n}\n```\n\n----------------------------------------\n\nTITLE: Installing KServe\nDESCRIPTION: This command sets the KServe version and applies its deployment configuration. Make sure to replace the version with the desired one before executing the command.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/OPENSHIFT_GUIDE.md#2025-04-21_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nKSERVE_VERSION=v0.14.1\\\noc apply --server-side -f \"https://github.com/kserve/kserve/releases/download/${KSERVE_VERSION}/kserve.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Displaying Message Dumper YAML in Python\nDESCRIPTION: This snippet uses the pygmentize command to display the contents of the message-dumper.yaml file.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/logger/basic/logger_demo.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pygmentize message-dumper.yaml\n```\n\n----------------------------------------\n\nTITLE: Data Chunk Example 5 - Adding \" artificial\"\nDESCRIPTION: This JSON snippet adds the content \" artificial\" to the response. It includes the token \" artificial\" along with its log probability and top log probabilities.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/test/fixtures/openai/chat_completion_chunk_stream.txt#2025-04-21_snippet_4\n\nLANGUAGE: JSON\nCODE:\n```\n\"data: {\\\"id\\\":\\\"chatcmpl-9GsTs1IiRnXYuM1CeKqWeiU1B7E4a\\\",\\\"object\\\":\\\"chat.completion.chunk\\\",\\\"created\\\":1713809700,\\\"model\\\":\\\"gpt-3.5-turbo-0125\\\",\\\"system_fingerprint\\\":\\\"fp_c2295e73ad\\\",\\\"choices\\\":[{\\\"index\\\":0,\\\"delta\\\":{\\\"content\\\":\\\" artificial\\\"},\\\"logprobs\\\":{\\\"content\\\":[{\\\"token\\\":\\\" artificial\\\",\\\"logprob\\\":-0.16576119,\\\"bytes\\\":[32,97,114,116,105,102,105,99,105,97,108],\\\"top_logprobs\\\":[{\\\"token\\\":\\\" artificial\\\",\\\"logprob\\\":-0.16576119,\\\"bytes\\\":[32,97,114,116,105,102,105,99,105,97,108]}]}]},\\\"finish_reason\\\":null}]}\"\n```\n\n----------------------------------------\n\nTITLE: Executing Version Bump Script for KServe Release\nDESCRIPTION: This command uses the prepare-for-release.sh script to automate version updates across multiple files in the KServe project. It updates the version from a previous release candidate to a new one.\nSOURCE: https://github.com/kserve/kserve/blob/master/release/RELEASE_PROCESS_v2.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmake bump-version NEW_VERSION=0.14.0-rc2 PRIOR_VERSION=0.14.0-rc1\n```\n\n----------------------------------------\n\nTITLE: Retrieving Cluster IP Address\nDESCRIPTION: Extracts the hostname of the Istio ingress gateway service\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/logger/logger_demo.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nCLUSTER_IPS = !(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')\nCLUSTER_IP = CLUSTER_IPS[0]\nprint(CLUSTER_IP)\n```\n\n----------------------------------------\n\nTITLE: Implementing GPL License Header Template in Source Files\nDESCRIPTION: A template for adding the GPL license header to source code files. This snippet shows how to format the copyright notice and license information that should be included at the beginning of each source file in GPL-licensed projects.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/third_party/library/license.txt#2025-04-21_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n    {description}\n    Copyright (C) {year}  {fullname}\n\n    This program is free software; you can redistribute it and/or modify\n    it under the terms of the GNU General Public License as published by\n    the Free Software Foundation; either version 2 of the License, or\n    (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU General Public License for more details.\n\n    You should have received a copy of the GNU General Public License along\n    with this program; if not, write to the Free Software Foundation, Inc.,\n    51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n```\n\n----------------------------------------\n\nTITLE: Installing Python dependencies\nDESCRIPTION: This snippet installs the required Python packages listed in the `requirements_notebook.txt` file.  It uses `pip`, the Python package installer, to install the dependencies. This is a prerequisite for running the rest of the notebook.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/drift-detection/alibi-detect/cifar10/cifar10_drift.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"!pip install -r requirements_notebook.txt\"\n```\n\n----------------------------------------\n\nTITLE: Waiting for InferenceService to be Ready\nDESCRIPTION: This command waits until the InferenceService 'sklearn-irisv2' is ready to accept traffic, with a specified timeout of 600 seconds.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/performance-testing/README.md#2025-04-21_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nkubectl wait --for=condition=Ready --timeout=600s isvc/sklearn-irisv2\n```\n\n----------------------------------------\n\nTITLE: Defining Server Live Request and Response in Protobuf\nDESCRIPTION: Defines the request and response messages for the ServerLive API, used to check if the inference server is live. The response includes a boolean 'live' indicating the server's state and helps in basic server monitoring.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md#2025-04-21_snippet_10\n\nLANGUAGE: protobuf\nCODE:\n```\nmessage ServerLiveRequest {}\n\nmessage ServerLiveResponse\n{\n  bool live = 1;\n}\n```\n\n----------------------------------------\n\nTITLE: Specifying MLServer Scikit-learn Extension Dependency\nDESCRIPTION: Lists the MLServer Scikit-learn extension package requirement\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/sklearn/v2/requirements.txt#2025-04-21_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nmlserver-sklearn\n```\n\n----------------------------------------\n\nTITLE: Generating Input File with Python Script\nDESCRIPTION: Uses tobytes.py script to convert text input into a file format. The script takes an input_text parameter containing the text to be processed.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v2/bert/sequence_classification/bytes/README.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npython tobytes.py --input_text \"this year business is good\"\n```\n\n----------------------------------------\n\nTITLE: Kubernetes Application CR Example\nDESCRIPTION: This YAML snippet demonstrates the use of Kubernetes labels recommended for resources that are part of an application. It provides an example of how to structure the `app.kubernetes.io` labels for consistent metadata and monitoring within Kubeflow.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/guidelines/kubeflow_kfserving_requirements.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n\"app.kubernetes.io/name: tf-job-operator\napp.kubernetes.io/instance: tf-job-operator-v0.6.1\napp.kubernetes.io/version: v0.6.1\napp.kubernetes.io/component: tf-job-operator\napp.kubernetes.io/part-of: kubeflow\napp.kubernetes.io/managed-by: kfctl\"\n```\n\n----------------------------------------\n\nTITLE: Importing Alibi Helper\nDESCRIPTION: Imports helper functions for working with the Alibi library. Ensures that the alibi_helper module is available for use post import.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/alibi/cifar10/cifar10_explanations.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport sys\n\nsys.path.append(\"../\")\nfrom alibi_helper import *\n```\n\n----------------------------------------\n\nTITLE: TFJob CRD schema example\nDESCRIPTION: This references a TFJob CRD schema example written in libsonnet. It serves as an example of how to define the CRD schema for training operators using Kubernetes, emphasizing validation and structure.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/guidelines/kubeflow_kfserving_requirements.md#2025-04-21_snippet_1\n\nLANGUAGE: jsonnet\nCODE:\n```\n\"https://github.com/kubeflow/kubeflow/blob/v0.6.1/kubeflow/tf-training/tf-job-operator.libsonnet#L81\"\n```\n\n----------------------------------------\n\nTITLE: Verifying XGBoost Server Installation in Python\nDESCRIPTION: Command to check if the XGBoost server is successfully installed. It displays the usage information and required arguments for running the server.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/xgbserver/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m xgbserver\n```\n\n----------------------------------------\n\nTITLE: Installing KServe SDK using pip\nDESCRIPTION: Command to install the KServe Python SDK using pip package manager. This installs the basic functionality without storage support.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/README.md#2025-04-21_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install kserve\n```\n\n----------------------------------------\n\nTITLE: Markdown Table of KnativeCondition Properties\nDESCRIPTION: Markdown representation of the KnativeCondition model properties, their types, and descriptions\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/docs/KnativeCondition.md#2025-04-21_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Name | Type | Description | Notes |\n| ------------ | ------------- | ------------- | ------------- |\n| **last_transition_time** | [**KnativeVolatileTime**](KnativeVolatileTime.md) | LastTransitionTime is the last time the condition transitioned from one status to another. | [optional] |\n| **message** | **str** | A human readable message indicating details about the transition. | [optional] |\n| **reason** | **str** | The reason for the condition's last transition. | [optional] |\n| **severity** | **str** | Severity with which to treat failures of this type of condition. | [optional] |\n| **status** | **str** | Status of the condition, one of True, False, Unknown. | |\n| **type** | **str** | Type of condition. | |\n```\n\n----------------------------------------\n\nTITLE: Specifying MLServer Base Package Dependency\nDESCRIPTION: Lists the core MLServer package requirement\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/sklearn/v2/requirements.txt#2025-04-21_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nmlserver\n```\n\n----------------------------------------\n\nTITLE: Installing PMML Server Dependencies\nDESCRIPTION: Command to install development dependencies for the PMML server locally\nSOURCE: https://github.com/kserve/kserve/blob/master/python/pmmlserver/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmake dev_install\n```\n\n----------------------------------------\n\nTITLE: Showing an Image - Python\nDESCRIPTION: This snippet defines a function to visualize an image from the CIFAR10 dataset using matplotlib. The function reshapes the input data and suppresses the axes for a cleaner view.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/outlier-detection/alibi-detect/cifar10/cifar10_outlier.ipynb#2025-04-21_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndef show(X):\n    plt.imshow(X.reshape(32, 32, 3))\n    plt.axis(\"off\")\n    plt.show()\n```\n\n----------------------------------------\n\nTITLE: Building ART Model Explainer Docker Image\nDESCRIPTION: Command to build a Docker image for ART Model Explainer from the KServe Python directory using artexplainer.Dockerfile.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/artexplainer/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t dockeruser/artserver:latest -f artexplainer.Dockerfile .\n```\n\n----------------------------------------\n\nTITLE: Running Static Type Checks with Mypy\nDESCRIPTION: This command runs static type checks on the `alibiexplainer` codebase using `mypy`. The `--ignore-missing-imports` flag is used to suppress errors related to missing imports, allowing the type checker to focus on the code within the project.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/alibi/alibiexplainer/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmypy --ignore-missing-imports alibiexplainer\n```\n\n----------------------------------------\n\nTITLE: Logged Request and Response Payloads\nDESCRIPTION: Sample output from the echo server showing the logged request and response payloads, including CloudEvents headers and prediction data.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/logger/local/README.md#2025-04-21_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{ path: '/',\n  headers: \n   { host: '0.0.0.0:8000',\n     'user-agent': 'Go-http-client/1.1',\n     'content-length': '23',\n     'ce-id': '3ba84918-209f-4d8d-ae38-b041e5e4a328',\n     'ce-inferenceservice': '\"iris\"',\n     'ce-namespace': '\"default\"',\n     'ce-endpoint': '\"default\"',\n     'ce-source': 'http://localhost:8081/',\n     'ce-specversion': '0.2',\n     'ce-time': '2020-02-27T09:52:15.820509751Z',\n     'ce-type': 'org.kubeflow.serving.inference.response',\n     'content-type': 'application/json',\n     'accept-encoding': 'gzip' },\n  method: 'POST',\n  body: '{\"predictions\": [1, 1]}',\n  cookies: undefined,\n  fresh: false,\n  hostname: '0.0.0.0',\n  ip: '::ffff:172.17.0.1',\n  ips: [],\n  protocol: 'http',\n  query: {},\n  subdomains: [],\n  xhr: false,\n  os: { hostname: '00ed2b10cbea' } }\n::ffff:172.17.0.1 - - [27/Feb/2020:09:52:15 +0000] \"POST / HTTP/1.1\" 200 798 \"-\" \"Go-http-client/1.1\"\n-----------------\n{ path: '/',\n  headers: \n   { host: '0.0.0.0:8000',\n     'user-agent': 'Go-http-client/1.1',\n     'content-length': '76',\n     'ce-id': '3ba84918-209f-4d8d-ae38-b041e5e4a328',\n     'ce-inferenceservice': '\"iris\"',\n     'ce-namespace': '\"default\"',\n     'ce-endpoint': '\"default\"',\n     'ce-source': 'http://localhost:8081/',\n     'ce-specversion': '0.2',\n     'ce-time': '2020-02-27T09:52:15.816441245Z',\n     'ce-type': 'org.kubeflow.serving.inference.request',\n     'content-type': 'application/json',\n     'accept-encoding': 'gzip' },\n  method: 'POST',\n  body: '{  \"instances\": [    [6.8,  2.8,  4.8,  1.4],    [6.0,  3.4,  4.5,  1.6]  ]}',\n  cookies: undefined,\n  fresh: false,\n  hostname: '0.0.0.0',\n  ip: '::ffff:172.17.0.1',\n  ips: [],\n  protocol: 'http',\n  query: {},\n  subdomains: [],\n  xhr: false,\n  os: { hostname: '00ed2b10cbea' } }\n::ffff:172.17.0.1 - - [27/Feb/2020:09:52:15 +0000] \"POST / HTTP/1.1\" 200 850 \"-\" \"Go-http-client/1.1\"\n```\n\n----------------------------------------\n\nTITLE: Setting Test PyPI Credentials in Poetry\nDESCRIPTION: This command configures the username and password credentials for Test PyPI in Poetry's configuration, allowing for authenticated uploads to the test repository.\nSOURCE: https://github.com/kserve/kserve/blob/master/hack/python-sdk/README.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npoetry config http-basic.test-pypi <username> <password>\n```\n\n----------------------------------------\n\nTITLE: Rolling Out Changes\nDESCRIPTION: This command restarts the KServe controller manager deployment to apply the changes made to the configmap, ensuring that the updated configurations take effect.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/OPENSHIFT_GUIDE.md#2025-04-21_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\noc rollout restart deployment kserve-controller-manager -n kserve\\\noc wait --for=condition=ready pod -l control-plane=kserve-controller-manager -n kserve --timeout=300s\n```\n\n----------------------------------------\n\nTITLE: Cloning TorchServe Repository\nDESCRIPTION: Command to clone the TorchServe repository for gRPC client setup.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v1/README.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/pytorch/serve\n```\n\n----------------------------------------\n\nTITLE: Defining Pod Scheduling Gates in Kubernetes\nDESCRIPTION: Configures scheduling gates that can block pod scheduling until specific conditions are met. Gates can only be set during pod creation and removed afterwards.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/docs/V1beta1CustomExplainer.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nscheduling_gates: list[V1PodSchedulingGate] = None\n```\n\n----------------------------------------\n\nTITLE: Defining V1alpha1ModelCacheNodeGroupList Properties in Python\nDESCRIPTION: Defines the structure of a ModelCacheNodeGroup list resource with standard Kubernetes list properties including optional API version, items, kind, and metadata\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/docs/V1alpha1ModelCacheNodeGroupList.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass V1alpha1ModelCacheNodeGroupList:\n    def __init__(self, \n        api_version=None, \n        items=None, \n        kind=None, \n        metadata=None\n    ):\n        self.api_version = api_version\n        self.items = items\n        self.kind = kind\n        self.metadata = metadata\n```\n\n----------------------------------------\n\nTITLE: Setting PyPI Credentials in Poetry\nDESCRIPTION: This command configures the username and password credentials for the main PyPI repository in Poetry's configuration, enabling authenticated uploads for official package publishing.\nSOURCE: https://github.com/kserve/kserve/blob/master/hack/python-sdk/README.md#2025-04-21_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npoetry config http-basic.pypi <username> <password>\n```\n\n----------------------------------------\n\nTITLE: Checking Provisioner Pod Status\nDESCRIPTION: Output of kubectl get pods showing the running status of the KServe controller and local NVMe persistent volume provisioner pods.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v1/bloom/README.md#2025-04-21_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nNAME                                         READY   STATUS    RESTARTS   AGE\nkserve-controller-manager-5c5c4d8c89-lrzbd   2/2     Running   0          4d2h\nlocal-nvme-pv-provisioner-vwxgt              1/1     Running   0          16m\n```\n\n----------------------------------------\n\nTITLE: Apply promotion configuration - Kubernetes CLI\nDESCRIPTION: This command applies the Kubernetes resource defined in the `promotion.yaml` file. It promotes the canary model, directing all traffic to the latest revision.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/rollout/README.md#2025-04-21_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f promotion.yaml\n```\n\n----------------------------------------\n\nTITLE: Apache License 2.0 Copyright Header\nDESCRIPTION: Standard licensing text for open source software, specifying copyright, license terms, and usage restrictions for the KServe project\nSOURCE: https://github.com/kserve/kserve/blob/master/hack/boilerplate.go.txt#2025-04-21_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n/*\nCopyright 2023 The KServe Authors.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n*/\n```\n\n----------------------------------------\n\nTITLE: Sample Prediction Response\nDESCRIPTION: Example of the expected output from the curl command, showing a successful prediction response from the SKLearn model.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/logger/local/README.md#2025-04-21_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"predictions\": [1, 1]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Test PyPI Repository in Poetry\nDESCRIPTION: This command configures Poetry to recognize the Test PyPI repository, which is used for testing package uploads before publishing to the main PyPI repository.\nSOURCE: https://github.com/kserve/kserve/blob/master/hack/python-sdk/README.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npoetry config repositories.test-pypi https://test.pypi.org/legacy/\n```\n\n----------------------------------------\n\nTITLE: Training Operators conventions code example\nDESCRIPTION: This refers to examples for Training Operators conventions in libsonnet format. The examples provided are for TFJob and PyTorchJob operators in Kubeflow, written in libsonnet.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/guidelines/kubeflow_kfserving_requirements.md#2025-04-21_snippet_2\n\nLANGUAGE: jsonnet\nCODE:\n```\n\"https://github.com/kubeflow/kubeflow/blob/v0.6.1/kubeflow/tf-training/tf-job-operator.libsonnet\"\n\n```\n\nLANGUAGE: jsonnet\nCODE:\n```\n\"https://github.com/kubeflow/kubeflow/blob/v0.6.1/kubeflow/pytorch-job/pytorch-operator.libsonnet\"\n```\n\n----------------------------------------\n\nTITLE: Thinc MIT License Declaration\nDESCRIPTION: MIT License terms for the Thinc library, granting permissions for use, modification and distribution with copyright notice requirements.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/third_party/library/license.txt#2025-04-21_snippet_5\n\nLANGUAGE: text\nCODE:\n```\nThe MIT License (MIT)\n\nCopyright (C) 2016 ExplosionAI GmbH, 2016 spaCy GmbH, 2015 Matthew Honnibal\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n```\n\n----------------------------------------\n\nTITLE: KServe Requirements\nDESCRIPTION: Basic requirements list specifying KServe and transformers as dependencies. This appears to be a minimal dependency specification for a Python project using KServe for model serving.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/triton/fastertransformer/transformer/requirements.txt#2025-04-21_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nkserve\ntransformers\n```\n\n----------------------------------------\n\nTITLE: Pushing Paddle Server Docker Image - Docker Command\nDESCRIPTION: This snippet shows the command to push the locally built Paddle Server Docker image to Docker Hub. This allows for easy sharing and deployment of the image.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/paddleserver/README.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker push ${docker_user_name}/paddleserver:latest\n```\n\n----------------------------------------\n\nTITLE: Matplotlib Visualization Library Dependency\nDESCRIPTION: Defines the minimum version for matplotlib, a popular Python plotting and data visualization library used for creating charts, graphs, and visual representations of data\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/outlier-detection/alibi-detect/cifar10/requirements_notebook.txt#2025-04-21_snippet_1\n\nLANGUAGE: requirements\nCODE:\n```\nmatplotlib>=3.1.1\n```\n\n----------------------------------------\n\nTITLE: Data Chunk Example 10 - Completion Signal\nDESCRIPTION: This snippet indicates the end of the data stream from the OpenAI Chat Completion API. It signifies that all chunks have been received.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/test/fixtures/openai/chat_completion_chunk_stream.txt#2025-04-21_snippet_9\n\nLANGUAGE: TEXT\nCODE:\n```\n\"data: [DONE]\"\n```\n\n----------------------------------------\n\nTITLE: Apply Prometheus and Grafana YAML\nDESCRIPTION: Applies a YAML file to deploy Prometheus and Grafana into the `knative-monitoring` namespace. This YAML file contains the necessary configurations and resources for running Prometheus and Grafana.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/docs/metrics.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n\"kubectl apply -f https://github.com/knative/serving/releases/download/v0.17.0/monitoring-metrics-prometheus.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Deploying BERT Model on GPU with KServe\nDESCRIPTION: Command to deploy the BERT model on GPU resources using a specific YAML configuration for KServe.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v2/bert/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f bert_gpu.yaml\n```\n\n----------------------------------------\n\nTITLE: Resolving CRD Annotations Too Long Error\nDESCRIPTION: This issue occurs when applying CRDs and the annotations exceed Kubernetes' size limit. The solution involves reducing annotation size or using server-side apply with `kubectl`. There are no specific dependencies, but requires knowledge of Kubernetes and `kubectl` usage.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/COMMON_ISSUES_AND_SOLUTIONS.md#2025-04-21_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nmetadata.annotations: Too long: must have at most 262144 bytes.\n```\n\nLANGUAGE: shell\nCODE:\n```\nkubectl apply --server-side=true -k ./config/default\n```\n\n----------------------------------------\n\nTITLE: Data Chunk Example 2 - Content Addition\nDESCRIPTION: This JSON snippet represents a subsequent chunk in the chat completion stream, adding the content \"AI\". It includes the token \"AI\" along with its log probability and top log probabilities.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/test/fixtures/openai/chat_completion_chunk_stream.txt#2025-04-21_snippet_1\n\nLANGUAGE: JSON\nCODE:\n```\n\"data: {\\\"id\\\":\\\"chatcmpl-9GsTs1IiRnXYuM1CeKqWeiU1B7E4a\\\",\\\"object\\\":\\\"chat.completion.chunk\\\",\\\"created\\\":1713809700,\\\"model\\\":\\\"gpt-3.5-turbo-0125\\\",\\\"system_fingerprint\\\":\\\"fp_c2295e73ad\\\",\\\"choices\\\":[{\\\"index\\\":0,\\\"delta\\\":{\\\"content\\\":\\\"AI\\\"},\\\"logprobs\\\":{\\\"content\\\":[{\\\"token\\\":\\\"AI\\\",\\\"logprob\\\":-0.012744601,\\\"bytes\\\":[65,73],\\\"top_logprobs\\\":[{\\\"token\\\":\\\"AI\\\",\\\"logprob\\\":-0.012744601,\\\"bytes\\\":[65,73]}]}]},\\\"finish_reason\\\":null}]}\"\n```\n\n----------------------------------------\n\nTITLE: Configuring InferenceService with Metric Aggregation\nDESCRIPTION: YAML configuration for InferenceService with metric aggregation and Prometheus scraping enabled.\nSOURCE: https://github.com/kserve/kserve/blob/master/qpext/README.md#2025-04-21_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"sklearn-irisv2\"\n  annotations:\n    serving.kserve.io/enable-metric-aggregation: \"true\"\n    serving.kserve.io/enable-prometheus-scraping: \"true\"\nspec:\n  predictor:\n    sklearn:\n      protocolVersion: v2\n      storageUri: \"gs://kfserving-examples/models/sklearn/1.0/model\"\n```\n\n----------------------------------------\n\nTITLE: Installing KServe CRD Helm Chart via Console\nDESCRIPTION: This command installs the KServe CRD Helm chart from the specified OCI registry. It uses version v0.15.0 of the chart and names the release 'kserve-crd'.\nSOURCE: https://github.com/kserve/kserve/blob/master/charts/kserve-crd/README.md#2025-04-21_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n$ helm install kserve-crd oci://ghcr.io/kserve/charts/kserve-crd --version v0.15.0\n```\n\n----------------------------------------\n\nTITLE: Creating StorageClass for Fast Disks in Kubernetes\nDESCRIPTION: YAML definition for a StorageClass that handles local persistent volumes. This configuration uses no dynamic provisioner and waits for the first consumer before binding volumes.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v1/bloom/README.md#2025-04-21_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nkind: StorageClass\napiVersion: storage.k8s.io/v1\nmetadata:\n  name: fast-disks\nprovisioner: kubernetes.io/no-provisioner\nvolumeBindingMode: WaitForFirstConsumer\n```\n\n----------------------------------------\n\nTITLE: Creating Knative Broker\nDESCRIPTION: This snippet creates a Knative Broker resource. Brokers are used for event delivery within Knative eventing. This broker will be used for sending inference requests which can then be used by the drift detector to determine drift.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/drift-detection/alibi-detect/cifar10/cifar10_drift.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n\"%%writefile broker.yaml\napiVersion: eventing.knative.dev/v1\nkind: broker\nmetadata:\n name: default\n namespace: cifar10\"\n```\n\n----------------------------------------\n\nTITLE: Applying Knative Trigger Configuration - Python\nDESCRIPTION: This snippet applies the trigger.yaml to the Kubernetes cluster, creating a trigger that directs events to the outlier detector service upon reception.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/outlier-detection/alibi-detect/cifar10/cifar10_outlier.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n!kubectl apply -f trigger.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining Supported Model Format Specification in Python\nDESCRIPTION: Represents a data structure for configuring model format properties in KServe, including optional auto-selection, naming, priority, and version details for serving runtimes\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/docs/V1alpha1SupportedModelFormat.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass V1alpha1SupportedModelFormat:\n    def __init__(self, auto_select=None, name='', priority=None, version=None):\n        self.auto_select = auto_select\n        self.name = name\n        self.priority = priority\n        self.version = version\n```\n\n----------------------------------------\n\nTITLE: Creating Knative Eventing Broker\nDESCRIPTION: This YAML snippet creates a Knative Eventing broker named 'default' for routing events. This setup assumes an MT-Channel-Based broker is configured with the In Memory Channel (IMC).\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/logger/knative-eventing/README.md#2025-04-21_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: eventing.knative.dev/v1\nkind: broker\nmetadata:\n name: default\n```\n\n----------------------------------------\n\nTITLE: Configuring fstab Entry for Persistent NVMe Mount\nDESCRIPTION: Example fstab entry to add to /etc/fstab for ensuring the NVMe drive is automatically mounted at system startup. Uses the UUID for consistent device identification.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v1/bloom/README.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nUUID=nvme_UUID      /mnt/data/vol1   xfs    defaults,nofail    0    2\n```\n\n----------------------------------------\n\nTITLE: Adding Annotation to Skip SSL Verification - YAML\nDESCRIPTION: This snippet demonstrates how to add an annotation to the storage-config Secret that will skip SSL verification, which is useful for testing when no CA bundle is used.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/cabundle/README.md#2025-04-21_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\ndata:\n  AWS_ACCESS_KEY_ID: VEhFQUNDRVNTS0VZ\n  AWS_SECRET_ACCESS_KEY: VEhFUEFTU1dPUkQ=\nkind: Secret\nmetadata:\n  annotations:\n       serving.kserve.io/s3-verifyssl: \"0\" # 1 is true, 0 is false\n    ...\n  name: storage-config\n  namespace: kserve-demo\ntype: Opaque\n```\n\n----------------------------------------\n\nTITLE: Benchmark Setup Commands\nDESCRIPTION: Bash commands for setting up and running the benchmark tests using vegeta.\nSOURCE: https://github.com/kserve/kserve/blob/master/test/benchmark/README.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f ./sklearn.yaml\nkubectl apply -f ./sklearn_vegeta_cfg.yaml\nkubectl create -f ./sk_benchmark.yaml\n```\n\n----------------------------------------\n\nTITLE: Running Tests Using Make Command\nDESCRIPTION: Use make test to execute tests for ensuring proper functionality of the LightGBM server implementation. This command finds all test cases and runs them accordingly.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/lgbserver/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmake test\n```\n\n----------------------------------------\n\nTITLE: Removing Prometheus and Prometheus Operator\nDESCRIPTION: This snippet removes Prometheus and Prometheus Operator from the Kubernetes cluster using Kustomize delete commands.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/metrics-and-monitoring/README.md#2025-04-21_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\ncd kfserving\nkubectl delete -k docs/samples/metrics-and-monitoring/prometheus\nkubectl delete -k docs/samples/metrics-and-monitoring/prometheus-operator\n```\n\n----------------------------------------\n\nTITLE: Formatting NVMe Drive with XFS Filesystem\nDESCRIPTION: Bash command to format an NVMe drive with the XFS filesystem, preparing it for use as persistent storage in Kubernetes.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v1/bloom/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ sudo mkfs.xfs /dev/nvme1n1\n```\n\n----------------------------------------\n\nTITLE: Removing Knative Trigger\nDESCRIPTION: Deletes the Knative trigger that routes events to the message logger.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/logger/knative-eventing/logger_demo.ipynb#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n!kubectl delete -f trigger.yaml\n```\n\n----------------------------------------\n\nTITLE: Retrieving Service Hostname in Python\nDESCRIPTION: This snippet retrieves the hostname of the sklearn-iris inference service and prints it.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/logger/basic/logger_demo.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nSERVICE_HOSTNAMES = !(kubectl get inferenceservice sklearn-iris -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\nSERVICE_HOSTNAME = SERVICE_HOSTNAMES[0]\nprint(SERVICE_HOSTNAME)\n```\n\n----------------------------------------\n\nTITLE: Patching Kourier Deployment\nDESCRIPTION: This command patches the Kourier deployment to resolve security context issues by removing the runAsUser and runAsGroup settings that might violate security constraints in OpenShift.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/OPENSHIFT_GUIDE.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\noc patch deployment 3scale-kourier-gateway -n kourier-system --type=json \\\n  -p='[{\"op\": \"remove\", \"path\": \"/spec/template/spec/containers/0/securityContext/runAsUser\"}, {\"op\": \"remove\", \"path\": \"/spec/template/spec/containers/0/securityContext/runAsGroup\"}]'\n```\n\n----------------------------------------\n\nTITLE: Printing Service Hostname\nDESCRIPTION: Prints the extracted service hostname\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/logger/logger_demo.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n!echo $SERVICE_HOSTNAME\n```\n\n----------------------------------------\n\nTITLE: Data Chunk Example 8 - Adding \" is\"\nDESCRIPTION: This JSON snippet adds the content \" is\" to the response, using the token \" is\" with it's log probability and top log probabilities.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/test/fixtures/openai/chat_completion_chunk_stream.txt#2025-04-21_snippet_7\n\nLANGUAGE: JSON\nCODE:\n```\n\"data: {\\\"id\\\":\\\"chatcmpl-9GsTs1IiRnXYuM1CeKqWeiU1B7E4a\\\",\\\"object\\\":\\\"chat.completion.chunk\\\",\\\"created\\\":1713809700,\\\"model\\\":\\\"gpt-3.5-turbo-0125\\\",\\\"system_fingerprint\\\":\\\"fp_c2295e73ad\\\",\\\"choices\\\":[{\\\"index\\\":0,\\\"delta\\\":{\\\"content\\\":\\\" is\\\"},\\\"logprobs\\\":{\\\"content\\\":[{\\\"token\\\":\\\" is\\\",\\\"logprob\\\":-1.5388204,\\\"bytes\\\":[32,105,115],\\\"top_logprobs\\\":[{\\\"token\\\":\\\" refers\\\",\\\"logprob\\\":-0.24170122,\\\"bytes\\\":[32,114,101,102,101,114,115]}]}]},\\\"finish_reason\\\":null}]}\"\n```\n\n----------------------------------------\n\nTITLE: Configuring TFServing Container Specification\nDESCRIPTION: Defines the detailed configuration for a Tensorflow serving container, including environment settings, image configuration, and runtime parameters. Supports advanced Kubernetes container deployment options.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/docs/V1beta1TFServingSpec.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nV1beta1TFServingSpec({\n  \"image\": \"tensorflow/serving\",\n  \"command\": [\"tensorflow_model_server\"],\n  \"args\": [\"--port=8501\"],\n  \"ports\": [{\"containerPort\": 8501}],\n  \"resources\": {\n    \"requests\": {\"cpu\": \"1\", \"memory\": \"2Gi\"},\n    \"limits\": {\"cpu\": \"2\", \"memory\": \"4Gi\"}\n  }\n})\n```\n\n----------------------------------------\n\nTITLE: Retrieving Cluster IP in Python\nDESCRIPTION: This snippet retrieves the IP address of the istio-ingressgateway service and prints it.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/logger/basic/logger_demo.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nCLUSTER_IPS = !(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\nCLUSTER_IP = CLUSTER_IPS[0]\nprint(CLUSTER_IP)\n```\n\n----------------------------------------\n\nTITLE: Data Chunk Example 3 - Adding Comma\nDESCRIPTION: This JSON snippet represents a chunk adding a comma to the response. It includes the token \",\" along with its log probability and top log probabilities.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/test/fixtures/openai/chat_completion_chunk_stream.txt#2025-04-21_snippet_2\n\nLANGUAGE: JSON\nCODE:\n```\n\"data: {\\\"id\\\":\\\"chatcmpl-9GsTs1IiRnXYuM1CeKqWeiU1B7E4a\\\",\\\"object\\\":\\\"chat.completion.chunk\\\",\\\"created\\\":1713809700,\\\"model\\\":\\\"gpt-3.5-turbo-0125\\\",\\\"system_fingerprint\\\":\\\"fp_c2295e73ad\\\",\\\"choices\\\":[{\\\"index\\\":0,\\\"delta\\\":{\\\"content\\\":\\\",\\\"},\\\"logprobs\\\":{\\\"content\\\":[{\\\"token\\\":\\\",\\\",\\\"logprob\\\":-0.20856616,\\\"bytes\\\":[44],\\\"top_logprobs\\\":[{\\\"token\\\":\\\",\\\",\\\"logprob\\\":-0.20856616,\\\"bytes\\\":[44]}]}]},\\\"finish_reason\\\":null}]}\"\n```\n\n----------------------------------------\n\nTITLE: Defining Pod Properties in Kubernetes Python SDK\nDESCRIPTION: Demonstrates the comprehensive set of configurable properties for a Kubernetes Pod specification, including container management, networking, and scheduling options\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/docs/V1beta1PodSpec.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Example Pod Specification Properties\npod_spec = {\n    \"active_deadline_seconds\": 3600,\n    \"containers\": [V1Container()],\n    \"dns_policy\": \"ClusterFirst\",\n    \"host_network\": False,\n    \"restart_policy\": \"Always\"\n}\n```\n\n----------------------------------------\n\nTITLE: Updating Poetry Lock Files for KServe Packages\nDESCRIPTION: This command updates the pyproject.toml files for all KServe packages using Poetry. It's an essential step in the release process to ensure all dependencies are correctly locked.\nSOURCE: https://github.com/kserve/kserve/blob/master/release/RELEASE_PROCESS_v2.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmake poetry-lock\n```\n\n----------------------------------------\n\nTITLE: Data Chunk Example 9 - Length Finish Reason\nDESCRIPTION: This JSON snippet signifies that the chat completion finished due to reaching the maximum length. The delta object is empty, indicating no new content was added in this chunk.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/test/fixtures/openai/chat_completion_chunk_stream.txt#2025-04-21_snippet_8\n\nLANGUAGE: JSON\nCODE:\n```\n\"data: {\\\"id\\\":\\\"chatcmpl-9GsTs1IiRnXYuM1CeKqWeiU1B7E4a\\\",\\\"object\\\":\\\"chat.completion.chunk\\\",\\\"created\\\":1713809700,\\\"model\\\":\\\"gpt-3.5-turbo-0125\\\",\\\"system_fingerprint\\\":\\\"fp_c2295e73ad\\\",\\\"choices\\\":[{\\\"index\\\":0,\\\"delta\\\":{},\\\"logprobs\\\":null,\\\"finish_reason\\\":\\\"length\\\"}]}\"\n```\n\n----------------------------------------\n\nTITLE: Alibi Detect Dependency with TensorFlow\nDESCRIPTION: Specifies the minimum version requirement for alibi-detect library with TensorFlow support, used for machine learning model detection and explanation\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/outlier-detection/alibi-detect/cifar10/requirements_notebook.txt#2025-04-21_snippet_0\n\nLANGUAGE: requirements\nCODE:\n```\nalibi-detect[tensorflow]>=0.11.0\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Version 3.9.13 for KServe Project\nDESCRIPTION: This file specifies Python version 3.9.13 as the required Python version for the KServe project. It's likely used by dependency management tools like pyenv, poetry, or other environment managers to ensure consistent Python runtime environments.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/custom_model/runtime.txt#2025-04-21_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\npython-3.9.13\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements for MLServer with XGBoost in KServe\nDESCRIPTION: A requirements list specifying MLServer and its XGBoost extension for model serving in KServe. This likely represents dependencies needed for deploying XGBoost models with MLServer as the serving runtime.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/xgboost/requirements.txt#2025-04-21_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nmlserver\nmlserver-xgboost\n```\n\n----------------------------------------\n\nTITLE: Data Chunk Example 7 - Adding Comma\nDESCRIPTION: This JSON snippet adds another comma to the response, following \"intelligence\".  It includes the token \",\" along with its log probability and top log probabilities.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/test/fixtures/openai/chat_completion_chunk_stream.txt#2025-04-21_snippet_6\n\nLANGUAGE: JSON\nCODE:\n```\n\"data: {\\\"id\\\":\\\"chatcmpl-9GsTs1IiRnXYuM1CeKqWeiU1B7E4a\\\",\\\"object\\\":\\\"chat.completion.chunk\\\",\\\"created\\\":1713809700,\\\"model\\\":\\\"gpt-3.5-turbo-0125\\\",\\\"system_fingerprint\\\":\\\"fp_c2295e73ad\\\",\\\"choices\\\":[{\\\"index\\\":0,\\\"delta\\\":{\\\"content\\\":\\\",\\\"},\\\"logprobs\\\":{\\\"content\\\":[{\\\"token\\\":\\\",\\\",\\\"logprob\\\":-1.7432603e-6,\\\"bytes\\\":[44],\\\"top_logprobs\\\":[{\\\"token\\\":\\\",\\\",\\\"logprob\\\":-1.7432603e-6,\\\"bytes\\\":[44]}]}]},\\\"finish_reason\\\":null}]}\"\n```\n\n----------------------------------------\n\nTITLE: Creating KServe Inference Service with Logger\nDESCRIPTION: This YAML snippet defines an InferenceService that uses a Sklearn model, with logging enabled to send events to the Knative broker's URL. It requires KServe installed and functional.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/logger/knative-eventing/README.md#2025-04-21_snippet_7\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: serving.kserve.io/v1beta1\nkind: InferenceService\nmetadata:\n  name: sklearn-iris\nspec:\n  predictor:\n    minReplicas: 1\n    logger:\n      mode: all\n      url: http://broker-ingress.knative-eventing.svc.cluster.local/default/default\n    sklearn:\n      storageUri: gs://kfserving-examples/models/sklearn/1.0/model\n```\n\n----------------------------------------\n\nTITLE: Creating Knative Service for Message Logging\nDESCRIPTION: This YAML snippet creates a Knative service named 'message-dumper' that logs CloudEvents it receives. It requires Knative Serving and Eventing installations. The service runs an image for event display from Google's container registry.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/logger/knative-eventing/README.md#2025-04-21_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\napiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: message-dumper\nspec:\n  template:\n    spec:\n      containers:\n      - image: gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display\n```\n\n----------------------------------------\n\nTITLE: Deploying Inference Service with kubectl\nDESCRIPTION: This snippet demonstrates deploying an inference service using the v1beta1 API with kubectl for AIX explainers. Ensure Kubernetes is set up and the 'aix-explainer.yaml' file is available. This command applies the YAML configuration to create the service, exposing it for further operations.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/aix/Fetch_20newsgroups/README.md#2025-04-21_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nkubectl apply -f aix-explainer.yaml\n```\n\n----------------------------------------\n\nTITLE: Implementing Interactive Mode GPL Notice\nDESCRIPTION: Example code for displaying a GPL license notice when a program starts in interactive mode. This snippet demonstrates how to create a short copyright notice with instructions for viewing full license details.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/third_party/library/license.txt#2025-04-21_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\n    Gnomovision version 69, Copyright (C) year name of author\n    Gnomovision comes with ABSOLUTELY NO WARRANTY; for details type `show w'.\n    This is free software, and you are welcome to redistribute it\n    under certain conditions; type `show c' for details.\n```\n\n----------------------------------------\n\nTITLE: Data Chunk Example 4 - Adding \" or\"\nDESCRIPTION: This JSON snippet adds the content \" or\" to the response. It includes the token \" or\" along with its log probability and top log probabilities.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/test/fixtures/openai/chat_completion_chunk_stream.txt#2025-04-21_snippet_3\n\nLANGUAGE: JSON\nCODE:\n```\n\"data: {\\\"id\\\":\\\"chatcmpl-9GsTs1IiRnXYuM1CeKqWeiU1B7E4a\\\",\\\"object\\\":\\\"chat.completion.chunk\\\",\\\"created\\\":1713809700,\\\"model\\\":\\\"gpt-3.5-turbo-0125\\\",\\\"system_fingerprint\\\":\\\"fp_c2295e73ad\\\",\\\"choices\\\":[{\\\"index\\\":0,\\\"delta\\\":{\\\"content\\\":\\\" or\\\"},\\\"logprobs\\\":{\\\"content\\\":[{\\\"token\\\":\\\" or\\\",\\\"logprob\\\":-0.015670527,\\\"bytes\\\":[32,111,114],\\\"top_logprobs\\\":[{\\\"token\\\":\\\" or\\\",\\\"logprob\\\":-0.015670527,\\\"bytes\\\":[32,111,114]}]}]},\\\"finish_reason\\\":null}]}\"\n```\n\n----------------------------------------\n\nTITLE: Torchserve Configuration Properties\nDESCRIPTION: Configuration file for Torchserve specifying network settings, worker configurations, and model installation preferences\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/bert-sample/hugging-face-bert-sample.md#2025-04-21_snippet_3\n\nLANGUAGE: json\nCODE:\n```\ninference_address=http://0.0.0.0:8080\nmanagement_address=http://0.0.0.0:8081\nnumber_of_netty_threads=4\njob_queue_size=100\ninstall_py_dep_per_model=true\nmodel_store=/mnt/models\nmodel_snapshot={\"name\":\"startup.cfg\",\"modelCount\":1,\"models\":{\"BERTSeqClassification\":{\"1.0\":{\"defaultVersion\":true,\"marName\":\"BERTSeqClassification.mar\",\"minWorkers\":1,\"maxWorkers\":5,\"batchSize\":1,\"maxBatchDelay\":5000,\"responseTimeout\":120}}}}\n```\n\n----------------------------------------\n\nTITLE: Creating PV Pod using kubectl\nDESCRIPTION: This command applies the configuration from the pvpod.yaml file to create a pod that uses the persistent volume for storage, using Kubernetes. Successful creation is confirmed by the expected output.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/custom-server-with-external-storage.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f pvpod.yaml\n```\n\n----------------------------------------\n\nTITLE: Canary Deployment with KServe - Candidate Model\nDESCRIPTION: Second step in a canary deployment, introducing a candidate model that receives only a portion of traffic (10%). This allows testing the new model with limited exposure.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/kfp/README.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nkserve_op(\n    action='apply',\n    model_name='tf-sample',\n    model_uri='gs://kfserving-examples/models/tensorflow/flowers-2',\n    framework='tensorflow',\n    canary_traffic_percent='10'\n)\n```\n\n----------------------------------------\n\nTITLE: Applying InferenceService Configuration with kubectl\nDESCRIPTION: This command deploys the InferenceService using the torchserve-custom-pv.yaml configuration file. Update the container image and specify the PV storage before applying. Successful deployment is indicated by the expected output.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/custom-server-with-external-storage.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f torchserve-custom-pv.yaml\n```\n\n----------------------------------------\n\nTITLE: V1beta1 Custom Transformer Kubernetes CRD Definition\nDESCRIPTION: Defines the Kubernetes Custom Resource Definition (CRD) for a custom transformer, specifying configuration options and structural schema for transformer components in machine learning model serving\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/docs/V1beta1CustomTransformer.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n# V1beta1CustomTransformer CRD Structure\ntype V1beta1CustomTransformer struct {\n    // Transformer container image configuration\n    Container *V1Container `json:\"container,omitempty\"`\n    // Optional runtime configuration\n    RuntimeConfig map[string]string `json:\"runtimeConfig,omitempty\"`\n}\n```\n\n----------------------------------------\n\nTITLE: Viewing Iter8 Experiment Report\nDESCRIPTION: This command generates an HTML report of the Iter8 experiment outcome and saves it as 'report.html'. It requires the Iter8 CLI and a completed experiment run. The input is the CLI command, and the expected outcome is an HTML report file.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/performance-testing/grpc.md#2025-04-21_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\niter8 k report -o html > report.html\n```\n\n----------------------------------------\n\nTITLE: Setting up environment variables for querying\nDESCRIPTION: Bash commands to set up environment variables for the model name, service hostname, and prepare the query command.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/art/mnist/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nMODEL_NAME=artserver\nSERVICE_HOSTNAME=$(kubectl get inferenceservice ${MODEL_NAME} -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\npython query_explain.py http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/${MODEL_NAME}:explain ${SERVICE_HOSTNAME}\n```\n\n----------------------------------------\n\nTITLE: InferenceService Deployment YAML\nDESCRIPTION: Defines a KServe InferenceService resource. It specifies the name of the service and the container image to use for the predictor component. This is the initial deployment configuration.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/custom/torchserve/docs/canary.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"torchserve-custom\"\nspec:\n  predictor:\n    containers:\n    - image: {username}/torchserve:latest\n      name: torchserve-container\n```\n\n----------------------------------------\n\nTITLE: Canary Deployment with KServe - Initial Model\nDESCRIPTION: First step in a canary deployment pattern, deploying the initial model with 100% traffic. This establishes the baseline model before introducing the candidate model.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/kfp/README.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nkserve_op(\n    action = 'apply',\n    model_name='tf-sample',\n    model_uri='gs://kfserving-examples/models/tensorflow/flowers',\n    framework='tensorflow',\n)\n```\n\n----------------------------------------\n\nTITLE: Sending Inference Request to Deployed Model\nDESCRIPTION: Bash commands to get service hostname and send inference request using curl.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/sklearn/v2/README.md#2025-04-21_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nSERVICE_HOSTNAME=$(kubectl get inferenceservice sklearn-irisv2 -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\n\ncurl -v \\\n  -H \"Host: ${SERVICE_HOSTNAME}\" \\\n  -d @./iris-input.json \\\n  http://${INGRESS_HOST}:${INGRESS_PORT}/v2/models/sklearn-irisv2/infer\n```\n\n----------------------------------------\n\nTITLE: Getting NVMe Drive UUID for Persistent Mounting\nDESCRIPTION: Bash command to retrieve the UUID of an NVMe drive, which is needed for persistently mounting the drive across system reboots.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/v1beta1/torchserve/v1/bloom/README.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ sudo blkid /dev/nvme1n1\n```\n\n----------------------------------------\n\nTITLE: Triggering Automatic Cherry-Pick for KServe Release\nDESCRIPTION: This GitHub comment triggers an automatic cherry-pick action to merge specific changes into the release branch. It's used during the release process to incorporate approved changes.\nSOURCE: https://github.com/kserve/kserve/blob/master/release/RELEASE_PROCESS_v2.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n/cherry-pick release-branch\n```\n\n----------------------------------------\n\nTITLE: Apache 2.0 License Boilerplate Notice Template\nDESCRIPTION: Template for applying Apache 2.0 license to source files, with placeholder fields in brackets for project-specific information.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/third_party/library/license.txt#2025-04-21_snippet_4\n\nLANGUAGE: text\nCODE:\n```\n   Copyright 2017, The TensorFlow Authors.\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n```\n\n----------------------------------------\n\nTITLE: Apply Kubernetes configuration\nDESCRIPTION: This snippet applies the Kubernetes configuration defined in the `moviesentiment.yaml` file, which is used to deploy the inference service.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/alibi/moviesentiment/movie_review_explanations.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n\"!kubectl apply -f moviesentiment.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Running Pre-commit Checks for KServe Release\nDESCRIPTION: This command runs pre-commit checks from the KServe root directory. It helps identify and address any lint errors before finalizing the release.\nSOURCE: https://github.com/kserve/kserve/blob/master/release/RELEASE_PROCESS_v2.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmake precommit\n```\n\n----------------------------------------\n\nTITLE: Checking Broker Status\nDESCRIPTION: This shell command retrieves the status of the 'default' broker using the 'kubectl' tool, providing information about the broker's URL and readiness state.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/logger/knative-eventing/README.md#2025-04-21_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nkubectl get broker default\n```\n\n----------------------------------------\n\nTITLE: Applying CRD Using kubectl in Bash\nDESCRIPTION: Command to apply a Kubernetes Custom Resource Definition using kubectl for the sklearn model deployment.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/storage/uri/README.md#2025-04-21_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f sklearn_uri.yaml\n```\n\n----------------------------------------\n\nTITLE: Stopping the InferenceService\nDESCRIPTION: Command to delete the deployed InferenceService using kubectl.\nSOURCE: https://github.com/kserve/kserve/blob/master/docs/samples/explanation/art/mnist/README.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete -f art.yaml\n```\n\n----------------------------------------\n\nTITLE: Data Chunk Example 6 - Adding \" intelligence\"\nDESCRIPTION: This JSON snippet adds the content \" intelligence\" to the response. It includes the token \" intelligence\" along with its log probability and top log probabilities.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/kserve/test/fixtures/openai/chat_completion_chunk_stream.txt#2025-04-21_snippet_5\n\nLANGUAGE: JSON\nCODE:\n```\n\"data: {\\\"id\\\":\\\"chatcmpl-9GsTs1IiRnXYuM1CeKqWeiU1B7E4a\\\",\\\"object\\\":\\\"chat.completion.chunk\\\",\\\"created\\\":1713809700,\\\"model\\\":\\\"gpt-3.5-turbo-0125\\\",\\\"system_fingerprint\\\":\\\"fp_c2295e73ad\\\",\\\"choices\\\":[{\\\"index\\\":0,\\\"delta\\\":{\\\"content\\\":\\\" intelligence\\\"},\\\"logprobs\\\":{\\\"content\\\":[{\\\"token\\\":\\\" intelligence\\\",\\\"logprob\\\":-0.000056338537,\\\"bytes\\\":[32,105,110,116,101,108,108,105,103,101,110,99,101],\\\"top_logprobs\\\":[{\\\"token\\\":\\\" intelligence\\\",\\\"logprob\\\":-0.000056338537,\\\"bytes\\\":[32,105,110,116,101,108,108,105,103,101,110,99,101]}]}]},\\\"finish_reason\\\":null}]}\"\n```\n\n----------------------------------------\n\nTITLE: Urllib3 MIT License Declaration\nDESCRIPTION: MIT License terms for the Urllib3 library, specifying usage rights and limitations with copyright attribution to Andrey Petrov and contributors.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/third_party/library/license.txt#2025-04-21_snippet_6\n\nLANGUAGE: text\nCODE:\n```\nMIT License\n\nCopyright (c) 2008-2019 Andrey Petrov and contributors (see CONTRIBUTORS.txt)\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n```\n\n----------------------------------------\n\nTITLE: Sample Copyright Disclaimer for GPL Projects\nDESCRIPTION: A template for creating a copyright disclaimer that can be signed by employers or institutions. This example shows how a company can disclaim copyright interest in a program created by an employee.\nSOURCE: https://github.com/kserve/kserve/blob/master/python/third_party/library/license.txt#2025-04-21_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\n  Yoyodyne, Inc., hereby disclaims all copyright interest in the program\n  `Gnomovision' (which makes passes at compilers) written by James Hacker.\n\n  {signature of Ty Coon}, 1 April 1989\n  Ty Coon, President of Vice\n```"
  }
]
[
  {
    "owner": "dolphindb",
    "repo": "tutorials_cn",
    "content": "TITLE: Creating and Initializing DolphinDB OLAP and TSDB Tables in DolphinDB Script\nDESCRIPTION: This code snippet creates a DolphinDB table schema 'model' with financial stock data fields. It uses OLAP and TSDB storage engines to create partitioned databases and tables for storing stock snapshot data between 2020.06.01 and 2020.06.07. It defines partitioning by date and symbol and enables sorting for TSDB. The code also includes functions mockHalfDayData and mockData to simulate half-day and multi-day stock data concurrently for OLAP and TSDB tables, handling insertion with append! commands. Prerequisites include DolphinDB server environment and knowledge of table creation and data generation APIs. Inputs are date intervals and start times, outputs are simulated data loaded into distributed DolphinDB tables.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_SQL_Case_Tutorial.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\nmodel = table(1:0, `SecurityID`DateTime`PreClosePx`OpenPx`HighPx`LowPx`LastPx`Volume`Amount`BidPrice1`BidPrice2`BidPrice3`BidPrice4`BidPrice5`BidOrderQty1`BidOrderQty2`BidOrderQty3`BidOrderQty4`BidOrderQty5`OfferPrice1`OfferPrice2`OfferPrice3`OfferPrice4`OfferPrice5`OfferQty1`OfferQty2`OfferQty3`OfferQty4`OfferQty5, [SYMBOL, DATETIME, DOUBLE, DOUBLE, DOUBLE, DOUBLE, DOUBLE, LONG, DOUBLE, DOUBLE, DOUBLE, DOUBLE, DOUBLE, DOUBLE, LONG, LONG, LONG, LONG, LONG, DOUBLE, DOUBLE, DOUBLE, DOUBLE, DOUBLE, LONG, LONG, LONG, LONG, LONG])\n\n// OLAP 存储引擎建库建表\ndbDate = database(\"\", VALUE, 2020.06.01..2020.06.07)\ndbSecurityID = database(\"\", HASH, [SYMBOL, 10])\ndb = database(\"dfs://Level1\", COMPO, [dbDate, dbSecurityID])\ncreatePartitionedTable(db, model, `Snapshot, `DateTime`SecurityID)\n\n// TSDB 存储引擎建库建表\ndbDate = database(\"\", VALUE, 2020.06.01..2020.06.07)\ndbSymbol = database(\"\", HASH, [SYMBOL, 10])\ndb = database(\"dfs://Level1_TSDB\", COMPO, [dbDate, dbSymbol], engine=\"TSDB\")\ncreatePartitionedTable(db, model, `Snapshot, `DateTime`SecurityID, sortColumns=`SecurityID`DateTime)\n\ndef mockHalfDayData(Date, StartTime) {\n\tt_SecurityID = table(format(600001..602000, \"000000\") + \".SH\" as SecurityID)\n\tt_DateTime = table(concatDateTime(Date, StartTime + 1..2400 * 3) as DateTime)\n\tt = cj(t_SecurityID, t_DateTime)\n\tsize = t.size()\n\treturn  table(t.SecurityID as SecurityID, t.DateTime as DateTime, rand(100.0, size) as PreClosePx, rand(100.0, size) as OpenPx, rand(100.0, size) as HighPx, rand(100.0, size) as LowPx, rand(100.0, size) as LastPx, rand(10000, size) as Volume, rand(100000.0, size) as Amount, rand(100.0, size) as BidPrice1, rand(100.0, size) as BidPrice2, rand(100.0, size) as BidPrice3, rand(100.0, size) as BidPrice4, rand(100.0, size) as BidPrice5, rand(100000, size) as BidOrderQty1, rand(100000, size) as BidOrderQty2, rand(100000, size) as BidOrderQty3, rand(100000, size) as BidOrderQty4, rand(100000, size) as BidOrderQty5, rand(100.0, size) as OfferPrice1, rand(100.0, size) as OfferPrice2, rand(100.0, size) as OfferPrice3, rand(100.0, size) as OfferPrice4, rand(100.0, size) as OfferPrice5, rand(100000, size) as OfferQty1, rand(100000, size) as OfferQty2, rand(100000, size) as OfferQty3, rand(100000, size) as OfferQty4, rand(100000, size) as OfferQty5)\n}\n\ndef mockData(DateVector, StartTimeVector) {\n\tfor(Date in DateVector) {\n\t\tfor(StartTime in StartTimeVector) {\n\t\t\tdata = mockHalfDayData(Date, StartTime)\n\n\t\t\t// OLAP 存储引擎分布表插入模拟数据\n\t\t\tloadTable(\"dfs://Level1\", \"Snapshot\").append!(data)\n\n\t\t\t// TSDB 存储引擎分布表插入模拟数据\n\t\t\tloadTable(\"dfs://Level1_TSDB\", \"Snapshot\").append!(data)\n\t\t}\n\t}\n}\n\nmockData(2020.06.01..2020.06.02, 09:30:00 13:00:00)\n```\n\n----------------------------------------\n\nTITLE: Asof Join Example in DolphinDB\nDESCRIPTION: This code demonstrates an asof join (`aj`) to find the latest humidity reading (id 4) before a temperature anomaly occurred. It involves creating tables, selecting data based on conditions, and then using the `aj` function with the columns to join on. The `aj` function helps in joining time-series data, matching records from the right table based on the nearest preceding timestamp.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/iot_examples.md#_snippet_10\n\nLANGUAGE: DolphinDB\nCODE:\n```\nt1 = table(3 53 as id, 1 2 as machineId, 2020.09.01T09:56:06 2020.09.01T09:58:12 as datetime, 50.6 50.7 as anomalyMetrics)\nt2_1 = select *, 1 as machineId from sensors where id=4, datetime between 2020.09.01T09:00:00 : 2020.09.01T09:54:00\nt2_2 = select *, 2 as machineId from sensors where id=54, datetime between 2020.09.01T09:00:00 : 2020.09.01T09:57:00\nt2 = unionAll(t2_1, t2_2)\nselect * from aj(t1, t2, `machineId`datetime);\n```\n\n----------------------------------------\n\nTITLE: Creating and Loading Market Snapshot Data into DolphinDB\nDESCRIPTION: This comprehensive function creates a composite database with time and hash partitions, defines a schema with over 60 columns for market snapshot data, creates a partitioned table, and loads data from a CSV file. It includes type conversions for timestamps, symbols, dates, and array columns.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/appendices_market_replay_bp/snap_upload.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef snapUpload(dbName, tbName)\n{\n    path=\"C:/Users/myhu/Desktop/snapshot.csv\"    \n\n    db1 = database(, VALUE, 2021.12.01..2021.12.31)\n    db2 = database(, HASH, [SYMBOL, 20])\n    db = database(dbName, COMPO, [db1, db2], , \"TSDB\")\n\n    schemaTable = table(\n        array(SYMBOL, 0) as SecurityID,\n        array(DATE, 0) as MDDate,\n        array(TIME, 0) as MDTime,\n        array(TIMESTAMP, 0) as DataTimestamp,\n        array(SYMBOL, 0) as TradingPhaseCode,\n        array(SYMBOL, 0) as SecurityIDSource,\n        array(SYMBOL, 0) as SecurityType,\n        array(INT, 0) as MaxPx,\n        array(INT, 0) as MinPx,\n        array(INT, 0) as PreClosePx,\n        array(INT, 0) as NumTrades,\n        array(INT, 0) as TotalVolumeTrade,\n        array(INT, 0) as TotalValueTrade,\n        array(INT, 0) as LastPx,\n        array(INT, 0) as OpenPx,\n        array(INT, 0) as ClosePx,\n        array(INT, 0) as HighPx,\n        array(INT, 0) as LowPx,\n        array(INT, 0) as DiffPx1,\n        array(INT, 0) as DiffPx2,\n        array(INT, 0) as TotalBuyQty,\n        array(INT, 0) as TotalSellQty,\n        array(INT, 0) as WeightedAvgBuyPx,\n        array(INT, 0) as WeightedAvgSellPx,\n        array(INT, 0) as WithdrawBuyNumber,\n        array(INT, 0) as WithdrawBuyAmount,\n        array(INT, 0) as WithdrawBuyMoney,\n        array(INT, 0) as WithdrawSellNumber,\n        array(INT, 0) as WithdrawSellAmount,\n        array(INT, 0) as WithdrawSellMoney,\n        array(INT, 0) as TotalBuyNumber,\n        array(INT, 0) as TotalSellNumber,\n        array(INT, 0) as BuyTradeMaxDuration,\n        array(INT, 0) as SellTradeMaxDuration,\n        array(INT, 0) as NumBuyOrders,\n        array(INT, 0) as NumSellOrders,\n        array(INT, 0) as NorminalPx,\n        array(INT, 0) as ShortSellSharesTraded,\n        array(INT, 0) as ShortSellTurnover,\n        array(INT, 0) as ReferencePx,\n        array(TIMESTAMP, 0) as ComplexEventStartTime,\n        array(TIMESTAMP, 0) as ComplexEventEndTime,\n        array(DATE, 0) as ExchangeDate,\n        array(TIME, 0) as ExchangeTime,\n        array(INT, 0) as AfterHoursNumTrades,\n        array(INT, 0) as AfterHoursTotalVolumeTrade,\n        array(INT, 0) as AfterHoursTotalValueTrade,\n        array(INT, 0) as ChannelNo,\n        array(INT[], 0) as BuyPriceQueue,\n        array(INT[], 0) as BuyOrderQtyQueue,\n        array(INT[], 0) as SellPriceQueue,\n        array(INT[], 0) as SellOrderQtyQueue,\n        array(INT[], 0) as BuyOrderQueue,\n        array(INT[], 0) as SellOrderQueue,\n        array(INT[], 0) as BuyNumOrdersQueue,\n        array(INT[], 0) as SellNumOrdersQueue,\n        array(INT, 0) as MaxBuyPrice,\n        array(INT, 0) as MinBuyPrice,\n        array(INT, 0) as MaxSellPrice,\n        array(INT, 0) as MinSellPrice,\n        array(INT, 0) as PreMarketLastPx,\n        array(INT, 0) as PreMarketTotalVolumeTrade,\n        array(INT, 0) as PreMarketTotalValueTrade,\n        array(INT, 0) as PreMarketHighPx,\n        array(INT, 0) as PreMarketLowPx,\n        array(INT, 0) as AfterHoursLastPx,\n        array(INT, 0) as AfterHoursHighPx,\n        array(INT, 0) as AfterHoursLowPx,\n        array(SYMBOL, 0) as MarketPhaseCode\n    )\n\n    db.createPartitionedTable(table=schemaTable, tableName=tbName, partitionColumns=`MDDate`SecurityID, sortColumns=`SecurityID`MDTime, keepDuplicates=ALL,compressMethods={MDDate:\"delta\", MDTime:\"delta\",DataTimestamp:\"delta\",ComplexEventStartTime:\"delta\",ComplexEventEndTime:\"delta\",ExchangeDate:\"delta\",ExchangeTime:\"delta\"})\n    \n    schema = extractTextSchema(path)\n    update schema set type = \"TIMESTAMP\" where name = \"DataTimestamp\"\n    update schema set type = \"SYMBOL\" where name = \"TradingPhaseCode\"\n    update schema set type = \"TIMESTAMP\" where name = \"ComplexEventStartTime\"\n    update schema set type = \"TIMESTAMP\" where name = \"ComplexEventEndTime\"\n    update schema set type = \"DATE\" where name = \"ExchangeDate\"\n    update schema set type = \"INT[]\" where name = \"BuyPriceQueue\"\n    update schema set type = \"INT[]\" where name = \"BuyOrderQtyQueue\"\n    update schema set type = \"INT[]\" where name = \"SellPriceQueue\"\n    update schema set type = \"INT[]\" where name = \"SellOrderQtyQueue\"\n    update schema set type = \"INT[]\" where name = \"BuyOrderQueue\"\n    update schema set type = \"INT[]\" where name = \"SellOrderQueue\"\n    update schema set type = \"INT[]\" where name = \"BuyNumOrdersQueue\"\n    update schema set type = \"INT[]\" where name = \"SellNumOrdersQueue\"\n    snapshot = loadTextEx(dbHandle=db, tableName=`snapshot, partitionColumns=`MDDate`SecurityID, filename=path, schema=schema, sortColumns=`SecurityID`MDTime, arrayDelimiter=\",\")\n}\n```\n\n----------------------------------------\n\nTITLE: Creating an Unpartitioned Dimension Table for Stock Market Data in DolphinDB Script\nDESCRIPTION: Creates a dimension table without partitions containing stock market data with specified column schema. This type of table loads entire data into memory and is suitable only for small volumes (<200,000 records). The example illustrates improper use for large datasets generating heavy memory usage and poor query performance (2.6.1). Requires DolphinDB environment and table schema. Input is schema and data; output is a non-partitioned, fully memory-resident dimension table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/database_and_table_creation_details.md#_snippet_11\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndb= database(\"dfs://testDB1\", VALUE, 2020.01.01..2021.01.01)\ncolName = `SecurityID`TradeDate`TradeTime`TradePrice`TradeQty`TradeAmount`BuyNo`SellNo\ncolType = `SYMBOL`DATE`TIME`DOUBLE`INT`DOUBLE`INT`INT\ntbSchema = table(1:0, colName, colType)\ndb.createTable(tbSchema,`dt)\n```\n\n----------------------------------------\n\nTITLE: Calculating Moving VWAP (Optimized - Context By)\nDESCRIPTION: This code snippet showcases an optimized approach to calculating moving weighted average (mwavg) using context by. It groups the data by symbol and calculates mwavg for each group, significantly improving performance.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_sql_cases.md#_snippet_13\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntimer res2 = select mwavg(price, volume, 4) from t \n\t\t\t   context by symbol\n```\n\n----------------------------------------\n\nTITLE: Fixed-Step Time-Based Sliding Window Sum Using TMSum Function in DolphinDB SQL\nDESCRIPTION: Calculates the sum over a sliding window defined by a fixed time length (5 seconds) with step size of 1 row using the tmsum function. It demonstrates creating a table with time and volume columns and then performs a time-based moving sum aggregation. DolphinDB versions 1.30.14 and 2.00.2 or later support the tmsum function. The output adds a column representing the sum of volume within the previous 5-second window for each timestamp. Intended for time-series data requiring time-interval-based window calculations.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/window_cal.md#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//1.30.14，2.00.2以上版本支持 tmsum 函数。\nt=table(2021.11.01T10:00:00 + 0 1 2 5 6 9 10 17 18 30 as time, 1..10 as vol)\nselect time, vol, tmsum(time,vol,5s) from t\n\n # output\ntime                vol tmsum_time\n------------------- --- ----------\n2021.11.01T10:00:00 1   1         \n2021.11.01T10:00:01 2   3         \n2021.11.01T10:00:02 3   6         \n2021.11.01T10:00:05 4   9         \n2021.11.01T10:00:06 5   12        \n2021.11.01T10:00:09 6   15        \n2021.11.01T10:00:10 7   18        \n2021.11.01T10:00:17 8   8         \n2021.11.01T10:00:18 9   17        \n2021.11.01T10:00:30 10  10  \n```\n\n----------------------------------------\n\nTITLE: Transforming and Importing Array Vector Data in DolphinDB\nDESCRIPTION: This snippet defines two DolphinDB functions for transforming raw CSV order book data and batch loading it into the TSDB snapshot table. The 'transform' function merges per-level bid/offer columns via fixedLengthArrayVector to produce Array Vector columns for each price, quantity, and order field, matching the schema designed earlier. The 'loadData' function extracts schema, updates column types accordingly, then loads and transforms the CSV data into the database table through loadTextEx. Dependencies: source CSV files with properly named columns, the fixedLengthArrayVector utility, and a live DolphinDB environment with the previously created table. Inputs are per-level price/order columns in the CSV; output is the population of the DolphinDB table in bulk with transformed data. Limitations include strict CSV schema requirements, and the functions should be executed with the necessary user/job permissions.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Array_Vector.md#_snippet_30\n\nLANGUAGE: DolphinDB\nCODE:\n```\n def transform(data){\n\tt = select SecurityID, DateTime, PreClosePx, OpenPx, HighPx, LowPx, LastPx, TotalVolumeTrade, TotalValueTrade, InstrumentStatus,\n\t\t      fixedLengthArrayVector(BidPrice0, BidPrice1, BidPrice2, BidPrice3,  BidPrice4, BidPrice5, BidPrice6, BidPrice7, BidPrice8, BidPrice9) as BidPrice,\n\t\t      fixedLengthArrayVector(BidOrderQty0, BidOrderQty1, BidOrderQty2, BidOrderQty3,  BidOrderQty4, BidOrderQty5, BidOrderQty6, BidOrderQty7, BidOrderQty8, BidOrderQty9) as BidOrderQty,\n\t\t      fixedLengthArrayVector(BidNumOrders0, BidNumOrders1, BidNumOrders2, BidNumOrders3,  BidNumOrders4, BidNumOrders5, BidNumOrders6, BidNumOrders7, BidNumOrders8, BidNumOrders9) as BidNumOrders,\n\t\t      fixedLengthArrayVector(BidOrders0, BidOrders1, BidOrders2, BidOrders3,  BidOrders4, BidOrders5, BidOrders6, BidOrders7, BidOrders8, BidOrders9, BidOrders10, BidOrders11, BidOrders12, BidOrders13,  BidOrders14, BidOrders15, BidOrders16, BidOrders17, BidOrders18, BidOrders19, BidOrders20, BidOrders21, BidOrders22, BidOrders23,  BidOrders24, BidOrders25, BidOrders26, BidOrders27, BidOrders28, BidOrders29, BidOrders30, BidOrders31, BidOrders32, BidOrders33,  BidOrders34, BidOrders35, BidOrders36, BidOrders37, BidOrders38, BidOrders39, BidOrders40, BidOrders41, BidOrders42, BidOrders43,  BidOrders44, BidOrders45, BidOrders46, BidOrders47, BidOrders48, BidOrders49) as BidOrders,\n\t\t      fixedLengthArrayVector(OfferPrice0, OfferPrice1, OfferPrice2, OfferPrice3,  OfferPrice4, OfferPrice5, OfferPrice6, OfferPrice7, OfferPrice8, OfferPrice9) as OfferPrice,\n\t\t      fixedLengthArrayVector(OfferOrderQty0, OfferOrderQty1, OfferOrderQty2, OfferOrderQty3,  OfferOrderQty4, OfferOrderQty5, OfferOrderQty6, OfferOrderQty7, OfferOrderQty8, OfferOrderQty9) as OfferOrderQty,\n\t\t      fixedLengthArrayVector(OfferNumOrders0, OfferNumOrders1, OfferNumOrders2, OfferNumOrders3,  OfferNumOrders4, OfferNumOrders5, OfferNumOrders6, OfferNumOrders7, OfferNumOrders8, OfferNumOrders9) as OfferNumOrders,\n\t\t      fixedLengthArrayVector(OfferOrders0, OfferOrders1, OfferOrders2, OfferOrders3,  OfferOrders4, OfferOrders5, OfferOrders6, OfferOrders7, OfferOrders8, OfferOrders9, OfferOrders10, OfferOrders11, OfferOrders12, OfferOrders13,  OfferOrders14, OfferOrders15, OfferOrders16, OfferOrders17, OfferOrders18, OfferOrders19, OfferOrders20, OfferOrders21, OfferOrders22, OfferOrders23,  OfferOrders24, OfferOrders25, OfferOrders26, OfferOrders27, OfferOrders28, OfferOrders29, OfferOrders30, OfferOrders31, OfferOrders32, OfferOrders33,  OfferOrders34, OfferOrders35, OfferOrders36, OfferOrders37, OfferOrders38, OfferOrders39, OfferOrders40, OfferOrders41, OfferOrders42, OfferOrders43,  OfferOrders44, OfferOrders45, OfferOrders46, OfferOrders47, OfferOrders48, OfferOrders49) as OfferOrders,\n\t\t      NumTrades, IOPV, TotalBidQty, TotalOfferQty, WeightedAvgBidPx, WeightedAvgOfferPx, TotalBidNumber, TotalOfferNumber, BidTradeMaxDuration,OfferTradeMaxDuration, NumBidOrders, NumOfferOrders, WithdrawBuyNumber, WithdrawBuyAmount, WithdrawBuyMoney,WithdrawSellNumber, WithdrawSellAmount, WithdrawSellMoney, ETFBuyNumber, ETFBuyAmount, ETFBuyMoney, ETFSellNumber, ETFSellAmount, ETFSellMoney\n\t\t      from data\n\treturn t\n}\n\ndef loadData(csvDir, dbName, tbName){\n\tschemaTB = extractTextSchema(csvDir)\n\tupdate schemaTB set type = \"SYMBOL\" where name = \"SecurityID\"\n\tloadTextEx(dbHandle=database(dbName), tableName=tbName, partitionColumns=`DateTime`SecurityID, sortColumns=`SecurityID`DateTime, filename=csvDir, schema=schemaTB, transform=transform)\n}\n// 后台提交导入任务\ncsvDir = \"/home/v2/下载/data/testdata/snapshot_100stocks_multi.csv\"\ndbName, tbName = \"dfs://SH_TSDB_snapshot_ArrayVector\", \"snapshot\"\nsubmitJob(\"loadData\", \"load Data\", loadData{csvDir, dbName, tbName})\ngetRecentJobs()\n```\n\n----------------------------------------\n\nTITLE: Creating and Using Session Window Engine in DolphinDB Script\nDESCRIPTION: Demonstrates creating a session window engine in DolphinDB that segments incoming streaming data into dynamic windows based on a specified session gap (5 milliseconds). The example defines a shared streaming table, an output keyed table, and sets up the session window engine with a sum aggregation metric on the volume column. It also shows subscribing the window engine to the stream for real-time aggregation, inserting multiple batches of time-stamped data, querying source and aggregated tables, and proper cleanup by unsubscribing and dropping the aggregator. Dependencies include DolphinDB environment and stream processing capabilities. Key parameters include sessionGap (time gap for window segmentation), timeColumn (timestamp column), and metrics (aggregation expressions). Input is a stream of timestamped volume data; output is an aggregated table with summed volume per session window. Limitations include dynamic window sizes depending on data arrival intervals.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/window_cal.md#_snippet_24\n\nLANGUAGE: DolphinDB\nCODE:\n```\nshare streamTable(1000:0, `time`volume, [TIMESTAMP, INT]) as trades\noutput1 = keyedTable(`time,10000:0, `time`sumVolume, [TIMESTAMP, INT])\nengine_sw = createSessionWindowEngine(name = \"engine_sw\", sessionGap = 5, metrics = <sum(volume)>, dummyTable = trades, outputTable = output1, timeColumn = `time)\nsubscribeTable(tableName=\"trades\", actionName=\"append_engine_sw\", offset=0, handler=append!{engine_sw}, msgAsTable=true)\n\nn = 5\ntimev = 2018.10.12T10:01:00.000 + (1..n)\nvolumev = (1..n)%1000\ninsert into trades values(timev, volumev)\n\nn = 5\ntimev = 2018.10.12T10:01:00.010 + (1..n)\nvolumev = (1..n)%1000\ninsert into trades values(timev, volumev)\n\nn = 3\ntimev = 2018.10.12T10:01:00.020 + (1..n)\nvolumev = (1..n)%1000\ntimev.append!(2018.10.12T10:01:00.027 + (1..n))\nvolumev.append!((1..n)%1000)\ninsert into trades values(timev, volumev)\n\nselect * from trades;\n\n// Sample output data provided in the document\n\nselect * from output1\n\n// Drop session window engine and cleanup\nunsubscribeTable(tableName=\"trades\", actionName=\"append_engine_sw\")\ndropAggregator(`engine_sw)\nundef(\"trades\",SHARED)\n```\n\n----------------------------------------\n\nTITLE: Creating Database and Partitioned Table on DolphinDB Data Node (DolphinDB script)\nDESCRIPTION: This DolphinDB script example shows how to create a distributed database and a partitioned table on a data node within a DolphinDB cluster. It logs into the database server, defines the database and table names, checks for and drops existing databases, creates a new distributed database partitioned by date range, defines a table schema with named columns and data types, and creates a partitioned table with 'DateTime' as the partition column. This snippet requires a running DolphinDB data node accessible via the web IDE or DolphinDB client. Inputs include the database name (dfs://testDB), table name (testTB), and the schema of the table columns.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/single_machine_cluster_deploy.md#_snippet_2\n\nLANGUAGE: DolphinDB script\nCODE:\n```\n// 创建存储的数据库和分区表\nlogin(\"admin\", \"123456\")\ndbName = \"dfs://testDB\"\ntbName = \"testTB\"\nif(existsDatabase(dbName)){\n        dropDatabase(dbName)\n}\ndb = database(dbName, VALUE, 2021.01.01..2021.12.31)\ncolNames = `SecurityID`DateTime`PreClosePx`OpenPx`HighPx`LowPx`LastPx`Volume`Amount\ncolTypes = [SYMBOL, DATETIME, DOUBLE, DOUBLE, DOUBLE, DOUBLE, DOUBLE, INT, DOUBLE]\nschemaTable = table(1:0, colNames, colTypes)\ndb.createPartitionedTable(table=schemaTable, tableName=tbName, partitionColumns=`DateTime)\n```\n\n----------------------------------------\n\nTITLE: Range-Based Trade Grouping with asof Function in DolphinDB Script\nDESCRIPTION: Utilizes a range boundary vector and the asof function to group trades directly by which value segment the transaction amount falls into, summarizing by date, symbol, side, and type (bucket index). This removes the need for custom classification functions, making the code concise and improving query performance. No external dependencies are required; uses built-in asof.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_SQL_Case_Tutorial.md#_snippet_36\n\nLANGUAGE: DolphinDB\nCODE:\n```\nrange = [0.0, 40000.0, 200000.0, 1000000.0, 100000000.0]\n\ntimer res3 = select sum(volume) as volume_sum, sum(volume*price) as amount_sum \n\t\t\t\tfrom t \n\t\t\t\twhere time <= 10:00:00 \n\t\t\t\tgroup by date, symbol, side, asof(range, volume*price) as type\n```\n\n----------------------------------------\n\nTITLE: Performance Comparison of WHERE Clause Using Comma vs AND in DolphinDB SQL Filtering\nDESCRIPTION: Compares performance and results of filtering queries using multiple conditions connected by comma ',' versus logical AND in the WHERE clause when conditions are sequence-unrelated. Using a large simulated dataset of 10 million rows with fields including date, sym, price, and qty, the snippet shows that queries with comma-separated and AND-separated filters yield equivalent results verified by eqObj but similar performance timings around 900 ms. Changing the order of the conditions can improve performance about 30% by applying more selective predicates first. This illustrates that for sequence-independent filters, condition order affects performance but not correctness, and that comma and AND can be used interchangeably in many cases.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_SQL_Case_Tutorial.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\nN = 10000000\nt = table(take(2019.01.01..2019.01.03, N) as date, \t\t  \n          take(`C`MS`MS`MS`IBM`IBM`IBM`C`C$SYMBOL, N) as sym, \n          take(49.6 29.46 29.52 30.02 174.97 175.23 50.76 50.32 51.29, N) as price, \n          take(2200 1900 2100 3200 6800 5400 1300 2500 8800, N) as qty)\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntimer(10) t1 = select * from t where qty > 2000, date = 2019.01.02, sym = `C\ntimer(10) t2 = select * from t where qty > 2000 and date = 2019.01.02 and sym = `C\n\neach(eqObj, t1.values(), t2.values()) // true\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntimer(10) t3 = select * from t where date = 2019.01.02, sym = `C, qty > 2000\ntimer(10) t4 = select * from t where date = 2019.01.02 and sym = `C and qty > 2000\n\neach(eqObj, t1.values(), t3.values()) // true\neach(eqObj, t2.values(), t4.values()) // true\n```\n\n----------------------------------------\n\nTITLE: Performance Benchmarking with summary on Large CSV - DolphinDB Script\nDESCRIPTION: This advanced DolphinDB snippet handles ingestion of a large CSV file into a distributed database, creating a table with a specified schema, and computes summary statistics while measuring execution time. The script demonstrates schema definition, conditional database dropping, parallelized loading and sorting, and summary analytics for benchmarking against pandas. Prerequisites: a valid TradesData.csv file, correct database permissions, and a DolphinDB cluster setup. Key parameters include sorting columns and schema; function outputs are timing metrics and summary tables. Limitations include requiring a large memory machine or cluster.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/generate_large_scale_statistics_with_summary.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin('admin','123456')\ndataDir = \"/path/to/TradesData.csv\"\ndbName = \"dfs://summary\"\nschematable = table(`symbol`date`second`price`size`g127`corr`cone`ex as name, `SYMBOL`DATE`SECOND`DOUBLE`INT`INT`INT`STRING`CHAR as type)\nif(existsDatabase(dbName)){\n\t\tdropDatabase(dbName)\n\t}\ndb = database(directory=dbName, partitionType=HASH, partitionScheme=[STRING, 10], engine=\"TSDB\", chunkGranularity=\"DATABASE\")\nloadTextEx(db, \"pt\", \"symbol\", dataDir, sortColumns = `symbol`size, schema=schematable)\npt = loadTable(dbName, \"pt\")\ntimer{\n\tsummary(pt)\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Partitioned Table with Composite Partition Scheme for High Volume Stock Data in DolphinDB Script\nDESCRIPTION: Defines a composite partitioned database combining a date range value partition and a symbol hash partition of size 25, followed by creating a partitioned table with specified stock market columns. This schema is designed for higher data volumes to overcome limitations of using dimension tables by enabling data sharding and manageable partition sizes as described in the solution 2.6.2. Requires DolphinDB with partitioning support.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/database_and_table_creation_details.md#_snippet_12\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndb1 = database(, VALUE, 2020.01.01..2021.01.01)\ndb2 = database(, HASH, [SYMBOL, 25])\ndb = database(directory=\"dfs://testDB2\", partitionType=COMPO, partitionScheme=[db1, db2])\ncolName = `SecurityID`TradeDate`TradeTime`TradePrice`TradeQty`TradeAmount`BuyNo`SellNo\ncolType = `SYMBOL`DATE`TIME`DOUBLE`INT`DOUBLE`INT`INT\ntbSchema = table(1:0, colName, colType)\ndb.createPartitionedTable(table=tbSchema, tableName=`pt, partitionColumns=`TradeDate`SecurityID)\n```\n\n----------------------------------------\n\nTITLE: Creating Level-2 Snapshot Partitioned Database and Table for Shanghai Stock Exchange in DolphinDB\nDESCRIPTION: Defines a distributed TSDB database 'dfs://split_SH_TB' partitioned by daily date range and HASH partition on SECURITYID with 25 buckets, for Level-2 snapshot stock data of Shanghai exchange from 2020-01-01 to 2020-12-31. It creates a table 'split_SH_snapshotTB' with comprehensive fields to represent trading details, including array vectors for order book prices and quantities, using delta compression on TradeDate and TradeTime columns. The sorting columns SECURITYID and TradeTime optimize query efficiency sorted by stock code and trade time. The keepDuplicates=ALL parameter allows storing multiple records within each partition having identical sort keys. This schema accommodates structural differences from Shenzhen exchange but keeps partitioning consistent for co-location and efficient distributed joins.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practices_for_partitioned_storage.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\ncreate database \"dfs://split_SH_TB\"\npartitioned by VALUE(2020.01.01..2021.01.01), HASH([SYMBOL, 25])\nengine='TSDB'\n\ncreate table \"dfs://split_SH_TB\".\"split_SH_snapshotTB\"(\n    TradeDate DATE[comment=\"交易日期\", compress=\"delta\"]   \n    TradeTime TIME[comment=\"交易时间\", compress=\"delta\"]\n    SecurityID SYMBOL\n    ImageStatus INT\n    PreCloPrice DOUBLE\n    OpenPrice DOUBLE\n    HighPrice DOUBLE\n    LowPrice DOUBLE\n    LastPrice DOUBLE\n    ClosePrice DOUBLE\n    TradingPhaseCode SYMBOL\n    NumTrades LONG\n    TotalVolumeTrade LONG\n    TotalValueTrade DOUBLE\n    TotalBidQty LONG\n    WeightedAvgBidPx DOUBLE\n    AltWAvgBidPri DOUBLE\n    TotalOfferQty LONG\n    WeightedAvgOfferPx DOUBLE\n    AltWAvgAskPri DOUBLE\n    ETFBuyNumber INT\n    ETFBuyAmount LONG\n    ETFBuyMoney DOUBLE\n    ETFSellNumber INT\n    ETFSellAmount LONG\n    ETFSellMoney DOUBLE\n    YieldToMatu DOUBLE\n    TotWarExNum DOUBLE\n    UpLimitPx DOUBLE\n    DownLimitPx DOUBLE\n    WithdrawBuyNumber INT\n    WithdrawBuyAmount LONG\n    WithdrawBuyMoney DOUBLE\n    WithdrawSellNumber INT\n    WithdrawSellAmount LONG\n    WithdrawSellMoney DOUBLE\n    TotalBidNumber INT\n    TotalOfferNumber INT\n    MaxBidDur INT\n    MaxSellDur INT\n    BidNum INT\n    SellNum INT\n    IOPV DOUBLE\n    OfferPrice DOUBLE[]\n    BidPrice DOUBLE[]\n    OfferOrderQty LONG[]\n    BidOrderQty LONG[]\n    BidNumOrders INT[]\n    OfferNumOrders INT[]\n    LocalTime TIME\n    SeqNo INT\n    OfferOrders LONG[]\n    BidOrders LONG[]\n)\npartitioned by TradeDate, SecurityID,\nsortColumns=[`SecurityID,`TradeTime],\nkeepDuplicates=ALL\n```\n\n----------------------------------------\n\nTITLE: Checking and Closing High Memory Sessions in DolphinDB\nDESCRIPTION: Example of identifying sessions with high memory usage using getSessionMemoryStat() function and closing specific sessions using the closeSessions function.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/handling_oom.md#_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\ncloseSessions(getSessionMemoryStat().sessionId[11]);\n```\n\n----------------------------------------\n\nTITLE: Group Permissions with TABLE_READ and TABLE_WRITE - DolphinDB\nDESCRIPTION: This code demonstrates how to assign TABLE_READ permission to a group and TABLE_WRITE permission to specific members of the group. This allows for granular control over data access. Requires admin privileges to create users and groups.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Permission_Management.md#_snippet_24\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(\"admin\",\"123456\")\ncreateUser(\"user1\",\"123456\")\ncreateUser(\"user2\",\"123456\")\ncreateUser(\"user3\",\"123456\")\ncreateGroup(\"group1\",[\"user1\",\"user2\",\"user3\"])\ngrant(\"group1\", TABLE_READ, \"dfs://valuedb/pt\")\ncreateUser(\"user4\",\"123456\")\naddGroupMember(\"user4\",\"group1\")\ngrant(\"user1\", TABLE_WRITE, \"dfs://valuedb/pt\")\ngrant(\"user4\", TABLE_WRITE, \"dfs://valuedb/pt\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Stoploss Logic with JIT in DolphinDB Script\nDESCRIPTION: Defines and compares three approaches for implementing stoploss logic based on return series drawdown: JIT-compiled (`stoploss_JIT`), non-JIT (`stoploss_no_JIT`), and vectorized (`stoploss_vectorization`). The functions iterate or use vectorized operations to find the first point where cumulative drawdown exceeds a given threshold. Performance comparison using timers shows JIT can be faster than both non-JIT and vectorized versions, especially when the stoploss condition is met early.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/jit.md#_snippet_2\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\n@jit\ndef stoploss_JIT(ret, threshold) {\n\tn = ret.size()\n\ti = 0\n\tcurRet = 1.0\n\tcurMaxRet = 1.0\n\tindicator = take(true, n)\n\n\tdo {\n\t\tindicator[i] = false\n\t\tcurRet *= (1 + ret[i])\n\t\tif(curRet > curMaxRet) { curMaxRet = curRet }\n\t\tdrawDown = 1 - curRet / curMaxRet;\n\t\tif(drawDown >= threshold) {\n      break\n\t\t}\n\t\ti += 1\n\t} while(i < n)\n\n\treturn indicator\n}\n\ndef stoploss_no_JIT(ret, threshold) {\n\tn = ret.size()\n\ti = 0\n\tcurRet = 1.0\n\tcurMaxRet = 1.0\n\tindicator = take(true, n)\n\n\tdo {\n\t\tindicator[i] = false\n\t\tcurRet *= (1 + ret[i])\n\t\tif(curRet > curMaxRet) { curMaxRet = curRet }\n\t\tdrawDown = 1 - curRet / curMaxRet;\n\t\tif(drawDown >= threshold) {\n      break\n\t\t}\n\t\ti += 1\n\t} while(i < n)\n\n\treturn indicator\n}\n\ndef stoploss_vectorization(ret, threshold){\n\tcumret = cumprod(1+ret)\n \tdrawDown = 1 - cumret / cumret.cummax()\n\tfirstCutIndex = at(drawDown >= threshold).first() + 1\n\tindicator = take(false, ret.size())\n\tif(isValid(firstCutIndex) and firstCutIndex < ret.size())\n\t\tindicator[firstCutIndex:] = true\n\treturn indicator\n}\nret = take(0.0008 -0.0008, 1000000)\nthreshold = 0.10\ntimer(10) stoploss_JIT(ret, threshold)              //      59 ms\ntimer(10) stoploss_no_JIT(ret, threshold)           //   14622 ms\ntimer(10) stoploss_vectorization(ret, threshold)    //     152 ms\n```\n\n----------------------------------------\n\nTITLE: Creating Window Join Engine in DolphinDB to Aggregate Trade Data Within Snapshot Time Windows\nDESCRIPTION: This snippet demonstrates how to initialize a Window Join engine in DolphinDB to aggregate detailed trade records within a time window bracketed by consecutive snapshot records. It defines streaming tables for trades, snapshots, and output including array vectors for trade quantities and times. Key parameters include the window size set to 0:0 (dynamic between snapshots), useSystemTime=false to base timing on data timestamps, and nullFill to specify default fills for missing values. The script subscribes snapshot and trade streams to the engine. Input tables provide sample snapshot and trade data with timestamps illustrating the windowing boundaries.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming-real-time-correlation-processing.md#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// create table\nshare streamTable(1:0, `Sym`TradeTime`Side`TradeQty, [SYMBOL, TIME, INT, LONG]) as trades\nshare streamTable(1:0, `Sym`Time`Open`High`Low`Close, [SYMBOL, TIME, DOUBLE, DOUBLE, DOUBLE, DOUBLE]) as snapshot\nshare streamTable(1:0, `Time`Sym`Open`High`Low`Close`BuyQty`SellQty`TradeQtyList`TradeTimeList, [TIME, SYMBOL, DOUBLE, DOUBLE, DOUBLE, DOUBLE, LONG, LONG, LONG[], TIME[]]) as output\n\n// create engine\nwjMetrics = <[Open, High, Low, Close, sum(iif(Side==1, TradeQty, 0)), sum(iif(Side==2, TradeQty, 0)), TradeQty, TradeTime]>\nfillArray = [00:00:00.000, \"\", 0, 0, 0, 0, 0, 0, [], []]\nwjEngine = createWindowJoinEngine(name=\"windowJoin\", leftTable=snapshot, rightTable=trades, outputTable=output, window=0:0, metrics=wjMetrics, matchingColumn=`Sym, timeColumn=`Time`TradeTime, useSystemTime=false, nullFill=fillArray)\n\n// subscribe topic\nsubscribeTable(tableName=\"snapshot\", actionName=\"appendLeftStream\", handler=getLeftStream(wjEngine), msgAsTable=true, offset=-1, hash=0)\nsubscribeTable(tableName=\"trades\", actionName=\"appendRightStream\", handler=getRightStream(wjEngine), msgAsTable=true, offset=-1, hash=1)\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// generate data: snapshot\nt1 = table(`A`B`A`B`A`B as Sym, 10:00:00.000+(3 3 6 6 9 9)*1000 as Time, (NULL NULL 3.5 7.6 3.5 7.6) as Open, (3.5 7.6 3.6 7.6 3.6 7.6) as High, (3.5 7.6 3.5 7.6 3.4 7.5) as Low, (3.5 7.6 3.5 7.6 3.6 7.5) as Close)\n// generate data: trade\nt2 = table(`A`A`B`A`B`B`A`B`A`A as Sym, 10:00:02.000+(1..10)*700 as TradeTime,  (1 2 1 1 1 1 2 1 2 2) as Side, (1..10) * 10 as TradeQty)\n// input data\ntrades.append!(t2)\nsnapshot.append!(t1)\n```\n\n----------------------------------------\n\nTITLE: Creating Hash-Partitioned Table with SecurityID as Partition Column Causing Concurrent Write Conflicts in DolphinDB Script\nDESCRIPTION: Defines a hash-partitioned database with only stock code as partition column, then creates a partitioned table. This leads to concurrent write conflicts if multiple transactions attempt to write to the same partition locked by symbol, causing transaction failure (2.8.1 error). Requires DolphinDB transaction support and hash partitions. Input is stock data fields partitioned by symbol only; output is a partitioned table susceptible to write conflicts.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/database_and_table_creation_details.md#_snippet_15\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//假设后续并发，是按照日期进行写入，只按股票代码进行分区\ndb = database(\"dfs://testDB\",HASH, [SYMBOL, 25])\ncolName = `SecurityID`TradeDate`TradeTime`TradePrice`TradeQty`TradeAmount`BuyNo`SellNo\ncolType = `SYMBOL`DATE`TIME`DOUBLE`INT`DOUBLE`INT`INT\ntbSchema = table(1:0, colName, colType)\n//只按股票代码进行分区,而并发写入按照日期进行会导致并发写入冲突，报错“has been owned by transaction”\ndb.createPartitionedTable(table=tbSchema, tableName=`pt, partitionColumns=`SecurityID)\n```\n\n----------------------------------------\n\nTITLE: Subscribing to DolphinDB Stream Table for Monitoring\nDESCRIPTION: Subscribes to the 'doorRecord' stream table using the `subscribeTable` function. It specifies 'monitor' as the action name, starts processing from the beginning of the stream (offset=0), appends incoming data to the 'reactivEngine1' engine using `append!`, and treats messages as tables (`msgAsTable=true`).\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_engine_anomaly_alerts.md#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\nsubscribeTable(tableName=\"doorRecord\", actionName=\"monitor\", offset=0,\n               handler=append!{reactivEngine1}, msgAsTable=true\n```\n\n----------------------------------------\n\nTITLE: Caching Multiple Partitions Below warningMemSize (OLAP) (DolphinDB Script)\nDESCRIPTION: Iteratively loads 9 different partitions residing on node1 into memory by selecting all columns. It prints the memory usage after loading each partition, demonstrating that DolphinDB accumulates cached data in memory as long as the total usage stays below the configured `warningMemSize` limit.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/memory_management.md#_snippet_10\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\ndbPath =  right(dbName,strlen(dbName)-5)\np = select top 9 * from rpc(getControllerAlias(),getClusterChunksStatus) \n    where file like dbPath +\"%\" and replicas like \"node1%\" //这里节点1的别名为node1\n    order by file\ndays = datetimeParse(t.file.substr(strlen(dbPath)+1,8),\"yyyyMMdd\")\nfor(d in days){\n    select * from loadTable(dbName,tableName) where  date= d\n    print mem().allocatedBytes - mem().freeBytes\n}\n```\n\n----------------------------------------\n\nTITLE: Calculating Sliding Window Sum Using M-Series Moving Function in DolphinDB SQL\nDESCRIPTION: Demonstrates calculating a sliding window sum of the 'vol' column with a fixed window length of 5 rows and step size of 1 row using the optimized msum function. It includes table creation with timestamps and volume, and a select query showing how to use msum for moving aggregation. Also illustrates the context by clause for group-wise window calculation by symbol 'sym'. DolphinDB 1.30.16/2.00.4 or later is required for window functions. The input is a table with time and volume columns, and the output adds the moving sum column. Limitations include fixed step size of 1 row for m-series functions.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/window_cal.md#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\nt=table(2021.11.01T10:00:00 + 0 1 2 5 6 9 10 17 18 30 as time, 1..10 as vol)\n\nselect time, vol, msum(vol,5,1) from t\n\n # output\n\ntime                vol msum_vol\n------------------- --- --------\n2021.11.01T10:00:00 1   1       \n2021.11.01T10:00:01 2   3       \n2021.11.01T10:00:02 3   6       \n2021.11.01T10:00:05 4   10      \n2021.11.01T10:00:06 5   15    \n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nt=table(2021.11.01T10:00:00 + 0 1 2 5 6 9 10 17 18 30 join 0 1 2 5 6 9 10 17 18 30 as time, 1..20 as vol, take(`A,10) join take(`B,10) as sym)\n\nselect time, sym, vol, msum(vol,5,1) from t context by sym\n\n # output\n\ntime                sym vol msum_vol\n------------------- --- --- --------\n2021.11.01T10:00:00 A   1   1       \n2021.11.01T10:00:01 A   2   3       \n2021.11.01T10:00:02 A   3   6       \n...    \n2021.11.01T10:00:30 A   10  40      \n2021.11.01T10:00:00 B   11  11      \n2021.11.01T10:00:01 B   12  23      \n...    \n2021.11.01T10:00:30 B   20  90 \n```\n\n----------------------------------------\n\nTITLE: Loading Distributed Table in DolphinDB\nDESCRIPTION: Shows how to load a distributed table from a DFS database into a variable for subsequent queries.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_SQL_Case_Tutorial.md#_snippet_6\n\nLANGUAGE: SQL\nCODE:\n```\nsnapshot = loadTable(\"dfs://Level1\", \"Snapshot\")\n```\n\n----------------------------------------\n\nTITLE: Nested Window Operations for Alpha98 Factor Calculation in DolphinDB SQL\nDESCRIPTION: Illustrates a complex nested sliding window calculation for financial factor computation (Alpha98), greatly reducing code size while improving performance. It defines a normalization rank function and an alpha98SQL function that performs multiple update statements with moving averages, rank calculations, correlation, and decay statistics grouped by ts_code or trade_date. Input is a trade table with code, date, open price, volume, and amount fields. The output is a table with the computed alpha98 factor per trade_date and ts_code. Requires DolphinDB framework supporting mavg, mcorr, msum, mrank, moving, and context by constructs.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/window_cal.md#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 输入表trade的schema如下，如需要可自行模拟数据。\n\nname       typeString typeInt \n---------- ---------- ------- \nts_code    SYMBOL     17             \ntrade_date DATE       6              \nopen       DOUBLE     16             \nvol        DOUBLE     16             \namount     DOUBLE     16    \n\n// alpha 98 计算：\n\ndef normRank(x){\n\treturn rank(x)\\x.size()\n}\n\ndef alpha98SQL(t){\n\tupdate t set adv5 = mavg(vol, 5), adv15 = mavg(vol, 15) context by ts_code\n\tupdate t set rank_open = normRank(open), rank_adv15 = normRank(adv15) context by trade_date\n\tupdate t set decay7 = mavg(mcorr(vwap, msum(adv5, 26), 5), 1..7), decay8 = mavg(mrank(9 - mimin(mcorr(rank_open, rank_adv15, 21), 9), true, 7), 1..8) context by ts_code\n\treturn select ts_code, trade_date, normRank(decay7)-normRank(decay8) as a98 from t context by trade_date \n}\n\ninput = select trade_date,ts_code,amount*1000/(vol*100 + 1) as vwap,vol,open from trade\ntimer alpha98DDBSql = alpha98SQL(input)\n```\n\n----------------------------------------\n\nTITLE: Initializing MultithreadedTableWriter for DolphinDB in C++\nDESCRIPTION: 本代码段展示如何初始化一个 MultithreadedTableWriter (MTW) 对象，该对象支持向 DolphinDB 数据库的内存表、流表、分区表或维度表高效并发写入数据。需提前包含关联头文件，并正确链接 DolphinDB 的 C++ SDK。构造函数参数包括服务器地址、端口、账户信息、数据库名、目标表、列压缩设置及线程数量等关键参数。每列压缩方式在 compress 向量指定。输出为已实例化并可并发写入的 MTW 对象。依赖：DolphinDB C++ API。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_cpp_api_connector.md#_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\nvector<COMPRESS_METHOD> compress;\nfor(int i=0;i<102;i++)compress.push_back(COMPRESS_LZ4);   // 每列的压缩方式\nMultithreadedTableWriter writer(\n      \"127.0.0.1\", 9900, \"admin\",\"123456\",\"dfs://test_MultithreadedTableWriter\",\"collect\",NULL,false,NULL,1000,1,10,\"deviceid\", &compress);  \n```\n\n----------------------------------------\n\nTITLE: Creating and Persisting Stream Tables for Real-Time Seismic Data DolphinDB Script\nDESCRIPTION: Defines four stream tables for receiving continuous real-time streaming data related to seismic sampling, MiniSeed metadata, delay calculation results, and abnormal event alerts. The script removes existing streams and their subscriptions to ensure a clean setup. It creates stream tables with specified columns and data types, then enables table sharing and persistence with asynchronous writing and compression enabled. This setup facilitates efficient real-time processing, subscription, and storage of seismic event streams.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Seismic_waveform_data_storage.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//删除流表及流数据引擎\nunsubscribeTable(tableName = `dataStream,actionName = `append_data_into_dfs)\nunsubscribeTable(tableName = `dataStream,actionName = `abnormalDetect)\nunsubscribeTable(tableName = `metaStream,actionName = `calculate_delay)\nunsubscribeTable(tableName = `delayStream,actionName = `append_delay_into_dfs) \ntry{ dropStreamTable(`metaStream) }catch(ex){ print(ex) }\ntry{ dropStreamTable(`dataStream) }catch(ex){ print(ex) }\ntry{ dropStreamTable(`delayStream) }catch(ex){ print(ex) }\ntry{ dropStreamEngine(`engine) }catch(ex){ print(ex) }\ntry{ dropStreamTable(`abnormalStream) }catch(ex){ print(ex) } \n\n  //创建建立接收实时流数据的流数据表\nst1 = streamTable(1000000:0,`id`tagid`startTime`receivedTime`actualCount`expectedCount`sampleRate,[INT,SYMBOL,TIMESTAMP,TIMESTAMP,INT,INT,DOUBLE])\nst2 = streamTable(1000000:0,`id`ts`data,[INT,TIMESTAMP,INT])\nst3 = streamTable(1000000:0,`id`tagid`startTime`receivedTime`delay,[INT,SYMBOL,TIMESTAMP,TIMESTAMP,INT])\nst4 = streamTable(10000:0,`Time`id`anomalyType`anomalyString,[TIMESTAMP,STRING,INT,STRING])\nenableTableShareAndPersistence(table=st1, tableName=`metaStream, asynWrite=true, compress=true, cacheSize=2000000, preCache = 100000)\nenableTableShareAndPersistence(table=st2, tableName=`dataStream, asynWrite=true, compress=true, cacheSize=2000000000, preCache = 100000)\nenableTableShareAndPersistence(table=st3, tableName=`delayStream, asynWrite=true, compress=true, cacheSize=2000000, preCache = 100000)\nenableTableShareAndPersistence(table=st4, tableName=`abnormalStream, asynWrite=true, compress=true, cacheSize=2000000, preCache = 100000)\n```\n\n----------------------------------------\n\nTITLE: Skipping Rows While Preserving Schema in DolphinDB\nDESCRIPTION: This script demonstrates how to skip header rows using `skipRows` while preserving the original column names. It first extracts the schema using `extractTextSchema`, then passes this schema to the `loadText` function via the `schema` parameter along with `skipRows`. This ensures the correct column names are applied even when the header row is skipped.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_csv.md#_snippet_3\n\nLANGUAGE: dolphindb\nCODE:\n```\nschema=extractTextSchema(dataFilePath)\ntmpTB=loadText(filename=dataFilePath,schema=schema,skipRows=1000)\nselect count(*) from tmpTB;\n\nselect top 5 * from tmpTB;\n```\n\n----------------------------------------\n\nTITLE: Calculate Price Sensitivity Regression Coefficient in Python with DolphinDB and Pandas\nDESCRIPTION: Defines a Python function `priceSensitivityOrderFlowImbalance` to calculate the regression coefficient (beta) between price change (ΔP, scaled by 10000) and first-level order flow imbalance (NVOL = BidQty1 - AskQty1) using Level 2 snapshot data. It loads data lazily from DolphinDB, calculates price differences using `diff()`, extracts first-level quantities from Array Vector columns ('BidOrderQty', 'OfferOrderQty') using `.values[0]`, and computes the beta coefficient using DolphinDB's native `beta` function accessed via the `.values` attribute. Demonstrates handling lazy vs. non-lazy data interaction using `compute()` and performs grouped calculations with `groupby.apply`. Requires `pandas` and `dolphindb` libraries.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_Python_Parser_Intro_for_Quantitative_Finance.md#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nimport pandas as pd\nimport dolphindb as ddb\n\n# 定义因子函数\ndef priceSensitivityOrderFlowImbalance(df):  \n    deltaP = 10000*df[\"LastPrice\"].diff().fillna(0)\n    bidQty1 = df[\"BidOrderQty\"].values[0]\n    askQty1 = df[\"OfferOrderQty\"].values[0]\n    NVOL = bidQty1 - askQty1\n    res = beta(deltaP.values, NVOL)\n    return pd.Series([res], [\"priceSensitivityOrderFlowImbalance\"])\n\n# 指定计算某一天一只股票的因子\nsnapshotTB = loadTable(\"dfs://TL_Level2\", \"snapshot\")\ndf = pd.DataFrame(snapshotTB, index=\"Market\", lazy=True)\ndf = df[(df[\"TradeTime\"].astype(ddb.DATE)==2023.02.01)&(df[\"SecurityID\"]==\"000001\")]\nres = priceSensitivityOrderFlowImbalance(df.compute())\n  \n# 指定计算某一天的因子\nsnapshotTB = loadTable(\"dfs://TL_Level2\", \"snapshot\")\ndf = pd.DataFrame(snapshotTB, index=\"Market\", lazy=True)\nres = df[df[\"TradeTime\"].astype(ddb.DATE)==2023.02.01][[\"SecurityID\", \"LastPrice\", \"BidOrderQty\", \"OfferOrderQty\"]].groupby([\"SecurityID\"]).apply(priceSensitivityOrderFlowImbalance) \n```\n\n----------------------------------------\n\nTITLE: Creating DolphinDB Database and Partitioned Table for Persisting K-Line Data\nDESCRIPTION: This function creates a DolphinDB database and partitioned table to permanently store synthesized 1-minute K-line data. It checks for database existence to avoid repeated creation, then creates a VALUE partitioned table partitioned by the trade time column. The table schema defines columns for security identification, timestamps, prices, volumes, trade counts, and other relevant market data fields. The function returns the partitioned table schema column definitions.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/K.md#_snippet_18\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef createStockFundOHLCDfsTB(dbName, tbName){\n\tif(existsDatabase(dbUrl=dbName)){\n\t\tprint(dbName + \" has been created !\")\n\t\tprint(tbName + \" has been created !\")\n\t}\n\telse{\n\t\tdb = database(dbName, VALUE, 2021.01.01..2021.12.31)\n\t\tprint(dbName + \" created successfully.\")\n\t\tcolNames = `SecurityID`TradeTime`OpenPrice`HighPrice`LowPrice`ClosePrice`Volume`Turnover`TradesCount`PreClosePrice`PreCloseIOPV`IOPV`UpLimitPx`DownLimitPx`ChangeRate\n\t\tcolTypes = [SYMBOL, TIMESTAMP, DOUBLE, DOUBLE, DOUBLE, DOUBLE, LONG, DOUBLE, INT, DOUBLE, DOUBLE, DOUBLE, DOUBLE, DOUBLE, DOUBLE]\n\t\tschemaTable = table(1:0, colNames, colTypes)\n\t\tdb.createPartitionedTable(table=schemaTable, tableName=tbName, partitionColumns=`TradeTime)\n\t\tprint(tbName + \" created successfully.\")\n\t}\n\treturn loadTable(dbName, tbName).schema().colDefs\n}\ndbName = \"dfs://stockFundStreamOHLC\"\ntbName = \"stockFundStreamOHLC\"\ncreateStockFundOHLCDfsTB(dbName, tbName)\n```\n\n----------------------------------------\n\nTITLE: Example SQL Query Splitting for Grouping Data in DolphinDB (SQL)\nDESCRIPTION: Demonstrates the principle of splitting a base SQL query into multiple subqueries based on specified group column and scheme values. This facilitates group-wise data loading for time series, e.g., loading per-stock data individually, avoiding cross-group mixing during training. This splitting is handled internally by the DDBDataLoader to generate multiple queries filtering on groupCol values.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_AI_DataLoader_for_Deep_Learning.md#_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nselect * from loadTable(dbName, tableName) where stockID = \"apple\"\nselect * from loadTable(dbName, tableName) where stockID = \"google\"\nselect * from loadTable(dbName, tableName) where stockID = \"amazon\"\n```\n\n----------------------------------------\n\nTITLE: Calculating Implied Volatility with JIT in DolphinDB Script\nDESCRIPTION: Defines JIT-compiled functions to calculate the Black-Scholes option price (`GBlackScholes`) and iteratively find the implied volatility (`ImpliedVolatility`) using a bisection method. A `test_jit` function applies this calculation element-wise to input arrays, leveraging JIT. Includes setup for test data and timer comparisons against a non-JIT version, highlighting performance benefits for non-vectorizable computations.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/jit.md#_snippet_0\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\n@jit\ndef GBlackScholes(future_price, strike, input_ttm, risk_rate, b_rate, input_vol, is_call) {\n  ttm = input_ttm + 0.000000000000001;\n  vol = input_vol + 0.000000000000001;\n\n  d1 = (log(future_price/strike) + (b_rate + vol*vol/2) * ttm) / (vol * sqrt(ttm));\n  d2 = d1 - vol * sqrt(ttm);\n\n  if (is_call) {\n    return future_price * exp((b_rate - risk_rate) * ttm) * cdfNormal(0, 1, d1) - strike * exp(-risk_rate*ttm) * cdfNormal(0, 1, d2);\n  } else {\n    return strike * exp(-risk_rate*ttm) * cdfNormal(0, 1, -d2) - future_price * exp((b_rate - risk_rate) * ttm) * cdfNormal(0, 1, -d1);\n  }\n}\n\n@jit\ndef ImpliedVolatility(future_price, strike, ttm, risk_rate, b_rate, option_price, is_call) {\n  high=5.0;\n  low = 0.0;\n\n  do {\n    if (GBlackScholes(future_price, strike, ttm, risk_rate, b_rate, (high+low)/2, is_call) > option_price) {\n      high = (high+low)/2;\n    } else {\n      low = (high + low) /2;\n    }\n  } while ((high-low) > 0.00001);\n\n  return (high + low) /2;\n}\n\n@jit\ndef test_jit(future_price, strike, ttm, risk_rate, b_rate, option_price, is_call) {\n\tn = size(future_price)\n\tret = array(DOUBLE, n, n)\n\ti = 0\n\tdo {\n\t\tret[i] = ImpliedVolatility(future_price[i], strike[i], ttm[i], risk_rate[i], b_rate[i], option_price[i], is_call[i])\n\t\ti += 1\n\t} while(i < n)\n\treturn ret\n}\n\nn = 100000\nfuture_price=take(rand(10.0,1)[0], n)\nstrike_price=take(rand(10.0,1)[0], n)\nstrike=take(rand(10.0,1)[0], n)\ninput_ttm=take(rand(10.0,1)[0], n)\nrisk_rate=take(rand(10.0,1)[0], n)\nb_rate=take(rand(10.0,1)[0], n)\nvol=take(rand(10.0,1)[0], n)\ninput_vol=take(rand(10.0,1)[0], n)\nmulti=take(rand(10.0,1)[0], n)\nis_call=take(rand(10.0,1)[0], n)\nttm=take(rand(10.0,1)[0], n)\noption_price=take(rand(10.0,1)[0], n)\n\ntimer(10) test_jit(future_price, strike, ttm, risk_rate, b_rate, option_price, is_call)          //  2621.73 ms\ntimer(10) test_non_jit(future_price, strike, ttm, risk_rate, b_rate, option_price, is_call)      //   302714.74 ms\n```\n\n----------------------------------------\n\nTITLE: Proper Cleanup When Abandoning Segmented Reads in DolphinDB Java API\nDESCRIPTION: This Java snippet demonstrates the correct use of the skipAll method on EntityBlockReader to abandon unfinished segmented reads. This prevents socket buffer obstruction that causes deserialization failures in later reads. The snippet shows running a segmented query, reading the first chunk, calling skipAll to discard remaining data, then running another query successfully. This usage avoids exceptions caused by residual unread data. Dependencies are DolphinDB Java API classes DBConnection, EntityBlockReader, and BasicTable. Inputs are database connection parameters and query strings; no outputs aside from successful resource cleanup and query execution.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/iot_examples.md#_snippet_23\n\nLANGUAGE: java\nCODE:\n```\nEntityBlockReader v = (EntityBlockReader)conn.run(\"select * from loadTable('dfs://svmDemo','sensors') where datetime between 2020.09.01T00:00:00 : 2020.09.07T23:59:59\",(ProgressListener) null,4,4,10000);\nBasicTable data = (BasicTable)v.read();\nv.skipAll();\nBasicTable t1 = (BasicTable)conn.run(\"select * from loadTable('dfs://svmDemo','sensors') where datetime between 2020.09.01T00:00:00 : 2020.09.03T23:59:59\"); //若没有 skipAll 此段会抛出异常。\n```\n\n----------------------------------------\n\nTITLE: Modifying DolphinDB Agent Node Configuration in Java-Like Parameter Format\nDESCRIPTION: This snippet demonstrates the parameter settings for the DolphinDB proxy (agent) node within the agent.cfg file. Key parameters include the operational mode ('agent'), the local node endpoint with IP, port, and alias (localSite), the controller node endpoint (controllerSite), and resource allocation settings such as worker number (workerNum) and maximum memory size (maxMemSize in GB). It requires consistency between controllerSite in agent.cfg and localSite in controller.cfg to ensure proper cluster communication. Users should adapt the parameters based on their hardware and deployment topology.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/single_machine_cluster_deploy.md#_snippet_19\n\nLANGUAGE: Java\nCODE:\n```\nmode=agent\nlocalSite=localhost:8901:agent1\ncontrollerSite=localhost:8900:controller8900\nworkerNum=4\nmaxMemSize=4\nlanCluster=0\n```\n\n----------------------------------------\n\nTITLE: Automatic NSQ Plugin Loading Configuration (DolphinDB Script)\nDESCRIPTION: This script content should be placed in the `startup.dos` file referenced by the `startup` parameter in `dolphindb.cfg`. It ensures the NSQ plugin is automatically loaded (with error handling) when the DolphinDB server starts.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_implementation_for_NSQ_Plugin.md#_snippet_4\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\ntry{loadPlugin(\"/DolphinDB/server/plugins/nsq/PluginNsq.txt\")} catch(ex){print(ex)}\n```\n\n----------------------------------------\n\nTITLE: Registering 1-Minute Window Daily Time Series Engine in DolphinDB Script\nDESCRIPTION: This snippet creates a `DailyTimeSeriesEngine` for rolling window aggregation of 1-minute K-line metrics. It defines aggregation methods to compute OHLC values and sums for volume, turnover, and trade counts over fixed 1-minute windows with a step of 60,000 milliseconds. Additional parameters control forced trigger timings to handle inactive securities and session boundary cases, ensuring timely and complete bar output. The engine subscribes to the processed snapshot table for its input data stream.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/K.md#_snippet_17\n\nLANGUAGE: DolphinDB\nCODE:\n```\nmdlStockFundOHLCTempEngineName = \"mdlStockFundOHLCTempEngine\"\n//Define engine calculation methods\nbarConvert = <[\n\tfirstNot(LastPrice, 0),\n\thigh(DeltasHighPrice, HighPrice, LastPrice),\n\tlow(DeltasLowPrice, LowPrice, LastPrice),\n\tlastNot(LastPrice, 0),\n\tsum(DeltasVolume),\n\tsum(DeltasTurnover),\n\tsum(DeltasTradesCount),\n\tfirst(PreCloPrice),\n\tfirst(PreCloseIOPV),\n\tlastNot(IOPV, 0),\n\tlast(UpLimitPx),\n\tlast(DownLimitPx),\n\tlastNot(LastPrice, 0)\\firstNot(LastPrice, 0)-1\n]>\n//Define engine fill methods\nfillList = [0, 0, 0, 'ffill', 0, 0, 0, 'ffill', 'ffill', 'ffill', 'ffill', 'ffill', 0]\ncreateDailyTimeSeriesEngine(\n\tname=mdlStockFundOHLCTempEngineName,\n\twindowSize=60000,\n\tstep=60000,\n\tmetrics=barConvert,\n\tdummyTable=objByName(mdlSnapshotProcessTBName),\n\toutputTable=getStreamEngine(mdlStockFundOHLCEngineName),\n\ttimeColumn=`TradeTime,\n\tkeyColumn=`SecurityID,\n\tuseWindowStartTime=true,\n\tforceTriggerTime=1000,\n\tfill=fillList,\n\tsessionBegin=09:30:00.000 13:00:00.000 15:00:00.000,\n\tsessionEnd=11:31:00.000 14:58:00.000 15:01:00.000,\n\tmergeSessionEnd=true,\n\tforceTriggerSessionEndTime=30000)\n//Subscribe to the processed snapshot table, input incremental data into the DailyTimeSeriesEngine of mdlStockFundOHLCTempEngineName\nsubscribeTable(\n\ttableName=mdlSnapshotProcessTBName,\n\tactionName=mdlStockFundOHLCTempEngineName,\n\thandler=getStreamEngine(mdlStockFundOHLCTempEngineName),\n\tmsgAsTable=true,\n\tbatchSize=100,\n\tthrottle=0.01,\n\thash=0,\n\treconnect=true)\n```\n\n----------------------------------------\n\nTITLE: Selecting Unique Values with Distinct Keyword in DolphinDB SQL\nDESCRIPTION: Illustrates usage of the distinct keyword in select/exec statements to eliminate duplicate rows and retrieve unique values. It explains limitations regarding combined usage with group by, context by, or pivot by clauses. The examples include usage as a keyword and function, the latter renaming output columns with 'distinct_colName'. Multi-column distinct selection and aggregation querying distinct counts are also demonstrated. Applicable to distributed queries with restrictions.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Standard_SQL_in_DolphinDB.md#_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nselect distinct COUNTRY_ID from locations\n```\n\nLANGUAGE: SQL\nCODE:\n```\nselect distinct(COUNTRY_ID) from locations\n```\n\nLANGUAGE: SQL\nCODE:\n```\nselect count(distinct JOB_ID) from employees\n```\n\nLANGUAGE: SQL\nCODE:\n```\nselect distinct DEPARTMENT_ID, MANAGER_ID from employees\n```\n\n----------------------------------------\n\nTITLE: Creating Minute Aggregations from Snapshot Data in DolphinDB\nDESCRIPTION: Aggregates tick-level snapshot data into minute OHLC bars with volume and VWAP calculations. Groups data by security ID and minute time bars to generate minute-level pricing data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practice_for_factor_calculation.md#_snippet_8\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//基于快照因子的分钟聚合OHLC，vwap\ntick_aggr = select first(LastPx) as open, max(LastPx) as high, min(LastPx) as low, last(LastPx) as close, sum(totalvolumetrade) as vol,sum(lastpx*totalvolumetrade) as val,wavg(lastpx, totalvolumetrade) as vwap from loadTable(\"dfs://LEVEL2_Snapshot_ArrayVector\",\"Snap\") where date(TradeTime) <= 2020.01.30 and date(TradeTime) >= 2020.01.01 group by SecurityID, bar(TradeTime,1m)\n```\n\n----------------------------------------\n\nTITLE: Batch Writing Data into DolphinDB using Java API\nDESCRIPTION: This Java example shows the process of connecting to a DolphinDB server and constructing a batch insert operation. It creates column name lists and corresponding Java ArrayLists of data, wraps them into BasicVector subclasses, assembles a BasicTable, and uses the tableInsert function to write the data to distributed tables. The example handles IOExceptions during connect and run calls. Dependencies include DolphinDB Java API and standard Java collections. Inputs are connection credentials, database table path/name, and data columns. Output is the writing of batch data to the DolphinDB server.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/iot_examples.md#_snippet_24\n\nLANGUAGE: java\nCODE:\n```\nDBConnection conn = new DBConnection();\ntry{\n    conn.connect(SERVER, PORT, USER, PASSWORD);\n    List<String> colNames =  Arrays.asList(\"id\",\"datetime\",\"value\")\n    ArrayList<Integer> idArray = new ArrayList<Integer>(Arrays.asList(1,2));\n    ArrayList<Long> datetimeArray = new ArrayList<Long>(Arrays.asList(1600735563458l,16007355634582l));\n    ArrayList<Double> valueArray = new ArrayList<Double>(Arrays.asList(22.3,43.1));\n    List<Vector> cols = Arrays.asList(new BasicIntVector(idArray),new BasicTimestampVector(datetimeArray),new BasicDoubleVector(valueArray));            \n\tBasicTable data = new BasicTable(colNames,cols);\n\n    List<Entity> args = new ArrayList<Entity>(1);\n    args.add(data);\n\tconn.run(String.format(\"tableInsert{loadTable('%s','%s')}\",dbPath,tableName), args);\n        \n}catch (IOException ex){\n   ex.printStackTrace();\n}\n```\n\n----------------------------------------\n\nTITLE: Applying High-Order Functions to Compute Returns and Correlation Matrix in DolphinDB - DolphinDB\nDESCRIPTION: Transforms the price pivot matrix into a return matrix by applying 'ratios' function to each column using the 'each' higher-order function, then subtracts one to get percentage returns. Uses the 'cross' higher-order function combined with 'corr' to compute pairwise correlation coefficients between every two stock return vectors, resulting in a correlation matrix. This snippet illustrates the use of higher-order and functional programming paradigms for quantitative financial computations on large datasets.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/hybrid_programming_paradigms.md#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\nretMatrix = each(ratios, priceMatrix) - 1\n\ncorrMatrix = cross(corr, retMatrix, retMatrix)\n```\n\n----------------------------------------\n\nTITLE: Parallel Factor Calculation with Map Reduce\nDESCRIPTION: This code snippet illustrates using the `mr` (map reduce) function for parallel factor calculation. It first repartitions the data based on `securityid` using `repartitionDS`, and then applies a custom map function (`factorDoubleEMAMap`) to each partition to calculate the `factorDoubleEMA` factor.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practice_for_factor_calculation.md#_snippet_31\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//将原数据按股票重新10个HASH分区\nds = repartitionDS(<select * from loadTable(\"dfs://k_minute_level\", \"k_minute\") where date(tradetime) between 2020.01.02 : 2020.03.31>, `securityid, HASH,10)\n\ndef factorDoubleEMAMap(table){\n\treturn select tradetime, securityid, `doubleEMA as factorname, factorDoubleEMA(close) as val from table context by securityid map\n}\n\nres = mr(ds,factorDoubleEMAMap,,unionAll)\n```\n\n----------------------------------------\n\nTITLE: Parallel Job Definition for Fund Performance Analysis Using DolphinDB\nDESCRIPTION: Defines parJob2 that loads fund data from distributed file systems, joins fund and benchmark prices, preprocesses data with forward filling missing values, subsets symbols by data length, computes daily log returns, and executes parallel loops for factor calculation and regression analysis. It uses timer blocks to measure execution time and returns factor results extracted from OLS regressions. The function orchestrates data preparation and statistical computation in parallel.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_factor_by_ddb/fund_factor_ten.txt#_snippet_11\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef parJob2(){\n\ttimer{fund_OLAP=select * from loadTable(\"dfs://fund_OLAP\", \"fund_OLAP\")\n\t          fund_hs_OLAP=select * from loadTable(\"dfs://fund_OLAP\", \"fund_hs_OLAP\")\n\t          ajResult = select Tradedate, fundNum, value, fund_hs_OLAP.Tradedate as hsTradedate, fund_hs_OLAP.value as price from aj(fund_OLAP, fund_hs_OLAP, `Tradedate)\n\t          result2 = select Tradedate, fundNum, iif(isNull(value), ffill!(value), value) as value,price from ajResult where Tradedate == hsTradedate\n                      symList = exec distinct(fundNum) as fundNum from result2 order by fundNum\n                      symList2 = symList.cut(250)\n\t          portfolio = select fundNum as fundNum, (deltas(value)\\prev(value)) as log, TradeDate as TradeDate from result2 where TradeDate in 2018.05.24..2021.05.27 and fundNum in symList\n                      m_log = exec log from portfolio pivot by TradeDate, fundNum\n                      mlog =  m_log[1:,]\n                      knum = 2..365\n}\n    timer{ploop(getFactor{result2}, symList2)\n          a = ploop(calAllRs2{mlog,symList}, knum).unionAll(false)\n          res2 = select fundNum, ols(factor1, kNum)[0] as hist, ols(factor1, kNum)[1] as hist2, ols(factor1, kNum)[2] as hist3 from a group by fundNum}\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Database for Futures Market Data with TSDB Engine\nDESCRIPTION: Creates a database for storing futures market data using a combined partitioning strategy with daily value partitioning and HASH 10 on instrument IDs. The TSDB engine is used with instrument ID and received time as sorting columns for optimal performance.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practices_for_partitioned_storage.md#_snippet_12\n\nLANGUAGE: SQL\nCODE:\n```\ncreate database \"dfs://ctp_futures\"\npartitioned by VALUE(2020.01.01..2021.01.01), HASH([SYMBOL, 10])\nengine='TSDB'\n\ncreate table \"dfs://ctp_futures\".\"futures\"(\n    TradingDay DATE[comment=\"交易日期\", compress=\"delta\"]\n    ExchangeID SYMBOL\n    LastPrice DOUBLE\n    PreSettlementPrice DOUBLE\n    PreClosePrice DOUBLE\n    PreOpenInterest DOUBLE\n    OpenPrice DOUBLE\n    HighestPrice DOUBLE\n    LowestPrice DOUBLE\n    Volume INT\n    Turnover DOUBLE\n    OpenInterest DOUBLE\n    ClosePrice DOUBLE\n    SettlementPrice DOUBLE\n    UpperLimitPrice DOUBLE\n    LowerLimitPrice DOUBLE\n    PreDelta DOUBLE\n    CurrDelta DOUBLE\n    UpdateTime SECOND\n    UpdateMillisec INT\n    BidPrice DOUBLE[]\n    BidVolume INT[]\n    AskPrice DOUBLE[]\n    AskVolume INT[]\n    AveragePrice DOUBLE\n    ActionDay DATE\n    InstrumentID SYMBOL\n    ExchangeInstID STRING\n    BandingUpperPrice DOUBLE\n    BandingLowerPrice DOUBLE\n    tradeTime TIME\n    receivedTime NANOTIMESTAMP\n    perPenetrationTime LONG\n)\npartitioned by TradingDay, InstrumentID,\nsortColumns=[`InstrumentID,`ReceivedTime],\nkeepDuplicates=ALL\n```\n\n----------------------------------------\n\nTITLE: Importing Data to Database Using MR Function\nDESCRIPTION: This code uses the `mr` (MapReduce) function to import data from the text file chunks into the database.  It specifies the data source as the output of `textChunkDS`, the map function as the `divideImport` function (with partial application to bind tb1 and tb2), and sets `parallel` to false to prevent concurrent writes to the same partition. The `divideImport` function sorts data to appropriate tables.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_csv.md#_snippet_29\n\nLANGUAGE: DolphinDB\nCODE:\n```\nmr(ds=ds, mapFunc=divideImport{,tb1,tb2}, parallel=false);\n```\n\n----------------------------------------\n\nTITLE: Stream Processing of Net Buy Order Amount Differential in DolphinDB\nDESCRIPTION: This example illustrates how to implement real-time calculation of the net buy order amount differential factor using DolphinDB's stream processing engine. It creates input/output tables, sets up a reactive state engine, and simulates streaming data for testing.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Array_Vector.md#_snippet_34\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 创建输入输出表\nshare(streamTable(1:0, snapshot.schema().colDefs.name, snapshot.schema().colDefs.typeString), `snapshotStreamTable)\nshare(streamTable(1:0, `SecurityID`DateTime`amtDiff, [SYMBOL, TIMESTAMP, DOUBLE]), `res2)\ngo\n// 创建流计算引擎\ncreateReactiveStateEngine(name=\"calAmtDiffDemo\", metrics=<[DateTime, calculateAmtDiff(BidPrice, OfferPrice, BidOrderQty, OfferOrderQty)]>, dummyTable=snapshotStreamTable, outputTable=res2, keyColumn=`SecurityID)\n// 创建订阅\nsubscribeTable(tableName=\"snapshotStreamTable\", actionName=\"calAmtDiffTest\", offset=-1, handler=getStreamEngine(\"calAmtDiffDemo\"), msgAsTable=true)\n// 取数据回放，模拟流数据\ntestData = select * from snapshot where date(DateTime)=2021.12.01 order by DateTime\nsubmitJob(\"replayData\", \"replay snapshot data\", replay{inputTables=testData, outputTables=snapshotStreamTable, dateColumn=`DateTime, timeColumn=`DateTime, replayRate=1000})\n```\n\n----------------------------------------\n\nTITLE: Subscribe to Stream and Print - Python\nDESCRIPTION: This Python code subscribes to the DolphinDB stream table 'OHLC' using the Python API. It enables streaming on local port 20001, defines a handler function to print received data, and then establishes a subscription to the OHLC table on the DolphinDB server (127.0.0.1:8848).  The subscription is named 'python_api_subscribe', starts from the current offset (0), and uses an event loop to keep the subscription active.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/OHLC.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nfrom threading import Event\nimport dolphindb as ddb\nimport pandas as pd\nimport numpy as np\ns=ddb.session()\n#设定本地端口20001用于订阅流数据\ns.enableStreaming(20001)\ndef handler(lst):         \n    print(lst)\n# 订阅DolphinDB(本机8848端口)上的OHLC流数据表\ns.subscribe(\"127.0.0.1\", 8848, handler, \"OHLC\",\"python_api_subscribe\",0)\nEvent().wait() \n```\n\n----------------------------------------\n\nTITLE: Loading and Transforming Single CSV File in DolphinDB\nDESCRIPTION: Defines a function `loadOneFile` to load data from a single CSV file (`csvFile`) using a provided schema (`schema1`). It derives 'market' (first 2 chars of Symbol) and modifies 'Symbol' (remaining chars), calculates incremental 'Volume' and 'Amount' using `eachPre` based on cumulative values in the CSV, reorders columns to match the target table schema (`orderbooktb`), and returns the processed in-memory table. Requires the target table schema for column reordering.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/csvImportDemo/importNewData.txt#_snippet_1\n\nLANGUAGE: dolphindb\nCODE:\n```\ndef loadOneFile(csvFile,orderbooktb,schema1){\n\tt = loadText(csvFile,,schema1)\n\tt[\"market\"] = left(t[\"Symbol\"],2)\n\tt[\"Symbol\"] = substr(t[\"Symbol\"],2)\n\tt[\"Volume\"] = eachPre(-, t[\"volume\"], 0)\n\tt[\"Amount\"] = eachPre(-, t[\"Amount\"], 0)\n\tt.reorderColumns!(orderbooktb.schema().colDefs[`name])\n\treturn t\n}\n```\n\n----------------------------------------\n\nTITLE: Batch Writing Data into DolphinDB using Python API\nDESCRIPTION: This Python snippet demonstrates connecting to a DolphinDB server and performing batch data insertion using the Python API. It organizes data into a dictionary of NumPy arrays, converts it into a pandas DataFrame, then uses the tableInsert function inside a run call to upload the batch data. Exception handling captures run errors. Dependencies are dolphindb Python package, pandas, and numpy. Inputs are server connection info and data arrays; output is data written to distributed tables.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/iot_examples.md#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ns = ddb.session()\ns.connect(SERVER, PORT, USER, PASSWORD)\n\ndata = {'id': np.array([1, 2], dtype=np.int32),\n'datetime': np.array([1600735563458, 16007355634582]),\n'value': np.array([22.3, 43.1])}\ntry:\n\ts.run(\"tableInsert{{loadTable('{db}', `{tb})}}\".format(db=dbPath,tb=tableName), pd.DataFrame(data))\nexcept Exception as e:\n\tprint e\n```\n\n----------------------------------------\n\nTITLE: Defining DECIMAL32/DECIMAL64 Vectors Using bigarray, array, arrayVector - DolphinDB\nDESCRIPTION: Illustrates multiple methods to create DECIMAL32 and DECIMAL64 type vectors or large arrays in DolphinDB. Shows using bigarray() for large arrays with specific initial size, capacity, and default value, array() for fixed-size arrays, and arrayVector for arrays of decimal vectors. All elements share the same scale and type. Demonstrates appending values and converting numeric lists to DECIMAL with scale.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DECIMAL.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\nx=bigarray(DECIMAL32(3),0,10000000);\nx.append!(1..1000)\n//output:[1.000,2.000,3.000,4.000,5.000,6.000,7.000,8.000,9.000,10.000]\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nx=array(DECIMAL32(3), 10, 10, 2.3)\n//output: [2.300,2.300,2.300,2.300,2.300,2.300,2.300,2.300,2.300,2.300]\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nm=[decimal32(1.2356, 3), decimal32(2.59874, 3), decimal32(-5.23564, 3)]\nn=decimal32([1.2356, 2.59874, -5.23564], 3)\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nx = array(DECIMAL32(5)[], 0, 10)\nval1 = [1.77, 2.8, -3.77, -3.77, 77.32, 1.77]\nval2 = [1.77, 2.8, NULL, -3.77, -3.77, 77.32, 1.77, NULL]\nx.append!([val1, val2])\n//output:[[1.77000,2.80000,-3.77000,-3.77000,77.31999,1.77000],[1.77000,2.80000,,-3.77000,-3.77000,77.31999,1.77000,]]\n```\n\n----------------------------------------\n\nTITLE: Creating a Table and Populating Data\nDESCRIPTION: This code creates a database and a partitioned table, then populates the table with sample data. This setup is used in the subsequent examples to demonstrate how to analyze the SQL execution plan.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_Explain.md#_snippet_1\n\nLANGUAGE: DolphinDB SQL\nCODE:\n```\n// valuedb 数据库\nn=1000000\nmonth=take(2000.01M..2016.12M, n)\nx=rand(1.0, n)\nid = rand(1..9,n)\nt=table(month, x, id)\n\ndb=database(\"dfs://valuedb\", VALUE, 2000.01M..2016.12M)\npt = db.createPartitionedTable(t, `pt, `month)\npt.append!(t)\n```\n\n----------------------------------------\n\nTITLE: Defining Distributed Partitioned Table Schema in DolphinDB Script\nDESCRIPTION: This DolphinDB snippet logs into the server and defines a distributed database with composite partitioning on date and symbol columns. It explicitly creates a table schema with detailed columns relevant to level 2 market quotes, including symbol, market, date, time, prices, volumes, and other market metrics, then creates a partitioned distributed table named `quotes` using the defined schema. Required dependencies include DolphinDB server connection with valid login. The input is no data but prepares the distributed table structure for subsequent data import. This setup is necessary because binary imports require explicit distributed table definitions.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/quant_finance_examples.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(\"admin\", \"123456\")\ndbDate = database(\"\", VALUE, 2020.01.01..2020.12.31)\ndbSymbol=database(\"\", HASH, [SYMBOL, 10])\ndb = database(\"dfs://level2\", COMPO, [dbDate, dbSymbol])\nschemaTable=table(\t\n\tarray(SYMBOL,0) as  symbol,\n\tarray(SYMBOL,0) as  market,\n\tarray(DATE,0) as  date,\n\tarray(TIME,0) as  time,\n\tarray(DOUBLE,0) as  preClose,\n\tarray(DOUBLE,0) as  open,\n\tarray(DOUBLE,0) as  high,\n\tarray(DOUBLE,0) as  low,\n\tarray(DOUBLE,0) as  last,\n\tarray(INT,0) as  numTrades,\n\tarray(INT,0) as  curNumTrades,\n\tarray(INT,0) as  volume,\n\tarray(INT,0) as  curVol,\n\tarray(DOUBLE,0) as  turnover,\n\tarray(INT,0) as  curTurnover,\n\tarray(INT,0) as  peratio1,\n\tarray(INT,0) as  peratio2,\n\tarray(INT,0) as  totalAskVolume,\n\tarray(DOUBLE,0) as  wavgAskPrice,\n\tarray(INT,0) as  askLevel,\n\tarray(INT,0) as  totalBidVolume,\n\tarray(DOUBLE,0) as  wavgBidPrice,\n\tarray(INT,0) as  bidLevel,\n\tarray(DOUBLE,0) as  iopv,\n\tarray(INT,0) as  ytm,\n\tarray(DOUBLE,0) as  askPrice1,\n\tarray(DOUBLE,0) as  askPrice2,\n\tarray(DOUBLE,0) as  askPrice3,\n\tarray(DOUBLE,0) as  askPrice4,\n\tarray(DOUBLE,0) as  askPrice5,\n\tarray(DOUBLE,0) as  askPrice6,\n\tarray(DOUBLE,0) as  askPrice7,\n\tarray(DOUBLE,0) as  askPrice8,\n\tarray(DOUBLE,0) as  askPrice9,\n\tarray(DOUBLE,0) as  askPrice10,\n\tarray(DOUBLE,0) as  bidPrice1,\n\tarray(DOUBLE,0) as  bidPrice2,\n\tarray(DOUBLE,0) as  bidPrice3,\n\tarray(DOUBLE,0) as  bidPrice4,\n\tarray(DOUBLE,0) as  bidPrice5,\n\tarray(DOUBLE,0) as  bidPrice6,\n\tarray(DOUBLE,0) as  bidPrice7,\n\tarray(DOUBLE,0) as  bidPrice8,\n\tarray(DOUBLE,0) as  bidPrice9,\n\tarray(DOUBLE,0) as  bidPrice10,\n\tarray(INT,0) as  askVolume1,\n\tarray(INT,0) as  askVolume2,\n\tarray(INT,0) as  askVolume3,\n\tarray(INT,0) as  askVolume4,\n\tarray(INT,0) as  askVolume5,\n\tarray(INT,0) as  askVolume6,\n\tarray(INT,0) as  askVolume7,\n\tarray(INT,0) as  askVolume8,\n\tarray(INT,0) as  askVolume9,\n\tarray(INT,0) as  askVolme10,\n\tarray(INT,0) as  bidVolume1,\n\tarray(INT,0) as  bidVolume2,\n\tarray(INT,0) as  bidVolume3,\n\tarray(INT,0) as  bidVolume4,\n\tarray(INT,0) as  bidVolume5,\n\tarray(INT,0) as  bidVolume6,\n\tarray(INT,0) as  bidVolume7,\n\tarray(INT,0) as  bidVolume8,\n\tarray(INT,0) as  bidVolume9,\n\tarray(INT,0) as  bidVolume10,\n\tarray(LONG,0) as  unixTime,\n\tarray(DOUBLE,0) as  upperLimit,\n\tarray(DOUBLE,0) as  lowerLimit\n)\ndb.createPartitionedTable(schemaTable,`quotes,`date`symbol)\n```\n\n----------------------------------------\n\nTITLE: Initializing DolphinDB Stock Daily Line Database and Calculating Pairwise Stock Correlations in DolphinDB\nDESCRIPTION: Defines a DolphinDB distributed database and partitioned table to store daily OHLC data for Shanghai and Shenzhen stocks from 2008 to 2022 based on Tushare's data format. Shows how to generate a stock return matrix using 'exec' combined with 'pivot by', then computes a pairwise Pearson correlation matrix using the high-order function 'cross'. Retrieves symbols and formats them for compatibility, then transforms the correlation matrix into a table for further queries, identifying the top 10 most correlated stocks for each stock. Dependencies include DolphinDB server access and Tushare data imported in the defined table. Input includes daily stock prices; output is a matrix/table of pairwise correlation coefficients.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/func_progr_cases.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(\"admin\",\"123456\")\ndbPath=\"dfs://tushare\"\nyearRange=date(2008.01M + 12*0..22)\nif(existsDatabase(dbPath)){\n\tdropDatabase(dbPath)\n}\ncolumns1=`ts_code`trade_date`open`high`low`close`pre_close`change`pct_chg`vol`amount\ntype1=`SYMBOL`NANOTIMESTAMP`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE\ndb=database(dbPath,RANGE,yearRange)\nhushen_daily_line=db.createPartitionedTable(table(100000000:0,columns1,type1),`hushen_daily_line,`trade_date)\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nretMatrix=exec pct_chg/100 as ret from daily_line pivot by trade_date, ts_code\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\ncorrMatrix=cross(corr,retMatrix)\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nsyms=(exec count(*) from daily_line group by ts_code).ts_code\nsyms=\"C\"+strReplace(syms, \".\", \"_\")\nmostCorrelated=select * from table(corrMatrix.columnNames() as ts_code, corrMatrix).rename!([`ts_code].append!(syms)).unpivot(`ts_code, syms).rename!(`ts_code`corr_ts_code`corr) context by ts_code having rank(corr,false) between 1:10\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(\"admin\",\"123456\")\ndaily_line= loadTable(\"dfs://tushare\",\"hushen_daily_line\")\n\nretMatrix=exec pct_chg/100 as ret from daily_line pivot by trade_date,ts_code\ncorrMatrix=cross(corr,retMatrix)\n\nsyms=(exec count(*) from daily_line group by ts_code).ts_code\nsyms=\"C\"+strReplace(syms, \".\", \"_\")\nmostCorrelated=select * from table(corrMatrix.columnNames() as ts_code, corrMatrix).rename!([`ts_code].append!(syms)).unpivot(`ts_code, syms).rename!(`ts_code`corr_ts_code`corr) context by ts_code having rank(corr,false) between 1:10\n```\n\n----------------------------------------\n\nTITLE: Backing Up Distributed DolphinDB Tables Using backup Function\nDESCRIPTION: This snippet shows how to use the DolphinDB backup function to perform a distributed backup of a table's data partitions filtered by a datetime range. The backup directory path includes a dynamically generated date string. Optional parameters such as force and parallel enable control over backup behavior and concurrency. The input is a SQL object selecting data from a loaded distributed table, and the output is backup files stored on disk as specified. The snippet assumes the backup directory exists and that permissions allow writing files. It supports incremental and full backup operations.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/iot_examples.md#_snippet_33\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nbackup(backupDir=\"/hdd/hdd1/backup/\"+today().format(\"yyyyMMdd\"),sqlObj=<select * from loadTable(\"dfs://svmDemo\",\"sensors\") where datetime between 2020.09.01T00:00:00 : 2020.09.03T23:59:59 >,force=false,parallel=true)\n```\n\n----------------------------------------\n\nTITLE: Creating Stream Tables with Persistence in DolphinDB Script\nDESCRIPTION: This function `createStreamTableFunc` creates three stream tables: `tradeOriginalStream`, `capitalFlowStream`, and `capitalFlowStream60min`. Each table is defined with specific column names and types reflecting market trade and capital flow data. The function sets table size limits, enables shared access and persistence with asynchronous write, compression, caching, retention policies, and pre-caching parameters for performance optimization. Exceptions during persistence enabling are caught and printed. After creation, temporary tables are undefined to free resources.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/streaming_capital_flow_daily/01.createStreamTB.txt#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//create stream table: tradeOriginalStream\ncolName = `SecurityID`Market`TradeTime`TradePrice`TradeQty`TradeAmount`BuyNum`SellNum\ncolType = [SYMBOL, SYMBOL, TIMESTAMP, DOUBLE, INT, DOUBLE, INT, INT]\ntradeOriginalStreamTemp = streamTable(20000000:0, colName, colType)\ntry{ enableTableShareAndPersistence(table=tradeOriginalStreamTemp, tableName=\"tradeOriginalStream\", asynWrite=true, compress=true, cacheSize=20000000, retentionMinutes=1440, flushMode=0, preCache=10000) } catch(ex){ print(ex) }\nundef(\"tradeOriginalStreamTemp\")\n//create stream table: capitalFlow\ncolName = `SecurityID`TradeTime`TotalAmount`SellSmallAmount`SellMediumAmount`SellBigAmount`SellSmallCount`SellMediumCount`SellBigCount`BuySmallAmount`BuyMediumAmount`BuyBigAmount`BuySmallCount`BuyMediumCount`BuyBigCount\ncolType =  [SYMBOL, TIMESTAMP, DOUBLE, DOUBLE, DOUBLE, DOUBLE, INT, INT, INT, DOUBLE, DOUBLE, DOUBLE, INT, INT, INT]\ncapitalFlowStreamTemp = streamTable(20000000:0, colName, colType)\ntry{ enableTableShareAndPersistence(table=capitalFlowStreamTemp, tableName=\"capitalFlowStream\", asynWrite=true, compress=true, cacheSize=20000000, retentionMinutes=1440, flushMode=0, preCache=10000) } catch(ex){ print(ex) }\nundef(\"capitalFlowStreamTemp\")\n//create stream table: capitalFlowStream60min\ncolName = `TradeTime`SecurityID`TotalAmount`SellSmallAmount`SellMediumAmount`SellBigAmount`SellSmallCount`SellMediumCount`SellBigCount`BuySmallAmount`BuyMediumAmount`BuyBigAmount`BuySmallCount`BuyMediumCount`BuyBigCount\ncolType =  [TIMESTAMP, SYMBOL, DOUBLE, DOUBLE, DOUBLE, DOUBLE, INT, INT, INT, DOUBLE, DOUBLE, DOUBLE, INT, INT, INT]\ncapitalFlowStream60minTemp = streamTable(1000000:0, colName, colType)\ntry{ enableTableShareAndPersistence(table=capitalFlowStream60minTemp, tableName=\"capitalFlowStream60min\", asynWrite=true, compress=true, cacheSize=1000000, retentionMinutes=1440, flushMode=0, preCache=10000) } catch(ex){ print(ex) }\nundef(\"capitalFlowStreamTemp\")\n```\n\n----------------------------------------\n\nTITLE: Covariance Matrix Calculation with cross function\nDESCRIPTION: This DolphinDB script calculates the covariance matrix of a matrix using the `cross` higher-order function. It applies the `covar` function to all pairs of columns in the matrix, providing a more concise and efficient way to compute the covariance matrix compared to using nested loops.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\ncross(covar, matt)\n```\n\n----------------------------------------\n\nTITLE: Inserting Rows into Array Vector Variables and Tables - DolphinDB Script\nDESCRIPTION: These examples detail how to insert new rows into Array Vector variables or tables containing Array Vector columns using the append! method or the tableInsert function. Both single and multiple rows can be appended. Requirements include an initialized variable or table structure; output is the expanded array/table. Modification or deletion of existing rows is not supported; only appending at the end is allowed.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Array_Vector.md#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 插入一行\nx = array(INT[], 0).append!([1 2 3, 4 5 6])\nx.append!(7)\nx.append!([8 9])\n/* x\n[[1,2,3],[4,5,6],[7],[8,9]]\n*/\n\ny = [1 2 3, 4 5 6].setColumnarTuple!()\ny.append!(7)\ny.append!([8 9])\n/* y\n([1,2,3],[4,5,6],7,[8,9])\n*/\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 插入多行\nx = array(INT[], 0).append!([1 2 3, 4 5 6])\nx.append!([7, 8 9])\n/* x\n[[1,2,3],[4,5,6],[7],[8,9]]\n*/\n\ny = [1 2 3, 4 5 6].setColumnarTuple!()\ny.append!([7, 8 9])\n/* y\n([1,2,3],[4,5,6],7,[8,9])\n*/\n```\n\n----------------------------------------\n\nTITLE: Loading Partitioned Table and Performing Queries on DolphinDB Compute Node (DolphinDB script)\nDESCRIPTION: In this DolphinDB script snippet executed on a compute node, a partitioned table object is loaded from the distributed database, which initially loads only metadata for efficient response times. Then, a SQL-style query is performed to count the number of records per day grouped by the date part of 'DateTime', with results displayed directly in the web interface. A second query computes OHLC (Open, High, Low, Close) values per stock per day, assigning the result to a server-side variable to reduce client memory usage and enabling paged display in the web interface. This snippet requires an active connection to a compute node with access to the distributed database.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/single_machine_cluster_deploy.md#_snippet_4\n\nLANGUAGE: DolphinDB script\nCODE:\n```\n// 加载分区表对象\npt = loadTable(\"dfs://testDB\", \"testTB\")\n```\n\nLANGUAGE: DolphinDB script\nCODE:\n```\n// SQL 返回数据量少的时候，可以直接取回客户端展示\nselect count(*) from pt group by date(DateTime) as Date\n```\n\nLANGUAGE: DolphinDB script\nCODE:\n```\n// SQL 返回数据量较大时，可以赋值给变量，占用 server 端内存，客户端分页取回展示\nresult = select first(LastPx) as Open, max(LastPx) as High, min(LastPx) as Low, last(LastPx) as Close from pt group by date(DateTime) as Date, SecurityID\n```\n\n----------------------------------------\n\nTITLE: Create Stream Table for OHLC Data - DolphinDB\nDESCRIPTION: Creates a shared stream table named `OHLC` to store Open, High, Low, Close, and Volume (OHLCV) data. The table is partitioned with an initial capacity of 100 and can grow dynamically. The columns include `datetime`, `Symbol`, `open`, `high`, `low`, `close`, and `volume` with corresponding data types.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/OHLC.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\nshare streamTable(100:0，`datetime`Symbol`open`high`low`close`volume,[DATETIME,SYMBOL,DOUBLE,DOUBLE,DOUBLE,DOUBLE,LONG]) as OHLC\n```\n\n----------------------------------------\n\nTITLE: 计算投资组合价值\nDESCRIPTION: 基于向前填充后的数据计算每个时间点的投资组合价值，将各股票的价格乘以相应的持仓数量并求和，得到每个时刻的投资组合总价值。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/quant_finance_examples.md#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect time, 100*C600000+200*C600300+400*C600400+800*C600500+600*C600600+400*C600800+300*C600900 from tmp;\n```\n\n----------------------------------------\n\nTITLE: Implementing linearreg_slope Function in DolphinDB\nDESCRIPTION: This DolphinDB script implements the `linearreg_slope` function. This function calculates the linear regression slope over a rolling window using vectorized operations. The implementation optimizes calculations by utilizing `mcount`, `msum`, and the `cumsum` function to minimize redundant computations associated with window sliding. This approach avoids explicit loops and achieves high performance.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ta.md#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef linearreg_slope(close, timePeriod){\n1\tn = close.size()\n2\tb = close.ifirstNot()\n3\tstart = b + timePeriod\n4\tif(b < 0 || start > n) return array(DOUBLE, n, n, NULL)\n5\tx = 0 .. (timePeriod - 1)\n6\tsumB = sum(x).double()\n7\tvarB = sum2(x) - sumB*sumB/timePeriod\n8\tobs = mcount(close, timePeriod)\n9\tmsumA = msum(close, timePeriod)\n10\tsumABDelta = (timePeriod - 1) * close + close.move(timePeriod) - msumA.prev() \n11\tsumABDelta[timePeriod - 1 + 0:b] = NULL\n12\tsumABDelta[start - 1] =  wsum(close.subarray(b:start), x)\n13\treturn (sumABDelta.cumsum() - msumA * sumB/obs)/varB\n}\n```\n\n----------------------------------------\n\nTITLE: Resampling Time Series with resample Function in DolphinDB\nDESCRIPTION: Demonstrates the use of the resample function to downsample a time-indexed series object from daily to monthly frequency, applying an aggregation function (sum). Requires indexedSeries with time/date indexes and the resample function from DolphinDB. Input: indexedSeries with daily observations. Output: resampled series at the specified (monthly) frequency with aggregation per period. The aggregation function must be specified; missing periods are not created.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/panel_data.md#_snippet_22\n\nLANGUAGE: DolphinDB\nCODE:\n```\nindex=2020.01.01..2020.06.30;\ns=indexedSeries(index, take(1,size(index)));\ns.resample(\"M\",sum);\n```\n\n----------------------------------------\n\nTITLE: PCA for Dimensionality Reduction\nDESCRIPTION: This code performs Principal Component Analysis (PCA) on the wine training dataset using the `pca` function. It specifies the columns to use for PCA (`xColNames`) and sets the `normalize` parameter to true to normalize the data. The results, including explained variance ratios and component matrices, are stored in the `pcaRes` dictionary.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/machine_learning.md#_snippet_11\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nxColNames = `Alcohol`MalicAcid`Ash`AlcalinityOfAsh`Magnesium`TotalPhenols`Flavanoids`NonflavanoidPhenols`Proanthocyanins`ColorIntensity`Hue`OD280_OD315`Proline\npcaRes = pca(\n    sqlDS(<select * from wineTrain>),\n    colNames=xColNames,\n    normalize=true\n)\n```\n\n----------------------------------------\n\nTITLE: Bulk Query Parameter Setup and Execution for bundleQuery in DolphinDB Script\nDESCRIPTION: Prepares all arguments, including date and multiple column names/values, for executing bundleQuery on a test table to simulate daily batch querying needs. Demonstrates actual invocation of dynamic filtering and consolidation of query results. AddFunctionView may be called subsequently for cluster registration.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_SQL_Case_Tutorial.md#_snippet_46\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndt = 2020.02.05\ndtColName = \"dsl\"\nmt = 52355979\nmtColName = \"mt\"\ncolNames = `vn`bc`cc`stt`vt\ncolValues = [50982208 50982208 51180116 41774759, 25 25 25 1180, 814 814 814 333, 11 12 12 3, 2 2 2 116]\n\nbundleQuery(t, dt, dtColName, mt, mtColName, colValues, colNames)\n```\n\n----------------------------------------\n\nTITLE: Creating and Assessing Memory for a Table\nDESCRIPTION: This snippet creates a table with 10 million rows and 5 integer columns. It then uses the `mem()` function to display the allocated memory, showing how much memory the table consumes.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/memory_management.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\nn = 10000000\nt = table(n:n,[\"tag1\",\"tag2\",\"tag3\",\"tag4\",\"tag5\"],[INT,INT,INT,INT,INT])\nmem().allocatedBytes - mem().freeBytes\n```\n\n----------------------------------------\n\nTITLE: Training XGBoost Model and Calculating Accuracy\nDESCRIPTION: This snippet trains an XGBoost model using the `xgboost::train` function from the XGBoost plugin. It passes the training data (Y and X), the parameters dictionary, and then predicts labels for the `wineTest` dataset using the trained model. Finally, calculates classification accuracy.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/machine_learning.md#_snippet_21\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nmodel = xgboost::train(Y, X, params)\n\ntestX = select Alcohol, MalicAcid, Ash, AlcalinityOfAsh, Magnesium, TotalPhenols, Flavanoids, NonflavanoidPhenols, Proanthocyanins, ColorIntensity, Hue, OD280_OD315, Proline from wineTest\npredicted = xgboost::predict(model, testX)\n\nsum(predicted == wineTest.Label) \\ wineTest.size()    // 0.962963\n```\n\n----------------------------------------\n\nTITLE: Creating Sample Data Table - DolphinDB\nDESCRIPTION: Creates a sample data table `t` with date, ticker, and various past values. This table is used for illustrating the OLS residual calculation example.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/func_progr_cases.md#_snippet_21\n\nLANGUAGE: DolphinDB\nCODE:\n```\nt=table(2020.11.01 2020.11.02 as date, `IBM`MSFT as ticker, 1.0 2 as past1, 2.0 2.5 as past3, 3.5 7 as past5, 4.2 2.4 as past10, 5.0 3.7 as past20, 5.5 6.2 as past30, 7.0 8.0 as past60)\n```\n\n----------------------------------------\n\nTITLE: Creating EMA Function in DolphinDB\nDESCRIPTION: This DolphinDB script defines an `ema` (Exponential Moving Average) function.  It calculates the EMA using the `iterate` function for vectorization, avoiding explicit loops for efficiency. The function takes `close` and `timePeriod` as input and returns the EMA values. The code includes error handling for invalid input and handles the initial values for the rolling calculations.  It is a good example of vectorized implementation in DolphinDB.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ta.md#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef ema(close, timePeriod) {\n1 \tn = close.size()\n2\tb = ifirstNot(close)\n3\tstart = b + timePeriod\n4\tif(b < 0 || start > n) return array(DOUBLE, n, n, NULL)\n5\tinit = close.subarray(:start).avg()\n6\tcoeff = 1 - 2.0/(timePeriod+1)\n7\tret = iterate(init, coeff, close.subarray(start:)*(1 - coeff))\n8\treturn array(DOUBLE, start - 1, n, NULL).append!(init).append!(ret)\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Stream Processing Example - DolphinDB\nDESCRIPTION: This complete example demonstrates loading data, defining a factor calculation function, creating a stream table and a dictionary for historical data, defining a message handler, subscribing to the stream, and replaying data. It orchestrates the entire process of real-time factor calculation.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/func_progr_cases.md#_snippet_15\n\nLANGUAGE: DolphinDB\nCODE:\n```\nquotesData = loadText(\"/data/ddb/data/sampleQuotes.csv\")\n\ndefg factorAskPriceRatio(x){\n\tcnt = x.size()\n\tif(cnt < 31) return double()\n\telse return x[cnt - 1]/x[cnt - 31]\n}\ndef factorHandler(mutable historyDict, mutable factors, msg){\n\thistoryDict.dictUpdate!(function=append!, keys=msg.symbol, parameters=msg.askPrice1, initFunc=x->array(x.type(), 0, 512).append!(x))\n\tsyms = msg.symbol.distinct()\n\tcnt = syms.size()\n\tv = array(DOUBLE, cnt)\n\tfor(i in 0:cnt){\n\t    v[i] = factorAskPriceRatio(historyDict[syms[i]])\n\t}\n\tfactors.tableInsert([take(now(), cnt), syms, v])\n}\n\nx=quotesData.schema().colDefs\nshare streamTable(100:0, x.name, x.typeString) as quotes1\nhistory = dict(STRING, ANY)\nshare streamTable(100000:0, `timestamp`symbol`factor, [TIMESTAMP,SYMBOL,DOUBLE]) as factors\nsubscribeTable(tableName = \"quotes1\", offset=0, handler=factorHandler{history, factors}, msgAsTable=true, batchSize=3000, throttle=0.005)\n\nreplay(inputTables=quotesData, outputTables=quotes1, dateColumn=`date, timeColumn=`time)\n```\n\n----------------------------------------\n\nTITLE: Executing Distributed SQL Query for Multi-Dimensional Filtering and Grouping in DolphinDB (DolphinDB script)\nDESCRIPTION: Performs an aggregate query on the 'quotes' distributed table to compute the average spread of stock quotes by minute for a specified date range. It filters records using multiple conditions including date, time, and price constraints. The query leverages distributed partitioning by date and stock symbol to enable efficient parallel processing and aggregation across cluster nodes.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Cluster_scale_out_performance_test.md#_snippet_0\n\nLANGUAGE: DolphinDB script\nCODE:\n```\nselect avg((ofr-bid)/(ofr+bid)*2) as avgSpread from quotes where date between 2007.08.01 : 2007.08.07, time between 09:30:00 : 15:59:59, ofr>bid, bid>0, ofr/bid<1.2 group by minute(time) as minute\n```\n\n----------------------------------------\n\nTITLE: Creating a Stream Table for Snapshot Data Using amdQuote::getSchema in DolphinDB\nDESCRIPTION: This snippet creates a streaming table with a schema based on AMD plugin's snapshot data by retrieving the field names and types via amdQuote::getSchema. It appends a 'tradeDate' DATE column at the front to store the date extracted from timestamps. The enableTableShareAndPersistence function enables table sharing and persistence with a specified cache size, facilitating real-time streaming of market data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_implementation_for_AMD_Plugin.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 获取行情数据的表格构造并建立流表以 snapshot 为例\nsnapshotTbName = \"snapshot\"\nsnapshotSchema = amdQuote::getSchema(`snapshot)\ncolName = append!([\"tradeDate\"], snapshotSchema.name)\ncolType = append!([\"DATE\"], snapshotSchema.type)\nenableTableShareAndPersistence(table=streamTable(20000000:0, colName, colType), tableName=snapshotTbName, cacheSize=20000000)\n```\n\n----------------------------------------\n\nTITLE: Defining WorldQuant Alpha 1 Streaming Pipeline in DolphinDB Script\nDESCRIPTION: This snippet constructs a real-time streaming pipeline to compute the WorldQuant Alpha1 factor in DolphinDB. It sets up input and result tables, initializes a stream engine with streamEngineParser, sets the 'securityID' as key and 'dateTime' as time column, and uses 'keyCount' pattern with an interval trigger. Pre-existing engines are dropped if present to avoid name conflicts. Example metrics and data ingestion commands are provided, culminating in a pivot table for factor output. Required dependencies are DolphinDB streaming and table APIs. Inputs are market tick data rows; outputs are computed alpha factors keyed by security and datetime. Table schemas and engine setup must match; data size or new times can trigger computation.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Streaming_computing_of_financial_quantifiers.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 定义输入输出的表结构\ncolName = `securityID`dateTime`preClosePx`openPx`highPx`lowPx`lastPx`volume`amount`iopv`fp_Volume`fp_Amount\ncolType = [\"SYMBOL\",\"TIMESTAMP\",\"DOUBLE\",\"DOUBLE\",\"DOUBLE\",\"DOUBLE\",\"DOUBLE\",\"INT\",\"DOUBLE\",\"DOUBLE\",\"INT\",\"DOUBLE\"]\ninputTable = table(1:0, colName, colType)\nresultTable = table(10000:0, [\"dateTime\", \"securityID\", \"factor\"], [TIMESTAMP, SYMBOL, DOUBLE])\n\n// 使用 streamEngineParser 创建引擎流水线\ntry{ dropStreamEngine(\"alpha1Parser0\")} catch(ex){ print(ex) }\ntry{ dropStreamEngine(\"alpha1Parser1\")} catch(ex){ print(ex) }\nmetrics = <[securityid, wqAlpha1(preClosePx)]>\nstreamEngine = streamEngineParser(name=\"alpha1Parser\", metrics=metrics, dummyTable=inputTable, outputTable=resultTable, keyColumn=\"securityID\", timeColumn=`dateTime, triggeringPattern='keyCount', triggeringInterval=3000)\n\n// 查看引擎\ngetStreamEngineStat()\n/*\nReactiveStreamEngine->\nname          user  status lastErrMsg numGroups numRows numMetrics memoryInUsed snapshotDir ...\n------------- ----- ------ ---------- --------- ------- ---------- ------------ ----------- \nalpha1Parser0 admin OK                0         0       2          13392                    ...\n\nCrossSectionalEngine->\nname         user  status lastErrMsg numRows numMetrics metrics      triggering...triggering......\n------------ ----- ------ ---------- ------- ---------- ------------ --------------- --------------- ---\nalpha1Parser1admin OK                0       2          securityid...keyCount     3000         ...\n*/\n```\n\n----------------------------------------\n\nTITLE: Reading String Vector Data with getDataArray C++\nDESCRIPTION: Shows how to efficiently read string data from a DolphinDB Vector. It obtains a pointer to the internal data array using `getDataArray`, casts it to `DolphinString*`, and iterates through the elements to read them. This is more efficient than accessing strings by index repeatedly.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_advance.md#_snippet_12\n\nLANGUAGE: C++\nCODE:\n```\nConstantSP readString(Heap *heap, vector<ConstantSP> &arguments) {\n    if (!(arguments[0]->isVector() && arguments[0]->getType() == DT_STRING)) {\n        throw IllegalArgumentException(\"readString\", \"argument must be a string vector\");\n    }\n    VectorSP pVec = arguments[0]; //aruments[0]为需要获取数据的String类型的Vector\n    size_t size = pVec->size();\n    DolphinString *pDolString = (DolphinString *)pVec->getDataArray(); //获取数据指针\n    for (size_t i = 0; i < size; i++) {\n        std::cout << pDolString[i].getString() << std::endl; //读取数据\n    }\n    return new Void();\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Partitioned Table with Hash Reduction on SecurityID and TradeTime Sort Columns\nDESCRIPTION: This code snippet demonstrates creating a partitioned table and applying hash reduction when using multiple sort columns (SecurityID and TradeTime).  It illustrates how `sortKeyMappingFunction` impacts hash reduction across sort columns and influences storage and query performance.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/database_and_table_creation_details.md#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndb=database(\"dfs://testDB1\",VALUE, 2020.01.01..2021.01.01,engine=\"TSDB\")\ncolName = `SecurityID`TradeDate`TradeTime`TradePrice`TradeQty`TradeAmount`BuyNo`SellNo\ncolType = `SYMBOL`DATE`TIME`DOUBLE`INT`DOUBLE`INT`INT\ntbSchema = table(1:0, colName, colType)\ndb.createPartitionedTable(table=tbSchema, tableName=`pt5, partitionColumns=`TradeDate, sortColumns=`SecurityID`TradeTime, keepDuplicates=ALL,sortKeyMappingFunction=[hashBucket{,500}])\n```\n\n----------------------------------------\n\nTITLE: Inserting Data into MultithreadedTableWriter with Threads in C++\nDESCRIPTION: 该代码段演示了如何在 DolphinDB C++ API 中使用多线程向 MultithreadedTableWriter 的缓冲区插入数据。需预先加载数据源至 TableSP，并按行列格式填充 datas 向量。插入操作要求传入 ErrorCodeInfo 及变长字段值列表。应处理异常以确保健壮性。每个插入线程可单独插入数据条目，适用于大批量、并发数据写入场景。依赖：已初始化的 writer 对象与数据表。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_cpp_api_connector.md#_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\nint rows = 1000; //行数\nint cols = 5;   //列数\nvector<ConstantSP> datas;\nTableSP bt = conn.run(\"t0 = loadText('\"+DATA_FIRE+\"');t0\");// 模拟数据源从 csv 文件导入\nfor(int i = 0; i< rows; ++i){\n    for(int j = 0; j < cols; ++j)\n        datas.emplace_back(bt->getColumn(j)->get(i));\n}\n// 创建线程\nthread t([&]() {\n    try {\n        for(int i=0;i < bt->rows();i++){\n           ErrorCodeInfo pErrorInfo;\n           writer.insert(pErrorInfo,\n                        datas[0], datas[1], datas[2], datas[3], datas[4] // 含5个字段的数据\n           );\n        }\n    }catch (exception &e) {\n        cerr << \"MTW exit with exception: \" << e.what() << endl;\n    }\n});\n// 等待插入线程结束\nt.join();\n```\n\n----------------------------------------\n\nTITLE: Configuring DataX Data Migration Task with JSON\nDESCRIPTION: This JSON snippet defines a DataX job configuration that reads data from a SQL Server source and writes it to a DolphinDB database. It specifies connection details, the columns to extract, authentication credentials, batch sizes, data types, and a custom DolphinDB function for data transformation and insertion. Dependencies include DataX runtime and valid JDBC connection parameters. Key parameters include source SQL Server login info, columns, and a custom saveFunctionDef that processes incoming data before appending it to a DolphinDB table. The JSON must be placed in the data/job directory and adheres to DataX configuration standards.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/SQLServer_to_DolphinDB.md#_snippet_10\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    \"core\": {\n        \"transport\": {\n            \"channel\": {\n                \"speed\": {\n                    \"byte\": 5242880\n                }\n            }\n        }\n    },\n    \"job\": {\n        \"setting\": {\n            \"speed\": {\n                \"byte\":10485760\n            }\n        },\n        \"content\": [\n            {\n                \"reader\": {\n                    \"name\": \"sqlserverreader\",\n                    \"parameter\": {\n                        \"username\": \"sa\",\n                        \"password\": \"DolphinDB123\",\n                        \"column\": [\n                            \"ChannelNo\",\"ApplSeqNum\",\"MDStreamID\",\"SecurityID\",\"SecurityIDSource\",\"Price\",\"OrderQty\",\"Side\",\"TransactTime\",\"OrderType\",\"LocalTime\"\n                        ],\n                        \"connection\": [\n                            {\n                                \"table\": [\n                                    \"data\"\n                                ],\n                                \"jdbcUrl\": [\n                                    \"jdbc:sqlserver://127.0.0.1:1433;databasename=historyData\"\n                                ]\n                            }\n                        ]\n                    }\n                },\n                \"writer\": {\n                    \"name\": \"dolphindbwriter\",\n                    \"parameter\": {\n                        \"userId\": \"admin\",\n                        \"pwd\": \"123456\",\n                        \"host\": \"127.0.0.1\",\n                        \"port\": 8888,\n                        \"dbPath\": \"dfs://TSDB_Entrust\",\n                        \"tableName\": \"entrust\",\n                        \"batchSize\": 100000,\n                        \"saveFunctionDef\": \"def customTableInsert(dbName, tbName, mutable data) {data.replaceColumn!(`LocalTime,time(temporalParse(data.LocalTime,\\\"HH:mm:ss.nnnnnn\\\")));data.replaceColumn!(`Price,double(data.Price));data[`SeqNo]=int(NULL);data[`DataStatus]=int(NULL);data[`BizIndex]=long(NULL);data[`Market]=`SZ;data.reorderColumns!(`ChannelNo`ApplSeqNum`MDStreamID`SecurityID`SecurityIDSource`Price`OrderQty`Side`TransactTime`OrderType`LocalTime`SeqNo`Market`DataStatus`BizIndex);pt = loadTable(dbName,tbName);pt.append!(data)}\",\n                        \"saveFunctionName\" : \"customTableInsert\",\n                        \"table\": [\n                            {\n                                \"type\": \"DT_INT\",\n                                \"name\": \"ChannelNo\"\n                            },\n                            {\n                                \"type\": \"DT_LONG\",\n                                \"name\": \"ApplSeqNum\"\n                            },\n                            {\n                                \"type\": \"DT_INT\",\n                                \"name\": \"MDStreamID\"\n                            },\n                            {\n                                \"type\": \"DT_SYMBOL\",\n                                \"name\": \"SecurityID\"\n                            },\n                            {\n                                \"type\": \"DT_INT\",\n                                \"name\": \"SecurityIDSource\"\n                            },\n                            {\n                                \"type\": \"DT_DOUBLE\",\n                                \"name\": \"Price\"\n                            },\n                            {\n                                \"type\": \"DT_INT\",\n                                \"name\": \"OrderQty\"\n                            },\n                            {\n                                \"type\": \"DT_SYMBOL\",\n                                \"name\": \"Side\"\n                            },\n                            {\n                                \"type\": \"DT_TIMESTAMP\",\n                                \"name\": \"TransactTime\"\n                            },\n                            {\n                                \"type\": \"DT_SYMBOL\",\n                                \"name\": \"OrderType\"\n                            },\n                            {\n                                \"type\": \"DT_STRING\",\n                                \"name\": \"LocalTime\"\n                            }\n                        ]\n\n                    }\n                }\n            }\n        ]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Load Text File into DolphinDB Table\nDESCRIPTION: This snippet loads a text file into a DolphinDB table using the `loadText` function. It automatically infers column data types.  If the inferred types are incorrect, a schema table can be created and passed as a parameter to `loadText` to explicitly define the data types for each column.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_data.md#_snippet_0\n\nLANGUAGE: txt\nCODE:\n```\ndataFilePath = \"/home/data/candle_201801.csv\"\ntmpTB = loadText(dataFilePath);\n```\n\n----------------------------------------\n\nTITLE: 执行多样化 SQL 查询示例 - DolphinDB 脚本\nDESCRIPTION: 该段示例展示了如何加载分布式表元数据，并进行计数、聚合和条件筛选，包括总数查询、最大最小值、平均值、分段查询、单点查询及时间范围查询。依赖 DolphinDB SQL 查询引擎，核心参数包括数据库路径、表名及筛选条件，输出为聚合结果或子集数据。适合海量时序数据快速查询。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/high_freq_data_storage_and_analysis.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\n/**** 数据查询 Demo ****/\n\n//加载表，赋值给 pt （表变量赋值仅加载元数据）\npt=loadTable(database=\"dfs://db_test\",tableName=`collect)  //pt(ts,deviceid,v1,...,v5000)\n\n//查看数据总数\nselect count(*) from pt \n\n//数据检查\nselect max(ts),min(v1),max(v2),avg(v3),sum(v4),last(v1) from pt     //最大值、最小值、平均值、最后一条数据 的查询\nselect top 10 v1,v2,v3 from pt where ts between 2022.01.01 00:00:00.000 : 2022.01.01 00:01:00.000\n\n//单点查询（返回单值）\ntimer t = select v1 from pt where deviceid='d001',ts=2022.01.01 00:00:00.001\n\n//范围查询\ntimer t=select ts,v1 from pt where deviceid='d001', ts between 2022.01.01 00:00:00.000 : 2022.01.01 00:00:01.000\n```\n\n----------------------------------------\n\nTITLE: Creating and Enabling Shared Stream Table\nDESCRIPTION: This code defines the column names and types for a table and then creates a shared, persistent stream table named `table1`. The `enableTableShareAndPersistence` function makes the table accessible globally within the DolphinDB instance and configures basic persistence settings.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/redis_plugin_tutorial/example2.txt#_snippet_1\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\ncolName=[\"key\", \"value\"]\ncolType=[\"string\", \"string\"]\nenableTableShareAndPersistence(table=streamTable(100:0, colName, colType), tableName=`table1, cacheSize=10000, asynWrite=false)\n```\n\n----------------------------------------\n\nTITLE: Enabling Persistence and Sharing for Stream Tables in DolphinDB Script\nDESCRIPTION: This snippet shows how to enable persistence and sharing for a stream table in DolphinDB. It creates a stream table, then uses enableTableShareAndPersistence to make it both shared and persisted. The function must be run on a new, empty stream table, and the persistence path should be configured in advance. After restart, re-executing this code reloads the persistent state.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/in_memory_table.md#_snippet_10\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nt=streamTable(1:0,`id`val,[INT,DOUBLE])\nenableTableShareAndPersistence(t,`st)\n```\n\n----------------------------------------\n\nTITLE: Creating Database for Merged Tick-by-Tick Trade Data with TSDB Engine\nDESCRIPTION: Creates a database for storing merged Shanghai and Shenzhen tick-by-tick trade data. Uses a combined partitioning strategy with daily value partitioning and HASH 50 on symbols. Market type, symbol, and trade time are used as sorting columns.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practices_for_partitioned_storage.md#_snippet_8\n\nLANGUAGE: SQL\nCODE:\n```\ncreate database \"dfs://merge_TB\"\npartitioned by VALUE(2020.01.01..2021.01.01), HASH([SYMBOL, 50])\nengine='TSDB'\n\ncreate table \"dfs://merge_TB\".\"merge_tradeTB\"(\n    ChannelNo INT\n    ApplSeqNum LONG\n    MDStreamID SYMBOL\n    BidApplSeqNum LONG\n    OfferApplSeqNum LONG\n    SecurityID SYMBOL\n    SecurityIDSource SYMBOL\n    TradePrice DOUBLE\n    TradeQty LONG\n    ExecType SYMBOL\n    TradeDate DATE[comment=\"交易日期\", compress=\"delta\"]   \n    TradeTime TIME[comment=\"交易时间\", compress=\"delta\"]   \n    LocalTime TIME\n    SeqNo LONG\n    DataStatus INT\n    TradeMoney DOUBLE\n    TradeBSFlag SYMBOL\n    BizIndex LONG\n    OrderKind SYMBOL\n    Market SYMBOL\n)\npartitioned by TradeDate, SecurityID,\nsortColumns=[`Market,`SecurityID,`TradeTime],\nkeepDuplicates=ALL\n```\n\n----------------------------------------\n\nTITLE: Loading and Querying Partitioned Tables in DolphinDB Using DolphinDB Script\nDESCRIPTION: This snippet demonstrates how to load a partitioned table object from a distributed file system and perform aggregation queries on it within the DolphinDB compute node's web interactive programming interface. The first query counts rows grouped by date; the second computes OHLC (Open, High, Low, Close) price values for each stock per day, assigning the result to a server-side variable to optimize client memory usage and allow paginated display.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/multi_machine_cluster_deployment.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 加载分区表对象\npt = loadTable(\"dfs://testDB\", \"testTB\")\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// SQL 返回数据量少的时候，可以直接取回客户端展示\nselect count(*) from pt group by date(DateTime) as Date\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// SQL 返回数据量较大时，可以赋值给变量，占用 server 章内存，客户端分页取回展示\nresult = select first(LastPx) as Open, max(LastPx) as High, min(LastPx) as Low, last(LastPx) as Close from pt group by date(DateTime) as Date, SecurityID\n```\n\n----------------------------------------\n\nTITLE: Creating Partitioned Table in DolphinDB with Multiple Sort Columns (SecurityID, TradeTime)\nDESCRIPTION: This code snippet demonstrates creating a partitioned table in DolphinDB using the TSDB engine with multiple sort columns (SecurityID, TradeTime).  It showcases a scenario where the combination of sort columns results in a substantial number of index keys, potentially impacting performance, although better than using only `TradeTime`.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/database_and_table_creation_details.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndb=database(\"dfs://testDB1\",VALUE, 2020.01.01..2021.01.01,engine=\"TSDB\")\ncolName = `SecurityID`TradeDate`TradeTime`TradePrice`TradeQty`TradeAmount`BuyNo`SellNo\ncolType = `SYMBOL`DATE`TIME`DOUBLE`INT`DOUBLE`INT`INT\ntbSchema = table(1:0, colName, colType)\ndb.createPartitionedTable(table=tbSchema, tableName=`pt2, partitionColumns=`TradeDate, sortColumns=`SecurityID`TradeTime, keepDuplicates=ALL)\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid - broker\nDESCRIPTION: Configuration settings for the Druid broker service including memory allocation (Xms, Xmx, MaxDirectMemorySize), thread counts for HTTP servers and processing threads, and query cache settings. Dependencies: Druid setup.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_Druid_appendix.md#_snippet_5\n\nLANGUAGE: Druid\nCODE:\n```\nXms24g\nXmx24g\nXX:MaxDirectMemorySize=4096m\n\n# HTTP server threads\ndruid.broker.http.numConnections=5\ndruid.server.http.numThreads=25\n\n# Processing threads and buffers\ndruid.processing.buffer.sizeBytes=2147483648\ndruid.processing.numThreads=7\n\n# Query cache\ndruid.broker.cache.useCache=false\ndruid.broker.cache.populateCache=false\n```\n\n----------------------------------------\n\nTITLE: Remove Shared Stream Table\nDESCRIPTION: This code demonstrates how to remove a shared stream table `pubTable` using the `undef` and `dropStreamTable` functions. `undef` releases the variable from memory, while `dropStreamTable` deletes the table itself. Note that `dropStreamTable` is required to delete persisted stream tables and all subscriptions must be cancelled first.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_tutorial.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\nundef(`pubTable, SHARED)\ndropStreamTable(`pubTable)\n```\n\n----------------------------------------\n\nTITLE: 将计算结果转换为窄表格式并准备写入数据库 - Python\nDESCRIPTION: 该代码将之前计算得到的因子结果 pandas Series 转换为符合 DolphinDB 窄表结构格式的 DataFrame，包含 tradetime、securityid、factorname 及 value 四列，用于后续写入分区表。通过 reset_index 打平成普通表结构，重命名索引列，并利用 melt 函数实现因子名列的扁平化处理。适配前述因子存储表设计要求，结果可持久化存储。无依赖外部函数，要求 pandas 环境支持。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_Python_Parser_Intro_for_Quantitative_Finance.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# res 是一个 Series, 需要将计算结果转化成窄表格式（共4列：tradetime, securityid, factorname, value）\nresult = res.reset_index().rename(columns={\"SecurityID\":\"securityid\"})\nresult[\"tradetime\"] = 2023.02.01\nresult = result.melt(id_vars=[\"tradetime\", \"securityid\"],value_vars=[\"BCVP\"],var_name=\"factorname\",value_name=\"value\")\n```\n\n----------------------------------------\n\nTITLE: Creating Cross-Sectional Engine for Factor Ranking in DolphinDB\nDESCRIPTION: Defines an input table schema `schemaTB` matching the output of the `calChange` engine. Creates a cross-sectional engine named `crossSectionalEngine` using `createCrossSectionalEngine`. This engine calculates the descending rank of the 1-minute, 5-minute, and 10-minute change factors (`factor_1min`, `factor_5min`, `factor_10min`) across all securities within each batch (`triggeringPattern='perBatch'`). Results are outputted to the shared keyed table `changeCrossSectionalTable`.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Real-Time_Stock_Price_Increase_Calculation.md#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\nschemaTB = table(1:0, `SecurityID`DateTime`factor_1min`factor_5min`factor_10min, [SYMBOL, TIMESTAMP, DOUBLE, DOUBLE, DOUBLE])\ncreateCrossSectionalEngine(name=\"crossSectionalEngine\", metrics=<[SecurityID, factor_1min, rank(factor_1min, ascending=false), factor_5min, rank(factor_5min, ascending=false), factor_10min, rank(factor_10min, ascending=false)]>, dummyTable=schemaTB, outputTable=objByName(\"changeCrossSectionalTable\"), keyColumn=`SecurityID, triggeringPattern='perBatch', useSystemTime=false, timeColumn=`DateTime)\n```\n\n----------------------------------------\n\nTITLE: Creating a Data Source with Data Preprocessing\nDESCRIPTION: This code loads the 'ohlc' table from a distributed database ('dfs://trades') and then creates a data source (`ds`) using `sqlDS`. The `transDS!` function is used to apply the `preprocess` function to the data source, transforming the data before it is used for model training.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/machine_learning.md#_snippet_8\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nohlc = database(\"dfs://trades\").loadTable(\"ohlc\")\nds = sqlDS(<select * from ohlc>).transDS!(preprocess)\n```\n\n----------------------------------------\n\nTITLE: Transforming Data with Principal Components\nDESCRIPTION: This code defines a function `principalComponents` that transforms a table `t` by applying the principal components. It multiplies the input data matrix by the component matrix, adds the original dependent variable (`yColName`) back to the result, and returns a new table with the reduced dimensionality and the target variable. Requires input table `t`, components matrix, target and feature column names.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/machine_learning.md#_snippet_14\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\ndef principalComponents(t, components, yColName, xColNames) {\n    res = matrix(t[xColNames]).dot(components).table()\n    res[yColName] = t[yColName]\n    return res\n}\n```\n\n----------------------------------------\n\nTITLE: Creating and Sharing a Stream Table - DolphinDB\nDESCRIPTION: This snippet creates a shared stream table named 'trades' that can be accessed by all users initially.  It defines the table schema and shares it using the `share` function. Requires admin privileges to create user.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Permission_Management.md#_snippet_27\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(`admin, \"123456\");\ncreateUser(\"MitchTrubisky\",\"JI3564^\",,true)\nlogin(`MitchTrubisky, \"JI3564^\")\nshare streamTable(1000:0, `time`sym`volume, [TIMESTAMP, SYMBOL, INT]) as trades\n```\n\n----------------------------------------\n\nTITLE: Creating a Stream Processing Engine with Cascaded Reactive State Engines in DolphinDB\nDESCRIPTION: This DolphinDB script creates a stream processing engine using three cascaded reactive state engines. Each engine performs a specific calculation stage, with the output of one engine feeding into the next.  The final engine calculates the small order net inflow to total trading volume ratio for each stock.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practice_for_factor_calculation.md#_snippet_12\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef createStreamEngine(result){\n\ttradeSchema = createTradeSchema()\n\tresult1Schema = createResult1Schema()\n\tresult2Schema = createResult2Schema()\n\tengineNames = [\"rse1\", \"rse2\", \"res3\"]\n\tcleanStreamEngines(engineNames)\n\t\n\tmetrics3 = <[TradeTime, factorSmallOrderNetAmountRatio(tradeAmount, sellCumAmount, sellOrderFlag, prevSellCumAmount, prevSellOrderFlag, buyCumAmount, buyOrderFlag, prevBuyCumAmount, prevBuyOrderFlag)]>\n\trse3 = createReactiveStateEngine(name=engineNames[2], metrics=metrics3, dummyTable=result2Schema, outputTable=result, keyColumn=\"SecurityID\")\n\t\n\tmetrics2 = <[BuyNo, SecurityID, TradeTime, TradeAmount, BuyCumAmount, PrevBuyCumAmount, BuyOrderFlag, PrevBuyOrderFlag, factorOrderCumAmount(TradeAmount)]>\n\trse2 = createReactiveStateEngine(name=engineNames[1], metrics=metrics2, dummyTable=result1Schema, outputTable=rse3, keyColumn=\"SellNo\")\n\t\n\tmetrics1 = <[SecurityID, SellNo, TradeTime, TradeAmount, factorOrderCumAmount(TradeAmount)]>\n\treturn createReactiveStateEngine(name=engineNames[0], metrics=metrics1, dummyTable=tradeSchema, outputTable=rse2, keyColumn=\"BuyNo\")\n}\n```\n\n----------------------------------------\n\nTITLE: Creating and populating a distributed database in DolphinDB\nDESCRIPTION: Creates a composite partitioned database with date and symbol partitioning, then defines and loads tables for both fund NAV and HS300 data. The database uses OLAP storage engine and includes tables with date, fund identifier, and value columns.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_data_load.txt#_snippet_4\n\nLANGUAGE: dolphindb\nCODE:\n```\ndbName = \"dfs://fund_OLAP\"\ndataDate = database(, VALUE, 2021.01.01..2021.12.31)\nsymbol = database(, HASH, [SYMBOL, 20])\nif(existsDatabase(dbName)){\n\tdropDatabase(dbName)\n}\ndb = database(dbName, COMPO, [dataDate, symbol])\nname = `tradingDate`fundNum`value\ntype = `DATE`SYMBOL`DOUBLE\ntbTemp = table(1:0, name, type)\ntbName1 = \"fund_OLAP\"\ndb.createTable(tbTemp, tbName1)\nloadTable(dbName, tbName1).append!(result)\ntbName2 = \"fund_hs_OLAP\"\ndb.createTable(tbTemp, tbName2)\nloadTable(dbName, tbName2).append!(result1)\n```\n\n----------------------------------------\n\nTITLE: 创建流数据表和设置过滤列\nDESCRIPTION: 定义函数创建三个流数据表：tradeOriginalStream用于接收实时数据源，capitalFlowStream用于接收流计算结果，capitalFlowStream60min用于分钟级汇总。并设置tradeOriginalStream的过滤列为SecurityID，以支持并行计算。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_capital_flow_daily.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef createStreamTableFunc(){\n\t//create stream table: tradeOriginalStream\n\tcolName = `SecurityID`Market`TradeTime`TradePrice`TradeQty`TradeAmount`BuyNum`SellNum\n\tcolType = [SYMBOL, SYMBOL, TIMESTAMP, DOUBLE, INT, DOUBLE, INT, INT]\n\ttradeOriginalStreamTemp = streamTable(20000000:0, colName, colType)\n\ttry{ enableTableShareAndPersistence(table=tradeOriginalStreamTemp, tableName=\"tradeOriginalStream\", asynWrite=true, compress=true, cacheSize=20000000, retentionMinutes=1440, flushMode=0, preCache=10000) }\n\tcatch(ex){ print(ex) }\n\tundef(\"tradeOriginalStreamTemp\")\n\t\n\t//create stream table: capitalFlow\n\tcolName = `SecurityID`TradeTime`TotalAmount`SellSmallAmount`SellMediumAmount`SellBigAmount`SellSmallCount`SellMediumCount`SellBigCount`BuySmallAmount`BuyMediumAmount`BuyBigAmount`BuySmallCount`BuyMediumCount`BuyBigCount\n\tcolType =  [SYMBOL, TIMESTAMP, DOUBLE, DOUBLE, DOUBLE, DOUBLE, INT, INT, INT, DOUBLE, DOUBLE, DOUBLE, INT, INT, INT]\n\tcapitalFlowStreamTemp = streamTable(20000000:0, colName, colType)\n\ttry{ enableTableShareAndPersistence(table=capitalFlowStreamTemp, tableName=\"capitalFlowStream\", asynWrite=true, compress=true, cacheSize=20000000, retentionMinutes=1440, flushMode=0, preCache=10000) }\n\tcatch(ex){ print(ex) }\n\tundef(\"capitalFlowStreamTemp\")\n\t\n\t//create stream table: capitalFlowStream60min\n\tcolName = `TradeTime`SecurityID`TotalAmount`SellSmallAmount`SellMediumAmount`SellBigAmount`SellSmallCount`SellMediumCount`SellBigCount`BuySmallAmount`BuyMediumAmount`BuyBigAmount`BuySmallCount`BuyMediumCount`BuyBigCount\n\tcolType =  [TIMESTAMP, SYMBOL, DOUBLE, DOUBLE, DOUBLE, DOUBLE, INT, INT, INT, DOUBLE, DOUBLE, DOUBLE, INT, INT, INT]\n\tcapitalFlowStream60minTemp = streamTable(1000000:0, colName, colType)\n\ttry{ enableTableShareAndPersistence(table=capitalFlowStream60minTemp, tableName=\"capitalFlowStream60min\", asynWrite=true, compress=true, cacheSize=1000000, retentionMinutes=1440, flushMode=0, preCache=10000) }\n\tcatch(ex){ print(ex) }\n\tundef(\"capitalFlowStreamTemp\")\n}\n\ncreateStreamTableFunc()\ngo\nsetStreamTableFilterColumn(tradeOriginalStream, `SecurityID)\n```\n\n----------------------------------------\n\nTITLE: Implementing Real-time Volatility Prediction with Model Integration in DolphinDB\nDESCRIPTION: Defines a prediction handler function and subscribes to the aggregated features table to perform real-time volatility predictions. The function applies a pre-trained model and writes results to the result1min table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/machine_learning_volatility.md#_snippet_6\n\nLANGUAGE: dolphindb\nCODE:\n```\ndef predictRV(mutable result1min, model, msg){\n\tstartTime = now()\n\tpredicted = model.predict(msg)\n\ttemp = select TradeTime, SecurityID, predicted as PredictRV, (now()-startTime) as CostTime from msg\n\tresult1min.append!(temp)\n}\nsubscribeTable(tableName=\"aggrFeatures10min\", actionName=\"predictRV\", offset=-1, handler=predictRV{result1min, model}, msgAsTable=true, hash=1, reconnect=true)\n```\n\n----------------------------------------\n\nTITLE: Migrating Chunks Across Volume in DolphinDB\nDESCRIPTION: This DolphinDB script migrates chunks from one path to another on the specified node. The input `srcPath` and `destPath` indicate where data should move from and to, `chunkIds` are the ID's of the chunks that need to be moved. The parameter `isDelSrc` indicates if the source data should be deleted after migrating.  This script uses `moveChunksAcrossVolume` to perform the data migration.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/data_move_rebalance.md#_snippet_14\n\nLANGUAGE: DolphinDB\nCODE:\n```\nsrcPath = \"/ssd/ssd1/jtwang/chunkData/CHUNKS\"\ndestPath = \"/ssd/ssd0/jtwang/chunkData/CHUNKS\"\n\nnode = \"P1-dn1\"\nchunkIds = exec chunkId from pnodeRun(getAllChunks) where site = node, path like (srcPath + \"%\")\nrpc(node, moveChunksAcrossVolume{ srcPath, destPath, chunkIds, isDelSrc=true })\n\nnode = \"P2-dn1\"\nchunkIds = exec chunkId from pnodeRun(getAllChunks) where site = node, path like (srcPath + \"%\")\nrpc(node, moveChunksAcrossVolume{ srcPath, destPath, chunkIds, isDelSrc=true })\n```\n\n----------------------------------------\n\nTITLE: Loading trade data into DolphinDB table from CSV file\nDESCRIPTION: This snippet loads CSV trade data into an existing DolphinDB table. It first loads the table schema, then uses loadTextEx to import data from specified CSV file, partitioning data by TradeTime and SecurityID columns. Dependencies include an existing database connection and correct CSV file path. Inputs are database and table names, CSV filename, and schema; output is populated table containing trade records.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/streaming_capital_flow_order_by_order/01.创建存储历史数据的库表并导入数据.txt#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\n/**\npart2: load data\n*/\ntrade = loadTable(dbName, tbName)\nschemaTable = table(trade.schema().colDefs.name as `name, trade.schema().colDefs.typeString as `type)\nloadTextEx(dbHandle=database(dbName), tableName=tbName, partitionColumns=`TradeTime`SecurityID, filename=csvDataPath, schema=schemaTable)\n\n```\n\n----------------------------------------\n\nTITLE: Querying - Multiple Group by, Order by DolphinDB\nDESCRIPTION: This snippet calculates the average CPU load, grouped by the hour of the day, within a specific time range, and then orders by the calculated load in descending order and by hour in ascending order.  It shows complex sorting and grouping.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs TimescaleDB/test_dolphindb_small.txt#_snippet_25\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 16. 经典查询：查找所有设备平均负载最高的时段，并按照负载降序排列、时间升序排列\ntimer\nselect floor(avg(cpu_avg_15min)) as load\nfrom readings\nwhere time between 2016.11.16 00:00:00 : 2016.11.18 00:00:00\ngroup by hour(time) as hour\norder by load desc, hour asc;\n```\n\n----------------------------------------\n\nTITLE: Initializing Asof Join Streaming Engine and Subscriptions - DolphinDB\nDESCRIPTION: Shows how to set up a real-time asof join using DolphinDB's createAsofJoinEngine and stream table subscription API. Requires DolphinDB 2.00.8+ or 1.30.20+ for full compatibility. Three stream tables are created for trade, snapshot, and output data. The asof join engine is constructed with key parameters such as left and right tables, matching columns, and time behavior. Subscriptions are set up to inject data into the engine's left and right handlers. Input: streaming tables populated in real time by external sources. Output: joined records are written in real time to the output table. This pattern supports scalable, low latency correlation for high-frequency data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming-real-time-correlation-processing.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// create table\nshare streamTable(1:0, `Sym`Time`Price, [SYMBOL, TIME, DOUBLE]) as trade\nshare streamTable(1:0, `Sym`Time`BidPrice, [SYMBOL, TIME, DOUBLE]) as snapshot\nshare table(1:0, `Time`Sym`Price`t2_Time`BidPrice, [TIME, SYMBOL, DOUBLE, TIME, DOUBLE]) as output\n\n// create engine\najEngine = createAsofJoinEngine(name=\"asofJoin\", leftTable=trade, rightTable=snapshot, outputTable=output, metrics=<[Price, snapshot.Time, BidPrice]>, matchingColumn=`Sym, timeColumn=`Time, useSystemTime=false, delayedTime=1000)\n\n// subscribe topic\nsubscribeTable(tableName=\"trade\", actionName=\"joinLeft\", offset=0, handler=getLeftStream(ajEngine), msgAsTable=true)\nsubscribeTable(tableName=\"snapshot\", actionName=\"joinRight\", offset=0, handler=getRightStream(ajEngine), msgAsTable=true)\n```\n\n----------------------------------------\n\nTITLE: Calculating Fund Sharpe Ratios in DolphinDB\nDESCRIPTION: Filters the daily returns matrix (`returnsMatrix`) to include only funds with more than 1000 data points, recent activity (last non-null value within the last 30 days), and excluding certain types (Money Market, REITs). Calculates annualized return (exp) and annualized volatility (vol) assuming 242 trading days per year. Computes the Sharpe ratio using a risk-free rate of 2.8%. Stores these performance metrics (SecurityID, Type, exp, vol, sharpe) in a table named 'perf'. Generates histograms for return, volatility, and Sharpe ratio distributions, and a scatter plot of risk (vol) versus return (exp) for funds with positive Sharpe ratios. Depends on `returnsMatrix` and `fundTypeMap`.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/public_fund_basic_analysis/basic_analysis_of_public_fund_net_value.txt#_snippet_7\n\nLANGUAGE: dolphindb\nCODE:\n```\n// data overview\nreturnsMatrix.resample(\"A\", size)[0]\n// filter analysis space\nuReturnsMatrix = returnsMatrix.loc(,(each(count, returnsMatrix) > 1000 && returnsMatrix.ilastNot() >=  returnsMatrix.rows() - 30)&&!(fundTypeMap[returnsMatrix.colNames()] in [\"货币市场型\", \"REITs\"]))\n// calculate annualized return and annualized volatility\nexp = mean(uReturnsMatrix)*242\nvol = std(uReturnsMatrix)*sqrt(242)\n// calculate sharpe rate\nsharpe = (exp - 0.028)/vol\n// generate annualized return, annualized volatility and sharp ratio table\nperf = table(uReturnsMatrix.colNames() as SecurityID, fundTypeMap[uReturnsMatrix.colNames()] as Type, exp*100 as exp, vol*100 as vol, sharpe)\n// plot annualized return\n(exec exp from perf where exp > -10, exp < 40).plotHist(400)\n// plot annualized volatility\n(exec vol from perf where vol < 40).plotHist(400)\n// plot sharp ratio\n(exec sharpe from perf where sharpe > 0).plotHist(200)\n// plot risk return scatter chart\nmask = select * from perf where sharpe>0, vol<40, exp<40 \nplot(mask[\"exp\"], mask[\"vol\"], ,SCATTER)\n```\n\n----------------------------------------\n\nTITLE: Removing Rows with Null Values - DolphinDB\nDESCRIPTION: This code snippet demonstrates how to remove rows from a matrix (m) that contain only null values after applying a mask. The dropna function is used to achieve this. First, a matrix 'm' is created. Then a condition `m%2!=0` is applied, converting even number places to NULL.  Then dropna is used to remove the all-null rows.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/matrix.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\n>m=1..100$10:10\n>dropna(m.at(m%2!=0))\ncol1\tcol2\tcol3\tcol4\tcol5\tcol6\tcol7\tcol8\tcol9\tcol10\n1\t11\t21\t31\t41\t51\t61\t71\t81\t91\n3\t13\t23\t33\t43\t53\t63\t73\t83\t93\n5\t15\t25\t35\t45\t55\t65\t75\t85\t95\n7\t17\t27\t37\t47\t57\t67\t77\t87\t97\n9\t19\t29\t39\t49\t59\t69\t79\t89\t99\n```\n\n----------------------------------------\n\nTITLE: Creating OLAP database and table schema\nDESCRIPTION: This code snippet demonstrates how to create a database and a partitioned table using the OLAP storage engine in DolphinDB. It defines the database partitions based on time and stock ID and creates a table schema with Timestamp, StockID, and Bid columns.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/tsdb_engine.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndbTime = database(\"\", VALUE, 2021.08.01..2021.09.01)\ndbStockID = database(\"\", HASH, [SYMBOL, 100])\n\ndb = database(directory=\"dfs://stock\",partitionType=COMPO,partitionScheme=[dbTime,dbStockID],engine=\"OLAP\")\n\nschema = table(1:0, `Timestamp`StockID`bid, [TIMESTAMP, SYMBOL, DOUBLE])\nstocks = db.createPartitionedTable(table=schema, tableName=`stocks, partitionColumns=`Timestamp`StockID)\n```\n\n----------------------------------------\n\nTITLE: Creating Partitioned Tables with Composite Partition Scheme Using Value and Hash Partitioning in DolphinDB Script\nDESCRIPTION: Creates a database partitioned by date (value) and by hash of symbol field with 25 buckets, then defines a partitioned table with stock market columns. This setup reduces partition count and increases partition size, improving query performance and memory usage. Prerequisites include DolphinDB with support for composite partitions and hash partitions. Inputs involve date range and symbol hash count; output is a partitioned table optimized for large data volumes. This snippet serves as the solution to small partition granularity issues (section 2.4.2) and shows improved access efficiency.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/database_and_table_creation_details.md#_snippet_8\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndb1 = database(, VALUE, 2020.01.01..2021.01.01)\ndb2 = database(, HASH, [SYMBOL, 25])\ndb = database(directory=\"dfs://testDB2\", partitionType=COMPO, partitionScheme=[db1, db2], engine=\"TSDB\")\ncolName = `SecurityID`TradeDate`TradeTime`TradePrice`TradeQty`TradeAmount`BuyNo`SellNo\ncolType = `SYMBOL`DATE`TIME`DOUBLE`INT`DOUBLE`INT`INT\ntbSchema = table(1:0, colName, colType)\ndb.createPartitionedTable(table=tbSchema, tableName=`pt, partitionColumns=`TradeDate`SecurityID, sortColumns=`SecurityID`TradeTime, keepDuplicates=ALL)\n```\n\n----------------------------------------\n\nTITLE: Calculating Option Gamma using Vectorized Operations (DolphinDB Script)\nDESCRIPTION: Defines functions to compute the option Gamma using vectorized operations for matrix inputs. `normpdf` calculates the standard normal probability density function. `calculateD1` computes the d1 term (reused from Delta calculation). `calculateGamma` calculates the Gamma for each element in the input matrices (`etfTodayPrice`, `KPrice`, `dayRatio`, `impvMatrix`) using `normpdf`, `calculateD1`, and vectorized conditional logic (`iif`) and arithmetic operations. Handles cases where implied volatility is non-positive.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/IV_Greeks_Calculation_for_ETF_Options_Using_JIT.md#_snippet_10\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\ndef normpdf(x){\n\treturn exp(-pow(x, 2)/2.0)/sqrt(2*pi)\n}\n\ndef calculateD1(etfTodayPrice, KPrice, r, dayRatio, HLMean){\n\tskRatio = etfTodayPrice / KPrice\n\tdenominator = HLMean * sqrt(dayRatio)\n\tresult = (log(skRatio) + (r + 0.5 * pow(HLMean, 2)) * dayRatio) / denominator\n\treturn result\n}\n\ndef calculateGamma(etfTodayPrice, KPrice, r, dayRatio, impvMatrix){\n\tgamma = iif(\n\t\t\timpvMatrix <= 0,\n\t\t\t0,\n\t\t\t(normpdf(calculateD1(etfTodayPrice,  KPrice, r, dayRatio, impvMatrix)) \\ (etfTodayPrice * impvMatrix * sqrt(dayRatio))) * pow(etfTodayPrice, 2) * 0.0001\n\t\t)\t\n\treturn gamma\n}\n```\n\n----------------------------------------\n\nTITLE: Comparing Performance of Row-wise and Column-wise String Replacement with each in DolphinDB\nDESCRIPTION: Creates a large table 't' with one million rows containing the string 'aaaa_bbbb'. Demonstrates two methods to transform each string from the format 'aaaa_bbbb' to 'bbbb_aaaa': a row-wise approach using 'each' with lambda that splits and reverses the string parts, and a column-wise vectorized approach using string functions 'strpos', 'substr', and string concatenation. Performance measurements using 'timer' reveal that the column-wise processing is roughly 20 times faster than the row-wise iteration, highlighting efficiency benefits of vectorized operations in DolphinDB. This example requires a table 't' with a string column 'str'.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/func_progr_cases.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\nt=table(take(\"aaaa_bbbb\", 1000000) as str);\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntimer r = each(x -> split(x, '_').reverse().concat('_'), t[`str])\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntimer {\n\tpos = strpos(t[`str], \"_\")\n\tr = substr(t[`str], pos+1)+\"_\"+t[`str].left(pos)\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Time-Series Aggregator DolphinDB Script\nDESCRIPTION: Configures a time-series aggregator named `demoAgg`. It is set up to process data from `sensorTemp`, calculate the average of specified temperature columns (`temp1`, `temp2`, `temp3`) over a 60-second window sliding every 2 seconds, grouped by `hardwareId`, and output the results to `sensorTempAvg`.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/iot_demo_script.txt#_snippet_4\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\ndemoAgg = createTimeSeriesAggregator(name=\"demoAgg\", windowSize=60000, step=2000, metrics=<[avg(temp1),avg(temp2),avg(temp3)]>, dummyTable=sensorTemp, outputTable=sensorTempAvg, timeColumn=`ts,  keyColumn=`hardwareId, garbageSize=2000)\n```\n\n----------------------------------------\n\nTITLE: 定义复合(组合)分区并创建分区表（DolphinDB, COMPO）\nDESCRIPTION: 该代码演示了如何使用两个不同类型的分区规则（范围和值）结合进行复合分区，支持多列多策略的灵活分区方案，并支持追加值分区。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/database.md#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\nn=1000000\nID=rand(100, n)\ndates=2017.08.07..2017.08.11\ndate=rand(dates, n)\n x=rand(10.0, n)\nt=table(ID, date, x)\n\n dbDate = database(, VALUE, 2017.08.07..2017.08.11)\n dbID=database(, RANGE, 0 50 100)\n db = database(\"dfs://compoDB\", COMPO, [dbDate, dbID])\n\npt = db.createPartitionedTable(t, `pt, `date`ID)\npt.append!(t)\n\npt=loadTable(db,`pt)\nselect count(x) from pt;\n```\n\n----------------------------------------\n\nTITLE: Defining a Function to Create Distributed Tables\nDESCRIPTION: This snippet defines a function `createDBAndTable` to create a distributed database and partitioned table. The database is partitioned by date and security ID. This function checks if table exists, and if not create it, then return the table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_sql_cases.md#_snippet_34\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef createDBAndTable(dbName, tableName) {\n\t if(existsTable(dbName, tableName)) return loadTable(dbName, tableName)\n\t dbDate = database(, VALUE, 2021.07.01..2021.07.31)\n\t dbSecurityID = database(, HASH, [SYMBOL, 10])\n\t db = database(dbName, COMPO, [dbDate, dbSecurityID])\n\t model = table(1:0, `SecurityID`Date`Time`FactorID`FactorValue, [SYMBOL, DATE, TIME, SYMBOL, DOUBLE])\n\t return createPartitionedTable(db, model, tableName, `Date`SecurityID)\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Market Snapshot Processing Rules in DolphinDB Script\nDESCRIPTION: This code defines the original snapshot table's key columns and creates derived columns based on incremental calculations. It uses DolphinDB's delta and conditional functions to compute price and volume changes between adjacent snapshots, ensuring null handling for first row cases. The definitions are stored as a list of SQL column aliases used as metrics in the reactive state engine for streaming computations.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/K.md#_snippet_12\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//Original columns in the snapshot table\ncolNames = `TradeTime`UpLimitPx`DownLimitPx`PreCloPrice`HighPrice`LowPrice`LastPrice`PreCloseIOPV`IOPV\n//Derived columns processed based on the original snapshot table\nconvert = sqlCol(colNames).append!(sqlColAlias(<iif(deltas(HighPrice)>0.000001, 1, 0)>, `DeltasHighPrice)).append!(sqlColAlias(<iif(abs(deltas(LowPrice))>0.000001, -1, 0)>, `DeltasLowPrice)).append!(sqlColAlias(<iif(deltas(TotalVolumeTrade)==NULL, TotalVolumeTrade, deltas(TotalVolumeTrade))>, `DeltasVolume)).append!(sqlColAlias(<iif(deltas(TotalValueTrade)==NULL, TotalValueTrade, deltas(TotalValueTrade))>, `DeltasTurnover)).append!(sqlColAlias(<iif(deltas(NumTrades)==NULL, NumTrades, deltas(NumTrades))>, `DeltasTradesCount))\n```\n\n----------------------------------------\n\nTITLE: Defining Function for Order Data Ingestion (DolphinDB)\nDESCRIPTION: This DolphinDB function defines a comprehensive workflow for ingesting order data from a CSV file into a partitioned TSDB database. It first defines database partitions (VALUE by date, HASH by SecurityID), creates the database and table schema, creates the partitioned table with specified sort columns and compression methods, and finally loads data from the CSV file using `loadTextEx`, applying schema updates during import. It depends on a DolphinDB server and the presence of the specified CSV file.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/appendices_market_replay_bp/order_upload.txt#_snippet_0\n\nLANGUAGE: dolphindb\nCODE:\n```\ndef orderUpload(dbName, tbName)\n{\n    path = \"C:/Users/myhu/Desktop/order.csv\"\n\n    db1 = database(, VALUE, 2021.12.01..2021.12.31)\n    db2 = database(, HASH, [SYMBOL, 25])\n    db = database(dbName, COMPO, [db1, db2], , 'TSDB')\n\n    schemaTable = table(\n        array(SYMBOL, 0) as SecurityID,\n        array(DATE, 0) as MDDate,\n        array(TIME, 0) as MDTime,\n        array(TIMESTAMP, 0) as DataTimestamp,\n        array(SYMBOL, 0) as SecurityIDSource,\n        array(SYMBOL, 0) as SecurityType,\n        array(LONG, 0) as OrderIndex,\n        array(INT, 0) as OrderType,\n        array(LONG, 0) as OrderPrice,\n        array(LONG, 0) as OrderQty,\n        array(INT, 0) as OrderBSFlag,\n        array(INT, 0) as ChannelNo,\n        array(DATE, 0) as ExchangeDate,\n        array(TIME, 0) as Exchanime,\n        array(LONG, 0) as OrderNO,\n        array(LONG, 0) as ApplSeqNum,\n        array(SYMBOL, 0) as SecurityStatus\n    )\n    \n    db.createPartitionedTable(table=schemaTable, tableName=tbName, partitionColumns=`MDDate`SecurityID, sortColumns=`SecurityID`MDTime, keepDuplicates=ALL,compressMethods={MDDate:\"delta\", MDTime:\"delta\",DataTimestamp:\"delta\",ExchangeDate:\"delta\",Exchanime:\"delta\"})\n    \n    schema = extractTextSchema(path)\n    update schema set type = \"TIMESTAMP\" where name = \"DataTimestamp\"\n    update schema set type = \"DATE\" where name = \"ExchangeDate\"\n    order = loadTextEx(dbHandle=db, tableName=`order, partitionColumns=`MDDate`SecurityID, filename=path, schema=schema, sortColumns=`SecurityID`MDTime, arrayDelimiter=\",\")\n\n}\n\ndbName = \"dfs://Test_order\"\ntbName = \"order\"\norderUpload(dbName, tbName)\n```\n\n----------------------------------------\n\nTITLE: Creating Database for Options Market Data with TSDB Engine\nDESCRIPTION: Creates a database for storing options market data using a combined partitioning strategy with daily value partitioning and HASH 20 on instrument IDs. The TSDB engine is used with instrument ID and received time as sorting columns.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practices_for_partitioned_storage.md#_snippet_11\n\nLANGUAGE: SQL\nCODE:\n```\ncreate database \"dfs://ctp_options\"\npartitioned by VALUE(2020.01.01..2021.01.01), HASH([SYMBOL, 20])\nengine='TSDB'\n\ncreate table \"dfs://ctp_options\".\"options\"(\n    TradingDay DATE[comment=\"交易日期\", compress=\"delta\"]\n    ExchangeID SYMBOL\n    LastPrice DOUBLE\n    PreSettlementPrice DOUBLE\n    PreClosePrice DOUBLE\n    PreOpenInterest DOUBLE\n    OpenPrice DOUBLE\n    HighestPrice DOUBLE\n    LowestPrice DOUBLE\n    Volume INT\n    Turnover DOUBLE\n    OpenInterest DOUBLE\n    ClosePrice DOUBLE\n    SettlementPrice DOUBLE\n    UpperLimitPrice DOUBLE\n    LowerLimitPrice DOUBLE\n    PreDelta DOUBLE\n    CurrDelta DOUBLE\n    UpdateTime SECOND\n    UpdateMillisec INT\n    BidPrice DOUBLE[]\n    BidVolume INT[]\n    AskPrice DOUBLE[]\n    AskVolume INT[]\n    AveragePrice DOUBLE\n    ActionDay DATE\n    InstrumentID SYMBOL\n    ExchangeInstID STRING\n    BandingUpperPrice DOUBLE\n    BandingLowerPrice DOUBLE\n    tradeTime TIME\n    receivedTime NANOTIMESTAMP\n    perPenetrationTime LONG\n)\npartitioned by TradingDay, InstrumentID,\nsortColumns=[`InstrumentID,`ReceivedTime],\nkeepDuplicates=ALL\n```\n\n----------------------------------------\n\nTITLE: Window Join Example in DolphinDB\nDESCRIPTION: This example demonstrates how to use a window join (`wj`). It calculates the average, maximum, and minimum values within a 5-second window before the anomaly timestamp. The `wj` function combines data from two tables (`t1` and `t2`), applying an aggregation function within a specified time window. It retrieves the summary of readings for each abnormal reading recorded in the t1 table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/iot_examples.md#_snippet_11\n\nLANGUAGE: DolphinDB\nCODE:\n```\nt1 = table(1 51 as id, 2020.09.01T09:56:06 2020.09.01T09:56:07 as datetime, 50.6 50.7 as anomalyMetrics)\nt2 = select * from sensors where id in [1,51], datetime between 2020.09.01T09:00:00 : 2020.09.01T09:59:59\nwj(t1, t2, -5000:0, <[avg(value),max(value),min(value)]>, `id`datetime)\n```\n\n----------------------------------------\n\nTITLE: Importing DolphinDB Module with use and Calling Function with Namespace\nDESCRIPTION: Imports the fileLog module using the `use` keyword. This snippet shows the method of calling a function by explicitly specifying its module namespace (fileLog::). This approach is recommended or required when multiple imported modules contain functions with the same name to avoid ambiguity. The use import is session-local.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/module_tutorial.md#_snippet_5\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nuse fileLog\nfileLog::appendLog(\"mylog.txt\", \"test my log\")\n```\n\n----------------------------------------\n\nTITLE: DolphinDB User and Permission Management Example 1\nDESCRIPTION: This example demonstrates how granting global TABLE_READ permission overrides a previously denied table-level TABLE_READ permission for a user in DolphinDB. It involves creating a user, a database, a partitioned table, denying table-level read access, granting global read access, and then verifying the user's read access.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Permission_Management.md#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(\"admin\", \"123456\")\ncreateUser(\"user1\",\"123456\")\ndbName = \"dfs://test\"\nif(existsDatabase(dbName)){\n        dropDatabase(dbName)\n}\nt = table(1..10 as id , rand(100, 10) as val)\ndb=database(dbName, VALUE, 1..10)\npt=  db.createPartitionedTable(t, \"pt\", \"id\")\npt.append!(t)\n\ndeny(\"user1\", TABLE_READ, dbName+\"/pt\")\ngrant(\"user1\", TABLE_READ, \"*\")\n\nlogin(\"user1\", \"123456\")\nselect * from loadTable(dbName, \"pt\")//user1 获得读 \"dfs://test\"的权限\n```\n\n----------------------------------------\n\nTITLE: Defining Aggregate Feature Engineering Function using DolphinDB\nDESCRIPTION: Defines a DolphinDB aggregate function that computes various financial features from level2 order book snapshots such as weighted average price (wap), price spreads, volume imbalances, and logarithmic returns. It then renames columns, creates a 10-minute bar interval, and performs SQL aggregations over different time offsets to generate feature matrices.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/metacode_derived_features/metacode_derived_features_arrayVector.txt#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndefg featureEngineering(DateTime, BidPrice, BidOrderQty, OfferPrice, OfferOrderQty, aggMetaCode){\n\twap = (BidPrice * OfferOrderQty + BidOrderQty * OfferPrice) \\ (BidOrderQty + OfferOrderQty)\n\twapBalance = abs(wap[0] - wap[1])\n\tpriceSpread = (OfferPrice[0] - BidPrice[0]) \\ ((OfferPrice[0] + BidPrice[0]) \\ 2)\n\tBidSpread = BidPrice[0] - BidPrice[1]\n\tOfferSpread = OfferPrice[0] - OfferPrice[1]\n\ttotalVolume = OfferOrderQty.rowSum() + BidOrderQty.rowSum()\n\tvolumeImbalance = abs(OfferOrderQty.rowSum() - BidOrderQty.rowSum())\n\tlogReturnWap = logReturn(wap)\n\tlogReturnOffer = logReturn(OfferPrice)\n\tlogReturnBid = logReturn(BidPrice)\n\tsubTable = table(DateTime as `DateTime, BidPrice, BidOrderQty, OfferPrice, OfferOrderQty, wap, wapBalance, priceSpread, BidSpread, OfferSpread, totalVolume, volumeImbalance, logReturnWap, logReturnOffer, logReturnBid)\n\tcolNum = 0..9$STRING\n\tcolName = `DateTime <- (`BidPrice + colNum) <- (`BidOrderQty + colNum) <- (`OfferPrice + colNum) <- (`OfferOrderQty + colNum) <- (`Wap + colNum) <- `WapBalance`PriceSpread`BidSpread`OfferSpread`TotalVolume`VolumeImbalance <- (`logReturn + colNum) <- (`logReturnOffer + colNum) <- (`logReturnBid + colNum)\n\tsubTable.rename!(colName)\n\tsubTable['BarDateTime'] = bar(subTable['DateTime'], 10m)\n\tresult = sql(select = aggMetaCode, from = subTable).eval().matrix()\n\tresult150 = sql(select = aggMetaCode, from = subTable, where = <time(DateTime) >= (time(BarDateTime) + 150*1000) >).eval().matrix()\n\tresult300 = sql(select = aggMetaCode, from = subTable, where = <time(DateTime) >= (time(BarDateTime) + 300*1000) >).eval().matrix()\n\tresult450 = sql(select = aggMetaCode, from = subTable, where = <time(DateTime) >= (time(BarDateTime) + 450*1000) >).eval().matrix()\n\treturn concatMatrix([result, result150, result300, result450])\n}\n```\n\n----------------------------------------\n\nTITLE: Using eachPre High-Order Function to Compute Conditional Cleaned Log Ratios on Sliding Window in DolphinDB\nDESCRIPTION: Creates a table 't' with random bid prices, then computes a new column 'ln' as the natural logarithm of the ratio of current 'bidPrice' over the previous 3-row moving average excluding current row, leveraging 'prev' and 'mavg'. Defines a user function 'cleanFun' applying conditional logic comparing value 'x' and previous 'y' with threshold 'F'. Then uses 'eachPre' to apply 'cleanFun' pairwise over elements in the 'ln' vector, producing a cleaned 'clean' column with suppressed large fluctuations. Illustrates functional programming with sliding window statistics and pairwise computation within DolphinDB, optimizing with built-in moving functions. Requires DolphinDB environment and table data as specified.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/func_progr_cases.md#_snippet_9\n\nLANGUAGE: DolphinDB\nCODE:\n```\nF = 0.02\nt = table(take(`a`b`c`d`e ,100) as sym, rand(100.0,100) as bidPrice)\nt2 = select *, log(bidPrice / prev(mavg(bidPrice,3))) as ln from t\ndef cleanFun(F,x,y) : iif(abs(x) > F, y,x)\nt2[`clean] = eachPre(cleanFun{F}, t2[`ln])\n```\n\n----------------------------------------\n\nTITLE: Configuring External Network Access for DolphinDB Cluster via Configuration Files Using Text Snippets\nDESCRIPTION: These configurations illustrate how to set the public/external IP addresses or domain names of cluster nodes in the cluster configuration files to enable external network access. The wildcard % is used to specify all nodes under a server alias. Adjustments must be applied to both cluster.cfg (for data and compute nodes) and controller.cfg (for the control node). Specifying domain names is mandatory when enabling HTTPS access.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/multi_machine_cluster_deployment.md#_snippet_1\n\nLANGUAGE: Configuration\nCODE:\n```\nP1-%.publicName=19.56.128.21\nP2-%.publicName=19.56.128.22\nP3-%.publicName=19.56.128.23\n```\n\nLANGUAGE: Configuration\nCODE:\n```\npublicName=19.56.128.21\n```\n\n----------------------------------------\n\nTITLE: Querying User CPU and Memory Usage Statistics in DolphinDB\nDESCRIPTION: SQL query to retrieve the sum of CPU and memory usage by user from all data nodes in a DolphinDB cluster since a specific date. Requires administrator access to execute successfully.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/user_level_resource_tracking.md#_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nlogin(\"admin\", \"123456\")\n// Get data node aliases\ndataNodeAlias = exec name from rpc(getControllerAlias(), getClusterPerf) where mode in [0,3]\n// Calculate CPU and memory usage by user\nselect sum(cpu), sum(memory) from pnodeRun(getUserHardwareUsage{2023.12.13}, dataNodeAlias) group by userId\n```\n\n----------------------------------------\n\nTITLE: Optimized Moving Weighted Average Calculation Using context by in DolphinDB\nDESCRIPTION: An optimized approach that calculates the moving weighted average for all stocks in a single pass using context by, which significantly improves performance.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_SQL_Case_Tutorial.md#_snippet_18\n\nLANGUAGE: SQL\nCODE:\n```\ntimer res2 = select mwavg(price, volume, 4) from t \n\t\t\t   context by symbol\n```\n\n----------------------------------------\n\nTITLE: Querying - Range Query (Single Dimension) DolphinDB\nDESCRIPTION: This snippet performs a range query to select all records within a specified time interval. This demonstrates a simple range query based on time, a single dimension partition in this case. This is useful to analyze data over a specific period.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs TimescaleDB/test_dolphindb_small.txt#_snippet_12\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 3. 范围查询.单分区维度：查询某时间段内的所有记录\ntimer\nselect *\nfrom readings\nwhere time between 2016.11.17 21:00:00 : 2016.11.17 21:30:00\n```\n\n----------------------------------------\n\nTITLE: Registering Snapshot Engine\nDESCRIPTION: Registers a snapshot engine for a distributed table.  It takes the database name, table name, and the name of the column to group by as input. This engine maintains a snapshot of the most recent record for each group, enabling fast queries of the latest data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/snapshot_engine.md#_snippet_0\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nregisterSnapshotEngine(dbName, tableName, keyColumnName)\n```\n\n----------------------------------------\n\nTITLE: Querying Job Execution Times for Single and Multi-User Jobs in DolphinDB\nDESCRIPTION: Provides queries to measure execution times from recent jobs metadata. One query calculates elapsed time for the single user job 'parallJob_single_nine' and the other calculates the total elapsed time across multiple users' jobs 'parallJob_multi_nine', facilitating performance comparison and monitoring.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_factor_by_ddb/fund_factor_nine.txt#_snippet_13\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect endTime - startTime from getRecentJobs() where jobDesc = \"parallJob_single_nine\"\n\nselect max(endTime) - min(startTime) from getRecentJobs() where jobDesc = \"parallJob_multi_nine\"\n```\n\n----------------------------------------\n\nTITLE: Query Wide Table Data in DolphinDB\nDESCRIPTION: This function queries factor data stored in a wide table format within a specified time range for a given set of factors. It constructs a SQL query string that selects the desired columns (tradetime, symbol, and the specified factors) from the table and then executes the query using `parseExpr` and `eval`. Dependencies are `loadTable`, `tradetime`, `symbol`, and the factor columns.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practices_for_multi_factor.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 宽表模式查询随机1000因子\ndef queryWideModel(dbname,tbname,start_time,end_time,aim_factor){\n\tll = aim_factor[0]\n\tfor(i in 1..(aim_factor.size()-1)){\n\t\tll = ll+\",\"+aim_factor[i]\n\t}\n\tscript = \"select tradetime,symbol,\"+ll+\"from loadTable(\"+'\"'+dbname+'\"'+\",'\"'+tbname+'\"'+\")\" + \"where tradetime>=\"+start_time+\"and tradetime<=\"+end_time\n\ttt = parseExpr(script).eval()\n\treturn tt\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Random ETF Components for Multiple Indices in DolphinDB\nDESCRIPTION: Defines a function to generate 100 ETF indices by randomly selecting 50 stocks from the Shenzhen market and assigning random positions for each component.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_IOPV.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ndef getBasketData(allSymbol, n){\n        return loop(x->table(take(x, 50) as BasketID, rand(allSymbol, 50) as SecurityID, rand(76339..145256, 50) as Vol), 1..n).unionAll(false)\n}\n\ntrade = loadTable(\"dfs://LEVEL2_SZ\",\"Trade\")\nallSyms = select count(*) from trade where date(tradetime) = 2020.01.02 group by SecurityID\nbasket = getBasketData(allSyms.SecurityID, 100)\n```\n\n----------------------------------------\n\nTITLE: Creating Daily Frequency Factor Library Table with TSDB Engine in DolphinDB\nDESCRIPTION: This DolphinDB snippet creates a TSDB database and table designed to store daily frequency factor data. Partitioning combines a yearly range on tradetime and multiple factor names (e.g., f1, f2), supporting composite partitioning by time dimension and factor name. The table includes tradetime, security ID, factor value, and factor name columns. Sorting is by securityid and tradetime with a custom sortKeyMappingFunction to limit sort key cardinality, optimizing write performance and query efficiency in time-series data environments.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practices_for_partitioned_storage.md#_snippet_21\n\nLANGUAGE: DolphinDB\nCODE:\n```\ncreate database \"dfs://dayFactorDB\" \npartitioned by RANGE(date(datetimeAdd(1980.01M,0..80*12,'M'))), VALUE(`f1`f2), \nengine='TSDB'\n\ncreate table \"dfs://dayFactorDB\".\"dayFactorTB\"(\n    tradetime DATE[comment=\"时间列\", compress=\"delta\"], \n    securityid SYMBOL, \n    value DOUBLE, \n    factorname SYMBOL\n)\npartitioned by tradetime, factorname,\nsortColumns=[`securityid, `tradetime], \nkeepDuplicates=ALL, \nsortKeyMappingFunction=[hashBucket{, 500}]\n```\n\n----------------------------------------\n\nTITLE: Implementing Streaming Buy Trade Ratio Calculation in DolphinDB\nDESCRIPTION: Creates a reactive state engine for real-time streaming calculation of the buy trade ratio factor. Demonstrates how the same factor function can be used in both batch and streaming contexts without modification.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practice_for_factor_calculation.md#_snippet_10\n\nLANGUAGE: DolphinDB\nCODE:\n```\n@state\ndef buyTradeRatio(buyNo, sellNo, tradeQty){\n    return cumsum(iif(buyNo>sellNo, tradeQty, 0))\\cumsum(tradeQty)\n}\n\ntickStream = table(1:0, `SecurityID`TradeTime`TradePrice`TradeQty`TradeAmount`BuyNo`SellNo, [SYMBOL,DATETIME,DOUBLE,INT,DOUBLE,LONG,LONG])\nresult = table(1:0, `SecurityID`TradeTime`Factor, [SYMBOL,DATETIME,DOUBLE])\nfactors = <[TradeTime, buyTradeRatio(BuyNo, SellNo, TradeQty)]>\ndemoEngine = createReactiveStateEngine(name=\"demo\", metrics=factors, dummyTable=tickStream, outputTable=result, keyColumn=\"SecurityID\")\n\ninsert into demoEngine values(`000155, 2020.01.01T09:30:00, 30.85, 100, 3085, 4951, 0)\ninsert into demoEngine values(`000155, 2020.01.01T09:30:01, 30.86, 100, 3086, 4951, 1)\ninsert into demoEngine values(`000155, 2020.01.01T09:30:02, 30.80, 200, 6160, 5501, 5600)\n```\n\n----------------------------------------\n\nTITLE: Creating Futures Minute K-Line Table Using DolphinDB\nDESCRIPTION: This snippet creates a DolphinDB database and table for storing futures minute-level K-line data. It shares the same database name and schema as the daily K data example but uses a time dimension partitioning scheme by day rather than by year, suitable for minute-frequency data. The storage engine is OLAP and the data is partitioned by trading time within each day, allowing efficient storage and retrieval of minute-level futures data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practices_for_partitioned_storage.md#_snippet_14\n\nLANGUAGE: DolphinDB\nCODE:\n```\ncreate database \"dfs://ctp_k_day_level\"\npartitioned by RANGE(2000.01M + (0..30)*12)\nengine='OLAP'\n\ncreate table \"dfs://ctp_k_day_level\".\"ctp_k_day\"(\n\tsecurityid SYMBOL  \n\ttradetime TIMESTAMP\n\topen DOUBLE        \n\tclose DOUBLE       \n\thigh DOUBLE        \n\tlow DOUBLE\n\tvol INT\n\tval DOUBLE\n\tvwap DOUBLE\n)\npartitioned by tradetime\n```\n\n----------------------------------------\n\nTITLE: Creating Database and Registering Snapshot Engine\nDESCRIPTION: Creates a composite distributed database and a partitioned table, then registers the snapshot engine to fetch the latest records for each stock symbol in the partitioned table. This snippet demonstrates the process of setting up the database, table, and snapshot engine to track the most recent stock data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/snapshot_engine.md#_snippet_1\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\ndb1=database(\"\",VALUE,2018.09.01..2018.09.30)\ndb2=database(\"\",VALUE,`AAPL`MSFT`MS`C)\ndb=database(\"dfs://compoDB\",COMPO,[db1,db2])\nt=table(1:0,`ts`sym`val,[DATETIME,SYMBOL,DOUBLE])\npt=db.createPartitionedTable(t,`pt,`ts`sym);\nregisterSnapshotEngine(\"dfs://compoDB\",\"pt\",\"sym\")\n```\n\n----------------------------------------\n\nTITLE: Creating Stream Tables for Trade and Market Data in DolphinDB\nDESCRIPTION: This snippet defines a function to create and configure two DolphinDB stream tables: one for message logs and another for prevailing market quotes. It enables sharing, persistence, and cache settings to optimize real-time data ingestion.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/stock_market_replay/03.calTradeCost_lookUpJoin.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\n/**\ncalTradeCost_lookUpJoin.txt\nScript to use look up join engine to calculate trade cost\nDolphinDB Inc.\nDolphinDB server version: 2.00.6 2022.05.31\nStorage engine: TSDB\nLast modification time: 2022.07.07\n*/\n\n//login account\nlogin(\"admin\", \"123456\")\n\ndef createStreamTableFunc(){\n\t//create stream table: messageStream\n\tcolName = `msgTime`msgType`msgBody\n\tcolType = [TIMESTAMP,SYMBOL, BLOB]\n\tmessageTemp = streamTable(5000000:0, colName, colType)\n\tenableTableShareAndPersistence(table=messageTemp, tableName=\"messageStream\", asynWrite=true, compress=true, cacheSize=5000000, retentionMinutes=1440, flushMode=0, preCache=10000)\n\tmessageTemp = NULL\n\t//create stream table: prevailingQuotes\n\tcolName = `SecurityID`tradeTime`Price`TradeQty`BidPX1`OfferPX1`Spread`snapshotTime\n\tcolType = [SYMBOL, TIME, DOUBLE, INT, DOUBLE, DOUBLE, DOUBLE, TIME]\n\tprevailingQuotesTemp = streamTable(100000:0, colName, colType)\n\tenableTableShareAndPersistence(table=prevailingQuotesTemp, tableName=\"prevailingQuotes\", asynWrite=true, compress=true, cacheSize=100000, retentionMinutes=1440, flushMode=0, preCache=10000)\n\tprevailingQuotesTemp = NULL\n}\ncreateStreamTableFunc()\ngo\n\n```\n\n----------------------------------------\n\nTITLE: Calculating Alpha 98 Factor Using DolphinDB Panel Data - DolphinDB\nDESCRIPTION: This snippet computes the Alpha 98 factor as a panel-based operation, involving complex windowed statistics such as correlation, moving sum, and ranking across multi-dimensional input data. The prepareDataForDDBPanel function shapes raw database tables into aligned panels of vwap, open, vol fields. The main alpha98Panel function then performs nested moving averages, decay, and rowRank calculations as per the Alpha 98 formula. The snippet requires the k_day table and expects consistent time ranges and security id matching for all input vectors.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practice_for_factor_calculation.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//alpha 98\n//Alpha #98计算公式:\n//(rank(decay_linear(correlation(vwap, sum(adv5, 26.4719), 4.58418), 7.18088)) -\n//rank(decay_linear(Ts_Rank(Ts_ArgMin(correlation(rank(open), rank(adv15), 20.8187), 8.62571), 6.95668), 8.07206)))\n\ndef prepareDataForDDBPanel(raw_data, start_time, end_time){\n\tt = select tradetime,securityid, vwap,vol,open from raw_data where date(tradetime) between start_time : end_time\n\treturn dict(`vwap`open`vol, panel(t.tradetime, t.securityid, [t.vwap, t.open, t.vol]))\n}\n\n@state\ndef alpha98Panel(vwap, open, vol){\n\treturn rowRank(X = mavg(mcorr(vwap, msum(mavg(vol, 5), 26), 5), 1..7),percent=true) - rowRank(X=mavg(mrank(9 - mimin(mcorr(rowRank(X=open,percent=true), rowRank(X=mavg(vol, 15),percent=true), 21), 9), true, 7), 1..8),percent=true)\n}\n\nraw_data = loadTable(\"dfs://k_minute\",\"k_day\")\nstart_time = 2020.01.01\nend_time = 2020.12.31\ninput = prepareDataForDDBPanel(raw_data, start_time, end_time)\ntimer alpha98DDBPanel = alpha98Panel(input.vwap, input.open, input.vol)\n\n```\n\n----------------------------------------\n\nTITLE: Statistical Measures and Data Analysis Functions in pandas and DolphinDB\nDESCRIPTION: This snippet includes statistical functions such as variance, covariance, correlation, standard deviation, median, skewness, kurtosis, and others. It explains their usage in pandas and various equivalents in DolphinDB for comprehensive data analysis.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/function_mapping_py.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\npandas.Series.cov  # Covariance between Series\npandas.DataFrame.ewm.cov  # Exponentially weighted covariance\npandas.DataFrame.corr / pandas.Series.corr  # Correlation coefficients\npandas.DataFrame.std / pandas.Series.std  # Standard deviation\npandas.DataFrame.median / pandas.Series.median  # Median value\npandas.DataFrame.kurt(kurtosis) / pandas.Series.kurt(kurtosis)  # Kurtosis\npandas.DataFrame.skew / pandas.Series.skew  # Skewness\n```\n\n----------------------------------------\n\nTITLE: Batch Writing Data into DolphinDB using C# API\nDESCRIPTION: This C# snippet demonstrates establishing a connection to DolphinDB and batch inserting data. It constructs lists for column names and values, converts these lists into BasicVector types, and creates a BasicTable for batch uploading via the tableInsert method. The snippet includes exception handling for IO errors. Dependencies include DolphinDB C# API and System.Collections.Generic. Inputs include server connection info and data arrays; outputs are the written data in the DolphinDB distributed table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/iot_examples.md#_snippet_25\n\nLANGUAGE: c#\nCODE:\n```\nDBConnection conn = new DBConnection();\ntry { \n    conn.connect(SERVER, PORT, USER, PASSWORD);\n\n\tList<String> colNames = new List<string>() { \"id\", \"datetime\", \"value\" };\n\n    List<int> idArray = new List<int>() { 1, 2 };\n    List<long> datetimeArray = new List<long>() { 1600735563458, 16007355634582 };\n    List<Double> valueArray = new List<Double>() { 22.3, 43.1 };\n\n    List<IVector> cols = new List<IVector>() { new BasicIntVector(idArray.ToArray()), new BasicTimestampVector(datetimeArray.ToArray()), new BasicDoubleVector(valueArray.ToArray()) };\n    BasicTable data = new BasicTable(colNames, cols);\n    List<IEntity> args = new List<IEntity>(1);\n    args.Add(data);\n    conn.run(String.Format(\"tableInsert{{loadTable('{0}','{1}')}}\", \"dfs://svmDemo\", \"sensors\"), args);\n}\ncatch (IOException ex)\n{\n    Console.WriteLine(ex.StackTrace);\n}\n```\n\n----------------------------------------\n\nTITLE: Logistic Regression Model Training\nDESCRIPTION: This code trains a logistic regression model using the `logisticRegression` function. It takes the data source `ds` (created in the previous snippet), the target column name `Target`, and a list of feature column names as input. The trained model is stored in the `model` variable. Assumes distributed database 'dfs://trades' exists.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/machine_learning.md#_snippet_9\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nmodel = logisticRegression(ds, `Target, `Open`High`Low`Close`OpenClose`OpenOpen`S_10`RSI`Corr)\n```\n\n----------------------------------------\n\nTITLE: Creating Reactive State Engine for Price Change Calculation in DolphinDB\nDESCRIPTION: Defines a stateful function `calculateChange` (marked with `@state`) to compute percentage price change relative to the price 1, 5, and 10 minutes ago using `tmfirst` and `tmove`. Creates a reactive state engine named `calChange` using `createReactiveStateEngine`, applying this function to calculate the metrics, keyed by `SecurityID`, filtering for data from 09:30:00 onwards, and outputting the results to another engine, `crossSectionalEngine`. Requires `crossSectionalEngine` to exist first.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Real-Time_Stock_Price_Increase_Calculation.md#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\n@state\ndef calculateChange(DateTime, LastPx, lag){\n\twindowFirstPx = tmfirst(DateTime, LastPx, lag)\n\tpreMinutePx = tmove(DateTime, LastPx, lag)\n\tprevLastPx = iif(preMinutePx == NULL, windowFirstPx, preMinutePx)\n\treturn 100 * (LastPx - prevLastPx) \\ prevLastPx\n}\n\ncreateReactiveStateEngine(name=\"calChange\", metrics=<[DateTime, calculateChange(DateTime, LastPx, lag=1m), calculateChange(DateTime, LastPx, lag=5m), calculateChange(DateTime, LastPx, lag=10m)]>, dummyTable=objByName(\"snapshotStreamTable\"), outputTable=getStreamEngine(\"crossSectionalEngine\"), keyColumn=`SecurityID, filter=<time(DateTime) >= 09:30:00.000>)\n```\n\n----------------------------------------\n\nTITLE: Creating an In-Memory Table in DolphinDB\nDESCRIPTION: This code snippet demonstrates how to create an in-memory table using the `table` function in DolphinDB. It creates a table with three columns (id, x, y) and 100 rows, populated with sample data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Single_Node_Tutorial.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\nt1 = table(take(1..10, 100) as id, rand(10, 100) as x, rand(10.0, 100) as y);\n```\n\n----------------------------------------\n\nTITLE: 定义范围分区并创建分区表（DolphinDB, RANGE）\nDESCRIPTION: 这段代码说明了如何在DolphinDB中使用范围分区（RANGE）类型创建分布式数据库，定义区间边界、创建分区表并插入数据，最后加载查询。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/database.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\nn=1000000\nID=rand(10, n)\n x=rand(1.0, n)\nt=table(ID, x)\ndb=database(\"dfs://rangedb\", RANGE,  0 5 10)\n\npt = db.createPartitionedTable(t, `pt, `ID)\npt.append!(t)\n\npt=loadTable(db,`pt)\nselect count(x) from pt;\n```\n\n----------------------------------------\n\nTITLE: Logging Into DolphinDB Server - DolphinDB Script\nDESCRIPTION: Logs in to the DolphinDB server using provided credentials to enable subsequent operations. This establishes the session required for interacting with data and plugins. Requires a running DolphinDB instance; inputs are username and password as strings.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/stock_market_replay/04.publishToKafka.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(\"admin\", \"123456\")\n```\n\n----------------------------------------\n\nTITLE: Querying with Partition Key for Optimization in DolphinDB\nDESCRIPTION: This DolphinDB script executes a query using the partition key ('dateb') in the WHERE clause. This allows DolphinDB to optimize the query by directly accessing only the relevant partition(s), leading to better performance.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_Explain.md#_snippet_13\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nselect [HINT_EXPLAIN] * from pt where dateb = 2000.01.01;\n```\n\n----------------------------------------\n\nTITLE: Replaying Historical Market Snapshot Data as Streaming Input in DolphinDB Script\nDESCRIPTION: This code loads historical snapshot data filtered by a specified date and replays it as a streaming data source into the raw snapshot streaming table. It specifies the date and time columns, and sets replayRate to -1 for fastest replay simulation, enabling testing and debugging of K-line computation workflows with historical data replayed in real time.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/K.md#_snippet_20\n\nLANGUAGE: DolphinDB\nCODE:\n```\nreplayData =\tselect *\n\t\tfrom loadTable(\"dfs://snapshotDB\", \"snapshotTB\")\n\t\twhere TradeTime.date()=2023.02.01\n\t\torder by TradeTime\nreplay(\n\tinputTables=replayData,\n\toutputTables=mdlSnapshot,\n\tdateColumn=`TradeTime,\n\ttimeColumn=`TradeTime,\n\treplayRate=-1)\n```\n\n----------------------------------------\n\nTITLE: Querying Across Partitions on Different Nodes (OLAP) (DolphinDB Script)\nDESCRIPTION: Calculates the maximum value of 'tag1' across two partitions ('2022.01.02', '2022.01.03') stored on different nodes. This demonstrates distributed query execution where computation happens locally on each node holding the relevant partition data, minimizing cross-node data transfer.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/memory_management.md#_snippet_7\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nselect max(tag1) from loadTable(dbName,tableName) where day in [2022.01.02,2022.01.03]\nmem().allocatedBytes - mem().freeBytes\n```\n\n----------------------------------------\n\nTITLE: Executing Various Join Operations Including Cross, Inner, Outer, and Left Semi Join in DolphinDB SQL\nDESCRIPTION: Includes multiple join operation examples showing different join types (cross join, inner join, left join, left semi join, right join, full join) benchmarking their SQL92 and SQL99 syntax forms where applicable. It highlights enhanced join capabilities in DolphinDB 2.00.10 including multi-table cascading joins and compatibility with various table types (memory, distributed, dimension, and subqueries). The left semi join, although non-standard, is supported for efficient distributed querying. The examples focus on employee, department, and job history data, illustrating relational data enrichment scenarios.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Standard_SQL_in_DolphinDB.md#_snippet_6\n\nLANGUAGE: SQL\nCODE:\n```\nselect * \nfrom employees a, employees b\nwhere a.employee_id <> b.employee_id\n```\n\nLANGUAGE: SQL\nCODE:\n```\nselect * \nfrom employees a\ncross join employees b\nwhere a.employee_id <> b.employee_id\n```\n\nLANGUAGE: SQL\nCODE:\n```\nselect e1.employee_id, e1.manager_id\nfrom employees e1, employees e2\nwhere e1.manager_id = e2.employee_id\norder by e1.employee_id, e1.manager_id\n```\n\nLANGUAGE: SQL\nCODE:\n```\nselect e1.employee_id, e1.manager_id\nfrom employees e1 \ninner join employees e2\non e1.manager_id = e2.employee_id\norder by e1.employee_id, e1.manager_id\n```\n\nLANGUAGE: SQL\nCODE:\n```\nselect e1.employee_id, e1.last_name, e2.last_name as manager_name  \nfrom employees e1\nleft join employees e2 \non e1.manager_id = e2.employee_id\n```\n\nLANGUAGE: SQL\nCODE:\n```\nselect department_id, department_name,employee_id, first_name, last_name, salary\nfrom departments \nleft semi join employees\non departments.department_id = employees.department_id \n       and employees.salary > 2500\norder by department_id\n```\n\nLANGUAGE: SQL\nCODE:\n```\nselect department_id, employee_id, first_name, last_name, salary\nfrom departments \nright join employees\non departments.department_id = employees.department_id \n       and employees.salary > 2500\norder by department_id\n```\n\nLANGUAGE: SQL\nCODE:\n```\nselect department_id, department_name, employee_id, first_name, last_name, salary\nfrom departments a\nfull join employees b\non a.department_id = b.department_id \n```\n\nLANGUAGE: SQL\nCODE:\n```\nselect j.job_id, j.job_title, j.min_salary\n  , h.start_date, h.end_date\nfrom jobs j  \nleft join job_history h \non h.job_id = j.job_id \n```\n\nLANGUAGE: SQL\nCODE:\n```\n select a.employee_id, a.last_name, a.manager_id, b.last_name as manager_name\n    , a.department_id, c.department_name\n from employees a\n inner join employees b \n on a.manager_id = b.employee_id \n inner join departments c \n on a.department_id = c.department_id \n```\n\n----------------------------------------\n\nTITLE: Creating and Subscribing to Time-Series Engine in DolphinDB\nDESCRIPTION: Configures the metrics for the stream engine by applying the `featureEngine` function to matrix representations of the bid/offer data from `snapshotStream`. Creates a time-series engine named `aggrFeatures10min` using `createTimeSeriesEngine` to calculate these metrics over a 10-minute sliding window (`windowSize=600000ms`) with a 1-minute step (`step=60000ms`), grouping by `SecurityID` and using `TradeTime` as the time column. The results are output to the `aggrFeatures10min` stream table. Finally, it subscribes the engine to the `snapshotStream` table using `subscribeTable` to process incoming data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/machine_learning_volatility/06.streamComputingReproduction.txt#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\nmetrics=<featureEngine(\n\tmatrix(BidPrice0,BidPrice1,BidPrice2,BidPrice3,BidPrice4,BidPrice5,BidPrice6,BidPrice7,BidPrice8,BidPrice9),\n\tmatrix(BidOrderQty0,BidOrderQty1,BidOrderQty2,BidOrderQty3,BidOrderQty4,BidOrderQty5,BidOrderQty6,BidOrderQty7,BidOrderQty8,BidOrderQty9),\n\tmatrix(OfferPrice0,OfferPrice1,OfferPrice2,OfferPrice3,OfferPrice4,OfferPrice5,OfferPrice6,OfferPrice7,OfferPrice8,OfferPrice9),\n\tmatrix(OfferOrderQty0,OfferOrderQty1,OfferOrderQty2,OfferOrderQty3,OfferOrderQty4,OfferOrderQty5,OfferOrderQty6,OfferOrderQty7,OfferOrderQty8,OfferOrderQty9)) as `BAS`DI0`DI1`DI2`DI3`DI4`DI5`DI6`DI7`DI8`DI9`Press`RV>\n//register stream computing engine\ncreateTimeSeriesEngine(name=\"aggrFeatures10min\", windowSize=600000, step=60000, metrics=metrics, dummyTable=snapshotStream, outputTable=aggrFeatures10min, timeColumn=`TradeTime, useWindowStartTime=true, keyColumn=`SecurityID)\n//subscribe data\nsubscribeTable(tableName=\"snapshotStream\", actionName=\"aggrFeatures10min\", offset=-1, handler=getStreamEngine(\"aggrFeatures10min\"), msgAsTable=true, batchSize=2000, throttle=1, hash=0, reconnect=true)\n```\n\n----------------------------------------\n\nTITLE: Filtering Records Using Predicate Keywords in DolphinDB SQL\nDESCRIPTION: Demonstrates filtering techniques in DolphinDB SQL using predicate keywords such as (not) in, (not) like, between, (not) exists, and is (not) null. Each snippet shows how to select records based on membership, pattern matching, range checks, existence in subqueries, or nullability tests. Dependencies include the existence of specific tables like 'employees', 'departments', and 'job_history'. Inputs are table data fields, and outputs are filtered record sets satisfying the predicate conditions. The exists predicate has a noted limitation when used in distributed queries.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Standard_SQL_in_DolphinDB.md#_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nselect * from employees where EMPLOYEE_ID in [101, 103, 152];\n```\n\nLANGUAGE: SQL\nCODE:\n```\nselect * from employees where EMPLOYEE_ID not in 100..150;\n```\n\nLANGUAGE: SQL\nCODE:\n```\nselect * from employees where PHONE_NUMBER like \"515%\";\n```\n\nLANGUAGE: SQL\nCODE:\n```\nselect * from employees where JOB_ID not like \"AD%\";\n```\n\nLANGUAGE: SQL\nCODE:\n```\nselect count(*) from employees where date(HIRE_DATE) between 2006.01.01 and 2006.12.31\n```\n\nLANGUAGE: SQL\nCODE:\n```\njob_history = select * from loadTable(\"dfs://hr\", \"job_history\")\nemployees = select * from loadTable(\"dfs://hr\", \"employees\")\n```\n\nLANGUAGE: SQL\nCODE:\n```\nselect * from employees where exists(select * from job_history where employees.EMPLOYEE_ID in job_history.EMPLOYEE_ID)\n```\n\nLANGUAGE: SQL\nCODE:\n```\nselect * from employees where not exists(select * from job_history where employees.EMPLOYEE_ID in job_history.EMPLOYEE_ID)\n```\n\nLANGUAGE: SQL\nCODE:\n```\nselect * from departments where MANAGER_ID is not null\n```\n\nLANGUAGE: SQL\nCODE:\n```\nselect * from employees where COMMISSION_PCT is null\n```\n\n----------------------------------------\n\nTITLE: Selecting Principal Components\nDESCRIPTION: This code selects the first three principal components from the `components` matrix obtained from the PCA result.  The transpose of the components matrix is taken, and then the first three rows (corresponding to the first three components) are selected.  These components will be used to transform the data into a lower-dimensional space.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/machine_learning.md#_snippet_13\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\ncomponents = pcaRes.components.transpose()[:3]\n```\n\n----------------------------------------\n\nTITLE: Calculating Hist Factor Using Rolling Statistics in DolphinDB\nDESCRIPTION: Defines 'calAllRs2' function that calculates a hist factor based on rolling cumulative returns statistics over a window k. It processes a matrix of returns (mret), a symbol list (symList), and window size k to compute demeaned cumulative sums, max-min ranges, standardized rolling metrics, and returns a table containing factor values. It requires the input of return matrix and corresponding symbol identifiers.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_factor_by_ddb/fund_factor_nine.txt#_snippet_9\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef calAllRs2(mret, symList, k){\n        rowCount = mret.rows()/k * k\n        demeanCum = rolling(cumsum, mret[0:rowCount,] - each(stretch{, rowCount}, rolling(avg, mret, k, k)), k, k)\n        a = rolling(max, demeanCum, k, k) - rolling(min, demeanCum, k, k)\n        RS = nullFill!(a/rolling(stdp, mret, k, k), 1.0).mean().log()\n        return table(symList as fundNum, take(log(k), symList.size()) as knum, RS as factor1)\n}\n```\n\n----------------------------------------\n\nTITLE: Creating and Joining a Simple Background Thread in Plugin C++\nDESCRIPTION: Illustrates the basic method for creating a background thread in a DolphinDB plugin using the `Thread` and `Runnable` classes. A custom class (`DemoRun`) implementing `Runnable` is defined to contain the thread logic, and the main function (`createThread`) starts and immediately joins the thread, blocking until it completes.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_advance.md#_snippet_21\n\nLANGUAGE: C++\nCODE:\n```\nclass DemoRun : public Runnable {\npublic:\n    DemoRun(ConstantSP data) : data_(data) {}\n    ~DemoRun() {}\n    void run() override {\n        size_t size = data_->size();\n        for (size_t i = 0; i < size; ++i) {\n            std::cout << data_->getInt(i) << std::endl;\n        }\n    }\nprivate:\n    ConstantSP data_;\n};\n\nConstantSP createThread(Heap *heap, vector<ConstantSP> &arguments) {\n    if (!(arguments[0]->isVector() && arguments[0]->getType() == DT_INT)) {\n        throw IllegalArgumentException(\"createThread\", \"argument must be an integral vector\");\n    }\n    SmartPointer<DemoRun> demoRun = new DemoRun(arguments[0]);\n    ThreadSP thread = new Thread(demoRun);\n    if (!thread->isStarted()) {\n        thread->start();\n    }\n    thread->join();\n    return new Void();\n}\n```\n\n----------------------------------------\n\nTITLE: Previewing Schema and Data of Historical Quote Table in DolphinDB Script\nDESCRIPTION: Illustrates loading the distributed quote table from the DFS database, inspecting its schema, and previewing data for validation prior to replay or analytics. The script outputs schema column definitions and selects sample rows for a specific trading day. Requires a pre-existing quotes table within DolphinDB’s distributed database.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/historical_data_replay.md#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\nquotes = loadTable(\"dfs://TAQ\", \"quotes\")\nquotes.schema().colDefs;\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect top 10 * from quotes where date=2007.08.17;\n```\n\n----------------------------------------\n\nTITLE: Configuring Cluster.cfg for Master Cluster\nDESCRIPTION: This snippet modifies the cluster.cfg file in the master cluster to configure the replication mode and the directory to store replication files, also sets the synchronization of persistent data. Prerequisites include access to the server and the DolphinDB installation directory. It sets the mode to master to define the cluster as a master cluster, specifies the working directory for replication, and sets the persistence to false by default.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Asynchronous_Replication.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nvim ./cluster.cfg\n```\n\n----------------------------------------\n\nTITLE: Cumulative Sum Computation with CumSum Function Grouped by Symbol in DolphinDB SQL\nDESCRIPTION: Demonstrates cumulative summation over a column 'vol' in a table ordered by time using the cumsum function. It shows basic cumulative sum calculation and an example applying cumulative sum grouped by symbol ('sym') using the context by clause. Inputs are tables with time, volume, and symbol columns. Output includes cumulative volume columns computed per group or overall. Useful for cumulative window operations with step size 1 row. No particular version limitation specified.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/window_cal.md#_snippet_10\n\nLANGUAGE: DolphinDB\nCODE:\n```\nt=table(2021.11.01T10:00:00..2021.11.01T10:00:04 join 2021.11.01T10:00:06..2021.11.01T10:00:10 as time,1..10 as vol)\nselect *, cumsum(vol) from t \n\n# output\n\ntime                vol cum_vol\n------------------- --- -------\n2021.11.01T10:00:00 1   1      \n2021.11.01T10:00:01 2   3      \n2021.11.01T10:00:02 3   6      \n2021.11.01T10:00:03 4   10     \n2021.11.01T10:00:04 5   15     \n2021.11.01T10:00:06 6   21     \n2021.11.01T10:00:07 7   28     \n2021.11.01T10:00:08 8   36     \n2021.11.01T10:00:09 9   45     \n2021.11.01T10:00:10 10  55     \n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nt=table(2021.11.01T10:00:00 + 0 1 2 5 6 9 10 17 18 30 join 0 1 2 5 6 9 10 17 18 30 as time, 1..20 as vol, take(`A,10) join take(`B,10) as sym)\nselect*, cumsum(vol) as cumsum_vol from t context by sym\n\n# output\n\ntime                vol sym cumsum_vol\n------------------- --- --- ----------\n2021.11.01T10:00:00 1   A   1         \n2021.11.01T10:00:01 2   A   3         \n...      \n2021.11.01T10:00:18 9   A   45        \n2021.11.01T10:00:30 10  A   55        \n2021.11.01T10:00:00 11  B   11        \n2021.11.01T10:00:01 12  B   23        \n...      \n2021.11.01T10:00:18 19  B   135       \n2021.11.01T10:00:30 20  B   155       \n```\n\n----------------------------------------\n\nTITLE: 结果验证连续区间最值\nDESCRIPTION: 验证两种实现方式计算结果的一致性。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_SQL_Case_Tutorial.md#_snippet_28\n\nLANGUAGE: DolphinDB\nCODE:\n```\neach(eqObj, res1.values(), res2.values()) // true\n```\n\n----------------------------------------\n\nTITLE: Loading a Model from Disk\nDESCRIPTION: This snippet demonstrates loading a previously saved model from disk using the `loadModel` function. The loaded model is then used to make predictions on the `wineTest` dataset. The path specified must match the path where the model was originally saved.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/machine_learning.md#_snippet_6\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nmodel = loadModel(\"D:/model/wineModel.bin\")\npredicted = model.predict(wineTest)\n```\n\n----------------------------------------\n\nTITLE: Row Weighted Average Calculation\nDESCRIPTION: Calculates the weighted average of each row in an Array Vector and Columnar Tuple using the `rowWavg` function in DolphinDB.  It demonstrates the function's application with a specified weight vector and includes examples for both Array Vectors and Columnar Tuples, including its integration within a table query.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Array_Vector.md#_snippet_12\n\nLANGUAGE: DolphinDB\nCODE:\n```\nx = array(INT[], 0).append!([1 2 3, 4 5 6, 6 7 8, 9 10 11])\nz = rowWavg(x, [1, 1, 2])\n/* z\n[2.25,5.25,7.25,10.25]\n*/\n\ny = [1 2 3, 4 5 6, 6 7 8, 9 10 11].setColumnarTuple!()\nz = rowWavg(y, [1, 1, 2])\n/* z\n[2.25,5.25,7.25,10.25]\n*/\n\nt = table(1 2 3 4 as id, x as x, y as y)\nnew_t = select *, rowWavg(x, [1, 1, 2]) as new_x, rowWavg(y, [1, 1, 2]) as new_y from t\n/* new_t\nid x         y         new_x new_y\n-- --------- --------- ----- -----\n1  [1,2,3]   [1,2,3]   2.25  2.25 \n2  [4,5,6]   [4,5,6]   5.25  5.25 \n3  [6,7,8]   [6,7,8]   7.25  7.25 \n4  [9,10,11] [9,10,11] 10.25 10.25\n*/\n```\n\n----------------------------------------\n\nTITLE: 定义值域分区并创建分区表（DolphinDB, VALUE）\nDESCRIPTION: 此代码描述了如何在DolphinDB中按时间（月份）值范围进行值分区，生成多个区间分区，并将表t保存到分区表中，支持后续追加分区。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/database.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\nn=1000000\nmonth=take(2000.01M..2016.12M, n)\n x=rand(1.0, n)\nt=table(month, x)\n\ndb=database(\"dfs://valuedb\", VALUE, 2000.01M..2016.12M)\n\npt = db.createPartitionedTable(t, `pt, `month)\npt.append!(t)\n\npt=loadTable(db,`pt)\nselect count(x) from pt;\n```\n\n----------------------------------------\n\nTITLE: Setup Parallel Buy Order Processing Engines (DolphinDB)\nDESCRIPTION: Defines a function `processBuyOrderFunc` to create and subscribe parallel `reactiveStateEngine` instances for processing buy trade data. It defines the metrics to calculate, including cumulative trade amount and tagged cumulative quantity, using `tagFunc`. It then iterates `parallel` times, creating an engine instance each time, subscribing it to the raw `tradeOriginalStream`, and setting its output to the corresponding `processSellOrder` engine instance.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/streaming_capital_flow_daily/02.createEngineSub.txt#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef processBuyOrderFunc(parallel){\n\tmetricsBuy = [\n\t\t<TradeTime>,\n\t\t<SellNum>,\n\t\t<TradeAmount>,\n\t\t<TradeQty>,\n\t\t<cumsum(TradeAmount)>,\n\t\t<tagFunc(cumsum(TradeQty))>,\n\t\t<prev(cumsum(TradeAmount))>,\n\t\t<prev(tagFunc(cumsum(TradeQty)))>]\n\tfor(i in 1..parallel){\n\t\tcreateReactiveStateEngine(name = \"processBuyOrder\"+string(i), metrics = metricsBuy, dummyTable = tradeOriginalStream, outputTable = getStreamEngine(\"processSellOrder\"+string(i)), keyColumn=`SecurityID`BuyNum, keepOrder =true)\n\t\tsubscribeTable(tableName = \"tradeOriginalStream\", actionName = \"processBuyOrder\"+string(i), offset = -1, handler = getStreamEngine(\"processBuyOrder\"+string(i)), msgAsTable = true, hash = i, filter = (parallel, i-1))\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Partitioned Database and Table in DolphinDB Scripting Language\nDESCRIPTION: Creates a compound-partitioned DolphinDB database and a partitioned table with a schema containing integer, datetime, and multiple float columns. It uses two-level partitioning: daily value partitions and wind turbine ID value partitions with 10 turbines identified. The snippet prepares the environment before performing backup operations. Dependencies include DolphinDB environment and permission to create databases.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/restore-backup.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\nm = \"tag\" + string(decimalFormat(1..523,'000'))\ntableSchema = table(100:0,`wntId`insertDate join m, [INT,DATETIME] join take(FLOAT,523) )\ndb1 = database(\"\",VALUE,2020.01.01..2020.12.30)\ndb2 = database(\"\",VALUE,1..10)\ndb = database(\"dfs://ddb\",COMPO,[db1,db2])\ndb.createPartitionedTable(tableSchema,\"windTurbine\",`insertDate`wntId)\n```\n\n----------------------------------------\n\nTITLE: Creating Cross-Sectional Ranking Engine in DolphinDB\nDESCRIPTION: Sets up a cross-sectional engine that ranks securities based on price change factors calculated over 1, 5, and 10 minute intervals.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/Real-Time_Stock_Price_Increase_Calculation/03.注册流计算引擎和订阅流数据表.txt#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// rank engine\nschemaTB = table(1:0, `SecurityID`DateTime`factor_1min`factor_5min`factor_10min, [SYMBOL, TIMESTAMP, DOUBLE, DOUBLE, DOUBLE])\ncreateCrossSectionalEngine(name=\"crossSectionalEngine\", metrics=<[SecurityID, factor_1min, rank(factor_1min, ascending=false), factor_5min, rank(factor_5min, ascending=false), factor_10min, rank(factor_10min, ascending=false)]>, dummyTable=schemaTB, outputTable=objByName(\"changeCrossSectionalTable\"), keyColumn=`SecurityID, triggeringPattern='perBatch', useSystemTime=false, timeColumn=`DateTime)\n```\n\n----------------------------------------\n\nTITLE: Backing Up Partitions of a DolphinDB Distributed Table Using backup Function\nDESCRIPTION: Uses the DolphinDB backup function to perform backup of selected partitions of a distributed table. The example backs up partitions with insertDate on or before 2020.01.02 23:59:59 using a SQL select statement on loadTable. The backupDir specifies the directory where backup files are stored. The parallel parameter enables parallel backup processing. The function returns the number of partitions backed up. Dependencies include existing database and table setup and access rights to backup directory path.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/restore-backup.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\nbackup(backupDir=\"/hdd/hdd1/backup/\",sqlObj=<select * from loadTable(\"dfs://ddb\",\"windTurbine\") where insertDate<=2020.01.02T23:59:59 >,parallel=true)\n```\n\n----------------------------------------\n\nTITLE: Creating Equi Join Engine in DolphinDB to Merge Minute-Level Aggregated Snapshot and Trade Data Streams\nDESCRIPTION: This code example uses DolphinDB's Equi Join engine to merge two minute-level aggregated streaming data sources: snapshot and trades. It defines respective streaming tables, creates two TimeSeriesEngines to perform 1-minute aggregation of trades and snapshot data, and sends the aggregated outputs as left and right inputs to the Equi Join engine. Key parameters include matching columns on symbol and time, useSystemTime=false for timestamp-based synchronization, and fill values for missing entries. Subscriptions are configured to route the original streaming tables through aggregation engines, culminating in a synchronized join output table. The example simulates real-time data feed with generated tables.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming-real-time-correlation-processing.md#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// create table\nshare streamTable(1:0, `Sym`TradeTime`Side`TradeQty, [SYMBOL, TIME, INT, LONG]) as trades\nshare streamTable(1:0, `UpdateTime`Sym`BuyTradeQty`SellTradeQty, [TIME, SYMBOL, LONG, LONG]) as tradesMin\nshare streamTable(1:0, `Sym`Time`Bid1Price`Bid1Qty, [SYMBOL, TIME, DOUBLE, LONG]) as snapshot\nshare streamTable(1:0, `UpdateTime`Sym`AvgBid1Amt, [TIME, SYMBOL, DOUBLE]) as snapshotMin\nshare streamTable(1:0, `UpdateTime`Sym`AvgBid1Amt`BuyTradeQty`SellTradeQty, [TIME, SYMBOL, DOUBLE, LONG, LONG]) as output\n\n// create engine: \neqJoinEngine = createEquiJoinEngine(name=\"EquiJoin\", leftTable=tradesMin, rightTable=snapshotMin, outputTable=output, metrics=<[AvgBid1Amt, BuyTradeQty, SellTradeQty]>, matchingColumn=`Sym, timeColumn=`UpdateTime)\n// create engine: \ntsEngine1 = createTimeSeriesEngine(name=\"tradesAggr\", windowSize=60000, step=60000, metrics=<[sum(iif(Side==1, 0, TradeQty)), sum(iif(Side==2, 0, TradeQty))]>, dummyTable=trades, outputTable=getLeftStream(eqJoinEngine), timeColumn=`TradeTime, keyColumn=`Sym, useSystemTime=false, fill=(0, 0))\n// create engine: \ntsEngine2 = createTimeSeriesEngine(name=\"snapshotAggr\", windowSize=60000, step=60000, metrics=<[avg(iif(Bid1Price!=NULL, Bid1Price*Bid1Qty, 0))]>, dummyTable=snapshot, outputTable=getRightStream(eqJoinEngine), timeColumn=`Time, keyColumn=`Sym, useSystemTime=false, fill=(0.0))\n\n// subscribe topic\nsubscribeTable(tableName=\"trades\", actionName=\"minAggr\", handler=tsEngine1, msgAsTable=true, offset=-1, hash=1)\nsubscribeTable(tableName=\"snapshot\", actionName=\"minAggr\", handler=tsEngine2, msgAsTable=true, offset=-1, hash=2) \n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// generate data: snapshot\nt1 = table(`A`B`A`B`A`B as Sym, 10:00:52.000+(3 3 6 6 9 9)*1000 as Time, (3.5 7.6 3.6 7.6 3.6 7.6) as Bid1Price, (1000 2000 500 1500 400 1800) as Bid1Qty)\n// generate data: trade\nt2 = table(`A`A`B`A`B`B`A`B`B`A as Sym, 10:00:54.000+(1..10)*700 as TradeTime,  (1 2 1 1 1 1 2 1 2 2) as Side, (1..10) * 10 as TradeQty)\n// input\ntrades.append!(t2)\nsnapshot.append!(t1)\n```\n\n----------------------------------------\n\nTITLE: Loading Saved XGBoost Model (DolphinDB Script)\nDESCRIPTION: Specifies the file path for a pre-trained XGBoost model and loads it into a variable `model` using the `xgboost::loadModel` function provided by the loaded plugin. This model will be used for real-time prediction.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/metacode_derived_features/metacode_derived_features_streaming.txt#_snippet_8\n\nLANGUAGE: DolphinDB\nCODE:\n```\nmodelSavePath = \"/hdd/hdd9/machineLearning/model/001.model\"\nmodel = xgboost::loadModel(modelSavePath)\n```\n\n----------------------------------------\n\nTITLE: Loading Historical Trade Data from CSV (DolphinDB Script)\nDESCRIPTION: This script loads historical trade data from a specified CSV file into the previously created partitioned table `dfs://trade/trade`. It defines the path to the CSV file (`csvDataPath`), retrieves the target table object, infers the schema from the existing table, and uses the `loadTextEx` function for efficient bulk loading, respecting the table's partitioning scheme.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_capital_flow_order_by_order.md#_snippet_2\n\nLANGUAGE: dolphindb\nCODE:\n```\n//load data\ncsvDataPath = \"/hdd/hdd9/data/streaming_capital_flow/20200102_SH_trade.csv\"\ndbName = \"dfs://trade\"\ntbName = \"trade\"\ntrade = loadTable(\"dfs://trade\", \"trade\")\nschemaTable = table(trade.schema().colDefs.name as `name, trade.schema().colDefs.typeString as `type)\nloadTextEx(dbHandle=database(dbName), tableName=tbName, partitionColumns=`TradeTime`SecurityID, filename=csvDataPath, schema=schemaTable)\n```\n\n----------------------------------------\n\nTITLE: Cleaning Environment Using DolphinDB in DolphinDB Script\nDESCRIPTION: Defines the 'cleanEnvironment' function to clear resources in DolphinDB environment, including unsubscribing from the 'messageStream' table for specified actions and dropping both stream tables and engines. It uses try-catch blocks around each operation to handle potential exceptions if objects do not exist. After cleanup, it undefines all variables to reset the environment. Requires DolphinDB server version 2.00.6 or higher.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/stock_market_replay/06.cleanEnvironment.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\n/**\ncleanEnvironment.txt\nScript to clean up environment\nDolphinDB Inc.\nDolphinDB server version: 2.00.6 2022.05.31\nLast modification time: 2022.07.07\n*/\n\n// clean up environment\ndef cleanEnvironment(){\n\ttry { unsubscribeTable(tableName=\"messageStream\", actionName=\"tradeJoinSnapshot\") } catch(ex) { print(ex) }\n\ttry { unsubscribeTable(tableName=\"messageStream\", actionName=\"sendMsgToKafka\") } catch(ex) { print(ex) }\n\ttry { dropStreamTable(`messageStream) } catch(ex) { print(ex) }\n\ttry { dropStreamTable(`prevailingQuotes) } catch(ex) { print(ex) }\n\ttry { dropStreamEngine(`tradeJoinSnapshot) } catch(ex) { print(ex) }\n\ttry { dropStreamEngine(`streamFilter) } catch(ex) { print(ex) }\n\tundef all\n}\ncleanEnvironment()\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading XGBoost Model\nDESCRIPTION: This code saves a trained XGBoost model to disk using `xgboost::saveModel` and loads it back using `xgboost::loadModel`. This allows trained models to be persisted and reused without retraining.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/machine_learning.md#_snippet_22\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nxgboost::saveModel(model, \"xgboost001.mdl\")\n\nmodel = xgboost::loadModel(\"xgboost001.mdl\")\n```\n\n----------------------------------------\n\nTITLE: Creating Composite Partitioned Stock Level 2 Entrust Data Table in DolphinDB - DolphinDB\nDESCRIPTION: Defines a DolphinDB module and function to create a composite partitioned table for storing stock level 2 entrust data. Uses a two-level composite partition: the first by date range (VALUE partition over one year), the second by hash partition on SecurityID into 25 sub-partitions. Initializes the schema with specific columns matching the data fields and invokes database methods to create and store the partitioned table. Requires DolphinDB environment with database management and TSDB engine support.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/dolphinscheduler_integration.md#_snippet_10\n\nLANGUAGE: DolphinDB\nCODE:\n```\nmodule createStockTable\n\n// 创建逐笔委托数据存储库表\ndef createEntrust(dbName, tbName,userName = \"admin\",password = \"123456\")\n{\n\tlogin(userName, password)\n\tif(!existsDatabase(dbName))\n\t{\n\t\tdb1 = database(, VALUE, 2020.01.01..2021.01.01)\n\t\tdb2 = database(, HASH, [SYMBOL, 25])\n        // 按天和股票组合分区\n\t\tdb = database(dbName, COMPO, [db1, db2], , \"TSDB\")\n\t}\n\telse\n\t{\n\t\tdb = database(dbName)\n\t}\n\tname=`ChannelNo`ApplSeqNum`MDStreamID`SecurityID`SecurityIDSource`Price`OrderQty`Side`TradeTIme`OrderType`OrderIndex`LocalTime`SeqNo`Market`DataStatus`BizIndex\n\ttype = [INT, LONG, INT, SYMBOL, INT, DOUBLE, INT, SYMBOL, TIMESTAMP, SYMBOL, INT, TIME, LONG, SYMBOL,INT,LONG]\n\tschemaTable = table(1:0, name, type)\n    // 创建分区表\n\tdb.createPartitionedTable(table=schemaTable, tableName=tbName, partitionColumns=`TradeTime`SecurityID, compressMethods={TradeTime:\"delta\"}, sortColumns=`Market`SecurityID`TradeTime, keepDuplicates=ALL)\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Metrics for Buy Order Processing in DolphinDB\nDESCRIPTION: This code defines the `metricsBuy` variable, which contains the expressions used within the reactive state engine for processing buy orders. It includes calculations such as cumulative sum of trade amount and a tag function based on cumulative trade quantity, along with the previous values of these calculations.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_capital_flow_daily.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\nmetricsBuy = [\n  \t<TradeTime>,\n  \t<SellNum>,\n  \t<TradeAmount>,\n  \t<TradeQty>,\n  \t<cumsum(TradeAmount)>,\n  \t<tagFunc(cumsum(TradeQty))>,\n  \t<prev(cumsum(TradeAmount))>,\n  \t<prev(tagFunc(cumsum(TradeQty)))>]\n```\n\n----------------------------------------\n\nTITLE: 查询设备5分钟内的噪声统计信息\nDESCRIPTION: 该脚本对指定设备及时间段内的噪声数据进行聚合，包括最值、均值等统计指标，适合监控短时段的噪声变化，SQL执行时间约22毫秒，首次大约13毫秒。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/iot_query_case.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nloadTable(database(\"dfs://NoiseDB\"),'noise')\nselect\n     min(ts) as startTs\n    ,max(ts) as endTs\n    ,max(soundPressureLevel)\n    ,avg(soundPressureLevel)\n    ,max(soundPowerLevel) \n    ,avg(soundPowerLevel) \nfrom noise\nwhere date=2022.01.01 and tenantId=1055 and deviceId=10067 and ts between 2022.01.01T00:50:15.518:2022.01.01T00:55:15.518\ngroup by tenantId, deviceId\n```\n\n----------------------------------------\n\nTITLE: Loading TSDB Partition and Checking Cache (DolphinDB Script)\nDESCRIPTION: Loads data from a specific partition ('2021.08.07') of a TSDB table. It then checks both the TSDB index cache usage (`getLevelFileIndexCacheStatus`) and the overall node memory usage (`mem`). This illustrates that querying TSDB data populates the index cache, while the data itself might not remain cached extensively post-query.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/memory_management.md#_snippet_14\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nselect * from loadTable(\"dfs://TSDB_db1\",`pt1) where date=2021.08.07\ngetLevelFileIndexCacheStatus().usage\n//输出结果为：39128\nmem().allocatedBytes - mem().freeBytes\n//输出结果为：28537352\n```\n\n----------------------------------------\n\nTITLE: Persisting Prediction & Warning Data - DolphinDB\nDESCRIPTION: This script persists prediction and warning data from stream tables to distributed tables. It defines functions `createPredictDataBase` and `createWarningDataBase` to create the distributed databases and tables with specified partitioning schemes and data modeling. It then subscribes to the `predictTable` and `warningTable` stream tables to load data into the distributed tables.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/knn_iot.md#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\n/*\n  * 将预测结果写入分布式表\n\t数据建模：\n\t\t1）每小时记录数：36,000,000\n\t\t2）每条记录大小：46字节\n\t\t3）每小时空间占用（压缩前）：1.54G\n\t\t4）建议以“id”和“天”进行值分区，每分区≈ 379.03M\n\t\t5）分区索引为“时间戳”+“设备号”\n\n*/\ndef createPredictDataBase(dbname,tbname,col_names,col_types){\n\tif(existsDatabase(dbname)){dropDatabase(dbname)}\n\tIds = 1..100\n  dbId = database(\"\",VALUE,Ids)\n  dbTime = database(\"\",VALUE,date(2023.01.01T00:00:00)..date(2023.12.31T20:00:00))\n\tdb = database(directory=dbname, partitionType=COMPO, partitionScheme=[dbTime,dbId],engine=\"TSDB\")\n\tfactor_partition = db.createPartitionedTable(table=table(1:0,col_names,col_types),tableName = tbname,partitionColumns =[\"time\",\"deviceCode\"],sortColumns =[\"deviceCode\",\"time\"],compressMethods={time:\"delta\"},keepDuplicates=LAST)\n}\npredict_dbname,predict_tbname = \"dfs://predict\",\"data\"\ncreatePredictDataBase(predict_dbname,predict_tbname,preColNames,preColTypes)\nsubscribeTable(tableName=\"predictTable\", actionName=\"append_Predict_into_dfs\", offset=0, handler=loadTable(predict_dbname,predict_tbname), msgAsTable=true,batchSize=100000, throttle=1, reconnect=true)\n\n/*\n  * 将预警结果写入分布式表\n\t数据建模：\n\t\t1）每小时记录数：360\n\t\t2）每条记录大小：20字节\n\t\t3）每小时空间占用（压缩前）：7200 字节\n\t\t4）建议以“年”进行值分区，每分区≈ 60.15M\n\t\t5）分区索引为“时间戳”\n*/\ndef createWarningDataBase(dbname,tbname,col_names,col_types){\n\tif(existsDatabase(dbname)){dropDatabase(dbname)}\n  db = database(dbname,RANGE,sort(distinct(yearBegin((2023.01.01T00:00:00..2024.12.31T20:00:00)))),engine='TSDB')\n  factor_partition = db.createPartitionedTable(table=table(1:0,col_names,col_types),tableName = tbname,partitionColumns=`time,sortColumns=`time)\n}\nwarning_dbname,warning_tbname = \"dfs://warning\",\"data\"\ncreateWarningDataBase(warning_dbname,warning_tbname,warnColNames,warnColTypes)\nsubscribeTable(tableName=\"warningTable\", actionName=\"append_Warning_into_dfs\", offset=0, handler=loadTable(warning_dbname,warning_tbname), msgAsTable=true,batchSize=100000, throttle=1, reconnect=true)\n```\n\n----------------------------------------\n\nTITLE: Importing and Transforming Binary Level 2 Data into DolphinDB Distributed Table Using DolphinDB Script\nDESCRIPTION: This function loads discrete binary Level 2 market data files from a specified directory, applying a prescribed schema to interpret binary columns, then performs column-type transformations such as parsing date and time integers into datetime formats and formatting integer codes into string symbols and market identifiers. It finally appends the transformed data into an existing distributed table. Dependencies include a pre-existing distributed table \"quotes\" and DolphinDB functions like loadRecord, replaceColumn!, datetimeParse, format, and append!. The input is a directory path containing binary files and the schema definition; the output is the distributed table updated with imported data. Note that date, time, symbol, and market columns are converted from numeric to typed values to enable meaningful querying.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/quant_finance_examples.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\nschema = [\n(\"symbol\", INT), (\"market\", INT), (\"date\", INT), (\"time\", INT), (\"preClose\", DOUBLE),\n(\"open\", DOUBLE), (\"high\", DOUBLE), (\"low\", DOUBLE), (\"last\", DOUBLE), (\"numTrades\", INT),\n(\"curNumTrades\", INT), (\"volume\", INT), (\"curVol\", INT), (\"turnover\", DOUBLE), (\"curTurnover\", INT),\n(\"peratio1\", INT), (\"peratio2\", INT), (\"totalAskVolume\", INT), (\"wavgAskPrice\", DOUBLE), (\"askLevel\", INT),\n(\"totalBidVolume\", INT), (\"wavgBidPrice\", DOUBLE), (\"bidLevel\", INT), (\"iopv\", DOUBLE), (\"ytm\", INT),\n(\"askPrice1\", DOUBLE), (\"askPrice2\", DOUBLE), (\"askPrice3\", DOUBLE), (\"askPrice4\", DOUBLE), (\"askPrice5\", DOUBLE),\n(\"askPrice6\", DOUBLE), (\"askPrice7\", DOUBLE), (\"askPrice8\", DOUBLE), (\"askPrice9\", DOUBLE), (\"askPrice10\", DOUBLE),\n(\"bidPrice1\", DOUBLE), (\"bidPrice2\", DOUBLE), (\"bidPrice3\", DOUBLE), (\"bidPrice4\", DOUBLE), (\"bidPrice5\", DOUBLE),\n(\"bidPrice6\", DOUBLE), (\"bidPrice7\", DOUBLE), (\"bidPrice8\", DOUBLE), (\"bidPrice9\", DOUBLE), (\"bidPrice10\", DOUBLE),\n(\"askVolume1\", INT), (\"askVolume2\", INT), (\"askVolume3\", INT), (\"askVolume4\", INT), (\"askVolume5\", INT),\n(\"askVolume6\", INT), (\"askVolume7\", INT), (\"askVolume8\", INT), (\"askVolume9\", INT), (\"askVolme10\", INT),\n(\"bidVolume1\", INT), (\"bidVolume2\", INT), (\"bidVolume3\", INT), (\"bidVolume4\", INT), (\"bidVolume5\", INT),\n(\"bidVolume6\", INT), (\"bidVolume7\", INT), (\"bidVolume8\", INT), (\"bidVolume9\", INT),(\"bidVolume10\", INT),\n(\"unixTime\", LONG), (\"upperLimit\", DOUBLE), (\"lowerLimit\", DOUBLE) ]\n\ndataDir=\"/hdd/hdd1/data/Level2BinFiles/\"\n\ndef importBinFiles(dataDir, schema){\n    tick1 = loadTable(\"dfs://level2\",\"quotes\")\n    dataFiles = exec filename from files(dataDir)\n    for(f in dataFiles){\n        t=loadRecord(dataDir + f, schema)\n        t.replaceColumn!(`date, t.date.string().datetimeParse(\"yyyyMMdd\"))\n        t.replaceColumn!(`time, t.time.format(\"000000000\").datetimeParse(\"HHmmssSSS\"))\n        t.replaceColumn!(`symbol, t.symbol.format(\"000000\"))\n        t.replaceColumn!(`market, iif(t.market==0,\"SH\",\"SZ\"))\n        tick1.append!(t)\n    }\n}\nimportBinFiles(dataDir, schema);\n```\n\n----------------------------------------\n\nTITLE: Rolling Time Window Calculation with SQL\nDESCRIPTION: Demonstrates using bar function with group by statement to implement rolling window aggregation, calculating the sum of trading volume every 2 minutes from 10:00:00 to 10:05:59.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/window_cal.md#_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nt=table(2021.11.01T10:00:00..2021.11.01T10:05:59 as time, 1..360 as volume)\nselect sum(volume) from t group by bar(time, 2m)\n```\n\n----------------------------------------\n\nTITLE: Matrix Alignment via align Function in DolphinDB\nDESCRIPTION: Illustrates use of DolphinDB's align function to explicitly align two matrices with potentially non-identical row and column labels. The code demonstrates aligning two matrices with both string and time labels, returning two matrices with matched labels ready for further operations. Dependencies include matrix, rename!, align. Inputs: two matrices with non-overlapping/duplicate row and column names. Outputs: tuples of aligned matrices with consistent dimension labels. Limitations: complex labels require careful matching for correct alignment.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/panel_data.md#_snippet_21\n\nLANGUAGE: DolphinDB\nCODE:\n```\nx1 = [09:00:00, 09:00:01, 09:00:03]\nx2 = [09:00:00, 09:00:03, 09:00:03, 09:00:04]\ny1 = `a`a`b\ny2 = `a`b`b\nm1 = matrix(1 2 3, 2 3 4, 3 4 5).rename!(y1,x1)\nm2 = matrix(11 12 13, 12 13 14, 13 14 15, 14 15 16).rename!(y2,x2)\na, b = align(m1, m2, 'ej,aj', false);\na;\nb;\n```\n\n----------------------------------------\n\nTITLE: 使用高阶函数计算股票收益率相关性矩阵\nDESCRIPTION: 利用高阶函数each和pcross简化代码，计算交易量最大的100只股票分钟级收益率之间的两两相关性。高阶函数可将复杂的数据分析任务简化为几行代码。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/quant_finance_examples.md#_snippet_14\n\nLANGUAGE: DolphinDB\nCODE:\n```\nminuteBar = select first(last) as open, max(last) as high, min(last) as low, last(last) as last, sum(curVol) as volume from quotes where date=2020.06.01, symbol>=`600000 group by symbol, date, minute(time) as minute;\nsyms = (exec sum(volume) from minuteBar group by symbol order by sum_volume desc).symbol[0:100]\npriceMatrix = exec last from minuteBar where symbol in syms pivot by minute, symbol\nretMatrix = each(def(x):ratios(x)-1, priceMatrix)\ncorrMatrix = pcross(corr, retMatrix);\n```\n\n----------------------------------------\n\nTITLE: PartitionedTableAppender Usage in C++\nDESCRIPTION: This code snippet demonstrates how to use PartitionedTableAppender (PTA) to write data to a distributed table in DolphinDB. It initializes a connection pool and a PTA object, specifies the partition column, appends data, and shuts down the connection pool. The partition column is crucial for parallel writing to multiple partitions. It depends on the DolphinDB C++ API.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_cpp_api_connector.md#_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\nDBConnectionPool pool(\"127.0.0.1\", 9900, 5, \"admin\", \"123456\");\n// 分区列传入 deviceid 或 ts 均可，保证可以使用多线程写入数据集多个分区，因为 DolphinDB 开启事务时不允许多个 writer 同时写入到一个分区内\nPartitionedTableAppender appender(\"dfs://test_PartitionedTableAppender\", \"collect\",\"deviceid\", pool); \nappender.append(bt);\npool.shutDown(); \n```\n\n----------------------------------------\n\nTITLE: 创建多窗口时序引擎示例\nDESCRIPTION: 演示如何创建一个时序引擎，同时使用两个不同大小的窗口(6毫秒和12毫秒)对相同的数据计算总和，展示DolphinDB对多窗口聚合的支持。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/stream_aggregator.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\nshare streamTable(1000:0, `time`volume, [TIMESTAMP, INT]) as trades\noutputTable = table(10000:0, `time`sumVolume1`sumVolume2, [TIMESTAMP, INT,INT])\ntradesAggregator = createTimeSeriesEngine(name=\"streamAggr1\", windowSize=[6,12], step=3, metrics=[<sum(volume)>,<sum(volume)>], dummyTable=trades, outputTable=outputTable, timeColumn=`time)\nsubscribeTable(tableName=\"trades\", actionName=\"append_tradesAggregator\", offset=0, handler=append!{tradesAggregator}, msgAsTable=true)\n```\n\n----------------------------------------\n\nTITLE: Creating Reactive State Engine with Filter\nDESCRIPTION: This code demonstrates how to filter output from the Reactive State Engine using the filter parameter in `createReactiveStateEngine`. Only records where the price has changed will be output. This shows how to control the output of the engine.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/reactive_state_engine.md#_snippet_7\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nshare streamTable(1:0, `sym`price, [STRING,DOUBLE]) as tickStream\nresult = table(1000:0, `sym`price, [STRING,DOUBLE])\nrse = createReactiveStateEngine(name=\"reactiveFilter\", metrics =[<price>], dummyTable=tickStream, outputTable=result, keyColumn=\"sym\", filter=<prev(price) != price>)\nsubscribeTable(tableName=`tickStream, actionName=\"filter\", handler=tableInsert{rse})\n```\n\n----------------------------------------\n\nTITLE: Initializing ODBC Connection and Executing Data Sync Jobs in DolphinDB\nDESCRIPTION: This block handles the setup and execution of the data synchronization process. It first attempts to load the ODBC plugin. Then, it establishes a connection to an Oracle database using the DSN 'orac'. It defines the target DolphinDB database path (`dfs://TSDB_tick`) and table name (`tick`). It demonstrates two execution methods: syncing the entire Oracle table by calling `syncData` with a NULL date, and submitting multiple background jobs using `submitJob` to sync data for specific dates (2021.01.04 to 2021.01.05) concurrently. Finally, it queries the status of the submitted jobs.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/Oracle_to_DolphinDB/迁移.txt#_snippet_2\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\ntry { loadPlugin(\"plugins/odbc/PluginODBC.txt\"); } catch(ex) { print ex }\nconn=odbc::connect(\"Dsn=orac\", `Oracle);\ndbName=\"dfs://TSDB_tick\"\ntbName=\"tick\"\n// case1 全表\nsyncData(conn, dbName, tbName, NULL)\n// case2 多任务\nfor(dt in 2021.01.04..2021.01.05){\n\tsubmitJob(`syncOracTick, `syncOracTick, syncData, conn, dbName, tbName, dt)\n}\nselect * from getRecentJobs() where jobDesc = `syncOracTick\n```\n\n----------------------------------------\n\nTITLE: Window Join Between Two Tables Using WJ Function in DolphinDB SQL\nDESCRIPTION: Demonstrates window join (wj) between two tables based on timestamp intervals with specified window boundaries and aggregation functions. The example joins t1 and t2 tables on symbol and time columns using a window from -5 seconds to 0 seconds relative to each record in the left table, computing average bid prices from the right table and joining results to t1. Inputs are tables containing symbol, time, bid, offer, price, and volume data. Output is enriched left table including aggregated window join results. Enables flexible, time-aware join and aggregation for multi-table time series.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/window_cal.md#_snippet_13\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//data\nt1 = table(1 1 2 as sym, 09:56:06 09:56:07 09:56:06 as time, 10.6 10.7 20.6 as price)\nt2 = table(take(1,10) join take(2,10) as sym, take(09:56:00+1..10,20) as time, (10+(1..10)\\10-0.05) join (20+(1..10)\\10-0.05) as bid, (10+(1..10)\\10+0.05) join (20+(1..10)\\10+0.05) as offer, take(100 300 800 200 600, 20) as volume);\n\n//window join\nwj(t1, t2, -5s:0s, <avg(bid)>, `sym`time);\n\n# output\n\nsym time     price  avg_bid           \n--- -------- ----- -------\n1   09:56:06 10.6 10.3\n1   09:56:07 10.7 10.4\n2   09:56:06 20.6 20.3        \n```\n\n----------------------------------------\n\nTITLE: Defining Function to Create Partitioned DolphinDB Table\nDESCRIPTION: Defines a DolphinDB function `createDB` that takes a database path (`dbName`) and table name (`tableName`) as input. It first drops the database if it exists, then creates a composite partitioned database (partitioned by date using VALUE and by symbol using HASH). Finally, it defines the schema for an order book table and creates the partitioned table within the specified database using `DateTime` and `Symbol` as partitioning columns.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/csvImportDemo/createDB.txt#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef createDB(dbName,tableName){\t\n\tif(existsDatabase(dbName))\n\t\tdropDatabase(dbName)\n\tdbDate = database(\"\", VALUE, 2020.01.01..2020.01.03)\n\tdbSymbol=database(\"\", HASH, [SYMBOL, 40])\n\tdb = database(dbName, COMPO, [dbDate, dbSymbol])\t\n\t\n\tcolumns = `Symbol`Market`DateTime`Status`PreClose`Open`High`Low`Price`Volume`Amount`AskPrice1`AskPrice2`AskPrice3`AskPrice4`AskPrice5`AskPrice6`AskPrice7`AskPrice8`AskPrice9`AskPrice10`BidPrice1`BidPrice2`BidPrice3`BidPrice4`BidPrice5`BidPrice6`BidPrice7`BidPrice8`BidPrice9`BidPrice10`AskVolume1`AskVolume2`AskVolume3`AskVolume4`AskVolume5`AskVolume6`AskVolume7`AskVolume8`AskVolume9`AskVolume10`BidVolume1`BidVolume2`BidVolume3`BidVolume4`BidVolume5`BidVolume6`BidVolume7`BidVolume8`BidVolume9`BidVolume10`TickCount`BidOrderTotalVolume`AskOrderTotalVolume`AvgBidOrderPrice`AvgAskOrderPrice`LimitHighestPrice`LimitLowestPrice\n\ttype=[SYMBOL,SYMBOL,DATETIME,INT,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,INT,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,INT,INT,INT,INT,INT,INT,INT,INT,INT,INT,INT,INT,INT,INT,INT,INT,INT,INT,INT,INT,INT,INT,INT,DOUBLE,DOUBLE,DOUBLE,DOUBLE]\n\torderData = table(1:0, columns,type)\n\tdb.createPartitionedTable(orderData, tableName,`DateTime`Symbol)\n}\n```\n\n----------------------------------------\n\nTITLE: Defining a DolphinDB Function to Create Partitioned Tables - DolphinDB Script\nDESCRIPTION: This DolphinDB script defines a function 'createTable' that logs into DolphinDB, checks for the existence of a database, and creates a composite partitioned database if missing. It then verifies if a table exists, and if not, defines and creates a partitioned table with specified schema, partitioning, compression, and sorting. If the table exists, it writes a log message. This function modularizes data schema creation to be called through DolphinScheduler tasks.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/dolphinscheduler_integration.md#_snippet_7\n\nLANGUAGE: DolphinDB script\nCODE:\n```\n// 在DolphinDB中定义一个函数，用于创建数据库表\ndef createTable(dbName, tbName){\n\tlogin(\"admin\", \"123456\")\n\n\tif(!existsDatabase(dbName)){\n\t\tdb1 = database(, VALUE, 2020.01.01..2021.01.01)\n\t\tdb2 = database(, HASH, [SYMBOL, 10])\n\t\tdb = database(dbName, COMPO, [db1, db2], , \"TSDB\")\n\t}else{\n\t\tdb = database(dbName)\n\t}\n    if(!existsTable(dbName,tbName)){\n        name =`SecurityID`ChannelNo`ApplSeqNum`MDStreamID`SecurityIDSource`Price\n              `OrderQty`Side`TradeTIme`OrderType`OrderIndex`LocalTime`SeqNo\n              `Market`DataStatus`BizIndex\n        type = [SYMBOL, INT, LONG, INT, INT, DOUBLE, INT, \n                SYMBOL, TIMESTAMP, SYMBOL, INT, TIME, LONG, SYMBOL,INT,LONG]\n        schemaTable = table(1:0, name, type)\n        db.createPartitionedTable(table=schemaTable, tableName=tbName, \n                partitionColumns=`TradeTime`SecurityID, \n                compressMethods={TradeTime:\"delta\"}, \n                sortColumns=`Market`SecurityID`TradeTime, keepDuplicates=ALL)\n    }\n    else{\n        writeRunLog(\"数据库：\" + dbName + \" 数据表：\" + tbName + \" 已存在...\")\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Persisting DolphinDB Stream Data to a DFS Table\nDESCRIPTION: This snippet demonstrates how to persist data from a stream table (`orderTable`) into a durable, partitioned DolphinDB DFS table (`dfs://taxi/newData`). It first defines the database path, checks if the target table exists (dropping it if it does), creates a partitioned table with the same schema as `orderTable`, and finally subscribes to `orderTable`, using the built-in `loadTable` function as the handler to write incoming messages to the `newData` table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Forecast_of_Taxi_Trip_Duration.md#_snippet_6\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\ndb = database(\"dfs://taxi\")\nif(existsTable(\"dfs://taxi\", \"newData\")) { dropTable(db, \"newData\") }\ndb.createPartitionedTable(table=table(1:0, orderTable.schema().colDefs.name, orderTable.schema().colDefs.typeString), tableName=`newData, partitionColumns=`pickup_datetime, sortColumns=`pickup_datetime, compressMethods={datetime:\"delta\"})\nsubscribeTable(tableName=\"orderTable\", actionName=\"saveToDisk\", offset=0, handler=loadTable(\"dfs://taxi\", \"newData\"), msgAsTable=true, batchSize=100000, throttle=1, reconnect=true)\n```\n\n----------------------------------------\n\nTITLE: Execution Plan Showing Successful Partition Pruning in DolphinDB\nDESCRIPTION: This JSON snippet displays part of a DolphinDB execution plan demonstrating successful partition pruning. The 'map.partitions.remote' field shows that only 2 remote partitions were accessed, matching the expected range specified by the `between` operator in the WHERE clause, significantly reducing query cost.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_Explain.md#_snippet_10\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    \"measurement\": \"microsecond\",\n    \"explain\": {\n        \"from\": {\n            \"cost\": 3\n        },\n        \"map\": {\n            \"partitions\": {\n                \"local\": 0,\n                \"remote\": 2\n            },\n            \"cost\": 912,\n            \"detail\": {\n                \"most\": {\n    ......[以下省略]\n```\n\n----------------------------------------\n\nTITLE: Importing Data from Text Files in DolphinDB\nDESCRIPTION: Shows three approaches for importing CSV data to DolphinDB: saving sample data to text files, using loadText to load data into memory before appending, and using loadTextEx to directly load data into a distributed table, which is the recommended method for large files.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/database.md#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\nworkDir = \"C:/DolphinDB/Data\"\nif(!exists(workDir)) mkdir(workDir)\nquotes.saveText(workDir + \"/quotes.csv\")\nquotes.saveText(workDir + \"/quotes_new.csv\")\n\nt=loadText(workDir + \"/quotes_new.csv\")\nloadTable(\"dfs://stockDB\", \"quotes\").append!(t)\n\ndb = database(\"dfs://stockDB\")\nloadTextEx(db, \"quotes\", `date`sym, workDir + \"/quotes.csv\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Reactive State Engine for Price Change Calculation in DolphinDB\nDESCRIPTION: Creates a reactive state engine that calculates price changes over 1, 5, and 10 minute periods for securities, filtering for trading hours after 09:30:00.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/Real-Time_Stock_Price_Increase_Calculation/03.注册流计算引擎和订阅流数据表.txt#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// calculate engine\ncreateReactiveStateEngine(name=\"calChange\", metrics=<[DateTime, calculateChange(DateTime, LastPx, lag=1m), calculateChange(DateTime, LastPx, lag=5m), calculateChange(DateTime, LastPx, lag=10m)]>, dummyTable=objByName(\"snapshotStreamTable\"), outputTable=getStreamEngine(\"crossSectionalEngine\"), keyColumn=`SecurityID, filter=<time(DateTime) >= 09:30:00.000>)\n```\n\n----------------------------------------\n\nTITLE: Defining createInOutTable Function\nDESCRIPTION: This function creates input and output stream tables. `inputSt` stores incoming data with `tag`, `ts`, and `value` columns. `outputSt1` stores the output of the reactive state engine, and `outputSt2` stores the output of the session window engine. The function also enables table sharing and persistence for each created stream table with specified parameters.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/data/DolphinDB_streaming_application_in_IOT/deviceState.txt#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef createInOutTable(){\n\tstream01=streamTable(100000:0,`tag`ts`value,[SYMBOL,TIMESTAMP, INT])\n\tenableTableShareAndPersistence(table=stream01,tableName=`inputSt,asynWrite=false,compress=true, cacheSize=100000)\n\t\n\tout1 =streamTable(10000:0,`tag`ts`value,[SYMBOL,TIMESTAMP, INT])\n\tenableTableShareAndPersistence(table=out1,tableName=`outputSt1,asynWrite=false,compress=true, cacheSize=100000)\n\t\n\tout2 =streamTable(10000:0,`ts`tag`lastValue,[TIMESTAMP,SYMBOL, INT])\n\tenableTableShareAndPersistence(table=out2,tableName=`outputSt2,asynWrite=false,compress=true, cacheSize=100000)\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Custom High/Low Aggregate Functions in DolphinDB Script\nDESCRIPTION: Defines two custom user-defined aggregate functions, `high` and `low`, using the `defg` keyword in DolphinDB script. The `high` function returns the maximum of `HighPrice` if `DeltasHighPrice` indicates a change, otherwise the maximum of `LastPrice`. The `low` function returns the minimum of non-zero `LowPrice` if `DeltasLowPrice` indicates a significant negative change, otherwise the minimum of `LastPrice`. These are likely used in streaming aggregations.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/K.md#_snippet_10\n\nLANGUAGE: dolphindb\nCODE:\n```\ndefg high(DeltasHighPrice, HighPrice, LastPrice){\n\tif(sum(DeltasHighPrice)>0.000001){\n\t\treturn max(HighPrice)\n\t}\n\telse{\n\t\treturn max(LastPrice)\n\t}\n}\n\ndefg low(DeltasLowPrice, LowPrice, LastPrice){\n\tsumDeltas = sum(DeltasLowPrice)\n\tif(sumDeltas<-0.000001 and sumDeltas!=NULL){\n\t\treturn min(iif(LowPrice==0.0, NULL, LowPrice))\n\t}\n\telse{\n\t\treturn min(LastPrice)\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: Releasing Variable Memory with undef\nDESCRIPTION: This code demonstrates how to release the memory occupied by a variable using the `undef` function or by assigning the value to `NULL`. It clarifies the process of freeing up memory previously used by a variable.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/memory_management.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\nundef(`v)\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nv = NULL\n```\n\n----------------------------------------\n\nTITLE: Serializing a DolphinDB Module (.dos to .dom)\nDESCRIPTION: Uses the `saveModule` function to serialize the specified DolphinDB module (e.g., fileLog.dos) into a binary .dom file with the same name. This process helps protect the source code of the module. The generated .dom file is saved in the same directory as the .dos file.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/module_tutorial.md#_snippet_2\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nsaveModule(\"fileLog\")\n```\n\n----------------------------------------\n\nTITLE: Defining Low Price Function for K-line Synthesis\nDESCRIPTION: This code defines a custom function `low` to calculate the low price for 1-minute K-lines based on snapshot data, including handling the edge cases of the snapshot's zero price on a stock that is not actively traded, and also optimizing performance by first calculating the sum of the DeltasLowPrice once, before the `if` condition check. Dependencies: DolphinDB environment.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/K.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndefg low(DeltasLowPrice, LowPrice, LastPrice){\n\tsumDeltas = sum(DeltasLowPrice)\n\tif(sumDeltas<-0.000001 and sumDeltas!=NULL){\n\t\treturn min(iif(LowPrice==0.0, NULL, LowPrice))\n\t}\n\telse{\n\t\treturn min(LastPrice)\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: Subscribing to a Stream Table with Access Control - DolphinDB\nDESCRIPTION: This code demonstrates subscribing to a stream table after adding access control. It requires both TABLE_READ on the published table and TABLE_WRITE and TABLE_READ on the subscribing table.  Uses `subscribeTable`, `addAccessControl` and `grant`. Requires admin privileges to create user.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Permission_Management.md#_snippet_34\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(`admin, `123456)\ncreateUser(`u1, \"111111\");\nlogin(`u1, \"111111\")\nshare streamTable(1000:0, `time`sym`volume, [TIMESTAMP, SYMBOL, INT]) as trades\nshare streamTable(10000:0, `time`sym`sumVolume, [TIMESTAMP, SYMBOL, INT]) as output1\naddAccessControl(`trades)\naddAccessControl(`output1)\nlogin(`admin, `123456)\ngrant(\"u2\", TABLE_READ, \"trades\")\ngrant(\"u2\", TABLE_WRITE, \"output1\")\ngrant(\"u2\", TABLE_READ, \"output1\")\nlogin(`u2, \"222222\")\nsubscribeTable(tableName=\"trades\", actionName=\"agg1\", offset=0, handler=append!{output1}, msgAsTable=true); \n```\n\n----------------------------------------\n\nTITLE: 向模拟撮合引擎输入数据\nDESCRIPTION: 使用appendMsg接口向引擎中插入行情数据或用户订单，演示如何重置引擎并输入测试数据进行撮合计算。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/matching_engine_simulator.md#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\nsymbol = \"000001\"\n//for hq order\nTYPE_ORDER = 0\nHQ_LIMIT_ORDER = 2\n\n//for user order\nLIMIT_ORDER = 5\nORDER_SEL = 2\nORDER_BUY = 1\n\nMatchingEngineSimulator::resetMatchEngine(engine)\nappendMsg(engine, (symbol, \"XSHE\", 2021.01.08 09:14:00.100, TYPE_ORDER,  HQ_LIMIT_ORDER, 7., 100, 1, 1, ORDER_BUY,1), 1)\nappendMsg(engine, (symbol, \"XSHE\", 2021.01.08 09:14:00.100, TYPE_ORDER,  HQ_LIMIT_ORDER, 6., 100, 2, 2, ORDER_BUY,1), 1)\nappendMsg(engine, (symbol, \"XSHE\", 2021.01.08 09:14:00.100, TYPE_ORDER,  HQ_LIMIT_ORDER, 5., 100, 3, 3, ORDER_BUY,1), 1)\nappendMsg(engine, (symbol, 2021.01.08 09:14:00.400, LIMIT_ORDER, 6., 100, ORDER_BUY, 1), 2)\n```\n\n----------------------------------------\n\nTITLE: Filtering and Handling Streams Based on Conditions in DolphinDB\nDESCRIPTION: This code defines a function to set up stream filters based on trade and snapshot conditions. It registers handlers for each condition and subscribes the message stream to the join engine, enabling real-time data processing and event handling.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/stock_market_replay/03.calTradeCost_lookUpJoin.txt#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef appendLeftStream(msg){\n\ttempMsg = select * from msg where Price > 0 and Time>=09:30:00.000\n\tgetLeftStream(getStreamEngine(`tradeJoinSnapshot)).tableInsert(tempMsg)\n}\n\n//register filter stream computing engine and subscribe the stream tables\ndef filterAndParseStreamFunc(tradeSchema, snapshotSchema){\n\tfilter1 = dict(STRING,ANY)\n\tfilter1[\"condition\"] = \"trade\"\n\tfilter1[\"handler\"] = appendLeftStream\n\tfilter2 = dict(STRING,ANY)\n\tfilter2[\"condition\"] = \"snapshot\"\n\tfilter2[\"handler\"] = getRightStream(getStreamEngine(`tradeJoinSnapshot))\n\tschema = dict([\"trade\", \"snapshot\"], [tradeSchema, snapshotSchema])\n\tenine = streamFilter(name=\"streamFilter\", dummyTable=messageStream, filter=[filter1, filter2], msgSchema=schema)\n\t\n\tsubscribeTable(tableName=\"messageStream\", actionName=\"tradeJoinSnapshot\", offset=-1, handler=engine, msgAsTable=true, reconnect=true)\n}\nfilterAndParseStreamFunc(tradeSchema, snapshotSchema)\n```\n\n----------------------------------------\n\nTITLE: Establishing Training and Testing Data Split in DolphinDB\nDESCRIPTION: This code snippet connects to DolphinDB, loads the snapshot dataset for 2020, and defines a function to split the dataset into training and testing subsets based on a specified ratio. It prepares data for subsequent model training and evaluation.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/machine_learning_volatility.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(\"admin\", \"123456\")\ndbName = \"dfs://sz50VolatilityDataSet\"\ntbName = \"sz50VolatilityDataSet\"\ndataset = select * from loadTable(dbName, tbName) where date(TradeTime) between 2020.01.01 : 2020.12.31\ndef trainTestSplit(x, testRatio) {\n    xSize = x.size()\n    testSize = (xSize * (1 - testRatio))$INT\n    return x[0: testSize], x[testSize:xSize]\n}\nTrain, Test = trainTestSplit(dataset, 0.3)\n```\n\n----------------------------------------\n\nTITLE: Calculating Price-Volume Delta using Reactive State Engine in DolphinDB\nDESCRIPTION: This code snippet illustrates how to calculate the delta (change) in `Price*Vol/1000` using DolphinDB's `createReactiveStateEngine`. It leverages the `<deltas(Price*Vol/1000)>` meta-code to compute the change in value. The result is then streamed to the `IOPVResult` stream engine. `tradeProcessDummy` is used as the dummy table, `BasketID` and `SecurityID` are used as the composite key, and `keepOrder=true` ensures data order is preserved.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_IOPV.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nmetricsProcess = [\n    <tradetime>,\n    <deltas(Price*Vol/1000)>]\ncreateReactiveStateEngine(name=\"tradeProcessIOPVChange\", metrics=metricsProcess, dummyTable=tradeProcessDummy, outputTable=getStreamEngine(`IOPVResult), keyColumn=`BasketID`SecurityID, keepOrder=true)\n```\n\n----------------------------------------\n\nTITLE: Creating Distributed Partitioned Snapshot Table in DolphinDB\nDESCRIPTION: This code initializes a partitioned table for Level 2 snapshot data using Array Vector columns in DolphinDB. It starts by checking and dropping any existing database, then sets up date and symbol-based partitions with 'TSDB' storage engine. The schema defines multi-dimensional fields like BidPrice and OfferPrice with types DOUBLE[] and INT[], representing order book levels. The db.createPartitionedTable call establishes partitioning on DateTime and SecurityID, compression, and sort keys. Prerequisites: DolphinDB server, schema adherence for each input column, and proper engine configuration. The expected input is Level 2 tick data, and the output is a ready partitioned TSDB table; it should be run with appropriate admin privileges.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Array_Vector.md#_snippet_29\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndbName = \"dfs://SH_TSDB_snapshot_ArrayVector\"\ntbName = \"snapshot\"\nif(existsDatabase(dbName)){\n\tdropDatabase(dbName)\n}\ndb1 = database(, VALUE, 2020.01.01..2021.01.01)\ndb2 = database(, HASH, [SYMBOL, 20])\ndb = database(dbName, COMPO, [db1, db2], , \"TSDB\")\nschemaTable = table(\n\tarray(SYMBOL, 0) as SecurityID,\n\tarray(TIMESTAMP, 0) as DateTime,\n\tarray(DOUBLE, 0) as PreClosePx,\n\tarray(DOUBLE, 0) as OpenPx,\n\tarray(DOUBLE, 0) as HighPx,\n\tarray(DOUBLE, 0) as LowPx,\n\tarray(DOUBLE, 0) as LastPx,\n\tarray(INT, 0) as TotalVolumeTrade,\n\tarray(DOUBLE, 0) as TotalValueTrade,\n\tarray(SYMBOL, 0) as InstrumentStatus,\n\tarray(DOUBLE[], 0) as BidPrice,\n\tarray(INT[], 0) as BidOrderQty,\n\tarray(INT[], 0) as BidNumOrders,\n\tarray(INT[], 0) as BidOrders,\n\tarray(DOUBLE[], 0) as OfferPrice,\n\tarray(INT[], 0) as OfferOrderQty,\n\tarray(INT[], 0) as OfferNumOrders,\n\tarray(INT[], 0) as OfferOrders,\n\tarray(INT, 0) as NumTrades,\n\tarray(DOUBLE, 0) as IOPV,\n\tarray(INT, 0) as TotalBidQty,\n\tarray(INT, 0) as TotalOfferQty,\n\tarray(DOUBLE, 0) as WeightedAvgBidPx,\n\tarray(DOUBLE, 0) as WeightedAvgOfferPx,\n\tarray(INT, 0) as TotalBidNumber,\n\tarray(INT, 0) as TotalOfferNumber,\n\tarray(INT, 0) as BidTradeMaxDuration,\n\tarray(INT, 0) as OfferTradeMaxDuration,\n\tarray(INT, 0) as NumBidOrders,\n\tarray(INT, 0) as NumOfferOrders,\n\tarray(INT, 0) as WithdrawBuyNumber,\n\tarray(INT, 0) as WithdrawBuyAmount,\n\tarray(DOUBLE, 0) as WithdrawBuyMoney,\n\tarray(INT, 0) as WithdrawSellNumber,\n\tarray(INT, 0) as WithdrawSellAmount,\n\tarray(DOUBLE, 0) as WithdrawSellMoney,\n\tarray(INT, 0) as ETFBuyNumber,\n\tarray(INT, 0) as ETFBuyAmount,\n\tarray(DOUBLE, 0) as ETFBuyMoney,\n\tarray(INT, 0) as ETFSellNumber,\n\tarray(INT, 0) as ETFSellAmount,\n\tarray(DOUBLE, 0) as ETFSellMoney\n)\ndb.createPartitionedTable(table=schemaTable, tableName=tbName, partitionColumns=`DateTime`SecurityID, compressMethods={DateTime:\"delta\"}, sortColumns=`SecurityID`DateTime, keepDuplicates=ALL)\n```\n\n----------------------------------------\n\nTITLE: Calling Function Directly from Imported Namespaced Module\nDESCRIPTION: Demonstrates calling a function directly by its name after importing a namespaced module using the use keyword. This method is possible if the function name is unique among all functions directly available in the current scope, including other imported modules and local definitions.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/module_tutorial.md#_snippet_10\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nappendLog(\"mylog.txt\", \"test my log\")\n```\n\n----------------------------------------\n\nTITLE: Constructing Data and Creating Table Object - Java\nDESCRIPTION: This code snippet constructs data in Java and creates a table object. Each column corresponds to a List. The Array Vector column is a List of vectors, where each row is a vector.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Array_Vector.md#_snippet_23\n\nLANGUAGE: java\nCODE:\n```\nList<String> colNames = Arrays.asList(\"id\", \"value\");\nList<Vector> cols = new ArrayList<>(6);\nint rowNum = 4;\nList<Integer> idCol = new ArrayList<>(rowNum);\nList<Vector> valueCol = new ArrayList<>(rowNum);    // Array Vector 列\nfor(int i = 0; i < rowNum; ++i) {\n    idCol.add(i + 1);\n    List<Integer> valueRow = new ArrayList<>(50);   // Array Vector 中的一行\n    for(int j = 0; j < 3; ++j) {\n        valueRow.add(i*3 +j);\n    }\n    valueCol.add(new BasicIntVector(valueRow));\n}\ncols.add(new BasicIntVector(idCol));\ncols.add(new BasicArrayVector(valueCol));\nBasicTable tb = new BasicTable(colNames, cols);\n```\n\n----------------------------------------\n\nTITLE: Logging into DolphinDB Server\nDESCRIPTION: Authenticates the session by logging into the DolphinDB server using the username 'admin' and password '123456'. This step is required to gain permissions for subsequent operations like creating tables and engines.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/machine_learning_volatility/06.streamComputingReproduction.txt#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//login account\nlogin(\"admin\", \"123456\")\n```\n\n----------------------------------------\n\nTITLE: Defining Realized Volatility Function (DolphinDB Script)\nDESCRIPTION: Defines a custom DolphinDB function `realizedVolatility` to calculate the realized volatility of a time series `s`. It computes the square root of the sum of squares of the values in `s` using `sqrt(sum2(s))`. This is typically applied to returns.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/metacode_derived_features/metacode_derived_features_streaming.txt#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef realizedVolatility(s){\n\treturn sqrt(sum2(s))\n}\n```\n\n----------------------------------------\n\nTITLE: Checking if Two Tables Have Identical Content Using each High-Order Function in DolphinDB\nDESCRIPTION: Determines if tables 't1' and 't2' are exactly equal by applying 'eqObj' function element-wise across all columns using 'each' on their values tuples. The 'all' function verifies that all column-wise comparisons return true, indicating complete equivalence of the two tables at the data level. Requires both tables to have identical schema and content for a true result.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/func_progr_cases.md#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\nall(each(eqObj, t1.values(), t2.values()))\n```\n\n----------------------------------------\n\nTITLE: Submitting a Replay Job for High-Performance Streaming backtest with DolphinDB Script\nDESCRIPTION: Shows how to start a parallelized, rate-limited replay job from partitioned replay sources into a shared stream table via submitJob and replay, typically for downstream aggregation or analytics. Designed for large-scale scenarios with controlled output rate and concurrency, this is the principal entrypoint for automated streaming backtests.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/historical_data_replay.md#_snippet_10\n\nLANGUAGE: DolphinDB\nCODE:\n```\nsubmitJob(\"replay_quotes\", \"replay_quotes_stream\", replay, rds, outQuotes, `date, `time, 100000, true, 4)\n```\n\n----------------------------------------\n\nTITLE: Creating a Shared Table Using DolphinDBOperator in Python\nDESCRIPTION: This snippet demonstrates using the DolphinDBOperator within Airflow to create a shared table in DolphinDB by running an embedded DolphinDB script passed via the sql parameter. The code defines a task with a unique task_id, specifies the DolphinDB connection identifier, and provides a multi-line DolphinDB script that undefines any existing shared table named paramTable, constructs a new empty table with specified schema, and shares it under the name paramTable. This setup enables Airflow to manage DolphinDB operations programmatically. Dependencies: Airflow with dolphinDB provider plugin, and an active DolphinDB connection configured as 'dolphindb_test'.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Best_Practices_for_DolphinDB_and_Python_AirFlow.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n//在 DolphinDB 中创建一个共享表\ncreate_parameter_table = DolphinDBOperator(\n        task_id='create_parameter_table',\n        dolphindb_conn_id='dolphindb_test',\n        sql='''\n            undef(`paramTable,SHARED)\n            t = table(1:0, `param`value, [STRING, STRING])\n            share t as paramTable\n            '''\n    )\n```\n\n----------------------------------------\n\nTITLE: Controlling Access to Factor Data Using Function View\nDESCRIPTION: This code shows how to control access to specific columns within a table using function views. It creates a user `u1` and grants them `VIEW_EXEC` permission to a function view that only returns data for the `factor1` column. This allows for fine-grained access control at the data level.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practice_for_factor_calculation.md#_snippet_35\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//创建用户u1,我们想授予u1 只能读取因子factor1的权限\ncreateUser(\"u1\", \"111111\")\n//定义只取因子的函数\ndef getFactor1Table(){\n    t=select * from loadTable(\"dfs://db1\",\"factor\") where factor_name=\"factor1\";\n    return t;\n}\n//将函数保存到系统中\naddFunctionView(getFactor1Table)\n//将该函数权限授予用户u1\ngrant(\"u1\", VIEW_EXEC, \"getFactor1Table\");\n//注意新授予的权限，用户需要重新登录才能加载\n\nfactor1_tab=getFactor1Table()\n```\n\n----------------------------------------\n\nTITLE: 使用TSDB引擎创建宽表示例\nDESCRIPTION: 使用TSDB引擎创建同样的宽表，该引擎采用行列混存的存储策略，更适合存储列数超过100的宽表，并添加了排序列以提高查询效率。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/database_and_table_creation_details.md#_snippet_21\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndb= database(\"dfs://testDB2\", VALUE, 2020.01.01..2021.01.01,engine=\"TSDB\")\n//203行的宽表，使用TSDB引擎\ncolName = `ID`Date`Time\ncolName.append!(take(\"factor\"+string(0..200),200))\ncolType = `SYMBOL`DATE`TIME\ncolType.append!(take([\"DOUBLE\"],200))\ntbSchema = table(1:0, colName, colType)\ndb.createPartitionedTable(table=tbSchema, tableName=`pt, partitionColumns=`Date,sortColumns=`ID`Time)\n```\n\n----------------------------------------\n\nTITLE: Using segmentby to calculate cumulative sums\nDESCRIPTION: This code snippet uses the `segmentby` higher-order function to calculate cumulative sums on a vector 'x', grouping the elements based on the values in the vector 'y'.  Consecutive identical values in 'y' define the segments over which the `cumsum` function is applied to 'x'.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_37\n\nLANGUAGE: shell\nCODE:\n```\nx=1 2 3 0 3 2 1 4 5\ny=1 1 1 -1 -1 -1 1 1 1\nsegmentby(cumsum,x,y);\n```\n\n----------------------------------------\n\nTITLE: Defining getFactor function for Factor Calculation in DolphinDB\nDESCRIPTION: This function `getFactor` calculates a set of financial factors for each fund in the input `result2` table. It uses the previously defined functions (e.g., `getAnnualReturn`, `getAnnualVolatility`, `getSharp`, `getTrackError`, `getVar`, `getInforRatio`, `getHM1`). It groups the results by fund number and filters data within a specific date range and for a given list of fund numbers (`symList`).\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_factor.txt#_snippet_12\n\nLANGUAGE: DolphinDB\nCODE:\n```\n/**\n * 因子执行时间统计\n */\n\n \ndef getFactor(result2, symList){\n\tReturn = select fundNum, \n\t            getAnnualReturn(value) as annualReturn,\n\t            getAnnualVolatility(value) as annualVolRat,\n\t            getAnnualSkew(value) as skewValue,\n\t            getAnnualKur(value) as kurValue,\n\t            getSharp(value) as sharpValue,\n\t            getTrackError(value, price) as trackError,\n\t            getVar(value) as var,\n\t            getInforRatio(value, price) as infoRatio,\n\t            getHM1(value, price) as intercept       \n             from result2\n             where TradeDate in 2019.05.24..2022.05.27 and fundNum in symList group by fundNum\n }//定义计算九个因子的函数\n```\n\n----------------------------------------\n\nTITLE: Panel Matrix Aggregation by Label with regroup in DolphinDB\nDESCRIPTION: Provides an example of aggregating a pivoted panel matrix (created via SQL pivotBy) along one label using regroup and avg. Input: panel matrix constructed from securities/time grid, row labels as timestamps. Output: aggregated matrix grouped by minute-level time. Dependencies: rowNames, regroup, avg. Useful for summarizing large panel datasets by periodic intervals.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/panel_data.md#_snippet_29\n\nLANGUAGE: DolphinDB\nCODE:\n```\nn=1000\ntimestamp = 09:00:00 + rand(10000, n).sort!()\nid = take(`st1`st2`st3, n)\nvol = 100 + rand(10.0, n)\nt = table(timestamp, id, vol)\n\nm = exec vol from t pivot by timestamp, id\nregroup(m,minute(m.rowNames()), avg)\n```\n\n----------------------------------------\n\nTITLE: Converting Encoding of Schema Names in DolphinDB\nDESCRIPTION: This code snippet updates the schema table to convert the encoding of the 'name' column from 'gbk' to 'utf-8'. This is necessary because the old format data has Chinese field names encoded in GB2312, which appear as garbled text when extracted using `extractTextSchema`. The `convertEncode` function is used to perform the encoding conversion.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/stockdata_csv_import_demo.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\n\tupdate schema1 set name=convertEncode(name,\"gbk\",\"utf-8\") \n```\n\n----------------------------------------\n\nTITLE: Segment Window Cumulative Sum Grouped by Consecutive Order_Type in DolphinDB SQL\nDESCRIPTION: Uses the segment function with context by to split data into windows of consecutive identical values in 'order_type' column, and performs cumulative sum of 'vol' per segment. Input is a table with alternating vol and order_type arrays. Output displays cumulative sums computed over dynamically sized segments defined by runs of identical order_type values. This method allows variable-length windows based on content rather than fixed row or time counts, common for stepwise analytic computations in trade/order data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/window_cal.md#_snippet_12\n\nLANGUAGE: DolphinDB\nCODE:\n```\nvol = 0.1 0.2 0.1 0.2 0.1 0.2 0.1 0.2 0.1 0.2 0.1 0.2\norder_type = 0 0 1 1 1 2 2 1 1 3 3 2;\nt = table(vol,order_type);\nselect *, cumsum(vol) as cumsum_vol from t context by segment(order_type);\n\n# output\n\nvol order_type cumsum_vol\n--- ---------- ----------\n0.1 0          0.1       \n0.2 0          0.3       \n0.1 1          0.1       \n0.2 1          0.3       \n0.1 1          0.4       \n0.2 2          0.2       \n0.1 2          0.3       \n0.2 1          0.2       \n0.1 1          0.3       \n0.2 3          0.2       \n0.1 3          0.3       \n0.2 2          0.2  \n```\n\n----------------------------------------\n\nTITLE: QR Decomposition with r mode (m<=n) in DolphinDB\nDESCRIPTION: Demonstrates QR decomposition of a matrix in DolphinDB using the `qr` function with `mode='r'` and `pivoting=false`.  The function returns the R matrix resulting from the QR decomposition with full mode.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/matrix.md#_snippet_19\n\nLANGUAGE: DolphinDB\nCODE:\n```\n>m=matrix([2 5 7 5, 5 2 5 4, 8 2 6 4]);\n>m;\n#0 #1 #2\n-- -- --\n2  5  8 \n5  2  2 \n7  5  6 \n5  4  4  \n\n>r=qr(m,mode='r'); //pivoting=false\n>r;\n#0         #1       #2       \n---------- -------- ---------\n-10.148892 -7.38997 -8.670898\n0          3.922799 6.608121 \n0          0        1.071571 \n0          0        0        \n\n```\n\n----------------------------------------\n\nTITLE: Stateful Factor Calculation for Small Order Net Amount Ratio in DolphinDB\nDESCRIPTION: This DolphinDB script defines a stateful function `factorSmallOrderNetAmountRatio` to calculate the ratio of small order net inflow to total trading volume. It uses `dynamicGroupCumsum` for incremental calculation of small and large order cumulative amounts.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practice_for_factor_calculation.md#_snippet_11\n\nLANGUAGE: DolphinDB\nCODE:\n```\n@state\ndef factorSmallOrderNetAmountRatio(tradeAmount, sellCumAmount, sellOrderFlag, prevSellCumAmount, prevSellOrderFlag, buyCumAmount, buyOrderFlag, prevBuyCumAmount, prevBuyOrderFlag){\n\tcumsumTradeAmount = cumsum(tradeAmount)\n\tsmallSellCumAmount, bigSellCumAmount = dynamicGroupCumsum(sellCumAmount, prevSellCumAmount, sellOrderFlag, prevSellOrderFlag, 2)\n\tsmallBuyCumAmount, bigBuyCumAmount = dynamicGroupCumsum(buyCumAmount, prevBuyCumAmount, buyOrderFlag, prevBuyOrderFlag, 2) \n\tf = (smallBuyCumAmount - smallSellCumAmount) \\ cumsumTradeAmount\n\treturn smallBuyCumAmount, smallSellCumAmount, cumsumTradeAmount, f\n}\n```\n\n----------------------------------------\n\nTITLE: Defining snapCreate Function in DolphinDB\nDESCRIPTION: This function `snapCreate` creates a DolphinDB database and a partitioned table within it. It takes the database name and table name as input. The function first checks if the database exists and drops it if it does. It then creates a composite database with value and hash partitioning. It defines a table schema with various columns and uses it to create a partitioned table with specified partition and sort columns and compression methods.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/appendices_market_replay_bp/snap_create.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef snapCreate(dbName, tbName)\n{\n    if(existsDatabase(dbName))\n    {\n\t    dropDatabase(dbName)\n    }\n\n    db1 = database(, VALUE, 2021.12.01..2021.12.31)\n    db2 = database(, HASH, [SYMBOL, 20])\n    db = database(dbName, COMPO, [db1, db2], , \"TSDB\")\n\n    schemaTable = table(\n        array(SYMBOL, 0) as SecurityID,\n        array(DATE, 0) as MDDate,\n        array(TIME, 0) as MDTime,\n        array(TIMESTAMP, 0) as DataTimestamp,\n        array(SYMBOL, 0) as TradingPhaseCode,\n        array(SYMBOL, 0) as SecurityIDSource,\n        array(SYMBOL, 0) as SecurityType,\n        array(INT, 0) as MaxPx,\n        array(INT, 0) as MinPx,\n        array(INT, 0) as PreClosePx,\n        array(INT, 0) as NumTrades,\n        array(INT, 0) as TotalVolumeTrade,\n        array(INT, 0) as TotalValueTrade,\n        array(INT, 0) as LastPx,\n        array(INT, 0) as OpenPx,\n        array(INT, 0) as ClosePx,\n        array(INT, 0) as HighPx,\n        array(INT, 0) as LowPx,\n        array(INT, 0) as DiffPx1,\n        array(INT, 0) as DiffPx2,\n        array(INT, 0) as TotalBuyQty,\n        array(INT, 0) as TotalSellQty,\n        array(INT, 0) as WeightedAvgBuyPx,\n        array(INT, 0) as WeightedAvgSellPx,\n        array(INT, 0) as WithdrawBuyNumber,\n        array(INT, 0) as WithdrawBuyAmount,\n        array(INT, 0) as WithdrawBuyMoney,\n        array(INT, 0) as WithdrawSellNumber,\n        array(INT, 0) as WithdrawSellAmount,\n        array(INT, 0) as WithdrawSellMoney,\n        array(INT, 0) as TotalBuyNumber,\n        array(INT, 0) as TotalSellNumber,\n        array(INT, 0) as BuyTradeMaxDuration,\n        array(INT, 0) as SellTradeMaxDuration,\n        array(INT, 0) as NumBuyOrders,\n        array(INT, 0) as NumSellOrders,\n        array(INT, 0) as NorminalPx,\n        array(INT, 0) as ShortSellSharesTraded,\n        array(INT, 0) as ShortSellTurnover,\n        array(INT, 0) as ReferencePx,\n        array(TIMESTAMP, 0) as ComplexEventStartTime,\n        array(TIMESTAMP, 0) as ComplexEventEndTime,\n        array(DATE, 0) as ExchangeDate,\n        array(TIME, 0) as ExchangeTime,\n        array(INT, 0) as AfterHoursNumTrades,\n        array(INT, 0) as AfterHoursTotalVolumeTrade,\n        array(INT, 0) as AfterHoursTotalValueTrade,\n        array(INT, 0) as ChannelNo,\n        array(INT[], 0) as BuyPriceQueue,\n        array(INT[], 0) as BuyOrderQtyQueue,\n        array(INT[], 0) as SellPriceQueue,\n        array(INT[], 0) as SellOrderQtyQueue,\n        array(INT[], 0) as BuyOrderQueue,\n        array(INT[], 0) as SellOrderQueue,\n        array(INT[], 0) as BuyNumOrdersQueue,\n        array(INT[], 0) as SellNumOrdersQueue,\n        array(INT, 0) as MaxBuyPrice,\n        array(INT, 0) as MinBuyPrice,\n        array(INT, 0) as MaxSellPrice,\n        array(INT, 0) as MinSellPrice,\n        array(INT, 0) as PreMarketLastPx,\n        array(INT, 0) as PreMarketTotalVolumeTrade,\n        array(INT, 0) as PreMarketTotalValueTrade,\n        array(INT, 0) as PreMarketHighPx,\n        array(INT, 0) as PreMarketLowPx,\n        array(INT, 0) as AfterHoursLastPx,\n        array(INT, 0) as AfterHoursHighPx,\n        array(INT, 0) as AfterHoursLowPx,\n        array(SYMBOL, 0) as MarketPhaseCode\n    )\n\n    db.createPartitionedTable(table=schemaTable, tableName=tbName, partitionColumns=`MDDate`SecurityID, sortColumns=`SecurityID`MDTime, keepDuplicates=ALL,compressMethods={MDDate:\"delta\", MDTime:\"delta\",DataTimestamp:\"delta\",ComplexEventStartTime:\"delta\",ComplexEventEndTime:\"delta\",ExchangeDate:\"delta\",ExchangeTime:\"delta\"})\n}\n\n```\n\n----------------------------------------\n\nTITLE: Defining Prediction Handler and Subscribing in DolphinDB Script\nDESCRIPTION: Defines a handler function `predictRV` that takes messages (aggregated features) from the `aggrFeatures10min` table, uses the pre-loaded `model` to predict values (presumably realized volatility), measures the prediction time cost, and appends the results (TradeTime, SecurityID, Prediction, CostTime) to the `result1min` stream table. It then subscribes this handler function to the `aggrFeatures10min` table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/machine_learning_volatility/05.streamComputingArrayVector.txt#_snippet_5\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\n//define handler\ndef predictRV(mutable result1min, model, msg){\n\tstartTime = now()\n\tpredicted = model.predict(msg)\n\ttemp = select TradeTime, SecurityID, predicted as PredictRV, (now()-startTime) as CostTime from msg\n\tresult1min.append!(temp)\n}\n//subscribe data\nsubscribeTable(tableName=\"aggrFeatures10min\", actionName=\"predictRV\", offset=-1, handler=predictRV{result1min, model}, msgAsTable=true, hash=1, reconnect=true)\ngo\n```\n\n----------------------------------------\n\nTITLE: Data Concatenation and Arithmetic Operations in pandas and DolphinDB\nDESCRIPTION: This snippet illustrates concatenation and element-wise arithmetic functions such as addition, subtraction, multiplication, and division in pandas, along with their DolphinDB counterparts. It emphasizes how combined null-filling logic (withNullFill) is integrated in DolphinDB operations.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/function_mapping_py.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\npandas.concat  # Concatenate DataFrames or Series\npandas.DataFrame.add / pandas.Series.add  # Element-wise addition with null filling\npandas.DataFrame.sub / pandas.Series.sub  # Element-wise subtraction\npandas.DataFrame.mul / pandas.Series.mul  # Element-wise multiplication\npandas.DataFrame.div / pandas.Series.div  # Element-wise division or ratio\n```\n\n----------------------------------------\n\nTITLE: Querying - Range Query (Multi-Dimension) DolphinDB\nDESCRIPTION: This snippet performs a range query to select records within a specific time interval and for a list of device IDs. This exemplifies querying across multiple partition dimensions which offers greater data filtering and analysis power.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs TimescaleDB/test_dolphindb_small.txt#_snippet_13\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 4. 范围查询.多分区维度: 查询某时间段内某些设备的所有记录\ntimer\nselect *\nfrom readings\nwhere\n\ttime between 2016.11.17 20:00:00 : 2016.11.17 20:30:00,\n\tdevice_id in ['demo000001', 'demo000010', 'demo000100', 'demo001000']\n```\n\n----------------------------------------\n\nTITLE: Optimized Range Segment Count with asof Grouping in DolphinDB Script\nDESCRIPTION: Counts records per numeric range efficiently using asof on value, grouping by date, code, and asof(range, value). This approach yields much better performance than the looped custom grouping and is syntactically concise, relying on built-in asof functionality.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_SQL_Case_Tutorial.md#_snippet_40\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntimer res2 = select count(*) from t \n\t\t\tgroup by date, code, asof(range, value) as grp\n```\n\n----------------------------------------\n\nTITLE: Registering a Lookup Join Engine in DolphinDB for Trade and Snapshot Data\nDESCRIPTION: This snippet creates a DolphinDB lookup join engine linking 'trade' and 'snapshot' schemas to facilitate real-time join operations. It specifies the output table and matching columns, enabling efficient data correlation for trade analysis.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/stock_market_replay/03.calTradeCost_lookUpJoin.txt#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// register look up join stream computing engine\njoinEngine = createLookupJoinEngine(name=\"tradeJoinSnapshot\", leftTable=tradeSchema, rightTable=snapshotSchema, outputTable=prevailingQuotes, metrics=<[tradeSchema.Time, Price, TradeQty, BidPX1, OfferPX1, abs(Price-(BidPX1+OfferPX1)/2), snapshotSchema.Time]>, matchingColumn=`SecurityID)\n```\n\n----------------------------------------\n\nTITLE: Multiple Writer Parallel Loading with JobScheduling in DolphinDB\nDESCRIPTION: Creates a partitioned database and implements parallel data loading using background jobs. It defines a function that loads stock quote data from multiple CSV files into a date-symbol composite partitioned database, ensuring that writers don't write to the same partition simultaneously.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/database.md#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndateDomain = database(\"\", VALUE, 2018.05.01..2018.07.01)\nsymDomain = database(\"\", RANGE, string('A'..'Z') join `ZZZZZ)\nstockDB = database(\"dfs://stockDB\", COMPO, [dateDomain, symDomain])\nquoteSchema = table(10:0, `sym`date`time`bid`bidSize`ask`askSize, [SYMBOL,DATE,TIME,DOUBLE,INT,DOUBLE,INT])\nstockDB.createPartitionedTable(quoteSchema, \"quotes\", `date`sym)\n\ndef loadJob(){\n\tfileDir='/stockData'\n\tfilenames = exec filename from files(fileDir)\n\tdb = database(\"dfs://stockDB\")\n\n\tfor(fname in filenames){\n\t\tjobId = fname.strReplace(\".csv\", \"\")\n\t\tsubmitJob(jobId,, loadTextEx{db, \"quotes\", `date`sym, fileDir+'/'+fname})\n\t}\n}\n\npnodeRun(loadJob);\n```\n\n----------------------------------------\n\nTITLE: 定义与共享门禁数据输入输出流数据表 - DolphinDB\nDESCRIPTION: 定义用于接收门禁监控设备数据的输入流数据表，包含事件类型、事件码、事件时间、读卡器类型、设备序列号、门号及卡号等字段，利用enableTableShareAndPersistence函数实现数据表共享及持久化，内存缓存大小设为10万。随后定义异常状态输出流数据表，满足响应式状态引擎对输出格式严格要求，设置分组列为门号doorNum，并持久化到磁盘。两者作为流数据输入输出的基础支撑表。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_engine_anomaly_alerts.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\nst=streamTable(\n\tarray(INT,0) as recordype, //记录类型\n\tarray(INT,0) as doorEventCode, //事件码\n    array(DATETIME,0) as eventDate, //事件时间 \n    array(BOOL,0) as readerType, //进出类型 1:入 0:出\n    \tarray(SYMBOL,0) as sn, //设备SN号\n    array(INT,0) as doorNum, //门号\n    array(SYMBOL,0) as card //卡号            \n\t)\nenableTableShareAndPersistence(st,`doorRecord, false, true, 100000, 100, 0);\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nout1 =streamTable(10000:0,`doorNum`eventDate`doorEventCode,[INT,DATETIME, INT])\nenableTableShareAndPersistence(out1,`outputSt,false,true,100000)\n```\n\n----------------------------------------\n\nTITLE: Dynamic SQL Query Generation and Execution using parseExpr and eval in DolphinDB\nDESCRIPTION: Constructs a SQL group-by statement for time-bar aggregation as a string, parses it into expression code with parseExpr, and executes it via eval. This approach demonstrates traditional metaprogramming to automate SQL assembly for cases where aggregation parameters (e.g., interval width) may vary at runtime. Dependencies: table t, variable barMinutes.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_sql_cases.md#_snippet_43\n\nLANGUAGE: DolphinDB\nCODE:\n```\nres = parseExpr(\"select \" + avg + \"(Px) as avgPx from t group by bar(TradeTime, \" + barMinutes + \"m) as minuteTradeTime, SecurityID, TradeDate\").eval()\n```\n\n----------------------------------------\n\nTITLE: Calculating Difference Between Hourly Averages using Stream Engines in DolphinDB Script\nDESCRIPTION: This script demonstrates real-time stream processing to calculate the difference between consecutive hourly average values. It sets up an input stream table ('inTable'), a TimeSeries engine ('hour') to compute hourly averages outputting to 'outTable', and a ReactiveState engine ('sub_hour') to calculate the difference ('deltas') between consecutive averages, writing the result to the 'result' table. It also shows how to load historical data into the input stream for testing.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Iot_intelligent_O&M.md#_snippet_10\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\n//案例 10：每个整点的平均值相减，比如 1~2 点的值减去 0~1 点的值，如此循环。\n//步骤说明：\n//步骤一：用时序引擎，数据按整点规整。\n//步骤二：用响应式状态引擎，进行差值计算。\n//步骤三：输出到 result 表\n\n//数据写入\nshare streamTable(1000:0, `ts`deviceCode`logicalPositionId`physicalPositionId`propertyCode`propertyValue,\n        [DATETIME,SYMBOL,SYMBOL,SYMBOL,SYMBOL,DOUBLE]) as inTable\n\n//时序引擎的输出表\nshare streamTable(1000:0, `ts`deviceCode`propertyCode`propertyValue,\n        [DATETIME,SYMBOL,SYMBOL,DOUBLE]) as outTable\n\n//时序引擎，实时计算均值指标（窗口 1 h，步长 1 h，每个设备的每个测点为一组进行分组计算）\nhour = createTimeSeriesEngine(name=\"hour\",  windowSize=3600, step=3600, metrics=<[avg(propertyValue)]>, dummyTable=inTable, outputTable=outTable, timeColumn=`ts, keyColumn=[`deviceCode,`propertyCode])\nsubscribeTable(tableName=\"inTable\", actionName=\"act_hour\", offset=0, handler=append!{hour}, msgAsTable=true, batchSize=50, throttle=1, hash=0)\n\n//状态响应引擎，每个相邻整点值相减\nshare streamTable(1000:0, `deviceCode`propertyCode`ts`sub_propertyValue,\n        [SYMBOL,SYMBOL,DATETIME,DOUBLE]) as result\nsub_hour = createReactiveStateEngine(name=\"sub_hour\", metrics = <[ts,deltas(propertyValue)]>, dummyTable=outTable, outputTable=result, keyColumn=[`deviceCode,`propertyCode])\nsubscribeTable(tableName=\"outTable\", actionName=\"act_sub_hour\", offset=0, handler=append!{sub_hour}, msgAsTable=true, batchSize=50, throttle=1, hash=0)\n\n//将某个设备的数据写入，启动流式框架\npt = loadTable(\"dfs://db_test\", \"collect\");\nselect count(*) from pt; \nt = select * from pt where deviceCode = \"361RP00\" and propertyCode = \"361RP00000\"\ninTable.append!(t);\n\n//流数据引擎计算窗口内的平均值\nselect * from outTable where deviceCode = \"361RP00\" and propertyCode = \"361RP00000\";\n//sql 查询窗口内的平均值\nselect format(avg(propertyValue),\"0.00\") from t where deviceCode = \"361RP00\" and propertyCode = \"361RP00000\" group by datehour(ts)\n```\n\n----------------------------------------\n\nTITLE: Parallel Data Loading with parallelLoad\nDESCRIPTION: The code shows how to load data in parallel using DolphinDB's asynchronous task submission feature. It includes the function `loadOneDayFile` to load data of a single day, and `parallelLoad` function that iterates through directories by date, and submits multiple asynchronous tasks to improve the loading performance. It utilizes `submitJob` to execute the import tasks in parallel.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/LoadDataForPoc.md#_snippet_10\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef loadOneDayFile(db,table,filePath)\n{\n\tcsvFiles = exec filename from files(filePath) where filename like\"%.csv\"\n\tfor(csvIdx in csvFiles)\n\t{\n\t\tloadTextEx(dbHandle = db, tableName = table, partitionColumns = `col1`col0, filename = filePath + \"/\"  + csvIdx, transform = transType, skipRows = 1)\n\t}\n}\n\ndef parallelLoad(allFileContents)\n{\n\tdb = database(\"dfs://sh_entrust\")\n\ttable = `entrust\n\tdateFiles = exec filename from files(allFileContents) where isDir = true\n\tfor(dateIdx in dateFiles)\n\t{\n\t\tsubmitJob(\"parallelLoad\" + dateIdx,\"parallelLoad\",loadOneDayFile{db,table,},allFileContents + \"/\" + dateIdx)\n\t}\n}\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nallFileContents = \"/home/ychan/data/loadForPoc/SH/Order\"\nparallelLoad(allFileContents)\n```\n\n----------------------------------------\n\nTITLE: Defining Schema and Processing CSV Entrust Data with Cleaning and Deduplication - DolphinDB\nDESCRIPTION: Provides a module that defines the schema for the entrust CSV data including field names and types. It implements a processing function that modifies input data by combining date and time fields into timestamps, removes duplicates based on multiple key fields, adds default or null values to missing columns, and reorders columns to match the target schema. Assumes the input table is mutable and uses DolphinDB table and update operators extensively.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/dolphinscheduler_integration.md#_snippet_11\n\nLANGUAGE: DolphinDB\nCODE:\n```\nmodule stockData::stockDataProcess\n\n// 定义逐笔委托csv数据文件中各个字段的名称和字段类型\ndef schemaEntrust()\n{\n\tname = `DataStatus`OrderIndex`ChannelNo`SecurityID`TradeTime`OrderType`ApplSeqNum`Price`OrderQty`Side`BizIndex`LocalTime`SeqNo\n\ttypeString = `INT`LONG`INT`SYMBOL`TIME`SYMBOL`INT`DOUBLE`INT`SYMBOL`INT`TIME`INT\n\treturn table(name, typeString)\n}\n\n// 数据处理函数，包括字段增加，数据去重等操作\ndef processEntrust(loadDate, mutable t)\n{\n\t// 字段名替换\n    t.replaceColumn!(`TradeTime, concatDateTime(day, t.TradeTime))\n\tn1 = t.size()\n    // 数据去重\n\tt = select * from t where isDuplicated([DataStatus, OrderIndex, ChannelNo, SecurityID, TradeTime, OrderType, ApplSeqNum, Price, OrderQty, Side, BizIndex],FIRST)=false\n\tn2 = t.size()\n    // 增加字段\n\tupdate t set Market = `sh\n\tupdate t set MDStreamID = int(NULL)\n\tupdate t set SecurityIDSource = int(NULL)\n\treorderColumns!(t, `ChannelNo`ApplSeqNum`MDStreamID`SecurityID`SecurityIDSource`Price`OrderQty`Side`TradeTime`OrderType`OrderIndex`LocalTime`SeqNo`Market`DataStatus`BizIndex)\n\treturn t,n1,n2\n}\n```\n\n----------------------------------------\n\nTITLE: 计算日频当日尾盘成交占比因子函数及调用示例 - Python\nDESCRIPTION: 本段 Python Parser 代码示例阐述如何基于逐笔交易数据，计算日频因子“当日尾盘成交占比”(BCVP)。定义了函数 beforeClosingVolumePercent 用于计算指定时间段(14:30到15:00)内成交量占比。依赖 pandas 和 dolphindb API，数据通过 loadTable 获取，转为 lazily 计算的 pandas DataFrame。通过过滤时间和分组调用实现并行因子计算。输入为逐笔交易的 DataFrame，输出为包含因子值的 Series。适用于 DolphinDB Python Parser 环境，支持时间类型的精确转换和分布式计算。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_Python_Parser_Intro_for_Quantitative_Finance.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport dolphindb as ddb\n\n# 定义因子函数\ndef beforeClosingVolumePercent(trade):\n    tradeTime = trade[\"TradeTime\"].astype(ddb.TIME)\n    beforeClosingVolume = trade[\"TradeQty\"][(tradeTime >= 14:30:00.000)&(tradeTime <= 15:00:00.000)].sum()\n    totalVolume = trade[\"TradeQty\"].sum()\n    res = beforeClosingVolume / totalVolume\n    return pd.Series([res], [\"BCVP\"])\n\n# 指定计算某一天的因子\ntradeTB = loadTable(\"dfs://TL_Level2\", \"trade\")\ndf = pd.DataFrame(tradeTB, index=\"Market\", lazy=True)\nres = df[df[\"TradeTime\"].astype(ddb.DATE)==2023.02.01][[\"TradeTime\", \"SecurityID\", \"TradeQty\"]].groupby([\"SecurityID\"]).apply(beforeClosingVolumePercent)\n```\n\n----------------------------------------\n\nTITLE: Recreating Stream Engine with Snapshot and Resuming Subscription in DolphinDB\nDESCRIPTION: This snippet illustrates recreating a stream engine with snapshot parameters, fetching the last message ID, and subscribing beginning from the next message to resume processing after a system restart or failure, ensuring no data is missed. It shows the final output matching non-interrupted subscription flow, by loading previous state and continuing data ingestion.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/stream_aggregator.md#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\nAgg1 = createTimeSeriesEngine(name=`Agg1, windowSize=100, step=50, metrics=<sum(price)>, dummyTable=trades, outputTable=output1, timeColumn=`time)\n\nofst=getSnapshotMsgId(Agg1)\nprint(ofst)\n\nsubscribeTable(server=\"\", tableName=\"trades\", actionName=\"Agg1\",offset= ofst+1, handler=appendMsg{Agg1}, msgAsTable=true, handlerNeedMsgId=true)\n\nselect * from output1\n```\n\n----------------------------------------\n\nTITLE: Creating Composite Partitioned Database by Date and Symbol Buckets in DolphinDB\nDESCRIPTION: This snippet defines composite partitioning schemas for a DolphinDB database. It creates date partitions between 2007.08.05 and 2007.08.12 using VALUE partitioning and symbol partitions using RANGE schema with the precomputed buckets array. It checks for the existing distributed database 'dfs://db_compound_dfs', drops it if present, then creates a new composite partitioned database on the distributed file system. This prepares the distributed environment for efficient data storage and retrieval.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs MongoDB/dolphindb_taq_partitioned.txt#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\nDATE_RANGE = 2007.08.05..2007.08.12\n// 创建数据库分区方案\ndate_schema   = database('', VALUE, DATE_RANGE)\nsymbol_schema = database('', RANGE, buckets)\n//FP_DB = FP_TAQ + 'db/'\n//db = database(FP_DB, COMPO, [date_schema, symbol_schema])\ndb_path=\"dfs://db_compound_dfs\"\nif(existsDatabase(db_path)){\n\tdropDatabase(db_path)\n}\ndb = database(db_path, COMPO, [date_schema, symbol_schema])\n```\n\n----------------------------------------\n\nTITLE: Querying Specific Stream Engine (DailyTimeSeriesEngine) in DolphinDB\nDESCRIPTION: This snippet queries the statistics of a specific stream computing engine, specifically the `DailyTimeSeriesEngine`. It calls the `getStreamEngineStat()` function and then accesses the `DailyTimeSeriesEngine` property to retrieve the engine's statistics.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/streaming_capital_flow_order_by_order/07.流计算状态监控函数.txt#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\ngetStreamEngineStat().DailyTimeSeriesEngine\n```\n\n----------------------------------------\n\nTITLE: Import multiple files with loop\nDESCRIPTION: This DolphinDB script imports multiple CSV files from a directory into a single DolphinDB in-memory table. It uses the `loop` function to apply the `loadText` function to each file and then uses `unionAll` to combine the imported tables. Requires the files to be structurally identical.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_23\n\nLANGUAGE: shell\nCODE:\n```\nloop(loadText, fileDir + \"/\" + files(fileDir).filename).unionAll(false)\n```\n\n----------------------------------------\n\nTITLE: Creating Alias for Complex Calculations with sqlColAlias\nDESCRIPTION: This snippet demonstrates the usage of `sqlColAlias` to assign an alias to a single-column SQL expression, created using `sqlCol`.  It shows how to assign a different name to the given column name using alias.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/macro_var_based_metaprogramming.md#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// sqlColAlias(sqlCol(\"col\"), \"newCol\") --> <col as newCol>\n```\n\n----------------------------------------\n\nTITLE: Calculating Implied Volatility using Bisection Method with JIT (DolphinDB Script)\nDESCRIPTION: Defines three functions (`calculateD1JIT`, `calculatePriceJIT`, `calculateImpvJIT`) optimized with the `@jit` decorator for faster execution. `calculateImpvJIT` implements the bisection method (iterative approximation) to find the implied volatility (IV) for a single option contract. It takes scalar inputs: option price, ETF price, strike price (KPrice), risk-free rate (r), time to maturity ratio (dayRatio), and option type (CPMode). The loop continues until the range [low, high] for IV is sufficiently small (<= 0.00001). `calculatePriceJIT` calculates the theoretical option price based on a given volatility (HLMean) using the Black-Scholes formula, and `calculateD1JIT` computes the d1 term needed.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/IV_Greeks_Calculation_for_ETF_Options_Using_JIT.md#_snippet_7\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\n@jit\ndef calculateD1JIT(etfTodayPrice, KPrice, r, dayRatio, HLMean){\n\tskRatio = etfTodayPrice / KPrice\n\tdenominator = HLMean * sqrt(dayRatio)\n\tresult = (log(skRatio) + (r + 0.5 * pow(HLMean, 2)) * dayRatio) / denominator\n\treturn result\n}\n\n@jit\ndef calculatePriceJIT(etfTodayPrice, KPrice , r , dayRatio , HLMean , CPMode){\n\ttestResult = 0.0\n\tif (HLMean <= 0){\n\t\ttestResult = CPMode * (etfTodayPrice - KPrice)\n\t\tif(testResult<0){\n\t\t\treturn 0.0\n\t\t}\n\t\treturn testResult\n\t}\n\td1 = calculateD1JIT(etfTodayPrice, KPrice, r, dayRatio, HLMean)\n\td2 = d1 - HLMean * sqrt(dayRatio)\n\tprice = CPMode * (etfTodayPrice * cdfNormal(0, 1, CPMode * d1) - KPrice * cdfNormal(0, 1, CPMode * d2) * exp(-r * dayRatio))\n\treturn price\n}\n\n@jit\ndef calculateImpvJIT(optionTodayClose, etfTodayPrice, KPrice, r, dayRatio, CPMode){\n\tv = 0.0\t\n\thigh = 2.0\n\tlow = 0.0\n\tdo{\n\t\tif ((high - low) <= 0.00001){\n\t\t\tbreak\n\t\t}\n\t\tHLMean = (high + low) / 2.0\n\t\tif (calculatePriceJIT(etfTodayPrice, KPrice, r, dayRatio, HLMean, CPMode) > optionTodayClose){\n\t\t\thigh = HLMean\n\t\t}\n\t\telse{\n\t\t\tlow = HLMean\n\t\t}\n\t}\n\twhile(true)\n\tv = (high + low) / 2.0\n\treturn v\n}\n```\n\n----------------------------------------\n\nTITLE: Calculating Alpha 98 Factor Using SQL and Table Updates - DolphinDB\nDESCRIPTION: This code illustrates factor calculation requiring both time-series and cross-sectional operations in DolphinDB's SQL mode. The alpha98SQL function updates or appends intermediate columns to the input table via windowed operations (moving averages, correlations, ranks) partitioned by security or date, then computes the final factor using these features. Expected input: a table with vwap, vol, and open columns, indexed by tradetime and securityid. The snippet relies heavily on DolphinDB’s SQL vector and context-by abilities to process large cross-sectional datasets.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practice_for_factor_calculation.md#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//alpha98\ndef alpha98SQL(mutable t){\n\tupdate t set adv5 = mavg(vol, 5), adv15 = mavg(vol, 15) context by securityid\n\tupdate t set rank_open = rank(X = open,percent=true), rank_adv15 =rank(X=adv15,percent=true) context by date(tradetime)\n\tupdate t set decay7 = mavg(mcorr(vwap, msum(adv5, 26), 5), 1..7), decay8 = mavg(mrank(9 - mimin(mcorr(rank_open, rank_adv15, 21), 9), true, 7), 1..8) context by securityid\n\treturn select tradetime,securityid, `alpha98 as factorname, rank(X =decay7,percent=true)-rank(X =decay8,percent=true) as val from t context by date(tradetime)\n}\ninput = select tradetime,securityid, vwap,vol,open from  loadTable(\"dfs://k_day_level\",\"k_day\") where tradetime between 2010.01.01 : 2010.12.31\nalpha98DDBSql = alpha98SQL(input)\n\n```\n\n----------------------------------------\n\nTITLE: Loading MySQL Plugin and Connecting to OceanBase\nDESCRIPTION: Commands to load the MySQL plugin and establish a connection to an OceanBase database using the MySQL protocol. This is a prerequisite step before data migration.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/OceanBase_to_DolphinDB.md#_snippet_1\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nloadPlugin(\"ServerPath/plugins/mysql/PluginMySQL.txt\")\nconn = mysql::connect(`127.0.0.1,2881,`root,`123456,`db1)\n```\n\n----------------------------------------\n\nTITLE: Registering Stream Engines and Subscribing to Tables (DolphinDB)\nDESCRIPTION: This DolphinDB code registers `ReactiveStateEngine` and `DailyTimeSeriesEngine` for real-time processing of trade data.  It creates ReactiveStateEngine to perform an initial data transformation. It then creates a DailyTimeSeriesEngine for calculating capital flow over a 60-second window.  The engines are subscribed to the `tradeOriginalStream` and `tradeProcessStream` tables, respectively, with parallel processing enabled based on the `parallel` parameter.  A third subscription sends `tradeOriginalStream` data to the database.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_capital_flow_order_by_order.md#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//real time calculation of capitalFlow\n//calculation parallel, developers need to modify according to the development environment\nparallel = 3\nfor(i in 1..parallel){\n\t//create ReactiveStateEngine: tradeProcess\n\tcreateReactiveStateEngine(name=\"tradeProcess\"+string(i), metrics=[<TradeTime>, <iif(BuyNum>SellNum, BuyNum, SellNum)>, <TradeQty>, <TradeAmount>, <iif(BuyNum>SellNum, \"B\", \"S\")>], dummyTable=tradeOriginalStream, outputTable=tradeProcessStream, keyColumn=\"SecurityID\")\n\tsubscribeTable(tableName=\"tradeOriginalStream\", actionName=\"tradeProcess\"+string(i), offset=-1, handler=getStreamEngine(\"tradeProcess\"+string(i)), msgAsTable=true, hash=i-1, filter = (parallel, i-1), reconnect=true)\n\t//create DailyTimeSeriesEngine: tradeTSAggr\n\tcreateDailyTimeSeriesEngine(name=\"tradeTSAggr\"+string(i), windowSize=60000, step=60000, metrics=[<calCapitalFlow(Num, BSFlag, TradeQty, TradeAmount) as `BuySmallAmount`BuyBigAmount`SellSmallAmount`SellBigAmount>], dummyTable=tradeProcessStream, outputTable=capitalFlowStream, timeColumn=\"TradeTime\", useSystemTime=false, keyColumn=`SecurityID, useWindowStartTime=true, forceTriggerTime=60000)\n\tsubscribeTable(tableName=\"tradeProcessStream\", actionName=\"tradeTSAggr\"+string(i), offset=-1, handler=getStreamEngine(\"tradeTSAggr\"+string(i)), msgAsTable=true, batchSize=2000, throttle=1, hash=parallel+i-1, filter = (parallel, i-1), reconnect=true)\n}\n\n//real time data to database\nsubscribeTable(tableName=\"tradeOriginalStream\", actionName=\"tradeToDatabase\", offset=-1, handler=loadTable(\"dfs://trade_stream\", \"trade\"), msgAsTable=true, batchSize=20000, throttle=1, hash=6, reconnect=true)\n```\n\n----------------------------------------\n\nTITLE: Appending Data to Partitioned Table in DolphinDB\nDESCRIPTION: Demonstrates generating sample stock quote data and using the append! function to load it into a previously created partitioned database. This is the foundational method for all data import operations in DolphinDB.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/database.md#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\nn = 1000000\nsyms = `IBM`MSFT`GM`C`FB`GOOG`V`F`XOM`AMZN`TSLA`PG`S\ntime = 09:30:00 + rand(21600000, n)\nbid = rand(10.0, n)\nbidSize = 1 + rand(100, n)\nask = rand(10.0, n)\naskSize = 1 + rand(100, n)\nquotes = table(rand(syms, n) as sym, take(2018.05.04..2018.05.11,n) as date, time, bid, bidSize, ask, askSize)\n\nloadTable(\"dfs://stockDB\", \"quotes\").append!(quotes);\n```\n\n----------------------------------------\n\nTITLE: Replaying Entrust and Trade Tables for Tick Data Generation in DolphinDB Script\nDESCRIPTION: This snippet performs the union/replay of tick-level 'entrust' and 'trade' tables into a unified stream for each thread, using market-specific data formats and sorting strategies. It defines a core function 'replayBySymbol' which structures the inputs for replayDS and replay functions, applies different ordering for Shenzhen (SZ) and Shanghai (SH) markets, and stores results in shared in-memory tables. Prerequisites: the 'entrust' and 'trade' tables must be available as distributed tables; requires thread-partitioned symbol lists from previous steps. Outputs include shared tables with replayed tick data, ready for simulation or persistence.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/matching_engine_simulator.md#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\nentrust_dfs = loadTable(\"dfs://TSDB_tradeAndentrust\", \"entrust\")\ntrade_dfs = loadTable(\"dfs://TSDB_tradeAndentrust\", \"trade\")\n\ndef replayBySymbol(market, marketName, symbolList, entrust, trade, i) {\n    if (market == \"sz\") {\n        ds1 = replayDS(sqlObj=<select SecurityID as symbol, marketName as symbolSource, TradeTime, 0 as sourceType, \n            iif(OrderType in [\"50\"], 2, iif(OrderType in [\"49\"], 1, 3)) as orderType, Price as price, OrderQty as qty, int(ApplSeqNum) as buyNo, int(ApplSeqNum) as sellNo,\n            int(string(char(string(side)))) as BSFlag, int(SeqNo) as seqNum from entrust\n            where Market = market and date(TradeTime)==2022.04.14 and SecurityID in symbolList>, dateColumn = \"TradeTime\", timeColumn = \"TradeTime\")\n        ds2 = replayDS(sqlObj=<select SecurityID as symbol, marketName as symbolSource, TradeTime, 1 as sourceType, \n            iif(BidApplSeqNum==0|| OfferApplSeqNum==0,1,0) as orderType, TradePrice as price, int(tradeQty as qty), int(BidApplSeqNum) as buyNo, int(OfferApplSeqNum) as sellNo,\n            0 as BSFlag, int(ApplSeqNum) as seqNum from trade\n            where Market = market and date(TradeTime)==2022.04.14 and SecurityID in symbolList>, dateColumn = \"TradeTime\", timeColumn = \"TradeTime\")\n    }\n    else {\n        ds1 = replayDS(sqlObj=<select SecurityID as symbol, marketName as symbolSource, TradeTime, 0 as sourceType, \n            iif(OrderType == \"A\", 2, 10) as orderType, Price as price, OrderQty as qty, int(ApplSeqNum) as buyNo, int(ApplSeqNum) as sellNo,\n            iif(Side == \"B\", 1, 2) as BSFlag, int(SeqNo) as seqNum from entrust\n            where Market = market and date(TradeTime)==2022.04.14 and SecurityID in symbolList>, dateColumn = \"TradeTime\", timeColumn = \"TradeTime\")\n        ds2 = replayDS(sqlObj=<select SecurityID as symbol, marketName as symbolSource, TradeTime, 1 as sourceType, \n            0 as orderType, TradePrice as price, int(tradeQty as qty), int(BidApplSeqNum) as buyNo, int(OfferApplSeqNum) as sellNo,\n            0 as BSFlag, int(TradeIndex) as seqNum from trade\n            where Market = market and date(TradeTime)==2022.04.14 and SecurityID in symbolList>, dateColumn = \"TradeTime\", timeColumn = \"TradeTime\")\n    }\n\n    inputDict  = dict([\"entrust\", \"trade\"], [ds1, ds2])\n\n    colName = `msgTime`msgType`msgBody`sourceType`seqNum\n    colType =  [TIMESTAMP, SYMBOL, BLOB, INT, INT]\n\n    messageTemp = table(100:0, colName, colType)\n    share(messageTemp, \"MatchEngineTest\" + i)\n    \n    // 当市场为深交所时，相同时间的数据需要按照先逐笔委托再逐笔成交的顺序排序\n    // 逐笔委托单的sourceType为0，逐笔成交单的sourceType为1，因此可以按照sourceType字段排序\n    if (market == \"sz\") {\n        replay(inputDict, \"MatchEngineTest\" + i,`TradeTime,`TradeTime,,,1,`sourceType`seqNum)\n    }\n    // 上交所需要按照先逐笔成交再逐笔委托的顺序排序\n    // 这里上交所的数据中seqNum是严格排序的，因此可以直接按照seqNum排序\n    else {\n        replay(inputDict, \"MatchEngineTest\" + i,`TradeTime,`TradeTime,,,1,`seqNum)\n    }\n}\n\n// 计算行情回放的总耗时\ntimer {\n    job_list = [] // job_list存储了提交的所有作业id列表\n    for (i in 1..thread_num) {\n        job_list.append!(submitJob(\"TestJob\" + i, \"\", replayBySymbol, market, market_name, symbol_list[i-1], entrust_dfs, trade_dfs, i))\n    }\n    // getJobReturn的参数true表示阻塞当前线程直到作业完成后返回\n    for (i in 0 : thread_num) {\n        getJobReturn(job_list[i], true)\n    }\n}\n\n// 回放结果可以以磁盘表的形式存储，方便后续测试\ndb = database(\"<path>/\" + market + \"_messages_\" + thread_num + \"_part\")\nfor (i in 1..thread_num) {\n     saveTable(db, objByName(\"MatchEngineTest\" + i), \"MatchEngineTest\" + i)\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Summary Statistics Using SQL Query as Data Source - DolphinDB Script\nDESCRIPTION: This snippet demonstrates using a SQL query as the data source for summary statistics in DolphinDB. sqlDS creates a virtual data source from an inline SQL select statement. summary is then called on the resulting data source object. This approach is useful when data is not originally in a table but can be generated on-the-fly via SQL. It requires DolphinDB's sqlDS function and a valid SQL query.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/generate_large_scale_statistics_with_summary.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\nds = sqlDS(<select * from table>)\nre = summary(ds, type)\n```\n\n----------------------------------------\n\nTITLE: Loading Readings from CSV DolphinDB\nDESCRIPTION: This snippet loads data from the `FP_READINGS` CSV file into the partitioned database `db` and creates the `readings` table.  It leverages the predefined `schema_readings` for data parsing. The `loadTextEx` function is used for loading data into a partitioned database, and includes timer functions to measure the execution time, as well as data sizes.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs TimescaleDB/test_dolphindb_small.txt#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// ----------------- 从 CSV 导入 readings 表的数据到 readings 数据库并完成数据分区操作\ntimer readings = loadTextEx(db, `readings, `time`device_id, FP_READINGS, , schema_readings)\n// 32 s    1.2 GB\n```\n\n----------------------------------------\n\nTITLE: Forcing Garbage Collection of Redo Log in DolphinDB\nDESCRIPTION: The imtForceGCRedolog function allows skipping long-running transactions that are blocking garbage collection, enabling the system to continue recycling subsequent transactions.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/redoLog_cacheEngine.md#_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nimtForceGCRedolog()\n```\n\n----------------------------------------\n\nTITLE: Creating Database and Table with Value Partitioning - DolphinDB\nDESCRIPTION: This code snippet demonstrates how to create a database and table in DolphinDB using value partitioning. It defines the schema for sensor data collected from industrial equipment, including timestamps, device codes, location IDs, property codes, and property values. The table is partitioned by date to improve query performance.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Iot_intelligent_O&M.md#_snippet_0\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\ncreate database \"dfs://db_test\" partitioned by VALUE([2022.11.01]),engine='TSDB'\ncreate table \"dfs://db_test\".\"collect\"(\n    ts                    DATETIME,//数采时间  \n    deviceCode            SYMBOL,//设备编号    \n    logicalPositionId      SYMBOL,//逻辑位置ID    \n    physicalPositionId     SYMBOL,//物理位置ID\n    propertyCode          SYMBOL,//属性测点编码\n    propertyValue         INT//测点值\n)\npartitioned by ts\nsortColumns=[`deviceCode,`ts]\n```\n\n----------------------------------------\n\nTITLE: Load Text File into Distributed Table\nDESCRIPTION: This snippet loads a text file into a distributed table within a DolphinDB database using `loadTextEx`. It divides the file into chunks and loads them into the distributed table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_data.md#_snippet_6\n\nLANGUAGE: txt\nCODE:\n```\ndataFilePath = \"/home/data/candle_201801.csv\"\nloadTextEx(db, \"cycle\", \"tradingDay\", dataFilePath)\n```\n\n----------------------------------------\n\nTITLE: Bulk Importing CSV Data into ElasticSearch via Python (Python)\nDESCRIPTION: This Python script reads multiple CSV files in batches, transforms each record into JSON, and uploads the data in bulk using ElasticSearch's _bulk API. It defines index mappings for optimal searching and disables index auto-refresh for import speed. Core dependencies: urllib3, json, csv, and time. Inputs are predefined filepaths; output is data indexed in ElasticSearch. Supports large-scale imports, with error handling for HTTP failures. Outputs timing metrics after completion. The mapping must reflect the correct types and format for all fields.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport urllib3\nimport json\nimport csv\nimport time\n\ndef main():\n    create_index_template()\n    delete_index()\n    create_index()\n    http = urllib3.PoolManager()\n    t1 = time.time()\n    for tmp in read_lines():\n        # 分段生成小文件来加载\n        data = '\\n'.join(bulk_import_lines(tmp))\n        data += '\\n'\n        response = http.request('PUT', 'http://localhost:9200/_bulk', body=data.encode('utf-8'), headers={'Content-Type': 'application/json'})\n        print(response.status)\n        print('\\n')\n        print(response.data)\n        print('\\n')\n    t2 = time.time()\n    print(\"\\u5bfc\\u5165\\u6570\\u636e\\u8017\\u65f6(ms):\", (t2-t1)*1000)\n\ndef bulk_import_lines(lines):\n    for line in lines:\n        yield json.dumps({'index': {'_index': 'hundred', '_type': 'type'}})\n        yield json.dumps(line)\n\n# 读每一行转成json\ndef read_lines():\n    for i in range(1, 5):\n        path = '/home/revenant/Documents/TAQ_8/' + str(i) + '.csv'\n        with open(path) as f:\n            f.readline()\n            field_name = ['SYMBOL', 'DATE', 'TIME', 'BID', 'OFR', 'BIDSIZ', 'OFRSIZ', 'MODE', 'EX', 'MMID']\n            symbols = csv.DictReader(f, fieldnames=field_name)\n            cnt = 0\n            temp = []\n            for symbol in symbols:\n                symbol.pop(None, None)\n                try:\n                    cnt = cnt+1\n                    temp.append(symbol)\n                except:\n                    pass\n                if(cnt%100000==0 and cnt!=0):\n                    print(cnt)\n                    yield temp\n                    temp = []\n            if(len(temp) > 0):\n                yield temp\n\ndef create_index():\n    http = urllib3.PoolManager()\n    try:\n        response = http.request('PUT', 'http://localhost:9200/hundred')\n        print(response.status)\n        print(response.data)\n    except urllib3.exceptions:\n        print('Connection failed.')\n\ndef delete_index():\n    http = urllib3.PoolManager()\n    try:\n        response = http.request('DELETE', 'http://localhost:9200/hundred')\n        print(response.status)\n        print(response.data)\n    except urllib3.exceptions:\n        pass\n\ndef create_index_template():\n    http = urllib3.PoolManager()\n    data = json.dumps({\n        'template': 'hundred',\n        'settings': {\n            'number_of_shards': 10,\n            'number_of_replicas': 1,\n            \"index.refresh_interval\": -1\n        },\n        'mappings': {\n            'type': {\n                '_source': {'enabled': True},\n                'properties': { \n                    'SYMBOL': {'type': 'keyword'},\n                    'DATE': {'type': 'date', \"format\": \"yyyyMMdd\"},\n                    'TIME': {'type': 'keyword'},\n                    'BID': {'type': 'double'},\n                    'OFR': {'type': 'double'},\n                    'BIDSIZ': {'type': 'integer'},\n                    'OFRSIZ': {'type': 'integer'},\n                    'MODE': {'type': 'integer'},\n                    'EX': {'type': 'keyword'},\n                    'MMID': {'type': 'keyword'}\n                }\n            }\n        }\n    }).encode('utf-8')\n    r = http.request('PUT', 'http://localhost:9200/_template/hundred', body=data, headers={'Content-Type': 'application/json'})\n    print(r.status)\n    print(r.data)\n\nmain()\n\n```\n\n----------------------------------------\n\nTITLE: Starting Controller Node\nDESCRIPTION: This shell command initiates the controller node of the DolphinDB cluster.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/single_machine_cluster_deploy.md#_snippet_14\n\nLANGUAGE: Shell\nCODE:\n```\nsh startController.sh\n```\n\n----------------------------------------\n\nTITLE: Ordering Results with Nulls First or Last in DolphinDB SQL\nDESCRIPTION: Shows how to control the ordering of null values when sorting query results. The nulls first option prioritizes nulls to appear before non-null values, while nulls last positions nulls at the end. Examples demonstrate sorting employee records by manager_id with different null positioning preferences.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Standard_SQL_in_DolphinDB.md#_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\nselect * from employees\norder by manager_id asc nulls first\n```\n\nLANGUAGE: SQL\nCODE:\n```\nselect * from employees\norder by manager_id asc nulls last\n```\n\n----------------------------------------\n\nTITLE: Defining Feature Engineering Function (DolphinDB Script)\nDESCRIPTION: Defines a function `featureEngineering` that takes raw snapshot data columns, calculates derived features like WAP, spreads, volumes, and log returns, and then aggregates these features over different time windows (0s, 150s, 300s, 450s within a 10-minute bar) using the pre-generated aggregation metadata (`aggMetaCode`).\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/metacode_derived_features/metacode_derived_features_streaming.txt#_snippet_10\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndefg featureEngineering(DateTime, BidPrice, BidOrderQty, OfferPrice, OfferOrderQty, aggMetaCode){\n\twap = (BidPrice * OfferOrderQty + BidOrderQty * OfferPrice) \\ (BidOrderQty + OfferOrderQty)\n\twapBalance = abs(wap[0] - wap[1])\n\tpriceSpread = (OfferPrice[0] - BidPrice[0]) \\ ((OfferPrice[0] + BidPrice[0]) \\ 2)\n\tBidSpread = BidPrice[0] - BidPrice[1]\n\tOfferSpread = OfferPrice[0] - OfferPrice[1]\n\ttotalVolume = OfferOrderQty.rowSum() + BidOrderQty.rowSum()\n\tvolumeImbalance = abs(OfferOrderQty.rowSum() - BidOrderQty.rowSum())\n\tLogReturnWap = logReturn(wap)\n\tLogReturnOffer = logReturn(OfferPrice)\n\tLogReturnBid = logReturn(BidPrice)\n\tsubTable = table(DateTime as `DateTime, BidPrice, BidOrderQty, OfferPrice, OfferOrderQty, wap, wapBalance, priceSpread, BidSpread, OfferSpread, totalVolume, volumeImbalance, LogReturnWap, LogReturnOffer, LogReturnBid)\n\tcolNum = 0..9$STRING\n\tcolName = `DateTime <- (`BidPrice + colNum) <- (`BidOrderQty + colNum) <- (`OfferPrice + colNum) <- (`OfferOrderQty + colNum) <- (`Wap + colNum) <- `WapBalance`PriceSpread`BidSpread`OfferSpread`TotalVolume`VolumeImbalance <- (`LogReturn + colNum) <- (`LogReturnOffer + colNum) <- (`LogReturnBid + colNum)\n\tsubTable.rename!(colName)\n\tsubTable['BarDateTime'] = bar(subTable['DateTime'], 10m)\n\tresult = sql(select = aggMetaCode, from = subTable).eval().matrix()\n\tresult150 = sql(select = aggMetaCode, from = subTable, where = <time(DateTime) >= (time(BarDateTime) + 150*1000) >).eval().matrix()\n\tresult300 = sql(select = aggMetaCode, from = subTable, where = <time(DateTime) >= (time(BarDateTime) + 300*1000) >).eval().matrix()\n\tresult450 = sql(select = aggMetaCode, from = subTable, where = <time(DateTime) >= (time(BarDateTime) + 450*1000) >).eval().matrix()\n\treturn concatMatrix([result, result150, result300, result450])\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Historical Trade Database and Table (DolphinDB Script)\nDESCRIPTION: DolphinDB script to create a distributed database (`dfs://trade`) and a partitioned table (`trade`) for storing historical tick-by-tick trade data. It utilizes a composite partitioning strategy (VALUE partitioning on `TradeTime` daily, HASH partitioning on `SecurityID` into 5 buckets) and applies delta compression to the `TradeTime` column for better storage efficiency. The script first logs in, drops the database if it exists, defines the partitioning schemes, specifies the table schema, and finally creates the partitioned table within the database.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_capital_flow_order_by_order.md#_snippet_1\n\nLANGUAGE: dolphindb\nCODE:\n```\n//login account\nlogin(\"admin\", \"123456\")\n//create database and table\ndbName = \"dfs://trade\"\ntbName = \"trade\"\nif(existsDatabase(dbName)){\n\tdropDatabase(dbName)\n}\ndb1 = database(, VALUE, 2020.01.01..2022.01.01)\ndb2 = database(, HASH, [SYMBOL, 5])\ndb = database(dbName, COMPO, [db1, db2])\nschemaTable = table(\n\tarray(SYMBOL, 0) as SecurityID,\n\tarray(SYMBOL, 0) as Market,\n\tarray(TIMESTAMP, 0) as TradeTime,\n\tarray(DOUBLE, 0) as TradePrice,\n\tarray(INT, 0) as TradeQty,\n\tarray(DOUBLE, 0) as TradeAmount,\n\tarray(INT, 0) as BuyNum,\n\tarray(INT, 0) as SellNum\n)\ndb.createPartitionedTable(table=schemaTable, tableName=tbName, partitionColumns=`TradeTime`SecurityID, compressMethods={TradeTime:\"delta\"})\n```\n\n----------------------------------------\n\nTITLE: Querying Streaming and DFS Tables for Result Verification in DolphinDB Script\nDESCRIPTION: These snippets select and order K-line results from the in-memory streaming table and from the DFS partitioned table filtered by a specific security ID. They enable verification of real-time streaming computations and persistent storage correctness.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/K.md#_snippet_21\n\nLANGUAGE: DolphinDB\nCODE:\n```\nresult = select * from mdlStockFundOHLC order by SecurityID\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nresult = select *\n\tfrom loadTable(\"dfs://stockFundStreamOHLC\", \"stockFundStreamOHLC\")\n\twhere SecurityID=`666666\n```\n\n----------------------------------------\n\nTITLE: Configuring Agent Node Parameters in agent.cfg File\nDESCRIPTION: Shows configuration for agent nodes including operational mode, local site address and alias, primary controller site info for initial communication, list of sites in the cluster, worker thread count and memory limits. Critical parameters localSite, controllerSite, and sites must be consistent with the cluster’s control node configuration. Proper configuration ensures agents manage start and stop commands for data and compute nodes correctly.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ha_cluster_deployment.md#_snippet_5\n\nLANGUAGE: config\nCODE:\n```\nmode=agent\nlocalSite=10.0.0.80:8801:agent1\ncontrollerSite=10.0.0.80:8800:controller1\nsites=10.0.0.80:8801:agent1:agent,10.0.0.80:8800:controller1:controller,10.0.0.81:8800:controller2:controller,10.0.0.82:8800:controller3:controller\nworkerNum=4\nmaxMemSize=4\nlanCluster=0\n```\n\n----------------------------------------\n\nTITLE: Backing up Metadata Linux\nDESCRIPTION: These commands create backup directories and copy the metadata files required for upgrades and version rollback. The first command creates a backup directory named backup, then the commands copy the dfsMeta and CHUNK_METADATA directory contents to the backup directory.  This is crucial for data recovery in case the upgrade fails.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/standalone_server.md#_snippet_13\n\nLANGUAGE: sh\nCODE:\n```\nmkdir backup\ncp -r local8848/dfsMeta/ backup/dfsMeta\ncp -r local8848/storage/CHUNK_METADATA/ backup/CHUNK_METADATA\n```\n\n----------------------------------------\n\nTITLE: Using PromQL Gauge Functions for Metric Prediction and Comparison\nDESCRIPTION: Examples of PromQL functions for gauge metrics that predict linear trends and calculate differences between time periods for monitoring purposes.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/cluster_monitor.md#_snippet_12\n\nLANGUAGE: PromQL\nCODE:\n```\npredict_linear(v range-vector,t,scalar)函数可以预测时间序列 v 在 t 秒后的值，他通过线性回归的方式来预测样本数据的 Gauge 变化趋势\n\ndelta(v range-vector)函数计算范围向量中的每个时间序列元素的第一个值与最后一个值之差，从而展示不同时间点上的样本值得差值。\n\ndelta(cpu_temp_celsius{host=\"hostname\"}[2h])，返回该服务器上的 CPU 温度与两小时之前的差异\n```\n\n----------------------------------------\n\nTITLE: Detecting Anomalies (Rate Change) in Real-Time using DolphinDB Stream Engine\nDESCRIPTION: This script demonstrates real-time anomaly detection based on the rate of change of a metric. It sets up an input stream table 'mainStream' and an output table 'anomalyRes' for anomalies. An AnomalyDetection engine 'detect_rate' is created to identify records where the absolute relative change between consecutive 'propertyValue' readings exceeds a threshold (100% or 1 in this case). Data is simulated using 'replay', and detected anomalies are written to 'anomalyRes'.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Iot_intelligent_O&M.md#_snippet_13\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\n//新建流数据表,接收实时写入数据\nshare streamTable(20000:0,pt.schema().colDefs.name,pt.schema().colDefs.typeString) as mainStream\n\n//新建流数据表，保存异常结果\nshare streamTable(1000:0, `ts`propertyCode`type`metric, [DATETIME, SYMBOL, INT, STRING]) as anomalyRes\n\n// 创建异常检测引擎\ndetect_rate = createAnomalyDetectionEngine(name=\"detect_rate\",metrics=<[abs(propertyValue \\ prev(propertyValue)-1)>1]>,dummyTable=mainStream,\n                                           outputTable=abnormalRes,timeColumn=`ts,keyColumn=`propertyCode)\n// 订阅实时数据\nsubscribeTable(tableName=`mainStream,actionName='detect_rate',offset=-1,handler=append!{detect_rate},msgAsTable=true)\n\n//使用回放，模拟持续写入流数据表的场景\ndevice=\"361RP017\"\npoint=\"361RP017006\"\nrate=5*60                           //回放倍速（每秒播放 5 分钟的数据，每小时数据 12 秒执行完毕）\nbegintime=2022.01.01 00:00:00       //数据开始时间\nendtime  =2022.01.01 02:00:00       //数据结束时间\ndt=select * from pt where deviceCode=device and propertyCode=point and ts between begintime:endtime order by ts\nsubmitJob(\"replay_output\", \"replay_output_stream\", replay, dt ,mainStream, `ts, `ts, rate,false)\n\nselect * from abnormalRes\n```\n\n----------------------------------------\n\nTITLE: Meta-Programming Bulk Query Function bundleQuery in DolphinDB Script\nDESCRIPTION: Defines a flexible meta-programming function bundleQuery that builds and executes a set of queries based on vectorized filter parameter lists and merges their results. This function accepts table, date, columns, and arbitrary filter column/value lists, using DolphinDB meta-SQL helpers to programmatically generate and union queries. Designed for high-frequency query automation and reuse in large codebases.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_SQL_Case_Tutorial.md#_snippet_45\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef bundleQuery(tbl, dt, dtColName, mt, mtColName, filterColValues, filterColNames){\n\tcnt = filterColValues[0].size()\n\tfilterColCnt = filterColValues.size()\n\torderByCol = sqlCol(mtColName)\n\tselCol = sqlCol(\"*\")\n\tfilters = array(ANY, filterColCnt + 2)\n\tfilters[filterColCnt] = expr(sqlCol(dtColName), ==, dt)\n\tfilters[filterColCnt+1] = expr(sqlCol(mtColName), <, mt)\n\t\n\tqueries = array(ANY, cnt)\n\tfor(i in 0:cnt)\t{\n\t\tfor(j in 0:filterColCnt){\n\t\t\tfilters[j] = expr(sqlCol(filterColNames[j]), ==, filterColValues[j][i])\n\t\t}\n\t\tqueries.append!(sql(select=selCol, from=tbl, where=filters, orderBy=orderByCol, ascOrder=false, limit=1))\n\t}\n\treturn loop(eval, queries).unionAll(false)\n}\n```\n\n----------------------------------------\n\nTITLE: Calculating N-Share VWAP (Optimized - Vectorized)\nDESCRIPTION: This code snippet showcases an optimized approach to calculating N-share VWAP using vectorized operations within a custom aggregation function. This avoids loops and leverages DolphinDB's vectorized processing capabilities, improving performance.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_sql_cases.md#_snippet_17\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndefg lastVolPx2(price, vol, bound) {\n\tcumVol = vol.cumsum()\n\tif(cumVol.tail() <= bound)\n\t\treturn wavg(price, vol)\n\telse {\n\t\tstart = (cumVol <= cumVol.tail() - bound).sum()\n\t\treturn wavg(price.subarray(start:), vol.subarray(start:))\n\t}\n}\n\ntimer lastVolPx_t2 = select lastVolPx2(price, vol, 1000) as lastVolPx from t group by sym\n```\n\n----------------------------------------\n\nTITLE: Data Cleaning, Filling, and Interpolation in pandas and DolphinDB\nDESCRIPTION: This snippet shows methods for handling missing data through filling, interpolation, masking, and conditional filtering. It highlights pandas functions such as fillna, interpolate, and DolphinDB equivalents like nullFill, move, and masks for robust data preprocessing.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/function_mapping_py.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\npandas.DataFrame.fillna / pandas.Series.fillna  # Replace nulls with specified values\npandas.DataFrame.interpolate / pandas.Series.interpolate  # Linear or other interpolations\npandas.DataFrame.mask / pandas.Series.mask  # Apply conditional masks\npandas.DataFrame.bfill / pandas.Series.bfill  # Backward fill\npandas.DataFrame.ffill / pandas.Series.ffill  # Forward fill\n```\n\n----------------------------------------\n\nTITLE: Defining Prediction Handler Function (DolphinDB Script)\nDESCRIPTION: Defines a function `predictRV` that acts as a handler for the aggregated features stream table. It takes the aggregated features (`msg`), preprocesses them for the XGBoost model (adds integer SecurityID, drops specific columns), performs prediction using the loaded `model`, and appends the results, SecurityID, and DateTime to the `result10min` stream table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/metacode_derived_features/metacode_derived_features_streaming.txt#_snippet_14\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef predictRV(mutable result10min, model, mutable msg){\n\ttemp_table = select SecurityID, DateTime from msg\n\tmsg.update!(`SecurityID_int, int(msg[`SecurityID])).dropColumns!(`SecurityID`DateTime`LogReturn0_realizedVolatility)\n\tPredicted = xgboost::predict(model , msg)\n\ttemp_table_2 = table(Predicted, temp_table)\n\tresult10min.append!(temp_table_2)\n}\n```\n\n----------------------------------------\n\nTITLE: Converting Date/Time Types in DolphinDB Shell\nDESCRIPTION: This snippet demonstrates the conversion between different DolphinDB time/date types. It takes a date or a time value as input and converts it into another time-based format, illustrating the function's ability to transform the representation of time-based data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/date_time.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\n>month(2016.02.14);\n2016.02M\n>date(2012.06.13 13:30:10)\n2012.06.13\n>second(2012.06.13 13:30:10)\n13:30:10\n>timestamp(2012.06.13 13:30:10)\n2012.06.13T13:30:10.000\n```\n\n----------------------------------------\n\nTITLE: Defining Annualized Volatility Calculation Function in DolphinDB\nDESCRIPTION: Defines the 'getAnnualVolatility' function that calculates the annualized volatility (standard deviation) of returns using daily return data in DolphinDB. It computes the standard deviation of the relative changes in the value series and annualizes it by multiplying by the square root of 252 trading days. Input is a numeric series, and the output is the annualized volatility measure.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_factor_by_ddb/fund_factor_nine.txt#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndefg getAnnualVolatility(value){\n\treturn std(deltas(value)\\prev(value)) * sqrt(252)\n}\n```\n\n----------------------------------------\n\nTITLE: Submitting Parallel Data Loading Jobs in DolphinDB\nDESCRIPTION: This script defines a function `writeData` that loops through a list of files and loads each into a specified database table using `loadTextEx`. It then uses the `cut` function to divide the list of file paths into chunks based on the desired `parallelLevel` (10 in this case). Finally, it iterates through these chunks and submits a background job (`submitJob`) for each chunk, executing the `writeData` function concurrently to achieve parallel import.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_csv.md#_snippet_8\n\nLANGUAGE: dolphindb\nCODE:\n```\ndef writeData(db,file){\n   loop(loadTextEx{db,`tb,`id,},file)\n}\nparallelLevel=10\nfor(x in dataFilePath.cut(100/parallelLevel)){\n    submitJob(\"loadData\"+parallelLevel,\"loadData\",writeData{db,x})\n};\n```\n\n----------------------------------------\n\nTITLE: Generating Simulated Level 2 Data Using SQL Data Source and Map-Reduce in DolphinDB Script\nDESCRIPTION: This script generates multi-day simulated high-frequency level 2 market data by parallelizing data duplication and modification. It defines a function to update the date column of a mutable table and append the modified data to a distributed database table for given dates. It uses sqlDS to create 10 data sources from one day's data partitioned by distributed table partitions, then runs a map-reduce with a parallel execution option to efficiently create new daily partitions with updated dates. Required dependencies include the previously loaded distributed table with original data for 2020-06-01. Inputs are the database name, table name, and a range of target dates (e.g., trade days). Outputs are distributed table partitions containing the simulated data for multiple days. The user may toggle the 'parallel' parameter depending on available memory.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/quant_finance_examples.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef writeData(mutable t,dbName,tableName, days){\n\tpt = loadTable(dbName,tableName)\n\tfor(day in days){\n\t\tupdate t set date = day\n\t\tpt.append!(t)        \n\t}\n}\ndef main(dbName,tableName,days){\n\trds = sqlDS(<select * from loadTable(dbName,tableName) where date=2020.06.01>)\n\tmr(ds=rds, mapFunc=writeData{,dbName,tableName,days}, parallel=true)\n}\nlogin(`admin,`123456)\ndays=2020.06.01..2020.06.30\ndays=days[weekday(days) between 1:5]\nmain(\"dfs://level2\",\"quotes\",days)\n```\n\n----------------------------------------\n\nTITLE: Creating DolphinDB Time-Series Database Function with Composite Partitioning\nDESCRIPTION: Defines a function to create a time-series database with temporal and hash-based composite partitioning, and creates a table schema for financial snapshot data with array vector columns for order book information.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/Real-Time_Stock_Price_Increase_Calculation/01.创建存储快照数据的库表并导入数据.txt#_snippet_1\n\nLANGUAGE: dolphindb\nCODE:\n```\ndef createDfsTb(dbName, tbName){\n\t//create database\n\tif(existsDatabase(dbName)){\n\t\tdropDatabase(dbName)\n\t}\n\tdb1 = database(, VALUE, 2020.01.01..2021.01.01)\n\tdb2 = database(, HASH, [SYMBOL, 30])\n\tdb = database(dbName, COMPO, [db1, db2], , \"TSDB\")\n\t//create table\n\tschemaTable = table(\n\t\tarray(SYMBOL, 0) as SecurityID,\n\t\tarray(TIMESTAMP, 0) as DateTime,\n\t\tarray(DOUBLE, 0) as PreClosePx,\n\t\tarray(DOUBLE, 0) as OpenPx,\n\t\tarray(DOUBLE, 0) as HighPx,\n\t\tarray(DOUBLE, 0) as LowPx,\n\t\tarray(DOUBLE, 0) as LastPx,\n\t\tarray(INT, 0) as TotalVolumeTrade,\n\t\tarray(DOUBLE, 0) as TotalValueTrade,\n\t\tarray(SYMBOL, 0) as InstrumentStatus,\n\t\tarray(DOUBLE[], 0) as BidPrice,\n\t\tarray(INT[], 0) as BidOrderQty,\n\t\tarray(INT[], 0) as BidNumOrders,\n\t\tarray(INT[], 0) as BidOrders,\n\t\tarray(DOUBLE[], 0) as OfferPrice,\n\t\tarray(INT[], 0) as OfferOrderQty,\n\t\tarray(INT[], 0) as OfferNumOrders,\n\t\tarray(INT[], 0) as OfferOrders,\n\t\tarray(INT, 0) as NumTrades,\n\t\tarray(DOUBLE, 0) as IOPV,\n\t\tarray(INT, 0) as TotalBidQty,\n\t\tarray(INT, 0) as TotalOfferQty,\n\t\tarray(DOUBLE, 0) as WeightedAvgBidPx,\n\t\tarray(DOUBLE, 0) as WeightedAvgOfferPx,\n\t\tarray(INT, 0) as TotalBidNumber,\n\t\tarray(INT, 0) as TotalOfferNumber,\n\t\tarray(INT, 0) as BidTradeMaxDuration,\n\t\tarray(INT, 0) as OfferTradeMaxDuration,\n\t\tarray(INT, 0) as NumBidOrders,\n\t\tarray(INT, 0) as NumOfferOrders,\n\t\tarray(INT, 0) as WithdrawBuyNumber,\n\t\tarray(INT, 0) as WithdrawBuyAmount,\n\t\tarray(DOUBLE, 0) as WithdrawBuyMoney,\n\t\tarray(INT, 0) as WithdrawSellNumber,\n\t\tarray(INT, 0) as WithdrawSellAmount,\n\t\tarray(DOUBLE, 0) as WithdrawSellMoney,\n\t\tarray(INT, 0) as ETFBuyNumber,\n\t\tarray(INT, 0) as ETFBuyAmount,\n\t\tarray(DOUBLE, 0) as ETFBuyMoney,\n\t\tarray(INT, 0) as ETFSellNumber,\n\t\tarray(INT, 0) as ETFSellAmount,\n\t\tarray(DOUBLE, 0) as ETFSellMoney\n\t)\n\tdb.createPartitionedTable(table=schemaTable, tableName=tbName, partitionColumns=`DateTime`SecurityID, compressMethods={DateTime:\"delta\"}, sortColumns=`SecurityID`DateTime, keepDuplicates=ALL)\n}\n```\n\n----------------------------------------\n\nTITLE: Benchmarking DolphinDB Diagonal Matrix Creation (JIT vs Standard)\nDESCRIPTION: This snippet benchmarks the performance of the JIT-compiled `diagonalMatrix_jit` function against the standard `diagonalMatrix` function using the `timer` function. It initializes matrix data and measures the time taken for 1000 executions of each function, demonstrating the significant performance improvement achieved with JIT compilation for this specific operation.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/matrix.md#_snippet_37\n\nLANGUAGE: DolphinDB\nCODE:\n```\n>m = matrix(DOUBLE,32,32)\n>data=1..32\n>timer(1000) diagonalMatrix(data,m)\nTime elapsed: 420.021 ms\n>timer(1000) diagonalMatrix_jit(data,m)\nTime elapsed:  41.026 ms\n```\n\n----------------------------------------\n\nTITLE: Define Capital Flow Processing Function in DolphinDB\nDESCRIPTION: This function, `processCapitalFlowFunc`, creates reactive state engines to calculate capital flow metrics in parallel. It defines the column names, column types, and the metrics to be calculated, including cumulative sums and counts based on sell and buy order flags. It takes the parallel degree as input.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_capital_flow_daily.md#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef processCapitalFlowFunc(parallel){\n\tcolName = `SecurityID`SellNum`TradeTime`TradeAmount`TotalSellAmount`SellOrderFlag`PrevTotalSellAmount`PrevSellOrderFlag`BuyNum`TotalBuyAmount`BuyOrderFlag`PrevTotalBuyAmount`PrevBuyOrderFlag\n\tcolType =  [SYMBOL, INT, TIMESTAMP, DOUBLE, DOUBLE, INT, DOUBLE, INT,  INT, DOUBLE, INT, DOUBLE, INT]\n\tprocessSellOrder = table(1:0, colName, colType)\n\tmetrics1 = <dynamicGroupCumsum(TotalSellAmount, PrevTotalSellAmount, SellOrderFlag, PrevSellOrderFlag, 3)> \n\tmetrics2 = <dynamicGroupCumcount(SellOrderFlag, PrevSellOrderFlag, 3)> \n\tmetrics3 = <dynamicGroupCumsum(TotalBuyAmount, PrevTotalBuyAmount, BuyOrderFlag, PrevBuyOrderFlag, 3)> \n\tmetrics4 = <dynamicGroupCumcount(BuyOrderFlag, PrevBuyOrderFlag, 3)>\n\tfor(i in 1..parallel){\n\t\tcreateReactiveStateEngine(name=\"processCapitalFlow\"+string(i), metrics=[<TradeTime>, <cumsum(TradeAmount)>, metrics1, metrics2, metrics3, metrics4], dummyTable=processSellOrder, outputTable=capitalFlowStream, keyColumn=`SecurityID, keepOrder=true)\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring DolphinDB High Availability for Stream Tables, Engines, and Subscriptions\nDESCRIPTION: Sets up a highly available streaming infrastructure by creating stream tables and a Reactive State Engine within a specified Raft group. Includes configuring a high-availability subscription with parameters like `reconnect`, `persistOffset`, and `handlerNeedMsgId` to ensure processing continuity upon failover.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_tutorial.md#_snippet_22\n\nLANGUAGE: DolphinDB\nCODE:\n```\nhaStreamTable(raftGroup=2, table=table(1:0, `sym`price, [STRING,DOUBLE]), tableName=\"haTickStream\", cacheLimit=10000)\nhaStreamTable(raftGroup=2, table=table(1:0, `sym`factor1, [STRING,DOUBLE]), tableName=\"result\", cacheLimit=10000)\t\t\n\nret = createReactiveStateEngine(name=\"haReact\", metrics=<cumsum(price)>, dummyTable=objByName(\"haTickStream\"), outputTable=objByName(\"result\"), keyColumn=`sym, snapshotDir= \"/home/data/snapshot\", snapshotIntervalInMsgCount=20000, raftGroup=2)\nsubscribeTable(tableName=\"haTickStream\", actionName=\"haFactors\", offset=-1, handler=getStreamEngine(\"haReact\"), msgAsTable=true, reconnect=true, persistOffset=true, handlerNeedMsgId=true, raftGroup=2)\n```\n\n----------------------------------------\n\nTITLE: Optimizing Query for Partition Pruning in DolphinDB\nDESCRIPTION: This DolphinDB script demonstrates an optimized query that enables partition pruning. By using the `between` operator (`month between 2016.11M : 2016.12M`) instead of separate comparison operators, DolphinDB can effectively prune partitions based on the 'month' partitioning key.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_Explain.md#_snippet_9\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nselect [HINT_EXPLAIN] * from pt where month between 2016.11M : 2016.12M\n```\n\n----------------------------------------\n\nTITLE: Configuring DolphinDB Plugin Build with CMake\nDESCRIPTION: This CMake script configures the build environment for a DolphinDB shared library plugin named 'PluginColumnAvg'. It sets the minimum required CMake version, specifies C++11 standard, enables position-independent code (-fPIC), and sets strict warning flags (-Wall, -Werror). It also includes platform-specific definitions (WINDOWS or LINUX), defines include and link directories for DolphinDB headers and libraries, adds the main source file './src/ColumnAvg.cpp', and links the resulting shared library with the 'DolphinDB' library.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin/ColumnAvg/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.0)\n\nproject(PluginColumnAvg)\n\nadd_definitions(-D_GLIBCXX_USE_CXX11_ABI=0)\n\nLINK_DIRECTORIES(\"../libs\")\nINCLUDE_DIRECTORIES(\"../include\")\nif(WIN32)\n    add_definitions(-DWINDOWS)\nelseif(UNIX)\n    add_definitions(-DLINUX)\nendif()\n\nadd_compile_options( \"-std=c++11\" \"-fPIC\" \"-Wall\" \"-Werror\")\n\nadd_library(PluginColumnAvg SHARED\n    \"./src/ColumnAvg.cpp\"\n)\ntarget_link_libraries(PluginColumnAvg DolphinDB)\n```\n\n----------------------------------------\n\nTITLE: Creating or Loading Distributed Table and Preparing Data Import in DolphinDB Plugin C++\nDESCRIPTION: This snippet demonstrates checking whether a partitioned table exists in a DolphinDB database and accordingly loading it or creating a new one with a predefined schema. It leverages DolphinDB API calls to verify existence, create an empty table schema, and construct or retrieve the target table for data appending. This method provides memory-efficient data integration support for large files via incremental inserts.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_development_tutorial.md#_snippet_20\n\nLANGUAGE: cpp\nCODE:\n```\nstring dbPath = ((SystemHandleSP) dbHandle)->getDatabaseDir();\nlong long fileLength = Util::getFileLength(path->getString());\nvector<ConstantSP> existsTableArgs = {new String(dbPath), tableName};\nbool existsTable = heap->currentSession()->getFunctionDef(\"existsTable\")->call(heap, existsTableArgs)->getBool();    // 相当于 existsTable(dbPath, tableName)\nConstantSP result;\nif (existsTable) {    // 若表存在，加载表\n\tvector<ConstantSP> loadTableArgs = {dbHandle, tableName};\n\tresult = heap->currentSession()->getFunctionDef(\"loadTable\")->call(heap, loadTableArgs);    // 相当于 loadTable(dbHandle, tableName)\n}\nelse {    // 若表不存在，创建表\n\tTableSP schema = extractMyDataSchema(new Void(), new Void());\n\tConstantSP dummyTable = DBFileIO::createEmptyTableFromSchema(schema);\n\tvector<ConstantSP> createTableArgs = {dbHandle, dummyTable, tableName, partitionColumns};\n\tresult = heap->currentSession()->getFunctionDef(\"createPartitionedTable\")->call(heap, createTableArgs);    // 相当于 createPartitionedTable(db, dummyTable, tableName, partitionColumns)\n}\n```\n\n----------------------------------------\n\nTITLE: Defining a DolphinDB Data Normalization Node Using JavaScript for Node-RED\nDESCRIPTION: This JavaScript snippet defines a custom Node-RED node named \"数据归一化\" (Data Normalization) that performs data normalization on a specified table and column within DolphinDB. It depends on the Node-RED runtime (RED) and a DolphinDB client configured elsewhere in the flow. The node retrieves its configuration, constructs DolphinDB scripts for min-max and ZScore normalization functions, dynamically combines them with parameters (table name, column name, normalization type), and registers an asynchronous listener for input events. Upon receiving a message, it executes the DolphinDB script via the client and handles the promise, forwarding messages on success or logging errors on failure. This implementation enables low-code integration of DolphinDB data transformations within Node-RED workflows.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/node_red_tutorial_iot.md#_snippet_6\n\nLANGUAGE: JavaScript\nCODE:\n```\n//构造函数\nfunction DolphinDataNormalizationNode(n) {\n        RED.nodes.createNode(this, n);\n\n        // 获取 DolphinDB 配置结点\n        this.dolphindb = n.dolphindb;\n        this.dolphindbConfig = RED.nodes.getNode(this.dolphindb);\n        if (!this.dolphindbConfig) { //检查是否成功获取了 DolphinDB 配置节点\n            this.error(RED._(\"dolphindb.errors.missingconfig\"));\n            return;\n        }\n        var client = this.dolphindbConfig.client;\n        \n        //将当前节点的上下文保存在变量 node 中\n        this.name=n.name;\n        this.tableName=n.tableName;\n        this.colName=n.colName;\n        this.normalizeType=n.normalizeType;\n\n        var node = this;\n        \n        //DolphinDB 脚本\n        var func=`def minMaxNormalization(tableName,colName){\n                      pt=objByName(tableName);\n                      minNum=min(pt[colName]);\n                      maxNum=max(pt[colName]);\n                      pt[colName]=(pt[colName]-minNum)/(maxNum-minNum);\n                  }\n                  def ZSoreNormalization(tableName,colName){\n                      pt=objByName(tableName);\n                      meanNum=mean(pt[colName]);\n                      stdNum=std(pt[colName]);\n                      pt[colName]=(pt[colName]-meanNum)/stdNum;\n                  }\n                  def dataNormalization(tableName,colName,type){\n                      if(type==1){\n                          minMaxNormalization(tableName,colName);\n                      }else if(type==2){\n                          ZSoreNormalization(tableName,colName);\n                      }\n                  }`;\n        var funcall =`dataNormalization(\"${this.tableName}\",`\\`${this.colName},${this.normalizeType})`;\n        var script =func+funcall;\n        \n        console.log(script) //打印生成的 DolphinDB 脚本，用于调试和查看。\n        \n        //注册监听器 input，一旦收到消息，则执行 DolphinDB 脚本\n        node.on('input', async function (msg, send, done) {\n            client.eval(script).then(res => {\n                send(msg)\n                done()\n            }).catch(err => {\n                console.log(err)\n                done(err)\n            })\n        });\n    }\n    \n//注册节点\nRED.nodes.registerType(\"数据归一化\", DolphinDataNormalizationNode);\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef minMaxNormalization(tableName,colName){\n    pt=objByName(tableName);\n    minNum=min(pt[colName]);\n    maxNum=max(pt[colName]);\n    pt[colName]=(pt[colName]-minNum)/(maxNum-minNum);\n}\ndef ZSoreNormalization(tableName,colName){\n    pt=objByName(tableName);\n    meanNum=mean(pt[colName]);\n    stdNum=std(pt[colName]);\n    pt[colName]=(pt[colName]-meanNum)/stdNum;\n}\ndef dataNormalization(tableName,colName,type){\n    if(type==1){\n        minMaxNormalization(tableName,colName);\n    }else if(type==2){\n        ZSoreNormalization(tableName,colName);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Recovering Metadata Linux\nDESCRIPTION: These commands restore the backup metadata files during a rollback. The first command copies the dfsMeta from the backup folder and replaces it with local8848/dfsMeta. The second command does the same for CHUNK_METADATA from the backup folder to replace local8848/storage/CHUNK_METADATA. This procedure is required for the downgrade of DolphinDB.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/standalone_server.md#_snippet_21\n\nLANGUAGE: sh\nCODE:\n```\ncp -r backup/dfsMeta/ local8848/dfsMeta\ncp -r backup/CHUNK_METADATA/ local8848/storage/CHUNK_METADATA\n```\n\n----------------------------------------\n\nTITLE: Measuring Execution Time of Parallel Job 1 in DolphinDB\nDESCRIPTION: This code retrieves the start and end times of recent jobs with specific descriptions and calculates the execution time. It separately calculates execution time for the single-user and multi-user job submissions.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_factor.txt#_snippet_16\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//获取单个用户的运行时间\nselect max(endTime) - min(startTime) from getRecentJobs() where jobDesc = \"parallJob_single_nine\"\n//获取多个用户的运行时间\nselect max(endTime) - min(startTime) from getRecentJobs() where jobDesc = \"parallJob_multi_nine\"\n```\n\n----------------------------------------\n\nTITLE: Cleaning up DolphinDB Streaming Environment with DolphinDB Script\nDESCRIPTION: This DolphinDB script snippet illustrates how to clean up all streaming data components used in previous tutorials, including unsubscribing actions and dropping stream tables and engines. Dependencies include access rights to the relevant DolphinDB database, and the existence of relevant stream tables and engines. Key steps include unsubscribing from all related tables using unsubscribeTable, dropping each stream table with dropStreamTable, and removing stream computation engines with dropStreamEngine. The script handles errors gracefully with try-catch blocks, and assumes that the relevant names are defined as variables.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/K.md#_snippet_24\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\n//Declare parameters\nmdlSnapshotTBName = \"mdlSnapshot\"\nmdlSnapshotProcessTBName = \"mdlSnapshotProcess\"\nmdlSnapshotProcessEngineName = \"mdlSnapshotProcessEngine\"\nmdlStockFundOHLCTempEngineName = \"mdlStockFundOHLCTempEngine\"\nmdlStockFundOHLCTBName = \"mdlStockFundOHLC\"\nmdlStockFundOHLCEngineName = \"mdlStockFundOHLCEngine\"\n//Cancel related subscriptions\ntry{unsubscribeTable(tableName=mdlSnapshotTBName, actionName=mdlSnapshotProcessEngineName)} catch(ex){print(ex)}\ntry{unsubscribeTable(tableName=mdlSnapshotProcessTBName, actionName=mdlStockFundOHLCTempEngineName)} catch(ex){print(ex)}\ntry{unsubscribeTable(tableName=mdlStockFundOHLCTBName, actionName=mdlStockFundOHLCTBName)} catch(ex){print(ex)}\n//Cancel the definition of related stream tables\ntry{dropStreamTable(mdlSnapshotTBName)} catch(ex){print(ex)}\ntry{dropStreamTable(mdlSnapshotProcessTBName)} catch(ex){print(ex)}\ntry{dropStreamTable(mdlStockFundOHLCTBName)} catch(ex){print(ex)}\n//Cancel the definition of related stream calculation engines\ntry{dropStreamEngine(mdlSnapshotProcessEngineName)} catch(ex){print(ex)}\ntry{dropStreamEngine(mdlStockFundOHLCEngineName)} catch(ex){print(ex)}\ntry{dropStreamEngine(mdlStockFundOHLCTempEngineName)} catch(ex){print(ex)}\n```\n\n----------------------------------------\n\nTITLE: Querying Controller Chunk Versions Using DolphinDB Scripting Language\nDESCRIPTION: This snippet queries the current version status of all chunks managed by the Controller node to identify those whose state is not 'COMPLETE'. It helps in detecting chunks in recovery or construction state that may indicate inconsistency and require repair. The snippet relies on the DolphinDB built-in function getClusterChunksStatus and querying rpc for the Controller's alias to access remote metadata.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/repair_chunk_status.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect * from rpc(getControllerAlias(), getClusterChunksStatus) where  state != 'COMPLETE'\n```\n\n----------------------------------------\n\nTITLE: 绘制绩效指标直方图和散点图\nDESCRIPTION: 通过多种图表表现基金的年化收益、波动和风险调整收益率，便于直观分析基金表现。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/public_fund_basic_analysis.md#_snippet_22\n\nLANGUAGE: DolphinDB\nCODE:\n```\n(exec exp from perf where exp > -10, exp < 40).plotHist(400)\n// Yearly return histogram\n(exec vol from perf where vol < 40).plotHist(400)\n// Volatility histogram\n(exec sharpe from perf where sharpe > 0).plotHist(200)\n// Sharpe ratio histogram\nmask = select * from perf where sharpe>0, vol<40, exp<40 \nplot(mask[\"exp\"], mask[\"vol\"], ,SCATTER)\n```\n\n----------------------------------------\n\nTITLE: Loading and filtering snapshot data from DolphinDB table in DolphinDB script\nDESCRIPTION: This snippet loads a table from a DolphinDB DFS database, filters the data for a specific date (2021-12-01), and orders the results by the 'DateTime' column. It demonstrates data selection and sorting techniques in DolphinDB scripting, requiring prior setup of the database and table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/Real-Time_Stock_Price_Increase_Calculation/04.历史数据回放.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndbName, tbName = \"dfs://SH_TSDB_snapshot_ArrayVector\", \"snapshot\"\nt = select * from loadTable(dbName, tbName) where date(DateTime)=2021.12.01 order by DateTime\n```\n\n----------------------------------------\n\nTITLE: Creating Database and Table for Public Fund Data in DolphinDB\nDESCRIPTION: Sets up a TSDB database with a date-range partitioning scheme and creates a table structure for storing public fund data. The table includes fields for fund identification, characteristics, and fee information.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/public_fund_basic_analysis/basic_analysis_of_public_fund_open_market_data.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\ncsvDataPath = \"/ssd/ssd2/data/fundData/publicFundData.csv\"\ndbName = \"dfs://publicFundDB\"\ntbName = \"publicFundData\"\n// create database and one-partition table\nif(existsDatabase(dbName)){\n\tdropDatabase(dbName)\n}\ntimeRange = 1990.01.01 join sort(distinct(yearBegin(2016.01.01..2050.01.01)))\ndb = database(dbName, RANGE, timeRange, engine = 'TSDB')\nnames = `SecurityID`FullName`Name`Management`Type`Custodian`IssueShare`InceptDate`MFee`CFee`SFee`Closed`Status\ntypes = `SYMBOL`STRING`STRING`SYMBOL`SYMBOL`SYMBOL`DOUBLE`DATE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`INT\nschemaTB = table(1:0, names, types)\ndb.createTable(table=schemaTB, tableName=tbName, sortColumns=`InceptDate)\n// load CSV data\ntmp = ploadText(filename=csvDataPath, schema=table(names, types))\nloadTable(dbName, tbName).append!(tmp)\n```\n\n----------------------------------------\n\nTITLE: Continuous Interval Max Value (Optimized - Segment)\nDESCRIPTION: This code snippet showcases an optimized approach to finding the maximum value within continuous intervals using the segment function. This simplifies the code and improves performance compared to the custom function approach.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_sql_cases.md#_snippet_21\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntimer(1000) res2 = select * from t \n\t\t\t\t   context by segment(value >= targetVal) \n\t\t\t\t   having value >= targetVal and value = max(value) limit 1\n```\n\n----------------------------------------\n\nTITLE: Updating Dictionary with Table Data - DolphinDB\nDESCRIPTION: Updates a dictionary `historyDict` with data from the `orders` table. It uses the `dictUpdate!` function to insert data into the dictionary, using `SecID` as keys and creating sub-tables as values.  Lambda functions handle insertion and initialization, ensuring proper table updates.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/func_progr_cases.md#_snippet_19\n\nLANGUAGE: DolphinDB\nCODE:\n```\nhistoryDict.dictUpdate!(function=def(x,y){tableInsert(x,y);return x}, keys=orders.SecID, parameters=orders,\n            initFunc=def(x){t = table(100:0, x.keys(), each(type, x.values())); tableInsert(t, x); return t})\n```\n\n----------------------------------------\n\nTITLE: Self Window Join for Intra-Table Time-Based Aggregation in DolphinDB SQL\nDESCRIPTION: Performs a window join of a table with itself to compute aggregate statistics over a dynamic time range relative to each record. The join window spans from time-6s to time+1s and calculates average bid price within that interval for each row in the t2 table, based on symbol and time. This internal window join implements a flexible sliding window aggregation using window join semantics, applicable to single-table time series analytics without external joins.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/window_cal.md#_snippet_14\n\nLANGUAGE: DolphinDB\nCODE:\n```\nt2 = table(take(1,10) join take(2,10) as sym, take(09:56:00+1..10,20) as time, (10+(1..10)\\10-0.05) join (20+(1..10)\\10-0.05) as bid, (10+(1..10)\\10+0.05) join (20+(1..10)\\10+0.05) as offer, take(100 300 800 200 600, 20) as volume);\n\nwj(t2, t2, -6s:1s, <avg(bid)>, `sym`time);\n```\n\n----------------------------------------\n\nTITLE: Synchronizing Data from ClickHouse to DolphinDB\nDESCRIPTION: This snippet defines a function to synchronize data from a ClickHouse table to a DolphinDB table using ODBC. It constructs an SQL query based on the provided date, executes the query using odbc::query, and loads the data into the specified DolphinDB table. It also shows the usage with sample parameters.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ClickHouse_to_DolphinDB.md#_snippet_5\n\nLANGUAGE: dolphindb\nCODE:\n```\ndef syncData(conn, dbName, tbName, dt){\n    sql = \"select SecurityID, TradeTime, TradePrice, TradeQty, TradeAmount, BuyNo, SellNo, ChannelNo, TradeIndex, TradeBSFlag, BizIndex from migrate.ticksh\"\n    if(!isNull(dt)) {\n        sql = sql + \" WHERE toDate(TradeTime) = '\"+temporalFormat(dt,'yyyy-MM-dd')+\"'\"\n    }\n    odbc::query(conn,sql, loadTable(dbName,tbName), 100000)\n}\ndbName=\"dfs://TSDB_tick\"\ntbName=\"tick\"\nsyncData(conn, dbName, tbName, NULL)\n```\n\n----------------------------------------\n\nTITLE: Creating Level-2 Snapshot Partitioned Database and Table for Shenzhen Stock Exchange in DolphinDB\nDESCRIPTION: Defines a distributed TSDB database 'dfs://split_SZ_TB' partitioned by daily date range and HASH partition on SECURITYID with 25 buckets, for Level-2 snapshot stock data of Shenzhen exchange from 2020-01-01 to 2020-12-31. It creates a table 'split_SZ_snapshotTB' with detailed columns describing trading info, including array vectors for order book prices and quantities, and uses delta compression for trade date and time columns. The sorting columns are SECURITYID and TradeTime, ensuring efficient queries sorted by stock code and time. The keepDuplicates=ALL parameter preserves multiple records having identical sort key values in each partition. This setup supports optimized distributed storage for large tick-level market data with array vectors and compression applied.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practices_for_partitioned_storage.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\ncreate database \"dfs://split_SZ_TB\"\npartitioned by VALUE(2020.01.01..2021.01.01), HASH([SYMBOL, 25])\nengine='TSDB'\n\ncreate table \"dfs://split_SZ_TB\".\"split_SZ_snapshotTB\"(\n    TradeDate DATE[comment=\"交易日期\", compress=\"delta\"]   \n    TradeTime TIME[comment=\"交易时间\", compress=\"delta\"]\n    MDStreamID SYMBOL\n    SecurityID SYMBOL\n    SecurityIDSource SYMBOL\n    TradingPhaseCode SYMBOL\n    PreCloPrice DOUBLE\n    NumTrades LONG\n    TotalVolumeTrade LONG\n    TotalValueTrade DOUBLE\n    LastPrice DOUBLE\n    OpenPrice DOUBLE\n    HighPrice DOUBLE\n    LowPrice DOUBLE\n    DifPrice1 DOUBLE\n    DifPrice2 DOUBLE\n    PE1 DOUBLE\n    PE2 DOUBLE\n    PreCloseIOPV DOUBLE\n    IOPV DOUBLE\n    TotalBidQty LONG\n    WeightedAvgBidPx DOUBLE\n    TotalOfferQty LONG\n    WeightedAvgOfferPx DOUBLE\n    UpLimitPx DOUBLE\n    DownLimitPx DOUBLE\n    OpenInt INT\n    OptPremiumRatio DOUBLE\n    OfferPrice DOUBLE[]\n    BidPrice DOUBLE[]\n    OfferOrderQty LONG[]\n    BidOrderQty LONG[]\n    BidNumOrders INT[]\n    OfferNumOrders INT[]\n    LocalTime TIME\n    SeqNo INT\n    OfferOrders LONG[]\n    BidOrders LONG[]\n)\npartitioned by TradeDate, SecurityID,\nsortColumns=[`SecurityID,`TradeTime],\nkeepDuplicates=ALL\n```\n\n----------------------------------------\n\nTITLE: Subscribing and Storing Real-Time Stream Data with subscribeTable (DolphinDB Script)\nDESCRIPTION: Demonstrates the built-in subscribeTable function used to subscribe to a streaming table for real-time ingestion of data into DolphinDB. The function supports specifying a handler, offset, filtering, batching, and more for advanced stream processing and fault tolerance. Dependencies include a running DolphinDB streaming engine and proper table/handler setup. Parameters include the target server, stream table name, action name, handler, and additional options. The output is automatic handling or storage of streaming table updates.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Seismic_waveform_data_storage.md#_snippet_8\n\nLANGUAGE: DolphinDB\nCODE:\n```\nsubscribeTable([server],tableName,[actionName],[offset=-1],handler,[msgAsTable=false],[batchSize=0],\n[throttle=1],[hash=-1],[reconnect=false],[filter],[persistOffset=false],[timeTrigger=false],\n[handlerNeedMsgId=false],[raftGroup],[userId=””],[password=””])\n```\n\n----------------------------------------\n\nTITLE: Optimized Date Filtering with date Function in DolphinDB SQL\nDESCRIPTION: An optimized query using the date function instead of temporalFormat, which enables partition pruning and significantly improves performance.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_SQL_Case_Tutorial.md#_snippet_8\n\nLANGUAGE: SQL\nCODE:\n```\ntimer t2 = select count(*) from snapshot \n\t\t   where date(DateTime) between 2020.06.01 : 2020.06.02 group by SecurityID\n```\n\n----------------------------------------\n\nTITLE: Using DolphinDB context by Clause for Panel Data Processing with Moving Windows and Aggregations\nDESCRIPTION: This DolphinDB code demonstrates panel data processing using the 'context by' clause, which returns vectors aligned with input data rather than aggregated scalars. Examples include computing cumulative maximum volume per symbol per day, calculating moving average prices over the past 20 ticks with context by symbol and date, computing the daily maximum volume assigned to all rows per group, and calculating a derived high-frequency factor across bid and ask volumes with sorting by symbol, date and time. A final example shows nested moving averages and moving correlation calculations using a single complex query. The context by clause supports aggregation and window functions over grouped panel data, facilitating efficient quantitative feature extraction on high-frequency market data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/quant_finance_examples.md#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\nt = select symbol, date, time, cummax(curVol) as volume_cummax from quotes where date=2020.06.01 context by symbol, date\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nt = select symbol, date, time, mavg(last, 20) as price_mavg1min from quotes where date=2020.06.01 context by symbol, date\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nt = select symbol, date, time, max(curVol) as volume_dailyMax from quotes where date=2020.06.01 context by symbol, date\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nt=select symbol, date, time, (2*bidVolume1+bidVolume2-2*askVolume1-askVolume2)\\(2*bidVolume1+bidVolume2+2*askVolume1+askVolume2) as factor1, (2*bidVolume1+bidVolume2-2*askVolume1-askVolume2)\\mavg((2*bidVolume1+bidVolume2+2*askVolume1+askVolume2),100) as factor2 from quotes where date=2020.06.01, symbol>=`600000, time between 09:30:00.000 : 15:00:00.000 context by symbol order by symbol, date, time\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nt=select symbol, date, time, mcorr(mavg(bidVolume1,100), mavg(askVolume1,100), 60) from quotes where date=2020.06.01, symbol>=`600000, time between 09:30:00.000 : 15:00:00.000 context by symbol order by symbol, date, time\n```\n\n----------------------------------------\n\nTITLE: Sending HTTP POST Request with Message to WeChat Work\nDESCRIPTION: Code showing how to send an HTTP POST request with the message payload to WeChat Work API. The request uses the constructed URL and payload with a timeout of 1000ms.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/send_messages_external_systems.md#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\nret=httpClient::httpPost(url,param,1000);\nprint ret['text'];\n```\n\n----------------------------------------\n\nTITLE: Group Filtering with context by and Aggregation Using aggrTopN in DolphinDB SQL\nDESCRIPTION: Illustrates filtering stock snapshot data to select the top 25% volume records per stock and then compute the standard deviation of LastPx. The non-optimized approach uses 'context by' to segment data and ‘having’ with percentile for filtering, followed by group by aggregation and ordering. The optimized approach leverages the aggrTopN function to combine top-N filtering and aggregation in one step, reducing intermediate data processing and halving query time from 242 ms to 124 ms. Inputs include snapshot table and filtering date; outputs are per-stock standard deviations post filtering. Key functions: loadTable, context by, having, percentile, group by, aggrTopN, and timer.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_SQL_Case_Tutorial.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\nsnapshot = loadTable(\"dfs://Level1\", \"Snapshot\")\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntimer res1 = select * from snapshot \n\t\t\t where date(DateTime) = 2020.06.01 \n\t\t\t context by SecurityID having Volume >= percentile(Volume, 75, \"linear\")\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntimer select std(LastPx) as std from (\n      select SecurityID, LastPx from snapshot \n      where date(DateTime) = 2020.06.01 \n      context by SecurityID \n      having Volume >= percentile(Volume, 75, \"linear\")) \n      group by SecurityID \n      order by SecurityID\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntimer select aggrTopN(std, LastPx, Volume, 0.25, false) as std from snapshot \n\t  where date(DateTime) = 2020.06.01 \n\t  group by SecurityID \n\t  order by SecurityID\n```\n\n----------------------------------------\n\nTITLE: Adding Oracle ODBC Environment Variables in DolphinDB Startup Scripts - Shell\nDESCRIPTION: Appends Oracle Instant Client environment variables (LD_LIBRARY_PATH, TNS_ADMIN, and NLS_LANG) to DolphinDB startup scripts for either single-node or cluster setups. This ensures DolphinDB inherits the correct ODBC client library paths at startup.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Oracle_to_DolphinDB.md#_snippet_9\n\nLANGUAGE: Shell\nCODE:\n```\nexport LD_LIBRARY_PATH=/usr/local/oracle/instantclient_21_7:$LD_LIBRARY_PATH\nexport TNS_ADMIN=/etc/oracle\nexport NLS_LANG='AMERICAN_AMERICA.AL32UTF8'\n\n```\n\n----------------------------------------\n\nTITLE: Filling Null Values during Data Import with DolphinDB - DolphinDB Script\nDESCRIPTION: This snippet demonstrates how to use the \"transform\" parameter with the built-in \"nullFill!\" function while importing a partitioned table from a text file in DolphinDB. It creates a partitioned database and table, then loads data from a text file, filling missing values with 0s as specified by the \"nullFill!{,0}\" partial application. Requires DolphinDB with relevant functions enabled, and the \"dbPath\" and \"dataFilePath\" variables pointing to valid storage locations. Inputs include the CSV file and output consists of a table with nulls appropriately filled.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_csv.md#_snippet_18\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndb=database(dbPath,VALUE,2018.01.02..2018.01.30)\ntb=db.createPartitionedTable(tb,`tb1,`date)\ntmpTB=loadTextEx(dbHandle=db,tableName=`tb1,partitionColumns=`date,filename=dataFilePath,transform=nullFill!{,0});\n```\n\n----------------------------------------\n\nTITLE: Creating and Subscribing Multiple Reactive State Engines for Trade Data in DolphinDB Script\nDESCRIPTION: Loops over a parallelism factor to dynamically create multiple ReactiveStateEngines named 'tradeProcess1', 'tradeProcess2', etc. Each engine monitors trade metrics such as trade time, max of buy/sell number, quantity, amount, and buy/sell flags derived via conditional expressions. The engines subscribe to the 'tradeOriginalStream' table with partitioned filtering and reconnect support, enabling scalable parallel trade data processing.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/streaming_capital_flow_order_by_order/04.注册流计算引擎和订阅流数据表.txt#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\nparallel = 3\nfor(i in 1..parallel){\n\t//create ReactiveStateEngine: tradeProcess\n\tcreateReactiveStateEngine(name=\"tradeProcess\"+string(i), metrics=[<TradeTime>, <iif(BuyNum>SellNum, BuyNum, SellNum)>, <TradeQty>, <TradeAmount>, <iif(BuyNum>SellNum, \"B\", \"S\")>], dummyTable=tradeOriginalStream, outputTable=tradeProcessStream, keyColumn=\"SecurityID\")\n\tsubscribeTable(tableName=\"tradeOriginalStream\", actionName=\"tradeProcess\"+string(i), offset=-1, handler=getStreamEngine(\"tradeProcess\"+string(i)), msgAsTable=true, hash=i-1, filter = (parallel, i-1), reconnect=true)\n}\n```\n\n----------------------------------------\n\nTITLE: 导出地震历史数据为MiniSeed格式 DolphinDB 脚本\nDESCRIPTION: 该脚本展示了如何将一天内特定台网、台站多通道的所有历史地震观测数据导出为MiniSeed格式文件。通过加载分布式表获取数据，再使用循环和并行调用(mseed::write与ploop)按tagid分文件写入，支持多线程加速导出。关键参数有采样频率(sampleRate)、起始时间、以及涉及的tagid列表。此方法依赖DolphinDB环境及mseed插件。输入为查询条件，输出为分割后的MiniSeed文件，同时考虑数据并行处理提升效率。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Seismic_waveform_data_storage.md#_snippet_10\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntimer {\n\taim = exec id from loadTable(\"dfs://real\",\"tagInfo\") where net = netAim and sta = staAim\n\tt = select * from loadTable(\"dfs://real\",\"realData\") where ts >= 2023.03.02T00:00:00.000 and ts < 2023.03.03T00:00:00.000 and id in aim\n\tt = select ts,loc,net,sta,chn,value,tagid from ej(loadTable(\"dfs://real\",\"tagInfo\"),t,`id) // 导出的结果集数据\n\t\n\tsidList = exec distinct tagid from t // 涉及的所有sid\n\tstartTimeTable = select min(ts) as startTime from t group by tagid order by tagid desc  // 文件开始采样时间\n\tdataList = [] // 涉及的所有采样值\n\tfor(i in 0..(sidList.size()-1)){\n\t\ttmpValue = exec value from t where tagid = sidList[i]\n\t\tdataList.append!(tmpValue)\n\t}\n\t\n\taimFilePath= \"<YOURDIR>/\"+sidList+'.20230302.mseed' // 目标文件\n\tsampleRate = 100.00 // 采样频率，表示1秒钟有100个采集点，即10ms采集一次\n\tploop(mseed::write{,,,sampleRate,},aimFilePath,sidListid,startTimeTable[`startTime],dataList) // 使用mseed::write函数写入文件，使用ploop并行加速\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Feature Engine Function for Financial Indicators in DolphinDB\nDESCRIPTION: A custom aggregate function that calculates various financial indicators including bid-ask spread, depth imbalance, pressure, and realized volatility using matrix operations on order book data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/sql_performance_optimization_wap_di_rv.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndefg featureEngine(bidPrice,bidQty,offerPrice,offerQty){\n\tbas = offerPrice[0]\\bidPrice[0]-1\n\twap = (bidPrice[0]*offerQty[0] + offerPrice[0]*bidQty[0])\\(bidQty[0]+offerQty[0])\n\tdi = (bidQty-offerQty)\\(bidQty+offerQty)\n\tbidw=(1.0\\(bidPrice-wap))\n\tbidw=bidw\\(bidw.rowSum())\n\tofferw=(1.0\\(offerPrice-wap))\n\tofferw=offerw\\(offerw.rowSum())\n\tpress=log((bidQty*bidw).rowSum())-log((offerQty*offerw).rowSum())\n\trv=sqrt(sum2(log(wap)-log(prev(wap))))\n\treturn avg(bas),avg(di[0]),avg(di[1]),avg(di[2]),avg(di[3]),avg(di[4]),avg(di[5]),avg(di[6]),avg(di[7]),avg(di[8]),avg(di[9]),avg(press),rv\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Latest Records with OLAP Storage Engine in DolphinDB\nDESCRIPTION: A query that retrieves the latest 10 records for each security using context by and csort with the OLAP storage engine.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_SQL_Case_Tutorial.md#_snippet_14\n\nLANGUAGE: SQL\nCODE:\n```\ntimer t1 = select * from loadTable(\"dfs://Level1\", \"Snapshot\") where date(DateTime) = 2020.06.01 context by SecurityID csort DateTime limit -10\n```\n\n----------------------------------------\n\nTITLE: Creating Database for Merged Order-by-Order Data with TSDB Engine\nDESCRIPTION: Creates a database for storing merged Shanghai and Shenzhen order-by-order data. Uses a combined partitioning strategy with daily value partitioning and HASH 50 on symbols. Market type, symbol, and trade time are used as sorting columns.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practices_for_partitioned_storage.md#_snippet_7\n\nLANGUAGE: SQL\nCODE:\n```\ncreate database \"dfs://merge_TB\"\npartitioned by VALUE(2020.01.01..2021.01.01), HASH([SYMBOL, 50])\nengine='TSDB'\n\ncreate table \"dfs://merge_TB\".\"merge_entrustTB\"(\n    ChannelNo INT\n    ApplSeqNum LONG\n    MDStreamID SYMBOL\n    SecurityID SYMBOL\n    SecurityIDSource SYMBOL\n    Price DOUBLE\n    OrderQty LONG\n    Side SYMBOL\n    TradeDate DATE[comment=\"交易日期\", compress=\"delta\"]   \n    TradeTime TIME[comment=\"交易时间\", compress=\"delta\"]   \n    OrderType SYMBOL\n    LocalTime TIME\n    SeqNo LONG\n    OrderNO LONG\n    DataStatus INT\n    BizIndex LONG\n    Market SYMBOL\n)\npartitioned by TradeDate, SecurityID,\nsortColumns=[`Market,`SecurityID,`TradeTime],\nkeepDuplicates=ALL\n```\n\n----------------------------------------\n\nTITLE: Calculating Cumulative Maximum Volume per Symbol using `context by` in DolphinDB Script\nDESCRIPTION: Shows how to compute the cumulative maximum volume (`cummax`) for each stock symbol (`sym`) within the table 't'. The `context by sym` clause ensures the `cummax(volume)` function operates independently on each symbol's data series.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/panel_data.md#_snippet_3\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nselect timestamp, sym, price,volume, cummax(volume) from t context by sym;\n```\n\n----------------------------------------\n\nTITLE: Loading DolphinDB Plugins Across Distributed Nodes in C++\nDESCRIPTION: This C++ snippet demonstrates using DolphinDB's RPC mechanism to load a plugin on all data nodes in a distributed environment. The `each` function applies an RPC call that invokes the `loadPlugin` function with the given plugin path across all nodes returned by `getDataNodes()`. This ensures the plugin is available on every remote node that might require it for computations. It requires the DolphinDB distributed execution environment and correct plugin path specification.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_development_tutorial.md#_snippet_12\n\nLANGUAGE: cpp\nCODE:\n```\neach(rpc{, loadPlugin, pathToPlugin}, getDataNodes())\n```\n\n----------------------------------------\n\nTITLE: Creating Partitioned Time Series Database in DolphinDB Script\nDESCRIPTION: Defines a function to create a DolphinDB partitioned database schema with both VALUE and RANGE partitions, tailored for machine tag metrics. Dependencies include the DolphinDB server runtime. Parameters include the database (dbName), table (tableName) names, partition specifications (ps1, ps2), and the number of metric columns. Output is a database/table ready for appending time series data, partitioned on device IDs and timestamp.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/multipleValueModeWrite.txt#_snippet_0\n\nLANGUAGE: dolphindb\nCODE:\n```\ndef createDatabase(dbName,tableName, ps1, ps2, numMetrics){\n\tm = \"tag\" + string(1..numMetrics)\n\tschema = table(1:0,`id`datetime join m, [INT,DATETIME] join take(FLOAT,50) )\n\tdb1 = database(\"\",VALUE,ps1)\n\tdb2 = database(\"\",RANGE,ps2)\n\tdb = database(dbName,COMPO,[db1,db2])\n\tdb.createPartitionedTable(schema,tableName,`datetime`id)\n}\n```\n\n----------------------------------------\n\nTITLE: Calculating Annual Volatility Using DolphinDB\nDESCRIPTION: Defines getAnnualVolatility function to compute the annualized volatility of returns. It calculates daily returns as relative changes between consecutive values, computes the standard deviation of these returns, and annualizes it by scaling with the square root of 252 trading days. It depends on std and deltas functions, taking a numerical vector of values and returning a numeric value representing annualized volatility.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_factor_by_ddb/fund_factor_ten.txt#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndefg getAnnualVolatility(value){\n\treturn std(deltas(value)\\prev(value)) * sqrt(252)\n}\n```\n\n----------------------------------------\n\nTITLE: Submitting Parallel Jobs to Load TAQ CSV Files into Distributed DolphinDB Database\nDESCRIPTION: This snippet automates distributed data ingestion by iterating over all CSV files in the TAQ data folder. It extracts job identifiers from file paths, constructs job names, and submits asynchronous loadTextEx jobs to import each CSV into the distributed database partitioned by date and symbol, binding on the columns `date` and `symbol`. It wraps the loading process in a timer to monitor job duration and prints loading progress timestamps. This enables scalable and efficient batch importation of large tick datasets.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs MongoDB/dolphindb_taq_partitioned.txt#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\nfps = FP_TAQ + (exec filename from files(FP_TAQ) order by filename)\n// 导入到分布式数据库中\ntimer {getJobStat()\n\n\tfor (fp in fps) {\n\t\tjob_id_tmp = fp.strReplace(\".csv\", \"\")\n\t\tjob_id_tmp1=split(job_id_tmp,\"/\")\n\t\tjob_id=job_id_tmp1[6]\n\t\tjob_name = job_id\n\t\tsubmitJob(job_id, job_name, loadTextEx{db, `taq, `date`symbol, fp})\n\t\tprint now() + \": 已导入 \" + fp\n\t}\n\tgetRecentJobs(size(fps))\n}\n```\n\n----------------------------------------\n\nTITLE: Descriptive Statistics, Null Handling, and Sorting in pandas and DolphinDB\nDESCRIPTION: This snippet outlines functions for descriptive statistics (count, max, min, mean, sum), null value handling, sorting, and filtering operations, demonstrating their application in pandas and corresponding DolphinDB functions for data cleaning and exploration.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/function_mapping_py.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\npandas.DataFrame.count / pandas.Series.count  # Count non-null entries\npandas.DataFrame.max / pandas.Series.max  # Maximum value\npandas.DataFrame.min / pandas.Series.min  # Minimum value\npandas.DataFrame.mean / pandas.Series.mean  # Mean value\npandas.DataFrame.sum / pandas.Series.sum  # Sum of values\npandas.DataFrame.sort_values / pandas.Series.sort_values  # Sort data based on values\npandas.DataFrame.dropna / pandas.Series.dropna  # Drop null entries\npandas.DataFrame.rename  # Rename columns or index\n```\n\n----------------------------------------\n\nTITLE: Creating Databases with Different Partition Schemes in DolphinDB\nDESCRIPTION: Demonstrates creating two composite-partitioned databases with differing secondary partition schemes (30 vs 50 partitions). The first database uses day-based value partitioning with 30 symbol range partitions, while the second uses the same day partitioning but with 50 symbol range partitions.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/database.md#_snippet_11\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(\"admin\",\"123456\")\nt=table(1:0,`timestamp`sym`qty`price,[TIMESTAMP,SYMBOL,DOUBLE,DOUBLE])\ndates=2010.01.01..2020.12.31\nsyms=\"A\"+string(1..500)\nsym_ranges=cutPoints(syms,30)\ndb1=database(\"\",VALUE,dates)\ndb2=database(\"\",RANGE,sym_ranges)\ndb=database(\"dfs://db1\",COMPO,[db1,db2])\ndb.createPartitionedTable(t,`tb1,`timestamp`sym)\n\nlogin(\"admin\",\"123456\")\nt=table(1:0,`timestamp`sym`qty`price,[TIMESTAMP,SYMBOL,DOUBLE,DOUBLE])\ndates=2010.01.01..2020.12.31\nsyms=\"A\"+string(1..500)\nsym_ranges=cutPoints(syms,50)\ndb1=database(\"\",VALUE,dates)\ndb2=database(\"\",RANGE,sym_ranges)\ndb=database(\"dfs://db2\",COMPO,[db1,db2])\ndb.createPartitionedTable(t,`tb2,`timestamp`sym)\n```\n\n----------------------------------------\n\nTITLE: Optimized Trade Order Type Aggregation using Custom Function and Group By in DolphinDB\nDESCRIPTION: Defines a function getType(amount) to classify orders into types using threshold checks via nested iif. The aggregation is then performed in a single query with group by on date, symbol, side, and derived order type. This optimization reduces code complexity, minimizes redundant computation, and improves query performance. Inputs: trade table t, requires INT return type for getType due to internal optimization benefits.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_sql_cases.md#_snippet_40\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef getType(amount) {\n\treturn iif(amount < 40000, 0, iif(amount >= 40000 && amount < 200000, 1, iif(amount >= 200000 && amount < 1000000, 2, 3)))\n}\n\ntimer res2 = select sum(volume) as volume_sum, sum(volume*price) as amount_sum \n\t\t\t\tfrom t \n\t\t\t\twhere time <= 10:00:00\n\t\t\t\tgroup by date, symbol, side, getType(volume * price) as type \n```\n\n----------------------------------------\n\nTITLE: Filtering Price-Unchanged Data using Reactive State Engine in DolphinDB\nDESCRIPTION: This code snippet demonstrates how to filter out unchanged price data in real-time tick data streams using DolphinDB's `createReactiveStateEngine`. It utilizes the `deltas` function to identify and retain only those records where the `Price` has changed. The filtered data is then passed on for subsequent IOPV calculation. `tradeOriginalStream` is the input stream table and `SecurityID` is used as the key column. `keepOrder=true` ensures that the order of the data is maintained.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_IOPV.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmetricsFuc = [\n    <tradetime>,\n    <Price>]\ncreateReactiveStateEngine(name=\"tradeProcessPriceChange\", metrics=metricsFuc, dummyTable=tradeOriginalStream, outputTable=tradeOriginalStream, keyColumn=`SecurityID, filter=<deltas(Price) != 0>, keepOrder=true)\n```\n\n----------------------------------------\n\nTITLE: Record-Based Rolling Window Using rolling Function\nDESCRIPTION: Demonstrates using the rolling function to create windows based on record count rather than time. The example calculates the sum of transaction volume for every 100 trades in the last minute of a trading day.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/window_cal.md#_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\nt=table(2021.01.05T02:59:00.000+(1..2000)*30 as time, take(`CL,2000) as sym, 10* rand(50, 2000) as vol)\n\nselect rolling(last,time,100,100) as last_time,rolling(last,t.sym,100,100) as sym, rolling(sum,vol,100,100) as vol_100_sum from t\n```\n\n----------------------------------------\n\nTITLE: Creating Input and Output Stream Tables - DolphinDB\nDESCRIPTION: This function creates the main input stream table (`doorRecord`) by calling `createDoorRecordStreamTable` and an output stream table (`outputSt1`) with a simplified schema (doorNum, eventDate, doorEventCode). Both tables are configured for sharing and persistence.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/streaming_engine_anomaly_alerts/流计算引擎级联检测门禁超时数据.txt#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 输出表的时间列若设置为第一列会导致最终输出结果被初始化\ndef createInOutTable(){\n\tcreateDoorRecordStreamTable(`doorRecord)\n\tout1 =streamTable(10000:0,`doorNum`eventDate`doorEventCode,[INT,DATETIME,INT])\n\tenableTableShareAndPersistence(table=out1,tableName=`outputSt1,asynWrite=false,compress=true, cacheSize=100000)\n}\n```\n\n----------------------------------------\n\nTITLE: Data Preprocessing Function for OHLC Data\nDESCRIPTION: This function `preprocess` takes a table `t` containing OHLC data (Open, High, Low, Close) and performs several preprocessing steps. It fills missing values with `ffill`, calculates derived features like `OpenClose`, `OpenOpen`, 10-day moving average (`S_10`), Relative Strength Index (`RSI`), correlation (`Corr`), and a target variable (`Target`).  It requires the 'ta' module for RSI calculation and removes the first 10 rows due to the moving average calculation.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/machine_learning.md#_snippet_7\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nuse ta\n\ndef preprocess(t) {\n    ohlc = select ffill(Open) as Open, ffill(High) as High, ffill(Low) as Low, ffill(Close) as Close from t\n    update ohlc set OpenClose = Open - prev(Close), OpenOpen = Open - prev(Open), S_10 = mavg(Close, 10), RSI = ta::rsi(Close, 10), Target = iif(next(Close) > Close, 1, 0)\n    update ohlc set Corr = mcorr(Close, S_10, 10)\n    return ohlc[10:]\n}\n```\n\n----------------------------------------\n\nTITLE: Subscribing and Persisting Data from DolphinDB Stream Table with Stream Computation\nDESCRIPTION: DolphinDB script that creates a partitioned database and table to persist streaming CPU data, subscribes the streaming table 'cpu_stream' into a distributed table 'dfs_cpu' for batch appends, and performs continuous stream computations to filter CPU usage idle values higher than or equal to 80%, appending these to another streaming table 'cpu_warning_result' for alerting. This setup enables persistent storage and real-time alert generation for CPU usage trends. Key dependencies: previously created 'cpu_stream' table and accessible database directory. Inputs: streaming CPU metric data. Outputs: persistent tables for metrics and warning results.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_Telegraf_Grafana.md#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//创建分布表 dfs_cpu ，并订阅 cpu_stream 中的数据导入到 dfs_cpu 中。\ndbName = \"dfs://telegraf\"\ndb_telegraf = database(directory=dbName, partitionType=VALUE,partitionScheme = 2022.01.01..2022.12.31)\ncpu = table(1:0,cpuColnames,cpuColtypes)\ndfs_cpu = createPartitionedTable(dbHandle = db_telegraf,table = cpu,tableName = \"cpu\",partitionColumns =\"timestamp\",compressMethods = {timestamp:\"delta\"});\nsubscribeTable(tableName=\"cpu_stream\", actionName=\"append_cpu_stream_into_dfs\", offset=0, handler=loadTable(dbName,\"cpu\"), msgAsTable=true,batchSize=100000, throttle=1, reconnect=true)\n\n\n//进行流计算，预警统计 cpu 使用率大于80%的指标数据，并将统计的数据存入流表 cpu_warning_result 中\nenableTableShareAndPersistence(table = streamTable(1000:0,cpuColnames,cpuColtypes), tableName=`cpu_warning_result, cacheSize = 5000000) \ndef handler_cpu(mutable warning_result, msg)\n{\n    t = select * from msg where usage_idle >= 80;\n\twarning_result.append!(t)\n}\nsubscribeTable(tableName=\"cpu_stream\", actionName=\"cpu_warning\", offset=0, handler=handler_cpu{cpu_warning_result}, msgAsTable=true,batchSize=100000, throttle=1, reconnect=true)\n```\n\n----------------------------------------\n\nTITLE: Deduplication with Context By\nDESCRIPTION: This snippet demonstrates removing duplicate records using the `context by` clause. It selects data from `sensors` based on ID and datetime, using `context by id, datetime` to group rows, and `limit -1` to select the last record from each group (effectively removing duplicates within the specified context). The `limit -1` ensures that only the last row for each unique combination of id and datetime will be kept.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/iot_examples.md#_snippet_18\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect * from sensors where id in [1,51,101,151,201], datetime between 2020.09.01T00:00:00 : 2020.09.03T23:59:59 context by id, datetime limit -1\n```\n\n----------------------------------------\n\nTITLE: 多种数据导入导出操作示例 - DolphinDB 脚本\nDESCRIPTION: 此示例演示数据在 DolphinDB 表与本地二进制数据库和CSV文本文件之间的导入导出操作，包括使用 saveTable 保存二进制表、saveText 保存为CSV、loadTable 从数据库导入表，以及基于 textChunkDS 分块导入 CSV 文件。关键参数包括文件路径、表对象及数据库操作句柄，满足不同场景下的数据交换需求。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/high_freq_data_storage_and_analysis.md#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\n/**** 数据导出和导入 ****/\n\n//数据准备：将1小时数据，赋值给临时表\nt=NULL\nt=select * from pt where ts between 2022.01.01 00:00:00.000 : 2022.01.01 01:00:00.000\n\n//数据准备：路径，文件名\npath=getHomeDir()  \nfilename=\"savetable_output.csv\"\n\n//1. 导出表\ndb=database(directory=path+\"/output_db\")\nsaveTable(dbHandle=db,table=t,tableName=`collect)\n\n//2. 导出 csv \nsaveText(obj=t, filename=path+\"/\"+filename)\n\n//3. 导入表\ndb=database(directory=path+\"/output_db\")\nt=loadTable(database=db,tableName=`collect)\n\n//4. 导入 csv\nt=NULL  //释放内存\nds=textChunkDS(path+\"/\"+filename,1000)\nmr(ds,append!{loadTable(\"dfs://db_temp2\",\"collect_temp\")},,,false);\n```\n\n----------------------------------------\n\nTITLE: Creating and Loading Partitioned Table in DolphinDB\nDESCRIPTION: Example showing how to create a partitioned database with composite partitioning (value and range), create a table, and append data to it. This demonstrates DolphinDB's data storage structure.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/iot_examples.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\nschema = table(1:0,`tagid`ts`data,[INT,TIMESTAMP,INT]);\ndb1 = database(\"\",VALUE,2020.01.01..2020.01.02)\ndb2 = database(\"\",RANGE,1 11 21 31)\ndb = database(\"dfs://demo\",COMPO,[db1,db2])\ndb.createPartitionedTable(schema,\"sensor\",`ts`tagid)\n\nt=table(1..30 as tagid,take(2020.01.01T00:00:00.000 2020.01.02T00:00:00.000,30) as time,rand(10,30) as data )\nloadTable(\"dfs://demo\",\"sensor\").append!(t)\n```\n\n----------------------------------------\n\nTITLE: Querying Latest Records with TSDB Storage Engine in DolphinDB\nDESCRIPTION: A query that retrieves the latest 10 records for each security using the TSDB storage engine, which offers better performance for time-series operations.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_SQL_Case_Tutorial.md#_snippet_15\n\nLANGUAGE: SQL\nCODE:\n```\ntimer t2 = select * from loadTable(\"dfs://Level1_TSDB\", \"Snapshot\") where date(DateTime) = 2020.06.01 context by SecurityID csort DateTime limit -10\n```\n\n----------------------------------------\n\nTITLE: Using streamEngineParser for Simplified Alpha #1 Factor Stream Processing in DolphinDB\nDESCRIPTION: This DolphinDB script demonstrates using `streamEngineParser` to create a stream processing engine for the Alpha #1 factor calculation. It simplifies the creation of the engine by automatically handling both time series and cross-sectional processing.  Requires the following inputs: SecurityID, TradeTime, close.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practice_for_factor_calculation.md#_snippet_15\n\nLANGUAGE: DolphinDB\nCODE:\n```\ninputSchema = table(1:0, [\"SecurityID\",\"TradeTime\",\"close\"], [SYMBOL,TIMESTAMP,DOUBLE])\nresult = table(10000:0, [\"TradeTime\",\"SecurityID\", \"factor\"], [TIMESTAMP,SYMBOL,DOUBLE])\nmetrics = <[SecurityID, alpha1Panel(close)]>\nstreamEngine = streamEngineParser(name=\"alpha1Parser\", metrics=metrics, dummyTable=inputSchema, outputTable=result, keyColumn=\"SecurityID\", timeColumn=`tradetime, triggeringPattern='keyCount', triggeringInterval=4000)\n```\n\n----------------------------------------\n\nTITLE: Creating Partitioned Table in DolphinDB\nDESCRIPTION: This DolphinDB script defines a function `createTick` to create a partitioned table named 'tick' within a database named 'dfs://TSDB_tick'. It specifies the partition scheme using `VALUE` and `HASH`, defines the table schema with column names and data types, and sets compression methods and sort columns for efficient querying.  It also handles dropping existing databases if they exist.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/migrate_data_from_Postgre_and_Greenplum_to_DolphinDB.md#_snippet_1\n\nLANGUAGE: dolphindb\nCODE:\n```\ndef createTick(dbName, tbName){\n\tif(existsDatabase(dbName)){\n\t\tdropDatabase(dbName)\n\t}\n\tdb1 = database(, VALUE, 2020.01.01..2021.01.01)\n\tdb2 = database(, HASH, [SYMBOL, 10])\n\tdb = database(dbName, COMPO, [db1, db2], , \"TSDB\")\n\tdb = database(dbName)\n\tname = `SecurityID`TradeTime`TradePrice`TradeQty`TradeAmount`BuyNo`SellNo`ChannelNo`TradeIndex`TradeBSFlag`BizIndex\n\ttype = `SYMBOL`TIMESTAMP`DOUBLE`INT`DOUBLE`INT`INT`INT`INT`SYMBOL`INT\n\tschemaTable = table(1:0, name, type)\n\tdb.createPartitionedTable(table=schemaTable, tableName=tbName, partitionColumns=`TradeTime`SecurityID, compressMethods={TradeTime:\"delta\"}, sortColumns=`SecurityID`TradeTime, keepDuplicates=ALL)\n}\n\ndbName=\"dfs://TSDB_tick\"\ntbName=\"tick\"\ncreateTick(dbName, tbName)\n```\n\n----------------------------------------\n\nTITLE: Defining Function to Create Partitioned Table in DolphinDB\nDESCRIPTION: This DolphinDB function `orderCreate` sets up a multi-level partitioned TSDB database and a table schema. It drops the database if it exists, creates a composite partition scheme (VALUE and HASH), defines the table columns and types, and then uses `createPartitionedTable` to create the table with specified partitioning columns (`MDDate`, `SecurityID`), sort columns (`SecurityID`, `MDTime`), and compression methods.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/appendices_market_replay_bp/order_create.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef orderCreate(dbName, tbName)\n{\n    if(existsDatabase(dbName))\n    {\n        dropDatabase(dbName)\n    }\n\n    db1 = database(, VALUE, 2021.12.01..2021.12.31)\n    db2 = database(, HASH, [SYMBOL, 25])\n    db = database(dbName, COMPO, [db1, db2], , 'TSDB')\n\n    schemaTable = table(\n        array(SYMBOL, 0) as SecurityID,\n        array(DATE, 0) as MDDate,\n        array(TIME, 0) as MDTime,\n        array(TIMESTAMP, 0) as DataTimestamp,\n        array(SYMBOL, 0) as SecurityIDSource,\n        array(SYMBOL, 0) as SecurityType,\n        array(LONG, 0) as OrderIndex,\n        array(INT, 0) as OrderType,\n        array(LONG, 0) as OrderPrice,\n        array(LONG, 0) as OrderQty,\n        array(INT, 0) as OrderBSFlag,\n        array(INT, 0) as ChannelNo,\n        array(DATE, 0) as ExchangeDate,\n        array(TIME, 0) as Exchanime,\n        array(LONG, 0) as OrderNO,\n        array(LONG, 0) as ApplSeqNum,\n        array(SYMBOL, 0) as SecurityStatus\n    )\n\n    db.createPartitionedTable(table=schemaTable, tableName=tbName, partitionColumns=`MDDate`SecurityID, sortColumns=`SecurityID`MDTime, keepDuplicates=ALL,compressMethods={MDDate:\"delta\", MDTime:\"delta\",DataTimestamp:\"delta\",ExchangeDate:\"delta\",Exchanime:\"delta\"})\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Reactive State Engine with Multiple Metrics for Multi-Column Data\nDESCRIPTION: Initializes a reactive state engine named 'reactiveDemo1' that processes multiple bid-ask price and quantity columns, computing average pressure metrics every 60 seconds. It drops any pre-existing engine with the same name to avoid conflicts, ensuring real-time updates based on incoming data streams.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Streaming_computing_of_financial_quantifiers.md#_snippet_26\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// Drop existing engine if exists\ntry{ dropStreamEngine(\"reactiveDemo1\")} catch(ex){ print(ex) }\n// Define metrics with multiple columns for bid/offer prices and quantities\nmetrics1 = <[dateTime, averagePress(bidPrice0, bidPrice1, bidPrice2, bidPrice3, bidPrice4, bidPrice5, bidPrice6, bidPrice7, bidPrice8, bidPrice9, bidOrderQty0, bidOrderQty1, bidOrderQty2, bidOrderQty3, bidOrderQty4, bidOrderQty5, bidOrderQty6, bidOrderQty7, bidOrderQty8, bidOrderQty9, offerPrice0, offerPrice1, offerPrice2, offerPrice3, offerPrice4, offerPrice5, offerPrice6, offerPrice7, offerPrice8, offerPrice9, offerOrderQty0, offerOrderQty1, offerOrderQty2, offerOrderQty3, offerOrderQty4, offerOrderQty5, offerOrderQty6, offerOrderQty7, offerOrderQty8, offerOrderQty9, 60)]>\n// Instantiate the reactive engine\nrse1 = createReactiveStateEngine(name=\"reactiveDemo1\", metrics=metrics1, dummyTable=inputTable, outputTable=resultTable1, keyColumn=\"securityID\", keepOrder=true)\n```\n\n----------------------------------------\n\nTITLE: Implementing JIT-Accelerated Diagonal Matrix Creation in DolphinDB\nDESCRIPTION: This snippet defines a DolphinDB function `diagonalMatrix_jit` annotated with `@jit`, enabling Just-In-Time compilation. The function creates a diagonal matrix from an input data vector and an initial matrix by iterating through elements. The `@jit` annotation is intended to significantly improve execution speed for this type of matrix manipulation.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/matrix.md#_snippet_35\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//定义对角矩阵，jit计算比非jit快了10倍左右\n@jit\ndef diagonalMatrix_jit(data, m){\n\tn=data.size()\n\tres=m\n\tfor( i in 0:n){\n\t\t//i in 0:n\n\t\tres[i*n+i]=data[i]\n\t}\n\treturn res\n}\n```\n\n----------------------------------------\n\nTITLE: Calculating weighted average price using pivot\nDESCRIPTION: This code snippet uses the 'pivot' higher-order function to restructure the data in table 't1' based on 'minute(t1.time)' and 't1.sym'. It calculates the weighted average price (wavg) for each minute and symbol combination, using the volume as the weight. The result is then rounded to two decimal places.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_42\n\nLANGUAGE: shell\nCODE:\n```\nstockprice=pivot(wavg, [t1.price, t1.volume], minute(t1.time), t1.sym)\nstockprice.round(2)\n```\n\n----------------------------------------\n\nTITLE: Define and Share Stream Table\nDESCRIPTION: This code snippet demonstrates how to define and share a stream table using the `streamTable` function. It creates a table named `pubTable` with specified initial capacity, columns, and data types. The table is then shared across all sessions to enable data publishing.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_tutorial.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\nshare streamTable(10000:0,`timestamp`temperature, [TIMESTAMP,DOUBLE]) as pubTable\n```\n\n----------------------------------------\n\nTITLE: Sending Email Alerts for Anomalies using DolphinDB httpClient Plugin\nDESCRIPTION: This script sets up an alerting mechanism using the DolphinDB httpClient plugin. It defines a function 'sendEmail' that formats a message containing the timestamp and property code from an anomaly event and sends it as an email using 'httpClient::sendEmail'. This function is then used as a handler to subscribe to the 'abnormalRes' table (containing anomaly detection results), triggering an email alert whenever a new anomaly is detected.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Iot_intelligent_O&M.md#_snippet_15\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\n//加载 httpClient 插件\ntry{loadPlugin('./plugins/httpClient/PluginHttpClient.txt')}catch(ex){}\n\n//定义邮件发送的订阅函数\ndef sendEmail(userId,pwd,recipient,mutable msg){\n    ts = exec ts from msg\n    propertyCode = exec propertyCode from msg\n    sendMsg = string(ts)+\" the propertyCode \"+string(propertyCode)+\" rate is greater than 1\"\n    sendMsg = concat(sendMsg,\"<br>\")\n    httpClient::sendEmail(userId,pwd,recipient,\"rate is greater than 1\",sendMsg)\n}\n\n//订阅异常检测结果表发送告警\nuserId = 'xxxx' //发送者邮箱\npwd='xxx'//发送者邮箱密码\nrecipient='xxx'//接收邮箱\n\n//订阅异常检测结果表\nsubscribeTable(tableName=`abnormalRes,actionName='sendEmail',offset=0,handler=sendEmail{userId,pwd,recipient},msgAsTable=true,batchSize=10000,throttle=10)\n```\n\n----------------------------------------\n\nTITLE: Defining Incremental Data Processing Tasks with DolphinDBOperator (Python/DolphinDB)\nDESCRIPTION: This snippet demonstrates defining DolphinDB tasks for incremental data processing using DolphinDBOperator. Similar to the full load, it defines operators for loading, processing, and calculating factors. However, these scripts operate only on the current day's data (`today()`) and include a check using `getMarketCalendar` to ensure the task runs only on market trading days. They rely on pre-defined DolphinDB modules (`addLoadSnapshot`, `addProcessSnapshot`, `addFactor`) and parameters from `paramTable`.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Best_Practices_for_DolphinDB_and_Python_AirFlow.md#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\naddLoadSnapshot = DolphinDBOperator(\n        task_id='addLoadSnapshot',\n        dolphindb_conn_id='dolphindb_test',\n        sql='''\n            pnodeRun(clearAllCache)\n            undef(all)\n            go;\n            //使用module，加载已封装好的入库函数\n            use  addLoadSnapshot::loadSnapshotData\n            //通过参数共享表获取参数\n            params = dict(paramTable[`param], paramTable[`value])\n            dbName = params[`ETL_dbName_origin]\n            tbName = params[`ETL_tbName_origin]\n            fileDir = params[`ETL_filedir]\n            //获取交易日历\n            MarketDays = getMarketCalendar(\"CFFEX\")\n            //是交易日则进行数据入库\n            if(today() in MarketDays ){\n                fileDir = params[`ETL_filedir]\n                addLoadSnapshot::loadSnapshotData::loadSnapshot(today(), dbName, tbName, fileDir)\n            }\n            '''\n    )\n    addProcessSnapshot = DolphinDBOperator(\n        task_id='addProcessSnapshot',\n        dolphindb_conn_id='dolphindb_test',\n        sql='''\n            pnodeRun(clearAllCache)\n            undef(all)\n            go;\n            //使用module，加载已封装好的清洗函数\n            use addProcessSnapshot::processSnapshotData\n            //通过参数共享表获取参数\n            params = dict(paramTable[`param], paramTable[`value])\n            dbName_orig = params[`ETL_dbName_origin]\n            tbName_orig = params[`ETL_tbName_origin]\n            dbName_process = params[`ETL_dbName_process]\n            tbName_process = params[`ETL_tbName_process]\n            //获取交易日历\n            MarketDays = getMarketCalendar(\"CFFEX\")\n            //是交易日则进行数据处理\n            if(today() in MarketDays ){\n                addProcessSnapshot::processSnapshotData::process(today(), dbName_orig, tbName_orig, dbName_process, tbName_process)\n            }\n            '''\n    )\n    addCalMinuteFactor= DolphinDBOperator(\n        task_id='addCalMinuteFactor',\n        dolphindb_conn_id='dolphindb_test',\n        sql='''\n            pnodeRun(clearAllCache)\n            undef(all)\n            go;\n            //使用module，加载已封装好的计算函数\n            use addFactor::calFactorOneMinute\n            //通过参数共享表获取参数\n            params = dict(paramTable[`param], paramTable[`value])\n            dbName = params[`ETL_dbName_process]\n            tbName = params[`ETL_tbName_process]\t\n            dbName_factor = params[`ETL_dbName_factor]\n            tbName_factor = params[`ETL_tbName_factor]\n            factorTable = loadTable(dbName_factor, tbName_factor)\n            //获取交易日历\n            MarketDays = getMarketCalendar(\"CFFEX\")\n            //是交易日则调用计算函数合成分钟K线\n            if(today() in MarketDays ){\n                \taddFactor::calFactorOneMinute::calFactorOneMinute(dbName, tbName,today(), factorTable)\n            }\n            '''\n    )\n    addCalDailyFactor= DolphinDBOperator(\n        task_id='addCalDailyFactor',\n        dolphindb_conn_id='dolphindb_test',\n        sql='''\n            pnodeRun(clearAllCache)\n            undef(all)\n            go;\n            //使用module，加载已封装好的计算函数\n            use addFactor::calFactorDaily1\t\n            //通过参数共享表获取参数\n            '''\n    )\n```\n\n----------------------------------------\n\nTITLE: Creating Partitioned Table for Volatility Data in DolphinDB\nDESCRIPTION: Defines variables for the target database name (`storeDBName`) and table name (`storeTBName`). It checks if the database exists and drops it if it does. A new distributed database (`dfs://sz50VolatilityDataSet`) is created with partitioning based on a range of years. A table schema (`tbSchema`) is defined for storing the calculated features (SecurityID, TradeTime, BAS, DI0-DI9, Press, RV, targetRV). Finally, a partitioned table (`sz50VolatilityDataSet`) is created within the database, using `TradeTime` as the partitioning column and applying delta compression.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/machine_learning_volatility/01.dataProcess.txt#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\nstoreDBName = \"dfs://sz50VolatilityDataSet\"\nstoreTBName = \"sz50VolatilityDataSet\"\nif(existsDatabase(storeDBName)){\n\tdropDatabase(storeDBName)\n}\ndb = database(storeDBName, RANGE, sort(distinct(yearBegin(2000.01.01..2040.01.01))))\nname = `SecurityID`TradeTime`BAS`DI0`DI1`DI2`DI3`DI4`DI5`DI6`DI7`DI8`DI9`Press`RV`targetRV\ntype = `SYMBOL`TIMESTAMP`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE\ntbSchema = table(1:0, name, type)\ndb.createPartitionedTable(table=tbSchema, tableName=storeTBName, partitionColumns=`TradeTime, compressMethods={TradeTime:\"delta\"})\ngo\n```\n\n----------------------------------------\n\nTITLE: 查看部分收益率矩阵样本\nDESCRIPTION: 快速检视收益率矩阵的前几列，验证收益率计算的正确性。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/public_fund_basic_analysis.md#_snippet_11\n\nLANGUAGE: DolphinDB\nCODE:\n```\nreturnsMatrix[0:3]\n```\n\n----------------------------------------\n\nTITLE: Writing Data (Multiple Threads) - Dolphindb\nDESCRIPTION: Defines a function to write data using multiple threads in Dolphindb. It calculates how to split the ID range for parallel processing and uses the `ploop` function to execute the `singleThreadWriting` function concurrently on different subsets of IDs across the specified number of threads.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/singleValueModeWrite.txt#_snippet_3\n\nLANGUAGE: Dolphindb\nCODE:\n```\ndef multipleThreadWriting(id, startDay, days, freqPerDay, numIdPerPartition, threads) {\n\t//split id to multiple part for parallel writing\n\tidCountPerThread = ceil(id.size()\\threads/numIdPerPartition)*numIdPerPartition\n\tploop(singleThreadWriting{, startDay, days, freqPerDay, numIdPerPartition}, id.cut(idCountPerThread))\n}\n```\n\n----------------------------------------\n\nTITLE: Aggregating Average Price per Minute/Symbol using `pivot by` (Table Output) in DolphinDB Script\nDESCRIPTION: Filters the table 't' for symbols 'C' and 'IBM', then calculates the average price per minute for each symbol. The `pivot by` clause reshapes the aggregated results into a table where rows represent minutes and columns represent symbols.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/panel_data.md#_snippet_6\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nselect avg(price) from t where sym in `C`IBM pivot by minute(timestamp) as minute, sym;\n```\n\n----------------------------------------\n\nTITLE: Converting Numeric Stock ID to SYMBOL\nDESCRIPTION: This code snippet, part of `transType`, converts numeric stock identifiers to the SYMBOL data type in DolphinDB. It uses the `string` function to cast the numeric value to a string and applies the `lpad` function to pad the string with leading zeros, ensuring a standard 6-digit format.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/LoadDataForPoc.md#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef transType(mutable memTable)\n{\n   return memTable.replaceColumn!(`securityId,lpad(string(memTable.securityId),6,`0))\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring and Using Stream Table Snapshot in DolphinDB\nDESCRIPTION: This snippet demonstrates how to set up a DolphinDB stream table with snapshot capabilities using snapshotDir and snapshotIntervalInMsgCount parameters. It showcases creating a time series engine that snapshots data periodically, subscribing with handler=appendMsg and handlerNeedMsgId=true for message position tracking, and handling system interruptions by unsubscribing and dropping the engine. When restarting, the code retrieves the last message ID via getSnapshotMsgId and resumes subscription from that point to maintain data continuity.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/stream_aggregator.md#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\nshare streamTable(10000:0,`time`sym`price`id, [TIMESTAMP,SYMBOL,INT,INT]) as trades\noutput1 =table(10000:0, `time`sumprice, [TIMESTAMP,INT])\nAgg1 = createTimeSeriesEngine(name=`Agg1, windowSize=100, step=50, metrics=<sum(price)>, dummyTable=trades, outputTable=output1, timeColumn=`time, snapshotDir=\"/home/server1/snapshotDir\", snapshotIntervalInMsgCount=100)\nsubscribeTable(server=\"\", tableName=\"trades\", actionName=\"Agg1\",offset= 0, handler=appendMsg{Agg1}, msgAsTable=true, handlerNeedMsgId=true)\n\nn=500\ntimev=timestamp(1..n) + 2021.03.12T15:00:00.000\nsymv = take(`abc`def, n)\npricev = int(1..n)\nid = take(-1, n)\ninsert into trades values(timev, symv, pricev, id)\n\nselect * from output1\n```\n\n----------------------------------------\n\nTITLE: Appending Calculated OHLC Data to DFS Table in DolphinDB Script\nDESCRIPTION: Loads the target DFS partitioned table \"dfs://stockFundOHLC\".\"stockFundOHLC\" using `loadTable` and then uses the `append!` method to insert the rows from the `oneDayResult` in-memory table (containing the calculated 1-minute OHLC data) into the distributed table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/K.md#_snippet_7\n\nLANGUAGE: dolphindb\nCODE:\n```\nloadTable(\"dfs://stockFundOHLC\", \"stockFundOHLC\").append!(oneDayResult)\n```\n\n----------------------------------------\n\nTITLE: Replay Job Submission - DolphinDB\nDESCRIPTION: The `replayJob` function replays data using the DolphinDB `replay` function based on the provided input dictionary, table name, date and time configurations, replay rate, and sorting column. If `sortColumn` is \"NULL\", it replays without sorting; otherwise, it replays with sorting based on the specified column. It then calls `createEnd` to create end-of-data markers for the output table. It uses `objByName` to resolve the table name to a table object. The input `replayRate` is converted to an integer before being used.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/appendices_market_replay_bp/replay.txt#_snippet_2\n\nLANGUAGE: dolphindb\nCODE:\n```\ndef replayJob(inputDict, tabName, dateDict, timeDict, replayRate, sortColumn)\n{\n    if(sortColumn == \"NULL\")\n    {\n        replay(inputTables=inputDict, outputTables=objByName(tabName), dateColumn=dateDict, timeColumn=timeDict, replayRate=int(replayRate), absoluteRate=false, parallelLevel=23)\n    }\n    else\n    {\n        replay(inputTables=inputDict, outputTables=objByName(tabName), dateColumn=dateDict, timeColumn=timeDict, replayRate=int(replayRate), absoluteRate=false, parallelLevel=23, sortColumns=sortColumn)    \n    }\n    createEnd(tabName, sortColumn)\n}\n```\n\n----------------------------------------\n\nTITLE: Simulating Stock Snapshot Data in DolphinDB\nDESCRIPTION: Defines schemas for OLAP ('dfs://Level1') and TSDB ('dfs://Level1_TSDB') databases using VALUE and HASH partitioning. Creates partitioned tables based on a predefined stock snapshot model. Includes functions `mockHalfDayData` and `mockData` to generate and insert simulated stock market snapshot data for specified dates (2020.06.01 to 2020.06.10) and times into both OLAP and TSDB tables. This script prepares the environment for subsequent data migration and rebalancing examples.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/data_move_rebalance.md#_snippet_0\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nmodel = table(1:0, `SecurityID`DateTime`PreClosePx`OpenPx`HighPx`LowPx`LastPx`Volume`Amount`BidPrice1`BidPrice2`BidPrice3`BidPrice4`BidPrice5`BidOrderQty1`BidOrderQty2`BidOrderQty3`BidOrderQty4`BidOrderQty5`OfferPrice1`OfferPrice2`OfferPrice3`OfferPrice4`OfferPrice5`OfferQty1`OfferQty2`OfferQty3`OfferQty4`OfferQty5, [SYMBOL, DATETIME, DOUBLE, DOUBLE, DOUBLE, DOUBLE, DOUBLE, LONG, DOUBLE, DOUBLE, DOUBLE, DOUBLE, DOUBLE, DOUBLE, LONG, LONG, LONG, LONG, LONG, DOUBLE, DOUBLE, DOUBLE, DOUBLE, DOUBLE, LONG, LONG, LONG, LONG, LONG])\n\n// OLAP存储引擎建库建表\ndbDate = database(\"\", VALUE, 2020.06.01..2020.06.07)\ndbSecurityID = database(\"\", HASH, [SYMBOL, 10])\ndb = database(\"dfs://Level1\", COMPO, [dbDate, dbSecurityID])\ncreatePartitionedTable(db, model, `Snapshot, `DateTime`SecurityID)\n\n// TSDB存储引擎建库建表\ndbDate = database(\"\", VALUE, 2020.06.01..2020.06.07)\ndbSymbol = database(\"\", HASH, [SYMBOL, 10])\ndb = database(\"dfs://Level1_TSDB\", COMPO, [dbDate, dbSymbol], engine=\"TSDB\")\ncreatePartitionedTable(db, model, `Snapshot, `DateTime`SecurityID, sortColumns=`SecurityID`DateTime)\n\ndef mockHalfDayData(Date, StartTime) {\n    t_SecurityID = table(format(600001..602000, \"000000\") + \".SH\" as SecurityID)\n    t_DateTime = table(concatDateTime(Date, StartTime + 1..2400 * 3) as DateTime)\n    t = cj(t_SecurityID, t_DateTime)\n    size = t.size()\n    return  table(t.SecurityID as SecurityID, t.DateTime as DateTime, rand(100.0, size) as PreClosePx, rand(100.0, size) as OpenPx, rand(100.0, size) as HighPx, rand(100.0, size) as LowPx, rand(100.0, size) as LastPx, rand(10000, size) as Volume, rand(100000.0, size) as Amount, rand(100.0, size) as BidPrice1, rand(100.0, size) as BidPrice2, rand(100.0, size) as BidPrice3, rand(100.0, size) as BidPrice4, rand(100.0, size) as BidPrice5, rand(100000, size) as BidOrderQty1, rand(100000, size) as BidOrderQty2, rand(100000, size) as BidOrderQty3, rand(100000, size) as BidOrderQty4, rand(100000, size) as BidOrderQty5, rand(100.0, size) as OfferPrice1, rand(100.0, size) as OfferPrice2, rand(100.0, size) as OfferPrice3, rand(100.0, size) as OfferPrice4, rand(100.0, size) as OfferPrice5, rand(100000, size) as OfferQty1, rand(100000, size) as OfferQty2, rand(100000, size) as OfferQty3, rand(100000, size) as OfferQty4, rand(100000, size) as OfferQty5)\n}\n\ndef mockData(DateVector, StartTimeVector) {\n    for(Date in DateVector) {\n        for(StartTime in StartTimeVector) {\n            data = mockHalfDayData(Date, StartTime)\n \n            // OLAP存储引擎分布式表插入模拟数据\n            loadTable(\"dfs://Level1\", \"Snapshot\").append!(data)\n  \n            // TSDB存储引擎分布式表插入模拟数据\n            loadTable(\"dfs://Level1_TSDB\", \"Snapshot\").append!(data)\n        }   \n    }\n}\n\nmockData(2020.06.01..2020.06.10, 09:30:00 13:00:00)\n```\n\n----------------------------------------\n\nTITLE: Creating Reactive State Engine with ArrayVector Metrics\nDESCRIPTION: Sets up a reactive engine named 'reactiveDemo2' to process pressure metrics using an array-based approach, suitable for collapsing multiple related columns into vector formats. This allows for more efficient batch processing of multiple data streams.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Streaming_computing_of_financial_quantifiers.md#_snippet_27\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// Drop existing engine if exists\ntry{ dropStreamEngine(\"reactiveDemo2\")} catch(ex){ print(ex) }\n// Define metrics with arrayvector approach\nmetrics2 = <[dateTime, averagePressArray(bidPrice, bidOrderQty, offerPrice, offerOrderQty, 60)]>\n// Create reactive engine for arrayvector metrics\nrse2 = createReactiveStateEngine(name=\"reactiveDemo2\", metrics=metrics2, dummyTable=inputTableArrayVector, outputTable=resultTable2, keyColumn=\"securityID\", keepOrder=true)\n```\n\n----------------------------------------\n\nTITLE: Querying Capital Flow Data for Grafana (SQL)\nDESCRIPTION: These SQL queries are designed for use with Grafana to visualize real-time capital flow data. They retrieve `BuySmallAmount`, `SellSmallAmount`, `BuyBigAmount`, and `SellBigAmount` from the `capitalFlowStream` table, filtering by `SecurityID` '600000'. The `gmtime` function is used to convert the `TradeTime` to UTC for Grafana's display. Sell amounts are negated to display them as negative values representing outflows.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_capital_flow_order_by_order.md#_snippet_9\n\nLANGUAGE: SQL\nCODE:\n```\nselect gmtime(TradeTime) as time_sec, BuySmallAmount from capitalFlowStream where SecurityID=`600000\n```\n\nLANGUAGE: SQL\nCODE:\n```\nselect gmtime(TradeTime) as time_sec, -SellSmallAmount as SellSmallAmount from capitalFlowStream where SecurityID=`600000\n```\n\nLANGUAGE: SQL\nCODE:\n```\nselect gmtime(TradeTime) as time_sec, BuyBigAmount from capitalFlowStream where SecurityID=`600000\n```\n\nLANGUAGE: SQL\nCODE:\n```\nselect gmtime(TradeTime) as time_sec, -SellBigAmount as SellBigAmount from capitalFlowStream where SecurityID=`600000\n```\n\n----------------------------------------\n\nTITLE: Re-backing Up a Specific DolphinDB Partition (Force)\nDESCRIPTION: Shows how to force a re-backup of a potentially corrupted partition using the `backup` function. The `force=true` parameter ensures that the specified partition (`/Key4/tp/20120103` of table `quotes_2` in `dbPath`) is backed up again to `backupDir`, overwriting the existing backup data for that partition. This is often used after `checkBackup` identifies issues.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/backup-restore-new.md#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\nbackup(backupDir=backupDir,dbPath=dbPath,force=true,parallel=true,snapshot=true,tableName=`quotes_2,partition=\"/Key4/tp/20120103\")\n```\n\n----------------------------------------\n\nTITLE: Pipelining Multiple DolphinDB Stream Engines for Composite Factor Computation Using Cross-Sectional and Reactive State Engines in DolphinDB Script\nDESCRIPTION: This snippet illustrates how to pipeline a reactive state engine with a cross-sectional aggregator engine to process streaming financial data for a composite World Quant Alpha factor. The state engine calculates a time series metric per symbol, and the cross-sectional engine consumes this output to compute rank-based factors across symbols. Input tables are defined with relevant time, symbol, and price columns. The reactive state engine outputs to the cross-sectional aggregator's input, facilitating sequential computations in a single stream processing pipeline. Dependencies include DolphinDB streaming tables, reactive state engine, and cross-sectional aggregator APIs. This approach reduces subscription overhead and improves performance compared to chained multiple streams.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/reactive_state_engine.md#_snippet_12\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//Alpha#001公式：rank(Ts_ArgMax(SignedPower((returns<0?stddev(returns,20):close), 2), 5))-0.5\n\n//创建横截面引擎，计算每个股票的rank\ndummy = table(1:0, `sym`time`maxIndex, [SYMBOL, TIMESTAMP, DOUBLE])\nresultTable = streamTable(10000:0, `time`sym`factor1, [TIMESTAMP, SYMBOL, DOUBLE])\nccsRank = createCrossSectionalAggregator(name=\"alpha1CCS\", metrics=<[sym, rank(maxIndex)\\count(maxIndex) - 0.5]>,  dummyTable=dummy, outputTable=resultTable,  keyColumn=`sym, triggeringPattern='keyCount', triggeringInterval=3000, timeColumn=`time)\n\n@state\ndef wqAlpha1TS(close){\n    ret = ratios(close) - 1\n    v = iif(ret < 0, mstd(ret, 20), close)\n    return mimax(signum(v)*v*v, 5)\n}\n\n//创建响应式状态引擎，输出到前面的横截面引擎ccsRank\ninput = table(1:0, `sym`time`close, [SYMBOL, TIMESTAMP, DOUBLE])\nrse = createReactiveStateEngine(name=\"alpha1\", metrics=<[time, wqAlpha1TS(close)]>, dummyTable=input, outputTable=ccsRank, keyColumn=\"sym\")\n```\n\n----------------------------------------\n\nTITLE: Inserting Data into Range-Partitioned Table Outside Defined Range\nDESCRIPTION: This code snippet illustrates data loss when inserting data into a range-partitioned table where some data falls outside the defined partition ranges. The `tableInsert` function returns the number of inserted rows, which can be used to detect such data loss.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/database_and_table_creation_details.md#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\nn=1000000\nID=rand(20, n)//数据范围是 0-20\nx=rand(1.0, n)\nt=table(ID, x);\n//设置分区时RANGE范围是 0-5，5-10\ndb=database(directory=\"dfs://rangedb\", partitionType=RANGE, partitionScheme=0 5 10)\n\npt = db.createPartitionedTable(t, `pt, `ID);\n//tableInsert之后只有在 0-10 部分的数据才被插入，其余的都被丢弃。\ntableInsert(pt,t);\n```\n\n----------------------------------------\n\nTITLE: Creating Price Matrix from Table using `panel` in DolphinDB Script\nDESCRIPTION: Converts the 'price' column from table 't' into a matrix representation using the `panel` function. The function uses `t.timestamp` for row labels and `t.sym` for column labels, organizing the prices accordingly.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/panel_data.md#_snippet_9\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nprice = panel(t.timestamp, t.sym, t.price);\nprice;\n```\n\n----------------------------------------\n\nTITLE: Converting Input Parameters from String to Date in DolphinDB Function - DolphinDB Script\nDESCRIPTION: This snippet explains the correct way to handle parameters passed from DolphinScheduler into DolphinDB functions. Because DolphinScheduler inputs parameters as strings, it is necessary to convert date strings to DolphinDB Date type inside the function using 'date()' before processing. This example modifies 'loadEntrustFV' to convert 'startDate' and 'endDate' when loading batch data, ensuring data type correctness and preventing errors.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/dolphinscheduler_integration.md#_snippet_23\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 在DolphinScheduler上执行的语句为：\nloadSnapshotFV(startDate=${startDate},endDate=${endDate},loadType=\"batch\");\n\n// 由于传入的 startDate 是字符串类型，因此在DolphinDB上定义该函数时需要先转成 Date 类型\nuse stockData::stockDataLoad\ndef loadEntrustFV(userName=\"admin\" , userPassword=\"123456\", startDate = 2023.02.01, endDate = 2023.02.01, dbName = \"dfs://stockData\", tbName = \"entrust\", filePath = \"/hdd/hdd/ymchen\", loadType = \"daily\")\n{\n    if(loadType == \"batch\")\n    {\n        // 使用 date(startDate) 转成 Date 类型\n        loadEntrust(userName, userPassword, date(startDate), date(endDate), dbName, tbName, filePath, loadType)\n    }\n}\n\n// 创建函数视图\naddFunctionView(loadEntrustFV)\n```\n\n----------------------------------------\n\nTITLE: Creating Multiple-Value Model Database in DolphinDB\nDESCRIPTION: Function to create a database using the multiple-value model, with temporal and device-based partitioning. This model stores all metrics for a device at a given timestamp in a single row.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/iot_examples.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\nnumMachines=100\nnumMachinesPerPartition=10\nps1=2020.09.01..2020.12.31\nps2=numMachinesPerPartition*(0..(numMachines/numMachinesPerPartition))+1\n\ndef createDatabase(dbName,tableName, ps1, ps2, numMetrics){\n\tm = \"tag\" + string(1..numMetrics)\n\tschema = table(1:0,`id`datetime join m, [INT,DATETIME] join take(FLOAT,50) )\n\tdb1 = database(\"\",VALUE,ps1)\n\tdb2 = database(\"\",RANGE,ps2)\n\tdb = database(dbName,COMPO,[db1,db2])\n\tdb.createPartitionedTable(schema,tableName,`datetime`id)\n}\ncreateDatabase(\"dfs://mvmDemo\",\"machines\", ps1, ps2, 50)\n```\n\n----------------------------------------\n\nTITLE: Creating Composite Partitioned Table by TradeDate and SecurityID to Manage Increasing Data Volume in DolphinDB Script\nDESCRIPTION: Creates a composite partitioned database combining date range and symbol hash partitions to avoid large partition growth due to increasing data volume over time. This setup facilitates manageable daily partitions automatically created with inserts, optimizing query performance over time-series data (2.9.2 solution). Requires DolphinDB composite partition support.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/database_and_table_creation_details.md#_snippet_18\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndb1 = database(, VALUE, 2020.01.01..2021.01.01)\ndb2 = database(, HASH, [SYMBOL, 25])\ndb = database(\"dfs://testDB2\", partitionType=COMPO, partitionScheme=[db1, db2])\ncolName = `SecurityID`TradeDate`TradeTime`TradePrice`TradeQty`TradeAmount`BuyNo`SellNo\ncolType = `SYMBOL`DATE`TIME`DOUBLE`INT`DOUBLE`INT`INT\ntbSchema = table(1:0, colName, colType)\ndb.createPartitionedTable(table=tbSchema, tableName=`pt, partitionColumns=`TradeDate`SecurityID)\n```\n\n----------------------------------------\n\nTITLE: Creating Public Fund Net Value Table in DolphinDB\nDESCRIPTION: Sets paths and names, checks if the 'publicFundNetValue' table exists in 'dfs://publicFundDB', drops it if needed, defines the schema (`SecurityID`, `TradeDate`, `NetValue`, `AccNetValue`, `AdjNetValue`), creates a distributed table partitioned by 'TradeDate' and sorted by 'SecurityID' and 'TradeDate', loads data from 'publicFundNetValue.csv', and appends it to the table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/public_fund_basic_analysis/basic_analysis_of_public_fund_net_value.txt#_snippet_1\n\nLANGUAGE: dolphindb\nCODE:\n```\ncsvDataPath = \"/ssd/ssd2/data/fundData/publicFundNetValue.csv\"\ndbName = \"dfs://publicFundDB\"\ntbName = \"publicFundNetValue\"\n// create distributed table in datase(\"dfs://publicFundDB\")\nif(existsTable(dbName, tbName)){\n\tdropTable(database(dbName), tbName)\n}\nnames = `SecurityID`TradeDate`NetValue`AccNetValue`AdjNetValue\ntypes = `SYMBOL`DATE`DOUBLE`DOUBLE`DOUBLE\nschemaTB = table(1:0, names, types)\ndb = database(dbName)\ndb.createPartitionedTable(table=schemaTB, tableName=tbName, partitionColumns=`TradeDate, sortColumns=`SecurityID`TradeDate)\n// load CSV data\ntmp = ploadText(filename=csvDataPath, schema=table(names, types))\nloadTable(dbName, tbName).append!(tmp)\n```\n\n----------------------------------------\n\nTITLE: Configuring DolphinScheduler Application YAML for MySQL - YAML\nDESCRIPTION: This YAML configuration sets up the Spring profile for MySQL in DolphinScheduler, specifying the JDBC driver, connection URL, username, and password for database connection. It also comments out PostgreSQL related entries since this deployment uses MySQL. Correct configuration here ensures DolphinScheduler reads and writes metadata to the MySQL database properly. Users should replace {user} and {password} with actual credentials.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/dolphinscheduler_integration.md#_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\n---\n#spring:\n#  config:\n#    activate:\n#      on-profile: postgresql\n#  quartz:\n#    properties:\n#      org.quartz.jobStore.driverDelegateClass: org.quartz.impl.jdbcjobstore.PostgreSQLDelegate\n#  datasource:\n#    driver-class-name: org.postgresql.Driver\n#    url: jdbc:postgresql://127.0.0.1:5432/dolphinscheduler\n#    username: root\n#    password: root\n\n---\nspring:\n  config:\n    activate:\n      on-profile: mysql\n  sql:\n     init:\n       schema-locations: classpath:sql/dolphinscheduler_mysql.sql\n  datasource:\n    driver-class-name: com.mysql.cj.jdbc.Driver\n    url: jdbc:mysql://127.0.0.1:3306/dolphinscheduler?useUnicode=true&characterEncoding=UTF-8\n    username: {user}\n    password: {password}\n```\n\n----------------------------------------\n\nTITLE: Optimized Direct Group By Query in DolphinDB SQL\nDESCRIPTION: An optimized approach that performs calculations directly on the distributed table without intermediate results, leveraging parallel processing across partitions.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_SQL_Case_Tutorial.md#_snippet_11\n\nLANGUAGE: SQL\nCODE:\n```\ntimer t2 = select iif(max(OfferPrice1) - min(BidPrice1) == 0, 0, 1) as Price1Diff, count(OfferPrice1) as OfferPrice1Count, sum(Volume) as Volumes \n\t\t\tfrom snapshot \n\t\t\twhere date(DateTime) = 2020.06.01, second(DateTime) >= 09:30:00 \n\t\t\tgroup by SecurityID, date(DateTime) as Date, iif(LastPx > OpenPx, 1, 0) as Flag\n```\n\n----------------------------------------\n\nTITLE: 定义计算9个投资因子的函数\nDESCRIPTION: 定义一个函数用于计算9个投资因子，包括年化收益率、年化波动率、偏度值、峰度值、夏普比率、最大回撤、回撤比例、Beta和Alpha。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/fund_factor_contrasted_by_py.md#_snippet_26\n\nLANGUAGE: dolphindb\nCODE:\n```\ndef getmetric(result2, symList){\n\tReturn = select fundNum, \n\t            getAnnualReturn(value) as annualReturn,\n\t            getAnnualVolatility(value) as annualVolRat,\n\t            getAnnualSkew(value) as skewValue,\n\t            getAnnualKur(value) as kurValue,\n\t            getSharp(value) as sharpValue,\n\t            getMaxDrawdown(value) as MaxDrawdown,\n\t            getDrawdownRatio(value) as DrawdownRatio,\n\t            getBeta(value, price) as Beta,\n\t            getAlpha(value, price) as Alpha\t\n             from result2\n             where TradeDate in 2018.05.24..2021.05.27 and fundNum in symList group by fundNum\n }\n```\n\n----------------------------------------\n\nTITLE: Advanced Matrix-Based Calculation SQL with OLAP Storage in DolphinDB\nDESCRIPTION: This SQL query transforms order book data into matrices and applies a custom feature engine function to calculate financial indicators. It uses the OLAP storage engine and matrix operations for more concise and maintainable code.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/sql_performance_optimization_wap_di_rv.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\n/**\npart1: Define calculation function\n*/\ndefg featureEngine(bidPrice,bidQty,offerPrice,offerQty){\n\tbas = offerPrice[0]\\bidPrice[0]-1\n\twap = (bidPrice[0]*offerQty[0] + offerPrice[0]*bidQty[0])\\(bidQty[0]+offerQty[0])\n\tdi = (bidQty-offerQty)\\(bidQty+offerQty)\n\tbidw=(1.0\\(bidPrice-wap))\n\tbidw=bidw\\(bidw.rowSum())\n\tofferw=(1.0\\(offerPrice-wap))\n\tofferw=offerw\\(offerw.rowSum())\n\tpress=log((bidQty*bidw).rowSum())-log((offerQty*offerw).rowSum())\n\trv=sqrt(sum2(log(wap)-log(prev(wap))))\n\treturn avg(bas),avg(di[0]),avg(di[1]),avg(di[2]),avg(di[3]),avg(di[4]),avg(di[5]),avg(di[6]),avg(di[7]),avg(di[8]),avg(di[9]),avg(press),rv\n}\n\n/**\npart2: Define variables and assign values\n*/\nstockList=`601318`600519`600036`600276`601166`600030`600887`600016`601328`601288`600000`600585`601398`600031`601668`600048`601888`600837`601601`601012`603259`601688`600309`601988`601211`600009`600104`600690`601818`600703`600028`601088`600050`601628`601857`601186`600547`601989`601336`600196`603993`601138`601066`601236`601319`603160`600588`601816`601658`600745\ndbName = \"dfs://snapshot_SH_L2_OLAP\"\ntableName = \"snapshot_SH_L2_OLAP\"\nsnapshot = loadTable(dbName, tableName)\n\n/**\npart3: Execute SQL\n*/\nresult1 = select\n            featureEngine(\n            matrix(BidPrice0,BidPrice1,BidPrice2,BidPrice3,BidPrice4,BidPrice5,BidPrice6,BidPrice7,BidPrice8,BidPrice9),\n            matrix(BidOrderQty0,BidOrderQty1,BidOrderQty2,BidOrderQty3,BidOrderQty4,BidOrderQty5,BidOrderQty6,BidOrderQty7, BidOrderQty8,BidOrderQty9),\n            matrix(OfferPrice0,OfferPrice1,OfferPrice2,OfferPrice3,OfferPrice4,OfferPrice5,OfferPrice6,OfferPrice7,OfferPrice8, OfferPrice9),\n            matrix(OfferOrderQty0,OfferOrderQty1,OfferOrderQty2,OfferOrderQty3,OfferOrderQty4,OfferOrderQty5,OfferOrderQty6, OfferOrderQty7,OfferOrderQty8,OfferOrderQty9)) as `BAS`DI0`DI1`DI2`DI3`DI4`DI5`DI6`DI7`DI8`DI9`Press`RV\n\t\tfrom snapshot\n\t\twhere date(TradeTime) between 2020.01.01 : 2020.12.31, SecurityID in stockList, (time(TradeTime) between 09:30:00.000 : 11:29:59.999) || (time(TradeTime) between 13:00:00.000 : 14:56:59.999)\n\t\tgroup by SecurityID, interval( TradeTime, 10m, \"none\" ) as TradeTime map\n```\n\n----------------------------------------\n\nTITLE: Loading httpClient Plugin in DolphinDB on Linux\nDESCRIPTION: Code snippet demonstrating how to load the httpClient plugin in DolphinDB on Linux systems. Requires libPluginHttpClient.so and PluginHttpClient.txt files in the same directory.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/send_messages_external_systems.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\nloadPlugin(\"<PluginDir>/httpClient/bin/linux64/PluginHttpClient.txt\");\n```\n\n----------------------------------------\n\nTITLE: Unit Testing with factorDoubleEMA\nDESCRIPTION: This code demonstrates unit testing for the `factorDoubleEMA` function using the DolphinDB built-in testing framework. It includes three test cases covering scenarios with and without null values, as well as a streaming data scenario. The `assert` and `eqObj` functions are used to verify the expected results.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practice_for_factor_calculation.md#_snippet_28\n\nLANGUAGE: DolphinDB\nCODE:\n```\n@testing: case = \"factorDoubleEMA_without_null\"\nre = factorDoubleEMA(0.1 0.1 0.2 0.2 0.15 0.3 0.2 0.5 0.1 0.2)\nassert 1, eqObj(re, NULL NULL NULL NULL NULL 5.788743 -7.291889 7.031123 -24.039933 -16.766359, 6)\n\n@testing: case = \"factorDoubleEMA_with_null\"\nre = factorDoubleEMA(NULL 0.1 0.2 0.2 0.15 NULL 0.2 0.5 0.1 0.2)\nassert 1, eqObj(re, NULL NULL NULL NULL NULL NULL 63.641310 60.256608  8.156385 -0.134531, 6)\n\n@testing: case = \"factorDoubleEMA_streaming\"\ntry{dropStreamEngine(\"factorDoubleEMA\")}catch(ex){}\ninput = table(take(1, 10) as id, 0.1 0.1 0.2 0.2 0.15 0.3 0.2 0.5 0.1 0.2 as price)\nout = table(10:0, `id`price, [INT,DOUBLE])\nrse = createReactiveStateEngine(name=\"factorDoubleEMA\", metrics=<factorDoubleEMA(price)>, dummyTable=input, outputTable=out, keyColumn='id')\nrse.append!(input)\nassert 1, eqObj(out.price, NULL NULL NULL NULL NULL 5.788743 -7.291889 7.031123 -24.039933 -16.766359, 6)\n```\n\n----------------------------------------\n\nTITLE: Initializing SQL Query Parameters\nDESCRIPTION: This snippet initializes variables that will be used to construct an SQL query. It includes variable declarations for the select columns (`sel`), the from table (`fm`), the where clause (`wre`), the context by column (`ctxBy`), the csort column (`cs`), and the limit value (`lim`). These variables are then used in the subsequent `sql` function to generate the SQL query.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/macro_var_based_metaprogramming.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\nsel=sqlColAlias(makeUnifiedCall(cumsum, sqlCol(\"price\")), \"cum_price\")\nfm=\"t\"\nwre=parseExpr(\"time between 09:00:00 and 15:00:00\")\nctxBy=sqlCol(\"securityID\")\ncs=sqlCol(\"time\")\nlim=-1\n```\n\n----------------------------------------\n\nTITLE: 使用asof join关联不同股票的行情数据\nDESCRIPTION: 通过asof join为某只股票每个数据更新时刻提供另一只股票的最新报价数据，用于交叉行情分析和比较。此方法可以处理不同股票更新频率和时刻不同的情况。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/quant_finance_examples.md#_snippet_11\n\nLANGUAGE: DolphinDB\nCODE:\n```\nt1 = select symbol, date, time, askPrice1, bidPrice1 from quotes where date=2020.06.01, symbol=`600000, time between 09:30:00.000 : 15:00:00.000 \nC600300 = select date, time, askPrice1, bidPrice1 from quotes where date=2020.06.01, symbol=`600300, time between 09:30:00.000 : 15:00:00.000 \nt = aj(t1, C600300, `date`time)\n```\n\n----------------------------------------\n\nTITLE: Implementing JIT-Accelerated Upper Triangular Matrix Creation in DolphinDB\nDESCRIPTION: This snippet defines a DolphinDB function `upperTriangularMatrix_jit` with the `@jit` annotation for JIT compilation. The function transforms an input matrix into an upper triangular matrix by iterating through elements and setting those below the main diagonal to zero. This showcases JIT support for loop-based matrix manipulation.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/matrix.md#_snippet_38\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//获取矩阵的上三角矩阵\n@jit\ndef upperTriangularMatrix_jit(m, rowNum,colNum){\n\tupperM=m\n  for( i in 0:colNum){\n\t\tfor(j in 0:rowNum){\n\t\t\tif(i<j){\n\t\t\t  upperM[i*rowNum+j]=0\n\t\t\t}\n\t\t}\n\t}\n\treturn upperM\n}\n```\n\n----------------------------------------\n\nTITLE: Calculating Buy Trade Ratio from Tick Data in DolphinDB\nDESCRIPTION: Defines a stateful function to calculate the proportion of buy-initiated trades to total trades. Uses context by for grouped computation and csort to ensure time-ordered processing within groups.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practice_for_factor_calculation.md#_snippet_9\n\nLANGUAGE: DolphinDB\nCODE:\n```\n@state\ndef buyTradeRatio(buyNo, sellNo, tradeQty){\n    return cumsum(iif(buyNo>sellNo, tradeQty, 0))\\cumsum(tradeQty)\n}\n\nfactor = select TradeTime, SecurityID, `buyTradeRatio as factorname, buyTradeRatio(BuyNo, SellNo, TradeQty) as val from loadTable(\"dfs://tick_SH_L2_TSDB\",\"tick_SH_L2_TSDB\") where date(TradeTime)<2020.01.31 and time(TradeTime)>=09:30:00.000 context by SecurityID, date(TradeTime) csort TradeTime\n```\n\n----------------------------------------\n\nTITLE: Creating Database for Merged Level-2 Market Snapshot Data with TSDB Engine\nDESCRIPTION: Creates a database for storing merged Shanghai and Shenzhen Level-2 market snapshot data. Uses a combined partitioning strategy with daily value partitioning and HASH 50 on symbols. Market type, symbol, and trade time are used as sorting columns.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practices_for_partitioned_storage.md#_snippet_6\n\nLANGUAGE: SQL\nCODE:\n```\ncreate database \"dfs://merge_TB\"\npartitioned by VALUE(2020.01.01..2021.01.01), HASH([SYMBOL, 50])\nengine='TSDB'\n\ncreate table \"dfs://merge_TB\".\"merge_snapshotTB\"(\n\tMarket SYMBOL\n    TradeDate DATE[comment=\"交易日期\", compress=\"delta\"]   \n    TradeTime TIME[comment=\"交易时间\", compress=\"delta\"]   \n    MDStreamID SYMBOL\n    SecurityID SYMBOL\n    SecurityIDSource SYMBOL\n    TradingPhaseCode SYMBOL\n    ImageStatus INT\n    PreCloPrice DOUBLE\n    NumTrades LONG\n    TotalVolumeTrade LONG\n    TotalValueTrade DOUBLE\n    LastPrice DOUBLE\n    OpenPrice DOUBLE\n    HighPrice DOUBLE\n    LowPrice DOUBLE\n    ClosePrice DOUBLE\n    DifPrice1 DOUBLE\n    DifPrice2 DOUBLE\n    PE1 DOUBLE\n    PE2 DOUBLE\n    PreCloseIOPV DOUBLE\n    IOPV DOUBLE\n    TotalBidQty LONG\n    WeightedAvgBidPx DOUBLE\n    AltWAvgBidPri DOUBLE\n    TotalOfferQty LONG\n    WeightedAvgOfferPx DOUBLE\n    AltWAvgAskPri DOUBLE\n    UpLimitPx DOUBLE\n    DownLimitPx DOUBLE\n    OpenInt INT\n    OptPremiumRatio DOUBLE\n    OfferPrice DOUBLE[]\n    BidPrice DOUBLE[]\n    OfferOrderQty LONG[]\n    BidOrderQty LONG[]\n    BidNumOrders INT[]\n    OfferNumOrders INT[]\n    ETFBuyNumber INT\n    ETFBuyAmount LONG\n    ETFBuyMoney DOUBLE\n    ETFSellNumber INT\n    ETFSellAmount LONG\n    ETFSellMoney DOUBLE\n    YieldToMatu DOUBLE\n    TotWarExNum DOUBLE\n    WithdrawBuyNumber INT\n    WithdrawBuyAmount LONG\n    WithdrawBuyMoney DOUBLE\n    WithdrawSellNumber INT\n    WithdrawSellAmount LONG\n    WithdrawSellMoney DOUBLE\n    TotalBidNumber INT\n    TotalOfferNumber INT\n    MaxBidDur INT\n    MaxSellDur INT\n    BidNum INT\n    SellNum INT\n    LocalTime TIME\n    SeqNo INT\n    OfferOrders LONG[]\n    BidOrders LONG[]\n)\npartitioned by TradeDate, SecurityID,\nsortColumns=[`Market,`SecurityID,`TradeTime],\nkeepDuplicates=ALL\n```\n\n----------------------------------------\n\nTITLE: Incremental Backup of DolphinDB Distributed Table Partitions\nDESCRIPTION: Executes an incremental backup of a DolphinDB distributed table by calling backup on all partitions without forcing full backup (force=false). The SQL object covers the entire loadTable. Parallel processing is enabled for faster backup. The function returns the count of partitions backed up incrementally. Requires prior backup state to exist in backupDir to realize incremental effects.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/restore-backup.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\nbackup(backupDir=\"/hdd/hdd1/backup/\",sqlObj=<select *  from loadTable(\"dfs://ddb\",\"windTurbine\") >,force=false,parallel=true)\n```\n\n----------------------------------------\n\nTITLE: Generating Panel Data (3 factors) from Wide Tables\nDESCRIPTION: This snippet shows a SQL query that retrieves panel data for three factors from a wide table. It selects all columns from `tsdb_wide_min_factor` where the factor name is within a specified set of factors ('f0001', 'f0002', 'f0003').\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practice_for_factor_calculation.md#_snippet_27\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//宽表模式取面板数据sql\nwide_tsdb_factor_year=select * from tsdb_wide_min_factor where factorname in ('f0001','f0002','f0003')\n```\n\n----------------------------------------\n\nTITLE: Creating Tables in DolphinDB for Migration\nDESCRIPTION: DolphinDB script to create a partitioned table for storing migrated data. The script defines table schema with appropriate column names and data types, and creates a time-partitioned table in DolphinDB.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Migrate_data_from_InfluxDB_to_DolphinDB.md#_snippet_1\n\nLANGUAGE: dql\nCODE:\n```\nlogin(\"admin\",\"123456\")\n\ndbName=\"dfs://demo\"\ntbName=\"pt\"\n\ncolNames=`time`stateionID`grinding_time`oil_temp`pressure`pressure_target`rework_time`state\ncolTypes=[NANOTIMESTAMP,SYMBOL,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,SYMBOL]\nschemaTb=table(1:0,colNames,colTypes)\n\ndb = database(dbName, VALUE,2021.08.01..2022.12.31)\npt = db.createPartitionedTable(schemaTb, \"pt\", `time)\n```\n\n----------------------------------------\n\nTITLE: Creating QB Quote Table with TSDB Engine Partitioned by Day in DolphinDB\nDESCRIPTION: This snippet creates a DolphinDB database and table for storing QB quote market data. Partitioned by daily date range, the table records detailed bid and ask prices, yields, volumes, and quote statuses, with metadata like contributor ID and timestamps. It uses the TSDB engine optimized for time-series storage. Partitioning and sorting by securityID and market data time enables efficient querying of quotes by bond and time.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practices_for_partitioned_storage.md#_snippet_19\n\nLANGUAGE: DolphinDB\nCODE:\n```\ncreate database \"dfs://QB_QUOTE\"\npartitioned by VALUE(2023.10.01..2023.10.31)\nengine='TSDB'\n\ncreate table \"dfs://QB_QUOTE\".\"qbTable\"(\n\tSENDINGTIME TIMESTAMP\n\tCONTRIBUTORID SYMBOL\n\tMARKETDATATIME TIMESTAMP\n\tSECURITYID SYMBOL\n\tBONDNAME SYMBOL\n\tDISPLAYLISTEDMARKET SYMBOL\n\tBIDQUOTESTATUS INT\n\tBIDYIELD DOUBLE\n\tBIDPX DOUBLE\n\tBIDPRICETYPE INT\n\tBIDPRICE DOUBLE\n\tBIDDIRTYPRICE DOUBLE\n\tBIDVOLUME INT\n\tBIDPRICEDESC STRING\n\tASKQUOTESTATUS INT\n\tASKYIELD DOUBLE\n\tOFFERPX DOUBLE\n\tASKPRICETYPE INT\n\tASKPRICE DOUBLE\n\tASKDIRTYPRICE DOUBLE\n\tASKVOLUME INT\n\tASKPRICEDESC STRING\n)\npartitioned by MARKETDATATIME,\nsortColumns=[`SECURITYID,`MARKETDATATIME]\n```\n\n----------------------------------------\n\nTITLE: Loading Stock Data from DFS\nDESCRIPTION: Defines the list of stock symbols, the path to the distributed file system (DFS) database, and the table name containing the snapshot data. It then loads the specified table into a table object in the current session for further processing. The database and table names may need to be modified based on the specific environment.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/metacode_derived_features/metacode_derived_features.txt#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\nstockList=`601318`600519`600036`600276`601166`600030`600887`600016`601328`601288`600000`600585`601398`600031`601668`600048\ndbName = \"dfs://SH_TSDB_snapshot_MultiColumn\"\ntableName = \"snapshot\"\nsnapshot = loadTable(dbName, tableName)\n```\n\n----------------------------------------\n\nTITLE: Replaying Historical Trade and Snapshot Data (DolphinDB Script)\nDESCRIPTION: Defines a function that replays historical trade and snapshot datasets for backtesting or batch computation. Utilizes replayDS for realistic time partitioning and submits a replay job via submitJob, streaming the data to the message stream as if in real time. Dependencies are data availability in 'dfs://trade' and 'dfs://snapshot', and job resource parameters should be tuned based on hardware and data volume.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/stock_market_replay/02.calTradeCost_asofJoin.txt#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef replayStockMarketData(){\n\ttimeRS = cutPoints(09:15:00.000..15:00:00.000, 100)\n\ttradeDS = replayDS(sqlObj=<select * from loadTable(\"dfs://trade\", \"trade\") where Date = 2020.12.31>, dateColumn=`Date, timeColumn=`Time, timeRepartitionSchema=timeRS)\n\tsnapshotDS = replayDS(sqlObj=<select * from loadTable(\"dfs://snapshot\", \"snapshot\") where Date =2020.12.31>, dateColumn=`Date, timeColumn=`Time, timeRepartitionSchema=timeRS)\n\tinputDict = dict([\"trade\", \"snapshot\"], [tradeDS, snapshotDS])\n\t\n\tsubmitJob(\"replay\", \"replay for factor calculation\", replay, inputDict, messageStream, `Date, `Time, 100000, true, 2)\n}\nreplayStockMarketData()\n```\n\n----------------------------------------\n\nTITLE: Improving Query Readability and Reuse with With Statement in DolphinDB SQL\nDESCRIPTION: Displays the use of the with statement (Common Table Expressions, CTEs) to create reusable intermediate result sets that improve readability and execution efficiency of SQL queries. The example builds a stepwise filtered employee list meeting specific tenure and salary requirements, using temporary named tables and ordering the final result by salary descending. It highlights breaking complex queries into manageable parts.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Standard_SQL_in_DolphinDB.md#_snippet_4\n\nLANGUAGE: SQL\nCODE:\n```\n//search for the employees  that has worked more than 5 years and with a good salary\nwith  \n  employees_with_salary_increase as (  \n    select employee_id, salary, year(now()) as current_year,   \n           case when  year(now()) - year(hire_date) > 5 then 1 else 0 end as has_5_years  \n    from employees  \n    where department_id = 100  \n  ),  \n  employees_with_raise as (  \n    select employee_id, salary, has_5_years  \n    from employees_with_salary_increase  \n    where salary > 8000  \n    and has_5_years = 1  \n  )  \nselect employee_id, salary, has_5_years  \nfrom employees_with_raise  \norder by salary desc;\n```\n\n----------------------------------------\n\nTITLE: DolphinDB Filtering and Grouping by Date and Stock Code\nDESCRIPTION: This DolphinDB script filters data by date and then groups the results by stock code (PERMNO). It calculates the average of the 'VOL' column, filtering records where the date is greater than 2014.01.12.  The timer() function measures execution time across 10 runs. This tests combines filtering and grouping based on date and stock codes.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_46\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\n//按日期过滤，按股票代码分组\ntimer(10) select avg(VOL) from trades where date > 2014.01.12 group by PERMNO\n```\n\n----------------------------------------\n\nTITLE: Creating Partitioned Tables with Composite Partition Scheme Using Value and Value Partitioning in DolphinDB Script\nDESCRIPTION: Defines a composite partitioned database using two value-type partitions (date range and discrete symbols) and creates a partitioned table with specified columns and sort order. Dependencies include DolphinDB environment and appropriate database directory setup. Key parameters include partitionScheme specifying date and symbol partitions, and table schema columns with types representing stock market data. Input is empty table schema and partition parameters; output is a partitioned table in the database. It demonstrates how multi-level value partitioning may create fine-grained partitions leading to small partition sizes (error example 2.4.1).\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/database_and_table_creation_details.md#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndb1 = database(, VALUE, 2020.01.01..2021.01.01)\ndb2 = database(, VALUE, `a`b)\ndb = database(directory=\"dfs://testDB1\", partitionType=COMPO, partitionScheme=[db1, db2], engine=\"TSDB\")\ncolName = `SecurityID`TradeDate`TradeTime`TradePrice`TradeQty`TradeAmount`BuyNo`SellNo\ncolType = `SYMBOL`DATE`TIME`DOUBLE`INT`DOUBLE`INT`INT\ntbSchema = table(1:0, colName, colType)\ndb.createPartitionedTable(table=tbSchema, tableName=`pt, partitionColumns=`TradeDate`SecurityID, sortColumns=`SecurityID`TradeTime, keepDuplicates=ALL)\n```\n\n----------------------------------------\n\nTITLE: Calculating Price Changes with State Functions in DolphinDB\nDESCRIPTION: Defines a stateful function to calculate percentage price changes over different time periods using time-based window operations.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/Real-Time_Stock_Price_Increase_Calculation/03.注册流计算引擎和订阅流数据表.txt#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// calculate\n@state\ndef calculateChange(DateTime, LastPx, lag){\n\twindowFirstPx = tmfirst(DateTime, LastPx, lag)\n\tpreMinutePx = tmove(DateTime, LastPx, lag)\n\tprevLastPx = iif(preMinutePx == NULL, windowFirstPx, preMinutePx)\n\treturn 100 * (LastPx - prevLastPx) \\ prevLastPx\n}\n```\n\n----------------------------------------\n\nTITLE: Real-time Stream Calculation of Price Percent Change in DolphinDB\nDESCRIPTION: This code demonstrates the real-time stream calculation of price percent change using a reactive state engine. It defines input and output table schemas, creates the reactive state engine named \"reactiveDemo\", specifies the metric to calculate, and inserts sample data. The `createReactiveStateEngine` function connects the input stream, the calculation logic, and the output table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Streaming_computing_of_financial_quantifiers.md#_snippet_8\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 定义输入输出的表结构\ninputTable = table(1:0, `securityID`tradeTime`tradePrice`tradeQty`tradeAmount`buyNo`sellNo`tradeBSFlag`tradeIndex`channelNo, [SYMBOL,DATETIME,DOUBLE,INT,DOUBLE,LONG,LONG,SYMBOL,INT,INT])\nresultTable = table(10000:0, [\"securityID\", \"tradeTime\", \"factor\"], [SYMBOL, TIMESTAMP, DOUBLE])\n\n// 使用 createReactiveStateEngine 创建响应式状态引擎\ntry{ dropStreamEngine(\"reactiveDemo\")} catch(ex){ print(ex) }\nmetrics = <[tradeTime, pricePercentChange(tradePrice, 1)]>\nrse = createReactiveStateEngine(name=\"reactiveDemo\", metrics =metrics, dummyTable=inputTable, outputTable=resultTable, keyColumn=\"securityID\")\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 输入数据\ninsert into rse values(`000155, 2020.01.01T09:30:00, 30.85, 100, 3085, 4951, 0, `B, 1, 1)\ninsert into rse values(`000155, 2020.01.01T09:30:01, 30.86, 100, 3086, 4951, 1, `B, 2, 1)\ninsert into rse values(`000155, 2020.01.01T09:30:02, 30.80, 200, 6160, 5501, 5600, `S, 3, 1)\n\n// 查看结果\nselect * from resultTable\n/*\nsecurityID tradeTime               factor            \n---------- ----------------------- ------------------\n000155     2020.01.01T09:30:00.000                   \n000155     2020.01.01T09:30:01.000 0.0003\n000155     2020.01.01T09:30:02.000 -0.001944\n*/\n```\n\n----------------------------------------\n\nTITLE: Creating and Persisting 'tradeOriginalStream' Table - DolphinDB\nDESCRIPTION: Creates a high-volume stream table 'tradeOriginalStream' to store raw trade data with columns for instrument, market, time, price, quantity, and more. Shares and persists the table with compression and custom cache and retention settings. Column filter is set for 'SecurityID' for efficient streaming. Requires DolphinDB 1.30.18+.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/streaming_capital_flow_order_by_order/03.清理环境并创建相关流数据表.txt#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//create stream table: tradeOriginalStream\ncolName = `SecurityID`Market`TradeTime`TradePrice`TradeQty`TradeAmount`BuyNum`SellNum\ncolType = `SYMBOL`SYMBOL`TIMESTAMP`DOUBLE`INT`DOUBLE`INT`INT\ntradeOriginalStreamTemp = streamTable(1000000:0, colName, colType)\ntry{ enableTableShareAndPersistence(table=tradeOriginalStreamTemp, tableName=\"tradeOriginalStream\", asynWrite=true, compress=true, cacheSize=1000000, retentionMinutes=1440, flushMode=0, preCache=10000) } catch(ex){ print(ex) }\nundef(\"tradeOriginalStreamTemp\")\ngo\nsetStreamTableFilterColumn(tradeOriginalStream, `SecurityID)\n```\n\n----------------------------------------\n\nTITLE: Querying Publish Queue in DolphinDB\nDESCRIPTION: This snippet queries the publish queue in DolphinDB.  It uses the `getStreamingStat()` function to retrieve streaming statistics and then accesses the `pubConns` property to obtain information about the publish queue.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/streaming_capital_flow_order_by_order/07.流计算状态监控函数.txt#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\ngetStreamingStat().pubConns\n```\n\n----------------------------------------\n\nTITLE: Deleting Memory Table by Assigning NULL\nDESCRIPTION: Demonstrates deleting a memory table by assigning NULL to the variable, which retains the namespace but removes the table data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/partitioned_in_memory_table.md#_snippet_27\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntrades=NULL\n```\n\n----------------------------------------\n\nTITLE: Setting Stream Table Filter Column in DolphinDB Script\nDESCRIPTION: This snippet calls the `setStreamTableFilterColumn` function to designate the `SecurityID` column as the filter column for the `tradeOriginalStream` stream table. Setting the filter column enables efficient stream processing and subscription filtering by specific keys, in this case, the security identifier. This operation is performed after the stream tables are created and is part of stream table configuration in DolphinDB.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/streaming_capital_flow_daily/01.createStreamTB.txt#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\nsetStreamTableFilterColumn(tradeOriginalStream, `SecurityID)\n```\n\n----------------------------------------\n\nTITLE: DolphinDB cgroup by example with HINT_EXPLAIN\nDESCRIPTION: This example demonstrates the usage of `cgroup by` in DolphinDB and retrieves the execution plan using `HINT_EXPLAIN`. The query calculates the weighted average price (wavg) grouped by symbol and then context-grouped by minute. The execution plan shows the cost associated with the `cgroupBy` operation.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_Explain.md#_snippet_15\n\nLANGUAGE: DolphinDB\nCODE:\n```\nt = table(`A`A`A`A`B`B`B`B as sym, \n          09:30:06 09:30:28 09:31:46 09:31:59 09:30:19 09:30:43 09:31:23 09:31:56 as time, \n          10 20 10 30 20 40 30 30 as volume, \n          10.05 10.06 10.07 10.05 20.12 20.13 20.14 20.15 as price)\n\nselect [HINT_EXPLAIN] wavg(price, volume) as wvap from t \n\t\t\t\t\t group by sym \n\t\t\t\t\t cgroup by minute(time) as minute \n\t\t\t\t\t order by sym, minute\n```\n\n----------------------------------------\n\nTITLE: Extracting and Setting Up MySQL Database for DolphinScheduler - SQL\nDESCRIPTION: This snippet demonstrates SQL commands to create a MySQL database and user dedicated to DolphinScheduler's metadata persistence. It includes creating the database 'dolphinscheduler' with UTF-8 encoding, adding a user with a specified password, granting full privileges on the database to the user, and flushing privileges. These steps are prerequisites to configure DolphinScheduler to use MySQL for reliable data storage and must be run in a MySQL client environment.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/dolphinscheduler_integration.md#_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\n// 创建数据库\nCREATE DATABASE dolphinscheduler DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;\n// 创建用户，并设置密码\nCREATE USER 'dolphinscheduler'@'%' IDENTIFIED BY '密码';\n// 给用户赋予库的权限\nGRANT ALL PRIVILEGES ON dolphinscheduler.* TO 'dolphinscheduler'@'%';\nflush privileges;\n```\n\n----------------------------------------\n\nTITLE: Creating Half-Hour Frequency Factor Library Table with TSDB Engine in DolphinDB\nDESCRIPTION: This DolphinDB snippet creates a TSDB database and table for storing half-hour frequency factors. It partitions the database annually by tradetime date ranges combined with factor names, using composite partitioning for time dimension and factor. The table schema includes timestamp tradetime, security id, factor value, and factor name, and partitions by tradetime and factorname. The table is sorted by securityid and tradetime, includes duplicate rows, and uses a hash bucket sortKeyMappingFunction with dimension 500 for query performance optimization.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practices_for_partitioned_storage.md#_snippet_22\n\nLANGUAGE: DolphinDB\nCODE:\n```\ncreate database \"dfs://halfhourFactorDB\"\npartitioned by RANGE(date(datetimeAdd(1980.01M,0..80*12,'M'))), VALUE(`f1`f2),\nengine='TSDB'\n\ncreate table \"dfs://halfhourFactorDB\".\"halfhourFactorTB\"(\n    tradetime TIMESTAMP[comment=\"时间列\", compress=\"delta\"],\n    securityid SYMBOL,\n    value DOUBLE,\n    factorname SYMBOL,\n)\npartitioned by tradetime, factorname,\nsortColumns=[`securityid, `tradetime],\nkeepDuplicates=ALL,\nsortKeyMappingFunction=[hashBucket{, 500}]\n```\n\n----------------------------------------\n\nTITLE: Querying Top 10 Rows from OHLC DFS Table in DolphinDB Script\nDESCRIPTION: Executes a SQL query using `select top 10 *` to retrieve the first 10 rows from the \"dfs://stockFundOHLC\".\"stockFundOHLC\" distributed table. The results are loaded into the `data` variable in memory, typically used for verifying the data insertion.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/K.md#_snippet_8\n\nLANGUAGE: dolphindb\nCODE:\n```\ndata = select top 10 * from loadTable(\"dfs://stockFundOHLC\", \"stockFundOHLC\")\n```\n\n----------------------------------------\n\nTITLE: Adding Columns to Memory Table with SQL\nDESCRIPTION: Demonstrates adding new columns to a memory table using SQL UPDATE statements.  Shows how to calculate new columns based on existing ones.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/partitioned_in_memory_table.md#_snippet_15\n\nLANGUAGE: DolphinDB\nCODE:\n```\nupdate trades set logPrice1=log(price1), newQty1=double(qty1);\n```\n\n----------------------------------------\n\nTITLE: Connecting to DolphinDB in Oracle Mode - Java\nDESCRIPTION: This Java code snippet demonstrates how to connect to a DolphinDB instance using the Java API and specify the Oracle SQL dialect. It creates a `DBConnection` object with `SqlStdEnum.Oracle`, connects to the server, and executes a SQL query using the `decode` function.  The result is then printed to the console.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Standard_SQL_in_DolphinDB.md#_snippet_9\n\nLANGUAGE: java\nCODE:\n```\npackage com.dolphindb.sqlstd;\n\nimport com.xxdb.DBConnection;\nimport com.xxdb.comm.SqlStdEnum;\nimport com.xxdb.data.Entity;\nimport java.io.IOException;\n\npublic class OracleMode {\n    public static void main(String[] args) throws IOException {\n        DBConnection connection = new DBConnection(SqlStdEnum.Oracle);\n        connection.connect(\"192.168.1.206\", 11702, \"admin\", \"123456\");\n        String sql = String.format(\n                \"select employee_id, first_name, last_name, \\n\" +\n                \"  decode(job_id, 'IT_PROG' , 'Programmer', 'FI_ACCOUNT', 'Accountant', 'Others') as jobs_title\\n\" +\n                \"from loadTable(%s, %s) a\"\n                , \"\\\"dfs://hr\\\"\", \"\\\"employees\\\"\"\n        );\n        Entity result = connection.run(sql);\n        System.out.println(result.getString());\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Adjusting Time Variables by Time Units in DolphinDB Shell\nDESCRIPTION: This example showcases the `temporalAdd` function in DolphinDB to modify time variables by adding or subtracting different time units (week, month, minute). The function takes the time variable, the value to adjust by, and the time unit as arguments.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/date_time.md#_snippet_4\n\nLANGUAGE: Shell\nCODE:\n```\n>temporalAdd(2017.01.16,1,\"w\");\n2017.01.23\n>temporalAdd(2016.12M,2,\"M\");\n2017.02M\n>temporalAdd(13:30m,-15,\"m\");\n13:15m\n```\n\n----------------------------------------\n\nTITLE: 配置模拟撮合引擎\nDESCRIPTION: 创建一个配置字典，设置用户订单时延、orderbook匹配时的成交百分比和行情类别（逐笔或快照）。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/matching_engine_simulator.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\nconfig = dict(STRING, DOUBLE);\nconfig[\"latency\"] = 0;                     //用户订单时延为0\nconfig[\"orderBookMatchingRatio\"] = 1;      //与 orderbook匹配时的成交百分比\nconfig[\"dataType\"] = 0;                    //行情类别：0表示逐笔，1表示快照\n```\n\n----------------------------------------\n\nTITLE: Backing Up a DolphinDB Database\nDESCRIPTION: Demonstrates submitting a background job to back up an entire DolphinDB database specified by `dbPath` to a target directory `backupDir` using the `backupDB` function. This function supports incremental backup at the partition level and ensures data consistency at the start time of the backup.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/backup-restore-new.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndbPath=\"dfs://testdb\"\nbackupDir=\"/home/$USER/backupDB\"\nsubmitJob(\"backupDB\",\"backup testdb\",backupDB,backupDir,dbPath)\n```\n\n----------------------------------------\n\nTITLE: Wrapping OLS Output with Custom Function to Include R-squared Statistic in DolphinDB - DolphinDB\nDESCRIPTION: Defines a user-defined function 'myols' to run OLS regression returning regression coefficients and R-squared statistics combined as composite columns. The function calls 'ols' with a flag to compute statistics, then joins coefficient output with the first regression statistic (R2). This example demonstrates usage of user-defined functions to extend OLS output and integrate multiple result components as one composite output for grouped SQL queries. Input parameters are endogenous variable vector y and explanatory variables list x.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/hybrid_programming_paradigms.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef myols(y,x){\n    r=ols(y,x,true,2)\n    return r.Coefficient.beta join r.RegressionStat.statistics[0]\n}\nselect myols(y,[factor1,factor2]) as `alpha`beta1`beta2`R2 from t group by id;\n```\n\n----------------------------------------\n\nTITLE: Creating Database and Partitioned Table in DolphinDB Script\nDESCRIPTION: This DolphinDB script initializes a partitioned database and table to store synchronized closing price data before performing data import with dataX. It first checks if the database exists and, if so, deletes it to ensure a clean setup. The script utilizes RANGE partitioning on TradeDate and defines three columns (SecurityID, TradeDate, Value) with types SYMBOL, DATE, and DOUBLE, respectively. Input requirements include having administrative privileges on the DolphinDB server. Outputs are a new partitioned table ready for data ingestion. Limitation: Assumes no concurrent access during dropDatabase execution.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Python_Celery.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndbName = \"dfs://tick_close\"\ntbName = \"tick_close\"\nif(existsDatabase(dbName)){\n\tdropDatabase(dbName)\n}\ndb = database(dbName, RANGE, date(datetimeAdd(2000.01M,0..50*12,'M')))\nname = `SecurityID`TradeDate`Value\ntype = `SYMBOL`DATE`DOUBLE\nschemaTable = table(1:0, name, type)\ndb.createPartitionedTable(table=schemaTable, tableName=tbName, partitionColumns=`TradeDate)\n```\n\n----------------------------------------\n\nTITLE: DDBDataLoader Initialization Interface in Python\nDESCRIPTION: Defines the constructor signature for the DDBDataLoader Python class which connects to a DolphinDB session and loads data for deep learning iteratively as PyTorch tensors. It includes support for targeted columns, batch sizing, shuffling, sliding windows, device allocation, data grouping and repartitioning, as well as prefetch and thread pool tuning. This interface facilitates flexible configuration to handle large-factor data efficiently for model training pipelines.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_AI_DataLoader_for_Deep_Learning.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nDDBDataLoader(\n    ddbSession: Session,\n    sql: str,\n    targetCol: List[str],\n    batchSize: int = 1,\n    shuffle: bool = True,\n    windowSize: Union[List[int], int, None] = None,\n    windowStride: Union[List[int], int, None] = None,\n    *,\n    inputCol: Optional[List[str]] = None,\n    excludeCol: Optional[List[str]] = None,\n    repartitionCol: str = None,\n    repartitionScheme: List[str] = None,\n    groupCol: str = None,   \n    groupScheme: List[str] = None,\n    seed: Optional[int] = None,\n    dropLast: bool = False,\n    offset: int = None,\n    device: str = \"cpu\",\n    prefetchBatch: int = 1,\n    prepartitionNum: int = 2,\n    groupPoolSize: int = 3,\n    **kwargs\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Partitioned Table Using Composite Partitioning by Date and Symbol Hash Buckets in DolphinDB Script\nDESCRIPTION: Creates a composite partitioned database combining a daily value partition and a hash partition by stock symbol (25 partitions). Then defines a partitioned table with stock data fields. This setup reduces partition size to manageable in-memory chunks improving query concurrency and resource usage, addressing large partition granularity issues (2.5.2). Expected input includes date range and symbol hash partition count; output is a multi-level partitioned table enabling parallel queries.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/database_and_table_creation_details.md#_snippet_10\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//先按交易日期分区，然后按股票代码进行哈希分区\ndb1 = database(, VALUE, 2020.01.01..2021.01.01)\ndb2 = database(, HASH, [SYMBOL, 25])\ndb = database(directory=\"dfs://testDB2\", partitionType=COMPO, partitionScheme=[db1, db2])\ncolName = `SecurityID`TradeDate`TradeTime`TradePrice`TradeQty`TradeAmount`BuyNo`SellNo\ncolType = `SYMBOL`DATE`TIME`DOUBLE`INT`DOUBLE`INT`INT\ntbSchema = table(1:0, colName, colType)\n//每天只有 25 个分区，假设每天数据量为 3GB（内存中），每个分区大小约为120MB\ndb.createPartitionedTable(table=tbSchema, tableName=`pt, partitionColumns=`TradeDate`SecurityID)\n```\n\n----------------------------------------\n\nTITLE: Transforming and Synchronizing Data from SQL Server to DolphinDB - DolphinDB Script\nDESCRIPTION: Defines a data transformation function that converts SQL Server query results into DolphinDB-compatible formats, adjusting column types and adding placeholder columns. The synchronization function performs an ODBC query on specified data columns with batch loading and applies the transformation function, loading results into a DolphinDB table. The submitJob call runs the synchronization asynchronously.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/SQLServer_to_DolphinDB.md#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef transform(mutable msg){\n\tmsg.replaceColumn!(`LocalTime,time(temporalParse(msg.LocalTime,\"HH:mm:ss.nnnnnn\")))\n    msg.replaceColumn!(`Price,double(msg.Price))\n\tmsg[`SeqNo]=int(NULL)\n\tmsg[`DataStatus]=int(NULL)\n\tmsg[`BizIndex]=long(NULL)\n\tmsg[`Market]=\"SZ\"\n\tmsg.reorderColumns!(`ChannelNo`ApplSeqNum`MDStreamID`SecurityID`SecurityIDSource`Price`OrderQty`Side`TransactTime`OrderType`LocalTime`SeqNo`Market`DataStatus`BizIndex)\n    return msg\n}\n\ndef synsData(conn,dbName,tbName){\n    odbc::query(conn,\"select ChannelNo,ApplSeqNum,MDStreamID,SecurityID,SecurityIDSource,Price,OrderQty,Side,TransactTime,OrderType,LocalTime from data\",loadTable(dbName,tbName),100000,transform)\n}\n\nsubmitJob(\"synsData\",\"synsData\",synsData,conn,dbName,tbName)\n```\n\n----------------------------------------\n\nTITLE: Subscribing to Heterogeneous Stream Data in DolphinDB (Python)\nDESCRIPTION: This snippet subscribes to the `messageStream` table in DolphinDB and directs the incoming data to the `engine` for processing. The `offset` parameter is set to -1, indicating that the subscription starts from the current end of the stream table. The `msgAsTable` parameter is set to `true`, treating incoming messages as tables. `reconnect=true` ensures reconnection in case of failure.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/stock_market_replay.md#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nsubscribeTable(tableName=\"messageStream\", actionName=\"tradeJoinSnapshot\", offset=-1, handler=engine, msgAsTable=true, reconnect=true)\n```\n\n----------------------------------------\n\nTITLE: Creating ESP Quote Table with TSDB Engine in DolphinDB\nDESCRIPTION: This DolphinDB script creates a database and table for storing ESP quote data using the TSDB engine with daily partitions for one year. The schema contains comprehensive arrays and scalar types for multiple quote fields, clearing methods, delivery types, quote types, security metadata, and timestamps. Partitioning is by createDate and sorting by securityID and createTime supports efficient bond time-series queries.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practices_for_partitioned_storage.md#_snippet_17\n\nLANGUAGE: DolphinDB\nCODE:\n```\ncreate database \"dfs://ESP\"\npartitioned by VALUE(2023.01.01..2023.12.31)\nengine='TSDB'\n\ncreate table \"dfs://ESP\".\"ESPDepthtable\"(\n    createDate DATE[comment=\"创建日期\", compress=\"delta\"]  \n    createTime TIME[comment=\"创建时间\", compress=\"delta\"]\n    bondCodeVal SYMBOL\n    marketDepth LONG\n    marketIndicator LONG\n    mdBookType LONG\n    mdSubBookType LONG\n    messageId LONG\n    messageSource STRING\n    msgSeqNum LONG\n    msgType STRING\n    askclearingMethod LONG[]\n    bidclearingMethod LONG[]\n    askdeliveryType LONG[]\n    biddeliveryType LONG[]\n    askinitAccountNumSixCode LONG[]\n    bidinitAccountNumSixCode LONG[]\n    asklastPx DOUBLE[]\n    bidlastPx DOUBLE[]\n    askmdEntryDate DATE[]\n    bidmdEntryDate DATE[]\n    askmdEntrySize LONG[]\n    bidmdEntrySize LONG[]\n    askmdEntryTime TIME[]\n    bidmdEntryTime TIME[]\n    askmdQuoteType LONG[]\n    bidmdQuoteType LONG[]\n    askquoteEntryID LONG[]\n    bidquoteEntryID LONG[]\n    asksettlType LONG[]\n    bidsettlType LONG[]\n    askyield DOUBLE[]\n    bidyield DOUBLE[]\n    ask1initPartyTradeCode STRING\n    bid1initPartyTradeCode STRING\n    ask2initPartyTradeCode STRING\n    bid2initPartyTradeCode STRING\n    ask3initPartyTradeCode STRING\n    bid3initPartyTradeCode STRING\n    ask4initPartyTradeCode STRING\n    bid4initPartyTradeCode STRING\n    ask5initPartyTradeCode STRING\n    bid5initPartyTradeCode STRING\n    ask6initPartyTradeCode STRING\n    bid6initPartyTradeCode STRING\n    ask7initPartyTradeCode STRING\n    bid7initPartyTradeCode STRING\n    ask8initPartyTradeCode STRING\n    bid8initPartyTradeCode STRING\n    ask9initPartyTradeCode STRING\n    bid9initPartyTradeCode STRING\n    ask10initPartyTradeCode STRING\n    bid10initPartyTradeCode STRING\n    securityID SYMBOL\n    securityType STRING\n    senderCompID STRING\n    senderSubID STRING\n    sendingTime TIMESTAMP\n    symbol STRING\n)\npartitioned by createDate,\nsortColumns=[`securityID, `createTime]\n```\n\n----------------------------------------\n\nTITLE: Defining Partitioned DFS Database DolphinDB Script\nDESCRIPTION: Defines a composite DFS database named `dfs://iotDemoDB` with a two-level partitioning scheme. The first level partitions by VALUE based on date (`ts`), and the second level partitions by RANGE based on `hardwareId`, setting up the structure for efficient data storage and querying.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/iot_demo_script.txt#_snippet_7\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\ndb1 = database(\"\",VALUE, today()..(today()+30))\ndb2 = database(\"\",RANGE, 0..10*100)\ndb = database(\"dfs://iotDemoDB\",COMPO,[db1,db2])\n```\n\n----------------------------------------\n\nTITLE: Daily Aligned Rolling Window for Trading Sessions\nDESCRIPTION: Demonstrates the dailyAlignedBar function to handle market data with multiple trading sessions, calculating 7-minute average prices for futures market data with two trading sessions per day.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/window_cal.md#_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nsessions = 13:30:00 21:00:00\nts = 2021.11.01T13:30:00..2021.11.01T15:00:00 join 2021.11.01T21:00:00..2021.11.02T02:30:00\nts = ts join (ts+60*60*24)\nt = table(ts, rand(10.0, size(ts)) as price)\n\nselect avg(price) as price, count(*) as count from t group by dailyAlignedBar(ts, sessions, 7m) as k7\n```\n\n----------------------------------------\n\nTITLE: Calculating the average price using contextby\nDESCRIPTION: This code snippet uses the 'contextby' higher-order function to calculate the average price for each symbol in the table. It groups the data by the 'sym' column and then applies the 'avg' function to the 'price' column within each group.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_43\n\nLANGUAGE: shell\nCODE:\n```\nsym=`IBM`IBM`IBM`MS`MS`MS\nprice=172.12 170.32 175.25 26.46 31.45 29.43\nqty=5800 700 9000 6300 2100 5300\ntrade_date=2013.05.08 2013.05.06 2013.05.07 2013.05.08 2013.05.06 2013.05.07;\ncontextby(avg, price, sym);\n```\n\n----------------------------------------\n\nTITLE: Creating Database for Shanghai Stock Exchange Tick-by-Tick Trade Data with TSDB Engine\nDESCRIPTION: Creates a database for storing Shanghai Stock Exchange tick-by-tick trade data using a combined partitioning strategy with date value and HASH on symbols. The schema differs slightly from Shenzhen Exchange but follows the same partitioning approach.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practices_for_partitioned_storage.md#_snippet_5\n\nLANGUAGE: SQL\nCODE:\n```\ncreate database \"dfs://split_SH_TB\"\npartitioned by VALUE(2020.01.01..2021.01.01), HASH([SYMBOL, 25])\nengine='TSDB'\n\ncreate table \"dfs://split_SH_TB\".\"split_SH_tradeTB\"(\n    DataStatus INT\n    ApplSeqNum INT\n    ChannelNo INT\n    SecurityID SYMBOL\n    TradeDate DATE[comment=\"交易日期\", compress=\"delta\"]   \n    TradeTime TIME[comment=\"交易时间\", compress=\"delta\"]   \n    TradePrice DOUBLE\n    TradeQty LONG\n    TradeMoney DOUBLE\n    BidApplSeqNum LONG\n    OfferApplSeqNum LONG\n    TradeBSFlag SYMBOL\n    BizIndex LONG\n    LocalTime TIME\n    SeqNo INT\n)\npartitioned by TradeDate, SecurityID,\nsortColumns=[`SecurityID,`TradeTime],\nkeepDuplicates=ALL\n```\n\n----------------------------------------\n\nTITLE: Counting Distributed Table Query Frequency in DolphinDB\nDESCRIPTION: SQL query to count the number of SQL queries made against each distributed database and table, grouped by user. This information can be used for usage pattern analysis and cost allocation.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/user_level_resource_tracking.md#_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\nlogin(\"admin\", \"123456\")\n// Get data node aliases\ndataNodeAlias = exec name from rpc(getControllerAlias(), getClusterPerf) where mode in [0,3]\n// Count queries by database and table\nresult = select count(*) from pnodeRun(getUserTableAccessRecords{2023.12.13}, dataNodeAlias) where type=\"sql\" group by userId, database, table\nresult\n```\n\n----------------------------------------\n\nTITLE: Real-time Subscription of Calculation Results (Python)\nDESCRIPTION: This Python script uses the DolphinDB Python API to subscribe to the `capitalFlowStream` table in real-time. It establishes a streaming session, enables streaming on a specified port (8800), and subscribes to the DolphinDB server at a given host and port. The `resultProcess` function is defined as a callback handler to print received data. The subscription is filtered for `SecurityID` '600000'. The script keeps running using `Event().wait()` to continuously receive and process the streaming data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_capital_flow_order_by_order.md#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\n# -*- coding: utf-8 -*-\n\"\"\"\nDolphinDB python api version: 1.30.17.2\npython version: 3.7.8\nDolphinDB server version:1.30.18 or 2.00.5\nlast modification time: 2022.05.12\nlast modification developer: DolpinDB\n\"\"\"\nimport dolphindb as ddb\nimport numpy as np\nfrom threading import Event\n\ndef resultProcess(lst):\n    print(lst)\ns = ddb.session()\ns.enableStreaming(8800)\ns.subscribe(host=\"192.192.168.8\", port=8848, handler=resultProcess, tableName=\"capitalFlowStream\", actionName=\"SH600000\", offset=-1, resub=False, filter=np.array(['600000']))\nEvent().wait()\n```\n\n----------------------------------------\n\nTITLE: Unoptimized Moving Weighted Average Calculation Using Loop in DolphinDB\nDESCRIPTION: A non-optimized approach that calculates the moving weighted average for each stock using a loop, resulting in multiple full table scans and poor performance.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_SQL_Case_Tutorial.md#_snippet_17\n\nLANGUAGE: SQL\nCODE:\n```\narr = array(ANY, syms.size())\n\ntimer {\n\tfor(i in 0 : syms.size()) {\n\t\tprice_vec = exec price from t where symbol = syms[i]\n\t\tvolume_vec = exec volume from t where symbol = syms[i]\n\t\tarr[i] = mwavg(price_vec, volume_vec, 4)\n\t}\n\tres1 = reduce(join, arr)\n}\n```\n\n----------------------------------------\n\nTITLE: Signal Calculation with moving window\nDESCRIPTION: This DolphinDB script calculates a signal based on a moving window of closing prices.  It defines a function `rangeTest` that checks if a certain percentage (75%) of the closing prices within a window are within a specified range defined by `downAvgPrice` and `upAvgPrice`.  Then use moving function to perform signal calculation.  Requires columns close, downAvgPrice, and upAvgPrice.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_24\n\nLANGUAGE: shell\nCODE:\n```\ndefg rangeTest(close, downlimit, uplimit){\n  size = close.size() - 1\n  return between(close.subarray(0:size), downlimit.last():uplimit.last()).sum() >= size*0.75\n}\n\nupdate t set signal = moving(rangeTest, [close, downAvgPrice, upAvgPrice], 21)\n```\n\n----------------------------------------\n\nTITLE: Time-Based Sliding Window Calculation on Matrix with msum in DolphinDB\nDESCRIPTION: Demonstrates a time-based sliding window calculation on an indexed matrix `m` using the `msum` function. The window size is set to 3 days (`3d`), meaning the calculation considers data within the last 3 days for each row. Requires a pre-existing indexed matrix 'm' with datetime row indices.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/window_cal.md#_snippet_19\n\nLANGUAGE: DolphinDB\nCODE:\n```\nmsum(m,3d)\n```\n\n----------------------------------------\n\nTITLE: 初始化DolphinDB分布式逐笔成交数据库和表 - Python\nDESCRIPTION: 本代码演示如何用 Python 语法在 DolphinDB 中创建一个基于时间和股票代码分区的 TSDB 数据库与分区表，适用于存储逐笔成交数据。包含删除已存在数据库、数据库与表结构定义、分区键和压缩方法的设置，以及表内数据的排序配置。数据字段涵盖逐笔交易的基本信息。依赖 DolphinDB Python API，要求环境中已部署 DolphinDB Server 并正确配置 Python Parser。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_Python_Parser_Intro_for_Quantitative_Finance.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef createTB():\n    dbName, tbName = \"dfs://TL_Level2\", \"trade\"\n    # 数据库如果存在，删除该数据库\n    if existsDatabase(dbName):\n        dropDatabase(dbName)\n    # 创建数据库：时间维度按天 VALUE 分区 + 股票代码 SecurityID HASH 50 分区\n    db1 = database(\"\", ddb.VALUE, seq(2020.01.01, 2021.01.01))\n    db2 = database(\"\", ddb.HASH, [ddb.SYMBOL, 50].toddb())\n    db = database(dbName, ddb.COMPO, [db1, db2].toddb(), engine=\"TSDB\")\n    schemaTB = table(array(ddb.INT, 0) as ChannelNo,\n                    array(ddb.LONG, 0) as ApplSeqNum,\n                    array(ddb.SYMBOL, 0) as MDStreamID,\n                    array(ddb.LONG, 0) as BidApplSeqNum,\n                    array(ddb.LONG, 0) as OfferApplSeqNum,\n                    array(ddb.SYMBOL, 0) as SecurityID,\n                    array(ddb.SYMBOL, 0) as SecurityIDSource,\n                    array(ddb.DOUBLE, 0) as TradePrice,\n                    array(ddb.LONG, 0) as TradeQty,\n                    array(ddb.SYMBOL, 0) as ExecType,\n                    array(ddb.TIMESTAMP, 0) as TradeTime,\n                    array(ddb.TIME, 0) as LocalTime,\n                    array(ddb.LONG, 0) as SeqNo,\n                    array(ddb.INT, 0) as DataStatus,\n                    array(ddb.DOUBLE, 0) as TradeMoney,\n                    array(ddb.SYMBOL, 0) as TradeBSFlag,\n                    array(ddb.LONG, 0) as BizIndex,\n                    array(ddb.SYMBOL, 0) as OrderKind,\n                    array(ddb.SYMBOL, 0) as Market)\n    db.createPartitionedTable(schemaTB, tbName, partitionColumns=[\"TradeTime\", \"SecurityID\"].toddb(), compressMethods={\"TradeTime\":\"delta\"}.toddb(), sortColumns=[\"SecurityID\", \"TradeTime\"].toddb(), keepDuplicates=ddb.ALL)\n\ndef loadData(csvDir):\n    # 创建存储逐笔成交的库表\n    createTB()\n\n    # 读示例数据\n    name = [\"ChannelNo\", \"ApplSeqNum\", \"MDStreamID\", \"BidApplSeqNum\", \"OfferApplSeqNum\", \"SecurityID\", \"SecurityIDSource\", \"TradePrice\", \"TradeQty\", \"ExecType\", \"TradeTime\", \"LocalTime\", \"SeqNo\", \"DataStatus\", \"TradeMoney\", \"TradeBSFlag\", \"BizIndex\", \"OrderKind\", \"Market\"].toddb()\n    type = [\"INT\", \"LONG\", \"SYMBOL\", \"LONG\", \"LONG\", \"SYMBOL\", \"SYMBOL\", \"DOUBLE\", \"LONG\", \"SYMBOL\", \"TIMESTAMP\", \"TIME\", \"LONG\", \"INT\", \"DOUBLE\", \"SYMBOL\", \"LONG\", \"SYMBOL\", \"SYMBOL\"].toddb()\n    t = loadText(csvDir, schema=table(name, type))\n\n    # append! 数据入库\n    loadTable(\"dfs://TL_Level2\", \"trade\").append!(t)\n\n    # 统计库内数据量\n    rowCount = select count(*) from loadTable(\"dfs://TL_Level2\", \"trade\")      #\t181,683\n    print(rowCount)\n\n# 执行 loadData 函数, 需要将 csvDir 变量修改为 ddb 部署服务器上 csv 的实际目录\ncsvDir = \"/home/v2/下载/data/tradeData.csv\"\nloadData(csvDir)\n```\n\n----------------------------------------\n\nTITLE: Initializing Heterogeneous Replay in Python\nDESCRIPTION: This code demonstrates heterogeneous replay in DolphinDB, where data from tables with different structures is replayed into a single stream table (`messageStream`). It uses `replayDS` to create data sources for `order`, `trade`, and `snapshot` tables, and then the `replay` function is called with a dictionary as the `inputTables` parameter and the `messageStream` table as the `outputTables` parameter.  The dictionary keys identify data sources, and the `messageStream` table contains the data from all sources along with source identifiers.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/stock_market_replay.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\norderDS = replayDS(sqlObj=<select * from loadTable(\"dfs://order\", \"order\") where Date = 2020.12.31>, dateColumn=`Date, timeColumn=`Time)\ntradeDS = replayDS(sqlObj=<select * from loadTable(\"dfs://trade\", \"trade\") where Date = 2020.12.31>, dateColumn=`Date, timeColumn=`Time)\nsnapshotDS = replayDS(sqlObj=<select * from loadTable(\"dfs://snapshot\", \"snapshot\") where Date =2020.12.31>, dateColumn=`Date, timeColumn=`Time)\ninputDict = dict([\"order\", \"trade\", \"snapshot\"], [orderDS, tradeDS, snapshotDS])\nreplay(inputTables=inputDict, outputTables=messageStream, dateColumn=`Date, timeColumn=`Time, replayRate=10000, absoluteRate=true)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Real-time Factor Calculation with Stream Table in DolphinDB\nDESCRIPTION: Creates a stream table to store factor results, initializes a dictionary with historical net amount data, and subscribes to the level2 stream table to calculate real-time factors using the factorHandler function.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/quant_finance_examples.md#_snippet_25\n\nLANGUAGE: DolphinDB\nCODE:\n```\nshare(streamTable(100:0, `timestamp`symbol`value`factorName,[TIMESTAMP,SYMBOL,DOUBLE,SYMBOL]),\"FACTOR\")\n\nd = dict(STRING, ANY)\nhis = select symbol,volume * iif(bidPrice1>=askPrice1, 1, -1) as net_amount from loadTable(\"dfs://level2\",\"quotes\") context by symbol limit -200\nfor(id in his[`symbol].distinct())\n\td[id]= exec net_amount from his where symbol == id\n\nsubscribeTable(tableName=\"level2\", actionName=\"act_factor\", offset=0, handler=factorHandler{FACTOR,d,\"factor1\"}, msgAsTable=true, batchSize=4000, throttle=1)\n```\n\n----------------------------------------\n\nTITLE: Making Predictions and Calculating Accuracy\nDESCRIPTION: This snippet uses the trained logistic regression model to make predictions on the OHLC data for the 'AAPL' ticker symbol. It first preprocesses the data using the `preprocess` function and then uses the `predict` method of the model. The accuracy is calculated by comparing the predicted values with the actual target values.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/machine_learning.md#_snippet_10\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\naapl = preprocess(select * from ohlc where Ticker = `AAPL)\npredicted = model.predict(aapl)\nscore = sum(predicted == aapl.Target) \\ aapl.size()    // 0.756522\n```\n\n----------------------------------------\n\nTITLE: Performance Comparison: Row-wise vs Column-wise string operation\nDESCRIPTION: This DolphinDB script compares the performance of row-wise and column-wise string operations on a table. It uses `timer` to measure the execution time of the `each` function (row-wise) and column operations (column-wise). The column-wise approach is significantly faster.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_20\n\nLANGUAGE: shell\nCODE:\n```\nt=table(take(\"aaaa_bbbb\", 1000000) as str);\n\ntimer r = each(x -> split(x, '_').reverse().concat('_'), t[`str])\n\ntimer {\n\tpos = strpos(t[`str], \"_\")\n\tr = substr(t[`str], pos+1)+\"_\"+t[`str].left(pos)\n}\n```\n\n----------------------------------------\n\nTITLE: Update Factor in Wide Table (DolphinDB)\nDESCRIPTION: This function updates an existing factor's values in a wide table. It iterates through time partitions and symbols, calling `wideModelSinglePartitionUpdate` (not defined here) to update the factor values for each symbol within each partition. It can update either sequentially or in parallel using `ploop`. Dependencies: `getTimeList`, `wideModelSinglePartitionUpdate`, and `ploop`.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practices_for_multi_factor.md#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//宽表模型更新1个因子\ndef wideModelUpdateFactor(dbname,tbname,start_date,end_date,update_factor,symbols,parallel = true){  //parallel=true表示并行更新,=false表示串行\n\ttime_list = getTimeList(start_date,end_date)\n\tstart_time_list,end_time_list = [],[] \n\tfor(i in 0..(time_list.size()-1)){\n\t\tstart_time_list.append!(time_list[i][0])\n\t\tidx = time_list[i].size()-1\n\t\tend_time_list.append!(time_list[i][idx])\n\t}\n\tif(!parallel){\n\t\tfor(i in 0..(start_time_list.size()-1)){\n\t\t\tfor(j in 0..(symbols.size()-1)){\n\t\t\t\twideModelSinglePartitionUpdate(dbname,tbname,start_time_list[i],end_time_list[i],update_factor,symbols[j])\t\n\t\t\t}\n\t\t}\n\t}else{\n\t\tfor(i in 0..(start_time_list.size()-1)){\n\t\t\tploop(wideModelSinglePartitionUpdate{dbname,tbname,start_time_list[i],end_time_list[i],update_factor,},symbols)\n\t\t}\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: Defining a Net Amount Ratio Factor Calculation Function in DolphinDB\nDESCRIPTION: Creates a custom function to calculate the ratio between the sum of the last n and the last 2n elements in an array. This is used to compute a high-frequency factor based on net money flow, comparing recent flow to a longer historical period.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/quant_finance_examples.md#_snippet_24\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//定义函数calcNetAmountRatio，对一个向量求前n个与前2n个元素之和的比值：\ndefg calcNetAmountRatio(x,n){\n\tsize = x.size()\n\treturn x.subarray((size - n):size).sum()\\x.subarray((size - 2*n):size).sum()\n}\n\n//因子计算函数\ndef factorHandler(mutable factorTable,mutable d, facName,msg){\n\t\tcodeList = msg.symbol.distinct()\n\t\tsymbolCount = codeList.size()\n\t\t//资金净流入（net_amount）= volume * iif(bidPrice1>=askPrice1, 1, -1)\n\t\tt2 = select symbol, volume * iif(bidPrice1>=askPrice1, 1, -1) as net_amount from msg\n\t\t//将本次数据的计算net_amount追加更新字典\n\t\tdictUpdate!(d,append!, t2.symbol, t2.net_amount)\n\t\t//计算因子\n\t  \tfactorValue = array(DOUBLE,symbolCount)\n\t\tfor(i in 0:symbolCount){\n\t\t\tfactorValue[i] = calcNetAmountRatio(d[codeList[i]],100)\n\t\t}\n\t\t//添加时间戳，写入因子结果表\n\t\tfactorTable.append!(table(take(now(),symbolCount) as timestamp, codeList as symbol,factorValue as value, take(facName,symbolCount) as factorName))\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a DolphinDB table\nDESCRIPTION: Creates a table named 'config' within the 'configDB' database with columns for frequency (int), maxvoltage (float), and maxec (float).\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/cachedTable/mysql_data.txt#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\ncreate table config (frequency int,\nmaxvoltage float,\nmaxec float\n);\n```\n\n----------------------------------------\n\nTITLE: Setting Up Temperature Anomaly Detection Engine in DolphinDB\nDESCRIPTION: Creates an anomaly detection engine that monitors temperature readings. It triggers an alert when a device records more than 2 readings above 40°C and 3 readings above 30°C within a 2-minute window. The engine processes data in 30-second steps.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/alarm.txt#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\nengine = createAnomalyDetectionEngine(name=\"engine1\", metrics=<[sum(temperature > 40) > 2 && sum(temperature > 30) > 3  ]>, dummyTable=sensor, outputTable=warningTable, timeColumn=`ts,keyColumn=`deviceID ,windowSize= 120,step=30)\nsubscribeTable(tableName=\"sensor\", actionName=\"sensorAnomalyDetection\", offset=0, handler= append!{engine}, msgAsTable=true)\n```\n\n----------------------------------------\n\nTITLE: Logging Error During Table Append in DolphinDB Plugin Function C++\nDESCRIPTION: This snippet extends the above streaming handler to include error logging by utilizing the `Logger.h` header. It evaluates the success of the `append` operation; upon failure, it logs an error message with details. The implementation requires compilation with macro `-DLOGGING_LEVEL_2` to activate the logging functionality in the plugin.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_development_tutorial.md#_snippet_15\n\nLANGUAGE: cpp\nCODE:\n```\n// ...\nbool success = table->append(msgToAppend, insertedRows, errMsg);\nif (!success)\n\tLOG_ERR(\"Failed to append to table:\", errMsg);\n```\n\n----------------------------------------\n\nTITLE: Implementing Stateful Factor Calculations and Performance Testing using Reactive State Engine in DolphinDB Script\nDESCRIPTION: This code defines custom stateful functions annotated with @state for complex factor computations, including simple linear regression and multi-factor calculations using various discounted moving averages and correlations. It prepares dummy streaming data and tests engine performance with different data and factor scales on a single-thread setup. Key functions include `slr` for regression and `multiFactors` combining multiple indicators. Data preparation simulates realistic market tick data. Several reactive state engines are created, warmed up with historical data, then fed real-time-like data to benchmark computing latency. The results help assess computing cost across factor numbers and stock universes. Dependencies involve DolphinDB’s reactive engine APIs and streaming table primitives.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/reactive_state_engine.md#_snippet_11\n\nLANGUAGE: DolphinDB\nCODE:\n```\n@state\ndef slr(y, x){\n    alpha, beta = mslr(y, x, 12)\n    residual = mavg(y, 12) - beta * mavg(x, 12) - alpha\n    return alpha, beta, residual\n}\n\n@state\ndef multiFactors(lowPrice, highPrice, volumeTrade, closePrice, buy_active, sell_active, tradePrice, askPrice1, bidPrice1, askPrice10, agg_vol, agg_amt){\n    a = ema(askPrice10, 30)\n    term0 = ema((lowPrice - a) / (ema(highPrice, 30) - a), 50)\n    term1 = mrank((highPrice - a) / (ema(highPrice, 5) - a), true,  15)\n    term2 = mcorr(askPrice10, volumeTrade, 10) * mrank(mstd(closePrice, 20, 20), true, 10)\n    buy_vol_ma = mavg(buy_active, 6)\n    sell_vol_ma = mavg(sell_active, 6)\n    zero_free_vol = iif(agg_vol==0, 1, agg_vol)\n    stl_prc = ffill(agg_amt \\ zero_free_vol \\ 20).nullFill(tradePrice)\n    buy_prop = stl_prc\n\t\n    spd = askPrice1 - bidPrice1\n    spd_ma = round(mavg(iif(spd < 0, 0, spd), 6), 5)\n    term3 = buy_prop * spd_ma\n    term4 = iif(spd_ma == 0, 0, buy_prop / spd_ma)\n    return term0, term1, term2, term3, term4\n}\n\nmetrics = array(ANY, 14)\nmetrics[0] = <ema(1000 * sum_diff(ema(close, 20), ema(close, 40)),10) -  ema(1000 * sum_diff(ema(close, 20), ema(close, 40)), 20)>\nmetrics[1] = <mslr(high, volume, 8)[1]>\nmetrics[2] = <mcorr(low, high, 11)>\nmetrics[3] = <mstdp(low, 15)>\nmetrics[4] = <mbeta(high, value, 63)>\nmetrics[5] = <mcovar(low, value, 71)>\nmetrics[6] = <(close/mavg(close, 1..6)-1)*100>\nmetrics[7] = <mmin(high, 15)>\nmetrics[8] = <mavg(((high+low)/2+(mavg(high, 2)+mavg(low, 2))/2)*(high-low)/volume, 7, 2)>\nmetrics[9] = <mslr(mavg(close, 14), volume, 63)[1]>\nmetrics[10] = <mcorr(mavg(open, 25), volume, 71)>\nmetrics[11] = <mbeta(high, mstdp(close, 8), 77)>\nmetrics[12] = <slr(close, volume)>\nmetrics[13] = <multiFactors(low, high, volume, close, numTrade, numTrade, close, value, close, open, volume, numTrade)>\n\ndummy = streamTable(10000:0, `symbol`market`date`time`quote_type`preclose`open`high`low`close`numTrade`volume`value`position`recvtime,[SYMBOL,SHORT,DATE,TIME,SHORT,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,LONG,DOUBLE,LONG,TIMESTAMP])\n\ndef prepareData(tickNum, batch){\n    total = tickNum*batch\n    data=table(total:total, `symbol`market`date`time`quote_type`preclose`open`high`low`close`numTrade`volume`value`position`recvtime,[SYMBOL,SHORT,DATE,TIME,SHORT,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,LONG,DOUBLE,LONG,TIMESTAMP])\n    data[`market]=rand(10, total)\n    data[`date]=take(date(now()), total)\n    data[`time]=take(time(now()), total)\n    data[`symbol]=take(\"A\"+string(1..tickNum), total)\n    data[`open]=rand(100.0, total)\n    data[`high]=rand(100.0, total)\n    data[`low]=rand(100.0, total)\n    data[`close]=rand(100.0, total)\n    data[`numTrade]=rand(100, total)\n    data[`volume]=rand(100, total)\n    data[`value]=rand(100.0, total)\n    data[`recvtime]=take(now(), total)\n    return data\n}\n\ndropStreamEngine(\"demo1\")\ndropStreamEngine(\"demo2\")\ndropStreamEngine(\"demo3\")\ndropStreamEngine(\"demo4\")\n\n//4000个股票，20个因子\nhisData = prepareData(4000, 100)\nrealData = prepareData(4000, 1)\ncolNames = [\"symbol\"].append!(\"factor\"+string(0..19))\ncolTypes = [SYMBOL].append!(take(DOUBLE, 20))\nresultTable = streamTable(10000:0, colNames, colTypes)\nengine1 = createReactiveStateEngine(name=\"demo1\", metrics=metrics, dummyTable=dummy, outputTable=resultTable, keyColumn=\"symbol\")\nwarmupStreamEngine(engine1, hisData)\ntimer(10) engine1.append!(realData)\ndropAggregator(\"demo1\")\n\n//1个股票，20个因子\nhisData = prepareData(1, 100)\nrealData = prepareData(1, 1)\ncolNames = [\"symbol\"].append!(\"factor\"+string(0..19))\ncolTypes = [SYMBOL].append!(take(DOUBLE, 20))\nresultTable = streamTable(10000:0, colNames, colTypes)\nengine2 = createReactiveStateEngine(name=\"demo2\", metrics=metrics, dummyTable=dummy, outputTable=resultTable, keyColumn=\"symbol\")\nwarmupStreamEngine(engine2, hisData)\ntimer(10) engine2.append!(realData)\ndropAggregator(\"demo2\")\n\n//4000个股票，1个因子\nhisData = prepareData(4000, 100)\nrealData = prepareData(4000, 1)\nmetrics3 = metrics[0]\ncolNames = [\"symbol\", \"factor0\"]\ncolTypes = [SYMBOL, DOUBLE]\nresultTable = streamTable(10000:0, colNames, colTypes)\nengine3 = createReactiveStateEngine(name=\"demo3\", metrics=metrics3, dummyTable=dummy, outputTable=resultTable, keyColumn=\"symbol\")\nwarmupStreamEngine(engine3, hisData)\ntimer(10) engine3.append!(realData)\n\n//200个股票，20个因子\nhisData = prepareData(200, 100)\nrealData = prepareData(200, 1)\ncolNames = [\"symbol\"].append!(\"factor\"+string(0..19))\ncolTypes = [SYMBOL].append!(take(DOUBLE, 20))\nresultTable = streamTable(10000:0, colNames, colTypes)\nengine4 = createReactiveStateEngine(name=\"demo4\", metrics=metrics, dummyTable=dummy, outputTable=resultTable, keyColumn=\"symbol\")\nwarmupStreamEngine(engine4, hisData)\ntimer(10) engine4.append!(realData)\n```\n\n----------------------------------------\n\nTITLE: Define Weighted Average Price Calculation in DolphinDB\nDESCRIPTION: This code defines a function `weightedAveragedPrice` to calculate the weighted average price based on bid and offer prices and quantities. It implements the formula `(bidPrice0*offerOrderQty0 + offerPrice0*bidOrderQty0) / (offerOrderQty0+bidOrderQty0)`. This function calculates the price weighted by the order quantities on both the bid and offer sides.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Streaming_computing_of_financial_quantifiers.md#_snippet_9\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef weightedAveragedPrice(bidPrice0, bidOrderQty0, offerPrice0, offerOrderQty0){\n    return (bidPrice0*offerOrderQty0 + offerPrice0*bidOrderQty0) \\ (offerOrderQty0+bidOrderQty0)\n}\n```\n\n----------------------------------------\n\nTITLE: Comparing Performance of loadText vs ploadText in DolphinDB\nDESCRIPTION: This script measures and compares the time taken to load a large text file using the single-threaded `loadText` function versus the multi-threaded `ploadText` function. The `timer` command is used to record the elapsed time for each function call, demonstrating the performance benefits of parallel loading with `ploadText`.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_csv.md#_snippet_5\n\nLANGUAGE: dolphindb\nCODE:\n```\ntimer loadText(filePath);\n\ntimer ploadText(filePath);\n```\n\n----------------------------------------\n\nTITLE: Querying Top N Records (OLAP Engine)\nDESCRIPTION: This code snippet retrieves the latest 10 records for each SecurityID using the OLAP storage engine. It uses context by to group the data, csort to sort within each group, and limit to select the top 10.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_sql_cases.md#_snippet_9\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntimer t1 = select * from loadTable(\"dfs://Level1\", \"Snapshot\") where date(DateTime) = 2020.06.01 context by SecurityID csort DateTime limit -10\n```\n\n----------------------------------------\n\nTITLE: Creating Session Window Engine in DolphinDB\nDESCRIPTION: Creates a session window engine named `swEngine` to detect sensor data loss. It uses `stream01` as the input, `outputSt1` as the output, groups by `tag`, uses `ts` as the time column, and sets a session gap of 30000 milliseconds (30 seconds).\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_streaming_application_in_IOT.md#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\nswEngine = createSessionWindowEngine(name = \"swEngine\", sessionGap = 30000, metrics = < last(value)>,\n\tdummyTable = stream01, outputTable = outputSt1, timeColumn = `ts, keyColumn=`tag,useSessionStartTime=false)\n```\n\n----------------------------------------\n\nTITLE: Appending Processed Volatility Data to Table in DolphinDB\nDESCRIPTION: Loads the target table defined by `storeDBName` and `storeTBName` and appends the final, processed `result` table containing the calculated volatility features into it.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/machine_learning_volatility/01.dataProcess.txt#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\nloadTable(storeDBName, storeTBName).append!(result)\n```\n\n----------------------------------------\n\nTITLE: Logging into DolphinDB as Admin (DolphinDB Script)\nDESCRIPTION: Logs into the DolphinDB server using the default administrator credentials ('admin', '123456'). This is typically required before performing administrative tasks like creating databases or tables.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_implementation_for_NSQ_Plugin.md#_snippet_6\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nlogin(\"admin\", \"123456\")\n```\n\n----------------------------------------\n\nTITLE: Subscribing Stream Table to Distributed Table for Data Persistence in DolphinDB\nDESCRIPTION: This snippet sets up a subscription from a streaming table named 'snapshot' to a distributed database table using subscribeTable. It specifies action parameters including offset for message consumption, batch size controlling processing volume, and throttle for rate limiting. The handler parameter points to the loaded distributed table, and msgAsTable true indicates messages are passed as tables to the handler. This enables streaming market data to be continuously persisted into the distributed file system table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_implementation_for_AMD_Plugin.md#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 入库赋阅，以 snapshot 为例\npt = loadTable(dbName, snapshotTbName)\nsubscribeTable(tableName=snapshotTbName, actionName=\"saveSnapshotToDFS\", offset=-2, handler=pt, msgAsTable=true, batchSize=200000, throttle=60)\n```\n\n----------------------------------------\n\nTITLE: Creating a Composite Partitioned Table in DolphinDB - dolphindb\nDESCRIPTION: This snippet demonstrates the creation of a DolphinDB database with composite (COMPO) partitions: the first partition by date (VALUE type), and the second using a HASH partition on stock symbol with 40 partitions. It defines a detailed column schema aligning with the latest data format, customizes data types, and finally creates an empty partitioned table using the specified structure. Prerequisite: DolphinDB server environment. Inputs include date and symbol ranges; outputs in a distributed, partitioned table ready for bulk import. Constraints: The example partitions dates from 2020-01-01 to 2020-01-03 and supports up to 40 symbol partitions for example purposes.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/stockdata_csv_import_demo.md#_snippet_0\n\nLANGUAGE: dolphindb\nCODE:\n```\ndbDate = database(\"\", VALUE, 2020.01.01..2020.01.03)\ndbSymbol=database(\"\", HASH, [SYMBOL, 40])\ndb = database(dbName, COMPO, [dbDate, dbSymbol])\n\ncolumns = `Symbol`Market`DateTime`Status`PreClose`Open`High`Low`Price`Volume`Amount`AskPrice1`AskPrice2`AskPrice3`AskPrice4`AskPrice5`AskPrice6`AskPrice7`AskPrice8`AskPrice9`AskPrice10`BidPrice1`BidPrice2`BidPrice3`BidPrice4`BidPrice5`BidPrice6`BidPrice7`BidPrice8`BidPrice9`BidPrice10`AskVolume1`AskVolume2`AskVolume3`AskVolume4`AskVolume5`AskVolume6`AskVolume7`AskVolume8`AskVolume9`AskVolume10`BidVolume1`BidVolume2`BidVolume3`BidVolume4`BidVolume5`BidVolume6`BidVolume7`BidVolume8`BidVolume9`BidVolume10`TickCount`BidOrderTotalVolume`AskOrderTotalVolume`AvgBidOrderPrice`AvgAskOrderPrice`LimitHighestPrice`LimitLowestPrice\ntype=[SYMBOL,SYMBOL,DATETIME,INT,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,INT,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,INT,INT,INT,INT,INT,INT,INT,INT,INT,INT,INT,INT,INT,INT,INT,INT,INT,INT,INT,INT,INT,INT,INT,DOUBLE,DOUBLE,DOUBLE,DOUBLE]\norderData = table(1:0, columns,type)\ndb.createPartitionedTable(orderData, tableName,`DateTime`Symbol)\n\n```\n\n----------------------------------------\n\nTITLE: Interval Function for Time Interpolation\nDESCRIPTION: Shows how to use the interval function to handle data with gaps, performing value interpolation at regular intervals. The example fills missing values with previous values every 2 seconds.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/window_cal.md#_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nt=table(2021.01.01T01:00:00+(1..5 join 9..11) as time, take(`CLF1,8) as contract, 50..57 as price)\n\nselect last(contract) as contract, last(price) as price from t group by interval(time, 2s,\"prev\")\n```\n\n----------------------------------------\n\nTITLE: Performance comparison: moving(sum) vs msum\nDESCRIPTION: This DolphinDB script compares the performance of `moving(sum)` and `msum` for calculating a moving sum. It demonstrates that `msum` is significantly faster due to its optimized implementation and incremental calculation approach.  It measures the execution time using timer.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_26\n\nLANGUAGE: shell\nCODE:\n```\nx=1..1000000\ntimer moving(sum, x, 10)\ntimer msum(x, 10)\n```\n\n----------------------------------------\n\nTITLE: Restoring a DolphinDB Table\nDESCRIPTION: Demonstrates submitting background jobs to restore a specific table (`tbName`) from a backup (`backupDir`) using `restoreTable`. Examples show restoring to the original location (`dbPath`, `tbName`), restoring to a different database (`restoreDBPath`), and restoring to the same database but with a different table name (`restoreTb`). The function handles table creation and supports incremental restore.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/backup-restore-new.md#_snippet_9\n\nLANGUAGE: DolphinDB\nCODE:\n```\n# Restore table to new cluster (original db/table name)\ndbPath=\"dfs://testdb\"\ntbName=`quotes_2\nbackupDir=\"/home/$USER/backupTb\"\nsubmitJob(\"restoreTable\",\"restore quotes_2 in testdb to new cluster\",restoreTable,backupDir,dbPath,tbName)\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\n# Restore table to a different database\n# 将1.2中备份的数据恢复到 restoredb2 的 quotes_2 表中\ndbPath=\"dfs://testdb\"\ntbName=`quotes_2\nbackupDir=\"/home/$USER/backupTb\"\nrestoreDBPath=\"dfs://restoredb2\"\nsubmitJob(\"restoreTable2\",\"restore quotes_2 in testdb to quotes_2 in restoredb\",restoreTable,backupDir,dbPath,tbName,restoreDBPath)\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\n# Restore table to a different table name in the same database\n# 将1.2中备份的数据恢复到 testdb 的 quotes_restore 表中。\ndbPath=\"dfs://testdb\"\ntbName=`quotes_2\nbackupDir=\"/home/$USER/backupTb\"\nrestoreTb=\"quotes_restore\"\nsubmitJob(\"restoreTable3\",\"restore quotes_2 to quotes_restore in testdb\",restoreTable,backupDir,dbPath,tbName,,restoreTb)\n```\n\n----------------------------------------\n\nTITLE: Aggregate Calculations using Multi-column Macro Variables and byColumn\nDESCRIPTION: This snippet uses macro variables to calculate aggregate functions (sum and cumsum) over multiple columns.  It demonstrates two distinct ways of applying aggregate functions to multiple columns represented by a multi-column macro variable: `sum:V` and `cumsum`. The `sum:V`  is modified by the function pattern `byColumn`, the cumulative sum applies the function automatically to each element of a vector.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/macro_var_based_metaprogramming.md#_snippet_15\n\nLANGUAGE: DolphinDB\nCODE:\n```\nalias=[\"v1\", \"v2\"]\n<select sum:V(_$$names) as _$$alias from t>.eval()\n<select cumsum(_$$names) as _$$alias from t>.eval()\n```\n\n----------------------------------------\n\nTITLE: Simulating Stock Market Data Generation and Inserting into Partitioned Table (DolphinDB Script)\nDESCRIPTION: This DolphinDB script simulates 5000 stock symbols' one-day minute-level K-line (candlestick) data by generating randomized price and volume metrics. It constructs relevant columns such as SecurityID, DateTime (covering trading sessions), price fields (Open, High, Low, Last), and computed Amount. The script performs a join operation to merge these columns into a comprehensive table and appends it to the previously created partitioned table in the testDB database. This demonstrates writing large-scale time series data for clustering scenarios.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ha_cluster_deployment.md#_snippet_12\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 模拟数据并写入分区表\nn = 1210000\nrandPrice = round(10+rand(1.0, 100), 2)\nrandVolume = 100+rand(100, 100)\nSecurityID = lpad(string(take(0..4999, 5000)), 6, `0)\nDateTime = (2023.01.08T09:30:00 + take(0..120, 121)*60).join(2023.01.08T13:00:00 + take(0..120, 121)*60)\nPreClosePx = rand(randPrice, n)\nOpenPx = rand(randPrice, n)\nHighPx = rand(randPrice, n)\nLowPx = rand(randPrice, n)\nLastPx = rand(randPrice, n)\nVolume = int(rand(randVolume, n))\nAmount = round(LastPx*Volume, 2)\ntmp = cj(table(SecurityID), table(DateTime))\nt = tmp.join!(table(PreClosePx, OpenPx, HighPx, LowPx, LastPx, Volume, Amount))\ndbName = \"dfs://testDB\"\ntbName = \"testTB\"\nloadTable(dbName, tbName).append!(t)\n```\n\n----------------------------------------\n\nTITLE: Synchronizing MySQL Data to DolphinDB CachedTable Using DolphinDB Script\nDESCRIPTION: This DolphinDB script shows how to establish a connection to MySQL using the MySQL plugin, define a function to load the 'config' table from MySQL, and create a cachedTable in DolphinDB that updates every 60 seconds by calling this function. Dependencies include DolphinDB’s MySQL plugin loaded via loadPlugin and a running MySQL server with the appropriate database and table setup. The syncFunc function encapsulates the logic to fetch data from MySQL, and cachedTable manages the periodic synchronization and caching in memory. The script includes logging in to DolphinDB, plugin loading, and verification of results with select queries. The input is the MySQL table data, and output is a DolphinDB in-memory cached table containing synchronized data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/cachedtable.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(\"admin\",\"123456\")\n//加载MySQL插件\nloadPlugin(\"yourPluginsPath/mysql/PluginMySQL.txt\")\nuse mysql\n//自定义数据同步函数\ndef syncFunc(){\n\t//获取MySQL数据\n\tconn = mysql::connect(\"127.0.0.1\",3306,\"root\",\"123456\",\"configDB\")\n\tt = load(conn,\"config\")\n\n\t//返回表\n\treturn t\n}\n\nconfig=cachedTable(syncFunc,60)\n\nselect * from config\n```\n\n----------------------------------------\n\nTITLE: Complex Window Calculation (Alpha 98) in DolphinDB\nDESCRIPTION: This code defines functions to calculate the Alpha 98 factor using matrix operations on trade data. It includes functions for data preparation (`prepareDataForDDBPanel`), ranking (`myrank`), and the main Alpha 98 calculation (`alpha98Panel`). It processes panel data of vwap, open, and vol to generate the factor.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/window_cal.md#_snippet_18\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 输入表trade的schema如下，如需要可自行模拟数据:\n\nname       typeString typeInt \n---------- ---------- ------- \nts_code    SYMBOL     17             \ntrade_date DATE       6              \nopen       DOUBLE     16             \nvol        DOUBLE     16             \namount     DOUBLE     16   \n\n// alpha 98 的矩阵计算\n\ndef prepareDataForDDBPanel(){\n\tt = select trade_date,ts_code,amount*1000/(vol*100 + 1) as vwap,vol,open from trade \n\treturn dict(`vwap`open`vol, panel(t.trade_date, t.ts_code, [t.vwap, t.open, t.vol]))\n}\n\ndef myrank(x) {\n\treturn rowRank(x)\\x.columns()\n}\n\ndef alpha98Panel(vwap, open, vol){\n\treturn myrank(mavg(mcorr(vwap, msum(mavg(vol, 5), 26), 5), 1..7)) - myrank(mavg(mrank(9 - mimin(mcorr(myrank(open), myrank(mavg(vol, 15)), 21), 9), true, 7), 1..8))\n}\n\ninput = prepareDataForDDBPanel()\nalpha98DDBPanel = alpha98Panel(input.vwap, input.open, input.vol)\n```\n\n----------------------------------------\n\nTITLE: Calculating Alpha98 Factor Using DolphinDB SQL Approach\nDESCRIPTION: Implements the Alpha98 factor from WorldQuant using DolphinDB's SQL-like syntax with context by for grouping. The code defines a function accepting a table of stock data, applies moving averages and correlations, ranks, decay and context updating to generate the factor series. Dependencies: select, update, mavg, mcorr, msum, mrank, mimin, rank, context by. Inputs: table with at least securityid, tradetime, vwap, open, vol columns, pre-loaded from storage. Output: factor series aligned by securityid and tradetime. Suitable for stock panel data, requires DolphinDB table storage.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/panel_data.md#_snippet_30\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef alpha98(stock){\n\tt = select securityid, tradetime, vwap, open, mavg(vol, 5) as adv5, mavg(vol,15) as adv15 from stock context by securityid\n\tupdate t set rank_open = rank(open), rank_adv15 = rank(adv15) context by tradetime\n\tupdate t set decay7 = mavg(mcorr(vwap, msum(adv5, 26), 5), 1..7), decay8 = mavg(mrank(9 - mimin(mcorr(rank_open, rank_adv15, 21), 9), true, 7), 1..8) context by securityid\n\treturn select securityid, tradetime, rank(decay7)-rank(decay8) as A98 from t context by tradetime\n}\n\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nt = loadTable(\"dfs://k_day_level\",\"k_day\")\ntimer alpha98(t)\n```\n\n----------------------------------------\n\nTITLE: Random Forest Classification\nDESCRIPTION: This snippet trains a random forest classification model using the `randomForestClassifier` function.  It uses a SQL data source (`sqlDS`) from the `wineTrain` table, specifies the dependent variable (`Label`) and independent variables, and sets the number of classes to 3. The trained model is stored in the `model` variable.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/machine_learning.md#_snippet_3\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nmodel = randomForestClassifier(\n    sqlDS(<select * from wineTrain>),\n    yColName=`Label,\n    xColNames=`Alcohol`MalicAcid`Ash`AlcalinityOfAsh`Magnesium`TotalPhenols`Flavanoids`NonflavanoidPhenols`Proanthocyanins`ColorIntensity`Hue`OD280_OD315`Proline,\n    numClasses=3\n)\n```\n\n----------------------------------------\n\nTITLE: Dictionary-based Computation with Table Partitioning by Stock Symbol in DolphinDB\nDESCRIPTION: Demonstrates how to create and update a dictionary where keys are stock symbols and values are subtables containing only records for that symbol. Uses the dictUpdate! function with custom lambda functions for initialization and updating.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_65\n\nLANGUAGE: DolphinDB\nCODE:\n```\norders = table(`IBM`IBM`IBM`GOOG as SecID, 1 2 3 4 as Value, 4 5 6 7 as Vol)\nhistoryDict = dict(STRING, ANY)\nhistoryDict.dictUpdate!(function=def(x,y){tableInsert(x,y);return x}, keys=orders.SecID, parameters=orders,\n            initFunc=def(x){t = table(100:0, x.keys(), each(type, x.values())); tableInsert(t, x); return t})\n```\n\n----------------------------------------\n\nTITLE: Cleaning DolphinDB Environment\nDESCRIPTION: This DolphinDB script logs in with specified credentials, undefines all user-defined variables and functions (`undef(all)`), and clears all cached data (`clearAllCache()`) to ensure a clean environment before running the main example script.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/PIP_in_DolphinDB.md#_snippet_1\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nlogin('admin', '123456')\nundef(all)\nclearAllCache()\n```\n\n----------------------------------------\n\nTITLE: Calculating Annualized Return in DolphinDB\nDESCRIPTION: Defines a DolphinDB function `getAnnualReturn` to calculate the annualized return of a fund. It takes a vector `value` (representing daily net values) as input and computes the return based on the first and last values, annualizing it assuming 252 trading days over a 730-day period.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/fund_factor_contrasted_by_py.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndefg getAnnualReturn(value){\n      return pow(1 + ((last(value) - first(value))\\first(value)), 252\\730) - 1\n}\n```\n\n----------------------------------------\n\nTITLE: Frequency Conversion with asfreq and fill Functions in DolphinDB\nDESCRIPTION: This snippet demonstrates increasing the frequency of an indexedSeries using asfreq, followed by forward filling of NULLs. Requires an indexedSeries with time-like indexes and use of asfreq and ffill. Input: series with values at sparse time points; asfreq creates a regular time index of finer granularity with NULLs for missing values. Output: densified series where intermediate points adopt the previous non-NULL value. Aggregation is not possible in asfreq, unlike resample.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/panel_data.md#_snippet_23\n\nLANGUAGE: DolphinDB\nCODE:\n```\nindex=2020.01.01 2020.01.05 2020.01.10\ns=indexedSeries(index, take(1,size(index)));\ns.asfreq(\"D\").ffill()\n```\n\n----------------------------------------\n\nTITLE: Defining PIP Downsampling Algorithm Function in DolphinDB Script\nDESCRIPTION: Defines a user-defined aggregate function `PIP(X, Y)` in DolphinDB script to implement the Perceptually Important Points (PIP) downsampling algorithm. It takes time-series coordinates X and Y as input, iteratively selects 'k' (hardcoded as 5 in this example) most significant points based on vertical distance, and returns a comma-separated string representation of the selected (x, y) coordinates. The function uses a `do-while` loop and vector operations for efficiency.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/PIP_in_DolphinDB.md#_snippet_0\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\ndefg PIP(X, Y){\n    data = table(X as x, Y as y)\n    rowTab = select x, y, rowNo(x) as rowNo from data\n    n = size(X)\n// k为每次滑动降采样的数据量，取值范围为 3~n（n 为滑动窗口大小） \n    k = 5  \n    samples = 2\n    result = string(X[0])+\"_\"+string(Y[0])\n    indexList = [0,n-1]\n    do{\n        distanceVec = [0.0]\n        for (i in 0..(size(indexList)-2)){\n            start, end = indexList[i], indexList[i+1]\n            x1, y1 = X[start], Y[start]\n            x2, y2 = X[end], Y[end]\n            a = (y1-y2)/(x1-x2)\n            b = y1-a*x1\n            distanceVec=join(distanceVec, abs(Y[(start+1): end] - (X[(start+1): end]*a + b)))\n            distanceVec.append!(0)\n        }\n        distanceMax = distanceVec.max()\n        tmp = table(rowTab, distanceVec as distance)\n        nextPoint = select x, y, rowNo from tmp where distance = distanceMax\n        result += \",\"+string(nextPoint.x[0])+\"_\"+string(nextPoint.y[0])\n        indexList = indexList.append!(nextPoint.rowNo[0]).sort()\n        samples = samples+1\n    }while(samples < k)\n    result += \",\"+string(X.last())+\"_\"+string(Y.last())\n    return result\n}\n```\n\n----------------------------------------\n\nTITLE: Verifying Data Import in DolphinDB\nDESCRIPTION: Executes a SQL query using DolphinDB's `loadTable` function to access the specified table (`dbName`, `tbName`) and counts the number of records, grouped by the date extracted from the `DateTime` column. This is used to confirm that data has been successfully imported into the table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Real-Time_Stock_Price_Increase_Calculation.md#_snippet_2\n\nLANGUAGE: DolphinDB (SQL)\nCODE:\n```\nselect count(*) from loadTable(dbName, tbName) group by date(DateTime) as TradeDate\n```\n\n----------------------------------------\n\nTITLE: Preparing and Uploading DataFrames to DolphinDB Streaming Tables - Python\nDESCRIPTION: This snippet demonstrates creating two Pandas DataFrames with random data representing tick information, converting date columns and uploading them to DolphinDB streaming tables. It shows the recommended way to append time-typed columns: upload the data frame, select and convert columns on the DolphinDB side, and finally insert into the target stream table. This prevents type mismatches. Dependencies: numpy, pandas, and a connected DolphinDB session. Main parameters: DataFrame structure, correct date conversion, server table names. Outputs the row count after insertion for verification.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_tutorial.md#_snippet_25\n\nLANGUAGE: Python\nCODE:\n```\nn = 10000\ndfd = pd.DataFrame({'Code': np.repeat(['SH000001', 'SH000002', 'SH000003', 'SH000004', 'SH000005'], n/5),\n                    'Date': np.repeat(pd.date_range('1990.01.01', periods=10000, freq='D'), n/10000),\n                    'DiffAskVol': np.random.choice(100, n),\n                    'DiffAskVolSum': np.random.choice(100, n),\n                    'DiffBidVol': np.random.choice(100, n),\n                    'DiffBidVolSum': np.random.choice(100, n),\n                    'FirstDerivedAskPrice': np.random.choice(100, n)*0.9,\n                    'FirstDerivedAskVolume': np.random.choice(100, n),\n                    'FirstDerivedBidPrice': np.random.choice(100, n)*0.9,\n                    'FirstDerivedBidVolume': np.random.choice(100, n)})\n\nn = 20000\ndff = pd.DataFrame({'Code': np.repeat(['SZ000001', 'SZ000002', 'SZ000003', 'SZ000004', 'SZ000005'], n/5),\n                    'Date': np.repeat(pd.date_range('1990.01.01', periods=10000, freq='D'), n/10000),\n                    'DiffAskVol': np.random.choice(100, n),\n                    'DiffAskVolSum': np.random.choice(100, n),\n                    'DiffBidVol': np.random.choice(100, n),\n                    'DiffBidVolSum': np.random.choice(100, n),\n                    'FirstDerivedAskPrice': np.random.choice(100, n)*0.9,\n                    'FirstDerivedAskVolume': np.random.choice(100, n),\n                    'FirstDerivedBidPrice': np.random.choice(100, n)*0.9,\n                    'FirstDerivedBidVolume': np.random.choice(100, n)})\n```\n\nLANGUAGE: Python\nCODE:\n```\ndbDir = \"dfs://ticks\"\ntableName = 'tick'\ns.upload({'dfd': dfd, 'dff': dff})\ninserts = \"\"\"tableInsert(mem_stream_d,select Code,date(Date) as Date,DiffAskVol,DiffAskVolSum,DiffBidVol,DiffBidVolSum,FirstDerivedAskPrice,FirstDerivedAskVolume,FirstDerivedBidPrice,FirstDerivedBidVolume from dfd);\ntableInsert(mem_stream_f,select Code,date(Date) as Date,DiffAskVol,DiffAskVolSum,DiffBidVol,DiffBidVolSum,FirstDerivedAskPrice,FirstDerivedAskVolume,FirstDerivedBidPrice,FirstDerivedBidVolume from dff)\"\"\"\ns.run(inserts)\ns.run(\"select count(*) from loadTable('{dbPath}', `{tbName})\".format(dbPath=dbDir,tbName=tableName))\n\n# output\n   count\n0  30000\n```\n\n----------------------------------------\n\nTITLE: Validating K-Minute Line Factor Price and Volume Indicators in DolphinDB - DolphinDB\nDESCRIPTION: Contains two functions to validate key K-minute line factor indicators for a given date. The first checks if any records report a highest price less than the lowest price, which indicates calculation errors. The second checks for inconsistent zero values between volume and amount fields, flagging logical errors where one is zero and the other is not. Both load the specified table from the database and throw descriptive errors if violations are found.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/dolphinscheduler_integration.md#_snippet_16\n\nLANGUAGE: DolphinDB\nCODE:\n```\nmodule dataCheck::minFactorCheck\n\ndef checkHighLowPrice(idate,dbName,tbName)\n{\n\t// 分钟线最高价指标与最低价指标校验\n\ttb= loadTable(dbName,tbName)\n\ttemp=select * from tb where tradedate=idate and High < Low \n\tif(size(temp)>0)\n\t{\n\t\tthrow \"分钟线计算错误！分钟线最高价小于最低价！\"\n\t}\n}\n\ndef checkVolumeAmount(idate,dbName,tbName)\n{\n\t// 分钟线交易量与交易额指标校验\n\ttb = loadTable(dbName,tbName)\n\ttemp = select * from loadTable(dbName,tbName) where tradedate=idate and ((Volume == 0 and Amount != 0) or (Volume != 0 and Amount == 0))\n\tif(size(temp)>0)\n\t{\n\t\tthrow \"分钟线计算错误！交易量和交易额不同时为0！\"\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: Calculating Weighted Sum Across Multiple Columns Using DolphinDB Function-Based Metaprogramming\nDESCRIPTION: This snippet implements a dynamic weighted sum calculation across 10 columns in a table using DolphinDB metaprogramming functions. It leverages binary expressions generated by binaryExpr to multiply each column by its corresponding weight, and unifiedExpr to sum these intermediate results into a final weighted value. The weights must be declared as a tuple to ensure correct element-wise multiplication rather than vector multiplication. Inputs include a table with columns named val1 to val10 and a weight vector. The output is a table extended with a new weightedVal column representing the weighted sum. This approach enables flexible and scalable construction of complex expressions by dynamically generating SQL snippets and evaluating them.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/macro_var_based_metaprogramming.md#_snippet_25\n\nLANGUAGE: DolphinDB\nCODE:\n```\nt = table(rand(10.0, 100) $ 10:10).rename!(\"val\" + string(1..10))\ncols = \"val\"+string(1..10)\nw = (1..10) \\ 10 $ ANY\n\nslt = sqlColAlias(unifiedExpr(binaryExpr(sqlCol(cols), w, *), take(+,cols.size()-1)),\"weightedVal\")\nsql(select=[sqlCol(\"*\"), slt], from=t).eval()\n```\n\n----------------------------------------\n\nTITLE: Updating Wine Data Labels\nDESCRIPTION: This code updates the 'Label' column in the 'wine' table by subtracting 1 from each value. This is done to adjust the class labels from 1, 2, 3 to 0, 1, 2, which is required by the `randomForestClassifier` function. The operation is performed directly on the 'wine' table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/machine_learning.md#_snippet_1\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nupdate wine set Label = Label - 1\n```\n\n----------------------------------------\n\nTITLE: Detecting State Changes in Real-Time using DolphinDB Stream Engine\nDESCRIPTION: This script sets up a real-time stream processing pipeline in DolphinDB to detect changes in device state. It defines an input stream table 'mainStream' and an output table 'calc_result'. A ReactiveState engine 'engine_status' is created to filter records where 'propertyValue' differs from the previous value for the same 'propertyCode', effectively capturing state changes. Data is simulated using 'replay' and results are published to 'calc_result', which can be subscribed to by external applications (like Python).\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Iot_intelligent_O&M.md#_snippet_11\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\n//案例 11：根据状态突变，抽取记录（实时流计算）\n\n//新建流数据表，接收实时写入数据\nshare streamTable(20000:0,pt.schema().colDefs.name,pt.schema().colDefs.typeString) as mainStream\n\n//新建流数据表，保存处理后结果数据\nshare streamTable(20000:0,`propertyCode`ts`deviceCode`logicalPositionId`physicalPositionId`propertyValue,[SYMBOL,DATETIME,SYMBOL,INT,INT,INT]) as calc_result\n\n//使用流数据异常检测引擎，进行数据筛选处理\nengine = createReactiveStateEngine(\"engine_status\", <[ts,deviceCode,logicalPositionId,physicalPositionId,propertyValue]>, mainStream, calc_result,`propertyCode,<propertyValue!=prev(propertyValue)>)\n\n//开启订阅\nsubscribeTable(tableName=\"mainStream\", actionName=\"act_subscribe_calculate\", offset=0, handler=append!{engine}, msgAsTable=true, batchSize=10000, throttle=1,hash=1);\n\n//使用回放，模拟持续写入流数据表的场景\ndt=select * from pt where deviceCode=\"361RP17\" and propertyCode=\"361RP17002\" and ts between 2022.01.01 00:00:00:2022.01.01 01:00:00 order by ts\nupdate!(dt,`propertyValue,rand(0 1,dt.size()))\n\n//批处理调用 replay 函数，后台执行回放\nrate=5*60    //回放倍速（每秒播放5分钟的数据，每小时数据12秒执行完毕）\nsubmitJob(\"replay_output\", \"replay_output_stream\", replay, dt ,mainStream, `ts, `ts, rate,false)\n```\n\n----------------------------------------\n\nTITLE: 使用asof join关联交易价格与最新报价\nDESCRIPTION: 使用asof join函数将交易记录与报价记录进行关联，查找每笔交易前的最新报价数据。asof join在处理不同时间点的关联数据时性能极高，比pandas快约200倍。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/quant_finance_examples.md#_snippet_10\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntrades=table(`600000`600300`600800 as symbol, take(2020.06.01,3) as date, [14:35:18.000, 14:30:30.000, 14:31:09.000] as time, [10.63, 3.12, 4.72] as tradePrice)\nquotesTmp=select symbol, date, time, bidPrice1, askPrice1 from quotes where symbol in `600000`600300`600800 and date=2020.06.01\nselect * from aj(trades, quotesTmp, `symbol`date`time)\n```\n\n----------------------------------------\n\nTITLE: Generating Multiple Sample CSV Files in DolphinDB\nDESCRIPTION: This script generates 100 separate CSV files, each containing 100,000 rows of sample trade data. It uses a `for` loop and the `saveText` function to create files named multiImport_1.csv, multiImport_2.csv, etc., in the specified directory.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_csv.md#_snippet_6\n\nLANGUAGE: dolphindb\nCODE:\n```\nn=100000\ndataFilePath=\"/home/data/multi/multiImport_\"+string(1..100)+\".csv\"\nfor (i in 0..99){\n    trades=table(sort(take(100*i+1..100,n)) as id,rand(`IBM`MSFT`GM`C`FB`GOOG`V`F`XOM`AMZN`TSLA`PG`S,n) as sym,take(2000.01.01..2000.06.30,n) as date,10.0+rand(2.0,n) as price1,100.0+rand(20.0,n) as price2,1000.0+rand(200.0,n) as price3,10000.0+rand(2000.0,n) as price4,10000.0+rand(3000.0,n) as price5)\n    trades.saveText(dataFilePath[i])\n};\n```\n\n----------------------------------------\n\nTITLE: Querying with last() function and group by\nDESCRIPTION: Queries the latest data using the `last()` function with `group by`.  A time filter is used to limit the scope of the query. The `timer` keyword is used to measure the execution time.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/snapshot_engine.md#_snippet_8\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\ntimer select last(ts), last(tag0001), last(tag0002), last(tag0003)\nfrom loadTable('dfs://testDB', 'trainInfoTable') where ts > datetimeAdd(now(),-1,`h) group by trainID\n```\n\n----------------------------------------\n\nTITLE: Installing Required Oracle Libraries via yum - Shell\nDESCRIPTION: Installs missing library dependencies for Oracle Instant Client using yum (for RHEL/CentOS). Also creates a symbolic link for libodbcinst if required. May be necessary for resolving shared library errors during ODBC driver usage.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Oracle_to_DolphinDB.md#_snippet_10\n\nLANGUAGE: Shell\nCODE:\n```\nyum install libaio\nyum install libnsl\n# 有些是缺少链接，而不是缺少文件\nln -s libodbcinst.so.2.0.0 libodbcinst.so.1\n\n```\n\n----------------------------------------\n\nTITLE: Verifying Loaded Historical Data (DolphinDB Script)\nDESCRIPTION: A simple DolphinDB query to verify the successful import of historical data into the `dfs://trade/trade` table. It counts the total number of records grouped by the date part of the `TradeTime` column, providing a quick check on the loaded data volume per day.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_capital_flow_order_by_order.md#_snippet_3\n\nLANGUAGE: dolphindb\nCODE:\n```\nselect count(*) from loadTable(\"dfs://trade\", \"trade\") group by date(TradeTime) as TradeDate\n```\n\n----------------------------------------\n\nTITLE: Calculating Matrix Eigenvalues and Eigenvectors in DolphinDB\nDESCRIPTION: This snippet shows how to calculate the eigenvalues and eigenvectors of a matrix in DolphinDB using the `eig(X)` function. It demonstrates creating a matrix, calling `eig`, and accessing the 'values' (eigenvalues) and 'vectors' (eigenvectors) from the returned dictionary structure. The function returns a dictionary with specific keys.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/matrix.md#_snippet_33\n\nLANGUAGE: DolphinDB\nCODE:\n```\n>m=matrix([2 5 7 5, 5 2 5 4, 8 2 6 4, 7 8 6 8]);\n>m;\n#0 #1 #2 #3\n-- -- -- --\n2  5  8  7 \n5  2  2  8 \n7  5  6  6 \n5  4  4  8 \n\n>v=eig(m);\n>v[`values];\n[19.750181,-3.807927,-1.551136,3.608882]\n\n>v[`vectors];\n#0       #1        #2        #3       \n-------- --------- --------- ---------\n0.485606 -0.853982 -0.034777 -0.183556\n0.413406 0.301775  -0.845881 -0.15004 \n0.553396 0.40595   0.507665  -0.520802\n0.535757 0.121868  0.15985   0.820098 \n```\n\n----------------------------------------\n\nTITLE: Batch Loading Daily CSV Order Book Files into DolphinDB Table\nDESCRIPTION: Function loadOneDayFiles scans a directory path for CSV files matching the '%.csv' pattern, chunks them into batches of 100 files each for performance consideration, loads each CSV using loadOneFile, and appends the loaded data into the target DolphinDB table. It initializes a temporary in-memory table per batch for aggregation. Errors during loading individual files are caught and printed but do not stop the batch process.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/csvImportDemo/importOldData.txt#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef loadOneDayFiles(dbName,tableName,path,schema1){\n\ttb = loadTable(dbName,tableName)\n\tfileList = exec filename from files(path, \"%.csv\")\n\tfs= fileList.cut(100)\n\tfor(i in 0:fs.size()){\n\t\tbigTable=table(500000:0,tb.schema().colDefs[`name],tb.schema().colDefs[`typeString])\n\t\tfor(f in fs[i])\t{\n\t\t\ttry\t{\n\t\t\t\tbigTable.append!(loadOneFile(path+\"/\"+f,bigTable,schema1))\n\t\t\t}\n\t\t\tcatch(ex){\n\t\t\t\tprint f + \": \"+ex\n\t\t\t}\n\n\t\t}\n\t\ttb.append!(bigTable)\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: Subscribing to Heterogeneous Stream Data via C++ API\nDESCRIPTION: This C++ code snippet demonstrates how to subscribe to a heterogeneous stream table in DolphinDB using the C++ API. It connects to a DolphinDB server, loads schemas for different table types, creates a stream deserializer, defines a callback function to process incoming messages, and subscribes to the stream table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/stock_market_replay.md#_snippet_14\n\nLANGUAGE: C++\nCODE:\n```\nint main(int argc, char *argv[]){\n    DBConnection conn;\n    string hostName = \"127.0.0.1\";\n    int port = 8848;\n    bool ret = conn.connect(hostName, port);\n\n    conn.run(\"login(\\\"admin\\\", \\\"123456\\\")\");\n    DictionarySP t1schema = conn.run(\"loadTable(\\\"dfs://snapshotL2\\\", \\\"snapshotL2\\\").schema()\");\n    DictionarySP t2schema = conn.run(\"loadTable(\\\"dfs://trade\\\", \\\"trade\\\").schema()\");\n    DictionarySP t3schema = conn.run(\"loadTable(\\\"dfs://order\\\", \\\"order\\\").schema()\");\n\n    unordered_map<string, DictionarySP> sym2schema;\n    sym2schema[\"snapshot\"] = t1schema;\n    sym2schema[\"trade\"] = t2schema;\n    sym2schema[\"order\"] = t3schema;\n    StreamDeserializerSP sdsp = new StreamDeserializer(sym2schema);\n    auto myHandler = [&](Message msg) {\n            const string &symbol = msg.getSymbol();\n            cout << symbol << \":\";\n            size_t len = msg->size();\n            for (int i = 0; i < len; i++) {\n                    cout <<msg->get(i)->getString() << \",\";\n            }\n            cout << endl;\n    };\n\n    int listenport = 10260;\n    ThreadedClient threadedClient(listenport);\n    auto thread = threadedClient.subscribe(hostName, port, myHandler, \"messageStream\", \"printMessageStream\", -1, true, nullptr, false, false, sdsp);\n    cout<<\"Successed to subscribe messageStream\"<<endl;\n    thread->join();\n\n    return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Initialize Parallelism and Start Engines (DolphinDB)\nDESCRIPTION: Sets the degree of parallelism (`parallel`) for the stream processing pipeline. It then calls the previously defined setup functions to create and configure the engines and subscriptions. The `go` keyword submits the preceding statements as asynchronous tasks, allowing the engine setup to proceed in the background.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/streaming_capital_flow_daily/02.createEngineSub.txt#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\nparallel = 3\nprocessCapitalFlowFunc(parallel)\ngo\nprocessSellOrderFunc(parallel)\ngo\nprocessBuyOrderFunc(parallel)\nprocessCapitalFlow60minFunc()\n```\n\n----------------------------------------\n\nTITLE: Complete Stock Correlation Analysis\nDESCRIPTION: This DolphinDB script performs a complete stock correlation analysis, including loading data, calculating the return matrix, computing the correlation matrix, and identifying the most correlated stocks. It uses `loadTable`, `exec`, `pivot by`, `cross`, `table`, `rename!`, `unpivot`, and `context by` to achieve this. Requires a distributed database setup.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\nlogin(\"admin\",\"123456\")\ndaily_line= loadTable(\"dfs://tushare\",\"hushen_daily_line\")\n\nretMatrix=exec pct_chg/100 as ret from daily_line pivot by trade_date,ts_code\ncorrMatrix=cross(corr,retMatrix)\n\nsyms=(exec count(*) from daily_line group by ts_code).ts_code\nsyms=\"C\"+strReplace(syms, \".\", \"_\")\nmostCorrelated=select * from table(corrMatrix.columnNames() as ts_code, corrMatrix).rename!([`ts_code].append!(syms)).unpivot(`ts_code, syms).rename!(`ts_code`corr_ts_code`corr) context by ts_code having rank(corr,false) between 1:10\n```\n\n----------------------------------------\n\nTITLE: Registering and Subscribing Time-Series Stream Engine - DolphinDB\nDESCRIPTION: Initializes a time-series stream engine in DolphinDB that windows over 10 minutes (600,000 ms), computes features via the metrics function, and writes results to the \"aggrFeatures10min\" table. It also subscribes the engine to the \"snapshotStream\" for continuous computation on incoming market data. Key parameters include windowSize, step, input/output tables, and TradeTime as the time index.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/machine_learning_volatility/04.streamComputing.txt#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//register stream computing engine\ncreateTimeSeriesEngine(name=\"aggrFeatures10min\", windowSize=600000, step=60000, metrics=metrics, dummyTable=snapshotStream, outputTable=aggrFeatures10min, timeColumn=`TradeTime, useWindowStartTime=true, keyColumn=`SecurityID)\n//subscribe data\nsubscribeTable(tableName=\"snapshotStream\", actionName=\"aggrFeatures10min\", offset=-1, handler=getStreamEngine(\"aggrFeatures10min\"), msgAsTable=true, batchSize=2000, throttle=1, hash=0, reconnect=true)\n\n```\n\n----------------------------------------\n\nTITLE: Python API - Submit Replay Task\nDESCRIPTION: This Python code snippet demonstrates submitting a data replay task through the DolphinDB Python API. It initializes the parameters for the replay, including stock list, start and end dates, replay rate, and the replay name, and calls `stkReplay`. The code then uploads the replay parameters using the `upload` method and runs `stkReplay` using `s.run()`. Prerequisites are setting up the DolphinDB Python API and the `stkReplay` function on the server.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/appendices_market_replay_bp.md#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nstk_list = ['000616.SZ','000681.SZ']\nstart_date = '20211201'\nend_date = '20211201'\nreplay_rate = -1\nreplay_name = ['snapshot']\ns.upload({'stk_list':stk_list, 'start_date':start_date, 'end_date':end_date, 'replay_rate':replay_rate, 'replay_uuid':uuidStr, 'replay_name':replay_name})\ns.run(\"stkReplay(stk_list, start_date, end_date, replay_rate, replay_uuid, replay_name)\")\n```\n\n----------------------------------------\n\nTITLE: Echarts Expected Data Format Example\nDESCRIPTION: Example of the JavaScript data structure required by Echarts to display a line chart. Shows how x-axis data and series data should be formatted.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/web_chart_integration.md#_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\noption = {\n\txAxis: {\n\t\tdata: [\"13:03:50\", \"13:03:51\", \"13:03:52\", \"13:03:53\", \"13:03:54\", \"13:03:55\", \"13:03:56\", \"13:03:57\", \"13:03:58\", \"13:03:59\", \"13:04:00\"]\n\t},\n\tyAxis: {\n\t\ttype: 'value'\n\t},\n\tseries: [{\n\t\tdata: [1.019094, 0.971753, 0.962792, 1.014048, 0.991746, 1.016851, 0.98674, 1.00463, 0.991642, 1.018987, 1.008604],\n\t\ttype: 'line'\n\t}]\n};\n```\n\n----------------------------------------\n\nTITLE: Creating Distributed Table for Shenzhen Trades (DolphinDB Script)\nDESCRIPTION: Checks if the table `trade_sz` exists in the `dfs://stockL2` database. If not, it creates a partitioned table using the schema stored in `tradeSchema`. The table is partitioned by `Tradedate` and `InstrumentID` and sorted by `InstrumentID` and `TransactTime`. `keepDuplicates=ALL` retains all records.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_implementation_for_NSQ_Plugin.md#_snippet_11\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\ndbName = \"dfs://stockL2\"\ntbName = \"trade_sz\"\nif(existsTable(dbName, tbName) == false){\n\tdb = database(dbName)\n\tdb.createPartitionedTable(table=table(1:0, tradeSchema.name, tradeSchema.type), tableName=tbName, partitionColumns=`Tradedate`InstrumentID, sortColumns=`InstrumentID`TransactTime, keepDuplicates=ALL)\n\tprint(\"DFS table created successfully !\")\n}\nelse{\n\tprint(\"DFS table have been created !\")\n}\n```\n\n----------------------------------------\n\nTITLE: 执行复杂多表关系查询示例 - DolphinDB 脚本\nDESCRIPTION: 示例展示如何使用 DolphinDB 支持的关系模型功能，进行多表关联查询。包括使用 left join 连接主时序表与设备信息表，按设备ID匹配并筛选特定时间数据。依赖 DolphinDB SQL 语法及分布式表加载，适合实现复杂业务数据整合分析。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/high_freq_data_storage_and_analysis.md#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\n/**** 关系模型 ****/\n//关联查询\npt=loadTable(\"dfs://db_test\",`collect)  //pt（ts,deviceid,v1,...,v5000） dt(deviceid,model,ip)\nselect pt.ts,pt.v1,pt.v2,dt.* from pt left join dt on pt.deviceid=dt.deviceid where pt.ts=2022.01.01 00:00:01.001\n```\n\n----------------------------------------\n\nTITLE: Creating DolphinDB DFS Database and Partitioned Trade Table\nDESCRIPTION: This DolphinDB script logs into the server using credentials 'admin'/'123456', defines variables for the database path (`dfs://trade_stream`) and table name (`trade`), and then proceeds to create the database and table. It first checks if the database exists and drops it if it does. A composite partitioned database is created using VALUE partitioning on a date range (2020.01.01 to 2022.01.01) and HASH partitioning on the SYMBOL type (with 5 partitions). Finally, it defines a table schema for trade data (SecurityID, Market, TradeTime, TradePrice, TradeQty, TradeAmount, BuyNum, SellNum) and creates the partitioned table `trade` within the database `dfs://trade_stream`, partitioned by `TradeTime` and `SecurityID`, applying delta compression to the `TradeTime` column. It requires a running DolphinDB server (version 1.30.18/2.00.6 or later) with the specified user credentials.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/streaming_capital_flow_order_by_order/02.创建存储实时数据的库表.txt#_snippet_0\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\n/**\ncreateDFSTable.txt\nScript to create database and table to store real-time data\nDolphinDB Inc.\nDolphinDB server version: 1.30.18 2022.05.09/2.00.6 2022.05.09\nStorage engine: OLAP\nLast modification time: 2022.05.30\n*/\n\n/**\nmodified location: dbName, tbName\n*/\ndbName = \"dfs://trade_stream\"\ntbName = \"trade\"\n//login account\nlogin(\"admin\", \"123456\")\n//create database and table\nif(existsDatabase(dbName)){\n\tdropDatabase(dbName)\n}\ndb1 = database(, VALUE, 2020.01.01..2022.01.01)\ndb2 = database(, HASH, [SYMBOL, 5])\ndb = database(dbName, COMPO, [db1, db2])\nschemaTable = table(\n\tarray(SYMBOL, 0) as SecurityID,\n\tarray(SYMBOL, 0) as Market,\n\tarray(TIMESTAMP, 0) as TradeTime,\n\tarray(DOUBLE, 0) as TradePrice,\n\tarray(INT, 0) as TradeQty,\n\tarray(DOUBLE, 0) as TradeAmount,\n\tarray(INT, 0) as BuyNum,\n\tarray(INT, 0) as SellNum\n)\ndb.createPartitionedTable(table=schemaTable, tableName=tbName, partitionColumns=`TradeTime`SecurityID, compressMethods={TradeTime:\"delta\"})\n```\n\n----------------------------------------\n\nTITLE: Revoking TABLE_READ Permission from Shared Stream Table - DolphinDB\nDESCRIPTION: This code revokes the TABLE_READ permission from a user for the 'trades' shared stream table. It uses the `revoke` function. Requires admin privileges to create user.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Permission_Management.md#_snippet_30\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(`MitchTrubisky, \"JI3564^\")\nrevoke(\"CliffLee\", TABLE_READ, \"trades\")\n```\n\n----------------------------------------\n\nTITLE: Granting TABLE_READ Permission to Shared Stream Table - DolphinDB\nDESCRIPTION: This grants a specific user (CliffLee) TABLE_READ permission for the shared stream table 'trades', limiting their access. It uses the `grant` function. Requires admin privileges to create user.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Permission_Management.md#_snippet_29\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(`MitchTrubisky, \"JI3564^\")\ngrant(\"CliffLee\", TABLE_READ, \"trades\") \n```\n\n----------------------------------------\n\nTITLE: Replaying Historical Data into Streaming (DolphinDB)\nDESCRIPTION: This DolphinDB script replays historical trade data into the `tradeOriginalStream` table. It loads data from a historical table, filters the data based on the time range between 09:30:00.000 and 14:57:00.000, and sorts the data by `TradeTime` and `SecurityID`. It then submits a job to replay the data at a rate of 100,000 records per second, ensuring chronological order and real-time simulation.  getRecentJobs() displays the status of the replay job.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_capital_flow_order_by_order.md#_snippet_10\n\nLANGUAGE: DolphinDB\nCODE:\n```\nt = select * from loadTable(\"dfs://trade\", \"trade\") where time(TradeTime) between 09:30:00.000 : 14:57:00.000 order by TradeTime, SecurityID\nsubmitJob(\"replay_trade\", \"trade\",  replay{t, tradeOriginalStream, `TradeTime, `TradeTime, 100000, true, 1})\ngetRecentJobs()\n```\n\n----------------------------------------\n\nTITLE: Importing Data with Time Transformation using loadTextEx in DolphinDB\nDESCRIPTION: This script imports data from a CSV file into a pre-defined partitioned table (`tb1`) using `loadTextEx`. It specifies the database handle (`db`), table name (`tb1`), partition columns (`date`), filename, and importantly, the `transform` parameter, providing the user-defined function `i2t` to convert the 'time' column from its original format (likely INT) to TIME type during the import process.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_csv.md#_snippet_14\n\nLANGUAGE: dolphindb\nCODE:\n```\ntmpTB=loadTextEx(dbHandle=db,tableName=`tb1,partitionColumns=`date,filename=dataFilePath,transform=i2t);\n```\n\n----------------------------------------\n\nTITLE: Defining Annualized Kurtosis Calculation Function in DolphinDB\nDESCRIPTION: Defines the 'getAnnualKur' function that calculates the kurtosis of daily returns for an asset value series. Kurtosis measures the tail extremity or peakedness of return distributions and is calculated on returns derived from relative daily changes. The function returns a numeric kurtosis value.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_factor_by_ddb/fund_factor_nine.txt#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndefg getAnnualKur(value){\n\treturn kurtosis(deltas(value)\\prev(value)) \n}\n```\n\n----------------------------------------\n\nTITLE: Creating 5-Minute K-line Charts with Time Series Aggregator in DolphinDB\nDESCRIPTION: Implements a time series aggregator that calculates 5-minute K-line data (OHLC) from the level2 stream table. This example creates non-overlapping time windows where both windowSize and step are set to 300 seconds (5 minutes).\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/quant_finance_examples.md#_snippet_22\n\nLANGUAGE: DolphinDB\nCODE:\n```\nmodal = table(100:0, `symbol`datetime`last`askPrice1`bidPrice1`askVolume1`bidVolume1`volume, [SYMBOL,DATETIME,DOUBLE,DOUBLE,DOUBLE,INT,INT,INT])\nshare streamTable(100:0, `datetime`symbol`open`high`low`close`volume,[DATETIME,SYMBOL,DOUBLE,DOUBLE,DOUBLE,DOUBLE,LONG]) as OHLC1\ntsAggr1 = createTimeSeriesAggregator(name=\"tsAggr1\", windowSize=300, step=300, metrics=<[first(last),max(last),min(last),last(last),sum(volume)]>, dummyTable=modal, outputTable=OHLC1, timeColumn=`datetime, keyColumn=`symbol)\nsubscribeTable(tableName=\"level2\", actionName=\"act_tsAggr1\", offset=0, handler=append!{tsAggr1}, msgAsTable=true);\n```\n\n----------------------------------------\n\nTITLE: Grafana Query for Visualizing Volatility Predictions in DolphinDB\nDESCRIPTION: SQL query for Grafana to visualize the real-time volatility predictions for a specific stock. The gmtime function is used to handle the 8-hour time zone difference between Grafana and DolphinDB server.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/machine_learning_volatility.md#_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\nselect gmtime(TradeTime), PredictRV from result1min where SecurityID=`600519\n```\n\n----------------------------------------\n\nTITLE: 使用ffill函数填充股票行情数据\nDESCRIPTION: 由于不同股票的行情数据更新时间点不同，使用ffill函数对pivot结果中的空值进行向前填充，确保每个时间点都有完整的数据，为后续计算投资组合价值做准备。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/quant_finance_examples.md#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntmp = select ffill(last) from quotes where date=2020.06.01, symbol in syms, time between 09:30:00.000 : 15:00:00.000 pivot by time, symbol\nselect top 10 * from tmp;\n```\n\n----------------------------------------\n\nTITLE: Creating a Logical Table for Pushing Stream Data to ZeroMQ in DolphinDB\nDESCRIPTION: This DolphinDB script defines a function `zmqPusherTable` to create a logical table that pushes stream data to a ZeroMQ message queue. It connects to the specified address and creates a pusher object for sending data according to the defined schema.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practice_for_factor_calculation.md#_snippet_20\n\nLANGUAGE: DolphinDB\nCODE:\n```\nresultSchema=table(1:0,[\"SecurityID\",\"TradeTime\",\"factor\"], [SYMBOL,TIMESTAMP,DOUBLE])//输出到消息队列的表结构\n\ndef zmqPusherTable(zmqSubscriberAddress,schemaTable){\n\tSignalSender=def (x) {return x}\n\tpushingSocket = zmq::socket(\"ZMQ_PUB\", SignalSender)\n\tzmq::connect(pushingSocket, zmqSubscriberAddress)\n\tpusher = zmq::createPusher(pushingSocket, schemaTable)\n\treturn pusher\n}\n\nzmqSubscriberAddress=\"tcp://192.168.1.195:55556\"//引擎demoEngine向zmq队列推送，使用时根据不同的zmq地址修改此字符串\n\npusherTable=zmqPusherTable(zmqSubscriberAddress,resultSchema)//生成一个逻辑表向上述地址发送zmq包，字段结构参照resultSchema\n\ndemoEngine = createReactiveStateEngine(name=\"reactiveDemo\", metrics=<[TradeTime,doubleEma(LastPx)]>, dummyTable=snapshotSchema, outputTable=pusherTable, keyColumn=\"SecurityID\",keepOrder=true)//创建流引擎，output指定输出到pusher表\n```\n\n----------------------------------------\n\nTITLE: Creating Partitioned DFS Table DolphinDB Script\nDESCRIPTION: Creates the actual partitioned table named `sensorTemp` within the `dfs://iotDemoDB` database instance (`db`). It utilizes the previously defined `tableSchema` and applies the specified partitioning columns (`ts` and `hardwareId`) based on the database's structure.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/iot_demo_script.txt#_snippet_9\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\ndfsTable = db.createPartitionedTable(tableSchema,\"sensorTemp\",`ts`hardwareId)\n```\n\n----------------------------------------\n\nTITLE: Defining Real-Time Capital Flow Calculation Function in DolphinDB Script\nDESCRIPTION: Defines the function 'calCapitalFlow' which calculates minute-level capital flow statistics from raw trade data. It partitions trades into 'small' and 'big' based on a trade quantity boundary, then sums up trade amounts separately for buy and sell orders across these partitions. The function returns a vector with nil-filled zeros for missing values to ensure consistent output. This function serves as a metric for the downstream time series engine.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/streaming_capital_flow_order_by_order/04.注册流计算引擎和订阅流数据表.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndefg calCapitalFlow(Num, BSFlag, TradeQty, TradeAmount){\n\t// You can define the smallBigBoundary by yourself\n\tsmallBigBoundary = 50000\n\ttempTable1 = table(Num as `Num, BSFlag as `BSFlag, TradeQty as `TradeQty, TradeAmount as `TradeAmount)\n\ttempTable2 = select sum(TradeQty) as TradeQty, sum(TradeAmount) as TradeAmount from tempTable1 group by Num, BSFlag\n\tBuySmallAmount = exec sum(TradeAmount) from  tempTable2 where TradeQty<=smallBigBoundary && BSFlag==`B\n\tBuyBigAmount = exec sum(TradeAmount) from tempTable2 where TradeQty>smallBigBoundary && BSFlag==`B\n\tSellSmallAmount = exec sum(TradeAmount) from  tempTable2 where TradeQty<=smallBigBoundary && BSFlag==`S\n\tSellBigAmount = exec sum(TradeAmount) from tempTable2 where TradeQty>smallBigBoundary && BSFlag==`S\n\treturn nullFill([BuySmallAmount, BuyBigAmount, SellSmallAmount, SellBigAmount], 0)\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Scheduled Job to Run a Script File in DolphinDB\nDESCRIPTION: This example shows how to create a scheduled job that runs a script file, but it will fail if the script has external dependencies.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/scheduledJob.md#_snippet_15\n\nLANGUAGE: DolphinDB\nCODE:\n```\nscheduleJob(`dailyfoofile1, \"Daily Job 1\", run {\"/home/user/testjob.dos\"}, 16:14m, 2020.01.01, 2020.12.31, `D`);\n```\n\n----------------------------------------\n\nTITLE: Null Value Handling Rules in Moving, TM, and Cumulative Window Functions in DolphinDB\nDESCRIPTION: Describes how DolphinDB handles nulls in moving-series (m), time-moving-series (tm), and cumulative (cum) window functions. Specifically, functions like mrank, tmrank, cumrank allow specifying whether NULL values participate in computation. Most other functions skip NULLs during aggregation. The optional parameter minPeriods controls the minimum number of valid observation points before returning non-null results; absent minPeriods defaults to the window size, resulting in leading NULL output until enough data accumulates. Examples demonstrate this behavior using msum on matrices with NULL values, highlighting differences when minPeriods is specified versus omitted. Dependencies are standard DolphinDB function semantics. Inputs are numeric arrays or matrices possibly containing NULLs; outputs are computed window aggregates with NULL-handling as described.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/window_cal.md#_snippet_25\n\nLANGUAGE: DolphinDB\nCODE:\n```\nm=matrix(1..5, 6 7 8 NULL 10)\n\n// No minPeriods specified; default equals window size, so leading rows are NULL\nmsum(m,3)\n\n// Specifying minPeriods=1 results in no leading NULLs\nmsum(m,3,1)\n```\n\n----------------------------------------\n\nTITLE: Defining Price Sensitivity Function in DolphinDB Script\nDESCRIPTION: Defines a user-defined function `priceSensitivityOrderFlowImbalance` that calculates the beta (sensitivity) between price changes (`deltaP`) and net order volume (`NVOL`). It takes `LastPrice`, `BidOrderQty`, and `OfferOrderQty` arrays as input, calculates price deltas (scaled by 10000) and net volume (difference between first bid and offer quantities), handles null values using `nullFill(0)`, and returns the beta coefficient.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/DolphinDB_Python_Parser_Intro_for_Quantitative_Finance/因子实现_DolphinDB版本/价格变动与一档量差的回归系数.txt#_snippet_0\n\nLANGUAGE: dolphindb script\nCODE:\n```\ndef priceSensitivityOrderFlowImbalance(LastPrice, BidOrderQty, OfferOrderQty){\n\tdeltaP = deltas(LastPrice)*10000\n\tNVOL = BidOrderQty[0].nullFill(0) - OfferOrderQty[0].nullFill(0)\n\treturn beta(deltaP.nullFill(0), NVOL)\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a DolphinDB Reactive State Engine\nDESCRIPTION: This code snippet creates a DolphinDB Reactive State Engine (`rse`). It defines the metrics to calculate ( `factor1`), the dummy input stream (`tickStream`), and the output table (`result`). The state engine is used for stream computing.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/reactive_state_engine.md#_snippet_2\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\ndef sum_diff(x, y){\n    return (x-y)/(x+y)\n}\nfactor1 = <ema(1000 * sum_diff(ema(price, 20), ema(price, 40)),10) -  ema(1000 * sum_diff(ema(price, 20), ema(price, 40)), 20)>\n\nshare streamTable(1:0, `sym`price, [STRING,DOUBLE]) as tickStream\nresult = table(1000:0, `sym`factor1, [STRING,DOUBLE])\nrse = createReactiveStateEngine(name=\"reactiveDemo\", metrics=factor1, dummyTable=tickStream, outputTable=result, keyColumn=\"sym\")\nsubscribeTable(tableName=`tickStream, actionName=\"factors\", handler=tableInsert{rse})\n```\n\n----------------------------------------\n\nTITLE: Logging Output of DolphinDB Stream Subscription Initialization - Shell\nDESCRIPTION: This excerpt shows example INFO level logging entries printed during the execution of the user startup script for stream computing auto-subscription. Logs detail the creation of distributed file system paths, sharing and persistence enabling of stream tables, reactive and time series engine initialization, and subscriptions to stream topics with hashing and reconnection enabled. Successful subscription logs such as 'subscribe3：snapshotStream subscribed successfully !' indicate the streaming subscription setup has succeeded without errors.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_auto_sub.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n2021-12-01 00:23:56.314159 <INFO> :---------------------------------------------------------------------\n2021-12-01 00:23:56.314172 <INFO> :dfs://snapshot created successfully !\n2021-12-01 00:23:56.314178 <INFO> :---------------------------------------------------------------------\n2021-12-01 00:23:56.315084 <INFO> :Prepare to share a stream table: tableName=snapshotStream raftGroup=-1\n2021-12-01 00:23:56.315132 <INFO> :enableTablePersistence tableName=snapshotStream hashValue=0 offset=0 cacheSize=5000000\n2021-12-01 00:23:56.315163 <INFO> :---------------------------------------------------------------------\n2021-12-01 00:23:56.315174 <INFO> :sharedTable1：snapshotStream created  successfully !\n2021-12-01 00:23:56.315182 <INFO> :---------------------------------------------------------------------\n2021-12-01 00:23:56.315512 <INFO> :Prepare to share a stream table: tableName=snapshotStreamProcess raftGroup=-1\n2021-12-01 00:23:56.315534 <INFO> :enableTablePersistence tableName=snapshotStreamProcess hashValue=1 offset=0 cacheSize=5000000\n2021-12-01 00:23:56.315549 <INFO> :---------------------------------------------------------------------\n2021-12-01 00:23:56.315562 <INFO> :sharedTable2：snapshotStreamProcess created successfully !\n2021-12-01 00:23:56.315569 <INFO> :---------------------------------------------------------------------\n2021-12-01 00:23:56.315783 <INFO> :Prepare to share a stream table: tableName=snapshotAggr1min raftGroup=-1\n2021-12-01 00:23:56.315806 <INFO> :enableTablePersistence tableName=snapshotAggr1min hashValue=2 offset=0 cacheSize=2000000\n2021-12-01 00:23:56.315821 <INFO> :---------------------------------------------------------------------\n2021-12-01 00:23:56.315833 <INFO> :sharedTable3：snapshotAggr1min created successfully !\n2021-12-01 00:23:56.315840 <INFO> :---------------------------------------------------------------------\n2021-12-01 00:23:56.316775 <INFO> :---------------------------------------------------------------------\n2021-12-01 00:23:56.316793 <INFO> :ReactiveStateEngine：snapshotProcessing created successfully !\n2021-12-01 00:23:56.316800 <INFO> :---------------------------------------------------------------------\n2021-12-01 00:23:56.316852 <INFO> :Begin to subscription topic=localhost:24110:local24110/snapshotStream/snapshotProcessing\n2021-12-01 00:23:56.316888 <INFO> :Enable reconnection for topic=localhost:24110:local24110/snapshotStream/snapshotProcessing site=local24110:1\n2021-12-01 00:23:56.316915 <INFO> :[subscribeTable] #attempt=0 topic=localhost:24110:local24110/snapshotStream/snapshotProcessing conn=\n2021-12-01 00:23:56.316940 <INFO> :Received a request to publish table [snapshotStream] to site localhost:24111.Offset=-1\n2021-12-01 00:23:56.317229 <INFO> :Subscription topic=localhost:24110:local24110/snapshotStream/snapshotProcessing hashValue=0\n2021-12-01 00:23:56.317252 <INFO> :---------------------------------------------------------------------\n2021-12-01 00:23:56.317259 <INFO> :subscribe1：snapshotStream subscribed successfully !\n2021-12-01 00:23:56.317264 <INFO> :---------------------------------------------------------------------\n2021-12-01 00:23:56.318486 <INFO> :Begin to subscription topic=localhost:24110:local24110/snapshotStreamProcess/snapshotAggr1min\n2021-12-01 00:23:56.318531 <INFO> :Enable reconnection for topic=localhost:24110:local24110/snapshotStreamProcess/snapshotAggr1min site=local24110:1\n2021-12-01 00:23:56.318555 <INFO> :[subscribeTable] #attempt=0 topic=localhost:24110:local24110/snapshotStreamProcess/snapshotAggr1min conn=\n2021-12-01 00:23:56.318574 <INFO> :Received a request to publish table [snapshotStreamProcess] to site localhost:24111.Offset=-1\n2021-12-01 00:23:56.318844 <INFO> :Subscription topic=localhost:24110:local24110/snapshotStreamProcess/snapshotAggr1min hashValue=1\n2021-12-01 00:23:56.318871 <INFO> :---------------------------------------------------------------------\n2021-12-01 00:23:56.318883 <INFO> :subscribe2：snapshotStreamProcess subscribed successfully !\n2021-12-01 00:23:56.318891 <INFO> :---------------------------------------------------------------------\n2021-12-01 00:23:56.318942 <INFO> :Begin to subscription topic=localhost:24110:local24110/snapshotStream/snapshotToDatabase\n2021-12-01 00:23:56.318968 <INFO> :Enable reconnection for topic=localhost:24110:local24110/snapshotStream/snapshotToDatabase site=local24110:1\n2021-12-01 00:23:56.318996 <INFO> :[subscribeTable] #attempt=0 topic=localhost:24110:local24110/snapshotStream/snapshotToDatabase conn=\n2021-12-01 00:23:56.319011 <INFO> :Received a request to publish table [snapshotStream] to site localhost:24111.Offset=-1\n2021-12-01 00:23:56.319042 <INFO> :Subscription topic=localhost:24110:local24110/snapshotStream/snapshotToDatabase hashValue=2\n2021-12-01 00:23:56.319058 <INFO> :---------------------------------------------------------------------\n2021-12-01 00:23:56.319065 <INFO> :subscribe3：snapshotStream subscribed successfully !\n2021-12-01 00:23:56.319071 <INFO> :---------------------------------------------------------------------\n```\n\n----------------------------------------\n\nTITLE: Calculating One-Year Cumulative Fund Returns in DolphinDB\nDESCRIPTION: Filters the original panel data (`panelData`) to the period 2010-2020. Calculates corresponding purchase dates one year prior for each date in the filtered panel. Aligns these purchase dates to the nearest preceding workday using the 'workday' table. Retrieves the adjusted net value data for both the end dates (2010-2020) and the aligned purchase dates. Aligns these two datasets and calculates the one-year cumulative return for each fund. Filters the cumulative returns to include only funds with more than 1000 valid return data points. Calculates and displays the average one-year return for each fund, sorted descending. Calculates and displays the probability (proportion of days) that the one-year buy-and-hold return exceeded 0.2 for each fund, showing the top 30. Depends on `panelData` and the 'workday' table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/public_fund_basic_analysis/basic_analysis_of_public_fund_net_value.txt#_snippet_10\n\nLANGUAGE: dolphindb\nCODE:\n```\n//  get the net value data at the time of purchase from 2010 to 2020\nfilterPanelData = panelData.loc(2010.01.01..2020.12.31, view=true)\n// The time index is pushed forward by one year\ndayIndex = panelData.rowNames().temporalAdd(-1,'y')\n// worker day alignment\nworkdays = select * from loadTable(\"dfs://publicFundDB\", \"workday\")\nworkeDayIndex = each(def(dayIndex){return exec last(Day) from workdays where Day <= dayIndex}, dayIndex)\n// get the corrected net value data at the time of purchase from 2010 to 2020\nfilterPanelDataTmp = panelData.loc(workeDayIndex>=panelData.rowNames()[0]&&workeDayIndex<=2020.12.31, ).rename!(workeDayIndex[workeDayIndex>=panelData.rowNames()[0]&&workeDayIndex<=2020.12.31], panelData.colNames())\n// calculate the cumulative rate of return of the fund\nfilterPanelDataTmp, filterPanelData = align(filterPanelDataTmp, filterPanelData)\ncumulativeReturn = (filterPanelDataTmp - filterPanelData) / filterPanelData\n// select funds with more than 1000 returns\nfilterCumulativeReturn = cumulativeReturn[x->count(x) > 1000]\n// calculate the average yield of each fund for one year\nselect SecurityID, mean from table(filterCumulativeReturn.colNames() as SecurityID, mean(filterCumulativeReturn) as mean) order by mean desc\n// calculate the possibility that the one-year yield of buying and holding is greater than 0.2\nresult = each(count, cumulativeReturn[cumulativeReturn>0.2]) \\ cumulativeReturn.rows()\n(select SecurityID, prop from table(cumulativeReturn.colNames() as SecurityID, result as prop) order by prop desc).head(30)\n```\n\n----------------------------------------\n\nTITLE: DolphinDB User and Permission Management Example 3\nDESCRIPTION: This example demonstrates how revoking global TABLE_READ permission does not affect a previously granted table-level TABLE_READ permission for a user in DolphinDB. It involves creating a user, a database, a partitioned table, granting table-level read access, revoking global read access, and then verifying the user's continuing read access.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Permission_Management.md#_snippet_9\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(\"admin\", \"123456\")\ncreateUser(\"user3\",\"123456\")\ndbName = \"dfs://test\"\nif(existsDatabase(dbName)){\n        dropDatabase(dbName)\n}\nt = table(1..10 as id , rand(100, 10) as val)\ndb=database(dbName, VALUE, 1..10)\npt=  db.createPartitionedTable(t, \"pt\", \"id\")\npt.append!(t)\n\ngrant(\"user3\", TABLE_READ, dbName+\"/pt\")\nrevoke(\"user3\", TABLE_READ, \"*\")\nlogin(\"user3\", \"123456\")\nselect * from loadTable(dbName, \"pt\")//user 被撤回读 \"dfs://test\" 的权限\n```\n\n----------------------------------------\n\nTITLE: Submitting Parallel Jobs in DolphinDB\nDESCRIPTION: Demonstrates submitting single and multiple parallel jobs using submitJob API in DolphinDB. It submits 'parallJob_single_ten' and multiple 'parallJob_multi_ten' jobs running the parJob2 function, facilitating concurrent execution. Also includes querying job execution times for monitoring.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_factor_by_ddb/fund_factor_ten.txt#_snippet_12\n\nLANGUAGE: DolphinDB\nCODE:\n```\nsubmitJob(\"parallJob1\", \"parallJob_single_ten\", parJob2)\n\nfor(i in 0..4){\n\tsubmitJob(\"parallJob5\", \"parallJob_multi_ten\", parJob2)\n}\n\nselect endTime - startTime from getRecentJobs() where jobDesc = \"parallJob_single_ten\"\n\nselect max(endTime) - min(startTime) from getRecentJobs() where jobDesc = \"parallJob_multi_ten\"\n```\n\n----------------------------------------\n\nTITLE: Creating a Group Chat in WeChat Work\nDESCRIPTION: Code for creating a group chat in WeChat Work by sending an HTTP POST request. The payload specifies the chat name, owner, member list, and chat ID.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/send_messages_external_systems.md#_snippet_8\n\nLANGUAGE: DolphinDB\nCODE:\n```\nurl = 'https://qyapi.weixin.qq.com/cgi-bin/appchat/create?';\nurl+='access_token='+ACCESS_TOKEN;\nparam='{\"name\" : \"NAME\",\"owner\" : \"userid1\",\"userlist\" : [\"userid1\", \"userid2\", \"userid3\"],\"chatid\" : \"CHATID\"}';\nret=httpClient::httpPost(url,param,1000);\nprint ret['text'];\n```\n\n----------------------------------------\n\nTITLE: Updating License Online\nDESCRIPTION: This command, executed in the interactive programming interface of the Web management interface, updates the DolphinDB license online.  It requires the license to have the same customer name and a license with equal or greater resource limits.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/standalone_server.md#_snippet_20\n\nLANGUAGE: sh\nCODE:\n```\nupdateLicense()\n```\n\n----------------------------------------\n\nTITLE: Implementing Snapshot Recovery for DolphinDB Stream Engine\nDESCRIPTION: Configures a stream engine for snapshotting and demonstrates how a subscriber can retrieve the last processed message ID from a snapshot using `getSnapshotMsgId` and resume the subscription from that point for state restoration.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_tutorial.md#_snippet_21\n\nLANGUAGE: DolphinDB\nCODE:\n```\nshare streamTable(1:0, `sym`price, [STRING,DOUBLE]) as tickStream\nresult = table(1000:0, `sym`factor1, [STRING,DOUBLE])\nrse = createReactiveStateEngine(name=\"reactiveDemo\", metrics =<cumsum(price)>, dummyTable=tickStream, outputTable=result, keyColumn=\"sym\", snapshotDir= \"/home/data/snapshot\", snapshotIntervalInMsgCount=20000)\nmsgId = getSnapshotMsgId(rse)\nif(msgId >= 0) msgId += 1\nsubscribeTable(tableName=`tickStream, actionName=\"factors\", offset=msgId, handler=appendMsg{rse}, handlerNeedMsgId=true)\n```\n\n----------------------------------------\n\nTITLE: Querying - Join, Filter, Order by, Top N DolphinDB\nDESCRIPTION: This snippet joins the `readings` and `device_info` tables and selects the top 5 devices based on specific criteria.  The criteria include battery level and charging status, as well as ordering and applying a top `n` constraint. This is a complex query with a join and a `top` operator.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs TimescaleDB/test_dolphindb_small.txt#_snippet_28\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 19. 经典查询：未在充电的、电量小于 33% 的、平均 1 分钟内最高负载的 5 个设备\ntimer\nselect top 5\n    readings.device_id,\n    battery_level,\n    battery_status,\n    cpu_avg_1min\nfrom ej(readings, device_info, `device_id)\nwhere battery_level < 33, battery_status = 'discharging'\norder by cpu_avg_1min desc, time desc\n```\n\n----------------------------------------\n\nTITLE: Building and Saving an Array Vector Table to CSV - DolphinDB Script\nDESCRIPTION: This snippet builds an in-memory DolphinDB table containing array vector columns for 'bid' and 'ask' prices, demonstrates how to construct and append arrays to these columns, and then saves the table to a CSV file. It highlights initialization of array vector columns using the array constructor with a DOUBLE[] type, and uses the 'saveText' function for CSV export. Required dependencies include DolphinDB v2.00.4 or higher for consistent array vector support as well as write permission on the specified file path.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_csv.md#_snippet_19\n\nLANGUAGE: DolphinDB\nCODE:\n```\nbid = array(DOUBLE[], 0, 20).append!([1.4799 1.479 1.4787, 1.4796 1.479 1.4784, 1.4791 1.479 1.4784])\nask = array(DOUBLE[], 0, 20).append!([1.4821 1.4825 1.4828, 1.4818 1.482 1.4821, 1.4814 1.4818 1.482])\nTradeDate = 2022.01.01 + 1..3\nSecurityID = rand(`APPL`AMZN`IBM, 3)\nt = table(SecurityID as `sid, TradeDate as `date, bid as `bid, ask as `ask)\nsaveText(t,filename=\"/home/data/t.csv\",delimiter=',',append=true)\n```\n\n----------------------------------------\n\nTITLE: Subscribing to IPC Table - DolphinDB\nDESCRIPTION: This code subscribes AMD data to an IPC (Inter-Process Communication) table. It defines the database name and table name, gets the schema for execution data, and creates a partitioned table in DFS if it does not already exist. It then creates an IPC in-memory table (`IPCExecutionTb`) and defines a transformation function `handleExecutionSubs` to modify the data by adding a tradeDate field, applying market specific security code suffixes and modifying the price, and adjusting column order. Finally, it subscribes to the execution stream using the IPC table, leveraging the transform function.  Requires creating a partitioned table and an IPC in-memory table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_implementation_for_AMD_Plugin.md#_snippet_13\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 订阅到跨进程共享内存表\ndbName = \"dfs://amd\"\nexecutionTbName = \"execution\"\ntbName = \"IPCExecutionTb\"\nexecutionSchema = amdQuote::getSchema(`execution)\ncolName = append!([\"tradeDate\"], executionSchema.name)\ncolType = append!([\"DATE\"], executionSchema.type)\ntmpColDefs = table(1:0, colName, colType).schema().colDefs\n\nif (!existsTable(dbName, executionTbName)) {\n    colName = append!([\"tradeDate\"], executionSchema.name)\n    colType = append!([\"DATE\"], executionSchema.type)\n    tbSchema = table(1:0, colName, colType)\n    pt = db.createPartitionedTable(table=tbSchema, tableName=executionTbName, partitionColumns=`tradeDate`securityCode,  sortColumns=`securityCode`execTime, keepDuplicates=ALL)\n}\n\ntry { dropIPCInMemoryTable(tbName) } catch(ex) { print(ex) }\nIPCExecutionTb = createIPCInMemoryTable(1000000, tbName, tmpColDefs.name, tmpColDefs.typeInt)\n\ndef handleExecutionSubs(mutable msg, reorderedColNames) {\n    // 增加一个日期字段 tradeDate，其值为对应的时间戳字段的日期部分\n    update msg set tradeDate = date(execTime)\n    update msg set securityCode = securityCode + \".SZ\" where marketType = 102\n    update msg set securityCode = securityCode + \".SH\" where marketType = 101\n    // 所有价格字段值除以 100\n    update msg set execPrice = execPrice / 100\n    // 调整列顺序为与流表、分布式表一致\n    reorderColumns!(msg, reorderedColNames)\n\n    return msg\n}\n\nreorderedColNames = loadTable(dbName, executionTbName).schema().colDefs.name\namdQuote::subscribe(handle, `execution, IPCExecutionTb, 101, , handleExecutionSubs{reorderedColNames=reorderedColNames})\namdQuote::subscribe(handle, `execution, IPCExecutionTb, 102, , handleExecutionSubs{reorderedColNames=reorderedColNames})\n```\n\n----------------------------------------\n\nTITLE: Importing Data with Date to Month Transformation in DolphinDB\nDESCRIPTION: This script demonstrates transforming a DATE column to MONTH type during import. It sets up the database and table similar to the previous example but modifies the schema to expect MONTH for the 'tradingDay' column. It defines a transformation function `d2m` using `replaceColumn!` and `month()` to perform the conversion. Finally, it calls `loadTextEx` with the `transform=d2m` parameter to import the data while applying the date-to-month conversion.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_csv.md#_snippet_16\n\nLANGUAGE: dolphindb\nCODE:\n```\nlogin(`admin,`123456)\ndbPath=\"dfs://DolphinDBdatabase\"\ndb=database(dbPath,VALUE,2018.01.02..2018.01.30)\nschemaTB=extractTextSchema(dataFilePath)\nupdate schemaTB set type=\"MONTH\" where name=\"tradingDay\"\ntb=table(1:0,schemaTB.name,schemaTB.type)\ntb=db.createPartitionedTable(tb,`tb1,`date)\ndef d2m(mutable t){\n    return t.replaceColumn!(`tradingDay,month(t.tradingDay))\n}\ntmpTB=loadTextEx(dbHandle=db,tableName=`tb1,partitionColumns=`date,filename=dataFilePath,transform=d2m);\n```\n\n----------------------------------------\n\nTITLE: Using wma Function within SQL Context in DolphinDB\nDESCRIPTION: This code demonstrates the use of the `wma` function within a SQL `update` statement to calculate the weighted moving average for each stock (`symbol`) in a table `t`. It first creates a sample table with `symbol`, `date`, and `close` columns. The `context by symbol` clause ensures that the `wma` calculation is performed separately for each stock group, showcasing the module's applicability within group-wise operations.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ta.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\nclose = 7.2 6.97 7.08 6.74 6.49 5.9 6.26 5.9 5.35 5.63 3.81 3.935 4.04 3.74 3.7 3.33 3.64 3.31 2.69 2.72\ndate = (2020.03.02 + 0..4 join 7..11).take(20)\nsymbol = take(`F,10) join take(`GPRO,10)\nt = table(symbol, date, close)\nupdate t set wma = wma(close, 5) context by symbol\n```\n\n----------------------------------------\n\nTITLE: Defining Input Stream Table in DolphinDB\nDESCRIPTION: Defines a stream table named `inputSt` to receive real-time sensor data. The table has three columns: `tag` (SYMBOL), `ts` (TIMESTAMP), and `value` (INT). It enables table sharing and persistence with a cache size of 100,000 rows.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_streaming_application_in_IOT.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\nstream01=streamTable(100000:0,`tag`ts`value,[SYMBOL,TIMESTAMP, INT])\nenableTableShareAndPersistence(table=stream01,tableName=`inputSt,asynWrite=false,compress=true, cacheSize=100000)\n```\n\n----------------------------------------\n\nTITLE: Creating DolphinDB Partitioned Table for Option Prices (DolphinDB Script)\nDESCRIPTION: Connects to DolphinDB, checks for and drops an existing database named 'dfs://optionPrice', then creates a new partitioned database and table ('optionPrice') to store daily option price data. The table uses DATE, SYMBOL, and DOUBLE types and is partitioned by 'tradedate' using a range partition scheme covering 50 years starting from 2000.01M.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/IV_Greeks_Calculation_for_ETF_Options_Using_JIT.md#_snippet_0\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nlogin(\"admin\", \"123456\")\ndbName = \"dfs://optionPrice\"\ntbName = \"optionPrice\"\nif(existsDatabase(dbName)){\n\tdropDatabase(dbName)\n}\ndb = database(dbName, RANGE, date(datetimeAdd(2000.01M,0..50*12,'M')))\ncolNames = `tradedate`sym`codes`closeprice`etf`etfprice\ncolTypes = [DATE, SYMBOL, SYMBOL, DOUBLE, SYMBOL, DOUBLE]\nschemaTable = table(1:0, colNames, colTypes)\ndb.createPartitionedTable(table=schemaTable, tableName=tbName, partitionColumns=`tradedate)\n```\n\n----------------------------------------\n\nTITLE: 整体文件内容—— DolphinDB 函数化编程与高阶函数教程\nDESCRIPTION: 全面介绍 DolphinDB 的函数化编程思想、主要高阶函数及其应用场景，包括数据导入、时间处理、相关性计算、性能优化等。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_0\n\n\n\n----------------------------------------\n\nTITLE: Scheduling Daily Partition Backup Job in DolphinDB\nDESCRIPTION: Defines a scheduled job in DolphinDB that runs daily at 00:05 AM to back up data from the previous day. It constructs the backup directory path dynamically using today's date minus one, selects the corresponding partitions from the distributed table, and calls backup with incremental and parallel flags. This automation facilitates routine daily backups without manual intervention. Prerequisites include configuring DolphinDB's job scheduler and ensuring correct permissions.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/restore-backup.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\nscheduleJob(`backupJob, \"backupDB\", backup{\"/hdd/hdd1/backup/\"+(today()-1).format(\"yyyyMMdd\"),<select * from loadTable(\"dfs://ddb\",\"windTurbine\") where tm between datetime(today()-1) : (today().datetime()-1) >,false,true}, 00:05m, today(), 2030.12.31, 'D');\n```\n\n----------------------------------------\n\nTITLE: Complete Dictionary Update Example - DolphinDB\nDESCRIPTION: A complete example showcasing dictionary updates using `dictUpdate!`. It includes the creation of the `orders` table, initialization of the `historyDict` dictionary, and the `dictUpdate!` call with custom function and initFunc definitions for inserting and initializing table data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/func_progr_cases.md#_snippet_20\n\nLANGUAGE: DolphinDB\nCODE:\n```\norders = table(`IBM`IBM`IBM`GOOG as SecID, 1 2 3 4 as Value, 4 5 6 7 as Vol)\nhistoryDict = dict(STRING, ANY)\nhistoryDict.dictUpdate!(function=def(x,y){tableInsert(x,y);return x}, keys=orders.SecID, parameters=orders,\n            initFunc=def(x){t = table(100:0, x.keys(), each(type, x.values())); tableInsert(t, x); return t})\n```\n\n----------------------------------------\n\nTITLE: Remove rows with NULL values (row-wise - method 1)\nDESCRIPTION: This DolphinDB script removes rows containing NULL values from a table by checking each row. Uses a lambda expression and the `each` function to iterate through the table rows and filter out rows where either 'id' or 'id2' column has a NULL value. Not efficient for large datasets.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\nt[each(x -> !(x.id == NULL || x.id2 == NULL), t)]\n```\n\n----------------------------------------\n\nTITLE: 性能测试：各引擎在不同场景的平均及首次耗时\nDESCRIPTION: 通过对设备最新100条、所有设备最新状态、短时统计和全天明细四个场景的对比，展示 DolphinDB TSDB 在点查上的优越性能，首次查询平均约2毫秒，明显优于 OLAP 和ClickHouse。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/iot_query_case.md#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n# 性能测试结果数据表（文本描述，无实际代码）\n```\n\n----------------------------------------\n\nTITLE: Importing Namespaced DolphinDB Module with use\nDESCRIPTION: Imports a module that is located within a subdirectory structure (namespace) under the modules directory using the use keyword. The import statement must specify the full namespace path to the module. This import is session-local.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/module_tutorial.md#_snippet_9\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nuse system::log::fileLog\n```\n\n----------------------------------------\n\nTITLE: Receiving MQTT Data with DolphinDB Plugin\nDESCRIPTION: Loads the DolphinDB MQTT plugin and subscribes to an MQTT topic. It first defines a JSON parser (`createJsonParser`) specifying the data types and column names for incoming JSON messages. Then, it uses `mqtt::subscribe` to connect to the MQTT broker (host, port), subscribe to a specific topic, apply the defined parser (`sp`), and direct the parsed data into the `doorRecord` stream table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_engine_anomaly_alerts.md#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\nloadPlugin(getHomeDir()+\"/plugins/mqtt/PluginMQTTClient.txt\")\nsp = createJsonParser([INT,INT,DATETIME, BOOL,SYMBOL,INT,SYMBOL], \n  `recordType`doorEventCode`eventDate`readerType`sn`doorNum`card)\nmqtt::subscribe(host, port, topic, sp, objByName(`doorRecord))\n```\n\n----------------------------------------\n\nTITLE: Calling snapCreate Function in DolphinDB\nDESCRIPTION: This snippet defines the database name and table name and calls the `snapCreate` function to create the database and table. It relies on the definition of `snapCreate` function. The result is a new DolphinDB database and a partitioned table with specified schema, partitioning, sorting, and compression.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/appendices_market_replay_bp/snap_create.txt#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndbName = \"dfs://Test_snapshot\"\ntbName = \"snapshot\"\nsnapCreate(dbName, tbName)\n```\n\n----------------------------------------\n\nTITLE: Executing DolphinDB Data Reload with Replay Function in DolphinDB\nDESCRIPTION: This snippet loads CSV stock snapshot data into a DolphinDB table and performs a high-speed replay (accelerated by 1000x) of historical stock data using the replay function. Dependencies include DolphinDB's loadTable, loadText, and replay functions, operating on data stored in specified paths. It facilitates time-accelerated backtesting or historical analysis with clear setup of schema and data ingestion parameters.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_auto_sub.md#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//replay\nfilePath = \"/data/snapshot/stockData.csv\"\nschema = table(loadTable(\"dfs://snapshot\", \"snapshot\").schema().colDefs.name as name, loadTable(\"dfs://snapshot\", \"snapshot\").schema().colDefs.typeString as type)\nsnapshot = loadText(filename=filePath, schema=schema)\nreplay(inputTables=snapshot, outputTables=snapshotStream, dateColumn=`Date, timeColumn=`Time, replayRate=1000, absoluteRate=true)\n```\n\n----------------------------------------\n\nTITLE: Loading Level2 Snapshot Data from DolphinDB Database using DolphinDB\nDESCRIPTION: Loads snapshot data from a specified DolphinDB distributed file system (DFS) database and table into memory. The database and table names are configurable and must be modified as appropriate for the deployment environment. The stock list used for filtering is predefined as a vector of SecurityIDs.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/metacode_derived_features/metacode_derived_features_arrayVector.txt#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\nstockList=`601318`600519`600036`600276`601166`600030`600887`600016`601328`601288`600000`600585`601398`600031`601668`600048\n=dbName = \"dfs://SH_TSDB_snapshot_ArrayVector\"\ntableName = \"snapshot\"\nsnapshot = loadTable(dbName, tableName)\n```\n\n----------------------------------------\n\nTITLE: Submitting a Job to Replay Historical Trade Data in DolphinDB\nDESCRIPTION: This snippet submits a job named \"replay_trade\" to DolphinDB, which replays the filtered trade data using specified parameters. It calls the replay function with data and stream configurations, including batch size and replay mode. Dependencies include submitJob and replay functions. The purpose is to automate historical data replay for testing or analysis.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/streaming_capital_flow_daily/03.replay.txt#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\nsubmitJob(\"replay_trade\", \"trade\",  replay{t, tradeOriginalStream, `TradeTime, `TradeTime, 50000, true, 1})\n```\n\n----------------------------------------\n\nTITLE: Defining sum_diff Function\nDESCRIPTION: This snippet defines a user-defined function `sum_diff` in DolphinDB script. The function calculates the difference of two inputs divided by their sum. It's a stateless function as it doesn't rely on historical data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/reactive_state_engine.md#_snippet_0\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\ndef sum_diff(x, y){\n    return (x-y)/(x+y)\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring NTP Client to Sync from Local Server via ntp.conf (config)\nDESCRIPTION: Modify `/etc/ntp.conf` on the NTP client to synchronize time from the specified local NTP server (e.g., 192.168.0.30), disabling external public pools as needed. This keeps cluster time consistent for DolphinDB reliability.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/deploy_dolphindb_on_new_server.md#_snippet_7\n\nLANGUAGE: config\nCODE:\n```\n# vi /etc/ntp.conf\n...\n# Use public servers from the pool.ntp.org project.\n# Please consider joining the pool (http://www.pool.ntp.org/join.html).\nserver 192.168.0.30 iburst\n# server 0.centos.pool.ntp.org iburst\n# server 1.centos.pool.ntp.org iburst\n# server 2.centos.pool.ntp.org iburst\n# server 3.centos.pool.ntp.org iburst\n...\n\n```\n\n----------------------------------------\n\nTITLE: Importing Transformed Data using a Custom Transform Function - DolphinDB Script\nDESCRIPTION: This snippet shows how to import data into a distributed table using 'loadTextEx' in DolphinDB with a custom transform function 'toArrayVector' applied to each batch. The function merges bid/ask columns into arrays and reorders table columns before insertion. Prerequisites include definition of 'dbpath', 'tbName', 'snapFile', the schema object, and the custom 'toArrayVector' function, along with suitable file permissions and access to DolphinDB distributed database functionality.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_csv.md#_snippet_24\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndb=database(dbpath)\ndb.loadTextEx(tbName, `Tradetime`SecurityID, snapFile, schema=schemas, transform=toArrayVector)\n```\n\n----------------------------------------\n\nTITLE: Implementing Detailed Runtime Logging in Entrust Data Load Function - DolphinDB Script\nDESCRIPTION: This detailed function 'loadEntrust' manages the entrust data import process across a date range, including deleting existing data partitions if present, verifying file existence, handling large data import in batches, and capturing detailed runtime logs via insertions into an in-memory mutable table 'infoTb'. Messages on deletion, CSV file size, and success are recorded along with intermediate prints. Key dependencies are modules 'stockData::stockDataLoad' and 'stockData::stockDataProcess'. It uses functions like 'dropPartition', 'loadText', 'processEntrust', and 'loadTable' for ETL operations. Throws exceptions if source files are missing, ensuring robust error handling.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/dolphinscheduler_integration.md#_snippet_20\n\nLANGUAGE: DolphinDB\nCODE:\n```\nmodule stockData::stockDataLoad\nuse stockData::stockDataProcess\n\ndef loadEntrust(userName, userPassword, startDate, endDate, dbName, tbName, filePath, loadType,mutable infoTb)\n{\n    \tfor(loadDate in startDate..endDate)\n    \t{\n    \t\t// 删除已有数据\n    \t\tdateString = temporalFormat(loadDate,\"yyyyMMdd\")\n    \t\tdataCount = exec count(*) from loadTable(dbName, tbName) where date(tradeTime)=loadDate\n    \t\t// 如果表里面已经存在当天要处理的数据，删除库里面已有数据\n    \t\tif(dataCount != 0){\n    \t\t\tmsg = \"Start to delete the entrust data, the delete date is: \" + dateString\n    \t\t\tprint(msg)\n            // 将运行信息添加到表中\n    \t\t\tinfoTb.tableInsert(msg)\n\n    \t\t\tdropPartition(database(dbName), loadDate, tbName)\n    \t\t\tmsg = \"Successfully deleted the entrust data, the delete date is: \" + dateString\n    \t\t\tprint(msg)\n    \t\t\tinfoTb.tableInsert(msg)\n    \t\t}\n    \t\t// 数据导入\n    \t\t// 判断数据csv文件是否存在\n    \t\tfileName = filePath + \"/\" + dateString + \"/\" + \"entrust.csv\"\n    \t\tif(!exists(fileName))\n    \t\t{\n    \t\t\tthrow fileName + \"不存在!请检查数据源!\"\n    \t\t}\n    \t\t// 如果是全市场数据，数据量较大，因此分批导入\n    \t\tschemaTB = schemaEntrust()\n    \t\ttmpData1 = loadText(filename=fileName, schema=schemaTB)\n    \t\ttmpData1,n1,n2 = processEntrust(loadDate,tmpData1)\n    \t\tpt = loadTable(dbName,tbName)\n    \t\tmsg = \"the data size in csv file is :\" + n2 + \", the duplicated count is \" + (n1 - n2)\n    \t\tprint(msg)\n    \t\tinfoTb.tableInsert(msg)\n    \t\tfor(i in 0..23)\n    \t\t{\n    \t\t\tstartTime = 08:00:00.000 + 1200 * 1000 * i\n    \t\t\ttmpData2 = select * from tmpData1 where time(TradeTime)>=startTime and time(TradeTime)<(startTime+ 1200 * 1000)\n    \t\t\tif(size(tmpData2) < 1)\n    \t\t\t{\n    \t\t\t\tcontinue\n    \t\t\t}\n    \t\t\t//数据入库\n    \t\t\tpt.append!(tmpData2)\n    \t\t}\n    \t\tmsg = \"successfully loaded!\"\n    \t\tprint(msg)\n    \t\tinfoTb.tableInsert(msg)\n    \t}\n}\n```\n\n----------------------------------------\n\nTITLE: Repartitioning Data Between DolphinDB Databases\nDESCRIPTION: Shows two approaches for moving data between databases with different partition schemes: directly loading all data into memory (suitable for small datasets) or using repartitionDS with map-reduce to process data in chunks (suitable for large datasets that exceed memory capacity).\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/database.md#_snippet_12\n\nLANGUAGE: DolphinDB\nCODE:\n```\nallData=select * from loadTable(\"dfs://db1\",\"tb1\")\ntb2=loadTable(\"dfs://db2\",\"tb2\")\ntb2.append!(allData)\n\ndef writeDataTo(dbPath, tbName, mutable tbdata){\n\tloadTable(dbPath,tbName).append!(tbdata)\n}\n\ndatasrc=repartitionDS(<select * from loadTable(\"dfs://db1\",\"tb1\")>,`date,VALUE,dates)\nmr(ds=datasrc, mapFunc=writeDataTo{\"dfs://db2\",\"tb2\"}, parallel=true)\n```\n\n----------------------------------------\n\nTITLE: Subscribing to DolphinDB Stream with Python API\nDESCRIPTION: This Python code enables streaming on a DolphinDB session 's' and subscribes asynchronously to a streaming data table with customized parameters. It includes hostname, port, user credentials, the handler callback function, streamDeserializer instance, and subscription details such as offset and resubscribe control. The subscription is asynchronous and requires synchronization like 'event.wait()' to keep the main thread active. Dependencies include DolphinDB Python API and a valid session 's'. Output is streamed data processed in the specified handler.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/appendices_market_replay_bp.md#_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\ns.enableStreaming(0)\ns.subscribe(host=hostname, port=portname, handler=myHandler, tableName=\"replay_\"+uuidStr, actionName=\"replay_stock_data\", offset=0, resub=False, msgAsTable=False, streamDeserializer=sd, userName=\"admin\", password=\"123456\")\nevent.wait()\n```\n\n----------------------------------------\n\nTITLE: Real-time Rolling Window Calculation with Time Series Engine in DolphinDB\nDESCRIPTION: This code sets up a time series engine to calculate the sum of the 'volume' column for each 'sym' within a 1-minute rolling window. It subscribes to the 'trades' stream table, appends incoming data to the engine, and outputs the results to the 'output1' table. The `createTimeSeriesEngine` function is used to define the engine's parameters.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/window_cal.md#_snippet_22\n\nLANGUAGE: DolphinDB\nCODE:\n```\nshare streamTable(1000:0, `time`sym`volume, [TIMESTAMP, SYMBOL, INT]) as trades\noutput1 = table(10000:0, `time`sym`sumVolume, [TIMESTAMP, SYMBOL, INT])\ntimeSeries1 = createTimeSeriesEngine(name=\"timeSeries1\", windowSize=60000, step=60000, metrics=<[sum(volume)]>, dummyTable=trades, outputTable=output1, timeColumn=`time, useSystemTime=false, keyColumn=`sym, garbageSize=50, useWindowStartTime=false)\nsubscribeTable(tableName=\"trades\", actionName=\"timeSeries1\", offset=0, handler=append!{timeSeries1}, msgAsTable=true);\n\ninsert into trades values(2018.10.08T01:01:01.785,`A,10)\ninsert into trades values(2018.10.08T01:01:02.125,`B,26)\ninsert into trades values(2018.10.08T01:01:10.263,`B,14)\ninsert into trades values(2018.10.08T01:01:12.457,`A,28)\ninsert into trades values(2018.10.08T01:02:10.789,`A,15)\ninsert into trades values(2018.10.08T01:02:12.005,`B,9)\ninsert into trades values(2018.10.08T01:02:30.021,`A,10)\ninsert into trades values(2018.10.08T01:04:02.236,`A,29)\ninsert into trades values(2018.10.08T01:04:04.412,`B,32)\ninsert into trades values(2018.10.08T01:04:05.152,`B,23)\n\nsleep(10)\n\nselect * from output1;\n\n\n//to drop the time series engine\ndropStreamEngine(`timeSeries1)\nunsubscribeTable(tableName=\"trades\", actionName=\"timeSeries1\")\nundef(\"trades\",SHARED)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Time Series Engine Status - DolphinDB\nDESCRIPTION: This code query retrieves status information for the Time Series Engine, a built-in DolphinDB component for incremental computation of minute-level metrics over streaming data. getStreamEngineStat().TimeSeriesEngine returns operational statistics confirming engine initialization and activity.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_auto_sub.md#_snippet_6\n\nLANGUAGE: dolphindb\nCODE:\n```\ngetStreamEngineStat().TimeSeriesEngine\n```\n\n----------------------------------------\n\nTITLE: Creating Composite Partitioned Table by TradeDate and SecurityID in DolphinDB Script to Optimize Query Performance\nDESCRIPTION: Defines a composite partition database combining value partition by date and hash partition by security symbol, then creates a partitioned table with relevant stock columns. This structure improves query speed for queries filtering by both fields, resolving the inefficiency caused by partitioning only by symbol (2.7.2 solution). Requires DolphinDB and composite partition support.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/database_and_table_creation_details.md#_snippet_14\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndb1 = database(, VALUE, 2020.01.01..2021.01.01)\ndb2 = database(, HASH, [SYMBOL, 25])\ndb = database(\"dfs://testDB2\", partitionType=COMPO, partitionScheme=[db1, db2])\ncolName = `SecurityID`TradeDate`TradeTime`TradePrice`TradeQty`TradeAmount`BuyNo`SellNo\ncolType = `SYMBOL`DATE`TIME`DOUBLE`INT`DOUBLE`INT`INT\ntbSchema = table(1:0, colName, colType)\ndb.createPartitionedTable(table=tbSchema, tableName=`pt, partitionColumns=`TradeDate`SecurityID)\n```\n\n----------------------------------------\n\nTITLE: Calculate NULL value counts for table columns\nDESCRIPTION: This DolphinDB script calculates the number of NULL values in each column of a table using the `each` higher-order function. It applies a lambda function to each column, subtracting the count of non-NULL elements from the total size of the column. Requires a table 't' to be defined.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\neach(x->x.size() - x.count(), t.values())\n```\n\n----------------------------------------\n\nTITLE: 生成基金调整净值的面板数据\nDESCRIPTION: 提取相关列，生成基于交易日期和基金ID的面板数据，用于时间序列分析，支持并行处理以提高效率。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/public_fund_basic_analysis.md#_snippet_9\n\nLANGUAGE: DolphinDB\nCODE:\n```\noriData = select TradeDate, SecurityID, AdjNetValue from fundNetValue\npanelData = panel(row=oriData.TradeDate, col=oriData.SecurityID, metrics=oriData.AdjNetValue, rowLabel=workdays, parallel=true)\n```\n\n----------------------------------------\n\nTITLE: Measuring Single-Threaded Import Time in DolphinDB\nDESCRIPTION: This script measures the time taken to import all the generated files sequentially using a single thread. It calls the previously defined `writeData` function (which uses `loadTextEx` internally) with the complete list of file paths and uses the `timer` command to report the total elapsed time.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_csv.md#_snippet_10\n\nLANGUAGE: dolphindb\nCODE:\n```\ntimer writeData(db, dataFilePath);\n```\n\n----------------------------------------\n\nTITLE: Update Factor in Narrow Table (DolphinDB)\nDESCRIPTION: This function updates an existing factor's values in a narrow table. It iterates through time partitions and calls `singleModelSinglePartitionUpdate` (not defined here) to update the factor values within each partition.  It can update either sequentially or in parallel using `ploop`. Dependencies: `getTimeList`, `singleModelSinglePartitionUpdate`, and `ploop`.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practices_for_multi_factor.md#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//窄表模式更新1个因子\ndef singleModelUpdateFactor(dbname,tbname,start_date,end_date,update_factor,parallel = false){   //parallel=true表示并行更新\n\ttime_list = getTimeList(start_date,end_date)\n\tstart_time_list,end_time_list = [],[] \n\tfor(i in 0..(time_list.size()-1)){\n\t\tstart_time_list.append!(time_list[i][0])\n\t\tidx = time_list[i].size()-1\n\t\tend_time_list.append!(time_list[i][idx])\n\t}\n\tif(!parallel){\n\t\tfor(i in 0..(start_time_list.size()-1)){\n\t\t\tsingleModelSinglePartitionUpdate(dbname,tbname,start_time_list[i],end_time_list[i],update_factor)\n\t\t}\t\t\n\t}else{\n\t\tploop(singleModelSinglePartitionUpdate{dbname,tbname,,,update_factor},start_time_list,end_time_list)\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: Creating and Persisting 'capitalFlowStream' Table - DolphinDB\nDESCRIPTION: Creates 'capitalFlowStream', a stream table for tracking capital flows with time, instrument, and categorized amounts. Shares and persists this table with similar performance and retention parameters as the others. Filter column is set for 'SecurityID.' Suited for real-time monitoring of financial data streams in DolphinDB.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/streaming_capital_flow_order_by_order/03.清理环境并创建相关流数据表.txt#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//create stream table: capitalFlow\ncolName = `TradeTime`SecurityID`BuySmallAmount`BuyBigAmount`SellSmallAmount`SellBigAmount\ncolType =  `TIMESTAMP`SYMBOL`DOUBLE`DOUBLE`DOUBLE`DOUBLE\ncapitalFlowStreamTemp = streamTable(1000000:0, colName, colType)\ntry{ enableTableShareAndPersistence(table=capitalFlowStreamTemp, tableName=\"capitalFlowStream\", asynWrite=true, compress=true, cacheSize=1000000, retentionMinutes=1440, flushMode=0, preCache=10000) } catch(ex){ print(ex) }\nundef(\"capitalFlowStreamTemp\")\ngo\nsetStreamTableFilterColumn(capitalFlowStream, `SecurityID)\n```\n\n----------------------------------------\n\nTITLE: Calculate Level 10 Net Order Imbalance Increase in Python with DolphinDB and Pandas\nDESCRIPTION: Defines a Python function `level10Diff` to compute the net order imbalance increase over a specified lag window using Level 2 snapshot data from DolphinDB. It loads data lazily from 'dfs://TL_Level2', filters for specific dates/securities, handles potential Array Vector columns ('BidPrice', 'BidOrderQty') using `apply` and `shift`, calculates amounts within valid price ranges, computes the difference sum, and applies a rolling sum. Demonstrates lazy evaluation (`lazy=True`, `compute()`) and grouped application (`groupby.apply`) for parallel processing. Requires `pandas` and `dolphindb` libraries.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_Python_Parser_Intro_for_Quantitative_Finance.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nimport pandas as pd\nimport dolphindb as ddb\n\n# 定义因子函数\ndef level10Diff(df, lag=20):\n    temp = df[[\"TradeTime\", \"SecurityID\"]]\n    temp[\"bid\"] = df[\"BidPrice\"].fillna(0)\n    temp[\"bidAmt\"] = df[\"BidOrderQty\"].fillna(0) * df[\"BidPrice\"].fillna(0)\n    temp[\"prevbid\"] = temp[\"bid\"].shift(1).fillna(0)\n    temp[\"prevbidAmt\"] = temp[\"bidAmt\"].shift(1).fillna(0)\n    temp[\"bidMin\"] = temp[\"bid\"].apply(\"min\")\n    temp[\"bidMax\"] = temp[\"bid\"].apply(\"max\")\n    temp[\"prevbidMin\"] = temp[\"bidMin\"].shift(1).fillna(0)\n    temp[\"prevbidMax\"] = temp[\"bidMax\"].shift(1).fillna(0)\n    temp[\"pmin\"] = temp[[\"bidMin\", \"prevbidMin\"]].max(axis=1)\n    temp[\"pmax\"] = temp[[\"bidMax\", \"prevbidMax\"]].max(axis=1)\n    amount = temp[\"bidAmt\"]*((temp[\"bid\"]>=temp[\"pmin\"])&(temp[\"bid\"]<=temp[\"pmax\"]))\n    lastAmount = temp[\"prevbidAmt\"]*((temp[\"prevbid\"]>=temp[\"pmin\"])&(temp[\"prevbid\"]<=temp[\"pmax\"]))\n    temp[\"amtDiff\"] = amount.apply(\"sum\") - lastAmount.apply(\"sum\")\n    temp[\"amtDiff\"] = temp[\"amtDiff\"].rolling(lag, 1).sum()\n    return temp[[\"TradeTime\", \"SecurityID\", \"amtDiff\"]].fillna(0)\n\n# 指定计算某一天一只股票的因子\nsnapshotTB = loadTable(\"dfs://TL_Level2\", \"snapshot\")\ndf = pd.DataFrame(snapshotTB, index=\"Market\", lazy=True)\ndf = df[(df[\"TradeTime\"].astype(ddb.DATE)==2023.02.01)&(df[\"SecurityID\"]==\"000001\")]\nres = level10Diff(df.compute(), 20)\n\n# 指定计算某一天的因子\nsnapshotTB = loadTable(\"dfs://TL_Level2\", \"snapshot\")\ndf = pd.DataFrame(snapshotTB, index=\"Market\", lazy=True)\nres = df[df[\"TradeTime\"].astype(ddb.DATE)==2023.02.01][[\"TradeTime\", \"SecurityID\", \"BidPrice\", \"BidOrderQty\"]].groupby([\"SecurityID\"]).apply(lambda x:level10Diff(x, 20))\n```\n\n----------------------------------------\n\nTITLE: Calculating Stock Return Volatility\nDESCRIPTION: This snippet calculates the monthly volatility of a stock.  It uses the `interval` function to group data by month and the `std` function to calculate the standard deviation of the return rates within each month. The `fill` parameter is set to 'prev', using the previous month's value to fill missing values if no rates exist in a given month.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_sql_cases.md#_snippet_25\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntimer res = select std(rate) from t group by code, interval(date(date), 1, \"prev\")\n```\n\n----------------------------------------\n\nTITLE: Querying - Left Join DolphinDB\nDESCRIPTION: This snippet performs a left join between the `readings` and `device_info` tables based on `device_id`. The results are then filtered by time and grouped by several columns. It uses `lsj` which is shorthand for a left semi join (or left join with filtering for matching records).  This demonstrates combining information from multiple tables.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs TimescaleDB/test_dolphindb_small.txt#_snippet_20\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 11. 关联查询.左连接：列出所有的 WiFi，及其连接设备的型号、系统版本，并去除重复条目\ntimer\nselect count(*)\nfrom lsj(readings, device_info, 'device_id')\nwhere time between 2016.11.15 07:00:00 : 2016.11.15 07:01:00\ngroup by ssid, bssid, time, model, os_name\norder by ssid, time\n```\n\n----------------------------------------\n\nTITLE: Top N Query with Nested SQL in DolphinDB\nDESCRIPTION: SQL query to find the top 3 devices with highest average CPU utilization in a specific time period. This demonstrates nested queries and the top/limit clause for ranking results.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/iot_examples.md#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect top 3 id \nfrom (\n\tselect avg(value) as avg\n\tfrom sensors \n\twhere mod(id, 50)=1, datetime between 2020.09.01T00:00:00 : 2020.09.01T00:59:59  \n\tgroup by id order by avg desc\n\t)\n```\n\n----------------------------------------\n\nTITLE: Calculating Daily Fund Returns in DolphinDB\nDESCRIPTION: Filters data for actual working days within the trade date range using the 'workday' table. Queries adjusted net values, generates a panel dataset (time series matrix) with dates as rows and securities as columns, forward-fills missing values (up to 10 consecutive days), calculates daily percentage changes (returns), and displays the first 10 rows of the panel data and the first 3 rows of the returns matrix. Requires 'publicFundNetValue' and 'workday' tables.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/public_fund_basic_analysis/basic_analysis_of_public_fund_net_value.txt#_snippet_3\n\nLANGUAGE: dolphindb\nCODE:\n```\n// filter data for working days\ndateRange = exec distinct(TradeDate) from fundNetValue\nfirstDate = min(dateRange) \nlastDate =  max(dateRange)\nworkdays = exec day from loadTable(\"dfs://publicFundDB\", \"workday\") where market=\"SSE\", day between firstDate : lastDate, day in dateRange\n// query the daily net value data of the fund and assign it to the variable oriData\noriData = select TradeDate, SecurityID, AdjNetValue from fundNetValue\n// generate panel data\npanelData = panel(row=oriData.TradeDate, col=oriData.SecurityID, metrics=oriData.AdjNetValue, rowLabel=workdays, parallel=true)\n// query panel data\npanelData[0:10]\n// Calculate daily rate of return on panel data\nreturnsMatrix = panelData.ffill(10).percentChange()\n// query partial results\nreturnsMatrix[0:3]\n```\n\n----------------------------------------\n\nTITLE: Parallel Processing of Reactive State Engine\nDESCRIPTION: This code demonstrates how to use multiple Reactive State Engines in parallel using the `subscribeTable` function's `hash` and `filter` parameters.  It sets up multiple engines, and distributes the processing load among them based on the hash value of the `sym` column and sets up the filter on the hash values.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/reactive_state_engine.md#_snippet_9\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\ndef sum_diff(x, y){\n    return (x-y)/(x+y)\n}\nfactor1 = <ema(1000 * sum_diff(ema(price, 20), ema(price, 40)),10) -  ema(1000 * sum_diff(ema(price, 20), ema(price, 40)), 20)>\n\nshare streamTable(1:0, `sym`price, [STRING,DOUBLE]) as tickStream\nsetStreamTableFilterColumn(tickStream, `sym)\nshare streamTable(1000:0, `sym`factor1, [STRING,DOUBLE]) as resultStream\n\nfor(i in 0..3){\n    rse = createReactiveStateEngine(name=\"reactiveDemo\"+string(i), metrics =factor1, dummyTable=tickStream, outputTable=resultStream, keyColumn=\"sym\")\n    subscribeTable(tableName=`tickStream, actionName=\"sub\"+string(i), handler=tableInsert{rse}, msgAsTable = true, hash = i, filter = (4,i))\n}\n\nn=2000000\ntmp = table(take(\"A\"+string(1..4000), n) as sym, rand(10.0, n) as price)\ntickStream.append!(tmp)\n```\n\n----------------------------------------\n\nTITLE: Querying Backup Metadata with getBackupList Function in DolphinDB\nDESCRIPTION: Retrieves all backup information for a distributed table's partitions using getBackupList. The function returns a table with details such as chunkID, chunkPath, and partition ID (cid) for each partition's backup, enabling users to view all existing backups. Input parameters specify backup directory, database path, and table name. This is useful for backup management and verification tasks.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/restore-backup.md#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\ngetBackupList(\"/hdd/hdd1/backup/\", \"dfs://ddb\", \"windTurbine\")\n```\n\n----------------------------------------\n\nTITLE: Creating and Subscribing Daily Time Series Engines for Real-Time Capital Flow Aggregation in DolphinDB Script\nDESCRIPTION: Within the parallel loop, creates multiple DailyTimeSeriesEngines named 'tradeTSAggr1', 'tradeTSAggr2', etc., each configured with a 60-second window and step to aggregate capital flow metrics computed by the 'calCapitalFlow' function. The engines process input from the 'tradeProcessStream' output, grouping by security ID and using trade time as the time column with system time disabled. Each engine subscribes to its corresponding 'tradeProcessStream' with appropriate batch, throttle, hash, and filter settings for parallel data handling.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/streaming_capital_flow_order_by_order/04.注册流计算引擎和订阅流数据表.txt#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\ncreateDailyTimeSeriesEngine(name=\"tradeTSAggr\"+string(i), windowSize=60000, step=60000, metrics=[<calCapitalFlow(Num, BSFlag, TradeQty, TradeAmount) as `BuySmallAmount`BuyBigAmount`SellSmallAmount`SellBigAmount>], dummyTable=tradeProcessStream, outputTable=capitalFlowStream, timeColumn=\"TradeTime\", useSystemTime=false, keyColumn=`SecurityID, useWindowStartTime=true, forceTriggerTime=60000)\n\tsubscribeTable(tableName=\"tradeProcessStream\", actionName=\"tradeTSAggr\"+string(i), offset=-1, handler=getStreamEngine(\"tradeTSAggr\"+string(i)), msgAsTable=true, batchSize=2000, throttle=1, hash=parallel+i-1, filter = (parallel, i-1), reconnect=true)\n```\n\n----------------------------------------\n\nTITLE: 创建基本时序引擎并订阅流数据表示例\nDESCRIPTION: 示范如何创建一个简单的时序引擎，设置6毫秒窗口和3毫秒步长，计算交易量总和，并将其订阅到流数据表。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/stream_aggregator.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\nshare streamTable(1000:0, `time`volume, [TIMESTAMP, INT]) as trades\noutputTable = table(10000:0, `time`sumVolume, [TIMESTAMP, INT])\ntradesAggregator = createTimeSeriesEngine(name=\"streamAggr1\", windowSize=6, step=3, metrics=<[sum(volume)]>, dummyTable=trades, outputTable=outputTable, timeColumn=`time)\nsubscribeTable(tableName=\"trades\", actionName=\"append_tradesAggregator\", offset=0, handler=append!{tradesAggregator}, msgAsTable=true)\n```\n\n----------------------------------------\n\nTITLE: Defining DFS Table Schema DolphinDB Script\nDESCRIPTION: Defines the structural blueprint (schema) for the partitioned table intended for the DFS. It specifies the column names (`hardwareId`, `ts`, `temp1`, `temp2`, `temp3`) and their respective data types (INT, TIMESTAMP, DOUBLE), used later when creating the actual persistent table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/iot_demo_script.txt#_snippet_8\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\ntableSchema=table(1:0,`hardwareId`ts`temp1`temp2`temp3,[INT,TIMESTAMP,DOUBLE,DOUBLE,DOUBLE])\n```\n\n----------------------------------------\n\nTITLE: Replay Historical Data in DolphinDB\nDESCRIPTION: This snippet replays historical trade data from a DFS table into a stream table for testing and simulation purposes.  It selects data within a specific time range, orders it by `TradeTime` and `SecurityID`, and uses the `replay` function to inject the data into the `tradeOriginalStream` table. It uses the submitJob function for asynchronous execution.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_capital_flow_daily.md#_snippet_9\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//replay history data\nt = select * from loadTable(\"dfs://trade\", \"trade\") where time(TradeTime) between 09:30:00.000 : 15:00:00.000 order by TradeTime, SecurityID\nsubmitJob(\"replay_trade\", \"trade\",  replay{t, tradeOriginalStream, `TradeTime, `TradeTime, 50000, true, 1})\ngetRecentJobs()\n```\n\n----------------------------------------\n\nTITLE: Creating a Scheduled Job with Module Function in DolphinDB\nDESCRIPTION: This example shows how to create a scheduled job that uses a function from a module, which is serialized at job creation time.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/scheduledJob.md#_snippet_11\n\nLANGUAGE: DolphinDB\nCODE:\n```\nuse printLog\ndef f5(){\n\tprintLogs(\"test my log\")\n}\nscheduleJob(`testModule, \"f5\", f5, 13:32m, today(), today(), 'D');\n```\n\n----------------------------------------\n\nTITLE: Creating Ten-Minute Frequency Factor Library Table with TSDB Engine in DolphinDB\nDESCRIPTION: This DolphinDB code snippet creates a TSDB database and table for storing ten-minute frequency factor data. The database is partitioned monthly by tradetime and by factor name. The table holds tradetime (timestamp), securityid, value, and factorname columns. It is partitioned by tradetime and factorname, sorted by securityid and tradetime, allows duplicates, and applies a sortKeyMappingFunction hashBucket with size 500 to manage key cardinality for optimal performance.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practices_for_partitioned_storage.md#_snippet_23\n\nLANGUAGE: DolphinDB\nCODE:\n```\ncreate database \"dfs://tenMinutesFactorDB\" \npartitioned by VALUE(2023.01M..2023.06M), \nVALUE(`f1`f2), \nengine='TSDB'\n\ncreate table \"dfs://tenMinutesFactorDB\".\"tenMinutesFactorTB\"(\n    tradetime TIMESTAMP[comment=\"时间列\", compress=\"delta\"], \n    securityid SYMBOL, \n    value DOUBLE, \n    factorname SYMBOL\n)\npartitioned by tradetime, factorname,\nsortColumns=[`securityid, `tradetime], \nkeepDuplicates=ALL, \nsortKeyMappingFunction=[hashBucket{, 500}]\n```\n\n----------------------------------------\n\nTITLE: Performing Set Operations: Union and Union All in DolphinDB SQL\nDESCRIPTION: Demonstrates combining results from two queries into one using set operations union all and union. 'union all' merges including duplicates, while 'union' returns distinct combined records. The examples show obtaining location IDs from different tables with and without removal of duplicates, ordering the results accordingly.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Standard_SQL_in_DolphinDB.md#_snippet_5\n\nLANGUAGE: SQL\nCODE:\n```\nselect location_id from locations \nunion all \nselect location_id from departments\norder by location_id\n```\n\nLANGUAGE: SQL\nCODE:\n```\nselect location_id from locations \nunion  \nselect location_id from departments\norder by location_id\n```\n\n----------------------------------------\n\nTITLE: Creating a Range-Partitioned Table for Query Optimization Examples in DolphinDB\nDESCRIPTION: Sets up a range-partitioned table with 10 million rows for demonstrating query optimization principles. The table is partitioned by date in 2-month intervals starting from 1990.01.01, used to illustrate which types of queries allow partition pruning.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/database.md#_snippet_14\n\nLANGUAGE: DolphinDB\nCODE:\n```\nn=10000000\nid=take(1..1000, n).sort()\ndate=1989.12.31+take(1..10000, n)\nx=rand(1.0, n)\ny=rand(10, n)\nt=table(id, date, x, y)\ndb=database(\"dfs://rangedb1\", RANGE, date(1990.01M+(0..200)*2))\npt = db.createPartitionedTable(t, `pt, `date)\npt.append!(t);\n\npt=db.loadTable(`pt);\n```\n\n----------------------------------------\n\nTITLE: Creating a Shared Stream Table for Replay Output with DolphinDB Script\nDESCRIPTION: Creates a shared streamTable according to the schema of the quotes table, preparing an output destination for replay or real-time streaming calculations. This output stream table is used for downstream subscription or aggregation pipelines. Requires schema extraction from an existing table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/historical_data_replay.md#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\nsch = quotes.schema().colDefs\nshare streamTable(100:0, sch.name, sch.typeString) as outQuotes\n```\n\n----------------------------------------\n\nTITLE: Creating Non-Partitioned In-Memory Table from Matrix\nDESCRIPTION: Shows how to create a non-partitioned in-memory table from a matrix using the `table` function. The matrix is converted into a table with columns C0, C1, etc.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/partitioned_in_memory_table.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\nm1=1..6$3:2\ntable(m1)\n\nC0 C1\n-- --\n1  4 \n2  5 \n3  6 \n\nm2=7..12$3:2\ntable(m1,m2)\n\nC0 C1 C2 C3\n-- -- -- --\n1  4  7  10\n2  5  8  11\n3  6  9  12\n\n```\n\n----------------------------------------\n\nTITLE: Subscribing 1-Minute K-Line Stream Table to DFS Partitioned Table in DolphinDB Script\nDESCRIPTION: This snippet subscribes the 1-minute OHLC streaming table output to a DFS partitioned table for persistent batch insertion. The subscription is configured with a large batch size (5000) and a throttle delay of 1 second to optimize throughput during real-time streaming data writes.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/K.md#_snippet_19\n\nLANGUAGE: DolphinDB\nCODE:\n```\nsubscribeTable(\n\ttableName=mdlStockFundOHLCTBName,\n\tactionName=mdlStockFundOHLCTBName,\n\thandler=loadTable(\"dfs://stockFundStreamOHLC\", \"stockFundStreamOHLC\"),\n\tmsgAsTable=true,\n\tbatchSize=5000,\n\tthrottle=1,\n\thash=0,\n\treconnect=true)\n```\n\n----------------------------------------\n\nTITLE: Creating Distributed Table for Shanghai Trades (DolphinDB Script)\nDESCRIPTION: Checks if the table `trade_sh` exists in the `dfs://stockL2` database. If not, it creates a partitioned table using the schema stored in `tradeSchema`. The table is partitioned by `Tradedate` and `InstrumentID` and sorted by `InstrumentID` and `TransactTime`. `keepDuplicates=ALL` retains all records.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_implementation_for_NSQ_Plugin.md#_snippet_10\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\ndbName = \"dfs://stockL2\"\ntbName = \"trade_sh\"\nif(existsTable(dbName, tbName) == false){\n\tdb = database(dbName)\n\tdb.createPartitionedTable(table=table(1:0, tradeSchema.name, tradeSchema.type), tableName=tbName, partitionColumns=`Tradedate`InstrumentID, sortColumns=`InstrumentID`TransactTime, keepDuplicates=ALL)\n\tprint(\"DFS table created successfully !\")\n}\nelse{\n\tprint(\"DFS table have been created !\")\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Distributed Table for Shenzhen Orders (DolphinDB Script)\nDESCRIPTION: Checks if the table `orders_sz` exists in the `dfs://stockL2` database. If not, it creates a partitioned table using the schema stored in `ordersSchema`. The table is partitioned by `TradeDate` and `InstrumentID` and sorted by `InstrumentID` and `TransactTime`. `keepDuplicates=ALL` retains all records.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_implementation_for_NSQ_Plugin.md#_snippet_9\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\ndbName = \"dfs://stockL2\"\ntbName = \"orders_sz\"\nif(existsTable(dbName, tbName) == false){\n\tdb = database(dbName)\n\tdb.createPartitionedTable(table=table(1:0, ordersSchema.name, ordersSchema.type), tableName=tbName, partitionColumns=`TradeDate`InstrumentID, sortColumns=`InstrumentID`TransactTime, keepDuplicates=ALL)\n\tprint(\"DFS table created successfully !\")\n}\nelse{\n\tprint(\"DFS table have been created !\")\n}\n```\n\n----------------------------------------\n\nTITLE: Prediction and Warning Function Definition - DolphinDB\nDESCRIPTION: This code defines the `predictAndwarning` function which performs model training, prediction, and warning calculation. It queries data from `DataTable_dfs` and `AggrTable_dfs` to train a KNN model, predicts values, and calculates anomaly rates. If the anomaly rate exceeds the `warning_threshold`, a warning is triggered and inserted into the `warningTable`. It simulates prediction values with random data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/knn_iot.md#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\n/*\n * 根据 已有周期数据，对未来数据进行预测\n */\n def predictAndwarning(devices_number,rate,interval_past,interval_future,warning_threshold,mutable whether_start,DataTable_dfs,AggrTable_dfs,mutable predictTable,mutable warningTable){\n    do{\n      if(whether_start==false) {\n        curtime = select top 1 time from DataTable_dfs  //第一次是从表中查询最开始的时间\n        curtime = curtime[`time][0]\n        curtime_aggr = select top 1 time from AggrTable_dfs //每次尽量去与训练数据的时间段同步\n        curtime_aggr = curtime_aggr[`time][0]\n        whether_start = true\n      }\n      curtime = curtime + interval_past*1000 //以后是直接往后推，最开始的时间往后推 10s中的间隔时间\n      table_origin = select * from DataTable_dfs where time<=curtime and time>(curtime - interval_past*1000) //查询当前时间前 interval 秒的数据\n      if(table_origin.size()<interval_past*devices_number*rate) //如果查询数据小于正常查询到的数据数目\n      {\n        \n         curtime = select top 1 time from DataTable_dfs order by time desc //从表中查询最近的时间作为开始时间\n         curtime = curtime[`time][0]\n         table_origin = select * from DataTable_dfs where time<=curtime and time>(curtime - interval_past*1000) //查询当前时间的前 interval 秒的数据\n      }\n\n\n      //训练模型\n      factors = sql(sqlCol([`wind,`humidity,`air_pressure,`temperature]), table_origin).eval()\n      labels = table_origin[`propertyValue]\n      model = knn(labels,factors,\"regressor\", 200);\n\n\n      //模型预测\n      curtime_aggr = curtime_aggr + interval_past*1000\n      table_aggr = select * from AggrTable_dfs where time<=curtime_aggr and time>(curtime_aggr - interval_past*1000) order by deviceCode\n\t //查询当前时间前 interval 秒的数据\n      if(table_aggr.size()<interval_past*devices_number*rate/10) //如果查询数据小于正常查询到的数据数目\n      {\n         curtime_aggr = select top 1 time from AggrTable_dfs order by time desc //从表中查询最近的时间作为开始时间\n         curtime_aggr = curtime_aggr[`time][0]\n         table_aggr = select * from AggrTable_dfs where time<=curtime_aggr and time>(curtime_aggr - interval_past*1000) //查询当前时间的前 interval 秒的数据\n      }\n      pre_labels = sql(sqlCol([`wind,`humidity,`air_pressure,`temperature]), table_aggr).eval()\n      //pre_values = predict(model,pre_labels)\n      //////////////////////////////暂时用随机值代替///////////////////////////////////////////\n      table_number =  table_aggr.size()\n      x1 = randNormal(25,2,table_number)  //训练数据\n      x2 = randNormal(55,5,table_number)\n      x3 = randNormal(1.01325,0.00001,table_number)\n      x4 = randNormal(75,5,table_number)\n      x5 = int(randNormal(10,3,table_number))\n      b1 = randNormal(0.4,0.05,table_number) \n      b2 = randNormal(0.3,0.05,table_number)\n      b3 = randNormal(0.2,0.05,table_number)\n      b4 = randNormal(0.09,0.05,table_number)\n      b5 = randNormal(0.01,0.001,table_number)\n      bias = randNormal(5,1,table_number)\n      propertyValue = int(b1*x1*10 + b2*x2*2 + b3*x3*1000 + b4*x4 + b5*x5 +bias)\n      pre_values = propertyValue //暂时用随机值代替\n      /////////////////////////////////////////////////////////////////////////////////////////////\n    \n      time =take(curtime_aggr + interval_future*1000+interval_future+(0..(1000-1)),table_number) //\n      deviceCode = sort(take(1..devices_number,table_number))\n      predicttempTable = table(time,deviceCode,pre_labels,pre_values as `propertyValue_predicted)\n      predictTable.append!(predicttempTable) //预测结果导入流表\n\n      //进行预警\n      contrastTable = select propertyValue,propertyValue_predicted from lj(table_aggr,predicttempTable,`wind`humidity`air_pressure`temperature) //利用左连接\n      abnormal_count = exec count(*) from contrastTable where propertyValue_predicted<0.8*propertyValue or propertyValue_predicted>1.2*propertyValue\n      \n      warning_time = curtime_aggr //进行预警的时间\n      abnormal_rate = abnormal_count*1.0 / table_number\n      whether_warning = 0 //默认不进行预警\n      if(abnormal_rate>warning_threshold) whether_warning = 1 //当异常率超过阈值进行预警\n\n      insert into warningTable values(warning_time,abnormal_rate, whether_warning);\n      \n      sleep(10000) //每10s进行一次预测\n    }while(true)\n}\ndevices_number = 100 //设备数目\nrate = 1000 //每台设备 每秒钟1000条数据\ninterval_past = 10 ////查询过去10秒的数据\ninterval_future = 10 //预测未来第10秒的数据\nwarning_threshold = 0.215 //当异常值率大于0.215时进行预警\nwhether_start = false //标记\nDataTable_dfs = loadTable(dataTable_dbname,dataTable_tbname)\nAggrTable_dfs = loadTable(aggr_dbname,aggr_tbname)\nsubmitJob(`predictAndwarning,`predictAndwarning,predictAndwarning,devices_number,rate,interval_past,interval_future,warning_threshold,whether_start,DataTable_dfs,AggrTable_dfs,predictTable,warningTable) //每10s 预测一次\n```\n\n----------------------------------------\n\nTITLE: Migrating Data in DolphinDB Cluster\nDESCRIPTION: This snippet demonstrates how to migrate data in a DolphinDB cluster by backing up databases in an old cluster and restoring them in a new cluster. It uses the `backupDB` function to backup the `testdb` and `testdb_tsdb` databases and the `migrate` function to restore these databases in the new cluster. The backup files are stored in the `/home/$USER/migrate` directory.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/backup-restore-new.md#_snippet_11\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//旧集群备份testdb和testdb_tsdb\ndbPath=\"dfs://testdb\"\ndbPath2=\"dfs://testdb_tsdb\"\nbackupDir=\"/home/$USER/migrate\"\nsubmitJob(\"backupForMigrate\",\"backup testdb for migrate\",backupDB,backupDir,dbPath)\nsubmitJob(\"backupForMigrate2\",\"backup testdb_tsdb for migrate\",backupDB,backupDir,dbPath2)\n\n//备份完成后，在新集群恢复这两个数据库\nbackupDir=\"/home/$USER/migrate\"\nsubmitJob(\"migrate\",\"migrate testdb and testdb_tsdb to new cluster\",migrate,backupDir)\n```\n\n----------------------------------------\n\nTITLE: Defining Feature Calculation Function in DolphinDB Script\nDESCRIPTION: Defines a user-defined function `featureEngine` that takes bid/offer prices and quantities (as array vectors) as input and calculates several financial metrics: Bid-Ask Spread (BAS), Depth Imbalance (DI) for multiple levels, Weighted Averaged Price (WAP), Order Book Pressure (Press), and Realized Volatility (RV). It leverages array vector operations for efficient computation.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/machine_learning_volatility/05.streamComputingArrayVector.txt#_snippet_3\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\n//define function to process data with matrix operation\ndefg featureEngine(bidPrice,bidQty,offerPrice,offerQty){\n\tbas = offerPrice[0]\\bidPrice[0]-1\n\twap = (bidPrice[0]*offerQty[0] + offerPrice[0]*bidQty[0])\\(bidQty[0]+offerQty[0])\n\tdi = (bidQty-offerQty)\\(bidQty+offerQty)\n\tbidw=(1.0\\(bidPrice-wap))\n\tbidw=bidw\\(bidw.rowSum())\n\tofferw=(1.0\\(offerPrice-wap))\n\tofferw=offerw\\(offerw.rowSum())\n\tpress=log((bidQty*bidw).rowSum())-log((offerQty*offerw).rowSum())\n\trv=std(log(wap)-log(prev(wap)))*sqrt(24*252*size(wap))\n\treturn avg(bas),avg(di[0]),avg(di[1]),avg(di[2]),avg(di[3]),avg(di[4]),avg(di[5]),avg(di[6]),avg(di[7]),avg(di[8]),avg(di[9]),avg(press),rv\n}\n```\n\n----------------------------------------\n\nTITLE: Connecting and Uploading Data - Python\nDESCRIPTION: This code snippet connects to a DolphinDB node and uploads the Pandas DataFrame as a table named 'myTable' to DolphinDB.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Array_Vector.md#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\n// 连接 DolphinDB 节点\ns = ddb.session()\ns.connect(\"127.0.0.1\", 8848, \"admin\", \"123456\")\n// 上传数据到内存表\ns.table(data=df, tableAliasName=\"myTable\")\n```\n\n----------------------------------------\n\nTITLE: Memory-Efficient Online Synchronization Using sqlDS and mr Functions - DolphinDB Script\nDESCRIPTION: This snippet enables online synchronization when the restore cluster memory is insufficient to hold the entire day's data, by partitioning the data source and processing each partition sequentially. It constructs multiple data sources using sqlDS from a filtered table, then applies a map-reduce style function mr to apply writeRemoteDB on each data source without parallel execution (parallel=false). The writeRemoteDB function connects to the remote database, logs in, and runs a remote writeData function to append data by partition. This approach significantly reduces memory footprint while ensuring incremental data transfer. Scheduling capability is also shown for daily execution.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/data_synchronization_between_clusters.md#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef writeData(dbName,tableName,t) : loadTable(dbName,tableName).append!(t)\ndef writeRemoteDB(t, ip, port, dbName,tableName,writeData){\n\tconn = xdb(ip, port)\n\tconn(login{`admin,`123456})\n\tremoteRun(conn,writeData,dbName,tableName,t)\n}\ndef synDataBaseOnline(ip, port){\n\tds = sqlDS(<select * from loadTable(\"dfs://db1\",\"mt\") where Timestamp > timestamp(date(now())) and Timestamp < now()>)\n\tmr(ds, writeRemoteDB{,ip,port,\"dfs://db1\",\"mt\",writeData},,, false)\n}\nlogin(`admin,`123456)\nrestoreServerIP = '115.239.209.234'\nrestoreServerPort = 18848\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nsynDataBaseOnline(restoreServerIP,restoreServerPort)\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nscheduleJob(\"syncDB\",\"syncDB\",synDataBaseOnline{restoreServerIP,restoreServerPort},22:30m,2019.01.01,2030.12.31,'D')\n```\n\n----------------------------------------\n\nTITLE: Subscribing to MQTT Topic for Data Ingestion - DolphinDB\nDESCRIPTION: This function subscribes to a specific MQTT topic (`topic`) on a given server (`host`, `port`). It uses a JSON parser (`sp`) to parse incoming messages into the structure expected by the `doorRecord` stream table and appends the parsed data to it.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/streaming_engine_anomaly_alerts/流计算引擎级联检测门禁超时数据.txt#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//4.5 从MQTT服务器接收数据\ndef writeStreamTable(host, port, topic){\n\tsp = createJsonParser([INT,INT,DATETIME, BOOL,SYMBOL,INT,SYMBOL], \n\t`recordType`doorEventCode`eventDate`readerType`sn`doorNum`card)\n\tmqtt::subscribe(host, port, topic, sp, objByName(`doorRecord))\n}\n```\n\n----------------------------------------\n\nTITLE: Covariance Matrix Calculation with nested loops\nDESCRIPTION: This DolphinDB script calculates the covariance matrix of a mutable matrix using nested for loops. It first fills NULL values with 0.0, then iterates through rows and columns to compute the covariance between each pair of columns. This is a less efficient approach compared to using the `cross` function.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ndef matlab_cov(mutable matt){\n\tnullFill!(matt,0.0)\n\trowss,colss=matt.shape()\n\tmsize = min(rowss, colss)\n\tdf=matrix(float,msize,msize)\n\tfor (r in 0..(msize-1)){\n\t\tfor (c in 0..(msize-1)){\n\t\t\tdf[r,c]=covar(matt[:,r],matt[:,c])\n\t\t}\n\t}\n\treturn df\n}\n```\n\n----------------------------------------\n\nTITLE: Starting Controller and Agent on Single Server\nDESCRIPTION: Commands to start the controller and agent processes for a pseudo-high availability cluster on a single server with multiple DolphinDB instances.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/service_deployment_and_migration.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd dolphindb_1/server/clusterdemo;\nsh startController.sh\nsh startAgent.sh\n\ncd dolphindb_2/server/clusterdemo;\nsh startController.sh\n\ncd dolphindb_3/server/clusterdemo;\nsh startController.sh\n```\n\n----------------------------------------\n\nTITLE: Adding Cluster Replication Configuration - Slave\nDESCRIPTION: This snippet adds the `clusterReplicationMasterCtl=10.0.0.1:8848` configuration to the slave cluster's controller.cfg file. This parameter specifies the address of the master cluster's control node. It directs the slave to connect to the specified control node for replication tasks. The configuration is applied using `vim`.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Asynchronous_Replication.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nclusterReplicationMasterCtl=10.0.0.1:8848\n```\n\n----------------------------------------\n\nTITLE: Schema Viewing DolphinDB\nDESCRIPTION: This snippet uses the `schema` function to display the schema of the `device_info` and `readings` tables. This is useful for verifying table structures and data types after loading data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs TimescaleDB/test_dolphindb_small.txt#_snippet_9\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// ----------------- 查看 schema\nschema(device_info)\nschema(readings)\n```\n\n----------------------------------------\n\nTITLE: Updating License and Starting DolphinDB Cluster Nodes on Linux\nDESCRIPTION: This snippet contains shell commands for license file replacement and cluster node startup on Linux. License files for enterprise or community editions are placed in '/DolphinDB/server/dolphindb.lic'. Permissions are adjusted for the base executable with 'chmod +x dolphindb'. Cluster nodes (controller and agent) are started using shell scripts in the clusterDemo directory with 'sh startController.sh' and 'sh startagent.sh'. Users can verify running nodes using 'ps aux|grep dolphindb'. This setup assumes a Linux shell environment with sufficient permissions and that the unzipped files reside in the expected directory structure.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/single_machine_cluster_deploy.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\nchmod +x dolphindb\n```\n\nLANGUAGE: Shell\nCODE:\n```\nsh startController.sh\n```\n\nLANGUAGE: Shell\nCODE:\n```\nsh startagent.sh\n```\n\nLANGUAGE: Shell\nCODE:\n```\nps aux|grep dolphindb\n```\n\n----------------------------------------\n\nTITLE: Batch Processing Configuration in subscribeTable Using batchSize and throttle in DolphinDB\nDESCRIPTION: Details use of batchSize and throttle parameters to control the rate at which handlers process incoming streamed messages. If batchSize is positive, the handler processes messages only after the batch reaches this count; throttle defines the maximum wait time in seconds to trigger handler execution if batchSize is not met. Provides an example where batchSize is 11: before reaching 11 messages the handler does not process data. Inputs are streamed data rows; outputs are updates on subscribed tables after batch processing. This helps control memory and processing efficiency during high data throughput scenarios.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_tutorial.md#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\nshare streamTable(10000:0,`timestamp`temperature, [TIMESTAMP,DOUBLE]) as pubTable\nshare streamTable(10000:0,`ts`temp, [TIMESTAMP,DOUBLE]) as subTable1\ntopic1 = subscribeTable(tableName=\"pubTable\", actionName=\"act1\", offset=-1, handler=subTable1, msgAsTable=true, batchSize=11)\nvtimestamp = 1..10\nvtemp = 2.0 2.2 2.3 2.4 2.5 2.6 2.7 0.13 0.23 2.9\ntableInsert(pubTable,vtimestamp,vtemp)\n\nprint size(subTable1)\n\ninsert into pubTable values(11,3.1)\nprint size(subTable1)\n```\n\n----------------------------------------\n\nTITLE: Using Handler and Offset Parameters in subscribeTable for Stream Data Processing in DolphinDB\nDESCRIPTION: Explains the handler parameter to process streamed data either as a direct data table insertion or via a user-defined function that can filter or transform data before storing. The offset parameter controls the start point of subscription data consumption relative to the stream data table rows. Examples include inserting raw data into subscribing tables or filtering data with a handler function before inserting. This snippet requires user’s custom handler functions or target tables. Inputs include messages from the subscription stream, either passed as tables or tuples. Outputs are updated subscription tables or modified data storage. The code shows data manipulation in handlers and how subscriptions react to different offset values.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_tutorial.md#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef myhandler(msg){\n\t t = select * from msg where temperature>0.2\n\t if(size(t)>0)\n\t\t subTable2.append!(t)\n}\nshare streamTable(10000:0,`timestamp`temperature, [TIMESTAMP,DOUBLE]) as pubTable\nshare streamTable(10000:0,`ts`temp, [TIMESTAMP,DOUBLE]) as subTable1\nshare streamTable(10000:0,`ts`temp, [TIMESTAMP,DOUBLE]) as subTable2\ntopic1 = subscribeTable(tableName=\"pubTable\", actionName=\"act1\", offset=-1, handler=subTable1, msgAsTable=true)\ntopic2 = subscribeTable(tableName=\"pubTable\", actionName=\"act2\", offset=-1, handler=myhandler, msgAsTable=true)\n\nvtimestamp = 1..10\nvtemp = 2.0 2.2 2.3 2.4 2.5 2.6 2.7 0.13 0.23 2.9\ntableInsert(pubTable,vtimestamp,vtemp)\n```\n\n----------------------------------------\n\nTITLE: Publishing Table Data to MQTT Topic - DolphinDB\nDESCRIPTION: This function connects to an MQTT server (`server`, `1883`) and publishes the contents of a table (`t`) to a specified topic (`topic`). It uses a given formatter (`f`) and sends data in batches (`batchsize`). The connection is closed after publishing.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/streaming_engine_anomaly_alerts/流计算引擎级联检测门禁超时数据.txt#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//推送表数据到mqtt服务器\ndef publishTableData(server,topic,f, batchsize,t){\n    \tconn=connect(server,1883,0,f,batchsize)\n   \t\tpublish(conn,topic,t)\n    \tclose(conn)\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Full Data Processing Tasks with DolphinDBOperator (Python/DolphinDB)\nDESCRIPTION: This snippet shows how to define multiple DolphinDB tasks (load snapshot, process snapshot, calculate minute factor, calculate daily factor) for full data processing using DolphinDBOperator within a Python environment (likely Airflow). Each operator instance executes a DolphinDB script that clears cache, undefines variables, loads necessary modules, retrieves parameters from a shared table (`paramTable`), potentially creates database/tables if they don't exist, processes data (often submitting background jobs for parallelism), and performs error checking.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Best_Practices_for_DolphinDB_and_Python_AirFlow.md#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\n   loadSnapshot = DolphinDBOperator(\n        task_id='loadSnapshot',\n        dolphindb_conn_id='dolphindb_test',\n        sql='''\n            pnodeRun(clearAllCache)\n            undef(all)\n            go;\n            //使用 module，加载已封装好的建表及入库函数\n            use loadSnapshot::createSnapshotTable\n            use  loadSnapshot::loadSnapshotData\n            //通过参数共享表获取参数\n            params = dict(paramTable[`param], paramTable[`value])\n            dbName = params[`ETL_dbName_origin]\n            tbName = params[`ETL_tbName_origin]\n            startDate = date(params[`ETL_start_date])\n            endDate = date(params[`ETL_end_date])\n            fileDir = params[`ETL_filedir]\n            //结果库表不存在则创建\n            if(not existsDatabase(dbName)){\n                loadSnapshot::createSnapshotTable::createSnapshot(dbName, tbName)\n            }\n            //调用清洗函数，后台多进程写入，提高写入效率\n            start = now()\n            for (loadDate in startDate..endDate){\n                submitJob(\"loadSnapshot\"+year(loadDate)+monthOfYear(loadDate)+dayOfMonth(loadDate), \"loadSnapshot\", loadSnapshot::loadSnapshotData::loadSnapshot{, dbName, tbName, fileDir}, loadDate)\n            }\n            //查看写入任务是否完成，以保证后续处理部分数据源完整\n            do{\n                cnt = exec count(*) from getRecentJobs() where jobDesc=\"loadSnapshot\" and endTime is null\n            }\n            while(cnt != 0)\n            //查看导入过程中是否有异常，有异常则抛出异常\n            cnt = exec count(*) from pnodeRun(getRecentJobs) where jobDesc=\"loadSnapshot\" and errorMsg is not null and receivedTime > start\n            if (cnt != 0){\n                error = exec errorMsg from pnodeRun(getRecentJobs) where jobDesc=\"loadSnapshot\" and errorMsg is not null and receivedTime > start\n                throw error[0]\n            }\n            '''\n    )\n    processSnapshot = DolphinDBOperator(\n        task_id='processSnapshot',\n        dolphindb_conn_id='dolphindb_test',\n        sql='''\n            pnodeRun(clearAllCache)\n            undef(all)\n            go;\n            //使用 module，加载已封装好的建表及入库函数\n            use processSnapshot::createSnapshot_array\n            use processSnapshot::processSnapshotData\n            //通过参数共享表获取参数\n            params = dict(paramTable[`param], paramTable[`value])\n            dbName_orig = params[`ETL_dbName_origin]\n            tbName_orig = params[`ETL_tbName_origin]\n            dbName_process = params[`ETL_dbName_process]\n            tbName_process = params[`ETL_tbName_process]\n            startDate = date(params[`ETL_start_date])\n            endDate = date(params[`ETL_end_date])\n            //结果库表不存在则创建\n            if(not existsDatabase(dbName_process)){\n                processSnapshot::createSnapshot_array::createProcessTable(dbName_process, tbName_process)\n            }\n            //后台多进程处理，提高处理效率\n            start = now()\n            for (processDate in startDate..endDate){\n                submitJob(\"processSnapshot\"+year(processDate)+monthOfYear(processDate)+dayOfMonth(processDate), \"processSnapshot\", processSnapshot::processSnapshotData::process{, dbName_orig, tbName_orig, dbName_process, tbName_process}, processDate)\n            }\n            //查看清洗任务是否完成，以保证后续处理部分数据源完整\n            do{\n                cnt = exec count(*) from getRecentJobs() where jobDesc=\"processSnapshot\" and endTime is null\n            }\n            while(cnt != 0)\n            //查看清洗过程中是否有异常，有异常则抛出异常\n            cnt = exec count(*) from pnodeRun(getRecentJobs) where jobDesc=\"processSnapshot\" and errorMsg is not null and receivedTime > start\n            if (cnt != 0){\n                error = exec errorMsg from pnodeRun(getRecentJobs) where jobDesc=\"processSnapshot\" and errorMsg is not null and receivedTime > start\n                throw error[0]\n            }\n            '''\n    )\n    calMinuteFactor = DolphinDBOperator(\n        task_id='calMinuteFactor',\n        dolphindb_conn_id='dolphindb_test',\n        sql='''\n            pnodeRun(clearAllCache)\n            undef(all)\n            go;\n            //使用 module，加载已封装好的建表及入库函数\n            use Factor::createFactorOneMinute\n            use Factor::calFactorOneMinute\n            //通过参数共享表获取参数\n            params = dict(paramTable[`param], paramTable[`value])\n            dbName = params[`ETL_dbName_process]\n            tbName = params[`ETL_tbName_process]\t\n            dbName_factor = params[`ETL_dbName_factor]\n            tbName_factor = params[`ETL_tbName_factor]\n            //结果库表不存在则创建\n            if(not existsDatabase(dbName_factor)){\n                createFactorOneMinute(dbName_factor, tbName_factor)\n            }\n            factorTable = loadTable(dbName_factor, tbName_factor)\n            //调用计算函数\n            calFactorOneMinute(dbName, tbName,factorTable)\n            '''\n    )\n    calDailyFactor = DolphinDBOperator(\n        task_id='calDailyFactor',\n        dolphindb_conn_id='dolphindb_test',\n        sql='''\n            pnodeRun(clearAllCache)\n            undef(all)\n            go;\n            //使用 module，加载已封装好的建表及入库函数\n            use Factor::createFactorDaily\n            use Factor::calFactorDaily1\t\n            //通过参数共享表获取参数\n            params = dict(paramTable[`param], paramTable[`value])\n            dbName = params[`ETL_dbName_process]\n            tbName = params[`ETL_tbName_process]\t\n            dbName_factor = params[`ETL_dbName_factor_daily]\n            tbName_factor = params[`ETL_tbName_factor_daily]\n            //结果库表不存在则创建\n            if(not existsDatabase(dbName_factor)){\n                createFactorDaily(dbName_factor, tbName_factor)\n            }\n            //调用计算函数\n            factorTable = loadTable(dbName_factor, tbName_factor)\n            Factor::calFactorDaily1::calFactorDaily(dbName, tbName,factorTable)\n            '''\n    )\n```\n\n----------------------------------------\n\nTITLE: Declaring Namespaced DolphinDB Modules\nDESCRIPTION: Illustrates how to declare modules when they are organized in subdirectories within the modules path. The module declaration includes the full path as the namespace, using :: separators. For example, `module system::log::fileLog` declares a module located at modules/system/log/fileLog.dos.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/module_tutorial.md#_snippet_8\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nmodule system::log::fileLog\n```\n\n----------------------------------------\n\nTITLE: Configuring DolphinDB Data Node in Properties File Format\nDESCRIPTION: A comprehensive configuration file for DolphinDB data nodes with settings for memory allocation, thread management, disk usage, networking, and streaming capabilities. Contains commented explanations for each parameter and their default values.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/ha_cluster_deployment/P2/config/config-specification.txt#_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\n# DolphinDB Configuration File for Data Node\n#\n############################# Thread-related Parameters #############################\n# The number of local executors. The default value is the number of CPU cores - 1.\n# e.g. if the system has 4 physical cores, the number of localExecutors will be 3 \n#localExecutors=3\n#\n# The maximum number of batch job workers. The default value is the value of workerNum.\n#maxBatchJobWorker=4\n#\n# The maximum number of dynamic workers. The default value is the value of workerNum.\n#maxDynamicWorker=4\n#\n# The size of the web worker pool to process HTTP requests. The default value is 1.\n#webWorkerNum=1\n#\n# The size of worker pool for regular interactive jobs. The default value is the number of CPU cores.\n#workerNum=4\n#\n############################# Memory-related Parameters #############################                         \n# The capacity of cache engine in units of GB. After cache engine is enabled, data is not written to disk until data in cache exceeds 30% of hunkCacheEngineMemSize. \n# The default value is 0 indicating the cache engine is not enabled. To enable the cache engine, we must set chunkCacheEngineMemSize>0 and dataSync=1.\nchunkCacheEngineMemSize=2\n#\n# The maximum memory (in units of GB) allocated to datanode. If set to 0, it means no limits on memory usage.\n#maxMemSize=0\n#\n# The rate at which unused memory is released to the operating system. It is a floating point number between 0 and 10. \n# memoryReleaseRate=0 means that unused memory will not be released actively; memoryReleaseRate=10 means that memory will be released at the fastest speed. \n# The default value is 5.\n#memoryReleaseRate=5\n#\n# The limit on the memory size (in units of MB) of a regular array. Must be a power of 2. The default value is 512.\n#regularArrayMemoryLimit=256\n#\n# When memory usage exceeds warningMemSize (in units of GB) , the system will automatically clean up the cache of some databases to avoid OOM exceptions. \n# The default value is 75% of maxMemSize.\n#warningMemSize=75\n#\n############################# Hard Disk Related Parameters #############################\n# The folder for batch job logs and results. The default value is <HomeDir>/batchJobs.\n#batchJobDir = \n#\n# The folder for the metadata of data chunks on each data node. The default value is <HomeDir>/storage/CHUNK_METADATA. \n#chunkMetaDir =\n#\n# If dataSync=1, DolphinDB generates redo log when the database is being updated. It can avoid data loss in the event of a system crash or power outage. \n# The default value of dataSync is 0.\n#dataSync=0\n#\n# The path and name of the job log file that contains descriptive information of all the queries that have been executed for each node. It must be a csv file. \n# The default folder for the job log file is the log folder. The default name of the job log file is nodeAlias_job.log.\n#jobLogFile=jobLog.csv\n#\n# The path and name of the log file. It displays the server configuration specifications, warnings and error messages.\n#logFile=DolphinDBlog\n#\n# The log file only keeps log information equal to higher than the specified level. From the lowest to the highest level, the possible values are DEBUG, INFO, WARNING and ERROR. The default value is INFO.\n#logLevel=INFO\n#\n# The system will archive the server log after the log reaches the specified size limit (in units of MB). \n# The default value is 1024 and the minimum value is 100. The archived log file name adds a prefix to the original log file name. \n# The prefix is in the format of <date><seq>, e.g. 20181109000. seq has 3 digits and starts with 000.\n#maxLogSize=1024\n#\n# The directory of the redo log. The default value is /log/redoLog.\n#redoLogDir =\n#\n# Clear the redo log of transactions whose data have been persisted at intervals specified by redoLogPurgeInterval in terms of seconds. The default value is 10.\n#redoLogPurgeInterval=10\n#\n#Clear the redo log of transactions whose data have been persisted if the size of the redo log exceeds redoLogPurgeLimit in terms of MB. The default value is 4000.\n#redoLogPurgeLimit=4000\n#\n# The folder where data chunks are saved in the distributed file system on a data node. The default value is <HomeDir>/<nodeAlias>/storage.\n#volumes=/hdd/hdd1/volumes,/hdd/hdd2/volumes,/hdd/hdd3/volumes,/hdd/hdd4/volumes\n#\n# The number of volumes that a data node can use.\n#diskIOConcurrencyLevel=4\n#\n############################# Network-related Parameters #############################\n# Whether to enable HTTPS protocol. The default value is false.\n#enableHTTPS=false\n#\n# The maximum number of connections (from GUI, API, other nodes, etc) to the local node.\nmaxConnections=512\n#\n# The maximum number of remote nodes that can be connected to from the local node.\n#maxConnectionPerSite=2\n#\n# Whether to enable the TCP_NODELAY socket option. The default value is false.\n#tcpNoDelay=1\n#\n############################# Streaming-related Parameters on Publisher node #############################\n# The maximum number of records in a message block. The default value is 1024.\n#maxMsgNumPerBlock=1024\n#\n# The maximum depth (number of records) of a message queue to persist a stream table to disk. The default value is 10,000,000.\n#maxPersistenceQueueDepth=10000000\n#\n# The maximum depth (number of records) of a message queue on the publisher node. The default value is 10,000,000.\n#maxPubQueueDepthPerSite=10000000\n#\n# The maximum number of subscriber nodes that the publisher node can connect to. \n# The default value is 0. For the node to server as a publisher, we must set maxPubConnections>0.\n#maxPubConnections=0\n#\n# The directory where shared streaming tables are persisted to. To enable persistence, persistenceDir must be specified. \n#persistenceDir=/hdd/hdd5/streamingData\n#\n# The number of workers responsible for persisting streaming tables to disk in asynchronous mode. The default value is 0.\n#persistenceWorkerNum=1\n#\n############################# Streaming-related Parameters on Subscriber node #############################\n# The maximum number of publishers that the subscriber node can connec to. The default value is 64.\n#maxSubConnections=64\n#\n# The maximum depth (number of records) of a message queue on the subscriber node.\n#maxSubQueueDepth=10000000\n#\n# The directory to save the offset of the last subscribed message that has been processed. \n# If persistOffsetDir is not specified, it will be saved under the directory specified by persistenceDir. \n# If persistenceDir is not specified, it will be saved in the streamLog folder under the home directory.\n#persistOffsetDir=streamlog\n#\n# A Boolean value indicating whether streaming executors use pooling mode. The default value is false.\n#subExecutorPooling=true\n#\n# The number of message processing threads in the subscriber node. Only when subscription is enabled is this parameter relevant. \n# The default value is 1. If it is set to 0, it means the thread can conduct message parsing and can also process messages.\n#subExecutors=2\n#\n# The port number that the subscription thread is listening on. This parameter must be specified to enable the node(s) to serve as subscriber(s).\n#subPort=8000\n#\n############################# System Management Parameters #############################\n# The maximum number of partitions that a single query can search. The default value is 65536.\n#maxPartitionNumPerQuery=65536\n#\n# The directory for the module files. The default value is the relative directory \"modules\" that needs to be created by the user. \n# The system searches the relative directory \"modules\" in the following order: home directory of the node, the working directory of the node, and the directory with the DolphinDB executable.\n#moduleDir=modules\n#\n# How the system deals with new data that is outside the partition scheme for a VALUE domain (or a VALUE domain in a COMPO domain). \n# If set to \"skip\", new data is not saved and no exception is thrown; if set to \"fail\", new data is not saved and an exception is thrown; \n# if set to \"add\", new partitions are created to save the new data.\nnewValuePartitionPolicy=add\n#\n# Whether to enable performance monitoring. The default value is false for the standalone mode and true for the cluster mode.\n#perfMonitoring=1\n#\n# The directory for the plugin files. The default value is the relative directory \"plugins\". \n# The system searches the relative directory \"plugins\" in the following order: home directory of the node, the working directory of the node, and the directory with the DolphinDB executable.\n#pluginDir=plugins\n#\n# The modules or plugins that are loaded after the system starts. Use commas to separate multiple modules/plugins.\n#preloadModules=plugins::mysql, system::log::fileLog\n#\n# This file is executed when the system starts. The default file is <HomeDir>/dolphindb.dos. It usually contains definitions of system-level functions that are visible to all users and cannot be overwritten.\n#init=dolphindb.dos\n#\n# This file is executed after the system starts. The default file is <HomeDir>/startup.dos. It can be used to load plugins, load tables and share them, define and load stream tables, etc.\n#startup=startup.dos\n#\n# This file is executed after the startup script (startup.dos) is executed. The default folder is DolphinDB home directory. It can be used to schedule jobs. After this file is executed, the system terminates.\n#run=dailyJobs.dos\n#\n# The directory of the time zone database (only applies to windows). The default value is home\\server\\tzdb where home is the DolphinDB home directory specified by parameter home.\n#tzdb=C:\\DolphinDB\\server\\tzdb\n#\n# The directory of the web server.\n#webRoot=/DolphinDB/server/web\n#\n# Whether a user must log in to use the web-based cluster manager. The default value is false.\n#webLoginRequired=false\n#\n############################# Streaming-related Parameters for High Availability #############################\n```\n\n----------------------------------------\n\nTITLE: Convert UTC Time to Local Time with localtime in DolphinDB\nDESCRIPTION: This snippet shows how to convert a UTC datetime back to local time using the `localtime` function in DolphinDB. This is used for displaying time data in the user's local time zone after it has been stored in UTC. Assumes a time zone of East 8.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/timezone.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\n$ localtime(2022.05.20 03:30:05)\n2022.05.20 11:30:05\n```\n\n----------------------------------------\n\nTITLE: Restoring Backup Data to DFS Database with restore Function - DolphinDB Script\nDESCRIPTION: This snippet demonstrates using DolphinDB's restore function to recover a table from backup files into a DFS database at a specified path. Parameters include the backup directory (restoreDir), database path (dbPath), target table name (tableName), and partition selection (partition=\"%\" means all partitions). The force parameter enables overwrite of existing data. It requires that the target database with the same name already exists, and the outputTable parameter specifies how to load the restored data into memory post-restore. This method completes the offline synchronization cycle.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/data_synchronization_between_clusters.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\nrestore(backupDir=restoreDir,dbPath=\"dfs://db1\",tableName=\"mt\",partition=\"%\",force=true,outputTable=loadTable(\"dfs://db1\",\"mt\"))\n```\n\n----------------------------------------\n\nTITLE: Calculating Sharpe Ratio in DolphinDB\nDESCRIPTION: This function calculates the Sharpe ratio of a given value series, assuming a risk-free rate of 3%. It takes a value series as input and returns the Sharpe ratio.  It uses the previously defined `getAnnualReturn` and `getAnnualVolatility` functions.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_factor.txt#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\n/**\n * 因子5：夏普比率 无风险收益率按3%\n夏普比率：(年化收益率 - 无风险利率) / 收益年化波动率\n */\ndefg getSharp(value){\n\treturn (getAnnualReturn(value) - 0.03)\\getAnnualVolatility(value) as sharpeRat\n}\n```\n\n----------------------------------------\n\nTITLE: Simulating Matching Engine per Thread and Aggregating Results in DolphinDB Script\nDESCRIPTION: This code orchestrates multi-threaded matching engine simulation by creating per-thread engine_config dictionaries on the fly (to ensure thread safety), calling the plugin test interface with message and symbol lists, and collecting job handles. After all jobs are done, outputs and snapshots from each thread are consolidated into result tables and the temporary tables are cleaned up. Inputs are prepared messages and symbols per thread, as well as the loaded plugin test interface; outputs include global result tables ('tradeResult', 'snapshotResult'). Assumes DolphinDB job APIs and shared-memory table conventions.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/matching_engine_simulator.md#_snippet_9\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef doMatchEngineTest(messageData, symbolList, engineName, marketName, hasSnapshotOutput) {\n    engine_config = dict(string, any)\n    engine_config[\"engineName\"] = engineName\n    engine_config[\"market\"] = marketName\n    engine_config[\"hasSnapshotOutput\"] = hasSnapshotOutput\n    MatchEngineTest::runTestTickData(messageData, symbolList, engine_config)\n}\n\nClearAllMatchEngine()\n\n// 计算模拟撮合的总耗时\ntimer {\n    test_job_list = [] // 存储了所有测试作业的id\n    hasSnapshotOutput = false\n\n    for (i in 1..thread_num) {\n        messages = objByName(\"MatchEngineTest\" + i)\n        test_job_list.append!(submitJob(\"EngineTestJob\" + i, \"\", doMatchEngineTest, \n                            messages, symbol_list[i-1], \"MatchEngineTest\" + i, \n                            market_name, hasSnapshotOutput))\n    }\n    // 等待所有作业结束后返回\n    for (i in 0 : thread_num) {\n        getJobReturn(test_job_list[i], true)\n    }\n}\n\ntradeResult = table(100:0, MatchEngineTest1_tradeOutputTable.schema().colDefs.name, MatchEngineTest1_tradeOutputTable.schema().colDefs.typeString)\nsnapshotResult = table(100:0, MatchEngineTest1_snapshotOutputTable.schema().colDefs.name, MatchEngineTest1_snapshotOutputTable.schema().colDefs.typeString)\nfor (i in 1..thread_num) {\n    tradeResult.append!(objByName(\"MatchEngineTest\"+i+\"_tradeOutputTable\"))\n    snapshotResult.append!(objByName(\"MatchEngineTest\"+i+\"_snapshotOutputTable\"))\n}\n\n// 结果合并到tradeResult表后，删除每个线程的tradeOutputTable表和snapshotOutputTable表\nfor (i in 1..thread_num) {\n    try {\n        undef(\"MatchEngineTest\" + i + \"_tradeOutputTable\", SHARED)\n        undef(\"MatchEngineTest\" + i + \"_snapshotOutputTable\", SHARED)\n    } catch(ex) {print(ex)}\n}\n```\n\n----------------------------------------\n\nTITLE: Creating DolphinDB Reactive State Engine\nDESCRIPTION: Initializes a stream calculation engine designed for stateful time-series calculations, demonstrating basic setup with metrics, input/output tables, grouping, and snapshot configuration for potential recovery.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_tutorial.md#_snippet_16\n\nLANGUAGE: DolphinDB\nCODE:\n```\nrse = createReactiveStateEngine(name=\"reactiveDemo\", metrics =<cumsum(price)>, dummyTable=tickStream, outputTable=result, keyColumn=\"sym\", snapshotDir= \"/home/data/snapshot\", snapshotIntervalInMsgCount=20000)\n```\n\n----------------------------------------\n\nTITLE: Creating Database and Table for Results in DolphinDB\nDESCRIPTION: This code segment creates a database and a partitioned table to store the processed results. It first checks if the database exists and drops it if it does. It then defines the table schema, creates the table with partitioning based on TradeTime, and specifies delta compression. This prepares the environment to store the final calculated results.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/machine_learning_volatility/02.dataProcessArrayVector.txt#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\nstoreDBName = \"dfs://sz50VolatilityDataSet\"\nstoreTBName = \"sz50VolatilityDataSet\"\nif(existsDatabase(storeDBName)){\n\tdropDatabase(storeDBName)\n}\ndb = database(storeDBName, RANGE, sort(distinct(yearBegin(2000.01.01..2040.01.01))))\nname = `SecurityID`TradeTime`BAS`DI0`DI1`DI2`DI3`DI4`DI5`DI6`DI7`DI8`DI9`Press`RV`targetRV\ntype = `SYMBOL`TIMESTAMP`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE\ntbSchema = table(1:0, name, type)\ndb.createPartitionedTable(table=tbSchema, tableName=storeTBName, partitionColumns=`TradeTime, compressMethods={TradeTime:\"delta\"})\ngo\n```\n\n----------------------------------------\n\nTITLE: Stopping DolphinDB Cluster Nodes and Backing Up Configuration (Shell)\nDESCRIPTION: This shell command sequence stops all running cluster nodes using the 'stopAllNode.sh' script and then creates backups of the configuration directory for recovery or auditing purposes. It is essential to execute these steps before any major cluster upgrade or migration. No parameters are required but must be run with permissions to access target directories; outputs are local configuration backups and service shutdown.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/service_deployment_and_migration.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\ncd /home/dolphindb_1/server/clusterDemo;\nsh stopAllNode.sh;\ncp -r ./config ./config.bak\n\ncd /home/dolphindb_2/server/clusterDemo;\nsh stopAllNode.sh;\ncp -r ./config ./config.bak\n\ncd /home/dolphindb_3/server/clusterDemo;\nsh stopAllNode.sh;\ncp -r ./config ./config.bak\n```\n\n----------------------------------------\n\nTITLE: Making Predictions with Ridge Regression Model\nDESCRIPTION: This code snippet shows how to use a trained lasso regression model to make predictions on a dataset `t`. The `predict` method is called on the model, and the predicted values are returned.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/machine_learning.md#_snippet_17\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nmodel.predict(t)\n```\n\n----------------------------------------\n\nTITLE: Creating and Configuring a Stream Table for Sensor Data in DolphinDB\nDESCRIPTION: Creates a stream table to store sensor data with deviceID, timestamp, and temperature columns. Enables sharing and persistence of the table for real-time data processing.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/alarm.txt#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\nst = streamTable(1000000:0, `deviceID`ts`temperature, [INT,DATETIME,FLOAT])\nenableTableShareAndPersistence(st, `sensor, false, true, 1000000)\n```\n\n----------------------------------------\n\nTITLE: Defining Shared Stream Tables (DolphinDB Script)\nDESCRIPTION: Defines the schema for three shared stream tables (`snapshotStream`, `aggrFeatures10min`, `result10min`) and creates them in memory. These tables serve as queues for data ingestion, processed features, and final prediction results in the streaming pipeline.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/metacode_derived_features/metacode_derived_features_streaming.txt#_snippet_9\n\nLANGUAGE: DolphinDB\nCODE:\n```\nname = `SecurityID`DateTime`PreClosePx`OpenPx`HighPx`LowPx`LastPx`TotalVolumeTrade`TotalValueTrade`BidPrice0`BidPrice1`BidPrice2`BidPrice3`BidPrice4`BidPrice5`BidPrice6`BidPrice7`BidPrice8`BidPrice9`BidOrderQty0`BidOrderQty1`BidOrderQty2`BidOrderQty3`BidOrderQty4`BidOrderQty5`BidOrderQty6`BidOrderQty7`BidOrderQty8`BidOrderQty9`OfferPrice0`OfferPrice1`OfferPrice2`OfferPrice3`OfferPrice4`OfferPrice5`OfferPrice6`OfferPrice7`OfferPrice8`OfferPrice9`OfferOrderQty0`OfferOrderQty1`OfferOrderQty2`OfferOrderQty3`OfferOrderQty4`OfferOrderQty5`OfferOrderQty6`OfferOrderQty7`OfferOrderQty8`OfferOrderQty9\ntype =`SYMBOL`TIMESTAMP`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`INT`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`INT`INT`INT`INT`INT`INT`INT`INT`INT`INT`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`INT`INT`INT`INT`INT`INT`INT`INT`INT`INT\nshare streamTable(100000:0, name, type) as snapshotStream\nshare streamTable(100000:0 , `DateTime`SecurityID <- metaCodeColName <- (metaCodeColName+\"_150\") <- (metaCodeColName+\"_300\") <- (metaCodeColName+\"_450\"),`TIMESTAMP`SYMBOL <- take(`DOUBLE, 676)) as aggrFeatures10min\nshare streamTable(100000:0 , `Predicted`SecurityID`DateTime, `FLOAT`SYMBOL`TIMESTAMP) as result10min\n```\n\n----------------------------------------\n\nTITLE: Defining Python Callback Handler for Stream Consumption\nDESCRIPTION: This Python function 'myHandler' processes data batches received from a DolphinDB streaming subscription, distinguishing message types by their last element which identifies the table type (snapshot, order, transaction, or end). It prints formatted output based on the message type. When the 'end' table type is received, it signals an event to unblock the main thread and terminate the subscription. It requires a threading event object 'event' for synchronization and is used as the callback handler in the subscription.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/appendices_market_replay_bp.md#_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\ndef myHandler(lst):\n    if lst[-1] == \"snapshot\":\n        print(\"SNAPSHOT: \", lst)\n    elif lst[-1] == 'order':\n        print(\"ORDER: \", lst)\n    elif lst[-1] == 'transaction':\n        print(\"TRANSACTION: \", lst)\n    else:\n        print(\"END: \", lst)\n        event.set()\n```\n\n----------------------------------------\n\nTITLE: Renaming Columns and Constructing Subtable for Derived Feature Aggregation in DolphinDB Script\nDESCRIPTION: This DolphinDB code constructs a sub-table combining first-level and second-level indicator columns along with original data columns, then renames columns systematically for uniformity in subsequent meta programming aggregations. It uses DolphinDB's join operator '<-' for vector concatenation of column names. This sub-table acts as the input table for aggregation calculations generating 676 derived features. Inputs must include columns for timestamps, bid/offer prices and quantities, and computed indicators like wap and volume imbalance. The output is a renamed table suitable for batch feature aggregation calls.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/metacode_derived_features.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\nsubTable = table(DateTime as `DateTime, BidPrice, BidOrderQty, OfferPrice, OfferOrderQty, wap, wapBalance, priceSpread, BidSpread, OfferSpread, totalVolume, volumeImbalance, LogReturnWap, LogReturnOffer, LogReturnBid)\\ncolNum = 0..9$STRING\\ncolName = `DateTime <- (`BidPrice + colNum) <- (`BidOrderQty + colNum) <- (`OfferPrice + colNum) <- (`OfferOrderQty + colNum) <- (`Wap + colNum) <- `WapBalance`PriceSpread`BidSpread`OfferSpread`TotalVolume`VolumeImbalance <- (`LogReturn + colNum) <- (`LogReturnOffer + colNum) <- (`LogReturnBid + colNum)\\nsubTable.rename!(colName)\n```\n\n----------------------------------------\n\nTITLE: Cleaning Stream Environment in DolphinDB Script\nDESCRIPTION: This snippet defines a function `cleanEnvironment` to clean up the existing stream environment by unsubscribing from table streams and dropping stream engines and stream tables. It handles exceptions gracefully by catching and printing errors for each operation. The function accepts a parameter `parallel` to iterate over multiple stream engines and unsubscribe or drop them accordingly, supporting parallel processing scenarios. This cleanup ensures a fresh environment before stream table creation.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/streaming_capital_flow_daily/01.createStreamTB.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef cleanEnvironment(parallel){\n\tfor(i in 1..parallel){\n\t\ttry { unsubscribeTable(tableName=\"tradeOriginalStream\", actionName=\"processBuyOrder\" + string(i)) } catch(ex) { print(ex) }\n\t\ttry { dropStreamEngine(\"processBuyOrder\" + string(i)) } catch(ex) { print(ex) }\n\t\ttry { dropStreamEngine(\"processSellOrder\" + string(i)) } catch(ex) { print(ex) }\n\t\ttry { dropStreamEngine(\"processCapitalFlow\" + string(i)) } catch(ex) { print(ex) }\n\t}\n\ttry { unsubscribeTable(tableName=\"capitalFlowStream\", actionName=\"processCapitalFlow60min\") } catch(ex) { print(ex) }\n\ttry { dropStreamEngine(\"processCapitalFlow60min\") } catch(ex) { print(ex) }\n\ttry{ dropStreamTable(`tradeOriginalStream) } catch(ex){ print(ex) }\n\ttry{ dropStreamTable(`capitalFlowStream) } catch(ex){ print(ex) }\n\ttry{ dropStreamTable(`capitalFlowStream60min) } catch(ex){ print(ex) }\n\tundef all\n}\n```\n\n----------------------------------------\n\nTITLE: Manually Flushing OLAP Cache in DolphinDB\nDESCRIPTION: The flushOLAPCache function manually clears completed transactions from the OLAP cache engine, forcing data to be written to disk. Only completed transactions are flushed; in-progress transactions remain in cache.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/redoLog_cacheEngine.md#_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nflushOLAPCache()\n```\n\n----------------------------------------\n\nTITLE: Initializing Environment and Libraries - Python\nDESCRIPTION: This snippet imports necessary Python libraries for data manipulation (pandas, numpy), parallel processing (multiprocessing, os, time), iteration (itertools), function application (functools), progress display (tqdm), and date/time handling (datetime). It also configures pandas display options for wider output and suppresses warnings.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/DolphinDB_Python_Parser_Intro_for_Quantitative_Finance/因子实现_Python版本/十档委买增额.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport numpy as np\nimport itertools\nfrom functools import reduce\nimport os\nimport multiprocessing\nimport time\nimport warnings\nfrom tqdm import tqdm\nimport datetime\n\nwarnings.filterwarnings(\"ignore\")\n\npd.options.display.width = 1200\npd.options.display.max_colwidth = 100\npd.options.display.max_columns = 10\npd.options.mode.chained_assignment = None\n```\n\n----------------------------------------\n\nTITLE: Nested Aggregation with Time Intervals\nDESCRIPTION: This code snippet performs nested aggregation.  It calculates the average value for each ID every 10 seconds, then calculates the minimum of these averages.  The goal is to find the minimum of average values across all IDs and is a good example of data aggregation across time intervals.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/iot_examples.md#_snippet_15\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect min(avg) as minAvg from(\n\tselect avg(value) as avg\n\tfrom sensors \n\twhere id in [1,51,101,151,201], datetime between 2020.09.01T00:00:00 : 2020.09.01T00:59:59  \n\tgroup by id, bar(datetime,10)\n) group by id\n```\n\n----------------------------------------\n\nTITLE: Restoring DolphinDB Table Partitions Using restore Function in Script\nDESCRIPTION: This snippet iterates over a date range to restore DolphinDB data partitions day-by-day from backup directories into an existing database and table. It constructs backup directory paths dynamically, checks for their existence, and calls the restore function with appropriate parameters including force overwrite. The restore process is partition-based and requires the target database and table to preexist. Inputs are backup directories and target database/table names; outputs are restored data available for queries and analysis.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/iot_examples.md#_snippet_35\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nday=2020.01.01\nfor(i in 1:31){\n\tpath=\"/hdd/hdd1/backupDemo/\"+temporalFormat(day, \"yyyyMMdd\") + \"/\";\n\tday=datetimeAdd(day,1,`d)\n\tif(!exists(path)) continue;\n\tprint \"restoring \" + path;\n\trestore(backupDir=path,dbPath=\"dfs://svmDemo\",tableName=\"sensors\",partition=\"%\",force=true);\n}\n```\n\n----------------------------------------\n\nTITLE: Calculating Stock Price Change Rate\nDESCRIPTION: This code snippet calculates the stock price change rate by grouping data by SecurityID and continuous segments of OfferPrice1. It uses the segment function to group consecutive identical values.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_sql_cases.md#_snippet_18\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntimer t = select last(OfferPrice1) \\ first(OfferPrice1) - 1 \n\t\t  from loadTable(\"dfs://Level1\", \"Snapshot\") \n\t\t  where date(DateTime) = 2020.06.01 \n\t\t  group by SecurityID, segment(OfferPrice1, false) \n```\n\n----------------------------------------\n\nTITLE: 使用linprog函数求解最大化期望收益率问题\nDESCRIPTION: 通过线性规划解决最大化期望收益率的投资组合优化问题。设置了行业暴露限制和单个股票权重上限约束，使用DolphinDB的linprog函数求解。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/MVO.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\nf = -1*[0.1,0.02,0.01,0.05,0.17,0.01,0.07,0.08,0.09,0.10]\nA = ([1,1,1,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,1,1,1,1,-1,-1,-1,0,0,0,0,0,0,0,0,0,0,-1,-1,-1,0,0,0,0,0,0,0,0,0,0,-1,-1,-1,-1]$10:6).transpose()\nb = [0.3,0.4,0.3]\n//参数设置\nexposure = 0.08\nb = (b + exposure) join (exposure - b)\nAeq = matrix(take(1, size(f))).transpose()\nbeq = array(DOUBLE).append!(1)\nres = linprog(f, A, b, Aeq, beq, 0, 0.15)\n\n//输出结果 \nmax_return = -res[0] \n//max_return = 0.0861\nweights = res[1]\n//weights = [0.15,0.15,0,0.15,0.15,0.02,0,0.08,0.15,0.15]\n```\n\n----------------------------------------\n\nTITLE: Querying - Nested Query DolphinDB\nDESCRIPTION: This snippet uses a nested query to select the minimum battery level from the readings table, filtered by specific device models. It uses a separate query for finding the valid device IDs, and then uses the results in the main query. It demonstrates a more complex query utilizing a nested subquery.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs TimescaleDB/test_dolphindb_small.txt#_snippet_29\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 20. 经典查询：某两个型号的设备每小时最低电量的前 20 条数据\ntimer {\n\tdevice_ids = \n\t\texec distinct device_id\n\t\tfrom device_info\n\t\twhere model = 'pinto' or model = 'focus';\n\n\tbattery_levels = \n\t\tselect min(battery_level) as min_battery_level\n\t\tfrom readings\n\t\twhere device_id in device_ids\n\t\tgroup by hour(time)\n\t\torder by hour_time asc;\n\n\tbattery_levels[0:20]\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Partitioned Time-Series Database and Table in DolphinDB\nDESCRIPTION: Checks for and removes any existing distributed database named 'dfs://signalData', then creates a new time-series database partitioned by a single date value with the TSDB engine and chunk atomicity. Defines a table schema with a TIMESTAMP column for time, a SYMBOL column for ID, and numeric columns for dwMileage, speed, longitude, latitude, and elevation. The table is partitioned by time and sorted by ID and time to optimize querying and storage.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/redis_plugin_tutorial/example3-kafka.txt#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\nif(existsDatabase(\"dfs://signalData\")){dropDatabase(\"dfs://signalData\")}\ncreate database \"dfs://signalData\" partitioned by VALUE([2024.03.05]),engine=\"TSDB\",atomic=\"CHUNK\"\ncreate table \"dfs://signalData\".\"data\"(\n    time TIMESTAMP[compress=\"delta\"],\n    ID SYMBOL,\n    dwMileage DOUBLE,\n    speed DOUBLE,\n    longitude DOUBLE,\n    latitude DOUBLE,\n    elevation DOUBLE\n)\npartitioned by time\nsortColumns = [`ID,`time]\n```\n\n----------------------------------------\n\nTITLE: Executing Scripts and Registered Functions Remotely Using RemoteRun Syntax in DolphinDB - DolphinDB\nDESCRIPTION: Demonstrates executing a script remotely on a connected DolphinDB node using 'remoteRun' function with a script string, alternatively calling the connection object 'h' directly with the script. Also shows calling a pre-registered remote function 'sum' with parameters by passing the function name and arguments separately. This snippet exemplifies running computations on remote nodes seamlessly over the RPC connection.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/hybrid_programming_paradigms.md#_snippet_11\n\nLANGUAGE: DolphinDB\nCODE:\n```\nremoteRun(h, \"sum(1 3 5 7)\");\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nh(\"sum(1 3 5 7)\");\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nh(\"sum\", 1 3 5 7);\n```\n\n----------------------------------------\n\nTITLE: Creating and Populating Partitioned Stream Tables in DolphinDB Script\nDESCRIPTION: This code defines three stream tables, partitions them, and demonstrates parallel data insertion for each partition, followed by a select query to retrieve all data from the partitioned table. Dependencies include streamTable and append! operations. The method partitions based on the 'id' column, allowing high-throughput stream ingestion. Each stream table corresponds to a partition.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/in_memory_table.md#_snippet_2\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nst1=streamTable(1:0,`id`val,[INT,INT])\nst2=streamTable(1:0,`id`val,[INT,INT])\nst3=streamTable(1:0,`id`val,[INT,INT])\ndb=database(\"\",RANGE,1 101 201 301)\npst=db.createPartitionedTable([st1,st2,st3],`pst,`id)\n\nst1.append!(table(1..100 as id,rand(100,100) as val))\nst2.append!(table(101..200 as id,rand(100,100) as val))\nst3.append!(table(201..300 as id,rand(100,100) as val))\n\nselect * from pst\n```\n\n----------------------------------------\n\nTITLE: Creating a monthly scheduled job to run a script in DolphinDB\nDESCRIPTION: Example showing how to schedule a job to run a script file on the first day of each month throughout 2020 using the run function with partial application.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/scheduledJob.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\nscheduleJob(`monthlyJob, \"Monthly Job 1\", run{\"/home/DolphinDB/script/monthlyJob.dos\"}, 00:00m, 2020.01.01, 2020.12.31, 'M', 1);\n```\n\n----------------------------------------\n\nTITLE: 实现数据分区备份与备份恢复操作 - DolphinDB 脚本\nDESCRIPTION: 该段代码使用 DolphinDB 提供的 backup 和 migrate 函数分别实现数据库分区数据的备份和从备份文件恢复。备份操作基于备份路径和目标表；恢复则支持迁移数据至新数据库及新表，便于数据保护及管理。适用对象为带分区的高频时序数据库，依赖 DolphinDB 内置备份模块。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/high_freq_data_storage_and_analysis.md#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\n/**** 数据备份和备份恢复 ****/\n\n//1. 路径\npath=getHomeDir()\n\n//2. 备份\nbackup(path,<select * from loadTable(\"dfs://db_test\",\"collect\")>,true);\n\n//3. 恢复备份\n//使用 migrate 函数，完整恢复备份。恢复到新建的表中。\nmigrate(backupDir=path,backupDBPath=\"dfs://db_test\",backupTableName=\"collect\",newDBPath=\"dfs://db_temp3\",newTableName=(\"collect_temp\"));\nloadTable(database=\"dfs://db_temp3\",tableName=\"collect_temp\") \n```\n\n----------------------------------------\n\nTITLE: Multi-threaded Parallel Writing to DolphinDB Table in DolphinDB Script\nDESCRIPTION: This function coordinates multi-threaded data ingestion, partitioning device IDs and dispatching single-threaded writers in parallel using ploop. Requires DolphinDB job submission and ploop utilities, plus prior table creation and data generator. Core parameters are device IDs, time range, table/database names, and thread count; function ensures that batches are distributed and appended in parallel, yielding accelerated ingestion for large data volumes.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/multipleValueModeWrite.txt#_snippet_3\n\nLANGUAGE: dolphindb\nCODE:\n```\ndef multipleThreadWriting(id, startDay, days,freqPerDay, numMachinesPerPartition,numMetrics, threads,dbName,tableName) {\n\t//split devVec to multiple part for parallel writing\n\tidCountPerThread = ceil(id.size() \\ threads/10)*10\n\tploop(singleThreadWriting{,startDay, days,freqPerDay, numMachinesPerPartition,numMetrics,dbName,tableName}, id.cut(idCountPerThread))\n}\n```\n\n----------------------------------------\n\nTITLE: Benchmarking DolphinDB Upper Triangular Matrix Creation (JIT vs Standard)\nDESCRIPTION: This snippet benchmarks the performance of the JIT-compiled `upperTriangularMatrix_jit` function against the standard `upperTriangularMatrix` function using the `timer` function. It initializes a matrix and measures the time taken for 1000 executions of each function, highlighting the performance advantage of JIT compilation for this matrix transformation.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/matrix.md#_snippet_40\n\nLANGUAGE: DolphinDB\nCODE:\n```\n>m=1..1024$32:32\ntimer(1000) upperTriangularMatrix_jit(m,32,32)\nTime elapsed: 24.049 ms\n>timer(1000) upperTriangularMatrix(m,32,32)\nTime elapsed: 355.052 ms\n```\n\n----------------------------------------\n\nTITLE: 使用 dropPartition 函数强制删除分区元数据\nDESCRIPTION: 该代码片段演示在分布式环境中，当chunk版本不一致或处于recovering状态无法正常删除时，利用 dropPartition 函数的第四个参数设为 true 来强制删除对应分区，避免常规删除失败的情况。操作前需要确认返回数据正常，再进行删除和重启操作。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/repair_chunk_status.md#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndbName=\"/stocks_orderbook\"\nfileCond=dbName + \"%\"\nt=exec substr(file,strlen(dbName)) from rpc(getControllerAlias(),getClusterChunksStatus) where file like fileCond, state != \"COMPLETE\"\ndropPartition(database(\"dfs:/\"+dbName), t, , true)\n```\n\n----------------------------------------\n\nTITLE: Building and Operating a Reactive State Engine for Real-time Factor Computation - DolphinDB\nDESCRIPTION: This code demonstrates setting up a streaming computation by defining table schemas for input and output, and initializing a reactive state engine that computes the custom factor (myFactor) over incoming data using createReactiveStateEngine in DolphinDB. It shows how to insert example data into the reactive engine and query results, mimicking real-time data flow for trading applications. The code depends on previously defined state functions, requires streaming engine functionality, and expects streaming tables with appropriate columns and types. Outputs are real-time computed factor values. Limitations include the necessity for correct table structuring and function definitions.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Streaming_computing_of_financial_quantifiers.md#_snippet_14\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 定义输入输出的表结构\ncolName = [\"securityID\",\"dateTime\",\"preClosePx\",\"openPx\",\"highPx\",\"lowPx\",\"lastPx\",\"totalVolumeTrade\",\"totalValueTrade\",\"instrumentStatus\"] <- flatten(eachLeft(+, [\"bidPrice\",\"bidOrderQty\",\"bidNumOrders\"], string(0..9))) <- (\"bidOrders\"+string(0..49)) <- flatten(eachLeft(+, [\"offerPrice\",\"offerOrderQty\",\"offerNumOrders\"], string(0..9))) <- (\"offerOrders\"+string(0..49)) <- [\"numTrades\",\"iopv\",\"totalBidQty\",\"totalOfferQty\",\"weightedAvgBidPx\",\"weightedAvgOfferPx\",\"totalBidNumber\",\"totalOfferNumber\",\"bidTradeMaxDuration\",\"offerTradeMaxDuration\",\"numBidOrders\",\"numOfferOrders\",\"withdrawBuyNumber\",\"withdrawBuyAmount\",\"withdrawBuyMoney\",\"withdrawSellNumber\",\"withdrawSellAmount\",\"withdrawSellMoney\",\"etfBuyNumber\",\"etfBuyAmount\",\"etfBuyMoney\",\"etfSellNumber\",\"etfSellAmount\",\"etfSellMoney\"]\ncolType = [\"SYMBOL\",\"TIMESTAMP\",\"DOUBLE\",\"DOUBLE\",\"DOUBLE\",\"DOUBLE\",\"DOUBLE\",\"INT\",\"DOUBLE\",\"SYMBOL\"] <- take(\"DOUBLE\", 10) <- take(\"INT\", 70)<- take(\"DOUBLE\", 10) <- take(\"INT\", 70) <- [\"INT\",\"INT\",\"INT\",\"INT\",\"DOUBLE\",\"DOUBLE\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"DOUBLE\",\"INT\",\"INT\",\"DOUBLE\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\"]\ninputTable = table(1:0, colName, colType)\nresultTable = table(10000:0, [\"securityID\", \"dateTime\", \"factor\"], [SYMBOL, TIMESTAMP, DOUBLE])\n\n// 使用 createReactiveStateEngine 创建响应式状态引擎\ntry{ dropStreamEngine(\"reactiveDemo\")} catch(ex){ print(ex) }\nmetrics = <[dateTime, myFactor(bidPrice0, bidOrderQty0, offerPrice0, offerOrderQty0, 3)]>\nrse = createReactiveStateEngine(name=\"reactiveDemo\", metrics =metrics, dummyTable=inputTable, outputTable=resultTable, keyColumn=\"securityID\")\n\n// 输入数据\ntableInsert(rse, {\"securityID\":\"000001\", \"dateTime\":2023.01.01T09:30:00.000, \"bidPrice0\":19.98, \"bidOrderQty0\":100, \"offerPrice0\":19.99, \"offerOrderQty0\":120})\ntableInsert(rse, {\"securityID\":\"000001\", \"dateTime\":2023.01.01T09:30:03.000, \"bidPrice0\":19.95, \"bidOrderQty0\":130, \"offerPrice0\":19.93, \"offerOrderQty0\":120})\ntableInsert(rse, {\"securityID\":\"000001\", \"dateTime\":2023.01.01T09:30:06.000, \"bidPrice0\":19.97, \"bidOrderQty0\":120, \"offerPrice0\":19.98, \"offerOrderQty0\":130})\ntableInsert(rse, {\"securityID\":\"000001\", \"dateTime\":2023.01.01T09:30:09.000, \"bidPrice0\":20.00, \"bidOrderQty0\":130, \"offerPrice0\":19.97, \"offerOrderQty0\":140})\n\n\n// 查看结果\nselect * from resultTable\n/*\n securityID dateTime                factor            \n---------- ----------------------- ------------------\n000001     2023.01.01T09:30:00.000 19.9845 \n000001     2023.01.01T09:30:03.000 19.9698\n000001     2023.01.01T09:30:06.000 19.9736\n000001     2023.01.01T09:30:09.000 19.9694\n*/\n```\n\n----------------------------------------\n\nTITLE: Creating a Shared Memory Table in DolphinDB\nDESCRIPTION: Creates a shared memory table in DolphinDB with TIMESTAMP and INT columns to store time-related data that will be inserted from Java.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/timezone.md#_snippet_9\n\nLANGUAGE: SQL\nCODE:\n```\nconn.run(\"t = table(100:0,`ts`value,[TIMESTAMP,INT]);share t as timeJava;\");\n```\n\n----------------------------------------\n\nTITLE: Starting Agent Node\nDESCRIPTION: This shell command initiates the agent node of the DolphinDB cluster.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/single_machine_cluster_deploy.md#_snippet_15\n\nLANGUAGE: Shell\nCODE:\n```\nsh startagent.sh\n```\n\n----------------------------------------\n\nTITLE: Filtering Trades and Appending to Join Engine (DolphinDB Script)\nDESCRIPTION: Defines a handler function to filter trades where price is positive and occurred after 09:30:00, then appends qualifying trades to the left stream of the asof join engine. Utilizes DolphinDB’s stream handling and selection syntax to process trading events in the join workflow. The expected input is a trade message, and the output is filtered data passed to the join engine; care should be taken to ensure timestamps and fields match the engine’s expectations.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/stock_market_replay/02.calTradeCost_asofJoin.txt#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef appendLeftStream(msg){\n\ttempMsg = select * from msg where Price > 0 and Time>=09:30:00.000\n\tgetLeftStream(getStreamEngine(`tradeJoinSnapshot)).tableInsert(tempMsg)\n}\n```\n\n----------------------------------------\n\nTITLE: Estimating Node Rebalancing Plan in DolphinDB\nDESCRIPTION: Uses the `rpc` function to execute `rebalanceChunksAmongDataNodes` on the controller node (obtained via `getControllerAlias()`). The `{ false }` argument signifies that the `exec` parameter is set to `false`, causing the function to only simulate the rebalancing process across all data nodes and return the anticipated partition movements without initiating actual data migration. This is useful for previewing the impact of rebalancing, typically after adding new nodes.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/data_move_rebalance.md#_snippet_2\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nrpc(getControllerAlias(), rebalanceChunksAmongDataNodes{ false })\n```\n\n----------------------------------------\n\nTITLE: Querying data with OLAP engine\nDESCRIPTION: This code snippet shows an example query that is suitable for the OLAP storage engine. It calculates the average bid price grouped by StockID for a specific date.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/tsdb_engine.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect avg(Bid) from t where date=2021.08.05 group by StockID\n```\n\n----------------------------------------\n\nTITLE: Executing Custom Local Function Remotely via RPC in DolphinDB - DolphinDB\nDESCRIPTION: Defines a local user function 'mysum' to perform reduction (summing elements of vector) and remotely executes this function on the connected DolphinDB node 'h' with specified parameters. This example shows how local functions with defined logic can be serialized and run remotely, with output equivalent to local execution. It requires the 'reduce' function available in DolphinDB environment.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/hybrid_programming_paradigms.md#_snippet_12\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef mysum(x) : reduce(+, x)\nh(mysum, 1 3 5 7);\n```\n\n----------------------------------------\n\nTITLE: Python Client Subscription to DolphinDB Streaming Data\nDESCRIPTION: This Python snippet demonstrates establishing a DolphinDB session with authentication, enabling real-time streaming subscription, and defining a simple callback handler to print received messages. It illustrates how external Python applications can consume DolphinDB streaming market data via the official API and streaming protocols.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/K.md#_snippet_22\n\nLANGUAGE: Python\nCODE:\n```\nimport dolphindb as ddb\n\n# 与 DolphinDB 建立会话和连接\ns = ddb.session()\ns.connect(host=\"192.198.1.39\", port=8988, userid=\"admin\", password=\"123456\")\n\n# 定义 Python 端的回调函数\ndef handlerTestPython(msg):\n    print(msg)\n    \n# python 客户端开启 DolphinDB 访问功能\ns.enableStreaming(0)\n```\n\n----------------------------------------\n\nTITLE: Map-Reduce Based Minute-Level K-Line Generation in DolphinDB (DolphinDB script)\nDESCRIPTION: Defines a map-reduce job that computes minute-level aggregated bid and offer prices across symbols and dates from a distributed dataset. The 'saveMinuteQuote' function performs the aggregation by grouping on 'symbol', 'date', and minute-extracted 'time', appends the results to a distributed table, and returns the count of processed records. The data source is created via 'sqlDS' with a query selecting necessary columns within a date range, and the map-reduce is executed using DolphinDB's 'mr' function.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Cluster_scale_out_performance_test.md#_snippet_1\n\nLANGUAGE: DolphinDB script\nCODE:\n```\ndef saveMinuteQuote(t){\n\tminuteQuotes=select avg(bid) as bid, avg(ofr) as ofr from t group by symbol, date, minute(time) as minute\n\tloadTable(\"dfs://TAQ\", \"quotes_minute\").append!(minuteQuotes)\n\treturn minuteQuotes.size()\n}\n\nds = sqlDS(<select symbol, date, time, bid, ofr from quotes where date between 2007.08.01 : 2007.08.31>)\nmr(ds, saveMinuteQuote, +)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Partitioned Table for Execution Plan Analysis in DolphinDB\nDESCRIPTION: This DolphinDB script prepares a distributed partitioned table ('pt') for demonstrating execution plan analysis, particularly partition pruning. It creates a database 'dfs://valuedb', generates sample data with 'month' and 'x' columns, partitions the table by 'month', and appends the data. It also includes a query to count the number of partitions created.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_Explain.md#_snippet_7\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\n// 创建数据库\nif( existsDatabase(\"dfs://valuedb\") ) dropDatabase(\"dfs://valuedb\");\n// 100万，列：x,month;按month 值分区\nn=1000000\nmonth=take(2000.01M..2016.12M, n)\nx=rand(1.0, n)\nt=table(month, x)\n\ndb=database(\"dfs://valuedb\", VALUE, 2000.01M..2016.12M)\n\npt = db.createPartitionedTable(t, `pt, `month)\npt.append!(t)\n\n// 查询出来有204个分区\nselect count(*) from pnodeRun(getAllChunks) where dfsPath like '%valuedb/%' and type != 0 group by dfsPath\n\nselect [HINT_EXPLAIN] * from pt where 2016.11M <= month<= 2016.12M\n```\n\n----------------------------------------\n\nTITLE: Using Any and All Logical Predicates for Comparisons in DolphinDB SQL\nDESCRIPTION: Demonstrates the use of any and all predicates to perform logical comparisons with operators (=, !=, >, <, <=, >=). The examples respectively find employees matching any salary from a department and employees earning a salary greater than or equal to all salaries in another department. It notes that comparisons involving a list like ALL (1400, 3000) are currently not supported. Inputs are subqueries returning scalar or multi-row values, and outputs are filtered employee data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Standard_SQL_in_DolphinDB.md#_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nselect * from employees\nwhere salary = \nany(select salary \n from employees\n where department_id = 30) \norder by employee_id\n```\n\nLANGUAGE: SQL\nCODE:\n```\nselect * from employees\n where salary >=\n all (select salary from employees where department_id=60)\n order by employee_id\n```\n\n----------------------------------------\n\nTITLE: Datetime Parsing, Resampling, Shifting, and Rolling Statistics in pandas and DolphinDB\nDESCRIPTION: This snippet covers time series operations including parsing datetime, resampling, shifting, and rolling window calculations such as moving averages and standard deviations. These functions support temporal data analysis workflows between pandas and DolphinDB.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/function_mapping_py.md#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\npandas.to_datetime  # Convert strings to datetime\npandas.Series.resample / pandas.DataFrame.resample  # Resample time series\npandas.Series.shift / pandas.DataFrame.shift  # Shift data for lagging or leading calculations\npandas.rolling_mean / pandas.DataFrame.ewm.mean  # Exponentially weighted moving average\npandas.rolling_std  # Rolling standard deviation\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Data Transformation to Merge Columns - DolphinDB Script\nDESCRIPTION: This code defines a custom DolphinDB function 'toArrayVector' which merges ten best bid/ask price/order columns into single array vector columns in-place, removes the original columns, and reorders the final set of columns for efficient storage. The function operates on a mutable table and is designed for use as a transform function during distributed import using loadTextEx. Key dependencies include the existence of all listed columns and the 'fixedLengthArrayVector' method.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_csv.md#_snippet_23\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef toArrayVector(mutable tmp){\n  //将10档数据合并为1列，添加到tmp表中。也可以使用update!方法添加。\n\ttmp[`BidPrice]=fixedLengthArrayVector(tmp.BidPrice0,tmp.BidPrice1,tmp.BidPrice2,tmp.BidPrice3,tmp.BidPrice4,tmp.BidPrice5,tmp.BidPrice6,tmp.BidPrice7,tmp.BidPrice8,tmp.BidPrice9)\n\ttmp[`BidOrderQty]=fixedLengthArrayVector(tmp.BidOrderQty0,tmp.BidOrderQty1,tmp.BidOrderQty2,tmp.BidOrderQty3,tmp.BidOrderQty4,tmp.BidOrderQty5,tmp.BidOrderQty6,tmp.BidOrderQty7,tmp.BidOrderQty8,tmp.BidOrderQty9)\n\ttmp[`BidOrders]=fixedLengthArrayVector(tmp.BidOrders0,tmp.BidOrders1,tmp.BidOrders2,tmp.BidOrders3,tmp.BidOrders4,tmp.BidOrders5,tmp.BidOrders6,tmp.BidOrders7,tmp.BidOrders8,tmp.BidOrders9)\n\ttmp[`OfferPrice]=fixedLengthArrayVector(tmp.OfferPrice0,tmp.OfferPrice1,tmp.OfferPrice2,tmp.OfferPrice3,tmp.OfferPrice4,tmp.OfferPrice5,tmp.OfferPrice6,tmp.OfferPrice7,tmp.OfferPrice8,tmp.OfferPrice9)\n\ttmp[`OfferOrderQty]=fixedLengthArrayVector(tmp.OfferOrderQty0,tmp.OfferOrderQty1,tmp.OfferOrderQty2,tmp.OfferOrderQty3,tmp.OfferOrderQty4,tmp.OfferOrderQty5,tmp.OfferOrderQty6,tmp.OfferOrderQty7,tmp.OfferOrderQty8,tmp.OfferOrderQty9)\n\ttmp[`OfferOrders]=fixedLengthArrayVector(tmp.OfferOrders0,tmp.OfferOrders1,tmp.OfferOrders2,tmp.OfferOrders3,tmp.OfferOrders4,tmp.OfferOrders5,tmp.OfferOrders6,tmp.OfferOrders7,tmp.OfferOrders8,tmp.OfferOrders9)\n  //删除合并前的列\n\ttmp.dropColumns!(`BidPrice0`BidPrice1`BidPrice2`BidPrice3`BidPrice4`BidPrice5`BidPrice6`BidPrice7`BidPrice8`BidPrice9`BidOrderQty0`BidOrderQty1`BidOrderQty2`BidOrderQty3`BidOrderQty4`BidOrderQty5`BidOrderQty6`BidOrderQty7`BidOrderQty8`BidOrderQty9`BidOrders0`BidOrders1`BidOrders2`BidOrders3`BidOrders4`BidOrders5`BidOrders6`BidOrders7`BidOrders8`BidOrders9`OfferPrice0`OfferPrice1`OfferPrice2`OfferPrice3`OfferPrice4`OfferPrice5`OfferPrice6`OfferPrice7`OfferPrice8`OfferPrice9`OfferOrderQty0`OfferOrderQty1`OfferOrderQty2`OfferOrderQty3`OfferOrderQty4`OfferOrderQty5`OfferOrderQty6`OfferOrderQty7`OfferOrderQty8`OfferOrderQty9`OfferOrders0`OfferOrders1`OfferOrders2`OfferOrders3`OfferOrders4`OfferOrders5`OfferOrders6`OfferOrders7`OfferOrders8`OfferOrders9)\n  //对列重新排序\n\ttmp.reorderColumns!(`SecurityID`TradeTime`PreClosePx`OpenPx`HighPx`LowPx`LastPx`TotalVolumeTrade`TotalValueTrade`InstrumentStatus`BidPrice`BidOrderQty`BidOrders`OfferPrice`OfferOrderQty`OfferOrders`NumTrades`IOPV`TotalBidQty`TotalOfferQty`WeightedAvgBidPx`WeightedAvgOfferPx`TotalBidNumber`TotalOfferNumber`BidTradeMaxDuration`OfferTradeMaxDuration`NumBidOrders`NumOfferOrders`WithdrawBuyNumber`WithdrawBuyAmount`WithdrawBuyMoney`WithdrawSellNumber`WithdrawSellAmount`WithdrawSellMoney`ETFBuyNumber`ETFBuyAmount`ETFBuyMoney`ETFSellNumber`ETFSellAmount`ETFSellMoney)\n\treturn tmp \n}\n```\n\n----------------------------------------\n\nTITLE: Grouping Trades by Stock Code (Multiple Columns) - DolphinDB\nDESCRIPTION: This DolphinDB script groups the \"trades\" table by \"ts_code\" and calculates the average low price and the sum of the high price for each group. It demonstrates grouping with multiple aggregate functions. The `timer(10)` function measures the execution time, and `clearAllCache()` clears the cache before each execution.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_20\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//按股票代码分组（多列）\ntimer(10) select avg(low), sum(higb) from trades group by ts_code\n```\n\n----------------------------------------\n\nTITLE: Defining Annualized Skewness Calculation Function in DolphinDB\nDESCRIPTION: Defines the 'getAnnualSkew' function to calculate the skewness of daily returns, providing insight into asymmetry in return distributions. The function calculates skewness on relative daily changes without explicit annualization by sqrt(N). It accepts a numeric vector of asset values and outputs the skewness statistic.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_factor_by_ddb/fund_factor_nine.txt#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndefg getAnnualSkew(value){\n\treturn skew(deltas(value)\\prev(value))\n}\n```\n\n----------------------------------------\n\nTITLE: Creating TSDB Partitioned Database and Table in DolphinDB\nDESCRIPTION: This code creates a partitioned database and table using the TSDB storage engine, which requires specifying the engine and sortColumns parameters. The database is partitioned by date and security ID.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/sql_performance_optimization_wap_di_rv.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndbName = \"dfs://snapshot_SH_L2_TSDB\"\ntableName = \"snapshot_SH_L2_TSDB\"\ndbTime = database(, VALUE, 2020.01.01..2020.12.31)\ndbSymbol = database(, HASH, [SYMBOL, 20])\ndb = database(dbName, COMPO, [dbTime, dbSymbol], engine='TSDB')\ncreatePartitionedTable(dbHandle=db, table=tbTemp, tableName=tableName, partitionColumns=`TradeTime`SecurityID, sortColumns=`SecurityID`TradeTime)\n```\n\n----------------------------------------\n\nTITLE: Registering Stream Filter Engine and Subscription (DolphinDB Script)\nDESCRIPTION: Implements and configures a filter engine to parse messages by type and dispatch them to appropriate stream handlers. Two filters are defined: one for trades and one for snapshots, each with dedicated handlers for processing with the asof join engine. The subscribeTable function binds the engine to the 'messageStream', enabling real-time processing and message parsing. Dependencies include presence of the stream tables and correct handler functions.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/stock_market_replay/02.calTradeCost_asofJoin.txt#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef filterAndParseStreamFunc(tradeSchema, snapshotSchema){\n\tfilter1 = dict(STRING,ANY)\n\tfilter1[\"condition\"] = \"trade\"\n\tfilter1[\"handler\"] = appendLeftStream\n\tfilter2 = dict(STRING,ANY)\n\tfilter2[\"condition\"] = \"snapshot\"\n\tfilter2[\"handler\"] = getRightStream(getStreamEngine(`tradeJoinSnapshot))\n\tschema = dict([\"trade\", \"snapshot\"], [tradeSchema, snapshotSchema])\n\tengine = streamFilter(name=\"streamFilter\", dummyTable=messageStream, filter=[filter1, filter2], msgSchema=schema)\n\t\n\tsubscribeTable(tableName=\"messageStream\", actionName=\"tradeJoinSnapshot\", offset=-1, handler=engine, msgAsTable=true, reconnect=true)\n}\nfilterAndParseStreamFunc(tradeSchema, snapshotSchema)\n```\n\n----------------------------------------\n\nTITLE: Loading Distributed Tables and Partitioning Symbols for Multi-Threaded Replay in DolphinDB Script\nDESCRIPTION: This snippet loads distributed tables for trade and entrust tick data and partitions stock codes into multiple groups for parallel processing via modulo operation. It defines a utility function to assign each stock code to a particular thread based on integer conversion and remainder arithmetic. Inputs include the desired market identification, thread count, and available stock codes, while outputs are structured lists of symbols for each thread. Requires DolphinDB distributed tables 'dfs://TSDB_tradeAndentrust', assumes stock codes are numeric strings, and depends on core table manipulation and for-loop constructs.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/matching_engine_simulator.md#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntrade_dfs = loadTable(\"dfs://TSDB_tradeAndentrust\", \"trade\")\nmarket = \"sz\"  // market = \"sz\" or \"sh\"\nif (market == \"sz\") {\n    market_name = \"XSHE\"\n    symbols = exec SecurityID from trade_dfs where SecurityID like \"0%\" or SecurityID like \"3%\" group by SecurityID\n}\nelse {\n    market_name = \"XSHG\"\n    symbols = exec SecurityID from trade_dfs where SecurityID like \"6%\" group by SecurityID\n}\n\ndef genSymbolListWithHash(symbolTotal, thread_num) {\n    symbol_list = []\n    for (i in 0:thread_num) {\n        symbol_list.append!([]$STRING)\n    }\n    for (symb in symbolTotal) {\n        idx = int(symb) % thread_num\n        tmp_list = symbol_list[idx]\n        tmp_list.append!(symb)\n        symbol_list[idx] = tmp_list\n    }\n    return symbol_list\n}\n\nthread_num = 20 // 线程数\nsymbol_list = genSymbolListWithHash(symbols, thread_num)\n```\n\n----------------------------------------\n\nTITLE: Performance Comparison: JIT vs Non-JIT Holding Cost in DolphinDB Script\nDESCRIPTION: Sets up test data representing trades (id, price, amount) in a DolphinDB table. It then uses the `timer` function to measure and compare the performance of calculating profit based on average holding cost, using both the non-JIT (`holdingCost_no_JIT`) and JIT-compiled (`holdingCost_JIT`) functions within SQL `select` statements. This demonstrates the significant speedup achieved by JIT for this path-dependent calculation.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/jit.md#_snippet_5\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nn=1000000\nid = 1..n\nprice = take(101..109,n)\namount =take(1 2 3 -2 -1 -3 4 -1 -2 2 -1,n)\ntrades = table(id, price, amount)\n\ntimer (10)\nt = select *, iif(amount < 0, amount*(avgPrice - price), 0) as profit\nfrom (\n  select *, holdingCost_no_JIT(price, amount) as avgPrice\n  from trades\n)    // 29,509ms\n\ntimer (10)\nselect *, iif(amount < 0, amount*(avgPrice - price), 0) as profit\nfrom (\n  select *, holdingCost_JIT(price, amount) as avgPrice\n  from trades\n)     // 148 ms\n```\n\n----------------------------------------\n\nTITLE: Authentication and Demo Ingestion Pipeline Setup in DolphinDB Script\nDESCRIPTION: Script section for logging in as a user, parameterizing the demo experiment, configuring ID and date vectors, and launching the main job. Assumes an accessible DolphinDB server and a valid admin user. Key parameters such as number of machines and days, partitioning specs, and thread count are assigned here. This block initializes the full end-to-end workflow for demonstration and benchmarking.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/multipleValueModeWrite.txt#_snippet_5\n\nLANGUAGE: dolphindb\nCODE:\n```\nlogin(\"admin\",\"123456\")\n\nfreqPerDay=86400\nnumMachines=100\nid=1..numMachines //设备编号\nstartDay=2020.09.01//开始日期\ndays=5 //写几天数据\nthreads = 20 //多个线程同时写\n\nnumMachinesPerPartition=10\nps1=2020.09.01..2020.12.31\nps2=numMachinesPerPartition*(0..(numMachines/numMachinesPerPartition))+1\n\nmainJob(id, startDay, days, ps1, ps2, freqPerDay, numMachinesPerPartition, threads)\n```\n\n----------------------------------------\n\nTITLE: Changing MySQL Time Zone and Observing Data Changes\nDESCRIPTION: Demonstrates how changing the MySQL time zone affects only TIMESTAMP columns, while DATE and TIME columns remain unchanged as they are not timezone-aware.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/timezone.md#_snippet_13\n\nLANGUAGE: SQL\nCODE:\n```\nmysql> set time_zone='+09:00';\nQuery OK, 0 rows affected (0.01 sec)\n\nmysql> show variables like '%time_zone%';\n+------------------+--------+\n| Variable_name    | Value  |\n+------------------+--------+\n| system_time_zone | CST    |\n| time_zone        | +09:00 |\n+------------------+--------+\n2 rows in set (0.00 sec)\n\nmysql> select * from testTable;\n+------------+----------+---------------------+\n| date       | time     | ts                  |\n+------------+----------+---------------------+\n| 2022-06-14 | 23:13:15 | 2022-06-15 00:13:15 |\n+------------+----------+---------------------+\n1 row in set (0.00 sec)\n```\n\n----------------------------------------\n\nTITLE: Function to Load Single Day Data into Partitioned Table - DolphinDB Script\nDESCRIPTION: Defines a function ('load_one_day_data') to append one day's records from a temporary table to a partitioned table, adjusting the time column by a day offset. Inputs include date, database path, table name, and input table; outputs are updated tables with daily data. Requires that tables are already defined in the given databases. The function accommodates time shifting per day for synthetic datasets.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/buildData.txt#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//Functions to write data in testdb and testdb_tsdb\ndef load_one_day_data (date,db_path,table_name,input_table){\n\tdb_table = loadTable(database=db_path, tableName=table_name)\n\ttmpt=copy(input_table)\n\ts=date-2020.01.01\n\tupdate tmpt set time=time.temporalAdd(s,'d')\n\n\tdb_table.append!(tmpt)\n}\n```\n\n----------------------------------------\n\nTITLE: Login and Environment Cleanup in DolphinDB\nDESCRIPTION: This snippet logs into the DolphinDB server using the specified credentials and clears the cache and environment variables. It is a prerequisite for the subsequent operations. It establishes a clean slate before the database and table creation and data processing steps begin.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/machine_learning_volatility/02.dataProcessArrayVector.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//login account and clean up the environment\nlogin(\"admin\", \"123456\")\nclearAllCache()\nundef(all)\ngo\n```\n\n----------------------------------------\n\nTITLE: Selecting records based on segmented data and max value\nDESCRIPTION: This code snippet uses a SQL query with `context by` and `having` to select records from the table 't'. The data is grouped by segments defined by the condition `v>= minV`. The `having` clause filters for groups where the value 'v' is equal to the maximum value within the group and is also greater than or equal to 'minV'. The `limit 1` clause ensures that only the first record satisfying the conditions is returned for each group.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_40\n\nLANGUAGE: shell\nCODE:\n```\nselect * from t context by segment(v>= minV) having (v=max(v) and v>=minV) limit 1\n```\n\n----------------------------------------\n\nTITLE: Finding the index of the maximum value in each row using rowImax\nDESCRIPTION: This code snippet finds the index of the maximum value in each row of the matrix 'm' using the `rowImax` function.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_36\n\nLANGUAGE: shell\nCODE:\n```\nprint rowImax(m)\n```\n\n----------------------------------------\n\nTITLE: Splitting Data into Training and Testing Sets\nDESCRIPTION: This code defines a function `trainTestSplit` that splits a table into training and testing sets based on a given test ratio. It uses `shuffle` to randomize the data and then divides it according to the specified ratio. The function returns two tables: `wineTrain` and `wineTest`.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/machine_learning.md#_snippet_2\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\ndef trainTestSplit(x, testRatio) {\n    xSize = x.size()\n    testSize = xSize * testRatio\n    r = (0..(xSize-1)).shuffle()\n    return x[r > testSize], x[r <= testSize]\n}\n\nwineTrain, wineTest = trainTestSplit(wine, 0.3)\nwineTrain.size()    // 124\nwineTest.size()     // 54\n```\n\n----------------------------------------\n\nTITLE: Statistical Grouping by Transaction Type (Standard Approach) in DolphinDB Script\nDESCRIPTION: Aggregates trading volume and amount by date, symbol, side, and custom transaction type buckets using four group by queries with conditional filters distinguishing small, medium, large, and extra-large trades. Merges the results into a unified summary table. Suitable for baseline performance measurement, but code is verbose and less maintainable.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_SQL_Case_Tutorial.md#_snippet_34\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntimer {\n\t// 小单\n\tresS = select sum(volume) as volume_sum, sum(volume * price) as amount_sum, 0 as type \n\t\t\tfrom t \n\t\t\twhere time <= 10:00:00, volume * price < 40000 \n\t\t\tgroup by date, symbol, side\n\t// 中单\n\tresM = select sum(volume) as volume_sum, sum(volume * price) as amount_sum, 1 as type \n\t\t\tfrom t \n\t\t\twhere time <= 10:00:00, 40000 <= volume * price < 200000 \n\t\t\tgroup by date, symbol, side\n\t// 大单\n\tresB = select sum(volume) as volume_sum, sum(volume * price) as amount_sum, 2 as type \n\t\t\tfrom t \n\t\t\twhere time <= 10:00:00, 200000 <= volume * price < 1000000 \n\t\t\tgroup by date, symbol, side\n\t// 特大单\n\tresX = select sum(volume) as volume_sum, sum(volume * price) as amount_sum, 3 as type \n\t\t\tfrom t \n\t\t\twhere time <= 10:00:00, volume * price >= 1000000 \n\t\t\tgroup by date, symbol, side\n\t\n\tres1 = table(N:0, `date`symbol`side`volume_sum`amount_sum`type, [DATE, SYMBOL, SYMBOL, LONG, DOUBLE, INT])\n\tres1.append!(resS).append!(resM).append!(resB).append!(resX)\n}\n```\n\n----------------------------------------\n\nTITLE: Granting, Revoking, and Denying Permissions in DolphinDB - DolphinDB\nDESCRIPTION: This collection of scripts manipulates user or group permissions using 'grant', 'revoke', and 'deny' functions, all requiring admin privileges. Parameters include identifiers for users/groups, permission types, and object scopes (such as '*'). After each operation, 'getUserAccess' is called to retrieve the permission state. Supports complex use cases where permissions overlap or conflict. Outputs show permission status updates.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Permission_Management.md#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\ngrant(`user2, DBOBJ_CREATE,\"*\") \ngetUserAccess(\"user2\")\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nrevoke(`user2, DBOBJ_CREATE,\"*\")\ngetUserAccess(\"user2\")\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndeny(`user2, DBOBJ_CREATE,\"*\") \ngetUserAccess(\"user2\")\n```\n\n----------------------------------------\n\nTITLE: Segmented Reading of Large Query Results Using DolphinDB Java API\nDESCRIPTION: This Java snippet shows how to handle large result sets by reading data in segments using EntityBlockReader. The example connects to the DolphinDB server, runs a query specifying a fetchSize to limit segments, reads multiple BasicTable chunks incrementally, and combines them. It includes proper handling to skip unread data segments to avoid socket buffer issues by calling skipAll when discarding incomplete data. Dependencies are DolphinDB Java classes DBConnection, EntityBlockReader, and BasicTable. Inputs are database connection parameters and the query string with time filters. Outputs are combined query results in BasicTable format.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/iot_examples.md#_snippet_22\n\nLANGUAGE: java\nCODE:\n```\nDBConnection conn = new DBConnection();\nconn.connect(SERVER, PORT, USER, PASSWORD);\nEntityBlockReader v = (EntityBlockReader)conn.run(\"select * from loadTable('dfs://svmDemo','sensors') where datetime between 2020.09.01T00:00:00 : 2020.09.07T23:59:59\",(ProgressListener) null,4,4,10000);\nBasicTable data = (BasicTable)v.read();\nwhile(v.hasNext()){\n    BasicTable t = (BasicTable)v.read();\n    data = data.combine(t);\n}\n```\n\n----------------------------------------\n\nTITLE: Submitting Factor Calculation Job in the Background\nDESCRIPTION: This snippet demonstrates submitting a factor calculation job to the background using `submitJob`. The `writeDayReturnSkew` function calculates the `dayReturnSkew` factor and writes it to a factor library table. The job is submitted for each month in 2020 to avoid conflicts during parallel writing.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practice_for_factor_calculation.md#_snippet_32\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef writeDayReturnSkew(dBegin,dEnd){\n\tdReturn = select `dayReturnSkew as factorname, dayReturnSkew(close) as val from loadTable(\"dfs://k_minute_level\", \"k_minute\") where date(tradetime) between dBegin : dEnd group by date(tradetime) as tradetime, securityid\n\t//写入因子库\n\tloadTable(\"dfs://K_FACTOR_VERTICAL\",\"factor_k\").append!(dReturn)\n\t}\n\nfor (i in 0..11){\n\tdBegin = monthBegin(temporalAdd(2020.01.01,i,\"M\"))\n\tdEnd = monthEnd(temporalAdd(2020.01.01,i,\"M\"))\n\tsubmitJob(\"writeDayReturnSkew\",\"writeDayReturnSkew_\"+dBegin+\"_\"+dEnd, writeDayReturnSkew,dBegin,dEnd)\n}\n```\n\n----------------------------------------\n\nTITLE: Querying with Snapshot Engine\nDESCRIPTION: Queries the latest records using the snapshot engine. Setting the hint value to 64 instructs the query to use the snapshot engine. This snippet includes two examples: one to query all the latest records, and another to query the latest stock data updated within the last minute.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/snapshot_engine.md#_snippet_2\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nselect [64] * from loadTable(\"dfs://compoDB\",\"pt\")\n```\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nselect [64] sym from loadTable(\"dfs://compoDB\",\"pt\") where ts > datetimeAdd(now(),-1,`m)\n```\n\n----------------------------------------\n\nTITLE: Querying Data from DolphinDB (Level 1 Dataset)\nDESCRIPTION: This code snippet queries data from a DolphinDB database table named 'quotes' which stores Level 1 market data. The query filters for a specific date and selects all columns.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/pickle_comparison.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\n＃读取Level 1数据集一天的数据\ntimer t1 = select * from loadTable(\"dfs://TAQ\", \"quotes\") where TradeDate = 2007.08.23\n```\n\n----------------------------------------\n\nTITLE: For Loop Examples for Summation and Iteration on Collections in DolphinDB - DolphinDB\nDESCRIPTION: Demonstrates various uses of the 'for' loop in DolphinDB iterating over a numeric range, vector elements, matrix columns, and table rows respectively. Examples include accumulation of sums from a range and vector, printing average of matrix columns, and calculating and printing product of selected fields in a table. Illustrates DolphinDB’s handling of iteration constructs over multiple collection types, emphasizing imperative looping for control flow.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/hybrid_programming_paradigms.md#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\ns = 0\nfor(x in 1:101) s += x\nprint s\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\ns = 0;\nfor(x in 1 3 5 9 15) s += x\nprint s\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nm = matrix(1 2 3, 4 5 6, 7 8 9)\nfor(c in m) print c.avg()\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nt= table([\"TV set\", \"Phone\", \"PC\"] as productId, 1200 600 800 as price, 10 20 7 as qty)\nfor(row in t) print row.productId + \": \" + row.price * row.qty\n```\n\n----------------------------------------\n\nTITLE: Updating Data in a Table in DolphinDB\nDESCRIPTION: This code snippet demonstrates how to update data in a table named 't' in DolphinDB using the SQL update statement. It sets the value of column 'y' to 1000.1 where the value of column 'x' is 5.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Single_Node_Tutorial.md#_snippet_10\n\nLANGUAGE: DolphinDB\nCODE:\n```\nupdate t set y = 1000.1 where x = 5\n```\n\n----------------------------------------\n\nTITLE: Querying Stock Data\nDESCRIPTION: This code queries the top 5 rows from the 'stock' table in the 'stocksDatabase' and displays the results. It uses `loadTable` to load the table and `select top 5 * from` to retrieve the first five rows.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_csv.md#_snippet_30\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect top 5 * from loadTable(dbPath1, `stock);\n```\n\n----------------------------------------\n\nTITLE: Querying Futures Data\nDESCRIPTION: This code queries the top 5 rows from the 'futures' table in the 'futuresDatabase' and displays the results.  It uses `loadTable` to load the table and `select top 5 * from` to retrieve the first five rows.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_csv.md#_snippet_31\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect top 5 * from loadTable(dbPath2, `futures);\n```\n\n----------------------------------------\n\nTITLE: Setting Up Cascaded Stream Processing Engines - DolphinDB\nDESCRIPTION: This function `consume` sets up a pipeline of stream processing engines. It creates two `ReactiveStateEngine` instances for filtering (removing duplicate events and closing-door alerts) and one `SessionWindowEngine` for detecting door open timeout events within 300-second sessions per door. The engines are cascaded, with the output of one feeding into the next, ultimately writing final results to `outputSt1`. It also subscribes the first engine (`reactivEngine1`) to the `doorRecord` table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/streaming_engine_anomaly_alerts/流计算引擎级联检测门禁超时数据.txt#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 三级引擎级联检测开门状态超时数据\ndef consume(){\n\t//创建响应式状态引擎2 过滤掉关门告警\n\tswOut1 =table(1:0,`eventDate`doorNum`doorEventCode,[DATETIME,INT,INT])\n\treactivEngine2 = createReactiveStateEngine(name=`reactivEngine2, metrics=<[eventDate,doorEventCode]>, dummyTable=swOut1,outputTable= objByName(`outputSt1),keyColumn=`doorNum,filter=<doorEventCode in [11,12,56,60,65,67]>)\n    \n\t//创建会话窗口引擎检测开关门超时\n\tswOut2 = table(1:0,`doorNum`eventDate`doorEventCode,[INT,DATETIME,INT])\n\tswEngine = createSessionWindowEngine(name=\"swEngine\", sessionGap=300, metrics=<last(doorEventCode)>, dummyTable=swOut2, outputTable=getStreamEngine(\"reactivEngine2\"), timeColumn=`eventDate,keyColumn=`doorNum,useSessionStartTime=false)\n\n    // 创建响应式状态引擎1 过滤重复数据\n    reactivEngine1 = createReactiveStateEngine(name=`reactivEngine1, metrics=<[eventDate,doorEventCode]>, dummyTable=objByName(`doorRecord),outputTable= getStreamEngine(\"swEngine\"),keyColumn=`doorNum,filter=<prev(doorEventCode)!=doorEventCode>)\n    \n\t//4.4 订阅流数据\n\tsubscribeTable(tableName=\"doorRecord\", actionName=\"monitor\", offset=0, handler=append!{reactivEngine1}, msgAsTable=true)\n}\n```\n\n----------------------------------------\n\nTITLE: Loading Data into DolphinDB\nDESCRIPTION: This snippet demonstrates how to load data from a text file into a DolphinDB table using the `loadText` function, defining the schema using `table` function to specify column names and data types. It assumes the data file 'D:/dataset/wine.data' exists and is accessible.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/machine_learning.md#_snippet_0\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nwineSchema = table(\n    `Label`Alcohol`MalicAcid`Ash`AlcalinityOfAsh`Magnesium`TotalPhenols`Flavanoids`NonflavanoidPhenols`Proanthocyanins`ColorIntensity`Hue`OD280_OD315`Proline as name,\n    `INT`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE as type\n)\nwine = loadText(\"D:/dataset/wine.data\", schema=wineSchema)\n```\n\n----------------------------------------\n\nTITLE: Cleaning Environment and Creating Stream Tables in DolphinDB\nDESCRIPTION: Defines and uses two DolphinDB functions: `cleanEnvironment` to remove existing streaming artifacts (jobs, subscriptions, engines, shared tables) with error handling, and `createStreamTable` to create a shared stream table (`snapshotStreamTable`) based on an existing table's schema and a shared keyed table (`changeCrossSectionalTable`) for storing calculation results. Requires the `ops` module.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Real-Time_Stock_Price_Increase_Calculation.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\nuse ops\n\n// clean up environment\ndef cleanEnvironment(){\n\tcancelJobEx()\n\ttry{ unsubscribeTable(tableName=`snapshotStreamTable, actionName=\"snapshotFilter\") } catch(ex){ print(ex) }\n\ttry{ dropStreamEngine(\"calChange\")} catch(ex){ print(ex) }\n\ttry{ dropStreamEngine(\"crossSectionalEngine\") } catch(ex){ print(ex) }\n\ttry{ undef(\"snapshotStreamTable\", SHARED) } catch(ex){ print(ex) }\n\ttry{ undef(\"changeCrossSectionalTable\", SHARED) } catch(ex){ print(ex) }\n}\n\n// create stream table\ndef createStreamTable(dbName, tbName){\n\tschemaTB = loadTable(dbName, tbName).schema().colDefs\n\tshare(streamTable(40000:0, schemaTB.name, schemaTB.typeString), `snapshotStreamTable)\n\tshare(keyedTable(`SecurityID, 50:0, `DateTime`SecurityID`factor_1min`rank_1min`factor_5min`rank_5min`factor_10min`rank_10min, [TIMESTAMP, SYMBOL, DOUBLE,  INT, DOUBLE, INT, DOUBLE, INT]), `changeCrossSectionalTable)\n}\n\ncleanEnvironment()\ndbName, tbName = \"dfs://snapshot\", \"snapshot\"\ncreateStreamTable(dbName, tbName)\n```\n\n----------------------------------------\n\nTITLE: Creating database and partitioned table for trade data in DolphinDB\nDESCRIPTION: This snippet creates a DolphinDB database with partitioned tables tailored for store stock trade data. It involves defining the schema, dropping existing database if present, creating a new database with specified partitioning and compression methods, and establishing a schema table with appropriate columns. Dependencies include DolphinDB server access and correct environment paths. Inputs are database name and table parameters; output is the setup database ready for data import.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/streaming_capital_flow_order_by_order/01.创建存储历史数据的库表并导入数据.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\n/**\npart1: create database and table to store the data\nmodified location: csvDataPath, dbName, tbName\n*/\ncsvDataPath = \"/hdd/hdd9/data/streaming_capital_flow/20200102_SH_trade.csv\"\ndbName = \"dfs://trade\"\ntbName = \"trade\"\n//login account\nlogin(\"admin\", \"123456\")\n//create database and table\nif(existsDatabase(dbName)){\n\tdropDatabase(dbName)\n}\n\n// Create database with specific partitioning schemes\n\n// Database instances with different partition schemes\n\ndb1 = database(, VALUE, 2020.01.01..2022.01.01)\n\ndb2 = database(, HASH, [SYMBOL, 5])\n\n// Combine into a composite database\n\ndb = database(dbName, COMPO, [db1, db2])\n\n// Define table schema with various data types\n\nschemaTable = table(\n\t array(SYMBOL, 0) as SecurityID,\n\t array(SYMBOL, 0) as Market,\n\t array(TIMESTAMP, 0) as TradeTime,\n\t array(DOUBLE, 0) as TradePrice,\n\t array(INT, 0) as TradeQty,\n\t array(DOUBLE, 0) as TradeAmount,\n\t array(INT, 0) as BuyNum,\n\t array(INT, 0) as SellNum\n)\n\n// Create a partitioned table with compression on TradeTime and SecurityID columns\n\ndb.createPartitionedTable(table=schemaTable, tableName=tbName, partitionColumns=`TradeTime`SecurityID, compressMethods={TradeTime:\"delta\"})\ngo\n\n```\n\n----------------------------------------\n\nTITLE: Convert Date and Time Columns\nDESCRIPTION: This snippet converts integer-based date and time columns to the proper DATE and TIME types using `temporalParse`. This step is needed because `readRecord!` imports date and time as integers.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_data.md#_snippet_10\n\nLANGUAGE: txt\nCODE:\n```\ntb.replaceColumn!(`date, tb.date.string().temporalParse(\"yyyyMMdd\"))\ntb.replaceColumn!(`time, tb.time.format(\"000000000\").temporalParse(\"HHmmssSSS\"))\n```\n\n----------------------------------------\n\nTITLE: Connecting and Querying DolphinDB using Python API\nDESCRIPTION: This Python code snippet connects to a DolphinDB server and queries data from a database table. It measures the time taken to execute the query and retrieve the data. It queries the level 1 dataset for a specific date.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/pickle_comparison.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\nimport pandas as pd\nimport time\n\ns = ddb.session()\ns.connect(\"192.168.1.13\",22172, \"admin\", \"123456\")\n\n＃读取Level 1数据集一天的数据\nst1 = time.time()\nquotes =s.run('''\nselect * from loadTable(\"dfs://TAQ\", \"quotes\") where TradeDate = 2007.08.23\n''')\net1 = time.time()\nprint(et1 - st1)\n\n＃读取Level 2数据集一天的数据\nst = time.time()\ntick = s.run('''\nselect * from loadTable(\"dfs://DataYesDB\", \"tick\") where TradeDate = 2019.09.10\n''')\net = time.time()\nprint(et-st)\n```\n\n----------------------------------------\n\nTITLE: 点查询示例查询语句\nDESCRIPTION: 用于性能测试的简单点查询语句，根据证券ID和交易日期查询平均交易价格。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/database_and_table_creation_details.md#_snippet_19\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect avg(TradePrice) from pt where SecurityID=`600 and TradeDate=2020.01.04\n```\n\n----------------------------------------\n\nTITLE: Subscribing Stream Table to Aggregator DolphinDB Script\nDESCRIPTION: Establishes a subscription for the `sensorTemp` stream table to feed data into the `demoAgg` time-series aggregator. The `handler` function `append!{demoAgg}` ensures that every new data batch appended to `sensorTemp` is immediately processed by the aggregator pipeline.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/iot_demo_script.txt#_snippet_5\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nsubscribeTable( tableName=\"sensorTemp\", actionName=\"demoAgg\", offset=-1, handler=append!{demoAgg}, msgAsTable=true)\n```\n\n----------------------------------------\n\nTITLE: Loading Data into Non-Partitioned Memory Table\nDESCRIPTION: Loads data from a text file into a non-partitioned in-memory table using the `loadText` function. This is a simple way to create a table from a file.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/partitioned_in_memory_table.md#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntrades=loadText(workDir + \"/trades.txt\")\n```\n\n----------------------------------------\n\nTITLE: Creating a Distributed Database for Level 2 Data (DolphinDB Script)\nDESCRIPTION: Checks if the database `dfs://stockL2` exists. If not, it creates a distributed database using the TSDB storage engine. The database uses a COMPO partitioning scheme: VALUE partitioning on date (ranging from 2022.01.01 to 2022.12.01) and HASH partitioning on SYMBOL (20 partitions).\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_implementation_for_NSQ_Plugin.md#_snippet_7\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\ndbName = \"dfs://stockL2\"\nif(existsDatabase(dbName) == false){\n\tdbDate = database(\"\", VALUE, 2022.01.01..2022.12.01)\n\tdbSymbol = database(\"\", HASH, [SYMBOL, 20])\n\tdatabase(dbName, COMPO, [dbDate, dbSymbol], , \"TSDB\")\n\tprint(\"Database created successfully !\")\n}\nelse{\n\tprint(\"Database and table have been created !\")\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Feature Engineering Function for Stock Data in DolphinDB\nDESCRIPTION: Defines a custom function `featureEngine` using `defg` for optimized group processing. It accepts matrices of bid prices, bid quantities, offer prices, and offer quantities. Inside the function, it calculates Bid-Ask Spread (BAS), Weighted Average Price (WAP), Depth Imbalance (DI) for 10 levels, Price Pressure (Press), and Realized Volatility (RV) based on log returns of WAP. It returns the average values of BAS, DI0-DI9, Press, and the calculated RV for the input window.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/machine_learning_volatility/01.dataProcess.txt#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndefg featureEngine(bidPrice,bidQty,offerPrice,offerQty){\n\tbas = offerPrice[0]\\bidPrice[0]-1\n\twap = (bidPrice[0]*offerQty[0] + offerPrice[0]*bidQty[0])\\(bidQty[0]+offerQty[0])\n\tdi = (bidQty-offerQty)\\(bidQty+offerQty)\n\tbidw=(1.0\\(bidPrice-wap))\n\tbidw=bidw\\(bidw.rowSum())\n\tofferw=(1.0\\(offerPrice-wap))\n\tofferw=offerw\\(offerw.rowSum())\n\tpress=log((bidQty*bidw).rowSum())-log((offerQty*offerw).rowSum())\n\trv=std(log(wap)-log(prev(wap)))*sqrt(24*252*size(wap))\n\treturn avg(bas),avg(di[0]),avg(di[1]),avg(di[2]),avg(di[3]),avg(di[4]),avg(di[5]),avg(di[6]),avg(di[7]),avg(di[8]),avg(di[9]),avg(press),rv\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Tables with DECIMAL Types and Inserting Data - DolphinDB\nDESCRIPTION: Shows how to create a DolphinDB table with DECIMAL32 and DECIMAL64 typed columns by specifying them in the column type list during table creation. Demonstrates inserting data either by explicitly converting values to DECIMAL types or using raw numeric or string values which DolphinDB implicitly converts. Supports in-memory tables as well as tables created in OLAP and TSDB engines with partitioning and appending data. Requires DolphinDB environment with database engines configured.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DECIMAL.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\nt=table(100:0, `id`val1`val2, [INT, DECIMAL32(4), DECIMAL64(8)])\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\ninsert into t values(1, decimal32(2.345, 4), decimal64(2.3654, 8));\nor\ninsert into t values(1, 2.345, 2.3654);\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 在olap引擎中：\ndbName=\"dfs://testDecimal_olap\"\nif(existsDatabase(dbName)){\n\tdropDatabase(dbName)\n}\nt=table(100:0, `id`sym`timev`val1`val2, [INT, SYMBOL, TIMESTAMP, DECIMAL32(4), DECIMAL64(8)])\ndb=database(dbName, VALUE, 1..10)\npt=db.createPartitionedTable(t, `pt, `id)\nn=1000\ndata=table(rand(1..10, n) as id, rand(\"A\"+string(1..10), n) as sym, rand(2022.11.24T12:23:45.456+1..100, n) as timev, rand(100.0, n) as val1, rand(100.0, n) as val2)\nt.append!(data)\npt.append!(t)\nselect * from loadTable(dbName, `pt)\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 在tsdb引擎中：\ndbName=\"dfs://testDecimal_tsdb\"\nif(existsDatabase(dbName)){\n\tdropDatabase(dbName)\n}\nt=table(100:0, `id`sym`timev`val1`val2, [INT, SYMBOL, TIMESTAMP, DECIMAL32(4), DECIMAL64(8)])\ndb=database(dbName, VALUE, 1..10, , \"TSDB\")\npt=db.createPartitionedTable(t, `pt, `id, , `sym`timev)\nn=1000\ndata=table(rand(1..10, n) as id, rand(\"A\"+string(1..10), n) as sym, rand(2022.11.24T12:23:45.456+1..100, n) as timev, rand(100.0, n) as val1, rand(100.0, n) as val2)\nt.append!(data)\npt.append!(t)\nselect * from loadTable(dbName, `pt)\n```\n\n----------------------------------------\n\nTITLE: 使用socp函数求解带有换手率约束的投资组合问题\nDESCRIPTION: 通过二阶锥规划求解带有换手率约束的投资组合优化问题。换手率表示股票权重变动的绝对值之和，限制了投资组合的交易成本，使用DolphinDB的socp函数进行求解。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/MVO.md#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\nbaseDir = \"/home/data/\"\"\nf = dropna(flatten(float(matrix(select col1, col2, col3, col4, col5, col6, col7, col8, col9, col10 from loadText(baseDir + \"C.csv\") where rowNo(col1) > 0)).transpose()))\nN = f.size()\n\n// Ax <= b\nA = matrix(select * from loadText(baseDir + \"A_ub.csv\"))\nx = sum(A[0,])\nsum(x);\n\nb = \n[0.025876723,\t0.092515275,\t0.035133942,\t0.053184884,\t0.067410565,\t0.009709433,\t0.04668745,\t0.00636804,\t0.022258664,\t0.11027537,\n0.018488302,\t0.027417204,\t0.028585,\t0.017228214,\t0.008055527,\t0.015727843,\t0.026132369,\t0.013646113,\t0.066000808,\t0.043606587,\n0.048325258,\t0.033868626,\t0.010790603,\t0.017737391,\t0.03252374,\t0.039329965,\t0.040665779,\t0.010868773,\t0.006819891,\t0.015879314,\n0.008882335,\t-0.025876723,\t-0.092515275,\t-0.035133942,\t-0.053184884,\t-0.067410565,\t-0.009709433,\t-0.04668745,\t-0.00636804,\t-0.022258664,\n-0.110275379,\t-0.018488302,\t-0.027417204,\t-0.028585,\t-0.017228214,\t-0.008055527,\t-0.015727843,\t-0.026132369,\t-0.013646113,\t-0.066000808,\n-0.043606587,\t-0.048325258,\t-0.033868626,\t-0.010790603,\t-0.017737391,\t-0.03252374,\t-0.039329965,\t-0.040665779,\t-0.010868773,\t-0.006819891,\n-0.015879314,\t-0.008882335]\n\nx0 = exec w0 from  loadText(baseDir + \"w0.csv\") \n\n\n// minimize f^T * x + 0 * u , c = [f, 0] 引入新的求解变量\nc = -f // f^T * x， \nc.append!(take(0, N)) // 0 * u\nc;\n\n// 根据二阶锥的锥形式与标准形式对应关系设置矩阵 G \nE = eye(N)\nzeros = matrix(DOUBLE, N, N, ,0)\n\nG = concatMatrix([-E,zeros]) // -x <= -lb \nG = concatMatrix([G, concatMatrix([E,zeros])], false) // x <= ub\nG = concatMatrix([G, concatMatrix([E,-E])], false) // x_i -u_i <= x`_i \nG = concatMatrix([G, concatMatrix([-E,-E])], false) // -x_i-u_i <= -x`_i\n\nG = concatMatrix([G, concatMatrix([matrix(DOUBLE,1,N,,0), matrix(DOUBLE,1,N,,1)])], false) // sum(u)=c\nG = concatMatrix([G, concatMatrix([A, matrix(DOUBLE,b.size(), N,,0)])], false) // A*x <= b\n\n//根据二阶锥的锥形式与标准形式对应关系设置向量 h\nh = array(DOUBLE).append!(take(0, N)) // -x <= -lb\nh.append!(take(0.3, N)) // x <= ub\nh.append!(x0) // x_i -u_i <= x`_i\nh.append!(-x0) // -x_i-u_i <= -x`_i\nh.append!(1) // sum(u)<=c 换手率约束\nh.append!(b) // A*x <= b \n\n// l, q\nl = 9891 // 所有约束都是线性的\nq = []\n\n// Aeq, beq\nAeq = concatMatrix([matrix(DOUBLE, 1, N, ,1), matrix(DOUBLE, 1, N, ,0)])\nbeq = array(DOUBLE).append!(1)\n\nres = socp(c, G, h, l, q, Aeq, beq);\nprint(res)\n// output > (\"Problem solved to optimality\",[4.030922466819474E-13,-1.38420918319221E-12,2.633003706864388E-12,7.170457424061185E-12,3.08367186410226E-13,-1.538707717541051E-12,0.00029999999826,4.494868392653513E-12,-1.046866787477076E-12,0.000799999998839,1.16264810755163E-12,1.610142862543512E-11,-1.056487154243985E-13,-2.157448203710766E-13,4.389916149869339E-12,1.875852354004975E-12,7.82477001121239E-12,0.001299999997416,4.727903065768357E-13,7.550671346161244E-13,5.591928646425624E-12,1.124989684680735E-11,5.482907889730026E-12,0.015985000026396,0.000700000000029,5.633638482685261E-12,1.67297174174449E-12,5.469150816162936E-12,-6.300522047286838E-13,2.303301153098362E-12...],0.964278008555949)\n```\n\n----------------------------------------\n\nTITLE: Time Format Conversion Function for RFC3339 to DolphinDB\nDESCRIPTION: DolphinDB functions for converting RFC3339 formatted time strings from InfluxDB into DolphinDB temporal types. The function handles different formats of RFC3339 timestamps and converts them to local time.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Migrate_data_from_InfluxDB_to_DolphinDB.md#_snippet_3\n\nLANGUAGE: dql\nCODE:\n```\ndef parseRFC3339(timeStr) {\n\tif(strlen(timeStr) == 20) {\n\t\treturn localtime(temporalParse(timeStr,'yyyy-MM-ddTHH:mm:ssZ'));\n\t} else if (strlen(timeStr) == 24) {\n\t\treturn localtime(temporalParse(timeStr,'yyyy-MM-ddTHH:mm:ss.SSSZ'));\n\t} else {\n\t\treturn timeStr;\n\t}\n}\n\ndef transData(dbName, tbName, mutable data) {\n\ttimeCol = exec time from data; writeLog(timeCol);\n\ttimeCol=each(parseRFC3339, timeCol);\n\treplaceColumn!(data, 'time', timeCol);\n\tloadTable(dbName,tbName).append!(data);\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Data Partitioning Function\nDESCRIPTION: This code defines a function to divide data from a table into two separate tables based on the 'type' column (stock or futures). It selects stock and futures data into temporary tables, then appends them to the specified stockTB and futuresTB tables respectively. `mutable` keyword indicates the tables can be modified.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_csv.md#_snippet_27\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef divideImport(tb, mutable stockTB, mutable futuresTB)\n{\n\ttdata1=select * from tb where type=\"stock\"\n\ttdata2=select * from tb where type=\"futures\"\n\tappend!(stockTB, tdata1)\n\tappend!(futuresTB, tdata2)\n}\n```\n\n----------------------------------------\n\nTITLE: Defining publishTableData Function\nDESCRIPTION: This function connects to an MQTT broker and publishes data from a DolphinDB table to a specified topic.  It uses the MQTT plugin's `connect`, `publish`, and `close` functions. Parameters include the server address, topic, data formatter, batch size, and the data table itself.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/data/DolphinDB_streaming_application_in_IOT/deviceState.txt#_snippet_8\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef publishTableData(server,topic,f, batchsize,t){\n    \tconn=connect(server,1883,0,f,batchsize)\n   \tpublish(conn,topic,t)\n    \tclose(conn)\n}\n```\n\n----------------------------------------\n\nTITLE: Scheduling a job with partial application\nDESCRIPTION: This code schedules a daily job that runs at midnight to execute the getMaxTemperature function with a fixed device ID of 1. It uses partial application to create a no-argument function from getMaxTemperature{1}, which is compatible with scheduleJob.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_57\n\nLANGUAGE: shell\nCODE:\n```\nscheduleJob(`testJob, \"getMaxTemperature\", getMaxTemperature{1}, 00:00m, today(), today()+30, 'D');\n```\n\n----------------------------------------\n\nTITLE: Execution Plan When Filtering by Partition Key in DolphinDB\nDESCRIPTION: This JSON object shows the optimized execution plan when filtering by the partition key ('dateb'). The 'map.partitions.remote' value is 1, indicating only the necessary partition was scanned. The presence of an 'optimize' section with 'field': 'single partition query' confirms the optimization, resulting in a significantly lower cost compared to filtering by a non-partition key.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_Explain.md#_snippet_14\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"measurement\": \"microsecond\",\n  \"explain\": {\n    \"from\": {\n      \"cost\": 1\n    },\n    \"map\": {\n      \"partitions\": {\n        \"local\": 0,\n        \"remote\": 1\n      },\n      \"cost\": 1275,\n      \"optimize\": {\n        \"cost\": 27,\n        \"field\": \"single partition query\",\n        \"sql\": \"select [98307] datea,dateb,x from pt [partition = /valuedb2/20000101/OD]\",\n        \"explain\": {\n          \"rows\": 500000,\n          \"cost\": 1248\n        }\n      }\n    },\n    \"rows\": 500000,\n    \"cost\": 2214\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Scheduling Daily DolphinDB Backup Job Using scheduleJob\nDESCRIPTION: This code schedules a daily backup job in DolphinDB that executes at 00:05 every day. It uses login credentials to authenticate and creates a scheduled job named 'backupJob' using the backup function invoked with dynamic date parameters. The SQL expression selects partitions for the previous day. Parameters include backup path, SQL object, force flag, and parallel execution flag. Inputs are date-dependent, and outputs trigger periodic backups stored on disk. This snippet requires DolphinDB server support for scheduling and authentication.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/iot_examples.md#_snippet_34\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nlogin(`admin,`123456)\nscheduleJob(`backupJob, \"backupDB\", backup{\"/hdd/hdd1/backupDemo/\"+(today()-1).format(\"yyyyMMdd\"),<select * from loadTable(\"dfs://svmDemo\",\"sensors\") where datetime between datetime(today()-1) : (today().datetime()-1) >,false,true}, 00:05m, today(), 2030.12.31, 'D');\n```\n\n----------------------------------------\n\nTITLE: Defining Cluster Node Membership in cluster.nodes Configuration\nDESCRIPTION: Specifies the cluster topology, listing control nodes, agent nodes, data nodes, and compute nodes with IP addresses, port numbers, and unique node aliases. The file is key for identifying cluster nodes and their types and must be identical on all servers in the cluster. The format uses comma separation between localSite definition and mode, with colon separation within localSite details.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ha_cluster_deployment.md#_snippet_3\n\nLANGUAGE: config\nCODE:\n```\nlocalSite,mode\n10.0.0.80:8800:controller1,controller\n10.0.0.81:8800:controller2,controller\n10.0.0.82:8800:controller3,controller\n10.0.0.80:8801:agent1,agent\n10.0.0.80:8802:datanode1,datanode\n10.0.0.80:8803:computenode1,computenode\n10.0.0.81:8801:agent2,agent\n10.0.0.81:8802:datanode2,datanode\n10.0.0.81:8803:computenode2,computenode\n10.0.0.82:8801:agent3,agent\n10.0.0.82:8802:datanode3,datanode\n10.0.0.82:8803:computenode3,computenode\n```\n\n----------------------------------------\n\nTITLE: Post-processing and Filtering Calculated Features in DolphinDB\nDESCRIPTION: Performs several post-processing steps on the `result` table. It calculates the next interval's realized volatility (`targetRV`) using `next(RV)` within groups defined by date and `SecurityID`. It removes rows containing any invalid (NULL) values. It then filters the data to keep only days where there are exactly 23 records (10-minute intervals) per `SecurityID`, ensuring data integrity for daily analysis. Finally, the temporary `count` column used for filtering is dropped.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/machine_learning_volatility/01.dataProcess.txt#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\nresult = select *, next(RV) as targetRV from result context by date(TradeTime), SecurityID\nresult = result[each(isValid, result.values()).rowAnd()]\nresult = select * from (select *, count(*) from result context by date(TradeTime), SecurityID) where count=23\ndropColumns!(result, `count)\n```\n\n----------------------------------------\n\nTITLE: Logging In and Importing Partitioned CSV Data with DolphinDB Script\nDESCRIPTION: This DolphinDB script logs into the database as the 'admin' user, defines schemas for device info and readings, creates range-partitioned databases, and imports data from CSV files into partitioned and in-memory tables. Dependencies include access to DolphinDB with appropriate privileges and access to the specified CSV file paths. The snippet partitions the readings table by 'time' and 'device_id' and loads data using 'loadTextEx', while device metadata is loaded to memory using 'loadText'. Schema, partitioning, and table creation steps are highlighted. The script assumes availability of local media and consistent CSV file structure.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs MongoDB/dolphindb_readings_partitioned.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(\"admin\",\"123456\")\n//导入csv文件为内存表\nfp_info = '/media/xllu/aa/device/devices_info.csv'\nfp_readings = '/media/xllu/aa/device/devices_readings.csv'\n\ndp_readings = \"dfs://db_range_dfs\"\n//dp_readings = \"/home/xllu/database/db_range\"\n// 创建两张表的 schema\ncolnames_info \t= `device_id`api_version`manufacturer`model`os_name\ncolnames_readings = `time`device_id`battery_level`battery_status`battery_temperature`bssid`cpu_avg_1min`cpu_avg_5min`cpu_avg_15min`mem_free`mem_used`rssi`ssid\n\ntypes_info = `SYMBOL`INT`STRING`STRING`STRING\ntypes_readings = `DATETIME`SYMBOL`INT`STRING`DOUBLE`STRING`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`SYMBOL\n\nschema_info \t= table(colnames_info, types_info)\nschema_readings = table(colnames_readings, types_readings)\n\ntime_range=2016.11.15T00:00:00 + 86400 * 0..4\nid_range=('demo'+lpad((0..10*300)$STRING,6,\"0\"))$SYMBOL\n\ntime_schema=database('',RANGE,time_range)\nid_schema=database('',RANGE,id_range)\n// 创建分区数据库并定义分区方式\nif(existsDatabase(dp_readings)){\n\tdropDatabase(dp_readings)\n}\ndb = database(dp_readings , COMPO, [time_schema,id_schema])\n\n// 从 CSV 导入 readings 表的数据到 readings 数据库并完成数据分区操作\ntimer loadTextEx(db, `readings_pt, `time`device_id, fp_readings, , schema_readings)\n//38s\n// 从 CSV 导入 device_info 表的数据到 device_info 内存表\ndevice_info = loadText(fp_info, , schema_info)\n\n// 加载 readings 表（不包括表连接）\nreadings = loadTable(db, `readings_pt)\n```\n\n----------------------------------------\n\nTITLE: Loading Data from Disk Partitioned Table with loadTable\nDESCRIPTION: Loads data from a disk-based partitioned table into memory using `loadTable`.  This is useful when the data is larger than available memory and only a subset is needed.  The `memoryMode` parameter must be set to 1 to load the data, otherwise only metadata is loaded.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/partitioned_in_memory_table.md#_snippet_10\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndb = database(workDir+\"/tradeDB\", RANGE, [\"A\",\"G\",\"M\",\"S\",\"ZZZZ\"])\ndb.loadTextEx(`trades, `sym, workDir + \"/trades.txt\");\n\ndb = database(workDir+\"/tradeDB\")\ntrades=loadTable(db, `trades, [\"A\", \"M\"], 1);\n```\n\n----------------------------------------\n\nTITLE: Pickle File Operations in Python\nDESCRIPTION: This Python code snippet demonstrates reading data from a pickle file. The code first connects to a DolphinDB server and retrieves data, saves the data to pickle files and then measures the time taken to load the data from a pickle file.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/pickle_comparison.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\nimport pandas as pd\nimport time\nimport pickle\n\ns = ddb.session()\ns.connect(\"192.168.1.13\", 22172, \"admin\", \"123456\")\ntick = s.run('''\nselect * from loadTable(\"dfs://DataYesDB\", \"tick\") where TradeDate = 2019.09.10\n''')\nquotes =s.run('''\nselect * from loadTable(\"dfs://TAQ\", \"quotes\")\n''')\n\n#将数据集1的Level 1的数据转换为pkl文件\nquotes.to_pickle(\"taq.pkl\")\n\n#将数据集2的Level 2一天的数据转换为pkl文件\ntick.to_pickle(\"level2.pkl\")\n\n#使用pickle模块读取数据集1的Level 1一天的数据\nst1 = time.time()\nf = open('taq.pkl', 'rb')\nc = pickle.load(f)\net1 = time.time()\nprint(et1 - st1)\nf.close()\n\n#使用pickle模块读取数据集2的Level 2一天的数据\nf = open('level2.pkl', 'rb')\nst = time.time()\nc = pickle.load(f)\net = time.time()\nprint(et - st)\nf.close()\n```\n\n----------------------------------------\n\nTITLE: Defining a Disk Usage Alert Rule in Prometheus (YAML)\nDESCRIPTION: This YAML snippet defines a Prometheus alert rule group named \"servers monitor\". It includes a specific rule called \"diskUsage\" that triggers when the calculated disk usage percentage for specified mount points (`/hdd`, `/ssd`, `/home`) exceeds 90% for a duration of 5 minutes (`for: 5m`). The rule includes labels (e.g., `status: warning`) and annotations (`summary`, `description`) to provide context in the alert message.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/cluster_monitor.md#_snippet_15\n\nLANGUAGE: yaml\nCODE:\n```\ngroups:\n- name: \"servers monitor\"\n  rules:  \n  - alert: \"diskUsage\" #服务器磁盘空间使用率\n    expr: (1-sum(node_filesystem_avail_bytes{mountpoint =~\"/hdd.*?|/ssd.*?|/home.*?\"})by (instance,mountpoint)/sum(node_filesystem_size_bytes{mountpoint =~\"/hdd.*?|/ssd.*?|/home.*?\"})by(instance,mountpoint))*100>90\n    for: 5m\n    labels:\n      status: warning\n    annotations:\n      summary: \"{{$labels.instance}} {{$labels.mountpoint}}: High disk Usage Detected\"\n      description: \"{{$labels.instance}} {{$labels.mountpoint}} disk Usage is: {{$value}}%, above 90%\"\n```\n\n----------------------------------------\n\nTITLE: Setting DolphinDB Controller Node Configurations in controller.cfg\nDESCRIPTION: This snippet configures a controller node in the DolphinDB cluster, setting the mode as controller, specifying the local site address, replication factors including DFS replication factor and reliability level, enabling Raft protocol for high availability mode, and tuning synchronization, connection limits, memory size, and worker threads. This configuration is critical for metadata management and leader election in the high availability cluster. Correct values ensure failover handling and data consistency.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/iot_examples.md#_snippet_30\n\nLANGUAGE: cfg\nCODE:\n```\nmode=controller\nlocalSite=192.168.1.13:22216:controller2\ndfsReplicationFactor=2\ndfsReplicaReliabilityLevel=1或dfsReplicaReliabilityLevel=2\ndfsHAMode=Raft\ndataSync=1\nmaxConnections=128\nmaxMemSize=16\nwebWorkerNum=4\nworkerNum=4\n```\n\n----------------------------------------\n\nTITLE: Defining and Computing High-Frequency Financial Factors with Stream Replay in DolphinDB Script\nDESCRIPTION: This snippet demonstrates defining a custom function for factor calculation, creating a streaming table for tick data, and using a reactive state engine to compute a specified factor from streaming data. It also shows how to replay historical data from a DolphinDB database into the stream table to enable factor computation on historical ticks. Dependencies include the predefined EMA and streamTable functions and a connection to the historical database 'dfs://TAQ'. The inputs are streaming tick data with fields like symbol, date, time, and price, and outputs are computed factor values per symbol. Note that factor computation done by replaying historical data into a streaming engine can be inefficient for large datasets.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/reactive_state_engine.md#_snippet_10\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef sum_diff(x, y){\n    return (x-y)/(x+y)\n}\nfactor1 = <ema(1000 * sum_diff(ema(price, 20), ema(price, 40)),10) -  ema(1000 * sum_diff(ema(price, 20), ema(price, 40)), 20)>\n\nshare streamTable(1:0, `sym`date`time`price, [STRING,DATE,TIME,DOUBLE]) as tickStream\nresult = table(1000:0, `sym`factor1, [STRING,DOUBLE])\nrse = createReactiveStateEngine(name=\"reactiveDemo\", metrics =factor1, dummyTable=tickStream, outputTable=result, keyColumn=\"sym\")\nsubscribeTable(tableName=`tickStream, actionName=\"factors\", handler=tableInsert{rse})\n\n//从历史数据库dfs://TAQ的trades表中加载一天的数据，回放到流表tickStream中\ninputDS = replayDS(<select sym, date, time, price from loadTable(\"dfs://TAQ\", \"trades\") where date=2021.03.08>, `date, `time, 08:00:00.000 + (1..10) * 3600000)\nreplay(inputDS, tickStream, `date, `time, 1000, true, 2)\n```\n\n----------------------------------------\n\nTITLE: Calculating OLS Regression Residuals on Tabular Data in DolphinDB\nDESCRIPTION: Shows how to calculate Ordinary Least Squares (OLS) regression residuals for each row of data against a benchmark vector. Uses matrix transformation, the ols function, and partial application with the each higher-order function.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_66\n\nLANGUAGE: DolphinDB\nCODE:\n```\nt=table(2020.11.01 2020.11.02 as date, `IBM`MSFT as ticker, 1.0 2 as past1, 2.0 2.5 as past3, 3.5 7 as past5, 4.2 2.4 as past10, 5.0 3.7 as past20, 5.5 6.2 as past30, 7.0 8.0 as past60)\n\nmt = matrix(t[`past1`past3`past5`past10`past20`past30`past60]).transpose()\nt[`residual] = each(def(y, x){ return ols(y, x, true, 2).ANOVA.SS[1]}{,benchX}, mt)\n```\n\n----------------------------------------\n\nTITLE: Removing Rows with NULL Values in DolphinDB Table Using each High-Order Function\nDESCRIPTION: Demonstrates multiple approaches to filter out rows containing NULL values in a DolphinDB table. First, creates table 't' with columns 'sym', 'id', and 'id2', possibly containing NULLs. The first method filters rows by checking the id and id2 fields explicitly for NULL on each row dictionary. The second method generalizes the check for all columns by testing 'isValid' on each value of the row dictionary, requiring less manual enumeration. The third method leverages DolphinDB's columnar storage and applies 'isValid' on each column vector with 'each', then uses 'rowAnd' to identify rows where all columns are valid, achieving better performance. If the data size is too large, the example warns of exceeding matrix cell limits and suggests 'reduce' for memory-efficient row filtering. These scripts require 't' table and a basic understanding of DolphinDB's null handling and functional programming.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/func_progr_cases.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\nsym = take(`a`b`c, 110)\nid = 1..100 join take(int(),10)\nid2 =  take(int(),10) join 1..100\nt = table(sym, id,id2)\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nt[each(x -> !(x.id == NULL || x.id2 == NULL), t)]\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nt[each(x -> all(isValid(x.values())), t)]\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nt[each(isValid, t.values()).rowAnd()]\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nt[reduce(def(x,y) -> x and isValid(y), t.values(), true)]\n```\n\n----------------------------------------\n\nTITLE: Starting the Alertmanager Service (Shell)\nDESCRIPTION: This shell command starts the Alertmanager service in the background using `nohup`. It specifies the path to the configuration file (`--config.file=alertmanager.yml`), the web interface listening address and port (`--web.listen-address=\":9093\"`), and the cluster communication address (`--cluster.listen-address=localhost:9095`). The `&` symbol ensures the process runs in the background.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/cluster_monitor.md#_snippet_17\n\nLANGUAGE: shell\nCODE:\n```\nnohup ./alertmanager --config.file=alertmanager.yml --web.listen-address=\":9093\" --cluster.listen-address=localhost:9095 &\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Shared Cache Usage (OLAP) (DolphinDB Script)\nDESCRIPTION: Queries all data from a specific partition ('2022.01.01') which resides on node1, executed from different clients (potentially connected to node1 and node2). This illustrates that the partition data is loaded into memory only once on the node where it resides (node1), and this cache is shared for subsequent requests, even from different clients/nodes.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/memory_management.md#_snippet_9\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nselect * from loadTable(dbName,tableName) where date = 2022.01.01\nmem().allocatedBytes - mem().freeBytes\n```\n\n----------------------------------------\n\nTITLE: Login and Database/Table Creation in DolphinDB\nDESCRIPTION: This code snippet logs into the DolphinDB system and creates a database named \"dfs://testDB\" with date-based partitioning. It then defines the schema for a table \"testTB\" and creates the partitioned table using the specified schema and partitioning column (DateTime). This is a prerequisite for loading and querying data later.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/single_machine_cluster_deploy.md#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 创建存储的数据库和分区表\nlogin(\"admin\", \"123456\")\ndbName = \"dfs://testDB\"\ntbName = \"testTB\"\nif(existsDatabase(dbName)){\n        dropDatabase(dbName)\n}\ndb = database(dbName, VALUE, 2021.01.01..2021.12.31)\ncolNames = `SecurityID`DateTime`PreClosePx`OpenPx`HighPx`LowPx`LastPx`Volume`Amount\ncolTypes = [SYMBOL, DATETIME, DOUBLE, DOUBLE, DOUBLE, DOUBLE, DOUBLE, INT, DOUBLE]\nschemaTable = table(1:0, colNames, colTypes)\ndb.createPartitionedTable(table=schemaTable, tableName=tbName, partitionColumns=`DateTime)\n```\n\n----------------------------------------\n\nTITLE: Scheduling Incremental Data Load from ClickHouse to DolphinDB\nDESCRIPTION: This snippet defines a function to schedule incremental data loading from ClickHouse to DolphinDB using ODBC. It builds an SQL query to fetch data for the previous day and uses scheduleJob to execute the function periodically.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ClickHouse_to_DolphinDB.md#_snippet_6\n\nLANGUAGE: dolphindb\nCODE:\n```\ndef scheduleLoad(conn,dbName,tbName){\n    sqlQuery = \"select SecurityID, TradeTime, TradePrice, TradeQty, TradeAmount, BuyNo, SellNo, ChannelNo, TradeIndex, TradeBSFlag, BizIndex from migrate.ticksh\"\n    sql = sqlQuery + \" WHERE toDate(TradeTime) = '\"+temporalFormat(today()-1,'yyyy-MM-dd')+\"'\"\n    odbc::query(conn,sql, loadTable(dbName,tbName), 100000)\n}\nscheduleJob(jobId=`test, jobDesc=\"test\",jobFunc=scheduleLoad{conn,dbName,tbName},scheduleTime=00:05m,startDate=2023.11.03, endDate=2024.11.03, frequency='D')\n```\n\n----------------------------------------\n\nTITLE: Configuring Debezium MySQL Connector\nDESCRIPTION: JSON configuration for the Debezium MySQL connector. It specifies the MySQL connection details, database to monitor, and Kafka topic settings for schema history.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Debezium_and_Kafka_data_sync.md#_snippet_9\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    \"name\": \"basicinfo-connector\",\n    \"config\":{\n        \"connector.class\": \"io.debezium.connector.mysql.MySqlConnector\",\n        \"tasks.max\": \"1\",\n        \"topic.prefix\":\"mysqlserver\",\n        \"database.hostname\": \"192.168.189.130\",\n        \"database.port\": \"3306\",\n        \"database.user\": \"datasyn\",\n        \"database.password\": \"1234\",\n        \"database.server.id\": \"2223314\",\n        \"database.include.list\": \"basicinfo\",\n        \"schema.history.internal.kafka.bootstrap.servers\": \"192.168.189.130:9092\",\n        \"schema.history.internal.kafka.topic\": \"schema-changes.basicinfo\",\n        \"heartbeat.interval.ms\":\"20000\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Restoring a DolphinDB Database\nDESCRIPTION: Illustrates submitting a background job to restore an entire database from a backup directory (`backupDir`) using `restoreDB`. The first example restores to the original path (`dbPath`), potentially on a new cluster. The second example restores the backup to a different database path (`restoreDBPath`) within the same or another cluster. `restoreDB` supports incremental restore and handles database/table creation if they don't exist.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/backup-restore-new.md#_snippet_8\n\nLANGUAGE: DolphinDB\nCODE:\n```\n# Restore to original path (e.g., new cluster)\ndbPath=\"dfs://testdb\"\nbackupDir=\"/home/$USER/backupDB\"\nsubmitJob(\"restoreDB\",\"restore testdb in new cluster\",restoreDB,backupDir,dbPath)\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\n# Restore to a different path (e.g., same cluster)\n# 将1.1节中备份的数据恢复到 restoredb 中\ndbPath=\"dfs://testdb\"\nbackupDir=\"/home/$USER/backupDB\"\nrestoreDBPath=\"dfs://restoredb\"\nsubmitJob(\"restoreDB2\",\"restore testdb to restoredb in the original cluster\",restoreDB,backupDir,dbPath,restoreDBPath)\n```\n\n----------------------------------------\n\nTITLE: Defining Parallel Job 1 for Factor Calculation in DolphinDB\nDESCRIPTION: This function `parJob1` defines a parallel job to calculate factors. It loads data from distributed file system tables `fund_OLAP` and `fund_hs_OLAP`, joins them, preprocesses the data (filling null values), and then calculates the factors using the `getFactor` function in parallel using `ploop`. The fund list `symList` is split into chunks of size 250 for parallel processing.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_factor.txt#_snippet_13\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef parJob1(){\n\ttimer{fund_OLAP=select * from loadTable(\"dfs://fund_OLAP\", \"fund_OLAP\")\n\t\t  fund_hs_OLAP=select * from loadTable(\"dfs://fund_OLAP\", \"fund_hs_OLAP\")\n\t\t  ajResult = select Tradedate, fundNum, value, fund_hs_OLAP.Tradedate as hsTradedate, fund_hs_OLAP.value as price from aj(fund_OLAP, fund_hs_OLAP, `Tradedate)\n\t\t  result2 = select Tradedate, fundNum, iif(isNull(value), ffill!(value), value) as value,price from ajResult where Tradedate == hsTradedate\n\t\t  symList = exec distinct(fundNum) as fundNum from result2 order by fundNum\n\t\t  symList2 = symList.cut(250)}//此处，将任务切分，按每次250个不同基金数据进行计算\n\ttimer{ploop(getFactor{result2}, symList2)}\n}//定义获取九个因子计算和数据操作的时间的函数\n```\n\n----------------------------------------\n\nTITLE: Define, Register, and Grant Permissions for WQAlpha1 Factor Function (DolphinDB)\nDESCRIPTION: This DolphinDB script first logs in as 'admin'. It then defines a global function `get_alpha1` within the 'wq101alpha' module context to compute the WorldQuant Alpha 1 factor. The function takes a vector of security IDs, a start date, and an end date as input, performs type checking, loads tick data from the distributed table 'dfs://tick_close/tick_close', calculates the alpha factor using the `WQAlpha1` function from the 'wq101alpha' module after arranging data into a panel, and returns the results as a table. Finally, it registers the function in the function view using `addFunctionView` and grants execution permission (`VIEW_EXEC`) to the 'admin' user using `grant`.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/Python_Celery/ddb_function.txt#_snippet_0\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nlogin(\"admin\",\"123456\")\n/**\n * 因子：WorldQuant 101 Alpha 因子指标库中的1号因子 WQAlpha1\n参数：\n       security_id：STRING VECTOR,股票代码序列\n       begin_date：DATE,区间开始日期\n       end_date：DATE,区间结束日期\n */\nuse wq101alpha\ndefg get_alpha1(security_id, begin_date, end_date){\n\tif (typestr(security_id) == 'STRING VECTOR' && typestr(begin_date) == `DATE && typestr(end_date) == `DATE){\n\ttick_list = select * from loadTable(\"dfs://tick_close\", \"tick_close\") where TradeDate >= begin_date and TradeDate <= end_date and SecurityID in security_id\n\talpha1_list=WQAlpha1(panel(tick_list.TradeDate, tick_list.SecurityID, tick_list.Value))\n\treturn table(alpha1_list.rowNames() as TradeDate, alpha1_list)\n\t}\n\telse {\n\t\tprint(\"What you have entered is a wrong type\")\n\t\treturn `NULLValue\n\t}\n}\n//将该函数加入到函数视图中\naddFunctionView(get_alpha1)\n//将该函数的调用权限赋予给admin用户\ngrant(\"admin\", VIEW_EXEC, \"get_alpha1\")\n```\n\n----------------------------------------\n\nTITLE: Loading Kafka Plugin and Creating Kafka Producer in DolphinDB (Python)\nDESCRIPTION: This snippet loads the Kafka plugin in DolphinDB and creates a Kafka producer. The `loadPlugin` function loads the specified plugin file. The producer configuration specifies the Kafka broker list. The `kafka::producer` function creates a new Kafka producer instance.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/stock_market_replay.md#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nloadPlugin(\"/DolphinDB/server/plugins/kafka/PluginKafka.txt\")\ngo\nproducerCfg = dict(STRING, ANY)\nproducerCfg[\"metadata.broker.list\"] = \"localhost\"\nproducer = kafka::producer(producerCfg)\n```\n\n----------------------------------------\n\nTITLE: Factor Calculation and Parallel Execution in DolphinDB\nDESCRIPTION: This SQL query demonstrates how to calculate both flow and mathWghtSkew factors for multiple stocks using DolphinDB's context by clause. It applies the calculations to data loaded from a table and organizes computation by security ID.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Python+HDF5_vs_DolphinDB.md#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\nm = 2020.01.02:2020.01.06\nw = 10 9 8 7 6 5 4 3 2 1\nres = select dbtime, SecurityID,flow(BidSize1,OfferSize1, OfferPX1, BidPX1) as Flow_val,mathWghtSkew(matrix(BidPX1,BidPX2,BidPX3,BidPX4,BidPX5,BidPX6,BidPX7,BidPX8,BidPX9,BidPX10),w) as Skew_val  from loadTable(\"dfs://LEVEL2_SZ\",\"Snap\") where TradeDate between  m context by SecurityID\n```\n\n----------------------------------------\n\nTITLE: Loading and Visualizing Public Fund Basic Data\nDESCRIPTION: Loads the public fund data table into DolphinDB for analysis, performs snapshot queries of the top 10 records, and loads the full table into an in-memory variable for visualization. These steps are essential for initial data exploration and validation, leveraging SQL commands and GUI-based data inspection.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/public_fund_basic_analysis.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\nfundData = loadTable(\"dfs://publicFundDB\", \"publicFundData\")\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect top 10 * from fundData\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\npublicFundData = select * from fundData\n```\n\n----------------------------------------\n\nTITLE: Defining Parallel OHLC Calculation Function in DolphinDB Script\nDESCRIPTION: Defines the `calOHLCBaseOnSnapshot` function in DolphinDB script. This function takes a start date, end date, database name, and table name as input. It creates a distributed data source (`sqlDS`) selecting necessary columns from the specified DFS table filtered by the date range, and then executes a parallel map-reduce (`mr`) job using the `calOHLCBaseOnSnapshotMapFuc` map function to calculate OHLC data efficiently.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/K.md#_snippet_3\n\nLANGUAGE: dolphindb\nCODE:\n```\ndef calOHLCBaseOnSnapshot(calStartDate, calEndDate, dbName, tbName){\n\t//Generate data source: If SQL only contains the required columns for calculation, it can improve calculation efficiency\n\tdataSource = sqlDS(<\tselect\tTradeTime, SecurityID, OpenPrice,\n\t\t\t\t\tPreCloPrice, HighPrice, LowPrice,\n\t\t\t\t\tLastPrice, PreCloseIOPV, IOPV,\n\t\t\t\t\tTotalVolumeTrade, TotalValueTrade, NumTrades,\n\t\t\t\t\tUpLimitPx, DownLimitPx\n\t\t\t\tfrom loadTable(dbName, tbName)\n\t\t\t\twhere TradeTime.date()>=calStartDate, TradeTime.date()<=calEndDate>)\n\tresult = mr(ds=dataSource, mapFunc=calOHLCBaseOnSnapshotMapFuc, finalFunc=unionAll{,false}, parallel=true)\n\treturn result\n}\n```\n\n----------------------------------------\n\nTITLE: Calculating Aggregate Value from Multiple IoT Sensors\nDESCRIPTION: This snippet calculates the sum of average values from three IoT sensors, aggregated by minute. It uses the `bar` function for minute-based aggregation, `pivot by` to rearrange data, `ffill` to fill NULLs, `avg` for average value calculation, and `rowSum` to calculate the sum of sensor values at each minute. The `interval` function handles missing minutes using the \"prev\" fill method.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_sql_cases.md#_snippet_30\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntimePeriod = 2021.01.01T00:00:00.000 : 2021.01.01T01:00:00.000\ntimer result = select sum(rowSum) as v from (\n    \t\t   select rowSum(ffill(avg(value))) from t \n    \t\t   where id in `id1`id2`id3, time between timePeriod \n    \t\t   pivot by bar(time, 60000) as minute, id) \n    \t\t   group by interval(minute, 1m, \"prev\") as minute\n```\n\n----------------------------------------\n\nTITLE: Filtering by Time Range and Aggregating Average Bid by Date using urllib3\nDESCRIPTION: This function queries Elasticsearch using `urllib3`. It filters documents based on a TIME range and then aggregates by DATE to calculate the average BID value for each date within that time window.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_67\n\nLANGUAGE: python\nCODE:\n```\nimport urllib3\nimport json\n\ndef search_7():\n    http = urllib3.PoolManager()\n    data = json.dumps({\n        \"query\": {\n            \"constant_score\": {\n                \"filter\": {\n                     \"range\": {\n                        \"TIME\": {\n                            \"gte\": \"7:45:16\",\n                            \"lte\": \"8:00:00\"\n                        }\n                    }\n                }\n            }\n        },\n        \"aggs\": {\n            \"group_by_date\": {\n                \"terms\": {\n                    \"field\": \"DATE\",\n                    \"size\": 4\n                },\n                \"aggs\": {\n                    \"avg_vol\": {\n                        \"avg\": {\"field\": \"BID\"}\n                    }\n                }\n            }\n        },\n        \"_source\": [\"\"]\n    }).encode(\"utf-8\")\n    r = http.request(\"GET\", \"http://localhost:9200/hundred/_search\", body=data,\n                     headers={'Content-Type': 'application/json'})\n    print(r.status)\n    print(json.loads(r.data.decode()))\n```\n\n----------------------------------------\n\nTITLE: Defining Feature Dictionary for Aggregation in DolphinDB\nDESCRIPTION: Constructs a feature dictionary mapping column names to lists of aggregation functions such as sum, mean, and standard deviation, as well as custom realized volatility. This dictionary drives dynamic aggregation code generation for multiple financial metrics including Wap, LogReturn, spread measures, volume, and imbalances.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/metacode_derived_features/metacode_derived_features_arrayVector.txt#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\nfeatures = {\n\t\"DateTime\":[`count]\n}\nfor( i in 0..9)\n{\n\tfeatures[\"Wap\"+i] = [`sum, `mean, `std]\n\tfeatures[\"LogReturn\"+i] = [`sum, `realizedVolatility, `mean, `std]\n\tfeatures[\"LogReturnOffer\"+i] = [`sum, `realizedVolatility, `mean, `std]\n\tfeatures[\"LogReturnBid\"+i] = [`sum, `realizedVolatility, `mean, `std]\n}\nfeatures[\"WapBalance\"] = [`sum, `mean, `std]\nfeatures[\"PriceSpread\"] = [`sum, `mean, `std]\nfeatures[\"BidSpread\"] = [`sum, `mean, `std]\nfeatures[\"OfferSpread\"] = [`sum, `mean, `std]\nfeatures[\"TotalVolume\"] = [`sum, `mean, `std]\nfeatures[\"VolumeImbalance\"] = [`sum, `mean, `std]\naggMetaCode, metaCodeColName = createAggMetaCode(features)\n```\n\n----------------------------------------\n\nTITLE: Concurrent Query Benchmark for Partitioned and Non-Partitioned Keyed Tables in DolphinDB Script\nDESCRIPTION: This code defines a query function and submits jobs to compare concurrent query performance between partitioned and non-partitioned keyed tables. It uses a client connection to submit jobs for 10 simulated clients, each querying 100,000 times and recording elapsed time, then gathers and prints the timing results. Dependencies include submitJob, evalTimer, and getJobReturn. This setup validates concurrency improvements gained via partitioning.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/in_memory_table.md#_snippet_5\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\ndef queryKeyedTable(tableName,id){\n\tfor(i in id){\n\t\tselect * from objByName(tableName) where id=i\n\t}\n}\nconn=xdb(\"192.168.1.135\",18102,\"admin\",\"123456\")\nn=5000000\n\njobid1=array(STRING,0)\nfor(i in 1..10){\n\trid=rand(1..n,100000)\n\ts=conn(submitJob,\"evalQueryUnPartitionTimer\"+string(i),\"\",evalTimer,queryKeyedTable{`skt,rid})\n\tjobid1.append!(s)\n}\ntime1=array(DOUBLE,0)\nfor(j in jobid1){\n\ttime1.append!(conn(getJobReturn,j,true))\n}\n\njobid2=array(STRING,0)\nfor(i in 1..10){\n\trid=rand(1..n,100000)\n\ts=conn(submitJob,\"evalQueryPartitionTimer\"+string(i),\"\",evalTimer,queryKeyedTable{`spkt,rid})\n\tjobid2.append!(s)\n}\ntime2=array(DOUBLE,0)\nfor(j in jobid2){\n\ttime2.append!(conn(getJobReturn,j,true))\n}\n\n```\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\ntime1\n[6719.266848,7160.349678,7271.465094,7346.452625,7371.821485,7363.87979,7357.024299,7332.747157,7298.920972,7255.876976]\n\ntime2\n[2382.154581,2456.586709,2560.380315,2577.602019,2599.724927,2611.944367,2590.131679,2587.706832,2564.305815,2498.027042]\n```\n\n----------------------------------------\n\nTITLE: Aggregating and Processing OHLC Data in DolphinDB Script\nDESCRIPTION: This snippet continues the `calOHLCBaseOnSnapshotMapFuc` function. It aggregates snapshot data from `tempTB1` into 60-second intervals, calculates OHLC, Volume, Turnover, TradesCount, and a first bar change rate. It then creates a complete time grid (`allTime`) for the trading day, joins it with distinct SecurityIDs (`tempTB3`), left-joins this grid with the aggregated data (`tempTB2`), and performs forward filling (`cumlastNot`) and conditional calculations (`iif`, `ratios`) to handle missing values and compute the final OHLC data including `ChangeRate`.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/K.md#_snippet_2\n\nLANGUAGE: dolphindb\nCODE:\n```\n\t\t\t\tlast(UpLimitPx) as UpLimitPx,\n\t\t\t\tlast(DownLimitPx) as DownLimitPx,\n\t\t\t\tlastNot(LastPrice, 0.0)\\firstNot(LastPrice, 0.0)-1 as FirstBarChangeRate\t\n\t\tfrom tempTB1\n\t\tgroup by SecurityID, TradeDate, interval(X=TradeTime, duration=60s, label='left', fill=0) as TradeTime\n\t//240 bars per day\n\tcodes = select distinct(SecurityID) as SecurityID from tempTB2 order by SecurityID\n\tallTime = table((take(0..120, 121)*60*1000+09:30:00.000).join(take(0..117, 118)*60*1000+13:00:00.000).join(15:00:00.000) as TradeTime)\n\ttempTB3 = cj(codes, allTime)\n\t//Processing missing data calculation window, excluding opening\n\tresult = select\tSecurityID,\n\t\t\tconcatDateTime(TradeDate, TradeTime) as TradeTime,\n\t\t\tiif(OpenPrice==0.0 and PreClosePrice==0.0, cumlastNot(ClosePrice, 0.0), OpenPrice) as OpenPrice,\n\t\t\tiif(HighPrice==0.0 and PreClosePrice==0.0, cumlastNot(ClosePrice, 0.0), HighPrice) as HighPrice,\n\t\t\tiif(LowPrice==0.0 and PreClosePrice==0.0, cumlastNot(ClosePrice, 0.0), LowPrice) as LowPrice,\n\t\t\tiif(ClosePrice==0.0 and PreClosePrice==0.0, cumlastNot(ClosePrice, 0.0), ClosePrice) as ClosePrice,\n\t\t\tVolume,\n\t\t\tTurnover,\n\t\t\tTradesCount,\n\t\t\tiif(PreClosePrice==0.0, cumlastNot(PreClosePrice, 0.0), PreClosePrice) as PreClosePrice,\n\t\t\tiif(PreCloseIOPV==0.0 and PreClosePrice==0.0, cumlastNot(PreCloseIOPV, 0.0), PreCloseIOPV).nullFill(0.0) as PreCloseIOPV,\n\t\t\tiif(IOPV==0.0 and PreCloseIOPV==0.0, cumlastNot(IOPV, 0.0), IOPV).nullFill(0.0) as IOPV,\n\t\t\tiif(UpLimitPx==0.0, cumlastNot(UpLimitPx, 0.0), UpLimitPx).nullFill(0.0) as UpLimitPx,\n\t\t\tiif(DownLimitPx==0.0, cumlastNot(DownLimitPx, 0.0), DownLimitPx).nullFill(0.0) as DownLimitPx,\n\t\t\tiif(\ttime(TradeTime)==09:30:00.000,\n\t\t\t\tiif(FirstBarChangeRate!=NULL, FirstBarChangeRate, 0.0),\n\t\t\t\tiif(ratios(ClosePrice)!=NULL and ClosePrice!=0.0, ratios(ClosePrice)-1, 0.0)) as ChangeRate\n\t\tfrom lj(tempTB3, tempTB2, `TradeTime`SecurityID)\n\t\tcontext by SecurityID\n\treturn result\n}\n```\n\n----------------------------------------\n\nTITLE: Creating X-Bond Quote Table with TSDB Engine in DolphinDB\nDESCRIPTION: This snippet defines a DolphinDB database and table for storing X-Bond quote data using the TSDB engine optimized for time-series data. The database is partitioned daily by a range covering one year. The table schema accommodates multiple array and scalar fields representing bond market depth, price arrays, yield arrays, quote types, and metadata. The table is partitioned by createDate and sorted by securityID and createTime for efficient querying on bond code and time.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practices_for_partitioned_storage.md#_snippet_15\n\nLANGUAGE: DolphinDB\nCODE:\n```\ncreate database \"dfs://XBond\"\npartitioned by VALUE(2023.01.01..2023.12.31)\nengine='TSDB'\n\ncreate table \"dfs://XBond\".\"XBondDepthTable\"(\n    bondCodeVal SYMBOL\n    createDate DATE[comment=\"创建日期\", compress=\"delta\"]  \n    createTime TIME[comment=\"创建时间\", compress=\"delta\"]\n    marketDepth LONG\n    mdBookType LONG\n    messageId LONG\n    messageSource STRING\n    msgSeqNum LONG\n    msgType STRING\n    bidmdEntryPrice DOUBLE[]\n    offermdEntryPrice DOUBLE[]\n    bidmdEntrySize LONG[]\n    offermdEntrySize LONG[]\n    bidsettlType LONG[]\n    offersettlType LONG[]\n    bidyield DOUBLE[]\n    offeryield DOUBLE[]\n    bid1yieldType STRING\n    offer1yieldType STRING\n    bid2yieldType STRING\n    offer2yieldType STRING\n    bid3yieldType STRING\n    offer3yieldType STRING\n    bid4yieldType STRING\n    offer4yieldType STRING\n    bid5yieldType STRING\n    offer5yieldType STRING\n    bid6yieldType STRING\n    offer6yieldType STRING\n    securityID SYMBOL\n    senderCompID STRING\n    senderSubID STRING\n    sendingTime TIMESTAMP\n)\npartitioned by createDate,\nsortColumns=[`securityID, `createTime]\n```\n\n----------------------------------------\n\nTITLE: Performance Testing: Range Query for Multiple Symbols and Time Range\nDESCRIPTION: Retrieves data for multiple symbols within a date and time interval, filtering on bid prices, and measuring query time.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs InfluxDB/test_dolphindb_big.txt#_snippet_10\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 2. 范围查询：查询某时间段内的某些股票的所有记录\n\ntimer\nselect symbol, time, bid, ofr \nfrom taq \nwhere \n\tsymbol in ('IBM', 'MSFT', 'GOOG', 'YHOO'),\n\tdate between 2007.08.03 : 2007.08.07,\n\ttime between 09:30:00 : 09:40:00,\n\tbid > 20\n```\n\n----------------------------------------\n\nTITLE: Defining Output Stream Table for Session Window Engine in DolphinDB\nDESCRIPTION: Defines a stream table named `outputSt2` to store alert information for sensor data loss detected by the session window engine. It includes a timestamp `ts`, the tag `tag`, and the last known value `lastValue` before the data loss.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_streaming_application_in_IOT.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\nout2 =streamTable(10000:0,`ts`tag`lastValue,[TIMESTAMP,SYMBOL, INT])\nenableTableShareAndPersistence(table=out2,tableName=`outputSt2,asynWrite=false,compress=true, cacheSize=100000)\n```\n\n----------------------------------------\n\nTITLE: Querying data from DolphinDB database using SQL\nDESCRIPTION: DolphinDB SQL queries to retrieve one day of data from Level 1 and Level 2 datasets, with time measurement using the timer function.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_pickle_comparison.md#_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\n＃读取Level 1数据集一天的数据\ntimer t1 = select * from loadTable(\"dfs://TAQ\", \"quotes\") where TradeDate = 2007.08.23\n\n＃读取Level 2数据集一天的数据\ntimer t2 = select * from loadTable(\"dfs://DataYesDB\", \"tick\") where TradeDate = 2019.09.10\n```\n\n----------------------------------------\n\nTITLE: Parallel Yearly Loading of Order Book Data from Directory Structure in DolphinDB\nDESCRIPTION: Defines loopLoadOneYearFiles that scans a parent directory for subdirectories (typically corresponding to daily folders in a year), and submits asynchronous jobs keyed by subdirectory name to load each day’s data files with loadOneDayFiles. This enables parallelized ingestion of large volumes of historical stock order book data by distributing workload over multiple tasks.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/csvImportDemo/importOldData.txt#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef loopLoadOneYearFiles(dbName,tableName, filePath,schema1){\n\tdirs = exec filename from files(filePath) where isDir=true\n\tfor (path in dirs){\n\t\tsubmitJob(\"old\"+path,\"loadOrderDir\"+path,loadOneDayFiles{dbName,tableName,filePath+\"/\"+path,schema1})\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: Matrix Determinant Calculation - DolphinDB\nDESCRIPTION: This code demonstrates how to calculate the determinant of a matrix in DolphinDB using the `det()` method. If the matrix contains NULL values, they are replaced with 0 during the calculation. The determinant is a scalar value that can be computed from the elements of a square matrix.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/matrix.md#_snippet_10\n\nLANGUAGE: DolphinDB\nCODE:\n```\n>x=1..4$2:2;\n>x;\n#0 #1\n-- --\n1  3\n2  4\n>X.det();\n-2\n\n>x=1 2 3 6 5 4 8 7 NULL $3:3;\n>x;\n#0 #1 #2\n-- -- --\n1  6  8\n2  5  7\n3  4\n>det x;\n42\n\n>x=1 2 3 6 5 4 3 6 9$3:3;\n>x;\n#0 #1 #2\n-- -- --\n1  6  3\n2  5  6\n3  4  9\n>det x;\n0\n```\n\n----------------------------------------\n\nTITLE: Connecting to NSQ and Retrieving Schemas (DolphinDB Script)\nDESCRIPTION: Loads the NSQ plugin (if not already loaded), connects to the NSQ server using the specified configuration file (`nsq_sdk_config.ini`), and retrieves the predefined table schemas for `orders`, `trade`, and `snapshot` data types using `nsq::getSchema`. These schemas are used later for table creation.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_implementation_for_NSQ_Plugin.md#_snippet_5\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\n//加载插件\ntry{loadPlugin(\"/DolphinDB/server/plugins/nsq/PluginNsq.txt\")} catch(ex){print(ex)}\n//连接行情服务器\nnsq::connect(\"/DolphinDB/server/plugins/nsq/nsq_sdk_config.ini\")\n//获取行情数据的表结构\nordersSchema = nsq::getSchema(`orders)\ntradeSchema = nsq::getSchema(`trade)\nsnapshotSchema = nsq::getSchema(`snapshot)\n```\n\n----------------------------------------\n\nTITLE: Importing Multiple CSV Files to DolphinDB Memory Table Using loop High-Order Function\nDESCRIPTION: Loads multiple CSV files from a specified directory and unions them into a single DolphinDB table in memory. The 'loop' high-order function iterates over file paths by concatenating directory and file names, applying 'loadText' on each file, and 'unionAll(false)' combines the loaded tables without removing duplicate rows. Requires valid 'fileDir' string path and DolphinDB access to files. Useful for bulk data import scenarios where files share identical schema.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/func_progr_cases.md#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\nloop(loadText, fileDir + \"/\" + files(fileDir).filename).unionAll(false)\n```\n\n----------------------------------------\n\nTITLE: Inserting Sample Data and Querying Results in DolphinDB\nDESCRIPTION: This DolphinDB script inserts sample trade data into the stream processing engine and then queries the result table to observe the calculated factors. This demonstrates how to populate the engine with data and retrieve the processed output.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practice_for_factor_calculation.md#_snippet_13\n\nLANGUAGE: DolphinDB\nCODE:\n```\nresult = createResultTable()\nrse = createStreamEngine(result)\ninsert into rse values(`000155, 1000, 1001, 2020.01.01T09:30:00, 20000)\ninsert into rse values(`000155, 1000, 1002, 2020.01.01T09:30:01, 40000)\ninsert into rse values(`000155, 1000, 1003, 2020.01.01T09:30:02, 60000)\ninsert into rse values(`000155, 1004, 1003, 2020.01.01T09:30:03, 30000)\n\nselect * from result\n```\n\n----------------------------------------\n\nTITLE: Calling Local Function/Function View Using Root Namespace\nDESCRIPTION: Demonstrates how to call the custom function `myfunc` that was defined at the root level (outside any module or loaded from initialization script) or registered as a function view. Using the root namespace prefix (`::`) ensures that the interpreter calls the function defined in the global scope, distinguishing it from functions with the same name in imported modules.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/module_tutorial.md#_snippet_17\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\n::myfunc()\n```\n\n----------------------------------------\n\nTITLE: 使用orca库查询DolphinDB数据\nDESCRIPTION: 使用dolphindb.orca库(pandas接口的兼容实现)从DolphinDB查询数据，支持类似pandas的语法进行数据过滤和计算。orca提供了pandas熟悉的接口同时利用DolphinDB的高性能计算。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/quant_finance_examples.md#_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\nimport dolphindb.orca as orca\norca.connect(\"127.0.0.1\", 8848, \"admin\", \"123456\")\nodf = orca.read_table(\"dfs://level2API\", \"quotes\")\nt = odf[(odf[\"bidVolume1\"]>5*odf[\"askVolume1\"]) & (odf[\"askVolume1\"]>1000000)].compute()\nprint(t)\n```\n\n----------------------------------------\n\nTITLE: 创建分布式数据库和维度表\nDESCRIPTION: 创建分布式数据库dfs://fund_OLAP并定义两个维度表fund_OLAP和fund_hs_OLAP，用于存储基金净值和指数数据。使用COMPO分区方案，基于日期和符号进行分区。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/fund_factor_contrasted_by_py.md#_snippet_22\n\nLANGUAGE: dolphindb\nCODE:\n```\n//创建数据库\ndbName = \"dfs://fund_OLAP\"\ndataDate = database(, VALUE, 2021.01.01..2021.12.31)\nsymbol = database(, HASH, [SYMBOL, 20])\nif(existsDatabase(dbName)){\n\tdropDatabase(dbName)\n}\ndb = database(dbName, COMPO, [dataDate, symbol])\n//定义表结构\nname = `tradingdate`fundNum`value\ntype = `DATE`SYMBOL`DOUBLE\ntbTemp = table(1:0, name, type)\n//创建维度表 fund_OLAP\ntbName1 = \"fund_OLAP\"\ndb.createTable(tbTemp, tbName1)\nloadTable(dbName, tbName1).append!(result)\n//创建维度表 fund_hs_OLAP\ntbName2 = \"fund_hs_OLAP\"\ndb.createTable(tbTemp, tbName2)\nloadTable(dbName, tbName2).append!(result1)\n```\n\n----------------------------------------\n\nTITLE: Solving QCLP with Gurobi Plugin in DolphinDB\nDESCRIPTION: This code snippet demonstrates solving an optimization problem using the Gurobi plugin within DolphinDB. It defines the parameters for the optimization, sets up the Gurobi model, and adds constraints using `gurobi::addConstr`. The objective function is set up using `gurobi::setObjective`. The model is then optimized using `gurobi::optimize`, and the results are retrieved using `gurobi::getResult` and `gurobi::getObjective`. Prerequisites include the Gurobi plugin and Gurobi license.  The solution involves creating variables, defining the objective, defining linear and quadratic constraints, setting the objective function, and then running the optimization. The expected output is the optimal weights and the objective value (maximum return).\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/MVO.md#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\nr = 0.18 0.25 0.36\nV = 0.0225 -0.003 -0.01125 -0.003 0.04 0.025 -0.01125 0.025 0.0625 $ 3:3\nk = pow(0.11, 2)\nA = (eye(3) join (-1 * eye(3)))\nb = 0.5 0.5 0.5 -0.1 -0.1 -0.1\n\n// 初始化模型及变量\nmodel = gurobi::model()\nvarNames = \"v\" + string(1..3)\nlb = 0 0 0\nub = 1 1 1\nvars = gurobi::addVars(model, lb, ub,,, varNames)\n\n//增加线性约束\nfor (i in 0:3) {\n    lhsExpr = gurobi::linExpr(model, A[i], vars)\n    gurobi::addConstr(model, lhsExpr, '<', b[i])\n}\nlhsExpr = gurobi::linExpr(model, take(1, 3), vars)\ngurobi::addConstr(model, lhsExpr, '=', 1)\n\n// 增加二次约束\nquadExp = gurobi::quadExpr(model, V, vars)\ngurobi::addConstr(model, quadExp, '<', k)\n\n//设置目标值并执行优化\nobj = gurobi::linExpr(model, r, vars)\ngurobi::setObjective(model, obj, 1)\ntimer gurobi::optimize(model)\n\n// 获取优化结果\nresult = gurobi::getResult(model) \n// [0.50, 0.3808, 0.119158]\nobj = gurobi::getObjective(model)\n// 0.228107\n```\n\n----------------------------------------\n\nTITLE: Creating Partitioned Table for Minute-Level K-Line Data in DolphinDB\nDESCRIPTION: Creates a distributed database table for storing minute-level K-line (candlestick) data. The table uses daily partitioning and includes columns for open, high, low, close prices, volume, amount, and volume-weighted average price (Vwap). It's optimized for time-series queries by security ID and trade time.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Best_Practices_for_DolphinDB_and_Python_AirFlow.md#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\nmodule Factor::createFactorOneMinute\n\n//创建分钟 k 线因子储存表\ndef createFactorOneMinute(dbName, tbName){\n\tif(existsDatabase(dbName)){\n\t\tdropDatabase(dbName)\n\t}\n\t//按天分区\n\tdb = database(dbName, VALUE, 2021.01.01..2021.01.03,engine = `TSDB)\n\tcolName = `TradeDate`TradeTime`SecurityID`Open`High`Low`Close`Volume`Amount`Vwap\n\tcolType =[DATE, MINUTE, SYMBOL, DOUBLE, DOUBLE, DOUBLE, DOUBLE, LONG, DOUBLE, DOUBLE]\n\ttbSchema = table(1:0, colName, colType)\n  \tdb.createPartitionedTable(table=tbSchema,tableName=tbName,partitionColumns=`TradeDate,sortColumns=`SecurityID`TradeTime,keepDuplicates=ALL)\n}\n```\n\n----------------------------------------\n\nTITLE: Calculating Option Charm (Greeks) with JIT in DolphinDB Script\nDESCRIPTION: Provides JIT-compiled functions (`myMax`, `NormDist`, `ND`, `CalculateCharm`) to compute the option charm (delta decay), a financial Greek. A `test_jit` function demonstrates applying this complex calculation iteratively using JIT. Includes validation functions (`ND_validate`, `NormDist_validate`), a vectorized implementation (`CalculateCharm_vectorized`) for comparison, test data generation, and performance timings showing JIT outperforming both non-JIT and vectorized versions in this case.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/jit.md#_snippet_1\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\n@jit\ndef myMax(a,b){\n\tif(a>b){\n\t\treturn a\n\t}else{\n\t\treturn b\n\t}\n}\n\n@jit\ndef NormDist(x) {\n  return cdfNormal(0, 1, x);\n}\n\n@jit\ndef ND(x) {\n  return (1.0/sqrt(2*pi)) * exp(-(x*x)/2.0)\n}\n\n@jit\ndef CalculateCharm(future_price, strike_price, input_ttm, risk_rate, b_rate, vol, multi, is_call) {\n  day_year = 245.0;\n\n  d1 = (log(future_price/strike_price) + (b_rate + (vol*vol)/2.0) * input_ttm) / (myMax(vol,0.00001) * sqrt(input_ttm));\n  d2 = d1 - vol * sqrt(input_ttm);\n\n  if (is_call) {\n    return -exp((b_rate - risk_rate) * input_ttm) * (ND(d1) * (b_rate/vol/sqrt(input_ttm) - d2/2.0/input_ttm) + (b_rate-risk_rate) * NormDist(d1)) * future_price * multi / day_year;\n  } else {\n    return -exp((b_rate - risk_rate) * input_ttm) * (ND(d1) * (b_rate/vol/sqrt(input_ttm) - d2/2.0/input_ttm) - (b_rate-risk_rate) * NormDist(-d1)) * future_price * multi / day_year;\n  }\n}\n\n@jit\ndef test_jit(future_price, strike_price, input_ttm, risk_rate, b_rate, vol, multi, is_call) {\n\tn = size(future_price)\n\tret = array(DOUBLE, n, n)\n\ti = 0\n\tdo {\n\t\tret[i] = CalculateCharm(future_price[i], strike_price[i], input_ttm[i], risk_rate[i], b_rate[i], vol[i], multi[i], is_call[i])\n\t\ti += 1\n\t} while(i < n)\n\treturn ret\n}\n\ndef ND_validate(x) {\n  return (1.0/sqrt(2*pi)) * exp(-(x*x)/2.0)\n}\n\ndef NormDist_validate(x) {\n  return cdfNormal(0, 1, x);\n}\n\ndef CalculateCharm_vectorized(future_price, strike_price, input_ttm, risk_rate, b_rate, vol, multi, is_call) {\n\tday_year = 245.0;\n\n\td1 = (log(future_price/strike_price) + (b_rate + pow(vol, 2)/2.0) * input_ttm) / (max(vol, 0.00001) * sqrt(input_ttm));\n\td2 = d1 - vol * sqrt(input_ttm);\n\treturn iif(is_call,-exp((b_rate - risk_rate) * input_ttm) * (ND_validate(d1) * (b_rate/vol/sqrt(input_ttm) - d2/2.0/input_ttm) + (b_rate-risk_rate) * NormDist_validate(d1)) * future_price * multi / day_year,-exp((b_rate - risk_rate) * input_ttm) * (ND_validate(d1) * (b_rate/vol/sqrt(input_ttm) - d2/2.0/input_ttm) - (b_rate-risk_rate) * NormDist_validate(-d1)) * future_price * multi / day_year)\n}\n\nn = 1000000\nfuture_price=rand(10.0,n)\nstrike_price=rand(10.0,n)\nstrike=rand(10.0,n)\ninput_ttm=rand(10.0,n)\nrisk_rate=rand(10.0,n)\nb_rate=rand(10.0,n)\nvol=rand(10.0,n)\ninput_vol=rand(10.0,n)\nmulti=rand(10.0,n)\nis_call=rand(true false,n)\nttm=rand(10.0,n)\noption_price=rand(10.0,n)\n\ntimer(10) test_jit(future_price, strike_price, input_ttm, risk_rate, b_rate, vol, multi, is_call)                     //   1834 ms\ntimer(10) test_none_jit(future_price, strike_price, input_ttm, risk_rate, b_rate, vol, multi, is_call)                // 224099 ms\ntimer(10) CalculateCharm_vectorized(future_price, strike_price, input_ttm, risk_rate, b_rate, vol, multi, is_call)    //   3118 ms\n```\n\n----------------------------------------\n\nTITLE: Loading DolphinDB Plugins for Seismic Data Processing - DolphinDB Script\nDESCRIPTION: This snippet demonstrates how to load various DolphinDB plugins (FilterPicker, RTSeis, TensorFlow) required for seismic data anomaly detection. It shows shell commands to set environment variables for shared library paths on Linux and DolphinDB scripting commands using loadPlugin() within try-catch blocks to safely load each plugin. These plugins enable template matching, signal filtering, and TensorFlow-based machine learning within DolphinDB.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Earthquake_Prediction_with_DolphinDB_and_Machine_Learning.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\nexport LD_LIBRARY_PATH=<YOUR DolphinDB Dir>/server/plugins/rtseis/:$LD_LIBRARY_PATH\nexport LD_LIBRARY_PATH=<YOUR DolphinDB Dir>/server/plugins/tf/:$LD_LIBRARY_PATH\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntry{ loadPlugin(\"./plugins/filterpicker/PluginFilterPicker.txt\") }catch(ex){print(ex) }\ntry{ loadPlugin(\"./plugins/rtseis/PluginRTSeis.txt\") }catch(ex){print(ex) }\ntry{ loadPlugin(\"./plugins/tf/PluginTf.txt\") }catch(ex){print(ex) }\n```\n\n----------------------------------------\n\nTITLE: Creating a weekday K-line calculation job in DolphinDB\nDESCRIPTION: Example showing how to schedule a job to compute minute-level K-lines every weekday at 3 PM, processing market data from a distributed database and storing results.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/scheduledJob.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef computeK(){\n\tbarMinutes = 7\n\tsessionsStart=09:30:00.000 13:00:00.000\n\tOHLC =  select first(price) as open, max(price) as high, min(price) as low,last(price) as close, sum(volume) as volume \n\t\tfrom loadTable(\"dfs://stock\",\"trades\")\n\t\twhere time > today() and time < now()\n\t\tgroup by symbol, dailyAlignedBar(timestamp, sessionsStart, barMinutes*60*1000) as barStart\n\tappend!(loadTable(\"dfs://stock\",\"OHLC\"),OHLC)\n}\nscheduleJob(`kJob, \"7 Minutes\", computeK, 15:00m, 2020.01.01, 2021.12.31, 'W', [1,2,3,4,5]);\n```\n\n----------------------------------------\n\nTITLE: 查询某设备一天的详细数据\nDESCRIPTION: 此脚本筛选出设备全天的完整采集数据并按时间排序，方便查看设备全貌，查询平均耗时约2毫秒，首次約16毫秒，适合日常数据回溯。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/iot_query_case.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nloadTable(database(\"dfs://NoiseDB\"),'noise')\nselect *\nfrom noise\nwhere date=2022.01.01 and tenantId=1055 and deviceId=10067\norder by ts\n```\n\n----------------------------------------\n\nTITLE: Fetching Latest Data and Saving to Redis in DolphinDB\nDESCRIPTION: Defines a job function that connects to Redis, authenticates, loads the latest per-ID time-series data from a DolphinDB partitioned table, and saves this data as Redis hashes using the previously defined saveHashDataToRedis function. Finally, it releases the Redis connection. It relies on the Redis plugin and an existing DolphinDB database 'dfs://signalData' with a 'data' table partitioned by ID and sorted by time.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/redis_plugin_tutorial/example3-kafka.txt#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef redisjob(){\n    conn=redis::connect(192.168.0.75, 6379)\n    redis::run(conn, \"AUTH\", \"password\")\n    go\n    // 获取每个 id 对应的最新时间的行情数据\n    signalData = select * from loadTable(\"dfs://signalData\", \"data\") context by id order by time limit -1\n    saveHashDataToRedis(conn, signalData)\n    redis::release(conn)\n}\n```\n\n----------------------------------------\n\nTITLE: Calculating Annual Kurtosis in Python\nDESCRIPTION: Defines a Python function `getAnnualKur` using NumPy and SciPy.stats (imported as `st`). It computes daily returns from the input `value` array and then calculates their kurtosis using `st.kurtosis` with `fisher=False` (Pearson's definition). Requires NumPy and SciPy libraries.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/fund_factor_contrasted_by_py.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport scipy.stats as st\n\ndef getAnnualKur(value):\n    diff_value = np.diff(value)\n    rolling_value = np.roll(value, 1)\n    rolling_value = np.delete(rolling_value, [0])\n    return st.kurtosis(np.true_divide(diff_value, rolling_value), fisher=False)\n```\n\n----------------------------------------\n\nTITLE: Inserting Data into a Shared Stream Table - DolphinDB\nDESCRIPTION: This example demonstrates inserting data into a shared stream table after it has been created. It shows that initially, all users have write access. Requires admin privileges to create user.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Permission_Management.md#_snippet_28\n\nLANGUAGE: DolphinDB\nCODE:\n```\ncreateUser(\"CliffLee\",\"JI3564^\",,false)\nlogin(\"CliffLee\",\"JI3564^\")\ninsert into trades values(1970.01.01T00:00:00.001,`sym,1)\nselect * from trades\n```\n\n----------------------------------------\n\nTITLE: Switching to the MQTT Namespace\nDESCRIPTION: This snippet switches the current namespace to `mqtt`. This is necessary to access functions and variables defined within the MQTT plugin.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/data/DolphinDB_streaming_application_in_IOT/deviceState.txt#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\nuse mqtt;\n```\n\n----------------------------------------\n\nTITLE: Loading AMD Plugin in DolphinDB Script\nDESCRIPTION: This snippet demonstrates how to programmatically load the AMD plugin within a DolphinDB script using the loadPlugin function. It handles potential errors from duplicate loading by enclosing the call in a try-catch block. The plugin path supports relative paths within the installation directory. This method avoids redundant loading if the plugin is preloaded via configuration.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_implementation_for_AMD_Plugin.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 加载插件\ntry{loadPlugin(\"plugins/amdquote/PluginAmdQuote.txt\")} catch(ex){print(ex)}\n```\n\n----------------------------------------\n\nTITLE: Creating Partitioned Historical Data Table in DolphinDB Script\nDESCRIPTION: Defines a DolphinDB function to create a partitioned database and table intended to store historical stock snapshot data. The function sets up a composite database partitioned by date and symbol hash, uses TSDB storage engine, and creates a table schema with various columns representing stock market snapshot attributes. It applies delta compression on timestamp fields and configures sorting and duplicate handling to optimize query and storage efficiency for large datasets. The snippet includes execution of the function to create the database and table named \"dfs://snapshot\" and \"snapshot\" respectively.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Real-Time_Stock_Price_Increase_Calculation.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef createDfsTb(dbName, tbName){\n\t//create database\n\tif(existsDatabase(dbName)){\n\t\tdropDatabase(dbName)\n\t}\n\tdb1 = database(, VALUE, 2020.01.01..2021.01.01)\n\tdb2 = database(, HASH, [SYMBOL, 30])\n\tdb = database(dbName, COMPO, [db1, db2], , \"TSDB\")\n\t//create table\n\tschemaTable = table(\n\t\tarray(SYMBOL, 0) as SecurityID,\n\t\tarray(TIMESTAMP, 0) as DateTime,\n\t\tarray(DOUBLE, 0) as PreClosePx,\n\t\tarray(DOUBLE, 0) as OpenPx,\n\t\tarray(DOUBLE, 0) as HighPx,\n\t\tarray(DOUBLE, 0) as LowPx,\n\t\tarray(DOUBLE, 0) as LastPx,\n\t\tarray(INT, 0) as TotalVolumeTrade,\n\t\tarray(DOUBLE, 0) as TotalValueTrade,\n\t\tarray(SYMBOL, 0) as InstrumentStatus,\n\t\tarray(DOUBLE[], 0) as BidPrice,\n\t\tarray(INT[], 0) as BidOrderQty,\n\t\tarray(INT[], 0) as BidNumOrders,\n\t\tarray(INT[], 0) as BidOrders,\n\t\tarray(DOUBLE[], 0) as OfferPrice,\n\t\tarray(INT[], 0) as OfferOrderQty,\n\t\tarray(INT[], 0) as OfferNumOrders,\n\t\tarray(INT[], 0) as OfferOrders,\n\t\tarray(INT, 0) as NumTrades,\n\t\tarray(DOUBLE, 0) as IOPV,\n\t\tarray(INT, 0) as TotalBidQty,\n\t\tarray(INT, 0) as TotalOfferQty,\n\t\tarray(DOUBLE, 0) as WeightedAvgBidPx,\n\t\tarray(DOUBLE, 0) as WeightedAvgOfferPx,\n\t\tarray(INT, 0) as TotalBidNumber,\n\t\tarray(INT, 0) as TotalOfferNumber,\n\t\tarray(INT, 0) as BidTradeMaxDuration,\n\t\tarray(INT, 0) as OfferTradeMaxDuration,\n\t\tarray(INT, 0) as NumBidOrders,\n\t\tarray(INT, 0) as NumOfferOrders,\n\t\tarray(INT, 0) as WithdrawBuyNumber,\n\t\tarray(INT, 0) as WithdrawBuyAmount,\n\t\tarray(DOUBLE, 0) as WithdrawBuyMoney,\n\t\tarray(INT, 0) as WithdrawSellNumber,\n\t\tarray(INT, 0) as WithdrawSellAmount,\n\t\tarray(DOUBLE, 0) as WithdrawSellMoney,\n\t\tarray(INT, 0) as ETFBuyNumber,\n\t\tarray(INT, 0) as ETFBuyAmount,\n\t\tarray(DOUBLE, 0) as ETFBuyMoney,\n\t\tarray(INT, 0) as ETFSellNumber,\n\t\tarray(INT, 0) as ETFSellAmount,\n\t\tarray(DOUBLE, 0) as ETFSellMoney\n\t)\n\tdb.createPartitionedTable(table=schemaTable, tableName=tbName, partitionColumns=`DateTime`SecurityID, compressMethods={DateTime:\"delta\"}, sortColumns=`SecurityID`DateTime, keepDuplicates=ALL)\n}\n\ndbName, tbName = \"dfs://snapshot\", \"snapshot\"\ncreateDfsTb(dbName, tbName)\n```\n\n----------------------------------------\n\nTITLE: DolphinDB pivot by example with HINT_EXPLAIN\nDESCRIPTION: This example shows how to use `pivot by` in DolphinDB with `HINT_EXPLAIN` to analyze the execution plan. The query calculates the row sum of the forward-filled last value of `preClose` from the `quotes` table in the distributed file system (DFS) `dfs://stocks`, pivoting by time and symbol.  The execution plan highlights the cost of the `pivotBy` operation.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_Explain.md#_snippet_16\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect [HINT_EXPLAIN] rowSum(ffill(last(preClose))) from loadTable(\"dfs://stocks\", `quotes) pivot by time, symbol\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Feature Engineering Results in DolphinDB\nDESCRIPTION: This code snippet processes the result table from prior feature engineering. It removes invalid entries, converts the SecurityID column to integer, separates the prediction label, and drops unnecessary columns. Dependencies: A feature-engineered variable 'result' must be present. Inputs: Table 'result' with required columns. Outputs: Prepared input table and target vector for model training.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/metacode_derived_features/metacode_derived_features_buildmodel.txt#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\nresult = result[each(isValid, result.values()).rowAnd()]\nresult_input = copy(result)\nlabel = result[`LogReturn0_realizedVolatility]\nresult_input.update!(`SecurityID_int, int(result[`SecurityID]))\nresult_input.dropColumns!(`SecurityID`DateTime`LogReturn0_realizedVolatility)\n```\n\n----------------------------------------\n\nTITLE: Define Press Factor (Non-Incremental Calculation)\nDESCRIPTION: Defines a function named `Press` that corresponds to the buying and selling pressure indicator. It takes ten levels of bid/ask price and quantity data (`BidPrice`, `BidOrderQty`, `OfferPrice`, `OfferOrderQty`) as input. The expression for this factor is complex and cannot be directly represented using DolphinDB's built-in aggregate operators, so it requires using the `defg` function to declare a custom aggregate calculation function. The platform automatically parses the aggregate function `Press` into the meta code format <Press()> and passes it to the time series engine.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Level2_Snapshot_Factor_Calculation.md#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndefg Press(BidPrice0,BidPrice1,BidPrice2,BidPrice3,BidPrice4,BidPrice5,BidPrice6,BidPrice7,BidPrice8,BidPrice9,BidOrderQty0,BidOrderQty1,BidOrderQty2,BidOrderQty3,BidOrderQty4,BidOrderQty5,BidOrderQty6,BidOrderQty7,BidOrderQty8,BidOrderQty9,OfferPrice0,OfferPrice1,OfferPrice2,OfferPrice3,OfferPrice4,OfferPrice5,OfferPrice6,OfferPrice7,OfferPrice8,OfferPrice9,OfferOrderQty0,OfferOrderQty1,OfferOrderQty2,OfferOrderQty3,OfferOrderQty4,OfferOrderQty5,OfferOrderQty6,OfferOrderQty7,OfferOrderQty8,OfferOrderQty9){\n\tbidPrice = matrix(BidPrice0,BidPrice1,BidPrice2,BidPrice3,BidPrice4,BidPrice5,BidPrice6,BidPrice7,BidPrice8,BidPrice9)\n\tbidQty = matrix(BidOrderQty0,BidOrderQty1,BidOrderQty2,BidOrderQty3,BidOrderQty4,BidOrderQty5,BidOrderQty6,BidOrderQty7,BidOrderQty8,BidOrderQty9)\n\tofferPrice = matrix(OfferPrice0,OfferPrice1,OfferPrice2,OfferPrice3,OfferPrice4,OfferPrice5,OfferPrice6,OfferPrice7,OfferPrice8,OfferPrice9)\n\tofferQty = matrix(OfferOrderQty0,OfferOrderQty1,OfferOrderQty2,OfferOrderQty3,OfferOrderQty4,OfferOrderQty5,OfferOrderQty6,OfferOrderQty7,OfferOrderQty8,OfferOrderQty9)\n\twap = (bidPrice[0]*offerQty[0] + offerPrice[0]*bidQty[0])\\(bidQty[0]+offerQty[0])\n\tbidw=(1.0\\(bidPrice-wap))\n\tbidw=bidw\\(bidw.rowSum())\n\tofferw=(1.0\\(offerPrice-wap))\n\tofferw=offerw\\(offerw.rowSum())\n\tpress = log((bidQty*bidw).rowSum())-log((offerQty*offerw).rowSum())\n\treturn avg(press)\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing DDBDataLoader in Python\nDESCRIPTION: This code snippet initializes a DDBDataLoader instance to load data from a DolphinDB distributed table using a SQL query. It configures parameters such as batch size, window size, shuffle, repartitioning, and device to optimize data loading and training performance. It depends on the `dolphindb` and `dolphindb_tools` libraries.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_AI_DataLoader_for_Deep_Learning.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\nfrom dolphindb_tools.dataloader import DDBDataLoader\n\nsess = ddb.Session()\nsess.connect('localhost', 8848, \"admin\", \"123456\")\n\ndbPath = \"dfs://test_ai_dataloader\"\ntbName = \"wide_factor_table\"\n\nsymbols = [\"`\" + f\"{i}.SH\".zfill(9) for i in range(1, 251)]\ntimes = [\"2020.01.\" + f\"{i+1}\".zfill(2) for i in range(31)]\n\nsql = f\"\"\"select * from loadTable(\"{dbPath}\", \"{tbName}\")\"\"\"\n\ndataloader = DDBDataLoader(\n    s, sql, targetCol=[\"f000001\"], batchSize=64, shuffle=True,\n    windowSize=[200, 1], windowStride=[1, 1],\n    repartitionCol=\"date(DateTime)\", repartitionScheme=times,\n    groupCol=\"Symbol\", groupScheme=symbols,\n    seed=0,\n    offset=200, excludeCol=[\"DateTime\", \"Symbol\"], device=\"cuda\",\n    prefetchBatch=5, prepartitionNum=3\n)\n```\n\n----------------------------------------\n\nTITLE: Extracting and Modifying CSV Schema in DolphinDB\nDESCRIPTION: Defines a function `getSchema` that takes a CSV file path as input. It extracts the text schema using `extractTextSchema` and then renames the 'TotalVolume' column to 'Volume' and 'TotalAmount' to 'Amount'. It returns the modified schema table, which is used for subsequent data loading.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/csvImportDemo/importNewData.txt#_snippet_0\n\nLANGUAGE: dolphindb\nCODE:\n```\ndef getSchema(csv){\n\tschema1=extractTextSchema(csv)\n\tupdate schema1 set name=`Volume where name=\"TotalVolume\"\n\tupdate schema1 set name=`Amount where name=\"TotalAmount\"\t\n\t\n\treturn schema1\n}\n```\n\n----------------------------------------\n\nTITLE: Replaying Historical Stock Market Data in DolphinDB\nDESCRIPTION: This snippet replays historical trade and snapshot data for a specific date, repartitioned into specified time intervals. It submits a job for processing the data streams, which is useful for backtesting or data analysis.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/stock_market_replay/03.calTradeCost_lookUpJoin.txt#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef replayStockMarketData(){\n\ttimeRS = cutPoints(09:15:00.000..15:00:00.000, 100)\n\ttradeDS = replayDS(sqlObj=<select * from loadTable(\"dfs://trade\", \"trade\") where Date = 2020.12.31>, dateColumn=`Date, timeColumn=`Time, timeRepartitionSchema=timeRS)\n\tsnapshotDS = replayDS(sqlObj=<select * from loadTable(\"dfs://snapshot\", \"snapshot\") where Date =2020.12.31>, dateColumn=`Date, timeColumn=`Time, timeRepartitionSchema=timeRS)\n\tinputDict = dict([\"trade\", \"snapshot\"], [tradeDS, snapshotDS])\n\t\n\tsubmitJob(\"replay\", \"replay for factor calculation\", replay, inputDict, messageStream, `Date, `Time, 100000, true, 2)\n}\nreplayStockMarketData()\n```\n\n----------------------------------------\n\nTITLE: Listing Updated File Contents (keepDuplicates=ALL)\nDESCRIPTION: This console command `ll machines_2_219` displays the file structure after update operations when keepDuplicates is set to ALL. The file listing reveals that each level file is assigned a single hard link. This is important to observe for the changes post-update reflecting how the update mechanism manages data storage.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/dolphindb_update.md#_snippet_11\n\nLANGUAGE: console\nCODE:\n```\n$ ll machines_2_219\ntotal 284764\n-rw-rw-r-- 1 dolphindb dolphindb 57151251 Sep  7 05:48 0_00000615\n-rw-rw-r-- 1 dolphindb dolphindb 57151818 Sep  7 05:48 0_00000616\n-rw-rw-r-- 1 dolphindb dolphindb 58317419 Sep  7 05:48 0_00000617\n-rw-rw-r-- 1 dolphindb dolphindb 59486006 Sep  7 05:48 0_00000618\n-rw-rw-r-- 1 dolphindb dolphindb 59482644 Sep  7 05:48 0_00000619\n```\n\n----------------------------------------\n\nTITLE: Creating K-Minute Factor Storage Table in DolphinDB - DolphinDB\nDESCRIPTION: Defines a module and function to create a partitioned table designed to store K-minute line factor results for stock data. The database is dropped and recreated with daily partitions over a specified date range. The table schema includes fields for trade date, time, security ID, OHLC prices, volume, amount, and VWAP. The table is configured with sorting and duplicate retention settings suitable for time-series financial data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/dolphinscheduler_integration.md#_snippet_13\n\nLANGUAGE: DolphinDB\nCODE:\n```\nmodule minFactor::createMinFactorTable\n\ndef createMinuteFactor(dbName, tbName)\n{\n\tif(existsDatabase(dbName)){\n\t\tdropDatabase(dbName)\n\t}\n\t//按天分区\n\tdb = database(dbName, VALUE, 2021.01.01..2021.01.03,engine = `TSDB)\n\tcolName = `TradeDate`TradeTime`SecurityID`Open`High`Low`Close`Volume`Amount`Vwap\n\tcolType =[DATE, MINUTE, SYMBOL, DOUBLE, DOUBLE, DOUBLE, DOUBLE, LONG, DOUBLE, DOUBLE]\n\ttbSchema = table(1:0, colName, colType)\n  \tdb.createPartitionedTable(table=tbSchema,tableName=tbName,partitionColumns=`TradeDate,sortColumns=`SecurityID`TradeTime,keepDuplicates=ALL)\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing var function in DolphinDB\nDESCRIPTION: This DolphinDB script implements the `var` function, which calculates the population variance. The function takes `close`, `timePeriod`, and `nddev` as arguments.  It uses built-in functions like `mcount` and `mvar` to compute the variance efficiently. The code handles cases where there are not enough valid data points for the calculation, using `mcount` to check for this condition.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ta.md#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef var(close, timePeriod, nddev){\n1\tn = close.size()\n2\tb = close.ifirstNot()\n3\tif(b < 0 || b + timePeriod > n) return array(DOUBLE, n, n, NULL)\n4\tmobs =  mcount(close, timePeriod)\n5\treturn (mvar(close, timePeriod) * (mobs - 1) \\ mobs).fill!(timePeriod - 1 + 0:b, NULL)\n}\n```\n\n----------------------------------------\n\nTITLE: Defining DolphinDB Market Data Replay Main Function stkReplay in DolphinDB Script\nDESCRIPTION: The stkReplay function is the core replay service entry point implemented in DolphinDB's own scripting language. It accepts parameters including a list of stock symbols, replay start and end dates, replay rate, unique replay identifier, and a data source list. It validates inputs such as maximum stocks per replay, data source names, converts date formats, and initializes a persistent stream table to hold the replayed data. The function constructs dictionaries for data sources and timestamps, then submits the replay job for backend execution via submitJob. Error handling returns descriptive messages on invalid inputs or execution issues. This function depends on DolphinDB's streaming tables, enableTableShareAndPersistence for persistence, datetimeParse for date parsing, and submitJob for task scheduling. The replay implements ordered insertion based on timestamp and an optional secondary sorting column (e.g., ApplSeqNum for tick data).\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/appendices_market_replay_bp.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef stkReplay(stkList, mutable startDate, mutable endDate, replayRate, replayUuid, replayName)\n{\n    maxCnt = 50\n    returnBody = dict(STRING, STRING)\n    startDate = datetimeParse(startDate, \"yyyyMMdd\")\n    endDate = datetimeParse(endDate, \"yyyyMMdd\") + 1\n    sortColumn = \"ApplSeqNum\"\n    if(stkList.size() > maxCnt)\n    {\n        returnBody[\"errorCode\"] = \"0\"\n        returnBody[\"errorMsg\"] = \"超过单次回放股票上限，最大回放上限：\" + string(maxCnt)\n        return returnBody\n    }\n    if(size(replayName) != 0)\n    { \n        for(name in replayName)\n        {\n            if(not name in [\"snapshot\", \"order\", \"transaction\"])\n            {\n                returnBody[\"errorCode\"] = \"0\"\n                returnBody[\"errorMsg\"] = \"请输入正确的数据源名称，不能识别的数据源名称：\" + name\n                return returnBody\n            }\n        }\n    }\n    else\n    {\n        returnBody[\"errorCode\"] = \"0\"\n        returnBody[\"errorMsg\"] = \"缺少回放数据源，请输入正确的数据源名称\"\n        return returnBody\n    }\n    try \n    {\n        if(size(replayName) == 1 && replayName[0] == \"snapshot\")\n        {\n            colName = [\"timestamp\", \"biz_type\", \"biz_data\"]\n            colType = [TIMESTAMP, SYMBOL, BLOB]\n            sortColumn = \"NULL\"\n        }\n        else\n        {\n            colName = [\"timestamp\", \"biz_type\", \"biz_data\", sortColumn]\n            colType = [TIMESTAMP, SYMBOL, BLOB, LONG]\n        }\n        msgTmp = streamTable(10000000:0, colName, colType)\n        tabName = \"replay_\" + replayUuid\n        enableTableShareAndPersistence(table=msgTmp, tableName=tabName, asynWrite=true, compress=true, cacheSize=10000000, retentionMinutes=60, flushMode=0, preCache=1000000)\n        \n        timeRS = cutPoints(09:30:00.000..15:00:00.000, 23)\n        \n        inputDict = dict(replayName, each(dsTb{timeRS, startDate, endDate, stkList}, replayName))\n        dateDict = dict(replayName, take(`MDDate, replayName.size()))\n        timeDict = dict(replayName, take(`MDTime, replayName.size()))\n        \n        jobId = \"replay_\" + replayUuid\n        jobDesc = \"replay stock data\"\n        submitJob(jobId, jobDesc, replayJob{inputDict, tabName, dateDict, timeDict, replayRate, sortColumn})\n        returnBody[\"errorCode\"] = \"1\"\n        returnBody[\"errorMsg\"] = \"后台回放成功\"\n        return returnBody\n    }\n    catch(ex)\n    {\n        returnBody[\"errorCode\"] = \"0\"\n        returnBody[\"errorMsg\"] = \"回放行情数据异常，异常信息：\" + ex\n        return returnBody\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Loading the XGBoost Plugin\nDESCRIPTION: This code loads the XGBoost plugin into DolphinDB using the `loadPlugin` function.  The `pathToXgboost` variable should contain the full path to the PluginXgboost.txt file. Ensure the xgboost plugin is correctly installed in the path specified.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/machine_learning.md#_snippet_18\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\npathToXgboost = \"C:/DolphinDB/plugin/xgboost/PluginXgboost.txt\"\nloadPlugin(pathToXgboost)\n```\n\n----------------------------------------\n\nTITLE: Creating Replay End Signal - DolphinDB\nDESCRIPTION: This DolphinDB code defines the `createEnd` function, which generates an end-of-replay signal. It creates a new database and partitioned table if one doesn't already exist. It then uses the `replayDS` function to get data for the specified table. The function then utilizes the `replay` function for an N to 1 heterogeneous replay to construct the end signal, it will write a record to the specified stream table to indicate the end of the replay. Prerequisites include a running DolphinDB server, the availability of `existsDatabase`, `database`, `table`, `createPartitionedTable`, `append!`, `replayDS`, `dict`, and `replay` functions. The `sortColumn` controls whether sort column should be specified in the replay.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/appendices_market_replay_bp.md#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef createEnd(tabName, sortColumn)\n{\n    dbName = \"dfs://End\"\n    tbName = \"endline\"\n    if(not existsDatabase(dbName))\n    {\n        db = database(directory=dbName, partitionType=VALUE, partitionScheme=2023.04.03..2023.04.04)\n        endTb = table(2200.01.01T23:59:59.000 as DateTime, `END as point, long(0) as ApplSeqNum)\n        endLine = db.createPartitionedTable(table=endTb, tableName=tbName, partitionColumns=`DateTime)\n        endLine.append!(endTb)\n    }\n     \n    ds = replayDS(sqlObj=<select * from loadTable(dbName, tbName)>, dateColumn='DateTime', timeColumn='DateTime')\n    \n    inputEnd = dict([\"end\"], [ds])\n    dateEnd = dict([\"end\"], [`DateTime])\n    timeEnd = dict([\"end\"], [`DateTime])\n    if(sortColumn == \"NULL\")\n    {\n        replay(inputTables=inputEnd, outputTables=objByName(tabName), dateColumn=dateEnd, timeColumn=timeEnd, replayRate=-1, absoluteRate=false, parallelLevel=1)\n    }\n    else\n    {\n        replay(inputTables=inputEnd, outputTables=objByName(tabName), dateColumn=dateEnd, timeColumn=timeEnd, replayRate=-1, absoluteRate=false, parallelLevel=1, sortColumns=sortColumn)\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Distributing Load Tasks and Appending Data Using Pipeline Framework in DolphinDB Plugin C++\nDESCRIPTION: This complex snippet partitions the large binary data file into several chunks, creating distributed load tasks for each chunk by partial application of the `loadMyData` function with specific range parameters. It uses DolphinDB's pipeline model to asynchronously execute these load tasks in parallel, followed by sequential appending of loaded data blocks to a central table using an `append!` partial function. This pattern aids in scalable and memory-efficient import of large external data sources.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_development_tutorial.md#_snippet_21\n\nLANGUAGE: cpp\nCODE:\n```\nint sizePerPartition = 16 * 1024 * 1024;\nint partitionNum = fileLength / sizePerPartition;\nvector<DistributedCallSP> tasks;\nFunctionDefSP func = Util::createSystemFunction(\"loadMyData\", loadMyData, 1, 3, false);\nint partitionStart = start;\nint partitionLength = length / partitionNum;\nfor (int i = 0; i < partitionNum; i++) {\n\tif (i == partitionNum - 1)\n\t\tpartitionLength = length - partitionLength * i;\n\tvector<ConstantSP> partitionArgs = {path, new Int(partitionStart), new Int(partitionLength)};\n\tObjectSP call = Util::createRegularFunctionCall(func, partitionArgs);    // 将会调用 loadMyData(path, partitionStart, partitionLength)\n\ttasks.push_back(new DistributedCall(call, true));\n\tpartitionStart += partitionLength;\n}\nvector<ConstantSP> appendToResultArgs = {result};\nFunctionDefSP appendToResult = Util::createPartialFunction(heap->currentSession()->getFunctionDef(\"append!\"), appendToResultArgs);    // 相当于 append!{result}\nvector<FunctionDefSP> functors = {appendToResult};\nPipelineStageExecutor executor(functors, false);\nexecutor.execute(heap, tasks);\n```\n\n----------------------------------------\n\nTITLE: Extracting column names from CSV files in DolphinDB\nDESCRIPTION: Defines a function to extract column names from a wide-format CSV file. The function uses text schema extraction to identify all column symbols in the first row of the CSV file.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_data_load.txt#_snippet_1\n\nLANGUAGE: dolphindb\nCODE:\n```\ndef readColumnsFromWideCSV(absoluteFilename){\n        schema1 = extractTextSchema(absoluteFilename)\n        update schema1 set type = `STRING \n        allSymbol = loadText(absoluteFilename,,schema1)[0, 1:]\n        titleSchema = extractTextSchema(absoluteFilename, skipRows = 0);\n        for(x in allSymbol){\n                testValue = exec x[name] from titleSchema\n                testValue = testValue[1:]\n        }\n        return testValue\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Minute-Level Market Data for Meta-Programming Query Examples in DolphinDB Script\nDESCRIPTION: Builds a large scale test table with repeating securities, dates, and simulated trade times, suitable for demonstrating dynamic SQL generation and meta-programming patterns such as eval and parseExpr. The min_num variable (interval granularity) is also initialized.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_SQL_Case_Tutorial.md#_snippet_41\n\nLANGUAGE: DolphinDB\nCODE:\n```\nN = 10000000\nt = table(take(format(1..4000, \"000000\") + \".SH\", N) as SecurityID, \n          take(2021.10.01..2021.10.31, N) as DataDate, \n          take(join(09:30:00 + 1..120 * 60, 13:00:00 + 1..120 * 60), N) as TradeTime, \n          rand(100.0, N) as cal_variable)\nmin_num = 10\n```\n\n----------------------------------------\n\nTITLE: Enabling Database Replication\nDESCRIPTION: This DolphinDB script enables asynchronous replication for a specified database.  It calls the `setDatabaseForClusterReplication` function with the database object `db` and the boolean value `true`. Prerequisites include a valid DolphinDB connection.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Asynchronous_Replication.md#_snippet_9\n\nLANGUAGE: dolphindb\nCODE:\n```\nsetDatabaseForClusterReplication(db, true)\n```\n\n----------------------------------------\n\nTITLE: Retrieving OLAP Cache Engine Status in DolphinDB\nDESCRIPTION: The getOLAPCacheEngineStat function returns detailed information about the current state of the OLAP cache engine, including memory usage, transaction status, and other metrics.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/redoLog_cacheEngine.md#_snippet_5\n\nLANGUAGE: SQL\nCODE:\n```\ngetOLAPCacheEngineStat()\n```\n\n----------------------------------------\n\nTITLE: Performance Test: High-Speed Data Replay Script with DolphinDB\nDESCRIPTION: Offers a complete example of replaying over 330 million records from a distributed quotes table using optimal partitioning, streaming table sharing, and parallel job execution for benchmark testing. Omits rate-limiting for maximum throughput, providing a repeatable script to measure system replay performance under realistic infrastructure settings.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/historical_data_replay.md#_snippet_12\n\nLANGUAGE: DolphinDB\nCODE:\n```\nsch = quotes.schema().colDefs\ntrs = cutPoints(09:30:00.000..16:00:00.001,60)\nrds = replayDS(<select * from quotes where date=2007.08.17>, `date, `time,  trs);\nshare streamTable(100:0, sch.name, sch.typeString) as outQuotes1\njobid = submitJob(\"replay_quotes\", \"replay_quotes_stream\", replay, rds, outQuotes, `date, `time, 100000, true, 4)\n```\n\n----------------------------------------\n\nTITLE: Adding a Custom Function as a Function View\nDESCRIPTION: Uses the `addFunctionView` command to register the previously defined custom function `myfunc` as a persistent function view within the DolphinDB database. Function views are system objects accessible to users based on permissions and are distinct from functions loaded from modules.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/module_tutorial.md#_snippet_13\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\naddFunctionView(myfunc)\n```\n\n----------------------------------------\n\nTITLE: Reading Rows from Distributed Table Using DolphinDB Java API\nDESCRIPTION: This Java snippet illustrates fetching columns from a distributed DolphinDB BasicTable object and iterating row-wise to access data since BasicTable stores data column-wise. It shows how to get typed column vectors and iterate over rows printing values. The example includes database connection setup and query execution returning a BasicTable, which is then processed by the function. Dependencies include DolphinDB Java API classes such as DBConnection and BasicTable. Inputs are database connection parameters, table path/name, and query filtering; output is row-wise data printed to standard output.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/iot_examples.md#_snippet_21\n\nLANGUAGE: java\nCODE:\n```\npublic void getBasicTable(BasicTable table1) throws Exception{\n    BasicIntVector idVec = (BasicIntVector) table1.getColumn(\"id\");\n    BasicDateTimeVector timeVec = (BasicDateTimeVector) table1.getColumn(\"time\");\n    BasicFloatVector valueVec = (BasicFloatVector) table1.getColumn(\"value\");\n    for(int ri=0; ri<table1.rows(); ri++){\n        System.out.println(idVec.getInt(ri));\n        LocalDateTime timestamp = timeVec.getDatetime(ri);\n        System.out.println(timestamp);\n        System.out.println(valueVec.getDouble(ri));\n    }\n}\nDBConnection conn = new DBConnection();\ntry {\n\tconn.connect(SERVER, PORT, USER, PASSWORD);\n\tBasicTable table1=conn.run(String.format(\"select * from loadTable('%s','%s')  where id=1, datetime between 2020.09.01T00:00:00 : 2020.09.07T23:59:59\",dbPath,tableName);\n\tgetBasicTable(table1) ;\n} catch (IOException e) {\n    e.printStackTrace();\n}\n```\n\n----------------------------------------\n\nTITLE: Computing and Inserting K-Minute Line Factor Indicators into DolphinDB Table - DolphinDB\nDESCRIPTION: Defines a function to compute 1-minute K-line factors from trade data loaded from a specified database over a date range. It limits calculation to no more than 12 days at once. The function aggregates trade data grouped by date, minute, and security ID to compute open, high, low, close prices, volume, amount, and volume weighted average price (VWAP). The computed results are appended to a mutable factor table and informational messages are logged to a mutable info table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/dolphinscheduler_integration.md#_snippet_14\n\nLANGUAGE: DolphinDB\nCODE:\n```\nmodule minFactor::computeMinFactor\n\ndef calFactorOneMinute(dbName, startDate, endDate, mutable factorTb,mutable infoTb)\n{\n\tpt = loadTable(dbName, \"trade\")\n\tdayList = startDate..endDate\n\tif(dayList.size()>12) dayList = dayList.cut(12)\n\tfor(days in dayList){\n\t\t//计算分钟 K 线\n\t\tres = select first(TradePrice) as open, max(TradePrice) as high, min(TradePrice) as low, last(TradePrice) as close, sum(tradeQty) as volume,sum(TradePrice*TradeQty) as amount,sum(TradePrice*TradeQty)\\sum(TradeQty) as vwap from pt where date(tradeTime) in days group by date(tradeTime) as TradeDate,minute(tradeTime) as TradeTime, SecurityID\n\t\tmsg = \"Start to append minute factor result , the days is: [\" + concat(days, \",\")+\"]\"\n\t\tprint(msg)\n\t\tinfoTb.tableInsert(msg)\n\t\t//分钟 K 线入库\n\t\tfactorTb.append!(res)\n\t\tmsg = \"Successfully append the minute factor result to databse, the days is: [\" + concat(days, \",\")+\"]\"\n\t\tprint(msg)\n\t\tinfoTb.tableInsert(msg)\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: Calculating Cumulative Device Idle Time in DolphinDB Script\nDESCRIPTION: This script calculates the total idle time for a specific device within a given time range. It queries data from the 'pt' table for a specific device and property code (representing current), calculates time differences between consecutive records using 'deltas', and sums the durations where the previous current value ('before') was below the idle threshold (60). The result is grouped hourly.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Iot_intelligent_O&M.md#_snippet_6\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\n//案例 6：根据电流计算一段时间内设备的累计空载时间\ndevice=\"361RP17\"     //设备编号\npoint=\"361RP17007\"   //测点编号，记录电流的测点，得到的 propertyValue 表示电流值\nt=select * from pt where deviceCode=device and propertyCode=point and ts between 2022.01.01 00:00:00 : 2022.01.01 01:59:59\n//统计空载时间\nt = select *,deltas(ts) as duration,nullFill(prev(propertyValue),propertyValue) as before from t where deviceCode=device and propertyCode=point and ts between 2022.01.01 00:00:00 : 2022.01.01 01:59:59 \n//输出\nselect sum(duration)/60 from t where before<=60 group by bar(ts,1H),deviceCode,logicalPositionId,physicalPositionId\n```\n\n----------------------------------------\n\nTITLE: Creating Sample Data for Stock Portfolio Value Calculation\nDESCRIPTION: This snippet creates sample data for calculating the value of a stock portfolio composed of AAPL and FB. The `quotes` table contains `Symbol` (stock code), `Time` (timestamp), and `Price` columns. The `weights` dictionary stores the weights for each stock in the portfolio.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_sql_cases.md#_snippet_26\n\nLANGUAGE: DolphinDB\nCODE:\n```\nsyms = take(`AAPL, 6) join take(`FB, 5)\ntime = 2019.02.27T09:45:01.000000000 + [146, 278, 412, 445, 496, 789, 212, 556, 598, 712, 989]\nprices = 173.27 173.26 173.24 173.25 173.26 173.27 161.51 161.50 161.49 161.50 161.51\nquotes = table(take(syms, 100000) as Symbol, \n               take(time, 100000) as Time, \n               take(prices, 100000) as Price)\nweights = dict(`AAPL`FB, 0.6 0.4)\nETF = select Symbol, Time, Price*weights[Symbol] as weightedPrice from quotes\n```\n\n----------------------------------------\n\nTITLE: Updating Existing Columns with update!\nDESCRIPTION: Demonstrates updating existing columns in a memory table using the `update!` function.  It includes updating all rows and updating rows based on a boolean expression.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/partitioned_in_memory_table.md#_snippet_19\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntrades.update!(`qty1, <qty1+10>);\n\ntrades.update!(`qty1, <qty1+10>, <sym=`IBM>);\n```\n\n----------------------------------------\n\nTITLE: Exporting Data to CSV DolphinDB\nDESCRIPTION: This code exports all data from the `readings` table to a CSV file.  It uses `saveText` to write the contents to `/data/devices/readings_dump_dolphindb.csv`.  The time taken to execute is also provided in the comments to give an indication of the performance.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs TimescaleDB/test_dolphindb_small.txt#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// ----------------- 导出数据\ntimer saveText((select * from readings), '/data/devices/readings_dump_dolphindb.csv')\n// Time elapsed: 6.6 s   ，该语句的执行时间未包括从缓存写入硬盘的时间，因为计时结束后系统监控中显示仍然有 200 MB/s 的速率正在写入磁盘。\n// 从语句开始执行到数据完全写入硬盘（系统监控中无磁盘写入）的时间为 29 s\n```\n\n----------------------------------------\n\nTITLE: Initializing Composite and TSDB Partitioned Databases - DolphinDB Script\nDESCRIPTION: This snippet initializes two partitioned databases, one using a composite (COMPO) partition scheme and the other a time-series (TSDB) scheme, in DolphinDB. Dependencies include DolphinDB server access and administrator permissions (login required). It checks for existing databases and drops them if found, then creates symbol, source, and time partitions. Key parameters are database paths, time range, source identifiers, and partitioning methods. Outputs include newly created databases prepared for table creation; input assumptions include valid path permissions.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/buildData.txt#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndbPath=\"dfs://testdb\"\ndbPathTSDB=\"dfs://testdb_tsdb\"\nlogin(`admin,`123456)\ndates=date(datetimeAdd(2012.01.01,0..1000,'d'))\n\nif(existsDatabase(dbPath)){\n\tdropDatabase(dbPath)\n}\nif(existsDatabase(dbPathTSDB)){\n\tdropDatabase(dbPathTSDB)\n}\n\ndbSource = database(\"\",VALUE,[\"tp\"])\ndbTime = database(\"\",VALUE,2012.01.01..2021.12.31)\ndbSym = database(\"\",HASH,[SYMBOL, 5])\ndb=database(dbPath,COMPO,[dbSym,dbSource,dbTime])\n\ndbTSDB=database(dbPathTSDB,COMPO,[dbSym,dbSource,dbTime],,'TSDB')\n```\n\n----------------------------------------\n\nTITLE: 定义性能测试函数并提交任务\nDESCRIPTION: 定义一个包含数据处理和10个因子计算的完整函数，并通过submitJob提交单用户和多用户任务，用于测试不同CPU核数和任务量下的性能。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/fund_factor_contrasted_by_py.md#_snippet_27\n\nLANGUAGE: dolphindb\nCODE:\n```\ndef parJob(){\n  \ttimer{fund_OLAP=select * from loadTable(\"dfs://fund_OLAP\", \"fund_OLAP\")\n  \t\t  fund_hs_OLAP=select * from loadTable(\"dfs://fund_OLAP\", \"fund_hs_OLAP\")\n  \t\t  ajResult = select tradingdate, fundNum, value, fund_hs_OLAP.tradingDate as hstradingDate, fund_hs_OLAP.value as price from aj(fund_OLAP, fund_hs_OLAP, `tradingDate)\n  \t\t  result2 = select tradingdate, fundNum, iif(isNull(value), ffill!(value), value) as value,price from ajResult where tradingDate == hstradingDate\n  \t\t  symList = exec distinct(fundNum) as fundNum from result2 order by fundNum\n          symList2 = symList.cut(250)\n  \t\t  portfolio = select fundNum as fundNum, (deltas(value)\\prev(value)) as log, tradingDate as tradingDate from result2 where tradingDate in 2018.05.24..2021.05.27 and fundNum in symList\n          m_log = exec log from portfolio pivot by tradingDate, fundNum\n          mlog =  m_log[1:,]\n          knum = 2..365\n            }//此处，将任务切分，按每次250个不同基金数据进行计算.\n    timer{ploop(getFactor{result2}, symList2)\n          a = ploop(calAllRs2{mlog,symList}, knum).unionAll(false)\n          res2 = select fundNum, ols(factor1, kNum)[0] as hist, ols(factor1, kNum)[1] as hist2, ols(factor1, kNum)[2] as hist3 from a group by fundNum}\n  }//定义获取10个因子计算和数据操作的时间的函数\n  /**\n   * 提交1个 job（单用户）\n   */\n  submitJob(\"parallJob1\", \"parallJob_single_ten\", parJob)\n  \n  /**\n   * 提交5个 job（多用户）\n   */\n  for(i in 0..4){\n  \tsubmitJob(\"parallJob5\", \"parallJob_multi_ten\", parJob)\n  }\n```\n\n----------------------------------------\n\nTITLE: Registering Stream Filter and Dispatch Engine for Kafka (Python)\nDESCRIPTION: This snippet registers a stream filter engine to process heterogeneous data from the `messageStream` and send it to Kafka.  It defines handlers for 'order', 'trade', and 'snapshot' data types, using the `sendMsgToKafkaFunc` function to send each type of data to Kafka. The `streamFilter` function handles deserialization based on the message schema and dispatches data to the appropriate handler.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/stock_market_replay.md#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nfilter1 = dict(STRING,ANY)\nfilter1[\"condition\"] =  \"order\"\nfilter1[\"handler\"] = sendMsgToKafkaFunc{\"order\", producer}\nfilter2 = dict(STRING,ANY)\nfilter2[\"condition\"] = \"trade\"\nfilter2[\"handler\"] = sendMsgToKafkaFunc{\"trade\", producer}\nfilter3 = dict(STRING,ANY)\nfilter3[\"condition\"] = \"snapshot\"\nfilter3[\"handler\"] = sendMsgToKafkaFunc{\"snapshot\", producer}\n\nschema = dict([\"order\",\"trade\", \"snapshot\"], [loadTable(\"dfs://order\", \"order\"), loadTable(\"dfs://trade\", \"trade\"), loadTable(\"dfs://snapshot\", \"snapshot\")])\n\nengine = streamFilter(name=\"streamFilter\", dummyTable=messageStream, filter=[filter1, filter2, filter3], msgSchema=schema)\n```\n\n----------------------------------------\n\nTITLE: Filtering Stock Data Stream in DolphinDB\nDESCRIPTION: Defines a filter function that selects securities starting with '60' and time after 09:25:00, then subscribes to a stream table to process incoming data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/Real-Time_Stock_Price_Increase_Calculation/03.注册流计算引擎和订阅流数据表.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// filter\ndef snapshotFilter(engineName, mutable data){\n\tt = select * from data where left(SecurityID, 2)=\"60\" and time(DateTime)>=09:25:00.000\n\tgetStreamEngine(engineName).append!(t)\n}\n\nsubscribeTable(tableName=\"snapshotStreamTable\", actionName=\"snapshotFilter\", offset=-1, handler=snapshotFilter{\"calChange\"}, msgAsTable=true, hash=0)\n```\n\n----------------------------------------\n\nTITLE: Processing Level2 Snapshot Data in DolphinDB\nDESCRIPTION: This part of the script loads level2 snapshot data, defines a function `featureEngine` for feature calculation, and uses SQL to process the data. It filters the data by date, security ID, and time, groups the data by security ID and a 10-minute interval, and applies the featureEngine function.  The result is then refined and stored into the pre-created table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/machine_learning_volatility/02.dataProcessArrayVector.txt#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndbName = \"dfs://snapshot_SH_L2_TSDB_ArrayVector\"\ntableName = \"snapshot_SH_L2_TSDB_ArrayVector\"\nsnapshot = loadTable(dbName, tableName)\nstockList=`601318`600519`600036`600276`601166`600030`600887`600016`601328`601288`600000`600585`601398`600031`601668`600048`601888`600837`601601`601012`603259`601688`600309`601988`601211`600009`600104`600690`601818`600703`600028`601088`600050`601628`601857`601186`600547`601989`601336`600196`603993`601138`601066`601236`601319`603160`600588`601816`601658`600745\n//define function to process data with matrix operation\ndefg featureEngine(bidPrice,bidQty,offerPrice,offerQty){\n\tbas = offerPrice[0]\\bidPrice[0]-1\n\twap = (bidPrice[0]*offerQty[0] + offerPrice[0]*bidQty[0])\\(bidQty[0]+offerQty[0])\n\tdi = (bidQty-offerQty)\\(bidQty+offerQty)\n\tbidw=(1.0\\(bidPrice-wap))\n\tbidw=bidw\\(bidw.rowSum())\n\tofferw=(1.0\\(offerPrice-wap))\n\tofferw=offerw\\(offerw.rowSum())\n\tpress=log((bidQty*bidw).rowSum())-log((offerQty*offerw).rowSum())\n\trv=std(log(wap)-log(prev(wap)))*sqrt(24*252*size(wap))\n\treturn avg(bas),avg(di[0]),avg(di[1]),avg(di[2]),avg(di[3]),avg(di[4]),avg(di[5]),avg(di[6]),avg(di[7]),avg(di[8]),avg(di[9]),avg(press),rv\n}\n//SQL for level2 snapshot data processing\nresult = select\n\t\tfeatureEngine(BidPrice,BidOrderQty,OfferPrice,OfferOrderQty) as `BAS`DI0`DI1`DI2`DI3`DI4`DI5`DI6`DI7`DI8`DI9`Press`RV\n\tfrom snapshot\n\twhere date(TradeTime) between 2020.01.01 : 2020.12.31, SecurityID in stockList, (time(TradeTime) between 09:30:00.000 : 11:29:59.999) || (time(TradeTime) between 13:00:00.000 : 14:56:59.999)\n\tgroup by SecurityID, interval( TradeTime, 10m, \"none\" ) as TradeTime map\nresult = select *, next(RV) as targetRV from result context by date(TradeTime), SecurityID\nresult = result[each(isValid, result.values()).rowAnd()]\nresult = select * from (select *, count(*) from result context by date(TradeTime), SecurityID) where count=23\ndropColumns!(result, `count)\n//store the processing results\nloadTable(storeDBName, storeTBName).append!(result)\n```\n\n----------------------------------------\n\nTITLE: Inserting Data into Memory Table with tableInsert\nDESCRIPTION: Demonstrates inserting data into a memory table using the `tableInsert` function.  For partitioned tables, the new data must be in table format. For non-partitioned tables the new data can be in tuple format.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/partitioned_in_memory_table.md#_snippet_14\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntmp=table(`S`IBM as col1,[2000.12.31,2000.12.30] as col2,[10.0,20.0] as col3,[10.0,20.0] as col4,[10.0,20.0] as col5,[10.0,20.0] as col6,[10.0,20.0] as col7,[10.0,20.0] as col8,[10,20] as col9,[10,20] as col10,[10,20] as col11,[10,20] as col12,[10,20] as col13,[10,20] as col14)\ntrades.tableInsert(tmp)\n\na=(`S`IBM,[2000.12.31,2000.12.30],[10.0,20.0],[10.0,20.0],[10.0,20.0],[10.0,20.0],[10.0,20.0],[10.0,20.0],[10,20],[10,20],[10,20],[10,20],[10,20],[10,20])\ntrades.tableInsert(a)\n```\n\n----------------------------------------\n\nTITLE: Moving Window Aggregation with Custom Functions in DolphinDB SQL\nDESCRIPTION: Shows how to implement a user-defined aggregation function inside moving window calculations in DolphinDB. Uses a sample table with code, date, volume, and close price columns, grouped by code and sorted by date. It calculates the average close price of top 5 volumes within a 20-row moving window. The snippet provides two approaches: inline anonymous aggregation (available in DolphinDB 1.30.15 and above) and a named defg function for backward compatibility. Inputs are the volume and close price columns; output is the custom aggregation per window. Dependencies include DolphinDB version supporting defg and moving functions.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/window_cal.md#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//t是模拟的四列数据\nt = table(take(`IBM, 100) as code, 2020.01.01 + 1..100 as date, rand(100,100) + 20 as volume, rand(10,100) + 100.0 as close)\n\n//1.30.15及以上版本可以用一行代码实现\n//moving 支持用户使用自定义匿名聚合函数\nselect code, date, moving(defg(vol, close){return close[isort(vol, false).subarray(0:min(5,close.size()))].avg()}, (volume, close), 20) from t context by code \n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//其他版本可以用自定义命名聚合函数实现：\ndefg top_5_close(vol,close){\nreturn close[isort(vol, false).subarray(0:min(5,close.size()))].avg()\n}\nselect code, date, moving(top_5_close,(volume, close), 20) from t context by code \n```\n\n----------------------------------------\n\nTITLE: Creating Asof Join Engine in DolphinDB (Python)\nDESCRIPTION: This snippet creates an asof join engine in DolphinDB to join trade and snapshot data streams based on `SecurityID` and `Time`. It defines a custom function `createSchemaTable` to load table schemas. The engine calculates metrics using the price from trade data and the bid/offer prices from snapshot data. It's configured to output results with the same number of rows as the left table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/stock_market_replay.md#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\ndef createSchemaTable(dbName, tableName){\n\tschema = loadTable(dbName, tableName).schema().colDefs\n\treturn table(1:0, schema.name, schema.typeString)\n}\ntradeSchema = createSchemaTable(\"dfs://trade\", \"trade\")\nsnapshotSchema = createSchemaTable(\"dfs://snapshot\", \"snapshot\")\n\njoinEngine=createAsofJoinEngine(name=\"tradeJoinSnapshot\", leftTable=tradeSchema, rightTable=snapshotSchema, outputTable=prevailingQuotes, metrics=<[Price, TradeQty, BidPX1, OfferPX1, abs(Price-(BidPX1+OfferPX1)/2), snapshotSchema.Time]>, matchingColumn=`SecurityID, timeColumn=`Time, useSystemTime=false, delayedTime=1)\n```\n\n----------------------------------------\n\nTITLE: Inserting Data into Memory Table with append!\nDESCRIPTION: Demonstrates inserting data into a memory table using the `append!` function. The new data must be in the form of a table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/partitioned_in_memory_table.md#_snippet_13\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntmp=table(`S`IBM as col1,[2000.12.31,2000.12.30] as col2,[10.0,20.0] as col3,[10.0,20.0] as col4,[10.0,20.0] as col5,[10.0,20.0] as col6,[10.0,20.0] as col7,[10.0,20.0] as col8,[10,20] as col9,[10,20] as col10,[10,20] as col11,[10,20] as col12,[10,20] as col13,[10,20] as col14)\ntrades.append!(tmp)\n```\n\n----------------------------------------\n\nTITLE: Defining Output Stream Table for Reactive Engine in DolphinDB\nDESCRIPTION: Defines a stream table named `outputSt1` as the output for the reactive state engine. This table stores the tracked state changes, mirroring the structure of the input stream table with columns `tag`, `ts`, and `value`.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_streaming_application_in_IOT.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\nout1 =streamTable(10000:0,`tag`ts`value,[SYMBOL,TIMESTAMP, INT])\nenableTableShareAndPersistence(table=out1,tableName=`outputSt1,asynWrite=false,compress=true, cacheSize=100000)\n```\n\n----------------------------------------\n\nTITLE: Connecting DolphinDB to Redshift via ODBC in script\nDESCRIPTION: DolphinDB script commands to initialize ODBC plugin, create partitioned table, and establish connection to Redshift database for data transfer.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Migrate_data_from_Redshift_to_DolphinDB.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(\"admin\",\"123456\")\nloadPlugin(\"./plugins/odbc/PluginODBC.txt\")\n\ndb=database(\"dfs://redshift\",HASH,[INT,10])\ndt=table(300000000:0,[\"channelno\",\"applseqnum\",\"mdstreamid\",\"bidapplseqnum\",\"offerapplseqnum\",\"securityid\",\"securityidsource\",\"lastpx\",\"lastqty\",\"exectype\",\"transacttime\",\"localtime\",\"seqno\",\"transactiondate\"],[INT,INT,SYMBOL,INT,INT,SYMBOL,SYMBOL,DOUBLE,DOUBLE,INT,TIME,TIME,INT,DATE])\npt=createPartitionedTable(db,dt,\"dfstable\",\"channelno\")\n\nconn=odbc::connect(\"Driver={Amazon Redshift (x64)}; Server=Your server; Database=Your database;User=UID;Password=PWD;\")\n```\n\n----------------------------------------\n\nTITLE: AutoFitTableUpsert Usage in C++\nDESCRIPTION: This code snippet demonstrates how to use AutoFitTableUpsert (AFTU) to write or update data in a DolphinDB table. It creates an AFTU object, specifies the key column(s), and calls `upsert()` to insert new data or update existing data based on the key column. This example relies on an existing connection object 'conn' and a table object 'bt'.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_cpp_api_connector.md#_snippet_6\n\nLANGUAGE: C++\nCODE:\n```\nvector<string> keycolName = {\"id\"};\nAutoFitTableUpsert aftu(\"dfs://test_AutoFitTableUpsert\", \"collect\", conn, false, &keycolName);\naftu.upsert(bt);\n```\n\n----------------------------------------\n\nTITLE: Generating Option ETF Price Matrix using Panel (DolphinDB Script)\nDESCRIPTION: Transforms the narrow table `data` into a wide matrix `etfPriceWideMatrix` using the `panel` function. Rows are indexed by option contract codes (`data.codes`), columns by trade dates (`data.tradeDate`), and the matrix cells contain the corresponding synthetic ETF prices (`data.etfprice`).\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/IV_Greeks_Calculation_for_ETF_Options_Using_JIT.md#_snippet_3\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\netfPriceWideMatrix = panel(data.codes, data.tradeDate, data.etfprice)\n```\n\n----------------------------------------\n\nTITLE: Calculating Max Continuous Running Time - DolphinDB\nDESCRIPTION: This code calculates the maximum continuous running time for a device. It uses `deltas` to get time differences, `segment` to group by device state, and `iif` to conditionally sum durations when the device is running (propertyValue == 1). It requires a table named 'dt' with columns 'ts', 'deviceCode', 'logicalPositionId', 'physicalPositionId', 'propertyValue'.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Iot_intelligent_O&M.md#_snippet_3\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\n//案例 3：根据状态值，计算设备最大持续运行时间\n//数据准备，累计时间\nt = select *,deltas(ts) as duration from dt where ts between 2022.01.01 00:00:00 : 2022.01.01 01:59:59 order by ts\nt=select sum(iif(propertyValue==1, duration, 0)) as duration_running_max,sum(iif(propertyValue==1, 0,duration)) as duration_stop_max from t group by deviceCode,logicalPositionId,physicalPositionId,segment(propertyValue)\n//输出（分钟）\nselect max(duration_running_max)/60 as duration_running_max,max(duration_stop_max)/60 as duration_stop_max from t group by deviceCode,logicalPositionId,physicalPositionId\n```\n\n----------------------------------------\n\nTITLE: Loading CSV Data into DolphinDB Composite Databases (DolphinDB Script)\nDESCRIPTION: This script loads and normalizes multiple CSV files into a DolphinDB composite database using date and symbol partitioning. It establishes the schema mapping, sets up composite key partitioning (by date and symbol), and performs loading in a loop for several consecutive files, renaming fields for consistency with DolphinDB conventions. Requires DolphinDB server environment and access to the given file paths. Input parameters are filepaths and database paths; outputs are loaded tables in the distributed database. Field mapping must be carefully aligned to ensure correctness.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_13\n\nLANGUAGE: dolphindb\nCODE:\n```\nfilepath = \"/home/revenant/Documents/TAQ_8/\"\ndbpath = \"dfs://rangedb_100\"\ndbDate = database(, VALUE, 2007.08.06 2007.08.07 2007.08.08 2007.08.09 )\ndbSym = database(, RANGE, `A `B `C `D `E `F `G `H `I `J `K `L `M `N `O `P `Q `R `S `T `U `V `W `X `Y `Z `ZZZ `ZZZZZZZZ)\ndb = database(dbpath, COMPO, [dbDate, dbSym])\n//dropDatabase(dbpath)\nfor(i in 1:5){\n\tpath = filepath + string(i) + \".csv\"\n\tschemaTb=extractTextSchema(path)\n\tupdate schemaTb set name=`sym where name=`Symbol\n\tupdate schemaTb set name=`trade_date where name = `Date\n\tupdate schemaTb set name=`trade_time where name = `Time\n\tupdate schemaTb set name=`bid where name = `BID\n\tupdate schemaTb set name=`ofr where name = `OFR\n\tupdate schemaTb set name=`bidsiz where name = `BIDSIZ\n\tupdate schemaTb set name=`ofrsiz where name = `OFRSIZ\n\tupdate schemaTb set name=`mode where name = `MODE\n\tupdate schemaTb set name=`ex where name = `EX\n\tupdate schemaTb set name=`mmid where name = `MMID\n\ttimer(1)\n\ttrades = db.loadTextEx(`trades,`trade_date `sym, path,,schemaTb)\n}\n\n```\n\n----------------------------------------\n\nTITLE: Initializing and Executing Yearly Stock Order Book Load in DolphinDB\nDESCRIPTION: This block sets up the configuration for the data loading process. It defines the target DolphinDB database URI (`dbName`), table name (`tableName`), the root directory containing the yearly data (`filePath`), and path to a sample CSV file (`csv`) used for schema extraction. It then calls `getSchema` to prepare the schema and finally initiates the parallel, year-long data loading process by invoking `loopLoadOneYearFiles` with the configured parameters.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/csvImportDemo/importNewData.txt#_snippet_4\n\nLANGUAGE: dolphindb\nCODE:\n```\ndbName = \"dfs://stocks_orderbook\"\ntableName = \"orderBook\"\nfilePath=\"/hdd/hdd9/data/quotes/2020\" \ncsv=\"/hdd/hdd9/data/quotes/2020/20200102/SH501000.csv\"\nschema1=getSchema(csv)\nloopLoadOneYearFiles(dbName,tableName, filePath,schema1)\n```\n\n----------------------------------------\n\nTITLE: MultithreadedTableWriter Initialization in C++\nDESCRIPTION: This code snippet initializes a MultithreadedTableWriter (MTW) object in C++. It specifies the server address, port, credentials, database path, table name, and other parameters such as compression, batch size, thread count, and partition column. The status variable is used to store the writer's status.  The `compress` variable is passed by reference, so its value will be used for compression.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_cpp_api_connector.md#_snippet_9\n\nLANGUAGE: C++\nCODE:\n```\n// 建立writer对象\nMultithreadedTableWriter writer(\n            \"183.136.170.167\", 9900, \"admin\",\"123456\",\"\",\"streamtable\",NULL,false,NULL,1000,1,5,\"deviceid\", &compress); \nMultithreadedTableWriter::Status status;  // 保存 writer 状态\n```\n\n----------------------------------------\n\nTITLE: Connecting to DolphinDB in JavaScript - Python Syntax\nDESCRIPTION: This snippet shows how to import the DolphinDB JavaScript client and initialize a connection to a DolphinDB instance using WebSocket. The configuration object supports customization of login, user, password, and session flags. Replace the WebSocket URL and credentials as needed. Upon calling await ddb.connect(), a connection to DolphinDB is established. Requires the 'dolphindb' JavaScript API library and an environment supporting top-level await.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/node_red_tutorial_iot.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { DDB } from 'dolphindb'\n\n// 使用 WebSocket URL 初始化连接到 DolphinDB 的实例（不建立实际的网络连接）\nlet ddb = new DDB('ws://127.0.0.1:8848', {\n    // 是否在建立连接后自动登录，默认 `true`\n    autologin: true,\n    \n    // DolphinDB 登录用户名，默认 `'admin'`\n    username: 'admin',\n    \n    // DolphinDB 登录密码，默认 `'123456'`\n    password: '123456',\n    \n    // 设置 python session flag，默认 `false`\n    python: false,\n    \n    // 设置该选项后，该数据库连接只用于流数据\n    streaming: undefined\n})\n\n// 建立到 DolphinDB 的连接\nawait ddb.connect()\n```\n\n----------------------------------------\n\nTITLE: Restoring DolphinDB Partitioned Table Data from Backup to Original or New Tables\nDESCRIPTION: Restores backed-up data from the specified backup directory to the original partitioned table \"pt\" or to a new table (\"temp\") or database. The 'restore' function handles full or partial data recovery based on partition filters. Parameters include backup path, source database and table, partition filter string, overwrite flag, and optionally the target table if restoring to a different one. Migration between databases is performed using the 'migrate' function, allowing full data transfer from one database/table to another. This supports disaster recovery and data migration scenarios.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/restore-backup.md#_snippet_13\n\nLANGUAGE: DolphinDB\nCODE:\n```\nrestore(\"/home/DolphinDB/backup\",\"dfs://compoDB\",\"pt\",\"%\",true);\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntemp=db.createPartitionedTable(t, `temp, `date`ID);\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nrestore(\"/home/DolphinDB/backup\",\"dfs://compoDB\",\"pt\",\"%20170810%\",true,temp);\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nmigrate(\"/home/DolphinDB/backup\",\"dfs://compoDB\",\"pt\",\"dfs://newCompoDB\",\"pt\");\n```\n\n----------------------------------------\n\nTITLE: 自定义函数实现连续区间最值计算\nDESCRIPTION: 通过自定义函数generateGrp对大于等于目标值的连续区间进行分组，并在每个分组内找出最大值的第一条记录。这是未优化版本。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_SQL_Case_Tutorial.md#_snippet_26\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef generateGrp(targetVal, val) {\n\tarr = array(INT, val.size())\n\tn = 1\n\tfor(i in 0 : val.size()) {\n\t\tif(val[i] >= targetVal) {\n\t\t\tarr[i] = n\n\t\t\tif(val[i + 1] < targetVal) n = n + 1\n\t\t}\n\t}\n\treturn arr\n}\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntimer(1000) {\n\ttmp = select date, value, generateGrp(targetVal, value) as grp from t\n\tres1 = select date, value from tmp where grp != 0 \n\t\t   context by grp \n\t\t   having value = max(value) limit 1\n}\n```\n\n----------------------------------------\n\nTITLE: Stock Data Replay with Error Handling - DolphinDB\nDESCRIPTION: The `stkReplay` function replays stock data, handling input validation and error scenarios. It takes a list of stock symbols, start and end dates, replay rate, replay UUID, and replay name(s) as input. It validates the size of the stock list and the replay names, returning specific error messages if these checks fail. It dynamically creates a stream table based on the replay name and submits a replay job using the `replayJob` function, using generated dictionaries for input tables, date columns, and time columns. It includes a try-catch block to handle potential exceptions and return error information.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/appendices_market_replay_bp/replay.txt#_snippet_3\n\nLANGUAGE: dolphindb\nCODE:\n```\ndef stkReplay(stkList, mutable startDate, mutable endDate, replayRate, replayUuid, replayName)\n{\n    maxCnt = 50\n    returnBody = dict(STRING, STRING)\n    startDate = datetimeParse(startDate, \"yyyyMMdd\")\n    endDate = datetimeParse(endDate, \"yyyyMMdd\") + 1\n    sortColumn = \"ApplSeqNum\"\n    if(stkList.size() > maxCnt)\n    {\n        returnBody[\"errorCode\"] = \"0\"\n        returnBody[\"errorMsg\"] = \"超过单次回放股票上限，最大回放上限：\" + string(maxCnt)\n        return returnBody\n    }\n    if(size(replayName) != 0)\n    {\n        for(name in replayName)\n        {\n            if(not name in [\"snapshot\", \"order\", \"transaction\"])\n            {\n                returnBody[\"errorCode\"] = \"0\"\n                returnBody[\"errorMsg\"] = \"请输入正确的数据源名称，不能识别的数据源名称：\" + name\n                return returnBody\n            }\n        }\n    }\n    else\n    {\n        returnBody[\"errorCode\"] = \"0\"\n        returnBody[\"errorMsg\"] = \"缺少回放数据源，请输入正确的数据源名称\"\n        return returnBody\n    }\n    try \n    {\n        if(size(replayName) == 1 && replayName[0] == \"snapshot\")\n        {\n            colName = [\"timestamp\", \"biz_type\", \"biz_data\"]\n            colType = [TIMESTAMP, SYMBOL, BLOB]\n            sortColumn = \"NULL\"\n        }\n        else\n        {\n            colName = [\"timestamp\", \"biz_type\", \"biz_data\", sortColumn]\n            colType = [TIMESTAMP, SYMBOL, BLOB, LONG]\n        }\n        msgTmp = streamTable(10000000:0, colName, colType)\n        tabName = \"replay_\" + replayUuid\n        enableTableShareAndPersistence(table=msgTmp, tableName=tabName, asynWrite=true, compress=true, cacheSize=10000000, retentionMinutes=60, flushMode=0, preCache=1000000)\n        \n        timeRS = cutPoints(09:30:00.000..15:00:00.000, 23)\n        \n        inputDict = dict(replayName, each(dsTb{timeRS, startDate, endDate, stkList}, replayName))\n        dateDict = dict(replayName, take(`MDDate, replayName.size()))\n        timeDict = dict(replayName, take(`MDTime, replayName.size()))\n        \n        jobId = \"replay_\" + replayUuid\n        jobDesc = \"replay stock data\"\n        submitJob(jobId, jobDesc, replayJob{inputDict, tabName, dateDict, timeDict, replayRate, sortColumn})\n        returnBody[\"errorCode\"] = \"1\"\n        returnBody[\"errorMsg\"] = \"后台回放成功\"\n        return returnBody\n    }\n    catch(ex)\n    {\n        returnBody[\"errorCode\"] = \"0\"\n        returnBody[\"errorMsg\"] = \"回放行情数据异常，异常信息：\" + ex\n        return returnBody\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Starting DolphinScheduler Standalone Server (Bash)\nDESCRIPTION: This command starts the DolphinScheduler standalone server process from the scheduler deployment directory. It is typically used to manually initiate the server after resolving startup issues, potentially involving the termination of previous stuck processes.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/dolphinscheduler_integration.md#_snippet_27\n\nLANGUAGE: Bash\nCODE:\n```\nbash ./bin/dolphinscheduler-daemon.sh start standalone-server\n```\n\n----------------------------------------\n\nTITLE: Calculating Return-to-Drawdown Ratio in DolphinDB\nDESCRIPTION: Defines 'getDrawdownRatio' to calculate the ratio of annualized return to maximum drawdown, a performance metric representing return relative to risk of loss. It calls previously defined annual return and max drawdown functions and returns a scalar ratio.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_factor_by_ddb/fund_factor_nine.txt#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef getDrawdownRatio(value){\n\treturn getAnnualReturn(value) \\ getMaxDrawdown(value)\n}\n```\n\n----------------------------------------\n\nTITLE: Example DolphinDB Stream Persistence Metadata Output\nDESCRIPTION: Displays the typical dictionary structure and key-value pairs returned by the `getPersistenceMeta` function, explaining the meaning of each field related to the stream table's persistence status and configuration.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_tutorial.md#_snippet_14\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//内存中的数据记录数\nsizeInMemory->0\n//启用异步持久化\nasynWrite->true\n//流数据表总记录数\ntotalSize->0\n//启用压缩存储\ncompress->true\n//当前内存中数据相对总记录数的偏移量，在持久化运行过程中遵循公式 memoryOffset = totalSize - sizeInMemory\nmemoryOffset->0\n//已经持久化到磁盘的数据记录数\nsizeOnDisk->0\n//日志文件的保留时间，默认值是1440分钟，即一天。\nretentionMinutes->1440\n//持久化路径\npersistenceDir->/hdd/persistencePath/pubTable\n//hashValue是对本表做持久化的工作线程标识。\nhashValue->0\n//磁盘上第一条数据相对总记录数的偏移量。例如，若diskOffset=10000，表示目前磁盘上的持久化流数据从第10000条记录开始。\ndiskOffset->0\n```\n\n----------------------------------------\n\nTITLE: Chaining DolphinDB Stream Engines for Pipeline Processing\nDESCRIPTION: Sets up a pipeline by connecting the output of a Reactive State Engine to the input of a Cross-Sectional Aggregator. This allows for multi-stage calculations where the result of one engine is processed by another, often used for complex factor computations.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_tutorial.md#_snippet_18\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//创建横截面引擎，计算每个股票的rank\ndummy = table(1:0, `sym`time`maxIndex, [SYMBOL, TIMESTAMP, DOUBLE])\nresultTable = streamTable(10000:0, `time`sym`factor1, [TIMESTAMP, SYMBOL, DOUBLE])\nccsRank = createCrossSectionalAggregator(name=\"alpha1CCS\", metrics=<[sym, rank(maxIndex, percent=true) - 0.5]>,  dummyTable=dummy, outputTable=resultTable,  keyColumn=`sym, triggeringPattern='keyCount', triggeringInterval=3000, timeColumn=`time, useSystemTime=false)\n\n@state\ndef wqAlpha1TS(close){\n    ret = ratios(close) - 1\n    v = iif(ret < 0, mstd(ret, 20), close)\n    return mimax(signum(v)*v*v, 5)\n}\n\n//创建响应式状态引擎，输出到前面的横截面引擎ccsRank\ninput = table(1:0, `sym`time`close, [SYMBOL, TIMESTAMP, DOUBLE])\nrse = createReactiveStateEngine(name=\"alpha1\", metrics=<[time, wqAlpha1TS(close)]>, dummyTable=input, outputTable=ccsRank, keyColumn=\"sym\")\n```\n\n----------------------------------------\n\nTITLE: Registering Missing Data Filling Stream Engine (Stream Engine 3) in DolphinDB Script\nDESCRIPTION: This snippet defines the calculations applied in the stream engine to fill missing K-line data for low-activity stocks or funds. It uses conditional null filling and previous close price substitutions to ensure consistent OHLC values for minutes lacking snapshot data. The reactive state engine is created with the defined metrics and outputs to the previously created 1-minute OHLC stream table. The dummy table simulates input for engine registration.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/K.md#_snippet_16\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//Declare parameters\nmdlStockFundOHLCEngineName = \"mdlStockFundOHLCEngine\"\n//Define engine calculation methods\nconvert = <[\n\tTradeTime,\n\tiif(OpenPrice==0, ClosePrice, OpenPrice).nullFill(0.0),\n\tiif(HighPrice==0, ClosePrice, HighPrice).nullFill(0.0),\n\tiif(LowPrice==0, ClosePrice, LowPrice).nullFill(0.0),\n\tClosePrice.nullFill(0.0),\n\tVolume,\n\tTurnover,\n\tTradesCount,\n\tPreClosePrice,\n\tPreCloseIOPV.nullFill(0.0),\n\tIOPV.nullFill(0.0),\n\tUpLimitPx,\n\tDownLimitPx,\n\tiif(time(TradeTime)==09:30:00.000, FirstBarChangeRate, iif(ratios(ClosePrice)!=NULL, ratios(ClosePrice)-1, 0)).nullFill(0.0)\n]>\n//Create ReactiveStateEngine: mdlStockFundOHLCEngineName\ncreateReactiveStateEngine(\n\tname=mdlStockFundOHLCEngineName,\n\tmetrics =convert,\n\tdummyTable=getMDLStockFundOHLCTempTB(1),\n\toutputTable=objByName(mdlStockFundOHLCTBName),\n\tkeyColumn=\"SecurityID\",\n\tkeepOrder = true)\n```\n\n----------------------------------------\n\nTITLE: Calculating Moving Average Temperature in DolphinDB Script\nDESCRIPTION: This script calculates the moving average of motor bearing temperature over a specified window. It selects temperature data ('propertyValue') for a specific device and point from the 'pt' table within a time range and applies the 'mavg' function with a window size of 2 to compute the moving average.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Iot_intelligent_O&M.md#_snippet_7\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\n//案例 7：计算指定时间段范围内（窗口时间）电机轴承的平均温度\ndevice=\"361RP17\"     //设备编号\npoint=\"361RP17008\"   //测点编号，记录电机轴承温度的测点，得到的 propertyValue 表示温度\n//输出\nselect *,mavg(propertyValue,2) from pt where deviceCode=device and propertyCode=point and ts between 2022.01.01 00:00:00 : 2022.01.01 01:59:59\n```\n\n----------------------------------------\n\nTITLE: Using Field Series with Macro Variables\nDESCRIPTION: This snippet uses both multi-column macro variables and a field series, showing an alternative way to select multiple columns from a table based on a defined range. It illustrates how the field series can be used as a substitute for `_$$names`.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/macro_var_based_metaprogramming.md#_snippet_18\n\nLANGUAGE: DolphinDB\nCODE:\n```\nnames = [col1, col2, ..., coln]\n<select _$$names from t>\n<select col1 ... coln from t>\n```\n\n----------------------------------------\n\nTITLE: Configuring odbc.ini for Oracle DSN - Shell\nDESCRIPTION: Specifies an ODBC data source configuration for connecting to Oracle in /etc/odbc.ini. Includes: DSN name, description, driver reference (matching odbcinst.ini), username, password, server name, and database name. Must be customized per deployment for credentials and environment.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Oracle_to_DolphinDB.md#_snippet_4\n\nLANGUAGE: Shell\nCODE:\n```\n[orac]\nDescription = odbc for oracle\nDriver      = ORAC21c\nUserID      = system\nPassword    = dolphindb123\nServerName = ORAC21\nDatabase  = test\n\n```\n\n----------------------------------------\n\nTITLE: Defining Replay Job - DolphinDB\nDESCRIPTION: This DolphinDB code defines the `replayJob` function, which manages the submission of data replay tasks. It takes input parameters `inputDict`, `tabName`, `dateDict`, `timeDict`, `replayRate`, and `sortColumn`. It calls the internal `replay` function. The `sortColumn` parameter is included to allow sorting data based on the given column. The function may call `createEnd` function to generate the end signal of the replay. Prerequisites involve having the `replay` and `objByName` functions available within the DolphinDB environment and correctly setup the input dictionaries for `replay` function.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/appendices_market_replay_bp.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef replayJob(inputDict, tabName, dateDict, timeDict, replayRate, sortColumn)\n{\n    if(sortColumn == \"NULL\")\n    {\n        replay(inputTables=inputDict, outputTables=objByName(tabName), dateColumn=dateDict, timeColumn=timeDict, replayRate=int(replayRate), absoluteRate=false, parallelLevel=23)\n    }\n    else\n    {\n        replay(inputTables=inputDict, outputTables=objByName(tabName), dateColumn=dateDict, timeColumn=timeDict, replayRate=int(replayRate), absoluteRate=false, parallelLevel=23, sortColumns=sortColumn)    \n    }\n    createEnd(tabName, sortColumn)\n}\n```\n\n----------------------------------------\n\nTITLE: Querying - Group by, Aggregation (Multi-Dimension) DolphinDB\nDESCRIPTION: This snippet calculates the average battery temperature, grouped by device ID, date, and hour.  It utilizes multiple group by dimensions, which demonstrates the ability to analyze trends at a more granular level.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs TimescaleDB/test_dolphindb_small.txt#_snippet_17\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 8. 聚合查询.多分区维度.avg：计算各时间段内设备电池平均温度\ntimer\nselect avg(battery_temperature)\nfrom readings\ngroup by device_id, date(time), hour(time)\n```\n\n----------------------------------------\n\nTITLE: Measuring Parallel Import Time in DolphinDB\nDESCRIPTION: This script retrieves the status of recently completed background jobs using `getRecentJobs`. It filters for jobs whose IDs match the pattern used in the parallel import submission (e.g., \"loadData10%\") and calculates the total time elapsed for the parallel import by subtracting the minimum start time from the maximum end time across all related jobs.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_csv.md#_snippet_9\n\nLANGUAGE: dolphindb\nCODE:\n```\nselect max(endTime) - min(startTime) from getRecentJobs() where jobId like (\"loadData\"+string(parallelLevel)+\"%\");\n```\n\n----------------------------------------\n\nTITLE: Creating Overlapping 5-Minute K-line Charts Updated Every Minute in DolphinDB\nDESCRIPTION: Implements a time series aggregator with overlapping windows. It calculates 5-minute K-line data but updates every minute, creating a rolling window effect. The windowSize is set to 300 seconds while the step is 60 seconds.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/quant_finance_examples.md#_snippet_23\n\nLANGUAGE: DolphinDB\nCODE:\n```\nmodal = table(100:0, `symbol`datetime`last`askPrice1`bidPrice1`askVolume1`bidVolume1`volume, [SYMBOL,DATETIME,DOUBLE,DOUBLE,DOUBLE,INT,INT,INT])\nshare streamTable(100:0, `datetime`symbol`open`high`low`close`volume,[DATETIME,SYMBOL,DOUBLE,DOUBLE,DOUBLE,DOUBLE,LONG]) as OHLC2\ntsAggr2 = createTimeSeriesAggregator(name=\"tsAggr2\", windowSize=300, step=60, metrics=<[first(last),max(last),min(last),last(last),sum(volume)]>, dummyTable=modal, outputTable=OHLC2, timeColumn=`datetime, keyColumn=`symbol)\nsubscribeTable(tableName=\"level2\", actionName=\"act_tsAggr2\", offset=0, handler=append!{tsAggr2}, msgAsTable=true);\n```\n\n----------------------------------------\n\nTITLE: Calculate Dual Moving Average Crossover Signals in Python with DolphinDB and Pandas\nDESCRIPTION: Loads daily adjusted stock price data from a DolphinDB table ('dfs://Daily_adj_price') into a Pandas DataFrame. Defines and applies a function `signal_ma` to calculate short-term (5-day) and long-term (20-day) simple moving averages ('ma_5', 'ma_20') and their previous values ('pre_ma5', 'pre_ma20'). It then identifies golden crosses (signal=1) and death crosses (signal=-1) based on the relative positions of these moving averages, grouping the results by 'SECURITY_ID'. Requires `pandas` and `dolphindb` libraries.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_Python_Parser_Intro_for_Quantitative_Finance.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nimport pandas as pd\nimport dolphindb as ddb\n\n# 调用数据\ndf = loadTable(\"dfs://Daily_adj_price\", \"data\")\ndf = pd.DataFrame(df, \"TRADE_DATE\", True)\n\n# 定义计算 ma 金叉死叉信号生成的方法\ndef signal_ma(data_chunk, short, long):\n    #分别计算出5日和20日均线及其前一根均线\n    data_chunk['ma_5'] = data_chunk['CLOSE_PRICE_1'].fillna(0).rolling(int(short)).mean()\n    data_chunk['ma_20'] = data_chunk['CLOSE_PRICE_1'].fillna(0).rolling(int(long)).mean()\n    data_chunk['pre_ma5'] = data_chunk['ma_5'].shift(1)\n    data_chunk['pre_ma20'] =  data_chunk['ma_20'].shift(1)\n    # 通过df[contion]方式按列进行条件，判断出金叉死叉信号\n    data_chunk['signal'] = 0\n    data_chunk.loc[((data_chunk.loc[:,'pre_ma5']< data_chunk.loc[:,'pre_ma20'])& (data_chunk.loc[:,'ma_5'] > data_chunk.loc[:,'ma_20'])), \"signal\"] = 1\n    data_chunk.loc[((data_chunk.loc[:,'pre_ma5']> data_chunk.loc[:,'pre_ma20']) & (data_chunk.loc[:,'ma_5'] < data_chunk.loc[:,'ma_20'])), \"signal\"] = -1\n    return data_chunk\n\n# 生成信号\ncombined_results = df.groupby('SECURITY_ID').apply(signal_ma,5,20)\n```\n\n----------------------------------------\n\nTITLE: Concatenating Date and Time in DolphinDB Shell\nDESCRIPTION: This snippet demonstrates the use of `concatDateTime` function to merge date and time values into a single DATETIME or TIMESTAMP object in DolphinDB. It accepts date(s) and time(s) as input, returning a single DATETIME or TIMESTAMP value.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/date_time.md#_snippet_5\n\nLANGUAGE: Shell\nCODE:\n```\n>concatDateTime(2019.06.15,13:25:10);\n2019.06.15T13:25:10\n>concatDateTime(2019.06.15 2019.06.16 2019.06.17,[13:25:10, 13:25:12, 13:25:13]);\n[2019.06.15T13:25:10,2019.06.16T13:25:12,2019.06.17T13:25:13]\n```\n\n----------------------------------------\n\nTITLE: Loading Processed Entrust Data CSV Files into DolphinDB Partitioned Table - DolphinDB\nDESCRIPTION: Defines a module and function to load entrust data CSV files into a DolphinDB database table over a specified date range. The function deletes existing data in the target partition if present, checks CSV file existence, loads and processes the CSV using the schema and processing functions, logs information including duplication counts, and appends data in 20-minute intervals to handle large volumes. Requires correct file path, database, and table names as inputs and mutable info table for logging.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/dolphinscheduler_integration.md#_snippet_12\n\nLANGUAGE: DolphinDB\nCODE:\n```\nmodule stockData::stockDataLoad\nuse stockData::stockDataProcess\n\ndef loadEntrust(userName, userPassword, startDate, endDate, dbName, tbName, filePath, loadType,mutable infoTb)\n{\n\tfor(loadDate in startDate..endDate)\n\t{\n\t\t// 删除已有数据\n\t\tdateString = temporalFormat(loadDate,\"yyyyMMdd\")\n\t\tdataCount = exec count(*) from loadTable(dbName, tbName) where date(tradeTime)=loadDate\n\t\t// 如果表里面已经存在当天要处理的数据，删除库里面已有数据\n\t\tif(dataCount != 0){\n\t\t\tmsg = \"Start to delete the entrust data, the delete date is: \" + dateString\n\t\t\tprint(msg)\n\t\t\tinfoTb.tableInsert(msg)\n\n\t\t\tdropPartition(database(dbName), loadDate, tbName)\n\t\t\tmsg = \"Successfully deleted the entrust data, the delete date is: \" + dateString\n\t\t\tprint(msg)\n\t\t\tinfoTb.tableInsert(msg)\n\t\t}\n\t\t// 数据导入\n\t\t// 判断数据csv文件是否存在\n\t\tfileName = filePath + \"/\" + dateString + \"/\" + \"entrust.csv\"\n\t\tif(!exists(fileName))\n\t\t{\n\t\t\tthrow fileName + \"不存在!请检查数据源!\"\n\t\t}\n\t\t// 如果是全市场数据，数据量较大，因此分批导入\n\t\tschemaTB = schemaEntrust()\n\t\ttmpData1 = loadText(filename=fileName, schema=schemaTB)\n\t\ttmpData1,n1,n2 = processEntrust(loadDate,tmpData1)\n\t\tpt = loadTable(dbName,tbName)\n\t\tmsg = \"the data size in csv file is :\" + n2 + \", the duplicated count is \" + (n1 - n2)\n\t\tprint(msg)\n\t\tinfoTb.tableInsert(msg)\n\t\tfor(i in 0..23)\n\t\t{\n\t\t\tstartTime = 08:00:00.000 + 1200 * 1000 * i\n\t\t\ttmpData2 = select * from tmpData1 where time(TradeTime)>=startTime and time(TradeTime)<(startTime+ 1200 * 1000)\n\t\t\tif(size(tmpData2) < 1)\n\t\t\t{\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t//数据入库\n\t\t\tpt.append!(tmpData2)\n\t\t}\n\t\tmsg = \"successfully loaded!\"\n\t\tprint(msg)\n\t\tinfoTb.tableInsert(msg)\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Aggregated Simulation Results in DolphinDB Script\nDESCRIPTION: This snippet demonstrates final querying of the aggregated results table after simulation, computing the total traded volume per stock where the order status is not 'cancelled' or 'invalid'. Depends on 'tradeResult' being filled with trade output records merged from all simulation threads; input fields must include at least 'Symbol', 'VolumeTraded', and 'OrderStatus'. The query groups trades by stock, filters by order status, and sorts by symbol.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/matching_engine_simulator.md#_snippet_10\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect Symbol, sum(VolumeTraded) from tradeResult where OrderStatus !=2 and OrderStatus != 3 group by Symbol order by Symbol\n```\n\n----------------------------------------\n\nTITLE: Cleaning Up DolphinDB Stream Environment\nDESCRIPTION: Defines and executes the `cleanEnvironment` function to remove existing stream tables (`snapshotStream`, `aggrFeatures10min`, `result1min`), subscriptions (`aggrFeatures10min`, `predictRV`), and the stream engine (`aggrFeatures10min`). It uses try-catch blocks to prevent errors if resources don't exist, ensuring a clean state. It also undefines all variables.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/machine_learning_volatility/06.streamComputingReproduction.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//clean up environment\ndef cleanEnvironment(){\n\ttry{ unsubscribeTable(tableName=`snapshotStream, actionName=\"aggrFeatures10min\") } catch(ex){ print(ex) }\n\ttry{ unsubscribeTable(tableName=`aggrFeatures10min, actionName=\"predictRV\") } catch(ex){ print(ex) }\n\ttry{ dropStreamEngine(\"aggrFeatures10min\") } catch(ex){ print(ex) }\n\ttry{ dropStreamTable(`snapshotStream) } catch(ex){ print(ex) }\n\ttry{ dropStreamTable(`aggrFeatures10min) } catch(ex){ print(ex) }\n\ttry{ dropStreamTable(`result1min) } catch(ex){ print(ex) }\n\tundef all\n}\ncleanEnvironment()\ngo\n```\n\n----------------------------------------\n\nTITLE: Data Types\nDESCRIPTION: This section specifies the data types supported in the DolphinDB API. The supported types include: BOOL, CHAR, SHORT, INT, LONG, DATE, MONTH, TIME, MINUTE, SECOND, DATETIME, TIMESTAMP, NANOTIME, NANOTIMESTAMP, FLOAT, DOUBLE, STRING, etc. It provides their length and example usages.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/api_protocol.md#_snippet_11\n\nLANGUAGE: text\nCODE:\n```\n数据类型 | 长度 | 说明 | 样例\n---|---|---|---\nBOOL|1|1b, 0b | true, false\nCHAR|1|-2^7+1 ~ 2^7-1|'a', 97c\nSHORT|2|-2^15+1 ~ 2^15-1|122h\nINT|4|-2^31+1 ~ 2^31-1|22\nLONG|8|-2^63+1 ~ 2^63-1|22l\nDATE|4|INT|2013.06.13\nMONTH|4|INT|2012.06M\nTIME|4|INT|13:30:10.008\nMINUTE|4|INT|13:30m\nSECOND|4|INT|13:30:10\nDATETIME|4|INT|2012.06.13 13:30:10 or 2012.06.13T13:30:10\nTIMESTAMP|8|LONG|2012.06.13 13:30:10.008 or 2012.06.13T13:30:10.008\nNANOTIME|8|LONG|13:30:10.008007006\nNANOTIMESTAMP|8|LONG|2012.06.13 13:30:10.008007006 or 2012.06.13T13:30:10.008007006\nFLOAT|4||2.1f\nDOUBLE|8||2.1\nSTRING|不固定|采用UTF8编码，每个字符串用0做终止符|\"String Hello World0\"\n```\n\n----------------------------------------\n\nTITLE: Distributed SQL with Context By for Time-Series Factor\nDESCRIPTION: This snippet demonstrates calculating a time-series factor (dayReturnSkew) using distributed SQL with the `context by` clause. The data is grouped by `tradetime` and `securityid`, allowing for parallel computation within each group. The `map` keyword accelerates result output.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practice_for_factor_calculation.md#_snippet_30\n\nLANGUAGE: DolphinDB\nCODE:\n```\nminReturn = select `dayReturnSkew as factorname, dayReturnSkew(close) as val from loadTable(\"dfs://k_minute_level\", \"k_minute\") where date(tradetime) between 2020.01.02 : 2020.01.31 group by date(tradetime) as tradetime, securityid map\n```\n\n----------------------------------------\n\nTITLE: Time Multi-Record 2 Column Update\nDESCRIPTION: This code snippet measures the time taken to update a set of records, in a loop. It updates 2 columns ('tag1' and 'tag5') of multiple records based on `id` and `datetime`.  The `timer` function is used to get execution time, for performance analysis.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/dolphindb_update.md#_snippet_14\n\nLANGUAGE: DolphinDB\nCODE:\n```\nmachines = loadTable(\"dfs://olapDemo\", \"machines\")\ntimer{\nfor(i in 0..20)\n update machines set tag1=i,tag5=i where id in 1..5,date(datetime)=2020.09.01\n}\n```\n\n----------------------------------------\n\nTITLE: 查询与降采样地震历史数据 DolphinDB 脚本\nDESCRIPTION: 该DolphinDB脚本实现了针对特定台网、台站及通道的地震数据历史查询测试。包括case1中的一分钟粒度降采样查询，通过时间戳和ID过滤实现查询优化；case2则针对事件发生前后时间区间的多台站数据查询。依赖于DolphinDB环境，loadTable函数加载远程分布式表，timer用于性能计时。输入为网络标识(netAim),台站(staAim),及位置(locAim)，输出为按时间和id聚合的观测数据。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Seismic_waveform_data_storage.md#_snippet_9\n\nLANGUAGE: DolphinDB\nCODE:\n```\nnetAim,staAim,locAim = `ZJ,`A0003, `40  \t//查询case的查询条件\n// case1\ntimer(10){\t\n\taim  = exec id from loadTable(\"dfs://real\",\"tagInfo\") where net = netAim and  sta = staAim and loc = locAim\n\tt = select avg(value)  as sample_value from loadTable(\"dfs://real\",\"realData\") where ts >= 2023.03.02T00:00:00.000 and ts < 2023.03.03T00:00:00.000 and id in  aim   group by bar(ts,60000),id\n}\n\n// case2\ntimer(10){\n\taim = exec id from loadTable(\"dfs://real\",\"tagInfo\") where net = netAim\n\tt = select *  from loadTable(\"dfs://real\",\"realData\") where ts >= 2023.03.02T00:08:00.000 and ts <= 2023.03.02T00:21:00.000 and id in aim\t \t\n}\n```\n\n----------------------------------------\n\nTITLE: Startup Script Configuration - DolphinDB\nDESCRIPTION: This configuration details how to set up a startup script to automatically subscribe to AMD real-time data upon node startup. The `startup` configuration parameter specifies the location of the startup script (`startup.dos`), either in the `dolphindb.cfg` file for single node mode, or in `cluster.cfg` for a cluster setup. The script then can include commands and functions that subscribe to AMD.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_implementation_for_AMD_Plugin.md#_snippet_15\n\nLANGUAGE: DolphinDB\nCODE:\n```\nstartup=/DolphinDB/server/startup.dos\n```\n\n----------------------------------------\n\nTITLE: Creating Sample Data for IoT Sensor Aggregation\nDESCRIPTION: This snippet creates a sample table `t` representing data from three IoT sensors (`id1`, `id2`, `id3`). The table has columns `id` (sensor ID), `time` (timestamp), and `value` (sensor reading).\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_sql_cases.md#_snippet_29\n\nLANGUAGE: DolphinDB\nCODE:\n```\nN = 10000\nt = table(take(`id1`id2`id3, N) as id, \n          rand(2021.01.01T00:00:00.000 +  100000 * (1..10000), N) as time, \n          rand(10.0, N) as value)\n```\n\n----------------------------------------\n\nTITLE: Creating Futures Daily K-Line Table Using DolphinDB\nDESCRIPTION: This DolphinDB snippet creates a database and table for storing futures daily K-line data. The database is partitioned by yearly ranges over time (time dimension by year) to include multiple years, while the table stores typical daily K-line fields such as security ID, timestamp, open/close/high/low prices, volume, and value. The storage engine used is OLAP for analytical processing. Partitioning is done by the tradetime timestamp, grouping all daily futures data for each year in one partition.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practices_for_partitioned_storage.md#_snippet_13\n\nLANGUAGE: DolphinDB\nCODE:\n```\ncreate database \"dfs://ctp_k_day_level\"\npartitioned by RANGE(2000.01M + (0..30)*12)\nengine='OLAP'\n\ncreate table \"dfs://ctp_k_day_level\".\"ctp_k_day\"(\n\tsecurityid SYMBOL  \n\ttradetime TIMESTAMP\n\topen DOUBLE        \n\tclose DOUBLE       \n\thigh DOUBLE        \n\tlow DOUBLE\n\tvol INT\n\tval DOUBLE\n\tvwap DOUBLE\n)\npartitioned by tradetime\n```\n\n----------------------------------------\n\nTITLE: Scheduling Incremental Data Synchronization via DolphinDB Scheduler - DolphinDB Script\nDESCRIPTION: Defines a scheduled DolphinDB job to synchronize previous day's data from SQL Server daily at 00:05. The function connects to SQL Server via ODBC, formulates an SQL select with date condition, and loads the filtered data incrementally into DolphinDB. The scheduleJob function registers this synchronization job with start/end dates and daily frequency. It requires the ODBC plugin to be preloaded to avoid scheduling failures on node restarts.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/SQLServer_to_DolphinDB.md#_snippet_8\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef synchronize(){\n\tlogin(`admin,`123456)\n    conn =odbc::connect(\"Driver={SQLServer};Servername=sqlserver;Uid=sa;Pwd=DolphinDB;database=historyData;;\")\n    sqlStatement = \"select ChannelNo,ApplSeqNum,MDStreamID,SecurityID,SecurityIDSource,Price,OrderQty,Side,TransactTime,OrderType,LocalTime from data where TradeDate ='\" + string(date(now())-1) + \"';\"\n    odbc::query(conn,sqlStatement,loadTable(\"dfs://TSDB_Entrust\",`entrust),100000,transform)\n}\nscheduleJob(jobId=`test, jobDesc=\"test\",jobFunc=synchronize,scheduleTime=00:05m,startDate=2022.11.11, endDate=2023.01.01, frequency='D')\n```\n\n----------------------------------------\n\nTITLE: Configuring GUI memory\nDESCRIPTION: This snippet shows how to modify the GUI's startup script to increase the allocated memory.  Modifying the `-Xmx` parameter within the `gui.bat` or `gui.sh` script adjusts the maximum heap size available to the GUI. This resolves `java.lang.OutOfMemoryError`.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/client_tool_tutorial.md#_snippet_2\n\nLANGUAGE: Batchfile\nCODE:\n```\nstart javaw -classpath dolphindb.jar;dolphingui.jar;jfreechart-1.0.1.jar;jcommon-1.0.0.jar;jxl-2.6.12.jar;rsyntaxarea.jar;autocomplete.jar -Dlook=cross -Xmx4096m com.xxdb.gui.XXDBMain\n```\n\n----------------------------------------\n\nTITLE: Executing Function to Create Order Table in DolphinDB\nDESCRIPTION: This DolphinDB script defines the database name and table name as variables. It then calls the `orderCreate` function, passing these variables to initiate the creation of the specified partitioned database and table structure.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/appendices_market_replay_bp/order_create.txt#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndbName = \"dfs://Test_order\"\ntbName = \"order\"\norderCreate(dbName, tbName)\n```\n\n----------------------------------------\n\nTITLE: Creating Database for Shanghai Stock Exchange Order-by-Order Data with TSDB Engine\nDESCRIPTION: Creates a database for storing Shanghai Stock Exchange order-by-order data using a combined partitioning strategy with date value and HASH on symbols. The schema differs slightly from Shenzhen Exchange but follows the same partitioning approach.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practices_for_partitioned_storage.md#_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\ncreate database \"dfs://split_SH_TB\"\npartitioned by VALUE(2020.01.01..2021.01.01), HASH([SYMBOL, 25])\nengine='TSDB'\n\ncreate table \"dfs://split_SH_TB\".\"split_SH_entrustTB\"(\n    DataStatus INT\n    ApplSeqNum LONG\n    ChannelNo INT\n    SecurityID SYMBOL\n    TradeDate DATE[comment=\"交易日期\", compress=\"delta\"]   \n    TradeTime TIME[comment=\"交易时间\", compress=\"delta\"]   \n    OrderType SYMBOL\n    OrderNO INT\n    Price DOUBLE\n    OrderQty LONG\n    Side SYMBOL\n    BizIndex INT\n    LocalTime TIME\n    SeqNo INT\n)\npartitioned by TradeDate, SecurityID,\nsortColumns=[`SecurityID,`TradeTime],\nkeepDuplicates=ALL\n```\n\n----------------------------------------\n\nTITLE: Flushing TSDB Cache to Disk in DolphinDB\nDESCRIPTION: The flushTSDBCache function forces completed transactions in the TSDB engine buffer to be written to the database, useful for ensuring data persistence or preparing for system maintenance.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/redoLog_cacheEngine.md#_snippet_6\n\nLANGUAGE: SQL\nCODE:\n```\nflushTSDBCache()\n```\n\n----------------------------------------\n\nTITLE: Elasticsearch Filtering by Floating-Point\nDESCRIPTION: This Python code searches Elasticsearch with a filter based on a range of floating-point values.  The code defines a query that filters records where the 'PRC' field falls within a specified range (greater than 25 and less than 35). It uses the Elasticsearch Python library to connect, execute the search, and scroll results in batches. The code uses the scroll parameter to retrieve results in batches. It assumes the 'uscsv' index exists in Elasticsearch and is populated.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_50\n\nLANGUAGE: Python\nCODE:\n```\ndef search_4():\n    es = Elasticsearch(['http://localhost:9200/'])\n    page = es.search(\n        index='uscsv',\n        doc_type='type',\n        scroll='2m',\n        size=10000,\n        body={\n            \"query\": {\n                \"constant_score\": {\n                    \"filter\": {\n                        \"range\": {\n                            \"PRC\": {\n                                \"gt\": 25,\n                                \"lt\": 35\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    )\n    sid = page['_scroll_id']\n    scroll_size = page['hits']['total']\n\n    print(sid)\n    print(scroll_size)\n    while (scroll_size > 0):\n        print(\"Scrolling...\")\n        page = es.scroll(scroll_id=sid, scroll='2m')\n        sid = page['_scroll_id']\n        scroll_size = len(page['hits']['hits'])\n        print(\"scroll size: \" + str(scroll_size))\n```\n\n----------------------------------------\n\nTITLE: Querying TopN Mean in Time-based Sliding Window - DolphinDB Script\nDESCRIPTION: Applies tmavgTopN to compute the average return for each security over a 3-minute sliding window, selecting only the top 2 volume observations. Illustrates time-based (as opposed to count-based) windowing. Requires DolphinDB table with suitable time and volume columns. Output: rolling per-stock windowed Top2 volume-mean returns indexed by minute.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_TopN.md#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect windCode, tradingTime, tmavgTopN(tradingTime, ratios(close), volume, 3m, 2, false) as tmavgTop2RatioClose from t context by windCode, date(tradingTime)\n```\n\n----------------------------------------\n\nTITLE: Defining Guotai Junan 001 Streaming Pipeline in DolphinDB Script\nDESCRIPTION: This code creates a real-time streaming factor computation pipeline for the Guotai Junan 001 alpha factor using DolphinDB's streamEngineParser. It defines the necessary input and output tables, drops pre-existing engine instances, configures the engine to process tick data with security ID and datetime as keys, and uses a keyCount based trigger at an interval of 3000 records. The pipeline enables automatic computation and updating of factors as new data arrives. Proper table and metric definition are required for correct operation.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Streaming_computing_of_financial_quantifiers.md#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 定义输入输出的表结构\ncolName = `securityID`dateTime`preClosePx`openPx`highPx`lowPx`lastPx`volume`amount`iopv`fp_Volume`fp_Amount\ncolType = [\"SYMBOL\",\"TIMESTAMP\",\"DOUBLE\",\"DOUBLE\",\"DOUBLE\",\"DOUBLE\",\"DOUBLE\",\"INT\",\"DOUBLE\",\"DOUBLE\",\"INT\",\"DOUBLE\"]\ninputTable = table(1:0, colName, colType)\nresultTable = table(10000:0, [\"securityID\", \"dateTime\", \"factor\"], [SYMBOL, TIMESTAMP, DOUBLE])\n\n// 使用 streamEngineParser 创建引擎流水线\ntry{ dropStreamEngine(\"gtja1Parser0\")} catch(ex){ print(ex) }\ntry{ dropStreamEngine(\"gtja1Parser1\")} catch(ex){ print(ex) }\ntry{ dropStreamEngine(\"gtja1Parser2\")} catch(ex){ print(ex) }\nmetrics = <[gtjaAlpha1(openPx, preClosePx, volume)]>\nstreamEngine = streamEngineParser(name=\"gtja1Parser\", metrics=metrics, dummyTable=inputTable, outputTable=resultTable, keyColumn=\"securityID\", timeColumn=`dateTime, triggeringPattern='keyCount', triggeringInterval=3000)\n\n// 查看引擎\ngetStreamEngineStat()\n/*\nReactiveStreamEngine->\nname         user  status lastErrMsg numGroups numRows numMetrics memoryInUsed snapshotDir ...\n------------ ----- ------ ---------- --------- ------- ---------- ------------ ----------- \ngtja1Parser0 admin OK                0         0       4          808                      ...\ngtja1Parser2 admin OK                0         0       2          872                      ...\n\nCrossSectionalEngine->\nname         user  status lastErrMsg numRows numMetrics metrics      triggering...triggering......\n------------ ----- ------ ---------- ------- ---------- ------------ --------------- --------------- ---\ngtja1Parser1 admin OK                0       3          securityID...keyCount     3000         ...\n*/\n```\n\n----------------------------------------\n\nTITLE: Initializing Kafka Producer - DolphinDB Script\nDESCRIPTION: Initializes a Kafka producer using specified broker metadata by setting up the producer configuration and returning a producer handle. Requires prior plugin load; expects metadata broker list as a string. Outputs a producer object for publishing messages to Kafka topics.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/stock_market_replay/04.publishToKafka.txt#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef initKafkaProducerFunc(metadataBrokerList){\n\tproducerCfg = dict(STRING, ANY)\n\tproducerCfg[\"metadata.broker.list\"] = metadataBrokerList\n\treturn kafka::producer(producerCfg)\n}\nproducer = initKafkaProducerFunc(\"localhost\")\n```\n\n----------------------------------------\n\nTITLE: Querying Job History with Conditional Logic - SQL\nDESCRIPTION: This SQL query retrieves employee job history, treating 'FI_ACCOUNT' and 'AC_ACCOUNT' job IDs as 'FI_ACCOUNT'.  It uses a CASE WHEN statement in the JOIN condition to normalize job IDs for matching. The query orders the results by employee ID.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Standard_SQL_in_DolphinDB.md#_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\nselect employee_id, j.job_id, j.job_title, j.min_salary\n  , h.start_date, h.end_date\nfrom job_history h \nleft join jobs j\non j.job_id = case when h.job_id in (\"FI_ACCOUNT\", \"AC_ACCOUNT\") then \"FI_ACCOUNT\" else h.job_id end  \norder by employee_id\n```\n\n----------------------------------------\n\nTITLE: Stopping Cluster Replication\nDESCRIPTION: This DolphinDB script stops cluster replication in either the master or the slave cluster by calling `stopClusterReplication` via the `rpc` function. Executing this on the master cluster disables new replication tasks from being added to the queue, while executing this on the slave cluster prevents it from fetching new tasks.  The function must be called from a control node. Requires a running DolphinDB cluster with replication enabled.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Asynchronous_Replication.md#_snippet_20\n\nLANGUAGE: dolphindb\nCODE:\n```\nrpc(getControllerAlias(), stopClusterReplication)\n```\n\n----------------------------------------\n\nTITLE: Deleting Memory Table with undef\nDESCRIPTION: Demonstrates deleting a memory table using the `undef` function, which removes the namespace.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/partitioned_in_memory_table.md#_snippet_26\n\nLANGUAGE: DolphinDB\nCODE:\n```\nundef(`trades)\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Weighted Moving Average and Stateful Factor Calculation - DolphinDB\nDESCRIPTION: This snippet defines a custom factor calculation in DolphinDB using window and iterative functions. It includes a weighted average calculation (myWavg), an iterative function (iterateFunc) for combining historical and current values, and a state function (myFactor) that chains moving window computations, a moving average, and a final iterative step to aggregate the factor using recent values and volumes. Dependencies include window-related DolphinDB functions such as moving, mavg, movingWindowData, and genericStateIterate, and key parameters are price/volume series and lag (window size). The outputs are computed factor series, and the code requires that aggregation functions passed to moving be defined using defg. Limitations such as state function recursion and function placement are described.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Streaming_computing_of_financial_quantifiers.md#_snippet_13\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndefg myWavg(x){\n\tweight = 1..size(x)\n\treturn wavg(x, weight)\n}\n\ndef iterateFunc(historyFactors, currentValue, weight){\n\treturn wavg(historyFactors join currentValue, weight)\n}\n\n@state\ndef myFactor(bidPrice0, bidOrderQty0, offerPrice0, offerOrderQty0, lag){\n\t// step1: 使用 moving\n\tbidPrice, askPrice, bidVolume, askVolume = moving(myWavg, bidPrice0, lag, 1), moving(myWavg, offerPrice0, lag, 1), moving(myWavg, bidOrderQty0, lag, 1), moving(myWavg, offerOrderQty0, lag, 1)\n\n\t// step2: 使用 mavg\n\twap = (bidPrice*askVolume + askPrice*bidVolume) \\ (bidVolume + askVolume)\n\tmaWap = mavg(wap, lag, 1)\n\t\n\t// step3: 使用 movingWindowData \n\tw = movingWindowData(bidVolume \\ askVolume, lag)\n\t//\t 使用 genericStateIterate\n\tfactorValue = genericStateIterate(X=[maWap, w], initial=maWap, window=lag-1, func=iterateFunc{ , , })\n\treturn factorValue\n}\n```\n\n----------------------------------------\n\nTITLE: 定义计算基金日志收益率的 `getLog()` 函数（Python）\nDESCRIPTION: 该函数通过差分和数组滚动操作，计算给定价值序列的对数收益率，主要依赖 numpy 操作。需要 numpy 作为依赖库。输入为数值数组，输出也是数组，表示每个交易日的收益率。用在因子计算中进行日收益率的得到。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/fund_factor_contrasted_by_py.md#_snippet_29\n\nLANGUAGE: Python\nCODE:\n```\ndef getLog(value):\n    diff_value = np.diff(value)\n    rolling_value = np.roll(value, 1)\n    rolling_value = np.delete(rolling_value, [0])\n    return np.insert(np.true_divide(diff_value, rolling_value), 0, np.nan)\n```\n\n----------------------------------------\n\nTITLE: Setting Stream Table Filter, Creating Aggregator, and Subscribing with DolphinDB Script\nDESCRIPTION: Configures streaming filters on the output table to reduce network and memory load, defines an output table for ETF values, initializes a cross-sectional aggregator for the ETF calculation, and subscribes the aggregator to filtered symbol data from the replay output stream. The pipeline enables real-time calculation as data streams in.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/historical_data_replay.md#_snippet_9\n\nLANGUAGE: DolphinDB\nCODE:\n```\nsetStreamTableFilterColumn(outQuotes, `symbol)\noutputTable = table(1:0, `time`etf, [TIMESTAMP,DOUBLE])\ntradesCrossAggregator=createCrossSectionalAggregator(\"etfvalue\", <[etfVal{weights}(symbol, ofr)]>, outQuotes, outputTable, `symbol, `perBatch)\nsubscribeTable(tableName=\"outQuotes\", actionName=\"tradesCrossAggregator\", offset=-1, handler=append!{tradesCrossAggregator}, msgAsTable=true, filter=`AAPL`IBM`MSFT`NTES`AMZN`GOOG) \n```\n\n----------------------------------------\n\nTITLE: Calculating First- and Second-Level Stock Indicators in DolphinDB Script\nDESCRIPTION: This DolphinDB script snippet computes first-level indicators such as weighted average price (wap), price spreads, total volume, and log returns based on matrices of bid/offer prices and quantities for a stock. Second-level indicators like wapBalance and log returns of wap are also calculated. The snippet makes extensive use of vectorized operations and matrix row-wise sums to efficiently compute these indicators from 10-level order book data. Dependencies include having matrices BidPrice, BidOrderQty, OfferPrice, and OfferOrderQty as inputs with 10 depth levels. Outputs are variables representing key market indicators required for further feature derivation.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/metacode_derived_features.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\nwap = (BidPrice * OfferOrderQty + BidOrderQty * OfferPrice) \\ (BidOrderQty + OfferOrderQty)\\nwapBalance = abs(wap[0] - wap[1])\\npriceSpread = (OfferPrice[0] - BidPrice[0]) \\ ((OfferPrice[0] + BidPrice[0]) \\ 2)\\nBidSpread = BidPrice[0] - BidPrice[1]\\nOfferSpread = OfferPrice[0] - OfferPrice[1]\\ntotalVolume = OfferOrderQty.rowSum() + BidOrderQty.rowSum()\\nvolumeImbalance = abs(OfferOrderQty.rowSum() - BidOrderQty.rowSum())\\nLogReturnWap = logReturn(wap)\\nLogReturnOffer = logReturn(OfferPrice)\\nLogReturnBid = logReturn(BidPrice)\n```\n\n----------------------------------------\n\nTITLE: Matrix Aggregation by Group Using regroup in DolphinDB\nDESCRIPTION: Demonstrates grouping a matrix by an external label (such as minute of a timestamp) and aggregating within each group using regroup and avg. Inputs: numeric matrix, timestamp vector, group labels derived from timestamps, and an aggregation function. Output: matrix of group-wise aggregations. Useful for transforming time-indexed matrices to lower frequencies or for subgroup analysis.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/panel_data.md#_snippet_28\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntimestamp = 09:00:00 + rand(10000, 1000).sort!()\nid= rand(['st1', 'st2'], 1000)\nprice = (190 + rand(10.0, 2000))$1000:2\nregroup(price, minute(timestamp), avg, true)\n```\n\n----------------------------------------\n\nTITLE: Creating and Granting VIEW_EXEC Permission - DolphinDB\nDESCRIPTION: This example creates a function view ('countTradeAll') and grants VIEW_EXEC permission to a user (NickFoles). This allows the user to execute the view without needing direct access to the underlying tables. Uses `addFunctionView` and `grant`. Requires admin privileges to create user.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Permission_Management.md#_snippet_37\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef countTradeAll(){  \n\treturn exec count(*) from loadTable(\"dfs://TAQ\",\"Trades\")  \n}\naddFunctionView(countTradeAll)  \ngrant(\"NickFoles\",VIEW_EXEC,\"countTradeAll\")  \n```\n\n----------------------------------------\n\nTITLE: 创建Table表\nDESCRIPTION: 展示两种创建Table表的方法：通过指定列名和列类型创建空表，以及通过列名和列向量创建表。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/c++api.md#_snippet_5\n\nLANGUAGE: C++\nCODE:\n```\n// 方法1：通过列名和列类型创建\nvector<string> colNames{\"col1\", \"col2\", \"col3\"};\nvector<DATA_TYPE> colTypes{DT_INT, DT_BOOL, DT_STRING};\nTableSP tbl = Util::createTable(colNames, colTypes, 0, 100);\n\n// 方法2：通过列名和列向量创建\nvector<string> colNames{\"col1\", \"col2\", \"col3\"};\nvector<ConstantSP> cols;\ncols.emplace_back(Util::createVector(DT_INT, 0));\ncols.emplace_back(Util::createVector(DT_BOOL, 0));\ncols.emplace_back(Util::createVector(DT_STRING, 0));\nTableSP tbl = Util::createTable(colNames, cols);\n```\n\n----------------------------------------\n\nTITLE: Executing Node Rebalancing in DolphinDB\nDESCRIPTION: Uses the `rpc` function to execute `rebalanceChunksAmongDataNodes` on the controller node (obtained via `getControllerAlias()`). The `{ true }` argument signifies that the `exec` parameter is set to `true`, triggering the actual rebalancing of partitions (chunks) across all data nodes in the cluster. This moves data according to the balancing algorithm, typically used after adding new nodes to distribute data onto them.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/data_move_rebalance.md#_snippet_3\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nrpc(getControllerAlias(), rebalanceChunksAmongDataNodes{ true })\n```\n\n----------------------------------------\n\nTITLE: Converting Data Encoding of a Column\nDESCRIPTION: This code snippet demonstrates how to convert the encoding of a specific column ('exchange') in a table ('tmpTB') from 'gbk' to 'utf-8' using the `convertEncode` function after loading the data from a CSV file. `replaceColumn!` is used to overwrite the original column with the converted values.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_csv.md#_snippet_36\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndataFilePath=\"/home/data/candle_201801.csv\"\ntmpTB=loadText(filename=dataFilePath, skipRows=0)\ntmpTB.replaceColumn!(`exchange, convertEncode(tmpTB.exchange,\"gbk\",\"utf-8\"));\n```\n\n----------------------------------------\n\nTITLE: Subscribing Raw Snapshot Stream Table to Processing Engine in DolphinDB Script\nDESCRIPTION: This code subscribes the raw snapshot streaming table so that its real-time incremental data is fed into the registered \"mdlSnapshotProcessEngine\" reactive state engine. Various subscription parameters control message batching, throttling, hash partitioning, and automatic reconnection to optimize real-time streaming data consumption.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/K.md#_snippet_14\n\nLANGUAGE: DolphinDB\nCODE:\n```\nsubscribeTable(\n\ttableName=mdlSnapshotTBName,\n\tactionName=mdlSnapshotProcessEngineName,\n\thandler=getStreamEngine(mdlSnapshotProcessEngineName),\n\tmsgAsTable=true,\n\tbatchSize=100,\n\tthrottle=0.002,\n\thash=0,\n\treconnect=true)\n```\n\n----------------------------------------\n\nTITLE: Defining Parameterized ETL Task Function View for Entrust Data Import - DolphinDB Script\nDESCRIPTION: This function view defines a reusable DolphinDB function 'loadEntrustFV' to handle both daily and batch ETL tasks for importing entrust data, controlled via parameters such as username, password, date range, database/table names, file path, and load type. It initializes a log table to collect runtime info, and based on 'loadType', calls the underlying data load function with appropriate date parameters. The function returns a table capturing execution logs. It is designed for easy scheduling and integration with DolphinScheduler task nodes.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/dolphinscheduler_integration.md#_snippet_18\n\nLANGUAGE: DolphinDB\nCODE:\n```\nuse stockData::stockDataLoad\n// 定义函数\ndef loadEntrustFV(userName=\"admin\" , userPassword=\"123456\", startDate = 2023.02.01, endDate = 2023.02.01, dbName = \"dfs://stockData\", tbName = \"entrust\", filePath = \"/hdd/hdd8/ymchen\", loadType = \"daily\")\n{\n\tinfoTb = table(1:0,[\"info\"] ,[STRING])\n\tif(loadType == \"daily\")\n\t{\n\t\tsDate = today()\n\t\teDate = today()\n\t\tloadEntrust(userName, userPassword, sDate, eDate, dbName, tbName, filePath, loadType,infoTb)\n\t}\n\telse if(loadType == \"batch\")\n\t{\n\t\tloadEntrust(userName, userPassword, date(startDate), date(endDate), dbName, tbName, filePath, loadType,infoTb)\n\t}\n\treturn infoTb\n}\n```\n\n----------------------------------------\n\nTITLE: Matrix Inversion - DolphinDB\nDESCRIPTION: This code demonstrates how to calculate the inverse of a matrix in DolphinDB using the `inverse()` method. The input matrix must be square and invertible (non-singular).\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/matrix.md#_snippet_8\n\nLANGUAGE: DolphinDB\nCODE:\n```\n>x=1..4$2:2;\n>x;\n#0 #1\n-- --\n1  3\n2  4\n\n>x.inverse();\n#0 #1\n-- --\n-2 1.5\n1  -0.5\n```\n\n----------------------------------------\n\nTITLE: Establishing Connection to AMD Market Data Server Using amdQuote::connect in DolphinDB\nDESCRIPTION: This snippet connects the DolphinDB server to the AMD market data server using amdQuote::connect, passing credentials, IP addresses, ports, and an optional dictionary of flags for extended options such as adding received timestamps or daily indexes. It returns a handle used for subsequent subscriptions and data operations. The 'option' dictionary enables configuration of added columns to the subscription data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_implementation_for_AMD_Plugin.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 连接行情服务器\noption = dict(STRING, ANY)\noption[`ReceivedTime] = true\noption[`DailyIndex] = true\nhandle = amdQuote::connect(\n    \"myUsername\", \n    \"myPassword\", \n    [\"110.110.10.10\"], \n    [7890], \n    option\n)\n```\n\n----------------------------------------\n\nTITLE: Creating and Using a Reactive State Engine in DolphinDB\nDESCRIPTION: This snippet demonstrates how to create and use a reactive state engine in DolphinDB. It defines input and output table structures, creates the engine with metrics, inserts data, and shows the results that highlight the data type limitations.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Streaming_computing_of_financial_quantifiers.md#_snippet_12\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 定义输入输出的表结构\ninputTable = table(1:0, `securityID`tradeTime`price1`price2`price3, [SYMBOL,DATETIME,DOUBLE,DOUBLE,DOUBLE])\nresultTable = table(10000:0, [\"securityID\", \"tradeTime\", \"sum\", \"avg\", \"sum_avg\", \"anyVector\"], [SYMBOL, TIMESTAMP, DOUBLE, DOUBLE,DOUBLE[], DOUBLE[]])\n\n// 使用 createReactiveStateEngine 创建响应式状态引擎\ntry{ dropStreamEngine(\"reactiveDemo\")} catch(ex){ print(ex) }\nmetrics = <[tradeTime, typeTestStateFunc(price1, price2, price3, 10) as `sum`avg`sum_avg`anyVector]>\nrse = createReactiveStateEngine(name=\"reactiveDemo\", metrics =metrics, dummyTable=inputTable, outputTable=resultTable, keyColumn=\"securityID\")\n\n// 输入数据\ninsert into rse values(`000155, 2020.01.01T09:30:00, 30.81, 30.82, 30.83)\ninsert into rse values(`000155, 2020.01.01T09:30:01, 30.86, 30.87, 30.88)\ninsert into rse values(`000155, 2020.01.01T09:30:02, 30.80, 30.81, 30.82)\n\n// 查看结果\nselect * from resultTable\n/*\nsecurityID tradeTime               sum    avg    sum_avg                 anyVector\n---------- ----------------------- ------ ------ ----------------------- ---------\n000155     2020.01.01T09:30:00.000 92.46  30.82  [92.46,30.82]           [00F]    \n000155     2020.01.01T09:30:01.000 92.61  30.87  [92.61,30.87]           [00F]    \n000155     2020.01.01T09:30:02.000 92.43  30.81  [92.43,30.81]           [00F]    \n*/\n```\n\n----------------------------------------\n\nTITLE: Addax Migration Task Configuration for InfluxDB to DolphinDB\nDESCRIPTION: JSON configuration file for Addax that defines the data migration task from InfluxDB to DolphinDB. It includes reader configuration for InfluxDB, writer configuration for DolphinDB, and custom functions for time format conversion.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Migrate_data_from_InfluxDB_to_DolphinDB.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"job\": {\n    \"content\": {\n      \"reader\": {\n        \"name\": \"influxdb2reader\",\n        \"parameter\": {\n          \"column\": [\"*\"],\n          \"connection\": [\n            {\n              \"endpoint\": \"http://183.136.170.168:8086\",\n              \"bucket\": \"demo-bucket\",\n              \"table\": [\n                  \"machinery\"\n              ],\n              \"org\": \"zhiyu\"\n            }\n          ],\n          \"token\": \"GLiPjQFQIxzVO0-atASJHH4b075sTlyEZGrqW20XURkelUT5pOlfhi_Yuo2fjcSKVZvyuO00kdXunWPrpJd_kg==\",\n          \"range\": [\n            \"2007-08-09\"\n          ]\n        }\n      },\n      \"writer\": {\n        \"name\": \"dolphindbwriter\",\n        \"parameter\": {\n          \"userId\": \"admin\",\n          \"pwd\": \"123456\",\n          \"host\": \"115.239.209.122\",\n          \"port\": 3134,\n          \"dbPath\": \"dfs://demo\",\n          \"tableName\": \"pt\",\n          \"batchSize\": 1000000,\n          \"saveFunctionName\": \"transData\",\n          \"saveFunctionDef\": \"def parseRFC3339(timeStr) {if(strlen(timeStr) == 20) {return localtime(temporalParse(timeStr,'yyyy-MM-ddTHH:mm:ssZ'));} else if (strlen(timeStr) == 24) {return localtime(temporalParse(timeStr,'yyyy-MM-ddTHH:mm:ss.SSSZ'));} else {return timeStr;}};def transData(dbName, tbName, mutable data) {timeCol = exec time from data; timeCol=each(parseRFC3339, timeCol);  writeLog(timeCol);replaceColumn!(data,'time', timeCol); loadTable(dbName,tbName).append!(data); }\",\n          \"table\": [\n              {\n                \"type\": \"DT_STRING\",\n                \"name\": \"time\"\n              },\n              {\n                \"type\": \"DT_SYMBOL\",\n                \"name\": \"stationID\"\n              },\n              {\n                \"type\": \"DT_DOUBLE\",\n                \"name\": \"grinding_time\"\n              },\n              {\n                \"type\": \"DT_DOUBLE\",\n                \"name\": \"oil_temp\"\n              },\n              {\n                \"type\": \"DT_DOUBLE\",\n                \"name\": \"pressure\"\n              },\n              {\n                \"type\": \"DT_DOUBLE\",\n                \"name\": \"pressure_target\"\n              },\n              {\n                \"type\": \"DT_DOUBLE\",\n                \"name\": \"rework_time\"\n              },\n              {\n                \"type\": \"DT_SYMBOL\",\n                \"name\": \"state\"\n              }\n          ]\n        }\n      }\n    },\n    \"setting\": {\n      \"speed\": {\n        \"bytes\": -1,\n        \"channel\": 1\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Distributed SQL for Stateless Factor\nDESCRIPTION: This snippet demonstrates using distributed SQL to calculate a stateless factor. It selects `TradeTime`, `SecurityID`, calculates the `mathWghtSkew` factor, and retrieves the results from a distributed table. The computation will be automatically parallelized across partitions of the distributed table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practice_for_factor_calculation.md#_snippet_29\n\nLANGUAGE: DolphinDB\nCODE:\n```\nresWeight =  select TradeTime, SecurityID, `mathWghtSkew as factorname, mathWghtSkew(BidPrice, w)  as val from loadTable(\"dfs://LEVEL2_Snapshot_ArrayVector\",\"Snap\")  where date(TradeTime) = 2020.01.02\n```\n\n----------------------------------------\n\nTITLE: Environment Cleanup Script - DolphinDB\nDESCRIPTION: This script performs environment cleanup tasks, including canceling background jobs, unsubscribing from tables, dropping stream tables and engines, clearing cache, and dropping databases. It ensures a clean environment before running the project. The script aims to remove all existing streaming tables, engines and data related to previous runs of the project.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/knn_iot.md#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//登录\nlogin(`admin,`123456)\nversion()\n\n//取消后台任务\ndef cancelAllBatchJob(){\njobids=exec jobid from getRecentJobs() where endTime=NULL\ncancelJob(jobids)\n}\npnodeRun(cancelAllBatchJob)\n\nt = getStreamingStat().pubTables\nfor(row in t){\n\ttableName = row.tableName\n  if(row.actions[0]=='[') actions = split(substr(row.actions, 1, (strlen(row.actions)-2)), \",\")\n\telse actions = split(substr(row.actions, 0, strlen(row.actions)), \",\")\n\tfor(action in actions){\n\t\tunsubscribeTable(tableName=tableName, actionName=action)\n\t}\n}\n\n//删除流表\ntry{dropStreamTable(`dataTable)}catch(ex){}\ntry{dropStreamTable(`aggrTable)}catch(ex){}\ntry{dropStreamTable(`predictTable)}catch(ex){}\ntry{dropStreamTable(`warningTable)}catch(ex){}\n\n//删除计算引擎\ntry{dropStreamEngine(\"streamAggr\")}catch(ex){}\n\n//清理缓存\nundef(all)\nclearAllCache()\n\n//清空数据库，环境初始化\nif(existsDatabase(\"dfs://Data\")){dropDatabase(\"dfs://Data\")}\nif(existsDatabase(\"dfs://Aggregate\")){dropDatabase(\"dfs://Aggregate\")}\nif(existsDatabase(\"dfs://predict\")){dropDatabase(\"dfs://predict\")}\nif(existsDatabase(\"dfs://warning\")){dropDatabase(\"dfs://warning\")}\n```\n\n----------------------------------------\n\nTITLE: Reading Symbol Vector Data with getInt and SymbolBase C++\nDESCRIPTION: Illustrates the process of efficiently reading symbol data from a DolphinDB Vector. Symbols are stored as integers internally, so the method retrieves the integer representation in batches using `getInt`, then uses `getSymbolBase` to map the integers to their corresponding string symbols.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_advance.md#_snippet_13\n\nLANGUAGE: C++\nCODE:\n```\nConstantSP readSymbol(Heap *heap, vector<ConstantSP> &arguments) {\n    if (!(arguments[0]->isVector() && arguments[0]->getType() == DT_SYMBOL)) {\n        throw IllegalArgumentException(\"readSymbol\", \"argument must be a symbol vector\");\n    }\n    VectorSP pVec = arguments[0]; //aruments[0]为需要获取数据的Symbol类型的Vector\n    int buf[1024];\n    int start = 0;\n    int N = pVec->size();\n    SymbolBaseSP pSymbol = pVec->getSymbolBase(); //获取SymbolBaseSP\n    while (start < N) {\n        int len = std::min(N - start, 1024);\n        pVec->getInt(start, len, buf);\n        for (int i = 0; i < len; ++i)\n        {\n            std::cout << pSymbol->getSymbol(buf[i]).getString() << std::endl; //读取数据\n        }\n        start += len;\n    }\n    return new Void();\n}\n```\n\n----------------------------------------\n\nTITLE: Triggering TSDB Level File Merge in DolphinDB\nDESCRIPTION: This code snippet triggers the merging of Level Files across all partitions in a DolphinDB cluster. It retrieves chunk IDs using `getChunksMeta`, iterates through them, and calls `triggerTSDBCompaction` on each. Finally, it checks the status of the compaction tasks using `getTSDBCompactionTaskStatus`. Requires a DolphinDB server with TSDB enabled and the `pnodeRun` function. The `dbName` variable needs to be replaced with the actual database name.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/tsdb_explained.md#_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\n// dbName 需要替换成自己的数据库名\nchunkIDs = exec * from pnodeRun(getChunksMeta{\"/dbName/%\"}) where type=1\nfor(chunkID in chunkIDs){\n    pnodeRun(triggerTSDBCompaction{chunkID})\n}\n// 检查是否完成 Level File 合并\nselect * from pnodeRun(getTSDBCompactionTaskStatus) where endTime is null\n```\n\n----------------------------------------\n\nTITLE: Calculating Alpha 1 Factor Using SQL and Row/Window Functions - DolphinDB\nDESCRIPTION: This snippet calculates the Alpha 1 factor by applying window and conditional functions in SQL style, with factor logic written as a custom function and evaluated on data grouped by securityid and tradetime. It first computes a rolling max of transformed returns per security, then applies a cross-sectional rank within each timestamp context. The function expects close prices structured as a table with tradetime and securityid. Data must be loaded with appropriate time range selection from the k_day_level table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practice_for_factor_calculation.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//alpha1\ndef alpha1SQL(t){\n\tres = select tradetime, securityid, mimax(pow(iif(ratios(close) - 1 < 0, mstd(ratios(close) - 1, 20), close), 2.0), 5) as val from t by securityid\n\treturn select tradetime, securityid, rank(val, percent=true) - 0.5 as val from res context by tradetime\n}\ninput = select tradetime,securityid, close from loadTable(\"dfs://k_day_level\",\"k_day\") where tradetime between 2010.01.01 : 2010.12.31\nalpha1DDBSql = alpha1SQL(input)\n\n```\n\n----------------------------------------\n\nTITLE: Defining a function to get max temperature\nDESCRIPTION: This code defines a function named 'getMaxTemperature' that takes a device ID as input and retrieves the maximum temperature for that device from the 'sensor' table in the distributed database 'dfs://dolphindb' for the previous day.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_56\n\nLANGUAGE: shell\nCODE:\n```\ndef getMaxTemperature(deviceID){\n    maxTemp=exec max(temperature) from loadTable(\"dfs://dolphindb\",\"sensor\")\n            where ID=deviceID ,date(ts) = today()-1\n    return  maxTemp\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing mathWghtSkew Factor in Python\nDESCRIPTION: This implementation includes helper functions for row-based weighted calculations in Python and the main mathWghtSkew function that calculates weighted skewness. It requires NumPy and implements equivalent functionality to the DolphinDB version.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Python+HDF5_vs_DolphinDB.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ndef rowWavg(x, w):\n    rows = x.shape[0]\n    res = [[0]*rows]\n    for row in range(rows):\n        res[0][row] = np.average(x[row], weights=w)\n    res = np.array(res)\n    return res\n\ndef rowWsum(x, y):\n    rows = x.shape[0]\n    res = [[0]*rows]\n    for row in range(rows):\n        res[0][row] = np.dot(x[row],y[row])\n    res = np.array(res)\n    return res\n\ndef mathWghtCovar(x, y, w):\n    v = (x - rowWavg(x, w).T)*(y - rowWavg(y, w).T)\n    return rowWavg(v, w)\n\ndef mathWghtSkew(x, w):\n    x_var = mathWghtCovar(x, x, w)\n    x_std = np.sqrt(x_var)\n    x_1 = x - rowWavg(x, w).T\n    x_2 = x_1*x_1\n    len = np.size(w)\n    adj = np.sqrt((len - 1) * len) / (len - 2)\n    skew = rowWsum(x_2, x_1) / (x_var*x_std)*adj / len\n    return np.where(x_std == 0, 0, skew)\n```\n\n----------------------------------------\n\nTITLE: Performing Asof Join with DolphinDB SQL - DolphinDB\nDESCRIPTION: Demonstrates an asof join on two batch tables in DolphinDB SQL. No external dependencies are required except DolphinDB with SQL interface support. Tables t1 and t2 contain time series trade and bid price data, joined by the 'Time' column to associate each record in t1 with the nearest prior record from t2. Expected input: time-aligned tables with appropriate schema. Output is a table where each record pairs a trade with the latest bid before that trade. Limitations: only suitable for bounded/batch datasets, not streaming.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming-real-time-correlation-processing.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// data\nt1 = table(take(`A, 4) as Sym, 10:10:03.000+(10 2100 2890 6030) as Time, 12.5 12.5 12.6 12.6 as Price)\nt2 = table(take(`A, 4) as Sym, 10:10:00.000+(0 3000 6000 9000) as Time, 12.6 12.5 12.7 12.6 as BidPrice)\n\n// asof join calculation\nselect *  from aj(t1, t2, `Time)\n```\n\n----------------------------------------\n\nTITLE: Applying PCA and Training Random Forest Classifier\nDESCRIPTION: This code applies the `principalComponents` function to the `wineTrain` dataset using the `transDS!` function, and then trains a random forest classifier on the transformed data. The `yColName` is set to `Class`, and the `xColNames` are set to `col0`, `col1`, and `col2`, representing the three principal components. Number of classes is specified to be 3.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/machine_learning.md#_snippet_15\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nds = sqlDS(<select * from wineTrain>)\nds.transDS!(principalComponents{, components, `Class, xColNames})\n\nmodel = randomForestClassifier(ds, yColName=`Class, xColNames=`col0`col1`col2, numClasses=3)\n```\n\n----------------------------------------\n\nTITLE: Defining Custom State Function slr\nDESCRIPTION: This DolphinDB script defines a custom state function named `slr` using the `@state` decorator.  It computes alpha, beta, and residual based on the `mslr` and `mavg` built-in state functions. The function demonstrates the use of user-defined state functions within the Reactive State Engine.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/reactive_state_engine.md#_snippet_4\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\n@state\ndef slr(y, x){\n    alpha, beta = mslr(y, x, 12)\n    residual = mavg(y, 12) - beta * mavg(x, 12) - alpha\n    return alpha, beta, residual\n}\n```\n\n----------------------------------------\n\nTITLE: Querying - Complex Grouping DolphinDB\nDESCRIPTION: This snippet sums the `cpu_avg_15min` values, grouped by the hour of the day and filtered for specific device IDs and a time range.  It orders the results by the calculated sum in descending order. This is another example of complex grouping and ordering.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs TimescaleDB/test_dolphindb_small.txt#_snippet_26\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 17. 经典查询：计算各个时间段内某些设备的总负载，并将时段按总负载降序排列\ntimer\nselect sum(cpu_avg_15min) as sum_load\nfrom readings\nwhere\n\ttime between 2016.11.15 12:00:00 : 2016.11.16 12:00:00,\n    device_id in ['demo000001', 'demo000010', 'demo000100', 'demo001000']\ngroup by hour(time)\norder by sum_load desc\n```\n\n----------------------------------------\n\nTITLE: Replaying a Single In-Memory Table with DolphinDB Script\nDESCRIPTION: Uses the replay function to stream historical data from a single in-memory table to an output stream table as if it were real-time data, enabling unified code for both backtesting and live execution. Requires a preloaded inputTable and outputTable, and mandates specification of date and time columns plus playback speed. Inputs are in-memory table objects, and outputs are published to the designated stream table. Limitations include memory constraints for large datasets.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/historical_data_replay.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\nreplay(inputTable, outputTable, `date, `time, 10)\n```\n\n----------------------------------------\n\nTITLE: Stream Table Creation - DolphinDB\nDESCRIPTION: Creates a stream table named `quotes1` based on the schema of the `quotesData` table. It defines the table's capacity and column types. This is a shared stream table, accessible across different parts of the application.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/func_progr_cases.md#_snippet_12\n\nLANGUAGE: DolphinDB\nCODE:\n```\nquotesData = loadText(\"/data/ddb/data/sampleQuotes.csv\")\n\nx=quotesData.schema().colDefs\nshare streamTable(100:0, x.name, x.typeString) as quotes1\n```\n\n----------------------------------------\n\nTITLE: Creating Distributed Databases and Tables\nDESCRIPTION: This code creates two distributed databases and partitioned tables for storing stock and futures data respectively.  It logs in as admin, defines database paths, creates databases with VALUE partitioning based on symbol (for stocks) and date (for futures), and then creates partitioned tables within those databases, using the 'trades' table as the source.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_csv.md#_snippet_26\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(`admin,`123456)\ndbPath1=\"dfs://stocksDatabase\"\ndbPath2=\"dfs://futuresDatabase\"\ndb1=database(dbPath1,VALUE,`IBM`MSFT`GM`C`FB`GOOG`V`F`XOM`AMZN`TSLA`PG`S)\ndb2=database(dbPath2,VALUE,2000.01.01..2000.06.30)\ntb1=db1.createPartitionedTable(trades,`stock,`sym)\ntb2=db2.createPartitionedTable(trades,`futures,`date);\n```\n\n----------------------------------------\n\nTITLE: Reshaping Price Data using `pivot by` in DolphinDB Script\nDESCRIPTION: Demonstrates reshaping the table 't' using the `pivot by` clause. It transforms the data so that unique timestamps become rows and unique stock symbols (`sym`) become columns, displaying the corresponding price in each cell.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/panel_data.md#_snippet_5\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nselect price from t pivot by timestamp, sym;\n```\n\n----------------------------------------\n\nTITLE: Querying Slave Replication Status\nDESCRIPTION: This DolphinDB script queries the slave cluster's replication status using the `getSlaveReplicationStatus` function through an `rpc` call. This function retrieves the execution status of tasks on the slave cluster.  The result includes details about the tasks being replicated. It requires an existing DolphinDB slave cluster.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Asynchronous_Replication.md#_snippet_17\n\nLANGUAGE: dolphindb\nCODE:\n```\nrpc(getControllerAlias(), getSlaveReplicationStatus)\n```\n\n----------------------------------------\n\nTITLE: Granting TABLE_WRITE Permission to Time Series Engine - DolphinDB\nDESCRIPTION: This snippet grants a user TABLE_WRITE permission to a time series engine ('agg1'), allowing them to insert data into the engine. It uses the `grant` function. Requires admin privileges to create user.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Permission_Management.md#_snippet_33\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(`admin, `123456)\ngrant(\"u2\", TABLE_WRITE, \"agg1\")\n```\n\n----------------------------------------\n\nTITLE: Creating Sample Panel Data Table in DolphinDB Script\nDESCRIPTION: Initializes sample panel data for three stock symbols (C, MS, IBM) with timestamp, price, and volume. This data is stored in a DolphinDB table named 't' and used in subsequent examples for demonstrating panel data manipulation techniques.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/panel_data.md#_snippet_0\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nsym = `C`C`C`C`MS`MS`MS`IBM`IBM\ntimestamp = [09:34:57,09:34:59,09:35:01,09:35:02,09:34:57,09:34:59,09:35:01,09:35:01,09:35:02]\nprice= 50.6 50.62 50.63 50.64 29.46 29.48 29.5 174.97 175.02\nvolume = 2200 1900 2100 3200 6800 5400 1300 2500 8800\nt = table(sym, timestamp, price, volume);\nt;\n```\n\n----------------------------------------\n\nTITLE: Calculating Implied Volatility Matrix using JIT Function (DolphinDB Script)\nDESCRIPTION: Defines the `calculateImpv` function which serves as a wrapper to apply the JIT-compiled `calculateImpvJIT` function (designed for scalar inputs) to matrix inputs. It accepts matrices for option close prices, ETF prices, strike prices, time ratios, and option types. It reshapes these input matrices into vectors, applies `calculateImpvJIT` element-wise using the `each` higher-order function, and finally reshapes the resulting vector of implied volatilities back into the original matrix shape.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/IV_Greeks_Calculation_for_ETF_Options_Using_JIT.md#_snippet_8\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\ndef calculateImpv(optionTodayClose, etfTodayPrice, KPrice, r, dayRatio, CPMode){\n\toriginalShape = optionTodayClose.shape()\n\toptionTodayClose_vec = optionTodayClose.reshape()\n\tetfTodayPrice_vec = etfTodayPrice.reshape()\n\tKPrice_vec = KPrice.reshape()\n\tdayRatio_vec = dayRatio.reshape()\n\tCPMode_vec = CPMode.reshape()\n\timpvTmp = each(calculateImpvJIT, optionTodayClose_vec, etfTodayPrice_vec, KPrice_vec, r, dayRatio_vec, CPMode_vec)\t\n\timpv = impvTmp.reshape(originalShape)\t\n\treturn impv\n}\n```\n\n----------------------------------------\n\nTITLE: History Dictionary Initialization - DolphinDB\nDESCRIPTION: Initializes a dictionary named `history` to store historical data for each stock. The keys are strings (stock symbols), and the values are of ANY type (tuples of historical prices). This dictionary is used to maintain the state for factor calculations.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/func_progr_cases.md#_snippet_13\n\nLANGUAGE: DolphinDB\nCODE:\n```\nhistory = dict(STRING, ANY)\n```\n\n----------------------------------------\n\nTITLE: Subscribing Trade Original Stream to Persist Real-Time Data into Distributed Database in DolphinDB Script\nDESCRIPTION: Sets up a subscription on the 'tradeOriginalStream' table to continuously load incoming trade records into the persistent distributed database table located at 'dfs://trade_stream'. This subscription uses batch processing with a batch size of 20000, a throttle rate of 1, and a fixed hash partition key, ensuring efficient real-time data synchronization with reconnection enabled to maintain reliability.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/streaming_capital_flow_order_by_order/04.注册流计算引擎和订阅流数据表.txt#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\nsubscribeTable(tableName=\"tradeOriginalStream\", actionName=\"tradeToDatabase\", offset=-1, handler=loadTable(\"dfs://trade_stream\", \"trade\"), msgAsTable=true, batchSize=20000, throttle=1, hash=6, reconnect=true)\n```\n\n----------------------------------------\n\nTITLE: Dividing and Replaying a Large Table from Disk using replayDS in DolphinDB Script\nDESCRIPTION: Demonstrates splitting a large on-disk table into multiple replay data sources via replayDS for parallel, memory-efficient replay into an output stream table using replay. Assumes inputTable is loaded from DFS, and timeRepartitionSchema and parallel levels are specified to optimize throughput. replayDS returns a vector of SQL queries, which are then replayed in parallel. Designed for large-scale datasets exceeding available memory.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/historical_data_replay.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\ninputTable = loadTable(\"dfs://source\", \"source\")\ninputDS = replayDS(<select * from inputTable>, `date, `time, 08:00:00.000 + (1..10) * 3600000)\nreplay(inputDS, outputTable, `date, `time, 1000, true, 2)\n```\n\n----------------------------------------\n\nTITLE: Converting Epoch Time to Timestamp\nDESCRIPTION: This code snippet is part of the `transType` function and demonstrates how to convert epoch time (seconds or milliseconds since 1970-01-01) to the timestamp format in DolphinDB. It uses the `timestamp` function for the conversion.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/LoadDataForPoc.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef transType(mutable memTable)\n{\n   return memTable.replaceColumn!(`epochTimeCol,timestamp(memTable.epochTimeCol))\n}\n```\n\n----------------------------------------\n\nTITLE: Reading Vector Data in DolphinDB C++ Plugin (getInt Batch)\nDESCRIPTION: This snippet shows how to read data from a DolphinDB vector in a C++ plugin using the `getInt(int start, int len, int* buf)` method for batch reading. This method copies data from the vector to a specified buffer in chunks, which can be more efficient than reading elements one by one.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_advance.md#_snippet_10\n\nLANGUAGE: c++\nCODE:\n```\nVectorSP pVec = Util::createVector(DT_INT, 100000000);\nint tmp;\nconst int BUF_SIZE = 1024;\nint buf[BUF_SIZE];\nint start = 0;\nint N = pVec->size();\nwhile (start < N) {\n    int len = std::min(N - start, BUF_SIZE);\n    pVec->getInt(start, len, buf);\n    for (int i = 0; i < len; ++i) {\n        tmp = buf[i];\n    }\n    start += len;\n}\n```\n\n----------------------------------------\n\nTITLE: Submitting a Trade Data Replay Job in DolphinDB\nDESCRIPTION: This snippet submits a job named 'replay_trade' to replay the trade data. It uses the 'replay' function, passing in the selected data 't', a stream named 'tradeOriginalStream', and several parameters including `TradeTime for both the source and the stream time, a batch size of 100000, and a boolean value \"true\". The replay function takes care of replaying data into the stream specified. Requires a running DolphinDB server and a stream named \"tradeOriginalStream\".\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/streaming_capital_flow_order_by_order/06.历史数据回放.txt#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\nsubmitJob(\"replay_trade\", \"trade\",  replay{t, tradeOriginalStream, `TradeTime, `TradeTime, 100000, true, 1})\n```\n\n----------------------------------------\n\nTITLE: Remote Execution of Custom Functions with Dependencies Using RPC in DolphinDB - DolphinDB\nDESCRIPTION: Defines a remote function 'salesSum' using custom function 'mysum' to query a shared table remotely by its string name and filter by date, returning aggregation of price multiplied by quantity from the remote table. Demonstrates automatic serialization and transfer of required dependent functions when invoking them remotely, enabling complex remote computations using user-defined logic. Inputs include table name and date; output is aggregated numeric result.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/hybrid_programming_paradigms.md#_snippet_14\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndefg salesSum(tableName, d): select mysum(price*qty) from objByName(tableName) where date=d\nh(salesSum, \"sales\", 2018.07.02);\n```\n\n----------------------------------------\n\nTITLE: Calculating Before Closing Volume Percentage - Python\nDESCRIPTION: Defines the `beforeClosingVolumePercent` function to compute a trading indicator. It takes a pandas DataFrame representing trade data. It filters trades occurring between 14:30:00.000 and 15:00:00.000 based on the 'TradeTime' column, calculates the sum of 'TradeQty' for these filtered trades, and divides it by the total 'TradeQty' for all trades in the DataFrame. The function returns the calculated percentage.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/DolphinDB_Python_Parser_Intro_for_Quantitative_Finance/因子实现_Python版本/当日尾盘成交占比.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef beforeClosingVolumePercent(trade):\n    tradeTime = trade[\"TradeTime\"].str.slice(11,23)\n    beforeClosingVolume = trade[\"TradeQty\"][(tradeTime >= \"14:30:00.000\")&(tradeTime <= \"15:00:00.000\")].sum()\n    totalVolume = trade[\"TradeQty\"].sum()\n    res = beforeClosingVolume / totalVolume\n    return res\n```\n\n----------------------------------------\n\nTITLE: Parameterized Multi-Condition Dynamic Query with Metaprogramming in DolphinDB\nDESCRIPTION: Showcases automated construction and union of multiple select queries with variable multi-column filters using the bundleQuery function. Each selector is dynamically generated using sql helpers and parameter vectors, facilitating scalable condition combinations and result union. Intended for batch processing of similarly-structured daily queries. Required inputs include data table, date and mt columns/values, and arrays of filter conditions and column names.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_sql_cases.md#_snippet_45\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef bundleQuery(tbl, dt, dtColName, mt, mtColName, filterColValues, filterColNames){\n\tcnt = filterColValues[0].size()\n\tfilterColCnt = filterColValues.size()\n\torderByCol = sqlCol(mtColName)\n\tselCol = sqlCol(\"*\")\n\tfilters = array(ANY, filterColCnt + 2)\n\tfilters[filterColCnt] = expr(sqlCol(dtColName), ==, dt)\n\tfilters[filterColCnt+1] = expr(sqlCol(mtColName), <, mt)\n\t\n\tqueries = array(ANY, cnt)\n\tfor(i in 0:cnt)\t{\n\t\tfor(j in 0:filterColCnt){\n\t\t\tfilters[j] = expr(sqlCol(filterColNames[j]), ==, filterColValues[j][i])\n\t\t}\n\t\tqueries.append!(sql(select=selCol, from=tbl, where=filters, orderBy=orderByCol, ascOrder=false, limit=1))\n\t}\n\treturn loop(eval, queries).unionAll(false)\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Shared Stream Tables in DolphinDB\nDESCRIPTION: Defines three shared stream tables using `share streamTable`. `snapshotStream` ingests raw market snapshot data. `aggrFeatures10min` stores features aggregated over time windows. `result1min` stores the final realized volatility predictions and processing time. Each table definition specifies initial capacity, column names, and data types.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/machine_learning_volatility/06.streamComputingReproduction.txt#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//define stream table\nname = `SecurityID`TradeTime`PreClosePx`OpenPx`HighPx`LowPx`LastPx`TotalVolumeTrade`TotalValueTrade`BidPrice0`BidPrice1`BidPrice2`BidPrice3`BidPrice4`BidPrice5`BidPrice6`BidPrice7`BidPrice8`BidPrice9`BidOrderQty0`BidOrderQty1`BidOrderQty2`BidOrderQty3`BidOrderQty4`BidOrderQty5`BidOrderQty6`BidOrderQty7`BidOrderQty8`BidOrderQty9`OfferPrice0`OfferPrice1`OfferPrice2`OfferPrice3`OfferPrice4`OfferPrice5`OfferPrice6`OfferPrice7`OfferPrice8`OfferPrice9`OfferOrderQty0`OfferOrderQty1`OfferOrderQty2`OfferOrderQty3`OfferOrderQty4`OfferOrderQty5`OfferOrderQty6`OfferOrderQty7`OfferOrderQty8`OfferOrderQty9\ntype =`SYMBOL`TIMESTAMP`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`INT`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`INT`INT`INT`INT`INT`INT`INT`INT`INT`INT`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`INT`INT`INT`INT`INT`INT`INT`INT`INT`INT\nshare streamTable(100000:0, name, type) as snapshotStream\nshare streamTable(100000:0 , `TradeTime`SecurityID`BAS`DI0`DI1`DI2`DI3`DI4`DI5`DI6`DI7`DI8`DI9`Press`RV,`TIMESTAMP`SYMBOL`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE) as aggrFeatures10min\nshare streamTable(100000:0 , `TradeTime`SecurityID`PredictRV`CostTime, `TIMESTAMP`SYMBOL`DOUBLE`INT) as result1min\ngo\n```\n\n----------------------------------------\n\nTITLE: Submitting Simulation Job DolphinDB Script\nDESCRIPTION: Submits the `writeData` function as a background job with the specified name and description. This executes the data simulation process asynchronously, allowing it to run in parallel and continuously feed data into the stream processing and persistence pipeline without blocking the main script execution.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/iot_demo_script.txt#_snippet_11\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\njobId = submitJob(\"simulateData\", \"simulate sensor data\", writeData)\n```\n\n----------------------------------------\n\nTITLE: Parsing MiniSeed Real-Time Stream Data into Stream Tables (DolphinDB Script)\nDESCRIPTION: Implements a continuous job to receive and parse MiniSeed real-time stream files, extracting both sample and metadata, and appending the results to DolphinDB stream tables. Utilizes a tagInfo dictionary for ID mapping and repeatedly checks for new data, appending only new rows to the stream tables 'dataStream' and 'metaStream'. The process is set up as a background submitJob and requires DolphinDB streaming tables and the MiniSeed plugin. Input parameters include stream table names and tag mappings, with the output being data appended to the in-memory stream tables.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Seismic_waveform_data_storage.md#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef streamSimulate(meta,data,tagInfo){\n\t/*\n\t * Description：\n\t * \t此函数用于接收 MiniSeed 实时流文件，并将其解析为结构化数据，存入流数据表中   \n\t * Input：\n\t * \tmeta：stream TABLE   存放 meta 数据的流表   \n\t * \tdata：stream TABLE  存放采样数据的流表  \n\t * \ttagInfo：dict   tagid 和 id 映射的字典     \n\t */\n\tret = mseed::parseStream(file(\"../streamMiniSeed/ZJ_A0001_40_E_I_E.20000101.mseed\").readBytes(30000000))\n\tdata_num_row = ret[`data].size()\n\tmeta_num_row = ret[`metaData].size()\n\tdo{\n\t\tret = mseed::parseStream(file(\"../streamMiniSeed/ZJ_A0001_40_E_I_E.20000101.mseed\").readBytes(30000000))\n\t\tif(ret[`data].size() > data_num_row){\n\t\t\tnow_data_num_row = ret[`data].size()\n\t\t\tnow_meta_num_row = ret[`metaData].size()\n\t\t\tqdata = ret[`data][data_num_row:now_data_num_row]\n\t\t\tqmeta= ret[`metaData][meta_num_row:now_meta_num_row]\n\t\t\tdata_num_row = now_data_num_row\n\t\t\tmeta_num_row = now_meta_num_row\n\t\t\ttmp = tagInfo[qmeta.id]\n\t\t\tqdata.replaceColumn!(`id,tagInfo[qdata.id])\n\t\t\tqmeta = select tmp as ids ,* from qmeta\n\t\t\tdelete from qdata where id = NULL\n\t\t\tdelete from qmeta where ids = NULL\n\t\t\tobjByName(data).append!(qdata)\n\t\t\tobjByName(meta).append!(qmeta)\n\t\t}\n\t}while(true)\n}\n\nmeta,data= \"metaStream\",\"dataStream\"\nd = exec tagid,id from loadTable(\"dfs://real\",\"tagInfo\")\ntagInfo=dict(d[`tagid],d[`id])\nsubmitJob(\"streamSimulate\",\"streamSimulate\",streamSimulate,meta,data,tagInfo)\n```\n\n----------------------------------------\n\nTITLE: Creating and Persisting DolphinDB Stream Table - DolphinDB Script\nDESCRIPTION: Defines and instantiates a DolphinDB stream table for high-throughput message ingest with timestamp, type, and body fields. Enables table persistence, shared access, compression, and sets stream retention and caching policies. No external dependencies; parameters include table name, columns, types, and persistence configuration; returns a persistent and shared stream table for further use.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/stock_market_replay/04.publishToKafka.txt#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef createStreamTableFunc(){\n\tcolName = `msgTime`msgType`msgBody\n\tcolType = [TIMESTAMP,SYMBOL, BLOB]\n\tmessageTemp = streamTable(1000000:0, colName, colType)\n\tenableTableShareAndPersistence(table=messageTemp, tableName=\"messageStream\", asynWrite=true, compress=true, cacheSize=1000000, retentionMinutes=1440, flushMode=0, preCache=10000)\n\tmessageTemp = NULL\n}\ncreateStreamTableFunc()\ngo\n```\n\n----------------------------------------\n\nTITLE: High-Performance Array Vector SQL with TSDB Storage Engine in DolphinDB\nDESCRIPTION: This SQL query uses array vectors with TSDB storage to optimize financial indicator calculations. It directly uses the array vector columns for bid and offer prices and quantities, eliminating the need for matrix conversion.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/sql_performance_optimization_wap_di_rv.md#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\n/**\npart1: Define calculation function\n*/\ndefg featureEngine(bidPrice,bidQty,offerPrice,offerQty){\n\tbas = offerPrice[0]\\bidPrice[0]-1\n\twap = (bidPrice[0]*offerQty[0] + offerPrice[0]*bidQty[0])\\(bidQty[0]+offerQty[0])\n\tdi = (bidQty-offerQty)\\(bidQty+offerQty)\n\tbidw=(1.0\\(bidPrice-wap))\n\tbidw=bidw\\(bidw.rowSum())\n\tofferw=(1.0\\(offerPrice-wap))\n\tofferw=offerw\\(offerw.rowSum())\n\tpress=log((bidQty*bidw).rowSum())-log((offerQty*offerw).rowSum())\n\trv=sqrt(sum2(log(wap)-log(prev(wap))))\n\treturn avg(bas),avg(di[0]),avg(di[1]),avg(di[2]),avg(di[3]),avg(di[4]),avg(di[5]),avg(di[6]),avg(di[7]),avg(di[8]),avg(di[9]),avg(press),rv\n}\n\n/**\npart2: Define variables and assign values\n*/\nstockList=`601318`600519`600036`600276`601166`600030`600887`600016`601328`601288`600000`600585`601398`600031`601668`600048`601888`600837`601601`601012`603259`601688`600309`601988`601211`600009`600104`600690`601818`600703`600028`601088`600050`601628`601857`601186`600547`601989`601336`600196`603993`601138`601066`601236`601319`603160`600588`601816`601658`600745\ndbName = \"dfs://snapshot_SH_L2_TSDB_ArrayVector\"\ntableName = \"snapshot_SH_L2_TSDB_ArrayVector\"\nsnapshot = loadTable(dbName, tableName)\n\n/**\npart3: Execute SQL\n*/\nresult = select\n\t\tfeatureEngine(BidPrice,BidOrderQty,OfferPrice,OfferOrderQty) as `BAS`DI0`DI1`DI2`DI3`DI4`DI5`DI6`DI7`DI8`DI9`Press`RV\n\t\tfrom snapshot\n\t\twhere date(TradeTime) between 2020.01.01 : 2020.12.31, SecurityID in stockList, (time(TradeTime) between 09:30:00.000 : 11:29:59.999) || (time(TradeTime) between 13:00:00.000 : 14:56:59.999)\n\t\tgroup by SecurityID, interval( TradeTime, 10m, \"none\" ) as TradeTime map\n```\n\n----------------------------------------\n\nTITLE: Calculating Alpha 1 Factor Using DolphinDB Panel Data - DolphinDB\nDESCRIPTION: This snippet defines and computes the Alpha 1 factor using the panel mode in DolphinDB. It uses built-in series and window functions such as mstd, pow, mimax, and rowRank, operating directly on panel-structured data (time-indexed, stock-wise columns). The custom functions expect input vectors or matrices representing stock closing prices. The computation outputs a ranked value per stock per timestamp. Requires the k_minute table with close prices and tradetime information. Inputs must be prepared/pivoted to panel shape prior to use.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practice_for_factor_calculation.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//alpha 1 \n//Alpha#001公式：rank(Ts_ArgMax(SignedPower((returns<0?stddev(returns,20):close), 2), 5))-0.5\n\n@state\ndef alpha1TS(close){\n\treturn mimax(pow(iif(ratios(close) - 1 < 0, mstd(ratios(close) - 1, 20),close), 2.0), 5)\n}\n\ndef alpha1Panel(close){\n\treturn rowRank(X=alpha1TS(close), percent=true) - 0.5\n}\n\ninput = exec close from loadTable(\"dfs://k_minute\",\"k_minute\") where date(tradetime) between 2020.01.01 : 2020.01.31 pivot by tradetime, securityid\nres = alpha1Panel(input)\n\n```\n\n----------------------------------------\n\nTITLE: Calculate 30-Second Averages\nDESCRIPTION: This code calculates the average `value` for specified IDs over a week, grouping by `bar(datetime, 30)`. This is an example of aggregation using `bar` function for time-based grouping by intervals (30 seconds in this case), instead of standard time units. It also demonstrates the ability to define time windows to perform calculations.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/iot_examples.md#_snippet_14\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect avg(value) as avg \nfrom sensors \nwhere id in [1,51,101,151,201], datetime between 2020.09.01T00:00:00 : 2020.09.07T23:59:59  \ngroup by id, bar(datetime,30)\n```\n\n----------------------------------------\n\nTITLE: Multiprocessing Pool Function for Batch Processing of Stock Data Files in Python\nDESCRIPTION: This function processes a list of stock CSV files within a specified directory, computes the market imbalance indicator for each file, and aggregates results into a DataFrame. It handles potential errors gracefully by assigning NaN when a file fails to load or process, and uses tqdm for progress visualization.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/DolphinDB_Python_Parser_Intro_for_Quantitative_Finance/因子实现_Python版本/价格变动与一档量差的回归系数.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ndef pool_func(tick_obj, trade_path_obj):\n    single_tick_res = pd.DataFrame(columns=[\"DATE\",\"priceSensitivityOrderFlowImbalance\"])\n    tmp_date = trade_path_obj.split('/')[-2]\n    # print(tmp_date)\n    tmp_date = tmp_date[0:4] + \"-\" + tmp_date[4:6] + \"-\" + tmp_date[6:8]\n    # print(tmp_date)\n    for tick in tqdm(tick_obj):\n        single_tick_res.at[tick[:6], \"DATE\"] = tmp_date\n        try:\n            df = pd.read_csv(os.path.join(trade_path_obj, tick))\n\n            Indicator = priceSensitivityOrderFlowImbalance(df)\n            # print(Indicator)\n            # print(\"开盘后大单净买入占比:\", Indicator)\n            single_tick_res.at[tick[:6], \"priceSensitivityOrderFlowImbalance\"] = Indicator\n\n        except Exception as error:\n            single_tick_res.at[tick[:6], \"priceSensitivityOrderFlowImbalance\"] = np.nan\n            continue\n\n    return single_tick_res\n```\n\n----------------------------------------\n\nTITLE: 更新Vector中的不连续数据点\nDESCRIPTION: 演示如何使用set方法更新Vector中的不连续数据点，通过提供索引向量和值向量来减少虚拟函数调用次数，提高性能。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/c++api.md#_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\nVectorSP index = Util::createVector(DT_INT, 3);\nVectorSP value = Util::createVector(DT_INT, 3);\nvector<int> tmp{1,100,1000};\nindex->setInt(0, 3, tmp.data());\n\nvector<int> tmp2{1, 2, 3};\nvalue->setInt(0, 3, tmp2.data());\n\nv->set(index, value); // v[1] = 1, v[100] = 3, v[1000] = 3\n```\n\n----------------------------------------\n\nTITLE: Importing Data via Java API to DolphinDB\nDESCRIPTION: Shows how to use the Java API to connect to DolphinDB, define a function for saving quotes, and append data from a Java application to a DolphinDB distributed table. The example includes connection, function definition, and function execution steps.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/database.md#_snippet_10\n\nLANGUAGE: Java\nCODE:\n```\nDBConnection conn = new DBConnection();\nconn.connect(\"localhost\", 8848, \"admin\", \"123456\");\nconn.run(\"def saveQuotes(t){ loadTable('dfs://stockDB','quotes').append!(t)}\");\nBasicTable quotes = ...\nList<Entity> args = new ArrayList<Entity>(1);\nargs.add(quotes);\nconn.run(\"saveQuotes\", args);\n```\n\n----------------------------------------\n\nTITLE: Creating Stateful Function Using Partial Application in DolphinDB - DolphinDB\nDESCRIPTION: Implements a stateful cumulative average function 'cumulativeAverage' that maintains mutable state (count and running average) across calls by storing it in an array 'stat'. This function is partially applied with initial mutable state '[0.0 0.0]' producing a message handler function 'msgHandler' which can then be applied over a sequence of numbers to obtain running averages. Demonstrates using partial application to generate closures with internal state for streaming computations.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/hybrid_programming_paradigms.md#_snippet_9\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef cumulativeAverage(mutable stat, newNum){\n    stat[0] = (stat[0] * stat[1] + newNum)/(stat[1] + 1)\n    stat[1] += 1\n    return stat[0]\n}\n\nmsgHandler = cumulativeAverage{0.0 0.0}\neach(msgHandler, 1 2 3 4 5)\n```\n\n----------------------------------------\n\nTITLE: Making HTTP GET Request to WeChat Work for Access Token\nDESCRIPTION: Code demonstrating how to use the httpGet function to request an access token from WeChat Work API. Sets a timeout of 1000ms and prints the response text.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/send_messages_external_systems.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\nret=httpClient::httpGet(url,param,1000);\nprint ret['text'];\n```\n\n----------------------------------------\n\nTITLE: Simulating and Inserting Data\nDESCRIPTION: This DolphinDB script simulates and appends data to a partitioned table.  It generates sample data for different fields such as SecurityID, DateTime, and price-related values and appends the data to the table. It involves generating random data to simulate market data and inserts it into the table.  This operation requires an existing partitioned table and database. The output is that the data is written to the table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Asynchronous_Replication.md#_snippet_14\n\nLANGUAGE: dolphindb\nCODE:\n```\n// 模拟数据并写入分区表\nn = 1210000\nrandPrice = round(10+rand(1.0, 100), 2)\nrandVolume = 100+rand(100, 100)\nSecurityID = lpad(string(take(0..4999, 5000)), 6, `0)\nDateTime = (2023.01.08T09:30:00 + take(0..120, 121)*60).join(2023.01.08T13:00:00 + take(0..120, 121)*60)\nPreClosePx = rand(randPrice, n)\nOpenPx = rand(randPrice, n)\nHighPx = rand(randPrice, n)\nLowPx = rand(randPrice, n)\nLastPx = rand(randPrice, n)\nVolume = int(rand(randVolume, n))\nAmount = round(LastPx*Volume, 2)\ntmp = cj(table(SecurityID), table(DateTime))\nt = tmp.join!(table(PreClosePx, OpenPx, HighPx, LowPx, LastPx, Volume, Amount))\ndbName = \"dfs://testDB\"\ntbName = \"testTB\"\nloadTable(dbName, tbName).append!(t)\n```\n\n----------------------------------------\n\nTITLE: 流数据流向描述及流表定义（DolphinDB 流式处理脚本）\nDESCRIPTION: 定义三张流表：signal（实时振动信号）、srms（计算所得 RMS 值）、warn（报警信息），设置持久化和共享。支持实时数据入库和后续分析处理。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Random_Vibration_Signal_Analysis_Solution.md#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\nt =  streamTable(100:0, `timestamp`source`signalnose,[TIMESTAMP,SYMBOL,DOUBLE])\nenableTableShareAndPersistence(table=t, tableName=`signal, cacheSize = 2000000)\n// share streamTable(100:0, `timestamp`source`signalnose,[TIMESTAMP,SYMBOL,DOUBLE]) as signal\nshare streamTable(100:0, `datetime`source`rmsAcc`rmsVel`rmsDis,[TIMESTAMP,SYMBOL,DOUBLE,DOUBLE,DOUBLE]) as srms\nshare streamTable(100:0, `datetime`source`type`metric,[TIMESTAMP,SYMBOL,INT,STRING]) as warn\n```\n\n----------------------------------------\n\nTITLE: Validating Anomaly Detection Results in DolphinDB\nDESCRIPTION: Verifies the accuracy of the streaming engine's anomaly detection. It first queries the raw `doorRecord` table using `context by doorNum` and time-series functions (`deltas`, `prev`) to calculate time differences between consecutive events for each door. It then filters this result (`resultTable`) to find records where the time difference is >= 300 seconds and the previous event code matches specific values, indicating a door-open timeout. Finally, it compares this manually derived result with the output table from the streaming engine (`outputSt1`) using `eqObj`.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_engine_anomaly_alerts.md#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\nt = select *, deltas(eventDate), prev(doorNum), prev(eventDate), prev(doorEventCode) \n    from doorRecord context by doorNum \nresultTable = select prev_doorNum as doorNum,prev_eventDate+300 as eventDate,\n              prev_doorEventCode as doorEventCode from t \n              where deltas_eventDate>= 300 and prev_doorEventCode in [11,12,56,60,65,67] \n              and (prev(eventDate)!=eventDate or prev(doorEventCode)!=doorEventCode)\n              order by eventDate\neqObj(resultTable.values(),outputSt1.values())\n```\n\n----------------------------------------\n\nTITLE: Calculating Weighted Average Price with JIT in DolphinDB\nDESCRIPTION: DolphinDB script defining a JIT-compiled (`@jit`) function `weightedAveragedPrice` to calculate the weighted average price based on level 1 bid/offer price/quantity. It highlights a key difference between JIT and non-JIT functions in the context of reactive state engines: JIT functions used in metrics receive scalar inputs when processing multiple keys, thus eliminating the need for explicit `each`/`loop` wrappers that are often required for non-JIT stateful/stateless functions.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Streaming_computing_of_financial_quantifiers.md#_snippet_23\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 因子实现\n@jit\ndef weightedAveragedPrice(bidPrice0, bidOrderQty0, offerPrice0, offerOrderQty0, default){\n    if(bidPrice0 > 0){  return (bidPrice0*offerOrderQty0 + offerPrice0*bidOrderQty0) \\ (offerOrderQty0+bidOrderQty0)}\n    return default\n}\n\nmetrics = <[dateTime, weightedAveragedPrice(bidPrice0, bidOrderQty0, offerPrice0, offerOrderQty0, 0.0)]>\n```\n\n----------------------------------------\n\nTITLE: Calculating Maximum Drawdown Using DolphinDB\nDESCRIPTION: Defines getMaxDrawdown that calculates the maximum drawdown from a value series. It finds the largest drop from a historical peak to a subsequent trough by identifying indices where drawdown is maximized. Returns zero if no drawdown exists. Input is a numeric vector of cumulative values, output is max drawdown ratio.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_factor_by_ddb/fund_factor_ten.txt#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef getMaxDrawdown(value){\n\ti = imax((cummax(value) - value) \\ cummax(value))\n\tif (i==0){\n\t\treturn 0\n\t}\n\tj = imax(value[:i])\n\treturn (value[j] - value[i]) \\ (value[j])\n}\n```\n\n----------------------------------------\n\nTITLE: Calculating Historical RS Factor Using DolphinDB\nDESCRIPTION: Defines calAllRs2 to compute a historical RS factor (relative strength) for given returns matrix mret, fund list, and window k. It calculates demeaned cumulative sums over rolling windows, derives the spread between max and min within window, normalizes by std deviation, replaces nulls with 1.0, averages, then takes log for final factor values. Returns a table mapping fund numbers to RS factors. Inputs are matrix of returns, symbol list, and integer window size; output is a table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_factor_by_ddb/fund_factor_ten.txt#_snippet_9\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef calAllRs2(mret, symList, k){\n        rowCount = mret.rows()/k * k\n        demeanCum = rolling(cumsum, mret[0:rowCount,] - each(stretch{, rowCount}, rolling(avg, mret, k, k)), k, k)\n        a = rolling(max, demeanCum, k, k) - rolling(min, demeanCum, k, k)\n        RS = nullFill!(a/rolling(stdp, mret, k, k), 1.0).mean().log()\n        return table(symList as fundNum, take(log(k), symList.size()) as knum, RS as factor1)\n}\n```\n\n----------------------------------------\n\nTITLE: Group By with MAP Keyword (Optimized)\nDESCRIPTION: This code snippet demonstrates the use of the `map` keyword in a group by query. The `map` keyword allows each partition to calculate its results independently, potentially improving performance when the partition granularity is larger than the group granularity.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_sql_cases.md#_snippet_8\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntimer result = select count(*) from snapshot group by SecurityID, bar(DateTime, 60) map\n```\n\n----------------------------------------\n\nTITLE: Creating Database and Table for Performance Testing\nDESCRIPTION: Creates a distributed database and a partitioned table named 'trainInfoTable' for performance testing of the snapshot engine. The database is partitioned by time (ts) and trainID. This setup is used for comparing the snapshot engine with other methods.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/snapshot_engine.md#_snippet_5\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\ndef createDatabase(dbName){\n\ttableSchema = table(100:0,`trainID`ts`tag0001`tag0002`tag0003 ,[INT,TIMESTAMP, FLOAT,DOUBLE,LONG])\n\t\n\tif(exists(dbName))\n\t\tdropDatabase(dbName)\n\tdb1 = database(\"\",VALUE,today()..(today()+3))\n\tdb2 = database(\"\",RANGE,0..30*10+1)\n\tdb = database(dbName,COMPO,[db1,db2])\n\tdfsTable = db.createPartitionedTable(tableSchema,\"trainInfoTable\",`ts`trainID)\n}\nlogin(\"admin\",\"123456\")\ncreateDatabase(\"dfs://testDB\")\n```\n\n----------------------------------------\n\nTITLE: Querying Single Column of a Partition (OLAP) (DolphinDB Script)\nDESCRIPTION: Calculates the maximum value of the 'tag1' column for a specific partition ('2022.01.01') in an OLAP table. This demonstrates that DolphinDB loads only the requested column data for that partition into memory, showcasing column-level granularity in caching.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/memory_management.md#_snippet_5\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nselect max(tag1) from loadTable(dbName,tableName) where day = 2022.01.01\nmem().allocatedBytes - mem().freeBytes)\n```\n\n----------------------------------------\n\nTITLE: Installing DolphinDB Node for Node-RED - Bash\nDESCRIPTION: This snippet demonstrates how to install the DolphinDB Node-RED node by navigating to the Node-RED user directory and running the npm install command. The installation path must be replaced with the actual download path of node-red-contrib-dolphindb-main. Ensure Node-RED and npm are properly installed before running this command. Outputs installation logs on success or error.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/node_red_tutorial_iot.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd C:\\Users\\<用户名>\\.node-red\nnpm install D:\\Node_Red_Project\\node-red-contrib-dolphindb-main\n```\n\n----------------------------------------\n\nTITLE: Simulating Data and Writing into Partitioned Table\nDESCRIPTION: This code snippet simulates 1-minute K-line data for 5000 stocks over one day, and writes this data into the created partitioned table.  It defines variables for generating random prices, volumes, SecurityIDs and DateTimes.  It then joins tables to create a single table 't', and appends the generated data into the testTB table within the testDB database.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/single_machine_cluster_deploy.md#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 模拟数据并写入分区表\nn = 1210000\nrandPrice = round(10+rand(1.0, 100), 2)\nrandVolume = 100+rand(100, 100)\nSecurityID = lpad(string(take(0..4999, 5000)), 6, `0)\nDateTime = (2023.01.08T09:30:00 + take(0..120, 121)*60).join(2023.01.08T13:00:00 + take(0..120, 121)*60)\nPreClosePx = rand(randPrice, n)\nOpenPx = rand(randPrice, n)\nHighPx = rand(randPrice, n)\nLowPx = rand(randPrice, n)\nLastPx = rand(randPrice, n)\nVolume = int(rand(randVolume, n))\nAmount = round(LastPx*Volume, 2)\ntmp = cj(table(SecurityID), table(DateTime))\nt = tmp.join!(table(PreClosePx, OpenPx, HighPx, LowPx, LastPx, Volume, Amount))\ndbName = \"dfs://testDB\"\ntbName = \"testTB\"\nloadTable(dbName, tbName).append!(t)\n```\n\n----------------------------------------\n\nTITLE: 使用Map-Reduce生成分钟级K线数据\nDESCRIPTION: 该代码通过Map-Reduce框架处理股票报价数据，计算并生成分钟级K线数据。函数saveMinuteQuote作为map函数处理数据，将结果保存到minuteQuotes表中。数据量约65亿条记录(172GB)，最终生成约6000万条K线数据。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/cluster_scaleout_perf_test.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef saveMinuteQuote(t){\n\tminuteQuotes=select avg(bid) as bid, avg(ofr) as ofr from t group by symbol, date, minute(time) as minute\n\tloadTable(\"dfs://TAQ\", \"quotes_minute\").append!(minuteQuotes)\n\treturn minuteQuotes.size()\n}\n\nds = sqlDS(<select symbol, date, time, bid, ofr from quotes where date between 2007.08.01 : 2007.08.31>)\nmr(ds, saveMinuteQuote, +)\n```\n\n----------------------------------------\n\nTITLE: Batch Updating Integer Vector Data with setData C++\nDESCRIPTION: Shows batch updating of integer data in a DolphinDB Vector using the `setData(start, len, buf)` method. This method is similar to `setInt(start, len, buf)` but does not perform type checking on the provided buffer, which can be slightly faster but requires careful use.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_advance.md#_snippet_16\n\nLANGUAGE: C++\nCODE:\n```\nconst int size = 100000000;\nconst int BUF_SIZE = 1024;\nint tmp[1024];\nVectorSP pVec = Util::createVector(DT_INT, size);\nint start = 0;\nwhile(start < size) {\n    int len =  std::min(size - start, BUF_SIZE);\n    for(int i = 0; i < len; ++i) {\n        tmp[i] = i;\n    }\n    pVec->setData(start, len, tmp);\n    start += len;\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a table with sym and bidPrice columns\nDESCRIPTION: This code snippet creates a DolphinDB table named 't' with two columns: 'sym' and 'bidPrice'. The 'sym' column contains a sequence of symbols taken from the list `a`b`c`d`e, repeated 100 times. The 'bidPrice' column contains 100 random floating-point numbers between 0 and 100.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_27\n\nLANGUAGE: shell\nCODE:\n```\nt = table(take(`a`b`c`d`e ,100) as sym, rand(100.0,100) as bidPrice)\n```\n\n----------------------------------------\n\nTITLE: Uploading Transaction Data and Managing Table Schema - DolphinDB Script\nDESCRIPTION: This DolphinDB script defines the function transacUpload that creates a compound-partitioned table with explicit schema tailored for transactional data, modifies the schema based on extracted CSV data, and uploads the data efficiently into the table using loadTextEx. Dependencies include DolphinDB  scripting, proper access to the specified CSV file, and the DolphinDB TSDB engine. Key parameters are dbName (database name), tbName (table name), and path (location of the CSV file). Input is the CSV file, processed into a partitioned DolphinDB table; output is a table populated in the specified TSDB database. All columns and partitioning requirements must be known in advance; file paths are system-dependent and must be accessible from the server.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/appendices_market_replay_bp/transac_upload.txt#_snippet_0\n\nLANGUAGE: dolphindb\nCODE:\n```\ndef transacUpload(dbName, tbName)\n{\n    path=\"C:/Users/myhu/Desktop/transaction.csv\"\n\n    db1 = database(, VALUE, 2021.12.01..2021.12.31)\n    db2 = database(, HASH, [SYMBOL, 25])\n    db = database(dbName, COMPO, [db1, db2], , 'TSDB')\n\n    schemaTable = table(\n        array(SYMBOL, 0) as SecurityID,\n        array(DATE, 0) as MDDate,\n        array(TIME, 0) as MDTime,\n        array(TIMESTAMP, 0) as DataTimestamp,\n        array(SYMBOL, 0) as SecurityIDSource,\n        array(SYMBOL, 0) as SecurityType,\n        array(LONG, 0) as TradeIndex,\n        array(LONG, 0) as TradeBuyNo,\n        array(LONG, 0) as TradeSellNo,\n        array(INT, 0) as TradeType,\n        array(INT, 0) as TradeBSFlag,\n        array(LONG, 0) as TradePrice,\n        array(LONG, 0) as TradeQty,\n        array(LONG, 0) as TradeMoney,\n        array(LONG, 0) as ApplSeqNum,\n        array(INT, 0) as ChannelNo,\n        array(DATE, 0) as ExchangeDate,\n        array(TIME, 0) as Exchanime\n    )\n    \n    db.createPartitionedTable(table=schemaTable, tableName=tbName, partitionColumns=`MDDate`SecurityID, sortColumns=`SecurityID`MDTime, keepDuplicates=ALL,compressMethods={MDDate:\"delta\", MDTime:\"delta\",DataTimestamp:\"delta\",ExchangeDate:\"delta\",Exchanime:\"delta\"})\n    \n    schema = extractTextSchema(path)\n    update schema set type = \"TIMESTAMP\" where name = \"DataTimestamp\"\n    update schema set type = \"DATE\" where name = \"ExchangeDate\"\n    transaction = loadTextEx(dbHandle=db, tableName=`transaction, partitionColumns=`MDDate`SecurityID, filename=path, schema=schema, sortColumns=`SecurityID`MDTime, arrayDelimiter=\",\")\n    \n}\n\ndbName = \"dfs://Test_transaction\"\ntbName = \"transaction\"\ntransacUpload(dbName, tbName)\n```\n\n----------------------------------------\n\nTITLE: DolphinDB Script for Stream Table and Subscription\nDESCRIPTION: This DolphinDB script creates a stream table (`streamtable`) and sets up a subscription to write data from the stream table to a partitioned table (`collect`). It defines a function `saveToDFS` that appends data from the stream table to the partitioned table. It then subscribes to the stream table, specifying the handler function, batch size, and throttle settings.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_cpp_api_connector.md#_snippet_8\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 建立流表\ndef saveToDFS(mutable dfstable, msg): dfstable.append!(msg)\nshare streamTable(1:0, cols_info, cols_type) as streamtable;\nsubscribeTable(tableName=\"streamtable\", actionName=\"savetodfs\", offset=0, handler=saveToDFS{pt}, msgAsTable=true, batchSize=1000, throttle=1)\n```\n\n----------------------------------------\n\nTITLE: Generating Sorted Trade Data\nDESCRIPTION: This code generates synthetic trade data, including stock symbols, sorted dates, prices, and quantities, and saves it to a CSV file. The `sort` function ensures the date column is ordered.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_csv.md#_snippet_32\n\nLANGUAGE: DolphinDB\nCODE:\n```\nn=10000000\ndataFilePath=\"/home/data/chunkText.csv\"\ntrades=table(rand(`IBM`MSFT`GM`C`FB`GOOG`V`F`XOM`AMZN`TSLA`PG`S,n) as sym,sort(take(2000.01.01..2000.06.30,n)) as date,10.0+rand(2.0,n) as price1,100.0+rand(20.0,n) as price2,1000.0+rand(200.0,n) as price3,10000.0+rand(2000.0,n) as price4,10000.0+rand(3000.0,n) as price5,10000.0+rand(4000.0,n) as price6,rand(10,n) as qty1,rand(100,n) as qty2,rand(1000,n) as qty3,rand(10000,n) as qty4, rand(10000,n) as qty5, rand(1000,n) as qty6)\ntrades.saveText(dataFilePath);\n```\n\n----------------------------------------\n\nTITLE: Define GTJA Alpha 1 Factor Calculation Logic in DolphinDB\nDESCRIPTION: This code defines a function `gtjaAlpha1` to calculate a specific alpha factor (GTJA 191 No. 1) based on open, close, and volume data. It utilizes `deltas`, `log`, `mcorr`, and `rowRank` functions. The function aims to capture relationships between volume changes and price movements.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Streaming_computing_of_financial_quantifiers.md#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef gtjaAlpha1(open, close, vol){\n\tdelta = deltas(log(vol)) \n    return -1 * (mcorr(rowRank(delta, percent=true), rowRank((close - open) \\ open, percent=true), 6))\n}\n```\n\n----------------------------------------\n\nTITLE: Generating One Day of Synthetic Time Series Data in DolphinDB Script\nDESCRIPTION: Implements a function to synthesize single-day time series data for a set of IDs, with configurable frequency per day and metric columns. Requires only DolphinDB standard functions. Parameters include the day, device IDs (id), records per day (freqPerDay), and metric count; output is a table with random float values for each metric per device per timestamp. Intended as input for database ingestion routines.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/multipleValueModeWrite.txt#_snippet_1\n\nLANGUAGE: dolphindb\nCODE:\n```\ndef generate1DayData(day,id,freqPerDay,numMetrics){\n\tstartTime = day.datetime()\n\tidSize = size(id)\n\tnumRecords = freqPerDay * idSize\n\tidVec = array(INT, numRecords)\n\t\n\tfor(i in 0:idSize) idVec[(i*freqPerDay) : ((i+1)*freqPerDay)] = id[i]\n\tt = table(idVec ,take(startTime + (0..(freqPerDay-1)), numRecords) as ts)\n\t\n\tm = \"tag\" + string(1..numMetrics)\n\tfor (i in 0 : numMetrics) t[m[i]] =rand(1.0, numRecords)\n\treturn t\n}\n```\n\n----------------------------------------\n\nTITLE: Example DolphinDB Execution Plan for Subquery\nDESCRIPTION: This JSON object shows an example execution plan for a DolphinDB query involving a subquery. It details the costs and steps involved, including the subquery execution (`from.detail.explain`), map and merge phases within the subquery, and the final filtering (`where`) applied to the subquery results. The 'measurement' indicates costs are in microseconds.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_Explain.md#_snippet_5\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"measurement\": \"microsecond\",\n  \"explain\": {\n    \"from\": {\n      \"cost\": 33571,\n      \"detail\": {\n        \"sql\": \"select [98304] max(x) as maxx from loadTable(\\\"dfs://valuedb\\\", \\\"pt\\\") group by month\",\n        \"explain\": {\n          \"from\": {\n            \"cost\": 16\n          },\n          \"map\": {\n            \"partitions\": {\n              \"local\": 0,\n              \"remote\": 204\n            },\n            \"cost\": 29744,\n            \"detail\": {\n              \"most\": {\n                \"sql\": \"select [114715] first(month) as month,max(x) as maxx from pt [partition = /valuedb/200412M]\",\n                \"explain\": {\n                  \"from\": {\n                    \"cost\": 4\n                  },\n                  \"rows\": 1,\n                  \"cost\": 8224\n                }\n              },\n              \"least\": {\n                \"sql\": \"select [114715] first(month) as month,max(x) as maxx from pt [partition = /valuedb/200112M]\",\n                \"explain\": {\n                  \"rows\": 1,\n                  \"cost\": 31\n                }\n              }\n            }\n          },\n          \"merge\": {\n            \"cost\": 1192,\n            \"rows\": 204,\n            \"detail\": {\n              \"most\": {\n                \"sql\": \"select [114715] first(month) as month,max(x) as maxx from pt [partition = /valuedb/201612M]\",\n                \"explain\": {\n                  \"rows\": 1,\n                  \"cost\": 65\n                }\n              },\n              \"least\": {\n                \"sql\": \"select [114707] first(month) as month,max(x) as maxx from pt [partition = /valuedb/200001M]\",\n                \"explain\": {\n                  \"from\": {\n                    \"cost\": 5\n                  },\n                  \"rows\": 1,\n                  \"cost\": 93\n                }\n              }\n            }\n          },\n          \"rows\": 204,\n          \"cost\": 33571\n        }\n      }\n    },\n    \"where\": {\n      \"rows\": 9,\n      \"cost\": 13\n    },\n    \"rows\": 9,\n    \"cost\": 33621\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Chaining Multiple Stream Engines with Different Computations in DolphinDB\nDESCRIPTION: This snippet shows how to link multiple DolphinDB stream engines where the output of one serves as the input to another. It creates a trades stream table and a K-line stream table, with the second representing real-time K-line data. Then, it creates a reactive state engine for computing moving averages and a time series engine performing aggregations, finally chaining them via stream subscriptions so that the output of the time series engine feeds into the reactive engine for complex layered analysis.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/stream_aggregator.md#_snippet_10\n\nLANGUAGE: DolphinDB\nCODE:\n```\nshare streamTable(1000:0, `time`sym`price`volume, [TIMESTAMP, SYMBOL, DOUBLE, INT]) as trades\n\nshare streamTable(1000:0, `time`sym`open`close`high`low`volume, [TIMESTAMP, SYMBOL, DOUBLE, DOUBLE, DOUBLE, DOUBLE, INT]) as kline\n\noutputTable=table(1000:0, `sym`factor1, [SYMBOL, DOUBLE])\n\nRengine=createReactiveStateEngine(name=\"reactive\", metrics=<[mavg(open, 3)]>, dummyTable=kline, outputTable=outputTable, keyColumn=\"sym\")\n\nTengine=createTimeSeriesEngine(name=\"timeseries\", windowSize=6000, step=6000, metrics=<[first(price), last(price), max(price), min(price), sum(volume)]>, dummyTable=trades, outputTable=Rengine, timeColumn=`time, useSystemTime=false, keyColumn=`sym)\n//时间序列引擎的结果输入响应式状态引擎\n\nsubscribeTable(server=\"\", tableName=\"trades\", actionName=\"timeseries\", offset=0, handler=append!{Tengine}, msgAsTable=true)\n```\n\n----------------------------------------\n\nTITLE: Running a DolphinDB Script File Using DolphinDBOperator in Python\nDESCRIPTION: This snippet shows how to execute an external DolphinDB script file (.dos) in Airflow using the DolphinDBOperator by specifying the file_path parameter. The task is defined with a unique task_id, the DolphinDB connection id, and the path to the DolphinDB script file 'CalAlpha001.dos'. This enables modular and reusable management of DolphinDB scripts for ETL tasks within Airflow DAGs. Requirements include Airflow with the DolphinDB provider plugin and proper DolphinDB connection setup.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Best_Practices_for_DolphinDB_and_Python_AirFlow.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n    //CalAlpha001.dos 为 DolphinDB 脚本\n    case1 = DolphinDBOperator(\n        task_id='case1',\n        dolphindb_conn_id='dolphindb_test',\n        file_path=path + \"/StreamCalculating/CalAlpha001.dos\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Processing Buy Orders with Reactive State Engine in DolphinDB\nDESCRIPTION: This function `processBuyOrderFunc` creates and subscribes to reactive state engines to process buy orders incrementally. It defines metrics for calculations like cumulative trade amount and order flags based on trade quantity. The data is partitioned using a hash algorithm for parallel processing.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_capital_flow_daily.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef processBuyOrderFunc(parallel){\n\tmetricsBuy = [\n\t\t<TradeTime>,\n\t\t<SellNum>,\n\t\t<TradeAmount>,\n\t\t<TradeQty>,\n\t\t<cumsum(TradeAmount)>,\n\t\t<tagFunc(cumsum(TradeQty))>,\n\t\t<prev(cumsum(TradeAmount))>,\n\t\t<prev(tagFunc(cumsum(TradeQty)))>]\n\tfor(i in 1..parallel){\n\t\tcreateReactiveStateEngine(name=\"processBuyOrder\"+string(i), metrics=metricsBuy, dummyTable=tradeOriginalStream, outputTable=getStreamEngine(\"processSellOrder\"+string(i)), keyColumn=`SecurityID`BuyNum, keepOrder=true)\n\t\tsubscribeTable(tableName=\"tradeOriginalStream\", actionName=\"processBuyOrder\"+string(i), offset=-1, handler=getStreamEngine(\"processBuyOrder\"+string(i)), msgAsTable=true, hash=i, filter=(parallel, i-1))\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: Setup 60min Time Series Aggregation Engine (DolphinDB)\nDESCRIPTION: Defines a function `processCapitalFlow60minFunc` to create and subscribe a `dailyTimeSeriesEngine`. This engine aggregates metrics from the `capitalFlowStream` into 60-minute windows, calculating the last observed values for various amounts and counts (total, small, medium, big, buy, sell). It subscribes to `capitalFlowStream` and outputs aggregated results to `capitalFlowStream60min`.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/streaming_capital_flow_daily/02.createEngineSub.txt#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef processCapitalFlow60minFunc(){\n\taggrMetrics = <[\n\t\tlast(TotalAmount),\n\t\tlast(SellSmallAmount),\n\t\tlast(SellMediumAmount),\n\t\tlast(SellBigAmount),\n\t\tlast(SellSmallCount),\n\t\tlast(SellMediumCount),\n\t\tlast(SellBigCount),\n\t\tlast(BuySmallAmount),\n\t\tlast(BuyMediumAmount),\n\t\tlast(BuyBigAmount),\n\t\tlast(BuySmallCount),\n\t\tlast(BuyMediumCount),\n\t\tlast(BuyBigCount)]>\n\tcreateDailyTimeSeriesEngine(name = \"processCapitalFlow60min\", windowSize = 60000*60, step = 60000*60, metrics = aggrMetrics, dummyTable = capitalFlowStream, outputTable =capitalFlowStream60min, timeColumn=\"TradeTime\", useSystemTime=false, keyColumn=`SecurityID, useWindowStartTime=false)\n\tsubscribeTable(tableName = \"capitalFlowStream\", actionName = \"processCapitalFlow60min\", offset = -1, handler = getStreamEngine(\"processCapitalFlow60min\"), msgAsTable = true, batchSize = 10000, throttle=1, hash = 0)\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Redshift Data and Inserting into DolphinDB\nDESCRIPTION: This snippet queries data from Redshift and inserts it into the previously created DolphinDB partitioned table. It utilizes the `odbc::query` function, taking the connection object, the SQL query, and the target table as input. The query selects all columns from the 'trades_sz' table in Redshift and inserts the data into the DolphinDB table 'pt'.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/Migrate_data_from_Redshift_to_DolphinDB/Redshift2DDB.txt#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//查询数据并插入DolphinDB\nodbc::query(conn, \"select * from trades_sz\",pt)\n```\n\n----------------------------------------\n\nTITLE: Cumulative Sum Calculation on Matrix with cumsum in DolphinDB\nDESCRIPTION: Calculates the cumulative sum for each column in the matrix `m` using the `cumsum` function. This function adds up values in each column sequentially, producing a matrix where each element is the sum of all preceding elements in that column.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/window_cal.md#_snippet_21\n\nLANGUAGE: DolphinDB\nCODE:\n```\ncumsum(m)\n```\n\n----------------------------------------\n\nTITLE: QR Decomposition with pivoting in DolphinDB\nDESCRIPTION: Demonstrates QR decomposition of a matrix in DolphinDB using the `qr` function with `mode=`full` and `pivoting=true`. The function computes the rank-revealing QR decomposition A*P=Q*R, where P is a permutation matrix. Returns Q, R, and piv.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/matrix.md#_snippet_21\n\nLANGUAGE: DolphinDB\nCODE:\n```\n>m=matrix([2 5 5, 5 5 4, 8 6 4, 7 6 8]);\n>m;\n#0 #1 #2 #3\n-- -- -- --\n2  5  8  7 \n5  5  6  6 \n5  4  4  8 \n\n>q,r,piv=qr(m,mode=`full,pivoting=true);\n>q;\n#0        #1        #2      \n--------- --------- --------\n-0.742781 0.629178  0.228934\n-0.557086 -0.391111 -0.73259\n-0.371391 -0.67169  0.641016\n>r;\n#0        #1        #2         #3       \n--------- --------- ---------- ---------\n-10.77033 -6.127946 -11.513111 -7.9849  \n0         -4.055647 -3.315938  -1.496423\n0         0         2.33513    0.045787 \n>piv;\n[2,0,3,1] \n\n//置换规则: 如果 piv[i] != i, 则m[i] = m[piv[i]]\n>q**r;\n#0 #1 #2 #3\n-- -- -- --\n8  2  7  5 \n6  5  6  5 \n4  5  8  4 \n\n```\n\n----------------------------------------\n\nTITLE: Creating Distributed TSDB Databases and Partitioned Tables DolphinDB Script\nDESCRIPTION: Creates distributed DolphinDB TSDB databases partitioned by date and device ID with delta compression on key columns. The script checks for existing databases or tables and drops them before creation to ensure a fresh environment. Three databases and associated tables are defined: one for real-time sampled data, one for delay calculation results, and one for dimension data of network, station, and channel info. The data model supports efficient partition pruning, indexing, and supports deduplication policies.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Seismic_waveform_data_storage.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//创建储实时数据的分布式数据库和分区表\nif( existsDatabase(\"dfs://real\") ){ dropDatabase(\"dfs://real\") }\ncreate database \"dfs://real\" partitioned by VALUE(2023.03.01..2023.03.10), VALUE(1..3900), engine='TSDB'\ncreate table \"dfs://real\".\"realData\"(\n\tid INT[compress=\"delta\"],\n\tts TIMESTAMP[compress=\"delta\"],\n\tvalue INT[compress=\"delta\"]\n)\npartitioned by ts, id,\nsortColumns=[`id, `ts],\nkeepDuplicates=ALL\n\n  //创建存储时延计算结果的分布式数据库和分区表\nif(existsDatabase(\"dfs://delay\")){dropDatabase(\"dfs://delay\")}\ncreate database \"dfs://delay\" partitioned by VALUE(2023.03.01..2023.03.10), HASH([INT, 10]), engine='TSDB' \ncreate table \"dfs://delay\".\"delayData\"(\n\tid INT[compress=\"delta\"],\n\ttagid SYMBOL,\n\tstartTime TIMESTAMP[compress=\"delta\"],\n\treceivedTime TIMESTAMP[compress=\"delta\"],\n\tdelay INT[compress=\"delta\"]\n)\npartitioned by startTime, id,\nsortColumns=[`id, `startTime],\nkeepDuplicates=ALL\t\t\n\n  //创建存储台网、台站、位置、通道等基础信息的维度表 \nif(existsTable(\"dfs://real\",\"tagInfo\")){ dropTable(database(\"dfs://real\"),\"tagInfo\") }\ncreate table \"dfs://real\".\"tagInfo\"(\n\tid INT[compress=\"delta\"],\n\tnet SYMBOL,\n\tsta SYMBOL,\n\tloc SYMBOL,\n\tchn SYMBOL,\n\ttagid SYMBOL\n)\nsortColumns=[`id]\n```\n\n----------------------------------------\n\nTITLE: Creating Stock Basic Table in DolphinDB\nDESCRIPTION: Creates a partitioned table with TSDB engine for storing stock basic information synchronized from MySQL. The table uses hash partitioning with sortColumns and keepDuplicates=LAST for data consistency.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Debezium_and_Kafka_data_sync.md#_snippet_18\n\nLANGUAGE: SQL\nCODE:\n```\ndef createStockBasicDB(dbName){\n\tif(existsDatabase(dbName)){\n\tdropDatabase(dbName)\n\t}\n\tdb=database(directory=dbName, partitionType=HASH, partitionScheme=[LONG, 1],engine=\"TSDB\")\n}\ndef createStockBasic(dbName,tbName){\n\tdb=database(dbName)\n             if(existsTable(dbName, tbName)){\n                   db.dropTable(tbName)\t\n\t}\n             mtable=table(100:5, `id`ts_code`symbol`name`area`industry`list_date, [LONG,SYMBOL,SYMBOL,SYMBOL,SYMBOL,SYMBOL,DATE]);\n\t db.createPartitionedTable(table=mtable, tableName=tbName, partitionColumns=`id,sortColumns=`ts_code`id,keepDuplicates=LAST,sortKeyMappingFunction=[hashBucket{,100}])\n}\ncreateStockBasicDB(\"dfs://wddb\")\ncreateStockBasic(\"dfs://wddb\", `stock_basic)\n```\n\n----------------------------------------\n\nTITLE: C++ API: Uploading Data to DolphinDB\nDESCRIPTION: Connects to a DolphinDB node using the C++ API, uploads the table created previously to a memory table named `myTable`. It uses the `upload` method to transfer the data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Array_Vector.md#_snippet_20\n\nLANGUAGE: C++\nCODE:\n```\n// 连接 DolphinDB 节点\nDBConnection conn;\nconn.connect(\"127.0.0.1\", 8848);\nconn.login(\"admin\", \"123456\", false);\n// 上传数据到内存表\nconn.upload(\"myTable\", table);\n```\n\n----------------------------------------\n\nTITLE: Calculating Option Vega using Vectorized Operations (DolphinDB Script)\nDESCRIPTION: Defines functions to compute the option Vega using vectorized operations for matrix inputs. `normpdf` calculates the standard normal probability density function (reused). `calculateD1` computes the d1 term (reused). `calculateVega` calculates the Vega for each element in the input matrices (`etfTodayPrice`, `KPrice`, `dayRatio`, `impvMatrix`) using `normpdf`, `calculateD1`, and vectorized conditional logic (`iif`) and arithmetic operations, scaled by 1/100. Handles cases where implied volatility is non-positive.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/IV_Greeks_Calculation_for_ETF_Options_Using_JIT.md#_snippet_11\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\ndef normpdf(x){\n\treturn exp(-pow(x, 2)/2.0)/sqrt(2*pi)\n}\n\ndef calculateD1(etfTodayPrice, KPrice, r, dayRatio, HLMean){\n\tskRatio = etfTodayPrice / KPrice\n\tdenominator = HLMean * sqrt(dayRatio)\n\tresult = (log(skRatio) + (r + 0.5 * pow(HLMean, 2)) * dayRatio) / denominator\n\treturn result\n}\n\ndef calculateVega(etfTodayPrice, KPrice, r, dayRatio, impvMatrix){\n\tvega = iif(\n\t\t\timpvMatrix <= 0,\n\t\t\t0,\n\t\t\tetfTodayPrice * normpdf(calculateD1(etfTodayPrice, KPrice, r, dayRatio, impvMatrix)) * sqrt(dayRatio)\n\t\t)\n\treturn vega \\ 100.0\n}\n```\n\n----------------------------------------\n\nTITLE: 向量化实现N股VWAP计算\nDESCRIPTION: 通过自定义聚合函数lastVolPx2使用向量化方式，计算每只股票最近1000股相关交易的VWAP。这是优化后的实现，性能提升约两倍。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_SQL_Case_Tutorial.md#_snippet_22\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndefg lastVolPx2(price, vol, bound) {\n\tcumVol = vol.cumsum()\n\tif(cumVol.tail() <= bound)\n\t\treturn wavg(price, vol)\n\telse {\n\t\tstart = (cumVol <= cumVol.tail() - bound).sum()\n\t\treturn wavg(price.subarray(start:), vol.subarray(start:))\n\t}\n}\n\ntimer lastVolPx_t2 = select lastVolPx2(price, vol, 1000) as lastVolPx from t group by sym\n```\n\n----------------------------------------\n\nTITLE: Calculating Moving Average Pressure Factor with Array Vectors in DolphinDB\nDESCRIPTION: DolphinDB script showing an optimized implementation of a moving average pressure factor using array vectors as input parameters (bidPrice, bidOrderQty, offerPrice, offerOrderQty). It defines `pressArrayVector` using row-wise functions like `rowWavg` suitable for array vector operations and a state function `averagePress3` that computes the moving average (`mavg`) of the pressure. This approach simplifies the code and can improve performance by avoiding manual data assembly using `fixedLengthArrayVector` within the function, assuming the input data is already structured as array vectors.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Streaming_computing_of_financial_quantifiers.md#_snippet_20\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef pressArrayVector(bidPrice, bidOrderQty, offerPrice, offerOrderQty){\n\twap = (bidPrice[0]*offerOrderQty[0] + offerPrice[0]*bidOrderQty[0]) \\ (offerOrderQty[0]+bidOrderQty[0])\n\tbidPress = rowWavg(bidOrderQty, wap \\ (bidPrice - wap))\n\taskPress = rowWavg(offerOrderQty, wap \\ (offerPrice - wap))\n\tpress = log(bidPress \\ askPress)\n\treturn press\n}\n\n@state\ndef averagePress3(bidPrice, bidOrderQty, offerPrice, offerOrderQty, lag){\n\tpress = pressArrayVector(bidPrice, bidOrderQty, offerPrice, offerOrderQty)\n\treturn mavg(press, lag, 1)\n}\n```\n\n----------------------------------------\n\nTITLE: Running Time-Windowed Aggregation Query on Snapshot Data in DolphinDB\nDESCRIPTION: Executes a SQL query with a timer on the snapshot table to calculate engineered features for specified stock securities within defined datetime and intraday time windows. The query groups by SecurityID and 10-minute bars, evaluates the feature engineering function using generated aggregation meta-code, and produces a final matrix of feature values for downstream analysis.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/metacode_derived_features/metacode_derived_features_arrayVector.txt#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\nwhereConditions = [<date(DateTime) between 2021.01.04 : 2021.12.31>, <SecurityID in stockList>, <(time(DateTime) between 09:30:00.000 : 11:29:59.999) or (time(DateTime) between 13:00:00.000 : 14:56:59.999)>]\n\ntimer result = sql(select = sqlColAlias(<featureEngineering(DateTime, matrix(BidPrice), matrix(BidOrderQty), matrix(OfferPrice), matrix(OfferOrderQty), aggMetaCode)>, metaCodeColName <- (metaCodeColName+\"_150\") <- (metaCodeColName+\"_300\") <- (metaCodeColName+\"_450\")), from = snapshot, where = whereConditions, groupBy = [<SecurityID>, <bar(DateTime, 10m) as DateTime>]).eval()\n```\n\n----------------------------------------\n\nTITLE: Efficient Cumulative-to-Incremental Volume Transformation - dolphindb\nDESCRIPTION: This snippet efficiently converts a cumulative volume column to incremental volume using the eachPre template function in DolphinDB. The preferred method (eachPre) is compared with an alternative using deltas and nullFill!, with eachPre being more performant for large-scale batch imports. Input: a table column of cumulative volumes. Output: a column with incremental (per-tick) volumes. Prerequisites: table with volume column in memory. Limitation: Intended for high-throughput scenarios; eachPre requires careful function and parameter order.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/stockdata_csv_import_demo.md#_snippet_1\n\nLANGUAGE: dolphindb\nCODE:\n```\nt[\"Volume\"] = eachPre(-, t[\"volume\"], 0)\nt[\"Volume\"] = nullFill!(deltas(t[\"Volume\"]),t[\"Volume\"])\n\n```\n\n----------------------------------------\n\nTITLE: Dynamic SQL Generation with sql and Helper Functions in DolphinDB\nDESCRIPTION: Utilizes built-in metaprogramming helpers (sql, sqlColAlias, makeCall, sqlCol) to programmatically build and execute a group-by query with bar aggregation on a specific interval. Designed for high-performance scenarios with frequently-changing grouping and aggregation columns. Prerequisites: table t, variable barMinutes, understanding of DolphinDB SQL metaprogramming framework.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_sql_cases.md#_snippet_44\n\nLANGUAGE: DolphinDB\nCODE:\n```\ngroupingCols = [sqlColAlias(makeCall(bar, sqlCol(\"TradeTime\"), duration(barMinutes.string() + \"m\")), \"minuteTradeTime\"), sqlCol(\"SecurityID\"), sqlCol(\"TradeDate\")]\nres = sql(select = sqlCol(\"Px\", funcByName(\"avg\"), \"avgPx\"), \n          from = t, groupBy = groupingCols, groupFlag = GROUPBY).eval()\n```\n\n----------------------------------------\n\nTITLE: Calculating Moving Average Price per Symbol using Matrix Operations in DolphinDB Script\nDESCRIPTION: Calculates the 2-period moving average (`mavg`) directly on the price matrix (assuming `price` matrix is already created via `panel`). The `mavg(price, 2)` function operates column-wise, computing the average for each stock symbol over a sliding window of size 2.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/panel_data.md#_snippet_14\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nmavg(price,2);\n```\n\n----------------------------------------\n\nTITLE: Creating Fee Summary Statistics by Fund Type in DolphinDB\nDESCRIPTION: Defines a custom function for calculating summary statistics and uses it to generate comprehensive fee statistics grouped by fund type, including count, mean, standard deviation, min, quartiles, max, and median.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/public_fund_basic_analysis/basic_analysis_of_public_fund_open_market_data.txt#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// user defined summary statistics function\ndef describe(x){\n\ty = stat(x)\n\tq_25 = quantile(x, 0.25)\n\tq_50 = quantile(x, 0.50)\n\tq_75 = quantile(x, 0.75)\n\treturn y.Count join y.Avg join y.Stdev join y.Min join q_25 join q_50 join q_75 join y.Max join y.Median\n\t\n}\n// query the summary of public fund fees\nselect describe(Fee) as `count`mean`std`min`q_25`q_50`q_75`max`median from fundFee group by Type\n```\n\n----------------------------------------\n\nTITLE: Performance Testing: Aggregate with Multiple Partitions and Sorting\nDESCRIPTION: Computes standard deviation of bid prices and sum of bid sizes for selected symbols over a specified time and date interval; results are grouped and sorted by symbol and time.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs InfluxDB/test_dolphindb_big.txt#_snippet_13\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 5. 聚合查询.多分区维度 + 排序\n\ntimer\nselect \n\tstd(bid)  as std_bid,\n\tsum(bidsiz) as sum_bidsiz \nfrom taq \nwhere \n\tdate == 2007.08.07,\n\ttime between 09:00:00 : 21:00:00,\n\tsymbol in `IBM`MSFT`GOOG`YHOO,\n\tbidsiz >= 20,\n\tofr > 20\ngroup by symbol, minute(time) \norder by symbol asc, minute_time asc\n```\n\n----------------------------------------\n\nTITLE: 实现类似 MATLAB pwelch 的功率谱密度估计函数（DolphinDB 脚本）\nDESCRIPTION: 此脚本实现了pwelch函数，用于对信号进行分段加窗、FFT变换、平均处理以估算功率谱密度。支持输入信号、窗函数、分段重叠、FFT点数和采样频率，输出加速度的PSD及对应频率，适用于振动分析。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Random_Vibration_Signal_Analysis_Solution.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef pwelch(data, window, noverlap, nfft, fs){\n    N = data.size();\n    M = nfft;\n    pfft = 10*log10(fs\\nfft);\n    n_overlap = ceil(M * noverlap * 0.01);\n    L = floor((N - n_overlap) / (M - n_overlap));\n    N_used = (M - n_overlap) * (L - 1) + M;\n    noise_used = data[0:(N_used)];\n    P_win = sum(window * window)/M;\n    if (mod(nfft, 2) == 1){\n        f = ((0 .. ((nfft + 1)/2)) * fs *1.0/ nfft); \n        fft_sum = array(DOUBLE, (nfft + 1)/2)\n        abs_fft_half = array(DOUBLE, (nfft + 1)/2)\n    } else {\n        f = ((0 .. (nfft / 2)) * fs *1.0/ nfft);\n        fft_sum = array(DOUBLE, (nfft/2 + 1))\n        abs_fft_half = array(DOUBLE, (nfft/2 + 1))\n    }\n    for(n in 1..L){\n        nstart = ( n - 1 )*(M - n_overlap);\n        temp_noise = noise_used[nstart : (nstart + M)] * window;\n        temp_fft = signal::mul(signal::fft(temp_noise), 1.0/nfft);\n        temp_fft_half = [temp_fft[0]]\n        temp_fft_half.append!(signal::mul(temp_fft[1:(nfft/2)], 2.0))\n        temp_fft_half.append!(temp_fft[nfft/2])\n        abs_fft_half[0] = pow(signal::abs(temp_fft_half[0]), 2.0);\n        end = pow(signal::abs(temp_fft_half[nfft/2]), 2);\n        abs_fft_half = abs_fft_half[0:1].append!(pow(signal::abs(temp_fft_half[1:(nfft/2)]), 2) * 0.5);\n        abs_fft_half.append!(end);\n        fft_sum = fft_sum + 1.0/ P_win * abs_fft_half;\n    }\n    fft_sum = fft_sum*1.0 / L;\n    return [pow(10, log10(fft_sum) - pfft*1.0 / 10), f]; \n}\n```\n\n----------------------------------------\n\nTITLE: Calculating Hourly Average Temperature with Filtering in DolphinDB Script\nDESCRIPTION: This script calculates the average temperature for a specific device and point during the hour preceding a given timestamp. It selects data from the 'pt' table within the calculated time window (now - 3600 seconds to now) and then uses 'group by' to compute the average 'propertyValue' for the specified device and property code.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Iot_intelligent_O&M.md#_snippet_9\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\n//案例 9：分组汇总计算，条件筛选\ndevice=\"361RP88\"        //设备编号\npoint=\"361RP88010\"      //测点编号，记录温度的测点，得到的 propertyValue 表示温度\n//过去一小时数据\nnow=2022.01.01 23:00:00\ndt=select * from pt where ts>=datetime(now)-3600 and ts<now and deviceCode=device and propertyCode=point\n//输出\nselect deviceCode,propertyCode,now as max_ts,avg(propertyValue) as hour_avg from dt group by deviceCode,propertyCode\n```\n\n----------------------------------------\n\nTITLE: Feature Extraction Engine for Market Data - DolphinDB\nDESCRIPTION: Defines the 'featureEngine' function to compute features such as bid-ask spread (BAS), weighted average price (WAP), directional indicators (DI0~DI9), pressure, and realized volatility (RV) using matrix operations on order book data. Requires matrices of bid/offer prices and quantities, returns multiple aggregated feature values as a tuple. Used to transform raw market data into suitable features for machine learning tasks.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/machine_learning_volatility/04.streamComputing.txt#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndefg featureEngine(bidPrice,bidQty,offerPrice,offerQty){\n\tbas = offerPrice[0]\\bidPrice[0]-1\n\twap = (bidPrice[0]*offerQty[0] + offerPrice[0]*bidQty[0])\\(bidQty[0]+offerQty[0])\n\tdi = (bidQty-offerQty)\\(bidQty+offerQty)\n\tbidw=(1.0\\(bidPrice-wap))\n\tbidw=bidw\\(bidw.rowSum())\n\tofferw=(1.0\\(offerPrice-wap))\n\tofferw=offerw\\(offerw.rowSum())\n\tpress=log((bidQty*bidw).rowSum())-log((offerQty*offerw).rowSum())\n\trv=std(log(wap)-log(prev(wap)))*sqrt(24*252*size(wap))\n\treturn avg(bas),avg(di[0]),avg(di[1]),avg(di[2]),avg(di[3]),avg(di[4]),avg(di[5]),avg(di[6]),avg(di[7]),avg(di[8]),avg(di[9]),avg(press),rv\n}\nmetrics=<featureEngine(\n\tmatrix(BidPrice0,BidPrice1,BidPrice2,BidPrice3,BidPrice4,BidPrice5,BidPrice6,BidPrice7,BidPrice8,BidPrice9),\n\tmatrix(BidOrderQty0,BidOrderQty1,BidOrderQty2,BidOrderQty3,BidOrderQty4,BidOrderQty5,BidOrderQty6,BidOrderQty7,BidOrderQty8,BidOrderQty9),\n\tmatrix(OfferPrice0,OfferPrice1,OfferPrice2,OfferPrice3,OfferPrice4,OfferPrice5,OfferPrice6,OfferPrice7,OfferPrice8,OfferPrice9),\n\tmatrix(OfferOrderQty0,OfferOrderQty1,OfferOrderQty2,OfferOrderQty3,OfferOrderQty4,OfferOrderQty5,OfferOrderQty6,OfferOrderQty7,OfferOrderQty8,OfferOrderQty9)) as `BAS`DI0`DI1`DI2`DI3`DI4`DI5`DI6`DI7`DI8`DI9`Press`RV>\n\n```\n\n----------------------------------------\n\nTITLE: Filtering by Floating-Point, Grouping by Stock Code - DolphinDB\nDESCRIPTION: This DolphinDB script filters the \"trades\" table based on the \"amount\" column, selecting records where the amount is between 5000 and 13000. Then, it groups the filtered data by \"ts_code\" and calculates the average high price for each group. The `timer(10)` function measures the performance, and `clearAllCache()` clears the cache before execution.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_23\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//按浮点型过滤，按股票代码分组\ntimer(10) select avg(higb) from trades where amount between 5000:13000 group by ts_code\n```\n\n----------------------------------------\n\nTITLE: Enabling High Availability in DolphinDB API using Java\nDESCRIPTION: The snippet demonstrates how to enable high availability when connecting to a DolphinDB cluster via its Java API client. It defines an array of node endpoints (sites) that represent data nodes in the cluster, then establishes a connection with high availability parameters set to true. The snippet requires a DolphinDB Java API client and relevant network access. Inputs are host, port, user credentials, and the list of cluster sites. Outputs should reflect either successful or failed connections with automatic failover handling.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/iot_examples.md#_snippet_32\n\nLANGUAGE: java\nCODE:\n```\nString[]  sites={\"192.168.1.12:22217\", \"192.168.1.12:22218\", \"192.168.1.13:22217\", \"192.168.1.13:22218\", \"192.168.1.14:22217\", \"192.168.1.14:22218\"}\ns.connect(host=\"192.168.1.12\", port=21117, userid=\"admin\", password=\"123456\", highAvailability=True, highAvailabilitySites=sites)\n```\n\n----------------------------------------\n\nTITLE: Cleaning Environment for XTP Subscription\nDESCRIPTION: Script to clean up existing XTP connections, unsubscribe from stream tables, and drop stream tables to ensure the example scripts can be executed repeatedly without conflicts.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/xtp.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntry {\n    xtpConn = XTP::getHandle(\"xtpConn\")\n    XTP::closeXTPConnection(xtpConn)\n}\ncatch (ex) {\n    print(ex)\n}\ngo\n// 取消订阅\ntry { unsubscribeTable(tableName=\"actualMarketDataStream\", actionName=\"actualMarketDataAction\") } catch(ex) { print(ex) }\ntry { unsubscribeTable(tableName=\"entrustStream\", actionName=\"entrustAction\") } catch(ex) { print(ex) }\ntry { unsubscribeTable(tableName=\"tradeStream\", actionName=\"tradeAction\") } catch(ex) { print(ex) }\ntry { unsubscribeTable(tableName=\"stateStream\", actionName=\"stateAction\") } catch(ex) { print(ex) }\ntry { unsubscribeTable(tableName=\"orderBookStream\", actionName=\"orderBookAction\") } catch(ex) { print(ex) }\ntry { unsubscribeTable(tableName=\"indexMarketDataStream\", actionName=\"indexMarketDataAction\") } catch(ex) { print(ex) }\ntry { unsubscribeTable(tableName=\"optionMarketDataStream\", actionName=\"optionMarketDataAction\") } catch(ex) { print(ex) }\ntry { unsubscribeTable(tableName=\"bondMarketDataStream\", actionName=\"bondMarketDataAction\") } catch(ex) { print(ex) }\ngo\n// 取消流表\ntry { dropStreamTable(tableName=\"actualMarketDataStream\") } catch(ex) { print(ex) }\ntry { dropStreamTable(tableName=\"entrustStream\") } catch(ex) { print(ex) }\ntry { dropStreamTable(tableName=\"tradeStream\") } catch(ex) { print(ex) }\ntry { dropStreamTable(tableName=\"stateStream\") } catch(ex) { print(ex) }\ntry { dropStreamTable(tableName=\"orderBookStream\") } catch(ex) { print(ex) }\ntry { dropStreamTable(tableName=\"indexMarketDataStream\") } catch(ex) { print(ex) }\ntry { dropStreamTable(tableName=\"optionMarketDataStream\") } catch(ex) { print(ex) }\ntry { dropStreamTable(tableName=\"bondMarketDataStream\") } catch(ex) { print(ex) }\n```\n\n----------------------------------------\n\nTITLE: Logging In to DolphinDB Server (DolphinDB Script)\nDESCRIPTION: Logs in to the DolphinDB server with the provided administrator credentials using the login function. This is a required step to obtain permissions for subsequent operations such as table creation and engine registration. The username and password are hardcoded as 'admin' and '123456', which should be replaced with secure credentials in production environments.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/stock_market_replay/02.calTradeCost_asofJoin.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(\"admin\", \"123456\")\n```\n\n----------------------------------------\n\nTITLE: Calling Function with Full Namespace Path from Imported Namespaced Module\nDESCRIPTION: Shows the explicit way to call a function from an imported namespaced module by providing its full namespace path (system::log::fileLog::). This method is always unambiguous and is required if the function name conflicts with other functions in the current scope.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/module_tutorial.md#_snippet_11\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nsystem::log::fileLog::appendLog(\"mylog.txt\", \"test my log\")\n```\n\n----------------------------------------\n\nTITLE: 使用Python API将数据写入DolphinDB分布式数据库\nDESCRIPTION: 通过Python API创建分布式数据库，并将pandas DataFrame数据写入到DolphinDB的分区表中。示例展示了数据库创建、表结构定义和数据写入的完整流程。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/quant_finance_examples.md#_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\nimport dolphindb as ddb\nimport dolphindb.settings as keys\nimport pandas as pd\nimport numpy as np\ndbPath = \"dfs://level2API\"\ntableName = \"quotes\"\ns = ddb.session()\ns.connect(\"127.0.0.1\",8848,\"admin\",\"123456\")\n\nif(s.existsDatabase(dbPath)):\n    s.dropDatabase(dbPath)\ns.database('dbDate',partitionType=keys.VALUE,partitions=\"2020.01.01..2020.12.31\")\ns.database('dbSymbol',partitionType=keys.HASH,partitions=\"[SYMBOL,10]\")\ns.database('db', partitionType=keys.COMPO, partitions=\"[dbDate, dbSymbol]\", dbPath=dbPath)\n\ndata=pd.DataFrame({\"symbol\":['600007','600104'],\"dt\":[np.datetime64('2020-01-01T20:01:01'),np.datetime64('2020-01-01T20:01:02')],\"last\":[100.36,99.3],\"askPrice1\":[100.36,101.22], \"bidPrice1\":[100.35,100.45],\"askVolume1\":[4138,2],\"bidVolume1\":[20,39],\"volume\":[1,5]})\ns.run(\"db.createPartitionedTable(table(1:0, `symbol`datetime`last`askPrice1`bidPrice1`askVolume1`bidVolume1`volume, [SYMBOL,DATETIME,DOUBLE,DOUBLE,DOUBLE,INT,INT,INT]),`quotes, `datetime`symbol)\")\ns.upload({\"tmpdata\":data})\ns.run(\"data=select symbol, datetime(dt) as datetime, last, askPrice1, bidPrice1, askVolume1, bidVolume1, volume from tmpdata\")\ns.run(\"tableInsert(loadTable('{db}','{tb}'),data)\".format(db=dbPath,tb=tableName))\n```\n\n----------------------------------------\n\nTITLE: Loop: Calculate maximum value for each column\nDESCRIPTION: This DolphinDB script calculates the maximum value for each column in a table using the `loop` function. The loop function always returns a tuple\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_22\n\nLANGUAGE: shell\nCODE:\n```\nt = table(1 2 3 as id, 4 5 6 as value, `IBM`MSFT`GOOG as name);\nt;\nloop(max, t.values());\n```\n\n----------------------------------------\n\nTITLE: Main Execution: Environment Cleanup and Stream Table Setup\nDESCRIPTION: This code executes the environment cleanup by invoking the cleanEnvironment() function, then defines database and table names, and calls createStreamTable to initialize and share new stream tables. It sets up a clean, ready state for streaming data analysis in DolphinDB.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/Real-Time_Stock_Price_Increase_Calculation/02.清理环境并创建相关流数据表.txt#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\ncleanEnvironment()\ndbName, tbName = \"dfs://SH_TSDB_snapshot_ArrayVector\", \"snapshot\"\ncreateStreamTable(dbName, tbName)\n```\n\n----------------------------------------\n\nTITLE: 全分区数据全量查询性能测试\nDESCRIPTION: 查询单天全量数据，比较合并前后在大量数据扫描场景中的耗时差异，验证合并对大规模扫描的提升作用。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/tsdb_explained.md#_snippet_5\n\nLANGUAGE: SQL\nCODE:\n```\nselect * from loadTable(dbName, tbName_all) where datetime=2023.07.10\n```\n\n----------------------------------------\n\nTITLE: Deleting Rows from Memory Table with erase!\nDESCRIPTION: Demonstrates deleting rows from a memory table using the `erase!` function with a boolean expression to specify the rows to delete.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/partitioned_in_memory_table.md#_snippet_22\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntrades.erase!(< qty3<30 >);\n```\n\n----------------------------------------\n\nTITLE: Implementing First-Level Anomaly Detection Using FilterPicker Plugin - DolphinDB Script\nDESCRIPTION: This snippet implements a real-time first-level seismic anomaly detection handler using DolphinDB. The pickerHandler function groups streaming data by tagid, creates FilterPicker plugin handlers per group, applies template matching to detect seismic spikes, and appends detected spikes with timestamps and channel ids to a result stream. The snippet also subscribes to the 'dataStream' streaming table with this handler, specifying batch size and reconnection options. It relies on the FilterPicker plugin, streaming tables, and real-time data availability.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Earthquake_Prediction_with_DolphinDB_and_Machine_Learning.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\n/*\n *一级异常检测处理函数\n *计算波形实时数据中的突峭时间点\n *返回值为突峭时间点和通道id\n */\ndef pickerHandler(mutable pickerSt, mutable res, vz, msg){\n\ttags=groups(msg[`tagid]) //按tagid分组\n\tfor (tag in tags.keys()){\n\t\tre=res[tag]\n\t\tif(re==NULL){\n\t\t\t//创建filter handler\n\t\t\tre= filterPicker::createFilterPicker()\n\t\t\tres[tag]=re\n\t\t}\n\t\t//将数据传入filterPicker进行计算\n\t\tvt=filterPicker::filterPicker(re,msg[tags[tag]][`ts], msg[tags[tag]][`data].float(), 1000)\n\t\tif(vt.size()>0){\n\t\t\tvid=take(tag,vt.size())\n\t\t\tpickerSt.append!(table(vt as ts,vid as id))\n\t\t}\n\t}\n}\n\nvz=exec id from loadTable(\"dfs://seis01\",\"tagInfo\") where chn ilike(\"%z\") //只需要计算Z分量的数据\nres=dict(INT, ANY)\n//订阅dataStream，进行一级异常检测，异常检测结果输出到filterStream中\nsubscribeTable(tableName = \"dataStream\", actionName=\"filterPickerPredict\", offset=-1,handler=pickerHandler{objByName(\"pickerStream\"), res, vz, }, msgAsTable=true, batchSize = 2000, throttle=1,reconnect=true)\n```\n\n----------------------------------------\n\nTITLE: Defining Environment Cleanup Function in DolphinDB Script\nDESCRIPTION: Defines a function `cleanEnvironment` to remove existing stream table subscriptions, drop stream engines, and drop stream tables to ensure a clean state before running the main script. It uses try-catch blocks to handle potential errors if elements don't exist.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/machine_learning_volatility/05.streamComputingArrayVector.txt#_snippet_0\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\ndef cleanEnvironment(){\n\ttry{ unsubscribeTable(tableName=`snapshotStream, actionName=\"aggrFeatures10min\") } catch(ex){ print(ex) }\n\ttry{ unsubscribeTable(tableName=`aggrFeatures10min, actionName=\"predictRV\") } catch(ex){ print(ex) }\n\ttry{ dropStreamEngine(\"aggrFeatures10min\") } catch(ex){ print(ex) }\n\ttry{ dropStreamTable(`snapshotStream) } catch(ex){ print(ex) }\n\ttry{ dropStreamTable(`aggrFeatures10min) } catch(ex){ print(ex) }\n\ttry{ dropStreamTable(`result1min) } catch(ex){ print(ex) }\n\tundef all\n}\ncleanEnvironment()\ngo\n```\n\n----------------------------------------\n\nTITLE: Left Join with Alarm and Action Tables\nDESCRIPTION: This code showcases a left join (`lj`) operation between an `alarm` table and an `alarmAction` table, using a temporary table `tmp` created from the joins of confirm and clear actions. It simulates an alarm system by creating alarm records, simulating the confirmation and clearing of these records, and then joining the results to display the alarms along with their confirmation and clearing information. This demonstrates how to track and manage events in a distributed database.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/iot_examples.md#_snippet_12\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//创建告警表和告警操作流水表\nalarm = table(1:0, `ID`deviceID`time`priority`desc, [UUID,SYMBOL, DATETIME, INT, STRING])\ndb1 = database(, VALUE, 2020.01.01..2020.12.31)\ndb2 = database(, HASH, [SYMBOL, 10])\ndb = database(\"dfs://demo\", COMPO, [db1,db2])\ndb.createPartitionedTable(alarm, \"alarm\", `time`deviceID)\n\nalarmAction = table(1:0, `ID`deviceID`time`action`actionTime`user`remark, [UUID,SYMBOL,DATETIME, SYMBOL, DATETIME, SYMBOL, STRING])\ndb = database(\"dfs://demo\")\ndb.createPartitionedTable(alarmAction, \"alarmAction\", `time`deviceID)\n\n//告警表中模拟写入3条记录\nalarm = loadTable(\"dfs://demo\",\"alarm\")\naction = loadTable(\"dfs://demo\",\"alarmAction\")\nt = table([uuid(\"b9f36c85-43ce-45a3-8427-b3529a6c41cc\"),uuid(\"baba6fe6-67b9-4c18-885a-3fd6edd3002e\"),uuid(\"b510c959-4f9b-4eaf-81a7-ee7e420810c0\")] as UUID,\"BDSA11_PSCADA\" \"DMS01\" \"XKZDEPOT_CCTV_DI20\" as deviceID,take(now().datetime(),3) as time,1..3 as priority,\"CONN_FAIL\" \"OFFLINE\" \"ERROR\" as descs)\nalarm.append!(t)\n\n//确认2条告警，写入告警操作流水表\narrUUID = uuid([\"b9f36c85-43ce-45a3-8427-b3529a6c41cc\",\"baba6fe6-67b9-4c18-885a-3fd6edd3002e\"])\ntmp = select ID,deviceId,  time,\"CONFIRM\" as action, now().datetime() as actionTime, \"admin\" as actionUser, \"test confirm\" as remark from alarm where time > today().datetime(),  ID in arrUUID\naction.tableInsert(tmp)\n\n//清除1条告警，写入告警操作流水表\ntmp = select ID,deviceId,  time,\"CLEAR\" as action, now().datetime() as actionTime, \"admin\" as actionUser, \"test clear\" as remark from alarm where time > today().datetime(),  ID =uuid(\"b9f36c85-43ce-45a3-8427-b3529a6c41cc\")\naction.tableInsert(tmp)\n\n//查询告警信息以及确认、清除信息\nt1 = select ID,time, deviceID, actionTime,user as confirmUser, remark as confirmRemark from action where time > today().datetime(), action=\"CONFIRM\"\nt2 = select ID, actionTime, user as clearUser, remark as clearRemark from action where time > today().datetime(), action=\"CLEAR\"\ntmp = lsj(t1,t2,`ID)\nselect * from lj(alarm, tmp,`time`deviceId`ID,`time`deviceID`ID) where time > today().datetime()\n```\n\n----------------------------------------\n\nTITLE: Column-wise and Row-wise Indexing in Array Vectors - DolphinDB Script\nDESCRIPTION: These DolphinDB code snippets show how to access elements in Array Vector or Columnar Tuple variables by first specifying columns and then rows, using both scalar and vector indices. They demonstrate flexible order of accessing and out-of-bound handling, where null values are returned as needed. Operations require valid initialized array or tuple variables, and results mirror the subscript/slicing chosen.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Array_Vector.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 用下标先定位某一列，再定位某一行\nx = array(INT[], 0).append!([1 2 3, 4 5, 6 7 8, 9 10])\nx[c][r]\n/*\n7\n*/\n\ny = [1 2 3, 4 5, 6 7 8, 9 10].setColumnarTuple!()\ny[c][r]\n/*\n7\n*/\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 用下标先定位某几列，再定位某几行\nx = array(INT[], 0).append!([1 2 3, 4 5, 6 7 8, 9 10])\nx[cols][rows]\n/*\n[[4,5],[6,7],[9,10]]\n*/\n\ny = array(INT[], 0).append!([1 2 3, 4 5, 6 7 8, 9 10])\ny[cols][rows]\n/*\n[[4,5],[6,7],[9,10]]\n*/\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 当 index 越界时，空值填充\nx = array(INT[], 0).append!([1 2 3, 4 5, 6 7 8, 9 10])\nx[1:3][2 3 4]\n/*\n[[7,8],[10,],]\n*/\n\ny = [1 2 3, 4 5, 6 7 8, 9 10].setColumnarTuple!()\ny[1:3][2 3 4]\n/*\n[[7,8],[10,],]\n*/\n```\n\n----------------------------------------\n\nTITLE: Sharing Aggregation Output Stream Table DolphinDB Script\nDESCRIPTION: Creates and shares a stream table named `sensorTempAvg`. This table is designated to receive the output data from the time-series aggregator, specifically designed to store aggregated temperature values (averages) with corresponding timestamps and hardware IDs.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/iot_demo_script.txt#_snippet_3\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nshare streamTable(1000000:0, `time`hardwareId`tempavg1`tempavg2`tempavg3, [TIMESTAMP,INT,DOUBLE,DOUBLE,DOUBLE]) as sensorTempAvg\n```\n\n----------------------------------------\n\nTITLE: Defining Weights Dictionary and Aggregation Function for ETF Value in DolphinDB Script\nDESCRIPTION: Defines a dictionary specifying the constituent weights for selected symbols and a custom aggregation function etfVal for computing ETF intrinsic value using weighted sums. This is a required preparation step for setting up cross-sectional streaming aggregators in subsequent analytics.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/historical_data_replay.md#_snippet_8\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndefg etfVal(weights,sym, price) {\n    return wsum(price, weights[sym])\n}\nweights = dict(STRING, DOUBLE)\nweights[`AAPL] = 0.1\nweights[`IBM] = 0.1\nweights[`MSFT] = 0.1\nweights[`NTES] = 0.1\nweights[`AMZN] = 0.1\nweights[`GOOG] = 0.5\n```\n\n----------------------------------------\n\nTITLE: Creating Partitioned Data Sources in DolphinDB Plugin using C++\nDESCRIPTION: This function, myDataDS, dynamically creates multiple data source objects for partitioned data reading in DolphinDB plugins using C++. It determines the file length, splits data into manageable partitions, and leverages Util::createRegularFunctionCall to construct deferred loadMyData exec calls for each chunk. Required dependencies include DolphinDB C++ plugin headers, particularly Util and DataSource support. Parameters: path (string), start (int, optional), length (int, optional); outputs a vector of DataSource instances encapsulating the partitioned calls. This is useful for efficient, on-demand, chunk-wise data processing.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_development_tutorial.md#_snippet_22\n\nLANGUAGE: C++\nCODE:\n```\nConstantSP myDataDS(Heap *heap, vector<ConstantSP> &args) {\n    ConstantSP path = args[0];\n    long long fileLength = Util::getFileLength(path->getString());\n    size_t bytesPerRow = 32;\n\n    int start = args.size()>= 2 ? args[1]->getInt() : 0;\n    int length = args.size()>= 3 ? args[2]->getInt() : fileLength / bytesPerRow - start;\n\n    int sizePerPartition = 16 * 1024 * 1024;\n    int partitionNum = fileLength / sizePerPartition;\n\n    int partitionStart = start;\n    int partitionLength = length / partitionNum;\n\n    FunctionDefSP func = Util::createSystemFunction(\"loadMyData\", loadMyData, 1, 3, false);\n    ConstantSP dataSources = Util::createVector(DT_ANY, partitionNum);\n    for (int i = 0; i < partitionNum; i++) {\n        if (i == partitionNum - 1)\n            partitionLength = length - partitionLength * i;\n        vector<ConstantSP> partitionArgs = {path, new Int(partitionStart), new Int(partitionLength)};\n        ObjectSP code = Util::createRegularFunctionCall(func, partitionArgs);    // 将会调用 loadMyData(path, partitionStart, partitionLength)\n        dataSources->set(i, new DataSource(code));\n    }\n    return dataSources;\n}\n```\n\n----------------------------------------\n\nTITLE: Querying - Cross Join DolphinDB\nDESCRIPTION: This snippet performs a cross join (Cartesian product) between a subset of the `readings` table and the `device_info` table. It selects all columns from the result.  This example shows how to combine every record from one table with every record from another.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs TimescaleDB/test_dolphindb_small.txt#_snippet_21\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 12. 关联查询.笛卡尔积（cross join）\ntimer\nselect *\nfrom cj((select * from readings where time = 2016.11.15 07:00:00), device_info)\n```\n\n----------------------------------------\n\nTITLE: Setting Up OLAP Distributed Table for Cache Examples (DolphinDB Script)\nDESCRIPTION: Initializes a distributed database ('dfs://mem') and a partitioned table ('pt1') using the OLAP engine. It defines the schema, creates partitions based on 'day', and populates the table with simulated data across 30 daily partitions. This setup is used as a base for subsequent memory management examples.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/memory_management.md#_snippet_4\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nlogin(`admin,`123456)\nif(existsDatabase(\"dfs://mem\")){\n\tdropDatabase(\"dfs://mem\")\n}\ndb = database(\"dfs://mem\",VALUE,2022.01.01..2022.01.30)\nm = \"tag\" + string(1..9)\nschema = table(1:0,`id`day join m, [INT,DATE] join take(LONG,9) )\ndb.createPartitionedTable(schema,\"pt1\",`day)\n//写入模拟数据\nfor (i in 0..29){\n\tt=table(1..10000000 as tagid,take(2022.01.01+i,10000000) as date,1..10000000 as tag1,1..10000000 as tag2,1..10000000 as tag3,1..10000000 as tag4,1..10000000 as tag5,1..10000000 as tag6,1..10000000 as tag7,1..10000000 as tag8,1..10000000 as tag9 )\n\tdfsTable=loadTable(\"dfs://mem\",\"pt1\")\n\tdfsTable.append!(t)\n}\n\n//查询15个分区的数据\nt=select top 15 * from rpc(getControllerAlias(),getClusterChunksStatus) where file like \"/mem%\" and replicas like \"node1%\" order by file\npnodeRun(clearAllCache)\ndays=datetimeParse(t.file.substr(5,8),\"yyyyMMdd\")\nfor(d in days){\n    select * from loadTable(\"dfs://mem\",\"pt1\") where  day= d\n    print mem().allocatedBytes - mem().freeBytes\n}\n```\n\n----------------------------------------\n\nTITLE: Calculating HM Model Parameters in DolphinDB\nDESCRIPTION: These functions calculate the parameters of the HM (presumably Henriksson-Merton) model. They use the `ols` (ordinary least squares) function to perform a regression. The inputs are the fund's value and the benchmark's price.  The model includes an intercept, a coefficient for the benchmark return, and a coefficient for the upside capture of the benchmark.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_factor.txt#_snippet_11\n\nLANGUAGE: DolphinDB\nCODE:\n```\n/**\n * 因子10：HM模型选股能力\n * y 为 基金日收益率的-(0.03\\252)\n * 截距为1\n * x1 = 基准收益率 - (0.03\\252)\n * x2 = if(x1 > 0, x1, 0)\n */\n defg getHM1(value, price){\n \triskRatio = 0.03\\252\t \n \treturn ols(deltas(value)\\prev(value) - riskRatio, (deltas(price)\\prev(price) - riskRatio, iif( deltas(price)\\prev(price) - riskRatio > 0, deltas(price)\\prev(price) - riskRatio, 0))) [0] as intercept\n\n }\n  defg getHM2(value, price){\n \triskRatio = 0.03\\252\t \n \treturn ols(deltas(value)\\prev(value) - riskRatio, (deltas(price)\\prev(price) - riskRatio, iif( deltas(price)\\prev(price) - riskRatio > 0, deltas(price)\\prev(price) - riskRatio, 0))) [1] as Cofficient1\n\t\n }\n \n  defg getHM3(value, price){\n \triskRatio = 0.03\\252\t \n \treturn ols(deltas(value)\\prev(value) - riskRatio, (deltas(price)\\prev(price) - riskRatio, iif( deltas(price)\\prev(price) - riskRatio > 0, deltas(price)\\prev(price) - riskRatio, 0))) [2] as Cofficient2\t\n }\n```\n\n----------------------------------------\n\nTITLE: Initializing Composite Partitioned Database and Table in DolphinDB Script\nDESCRIPTION: Creates a composite partitioned database \"dfs://compoDB\" combining date (value) and ID (range) partitions, generates a table with random data, and appends it to a partitioned table in the created database. Dependencies include the DolphinDB environment and built-in functions for partitioning. Key parameters are the record count (n), ID value range, date range, and table schema. Inputs include random ID, date, and numeric columns, while output results in a persistent partitioned table. The snippet sets up the initial database and table structures necessary for subsequent backup and restore operations.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/restore-backup.md#_snippet_9\n\nLANGUAGE: DolphinDB\nCODE:\n```\nn=1000000\nID=rand(100, n)\ndates=2017.08.07..2017.08.11\ndate=rand(dates, n)\nx=rand(10.0, n)\nt=table(ID, date, x);\n\ndbDate = database(, VALUE, 2017.08.07..2017.08.11)\ndbID=database(, RANGE, 0 50 100);\ndb = database(\"dfs://compoDB\", COMPO, [dbDate, dbID]);\npt = db.createPartitionedTable(t, `pt, `date`ID)\npt.append!(t);\n```\n\n----------------------------------------\n\nTITLE: Telegraf Output Plugin Configuration for DolphinDB in TOML\nDESCRIPTION: TOML configuration snippet for Telegraf's DolphinDB output plugin. It configures connection parameters such as address, user, password, and batch write settings (batch_size, throttle). The database parameter is commented out to use a stream table instead of a partitioned database. Key parameters include specifying the metrics name, the table name 'cpu_stream', goroutine_count for concurrency control, partition column, and enabling debug mode for verbose logging. Inputs: metrics collected by Telegraf. Outputs: batched and throttled write to DolphinDB streaming table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_Telegraf_Grafana.md#_snippet_3\n\nLANGUAGE: TOML\nCODE:\n```\n[[outputs.dolphindb]]\n# The address of DolphinDB formatted as {ip}:{port}\naddress = \"localhost:8848\"\n# The user of DolphinDB.\nuser = \"admin\"\n# The password of DolphinDB.\npassword = \"123456\"\n# Name of the database to store metrics in.\n# database = \"dfs://telegraf\"\n# Name of the table to store metrics in.\ntable_name = \"cpu_stream\"\n# The number of messages to be batched.\n# If batch_size is 1, the data is sent to the server immediately after it is written.\n# If batch_size is more than 1, the data is sent to the server only when the amount of data reaches batch_size.\nbatch_size = 10\n# An integer more than 0 in milliseconds.\n# The data will be sent to the server after written for throttle time,\n# even through the data amount is less than the batch_size.\nthrottle = 1000\n# The name of the metrics.\nmetric_name = \"cpu\"\n# The count of the goroutine to send the metrics data to the DolphinDB.\ngoroutine_count = 1\n# The partition column of the table, only works when the goroutine_count is more than 1.\npartition_col = \"timestamp\"\n# If use the debug mode.\ndebug = true\n```\n\n----------------------------------------\n\nTITLE: Defining Custom State Function multiFactors\nDESCRIPTION: This DolphinDB script defines another custom state function named `multiFactors` using the `@state` decorator. This function computes multiple factors using a combination of built-in state functions. It demonstrates the flexibility of defining multiple factors and using intermediate results within a single function.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/reactive_state_engine.md#_snippet_5\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\n@state\ndef multiFactors(lowPrice, highPrice, volumeTrade, closePrice, buy_active, sell_active, tradePrice, askPrice1, bidPrice1, askPrice10, agg_vol, agg_amt){\n    a = ema(askPrice10, 30)\n    term0 = ema((lowPrice - a) / (ema(highPrice, 30) - a), 50)\n    term1 = mrank((highPrice - a) / (ema(highPrice, 5) - a), true,  15)\n    term2 = mcorr(askPrice10, volumeTrade, 10) * mrank(mstd(closePrice, 20, 20), true, 10)\n    buy_vol_ma = mavg(buy_active, 6)\n    sell_vol_ma = mavg(sell_active, 6)\n    zero_free_vol = iif(agg_vol==0, 1, agg_vol)\n    stl_prc = ffill(agg_amt \\ zero_free_vol \\ 20).nullFill(tradePrice)\n    buy_prop = stl_prc\n\n    spd = askPrice1 - bidPrice1\n    spd_ma = round(mavg(iif(spd < 0, 0, spd), 6), 5)\n    term3 = buy_prop * spd_ma\n    term4 = iif(spd_ma == 0, 0, buy_prop / spd_ma)\n    return term0, term1, term2, term3, term4\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Dictionaries in DolphinDB C++ Plugin\nDESCRIPTION: This snippet demonstrates how to create dictionaries in a DolphinDB C++ plugin using the `Util::createDictionary` function. It shows how to create vectors for keys and values and then set them in the dictionary.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_advance.md#_snippet_4\n\nLANGUAGE: c++\nCODE:\n```\nVectorSP keyVec = Util::createIndexVector(1, 5);        // 相当于 1..5，作为key值\nVectorSP valVec = Util::createVector(DT_DOUBLE, 0, 5);  // 创建一个初始长度为0，容量为5的double类型，作为value\nstd::vector<double> tem{2.5, 3.3, 1.0, 6.6, 8.8};\nvalVec->appendDouble(tem.data(), tem.size());           // 向valVec添加数据\n//创建一个key类型为int、value类型为double的字典对象，第2、第4参数为SymbolBaseSP，与symbol类型相关，非symbol类型则设置为nullptr\nDictionarySP d = Util::createDictionary(DT_INT, nullptr, DT_DOUBLE, nullptr);\nd->set(keyVec, valVec);                                 // 设置key和value\n```\n\n----------------------------------------\n\nTITLE: Optimizing Heterogeneous Replay with Time Partitioning and Parallelism in Python\nDESCRIPTION: This code snippet optimizes the heterogeneous replay process by using `timeRepartitionSchema` in `replayDS` and the `parallelLevel` parameter in `submitJob`. It partitions the time range into 100 intervals, and uses the `parallelLevel` to define the number of working threads for loading data from database, to improve the replay performance and reduce memory consumption. This helps manage large datasets efficiently. The `submitJob` function submits the replay as a background job.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/stock_market_replay.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntimeRS = cutPoints(09:15:00.000..15:00:00.000, 100)\norderDS = replayDS(sqlObj=<select * from loadTable(\"dfs://order\", \"order\") where Date = 2020.12.31>, dateColumn=`Date, timeColumn=`Time, timeRepartitionSchema=timeRS)\ntradeDS = replayDS(sqlObj=<select * from loadTable(\"dfs://trade\", \"trade\") where Date = 2020.12.31>, dateColumn=`Date, timeColumn=`Time, timeRepartitionSchema=timeRS)\nsnapshotDS = replayDS(sqlObj=<select * from loadTable(\"dfs://snapshot\", \"snapshot\") where Date =2020.12.31>, dateColumn=`Date, timeColumn=`Time, timeRepartitionSchema=timeRS)\ninputDict = dict([\"order\", \"trade\", \"snapshot\"], [orderDS, tradeDS, snapshotDS])\n\nsubmitJob(\"replay\", \"replay stock market\", replay, inputDict, messageStream, `Date, `Time, , , 3)\n```\n\n----------------------------------------\n\nTITLE: 定义资金流大小单判断函数\nDESCRIPTION: 创建一个状态函数tagFunc，用于根据成交股数对订单进行大小单的分类。小于等于2万股的为小单(0)，2万到20万股的为中单(1)，大于20万股的为大单(2)。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_capital_flow_daily.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\n/* \n * Label small, medium and large order\n * small : 0\n * medium : 1\n * large : 2\n */\n@state\ndef tagFunc(qty){\n    return iif(qty <= 20000, 0, iif(qty <= 200000 and qty > 20000, 1, 2))\n}\n```\n\n----------------------------------------\n\nTITLE: Calculating Volume Ratios per Symbol using `context by` in DolphinDB Script\nDESCRIPTION: Demonstrates using the `context by` clause in a DolphinDB SQL query to group data by stock symbol (`sym`). Within each group (symbol), it calculates the ratio of the current volume to the previous volume using the `ratios()` sequence function.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/panel_data.md#_snippet_1\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nselect timestamp, sym, price, ratios(volume) ,volume from t context by sym;\n```\n\n----------------------------------------\n\nTITLE: Safe Concurrent Writing to Shared Partitioned Table in DolphinDB Script\nDESCRIPTION: This code shares a partitioned table object to enable thread-safe concurrent writing from multiple jobs, even to the same partition. It uses the share function and launches jobs with overlapping partition ID ranges. Dependencies include share, submitJob, and the previously defined writeData function. Enables high-concurrency writes without risking data corruption.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/in_memory_table.md#_snippet_8\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nshare pt as spt\njob1=submitJob(\"write1\",\"\",writeData,spt,1..300,1000,1000)\njob2=submitJob(\"write2\",\"\",writeData,spt,1..300,1000,1000)\n```\n\n----------------------------------------\n\nTITLE: Creating Sample Data for Volume-Based Time Windowing\nDESCRIPTION: This snippet creates a sample table `t` representing stock market data with columns `wind_code` (stock code), `date` (date), `time` (time), and `volume` (trading volume).\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_sql_cases.md#_snippet_31\n\nLANGUAGE: DolphinDB\nCODE:\n```\nN = 28\nt = table(take(`600000.SH, N) as wind_code, \n          take(2015.02.11, N) as date, \n          take(13:03:00..13:30:00, N) as time, \n          take([288656, 234804, 182714, 371986, 265882, 174778, 153657, 201388, 175937, 138388, 169086, 203013, 261230, 398971, 692212, 494300, 581400, 348160, 250354, 220064, 218116, 458865, 673619, 477386, 454563, 622870, 458177, 880992], N) as volume)\n```\n\n----------------------------------------\n\nTITLE: Submitting Batch Synchronization Jobs in DolphinDB - DolphinDB\nDESCRIPTION: Loops over a set of dates, submitting background jobs to DolphinDB for parallel data synchronization from Oracle to DolphinDB via ODBC. Uses submitJob and getRecentJobs for job management and status tracking. Useful for migrating multiple days of partitioned data efficiently.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Oracle_to_DolphinDB.md#_snippet_14\n\nLANGUAGE: DolphinDB\nCODE:\n```\nfor(dt in 2021.01.04..2021.01.05){\n\tsubmitJob(`syncOracTick, `syncOracTick, syncData, conn, dbName, tbName, dt)\n}\n// 查看后台任务\nselect * from getRecentJobs() where jobDesc = `syncOracTick\n\n```\n\n----------------------------------------\n\nTITLE: Creating Partitioned Table in DolphinDB with Single Sort Column (TradeTime)\nDESCRIPTION: This code snippet demonstrates creating a partitioned table in DolphinDB using the TSDB engine with a single sort column (TradeTime). It illustrates a scenario where the sort column leads to a high number of index keys with very few data rows per key, potentially impacting performance.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/database_and_table_creation_details.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndb=database(\"dfs://testDB1\",VALUE, 2020.01.01..2021.01.01,engine=\"TSDB\")\ncolName = `SecurityID`TradeDate`TradeTime`TradePrice`TradeQty`TradeAmount`BuyNo`SellNo\ncolType = `SYMBOL`DATE`TIME`DOUBLE`INT`DOUBLE`INT`INT\ntbSchema = table(1:0, colName, colType)\ndb.createPartitionedTable(table=tbSchema, tableName=`pt1, partitionColumns=`TradeDate, sortColumns=`TradeTime, keepDuplicates=ALL)\n```\n\n----------------------------------------\n\nTITLE: DolphinDB Client On Close Callback Function\nDESCRIPTION: Defines a static callback function `clientOnClose` that is called when a client resource is released in DolphinDB. This function safely deletes the `Client` instance and sets the resource handle to null. Prevents memory leaks when the DolphinDB session is closed.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_advance.md#_snippet_26\n\nLANGUAGE: c++\nCODE:\n```\n    static void clientOnClose(Heap* heap, vector<ConstantSP>& args) {\n        Client* pClient = (Client*)(args[0]->getLong());\n        if(pClient != nullptr) {\n            delete pClient;\n            args[0]->setLong(0);\n        }\n    }\n```\n\n----------------------------------------\n\nTITLE: Calculating ln using mavg\nDESCRIPTION: This code calculates 'ln' using the built-in `mavg` function for better performance.  It computes the moving average of 'bidPrice' with a window of 3, divides 'bidPrice' by the prior moving average (using `prev`), and takes the natural logarithm.  This method is an optimized alternative to the `moving` higher-order function.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_29\n\nLANGUAGE: shell\nCODE:\n```\n//method 1\nt2 = select *, log(bidPrice / prev(mavg(bidPrice,3))) as ln from t\n\n//method 2\nt22 = select *, log(bidPrice / mavg(prev(bidPrice),3)) as ln from t\n```\n\n----------------------------------------\n\nTITLE: Replaying data with replayDS function\nDESCRIPTION: The `replayDS` function divides the source data into smaller data sources based on the time dimension, enabling further segmentation of the data source. It takes an SQL object, date column, time column, and time repartition schema as parameters.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/faultAnalysis.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\nreplayDS(sqlObj, [dateColumn], [timeColumn],[timeRepartitionSchema])\n`ionSchema])`\n```\n\n----------------------------------------\n\nTITLE: Multi-Day Parallel Option Greeks Performance Testing in DolphinDB\nDESCRIPTION: This code initializes a table to store multi-day calculation results for option Greeks and implied volatilities. It then calls a parallel computation function to process all trading dates asynchronously, optimizing performance over the dataset.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/IV_Greeks_Calculation_for_ETF_Options_Using_JIT/calculation_scripts.txt#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\nresult = table(\n\t\tarray(SYMBOL, 0) as optionID,\n\t\tarray(DATE, 0) as tradingDate,\n\t\tarray(DOUBLE, 0) as impv,\n\t\tarray(DOUBLE, 0) as delta,\n\t\tarray(DOUBLE, 0) as gamma,\n\t\tarray(DOUBLE, 0) as vega,\n\t\tarray(DOUBLE, 0) as theta\n\t)\n//执行多日并行计算函数\ncalculateAll(closPriceWideMatrix, etfPriceWideMatrix, contractInfo, tradingDates, result)\n```\n\n----------------------------------------\n\nTITLE: Loading Text Data without Schema\nDESCRIPTION: This code loads text data from a CSV file without specifying a schema.  DolphinDB will automatically infer the data types based on the contents of the file. In this case, 'price1' and 'price2' columns with mixed numeric and character data are interpreted as SYMBOL.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_csv.md#_snippet_38\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntmpTB=loadText(dataFilePath)\ntmpTB;\n```\n\n----------------------------------------\n\nTITLE: Defining WorldQuant Alpha 1 Factor Function in DolphinDB Script\nDESCRIPTION: This snippet defines the function wqAlpha1 which implements the WorldQuant Alpha001 factor computation in DolphinDB. The function takes a 'close' price series, applies a power transformation based on the returns' sign, uses a 20-period standard deviation when returns are negative, and performs a rolling argmax (via mimax) over a five-period window. Results undergo percentile ranking and normalization. Dependencies include core DolphinDB time series and math functions such as pow, iif, mstd, ratios, mimax, and rowRank. Input is a series of closing prices; output is a normalized factor vector. The function should be integrated within a broader DolphinDB dataflow.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Streaming_computing_of_financial_quantifiers.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef wqAlpha1(close){\n    ts = mimax(pow(iif(ratios(close) - 1 < 0, mstd(ratios(close) - 1, 20), close), 2.0), 5)\n    return rowRank(X=ts, percent=true) - 0.5\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Stateful Factor Calculation on Snapshot Data in DolphinDB\nDESCRIPTION: Defines a stateful flow factor function that calculates the ratio of moving average buy volume to total volume, divided by moving average spread. Uses moving average and conditional logic to compute values from snapshot data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practice_for_factor_calculation.md#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\n@state\ndef flow(buy_vol, sell_vol, askPrice1, bidPrice1){\n        buy_vol_ma = round(mavg(buy_vol, 5*60), 5)\n        sell_vol_ma = round(mavg(sell_vol, 5*60), 5)\n        buy_prop = iif(abs(buy_vol_ma+sell_vol_ma) < 0, 0.5 , buy_vol_ma/ (buy_vol_ma+sell_vol_ma))\n        spd = askPrice1 - bidPrice1\n        spd = iif(spd < 0, 0, spd)\n        spd_ma = round(mavg(spd, 5*60), 5)\n        return iif(spd_ma == 0, 0, buy_prop / spd_ma)\n}\n\nres_flow = select TradeTime, SecurityID, `flow as factorname, flow(BidOrderQty[1],OfferOrderQty[1], OfferPrice[1], BidPrice[1]) as val from loadTable(\"dfs://LEVEL2_Snapshot_ArrayVector\",\"Snap\") where date(TradeTime) <= 2020.01.30 and date(TradeTime) >= 2020.01.01 context by SecurityID\n```\n\n----------------------------------------\n\nTITLE: Exporting Distinct Values DolphinDB\nDESCRIPTION: This code exports distinct values from the `device_info` table for several columns (`api_version`, `manufacturer`, `model`, `os_name`).  The `exec distinct` statement is used to extract unique values. This can be helpful to generate enumeration values for use in another system or for filtering data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs TimescaleDB/test_dolphindb_small.txt#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// ----------------- 导出不同的值作为 TimescaleDB 枚举类型的属性\nexec distinct api_version from device_info\nexec distinct manufacturer from device_info\nexec distinct model from device_info\nexec distinct os_name from device_info\n```\n\n----------------------------------------\n\nTITLE: Calculating Prediction Latency Statistics in DolphinDB\nDESCRIPTION: SQL query to compute average, minimum, and maximum prediction latency from the result1min table. This helps evaluate the performance of the real-time prediction system.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/machine_learning_volatility.md#_snippet_9\n\nLANGUAGE: sql\nCODE:\n```\nselect avg(CostTime) as avgCostTime, min(CostTime) as minCostTime, max(CostTime) as maxCostTime from result1min\n```\n\n----------------------------------------\n\nTITLE: Defining Transformation Function for Time Conversion in DolphinDB\nDESCRIPTION: This script defines a user function `i2t` intended for use with the `transform` parameter of `loadTextEx`. The function takes a mutable table `t` as input, converts the integer values in the `time` column to DolphinDB's TIME type (assuming the original integers represent milliseconds * 10), and returns the modified table. It uses the in-place modification function `replaceColumn!` for efficiency.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_csv.md#_snippet_13\n\nLANGUAGE: dolphindb\nCODE:\n```\ndef i2t(mutable t){\n    return t.replaceColumn!(`time,time(t.time/10))\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Grafana Data Source and Querying DolphinDB\nDESCRIPTION: Instructions within Grafana to add DolphinDB as a data source using user credentials, followed by a sample DolphinDB query embedded in a Grafana panel to group and average disk metrics by timestamp. This setup enables visualization of disk total, used, free space, and used percentage metrics over time. Dependencies: Grafana installed and running, DolphinDB data source plugin configured. Inputs: DolphinDB query fetching grouped metrics data, outputs: Grafana panel displaying aggregated metric visualizations.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_Telegraf_Grafana.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndfs_disk = loadTable(\"dfs://telegraf\",\"disk\")\nselect timestamp,avg(total),avg(used),avg(free),avg(used_percent) from dfs_disk group by timestamp \n```\n\n----------------------------------------\n\nTITLE: Subscribing Order Execution Data - DolphinDB\nDESCRIPTION: This snippet demonstrates subscribing to order execution data, specifically for different channels.  It first defines `executionSchema` using `amdQuote::getSchema()`. Then, it creates two stream tables (`orderExecution1` and `orderExecution2`) with persistence enabled. A dictionary is created to map channel numbers (1 and 2) to these tables, and finally, the `amdQuote::subscribe` method is used to subscribe to the order execution stream for different channels, directing data to the respective stream tables. Requires `enableTableShareAndPersistence` function.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_implementation_for_AMD_Plugin.md#_snippet_12\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 以 orderExecution 为例\nexecutionSchema = amdQuote::getSchema(`orderExecution)\nenableTableShareAndPersistence(table=streamTable(20000000:0, executionSchema.name, executionSchema.type), tableName=`orderExecution1, cacheSize=20000000)\nenableTableShareAndPersistence(table=streamTable(20000000:0, executionSchema.name, executionSchema.type), tableName=`orderExecution2, cacheSize=20000000)\n\nd = dict(INT, ANY)\nd[1] = orderExecution1\nd[2] = orderExecution2\namdQuote::subscribe(handle, `orderExecution, d, 101)\n```\n\n----------------------------------------\n\nTITLE: Using each with call to apply multiple functions\nDESCRIPTION: This code snippet demonstrates the use of 'each' and 'call' to apply multiple functions (`sin` and `log`) to a fixed vector (1..3). The 'call' function partially applies the arguments to the specified functions.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_45\n\nLANGUAGE: shell\nCODE:\n```\neach(call{, 1..3},(sin,log));\n```\n\n----------------------------------------\n\nTITLE: Optimized Group By Query with Map Keyword in DolphinDB SQL\nDESCRIPTION: An optimized group by query using the map keyword to avoid unnecessary global aggregation when partition granularity is larger than group granularity.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_SQL_Case_Tutorial.md#_snippet_13\n\nLANGUAGE: SQL\nCODE:\n```\ntimer result = select count(*) from snapshot group by SecurityID, bar(DateTime, 60) map\n```\n\n----------------------------------------\n\nTITLE: DolphinDB interval example with HINT_EXPLAIN\nDESCRIPTION: This example demonstrates the usage of the `interval` function within a `group by` clause in DolphinDB. The query calculates the standard deviation (std) of the `rate` column, grouping by `code` and intervals of 30 days based on the `date` column, with no filling for missing intervals. `HINT_EXPLAIN` is used to get the execution plan, which includes the cost associated with `fill` (handling interval boundaries).\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_Explain.md#_snippet_17\n\nLANGUAGE: DolphinDB\nCODE:\n```\nN = 3653\nt = table(2011.11.01..2021.10.31 as date, \n          take(`AAPL, N) as code, \n          rand([0.0573, -0.0231, 0.0765, 0.0174, -0.0025, 0.0267, 0.0304, -0.0143, -0.0256, 0.0412, 0.0810, -0.0159, 0.0058, -0.0107, -0.0090, 0.0209, -0.0053, 0.0317, -0.0117, 0.0123], N) as rate)\n\nselect [HINT_EXPLAIN] std(rate) from t group by code, interval(date, 30d, \"none\")\n```\n\n----------------------------------------\n\nTITLE: Performing 10-Minute Interval Derived Feature Aggregation with Filtering in DolphinDB Script\nDESCRIPTION: This DolphinDB snippet performs batch aggregation of derived features over multiple 10-minute sliding windows with filters for different sub-intervals (0-600s, 150-600s, 300-600s, 450-600s) using meta code expressions generated earlier. It assigns a 10-minute bar datetime via 'bar' function, then calls 'sql' with select=aggMetaCode for each time-filtered subset, extracting the aggregation results as matrices. Finally, the matrices for the four intervals are concatenated horizontally for comprehensive feature sets. This enables time-segmented feature representation critical for volatility modeling. Inputs include the sub-table and meta code vector; outputs are joined matrices of features.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/metacode_derived_features.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\nsubTable['BarDateTime'] = bar(subTable['DateTime'], 10m)\\nresult = sql(select = aggMetaCode, from = subTable).eval().matrix()\\nresult150 = sql(select = aggMetaCode, from = subTable, where = <time(DateTime) >= (time(BarDateTime) + 150*1000) >).eval().matrix()\\nresult300 = sql(select = aggMetaCode, from = subTable, where = <time(DateTime) >= (time(BarDateTime) + 300*1000) >).eval().matrix()\\nresult450 = sql(select = aggMetaCode, from = subTable, where = <time(DateTime) >= (time(BarDateTime) + 450*1000) >).eval().matrix()\\nreturn concatMatrix([result, result150, result300, result450])\n```\n\n----------------------------------------\n\nTITLE: Batch Processing of Net Buy Order Amount Differential in DolphinDB\nDESCRIPTION: This snippet demonstrates how to calculate the net buy order amount differential factor using batch processing on historical snapshot data stored in a DolphinDB database.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Array_Vector.md#_snippet_33\n\nLANGUAGE: DolphinDB\nCODE:\n```\nsnapshot = loadTable(\"dfs://SH_TSDB_snapshot_ArrayVector\", \"snapshot\")\nres1 = select SecurityID, DateTime, calculateAmtDiff(BidPrice, OfferPrice, BidOrderQty, OfferOrderQty) as amtDiff from snapshot context by SecurityID csort DateTime\n```\n\n----------------------------------------\n\nTITLE: Querying DolphinDB Cluster Performance - DolphinDB Script\nDESCRIPTION: This DolphinDB script queries cluster performance and status information using the `getClusterPerf` system function. It then processes the results to map numeric mode and state codes to human-readable strings before returning the final table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/cluster_monitor.md#_snippet_7\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\ntemp = getClusterPerf(includeMaster=true)\nt = select  host,port,mode,state,name,maxMemSize,runningTasks,queuedTasks,runningJobs,queuedJobs,connectionNum from temp\nmapper1 = dict(0 1 2 3 4,[\"datanode\",\"agentnode\",\"controlnode\",\"singlemode\",\"computenode\"])\nmapper2 = dict(0 1,[\"unsurvive\",\"survive\"])\ntempvalue1 = t.mode\ntempvalue2 = t.state\nfinalvalue1 = mapper1[tempvalue1]\nfinalvalue2 = mapper2[tempvalue2]\nreplaceColumn!(t,`mode,finalvalue1)\nreplaceColumn!(t,`state,finalvalue2)\nselect * from t\n```\n\n----------------------------------------\n\nTITLE: Creating Distributed Table for Shanghai Orders (DolphinDB Script)\nDESCRIPTION: Checks if the table `orders_sh` exists in the `dfs://stockL2` database. If not, it creates a partitioned table using the schema stored in `ordersSchema`. The table is partitioned by `TradeDate` and `InstrumentID` and sorted by `InstrumentID` and `TransactTime`. `keepDuplicates=ALL` retains all records.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_implementation_for_NSQ_Plugin.md#_snippet_8\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\ndbName = \"dfs://stockL2\"\ntbName = \"orders_sh\"\nif(existsTable(dbName, tbName) == false){\n\tdb = database(dbName)\n\tdb.createPartitionedTable(table=table(1:0, ordersSchema.name, ordersSchema.type), tableName=tbName, partitionColumns=`TradeDate`InstrumentID, sortColumns=`InstrumentID`TransactTime, keepDuplicates=ALL)\n\tprint(\"DFS table created successfully !\")\n}\nelse{\n\tprint(\"DFS table have been created !\")\n}\n```\n\n----------------------------------------\n\nTITLE: Executing the Main Function - DolphinDB\nDESCRIPTION: This line calls the `main` function, passing the previously initialized mutable stream table `st` as an argument. This begins the setup of the stream processing pipeline and the simulation of data ingestion.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/streaming_engine_anomaly_alerts/流计算引擎级联检测门禁超时数据.txt#_snippet_11\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//运行\nmain(st)\n```\n\n----------------------------------------\n\nTITLE: Creating Table Object - Python\nDESCRIPTION: This code snippet creates a Pandas DataFrame in Python, which includes a column ('value') with an Array Vector data type (INT).\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Array_Vector.md#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\n    'id': [1, 2, 3, 4],\n    'value': [np.array([1,2,3],dtype=np.int64), np.array([4,5,6],dtype=np.int64), np.array([7,8,9],dtype=np.int64), np.array([10,11,12],dtype=np.int64)]\n})\n```\n\n----------------------------------------\n\nTITLE: Loading a Text File as a Table in DolphinDB\nDESCRIPTION: This code snippet demonstrates how to load a text file (CSV) into memory as a table using the `loadText` function in DolphinDB. The default delimiter is a comma (,).\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Single_Node_Tutorial.md#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\nt = loadText(C:/test.csv);\n```\n\n----------------------------------------\n\nTITLE: Optimizing SQL Queries to Prevent GUI Client OOM Issues\nDESCRIPTION: Examples of how to modify SQL queries to prevent Java heap space errors in the DolphinDB GUI client by avoiding returning large result sets directly.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/handling_oom.md#_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nselect * from xx\n```\n\n----------------------------------------\n\nTITLE: Checking Data Type of a Matrix in DolphinDB Script\nDESCRIPTION: Uses the `typestr()` function to inspect and return the data type of the variable `resM`, which is expected to be a matrix as created by the `exec pivot by` statement in the preceding example.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/panel_data.md#_snippet_8\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\ntypestr(resM)\n```\n\n----------------------------------------\n\nTITLE: Generating Jupyter Notebook config file\nDESCRIPTION: This command creates the Jupyter Notebook configuration file, `jupyter_notebook_config.py`.  This file is used to customize Jupyter Notebook settings, such as the working directory for kernels. The file can be found in the users home directory inside a .jupyter folder.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/client_tool_tutorial.md#_snippet_6\n\nLANGUAGE: Shell\nCODE:\n```\njupyter notebook --generate-config\n```\n\n----------------------------------------\n\nTITLE: Defining PIP Function and Result Parser in DolphinDB Script\nDESCRIPTION: This snippet includes the definition of the `PIP` user-defined aggregate function (implementing PIP downsampling) and an additional function `strToTable(result)`. The `strToTable` function parses the string output generated by the `PIP` function (a comma-separated list of 'x_y' coordinate pairs) and converts it into a DolphinDB keyed table with columns 'x' and 'y', sorted by 'x'.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/PIP_in_DolphinDB.md#_snippet_2\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\n// PIP 算法聚合函数\ndefg PIP(X, Y){\n    data = table(X as x, Y as y)\n    rowTab = select x, y, rowNo(x) as rowNo from data\n    n = size(X)\n// k 为每次滑动降采样的数据量，取值范围为 3~n（n 为滑动窗口大小） \n    k = 5  \n    samples = 2\n    result = string(X[0])+\"_\"+string(Y[0])\n    indexList = [0,n-1]\n    do{\n        distanceVec = [0.0]\n        for (i in 0..(size(indexList)-2)){\n            start, end = indexList[i], indexList[i+1]\n            x1, y1 = X[start], Y[start]\n            x2, y2 = X[end], Y[end]\n            a = (y1-y2)/(x1-x2)\n            b = y1-a*x1\n            distanceVec=join(distanceVec, abs(Y[(start+1): end] - (X[(start+1): end]*a + b)))\n            distanceVec.append!(0)\n        }\n        distanceMax = distanceVec.max()\n        tmp = table(rowTab, distanceVec as distance)\n        nextPoint = select x, y, rowNo from tmp where distance = distanceMax\n        result += \",\"+string(nextPoint.x[0])+\"_\"+string(nextPoint.y[0])\n        indexList = indexList.append!(nextPoint.rowNo[0]).sort()\n        samples = samples+1\n    }while(samples < k)\n    result += \",\"+string(X.last())+\"_\"+string(Y.last())\n    return result\n}\n\n// 降采样结果解析函数\ndef strToTable(result){\n    samplesPIP = keyedTable(`x`y, 1:0, `x`y, `DOUBLE`DOUBLE)\n    for (res in result){\n        windows = res.split(',')\n        for (window in windows){\n            row = window.split('_')\n            samplesPIP.append!(table([double(row[0])] as x, [double(row[1])] as y))\n        }\n    }\n    return samplesPIP.sortBy!(`x, 1)  \n}\n```\n\n----------------------------------------\n\nTITLE: Dropping Columns from Memory Table\nDESCRIPTION: Demonstrates dropping a column from a memory table using the `drop!` function. Specify the column name as a string.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/partitioned_in_memory_table.md#_snippet_23\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntrades.drop!(\"qty1\");\n```\n\n----------------------------------------\n\nTITLE: Statistical Summary and Distribution of Fund Fees\nDESCRIPTION: Defines a user function to compute comprehensive statistical summaries including count, mean, standard deviation, min, median, and max, and includes quantile-based metrics. Performs grouping by fund type to obtain segment-specific characteristics. Facilitates understanding of fee patterns across fund categories.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/public_fund_basic_analysis.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef describe(x){\n\ny = stat(x)\nq_25 = quantile(x, 0.25)\nq_50 = quantile(x, 0.50)\nq_75 = quantile(x, 0.75)\n\treturn y.Count join y.Avg join y.Stdev join y.Min join q_25 join q_50 join q_75 join y.Max join y.Median\n}\n\n// query the summary of public fund fees\nselect describe(Fee) as `count`mean`std`min`q_25`q_50`q_75`max`median from fundFee group by Type\n```\n\n----------------------------------------\n\nTITLE: Loading Machine Learning Model in DolphinDB Script - DolphinDB\nDESCRIPTION: Loads a pre-trained binary model file for real-time inference. The modelSavePath must point to a valid .bin model, compatible with the server version. The model object is used subsequently for making volatility predictions via inference.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/machine_learning_volatility/04.streamComputing.txt#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\nmodelSavePath = \"/hdd/hdd9/machineLearning/realizedVolatilityModel_1.30.18.bin\"\n//modelSavePath = \"/hdd/hdd9/machineLearning/realizedVolatilityModel_2.00.6.bin\"\n...\nmodel = loadModel(modelSavePath)\n\n```\n\n----------------------------------------\n\nTITLE: Schur Decomposition with sorting (iuc) in DolphinDB\nDESCRIPTION: Demonstrates Schur decomposition of a matrix in DolphinDB using the `schur` function with the `'iuc'` sort option (inside unit circle). The eigenvalues are sorted based on having an absolute value less than 1. sdim shows the number of eigenvalues satisfying the sort criteria.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/matrix.md#_snippet_30\n\nLANGUAGE: DolphinDB\nCODE:\n```\n>t,u, sdim=schur(m,'iuc') //'iuc':(abs(e) < 1.0)\n>t;\n#0        #1       #2        #3       \n--------- -------- --------- ---------\n-0.995651 -0.02048 1.390574  3.449116 \n0         21.16354 1.174494  -4.266858\n0         0        -4.306007 -0.764178\n0         0        0         2.138117 \n>u;\n#0        #1       #2        #3       \n--------- -------- --------- ---------\n0.133953  0.522264 -0.831085 0.136364 \n-0.903631 0.400552 0.121062  0.091394 \n0.373493  0.568825 0.504805  0.531143 \n0.161277  0.49319  0.199534  -0.831228\n>sdim;\n1\n\n```\n\n----------------------------------------\n\nTITLE: Updating All DolphinDB Licenses Online Using ops Module\nDESCRIPTION: This command updates all license files on the executing node in the cluster by calling updateAllLicenses() from the ops module. The online update requires license parameters matching the original license and must be run on all computing, control, and data nodes. Only commercial and free license types support online updates; trial licenses do not.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/multi_machine_cluster_deployment.md#_snippet_9\n\nLANGUAGE: DolphinDB\nCODE:\n```\nuse ops\nupdateAllLicenses()\n```\n\n----------------------------------------\n\nTITLE: Querying by Time Range in MongoDB - JavaScript\nDESCRIPTION: This snippet fetches records within a specified time range from the 'device_readings' collection using MongoDB's .find() and comparison operators '$gte' and '$lte'. Inputs are start and end ISODate values. The output includes all documents within this interval. MongoDB must be set up with the relevant collection and indexed accordingly.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs MongoDB/mongodb_readings_query.txt#_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\ndb.device_readings.explain(\"executionStats\").find({time:{\"$gte\":ISODate(\"2016-11-16 21:00:00.000Z\"),\"$lte\":ISODate(\"2016-11-17 21:30:00.000Z\")}},{})\n```\n\n----------------------------------------\n\nTITLE: Loading 'Trades' Data into DolphinDB (DolphinDB Script)\nDESCRIPTION: DolphinDB script to load trade data from a CSV file into a distributed DolphinDB database. It defines the file path, specifies the DFS database path, creates a ranged partitioned database based on month (starting Jan 2008, partitioned every 6 months), measures execution time, and loads the CSV data into the 'trades' table, partitioning by the 'trade_date' column.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_7\n\nLANGUAGE: dolphindb\nCODE:\n```\nfilepath = \"/home/revenant/data/tushare_daily_data.csv\"\ndbpath = \"dfs://rangedb\"\ndb = database(dbpath, RANGE, 2008.01M+(0..20)*6)\ntimer(1)\ntrades = db.loadTextEx(`trades, `trade_date, filepath)\n```\n\n----------------------------------------\n\nTITLE: Calling getRecentJobs with argument using rpc and partial application\nDESCRIPTION: This code snippet demonstrates the use of partial application to pass an argument (3) to the 'getRecentJobs' function when calling it remotely via 'rpc'. This is necessary because 'rpc' expects a function as its second argument, not a function call with arguments.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_61\n\nLANGUAGE: shell\nCODE:\n```\nrpc(\"P1-node1\",getRecentJobs{3})\n```\n\n----------------------------------------\n\nTITLE: Creating Data with Double Quotes\nDESCRIPTION: This code creates a table with numerical values enclosed in double quotes and saves it to a CSV file.  It's intended to demonstrate how DolphinDB automatically handles double quotes surrounding numerical data during loading.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_csv.md#_snippet_40\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndataFilePath=\"/home/data/test.csv\"\ntt=table(1..3 as id,  [\"\\\"500\\\"\",\"\\\"3,500\\\"\",\"\\\"9,000,000\\\"\"] as num)\nsaveText(tt,dataFilePath);\n```\n\n----------------------------------------\n\nTITLE: Creating a weekly system management job in DolphinDB\nDESCRIPTION: Example demonstrating how to schedule a system management job to delete log files every Sunday at 1 AM using the shell function.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/scheduledJob.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\nscheduleJob(`weeklyjob, \"rm log\", shell{\"rm /home/DolphinDB/server/dolphindb.log\"}, 01:00m, 2020.01.01, 2021.12.31, `W`, 0);\n```\n\n----------------------------------------\n\nTITLE: Generating Sample Data for N-Share VWAP Calculation\nDESCRIPTION: This code snippet generates sample data for demonstrating the calculation of N-share VWAP. It creates a table with symbol, price, and volume columns.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_sql_cases.md#_snippet_15\n\nLANGUAGE: DolphinDB\nCODE:\n```\nn = 500000\nt = table(rand(string(1..4000), n) as sym, rand(10.0, n) as price, rand(500, n) as vol)\n```\n\n----------------------------------------\n\nTITLE: Converting YMD Time to Datetime\nDESCRIPTION: This code snippet is part of the `transType` function and shows the conversion of date and time information in yyyyMMddHHmmssSSS format (string format) to DolphinDB's datetime format using `datetimeParse` function.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/LoadDataForPoc.md#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef transType(mutable memTable)\n{\n   return memTable.replaceColumn!(`ymdTimeCol,datetimeParse(string(memTable.ymdTimeCol),\"yyyyMMddHHmmssSSS\"))\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring conda for Windows and offline package storage\nDESCRIPTION: Adds a custom package directory to conda configuration to enable offline package storage and retrieval. Requires conda CLI and admin or user command prompt.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/python_api_install_offline.md#_snippet_6\n\nLANGUAGE: Shell Script\nCODE:\n```\nconda config --add pkgs_dirs D:\\pkgs\n```\n\n----------------------------------------\n\nTITLE: Simulating and Inserting Time-Series Data into DolphinDB Table\nDESCRIPTION: Generates synthetic data arrays for one thousand rows randomly, including increasing timestamps and random values for dwMileage, speed, longitude, latitude, and elevation. Combines these arrays into a DolphinDB table and defines a function to simulate continuous data updates by incrementing timestamps by 1000 ms in each iteration, appending the updated data to the DolphinDB table, and sleeping for 1000 ms. This simulates live streaming sensor data insertion. The simulation is submitted as an asynchronous job named 'simulateData'.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/redis_plugin_tutorial/example3-kafka.txt#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 模拟数据写入\n//测试写入极限可调整 num 参数，以及simulateData任务中的sleep时间\nnum = 1000\nid = take(string(1..1000),num)\ntime = stretch(2024.01.01 00:00:00.000+(0..999)*1000,num)\ndwMileage = rand(10.0,num)\nspeed = rand(10.0,num)\nlongitude = rand(10.0,num)\nlatitude = rand(10.0,num)\nelevation = rand(10.0,num)\nt = table(time,id,dwMileage,speed,longitude,latitude,elevation)\n\ndef simulateData(dbName,tbName,mutable t){\n    do{\n        update t set time = (t[`time]+1000)\n        loadTable(dbName,tbName).append!(t)\n        sleep(1000)\n    }while(true)\n}\n\nsubmitJob(\"simulateData\",\"simulateData\",simulateData{\"dfs://signalData\",\"data\"},t)\n```\n\n----------------------------------------\n\nTITLE: 按基金类型绘制年度收益率柱状图\nDESCRIPTION: 选择部分基金类型，绘制年度平均收益的柱状图，展示不同类型基金的年度表现。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/public_fund_basic_analysis.md#_snippet_18\n\nLANGUAGE: DolphinDB\nCODE:\n```\nyearReturnsMatrix = yearReturnsMatrix.loc( , [\"债券型\", \"股票型\", \"混合型\"])\nyearReturnsMatrix.loc(year(yearReturnsMatrix.rowNames())>=2014, ).plot(chartType=BAR)\n```\n\n----------------------------------------\n\nTITLE: Querying a Partitioned Table\nDESCRIPTION: This code demonstrates querying a partitioned table and retrieving the execution plan. The example uses `loadTable` to access the table. The execution plan reveals the steps and timings involved in the query process.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_Explain.md#_snippet_2\n\nLANGUAGE: DolphinDB SQL\nCODE:\n```\n// 查询\nselect [HINT_EXPLAIN] * from loadTable(\"dfs://valuedb\",`pt);\n```\n\n----------------------------------------\n\nTITLE: Editing DolphinDB Cluster Node Configuration with sed and echo (Shell)\nDESCRIPTION: Performs in-place modification of cluster configuration files using 'sed' with extended regex, updating IP, port, and node names for controllers, agents, and data nodes. Additional agents are appended to the 'cluster.nodes' file using 'echo'. Requires 'sed' with backup support (-i.bak) and write permissions to configuration files. The script operates recursively on all files in the config directory.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/service_deployment_and_migration.md#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nsed -i.bak -E -e 's/([0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+):([0-9]+):controller1/172.0.0.1:\\2:controller1/' -e 's/([0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+):([0-9]+):controller2/172.0.0.2:\\2:controller2/' -e 's/([0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+):([0-9]+):controller3/172.0.0.3:\\2:controller3/' -e 's/([0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+):([0-9]+):agent1/172.0.0.1:\\2:agent1/' -e 's/([0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+):([0-9]+):agent2/172.0.0.2:\\2:agent2/' -e 's/([0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+):([0-9]+):agent3/172.0.0.3:\\2:agent3/' -e 's/([0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+):([0-9]+):node1/172.0.0.1:\\2:\\node1/' -e 's/([0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+):([0-9]+):node2/172.0.0.2:\\2:node2/' -e 's/([0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+):([0-9]+):node3/172.0.0.3:\\2:node3/'  ./*\n\necho '172.0.0.2:9905:agent2,agent' >> cluster.nodes\necho '172.0.0.3:9906:agent3,agent' >> cluster.nodes\n```\n\n----------------------------------------\n\nTITLE: Querying - Complex Selection DolphinDB\nDESCRIPTION: This snippet demonstrates a more complex query that selects the maximum date and the sum of `mem_free` and `mem_used`, grouped by hour and device ID. It filters records by date, battery level, and CPU usage.  This demonstrates a more advanced data analysis query.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs TimescaleDB/test_dolphindb_small.txt#_snippet_23\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 14. 经典查询：计算某时间段内高负载高电量设备的内存大小\ntimer\nselect\n\tmax(date(time)) as date,\n\tmax(mem_free + mem_used) as mem_all\nfrom readings\nwhere\n    time <= 2016.11.18 21:00:00,\n    battery_level >= 90,\n    cpu_avg_1min > 90\ngroup by hour(time), device_id\n```\n\n----------------------------------------\n\nTITLE: Defining Logarithmic Return Function (DolphinDB Script)\nDESCRIPTION: Defines a custom DolphinDB function `logReturn` to calculate the logarithmic return of a time series `s`. It computes `log(s) - log(prev(s))`, where `prev(s)` is the previous value in the series.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/metacode_derived_features/metacode_derived_features_streaming.txt#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef logReturn(s){\n\treturn log(s)-log(prev(s))\n}\n```\n\n----------------------------------------\n\nTITLE: Calculating Net Buy Order Amount Differential Factor in DolphinDB\nDESCRIPTION: This function calculates the net buy order amount differential, a high-frequency factor based on snapshot data. It calculates the difference between the increase in buy orders and sell orders compared to the previous tick, using array vector operations.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Array_Vector.md#_snippet_32\n\nLANGUAGE: DolphinDB\nCODE:\n```\n@state\ndef calculateAmtDiff(bid, ask, bidvol, askvol){\n\tlastBidPrice = prev(bid[0])\t\t// 上一个 tick 的买一价\n\tlastAskPrice = prev(ask[0])\t\t// 上一个 tick 的卖一价\n\tlastBidQty = prev(bidvol[0])\t\t// 上一个 tick 的买一量\n\tlastAskQty = prev(askvol[0])\t// 上一个 tick 的卖一量\n\t// 求委买增额\n\tbidAmtDiff = rowSum(bid*bidvol*(bid >= lastBidPrice)) - lastBidPrice*lastBidQty\n\t// 求委卖增额\n\taskAmtDiff = rowSum(ask*askvol*(ask <= lastAskPrice)) - lastAskPrice*lastAskQty\n\treturn bidAmtDiff - askAmtDiff\n}\n```\n\n----------------------------------------\n\nTITLE: 定义成交输出表和快照输出表\nDESCRIPTION: 创建用于存储撮合结果的表格，包括成交输出表（记录成交明细）和快照输出表（记录市场状态快照）。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/matching_engine_simulator.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntradeOutputTable  = table(1:0, `OrderSysID`Symbol`Direction`SendingTime`LimitPrice`VolumeTotalOriginal`TradeTime`TradePrice`VolumeTraded`OrderStatus`orderReceiveTime, \n                               [LONG, STRING, INT, TIMESTAMP,DOUBLE,LONG, TIMESTAMP,DOUBLE,LONG, INT, NANOTIMESTAMP])\nsnapshotOutputTable  = table(1:0, `symbol`time`avgBidPrice`avgOfferPrice`totalBidQty`totalOfferQty`bidPrice`bidQty`offerPrice`offerQty`lastPrice`highPrice`lowPrice, \n                                   [STRING, TIMESTAMP,DOUBLE,DOUBLE, LONG, LONG,DOUBLE[],LONG[], DOUBLE[], LONG[], DOUBLE, DOUBLE, DOUBLE])\n```\n\n----------------------------------------\n\nTITLE: Listing Directory Contents for TSDB (keepDuplicates=LAST, Merged)\nDESCRIPTION: This snippet showcases the file structure after updates and the subsequent merging of level files in a TSDB table configured with `keepDuplicates=LAST`. The `tree` command displays the resulting directory structure after the system merges redundant data from the updates.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/dolphindb_update.md#_snippet_7\n\nLANGUAGE: console\nCODE:\n```\ntree\n.\n├── chunk.dict\n└── machines_2\n    ├── 0_00000272\n    ├── 0_00000274\n    ├── 0_00000276\n    ├── 1_00000002\n    ├── 1_00000050\n    └── 1_00000051\n\n1 directory, 7 files\n```\n\n----------------------------------------\n\nTITLE: Parallel Load Text File\nDESCRIPTION: This snippet loads a text file using `ploadText`, which leverages multiple CPU cores for faster loading. The performance improvement depends on the number of CPU cores and the `workerNum` configuration.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_data.md#_snippet_3\n\nLANGUAGE: txt\nCODE:\n```\ntimer loadText(filePath);\nTime elapsed: 39728.393 ms\n\ntimer ploadText(filePath);\nTime elapsed: 10685.838 ms\n```\n\n----------------------------------------\n\nTITLE: C++ API: Querying Data from DolphinDB\nDESCRIPTION: Queries data from the DolphinDB memory table `myTable` using a SQL script executed through the C++ API. The result is then printed to the console.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Array_Vector.md#_snippet_21\n\nLANGUAGE: C++\nCODE:\n```\nstring script = \"select * from myTable;\";\nConstantSP result = conn.run(script);\nstd::cout<<\"------ check data ------\"<<std::endl;\nstd::cout<<result->getString()<<std::endl;\n```\n\n----------------------------------------\n\nTITLE: Calculating Annual Returns for Top Sharpe Ratio Funds in DolphinDB\nDESCRIPTION: Filters the 'perf' table to select funds with specific performance characteristics (exp<40, vol<40, sharpe>0). Within each fund type, it selects the top 50 funds based on descending Sharpe ratio. It then filters the original daily returns matrix (`returnsMatrix`) to include only these selected top 50 funds and data from 2015.01.01 onwards. Calculates the annual rate of return for these funds using resampling ('A' for annual) and product aggregation, handling nulls, and transposing the result. Finally, it displays the annual returns for funds belonging to specific types (Stock, Bond, Hybrid). Depends on `perf` table, `returnsMatrix`, and `fundTypeMap`.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/public_fund_basic_analysis/basic_analysis_of_public_fund_net_value.txt#_snippet_8\n\nLANGUAGE: dolphindb\nCODE:\n```\n// filter analysis space\nfilterTB = select * from perf where exp<40, vol<40, sharpe>0 context by Type csort sharpe desc limit 50\nreturnsMatrix50 = returnsMatrix.loc(2015.01.01:, returnsMatrix.colNames() in filterTB[\"SecurityID\"])\n// calculate annual rate of return\nyearReturnsMatrix50 = transpose((returnsMatrix50 .setIndexedMatrix!()+1).resample(\"A\", prod)-1).nullFill(0)\n//view annual rate of return of a specified type of fund\nyearReturnsMatrix50.loc(fundTypeMap[yearReturnsMatrix50.rowNames()] == \"股票型\", )\nyearReturnsMatrix50.loc(fundTypeMap[yearReturnsMatrix50.rowNames()] == \"债券型\", )\nyearReturnsMatrix50.loc(fundTypeMap[yearReturnsMatrix50.rowNames()] == \"混合型\", )\n```\n\n----------------------------------------\n\nTITLE: Define MACD Factor (Complex Factor)\nDESCRIPTION: Defines a function named `MACD` that calculates the MACD factor using the minute closing price.  `Close` refers to the minute closing price and must be included in the output of the time series aggregation engine.  The function is declared with `@state`, indicating that it's stateful. This means the current row's return value depends on data from previous rows. The stateful EMA (Exponential Moving Average) function, `ewmMean`, is used for calculation. The code returns the rounded DIF, DEA and MACD values.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Level2_Snapshot_Factor_Calculation.md#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\n@state\ndef MACD(Close, SHORT_ = 12, LONG_ = 26, M = 9) {\n\tDIF = ewmMean(Close, span = SHORT_, adjust = false) - ewmMean(Close, span = LONG_, adjust = false)\n\tDEA = ewmMean(DIF, span = M, adjust = false)\n\tMACD = (DIF - DEA) * 2\n\treturn round(DIF, 3), round(DEA, 3), round(MACD, 3)\n}\n```\n\n----------------------------------------\n\nTITLE: Scheduling a Periodic Batch Task\nDESCRIPTION: This snippet demonstrates scheduling a periodic batch task using the `scheduleJob` function. The `jobFunc` is set to `bacthExeCute`, `scheduleTime` is set to 17:23 daily, and the `startDate` and `endDate` define the period during which the job should run.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practice_for_factor_calculation.md#_snippet_37\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//设置一段时间每天执行\nscheduleJob(jobId=`daily, jobDesc=\"Daily Job 1\", jobFunc=bacthExeCute, scheduleTime=17:23m, startDate=2018.01.01, endDate=2018.12.31, frequency='D')\n```\n\n----------------------------------------\n\nTITLE: 处理CSV数据并创建内存表\nDESCRIPTION: 通过调用之前定义的函数读取CSV数据，将矩阵数据转换为表格形式，分别处理基金日净值数据和沪深300指数数据。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/fund_factor_contrasted_by_py.md#_snippet_21\n\nLANGUAGE: dolphindb\nCODE:\n```\n//基金日净值数据\nallSymbols = readColumnsFromWideCSV(csvPath)$STRING\ndataMatrix = readIndexedMatrixFromWideCSV(csvPath)\nfundTable = table(dataMatrix.rowNames() as tradingdate, dataMatrix)\nresult = fundTable.unpivot(`tradingDate, allSymbols).rename!(`tradingdate`fundNum`value)\n  \n//沪深300指数\nallSymbols1 = readColumnsFromWideCSV(csvPath1)$STRING\ndataMatrix1 = readIndexedMatrixFromWideCSV(csvPath1)\nfundTable1 = table(dataMatrix1.rowNames() as tradingDate, dataMatrix1)\nresult1 = fundTable1.unpivot(`tradingDate, allSymbols1).rename!(`tradingdate`fundNum`value)\n```\n\n----------------------------------------\n\nTITLE: Solving QCLP with DolphinDB qclp Function\nDESCRIPTION: This code snippet demonstrates solving a Quadratic Constrained Linear Program (QCLP) using DolphinDB's built-in `qclp` function. It defines the parameters for the optimization problem, including return (r), variance matrix (V), a constant k, the linear constraints (A, b), and equality constraints (Aeq, beq). The `qclp` function is then called to solve the problem, and the results (max_return, weights) are extracted and displayed. The snippet assumes the input data (r, V, k, A, b, Aeq, beq) is pre-defined. The output includes the maximum return and the optimal weights. The function expects that the input parameters conform to the requirements for a QCLP problem specification.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/MVO.md#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\nr = 0.18 0.25 0.36\nV = 0.0225 -0.003 -0.01125 -0.003 0.04 0.025 -0.01125 0.025 0.0625 $ 3:3\nk = pow(0.11, 2)\nA = (eye(3) join (-1 * eye(3))).transpose()\nb = 0.5 0.5 0.5 -0.1 -0.1 -0.1\nAeq = (1 1 1)$1:3\nbeq = [1]\n\nres = qclp(-r, V, k, A, b, Aeq, beq) \n//输出结果\nmax_return = res[0]\n//max_return = 0.2281\nweights = res[1]\n//weights = [0.50, 0.381, 0.119]\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Single File Data Processing - Python\nDESCRIPTION: This snippet demonstrates how to use the `level10Diff` function to process a single stock snapshot CSV file. It reads the specified file into a pandas DataFrame, calls `level10Diff` with a lag of 20, measures the execution time, and prints the calculated time and the resulting DataFrame.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/DolphinDB_Python_Parser_Intro_for_Quantitative_Finance/因子实现_Python版本/十档委买增额.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_csv(\"/ssd/ssd3/data/oneStock_oneFile_TL/20230201/snapshot/000001.csv\")\nt0 = time.time()\nres = level10Diff(df, lag=20)\nprint(\"cal time: \", time.time() - t0, \"s\")\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Dynamically Adjusting TSDB Cache Engine Size in DolphinDB\nDESCRIPTION: The setTSDBCacheEngineSize function allows changing the TSDB cache engine capacity at runtime without restarting the server, useful for performance tuning based on workload requirements.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/redoLog_cacheEngine.md#_snippet_8\n\nLANGUAGE: SQL\nCODE:\n```\nsetTSDBCacheEngineSize(newSizeInGB)\n```\n\n----------------------------------------\n\nTITLE: Filtering Rows with Multiple Dynamic Conditions Using DolphinDB Macro Variable Metaprogramming\nDESCRIPTION: This snippet illustrates using DolphinDB macro variable metaprogramming to filter rows where the flagName column matches multiple patterns. It defines a custom filter function combining pattern matches with rowOr. The macro variables dynamically inject the column and value parameters into the SQL expression, enabling clean and flexible dynamic SQL construction. This approach is best suited for simpler or moderately complex filtering conditions where macro usage improves script readability and ease of use.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/macro_var_based_metaprogramming.md#_snippet_28\n\nLANGUAGE: DolphinDB\nCODE:\n```\nval=\"val\"\nflag=\"flagName\"\ndef filter(col, pattern){return rowOr(like:R(col,pattern))}\n<select _$val from t where filter(_$flag,[\"%新能源%\", \"%光伏%\"])>.eval()\n```\n\n----------------------------------------\n\nTITLE: Schur Decomposition with sorting (ouc) in DolphinDB\nDESCRIPTION: Demonstrates Schur decomposition of a matrix in DolphinDB using the `schur` function with the `'ouc'` sort option (outside unit circle). The eigenvalues are sorted based on having an absolute value greater than or equal to 1. sdim shows the number of eigenvalues satisfying the sort criteria.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/matrix.md#_snippet_31\n\nLANGUAGE: DolphinDB\nCODE:\n```\n>t,u, sdim=schur(m,'ouc') //'ouc':(abs(e) >= 1.0)\n>t;\n#0       #1        #2        #3       \n-------- --------- --------- ---------\n21.16354 -1.073588 -2.823677 3.237958 \n0        -4.306007 2.443445  -0.355379\n0        0         2.138117  -2.879786\n0        0         0         -0.995651\n>u;\n#0       #1        #2        #3       \n-------- --------- --------- ---------\n0.52214  0.818236  -0.03367  -0.238171\n0.401387 -0.461653 -0.464378 -0.640405\n0.568479 -0.320408 0.75676   0.03853  \n0.493041 -0.121263 -0.45884  0.729158 \n0.161277  0.49319  0.199534  -0.831228\n>sdim;\n3\n\n```\n\n----------------------------------------\n\nTITLE: Setting Up Kafka-DolphinDB Connector\nDESCRIPTION: Commands to create directories and install the Kafka-DolphinDB connector plugin files required for data synchronization from Kafka to DolphinDB.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Debezium_and_Kafka_data_sync.md#_snippet_13\n\nLANGUAGE: Bash\nCODE:\n```\nsudo mkdir -p /opt/confluent/share/java/plugin/kafka-connect-jdbc\n```\n\n----------------------------------------\n\nTITLE: Getting Time Variable Information in DolphinDB Shell\nDESCRIPTION: This code snippet demonstrates how to extract specific components (year, month, day, hour, minute, second, millisecond) from time variables in DolphinDB using functions like `year`, `monthOfYear`, etc. The input is a time variable and output is the extracted component.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/date_time.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\n>year(2016.02.14);\n2016\n>monthOfYear(2016.02.14);\n2\n>dayOfMonth(2016.02.14);\n14\n>x=01:02:03.456;\n>hour(x);\n1\n>minuteOfHour(x);\n2\n>secondOfMinute(x);\n3\n>x mod 1000;\n456\n```\n\n----------------------------------------\n\nTITLE: Factor Calculation Function Definition - DolphinDB\nDESCRIPTION: Defines a function `factorAskPriceRatio` to calculate the ratio of the current ask price to the ask price 30 periods ago. It handles cases where there are fewer than 31 data points. The function takes a vector `x` of ask prices as input and returns a double representing the ratio or null if there isn't enough data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/func_progr_cases.md#_snippet_11\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndefg factorAskPriceRatio(x){\n\tcnt = x.size()\n\tif(cnt < 31) return double()\n\telse return x[cnt - 1]/x[cnt - 31]\n}\n```\n\n----------------------------------------\n\nTITLE: Calculating Alpha Coefficient in Python\nDESCRIPTION: Defines a Python function `getAlpha`. It calculates Jensen's Alpha using the Python versions of `getAnnualReturn` (for fund and benchmark) and `getBeta`. Assumes a risk-free rate of 3% (0.03). Requires the helper functions.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/fund_factor_contrasted_by_py.md#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndef getAlpha(value, price):\n    return getAnnualReturn(value) - 0.03 - getBeta(value, price) * (getAnnualReturn(price) - 0.03)\n```\n\n----------------------------------------\n\nTITLE: Calculating Maximum Volume per Symbol per Minute using `context by` in DolphinDB Script\nDESCRIPTION: Calculates the maximum volume within each minute for each stock symbol. It uses `context by sym, timestamp.minute()` to group data first by symbol and then by the minute extracted from the timestamp, applying the `max(volume)` aggregation within these groups.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/panel_data.md#_snippet_4\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nselect *, max(volume) from t context by sym, timestamp.minute();\n```\n\n----------------------------------------\n\nTITLE: Simulating Real-Time Seismic Waveform Data Insertion - DolphinDB Script\nDESCRIPTION: This code snippet simulates real-time seismic waveform data streams by generating data for nine channels sampled every 10 milliseconds with normally distributed random values. The insertIntoDataStream() function continuously creates timestamped data tables and appends them to the 'dataStream' streaming table. The simulation runs asynchronously as a submitted job, which can be canceled using cancelJob(). Key dependencies include predefined streaming tables and functioning appending mechanism.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Earthquake_Prediction_with_DolphinDB_and_Machine_Learning.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\n/*\n * 模拟实时数据\n */\ndef insertIntoDataStream(){\n\tdo{\n\t\ttagidList = 1 2 3 4 5 6 7 8 9\n\t\tts = now()+(0..400)*10\n\t\tt = table(stretch(tagidList,3600)as tagid,take(ts,3600) as ts,randNormal(-2500, 1000, 3600) as data)\n\t\tobjByName(`dataStream).append!(t)\n\t\tsleep(3500)\n\t}while(true)\n}\njobId = submitJob(\"simulate\",\"simulate\",insertIntoDataStream);\n//通过以下方式取消Job\n//cancelJob(jobId)\n```\n\n----------------------------------------\n\nTITLE: Calculating Sharpe Ratio in DolphinDB\nDESCRIPTION: Defines a DolphinDB function `getSharp` to calculate the Sharpe Ratio. It takes a vector `value` (daily net values) and utilizes the previously defined `getAnnualReturn` and `getAnnualVolatility` functions. Assumes a risk-free rate of 3% (0.03).\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/fund_factor_contrasted_by_py.md#_snippet_8\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndefg getSharp(value){\n\treturn (getAnnualReturn(value) - 0.03)\\getAnnualVolatility(value) as sharpeRat\n}\n```\n\n----------------------------------------\n\nTITLE: Creating, Sharing, and Persisting Stream Tables (DolphinDB Script)\nDESCRIPTION: Defines a function that creates two stream tables, 'messageStream' for receiving market messages and 'prevailingQuotes' for storing processed quote data. Both tables are configured for persistence, compression, asynchronous writes, and have retention settings, making them suitable for large-scale and real-time market data ingestion. Requires calling enableTableShareAndPersistence for table durability, and customizing the schema by defining column names and types.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/stock_market_replay/02.calTradeCost_asofJoin.txt#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef createStreamTableFunc(){\n\t//create stream table: messageStream\n\tcolName = `msgTime`msgType`msgBody\n\tcolType = [TIMESTAMP,SYMBOL, BLOB]\n\tmessageTemp = streamTable(5000000:0, colName, colType)\n\tenableTableShareAndPersistence(table=messageTemp, tableName=\"messageStream\", asynWrite=true, compress=true, cacheSize=5000000, retentionMinutes=1440, flushMode=0, preCache=10000)\n\tmessageTemp = NULL\n\t//create stream table: prevailingQuotes\n\tcolName = `TradeTime`SecurityID`Price`TradeQty`BidPX1`OfferPX1`TradeCost`SnapshotTime\n\tcolType = [TIME, SYMBOL, DOUBLE, INT, DOUBLE, DOUBLE, DOUBLE, TIME]\n\tprevailingQuotesTemp = streamTable(1000000:0, colName, colType)\n\tenableTableShareAndPersistence(table=prevailingQuotesTemp, tableName=\"prevailingQuotes\", asynWrite=true, compress=true, cacheSize=1000000, retentionMinutes=1440, flushMode=0, preCache=10000)\n\tprevailingQuotesTemp = NULL\n\t\n}\ncreateStreamTableFunc()\ngo\n```\n\n----------------------------------------\n\nTITLE: 向流数据表写入测试数据并查看结果\nDESCRIPTION: 定义一个函数向流数据表写入测试数据，模拟每毫秒一条数据的场景，并检查原始数据和时序引擎计算结果。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/stream_aggregator.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef writeData(t, n){\n    timev = 2018.10.08T01:01:01.001 + timestamp(1..n)\n    volumev = take(1, n)\n    insert into t values(timev, volumev)\n}\nwriteData(trades, 10)\n\nselect * from trades;\nselect * from outputTable;\n```\n\n----------------------------------------\n\nTITLE: Constructing Stream Deserializer with DolphinDB Python API\nDESCRIPTION: This Python snippet creates a streamDeserializer object to deserialize heterogeneous stream data tables in DolphinDB. It maps stream identifiers like 'snapshot', 'order', 'transaction', and 'end' to their corresponding DFS paths and table names. The deserializer allows asynchronous streaming subscription to receive structured data frames corresponding to these tables. Dependency includes the DolphinDB Python API with proper connection object 's'. The 'end' table signals stream termination and must be included. The 's' argument is a DolphinDB session instance.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/appendices_market_replay_bp.md#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\nsd = ddb.streamDeserializer({\n    'snapshot':  [\"dfs://Test_snapshot\", \"snapshot\"],\n    'order': [\"dfs://Test_order\", \"order\"],\n    'transaction': [\"dfs://Test_transaction\", \"transaction\"],\n    'end': [\"dfs://End\", \"endline\"],\n}, s)\n```\n\n----------------------------------------\n\nTITLE: 宽表点查询性能测试语句\nDESCRIPTION: 用于对比OLAP和TSDB引擎存储宽表时的点查询性能测试语句，根据ID和日期查询特定因子的平均值。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/database_and_table_creation_details.md#_snippet_22\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect avg(factor2) from pt where ID=`60 and Date=2020.01.04\n```\n\n----------------------------------------\n\nTITLE: Defining Functions to Create Stream Tables for Real-time OHLC Calculation in DolphinDB Script\nDESCRIPTION: Defines four separate functions (`getMDLSnapshotTB`, `getMDLSnapshotProcessTB`, `getMDLStockFundOHLCTempTB`, `getMDLStockFundOHLCTB`) in DolphinDB script. Each function returns an empty stream table (`streamTable`) with a specific, predefined schema (column names and types), designed for use in different stages of a real-time streaming pipeline for calculating OHLC data from incoming snapshot ticks.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/K.md#_snippet_9\n\nLANGUAGE: dolphindb\nCODE:\n```\ndef getMDLSnapshotTB(tableCapacity=1000000){\n\tcolNames = `Market`TradeTime`MDStreamID`SecurityID`SecurityIDSource`TradingPhaseCode`ImageStatus`PreCloPrice`NumTrades`TotalVolumeTrade`TotalValueTrade`LastPrice`OpenPrice`HighPrice`LowPrice`ClosePrice`DifPrice1`DifPrice2`PE1`PE2`PreCloseIOPV`IOPV`TotalBidQty`WeightedAvgBidPx`AltWAvgBidPri`TotalOfferQty`WeightedAvgOfferPx`AltWAvgAskPri`UpLimitPx`DownLimitPx`OpenInt`OptPremiumRatio`OfferPrice`BidPrice`OfferOrderQty`BidOrderQty`BidNumOrders`OfferNumOrders`ETFBuyNumber`ETFBuyAmount`ETFBuyMoney`ETFSellNumber`ETFSellAmount`ETFSellMoney`YieldToMatu`TotWarExNum`WithdrawBuyNumber`WithdrawBuyAmount`WithdrawBuyMoney`WithdrawSellNumber`WithdrawSellAmount`WithdrawSellMoney`TotalBidNumber`TotalOfferNumber`MaxBidDur`MaxSellDur`BidNum`SellNum`LocalTime`SeqNo`OfferOrders`BidOrders\n\tcolTypes = [SYMBOL,TIMESTAMP,SYMBOL,SYMBOL,SYMBOL,SYMBOL,INT,DOUBLE,LONG,LONG,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,LONG,DOUBLE,DOUBLE,LONG,DOUBLE,DOUBLE,DOUBLE,DOUBLE,INT,DOUBLE,DOUBLE[],DOUBLE[],LONG[],LONG[],INT[],INT[],INT,LONG,DOUBLE,INT,LONG,DOUBLE,DOUBLE,DOUBLE,INT,LONG,DOUBLE,INT,LONG,DOUBLE,INT,INT,INT,INT,INT,INT,TIME,INT,LONG[],LONG[]]\n\treturn streamTable(tableCapacity:0, colNames, colTypes)\n}\n\ndef getMDLSnapshotProcessTB(tableCapacity=1000000){\n\tcolNames = `SecurityID`TradeTime`UpLimitPx`DownLimitPx`PreCloPrice`HighPrice`LowPrice`LastPrice`PreCloseIOPV`IOPV`DeltasHighPrice`DeltasLowPrice`DeltasVolume`DeltasTurnover`DeltasTradesCount\n\tcolTypes = [SYMBOL, TIMESTAMP, DOUBLE, DOUBLE, DOUBLE, DOUBLE, DOUBLE, DOUBLE, DOUBLE, DOUBLE, DOUBLE, DOUBLE, LONG, DOUBLE, INT]\n\treturn streamTable(tableCapacity:0, colNames, colTypes)\n}\n\ndef getMDLStockFundOHLCTempTB(tableCapacity=1000000){\n\tcolNames = `TradeTime`SecurityID`OpenPrice`HighPrice`LowPrice`ClosePrice`Volume`Turnover`TradesCount`PreClosePrice`PreCloseIOPV`IOPV`UpLimitPx`DownLimitPx`FirstBarChangeRate\n\tcolTypes = [TIMESTAMP, SYMBOL, DOUBLE, DOUBLE, DOUBLE, DOUBLE, LONG, DOUBLE, INT, DOUBLE, DOUBLE, DOUBLE, DOUBLE, DOUBLE, DOUBLE]\n\treturn streamTable(tableCapacity:0, colNames, colTypes)\n}\n\ndef getMDLStockFundOHLCTB(tableCapacity=1000000){\n\tcolNames = `SecurityID`TradeTime`OpenPrice`HighPrice`LowPrice`ClosePrice`Volume`Turnover`TradesCount`PreClosePrice`PreCloseIOPV`IOPV`UpLimitPx`DownLimitPx`ChangeRate\n\tcolTypes = [SYMBOL, TIMESTAMP, DOUBLE, DOUBLE, DOUBLE, DOUBLE, LONG, DOUBLE, INT, DOUBLE, DOUBLE, DOUBLE, DOUBLE, DOUBLE, DOUBLE]\n\treturn streamTable(tableCapacity:0, colNames, colTypes)\n}\n```\n\n----------------------------------------\n\nTITLE: Group By with MAP Keyword (Unoptimized)\nDESCRIPTION: This code snippet shows a standard group by query without the `map` keyword. The query calculates the count of records for each SecurityID grouped by minute intervals.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_sql_cases.md#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntimer result = select count(*) from snapshot group by SecurityID, bar(DateTime, 60)\n```\n\n----------------------------------------\n\nTITLE: Complete code for scheduling a max temperature job\nDESCRIPTION: This code defines the getMaxTemperature function and schedules a job that executes it with partial application to fix the device ID. It calculates the maximum temperature from the distributed table and schedules the operation.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_58\n\nLANGUAGE: shell\nCODE:\n```\ndef getMaxTemperature(deviceID){\n    maxTemp=exec max(temperature) from loadTable(\"dfs://dolphindb\",\"sensor\")\n            where ID=deviceID ,date(ts) = today()-1\n    return  maxTemp\n}\n\nscheduleJob(`testJob, \"getMaxTemperature\", getMaxTemperature{1}, 00:00m, today(), today()+30, 'D');\n```\n\n----------------------------------------\n\nTITLE: Calculating Cumulative VWAP\nDESCRIPTION: This code snippet calculates the cumulative VWAP for each stock per minute. It uses group by and cgroup by to aggregate data, and order by to sort the results.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_sql_cases.md#_snippet_14\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntimer result = select wavg(LastPx, Volume) as vwap \n\t\t\t   from snapshot \n\t\t\t   group by SecurityID \n\t\t\t   cgroup by minute(DateTime) as Minute \n\t\t\t   order by SecurityID, Minute\n```\n\n----------------------------------------\n\nTITLE: Anomaly and Log Replay Configuration (JSON)\nDESCRIPTION: This JSON configures the replay of anomaly and log data for fault diagnosis. It specifies the database and table names for both the anomaly and log tables, the device ID, time ranges, and the match column ('deviceId'). The replay rate is set to 50, and the job name is 'warnAndLogStream'.  This configuration is then passed to the replayIoT function to initiate the replay process.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/faultAnalysis.md#_snippet_4\n\nLANGUAGE: JSON\nCODE:\n```\nargs1 = '\n{\n    \"leftTable\":{\n        \"dbName\":\"dfs://test_anomaly\",\n        \"tbName\":\"anomaly\",\n        \"dateColumn\":\"ts\",\n        \"deviceId\":\"device0001\",\n        \"deviceColumn\":\"deviceId\",\n        \"timeBegin\":2024.01.01T00:00:00.000,\n        \"timeEnd\":2024.01.01T02:00:00.000\n    },\n    \"rightTable\":{\n        \"dbName\":\"dfs://test_log\",\n        \"tbName\":\"log\",\n        \"dateColumn\":\"ts\",\n        \"deviceId\":\"device0001\",\n        \"deviceColumn\":\"deviceId\",\n        \"timeBegin\":2024.01.01T00:00:00.000,\n        \"timeEnd\":2024.01.01T02:00:00.000\n    },\n    \"matchColumn\": [\"deviceId\"],\n    \"jobName\":\"warnAndLogStream\",\n    \"replayRate\":50\n}\n'\nreplayIoT(args1)\n```\n\n----------------------------------------\n\nTITLE: 计算基金日收益率\nDESCRIPTION: 计算多个基金的日收益率，为后续计算赫斯特指数做准备。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/fund_factor_contrasted_by_py.md#_snippet_25\n\nLANGUAGE: dolphindb\nCODE:\n```\nsymList = exec distinct(fundNum) as fundNum from result2 order by fundNum\nportfolio = select fundNum as fundNum, (deltas(value)\\prev(value)) as log, tradingDate as tradingDate from result2 where tradingDate in 2018.05.24..2021.05.27 and fundNum in symList\nm_log = exec log from portfolio pivot by tradingDate, fundNum\nmlog =  m_log[1:,]\n```\n\n----------------------------------------\n\nTITLE: Constructing URL-based Query String for DolphinDB Connection\nDESCRIPTION: This snippet illustrates how to formulate a URL query string for Redash's URL data source to connect to DolphinDB. It specifies the required parameters ('client' and 'queries') and the subpath '/json', emphasizing URL encoding of special characters to ensure proper recognition and security during transmission.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/data_interface_for_redash.md#_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n/json?client=redash&queries=select * from typeTable where id between (1..10)\n```\n\nLANGUAGE: plaintext\nCODE:\n```\n/json?client=redash&queries=login('admin','123456');select avg(ofr-bid) from loadTable('dfs%3a%2f%2fTAQ','quotes') group by minute(time) as minute\n```\n\n----------------------------------------\n\nTITLE: String Operations, Duplicate Detection, and Ranking in pandas and DolphinDB\nDESCRIPTION: This snippet details string processing methods like isspace, isalnum, and others, along with duplicate detection and ranking functions. These are used for text data cleaning and order-based analysis in pandas, with their DolphinDB counterparts for similar tasks.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/function_mapping_py.md#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\npandas.Series.str.isspace  # Check for whitespace-only strings\npandas.Series.str.isalnum  # Check for alphanumeric strings\npandas.Series.str.isalpha  # Alphabetic?\npandas.Series.str.isnumeric  # Numeric strings\npandas.Series.str.isdigit  # Digit strings\npandas.Series.str.startswith  # String start pattern\npandas.Series.str.endswith  # String end pattern\npandas.Series.str.find  # Find substring\npandas.Series.duplicated / pandas.DataFrame.duplicated  # Duplicate detection\npandas.Series.rank / pandas.DataFrame.rank  # Ranking data\npandas.Series.rank(method='dense') / pandas.DataFrame.rank(method='dense')  # Dense ranking\n```\n\n----------------------------------------\n\nTITLE: Partition Pruning Optimization (Date Function)\nDESCRIPTION: This code snippet showcases an optimized query using the `date` function to extract the date from the DateTime column, enabling efficient partition pruning. The query calculates the count of records grouped by SecurityID within a specific date range.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_sql_cases.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntimer t2 = select count(*) from snapshot \n\t\t   where date(DateTime) between 2020.06.01 : 2020.06.02 group by SecurityID \n```\n\n----------------------------------------\n\nTITLE: Creating a Distributed Database Table for 5-Minute Aggregated Market Data in DolphinDB\nDESCRIPTION: This snippet defines a function to create a distributed database table for storing 5-minute tick data aggregated from snapshot market data. The table includes open/high/low/close prices, volume, trade value, and array vectors for bid/ask price and quantity slices.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Array_Vector.md#_snippet_31\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 创建存储 5 min tick 数据的分布式库表\ndef createFiveMinuteBarDB(dbName, tbName){\n\tif(existsDatabase(dbName)){\n\t\tdropDatabase(dbName)\n\t}\n\tdb = database(dbName, VALUE, 2021.01.01..2021.12.31, , \"TSDB\")\n\tcolNames = `SecurityID`DateTime`OpenPx`HighPx`LowPx`LastPx`TotalVolume`TotalValueTrade`BidPrice0`BidOrderQty0`OfferPrice0`OfferOrderQty0\n\tcolTypes = [SYMBOL, TIMESTAMP, DOUBLE, DOUBLE, DOUBLE, DOUBLE, LONG, DOUBLE, DOUBLE[], INT[], DOUBLE[], INT[]]\n\tschemaTable = table(1:0, colNames, colTypes)\n\tdb.createPartitionedTable(table=schemaTable, tableName=tbName, partitionColumns=`DateTime, compressMethods={DateTime:\"delta\"}, sortColumns=`SecurityID`DateTime, keepDuplicates=ALL)\n}\ndbName, tbName = \"dfs://fiveMinuteBar\", \"fiveMinuteBar\"\ncreateFiveMinuteBarDB(dbName, tbName)\n\n// 根据快照数计算 5 min tick 数据\nt = select first(OpenPx) as OpenPx, max(HighPx) as HighPx, min(LowPx) as LowPx, last(LastPx) as LastPx, last(TotalVolumeTrade) as TotalVolumeTrade, last(TotalValueTrade) as TotalValueTrade, toArray(BidPrice[0]) as BidPrice0,  toArray(BidOrderQty[0]) as BidOrderQty0, toArray(OfferPrice[0]) as OfferPrice0, toArray(OfferOrderQty[0]) as OfferOrderQty0 from loadTable(\"dfs://SH_TSDB_snapshot_ArrayVector\", \"snapshot\") group by SecurityID, interval(DateTime, 5m, \"none\") as DateTime map\n\n// 将数据存入数据库\nloadTable(dbName, tbName).append!(t)\n```\n\n----------------------------------------\n\nTITLE: Processing Level 2 Snapshot Data using SQL and Feature Function in DolphinDB\nDESCRIPTION: Executes a SQL `select` query on the `snapshot` table to calculate features. It filters data for dates between 2020.01.01 and 2020.12.31, SecurityIDs present in `stockList`, and specific trading hours (excluding lunch break). Data is grouped by `SecurityID` and 10-minute intervals (`TradeTime` using `interval` function with 'none' mapping). The `featureEngine` function is applied to matrices constructed from the level 2 order book data (prices and quantities) for each group. The results (BAS, DI0-DI9, Press, RV) are stored in the `result` table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/machine_learning_volatility/01.dataProcess.txt#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\nresult = select\n\t\tfeatureEngine(\n\t\tmatrix(BidPrice0,BidPrice1,BidPrice2,BidPrice3,BidPrice4,BidPrice5,BidPrice6,BidPrice7,BidPrice8,BidPrice9),\n\t\tmatrix(BidOrderQty0,BidOrderQty1,BidOrderQty2,BidOrderQty3,BidOrderQty4,BidOrderQty5,BidOrderQty6,BidOrderQty7,BidOrderQty8,BidOrderQty9),\n\t\tmatrix(OfferPrice0,OfferPrice1,OfferPrice2,OfferPrice3,OfferPrice4,OfferPrice5,OfferPrice6,OfferPrice7,OfferPrice8,OfferPrice9),\n\t\tmatrix(OfferOrderQty0,OfferOrderQty1,OfferOrderQty2,OfferOrderQty3,OfferOrderQty4,OfferOrderQty5,OfferOrderQty6,OfferOrderQty7,OfferOrderQty8,OfferOrderQty9)) as `BAS`DI0`DI1`DI2`DI3`DI4`DI5`DI6`DI7`DI8`DI9`Press`RV\n\tfrom snapshot\n\twhere date(TradeTime) between 2020.01.01 : 2020.12.31, SecurityID in stockList, (time(TradeTime) between 09:30:00.000 : 11:29:59.999) || (time(TradeTime) between 13:00:00.000 : 14:56:59.999)\n\tgroup by SecurityID, interval( TradeTime, 10m, \"none\" ) as TradeTime map\n```\n\n----------------------------------------\n\nTITLE: Querying with context by, csort, and top\nDESCRIPTION: Queries the latest data using the `context by`, `csort`, and `top` functions. `context by` partitions the data, `csort` sorts it within each partition, and `top` selects the top N records. A time filter is used to limit the scope of the query. The `timer` keyword is used to measure the execution time.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/snapshot_engine.md#_snippet_10\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\ntimer select top 1 * from loadTable('dfs://testDB', 'trainInfoTable') where ts >datetimeAdd(now(),-1,`h) context by trainID csort ts desc\n```\n\n----------------------------------------\n\nTITLE: Calculating Weighted Sum Across Multiple Columns Using DolphinDB Macro Variable Metaprogramming\nDESCRIPTION: This snippet demonstrates dynamically computing a weighted sum of multiple columns using DolphinDB macro variable metaprogramming. Instead of manually composing complex SQL, the approach multiplies multiple columns by weights then sums row-wise using rowSum, wrapped within a macro variable. The weightedVal column is generated by rowSum of the weighted columns. Inputs include the column names and a weight vector declared as a tuple. The macro variable generation simplifies SQL script complexity and enhances readability while enabling dynamic expression construction.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/macro_var_based_metaprogramming.md#_snippet_26\n\nLANGUAGE: DolphinDB\nCODE:\n```\ncols = \"val\"+string(1..10)\nw = (1..10) \\ 10 $ ANY\n<select *, rowSum(_$$cols * w) as weightedVal from t>.eval()\n```\n\n----------------------------------------\n\nTITLE: Factor Calculation Handler - DolphinDB\nDESCRIPTION: Defines a function `factorHandler` that updates a dictionary with historical ask prices, calculates a factor based on the historical data, and inserts the result into a table. It uses `dictUpdate!` to efficiently update the historical data in the dictionary. Requires a mutable dictionary `historyDict`, a mutable table `factors`, and a message `msg` representing the current data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/func_progr_cases.md#_snippet_14\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef factorHandler(mutable historyDict, mutable factors, msg){\n\thistoryDict.dictUpdate!(function=append!, keys=msg.symbol, parameters=msg.askPrice1, initFunc=x->array(x.type(), 0, 512).append!(x))\n\tsyms = msg.symbol.distinct()\n\tcnt = syms.size()\n\tv = array(DOUBLE, cnt)\n\tfor(i in 0:cnt){\n\t    v[i] = factorAskPriceRatio(historyDict[syms[i]])\n\t}\n\tfactors.tableInsert([take(now(), cnt), syms, v])\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Feature Engineering Logic in DolphinDB\nDESCRIPTION: Defines a globally accessible function `featureEngine` using `defg`. This function takes matrices of bid/offer prices and quantities as input and calculates several financial microstructure features: Bid-Ask Spread (BAS), Weighted Average Price (WAP), Order Book Imbalance (DI), Order Flow Pressure (Press), and Realized Volatility (RV) using efficient matrix operations.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/machine_learning_volatility/06.streamComputingReproduction.txt#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//define function to process data with matrix operation\ndefg featureEngine(bidPrice,bidQty,offerPrice,offerQty){\n\tbas = offerPrice[0]\\bidPrice[0]-1\n\twap = (bidPrice[0]*offerQty[0] + offerPrice[0]*bidQty[0])\\(bidQty[0]+offerQty[0])\n\tdi = (bidQty-offerQty)\\(bidQty+offerQty)\n\tbidw=(1.0\\(bidPrice-wap))\n\tbidw=bidw\\(bidw.rowSum())\n\tofferw=(1.0\\(offerPrice-wap))\n\tofferw=offerw\\(offerw.rowSum())\n\tpress=log((bidQty*bidw).rowSum())-log((offerQty*offerw).rowSum())\n\trv=std(log(wap)-log(prev(wap)))*sqrt(24*252*size(wap))\n\treturn avg(bas),avg(di[0]),avg(di[1]),avg(di[2]),avg(di[3]),avg(di[4]),avg(di[5]),avg(di[6]),avg(di[7]),avg(di[8]),avg(di[9]),avg(press),rv\n}\n```\n\n----------------------------------------\n\nTITLE: Performing Principal Component Analysis (PCA) in DolphinDB\nDESCRIPTION: This snippet illustrates how to perform Principal Component Analysis (PCA) on data originating from a DolphinDB table using the `pca` function. It demonstrates the creation of a table, converting it into a data source using `sqlDS`, and applying `pca` to the specified columns, showing how to access the returned dictionary containing results like components and explained variance ratio.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/matrix.md#_snippet_34\n\nLANGUAGE: DolphinDB\nCODE:\n```\n>x = [7,1,1,0,5,2]\n>y = [0.7, 0.9, 0.01, 0.8, 0.09, 0.23]\n>t=table(x, y)\n>ds = sqlDS(<select * from t>);\n\n>pca(ds);\ncomponents->\n#0        #1\n--------- ---------\n-0.999883 0.015306\n-0.015306 -0.999883\n\nexplainedVarianceRatio->[0.980301,0.019699]\nsingularValues->[6.110802,0.866243]\n```\n\n----------------------------------------\n\nTITLE: Calculating Annual Return in DolphinDB\nDESCRIPTION: This function calculates the annualized return of a given value series. It takes a value series as input and returns the annualized return based on the formula (1 + current return) ** (252 / interval days) - 1. It uses the first and last values of the input series.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_factor.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\n/**\n * 因子1：年化收益率\n            (1 + 当前收益率） ** (252\\区间天数) -1\n */\ndefg getAnnualReturn(value){\n      return pow(1 + ((last(value) - first(value))\\first(value)), 252\\730) - 1\n}\n```\n\n----------------------------------------\n\nTITLE: Precision Differences Between DECIMAL and FLOAT/DOUBLE Types - DolphinDB\nDESCRIPTION: Explains that FLOAT and DOUBLE types follow the IEEE 754 standard where decimal values are rounded when truncated, while DECIMAL32 and DECIMAL64 follow IEEE 754-2008 decimal floating-point standard providing exact decimal precision without rounding errors typical in binary floating-point. Floating point types internally approximate decimals leading to precision loss; DECIMAL types avoid that but have stricter range limits and overflow behaviors. Illustrates the difference in truncation results for the decimal number 1.2356789 to three decimal places.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DECIMAL.md#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// FLOAT/DOUBLE output example: 1.236\n// DECIMAL32/DECIMAL64 output example: 1.235\n```\n\n----------------------------------------\n\nTITLE: Filtering by Date, Grouping by Stock Code - DolphinDB\nDESCRIPTION: This DolphinDB script filters the \"trades\" table based on the \"trade_date\" column, selecting records where the trade date is greater than January 12, 2014. It then groups the filtered data by \"ts_code\" and calculates the average low price for each group.  The `timer(10)` function is used for performance measurement, and `clearAllCache()` is used before each run.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_24\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//按日期过滤，按股票代码分组\ntimer(10) select avg(low) from trades where trade_date > 2014.01.12 group by ts_code\n```\n\n----------------------------------------\n\nTITLE: Calculating Alpha Coefficient in DolphinDB\nDESCRIPTION: Defines a DolphinDB function `getAlpha` to calculate Jensen's Alpha. It takes `value` (fund net values) and `price` (benchmark values). It uses previously defined `getAnnualReturn` (for both fund and benchmark) and `getBeta`, assuming a risk-free rate of 3% (0.03).\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/fund_factor_contrasted_by_py.md#_snippet_16\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef getAlpha(value, price){\n\treturn getAnnualReturn(value) - 0.03 - getBeta(value, price) * (getAnnualReturn(price) - 0.03)\n}\n```\n\n----------------------------------------\n\nTITLE: Remove rows with NULL values (column-wise)\nDESCRIPTION: This DolphinDB script removes rows containing NULL values from a table using a column-wise approach with `each` and `rowAnd`. It applies `isValid` to each column and then uses `rowAnd` to check if all columns in a row are valid, filtering out rows with any NULL values. More efficient than row-wise methods.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\nt[each(isValid, t.values()).rowAnd()]\n```\n\n----------------------------------------\n\nTITLE: Compare two tables using each\nDESCRIPTION: This DolphinDB script compares two tables, `t1` and `t2`, to check if they are identical.  It uses the `each` function to compare corresponding columns of the two tables and then uses `all` to confirm that all columns are equal. Assumes both tables have the same schema.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_21\n\nLANGUAGE: shell\nCODE:\n```\nall(each(eqObj, t1.values(), t2.values()))\n```\n\n----------------------------------------\n\nTITLE: Querying DolphinDB Server Version Using DolphinDB Script\nDESCRIPTION: This simple snippet calls the version() function in the DolphinDB interactive programming interface to retrieve the current DolphinDB server version. Useful for verifying successful cluster upgrades or identifying installed software versions.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/multi_machine_cluster_deployment.md#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\nversion()\n```\n\n----------------------------------------\n\nTITLE: Constructing Daily Multi-Column Filter Queries Manually in DolphinDB Script\nDESCRIPTION: Explicitly writes four separate select-from-where-order-limit queries for a daily snapshot, each varying filter columns and specific values, then merges all results using reduce with unionAll. Serves as a baseline for dynamic query generation and emphasizes the scaling limitations of manual query specification.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_SQL_Case_Tutorial.md#_snippet_44\n\nLANGUAGE: DolphinDB\nCODE:\n```\nt1 = select * from t where vn=50982208, bc=25, cc=814, stt=11, vt=2, dsl=2020.02.05, mt < 52355979 order by mt desc limit 1\nt2 = select * from t where vn=50982208, bc=25, cc=814, stt=12, vt=2, dsl=2020.02.05, mt < 52355979 order by mt desc limit 1\nt3 = select * from t where vn=51180116, bc=25, cc=814, stt=12, vt=2, dsl=2020.02.05, mt < 52354979 order by mt desc limit 1\nt4 = select * from t where vn=41774759, bc=1180, cc=333, stt=3, vt=116, dsl=2020.02.05, mt < 52355979 order by mt desc limit 1\n\nreduce(unionAll, [t1, t2, t3, t4])\n```\n\n----------------------------------------\n\nTITLE: Finding the index of the maximum value in each row using imax and transpose\nDESCRIPTION: This code snippet calculates the index of the maximum value in each row of the matrix 'm' by transposing the matrix and then applying the 'imax' function. `imax` function returns the index of the maximum value in each column.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_34\n\nLANGUAGE: shell\nCODE:\n```\nimax(m.transpose())\n```\n\n----------------------------------------\n\nTITLE: Adding Cluster Replication Configurations - Slave\nDESCRIPTION: This code snippet adds the following configurations to the cluster.cfg file in the slave cluster: `clusterReplicationMode=slave` specifies the cluster as a slave, `clusterReplicationExecutionUsername=admin` and `clusterReplicationExecutionPassword=123456` specify the user credentials to perform the replication tasks. Ensure that the user has sufficient permissions in the master cluster. These settings are performed via a text editor (vim).\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Asynchronous_Replication.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nclusterReplicationMode=slave\nclusterReplicationExecutionUsername=admin\nclusterReplicationExecutionPassword=123456\n```\n\n----------------------------------------\n\nTITLE: Testing Stream Engine with Single Data Point in DolphinDB\nDESCRIPTION: This code tests the stream engine by appending a single data point to it. It selects one row from the table `t` where the time is equal to 09:30:00.000.  The `timer` function measures the execution time of the append operation, allowing for performance analysis.  The `getStreamEngine` function retrieves the stream engine instance and then uses `append!` to pass the test data to the engine. It depends on the stream engine 'calChange' being initialized and the table `t` containing historical data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/Real-Time_Stock_Price_Increase_Calculation/05.性能测试.txt#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// test once 1 \ntestData = select * from t where time(DateTime) = 09:30:00.000 limit 1\ntimer(10){getStreamEngine(\"calChange\").append!(testData)}\n```\n\n----------------------------------------\n\nTITLE: Elasticsearch Grouping by Trade Date (Multiple Aggregations) - Python\nDESCRIPTION: This Python function performs an aggregation query on Elasticsearch, grouping documents by 'trade_date' and calculating the maximum 'open' price and the sum of 'pre_close' prices for each date. It uses `urllib3` to send a GET request to the Elasticsearch API, including a JSON payload that defines the aggregations. The function prints the HTTP status code and the parsed JSON response.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_33\n\nLANGUAGE: Python\nCODE:\n```\ndef search_8():\n    http = urllib3.PoolManager()\n    data = json.dumps({\n        \"aggs\": {\n            \"group_by_trade_date\": {\n                \"terms\": {\n                    \"field\": \"trade_date\",\n                    \"size\": 5000\n                },\n                \"aggs\": {\n                    \"max_open\": {\n                        \"max\": {\"field\": \"open\"}\n                    },\n                    \"sum_pre_close\": {\n                        \"sum\": {\"field\": \"pre_close\"}\n                    }\n                }\n            }\n        },\n        \"_source\": [\"\"]\n    }).encode(\"utf-8\")\n    r = http.request(\"GET\", \"http://localhost:9200/elastic/_search\", body=data,\n                     headers={'Content-Type': 'application/json'})\n    print(r.status)\n    print(json.loads(r.data.decode()))\n```\n\n----------------------------------------\n\nTITLE: Validating Stock Level 2 Data Counts Across Different Tables in DolphinDB - DolphinDB\nDESCRIPTION: Defines a function to verify the consistency of stock counts across entrust, snapshot, and trade tables for a given date in the specified database. It retrieves distinct security IDs subjected to market and code prefix filters in each table, then compares counts and throws an error if mismatched. Implements a local helper function to encapsulate loading and filtering logic. This validation is crucial for data integrity in downstream business processes.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/dolphinscheduler_integration.md#_snippet_15\n\nLANGUAGE: DolphinDB\nCODE:\n```\nmodule dataCheck::stockCheck \n\ndef checkStockCounts(idate,dbName)\n{\n\t// 校验逐笔委托、快照行情、逐笔成交表的股票个数是否一致\n\n\tgetCodes = def (dbName,tbName,idate) {\n\t\ttb = loadTable(dbName,tbName)\n\t\treturn exec distinct(SecurityID) from tb where date(tradetime)=idate and ((Market=`sh and SecurityID like \"6%\")or(Market=`sz and (SecurityID like \"0%\" or SecurityID like \"3%\" ) )) \n\t}\n\tentrustCodes = getCodes(dbName,\"entrust\",idate)\n\ttradeCodes = getCodes(dbName,\"trade\",idate)\n    snapshotCodes = exec distinct(SecurityID) from loadTable(dbName,\"snapshot\") where date(tradetime)=idate and ((Market=`sh and SecurityID like \"6%\")or(Market=`sz and (SecurityID like \"0%\" or SecurityID like \"3%\" ))) and  HighPrice != 0\n\tif(entrustCodes.size() != snapshotCodes.size() or entrustCodes.size() != tradeCodes.size() or snapshotCodes.size() != tradeCodes.size())\n\t{\n\t\tthrow \"逐笔委托股票数量：\" + size(entrustCodes) + \" 快照行情股票数量：\" + size(snapshotCodes) + \" 逐笔成交股票数量：\" + size(tradeCodes) + \", 它们数量不一致！\"\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: Verifying DolphinDB Python API on Windows\nDESCRIPTION: Connects to DolphinDB server and executes a print command to confirm that the API and connection setup are successful. Requires valid server IP, port, username, and password.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/python_api_install_offline.md#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nimport dolphindb as ddb\ns = ddb.session()\ns.connect(\"192.168.1.157\", 8848, \"admin\", \"123456\")\ns.run(\"print(\\\"Welcome to DolphinDB!\\\")\")\ns.close()\n```\n\n----------------------------------------\n\nTITLE: Downloading DolphinDB ARM64 Version - Shell\nDESCRIPTION: This snippet demonstrates how to download the DolphinDB ARM64 server package using wget with a dynamic version variable `${release}` to specify the version. It includes commands for downloading a specific version and decompressing the zip archive to a user-defined directory. The unzip command requires the destination directory `<\\/path\\/to\\/directory>` without spaces or non-ASCII characters to avoid startup issues.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ARM_standalone_deploy.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\nwget https://www.dolphindb.cn/downloads/DolphinDB_ARM64_V${release}.zip -O dolphindb.zip\n```\n\nLANGUAGE: Shell\nCODE:\n```\nwget https://www.dolphindb.cn/downloads/DolphinDB_ARM64_V2.00.11.3.zip -O dolphindb.zip\n```\n\nLANGUAGE: Shell\nCODE:\n```\nunzip dolphindb.zip -d </path/to/directory>\n```\n\n----------------------------------------\n\nTITLE: Loading Historical Data and Replaying to Stream in DolphinDB\nDESCRIPTION: Loads historical market data from the CSV file specified by `csvDataPath` into memory using `loadText`, matching the schema of the `snapshotStream` table. The data is ordered by `TradeTime` and `SecurityID`. A background job named 'replay_trade' is submitted using `submitJob` and the `replay` function. This job iterates through the loaded historical data and appends it to the `snapshotStream` table at a controlled rate (20000 records per interval), simulating a live data feed for testing the stream processing pipeline. The loaded data is then cleared from memory (`data = NULL`), and `getRecentJobs` can be used to check the status of the replay job.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/machine_learning_volatility/06.streamComputingReproduction.txt#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//replay history data\ndata = select SecurityID, TradeTime, PreClosePx, OpenPx, HighPx, LowPx, LastPx, TotalVolumeTrade, TotalValueTrade, BidPrice0, BidPrice1, BidPrice2, BidPrice3, BidPrice4, BidPrice5, BidPrice6, BidPrice7, BidPrice8, BidPrice9, BidOrderQty0, BidOrderQty1, BidOrderQty2, BidOrderQty3, BidOrderQty4, BidOrderQty5, BidOrderQty6, BidOrderQty7, BidOrderQty8, BidOrderQty9, OfferPrice0, OfferPrice1, OfferPrice2, OfferPrice3, OfferPrice4, OfferPrice5, OfferPrice6, OfferPrice7, OfferPrice8, OfferPrice9, OfferOrderQty0, OfferOrderQty1, OfferOrderQty2, OfferOrderQty3, OfferOrderQty4, OfferOrderQty5, OfferOrderQty6, OfferOrderQty7, OfferOrderQty8, OfferOrderQty9\nfrom loadText(filename=csvDataPath, schema=table(snapshotStream.schema().colDefs.name, snapshotStream.schema().colDefs.typeString)\n)\norder by TradeTime, SecurityID\nsubmitJob(\"replay_trade\", \"trade\",  replay{data, snapshotStream, `TradeTime, `TradeTime, 20000, true, 1})\ndata = NULL\ngetRecentJobs()\n```\n\n----------------------------------------\n\nTITLE: Reading and Yielding CSV Records as JSON (Python)\nDESCRIPTION: This function sequentially reads rows from a CSV file, converting each line into a Python dictionary according to predefined field names. It batches these dictionaries into lists of up to 100,000 entries each, yielding them for further processing (such as bulk import). Requires the standard csv library; the CSV file must be available at the provided path. Input is implicit (filename hardcoded), output is an iterator yielding lists of parsed records. Large files are supported, but memory constraints should be considered.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef read_lines():\n    with open('/home/revenant/data/US.csv') as f:\n        f.readline()\n        field_name = ['PERMNO', 'date', 'SHRCD', 'TICKER', 'TRDSTAT', 'PERMCO', 'HSICCD', 'CUSIP', 'DLSTCD', 'DLPRC', 'DLRET', 'BIDLO', 'ASKHI', 'PRC', 'VOL', 'RET', 'BID', 'ASK', 'SHROUT', 'CFACPR', 'CFACSHR', 'OPENPRC']\n        symbols = csv.DictReader(f, fieldnames=field_name)\n        cnt = 0\n        temp = []\n        for symbol in symbols:\n            symbol.pop(None, None)\n            try:\n                cnt = cnt+1\n                temp.append(symbol)\n            except:\n                pass\n            if(cnt%100000==0 and cnt!=0):\n                print(cnt)\n                yield temp\n                temp = []\n        if(len(temp) > 0):\n            yield temp\n\n```\n\n----------------------------------------\n\nTITLE: 确保运行环境变量一致以避免连接崩溃\nDESCRIPTION: 建议在运行DolphinDB和使用`isql`时，保证两者环境变量一致，特别是`LD_LIBRARY_PATH`，以确保动态库路径设置正确，避免因环境变量不匹配导致的连接失败或崩溃。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ODBC_plugin_user_guide.md#_snippet_11\n\nLANGUAGE: Shell\nCODE:\n```\nexport LD_LIBRARY_PATH=/path/to/libs:$LD_LIBRARY_PATH\n# 运行DolphinDB或isql验证环境变量配置是否一致\n```\n\n----------------------------------------\n\nTITLE: Analyzing Frequency Distribution and Creating Buckets for Partitioning in DolphinDB\nDESCRIPTION: This snippet analyzes the loaded tick data by grouping counts by symbol and mmid to understand data distribution. It then defines 100 buckets for symbols based on their frequency counts using cutPoints to enable balanced data partitioning. The rightmost bucket boundary is manually adjusted to a high constant symbol value to cover all cases. This setup supports creating range-based composite partitions.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs MongoDB/dolphindb_taq_partitioned.txt#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\nsample_freq_tb = select count(*) from sample_tb group by symbol\nmmid_tb = select count(*) from sample_tb group by mmid\n// 8369 rows, [symbol, count], 分到 100 个 buckets\nBIN_NUM = 100\nbuckets = cutPoints(sample_freq_tb.symbol, BIN_NUM, sample_freq_tb.count)\n// [A, ABL, ACU, ..., ZZZ], 101 个边界\nbuckets[BIN_NUM] = `ZZZZZZ\t\t// 调整最右边界\n```\n\n----------------------------------------\n\nTITLE: Creating and Using Asof Join Engine in DolphinDB for Trade-Cost Calculation\nDESCRIPTION: This script sets up a streaming Asof Join engine in DolphinDB to associate each trade record with the most recent prior quote snapshot. It defines streaming tables for trades, snapshots, and output, configures the engine with matching and time columns, sets delayedTime to enforce timely output even if no future right records arrive, and subscribes the tables to the engine. Dependencies include DolphinDB streaming environment and relevant data columns. Input trade and snapshot data tables simulate real-time streams while the output table captures enriched trade data with matched snapshot info.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming-real-time-correlation-processing.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// create table\nshare streamTable(1:0, `Sym`TradeTime`TradePrice, [SYMBOL, TIME, DOUBLE]) as trades\nshare streamTable(1:0, `Sym`Time`Bid1Price`Ask1Price, [SYMBOL, TIME, DOUBLE, DOUBLE]) as snapshot\nshare streamTable(1:0, `TradeTime`Sym`TradePrice`TradeCost`SnapshotTime, [TIME, SYMBOL, DOUBLE, DOUBLE, TIME]) as output\n\n// create engine\najEngine = createAsofJoinEngine(name=\"asofJoin\", leftTable=trades, rightTable=snapshot, outputTable=output, metrics=<[TradePrice, abs(TradePrice-(Bid1Price+Ask1Price)/2), snapshot.Time]>, matchingColumn=`Sym, timeColumn=`TradeTime`Time, useSystemTime=false, delayedTime=1000)\n\n// subscribe topic\nsubscribeTable(tableName=\"trades\", actionName=\"appendLeftStream\", handler=getLeftStream(ajEngine), msgAsTable=true, offset=-1, hash=0)\nsubscribeTable(tableName=\"snapshot\", actionName=\"appendRightStream\", handler=getRightStream(ajEngine), msgAsTable=true, offset=-1, hash=1)\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// generate data: trade\nt1 = table(`A`A`B`A`B`B as Sym, 10:00:02.000+(1..6)*700 as TradeTime,  (3.4 3.5 7.7 3.5 7.5 7.6) as TradePrice)\n// generate data: snapshot\nt2 = table(`A`B`A`B as Sym, 10:00:00.000+(3 3 6 6)*1000 as Time, (3.5 7.6 3.5 7.6) as Bid1Price, (3.5 7.6 3.6 7.6) as Ask1Price)\n// input data\nsnapshot.append!(t2)\ntrades.append!(t1)\n```\n\n----------------------------------------\n\nTITLE: Cumulative Sum Calculation with byRow\nDESCRIPTION: Calculates the cumulative sum of each row in a Fast Array Vector using the `byRow` function and the `cumsum` function in DolphinDB. The example showcases its application on a Fast Array Vector and within a table query.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Array_Vector.md#_snippet_15\n\nLANGUAGE: DolphinDB\nCODE:\n```\nx = array(INT[], 0).append!([1 2 3, 4 5, 6 7 8, 9 10])\nz = byRow(cumsum, x)\n/* z\n[[1,3,6],[4,9],[6,13,21],[9,19]]\n*/\n\nt = table(1 2 3 4 as id, x as x)\nnew_t = select *, byRow(cumsum, x) as new_x from t\n/* new_t\nid x       new_x        \n-- ------- ---------\n1  [1,2,3] [1,3,6]  \n2  [4,5]   [4,9]    \n3  [6,7,8] [6,13,21]\n4  [9,10]  [9,19]   \n*/\n```\n\n----------------------------------------\n\nTITLE: Python API 查询股票历史交易数据\nDESCRIPTION: 该代码示例使用 DolphinDB Python API 连接计算节点，加载股票交易数据表，筛选特定日期和股票代码的历史交易信息，并限制返回行数，便于进行后续分析或处理。主要依赖于 DolphinDB Python SDK，参数包括连接信息、表名、筛选条件和行数限制，输入为日期和股票ID，输出为数据框。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Compute_Node.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nimport DolphinDB as ddb\n\ndef getData(date, securityID, rowNum=1000000):\n    s = ddb.session()\n    s.connect(\"computenode1\", 8961, \"admin\", \"123456\")\n    trade = s.loadTable(tableName=\"sztrade\", dbPath=\"dfs://szstock\")\n    rst = trade.select(\"*\").where(\"tradedate = %d\" % date)\\\n        .where(\"securityID = '%s'\" % securityID) \\ \n        .where(\"execType=\\\"F\\\"\")\\\n        .limit(rowNum).toDF()\n    s.close()\n    return rst\n\nif __name__ == '__main__' :\n    print(getData(20200102, \"000014\"))\n```\n\n----------------------------------------\n\nTITLE: Configuring NTP Server on Linux via ntp.conf (config)\nDESCRIPTION: Edit `/etc/ntp.conf` to set up a machine as an NTP server. The config snippet allows local subnet clients to synchronize with the host and utilizes public NTP servers for time accuracy. Change IP/subnet to fit your network environment. NTPD service must be restarted for changes to take effect.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/deploy_dolphindb_on_new_server.md#_snippet_5\n\nLANGUAGE: config\nCODE:\n```\n# vi /etc/ntp.conf\n...\n# Hosts on local network are less restricted.\nrestrict 192.168.0.0 mask 255.255.254.0 nomodify notrap\n\n# Use public servers from the pool.ntp.org project.\n# Please consider joining the pool (http://www.pool.ntp.org/join.html).\nserver 0.cn.pool.ntp.org iburst\nserver 1.cn.pool.ntp.org iburst\nserver 2.cn.pool.ntp.org iburst\nserver 3.cn.pool.ntp.org iburst\n...\n\n```\n\n----------------------------------------\n\nTITLE: Terminating DolphinScheduler Processes (Bash)\nDESCRIPTION: This command sends a SIGTERM signal (`-15`) to the process specified by `进程ID` (Process ID), requesting it to terminate gracefully. This is used to stop stuck DolphinScheduler processes identified by `ps aux | grep dolphinscheduler` during troubleshooting. Replace `进程ID` with the actual process ID obtained from the `ps` command.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/dolphinscheduler_integration.md#_snippet_26\n\nLANGUAGE: Bash\nCODE:\n```\nkill -15 进程ID\n```\n\n----------------------------------------\n\nTITLE: Listing Shared Library Dependencies\nDESCRIPTION: This command utilizes the `ldd` utility to list the shared library dependencies of a given ODBC driver library. It helps identify missing or unresolved dependencies that could prevent the driver from loading and functioning correctly. Replace `libodbcdriver.so` with the actual driver library file name.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ODBC_plugin_user_guide.md#_snippet_4\n\nLANGUAGE: Shell\nCODE:\n```\nldd libodbcdriver.so      #libodbcdriver.so 替换为对应 ODBC Driver 库文件名\n```\n\n----------------------------------------\n\nTITLE: Calculating and Sorting Funds by Minimum Fees\nDESCRIPTION: Adds a new column 'Fee' to the data by summing management, custody, and sales service fees. Performs top-N selection and filtering based on fund type and name characteristics. Enables identification of funds with lowest fees, aiding investment decision-making.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/public_fund_basic_analysis.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\nfundFee = select *, (MFee + CFee + SFee) as Fee from fundData\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect top 50 * from fundFee where Type == \"债券型\" order by Fee\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect top 50 * from  fundFee where Type == \"债券型\", not(FullName like \"%指数%\") order by Fee\n```\n\n----------------------------------------\n\nTITLE: Retrieving and Re-inserting Unwritten Data after Failure in DolphinDB C++ API\nDESCRIPTION: 当 MTW 发生异常导致部分数据未写入时，此代码段用于检索未写入缓冲区数据，并重建新的 MultithreadedTableWriter 实例进行二次写入操作。需先调用 getUnwrittenData 获取未提交数据，再用 insertUnwrittenData 完成补写流程。错误与异常均需详细处理。关键参数包括服务器信息、目标库表、压缩方式与错误信息。适用于高可靠、高可用数据写入场景，依赖：DolphinDB C++ API, MTW。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_cpp_api_connector.md#_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\nwriter.getStatus(status);\n    if (status.hasError()) {\n        cout << \"error after write complete: \" << status.errorInfo << endl;\n        // 获取未写入的数据\n        std::vector<std::vector<ConstantSP>*> unwrittenData;\n        writer.getUnwrittenData(unwrittenData);\n        cout << \"unwriterdata length \" << unwrittenData.size() << endl;\n        if (!unwrittenData.empty()) {\n            try {\n                // 重新写入这些数据，原有的 MTW 因为异常退出已经不能用了，需要创建新的 MTW\n                cout << \"create new MTW and write again.\" << endl;\n                MultithreadedTableWriter newWriter(\"183.136.170.167\", 9900, \"admin\", \"123456\", \"dfs://test_MultithreadedTableWriter\", \"collect\", NULL,false,NULL,1000,1,5,\"deviceid\", &compress);\n                ErrorCodeInfo errorInfo;\n                // 插入未写入的数据\n                if (newWriter.insertUnwrittenData(unwrittenData, errorInfo)) {\n                    // 等待写入完成后检查状态\n                    newWriter.waitForThreadCompletion();\n                    newWriter.getStatus(status);\n                    if (status.hasError()) {\n                        cout << \"error in write again: \" << status.errorInfo << endl;\n                    }\n                }\n                else {\n                    cout << \"error in write again: \" << errorInfo.errorInfo << endl;\n                }\n            }\n            catch (exception &e) {\n                cerr << \"new MTW exit with exception: \" << e.what() << endl;\n            }\n        }\n    }\n```\n\n----------------------------------------\n\nTITLE: Creating a Distributed Time-Series Database Table for Snapshot Data in DolphinDB\nDESCRIPTION: This snippet establishes a partitioned distributed TSDB table in DolphinDB for snapshot data using schema obtained via amdQuote::getSchema. It creates a database with a date and symbol composite partition, then creates the partitioned table with 'tradeDate' and 'securityCode' as partition columns. It checks for existence before creation to avoid duplication. The schema includes an added 'tradeDate' column aligned with the streaming table and supports keeping duplicate records.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_implementation_for_AMD_Plugin.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 获取行情数据的表格构造并建立分布存储表 snapshot 为例\nsnapshotSchema = amdQuote::getSchema(`snapshot)\ndbName = \"dfs://amd\"\nsnapshotTbName = \"snapshot\"\nif (!existsDatabase(dbName)) {\n    dbDate = database(\"\", VALUE, 2023.01.01..2023.12.31)\n    dbCode = database(\"\", HASH, [SYMBOL, 10])\n    db = database(dbName, COMPO, [dbDate, dbCode], engine=\"TSDB\")\n} else {\n    db = database(dbName)\n}\n\nif (!existsTable(dbName, snapshotTbName)) {\n    colName = append!([\"tradeDate\"], snapshotSchema.name)\n    colType = append!([\"DATE\"], snapshotSchema.type)\n    tbSchema = table(1:0, colName, colType)\n    pt = db.createPartitionedTable(table=tbSchema, tableName=snapshotTbName, partitionColumns=`tradeDate`securityCode,  sortColumns=`securityCode`origTime, keepDuplicates=ALL)\n}\n```\n\n----------------------------------------\n\nTITLE: 计算分钟级K线数据\nDESCRIPTION: 通过group by与聚合函数计算某只股票的分钟级K线数据，包括开盘价、最高价、最低价、收盘价和成交量等信息。此示例仅计算特定日期的数据。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/quant_finance_examples.md#_snippet_8\n\nLANGUAGE: DolphinDB\nCODE:\n```\nminuteBar = select first(last) as open, max(last) as high, min(last) as low, last(last) as last, sum(curVol) as volume from quotes where date=2020.06.01, symbol>=`600000 group by symbol, date, minute(time) as minute;\n```\n\n----------------------------------------\n\nTITLE: Creating Composite Partitioned Table by TradeDate and SecurityID in DolphinDB Script to Avoid Concurrent Write Conflicts\nDESCRIPTION: Recommends creating composite partitions using trade date and security ID to separate writes across partitions in concurrent write scenarios, avoiding transaction conflicts. This approach follows best practice to partition data along concurrency-sensitive fields (2.8.2 solution). Requires DolphinDB composite partitioning and concurrency management. Input is database schema and partition fields; output is optimized partitioned table supporting parallel writes safely.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/database_and_table_creation_details.md#_snippet_16\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndb1 = database(, VALUE, 2020.01.01..2021.01.01)\ndb2 = database(, HASH, [SYMBOL, 25])\ndb = database(\"dfs://testDB2\", partitionType=COMPO, partitionScheme=[db1, db2])\ncolName = `SecurityID`TradeDate`TradeTime`TradePrice`TradeQty`TradeAmount`BuyNo`SellNo\ncolType = `SYMBOL`DATE`TIME`DOUBLE`INT`DOUBLE`INT`INT\ntbSchema = table(1:0, colName, colType)\ndb.createPartitionedTable(table=tbSchema, tableName=`pt, partitionColumns=`TradeDate`SecurityID)\n```\n\n----------------------------------------\n\nTITLE: Group By Parallel Query (Unoptimized - Memory Table)\nDESCRIPTION: This code snippet demonstrates an unoptimized approach to grouping and aggregating data by first creating a temporary in-memory table. This introduces extra steps of merging and splitting data, reducing performance.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_sql_cases.md#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntimer {\n\ttmp_t = select *, iif(LastPx > OpenPx, 1, 0) as Flag \n\t\t\tfrom snapshot \n\t\t\twhere date(DateTime) = 2020.06.01, second(DateTime) >= 09:30:00\n\tt1 = select iif(max(OfferPrice1) - min(BidPrice1) == 0, 0, 1) as Price1Diff, count(OfferPrice1) as OfferPrice1Count, sum(Volume) as Volumes \n\t\t\tfrom tmp_t \n\t\t\tgroup by SecurityID, date(DateTime) as Date, Flag\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Partitioned Database - Dolphindb\nDESCRIPTION: Defines a function to create a two-level partitioned database in Dolphindb. The first level is value-partitioned by a datetime column, and the second is range-partitioned by an ID column. It creates the database and an associated partitioned table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/singleValueModeWrite.txt#_snippet_0\n\nLANGUAGE: Dolphindb\nCODE:\n```\ndef createDatabase(dbName,tableName, ps1, ps2){\n\ttableSchema = table(1:0,`id`datetime`value,[INT,DATETIME,FLOAT]);\n\tdb1 = database(\"\", VALUE, ps1)\n\tdb2 = database(\"\", RANGE, ps2)\n\tdb = database(dbName,COMPO,[db1,db2])\n\tdfsTable = db.createPartitionedTable(tableSchema,tableName,`datetime`id)\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Registered Subscription Information in DolphinDB\nDESCRIPTION: This snippet retrieves information about registered subscription tables in DolphinDB. It uses the `getStreamingStat()` function to access streaming statistics and then accesses the `pubTables` property to obtain the subscription table information.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/streaming_capital_flow_order_by_order/07.流计算状态监控函数.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\ngetStreamingStat().pubTables\n```\n\n----------------------------------------\n\nTITLE: Creating Sample Data for Stock Volatility Calculation\nDESCRIPTION: This snippet creates a sample table `t` with columns `date`, `code`, and `rate` representing daily stock return rates. The `date` column contains dates from 2011-11-01 to 2021-10-31. The `code` column contains the stock symbol 'AAPL'. The `rate` column contains randomly generated return rates.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_sql_cases.md#_snippet_24\n\nLANGUAGE: DolphinDB\nCODE:\n```\nN = 3653\nt = table(2011.11.01..2021.10.31 as date, \n          take(`AAPL, N) as code, \n          rand([0.0573, -0.0231, 0.0765, 0.0174, -0.0025, 0.0267, 0.0304, -0.0143, -0.0256, 0.0412, 0.0810, -0.0159, 0.0058, -0.0107, -0.0090, 0.0209, -0.0053, 0.0317, -0.0117, 0.0123], N) as rate)\n```\n\n----------------------------------------\n\nTITLE: Listing Directory Contents for TSDB (keepDuplicates=LAST)\nDESCRIPTION: This snippet shows the directory structure after updates in a TSDB table with `keepDuplicates=LAST`. It uses the `tree` command to demonstrate the addition of new level files as a result of the update, reflecting the append-only behavior.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/dolphindb_update.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\n$ tree\n.\n├── chunk.dict\n└── machines_2\n    ├── 0_00000010\n    ├── 0_00000011\n    ├── 0_00000012\n    ├── 0_00000013\n    ├── 0_00000014\n    └── 1_00000002\n\n1 directory, 7 files\n```\n\n----------------------------------------\n\nTITLE: Installing DolphinDB Jupyter Notebook plugin\nDESCRIPTION: These are shell commands used to install and enable the DolphinDB Jupyter Notebook plugin. The first command installs the plugin using pip, and the second command enables the plugin within Jupyter Notebook.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/client_tool_tutorial.md#_snippet_4\n\nLANGUAGE: Shell\nCODE:\n```\npip install dolphindb_notebook\n```\n\nLANGUAGE: Shell\nCODE:\n```\njupyter nbextension enable dolphindb/main\n```\n\n----------------------------------------\n\nTITLE: Defining a DolphinDB Module with a Function\nDESCRIPTION: Illustrates the structure of a basic DolphinDB module file (fileLog.dos). It starts with the module declaration `module fileLog` and includes a function definition `appendLog` which writes a log message to a specified file, including the current timestamp. Only module declarations, import statements, and function definitions are allowed within the module file.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/module_tutorial.md#_snippet_1\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nmodule fileLog\n\ndef appendLog(filePath, logText){\n\tf = file(filePath,\"a+\")\n\tf.writeLine(string(now()) + \" : \" + logText)\n\tf.close()\n}\n```\n\n----------------------------------------\n\nTITLE: Calculating Factor Correlation Matrix\nDESCRIPTION: This code snippet calculates the correlation matrix of factors using an array vector approach. It first groups data by trade time and security ID, converting factor values into array vectors. Then, it computes the correlation matrix for each security and averages the results.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practice_for_factor_calculation.md#_snippet_21\n\nLANGUAGE: DolphinDB\nCODE:\n```\nday_data = select toArray(val) as factor_value from loadTable(\"dfs://MIN_FACTOR_OLAP_VERTICAL\",\"min_factor\") where date(tradetime) = 2020.01.03 group by tradetime, securityid\nresult = select toArray(matrix(factor_value).corrMatrix()) as corr from day_data group by securityid\ncorrMatrix = result.corr.matrix().avg().reshape(size(distinct(day_data.factorname)):size(distinct(day_data.factorname)))\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Differences Between loop and each High-Order Functions in DolphinDB\nDESCRIPTION: Illustrates the difference in return types between 'each' and 'loop' DolphinDB high-order functions. 'each' returns a vector, matrix, or table based on the uniformity and structure of sub-task results, while 'loop' consistently returns a tuple, accommodating heterogeneous results. Using example matrix 'm', applies 'each' with a adding function to produce a matrix result. Then using table 't' with multiple columns, applies 'loop' with 'max' function returning a tuple containing the maximum value for each column. No external dependencies besides DolphinDB environment.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/func_progr_cases.md#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\nm=1..12$4:3;\nm;\neach(add{1 2 3 4}, m);\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nt = table(1 2 3 as id, 4 5 6 as value, `IBM`MSFT`GOOG as name);\nt;\nloop(max, t.values());\n```\n\n----------------------------------------\n\nTITLE: Defining and Querying Market Pressure Factors in DolphinDB SQL Script - dolphindb\nDESCRIPTION: This DolphinDB script defines a custom function 'calPress' to compute the trade pressure indicator based on bid/offer prices and quantities from a Level 2 order book. It initializes stock lists and loads a distributed table, then executes a SQL aggregation to compute price spread, depth imbalance (across all levels), order flow pressure, and realized volatility at 10-minute intervals by security. Required dependencies: DolphinDB distributed database with Level 2 order book data, at least 8 logical CPU cores, and loaded Snapshot data containing the 40 required columns. The function parameters correspond to price and size vectors for 10 bid and 10 offer levels; the expected output is a grouped, aggregated table with computed financial metrics, best suited for further modeling or feature analysis.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/sql_performance_optimization_wap_di_rv.md#_snippet_0\n\nLANGUAGE: dolphindb\nCODE:\n```\n/**\npart1: Define calculation function\n*/\ndef calPress(BidPrice0,BidPrice1,BidPrice2,BidPrice3,BidPrice4,BidPrice5,BidPrice6,BidPrice7,BidPrice8,BidPrice9,BidOrderQty0,BidOrderQty1,BidOrderQty2,BidOrderQty3,BidOrderQty4,BidOrderQty5,BidOrderQty6,BidOrderQty7,BidOrderQty8,BidOrderQty9,OfferPrice0,OfferPrice1,OfferPrice2,OfferPrice3,OfferPrice4,OfferPrice5,OfferPrice6,OfferPrice7,OfferPrice8,OfferPrice9,OfferOrderQty0,OfferOrderQty1,OfferOrderQty2,OfferOrderQty3,OfferOrderQty4,OfferOrderQty5,OfferOrderQty6,OfferOrderQty7,OfferOrderQty8,OfferOrderQty9){\n\tWAP = (BidPrice0*OfferOrderQty0+OfferPrice0*BidOrderQty0)\\(BidOrderQty0+OfferOrderQty0)\n\tBid_1_P_WAP_SUM = 1\\(BidPrice0-WAP) + 1\\(BidPrice1-WAP) + 1\\(BidPrice2-WAP) + 1\\(BidPrice3-WAP) + 1\\(BidPrice4-WAP) + 1\\(BidPrice5-WAP) + 1\\(BidPrice6-WAP) + 1\\(BidPrice7-WAP) + 1\\(BidPrice8-WAP) + 1\\(BidPrice9-WAP)\n\tOffer_1_P_WAP_SUM = 1\\(OfferPrice0-WAP)+1\\(OfferPrice1-WAP)+1\\(OfferPrice2-WAP)+1\\(OfferPrice3-WAP)+1\\(OfferPrice4-WAP)+1\\(OfferPrice5-WAP)+1\\(OfferPrice6-WAP)+1\\(OfferPrice7-WAP)+1\\(OfferPrice8-WAP)+1\\(OfferPrice9-WAP)\n\tBidPress = BidOrderQty0*((1\\(BidPrice0-WAP))\\Bid_1_P_WAP_SUM) + BidOrderQty1*((1\\(BidPrice1-WAP))\\Bid_1_P_WAP_SUM) + BidOrderQty2*((1\\(BidPrice2-WAP))\\Bid_1_P_WAP_SUM) + BidOrderQty3*((1\\(BidPrice3-WAP))\\Bid_1_P_WAP_SUM) + BidOrderQty4*((1\\(BidPrice4-WAP))\\Bid_1_P_WAP_SUM) + BidOrderQty5*((1\\(BidPrice5-WAP))\\Bid_1_P_WAP_SUM) + BidOrderQty6*((1\\(BidPrice6-WAP))\\Bid_1_P_WAP_SUM) + BidOrderQty7*((1\\(BidPrice7-WAP))\\Bid_1_P_WAP_SUM) + BidOrderQty8*((1\\(BidPrice8-WAP))\\Bid_1_P_WAP_SUM) + BidOrderQty9*((1\\(BidPrice9-WAP))\\Bid_1_P_WAP_SUM)\n\tOfferPress = OfferOrderQty0*((1\\(OfferPrice0-WAP))\\Offer_1_P_WAP_SUM) + OfferOrderQty1*((1\\(OfferPrice1-WAP))\\Offer_1_P_WAP_SUM) + OfferOrderQty2*((1\\(OfferPrice2-WAP))\\Offer_1_P_WAP_SUM) + OfferOrderQty3*((1\\(OfferPrice3-WAP))\\Offer_1_P_WAP_SUM) + OfferOrderQty4*((1\\(OfferPrice4-WAP))\\Offer_1_P_WAP_SUM) + OfferOrderQty5*((1\\(OfferPrice5-WAP))\\Offer_1_P_WAP_SUM) + OfferOrderQty6*((1\\(OfferPrice6-WAP))\\Offer_1_P_WAP_SUM) + OfferOrderQty7*((1\\(OfferPrice7-WAP))\\Offer_1_P_WAP_SUM) + OfferOrderQty8*((1\\(OfferPrice8-WAP))\\Offer_1_P_WAP_SUM) + OfferOrderQty9*((1\\(OfferPrice9-WAP))\\Offer_1_P_WAP_SUM)\n\treturn log(BidPress)-log(OfferPress)\n}\n\n/**\npart2: Define variables and assign values\n*/\nstockList=`601318`600519`600036`600276`601166`600030`600887`600016`601328`601288`600000`600585`601398`600031`601668`600048`601888`600837`601601`601012`603259`601688`600309`601988`601211`600009`600104`600690`601818`600703`600028`601088`600050`601628`601857`601186`600547`601989`601336`600196`603993`601138`601066`601236`601319`603160`600588`601816`601658`600745\ndbName = \"dfs://snapshot_SH_L2_OLAP\"\ntableName = \"snapshot_SH_L2_OLAP\"\nsnapshot = loadTable(dbName, tableName)\n\n/**\npart3: Execute SQL\n*/\nresult = select\n            avg((OfferPrice0\\BidPrice0-1)) as BAS,\n            avg((BidOrderQty0-OfferOrderQty0)\\(BidOrderQty0+OfferOrderQty0)) as DI0,\n            avg((BidOrderQty1-OfferOrderQty1)\\(BidOrderQty1+OfferOrderQty1)) as DI1,\n            avg((BidOrderQty2-OfferOrderQty2)\\(BidOrderQty2+OfferOrderQty2)) as DI2,\n            avg((BidOrderQty3-OfferOrderQty3)\\(BidOrderQty3+OfferOrderQty3)) as DI3,\n            avg((BidOrderQty4-OfferOrderQty4)\\(BidOrderQty4+OfferOrderQty4)) as DI4,\n            avg((BidOrderQty5-OfferOrderQty5)\\(BidOrderQty5+OfferOrderQty5)) as DI5,\n            avg((BidOrderQty6-OfferOrderQty6)\\(BidOrderQty6+OfferOrderQty6)) as DI6,\n            avg((BidOrderQty7-OfferOrderQty7)\\(BidOrderQty7+OfferOrderQty7)) as DI7,\n            avg((BidOrderQty8-OfferOrderQty8)\\(BidOrderQty8+OfferOrderQty8)) as DI8,\n            avg((BidOrderQty9-OfferOrderQty9)\\(BidOrderQty9+OfferOrderQty9)) as DI9,\n            avg(calPress(BidPrice0,BidPrice1,BidPrice2,BidPrice3,BidPrice4,BidPrice5,BidPrice6,BidPrice7,BidPrice8,BidPrice9, BidOrderQty0,BidOrderQty1,BidOrderQty2,BidOrderQty3,BidOrderQty4,BidOrderQty5,BidOrderQty6,BidOrderQty7,BidOrderQty8,BidOrderQty9,OfferPrice0,OfferPrice1,OfferPrice2,OfferPrice3,OfferPrice4,OfferPrice5,OfferPrice6,OfferPrice7,OfferPrice8,OfferPrice9,OfferOrderQty0,OfferOrderQty1,OfferOrderQty2,OfferOrderQty3,OfferOrderQty4,OfferOrderQty5,OfferOrderQty6,OfferOrderQty7,OfferOrderQty8,OfferOrderQty9)) as Press,\n            sqrt(sum(pow((log((BidPrice0*OfferOrderQty0+OfferPrice0*BidOrderQty0)\\(BidOrderQty0+OfferOrderQty0))-prev(log((BidPrice0*OfferOrderQty0+OfferPrice0*BidOrderQty0)\\(BidOrderQty0+OfferOrderQty0)))),2))) as RV\n\t\tfrom snapshot\n\t\twhere date(TradeTime) between 2020.01.01 : 2020.12.31, SecurityID in stockList, (time(TradeTime) between 09:30:00.000 : 11:29:59.999) || (time(TradeTime) between 13:00:00.000 : 14:56:59.999)\n\t\tgroup by SecurityID, interval( TradeTime, 10m, \"none\" ) as TradeTime\n```\n\n----------------------------------------\n\nTITLE: Calculating Annual Volatility in DolphinDB\nDESCRIPTION: This function calculates the annualized volatility of a given value series. It takes a value series as input and returns the annualized volatility. It calculates the standard deviation of the percentage change in the value series and multiplies it by the square root of 252 (number of trading days in a year).\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_factor.txt#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\n/**\n * 因子2：年化波动率\n净值波动率：指净值的波动程度，某段时间内，净值的变动的标准差。\n净值年化波动率：指将净值波动率年化处理。计算方式为波动率* sqrt（N） 。 （日净值N=250，周净值N=52，月净值N=12）\n */\ndefg getAnnualVolatility(value){\n\treturn std(deltas(value)\\prev(value)) * sqrt(252)\n}\n```\n\n----------------------------------------\n\nTITLE: Calculating Annual Return Using DolphinDB\nDESCRIPTION: Defines a function getAnnualReturn that calculates the annualized return over a value series. It computes the relative return based on the first and last values of the input series, annualizes it assuming 252 trading days, and returns the compounded return. The function depends on DolphinDB's pow function and requires the input series to have valid numeric time series data. The input is a numerical vector of prices or values, and the output is a single numeric representing annual return.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_factor_by_ddb/fund_factor_ten.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndefg getAnnualReturn(value){\n      return pow(1 + ((last(value) - first(value))\\first(value)), 252\\730) - 1\n}\n```\n\n----------------------------------------\n\nTITLE: 定义DDBDataLoader配置\nDESCRIPTION: 展示如何创建和配置DDBDataLoader实例，包括设置DolphinDB会话连接、定义数据源和指定各种加载参数（如批量大小、窗口大小、分组方案等）。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ai_dataloader_ml.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\nfrom dolphindb_tools.dataloader import DDBDataLoader\n\nsess = ddb.Session()\nsess.connect('localhost', 8848, \"admin\", \"123456\")\n\ndbPath = \"dfs://test_ai_dataloader\"\ntbName = \"wide_factor_table\"\n\nsymbols = [\"`\" + f\"{i}.SH\".zfill(9) for i in range(1, 251)]\ntimes = [\"2020.01.\" + f\"{i+1}\".zfill(2) for i in range(31)]\n\nsql = f\"\"\"select * from loadTable(\"{dbPath}\", \"{tbName}\")\"\"\"\n\ndataloader = DDBDataLoader(\n    s, sql, targetCol=[\"f000001\"], batchSize=64, shuffle=True,\n    windowSize=[200, 1], windowStride=[1, 1],\n    repartitionCol=\"date(DateTime)\", repartitionScheme=times,\n    groupCol=\"Symbol\", groupScheme=symbols,\n    seed=0,\n    offset=200, excludeCol=[\"DateTime\", \"Symbol\"], device=\"cuda\",\n    prefetchBatch=5, prepartitionNum=3\n)\n```\n\n----------------------------------------\n\nTITLE: Querying DolphinDB Tables for Metrics Using DolphinDB Query Language\nDESCRIPTION: DolphinDB scripts to load tables named \"disk\" and \"system\" from the database `dfs://telegraf` and query the latest 100 records ordered by timestamp descending. These commands are used to inspect recently collected metric data for disk and system resources. Dependencies include a running DolphinDB database containing these tables. Inputs: table names and database path. Outputs: query results displaying latest metric data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_Telegraf_Grafana.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndisk = loadTable(\"dfs://telegraf\",\"disk\")\nselect top 100 * from disk order by timestamp desc\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nsystem = loadTable(\"dfs://telegraf\",\"system\")\nselect top 100 * from system order by timestamp desc\n```\n\n----------------------------------------\n\nTITLE: Full Example: Executing 'select * from t' on a Created Table in DolphinDB Plugin C++\nDESCRIPTION: Provides a comprehensive example for creating a symbol-double table, populating it with a row, adding it to the plugin heap, and executing the SQL 'select * from t' in C++ plugin code. It uses Util::createTable, appends data to vectors, handles errors in row appending, and follows with SQL parsing and evaluation as per the DolphinDB plugin interface. Dependencies: DolphinDB table/vector types, ScalarImp.h for string support. Inputs: column/value data and heap object; output: ConstantSP with the query result. Suitable as a template for embedding entire SQL-using workflows into plugins.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_development_tutorial.md#_snippet_25\n\nLANGUAGE: C++\nCODE:\n```\nvector<string> colNames {\"id\", \"x\"};\nvector<DATA_TYPE> colTypes {DT_SYMBOL, DT_DOUBLE};\nTableSP t = Util::createTable(colNames, colTypes, 0, 8);\nConstantSP idData = Util::createVector(DT_SYMBOL, 0, 0);\nConstantSP xData = Util::createVector(DT_DOUBLE, 0, 0);\nstring testId(\"APPL\");\ndouble testX(100.1);\n((Vector*)(idData.get()))->appendString(&testId, 1);\n((Vector*)(xData.get()))->appendDouble(&testX, 1);\nvector<ConstantSP> data = {idData, xData};\nINDEX rows=0;\nstring errMsg;\nt->append(data, rows, errMsg);\nheap->addItem(\"t\", t);\nstring strData = t->getString();\nstring sql = \"select * from t\";\nConstantSP sqlArg =  new String(DolphinString(sql));\nvector<ConstantSP> args{sqlArg};\nObjectSP sqlObj = heap->currentSession()->getFunctionDef(\"parseExpr\")->call(heap, args);\nvector<ConstantSP> evalArgs{sqlObj};\nConstantSP ret = heap->currentSession()->getFunctionDef(\"eval\")->call(heap, evalArgs);\n```\n\n----------------------------------------\n\nTITLE: Executing Multiprocessing Pool for Indicator Calculation - Python\nDESCRIPTION: Configures and initiates a multiprocessing pool to execute the `pool_func` across multiple CPU cores. It sets the data directory path, specifies the desired number of processes, uses the `multi_task_split` class to prepare the arguments for the worker function, creates a multiprocessing Pool, and maps the worker function to the prepared arguments using `starmap`. It measures the total execution time and concatenates the results from all processes into a single pandas DataFrame.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/DolphinDB_Python_Parser_Intro_for_Quantitative_Finance/因子实现_Python版本/当日尾盘成交占比.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nn_use = 24\n# 路径修改为存放数据路径\ntrade_path = r\"/ssd/ssd3/data/oneStock_oneFile_TL/20230201/trade\"\nstock_pool = os.listdir(trade_path)\nprocesses_decided = multi_task_split(stock_pool, n_use).num_of_jobs()\nprint(\"进程数：\", processes_decided)\nsplit_args_to_process = list(multi_task_split(stock_pool, n_use).split_args())\nargs = [(split_args_to_process[i], trade_path) for i in range(len(split_args_to_process))]\nprint(\"#\" * 50 + \"Multiprocessing Start\" + \"#\" * 50)\nt0 = time.time()\nwith multiprocessing.Pool(processes=processes_decided) as pool:\n    res = tqdm(pool.starmap(pool_func, args))\n    print(\"cal time: \", time.time() - t0, \"s\")\n    res_combined = pd.concat(res, axis=0)\n    pool.close()\n    print(\"cal time: \", time.time() - t0, \"s\")\nprint(res_combined)\n```\n\n----------------------------------------\n\nTITLE: Stopping All Cluster Nodes Script\nDESCRIPTION: Shell command sequence to stop all DolphinDB nodes on a specific server, used during node upgrade preparation.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/gray_scale_upgrade_ha.md#_snippet_7\n\nLANGUAGE: Shell\nCODE:\n```\ncd /home/zjchen/HA/server/clusterDemo/\nsh stopAllNodes.sh\n```\n\n----------------------------------------\n\nTITLE: Replaying Historical Data via submitJob in DolphinDB\nDESCRIPTION: Selects historical market data from a DolphinDB distributed database for a specified date and stock list, then submits a job to replay these records through the streaming pipeline for backtesting. The snippet demonstrates filtering, ordering, and replay configuration (batch size, speed, etc.). After job submission, historical data is cleared to reclaim memory and recent job statuses are queried.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/machine_learning_volatility/04.streamComputing.txt#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//replay history data, the day is 2020.10.19\nstockList=`601318`600519`600036`600276`601166`600030`600887`600016`601328`601288`600000`600585`601398`600031`601668`600048`601888`600837`601601`601012`603259`601688`600309`601988`601211`600009`600104`600690`601818`600703`600028`601088`600050`601628`601857`601186`600547`601989`601336`600196`603993`601138`601066`601236`601319`603160`600588`601816`601658`600745\ndata = select SecurityID, TradeTime, PreClosePx, OpenPx, HighPx, LowPx, LastPx, TotalVolumeTrade, TotalValueTrade, BidPrice0, BidPrice1, BidPrice2, BidPrice3, BidPrice4, BidPrice5, BidPrice6, BidPrice7, BidPrice8, BidPrice9, BidOrderQty0, BidOrderQty1, BidOrderQty2, BidOrderQty3, BidOrderQty4, BidOrderQty5, BidOrderQty6, BidOrderQty7, BidOrderQty8, BidOrderQty9, OfferPrice0, OfferPrice1, OfferPrice2, OfferPrice3, OfferPrice4, OfferPrice5, OfferPrice6, OfferPrice7, OfferPrice8, OfferPrice9, OfferOrderQty0, OfferOrderQty1, OfferOrderQty2, OfferOrderQty3, OfferOrderQty4, OfferOrderQty5, OfferOrderQty6, OfferOrderQty7, OfferOrderQty8, OfferOrderQty9\nfrom loadTable(dbName, tableName) where date(TradeTime)=2020.10.19, SecurityID in stockList, (time(TradeTime) between 09:30:00.000 : 11:29:59.999) || (time(TradeTime) between 13:00:00.000 : 14:56:59.999) \norder by TradeTime, SecurityID\nsubmitJob(\"replay_trade\", \"trade\",  replay{data, snapshotStream, `TradeTime, `TradeTime, 20000, true, 1})\ndata = NULL\ngetRecentJobs()\n\n```\n\n----------------------------------------\n\nTITLE: Assigning and Revoking Stream Table and Time Series Engine Permissions in DolphinDB Script\nDESCRIPTION: This set of snippets covers granting read and write permissions to a user on stream tables and a time series engine, verifying access rights, and demonstrating how permissions are lost after a server restart or deletion and recreation of those streaming objects. It includes creation of users, stream tables, time series engines, granting permissions, applying access controls after recreation, and permission enforcement during data insertion. Expected inputs are the user credentials and streaming object configurations; outputs include permission verification and error messages on unauthorized access.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Permission_Management.md#_snippet_18\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(`admin, `123456)\ncreateUser(`u1, \"111111\");\nenableTableShareAndPersistence(streamTable(1000:0, `time`sym`volume, [TIMESTAMP, SYMBOL, INT]),`trades)\nshare streamTable(1000:0, `time`sym`volume, [TIMESTAMP, SYMBOL, INT]) as  output1\nagg1 = createTimeSeriesEngine(name=\"agg1\", windowSize=600, step=600, metrics=<[sum(volume)]>, dummyTable=trades, outputTable=output1, timeColumn=`time, useSystemTime=false, keyColumn=`sym, garbageSize=50, useWindowStartTime=false)\ngrant(\"u1\", TABLE_READ, \"agg1\")\ngrant(\"u1\", TABLE_WRITE, \"agg1\")\ngrant(\"u1\", TABLE_READ, \"trades\")\ngrant(\"u1\", TABLE_WRITE, \"trades\")\ngrant(\"u1\", TABLE_READ, \"output1\")\ngrant(\"u1\", TABLE_WRITE, \"output1\")\ngetUserAccess(`u1)\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(`admin, `123456)\nenableTableShareAndPersistence(streamTable(1000:0, `time`sym`volume, [TIMESTAMP, SYMBOL, INT]),`trades)\nshare streamTable(1000:0, `time`sym`volume, [TIMESTAMP, SYMBOL, INT]) as  output1\nagg1 = createTimeSeriesEngine(name=\"agg1\", windowSize=600, step=600, metrics=<[sum(volume)]>, dummyTable=trades, outputTable=output1, timeColumn=`time, useSystemTime=false, keyColumn=`sym, garbageSize=50, useWindowStartTime=false)\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\naddAccessControl(`trades)\naddAccessControl(output1)\naddAccessControl(agg1)\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(`u1, \"111111\")\ninsert into agg1 values(2018.10.08T01:01:01.785,`A,10) \ninsert into output1 values(2018.10.08T01:01:01.785,`A,10) \ninsert into trades values(2018.10.08T01:01:01.785,`A,10) \n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(`u2, \"111111\")\ninsert into agg1 values(2018.10.08T01:01:01.785,`A,10)=> No access to table [agg1]'\ninsert into output1 values(2018.10.08T01:01:01.785,`A,10) => No access to table [output1]'\ninsert into trades values(2018.10.08T01:01:01.785,`A,10) => No access to table [trades]'\n```\n\n----------------------------------------\n\nTITLE: Full Example: Executing 'select avg(x) from t' in DolphinDB Plugin C++\nDESCRIPTION: Shows a nearly identical workflow to previous snippet but illustrates executing an aggregate SQL query. The C++ code builds a table, inserts test data, and executes 'select avg(x) from t' using the parseExpr/eval sequence. Dependencies consist of DolphinDB plugin vectors/tables and string handling support. Inputs: columns, values, heap; output: ConstantSP with average calculation result, demonstrating aggregation from plugin SQL. Limitations: ensure column types and names match the SQL command and the table is non-empty for valid aggregation.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_development_tutorial.md#_snippet_26\n\nLANGUAGE: C++\nCODE:\n```\nvector<string> colNames {\"id\", \"x\"};\nvector<DATA_TYPE> colTypes {DT_SYMBOL, DT_DOUBLE};\nTableSP t = Util::createTable(colNames, colTypes, 0, 8);\nConstantSP idData = Util::createVector(DT_SYMBOL, 0, 0);\nConstantSP xData = Util::createVector(DT_DOUBLE, 0, 0);\nstring testId(\"APPL\");\ndouble testX(100.1);\n((Vector*)(idData.get()))->appendString(&testId, 1);\n((Vector*)(xData.get()))->appendDouble(&testX, 1);\nvector<ConstantSP> data = {idData, xData};\nINDEX rows=0;\nstring errMsg;\nt->append(data, rows, errMsg);\nheap->addItem(\"t\", t);\nstring strData = t->getString();\nstring sql = \"select avg(x) from t\";\nConstantSP sqlArg =  new String(DolphinString(sql));\nvector<ConstantSP> args{sqlArg};\nObjectSP sqlObj = heap->currentSession()->getFunctionDef(\"parseExpr\")->call(heap, args);\nvector<ConstantSP> evalArgs{sqlObj};\nConstantSP ret = heap->currentSession()->getFunctionDef(\"eval\")->call(heap, evalArgs);\n```\n\n----------------------------------------\n\nTITLE: Simulating MiniSeed Historical Data with DolphinDB Script\nDESCRIPTION: Defines functions to write MiniSeed files and simulate historical waveform data for all channels over a day. Uses the mseed::write plugin and parallel processing (ploop) to operate across multiple file paths and IDs. Key parameters include file paths, signal IDs, start time, sample rate, and the sampled values vector. The main output is MiniSeed files, simulated and written to disk using parallel jobs with submitJob.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Seismic_waveform_data_storage.md#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef writeMseed(filePath,sid,startTime,sampleRate,valueList){\n\t/*\n\t * Description：\n\t * \t此函数用于将一段连续的采样值写入到 MiniSeed 文件\n\t * Input：\n\t * \tfilePath：STRING MiniSeed 文件存储路径\n\t * \tsid：STRING 文件块id \n\t * \tstartTime：TIMESTAMP 开始时间\n\t * \tsampleRate：DOUBLE 采样频率\n\t * \tvalueList：INT VECTOR 采样值向量\n\t */\n\tmseed::write(filePath, sid, startTime, sampleRate, take(valueList,8639999))\n}\n\ndef parallelWrite(){\n\t/*\n\t * Description：\n\t * \t此函数用于多线程 写 MiniSeed 文件\n\t */\n\trealDbName,dtName = \"dfs://real\",\"tagInfo\"\n\tfilePathList = exec \"../miniSeed/\"+net+\".\"+sta+\".\"+loc+\".\"+chn+\".\"+\"20230302.mseed\" from loadTable(realDbName,dtName)\n\tsidList = exec tagid from loadTable(realDbName,dtName)\n\tstartTime = 2023.03.02T00:00:00.000\n\tsampleRate = 100\n\tploop(writeMseed{,,startTime,sampleRate, rand(-3000..500,300)},filePathList,sidList)\n}\n\nsubmitJob(\"writeMseed\",\"writeMseed\",parallelWrite)\n```\n\n----------------------------------------\n\nTITLE: Executing DolphinDB User Login Command - DolphinDB\nDESCRIPTION: This DolphinDB code snippet shows how to log in to a DolphinDB server session via the GUI or script interface using admin credentials. Login is required to access persistent stream tables and execute streaming statistics queries after successful subscription initialization.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_auto_sub.md#_snippet_3\n\nLANGUAGE: dolphindb\nCODE:\n```\nlogin(\"admin\", \"123456\")\n```\n\n----------------------------------------\n\nTITLE: Create Large CSV File\nDESCRIPTION: This snippet creates a large CSV file for performance testing of `loadText` and `ploadText`. It generates random data and saves it to a file.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_data.md#_snippet_4\n\nLANGUAGE: txt\nCODE:\n```\nfilePath = \"/home/data/testFile.csv\"\nappendRows = 100000000\ndateRange = 2010.01.01..2018.12.30\nints = rand(100, appendRows)\nsymbols = take(string('A'..'Z'), appendRows)\ndates = take(dateRange, appendRows)\nfloats = rand(float(100), appendRows)\ntimes = 00:00:00.000 + rand(60 * 60 * 24 * 1000, appendRows)\nt = table(ints as int, symbols as symbol, dates as date, floats as float, times as time)\nt.saveText(filePath)\n```\n\n----------------------------------------\n\nTITLE: Performance Testing: Aggregation by Minute for a Stock\nDESCRIPTION: Calculates maximum bid and minimum offer prices for IBM on a specific date, grouped by minute of the timestamp.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs InfluxDB/test_dolphindb_big.txt#_snippet_12\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 4. 聚合查询.单分区维度\n\ntimer \nselect \n\tmax(bid) as max_bid,\n\tmin(ofr) as min_ofr\nfrom taq\nwhere \n\tdate == 2007.08.02,\n\tsymbol == 'IBM',\n\tofr > bid\n\ngroup by minute(time)\n```\n\n----------------------------------------\n\nTITLE: Calculating Amount using Field Series and Macro Variables\nDESCRIPTION: This snippet demonstrates calculating the amount using field series and macro variables to simplify and make the code more readable.  It calculates amount by directly multiplying the corresponding price and quantity columns, using a field series for the alias names.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/macro_var_based_metaprogramming.md#_snippet_24\n\nLANGUAGE: DolphinDB\nCODE:\n```\n<select _$$priceCols * _$$qtyCols as askAmount1...askAmount50 from t>.eval()\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect (price1...price50) * (qty1...qty50)  as amount1...amount50 from t\n```\n\n----------------------------------------\n\nTITLE: Initializing Multi-Table Replay in Python\nDESCRIPTION: This code shows how to perform multi-table replay, where data from multiple database tables (`order`, `trade`, and `snapshot`) are replayed into separate stream tables (`orderStream`, `tradeStream`, and `snapshotStream`).  `replayDS` is used to create replay data sources for each table. Then, the `replay` function is called with a list of input tables and a corresponding list of output tables. This approach, however, does not guarantee strict ordering of events across tables.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/stock_market_replay.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\norderDS = replayDS(sqlObj=<select * from loadTable(\"dfs://order\", \"order\") where Date = 2020.12.31>, dateColumn=`Date, timeColumn=`Time)\ntradeDS = replayDS(sqlObj=<select * from loadTable(\"dfs://trade\", \"trade\") where Date = 2020.12.31>, dateColumn=`Date, timeColumn=`Time)\nsnapshotDS = replayDS(sqlObj=<select * from loadTable(\"dfs://snapshot\", \"snapshot\") where Date =2020.12.31>, dateColumn=`Date, timeColumn=`Time)\nreplay(inputTables=[orderDS, tradeDS, snapshotDS], outputTables=[orderStream, tradeStream, snapshotStream], dateColumn=`Date, timeColumn=`Time, replayRate=10000, absoluteRate=true)\n```\n\n----------------------------------------\n\nTITLE: Querying Data for Grafana Visualization using Pivot By in DolphinDB Script\nDESCRIPTION: This script demonstrates how to query and format data for visualization in Grafana, specifically for monitoring the hourly production amount for multiple devices. It first calculates the sum of 'propertyValue' grouped by hour and device code, then uses DolphinDB's 'pivot by' clause to transform the data into a wide format where each device's production amount becomes a separate column, suitable for time-series plotting in Grafana.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Iot_intelligent_O&M.md#_snippet_14\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nt = select sum(propertyValue) as production_amount_hour from pt where date(ts)=2022.01.01 group by bar(ts,1H) as ts,deviceCode\nselect production_amount_hour from t pivot by ts,deviceCode\n```\n\n----------------------------------------\n\nTITLE: Calculating Residual with Function Based Metaprogramming\nDESCRIPTION: This code block performs function-based metaprogramming to calculate the residual value of the ordinary least squares (OLS) regression. It demonstrates a complex scenario with nested function calls and the usage of functions such as `makeUnifiedCall`, `matrix`, `sqlCol` and `sqlColAlias` within the `sql` function.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/macro_var_based_metaprogramming.md#_snippet_20\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 模拟数据脚本\nx1=1 3 5 7 11 16 23\nx2=2 8 11 34 56 54 100\nx3=8 12 81 223 501 699 521\ny=0.1 4.2 5.6 8.8 22.1 35.6 77.2;\nt = table(y, x1, x2, x3)\n\n// 批计算 SQL 脚本示意\nselect ols(y, (x1, x2, x3), 1, 2).Residual as residual from t\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\ny = \"y\"\nx = `x1`x2`x3\nresidual = makeCall(member, makeCall(ols, sqlCol(y), makeUnifiedCall(matrix, sqlCol(x)), 1, 2), \"Residual\")\n// output：< member(ols(y, matrix(x1, x2, x3), 1, 2), \"Residual\") >\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nsql(select=sqlColAlias(residual, \"residual\"), from=t).eval()\n```\n\n----------------------------------------\n\nTITLE: Loading MiniSeed Plugin in DolphinDB DolphinDB Script\nDESCRIPTION: Loads the MiniSeed plugin by specifying the plugin file path. The plugin provides essential interfaces such as read, write, parse, and parseStream for processing MiniSeed files. This requires access to the DolphinDBPlugin repository to download the correct plugin version and then load it using loadPlugin. It prepares the DolphinDB environment for MiniSeed data manipulation.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Seismic_waveform_data_storage.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\nloadPlugin(\"./plugins/mseed/PluginMseed.txt\");\n```\n\n----------------------------------------\n\nTITLE: Generating One Day Data - Dolphindb\nDESCRIPTION: Defines a function to generate one day's worth of simulated time-series data for a given set of IDs at a specified frequency. It creates a table with 'id', 'datetime', and 'value' columns, distributing the IDs and generating sequential datetimes and random values.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/singleValueModeWrite.txt#_snippet_1\n\nLANGUAGE: Dolphindb\nCODE:\n```\ndef generate1DayData(day, id, freqPerDay){\n\tstartTime = day.datetime()\n\tidSize = size(id)\n\tnumRecords = freqPerDay * idSize\n\tidVec = array(INT, numRecords)\n\tfor(i in 0:idSize) idVec[(i*freqPerDay) : ((i+1)*freqPerDay)] = id[i]\n\treturn table(idVec, take(startTime+0..(freqPerDay-1),numRecords) as datetime, rand(1.0, numRecords) as value)\n}\n```\n\n----------------------------------------\n\nTITLE: Arithmetic Operations Between DECIMAL Types and Type Conversion Rules - DolphinDB\nDESCRIPTION: Defines arithmetic operation rules between DECIMAL32 and DECIMAL64 types with varying scales in DolphinDB. Shows that operations between DECIMAL32 and DECIMAL32 produce DECIMAL32 results, similarly DECIMAL64 with DECIMAL64 yields DECIMAL64 results, while any cross-type decimal operation results in DECIMAL64. Scale of the result depends on operation type: addition/subtraction uses max scale, multiplication sums scales, division uses left operand scale. Demonstrates implicit conversion of integer types to DECIMAL with scale zero for mixed-type arithmetic. Floating-point operations with DECIMAL are undefined but do not raise errors.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DECIMAL.md#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\nm=decimal32(1.23, 3)+decimal32(2.45, 2)\n//output:3.680\n//typestr:DECIMAL32\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nm=decimal64(1.23, 3)+decimal64(2.45, 2)\n//output:3.680\n//typestr:DECIMAL64\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nm=decimal64(1.23, 3)+decimal32(2.45, 2)\n//output:3.680\n//typestr:DECIMAL64\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nm=decimal32(1.23, 3)+decimal64(2.45, 2)\n//output:3.680\n//typestr:DECIMAL64\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nm=decimal64(1.23, 3)*decimal32(2.45, 2)\n//output:3.01350\n//typestr:DECIMAL64\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nm=decimal64(1.23, 3)/decimal32(2.45, 2)\n//output:0.502\n//typestr:DECIMAL64\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndecimal32(10, 2)*6\n```\n\n----------------------------------------\n\nTITLE: Optimized Trade Type Grouping with Nested Condition Function in DolphinDB Script\nDESCRIPTION: Defines a getType function using nested iif to map transaction amount into categories (small, medium, large, extra-large), then groups trades by date, symbol, side, and derived type for volume and amount summaries. Relies on getType returning INT for optimal group by performance. This approach simplifies code and improves efficiency over separate queries.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_SQL_Case_Tutorial.md#_snippet_35\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef getType(amount) {\n\treturn iif(amount < 40000, 0, iif(amount >= 40000 && amount < 200000, 1, iif(amount >= 200000 && amount < 1000000, 2, 3)))\n}\n\ntimer res2 = select sum(volume) as volume_sum, sum(volume*price) as amount_sum \n\t\t\t\tfrom t \n\t\t\t\twhere time <= 10:00:00\n\t\t\t\tgroup by date, symbol, side, getType(volume * price) as type \n```\n\n----------------------------------------\n\nTITLE: Calculating Annual Kurtosis in DolphinDB\nDESCRIPTION: Defines a DolphinDB function `getAnnualKur` to calculate the kurtosis of daily returns. It takes a vector `value` (daily net values), computes daily returns, and uses the built-in `kurtosis` function to measure the tailedness or peakedness of the return distribution.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/fund_factor_contrasted_by_py.md#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndefg getAnnualKur(value){\n\treturn kurtosis(deltas(value)\\prev(value)) \n}\n```\n\n----------------------------------------\n\nTITLE: ZeroMQ Consumer for Receiving DolphinDB Stream Data in Python\nDESCRIPTION: This Python code snippet sets up a ZeroMQ subscriber to receive stream data pushed from a DolphinDB server.  It binds to a specified TCP address and port, then asynchronously loops, receives messages, and prints them.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practice_for_factor_calculation.md#_snippet_19\n\nLANGUAGE: Python\nCODE:\n```\nzmq_context = Context()\nzmq_bingding_socket = zmq_context.socket(SUB)#见完整版代码设定socket选项\nzmq_bingding_socket.bind(\"tcp://*:55556\")\t\t\nasync def loop_runner():    \n    while True:\n        msg=await zmq_bingding_socket.recv()#阻塞循环until收到流数据\n        print(msg)#在此编写下游消息处理代码\t\nasyncio.run(loop_runner())\n```\n\n----------------------------------------\n\nTITLE: SVD Decomposition with computeUV=false in DolphinDB\nDESCRIPTION: Demonstrates Singular Value Decomposition (SVD) of a matrix in DolphinDB using the `svd` function with `computeUV=false`.  The function only returns the singular values s.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/matrix.md#_snippet_24\n\nLANGUAGE: DolphinDB\nCODE:\n```\n>s = svd(m, computeUV=false);\n>s;\n[19.202978,3.644527,1.721349]\n\n```\n\n----------------------------------------\n\nTITLE: Creating DolphinDB Database and Table using Function\nDESCRIPTION: Assigns the database path \"dfs://stocks_orderbook\" to the `dbName` variable and the table name \"orderBook\" to the `tableName` variable. It then calls the previously defined `createDB` function with these parameters to create the partitioned database and table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/csvImportDemo/createDB.txt#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndbName = \"dfs://stocks_orderbook\"\ntableName = \"orderBook\"\ncreateDB(dbName,tableName)\n```\n\n----------------------------------------\n\nTITLE: Creating Partitioned Keyed In-Memory Table in DolphinDB Script\nDESCRIPTION: This snippet showcases creation of a partitioned keyed in-memory table on the 'id' column. It constructs an empty keyed table, initializes a database with range partitions, and partitions the keyed table. It requires DolphinDB's keyedTable and createPartitionedTable functions. The result is a keyed, partitioned table optimized for lookups by key.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/in_memory_table.md#_snippet_1\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nkt=keyedTable(1:0,`id`val,[INT,INT])\ndb=database(\"\",RANGE,0 101 201 301)\npkt=db.createPartitionedTable(kt,`pkt,`id)\n```\n\n----------------------------------------\n\nTITLE: Creating Data Source and Invoking Plugin Function in DolphinDB C++\nDESCRIPTION: This snippet shows creating a partitioned database and table within DolphinDB and populating it with synthetic data. It then generates a data source using `sqlDS` based on a selection from the table and calls a plugin function `columnAvg` to compute the average of selected columns. Key parameters include the database path, table schema definition, and columns passed to the plugin. It assumes the plugin `columnAvg` is loaded and accessible.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_development_tutorial.md#_snippet_13\n\nLANGUAGE: cpp\nCODE:\n```\nn = 100\ndb = database(\"dfs://testColumnAvg\", VALUE, 1..4)\nt = db.createPartitionedTable(table(10:0, `id`v1`v2, [INT,DOUBLE,DOUBLE]), `t, `id)\nt.append!(table(take(1..4, n) as id, rand(10.0, n) as v1, rand(100.0, n) as v2))\n\nds = sqlDS(<select * from t>)\ncolumnAvg::columnAvg(ds, `v1`v2)\n```\n\n----------------------------------------\n\nTITLE: 2. 使用 Lambda 表达式定义匿名函数\nDESCRIPTION: 展示如何在 DolphinDB 中定义匿名函数（lambda），并作为高阶函数参数传递，实现对数组元素的处理。例如，使用 each() 方法和 lambda 表达式，将数组中的每个元素加 1，方便自定义数据处理逻辑。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\nx=1..10\neach(x -> x + 1, [1, 2, 3])\n```\n\n----------------------------------------\n\nTITLE: Creating Time Series Engine for Feature Aggregation in DolphinDB\nDESCRIPTION: Creates a time series aggregation engine with a 10-minute window and 1-minute step to process streaming snapshot data. The engine computes features needed for volatility prediction.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/machine_learning_volatility.md#_snippet_4\n\nLANGUAGE: dolphindb\nCODE:\n```\ncreateTimeSeriesEngine(name=\"aggrFeatures10min\", windowSize=600000, step=60000, metrics=metrics, dummyTable=snapshotStream, outputTable=aggrFeatures10min, timeColumn=`TradeTime, useWindowStartTime=true, keyColumn=`SecurityID)\n```\n\n----------------------------------------\n\nTITLE: Generating and Appending Data to Stream Table\nDESCRIPTION: This snippet generates random data and appends it to the `tickStream` stream table. It uses the built-in `rand` and `take` functions to create data. Also verifies the results against a batch calculation.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/reactive_state_engine.md#_snippet_3\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\ndata = table(take(\"000001.SH\", 100) as sym, rand(10.0, 100) as price)\ntickStream.append!(data)\nfactor1Hist = select sym, ema(1000 * sum_diff(ema(price, 20), ema(price, 40)),10) -  ema(1000 * sum_diff(ema(price, 20), ema(price, 40)), 20) as factor1 from data context by sym\nassert each(eqObj, result.values(), factor1Hist.values())\n```\n\n----------------------------------------\n\nTITLE: Reading Integer Vector Data with getIntConst C++\nDESCRIPTION: Demonstrates efficient batch reading of integer data from a DolphinDB Vector using the `getIntConst` method. This method avoids data copying when the memory segment is continuous, improving read performance compared to single-element access.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_advance.md#_snippet_11\n\nLANGUAGE: C++\nCODE:\n```\nVectorSP pVec = Util::createVector(DT_INT, 100000000);\nint tmp;\nconst int BUF_SIZE = 1024;\nint buf[BUF_SIZE];\nint start = 0;\nint N = pVec->size();\nwhile (start < N) {\n    int len = std::min(N - start, BUF_SIZE);\n    const int* p = pVec->getIntConst(start, len, buf);\n    for (int i = 0; i < len; ++i) {\n        tmp = p[i];\n    }\n    start += len;\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Output Tables\nDESCRIPTION: These lines demonstrate how to query the `outputSt1` and `outputSt2` tables, filtering data based on the `tag` column. These queries allow users to examine the processed data generated by the reactive and session window engines.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/data/DolphinDB_streaming_application_in_IOT/deviceState.txt#_snippet_13\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect * from outputSt1 where tag=`motor.C17156B.m1\nselect * from outputSt2 where tag=`motor.C17156B.m1\n```\n\n----------------------------------------\n\nTITLE: Mapping pandas Selection and Indexing Functions to DolphinDB\nDESCRIPTION: This snippet details functions used for selecting, locating, and indexing data in pandas, with their DolphinDB equivalents. It covers label-based and position-based indexing mechanisms, as well as alignment and uniqueness operations essential for data retrieval and preparation.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/function_mapping_py.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ndf[column]  # pandas column selection\npandas.Series.loc / pandas.DataFrame.loc  # label-based data selection\npandas.Series.iat / pandas.DataFrame.iat  # position-based data access\npandas.Series.iloc / pandas.DataFrame.iloc  # positional indexing\npandas.Series.align / pandas.DataFrame.align  # data alignment between Series/DataFrame\npandas.unique / pandas.DataFrame.unique / pandas.Series.unique  # retrieve unique values\n```\n\n----------------------------------------\n\nTITLE: Cancelling a Replay Job using DolphinDB Script\nDESCRIPTION: Shows how to identify and cancel a currently running replay job, either by obtaining a jobId from getRecentJobs or getConsoleJobs, and issuing the corresponding cancelJob or cancelConsoleJob command. Useful for managing long-running or errant replay tasks in both batch and interactive GUI sessions.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/historical_data_replay.md#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\ngetRecentJobs()\ncancelJob(jobid)\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\ngetConsoleJobs()\ncancelConsoleJob(jobId)\n```\n\n----------------------------------------\n\nTITLE: Initializing Stream Data Tables in DolphinDB Script\nDESCRIPTION: This snippet declares parameters for stream table capacity and table names, then creates two shared streaming tables for market snapshot data reception and processed snapshot results. The tables are initially allocated with specified memory capacity, which can dynamically expand based on runtime data volume. Tables facilitate real-time incremental data updates for further processing by stream engines.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/K.md#_snippet_11\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//Declare parameters\ntableCapacity = 1000000\nmdlSnapshotTBName = \"mdlSnapshot\"\nmdlSnapshotProcessTBName = \"mdlSnapshotProcess\"\n//Create MDL snapshot table\nshare(getMDLSnapshotTB(tableCapacity), mdlSnapshotTBName)\n//Create MDL processed snapshot table\nshare(getMDLSnapshotProcessTB(tableCapacity), mdlSnapshotProcessTBName)\n```\n\n----------------------------------------\n\nTITLE: Registering columnAvg Plugin Function in DolphinDB\nDESCRIPTION: This snippet registers the 'columnAvg' plugin function from the 'libPluginColumnAvg.so' shared library with the DolphinDB system. The function is assigned system-level access with 2 minimum and 2 maximum parameters, and 0 as the additional parameter (likely indicating it returns a value).\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin/ColumnAvg/PluginColumnAvg.txt#_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ncolumnAvg,libPluginColumnAvg.so\ncolumnAvg,columnAvg,system,2,2,0\n```\n\n----------------------------------------\n\nTITLE: Creating Dimension Table in DolphinDB\nDESCRIPTION: This code snippet demonstrates how to create a dimension table named `tagInfo` and populate it with sample data. Dimension tables are used to store information that doesn't change frequently and are typically used for joins with distributed tables. The code defines the schema, creates the table, and then inserts sample data into the table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/iot_examples.md#_snippet_8\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndb = database(\"dfs://svmDemo\")\nschema = table(1:0, `tagId`machineId`tagName`organization`description, [INT,INT,SYMBOL,SYMBOL,SYMBOL])\ndt = db.createTable(schema,`tagInfo)\n\ntagInfo=loadTable(\"dfs://svmDemo\",\"tagInfo\")\nt = table(1 2 51 52 as tagId,1 1 2 2 as machineId,`speed`temperature`speed`temperature as tagName,`hangzhou`hangzhou`shanghai`shanghai as organization,\n  \"Speed/No.1 Wind Turbine in Hangzhou\" \"Temperature/No.1 Wind Turbine in Hangzhou\" \"Speed/No.2 Wind Turbine in Shanghai\" \"Temperature/No.2 Wind Turbine in Shanghai\" as description )\ntagInfo.append!(t)\n```\n\n----------------------------------------\n\nTITLE: String replacement (column-wise)\nDESCRIPTION: This DolphinDB script performs a string replacement operation on the 'str' column in a table using column operations. It finds the position of '_', extracts the substrings before and after the separator, and then concatenates them in reverse order. This is more performant than row-wise processing.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_19\n\nLANGUAGE: shell\nCODE:\n```\npos = strpos(t[`str], \"_\")\nsubstr(t[`str], pos+1)+\"_\"+t[`str].left(pos)\n```\n\n----------------------------------------\n\nTITLE: Creating Dimension Table in DolphinDB - Java\nDESCRIPTION: This code snippet establishes a connection to a DolphinDB node and creates a dimension table for receiving data. It specifies the data type of a column as an Array Vector (INT[]).\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Array_Vector.md#_snippet_22\n\nLANGUAGE: java\nCODE:\n```\n// 连接 DolphinDB 的节点\nDBConnection conn = new DBConnection();\nboolean success = conn.connect(\"127.0.0.1\", 8848, \"admin\", \"123456\");\n// 创建维度表\nString ddb_script = \"dbName = \\\"dfs://testDB\\\"\\n\" +\"../py_api.dita\"est\\\"\\n\" +\n                \"if(existsDatabase(dbName)) {\\n\" +\n                \"    dropDatabase(dbName)\\n\" +\n                \"}\\n\" +\n                \"db = database(dbName, VALUE, 1 2 3 4, , 'TSDB')\\n\" +\n                \"schemaTB = table(1:0, `id`value, [INT, INT[]])\\n\" +\n                \"pt = createTable(db, schemaTB, tbName, sortColumns=`id)\";\nconn.run(ddb_script);\n```\n\n----------------------------------------\n\nTITLE: Elasticsearch Filtering by Date - Python\nDESCRIPTION: This Python function queries an Elasticsearch index named 'elastic' to retrieve data filtered by date. It uses the `elasticsearch` library to connect to Elasticsearch and execute a search query with a range filter on the 'trade_date' field. The function utilizes scrolling to handle large result sets, printing the scroll ID and the total number of hits.  It then iterates through the scroll results, printing the scroll size for each batch.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_26\n\nLANGUAGE: Python\nCODE:\n```\ndef search_1():\n    es = Elasticsearch(['http://localhost:9200/'])\n    page = es.search(\n        index='elastic',\n        doc_type='type',\n        scroll='2m',\n        size=10000,\n        body={\n            \"query\": {\n                \"constant_score\": {\n                    \"filter\": {\n                        \"range\": {\n                            \"trade_date\": {\n                                \"lte\": \"2014.01.11\"\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    )\n    sid = page['_scroll_id']\n    scroll_size = page['hits']['total']\n\n    print(sid)\n    print(scroll_size)\n    # Start scrolling\n    while (scroll_size > 0):\n        print(\"Scrolling...\")\n        page = es.scroll(scroll_id=sid, scroll='2m')\n        # Update the scroll ID\n        sid = page['_scroll_id']\n        # Get the number of results that we returned in the last scroll\n        scroll_size = len(page['hits']['hits'])\n        print(\"scroll size: \" + str(scroll_size))\n```\n\n----------------------------------------\n\nTITLE: Defining Parallel Job 2 for Factor Calculation in DolphinDB\nDESCRIPTION: This function `parJob2` defines another parallel job, similar to `parJob1`, but it also calculates the 'hist' factor. It loads data, performs joins and data preprocessing, calculates factors with getFactor. Furthermore, it calculates 'hist' factor via `calAllRs2` with `ploop` and combines results into table res2.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_factor.txt#_snippet_17\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef parJob2(){\n\ttimer{fund_OLAP=select * from loadTable(\"dfs://fund_OLAP\", \"fund_OLAP\")\n\t\t    fund_hs_OLAP=select * from loadTable(\"dfs://fund_OLAP\", \"fund_hs_OLAP\")\n\t\t    ajResult = select Tradedate, fundNum, value, fund_hs_OLAP.Tradedate as hsTradedate, fund_hs_OLAP.value as price from aj(fund_OLAP, fund_hs_OLAP, `Tradedate)\n\t\t    result2 = select Tradedate, fundNum, iif(isNull(value), ffill!(value), value) as value,price from ajResult where Tradedate == hsTradedate\n        symList = exec distinct(fundNum) as fundNum from result2 order by fundNum\n        symList2 = symList.cut(250)//此处，将任务切分，按每次250个不同基金数据进行计算\n\t\t    portfolio = select fundNum as fundNum, (deltas(value)\\prev(value)) as log, TradeDate as TradeDate from result2 where TradeDate in 2019.05.24..2022.05.27 and fundNum in symList\n        m_log = exec log from portfolio pivot by TradeDate, fundNum\n        mlog =  m_log[1:,]\n        knum = 2..365\n}\n  timer{ploop(getFactor{result2}, symList2)\n        a = ploop(calAllRs2{mlog,symList}, knum).unionAll(false)\n        res2 = select fundNum, ols(factor1, kNum)[0] as hist, ols(factor1, kNum)[1] as hist2, ols(factor1, kNum)[2] as hist3 from a group by fundNum}\n}//定义获取十个因子计算和数据操作的时间的函数\n```\n\n----------------------------------------\n\nTITLE: Calculating Alpha Performance Metric in DolphinDB\nDESCRIPTION: Defines 'getAlpha' to calculate alpha as the excess annualized return of an asset over a risk-free rate and adjusted by asset beta times market premium. Uses risk-free rate as 3%, depends on annual return and beta functions, and outputs a scalar alpha value.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_factor_by_ddb/fund_factor_nine.txt#_snippet_8\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef getAlpha(value, price){\n\treturn getAnnualReturn(value) - 0.03 - getBeta(value, price) * (getAnnualReturn(price) - 0.03)\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Controller Startup Script for DolphinDB Cluster (Shell)\nDESCRIPTION: This shell script functions as a service management wrapper for the DolphinDB cluster controller node. It offers start, stop, and restart actions by manipulating the controller process via nohup for background execution and killing running processes with proper filtering. The script requires DolphinDB binaries, access to config files (controller.cfg, cluster.nodes, cluster.cfg), and expects to be run in the cluster workspace. Inputs: start|stop|restart as argument. Outputs: Starts or stops the controller process accordingly. Limitations: Environment variables such as LD_LIBRARY_PATH are set locally and scripts need execute permissions.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ha_cluster_deployment.md#_snippet_14\n\nLANGUAGE: Shell\nCODE:\n```\n#!/bin/bash\n#controller.sh\nworkDir=$PWD\n\nstart(){\n    cd ${workDir} && export LD_LIBRARY_PATH=$(dirname \"$workDir\"):$LD_LIBRARY_PATH\n    nohup ./../dolphindb -console 0 -mode controller -home data -script dolphindb.dos -config config/controller.cfg -logFile log/controller.log -nodesFile config/cluster.nodes -clusterConfig config/cluster.cfg > controller.nohup 2>&1 &\n}\n\nstop(){\n    ps -o ruser=userForLongName -e -o pid,ppid,c,time,cmd |grep dolphindb|grep -v grep|grep $USER|grep controller| awk '{print $2}'| xargs kill -TERM\n}\n\ncase $1 in\n    start)\n        start\n        ;;\n    stop)\n        stop\n        ;;\n    restart)\n        stop\n        start\n        ;;\nesac\n```\n\n----------------------------------------\n\nTITLE: Defining a User-Defined Function with Lambda Filtering in DolphinDB - DolphinDB\nDESCRIPTION: Defines a pure user-defined function 'getWeekDays' which filters a vector of dates, returning only those that fall between Monday and Friday inclusive. Utilizes DolphinDB’s vectorized filtering with a lambda function embedded inline that applies the weekday predicate. The function strictly only accesses passed parameters and local variables per pure function constraints. Demonstrates functional programming and vectorized filtering.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/hybrid_programming_paradigms.md#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef getWeekDays(dates){\n    return dates[def(x):weekday(x) between 1:5]\n}\n\ngetWeekDays(2018.07.01 2018.08.01 2018.09.01 2018.10.01)\n```\n\n----------------------------------------\n\nTITLE: Defining Shared Stream Tables in DolphinDB Script\nDESCRIPTION: Defines three shared stream tables: `snapshotStream` for raw L2 market data snapshots (including array vector types for bid/offer prices and quantities), `aggrFeatures10min` to store aggregated features calculated over a time window, and `result1min` to store the final prediction results.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/machine_learning_volatility/05.streamComputingArrayVector.txt#_snippet_2\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\n//define stream table\nname = `SecurityID`TradeTime`PreClosePx`OpenPx`HighPx`LowPx`LastPx`TotalVolumeTrade`TotalValueTrade`BidPrice`BidOrderQty`OfferPrice`OfferOrderQty\ntype =`SYMBOL`TIMESTAMP`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`INT`DOUBLE \"DOUBLE[]\" \"INT[]\" \"DOUBLE[]\" \"INT[]\"\nshare streamTable(100000:0, name, type) as snapshotStream\nshare streamTable(100000:0 , `TradeTime`SecurityID`BAS`DI0`DI1`DI2`DI3`DI4`DI5`DI6`DI7`DI8`DI9`Press`RV,`TIMESTAMP`SYMBOL`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE) as aggrFeatures10min\nshare streamTable(100000:0 , `TradeTime`SecurityID`PredictRV`CostTime, `TIMESTAMP`SYMBOL`DOUBLE`INT) as result1min\ngo\n```\n\n----------------------------------------\n\nTITLE: Logging into DolphinDB Server\nDESCRIPTION: Authenticates a user session with the DolphinDB server using the provided username (\"admin\") and password (\"123456\"). This is typically the first step before executing other database commands.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/csvImportDemo/createDB.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(\"admin\",\"123456\")\n```\n\n----------------------------------------\n\nTITLE: Determining Effective Permissions Based on User and Group Assignments - DolphinDB\nDESCRIPTION: A series of scenario-driven code blocks demonstrates how effective permissions are determined in DolphinDB when users belong to various groups with different allow/deny states. Each snippet shows combinations of 'grant', 'deny', and 'revoke' on users and groups, followed by attempted privileged operations (such as database creation). Inputs require admin sessions, user/group identifiers, and permission types. Outputs are success or specific error messages enforcing permission logic.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Permission_Management.md#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(\"admin\",\"123456\")\ncreateUser(\"user2\",\"123456\")\ncreateGroup(\"group1\",[\"user2\"])\ndeny(`user2, DB_OWNER)//禁止 user2 拥有 DB_OWNER 权限\ngrant(`group1, DB_OWNER)//赋予 group1DB_OWNER 权限\nlogin(\"user2\",\"123456\")\ndatabase(\"dfs://test\",VALUE,1..10)//user2 没有 DB_OWNER 权限\n=> <NoPrivilege>Not granted to create or delete databases\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(\"admin\",\"123456\")\nrevoke(`user2, DB_OWNER)//撤销禁止 user2 的 DB_OWNER 权限\ngrant(`group1, DB_OWNER)//赋予 group1DB_OWNER 权限\nlogin(\"user2\",\"123456\")\ndatabase(\"dfs://test\",VALUE,1..10)//user2 拥有 DB_OWNER 权限\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(\"admin\",\"123456\")\nrevoke(`user2, DB_OWNER)//撤销禁止 user2 的 DB_OWNER 权限\ndeny(`group1, DB_OWNER)//赋予 group1DB_OWNER 权限\nlogin(\"user2\",\"123456\")\ndatabase(\"dfs://test\",VALUE,1..10)//user2 没有 DB_OWNER 权限\n=> <NoPrivilege>Not granted to create or delete databases\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(\"admin\",\"123456\")\ncreateGroup(\"group2\",[\"user2\"])\ncreateGroup(\"group3\",[\"user2\"])\ndeny(`group1, DB_OWNER)\ngrant(`group2, DB_OWNER)\ngrant(`group3, DB_OWNER)\nlogin(\"user2\",\"123456\")\ndatabase(\"dfs://test\",VALUE,1..10)//user2 没有 DB_OWNER 权限\n=> <NoPrivilege>Not granted to create or delete databases\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(\"admin\",\"123456\")\nrevoke(`group1, DB_OWNER)\ngrant(`group2, DB_OWNER)\ngrant(`group3, DB_OWNER)\nlogin(\"user2\",\"123456\")\ndatabase(\"dfs://test\",VALUE,1..10)//user2 拥有 DB_OWNER 权限\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(\"admin\",\"123456\")\nrevoke(`group1, DB_OWNER)\ndeny(`group2, DB_OWNER)\ndeny(`group3, DB_OWNER)\nlogin(\"user2\",\"123456\")\ndatabase(\"dfs://test\",VALUE,1..10)//user2 没有 DB_OWNER 权限\n=> <NoPrivilege>Not granted to create or delete databases\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(\"admin\",\"123456\")\ncreateUser(\"user2\",\"123456\")\ncreateGroup(\"group1\",[\"user2\"])\ngrant(`user2, DB_OWNER)//赋予 user2 DB_OWNER 权限\ndeny(`group1, DB_OWNER)//禁止 group1 DB_OWNER 权限\ndeleteGroup(`group1)//删除 group1\nlogin(\"user2\",\"123456\")\ndatabase(\"dfs://test\",VALUE,1..10)//user2 拥有 DB_OWNER 权限\n----------------------------------------------------------------\nlogin(\"admin\",\"123456\")\ncreateGroup(\"group1\",[\"user2\"])\nrevoke(`user2, DB_OWNER)//回收 user2 DB_OWNER 权限\ngrant(`group1, DB_OWNER)//赋予 group1 DB_OWNER 权限\ndeleteGroup(`group1)//删除 group1\nlogin(\"user2\",\"123456\")\ndatabase(\"dfs://test\",VALUE,1..10)//user2 没有 DB_OWNER权限\n```\n\n----------------------------------------\n\nTITLE: Calculating Annualized Volatility in DolphinDB\nDESCRIPTION: Defines a DolphinDB function `getAnnualVolatility` to compute the annualized volatility (standard deviation) of daily returns. It takes a vector `value` (daily net values), calculates daily returns using `deltas(value)\\prev(value)`, finds their standard deviation (`std`), and annualizes by multiplying with the square root of 252.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/fund_factor_contrasted_by_py.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndefg getAnnualVolatility(value){\n\treturn std(deltas(value)\\prev(value)) * sqrt(252)\n}\n```\n\n----------------------------------------\n\nTITLE: Using Keyed Table to Cache Latest Quote Data in DolphinDB\nDESCRIPTION: Creates a keyed table with symbol as the key to store only the latest level2 data for each stock. This optimizes query performance for scenarios where only the most recent quote for each symbol is needed.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/quant_finance_examples.md#_snippet_26\n\nLANGUAGE: DolphinDB\nCODE:\n```\nnewestLevel2 = keyedTable(`symbol, 100:0, `symbol`datetime`last`askPrice1`bidPrice1`askVolume1`bidVolume1`volume, [SYMBOL,DATETIME,DOUBLE,DOUBLE,DOUBLE,INT,INT,INT])\nsubscribeTable(tableName=\"level2\", actionName=\"newestLevel2data\", offset=0, handler=append!{newestLevel2}, msgAsTable=true)\n```\n\n----------------------------------------\n\nTITLE: Loading Device Info Table DolphinDB\nDESCRIPTION: This snippet loads data from a CSV file (`FP_INFO`) into an in-memory table named `device_info`.  It uses the pre-defined schema `schema_info` for proper data parsing.  The `loadText` function is used for this purpose, and relies on the file path and schema definitions.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs TimescaleDB/test_dolphindb_small.txt#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// ----------------- 从 CSV 导入 device_info 表的数据到 device_info 内存表\ndevice_info = loadText(FP_INFO, , schema_info)\n```\n\n----------------------------------------\n\nTITLE: Connecting DolphinDB to SQL Server via ODBC - DolphinDB Script\nDESCRIPTION: Script establishing a connection from DolphinDB to SQL Server using the ODBC interface. The connection string includes driver name, server, user credentials, and database name parameters. Returns a connection handle used in subsequent queries.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/SQLServer_to_DolphinDB.md#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\nconn =odbc::connect(\"Driver={SQLServer};Servername=sqlserver;Uid=sa;Pwd=DolphinDB;database=historyData;;\");\n```\n\n----------------------------------------\n\nTITLE: DolphinDB Stock Code Filtering\nDESCRIPTION: This DolphinDB script filters data from the 'trades' table based on a range of 'PERMNO' values, effectively filtering based on stock codes.  It uses the `timer()` function for performance measurement, running the query 10 times.  The script selects all columns from the 'trades' table where the 'PERMNO' column falls between 23230 and 30000, inclusive. This tests filtering by numeric stock identifiers.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_38\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\n//根据股票代码过滤\ntimer(10) select * from trades where PERMNO between 23230:30000\n```\n\n----------------------------------------\n\nTITLE: Calculating Annual Skewness in DolphinDB\nDESCRIPTION: This function calculates the annualized skewness of a given value series. It takes a value series as input and returns the skewness. It calculates the skewness of the percentage change in the value series.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_factor.txt#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\n/**\n * 因子3：偏度\n  偏度：指将收益率偏度年化处理。计算方式为偏度* sqrt（N） 。 （日净值N=250，周净值N=52，月净值N=12）\n */\ndefg getAnnualSkew(value){\n\treturn skew(deltas(value)\\prev(value))\n}\n```\n\n----------------------------------------\n\nTITLE: Stopping and Clearing Streaming Subscriptions and Persistence - DolphinDB Script\nDESCRIPTION: This code snippet provides a utility function on the DolphinDB server for terminating stream subscriptions, clearing stream persistence, and deallocating shared table resources. It first unsubscribes the specified action from the table, then clears the persistent queue and removes the shared object. Prerequisite: streaming tables and subscriptions must have been established using DolphinDB functions. Key parameters are the table name and subscription action name.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_tutorial.md#_snippet_26\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef clears(tbName,action)\n{\n\tunsubscribeTable(tableName=tbName, actionName=action)\n\tclearTablePersistence(objByName(tbName))\n\tundef(tbName,SHARED)\n}\nclears(`ticks_stream, `action_to_dfsTB)\nclears(`mem_stream_d,`action_to_ticksStream_tde)\nclears(`mem_stream_f,`action_to_ticksStream_tfe)\n```\n\n----------------------------------------\n\nTITLE: Weighted Average Price per Day and Stock with Sorting\nDESCRIPTION: Computes the volume-weighted average bid price for each stock and day within a date range, filtering for active days, ordered by date and symbol.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs InfluxDB/test_dolphindb_big.txt#_snippet_19\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 11. 经典查询：计算 某个日期段 有成交记录的 (每天, 每股票) 加权均价，并按 (日期，股票代码) 排序\n\ntimer\nselect wavg(bid, bidsiz) as vwab \nfrom taq\nwhere date between 2007.08.05 : 2007.08.06\ngroup by date, symbol\n\thaving sum(bidsiz) > 0\norder by date desc, symbol\n```\n\n----------------------------------------\n\nTITLE: Summarizing Query Counts by User for Cost Allocation\nDESCRIPTION: SQL query to aggregate the total number of queries made by each user, providing a simple metric for allocating usage costs among users.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/user_level_resource_tracking.md#_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\nselect sum(count) as count from result group by userId\n```\n\n----------------------------------------\n\nTITLE: Defining Logarithmic Ratio Function in DolphinDB\nDESCRIPTION: Defines a DolphinDB function openBidVolDvdAskVol that calculates the natural logarithm of the ratio between the average order quantities for buy-side orders versus sell-side orders. The function uses conditional expressions to filter OrderQty values based on the Side parameter, which can be '1' or 'B' representing buy orders, and '2' or 'S' representing sell orders. It returns a floating-point log value representing market dynamics between buying and selling volumes.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/DolphinDB_Python_Parser_Intro_for_Quantitative_Finance/因子实现_DolphinDB版本/早盘买卖单大小比.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef openBidVolDvdAskVol(OrderQty, Side){\n\treturn log(avg(iif(Side in [\"1\", \"B\"], OrderQty, NULL)) \\\\ avg(iif(Side in [\"2\", \"S\"], OrderQty, NULL)))\n}\n```\n\n----------------------------------------\n\nTITLE: Solving Linear Equations using DolphinDB solve\nDESCRIPTION: This snippet demonstrates how to define a matrix 'a' and a vector 'y' in DolphinDB, solve the linear equation a*x=y for 'x' using the built-in `solve` function, and verify the result by multiplying 'a' with the calculated 'x'. It illustrates basic matrix/vector definition and direct equation solving.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/matrix.md#_snippet_32\n\nLANGUAGE: DolphinDB\nCODE:\n```\n>a=[1,0,2,1,2,5,1,5,-1]$3:3;\n>y=[6,-4,27];\n>a;\n#0 #1 #2\n-- -- --\n1  1  1 \n0  2  5 \n2  5  -1\n\n>x = solve(a,y);\n>x;\n[5,3,-2]\n\n>a ** matrix(x);\n#0\n--\n6 \n-4\n27\n```\n\n----------------------------------------\n\nTITLE: Loading and Importing Historical Net Value Data with Partitioning\nDESCRIPTION: Creates a distributed, partitioned table for storing historical fund net values, partitioned by trade date on a yearly basis. Uses CSV data loading functions to ingest large datasets efficiently, with emphasis on partitioning strategies for scalable data storage and retrieval.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/public_fund_basic_analysis.md#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\ncsvDataPath = \"/ssd/ssd2/data/fundData/publicFundNetValue.csv\"\ndbName = \"dfs://publicFundDB\"\ntbName = \"publicFundNetValue\"\n// create distributed table in dataset\nif(existsTable(dbName, tbName)){\n\tdropTable(database(dbName), tbName)\n}\nnames = `SecurityID`TradeDate`NetValue`AccNetValue`AdjNetValue\ntypes = `SYMBOL`DATE`DOUBLE`DOUBLE`DOUBLE\nschemaTB = table(1:0, names, types)\ndb = database(dbName)\ndb.createPartitionedTable(table=schemaTB, tableName=tbName, partitionColumns=`TradeDate, sortColumns=`SecurityID`TradeDate)\n```\n\n----------------------------------------\n\nTITLE: Loading Partitioned Backup Data into Memory in DolphinDB\nDESCRIPTION: Loads backed-up data for a specific partition (e.g., \"20170810/0_50\") of the partitioned table \"pt\" into DolphinDB memory for further analysis or processing. The 'loadBackup' function takes the backup location, database path, partition path, and table name as inputs. This facilitates accessing historical backup data without restoring it to the original table. Users with DolphinDB versions 1.30.16/2.00.4 and above can dynamically specify partition paths extracted from backup lists for flexible loading. The snippet is useful for incremental data recovery or inspection.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/restore-backup.md#_snippet_12\n\nLANGUAGE: DolphinDB\nCODE:\n```\nloadBackup(\"/home/DolphinDB/backup\",\"dfs://compoDB\",\"/20170810/0_50\",\"pt\");\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlist=getBackupList(\"/home/DolphinDB/backup\",\"dfs://compoDB\",\"pt\").chunkPath;\npath=list[0][regexFind(list[0],\"/20170807/0_50\"):];\ngetBackupMeta(\"/home/DolphinDB/backup\",\"dfs://compoDB\", path, \"pt\");\nloadBackup(\"/home/DolphinDB/backup\",\"dfs://compoDB\", path, \"pt\");\n```\n\n----------------------------------------\n\nTITLE: Subscribing to Stream Data in DolphinDB\nDESCRIPTION: Subscribes to the stream table `inputSt` and defines a handler function `process` that appends received messages to both the session window engine (`swEngine`) and the reactive state engine (`reactivEngine`).\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_streaming_application_in_IOT.md#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef process(mutable  engine1,mutable  engine2,msg){\n\tengine1.append!(msg)\n\tengine2.append!(msg)\n}\nsubscribeTable(tableName=\"inputSt\", actionName=\"monitor\", offset=0,\n\t\thandler=process{swEngine,reactivEngine}, msgAsTable=true)\n```\n\n----------------------------------------\n\nTITLE: Executing GUI start script on Linux/Mac\nDESCRIPTION: This shell script executes the DolphinDB GUI on Linux and Mac operating systems.  It navigates to the GUI directory and then executes the `gui.sh` script. Assumes java is correctly installed and configured.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/client_tool_tutorial.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\ncd /your/gui/folder\n./gui.sh\n```\n\n----------------------------------------\n\nTITLE: DolphinDB Floating-Point Filtering\nDESCRIPTION: This DolphinDB script filters the 'trades' table based on a floating-point condition on the 'PRC' column. It tests performance with a `PRC` column and selects rows where PRC is greater than 25 and less than 35. The script uses `timer()` to measure execution time over 10 runs. This test assesses the efficiency of filtering on numeric values.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_40\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\n//根据浮点型过滤\ntimer(10) select * from trades where PRC > 25 and PRC < 35\n```\n\n----------------------------------------\n\nTITLE: Executing the Snapshot Upload Function in DolphinDB\nDESCRIPTION: This snippet initializes database and table name parameters and calls the snapUpload function to load market snapshot data into the specified DolphinDB database.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/appendices_market_replay_bp/snap_upload.txt#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndbName = \"dfs://Test_snapshot\"\ntbName = \"snapshot\"\nsnapUpload(dbName, tbName)\n```\n\n----------------------------------------\n\nTITLE: Calculating Max with JIT, `for` loop, and `if-else` in DolphinDB\nDESCRIPTION: DolphinDB script showing how to implement a maximum value calculation (finding the maximum bid amount across 10 levels) using JIT (`@jit`). It first calculates individual level amounts into a vector, then iterates through this vector using a `for` loop and `if-else` statements to find the maximum value. This illustrates how to manually implement logic for functions like `max` when they might not be directly supported or optimal in JIT, while emphasizing the need for type consistency (e.g., initializing `maxRes` as -1.0 for DOUBLE type).\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Streaming_computing_of_financial_quantifiers.md#_snippet_22\n\nLANGUAGE: DolphinDB\nCODE:\n```\n@jit def calAmountMax(bidPrice0, bidPrice1, bidPrice2, bidPrice3, bidPrice4, bidPrice5, bidPrice6, bidPrice7, bidPrice8, bidPrice9, bidOrderQty0, bidOrderQty1, bidOrderQty2, bidOrderQty3, bidOrderQty4, bidOrderQty5, bidOrderQty6, bidOrderQty7, bidOrderQty8, bidOrderQty9){\n\tamount = [bidPrice0*bidOrderQty0, bidPrice1*bidOrderQty1, bidPrice2*bidOrderQty2, bidPrice3*bidOrderQty3, bidPrice4*bidOrderQty4, bidPrice5*bidOrderQty5, bidPrice6*bidOrderQty6, bidPrice7*bidOrderQty7, bidPrice8*bidOrderQty8, bidPrice9*bidOrderQty9]\n\tmaxRes = -1.0\n\tfor(i in 0:10){\n\t\tif(amount[i] > maxRes) maxRes = amount[i]\n\t}\n\treturn maxRes\n}\n```\n\n----------------------------------------\n\nTITLE: Simulating Access Control Event Data in DolphinDB\nDESCRIPTION: Defines a function `duplicateData` to generate simulated access control event records and append them to a mutable table `st`. The function takes the target table, number of records, door code, and start time as input, generating random data for other fields. The subsequent lines call this function repeatedly with varying parameters and time offsets to create a dataset simulating different scenarios, including potential timeout conditions.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_engine_anomaly_alerts.md#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef duplicateData(mutable st, num, doorCode, time){\n    for(i in 0:num){\n        eventTime = time\n        st.append!(table(rand(0..5,1) as recordType, doorCode as doorEventCode, eventTime as eventDate, rand([true,false],1) as readerType, rand(`a+string(1000..1010),1) as sn, 1 as doorNum, rand(`ic+string(100000..100000),1) as card))\n        eventTime = datetimeAdd(eventTime, 5, `s)\n    }\n}\nstartEventDate = 2022.12.01T00:00:00\nduplicateData(st, 75, 11, startEventDate)\nstartEventDate=datetimeAdd(startEventDate , 375, `s)\nduplicateData(st, 25, 56, startEventDate)\nstartEventDate=datetimeAdd(startEventDate , 125, `s)\nduplicateData(st, 100, 61, startEventDate)\nstartEventDate=datetimeAdd(startEventDate , 500, `s)\nduplicateData(st, 25, 66, startEventDate)\nstartEventDate=datetimeAdd(startEventDate , 125, `s)\nduplicateData(st, 70, 12, startEventDate)\nstartEventDate=datetimeAdd(startEventDate , 350, `s)\nduplicateData(st, 30, 60, startEventDate)\nstartEventDate=datetimeAdd(startEventDate , 150, `s)\nduplicateData(st, 25, 67, startEventDate)\nstartEventDate=datetimeAdd(startEventDate , 125, `s)\n```\n\n----------------------------------------\n\nTITLE: Extracting and Adjusting Schema from CSV Sample\nDESCRIPTION: Extracts data schema from sample CSV, converts column names to lowercase to prevent keyword conflicts, and constructs a schema table for data loading.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs InfluxDB/test_dolphindb_big.txt#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// ----------------- 创建 schema\norig_tb_schema = extractTextSchema(FP_SAMPLE_TB)\n// 查看 orig_tb_schema\n\n// 将列名调整为小写避免与 DolphinDB 内置的 SYMBOL, DATE, TIME 等保留关键字产生冲突\ncols = lower(orig_tb_schema.name)\nschema = table(cols, orig_tb_schema.type)\n```\n\n----------------------------------------\n\nTITLE: Calculating Option Delta using Vectorized Operations (DolphinDB Script)\nDESCRIPTION: Defines functions to compute the option Delta using vectorized operations, suitable for matrix inputs without needing JIT. `calculateD1` calculates the d1 term of the Black-Scholes model. `cdfNormalMatrix` applies the standard normal cumulative distribution function (`cdfNormal`) element-wise to a matrix input by reshaping. `calculateDelta` computes the Delta for each element in the input matrices (`etfTodayPrice`, `KPrice`, `dayRatio`, `impvMatrix`, `CPMode`) using `calculateD1`, `cdfNormalMatrix`, and vectorized conditional logic (`iif`) and arithmetic operations. Handles cases where implied volatility is non-positive.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/IV_Greeks_Calculation_for_ETF_Options_Using_JIT.md#_snippet_9\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\ndef calculateD1(etfTodayPrice, KPrice, r, dayRatio, HLMean){\n\tskRatio = etfTodayPrice / KPrice\n\tdenominator = HLMean * sqrt(dayRatio)\n\tresult = (log(skRatio) + (r + 0.5 * pow(HLMean, 2)) * dayRatio) / denominator\n\treturn result\n}\n\ndef cdfNormalMatrix(mean, stdev, X){\n\toriginalShape = X.shape()\n\tX_vec = X.reshape()\n\tresult = cdfNormal(mean, stdev, X_vec)\n\treturn result.reshape(originalShape)\n}\n\ndef calculateDelta(etfTodayPrice, KPrice, r, dayRatio, impvMatrix, CPMode){\n\tdelta = iif(\n\t\t\timpvMatrix <= 0,\n\t\t\t0,\n\t\t\t0.01*etfTodayPrice*CPMode*cdfNormalMatrix(0, 1, CPMode * calculateD1(etfTodayPrice, KPrice, r, dayRatio, impvMatrix))\n\t\t)\n\treturn delta\n}\n```\n\n----------------------------------------\n\nTITLE: 服务器资源监控PromQL指标计算公式\nDESCRIPTION: 这组计算公式用于基于Node Exporter数据源的第一套监控方案，包含CPU使用率、内存使用率、磁盘容量和IO以及网络IO等指标的计算方法。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/cluster_monitor.md#_snippet_8\n\nLANGUAGE: PromQL\nCODE:\n```\n#--------------------CPU-----------------------------#\n#服务器cpu使用率（%）\n(1-sum(rate(node_cpu_seconds_total{mode=\"idle\"}[1m])) by (job) / sum(rate(node_cpu_seconds_total[1m])) by (job)) *100\n\n#--------------------内存-----------------------------#\n#服务器内存使用率（%）\n(node_memory_MemTotal_bytes - (node_memory_MemFree_bytes + node_memory_Buffers_bytes+node_memory_Cached_bytes ))/node_memory_MemTotal_bytes * 100\n\n#--------------------磁盘容量-----------------------------#\n#服务器磁盘空间使用率(%)\n(1 - sum(node_filesystem_free_bytes)by(job) / sum(node_filesystem_size_bytes) by(job))*100\n\n#服务器磁盘挂载点空间使用率(%)\n(1-sum(node_filesystem_free_bytes{mountpoint =~\"/hdd.*?|/ssd.*?|/home.*?\"})by (job,mountpoint)/sum(node_filesystem_size_bytes{mountpoint =~\"/hdd.*?|/ssd.*?|/home.*?\"})by(job,mountpoint))*100\n\n#--------------------磁盘 IO-----------------------------#\n#服务器各磁盘读速率(MB/s)\nirate(node_disk_read_bytes_total{device=~'sd[a-z]'}[1m])/1024/1024\n\n#服务器各磁盘写速率(MB/s)\nirate(node_disk_written_bytes_total{device=~\"sd[a-z]\"}[1m])/1024/1024\n\n#--------------------网络-----------------------------#\n#服务器网络发送速率(MB/s)\nsum(irate(node_network_transmit_bytes_total[1m]))by(job)/1024/1024\n\n#服务器网络接收速率(MB/s)\nsum(irate(node_network_receive_bytes_total[1m]))by(job)/1024/102\n\n#服务器每块网卡发送速率(MB/s)\nirate(node_network_transmit_bytes_total[1m])/1024/1024\n\n#服务器每块网卡接收速率(MB/s)\nirate(node_network_reveive_bytes_total[1m])/1024/1024\n```\n\n----------------------------------------\n\nTITLE: Creating OLAP Partitioned Table in DolphinDB\nDESCRIPTION: This code snippet shows how to create a partitioned table in an OLAP database. It specifies the partition columns but does not include sort columns which are required in TSDB tables.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/sql_performance_optimization_wap_di_rv.md#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndbName = \"dfs://snapshot_SH_L2_OLAP\"\ndb = database(dbName)\ntableName = \"snapshot_SH_L2_OLAP\"\ncreatePartitionedTable(dbHandle=db, table=tbTemp, tableName=tbName, partitionColumns=`TradeTime`SecurityID)\n```\n\n----------------------------------------\n\nTITLE: Appending Data to a Table in DolphinDB\nDESCRIPTION: This code snippet demonstrates how to append data from one table (tb) to another table (ta) using the `append!` function in DolphinDB. It also shows how to verify the row counts before and after the append operation.  Tables `t1` and `db` must be already defined.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Single_Node_Tutorial.md#_snippet_9\n\nLANGUAGE: DolphinDB\nCODE:\n```\nta = loadTable(db, \"t1\")\ntb = loadTable(db, \"t1\")\nselect count(*) from ta\nselect count(*) from tb\nta.append!(tb)\nselect count(*) from ta\n```\n\n----------------------------------------\n\nTITLE: Selecting Multiple Fields using Multi-column Macro Variable\nDESCRIPTION: This snippet selects multiple columns from a table using a multi-column macro variable (`_$$names`). The `names` variable is an array of strings, and the query returns all columns specified within this array. The output will contain multiple columns according to the `names` array.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/macro_var_based_metaprogramming.md#_snippet_13\n\nLANGUAGE: DolphinDB\nCODE:\n```\nt = table(`a`a`b as sym, 1 2 3 as val1, 2 3 4 as val2)\nnames=[\"val1\", \"val2\"]\n<select _$$names from t>.eval()\n```\n\n----------------------------------------\n\nTITLE: Checking Query Result Memory Limits\nDESCRIPTION: DolphinDB script to check the maximum memory allowed for a single query result.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/handling_oom.md#_snippet_6\n\nLANGUAGE: SQL\nCODE:\n```\ngetMemLimitOfQueryResult()\n//8\n```\n\n----------------------------------------\n\nTITLE: Defining Data Simulation Function DolphinDB Script\nDESCRIPTION: Defines a function `writeData` to simulate sensor data. It generates batches of data for 1000 hardware IDs, appends them to the shared `sensorTemp` stream table, and includes a small delay between batches to mimic real-time data flow.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/iot_demo_script.txt#_snippet_2\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\ndef writeData(){\n\thardwareNumber = 1000\n\tfor (i in 0:10000) {\n\t\tdata = table(take(1..hardwareNumber,hardwareNumber) as hardwareId ,take(now(),hardwareNumber) as ts,rand(20..41,hardwareNumber) as temp1,rand(30..71,hardwareNumber) as temp2,rand(70..151,hardwareNumber) as temp3)\n\t\tobjByName(\"sensorTemp\").append!(data)\n\t\tsleep(10)\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: 响应式状态引擎过滤关门告警保留开门事件 - DolphinDB\nDESCRIPTION: 创建第二个响应式状态引擎，用于过滤关门告警，仅保留开门相关事件码的异常数据。设置分组列doorNum，保持计算指标eventDate和doorEventCode原样输出，通过filter参数筛选仅包含开门事件码的记录。输入表结构需匹配上一级引擎输出，为最终异常事件流提供准确数据，实现门禁异常开门状态告警的精确过滤。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_engine_anomaly_alerts.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\nswOut1 =table(1:0,`eventDate`doorNum`doorEventCode,[DATETIME,INT, INT])\nreactivEngine = createReactiveStateEngine(name=`reactivEngine, metrics=<[eventDate,doorEventCode]>, \n    dummyTable=swOut1,outputTable= objByName(`outputSt),keyColumn= \"doorNum\",\n    filter=<doorEventCode in [11,12,56,60,65,67]>)\n```\n\n----------------------------------------\n\nTITLE: DolphinDB JSON Response Format Example\nDESCRIPTION: Example of the JSON response format returned by DolphinDB's Web API. Shows the column-oriented structure with metadata and values for a table with timestamp and temperature data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/web_chart_integration.md#_snippet_1\n\nLANGUAGE: JSON\nCODE:\n```\n{\n\t\"sessionID\": \"3691974869\",\n\t\"userId\": \"admin\",\n\t\"resultCode\": \"0\",\n\t\"msg\": \"\",\n\t\"object\": [{\n\t\t\"name\": \"\",\n\t\t\"form\": \"table\",\n\t\t\"size\": \"11\",\n\t\t\"value\": [{\n\t\t\t\"name\": \"second_time\",\n\t\t\t\"form\": \"vector\",\n\t\t\t\"type\": \"second\",\n\t\t\t\"size\": \"11\",\n\t\t\t\"value\": [\"13:03:50\", \"13:03:51\", \"13:03:52\", \"13:03:53\", \"13:03:54\", \"13:03:55\", \"13:03:56\", \"13:03:57\", \"13:03:58\", \"13:03:59\", \"13:04:00\"]\n\t\t}, {\n\t\t\t\"name\": \"ec\",\n\t\t\t\"form\": \"vector\",\n\t\t\t\"type\": \"double\",\n\t\t\t\"size\": \"11\",\n\t\t\t\"value\": [1.019094, 0.971753, 0.962792, 1.014048, 0.991746, 1.016851, 0.98674, 1.00463, 0.991642, 1.018987, 1.008604]\n\t\t}]\n\t}]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring DolphinDB Agent Node High Availability Settings in .cfg File\nDESCRIPTION: This snippet shows configuration parameters required for an agent node in DolphinDB's high availability cluster. It sets the mode to 'agent', specifies the local site IP and port, defines the corresponding controller site, enumerates all sites in the cluster with their roles, and configures cache and cluster behavior settings. These configurations enable the agent to participate properly in metadata management and data coordination within the cluster. Dependency is on the DolphinDB cluster deployment environment supporting agent-controller coordination.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/iot_examples.md#_snippet_28\n\nLANGUAGE: cfg\nCODE:\n```\nmode =agent\nlocalSite=192.168.1.13:22215:agent2\ncontrollerSite=192.168.1.12:22216:controller1\nsites=192.168.1.13:22215:agent2:agent,192.168.1.12:22216:controller1:controller,192.168.1.13:22216:controller2:controller,192.168.1.14:22216:controller3:controller\nlanCluster=0\nchunkCacheEngineMemSize=4\n```\n\n----------------------------------------\n\nTITLE: Simulating Stream Data Writing and Validating Handler Function in DolphinDB C++\nDESCRIPTION: This DolphinDB script loads the compiled plugin, defines a shared streaming table and a target table, and subscribes the streaming table with the handler function to process incoming stream data. It appends sample generated data to the stream and then queries the target table to verify if the handler function correctly inserted the expected data columns. The snippet illustrates practical usage and validation of the plugin's streaming handler capabilities.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_development_tutorial.md#_snippet_16\n\nLANGUAGE: cpp\nCODE:\n```\nloadPlugin(\"/path/to/PluginHandler.txt\")\n\nshare streamTable(10:0, `id`sym`timestamp, [INT,SYMBOL,TIMESTAMP]) as t0\nt1 = table(10:0, `sym`timestamp, [SYMBOL,TIMESTAMP])\nsubscribeTable(, `t0, , , handler::handler{[1,2], t1})\n\nt0.append!(table(1..100 as id, take(`a`b`c`d, 100) as symbol, now() + 1..100 as timestamp))\n\nselect * from t1\n```\n\n----------------------------------------\n\nTITLE: Calculating Sum with JIT and Formula Expansion in DolphinDB\nDESCRIPTION: DolphinDB script demonstrating how to implement a sum calculation (total bid amount across 10 levels) using JIT compilation (`@jit`) by manually expanding the formula. This approach is used when a built-in function like `sum` might not be supported or efficient within a JIT context for this specific pattern, ensuring maximum performance by translating the expanded arithmetic directly into machine code.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Streaming_computing_of_financial_quantifiers.md#_snippet_21\n\nLANGUAGE: DolphinDB\nCODE:\n```\n@jit\ndef calAmount(bidPrice0, bidPrice1, bidPrice2, bidPrice3, bidPrice4, bidPrice5, bidPrice6, bidPrice7, bidPrice8, bidPrice9, bidOrderQty0, bidOrderQty1, bidOrderQty2, bidOrderQty3, bidOrderQty4, bidOrderQty5, bidOrderQty6, bidOrderQty7, bidOrderQty8, bidOrderQty9){\n\treturn bidPrice0*bidOrderQty0+bidPrice1*bidOrderQty1+bidPrice2*bidOrderQty2+bidPrice3*bidOrderQty3+bidPrice4*bidOrderQty4+bidPrice5*bidOrderQty5+bidPrice6*bidOrderQty6+bidPrice7*bidOrderQty7+bidPrice8*bidOrderQty8+bidPrice9*bidOrderQty9\n}\n```\n\n----------------------------------------\n\nTITLE: Comparing Filter Conditions with Comma vs AND in DolphinDB SQL\nDESCRIPTION: Demonstrates the performance difference between using comma and AND operators for filter conditions in DolphinDB queries, especially when sequence-related conditions are involved.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_SQL_Case_Tutorial.md#_snippet_4\n\nLANGUAGE: SQL\nCODE:\n```\ntimer(10) t1 = select * from t where ratios(qty) > 1, date = 2019.01.02, sym = `C\ntimer(10) t2 = select * from t where ratios(qty) > 1 and date = 2019.01.02 and sym = `C\n\neach(eqObj, t1.values(), t2.values()) // true\n```\n\n----------------------------------------\n\nTITLE: 建立订阅关系并持久化流表的DolphinDB脚本（DolphinDB脚本）\nDESCRIPTION: 该脚本定义了多个流表的结构、持久化配置、入库处理函数，以及订阅这些流表。主要用途是订阅不同类型的市场行情（现货、指数、期权、债券），并将数据持久化存储到分布式文件系统中。依赖于XTP接口的表结构和DolphinDB的流表持久化功能。订阅后，行情数据将实时处理并写入指定存储路径。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/xtp.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 现货快照 + 逐笔成交 + 逐笔委托 + 逐笔状态 + 订单簿\n// 获取表结构\nactualSchema = streamTable(1:0, XTP::getSchema(`actualMarketData)['name'], XTP::getSchema(`actualMarketData)['typeString'])\nentrustSchema = streamTable(1:0, XTP::getSchema(`entrust)['name'], XTP::getSchema(`entrust)['typeString'])\ntradeSchema = streamTable(1:0, XTP::getSchema(`trade)['name'], XTP::getSchema(`trade)['typeString'])\nstateSchema = streamTable(1:0, XTP::getSchema(`state)['name'], XTP::getSchema(`state)['typeString'])\norderBookSchema = streamTable(1:0, XTP::getSchema(`orderBook)['name'], XTP::getSchema(`orderBook)['typeString'])\n// 建立持久化流表\nenableTableShareAndPersistence(table=actualSchema, tableName=`actualMarketDataStream, cacheSize=100000, preCache=1000)\nenableTableShareAndPersistence(table=entrustSchema, tableName=`entrustStream, cacheSize=100000, preCache=1000)\nenableTableShareAndPersistence(table=tradeSchema, tableName=`tradeStream, cacheSize=100000, preCache=1000)\nenableTableShareAndPersistence(table=stateSchema, tableName=`stateStream, cacheSize=100000, preCache=1000)\nenableTableShareAndPersistence(table=orderBookSchema, tableName=`orderBookStream, cacheSize=100000, preCache=1000)\ngo\n// 定义入库函数\nactualHandler = append!{loadTable(\"dfs://XTP.actual\", \"actualMarketData\")}\nentrustHandler = append!{loadTable(\"dfs://XTP.actual\", \"entrust\")}\ntradeHandler = append!{loadTable(\"dfs://XTP.actual\", \"trade\")}\nstateHandler = append!{loadTable(\"dfs://XTP.actual\", \"state\")}\norderBookHandler = append!{loadTable(\"dfs://XTP.actual\", \"orderBook\")}\n// 订阅\nsubscribeTable(tableName=`actualMarketDataStream, actionName=`actualMarketDataAction, offset=0, handler=actualHandler, msgAsTable=true, batchSize=1000, throttle=0.1)\nsubscribeTable(tableName=`entrustStream, actionName=`entrustAction, offset=0, handler=entrustHandler, msgAsTable=true, batchSize=1000, throttle=0.1)\nsubscribeTable(tableName=`tradeStream, actionName=`tradeAction, offset=0, handler=tradeHandler, msgAsTable=true, batchSize=1000, throttle=0.1)\nsubscribeTable(tableName=`stateStream, actionName=`stateAction, offset=0, handler=stateHandler, msgAsTable=true, batchSize=1000, throttle=0.1)\nsubscribeTable(tableName=`orderBookStream, actionName=`orderBookAction, offset=0, handler=orderBookHandler, msgAsTable=true, batchSize=1000, throttle=0.1)\n\n// 指数快照\nindexSchema = streamTable(1:0, XTP::getSchema(`indexMarketData)['name'], XTP::getSchema(`indexMarketData)['typeString'])\nenableTableShareAndPersistence(table=indexSchema, tableName=`indexMarketDataStream, cacheSize=100000, preCache=1000)\ngo\nindexHandler = append!{loadTable(\"dfs://XTP.index\", \"indexMarketData\")}\nsubscribeTable(tableName=`indexMarketDataStream, actionName=`indexMarketDataAction, offset=0, handler=indexHandler, msgAsTable=true, batchSize=1000, throttle=0.1)\n\n// 期权快照\noptionSchema = streamTable(1:0, XTP::getSchema(`optionMarketData)['name'], XTP::getSchema(`optionMarketData)['typeString'])\nenableTableShareAndPersistence(table=optionSchema, tableName=`optionMarketDataStream, cacheSize=100000, preCache=1000)\ngo\noptionHandler = append!{loadTable(\"dfs://XTP.option\", \"optionMarketData\")}\nsubscribeTable(tableName=`optionMarketDataStream, actionName=`optionMarketDataAction, offset=0, handler=optionHandler, msgAsTable=true, batchSize=1000, throttle=0.1)\n\n// 债券快照\nbondSchema = streamTable(1:0, XTP::getSchema(`bondMarketData)['name'], XTP::getSchema(`bondMarketData)['typeString'])\nenableTableShareAndPersistence(table=bondSchema, tableName=`bondMarketDataStream, cacheSize=100000, preCache=1000)\ngo\nbondHandler = append!{loadTable(\"dfs://XTP.bond\", \"bondMarketData\")}\nsubscribeTable(tableName=`bondMarketDataStream, actionName=`bondMarketDataAction, offset=0, handler=bondHandler, msgAsTable=true, batchSize=1000, throttle=0.1)\n\n```\n\n----------------------------------------\n\nTITLE: Aggregating Maximum Memory Usage per Hour with MongoDB - JavaScript\nDESCRIPTION: This snippet uses MongoDB's .aggregate() with $group to compute the maximum memory usage per hour from the 'device_readings' collection. Key input is the time field for hourly aggregation. Requires numeric mem_used field and indexed time field for efficient grouping. Returns documents of hourly max memory used values.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs MongoDB/mongodb_readings_query.txt#_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\ndb.device_readings.explain(\"executionStats\").aggregate([{$group:{_id:{hour_new:{$hour:\"$time\"}},max_mem:{$max:\"$mem_used\"}}}])\n```\n\n----------------------------------------\n\nTITLE: 使用Python API从DolphinDB读取数据\nDESCRIPTION: 通过Python API连接DolphinDB服务器并从分布式数据库中查询数据，将结果转换为Python对象进行后续处理。这个简单示例展示了基本的数据查询操作。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/quant_finance_examples.md#_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\nimport dolphindb as ddb\nimport pandas as pd\ndbPath = \"dfs://level2API\"\ntableName = \"quotes\"\ns = ddb.session()\ns.connect(\"127.0.0.1\",8848,\"admin\",\"123456\")\nt = s.run(\"select * from loadTable('{db}','{tb}') where symbol=`600007\".format(db=dbPath,tb=tableName))\nprint(t)\n```\n\n----------------------------------------\n\nTITLE: Ingesting Tick Data and Querying Guotai Junan 001 Streaming Results in DolphinDB Script\nDESCRIPTION: This snippet inserts sample tick data for two securities into a previously configured stream engine and queries the computed Guotai Junan 001 factor using a pivot by security and date. All table and engine configurations must match those in the prior streaming pipeline definition for correct operation. The output illustrates factor progression through time, triggered when 3000 new records are inserted or a new datetime appears.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Streaming_computing_of_financial_quantifiers.md#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 向引擎添加数据\ninsert into streamEngine values(`000001, 2023.01.01, 30.85, 30.90, 31.65, 30.55, 31.45, 100, 3085, 0, 0, 0)\ninsert into streamEngine values(`000002, 2023.01.01, 30.86, 30.55, 31.35, 29.85, 30.75, 120, 3703.2, 0, 0, 0)\ninsert into streamEngine values(`000001, 2023.01.02, 30.80, 30.95, 31.05, 30.05, 30.85, 200, 6160, 0, 0, 0)\ninsert into streamEngine values(`000002, 2023.01.02, 30.81, 30.99, 31.55, 30.15, 30.65, 180, 5545.8, 0, 0, 0)\ninsert into streamEngine values(`000001, 2023.01.03, 30.83, 31.00, 31.35, 30.35, 30.55, 230, 7090.9, 0, 0, 0)\ninsert into streamEngine values(`000002, 2023.01.03, 30.89, 30.85, 31.10, 30.00, 30.45, 250, 7722.5, 0, 0, 0)\ninsert into streamEngine values(`000001, 2023.01.04, 30.90, 30.86, 31.10, 30.40, 30.75, 300, 9270, 0, 0, 0)\ninsert into streamEngine values(`000002, 2023.01.04, 30.85, 30.95, 31.65, 30.55, 31.45, 270, 8329.5, 0, 0, 0)\ninsert into streamEngine values(`000001, 2023.01.05, 30.86, 30.55, 31.35, 29.85, 30.75, 360, 11109.6, 0, 0, 0)\ninsert into streamEngine values(`000002, 2023.01.05, 30.80, 30.95, 31.05, 30.05, 30.85, 200, 6160, 0, 0, 0)\ninsert into streamEngine values(`000001, 2023.01.06, 30.81, 30.99, 31.55, 30.15, 30.65, 180, 5545.8, 0, 0, 0)\ninsert into streamEngine values(`000002, 2023.01.06, 30.83, 31.00, 31.35, 30.35, 30.55, 230, 7090.9, 0, 0, 0)\ninsert into streamEngine values(`000001, 2023.01.07, 30.89, 30.85, 31.10, 30.00, 30.45, 250, 7722.5, 0, 0, 0)\ninsert into streamEngine values(`000002, 2023.01.07, 30.90, 30.86, 31.10, 30.40, 30.75, 300, 9270, 0, 0, 0)\ninsert into streamEngine values(`000001, 2023.01.08, 30.89, 30.85, 31.10, 30.00, 30.45, 250, 7722.5, 0, 0, 0)\ninsert into streamEngine values(`000002, 2023.01.08, 30.90, 30.86, 31.10, 30.40, 30.75, 300, 9270, 0, 0, 0)\n\n// 查看结果\nselect factor from resultTable pivot by dateTime, securityID\n/*\ndateTime                000001 000002            \n----------------------- ------ ------------------\n2023.01.01T00:00:00.000                          \n2023.01.02T00:00:00.000                          \n2023.01.03T00:00:00.000                          \n2023.01.04T00:00:00.000                          \n2023.01.05T00:00:00.000                          \n2023.01.06T00:00:00.000 -1     -1\n2023.01.07T00:00:00.000 -1     -1          \n*/\n```\n\n----------------------------------------\n\nTITLE: DolphinDB Script for Database and Table Creation\nDESCRIPTION: This DolphinDB script creates a distributed database (`db_demo`) and a partitioned table (`collect`). It defines the table schema (column names and types), creates a database with time-series engine, and then creates a partitioned table using the specified schema and partitioning scheme (by `ts` and `deviceCdoe` + `ts`).\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_cpp_api_connector.md#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 建立分布式数据库及分区表\ndbname=\"dfs://db_demo\"\ntablename=\"collect\"\ncols_info=`ts`deviceCdoe`logicalPostionId`physicalPostionId`propertyCode`propertyValue\ncols_type=[DATETIME,SYMBOL,SYMBOL,SYMBOL,SYMBOL,INT]\nt=table(1:0,cols_info,cols_type)\ndb=database(dbname,VALUE,[2022.11.01],engine=`TSDB)\npt=createPartitionedTable(db,t,tablename,`ts,,`deviceCdoe`ts)\n```\n\n----------------------------------------\n\nTITLE: Creating Stream Data Table for 1-Minute OHLC Market Data in DolphinDB Script\nDESCRIPTION: This snippet declares the stream table name parameter and creates a shared streaming table to receive synthesized 1-minute OHLC (open, high, low, close) data from the output of downstream processing engines. The table serves as the final output stream that can be consumed by external applications via DolphinDB's API.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/K.md#_snippet_15\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//Declare parameters\nmdlStockFundOHLCTBName = \"mdlStockFundOHLC\"\n//Create MDL 1-minute OHLC table\nshare(getMDLStockFundOHLCTB(100000), mdlStockFundOHLCTBName)\n```\n\n----------------------------------------\n\nTITLE: Loading a Table in DolphinDB\nDESCRIPTION: This code snippet loads a table named \"Snapshot\" from the distributed file system \"dfs://Level1\" and assigns it to the variable `snapshot`. This is a common initial step in many DolphinDB scripts that work with distributed data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_sql_cases.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\nsnapshot = loadTable(\"dfs://Level1\", \"Snapshot\")\n```\n\n----------------------------------------\n\nTITLE: Generating Table Schema from Distributed Database (DolphinDB Script)\nDESCRIPTION: Defines a function to extract and construct schemas from tables in a DolphinDB distributed database. Uses loadTable to fetch the schema details, formats them into a new in-memory table, and is used here to retrieve schemas for 'trade' and 'snapshot' tables for further processing. The key dependencies are access to the named databases and tables ('dfs://trade', 'dfs://snapshot'), and outputs are schema tables containing column names and types.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/stock_market_replay/02.calTradeCost_asofJoin.txt#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef createSchemaTable(dbName, tableName){\n\tschema = loadTable(dbName, tableName).schema().colDefs\n\treturn table(1:0, schema.name, schema.typeString)\n}\ntradeSchema = createSchemaTable(\"dfs://trade\", \"trade\") \nsnapshotSchema = createSchemaTable(\"dfs://snapshot\", \"snapshot\") \n```\n\n----------------------------------------\n\nTITLE: Importing Array Vector Data from CSV with Array Delimiter - DolphinDB Script\nDESCRIPTION: This snippet demonstrates two ways to import a CSV file containing array vector columns into DolphinDB: loading into an in-memory table with 'loadText', and importing to a distributed TSDB database using 'loadTextEx'. The 'arrayDelimiter' parameter is specified to ensure correct parsing of array vectors. The distributed example includes steps to define database schema, table structure, and partition information. DolphinDB 2.00.4 or later is required, and the CSV file must use consistent delimiters for array columns.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_csv.md#_snippet_21\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//用 loadText 将文本文件导入内存表\nt = loadText(path, schema=schema, arrayDelimiter=\",\")\n\n//用 loadTextEx 将文本文件导入分布式数据库\n//创建 TSDB 引擎下的数据库表\ndb = database(directory=\"dfs://testTSDB\", partitionType=VALUE, partitionScheme=`APPL`AMZN`IBM, engine=\"TSDB\" )\nname = `sid`date`bid`ask\ntype = [\"SYMBOL\",\"DATE\",\"DOUBLE[]\",\"DOUBLE[]\"]\ntbTemp = table(1:0, name, type)\ndb.createPartitionedTable(tbTemp, `pt, `sid, sortColumns=`date)\npt = loadTextEx(dbHandle=db, tableName=`pt, partitionColumns=`sid, filename=path, schema=schema, arrayDelimiter=\",\")\n```\n\n----------------------------------------\n\nTITLE: Importing Multiple CSV Files into DolphinDB Database\nDESCRIPTION: Sequentially loads multiple CSV files into the database's 'taq' table, printing progress and measuring total import time.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs InfluxDB/test_dolphindb_big.txt#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// --------------------- 导入数据\nFP_CSV = FP_TAQ + 'csv/'\nfps = FP_CSV + (exec filename from files(FP_CSV) order by filename)\n\n// pt = loadTextEx(db, `taq, `date`symbol, fps[0], ,schema)\n\n// 顺序读取 23 个文件\ntimer {\n\tfor (fp in fps) {\n\t\tloadTextEx(db, `taq, `date`symbol, fp, ,schema)\n\t\tprint now() + \": 已导入 \" + fp\n\t}\n}\n// 用时 37m 58s\n\nclose(db)\n```\n\n----------------------------------------\n\nTITLE: 基金数量变化可视化\nDESCRIPTION: 统计基金的数量随时间变化，并绘制折线图反映历史上的基金数量趋势。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/public_fund_basic_analysis.md#_snippet_12\n\nLANGUAGE: DolphinDB\nCODE:\n```\nfundNum = matrix(rowCount(returnsMatrix)).rename!(returnsMatrix.rowNames(), [\"count\"])\nplot(fundNum.loc( ,`count), fundNum.rowNames(), '公募基金在历史上的数量变化', LINE)\n```\n\n----------------------------------------\n\nTITLE: Configuring Alertmanager Targets in Prometheus (YAML)\nDESCRIPTION: This YAML snippet defines the Alertmanager configuration section within Prometheus. It specifies the static configuration for Alertmanager targets, providing the IP address and port where Alertmanager is listening. It also points to the location of the rule files used by Prometheus.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/cluster_monitor.md#_snippet_14\n\nLANGUAGE: yaml\nCODE:\n```\nalerting:\n  alertmanagers:\n    - static_configs:\n        - targets: [\"xxxxxxxx:xxx\"] #alertmanager 的 IP 地址和端口号\n\nrule_files:\n  - \"rules/node_alerts.yml\" #告警规则的存放路径\n```\n\n----------------------------------------\n\nTITLE: Accumulating Binary Char Data into DolphinDB Vector During File Reading in Plugin C++\nDESCRIPTION: This snippet illustrates reading binary data from an input stream by repeatedly filling a buffer and appending the read bytes into a dynamically growing CHAR vector. After all data is read, it assembles a single-column DolphinDB table from the vector. This approach is typical for custom binary data import where raw bytes are processed before conversion to meaningful types or schema mapping.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_development_tutorial.md#_snippet_19\n\nLANGUAGE: cpp\nCODE:\n```\nchar buf[Util::BUF_SIZE];\nVectorSP vec = Util::createVector(DT_CHAR, 0);\nsize_t actualLength;\n\nwhile (true) {\n\tinputStream->readBytes(buf, Util::BUF_SIZE, actualLength);\n\tif (actualLength <= 0)\n\t\tbreak;\n\tvec->appendChar(buf, actualLength);\n}\n\nvector<ConstantSP> cols = {vec};\nvector<string> colNames = {\"col0\"};\n\nreturn Util::createTable(colNames, cols);\n```\n\n----------------------------------------\n\nTITLE: Finding Minimum Price per Symbol using Matrix Aggregation in DolphinDB Script\nDESCRIPTION: Applies the `min()` aggregation function directly to the price matrix (assuming `price` matrix is already created). This performs a column-wise aggregation, returning a vector containing the minimum price found in each column (i.e., the minimum price for each stock symbol).\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/panel_data.md#_snippet_16\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\n min(price);\n```\n\n----------------------------------------\n\nTITLE: tnsnames.ora Oracle Network Configuration Example - Shell\nDESCRIPTION: Shows a network service descriptor to be added to /etc/oracle/tnsnames.ora, mapping logical service name to Oracle instance network information, including IP address, TCP port, and SID. Replace placeholder values (<oracle ip>) with actual server details.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Oracle_to_DolphinDB.md#_snippet_5\n\nLANGUAGE: Shell\nCODE:\n```\nORAC=\n(DESCRIPTION =\n  (ADDRESS_LIST =\n    (ADDRESS = (PROTOCOL = TCP)(HOST = <oracle ip>)(PORT = 1521))\n  )\n  (CONNECT_DATA =\n    (SID=ora21c)\n  )\n)\n\n```\n\n----------------------------------------\n\nTITLE: Submitting a Batch Task via submitJob\nDESCRIPTION: This code demonstrates how to submit a batch task using the `submitJob` function.  The task is encapsulated in a function `bacthExeCute`, and `submitJob` schedules this function to run on the server, independent of the client.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practice_for_factor_calculation.md#_snippet_36\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//对于跑批的任务封装函数\ndef bacthExeCute(){}\n// 通过summitjob进行提交\nsubmitJob(\"batchTask\",\"batchTask\", bacthExeCute)\n```\n\n----------------------------------------\n\nTITLE: Setup Parallel Capital Flow Processing Engines (DolphinDB)\nDESCRIPTION: Defines a function `processCapitalFlowFunc` to create parallel `reactiveStateEngine` instances for calculating capital flow metrics. It defines the schema for a dummy table (receiving input from `processSellOrder` engines) and complex metrics using `dynamicGroupCumsum` and `dynamicGroupCumcount`. It iterates `parallel` times, creating engines that aggregate data keyed by `SecurityID` and output results to the shared `capitalFlowStream` table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/streaming_capital_flow_daily/02.createEngineSub.txt#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef processCapitalFlowFunc(parallel){\n\tcolName = `SecurityID`SellNum`TradeTime`TradeAmount`TotalSellAmount`SellOrderFlag`PrevTotalSellAmount`PrevSellOrderFlag`BuyNum`TotalBuyAmount`BuyOrderFlag`PrevTotalBuyAmount`PrevBuyOrderFlag\n\tcolType =  [SYMBOL, INT, TIMESTAMP, DOUBLE, DOUBLE, INT, DOUBLE, INT,  INT, DOUBLE, INT, DOUBLE, INT]\n\tprocessSellOrder = table(1:0, colName, colType)\n\tmetrics1 = <dynamicGroupCumsum(TotalSellAmount, PrevTotalSellAmount, SellOrderFlag, PrevSellOrderFlag, 3)> \n\tmetrics2 = <dynamicGroupCumcount(SellOrderFlag, PrevSellOrderFlag, 3)> \n\tmetrics3 = <dynamicGroupCumsum(TotalBuyAmount, PrevTotalBuyAmount, BuyOrderFlag, PrevBuyOrderFlag, 3)> \n\tmetrics4 = <dynamicGroupCumcount(BuyOrderFlag, PrevBuyOrderFlag, 3)>\n\tfor(i in 1..parallel){\n\t\tcreateReactiveStateEngine(name = \"processCapitalFlow\"+string(i), metrics = [<TradeTime>, <cumsum(TradeAmount)>, metrics1, metrics2, metrics3, metrics4], dummyTable =processSellOrder, outputTable = capitalFlowStream, keyColumn = `SecurityID, keepOrder = true)\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: Loading and Filtering Trade Data from Distributed File System in DolphinDB\nDESCRIPTION: This snippet loads trade data from a distributed file system table, filters records within specified trading hours, and sorts them by trade time and security ID. Dependencies include the loadTable function and the data source path. It prepares data for replay by selecting relevant trading periods and ordering records for sequential processing.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/streaming_capital_flow_daily/03.replay.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\nt = select * from loadTable(\"dfs://trade\", \"trade\") where time(TradeTime) between 09:30:00.000 : 15:00:00.000 order by TradeTime, SecurityID\n```\n\n----------------------------------------\n\nTITLE: Importing and Merging Multiple Columns to Array Vector after Loading - DolphinDB Script\nDESCRIPTION: This snippet showcases the process of loading market snapshot data from a CSV into memory, merging ten price and order columns into single array vector columns using the 'fixedLengthArrayVector' function, and appending the transformed table into a DolphinDB distributed database. It involves schema extraction and column type adjustment, in-memory transformation using SQL-style select queries, and then a batch append to the database. This is ideal for efficiently storing high-dimensional trading data and requires the source CSV, schema extraction, and database connectivity.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_csv.md#_snippet_22\n\nLANGUAGE: DolphinDB\nCODE:\n```\nsnapFile=\"/home/data/snapshot.csv\"\ndbpath=\"dfs://LEVEL2_Snapshot_ArrayVector\"\ntbName=\"Snap\"\n\nschemas=extractTextSchema(snapFile)\nupdate schemas set type = `SYMBOL where name = `InstrumentStatus\n\n//使用 loadText 加载文本，耗时约1分30秒\nrawTb = loadText(snapFile,schema=schemas)\n//合并10档数据为1列，耗时约15秒\narrayVectorTb = select SecurityID,TradeTime,PreClosePx,OpenPx,HighPx,LowPx,LastPx,TotalVolumeTrade,TotalValueTrade,InstrumentStatus,fixedLengthArrayVector(BidPrice0,BidPrice1,BidPrice2,BidPrice3,BidPrice4,BidPrice5,BidPrice6,BidPrice7,BidPrice8,BidPrice9) as BidPrice,fixedLengthArrayVector(BidOrderQty0,BidOrderQty1,BidOrderQty2,BidOrderQty3,BidOrderQty4,BidOrderQty5,BidOrderQty6,BidOrderQty7,BidOrderQty8,BidOrderQty9) as BidOrderQty,fixedLengthArrayVector(BidOrders0,BidOrders1,BidOrders2,BidOrders3,BidOrders4,BidOrders5,BidOrders6,BidOrders7,BidOrders8,BidOrders9) as BidOrders ,fixedLengthArrayVector(OfferPrice0,OfferPrice1,OfferPrice2,OfferPrice3,OfferPrice4,OfferPrice5,OfferPrice6,OfferPrice7,OfferPrice8,OfferPrice9) as OfferPrice,fixedLengthArrayVector(OfferOrderQty0,OfferOrderQty1,OfferOrderQty2,OfferOrderQty3,OfferOrderQty4,OfferOrderQty5,OfferOrderQty6,OfferOrderQty7,OfferOrderQty8,OfferOrderQty9) as OfferOrderQty,fixedLengthArrayVector(OfferOrders0,OfferOrders1,OfferOrders2,OfferOrders3,OfferOrders4,OfferOrders5,OfferOrders6,OfferOrders7,OfferOrders8,OfferOrders9) as OfferOrders,NumTrades,IOPV,TotalBidQty,TotalOfferQty,WeightedAvgBidPx,WeightedAvgOfferPx,TotalBidNumber,TotalOfferNumber,BidTradeMaxDuration,OfferTradeMaxDuration,NumBidOrders,NumOfferOrders,WithdrawBuyNumber,WithdrawBuyAmount,WithdrawBuyMoney,WithdrawSellNumber,WithdrawSellAmount,WithdrawSellMoney,ETFBuyNumber,ETFBuyAmount,ETFBuyMoney,ETFSellNumber,ETFSellAmount,ETFSellMoney from rawTb\n//载入数据库，耗时约60秒\nloadTable(dbpath, tbName).append!(arrayVectorTb)\n```\n\n----------------------------------------\n\nTITLE: Creating Additional DolphinDB Instance on First Server\nDESCRIPTION: Command to create an additional DolphinDB instance on the first server for a dual-server pseudo-high availability configuration.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/service_deployment_and_migration.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncp /home/dolphindb_1 /home/dolphindb_3;\n```\n\n----------------------------------------\n\nTITLE: Loading Single CSV Order Book File into Table in DolphinDB Script\nDESCRIPTION: Function loadOneFile loads data from a specified CSV file into a DolphinDB table using the provided schema. It converts the 'market' column values to uppercase, scales the 'Volume' column by 100, and adds multiple additional columns with specified data types (INT, DOUBLE) to enhance the dataset with extra price and volume fields. Finally, it reorders the columns to match the target order book table's schema before returning the loaded data table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/csvImportDemo/importOldData.txt#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef loadOneFile(csvFile,orderbooktb,schema1){\n\tt = loadText(csvFile,,schema1)\n\t t[\"market\"] = upper(t[\"market\"])\n \tt[`Volume] = t[\"Volume\"]*100\n \tt.addColumn(`Status`PreClose`Open`High`Low`AskPrice6`AskPrice7`AskPrice8`AskPrice9`AskPrice10`BidPrice6`BidPrice7`BidPrice8`BidPrice9`BidPrice10`AskVolume6`AskVolume7`AskVolume8`AskVolume9`AskVolume10`BidVolume6`BidVolume7`BidVolume8`BidVolume9`BidVolume10`BidOrderTotalVolume`AskOrderTotalVolume`AvgBidOrderPrice`AvgAskOrderPrice`LimitHighestPrice`LimitLowestPrice,[INT,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE,DOUBLE])\n\tt.reorderColumns!(orderbooktb.schema().colDefs[`name])\n\treturn t\n}\n```\n\n----------------------------------------\n\nTITLE: Appending Data to a Segmented Table with append! System Call C++\nDESCRIPTION: Demonstrates how to append data specifically to a segmented DolphinDB table within a plugin using the built-in `append!` system function. It retrieves the function definition via `getFunctionDef` and calls it using the heap and arguments.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_advance.md#_snippet_20\n\nLANGUAGE: C++\nCODE:\n```\nif(t->isSegmentedTable()){\n    vector<ConstantSP> args = {t, resultTable};\n    Heap* h = sub->getHeap();\n    h->currentSession()->getFunctionDef(\"append!\")->call(h, args);\n}\n```\n\n----------------------------------------\n\nTITLE: Backing Up Specific DolphinDB Table Partitions\nDESCRIPTION: Illustrates submitting a background job to back up a list of specified partitions (`pars`) of a table (`tbName` in `dbPath`) to a directory (`backupDir`) using the generic `backup` function. Key parameters like `parallel` and `snapshot` control the backup behavior.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/backup-restore-new.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndbPath=\"dfs://testdb\"\ntbName=`quotes_2\nbackupDir=\"/home/$USER/backupPar\"\npars=[\"/Key3/tp/20120101\",\"/Key4/tp/20120101\"]\nsubmitJob(\"backupPartitions\",\"backup some partitions in quotes_2 in testdb\",backup,backupDir,dbPath,false,true,true,tbName,pars)\n```\n\n----------------------------------------\n\nTITLE: Checking out a Pull Request using GitHub CLI\nDESCRIPTION: This command uses the GitHub CLI to checkout a specific pull request to a local branch, allowing for testing and review of the proposed changes. Replace <PULL-REQUEST ID> with the actual ID of the pull request.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_Open_Source_Project_Contribution.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ngh pr checkout <PULL-REQUEST ID>\n```\n\n----------------------------------------\n\nTITLE: Ingesting Tick Data and Querying WorldQuant Alpha 1 Streaming Results in DolphinDB Script\nDESCRIPTION: This snippet demonstrates the process of inserting tick-level market data into a previously defined stream engine and querying real-time computed factor results. The insert commands pass rows representing different securities and dates. The select statement pivots the resultTable to display the computed factor per security and datetime. This operation is intended for use after the streaming engine has been properly set up. Ensure that engine and table schemas are consistent with the earlier pipeline setup for correct ingestion and output.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Streaming_computing_of_financial_quantifiers.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 向引擎添加数据\ninsert into streamEngine values(`000001, 2023.01.01, 30.85, 30.90, 31.65, 30.55, 31.45, 100, 3085, 0, 0, 0)\ninsert into streamEngine values(`000002, 2023.01.01, 30.86, 30.55, 31.35, 29.85, 30.75, 120, 3703.2, 0, 0, 0)\ninsert into streamEngine values(`000001, 2023.01.02, 30.80, 30.95, 31.05, 30.05, 30.85, 200, 6160, 0, 0, 0)\ninsert into streamEngine values(`000002, 2023.01.02, 30.81, 30.99, 31.55, 30.15, 30.65, 180, 5545.8, 0, 0, 0)\ninsert into streamEngine values(`000001, 2023.01.03, 30.83, 31.00, 31.35, 30.35, 30.55, 230, 7090.9, 0, 0, 0)\ninsert into streamEngine values(`000002, 2023.01.03, 30.89, 30.85, 31.10, 30.00, 30.45, 250, 7722.5, 0, 0, 0)\ninsert into streamEngine values(`000001, 2023.01.04, 30.90, 30.86, 31.10, 30.40, 30.75, 300, 9270, 0, 0, 0)\ninsert into streamEngine values(`000002, 2023.01.04, 30.85, 30.95, 31.65, 30.55, 31.45, 270, 8329.5, 0, 0, 0)\ninsert into streamEngine values(`000001, 2023.01.05, 30.86, 30.55, 31.35, 29.85, 30.75, 360, 11109.6, 0, 0, 0)\ninsert into streamEngine values(`000002, 2023.01.05, 30.80, 30.95, 31.05, 30.05, 30.85, 200, 6160, 0, 0, 0)\ninsert into streamEngine values(`000001, 2023.01.06, 30.81, 30.99, 31.55, 30.15, 30.65, 180, 5545.8, 0, 0, 0)\ninsert into streamEngine values(`000002, 2023.01.06, 30.83, 31.00, 31.35, 30.35, 30.55, 230, 7090.9, 0, 0, 0)\ninsert into streamEngine values(`000001, 2023.01.07, 30.89, 30.85, 31.10, 30.00, 30.45, 250, 7722.5, 0, 0, 0)\ninsert into streamEngine values(`000002, 2023.01.07, 30.90, 30.86, 31.10, 30.40, 30.75, 300, 9270, 0, 0, 0)\ninsert into streamEngine values(`000001, 2023.01.08, 30.89, 30.85, 31.10, 30.00, 30.45, 250, 7722.5, 0, 0, 0)\ninsert into streamEngine values(`000002, 2023.01.08, 30.90, 30.86, 31.10, 30.40, 30.75, 300, 9270, 0, 0, 0)\n\n// 查看结果\nselect factor from resultTable pivot by dateTime, securityID\n/*\ndateTime                000001 000002\n----------------------- ------ ------\n2023.01.01T00:00:00.000              \n2023.01.02T00:00:00.000              \n2023.01.03T00:00:00.000              \n2023.01.04T00:00:00.000              \n2023.01.05T00:00:00.000 0.5    0     \n2023.01.06T00:00:00.000 0.5    0     \n2023.01.07T00:00:00.000 0      0.5   \n*/\n```\n\n----------------------------------------\n\nTITLE: Retrieving Message ID for Snapshot and Resuming Subscription in DolphinDB\nDESCRIPTION: This snippet shows how to obtain the last processed message ID from a snapshot-enabled stream engine using getSnapshotMsgId, and then resume subscription from that message offset by adding 1 to ensure no data duplication or loss. It demonstrates re-creating the engine, loading the last message ID, and subscribing again at the correct offset to maintain data continuity after system recovery or interruption.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/stream_aggregator.md#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\nofst=getSnapshotMsgId(Agg1)\nprint(ofst)\n>499\n\nsubscribeTable(server=\"\", tableName=\"trades\", actionName=\"Agg1\",offset=ofst+1, handler=appendMsg{Agg1}, msgAsTable=true, handlerNeedMsgId=true)\n```\n\n----------------------------------------\n\nTITLE: 用 Left Semi Join 引擎关联股票与指数行情并计算相关性脚本\nDESCRIPTION: 该脚本创建股票和指数的分钟频率行情表，关联后通过响应式状态引擎计算股票与指数的相关性，包括相关系数等指标。实现实时关联分析，订阅指数和股票行情数据流后，动态更新相关性指标，适合用在高频交易因子研究中。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming-real-time-correlation-processing.md#_snippet_8\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// create table\nshare streamTable(1:0, `Sym`Time`Close, [SYMBOL, TIME, DOUBLE]) as stockKline\nshare streamTable(1:0, `Sym`Time`Close, [SYMBOL, TIME, DOUBLE]) as indexKline\nshare streamTable(1:0, `Time`Sym`Close`Index1Close, [TIME, SYMBOL, DOUBLE, DOUBLE]) as stockKlineAddIndex1\nshare streamTable(1:0, `Sym`Time`Close`Index1Close`Index1Corr, [SYMBOL, TIME, DOUBLE, DOUBLE, DOUBLE]) as output\n\n// create engine: calculate correlation\nrsEngine = createReactiveStateEngine(name=\"calCorr\", dummyTable=stockKlineAddIndex1, outputTable=output, metrics=[<Time>, <Close>, <Index1Close>, <mcorr(ratios(Close)-1, ratios(Index1Close)-1, 3)>], keyColumn=\"Sym\")\n\n// create engine: left join Index1\nljEngine1 = createLeftSemiJoinEngine(name=\"leftJoinIndex1\", leftTable=stockKline, rightTable=indexKline, outputTable=getStreamEngine(\"calCorr\"), metrics=<[Sym, Close, indexKline.Close]>, matchingColumn=`Time)\n\n// subscribe index data\n def appendIndex(engineName, indexName, msg){\n tmp = select * from msg where Sym = indexName\n getRightStream(getStreamEngine(engineName)).append!(tmp)\n }\nsubscribeTable(tableName=\"indexKline\", actionName=\"appendIndex1\", handler=appendIndex{\"leftJoinIndex1\", \"idx1\"}, msgAsTable=true, offset=-1, hash=1)\nsubscribeTable(tableName=\"stockKline\", actionName=\"appendStock\", handler=getLeftStream(ljEngine1), msgAsTable=true, offset=-1, hash=0)\n```\n\n----------------------------------------\n\nTITLE: 创建响应式状态引擎过滤重复门禁事件数据 - DolphinDB\nDESCRIPTION: 利用createReactiveStateEngine函数创建第一个响应式状态引擎，输入原始门禁设备数据表，检测事件码doorEventCode是否变化，只有变化的记录才输出，实现针对重复数据的过滤处理。设置分组列为doorNum，计算指标涵盖事件时间与事件码，并指定输出表绑定到下一级引擎的数据流，支持多级引擎流水线处理。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_engine_anomaly_alerts.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\nreactivEngine1 = createReactiveStateEngine(name=`reactivEngine1,metrics=<[eventDate,doorEventCode]>,\n    dummyTable=objByName(`doorRecord),outputTable= getStreamEngine(\"swEngine\"),keyColumn=`doorNum,\n    filter=<prev(doorEventCode)!=doorEventCode>)\n```\n\n----------------------------------------\n\nTITLE: Creating Fixed ETF Components Portfolio in DolphinDB\nDESCRIPTION: Creates a fixed portfolio of 50 stocks with randomly assigned positions to be used as the components of a single ETF index for IOPV calculation.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_IOPV.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nsymbols = `300073`300363`300373`300474`300682`300769`301029`300390`300957`300763`300979`300919`300037`300832`300866`300896`300751`300223`300676`300274`300413`300496`300661`300782`300142`300759`300595`300285`300146`300207`300316`300454`300529`300699`300628`300760`300601`300450`300433`300408`300347`300124`300122`300059`300033`300015`300014`300012`300003`300750\npositions = rand(76339..145256, 50)\nportfolio = dict(symbols, positions)\n```\n\n----------------------------------------\n\nTITLE: Writing Real-time Data to DolphinDB Stream Table with Python API\nDESCRIPTION: Uses Python API to connect to DolphinDB server and insert data into the level2 stream table. This example simulates real-time data with a sample dataset, but in production, it would be replaced with actual real-time data subscriptions.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/quant_finance_examples.md#_snippet_19\n\nLANGUAGE: Python\nCODE:\n```\nimport dolphindb as ddb\nimport pandas as pd\nimport numpy as np\ndata = [['600007','600104'],[np.datetime64('2019-01-01T20:01:01'),np.datetime64('2019-01-01T20:01:02')],[100.36,99.3],[100.36,101.22], [100.35,100.45],[4138,2],[20,39],[1,5]]\ns = ddb.session()\ns.connect(\"127.0.0.1\",8848,\"admin\",\"123456\")\ns.run(\"tableInsert{level2}\",data)\n```\n\n----------------------------------------\n\nTITLE: Coordinating Complete Ingestion Workflows and Job Submission in DolphinDB Script\nDESCRIPTION: Wraps the complete ingestion pipeline into a main operation function, handling database existence, (re-)creation, and conditional dispatch to single or multi-threaded write jobs. Relies on previously defined functions and job submission API. Inputs are global parameters and partition specs, and the function ensures the full workflow from clean slate to new data. Designed to be called after configuration and authentication.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/multipleValueModeWrite.txt#_snippet_4\n\nLANGUAGE: dolphindb\nCODE:\n```\ndef mainJob(id,startDay,days, ps1, ps2,freqPerDay, numMachinesPerPartition,threads) {\n\tdbName=\"dfs://mvmDemo\"\n\ttableName=\"machines\"\n\tnumMetrics = 50 \n\tif(existsDatabase(dbName))\n\t\tdropDatabase(dbName)\n\n\tcreateDatabase(dbName,tableName, ps1, ps2, numMetrics)\n    if(threads == 1)\n    \tsubmitJob(\"submit_singleThreadWriting\", \"write data\", singleThreadWriting{id, startDay, days,freqPerDay, numMachinesPerPartition,numMetrics,dbName,tableName})\n    else\n    \tsubmitJob(\"submit_multiThreadWriting\", \"write data\", multipleThreadWriting{id, startDay, days,freqPerDay, numMachinesPerPartition,numMetrics, threads,dbName,tableName})\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Multiple DolphinDB Instances on Single Server\nDESCRIPTION: Commands to copy the DolphinDB installation to create multiple instances on the same server for pseudo-high availability configuration.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/service_deployment_and_migration.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncp -r /home/dolphindb_1 /home/dolphindb_2;\ncp -r /home/dolphindb_1 /home/dolphindb_3;\n```\n\n----------------------------------------\n\nTITLE: 分布式线性回归分析股票报价数据\nDESCRIPTION: 该代码对股票报价数据执行分布式线性回归计算，分析spread(出价与要价的比率减1)和quoteSize(买卖量与报价均值的乘积)之间的关系。使用sqlDS定义数据源，然后使用olsEx函数进行回归分析。数据量约10亿条记录(26.5GB)。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/cluster_scaleout_perf_test.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\nds = sqlDS(<select ofr/bid-1 as spread, (bidsiz+ofrsiz)*(ofr+bid)\\2 as quoteSize from quotes where date between 2007.08.01 : 2007.08.05, time between 09:30:00 : 15:59:59, ofr>bid, ofr>0, bid>0, ofr/bid<1.2>)\nolsEx(ds, `spread, `quoteSize, true, 2)\n```\n\n----------------------------------------\n\nTITLE: Loading Sample Table Data from CSV\nDESCRIPTION: Loads the sample CSV data into a DolphinDB table using the inferred schema, with timing information provided.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs InfluxDB/test_dolphindb_big.txt#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 加载 sample table, 用第一个 CSV 中的股票代码频率作为分区范围的依据\nsample_tb = ploadText(FP_SAMPLE_TB, , schema)\n// 用时 40s\n```\n\n----------------------------------------\n\nTITLE: Average Price per Stock and Day within Date Range\nDESCRIPTION: Calculates daily average prices for a specified stock over a date range, filtered by time, providing day-level price summaries.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs InfluxDB/test_dolphindb_big.txt#_snippet_17\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 9. 经典查询：按 [股票代码、日期段、时间段] 过滤, 查询 [每天，时间段内每分钟) 均价\n\ntimer\nselect avg(ofr + bid) / 2.0 as avg_price\nfrom taq \nwhere \n\tsymbol = 'IBM', \n\tdate between 2007.08.01 : 2007.08.07,\n\ttime between 09:30:00 : 16:00:00\n\ngroup by date, minute(time) as minute\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus Scrape Targets for DolphinDB Nodes - YAML\nDESCRIPTION: This configuration block in prometheus.yml lists multiple DolphinDB nodes (control, agent, data) as scrape targets. Prometheus will periodically scrape metrics exposed by each specified DolphinDB node's IP and port.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/cluster_monitor.md#_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\n  # 用于抓取 DolphinDB 集群对服务器资源的使用情况 对应第二套方案   \n  - job_name: \"controlnode1\"\n    static_configs:\n      - targets: [\"xxxxxxxx:xxx\"]#IP:controlnode1端口\n      \n  - job_name: \"controlnode2\"\n    static_configs:\n      - targets: [\"xxxxxxxx:xxx\"]#IP:controlnode2端口\n      \n  - job_name: \"controlnode3\"\n    static_configs:\n      - targets: [\"xxxxxxxx:xxx\"]#IP:controlnode3端口\n      \n  - job_name: \"agentnode1\"\n    static_configs:\n      - targets: [\"xxxxxxxx:xxx\"]#IP:agentnode1端口\n      \n  - job_name: \"agentnode2\"\n    static_configs:\n      - targets: [\"\"xxxxxxxx:xxx\"\"]#IP:agentnode2端口\n      \n  - job_name: \"agentnode3\"\n    static_configs:\n      - targets: [\"\"xxxxxxxx:xxx\"\"]#IP:agentnode3端口\n      \n  - job_name: \"datanode1\"\n    static_configs:\n      - targets: [\"\"xxxxxxxx:xxx\"\"]#IP:datanode1端口\n      \n  - job_name: \"datanode2\"\n    static_configs:\n      - targets: [\"\"xxxxxxxx:xxx\"\"]#IP:datanode2端口\n      \n  - job_name: \"datanode3\"\n    static_configs:\n      - targets: [\"xxxxxxxx:xxx\"]#IP:datanode3端口\n```\n\n----------------------------------------\n\nTITLE: 计算基金间的相关系数\nDESCRIPTION: 使用皮尔逊相关系数矩阵，分析基金组合的联动性，帮助投资者了解基金间的关系，降低重复投资风险。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/public_fund_basic_analysis.md#_snippet_24\n\nLANGUAGE: DolphinDB\nCODE:\n```\ncorrMatrix = pcross(corr, returnsMatrix50)\n// 查看股票型基金间相关系数\ncorrMatrix.loc(fundTypeMap[corrMatrix.rowNames()]==\"股票型\", fundTypeMap[corrMatrix.rowNames()]==\"股票型\")\n```\n\n----------------------------------------\n\nTITLE: Querying CPU Metrics and Alerts from DolphinDB Using DolphinDB Query Language\nDESCRIPTION: DolphinDB queries to load the distributed CPU table and retrieve the latest 100 rows from 'cpu_stream', 'dfs_cpu', and 'cpu_warning_result' tables ordered by descending timestamp. These queries allow users to inspect recent raw CPU streaming data, persisted distributed table data, and detected CPU usage warning alerts. Dependencies: Running DolphinDB with data tables created and populated. Inputs: none. Outputs: latest metric and alert data for monitoring.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_Telegraf_Grafana.md#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndbName = \"dfs://telegraf\"\ndfs_cpu = loadTable(dbName,\"cpu\")\nselect top 100 * from cpu_stream order by timestamp desc\nselect top 100 * from dfs_cpu order by timestamp desc \nselect top 100 * from cpu_warning_result order by timestamp desc\n```\n\n----------------------------------------\n\nTITLE: Subscribing to Distributed Table with Access Control - DolphinDB\nDESCRIPTION: This code demonstrates subscribing to a distributed table from a stream table. It requires TABLE_WRITE permission on the distributed table. Uses `subscribeTable`, `database`, `createPartitionedTable`, `share` and `grant`. Requires admin privileges to create user.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Permission_Management.md#_snippet_36\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(`admin, `123456)\ndbName = \"dfs://valuedb\"\nt = table(10000:0, `time`sym`sumVolume, [TIMESTAMP, SYMBOL, INT])\ninsert into t values(2018.10.08T01:01:01.785+1..600,take(`A,600),1..600) \nif(existsDatabase(dbName)){\n        dropDatabase(dbName)\n}\ndb=database(dbName, VALUE, 1..10)\npt=  db.createPartitionedTable(t, \"pt\", \"sumVolume\").append!(t)\nshare streamTable(1000:0, `time`sym`volume, [TIMESTAMP, SYMBOL, INT]) as trades\ncreateUser(`u2, \"222222\");\ngrant(\"u2\", TABLE_WRITE,\"dfs://valuedb/pt\")\nlogin(`u2, \"222222\")\ndef saveTradesToDFS(mutable dfsTrades, msg): dfsTrades.append!(msg)\nsubscribeTable(tableName=\"trades\", actionName=\"agg1\", offset=0, handler=saveTradesToDFS{pt}, msgAsTable=true);\nlogin(`admin, `123456)\nselect * from loadTable(dbName,`pt)\n```\n\n----------------------------------------\n\nTITLE: Elasticsearch Python Script for Date and Permno Aggregations\nDESCRIPTION: This Python code connects to Elasticsearch using urllib3, constructs a JSON query for aggregating data by date and PERMNO with average ask price, and executes the search request. It outputs the HTTP response status and parsed JSON results, facilitating performance testing of date-based aggregations.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_57\n\nLANGUAGE: Python\nCODE:\n```\ndata = json.dumps({\n    \"group_by_date\": {\n        \"terms\": {\n            \"field\": \"date\",\n            \"size\": 6828\n        },\n        \"aggs\": {\n            \"avg_ask\": {\n                \"avg\": {\"field\": \"ASK\"}\n            },\n            \"max_bid\": {\n                \"max\": {\"field\": \"BID\"}\n            }\n        }\n    }\n}).encode(\"utf-8\")\n\nr = http.request(\"GET\", \"http://localhost:9200/uscsv/_search\", body=data,\n                 headers={'Content-Type': 'application/json'})\nprint(r.status)\nprint(json.loads(r.data.decode()))\n```\n\n----------------------------------------\n\nTITLE: Grouping Trades by Date (Multiple Columns) - DolphinDB\nDESCRIPTION: This DolphinDB script groups the \"trades\" table by \"trade_date\" and calculates the maximum open price and the sum of the previous close price for each group. It showcases grouping with multiple aggregate functions based on the date. The `timer(10)` function measures the query performance, and `clearAllCache()` clears the cache beforehand.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_22\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//按时间分组（多列）\ntimer(10) select max(open), sum(pre_close) from trades group by trade_date\n```\n\n----------------------------------------\n\nTITLE: Selecting records with price higher than average using contextby in SQL\nDESCRIPTION: This code snippet demonstrates how to use `contextby` within a SQL query to select records where the price is higher than the average price for the corresponding symbol. It creates a table `t1` and then uses `contextby` to calculate the average price for each symbol and filters for records where the individual price exceeds this average.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_44\n\nLANGUAGE: shell\nCODE:\n```\nt1=table(trade_date,sym,qty,price);\nselect trade_date, sym, qty, price from t1 where price > contextby(avg, price,sym);\n```\n\n----------------------------------------\n\nTITLE: Element-wise and Scalar Calculations with Array Vector Variables - DolphinDB Script\nDESCRIPTION: These snippets illustrate how to perform arithmetic operations between Array Vectors and scalars in DolphinDB. The scalar value is broadcast to each element or row of the Array Vector or Columnar Tuple. Operations are compatible with DolphinDB tables as well. Entities to operate on must be properly initialized; outputs are the result of arithmetic for each row or tuple. No direct calculation is allowed between Fast Array Vector and Columnar Tuple.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Array_Vector.md#_snippet_8\n\nLANGUAGE: DolphinDB\nCODE:\n```\nx = array(INT[], 0).append!([1 2 3, 4 5, 6 7 8, 9 10])\nz = x + 1\n/* z\n[[2,3,4],[5,6],[7,8,9],[10,11]]\n*/\n\ny = [1 2 3, 4 5, 6 7 8, 9 10].setColumnarTuple!()\nz = y + 1\n/* z\n([2,3,4],[5,6],[7,8,9],[10,11])\n*/\n\nt = table(1 2 3 4 as id, x as x, y as y)\nnew_t = select *, x + 1 as new_x, y + 1 as new_y from t\n/* new_t\nid x       y       new_x   new_y  \n-- ------- ------- ------- -------\n1  [1,2,3] [1,2,3] [2,3,4] [2,3,4]\n2  [4,5]   [4,5]   [5,6]   [5,6]  \n3  [6,7,8] [6,7,8] [7,8,9] [7,8,9]\n4  [9,10]  [9,10]  [10,11] [10,11]\n*/\n```\n\n----------------------------------------\n\nTITLE: Submitting Parallel Job 1 (Single User) in DolphinDB\nDESCRIPTION: This code submits the `parJob1` function as a job named \"parallJob_single_nine\" under the job group \"parallJob1\".  This is designed for single-user execution.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_factor.txt#_snippet_14\n\nLANGUAGE: DolphinDB\nCODE:\n```\n/**\n * 提交1个job（单用户）\n */\nsubmitJob(\"parallJob1\", \"parallJob_single_nine\", parJob1)\n```\n\n----------------------------------------\n\nTITLE: DolphinDB Grouping by Stock Code (Multiple Columns)\nDESCRIPTION: This DolphinDB script performs aggregation with multiple columns in the select statement and groups by 'TICKER'. It calculates both the average of the 'VOL' column and the maximum of the 'OPENPRC' column, grouping the results by 'TICKER'. The `timer()` function measures the execution performance across 10 runs. It tests the efficiency of multi-column aggregations in DolphinDB.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_42\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\n//按股票代码分组（多列）\ntimer(10) select avg(VOL), max(OPENPRC) from trades group by TICKER\n```\n\n----------------------------------------\n\nTITLE: Handler Data Format Control in subscribeTable with msgAsTable Parameter in DolphinDB\nDESCRIPTION: Shows how the msgAsTable boolean parameter affects the data format of messages passed to the subscription handler. If true, messages are passed as tables that can be processed with SQL; if false, messages are passed as tuples representing columns. Two handler functions demonstrate the different expected data formats and how to insert messages into respective subscription tables accordingly. Requires the publish stream tables and subscriptions configured with correct msgAsTable values. Inputs are stream data messages; outputs are subscription tables updated with handled data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_tutorial.md#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef myhandler1(table){\n\tsubTable1.append!(table)\n}\ndef myhandler2(tuple){\n\ttableInsert(subTable2,tuple[0],tuple[1])\n}\nshare streamTable(10000:0,`timestamp`temperature, [TIMESTAMP,DOUBLE]) as pubTable\nshare streamTable(10000:0,`ts`temp, [TIMESTAMP,DOUBLE]) as subTable1\nshare streamTable(10000:0,`ts`temp, [TIMESTAMP,DOUBLE]) as subTable2\n\ntopic1 = subscribeTable(tableName=\"pubTable\", actionName=\"act1\", offset=-1, handler=myhandler1, msgAsTable=true)\ntopic2 = subscribeTable(tableName=\"pubTable\", actionName=\"act2\", offset=-1, handler=myhandler2, msgAsTable=false)\n\nvtimestamp = 1..10\nvtemp = 2.0 2.2 2.3 2.4 2.5 2.6 2.7 0.13 0.23 2.9\ntableInsert(pubTable,vtimestamp,vtemp)\n```\n\n----------------------------------------\n\nTITLE: 创建宽表结构的分布式采集数据表 - DolphinDB 脚本\nDESCRIPTION: 该代码创建了一个宽表结构的采集表，用于存储设备的时间戳、设备ID及5000个测点的浮点型数据。流程包含定义字段名称、字段类型，初始化空表结构，进而基于前述组合分区数据库创建分布式表并指定双重分区列及排序键。实现高性能存储具有多千测点的高频时序数据。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/high_freq_data_storage_and_analysis.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//新建“采集表”，宽表，单条记录 5002 个字段\nn=5000   //采集点数量\ntablename=\"collect\"          //表名\ncolNames=`ts`deviceid <- (\"v\"+string(1..5000))      //表字段\ncolTypes=`TIMESTAMP`SYMBOL <- take(`FLOAT,5000)  //字段类型\n//创建表t\nt=table(1:0,colNames,colTypes) \n\n//构建分布式表\npt=createPartitionedTable(dbHandle=db,table=t,tableName=tablename,partitionColumns=`ts`deviceid,sortColumns=`deviceid`ts)\n```\n\n----------------------------------------\n\nTITLE: Generating Aggregation Metadata (DolphinDB Script)\nDESCRIPTION: Calls the `createAggMetaCode` function with the defined `features` dictionary to generate the list of `sqlCol` metadata (`aggMetaCode`) and the corresponding list of output column names (`metaCodeColName`) for the aggregated features.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/metacode_derived_features/metacode_derived_features_streaming.txt#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\naggMetaCode, metaCodeColName = createAggMetaCode(features)\n```\n\n----------------------------------------\n\nTITLE: Implementing mathWghtSkew Factor in DolphinDB\nDESCRIPTION: This code implements the mathWghtSkew factor using DolphinDB's rowWavg for weighted row calculations. It first defines a helper function mathWghtCovar for weighted covariance, then uses it to calculate weighted skewness.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Python+HDF5_vs_DolphinDB.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef mathWghtCovar(x, y, w){\n\tv = (x - rowWavg(x, w)) * (y - rowWavg(y, w))\n\treturn rowWavg(v, w)\n}\ndef mathWghtSkew(x, w){\n\tx_var = mathWghtCovar(x, x, w)\n\tx_std = sqrt(x_var)\n\tx_1 = x - rowWavg(x, w)\n\tx_2 = x_1*x_1\n\tlen = size(w)\n\tadj = sqrt((len - 1) * len) \\ (len - 2)\n\tskew = rowWsum(x_2, x_1) \\ (x_var * x_std) * adj \\ len\n\treturn iif(x_std==0, 0, skew)\n}\n```\n\n----------------------------------------\n\nTITLE: Calculating OLS Residual - DolphinDB\nDESCRIPTION: Calculates the OLS residual for each row of the table `t` using the `ols` function. The table columns used in the OLS calculations are first converted into a matrix, then residuals are computed using `each` and a partially applied lambda function. The results are stored in a new column named `residual` in the table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/func_progr_cases.md#_snippet_23\n\nLANGUAGE: DolphinDB\nCODE:\n```\nt=table(2020.11.01 2020.11.02 as date, `IBM`MSFT as ticker, 1.0 2 as past1, 2.0 2.5 as past3, 3.5 7 as past5, 4.2 2.4 as past10, 5.0 3.7 as past20, 5.5 6.2 as past30, 7.0 8.0 as past60)\n\nmt = matrix(t[`past1`past3`past5`past10`past20`past30`past60]).transpose()\nt[`residual] = each(def(y, x){ return ols(y, x, true, 2).ANOVA.SS[1]}{,benchX}, mt)\n```\n\n----------------------------------------\n\nTITLE: DolphinDB-Python API for Subscribing to Stream Data\nDESCRIPTION: This Python code snippet demonstrates how to use the DolphinDB-Python API to subscribe to a stream table on a DolphinDB server. It specifies the host, port, table name, and a callback function to handle incoming stream data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practice_for_factor_calculation.md#_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\ncurrent_ddb_session.subscribe(host=DDB_datanode_host,tableName=stream_table_shared_name,actionName=action_name,offset=0,resub=False,filter=None,port=DDB_server_port,\nhandler=python_callback_handler,#此处传入python端要接收消息的回调函数\n)\n```\n\n----------------------------------------\n\nTITLE: Loading and Preparing Plugins and Replay Data for Matching Engine Simulation in DolphinDB Script\nDESCRIPTION: This snippet details plugin loading for a matching engine simulator and a custom algorithmic trading test module. It demonstrates reading replay results from disk per thread, extracting relevant symbol lists, and building message lists for subsequent simulation phases. Prerequisites include previously stored replayed data tables on disk (with BLOB fields), and plugin text files available in the designated file paths. Outputs are two lists: one of tick-data tables for each thread and another of assigned symbols, ready for job-based multi-threaded simulation.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/matching_engine_simulator.md#_snippet_8\n\nLANGUAGE: DolphinDB\nCODE:\n```\nloadPlugin(\"<path>/PluginMatchingEngineSimulator.txt\")\nloadPlugin(\"<path>/MatchEngineTest.txt\")\n\nthread_num = 20\nmarket = \"sz\"\nmarket_name = \"XSHE\"\n\nmessagesList = [] // 存储每个线程的行情数据表\nsymbolList = []  // 存储每个线程的股票代码\nmessage_num = 0\nfor (i in 1..thread_num) {\n    // 从磁盘表中读取回放后的行情数据表，并获取表中的股票代码\n    messages = select * from loadTable(\"<path>/\" + market + \"_messages_\" + thread_num + \"_part\", \"MatchEngineTest\"+i)\n    symbols = exec distinct left(msgBody, 6) from messages\n    message_num += messages.size()\n    messagesList.append!(messages)\n    symbolList.append!(symbols)\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Multiprocessing Worker and Task Splitter - Python\nDESCRIPTION: Defines `pool_func`, a worker function intended for multiprocessing, which processes a list of stock files within a given directory, calculates the 'Before Closing Volume Percentage' for each file, and stores the results in a DataFrame, handling potential errors. Also defines the `multi_task_split` class, a utility for dividing a list of tasks (data) among a specified number of processes, ensuring efficient distribution for parallel execution.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/DolphinDB_Python_Parser_Intro_for_Quantitative_Finance/因子实现_Python版本/当日尾盘成交占比.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef pool_func(tick_obj, trade_path_obj):\n    single_tick_res = pd.DataFrame(columns=[\"DATE\",\"BCVP\"])\n    tmp_date = trade_path_obj.split('/')[-2]\n    # print(tmp_date)\n    tmp_date = tmp_date[0:4] + \"-\" + tmp_date[4:6] + \"-\" + tmp_date[6:8]\n    # print(tmp_date)\n    for tick in tqdm(tick_obj):\n        single_tick_res.at[tick[:6], \"DATE\"] = tmp_date\n        try:\n            df = pd.read_csv(os.path.join(trade_path_obj, tick))\n\n            Indicator = beforeClosingVolumePercent(df)\n            # print(Indicator)\n            # print(\"开盘后大单净买入占比:\", Indicator)\n            single_tick_res.at[tick[:6], \"BCVP\"] = Indicator\n\n        except Exception as error:\n            single_tick_res.at[tick[:6], \"BCVP\"] = np.nan\n            continue\n\n    return single_tick_res\n\n\nclass multi_task_split:\n\n    def __init__(self, data, processes_to_use):\n        self.data = data\n        self.processes_to_use = processes_to_use\n\n    def num_of_jobs(self):\n        return min(len(self.data), self.processes_to_use, multiprocessing.cpu_count())\n\n    def split_args(self):\n        q, r = divmod(len(self.data), self.num_of_jobs())\n        return (self.data[i * q + min(i, r): (i + 1) * q + min(i + 1, r)] for i in range(self.num_of_jobs()))\n```\n\n----------------------------------------\n\nTITLE: Creating Partitioned Table for Cleaned Snapshot Data with Arrays in DolphinDB\nDESCRIPTION: Creates a distributed database table for storing cleaned market snapshot data. This table uses array types for order book data to efficiently store multi-level market depth information, reducing the number of columns compared to the raw data format while maintaining the same composite partitioning scheme.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Best_Practices_for_DolphinDB_and_Python_AirFlow.md#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\nmodule processSnapshot::createSnapshot_array\n\n//创建清洗后的 snapshot 数据存储表\ndef createProcessTable(dbName, tbName){\n\tif(existsDatabase(dbName)){\n\t\tdropDatabase(dbName)\n\t}\n\tdb1 = database(, VALUE, 2020.01.01..2021.01.01)\n\tdb2 = database(, HASH, [SYMBOL, 50])\n\t//按天和股票代码组合分区\n\tdb = database(dbName,COMPO,[db1,db2],engine='TSDB')\n\tcolName = [\"SecurityID\",\"DateTime\",\"PreClosePx\",\"OpenPx\",\"HighPx\",\"LowPx\",\"LastPx\",\"TotalVolumeTrade\",\"TotalValueTrade\",\"InstrumentStatus\",\"BidPrice\",\"BidOrderQty\",\"BidNumOrders\",\"BidOrders\",\"OfferPrice\",\"OfferOrderQty\",\"OfferNumOrders\",\"OfferOrders\",\"NumTrades\",\"IOPV\",\"TotalBidQty\",\"TotalOfferQty\",\"WeightedAvgBidPx\",\"WeightedAvgOfferPx\",\"TotalBidNumber\",\"TotalOfferNumber\",\"BidTradeMaxDuration\",\"OfferTradeMaxDuration\",\"NumBidOrders\",\"NumOfferOrders\",\"WithdrawBuyNumber\",\"WithdrawBuyAmount\",\"WithdrawBuyMoney\",\"WithdrawSellNumber\",\"WithdrawSellAmount\",\"WithdrawSellMoney\",\"ETFBuyNumber\",\"ETFBuyAmount\",\"ETFBuyMoney\",\"ETFSellNumber\",\"ETFSellAmount\",\"ETFSellMoney\"]\n\tcolType = [\"SYMBOL\",\"TIMESTAMP\",\"DOUBLE\",\"DOUBLE\",\"DOUBLE\",\"DOUBLE\",\"DOUBLE\",\"INT\",\"DOUBLE\",\"SYMBOL\",\"DOUBLE[]\",\"INT[]\",\"INT[]\",\"INT[]\",\"DOUBLE[]\",\"INT[]\",\"INT[]\",\"INT[]\",\"INT\",\"INT\",\"INT\",\"INT\",\"DOUBLE\",\"DOUBLE\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"DOUBLE\",\"INT\",\"INT\",\"DOUBLE\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\"]\n\tschemaTable = table(1:0, colName, colType)\n\tdb.createPartitionedTable(table=schemaTable, tableName=tbName, partitionColumns=`DateTime`SecurityID, compressMethods={DateTime:\"delta\"}, sortColumns=`SecurityID`DateTime, keepDuplicates=ALL)\n}\n```\n\n----------------------------------------\n\nTITLE: Performance Testing: Filtered Price and Time Cyclic Averaging\nDESCRIPTION: Calculates average spread for selected symbols within a specified date and time range, grouped by symbol and minute, measuring query duration.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs InfluxDB/test_dolphindb_big.txt#_snippet_14\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 6. 经典查询：按 [多个股票代码、日期，时间范围、报价范围] 过滤，查询 [股票代码、时间、买入价、卖出价]\ntimer\nselect symbol, time, bid, ofr\nfrom taq\nwhere\n\tsymbol in ('IBM', 'MSFT', 'GOOG', 'YHOO'), \n\tdate = 2007.08.03, \n\ttime between 09:30:00 : 14:30:00, \n\tbid > 0, \n\tofr > bid\n```\n\n----------------------------------------\n\nTITLE: Starting Agent Services on Data Nodes\nDESCRIPTION: Executes 'startAgent.sh' script on server 1, 2, and 3 to launch agent processes that connect to controllers, facilitating data node participation in the cluster.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/service_deployment_and_migration.md#_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\nsh startAgent.sh\n```\n\n----------------------------------------\n\nTITLE: Creating Index Components Table in DolphinDB\nDESCRIPTION: Creates a partitioned table with TSDB engine for storing index component data synchronized from MySQL. The table is configured with keepDuplicates=LAST to ensure data idempotence when duplicate data is written.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Debezium_and_Kafka_data_sync.md#_snippet_17\n\nLANGUAGE: SQL\nCODE:\n```\ndef createIndexComDB(dbName){\n\tif(existsDatabase(dbName)){\n\tdropDatabase(dbName)\n\t}\n\tdatabase(directory=dbName, partitionType=RANGE, partitionScheme= 1999.01M + (0..26)*12,engine=\"TSDB\")\n}\ndef createIndexCom(dbName,tbName){\n\tdb=database(dbName)\n             if(existsTable(dbName, tbName)){\n                   db.dropTable(tbName)\t\n\t}\n\tmtable=table(100:0, `trade_date`code`effDate`indexShortName`indexCode`secShortName`exchangeCD`weight`timestamp`flag, [TIMESTAMP,SYMBOL,TIMESTAMP,SYMBOL,SYMBOL,SYMBOL,SYMBOL,DOUBLE,TIMESTAMP,INT]);\n\tdb.createPartitionedTable(table=mtable, tableName=tbName, partitionColumns=`trade_date,sortColumns=`code`indexCode`flag`trade_date,compressMethods={trade_date:\"delta\"},keepDuplicates=LAST)\n}\ncreateIndexComDB(\"dfs://index_data\")\ncreateIndexCom(\"dfs://index_data\",`index_components)\n```\n\n----------------------------------------\n\nTITLE: 加载基金净值表数据\nDESCRIPTION: 从分布式文件系统加载包含基金净值的表对象，为后续分析提供基础数据。该代码未加载分区表的全部数据，主要用于加载元数据，节省内存资源。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/public_fund_basic_analysis.md#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\nfundNetValue = loadTable(\"dfs://publicFundDB\", \"publicFundNetValue\")\n```\n\n----------------------------------------\n\nTITLE: Extracting Various Financial Factors Using DolphinDB\nDESCRIPTION: Defines getFactor function that extracts multiple financial metrics from a results table grouping by fundNum. It computes annual return, volatility, skew, kurtosis, Sharpe ratio, max drawdown, drawdown ratio, beta, and alpha over a specified date range and for given fund numbers. Outputs a table with fund-level aggregated metrics. Inputs are result table and list of fund symbols; outputs a table with factors.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_factor_by_ddb/fund_factor_ten.txt#_snippet_10\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef getFactor(result2, symList){\n\tReturn = select fundNum, \n\t\t    getAnnualReturn(value) as annualReturn,\n\t\t    getAnnualVolatility(value) as annualVolRat,\n\t\t    getAnnualSkew(value) as skewValue,\n\t\t    getAnnualKur(value) as kurValue,\n\t\t    getSharp(value) as sharpValue,\n\t\t    getMaxDrawdown(value) as MaxDrawdown,\n              getDrawdownRatio(value) as DrawdownRatio,\n              getBeta(value, price) as Beta,\n              getAlpha(value, price) as Alpha\t\n             from result2\n             where TradeDate in 2018.05.24..2021.05.27 and fundNum in symList group by fundNum\n }\n```\n\n----------------------------------------\n\nTITLE: Defining and Training CNN with DDBDataLoader in Python\nDESCRIPTION: This code defines a simple CNN neural network, loss function, and optimizer. It then iterates through the DDBDataLoader to retrieve data, and inputs it into the network for training. This showcases how DDBDataLoader can be used seamlessly like a torch DataLoader for training. It relies on the `torch` library.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_AI_DataLoader_for_Deep_Learning.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmodel = SimpleNet()\nmodel = model.to(\"cuda\")\nloss_fn = nn.MSELoss()\nloss_fn = loss_fn.to(\"cuda\")\noptimizer = torch.optim.Adam(model.parameters(), lr=0.05)\nnum_epochs = 10\n\nmodel.train()\nfor epoch in range(num_epochs):\n    for X, y in dataloader:\n        X = X.to(\"cuda\")\n        y = y.to(\"cuda\")\n        y_pred = model(X)\n        loss = loss_fn(y_pred, y)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n```\n\n----------------------------------------\n\nTITLE: Selecting and Ordering Trade Data in DolphinDB\nDESCRIPTION: This code snippet selects all columns from the 'trade' table stored in the DFS named 'dfs://trade'. The selection is filtered based on the 'TradeTime' column, retaining data within a time range of 09:30:00.000 to 15:00:00.000, and orders the results by 'TradeTime' and then 'SecurityID'. The output is assigned to the variable 't'.  Requires a DolphinDB server and a table named \"trade\" within a DFS partition named \"dfs://trade\".\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/streaming_capital_flow_order_by_order/06.历史数据回放.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//replay history data\nt = select * from loadTable(\"dfs://trade\", \"trade\") where time(TradeTime) between 09:30:00.000 : 15:00:00.000 order by TradeTime, SecurityID\n```\n\n----------------------------------------\n\nTITLE: Start DolphinDB Single Node Foreground and Background - Shell\nDESCRIPTION: Commands to start the DolphinDB single node server either in the foreground (`./dolphindb`) or background mode via a shell script (`sh startSingle.sh`). It also includes a process verification command (`ps aux|grep dolphindb`) to confirm successful background startup by checking for running processes related to dolphindb.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ARM_standalone_deploy.md#_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\n./dolphindb\n```\n\nLANGUAGE: Shell\nCODE:\n```\nsh startSingle.sh\n```\n\nLANGUAGE: Shell\nCODE:\n```\nps aux|grep dolphindb\n```\n\n----------------------------------------\n\nTITLE: Rolling Window Function Null Value Processing Example in DolphinDB\nDESCRIPTION: Illustrates the behavior differences between rolling and moving window aggregate functions regarding NULL values in DolphinDB. The rolling function does not output leading NULLs for the first (window-1) elements, unlike moving which does. The example creates a table with NULLs in a volume column and computes sliding sums with both functions, showing rolling returns shorter arrays starting immediately from the first complete window while moving includes leading NULLs. Additionally, it demonstrates rolling with a window of 3 and step size of 2, showing partial last window omitted due to insufficient elements. Inputs include a DolphinDB table with mixed NULL and integer data; outputs are arrays with partial sums following described null handling. Dependencies include the DolphinDB environment and rolling/moving function implementations.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/window_cal.md#_snippet_26\n\nLANGUAGE: DolphinDB\nCODE:\n```\nvol=1 2 3 4 NULL NULL NULL 6 7 8\nt= table(vol)\n\n// rolling window sum with window size 3\nrolling(sum,t.vol,3)\n\n// moving window sum with window size 3\nmoving(sum,t.vol,3)\n\n// rolling window sum with window size 3, step 2\nrolling(sum,t.vol,3,2)\n```\n\n----------------------------------------\n\nTITLE: Querying - Group by, Aggregation (Single Dimension) DolphinDB\nDESCRIPTION: This snippet queries for the maximum battery temperature for each device. It uses `group by` to perform the aggregation, demonstrating the calculation of a maximum value within grouped data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs TimescaleDB/test_dolphindb_small.txt#_snippet_16\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 7. 聚合查询.单分区维度.max：设备电池最高温度\ntimer\nselect max(battery_temperature)\nfrom readings\ngroup by device_id\n```\n\n----------------------------------------\n\nTITLE: Updating Existing Columns with SQL\nDESCRIPTION: Demonstrates updating existing columns in a memory table using SQL UPDATE statements.  Includes updating all rows and updating rows based on a WHERE clause.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/partitioned_in_memory_table.md#_snippet_18\n\nLANGUAGE: DolphinDB\nCODE:\n```\nupdate trades set qty1=qty1+10;\n\nupdate trades set qty1=qty1+10 where sym=`IBM;\n```\n\n----------------------------------------\n\nTITLE: Using Price Transform Functions in DolphinDB\nDESCRIPTION: Price transformation functions in the DolphinDB ta module. These functions convert OHLC (Open, High, Low, Close) price data into alternative price representations that can be used for further technical analysis.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ta.md#_snippet_10\n\nLANGUAGE: DolphinDB\nCODE:\n```\navgPrice(open, high, low, close)\nmedPrice(high, low)\ntypPrice(high, low, close)\nwclPrice(high, low, close)\n```\n\n----------------------------------------\n\nTITLE: Collecting wheel packages and generating requirements.txt\nDESCRIPTION: Uses pip wheel to download all dependencies for dolphinindb into current directory, then performs pip freeze to save dependency list into requirements.txt, facilitating environment recreation.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/python_api_install_offline.md#_snippet_10\n\nLANGUAGE: Shell Script\nCODE:\n```\npip wheel dolphindb\npip freeze dolphindb > requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Verifying DolphinDB Version\nDESCRIPTION: This DolphinDB script checks and displays the current DolphinDB server version after an upgrade. This confirms the upgrade was successful.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/single_machine_cluster_deploy.md#_snippet_16\n\nLANGUAGE: DolphinDB\nCODE:\n```\nversion()\n```\n\n----------------------------------------\n\nTITLE: Calculating RS Factor in DolphinDB\nDESCRIPTION: This function, `calAllRs2`, calculates a specific factor (factor1) based on rolling window calculations applied to the input `mret` (likely market return data).  It uses functions like `cumsum`, `avg`, `max`, `min`, `stdp`, `nullFill`, and `log` within a rolling window.  The result is a table containing `fundNum`, `knum`, and the calculated `factor1`.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_factor.txt#_snippet_10\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef calAllRs2(mret, symList, k){\n        rowCount = mret.rows()/k * k\n        demeanCum = rolling(cumsum, mret[0:rowCount,] - each(stretch{, rowCount}, rolling(avg, mret, k, k)), k, k)\n        a = rolling(max, demeanCum, k, k) - rolling(min, demeanCum, k, k)\n        RS = nullFill!(a/rolling(stdp, mret, k, k), 1.0).mean().log()\n        return table(symList as fundNum, take(log(k), symList.size()) as knum, RS as factor1)\n}\n```\n\n----------------------------------------\n\nTITLE: Alternative Top N Query Using Limit in DolphinDB\nDESCRIPTION: Alternative SQL query using the limit clause instead of top to find the devices with highest average values. Both approaches achieve the same result with slightly different syntax.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/iot_examples.md#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect id \nfrom (\n\tselect avg(value) as avg\n\tfrom sensors \n\twhere mod(id, 50)=1, datetime between 2020.09.01T00:00:00 : 2020.09.01T00:59:59  \n\tgroup by id order by avg desc\n\t) limit 3\n```\n\n----------------------------------------\n\nTITLE: Creating a Partitioned Database and Table in DolphinDB\nDESCRIPTION: This script logs into the DolphinDB server, defines a DFS database path, creates a VALUE-partitioned database using the `database` function, and then creates a partitioned table named `tb` within that database based on the schema of the `trades` table (presumably created in a previous step). The table is partitioned by the `id` column.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_csv.md#_snippet_7\n\nLANGUAGE: dolphindb\nCODE:\n```\nlogin(`admin,`123456)\ndbPath=\"dfs://DolphinDBdatabase\"\ndb=database(dbPath,VALUE,1..10000)\ntb=db.createPartitionedTable(trades,`tb,`id);\n```\n\n----------------------------------------\n\nTITLE: Creating a basic scheduled job in DolphinDB\nDESCRIPTION: Example showing the scheduleJob function syntax, which creates a scheduled job to calculate the maximum temperature for a device every day at midnight.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/scheduledJob.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef getMaxTemperature(deviceID){\n    maxTemp=exec max(temperature) from loadTable(\"dfs://dolphindb\",\"sensor\")\n            where ID=deviceID ,ts between (today()-1).datetime():(today().datetime()-1)\n    return  maxTemp\n}\nscheduleJob(`testJob, \"getMaxTemperature\", getMaxTemperature{1}, 00:00m, today(), today()+30, 'D');\n```\n\n----------------------------------------\n\nTITLE: Cluster Partition State Check Query\nDESCRIPTION: DolphinDB query to verify if all cluster partitions are in 'COMPLETE' state before proceeding with upgrade, ensuring cluster health and data integrity.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/gray_scale_upgrade_ha.md#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect * from rpc(getControllerAlias(), getClusterChunksStatus) where state != \"COMPLETE\"\n```\n\n----------------------------------------\n\nTITLE: Setting DolphinDB Data and Compute Node Parameters in Java-Like Configuration Format\nDESCRIPTION: This configuration snippet illustrates key tuning parameters within the cluster.cfg file that govern the global behavior of data and compute nodes in the DolphinDB cluster. Parameters include resource limits like maximum memory size (maxMemSize), maximum concurrent connections (maxConnections), number of worker threads (workerNum), and caching sizes for OLAP and TSDB engines. Features like newValuePartitionPolicy and chunk granularity configuration enable fine control over data partitioning and storage. Proper adjustment of these parameters should reflect the underlying server hardware capabilities to optimize cluster performance.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/single_machine_cluster_deploy.md#_snippet_21\n\nLANGUAGE: Java\nCODE:\n```\nmaxMemSize=32\nmaxConnections=512\nworkerNum=4\nmaxBatchJobWorker=4\nOLAPCacheEngineSize=2\nTSDBCacheEngineSize=1\nnewValuePartitionPolicy=add\nmaxPubConnections=64\nsubExecutors=4\nlanCluster=0\nenableChunkGranularityConfig=true\n```\n\n----------------------------------------\n\nTITLE: Loading and Creating Countries Table in DolphinDB\nDESCRIPTION: Loads country data from CSV and creates the 'countries' table with country ID, name, and region association. The data is appended for geographic analysis within HR.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/Standard_SQL_in_DolphinDB/create_db_table_sql.txt#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\ncountries_tmp=loadText(dir+\"COUNTRIES.csv\")\ncreate table \"dfs://hr\".\"countries\" (\n\tCOUNTRY_ID SYMBOL,\n\tCOUNTRY_NAME STRING,\n\tREGION_ID INT\n)\ncountries= loadTable(\"dfs://hr\", \"countries\")\ncountries.append!(countries_tmp)\nselect * from countries\n```\n\n----------------------------------------\n\nTITLE: Aggregating Employee Salary by Department (Oracle Dialect) - SQL\nDESCRIPTION: This SQL query aggregates employee salary information by department, using the Oracle-specific `decode` function to categorize job titles. It calculates the number of employees, total salary, average salary, and maximum salary for each department.  It requires Oracle dialect compatibility to be enabled.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Standard_SQL_in_DolphinDB.md#_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\nselect   \n  d.department_id, \n  d.department_name,  \n  count(a.employee_id) as num_of_employee_id,\n  sum(a.salary) as total_salary,  \n  avg(a.salary) as avg_salary,  \n  max(a.salary) as max_salary,\n  decode(a.job_id, 'IT_PROG' , 'Programmer', 'FI_ACCOUNT', 'Accountant', 'Others') as job_title\nfrom employees a \ninner join departments d\non a.department_id = d.department_id  \ngroup by   \n  d.department_id,  \n  d.department_name,  \n  decode(a.job_id, 'IT_PROG' , 'Programmer', 'FI_ACCOUNT', 'Accountant', 'Others') as job_title\n```\n\n----------------------------------------\n\nTITLE: Running Node Exporter - Shell\nDESCRIPTION: This command starts the Node Exporter application in the background, making server metrics available for Prometheus. The `--web.listen-address` flag specifies the IP address and port where Node Exporter will listen for connections.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/cluster_monitor.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\nnohup ./node_exporter --web.listen-address IP:Port &\n```\n\n----------------------------------------\n\nTITLE: Analyzing Stock Frequency Data for Bucketing\nDESCRIPTION: Counts stock symbols in the sample table, then calculates cut points for data bucketing into 100 partitions based on stock trading frequency.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs InfluxDB/test_dolphindb_big.txt#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 用时 40s\nsample_freq_tb = select count(*) from sample_tb group by symbol\n\nsample_tb = NULL\n\n// 8369 rows, [symbol, count], 分到 100 个 buckets\nBIN_NUM = 100\n\nbuckets = cutPoints(sample_freq_tb.symbol, BIN_NUM, sample_freq_tb.count)\n// [A, ABL, ACU, ..., ZZZ], 101 个边界\nbuckets[BIN_NUM] = `ZZZZZZ\t\t// 调整最右边界\n```\n\n----------------------------------------\n\nTITLE: Aggregating Average Price by Symbol using urllib3 in Python\nDESCRIPTION: This function uses `urllib3` to send a custom search query to Elasticsearch. It filters documents by a specific DATE and performs a terms aggregation on the SYMBOL field, calculating the average OFR price for each symbol. The query is sent as a JSON body in a GET request.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_62\n\nLANGUAGE: python\nCODE:\n```\nimport urllib3\nimport json\n\ndef search_2():\n    http = urllib3.PoolManager()\n    data = json.dumps({\n        \"query\": {\n            \"constant_score\": {\n                \"filter\": {\n                    \"term\": {\n                        \"DATE\": \"20070809\"\n                    }\n                }\n            }\n        },\n        \"aggs\": {\n            \"group_by_symbol\": {\n                \"terms\": {\n                    \"field\": \"SYMBOL\",\n                    \"size\": 8374\n                },\n                \"aggs\": {\n                    \"avg_price\": {\n                        \"avg\": {\"field\": \"OFR\"}\n                    }\n                }\n            }\n        },\n        \"_source\": [\"\"]\n    }).encode(\"utf-8\")\n    r = http.request(\"GET\", \"http://localhost:9200/hundred/_search\", body=data,\n                     headers={'Content-Type': 'application/json'})\n    print(r.status)\n    print(json.loads(r.data.decode()))\n```\n\n----------------------------------------\n\nTITLE: Calculate stock return matrix using pivot\nDESCRIPTION: This DolphinDB script calculates the stock return matrix using `exec` and `pivot by`. It calculates the percentage change (`pct_chg`) for each stock and then pivots the data by `trade_date` and `ts_code` to create a matrix where rows represent dates and columns represent stocks.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nretMatrix=exec pct_chg/100 as ret from daily_line pivot by trade_date, ts_code\n```\n\n----------------------------------------\n\nTITLE: Comparing Computational Complexity of Common DolphinDB Window Functions\nDESCRIPTION: This snippet presents an empirical time comparison of the execution performance between moving, rolling, and m-series window functions for a large vector of one million normal-distributed values. The documented complexities note that m-series and tm-series functions are optimized with O(n) complexity by incremental updates per sliding step, while functions like mrank are O(mn) due to full re-computations. The example shows timers measuring average calculations using avg function on a window size of 10, confirming that mavg is significantly faster than moving or rolling due to its incremental computation approach. Dependencies include DolphinDB statistical functions and timer utility. Inputs include a large numerical vector; outputs are execution times illustrating performance differences.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/window_cal.md#_snippet_27\n\nLANGUAGE: DolphinDB\nCODE:\n```\nn=1000000\nx=norm(0,1, n);\n\n// moving\n timer moving(avg, x, 10);\n\n// rolling\n timer moving(avg, x, 10);\n\n// mavg\n timer mavg(x, 10);\n```\n\n----------------------------------------\n\nTITLE: 计算年度化收益和波动率\nDESCRIPTION: 利用日收益率计算年化收益、波动率，用于夏普比率的评估。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/public_fund_basic_analysis.md#_snippet_20\n\nLANGUAGE: DolphinDB\nCODE:\n```\nexp = mean(uReturnsMatrix)*242\nvol = std(uReturnsMatrix)*sqrt(242)\nsharpe = (exp - 0.028)/vol\n```\n\n----------------------------------------\n\nTITLE: Configuring DolphinDB Cluster for Resource Tracking\nDESCRIPTION: Shell commands to modify the cluster.cfg configuration file to enable resource tracking functionality across a DolphinDB cluster, using the same parameters as for a single-node server.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/user_level_resource_tracking.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ncd <DolphinDBInstallDir>/server/clusterDemo/config\nvim ./cluster.cfg\n```\n\nLANGUAGE: shell\nCODE:\n```\nresourceSamplingInterval=30\nresourceSamplingMaxLogSize=1024\nresourceSamplingLogRetentionTime=-1\n```\n\n----------------------------------------\n\nTITLE: 利用 restoreControllerMetaFromChunkNode 函数恢复控制节点元数据\nDESCRIPTION: 此脚本示例说明如何在控制节点上调用 restoreControllerMetaFromChunkNode 函数，用于修复因元数据缺失或错误导致的查询异常。操作前需关闭集群、备份、删除旧元数据文件，并配置存储路径，并在重启后等待数据加载完成后执行恢复操作。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/repair_chunk_status.md#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\nrestoreControllerMetaFromChunkNode()\n```\n\n----------------------------------------\n\nTITLE: Viewing SQL Query Access Log Format in CSV\nDESCRIPTION: Example of the CSV format used for logging distributed SQL query information in the access.log file. Records include timestamp, query ID, user ID, database, table, record type, value, and the SQL script executed.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/user_level_resource_tracking.md#_snippet_1\n\nLANGUAGE: csv\nCODE:\n```\ntimestamp,rootQueryId,userId,database,table,type,value,script\n2023.12.15T09:53:52.806155145,314d9a90-8111-bf90-3443-671fd71a7d82,admin,dfs://test,pt,sql,1,\"select timestamp,price * count as price_mul from ej(pt,dt,(pt.sym),(dt.sym),,) where timestamp < 2001.01.01\"\n2023.12.15T09:53:52.950018450,314d9a90-8111-bf90-3443-671fd71a7d82,admin,dfs://test,pt,rowCount,46080,\"\"\n2023.12.15T09:53:52.950068788,314d9a90-8111-bf90-3443-671fd71a7d82,admin,dfs://test,pt,memUsage,15409424,\"\"\n```\n\n----------------------------------------\n\nTITLE: Python Script for High Availability Data Writing\nDESCRIPTION: Python code utilizing DolphinDB's Python API to connect with the cluster in high availability mode and continuously append data to the 'stock' distributed table, ensuring writes are performed without interruption during upgrade.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/gray_scale_upgrade_ha.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nimport dolphindb as ddb\nimport pandas as pd\nimport datetime\nimport time\nimport random\ns = ddb.session()\ns.connect(host=\"192.168.100.43\",port=8922,userid=\"admin\",password=\"123456\", highAvailability=True, highAvailabilitySites=sites)\nappender = ddb.tableAppender(dbPath=\"dfs://stock\",tableName='stock',ddbSession=s,action=\"fitColumnType\")\nx = ['Apple','Microsoft','Meta','Google','Amazon']\ni = 1\nwhile i<=10000000:\n    now = datetime.datetime.now()\n    name = random.sample(x,1)\n    data = pd.DataFrame({'time':now,'name':name,'id':i})\n    appender.append(data)\n    print(i)\n    i = i+1\n    time.sleep(1)\n```\n\n----------------------------------------\n\nTITLE: Check Port Occupation and Error Logs - Text/Log Example\nDESCRIPTION: Log snippet illustrating an error caused by port 8848 being occupied, indicated by the socket bind failure error message. This is used to diagnose startup failures related to port conflicts, prompting users to change their DolphinDB configuration to a free port.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ARM_standalone_deploy.md#_snippet_8\n\nLANGUAGE: Text\nCODE:\n```\n<ERROR> :Failed to bind the socket on port 8848 with error code 98\n```\n\n----------------------------------------\n\nTITLE: Computing Log Ratio of Opening Bid Volume to Ask Volume in Python with Pandas\nDESCRIPTION: Defines a function openBidVolDvdAskVol that calculates the logarithm of the ratio between average bid volume and ask volume during the opening hour (09:30 to 10:30) from a given trading dataframe. Dependencies include pandas for data handling and numpy for logarithmic calculations. The function extracts the trade time from the 'TradeTime' column, filters orders by side (buy or sell), computes mean volumes, and returns None if volumes are non-positive. It expects a dataframe with columns 'TradeTime', 'OrderQty', and 'Side', and returns a float or None.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/DolphinDB_Python_Parser_Intro_for_Quantitative_Finance/因子实现_Python版本/早盘买卖单大小比.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef openBidVolDvdAskVol(df):\n    tradeTime = df[\"TradeTime\"].str.slice(11,23)\n    openBidVolume = df[\"OrderQty\"][(tradeTime >= \"09:30:00.000\")&(tradeTime <= \"10:30:00.000\")&((df[\"Side\"]==1)|(df[\"Side\"]==\"B\"))].mean()\n    openAskVolume = df[\"OrderQty\"][(tradeTime >= \"09:30:00.000\")&(tradeTime <= \"10:30:00.000\")&((df[\"Side\"]==2)|(df[\"Side\"]==\"S\"))].mean()\n    if((openBidVolume>0)&(openAskVolume>0)):\n        res = np.log(openBidVolume / openAskVolume)\n    else:\n        res = None\n    return res\n```\n\n----------------------------------------\n\nTITLE: Creating JSON Payload for WeChat Work Message\nDESCRIPTION: Code demonstrating how to create a JSON payload for sending a text message to all users through WeChat Work. The payload specifies the message type, recipient, agent ID, and content.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/send_messages_external_systems.md#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\nparam='{\"touser\" : \"@all\",\"agentid\" : xxxx,\"msgtype\" : \"text\",\"text\" : {\"content\" : \"这是一条测试信息\"}';\n```\n\n----------------------------------------\n\nTITLE: Model Parameter Tuning for Improved RMSPE in DolphinDB\nDESCRIPTION: This table documents various parameter configurations for the adaBoostRegressor, showing their impact on RMSPE and training time. It highlights the importance of tuning tree number, max depth, and feature set to optimize regression performance.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/machine_learning_volatility.md#_snippet_2\n\n\n\n----------------------------------------\n\nTITLE: Monitoring Streaming Status (DolphinDB)\nDESCRIPTION: These DolphinDB commands provide methods for monitoring the status of the streaming data framework. `getStreamingStat().pubTables` retrieves subscription information for streaming tables.  `getStreamingStat().pubConns` shows the publishing queue status.  `getStreamingStat().subWorkers` displays the consumption status of internal subscribers, aiding in diagnosing potential congestion issues.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_capital_flow_order_by_order.md#_snippet_11\n\nLANGUAGE: DolphinDB\nCODE:\n```\ngetStreamingStat().pubTables\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\ngetStreamingStat().pubConns\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\ngetStreamingStat().subWorkers\n```\n\n----------------------------------------\n\nTITLE: Simulating Sensor Data Generation in DolphinDB\nDESCRIPTION: Defines a function to simulate sensor data for testing the anomaly detection system. It generates random temperature readings for multiple devices and appends them to the sensor stream table. The simulation runs in two phases with different device counts and temperature ranges.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/alarm.txt#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef writeData(){\n\tdeviceNum = 3\n\tfor (i in 0:60) {\n\t\tdata = table(take(1..deviceNum,deviceNum) as deviceID ,take(now().datetime(),deviceNum) as ts,rand(10..41,deviceNum) as temperature)\n\t\tsensor.append!(data)\n\t\tsleep(1000)\n\t}\n\tdeviceNum = 2\n\tfor (i in 0:600) {\n\t\tdata = table(take(1..deviceNum,deviceNum) as deviceID ,take(now().datetime(),deviceNum) as ts,rand(10..45,deviceNum) as temperature)\n\t\tsensor.append!(data)\n\t\tsleep(1000)\n\t}\t\n}\nsubmitJob(\"simulateData\", \"simulate sensor data\", writeData)\n```\n\n----------------------------------------\n\nTITLE: Upgrading DolphinDB Linux Online\nDESCRIPTION: This command executes the upgrade script for DolphinDB. After running the script, the user is prompted to select whether to upgrade online or offline, and then to input the desired version. The upgrade script will handle the process of downloading and installing the new version from the official website.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/standalone_server.md#_snippet_14\n\nLANGUAGE: sh\nCODE:\n```\n./upgrade.sh\n```\n\n----------------------------------------\n\nTITLE: Setting Up Incremental Data Synchronization Schedule\nDESCRIPTION: DolphinDB script that sets up a daily scheduled job to incrementally synchronize the previous day's data from OceanBase to DolphinDB using a SQL query with date filtering.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/OceanBase_to_DolphinDB.md#_snippet_3\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\ndef scheduleLoad(){\n  sqlQuery = \"select * from tick where date(TradeTime) =  '\" +temporalFormat(today()-1, 'y-MM-dd') +\"' ;\"\n  mysql::loadEx(conn, database('dfs://TSDB_tick'), `tick, `TradeTime`SecurityID,sqlQuery)\n}\nscheduleJob(jobId=`test, jobDesc=\"test\",jobFunc=scheduleLoad,scheduleTime=00:05m,startDate=2023.04.04, endDate=2024.01.01, frequency='D')\n```\n\n----------------------------------------\n\nTITLE: Restoring DolphinDB Metadata Backup on Linux via Shell Commands\nDESCRIPTION: This snippet provides shell commands to restore backup metadata files for DolphinDB cluster nodes on Linux systems. It copies critical metadata logs and checkpoint files from a backup directory into the active dfsMeta and data storage directories, which is an essential step when rolling back to an older version following a failed upgrade. Users must execute these commands with appropriate permissions from the specified directories.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/single_machine_cluster_deploy.md#_snippet_23\n\nLANGUAGE: Shell\nCODE:\n```\ncp -r backup/DFSMetaLog.0 ./\ncp -r backup/DFSMasterMetaCheckpoint.0 ./\ncp -r backup/CHUNK_METADATA ./dnode1/storage\n```\n\n----------------------------------------\n\nTITLE: Counting Chunk Partitions After Migration in DolphinDB\nDESCRIPTION: This DolphinDB script queries the cluster to count the number of partitions per data node.  It runs the `getAllChunks` function on each node and groups the results by site (data node).  The output is a table showing the distribution of chunks after a migration. The output is used to verify that data has been successfully migrated.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/data_move_rebalance.md#_snippet_9\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect count(*) from pnodeRun(getAllChunks) group by site\n```\n\n----------------------------------------\n\nTITLE: Generating Panel Data from Wide Tables\nDESCRIPTION: These SQL queries demonstrate how to generate panel data from a wide table. It selects all the columns of the table with given factorname, effectively creating a panel data format by selecting data by trading time and symbol from the wide table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practice_for_factor_calculation.md#_snippet_25\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//宽表模式取面板数据sql\nwide_tsdb_factor_year=select * from tsdb_wide_min_factor where factorname =`f0001\n```\n\n----------------------------------------\n\nTITLE: Constructing Data Source - DolphinDB\nDESCRIPTION: This DolphinDB code defines the `dsTb` function, designed to construct data sources for replay based on user-specified criteria. It takes `timeRS`, `startDate`, `endDate`, `stkList`, and `replayName` as inputs. Depending on `replayName`, it loads a table from DFS. It utilizes the `replayDS` function to partition the data source based on the `timeRS` parameter, which divides data into smaller segments for efficiency. The function returns a list of data sources. Required dependencies include the DolphinDB environment with the `loadTable` and `replayDS` functions available.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/appendices_market_replay_bp.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef dsTb(timeRS, startDate, endDate, stkList, replayName)\n{\n    if(replayName == \"snapshot\"){\n        tab = loadTable(\"dfs://Test_snapshot\", \"snapshot\")\n\t}\n\telse if(replayName == \"order\") {\n\t\ttab = loadTable(\"dfs://Test_order\", \"order\")\n\t}\n\telse if(replayName == \"transaction\") {\n\t\ttab = loadTable(\"dfs://Test_transaction\", \"transaction\")\n\t}\n\telse {\n\t\treturn NULL\n\t}\n    ds = replayDS(sqlObj=<select * from tab where MDDate>=startDate and MDDate<endDate and HTSCSecurityID in stkList>, dateColumn='MDDate', timeColumn='MDTime', timeRepartitionSchema=timeRS)\n    return ds\n}\n```\n\n----------------------------------------\n\nTITLE: Analyzing Execution Plan for a Subquery in DolphinDB\nDESCRIPTION: This DolphinDB script demonstrates how to obtain and interpret the execution plan for a query containing a subquery in the FROM clause. The query first calculates the maximum 'x' value for each 'month' group in the 'pt' table and then filters the results where 'maxx' is less than 0.9994. The HINT_EXPLAIN directive is used to generate the plan.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_Explain.md#_snippet_4\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nselect [HINT_EXPLAIN] * from (select max(x) as maxx from loadTable(\"dfs://valuedb\",`pt) group by month ) where maxx < 0.9994\n```\n\n----------------------------------------\n\nTITLE: Calculating Annual Skewness in Python\nDESCRIPTION: Defines a Python function `getAnnualSkew` using NumPy and SciPy.stats (imported as `st`). It computes daily returns from the input `value` array and then calculates their skewness using `st.skew`. Requires NumPy and SciPy libraries.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/fund_factor_contrasted_by_py.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport scipy.stats as st\n\ndef getAnnualSkew(value):\n    diff_value = np.diff(value)\n    rolling_value = np.roll(value, 1)\n    rolling_value = np.delete(rolling_value, [0])\n    return st.skew(np.true_divide(diff_value, rolling_value))\n```\n\n----------------------------------------\n\nTITLE: Loading Partitioned Table in DolphinDB\nDESCRIPTION: This code snippet loads a partitioned table from the database. It establishes a connection to the database testDB and table testTB, but does not load the full dataset at this stage. This function is a pre-requisite for querying or performing computations on the table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/single_machine_cluster_deploy.md#_snippet_8\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 加载分区表对象\npt = loadTable(\"dfs://testDB\", \"testTB\")\n```\n\n----------------------------------------\n\nTITLE: Logging into DolphinDB\nDESCRIPTION: This snippet logs into the DolphinDB server with the provided username and password. This is typically the first step in interacting with a DolphinDB instance.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/data/DolphinDB_streaming_application_in_IOT/deviceState.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(`admin,`123456)\n```\n\n----------------------------------------\n\nTITLE: Sample Output Displaying DataX Synchronization Results in Terminal\nDESCRIPTION: This console output snippet shows the result summary after running the DataX migration task. It includes timestamps marking job start and end, total elapsed time in seconds, average throughput in MB/s, record writing speed per second, total number of records read, and number of read/write failures (zero in this case). Such metrics help verify task success and performance.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/OceanBase_to_DolphinDB.md#_snippet_6\n\nLANGUAGE: Shell\nCODE:\n```\n任务启动时刻                    : 2023-04-03 14:58:52\n任务结束时刻                    : 2023-04-03 15:00:52\n任务总计耗时                    :                120s\n任务平均流量                    :           12.32MB/s\n记录写入速度                    :         226766rec/s\n读出记录总数                    :            27211975\n读写失败总数                    :                   0\n```\n\n----------------------------------------\n\nTITLE: 调整收益率矩阵列名以区分基金类型\nDESCRIPTION: 根据基金类型，将收益率矩阵的列改名，便于按类型分析和可视化。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/public_fund_basic_analysis.md#_snippet_16\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntReturnsMatrix = returnsMatrix[fundType[\"SecurityID\"]]\nnewNames = fundType[\"Type\"] + \"_\" + fundType[\"SecurityID\"].strReplace(\".\", \"_\").strReplace(\"!\", \"1\")\n tReturnsMatrix.rename!(newNames)\n tReturnsMatrix[0:3]\n```\n\n----------------------------------------\n\nTITLE: Getting department members detail from DingTalk API\nDESCRIPTION: This code uses httpClient::httpGet to request the member list of a specific department by department ID, using the access_token for authentication. It processes the response to retrieve user IDs for further use, such as creating a group chat.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/send_messages_external_systems.md#_snippet_13\n\nLANGUAGE: DolphinDB\nCODE:\n```\nurl = 'https://oapi.dingtalk.com/user/getDeptMember';\nparam=dict(string,string);\nACCESS_TOKEN='xxxxx';\nDEPTID='xxxxx';\nparam['access_token']=ACCESS_TOKEN;\nparam['deptId']=DEPTID;\nret=httpClient::httpGet(url,param,1000);\nprint ret['text'];\nbody = parseExpr(ret.text).eval();\nUserIds=body.userIds;\nERRCODE=body.errcode;\n```\n\n----------------------------------------\n\nTITLE: Define High Price Factor (Incremental Calculation)\nDESCRIPTION: Defines a function named `High` that corresponds to the factor name for the highest price of a minute candlestick. The calculation logic uses the `max` aggregate function to find the maximum value of `LastPx` (the latest transaction price) within the calculation window. This is an example of an incremental calculation, where DolphinDB can optimize the computation process. The platform parses the string \"max(LastPx)\" as meta code and passes it to the time-series engine.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Level2_Snapshot_Factor_Calculation.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef High(){\n\treturn \"max(LastPx)\"\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Database for Shenzhen Stock Exchange Order-by-Order Data with TSDB Engine\nDESCRIPTION: Creates a database for storing Shenzhen Stock Exchange order-by-order data using a combined partitioning strategy with date value and HASH on symbols. The TSDB engine is used with symbol and trade time as sorting columns.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practices_for_partitioned_storage.md#_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\ncreate database \"dfs://split_SZ_TB\"\npartitioned by VALUE(2020.01.01..2021.01.01), HASH([SYMBOL, 25])\nengine='TSDB'\n\ncreate table \"dfs://split_SZ_TB\".\"split_SZ_entrustTB\"(\n    ChannelNo INT\n    ApplSeqNum LONG\n    MDStreamID SYMBOL\n    SecurityID SYMBOL\n    SecurityIDSource SYMBOL\n    Price DOUBLE\n    OrderQty LONG\n    Side SYMBOL\n    TradeDate DATE[comment=\"交易日期\", compress=\"delta\"]   \n    TradeTime TIME[comment=\"交易时间\", compress=\"delta\"]   \n    OrderType SYMBOL\n    LocalTime TIME\n    SeqNo INT\n)\npartitioned by TradeDate, SecurityID,\nsortColumns=[`SecurityID,`TradeTime],\nkeepDuplicates=ALL\n```\n\n----------------------------------------\n\nTITLE: Correlation Matrix Calculation with cross\nDESCRIPTION: This DolphinDB script calculates the correlation matrix between stocks using the `cross` function. It takes the stock return matrix (`retMatrix`) as input and applies the `corr` function to all pairs of stock returns, generating a correlation matrix.  Requires `retMatrix` to be defined.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\ncorrMatrix=cross(corr,retMatrix)\n```\n\n----------------------------------------\n\nTITLE: Calculating cumulative volume with caclCumVol and accumulate\nDESCRIPTION: This code applies the custom `caclCumVol` function using the `accumulate` higher-order function to calculate the cumulative volume.  The `accumulate` function computes a running cumulative volume. The function group splits at the most appropriate point when nearing a target volume.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_49\n\nLANGUAGE: shell\nCODE:\n```\naccumulate(caclCumVol{1500000}, volume)\n```\n\n----------------------------------------\n\nTITLE: 初始化组合分区 DolphinDB 数据库 - DolphinDB 脚本\nDESCRIPTION: 该代码定义了一个组合分区数据库，采用按小时的时间分区和基于设备 ID 的哈希二级分区，适用于海量高频传感器数据的分布式存储。依赖 DolphinDB 数据库系统，partitionType 和 partitionScheme 用于配置数据切分方案，engine 参数指定时序数据库引擎，支持高效数据写入和查询。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/high_freq_data_storage_and_analysis.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//构建分区方式，构建小时+哈希5的组合分区\ndbname=\"dfs://db_test\"   \ndb_hour=database(partitionType=VALUE,partitionScheme=[datehour(2022.10.01)])\ndb_hash=database(partitionType=HASH, partitionScheme=[SYMBOL,5])\ndb=database(directory=dbname,partitionType=COMPO,partitionScheme=[db_hour,db_hash],engine=`TSDB)\n```\n\n----------------------------------------\n\nTITLE: Subscribing to and Filtering Snapshot Stream in DolphinDB\nDESCRIPTION: Defines a handler function `snapshotFilter` that filters incoming data from a stream table based on SecurityID prefix ('60') and timestamp (>= 09:25:00.000). Subscribes this handler to the `snapshotStreamTable` using `subscribeTable`, directing the filtered data (`t`) to the streaming engine named `calChange`. The subscription processes messages as tables (`msgAsTable=true`).\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Real-Time_Stock_Price_Increase_Calculation.md#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef snapshotFilter(engineName, mutable data){\n\tt = select * from data where left(SecurityID, 2)=\"60\" and time(DateTime)>=09:25:00.000\n\tgetStreamEngine(engineName).append!(t)\n}\n\nsubscribeTable(tableName=\"snapshotStreamTable\", actionName=\"snapshotFilter\", offset=-1, handler=snapshotFilter{\"calChange\"}, msgAsTable=true, hash=0)\n```\n\n----------------------------------------\n\nTITLE: Creating Prediction & Warning Stream Tables - DolphinDB\nDESCRIPTION: This code creates stream tables named `predictTable` and `warningTable` for storing prediction and warning data, respectively. It defines the column names and types for each table and enables table sharing and persistence. The cache size is set to 5000000 for both tables.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/knn_iot.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//新建预测表\n preColNames = `time`deviceCode`wind`humidity`air_pressure`temperature`propertyValue_predicted\n preColTypes = orgColTypes\n enableTableShareAndPersistence(table = streamTable(100000:0,preColNames,preColTypes), tableName=`predictTable, cacheSize = 5000000)\n\n //新建预警表\n warnColNames = `time`abnormal_rate`whether_warning\n warnColTypes = [TIMESTAMP,DOUBLE,INT]\n enableTableShareAndPersistence(table = streamTable(100000:0,warnColNames,warnColTypes), tableName=`warningTable, cacheSize = 5000000)\n```\n\n----------------------------------------\n\nTITLE: Aggregating Multiple Financial Factors per Fund in DolphinDB\nDESCRIPTION: Defines 'getFactor', which groups and calculates nine different financial factor metrics for multiple funds from a result table. It uses filters on trade dates and fund symbols, calling factor functions for annual return, volatility, skew, kurtosis, Sharpe, max drawdown, drawdown ratio, beta, and alpha, returning a table grouped by fund. Inputs include a result2 table with value and price columns, and a list of symbol strings.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_factor_by_ddb/fund_factor_nine.txt#_snippet_10\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef getFactor(result2, symList){\n\tReturn = select fundNum, \n\t\t\t    getAnnualReturn(value) as annualReturn,\n\t\t\t    getAnnualVolatility(value) as annualVolRat,\n\t\t\t    getAnnualSkew(value) as skewValue,\n\t\t\t    getAnnualKur(value) as kurValue,\n\t\t\t    getSharp(value) as sharpValue,\n\t\t\t    getMaxDrawdown(value) as MaxDrawdown,\n              getDrawdownRatio(value) as DrawdownRatio,\n              getBeta(value, price) as Beta,\n              getAlpha(value, price) as Alpha\t\n             from result2\n             where TradeDate in 2018.05.24..2021.05.27 and fundNum in symList group by fundNum\n }\n```\n\n----------------------------------------\n\nTITLE: Subscribing to DolphinDB Stream Table in Python\nDESCRIPTION: This Python script demonstrates how to subscribe to a DolphinDB stream table ('calc_result' from the previous example). It uses the DolphinDB Python API to establish a streaming connection, define a handler function ('handler') to process incoming messages (printing them in this case), and subscribes to the specified table on the DolphinDB server.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Iot_intelligent_O&M.md#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nfrom threading import Event\nimport dolphindb as ddb\nimport pandas as pd\nimport numpy as np\ns = ddb.session()\ns.enableStreaming(8000)\ndef handler(lst):\n   print(lst)\ns.subscribe(\"127.0.0.1\", 8848, handler, \"calc_result\")\nEvent().wait()\n```\n\n----------------------------------------\n\nTITLE: Creating Partitioned Regular In-Memory Table in DolphinDB Script\nDESCRIPTION: This snippet demonstrates how to create a partitioned regular in-memory table in DolphinDB using range partitions. The code defines an empty base table schema, sets up a range-based database, and creates a partitioned table on the 'id' column. Key steps include specifying partition points and assigning the resulting partitioned table object. Dependencies include an active DolphinDB environment. Inputs are the base table and partition points; the output is a partitioned in-memory table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/in_memory_table.md#_snippet_0\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nt=table(1:0,`id`val,[INT,INT])\ndb=database(\"\",RANGE,0 101 201 301)\npt=db.createPartitionedTable(t,`pt,`id)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Data Replay for Real-time Simulation in DolphinDB\nDESCRIPTION: Initializes a stream table and uses replayDS function to replay historical tick data to simulate real-time market data feed for IOPV calculation testing.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_IOPV.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nt =  streamTable(100:0, `SecurityID`tradedate`tradetime`price,[SYMBOL, DATE,TIMESTAMP,DOUBLE])\nenableTableShareAndPersistence(table=t, tableName=`TradeStreamData, cacheSize=1000000)\nrds = replayDS(<select   SecurityID, tradedate, tradetime , price from loadTable(\"dfs://LEVEL2_SZ\",\"Trade\") where tradedate = 2020.12.01, price>0  >, `tradedate, `tradetime,  cutPoints(09:30:00.000..16:00:00.000, 60));\nsubmitJob(\"replay_order\", \"replay_trades_stream\",  replay,  rds,  `TradeStreamData, `tradedate, `tradetime, 1000000, true, 4)\n```\n\n----------------------------------------\n\nTITLE: Querying - Order by, Top N DolphinDB\nDESCRIPTION: This snippet selects the top 20 records of charging devices, ordering by time in descending order.  It demonstrates how to retrieve a specific number of rows.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs TimescaleDB/test_dolphindb_small.txt#_snippet_27\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 18. 经典查询：查询充电设备的最近 20 条电池温度记录\ntimer\nselect top 20\n    time,\n    device_id,\n    battery_temperature\nfrom readings\nwhere battery_status = 'charging'\norder by time desc\n```\n\n----------------------------------------\n\nTITLE: Defining Annualized Return Calculation Function in DolphinDB\nDESCRIPTION: Defines the 'getAnnualReturn' function to calculate the annualized return of an asset's value series within DolphinDB. The function computes return based on the first and last elements of the value series using a 730-day period as the time interval. It requires the input to be a series of numeric values representing asset prices or net values. The output is a numeric annualized return percentage.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_factor_by_ddb/fund_factor_nine.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndefg getAnnualReturn(value){\n      return pow(1 + ((last(value) - first(value))\\first(value)), 252\\730) - 1\n}\n```\n\n----------------------------------------\n\nTITLE: Calculating Residual with Macro Variable Metaprogramming\nDESCRIPTION: This snippet demonstrates the calculation of residuals using macro variables in DolphinDB. The key variables are the column name (`colName`) and the set of x-columns represented by the macro variable `x`. It provides a more direct and readable approach by embedding macro variables in a SQL query.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/macro_var_based_metaprogramming.md#_snippet_21\n\nLANGUAGE: DolphinDB\nCODE:\n```\ncolName = \"y\"\nx = `x1`x2`x3\n<select ols(_$colName, _$$x, 1, 2).Residual as residual from t>.eval()\n// 或者\n<select ols(_$colName, x1...x3, 1, 2).Residual as residual from t>.eval()\n```\n\n----------------------------------------\n\nTITLE: Parsing and Importing MiniSeed Historical Data into DolphinDB Distributed Table (DolphinDB Script)\nDESCRIPTION: Implements multithreaded parsing and importing of MiniSeed files into a DolphinDB distributed partitioned table using mseed::parseStream. The parser extracts sample data and metadata, replaces tag IDs, removes invalid entries, and appends results to the destination table in batches. Dependencies include the MiniSeed plugin and a mapping dictionary for tag IDs. Inputs include distributed DB/table names, tag info dictionary, and a vector of file paths. The process groups files for batch parallel job submission using submitJobEx.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Seismic_waveform_data_storage.md#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef parseAndImportIntoDfs(realDbName,realTbNname,dtName,tagInfo,fileParse){\n\t/*\n\t * Description：\n\t * \t此函数用于解析 miniSeed 文件，并将结构化数据存入分布式数据库\n\t * Input：\n\t * \trealDbName,realTbNname,dtNam：STRING \n\t * \ttagInfo：dict 键为台网、台站、位置通道拼接起的字符串，值为对应的 id\n\t * \tfileParse：STRING 解析的miniSeed文件存储路径\n\t */\n\tret = mseed::parseStream(file(fileParse).readBytes(30000000))\n\tdata = ret[`data]\n\tdata.replaceColumn!(`id,tagInfo[data.id])\n\tdelete from data where id = NULL\n\tpt = loadTable(realDbName,realTbNname)\n\tpt.append!(data)\t \t\n}\n\ndef parallelInToDfs(realDbName,realTbNname,dtName,tagInfo,filePathList){\n\t/*\n\t * Description：\n\t * \t此函数用于并行解析 MiniSeed 文件，并存入分布式数据库\n\t * Input：\n\t * \trealDbName,realTbNname,dtName：均为 STRING 类型常量，分别代表分布式数据库名、分区表名、维度表名\n\t * \ttagInfo：dict 键为由台网、台站、位置、通道组成的字符串，值为对应的id\n\t * \tfilePathList：STRING VECTOR 需要解析的MiniSeed文件路径向量\n\t */\n\tploop(parseAndImportIntoDfs{realDbName,realTbNname,dtName,tagInfo,},filePathList)\n}\n\nrealDbName,realTbNname,dtName = \"dfs://real\",\"realData\",\"tagInfo\"\nfilePathList = exec \"../miniSeed/\"+net+\".\"+sta+\".\"+loc+\".\"+chn+\".\"+\"20230302.mseed\" from loadTable(realDbName,dtName)\nfilePathListGroup = cut(filePathList,48)\nd = exec tagid,id from loadTable(realDbName,dtName)\ntagInfo=dict(d[`tagid],d[`id])\nfor(i in 0..(filePathListGroup.size()-1)){\n\tsubmitJobEx(\"parse_mseed_into_dfs\"+string(i),\"parse_mseed_into_dfs\"+string(i),i%10,48,parallelInToDfs,realDbName,realTbNname,dtName,tagInfo,filePathListGroup[i])\n}\t\n```\n\n----------------------------------------\n\nTITLE: Loading and Creating Jobs Table in DolphinDB\nDESCRIPTION: Loads job data from the specified CSV file and creates a 'jobs' table within the 'hr' database, defining schema with columns such as JOB_ID, JOB_TITLE, MIN_SALARY, and MAX_SALARY. The data is then appended to the table for analysis.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/Standard_SQL_in_DolphinDB/create_db_table_sql.txt#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\njobs_tmp = loadText(dir+\"JOBS.csv\")\ncreate table \"dfs://hr\".\"jobs\" (\n\tJOB_ID SYMBOL,\n\tJOB_TITLE STRING,\n\tMIN_SALARY INT,\n\tMAX_SALARY INT\t\n)\njobs = loadTable(\"dfs://hr\", \"jobs\")\njobs.append!(jobs_tmp)\nselect * from jobs\n```\n\n----------------------------------------\n\nTITLE: Real-time Stream Calculation of Weighted Average Price in DolphinDB\nDESCRIPTION: This code demonstrates the real-time stream calculation of the weighted average price using a reactive state engine. It defines input and output table schemas, creates the reactive state engine named \"reactiveDemo\", specifies the metric to calculate, and inserts sample data using `tableInsert`.  The `colName` and `colType` are defined to create the appropriate schema for the market data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Streaming_computing_of_financial_quantifiers.md#_snippet_10\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 定义输入输出的表结构\ncolName = [\"securityID\",\"dateTime\",\"preClosePx\",\"openPx\",\"highPx\",\"lowPx\",\"lastPx\",\"totalVolumeTrade\",\"totalValueTrade\",\"instrumentStatus\"] <- flatten(eachLeft(+, [\"bidPrice\",\"bidOrderQty\",\"bidNumOrders\"], string(0..9))) <- (\"bidOrders\"+string(0..49)) <- flatten(eachLeft(+, [\"offerPrice\",\"offerOrderQty\",\"offerNumOrders\"], string(0..9))) <- (\"offerOrders\"+string(0..49)) <- [\"numTrades\",\"iopv\",\"totalBidQty\",\"totalOfferQty\",\"weightedAvgBidPx\",\"weightedAvgOfferPx\",\"totalBidNumber\",\"totalOfferNumber\",\"bidTradeMaxDuration\",\"offerTradeMaxDuration\",\"numBidOrders\",\"numOfferOrders\",\"withdrawBuyNumber\",\"withdrawBuyAmount\",\"withdrawBuyMoney\",\"withdrawSellNumber\",\"withdrawSellAmount\",\"withdrawSellMoney\",\"etfBuyNumber\",\"etfBuyAmount\",\"etfBuyMoney\",\"etfSellNumber\",\"etfSellAmount\",\"etfSellMoney\"]\ncolType = [\"SYMBOL\",\"TIMESTAMP\",\"DOUBLE\",\"DOUBLE\",\"DOUBLE\",\"DOUBLE\",\"DOUBLE\",\"INT\",\"DOUBLE\",\"SYMBOL\"] <- take(\"DOUBLE\", 10) <- take(\"INT\", 70)<- take(\"DOUBLE\", 10) <- take(\"INT\", 70) <- [\"INT\",\"DOUBLE\",\"INT\",\"INT\",\"DOUBLE\",\"DOUBLE\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"DOUBLE\",\"INT\",\"INT\",\"DOUBLE\",\"INT\",\"INT\",\"DOUBLE\",\"INT\",\"INT\",\"DOUBLE\"]\nresultTable = table(10000:0, [\"securityID\", \"dateTime\", \"factor\"], [SYMBOL, TIMESTAMP, DOUBLE])\ninputTable = table(1:0, colName, colType)\n\n// 使用 createReactiveStateEngine 创建响应式状态引擎\ntry{ dropStreamEngine(\"reactiveDemo\")} catch(ex){ print(ex) }\nmetrics = <[dateTime, weightedAveragedPrice(bidPrice0, bidOrderQty0, offerPrice0, offerOrderQty0)]>\nrse = createReactiveStateEngine(name=\"reactiveDemo\", metrics =metrics, dummyTable=inputTable, outputTable=resultTable, keyColumn=\"securityID\")\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 输入数据\ntableInsert(rse, {\"securityID\":\"000001\", \"dateTime\":2023.01.01T09:30:00.000, \"bidPrice0\":19.98, \"bidOrderQty0\":100, \"offerPrice0\":19.99, \"offerOrderQty0\":120})\ntableInsert(rse, {\"securityID\":\"000001\", \"dateTime\":2023.01.01T09:30:03.000, \"bidPrice0\":19.95, \"bidOrderQty0\":130, \"offerPrice0\":19.93, \"offerOrderQty0\":120})\ntableInsert(rse, {\"securityID\":\"000001\", \"dateTime\":2023.01.01T09:30:06.000, \"bidPrice0\":19.97, \"bidOrderQty0\":120, \"offerPrice0\":19.98, \"offerOrderQty0\":130})\n\n// 查看结果\nselect * from resultTable\n/*\nsecurityID dateTime                factor            \n---------- ----------------------- ------------------\n000001     2023.01.01T09:30:00.000 19.9845 \n000001     2023.01.01T09:30:03.000 19.9396\n000001     2023.01.01T09:30:06.000 19.9748\n*/\n```\n\n----------------------------------------\n\nTITLE: Define Price Percent Change Calculation with State in DolphinDB\nDESCRIPTION: This code defines a state function `pricePercentChange` to calculate the percentage change in price compared to a lagged value. It uses the `move` function to access the price from a previous time step. The `@state` decorator indicates that this function maintains state between invocations.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Streaming_computing_of_financial_quantifiers.md#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\n@state\ndef pricePercentChange(price, lag){\n    return price \\ move(price, lag) - 1\n}\n```\n\n----------------------------------------\n\nTITLE: Subscribing to DolphinDB Stream Tables and Replaying Data\nDESCRIPTION: This code sets up subscriptions to DolphinDB stream tables. It subscribes to `orderTable` with `process` as the handler function (processing data into `traitTable`) and subscribes to `traitTable` with `predictDuration` as the handler (predicting results into `predictTable`). It then simulates real-time data ingestion by submitting a job to replay historical data from `data` into `orderTable`.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Forecast_of_Taxi_Trip_Duration.md#_snippet_1\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\n// 订阅订单信息表，数据从订单表流向特征表\nsubscribeTable(tableName=\"orderTable\", actionName=\"orderProcess\", offset=0, handler=process{traitTable, hisData}, msgAsTable=true, batchSize=1, throttle=1, hash=0, reconnect=true)\n// 订阅特征表，数据从特征表流向预测表\nsubscribeTable(tableName=\"traitTable\", actionName=\"predict\", offset=0, handler=predictDuration{predictTable}, msgAsTable=true, hash=1, reconnect=true)\n// 回放历史数据，模拟实时产生的生产数据\nsubmitJob(\"replay\", \"trade\",  replay{inputTables=data, outputTables=orderTable, dateColumn=`pickup_datetime, timeColumn=`pickup_datetime, replayRate=25, absoluteRate=true, parallelLevel=1})\n```\n\n----------------------------------------\n\nTITLE: Simulating Real-time Production Data in Python\nDESCRIPTION: This Python script simulates real-time production data for anomaly detection. It generates data with specified parameters like devices number, rate, and time range, and then appends this data to a DolphinDB stream table. Linear relationships with randomized parameters are used to model the sensor data, to introduce controlled inaccuracy for anomaly comparison.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/knn_iot.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\norgColNames= `time`deviceCode`wind`humidity`air_pressure`temperature`life`propertyValue\norgColTypes = [TIMESTAMP,INT,DOUBLE,DOUBLE,DOUBLE,DOUBLE,INT,INT]\nenableTableShareAndPersistence(table = streamTable(10000:0, orgColNames,orgColTypes), tableName=`dataTable, cacheSize = 6000000) \n\ndef send_data(begintime,endtime,hour,devices_number,rate,mutable dest)\n{\n\n    btime=timestamp(begintime)\n\tdo{\n        seconds  = int(60*60*hour)  //定义需要压入的批次，每秒钟1批\n\n        n = devices_number * rate* 1 // 每秒钟生产10万条数据\n        time =sort(take(btime+(0..(1000-1)),n)) //\n        deviceCode = take(1..devices_number,n)\n        x1 = randNormal(25,2,n)  \n        x2 = randNormal(55,5,n)\n        x3 = randNormal(1.01325,0.00001,n)\n        x4 = randNormal(75,5,n)\n        x5 = int(randNormal(10,3,n))\n        b1 = randNormal(0.4,0.05,n) //方差0.05 降低模型准确率\n        b2 = randNormal(0.3,0.05,n)\n        b3 = randNormal(0.2,0.05,n)\n        b4 = randNormal(0.09,0.05,n)\n        b5 = randNormal(0.01,0.001,n)\n        bias = randNormal(5,1,n)\n        propertyValue = int(b1*x1*10 + b2*x2*2 + b3*x3*1000 + b4*x4 + b5*x5 +bias)\n\n        table_ps = table(time,deviceCode,x1,x2,x3,x4,x5,propertyValue)\n        dest.append!(table_ps)\n\n        btime=btime+1000\n        etime=timestamp(now())\n        timediff=btime-etime\n        if(timediff>0){sleep(timediff)}\n    }while(btime<=endtime)\n}\nhour = 0.5 //用户自定义压入多少小时的数据,1 为 1个小时\ndevices_number = 100 //设备数目\nrate = 1000 //每台设备 每秒钟1000条数据\nbegintime = datetime(now()) //开始时间\nendtime = begintime + int(hour * 3600-1)  //结束时间\nsubmitJob(`send_data_to_kafka,`send_data_to_kafka,send_data,begintime,endtime,hour,devices_number,rate,dataTable) \n```\n\n----------------------------------------\n\nTITLE: Add Function Views - DolphinDB\nDESCRIPTION: This section adds function views for the functions `dsTb`, `createEnd`, `replayJob`, and `stkReplay`. This allows these functions to be accessed as views in the DolphinDB system. These functions enable the system to invoke functions by name.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/appendices_market_replay_bp/replay.txt#_snippet_4\n\nLANGUAGE: dolphindb\nCODE:\n```\naddFunctionView(dsTb)\naddFunctionView(createEnd)\naddFunctionView(replayJob)\naddFunctionView(stkReplay)\n```\n\n----------------------------------------\n\nTITLE: Configuring DolphinDB Agent Node via agent.cfg (config)\nDESCRIPTION: Defines the agent node's local site address and cluster site list, which must include all agents and controllers in the cluster. Update IP and port information to match the actual cluster topology. This file is critical for agent-controller communication in distributed deployments.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/deploy_dolphindb_on_new_server.md#_snippet_16\n\nLANGUAGE: config\nCODE:\n```\nlocalSite=175.178.100.3:8960:agent1\nsites=175.178.100.3:8960:agent1:agent,175.178.100.3:8990:controller1:controller,119.91.229.229:8990:controller2:controller,175.178.100.213:8990:controller3:controller\n...\n\n```\n\n----------------------------------------\n\nTITLE: Executing DolphinDB Scripts Dynamically in JavaScript - Python Syntax\nDESCRIPTION: This snippet demonstrates how to prepare a DolphinDB script consisting of a function definition and a function call, then execute it dynamically using the eval() method provided by the DolphinDB JavaScript API. The script is built as a string and may contain arbitrary DolphinDB code. Returns the result of execution. Requires an established ddb (DolphinDB client) object and async context.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/node_red_tutorial_iot.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nvar func=`def foo(a,b){\n               return a+b;\n          }`\nvar funcall=`foo(11,11)`\nvar secipt=func+funcall\nconst result = await ddb.eval(script)\n```\n\n----------------------------------------\n\nTITLE: DolphinDB节点监控状态查询脚本\nDESCRIPTION: 这组DolphinDB脚本用于第三套监控方案，用于查询流表状态、订阅状态和节点状态等信息，支持通过dolphindb-datasource在Grafana中展示。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/cluster_monitor.md#_snippet_10\n\nLANGUAGE: DolphinDB\nCODE:\n```\n#流表状态\ngetStreamingStat().pubTables\n\n#订阅状态\ngetStreamingStat().subWorkers\n\n#节点状态\ntemp = getClusterPerf(includeMaster=true)\nt = select  host,port,mode,state,name,maxMemSize,runningTasks,queuedTasks,runningJobs,queuedJobs,connectionNum from temp\nmapper1 = dict(0 1 2 3 4,[\"datanode\",\"agentnode\",\"controlnode\",\"singlemode\",\"computenode\"])\nmapper2 = dict(0 1,[\"未存活\",\"存活\"])\ntempvalue1 = t.mode\ntempvalue2 = t.state\nfinalvalue1 = mapper1[tempvalue1]\nfinalvalue2 = mapper2[tempvalue2]\nreplaceColumn!(t,`mode,finalvalue1)\nreplaceColumn!(t,`state,finalvalue2)\nselect * from t\n```\n\n----------------------------------------\n\nTITLE: Processing Long-Form and Wide-Form Financial Data\nDESCRIPTION: Provides an overview of data organization styles—long-form versus wide-form—and references DolphinDB functions 'pivot' and 'panel' for transforming data between these formats. Enables flexible time series analysis and data structuring based on analytical needs.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/public_fund_basic_analysis.md#_snippet_6\n\n\n\n----------------------------------------\n\nTITLE: Verifying Data Count from Redis\nDESCRIPTION: This snippet counts the number of rows in the temporary table `t`, which contains data read from Redis. It then uses the `assert` command to check if this count (`ret`) is equal to the number of records (`n`) that were initially inserted into the stream table. This verifies that all records inserted into the stream table were successfully written to and read from Redis.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/redis_plugin_tutorial/example2.txt#_snippet_6\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nret = exec count(*) from t\nassert \"test\", n==ret\n```\n\n----------------------------------------\n\nTITLE: Querying scheduled jobs in DolphinDB\nDESCRIPTION: Example demonstrating how to create and query a scheduled job, showing the creation of a simple job that runs multiple times and prints timestamps.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/scheduledJob.md#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef foo(){\n\tprint \"test scheduled job at\"+ now()\n\treturn now()\n}\nscheduleJob(`testJob, \"foo\", foo, 17:00m+0..2*30, today(), today(), 'D');\n```\n\n----------------------------------------\n\nTITLE: Downloading DolphinDB Linux\nDESCRIPTION: This snippet downloads the DolphinDB Linux server installation package using the `wget` command.  It fetches the package from the official DolphinDB website. The `${release}` variable represents the version, allowing for flexible version selection.  It downloads the package to the current directory with the filename `dolphindb.zip`.  This is a prerequisite for installing or upgrading a DolphinDB instance on Linux.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/standalone_server.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nwget https://www.dolphindb.cn/downloads/DolphinDB_Linux64_V${release}.zip -O dolphindb.zip\n```\n\n----------------------------------------\n\nTITLE: Testing Filter Condition Order Effects in DolphinDB SQL\nDESCRIPTION: Examines how the order of filter conditions affects query performance and results when using comma vs AND operators, particularly with sequence-related conditions.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_SQL_Case_Tutorial.md#_snippet_5\n\nLANGUAGE: SQL\nCODE:\n```\ntimer(10) t3 = select * from t where date = 2019.01.02, sym = `C, ratios(qty) > 1\ntimer(10) t4 = select * from t where date = 2019.01.02 and sym = `C and ratios(qty) > 1\n\neach(eqObj, t2.values(), t4.values()) // true\neach(eqObj, t1.values(), t3.values()) // false\n```\n\n----------------------------------------\n\nTITLE: Implementing Waveform Recording in DolphinDB Script\nDESCRIPTION: Creates a partitioned TSDB table (`recode`) to store recorded data and defines a function `funRecodeData` to select data from a source table (`collect`) for a specific device (`wtid`) within a given time range (`begintime` +/- `second`) and append it to the `recode` table. Includes error handling for recording duration and time type conversion.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/high_freq_data_storage_and_analysis.md#_snippet_8\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\n/**** 录制某段波形数据 ****/\n\n//1. 建表，保存录制信息\nn=5000 //采集点数量\ndbname=\"dfs://db_recodeing\"\ntablename=\"recode\"\ncols_info=`ts`deviceid\ncols_type=`TIMESTAMP`SYMBOL\nt=table(1:0,cols_info,cols_type)\n\nfor(i in 1..n){\n    addColumn(t,`v+string(i),DOUBLE)\n}\n  \ndb_hour=database(partitionType=VALUE,partitionScheme=[datehour(2022.10.01)])\ndb_hash=database(partitionType=HASH, partitionScheme=[SYMBOL,5])\ndb=database(directory=dbname,partitionType=COMPO,partitionScheme=[db_hour,db_hash],engine=`TSDB)\npt=createPartitionedTable(dbHandle=db,table=t,tableName=tablename,partitionColumns=`ts`deviceid,sortColumns=`deviceid`ts)\n\n//2. 自定义录制函数\ndef funRecodeData(wtid,mutable begintime,second){\n\n    //合法性判断\n    if(second>180){\n        strMsg='录制时间需小于3分钟'\n        return strMsg\n    }\n\n    if(typestr(begintime)==\"STRING\"){ begintime=timestamp(begintime) }\n    endtime=temporalAdd(begintime,second,`s)\n\n    //录制未来数据\n    diff=endtime-now()\n    if(diff>0){\n        sleep(diff)\n    }\n\n    //录制\n    t=select * from loadTable(\"dfs://db_test\",`collect) where ts between begintime:endtime and deviceid=wtid\n    pt=loadTable(database=\"dfs://db_recodeing\",tableName=`recode)\n    pt.append!(t)\n\n    //返回消息，供应用端展示\n    rownum=t.size()\n    strMsg='录制行数：'+string(rownum)+''\n    return strMsg\n}\n\n//3. 调用函数\ndeviceid = 'd001'\nsecond = 5\nbegintime = 2022.01.01 00:10:00.000\nmsg=funRecodeData(deviceid,begintime,second)\nmsg\n```\n\n----------------------------------------\n\nTITLE: Execution Plan Showing No Partition Pruning in DolphinDB\nDESCRIPTION: This JSON snippet shows part of a DolphinDB execution plan where partition pruning did not occur. The 'map.partitions.remote' field indicates that all 204 remote partitions were scanned, despite the WHERE clause filtering by month. This happens because the condition `2016.11M <= month <= 2016.12M` is not optimized for pruning in this context.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_Explain.md#_snippet_8\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    \"measurement\": \"microsecond\",\n    \"explain\": {\n        \"from\": {\n            \"cost\": 2\n        },\n        \"map\": {\n            \"partitions\": {\n                \"local\": 0,\n                \"remote\": 204\n            },\n            \"cost\": 12966,\n            \"detail\": {\n                \"most\": {\n                    \"sql\": \"select [114699] month,x from pt where 2016.11M <= month <= 2016.12M [partition = /valuedb/200107M/2JCB]\",\n                    \"explain\": {\n        ......[以下省略]\n```\n\n----------------------------------------\n\nTITLE: Subscribing to Stream Data in DolphinDB\nDESCRIPTION: Demonstrates how to subscribe to a stream data table and batch write the received data to a distributed table. The code sets up a subscription that batches data either when 10,000 records are received or after 6 seconds.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/database.md#_snippet_8\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndfsQuotes = loadTable(\"dfs://stockDB\", \"quotes\")\nsaveQuotesToDFS=def(mutable t, msg): t.append!(select today() as date,* from msg)\nsubscribeTable(, \"quotes_stream\", \"quotes\", -1, saveQuotesToDFS{dfsQuotes}, true, 10000, 6)\n```\n\n----------------------------------------\n\nTITLE: Configuring Cluster.cfg for Slave Cluster\nDESCRIPTION: This snippet modifies the cluster.cfg file in the slave cluster.  It configures the mode as slave and specifies the username and password used for the asynchronous replication. Requires access to the server and modification of the DolphinDB installation files.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Asynchronous_Replication.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nvim ./cluster.cfg\n```\n\n----------------------------------------\n\nTITLE: Configuring DolphinDB Cluster Node Parameters in cluster.cfg File\nDESCRIPTION: This snippet provides core configuration options for DolphinDB cluster nodes, including disabling LAN clustering, setting maximum publisher connections, maximum memory allocation, number of worker threads, partition policy adjustment, and caching engine memory size. These settings affect cluster behavior, performance, and resource management. The snippet is typical for tuning cluster nodes to optimize distributed database operations. Accurate parameter tuning should consider underlying hardware and workload.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/iot_examples.md#_snippet_29\n\nLANGUAGE: cfg\nCODE:\n```\nlanCluster=0\nmaxPubConnections=64\nmaxMemSize=16\nworkerNum=4\nnewValuePartitionPolicy=add\nchunkCacheEngineMemSize=4\n```\n\n----------------------------------------\n\nTITLE: Installing Miniconda on Windows Command Line\nDESCRIPTION: Guidance on installing Miniconda for Windows via GUI, setting environment variables to add conda to PATH, and verifying installation through conda -V command. Dependencies include access to the Miniconda installer executable.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/python_api_install_offline.md#_snippet_5\n\nLANGUAGE: Batch Script\nCODE:\n```\nREM Downloading and running Miniconda installer manually\n# Add conda to PATH\nset PATH=%PATH%;D:\\ProgramData\\Miniconda3\\condabin\n\n# Verify conda version\necho %PATH%\nconda -V\n```\n\n----------------------------------------\n\nTITLE: Implementing a Streaming Data Handler Plugin Function in DolphinDB C++\nDESCRIPTION: This C++ function acts as a handler to process streaming data in the DolphinDB environment. It accepts a tuple message, selects specific elements by indices, and appends them as columns into a target table. The function uses the `append` interface of the DolphinDB Table API, handling the insertion results but omitting detailed error processing in the base implementation. This snippet requires including DolphinDB plugin SDK headers and setting up the plugin environment properly.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_development_tutorial.md#_snippet_14\n\nLANGUAGE: cpp\nCODE:\n```\nConstantSP handler(Heap *heap, vector<ConstantSP> &args) {\n\tConstantSP indices = args[0];\n\tTableSP table = args[1];\n\tConstantSP msg = args[2];\n\n\tvector<ConstantSP> msgToAppend;\n\tfor (int i = 0; i < indices->size(); i++) {\n\t\tint index = indices->getIndex(i)\n\t\tmsgToAppend.push_back(msg->get(index));\n\t}\n\n\tINDEX insertedRows;\n\tstring errMsg;\n\ttable->append(msgToAppend, insertedRows, errMsg);\n\treturn new Void();\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Partitioned Database and Table in DolphinDB (DolphinDB Script)\nDESCRIPTION: This DolphinDB script logs into a data node's interactive programming interface using default admin credentials, deletes any existing database named 'dfs://testDB', then creates a new partitioned database covering dates in 2021. It defines schema column names and data types for stock market data columns. The script creates a partitioned table with partition column DateTime, preparing the node to store structured stock data efficiently for time-series partitioned queries.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ha_cluster_deployment.md#_snippet_11\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 创建存储的数据库和分区表\nlogin(\"admin\", \"123456\")\ndbName = \"dfs://testDB\"\ntbName = \"testTB\"\nif(existsDatabase(dbName)){\n        dropDatabase(dbName)\n}\ndb = database(dbName, VALUE, 2021.01.01..2021.12.31)\ncolNames = `SecurityID`DateTime`PreClosePx`OpenPx`HighPx`LowPx`LastPx`Volume`Amount\ncolTypes = [SYMBOL, DATETIME, DOUBLE, DOUBLE, DOUBLE, DOUBLE, DOUBLE, INT, DOUBLE]\nschemaTable = table(1:0, colNames, colTypes)\ndb.createPartitionedTable(table=schemaTable, tableName=tbName, partitionColumns=`DateTime)\n```\n\n----------------------------------------\n\nTITLE: Identifying start times and filling null values\nDESCRIPTION: This code snippet identifies the start times of each group by comparing the results of `accumulate(caclCumVol{1500000}, volume)` with the original 'volume' vector. It uses an 'iif' statement to assign 'timex' values (representing start times) where the cumulative value equals the volume, otherwise assigns NULL. The ffill() forward fills the null values in the result to propagate the last valid start time.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_50\n\nLANGUAGE: shell\nCODE:\n```\niif(accumulate(caclCumVol{1500000}, volume) ==volume, timex, NULL).ffill()\n```\n\n----------------------------------------\n\nTITLE: Creating and Using a Function View\nDESCRIPTION: This code defines a function view named `task1`.  The `task1` view retrieves data from the `sensors` table based on a MachineId, performs calculations (average, pivot, rename, and multiple mathematical functions) and returns the results. Function views encapsulate a database query with computation, and can be stored and reused across sessions. This snippet shows how to create, define, and use the function view for a reusable and modular query.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/iot_examples.md#_snippet_19\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef task1(MachineId){\n\tt=select avg(value) from loadTable(\"dfs://svmDemo\",\"sensors\")  \n\twhere id in 1..6 + (MachineId-1)*50, datetime >datetimeAdd(now(),-1,`h).datetime()   \n\tpivot by datetime.minute() as time, id\n\tt.rename!(`time`tag1`tag2`tag3`tag4`tag5`tag6)\n\treturn select time, log(tag1) as target1, sqrt(tag2) as target2, tan(tag3) as target3, sin(tag4) as target4, floor(tag5) as target5, abs(tag6) as target6 from t\n}\naddFunctionView(task1)\n```\n\n----------------------------------------\n\nTITLE: Batch Updating Integer Vector Data with getIntBuffer and setInt C++\nDESCRIPTION: Presents the most efficient method for batch updates by first obtaining a pointer to the vector's internal buffer using `getIntBuffer`, modifying the data directly in the buffer, and then calling `setInt` with the same buffer pointer. This minimizes memory copies when the requested range aligns with internal segments.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_advance.md#_snippet_17\n\nLANGUAGE: C++\nCODE:\n```\nconst int size = 100000000;\nconst int BUF_SIZE = 1024;\nint buf[1024];\nVectorSP pVec = Util::createVector(DT_INT, size);\nint start = 0;\nwhile(start < size) {\n    int len =  std::min(size - start, BUF_SIZE);\n    int* p = pVec->getIntBuffer(start, len, buf);\n    for(int i = 0; i < len; ++i) {\n        p[i] = i;\n    }\n    pVec->setInt(start, len, p);\n    start += len;\n}\n```\n\n----------------------------------------\n\nTITLE: Computing DoubleEMA Factor with SQL Context Grouping - DolphinDB\nDESCRIPTION: This code snippet demonstrates the use of a custom factor function within a DolphinDB SQL query, leveraging `context by` for grouped time series computation per security. The factorDoubleEMA function chains exponential moving average (ema) calculations of closing prices and applies further smoothing and differencing. Inputs are provided as security-specific close price series from the k_minute table. Dependencies: DolphinDB built-ins such as ema, and the data must be partitioned by securityid. Outputs include tradetime, securityid, factor name, and value.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practice_for_factor_calculation.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef sum_diff(x, y){\n    return (x-y)\\(x+y)\n}\n\n@state\ndef factorDoubleEMA(price){\n    ema_2 = ema(price, 2)\n    ema_4 = ema(price, 4)\n    sum_diff_1000 = 1000 * sum_diff(ema_2, ema_4)\n    return ema(sum_diff_1000, 2) - ema(sum_diff_1000, 3)\n}\n\nres = select tradetime, securityid, `doubleEMA as factorname, factorDoubleEMA(close) as val from loadTable(\"dfs://k_minute\",\"k_minute\") where  tradetime between 2020.01.01 : 2020.01.31 context by securityid\n\n```\n\n----------------------------------------\n\nTITLE: Filtering Stream Data with Custom Handler in DolphinDB\nDESCRIPTION: This example demonstrates how to filter stream data during subscription using a custom handler function. It sets up a stream table capturing voltage and current data, then defines a handler to discard invalid records where voltage exceeds 122 or current is NULL before inserting valid records into the processing table. The resulting aggregation computes average voltage and current over sliding windows, showing filtered data inputs effectively reduce invalid data processing in real-time stream analytics.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/stream_aggregator.md#_snippet_9\n\nLANGUAGE: DolphinDB\nCODE:\n```\nshare streamTable(1000:0, `time`voltage`current, [TIMESTAMP, DOUBLE, DOUBLE]) as electricity\noutputTable = table(10000:0, `time`avgVoltage`avgCurrent, [TIMESTAMP, DOUBLE, DOUBLE])\n\ndef append_after_filtering(inputTable, msg){\n\tt = select * from msg where voltage>122, isValid(current)\n\tif(size(t)>0){\n\t\tinsert into inputTable values(t.time,t.voltage,t.current)\t\n\t}\n}\nelectricityAggregator = createTimeSeriesEngine(name=\"electricityAggregator\", windowSize=6, step=3, metrics=<[avg(voltage), avg(current)]>, dummyTable=electricity, outputTable=outputTable, timeColumn=`time, garbageSize=2000)\nsubscribeTable(tableName=\"electricity\", actionName=\"avgElectricity\", offset=0, handler=append_after_filtering{electricityAggregator}, msgAsTable=true)\n\n//模拟产生数据\ndef writeData(t, n){\n        timev = 2018.10.08T01:01:01.001 + timestamp(1..n)\n        voltage = 120+1..n * 1.0\n        current = take([1,NULL,2]*0.1, n)\n        insert into t values(timev, voltage, current);\n}\nwriteData(electricity, 10)\n\n//流数据表：\n// select * from electricity\n```\n\n----------------------------------------\n\nTITLE: Querying Daily Order and Passenger Counts in DolphinDB for Grafana\nDESCRIPTION: These two DolphinDB SQL-like queries calculate aggregate statistics for the latest date in `predictTable`. The first query counts the total number of trips (orders), and the second query sums the passenger count for all trips on that day. These are intended for display as key performance indicators (KPIs) in Grafana.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Forecast_of_Taxi_Trip_Duration.md#_snippet_3\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nselect count(*) from predictTable \nwhere date(predictTable.pickup_datetime) == date(select max(pickup_datetime) from predictTable)\nselect sum(passenger_count) from predictTable \nwhere date(predictTable.pickup_datetime) == date(select max(pickup_datetime) from predictTable)\n```\n\n----------------------------------------\n\nTITLE: Extracting Oracle Instant Client Packages - Shell\nDESCRIPTION: Creates directories for Oracle Instant Client and supporting files, then unzips the downloaded client and ODBC archives into the target location. These steps are required prior to driver registration and configuration. Ensure you have the appropriate permissions and unpack to the correct paths.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Oracle_to_DolphinDB.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\nmkdir /usr/local/oracle\nmkdir /etc/oracle\nunzip instantclient-basic-linux.x64-21.7.0.0.0dbru.zip -d /usr/local/oracle/\nunzip instantclient-odbc-linux.x64-21.7.0.0.0dbru.zip -d /usr/local/oracle/\n\n```\n\n----------------------------------------\n\nTITLE: DolphinDB Multiple Conditions Filtering\nDESCRIPTION: This DolphinDB script filters data using multiple conditions: date range, PERMNO range, and a VOL range. The date and PERMNO filtering are combined using AND logic, and this combined logic is ORed with the VOL filtering. The script measures the execution time using the `timer()` function, running the query 10 times. The goal is to test complex filtering scenarios.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_39\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\n//根据股票代码、浮点型和时间过滤\ntimer(10) select * from trades where date between 2015.05.12:2016.05.12 and PERMNO between 23240:30000 or VOL between 1500000:2000000\n```\n\n----------------------------------------\n\nTITLE: Initializing Single-Table Replay in Python\nDESCRIPTION: This code snippet demonstrates a basic single-table replay using the `replay` function in DolphinDB. It reads data from a database table, `trade`, for a specific date and replays it into a stream table, `tradeStream`. The function `replayDS` is used to create a replay data source from the database table. The `replay` function is then called to inject data into the stream table at a specified rate.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/stock_market_replay.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntradeDS = replayDS(sqlObj=<select * from loadTable(\"dfs://trade\", \"trade\") where Date = 2020.12.31>, dateColumn=`Date, timeColumn=`Time)\nreplay(inputTables=tradeDS, outputTables=tradeStream, dateColumn=`Date, timeColumn=`Time, replayRate=10000, absoluteRate=true)\n```\n\n----------------------------------------\n\nTITLE: Creating Database and Partitioned Tables for XTP Market Data\nDESCRIPTION: Script to create databases and partitioned tables for different types of XTP market data including actual market data, entrust data, trade data, state data, order book, index market data, option market data, and bond market data, with appropriate partition schemes.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/xtp.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(\"admin\", \"123456\")\n// 现货快照 + 逐笔成交 + 逐笔委托 + 逐笔状态 + 订单簿\n// 现货快照包含：股票、基金ETF、可转债的快照数据\n// 创建数据库\nif (!existsDatabase(\"dfs://XTP.actual\")) {\n    dbVALUE = database(, VALUE, 2023.01.01..2023.01.02)\n    dbHASH = database(, HASH, [SYMBOL, 50])\n    db = database(\"dfs://XTP.actual\", COMPO, [dbVALUE, dbHASH], , `TSDB)\n}\nelse {\n    db = database(\"dfs://XTP.actual\")\n}\n// 获取表结构\nactualSchema = table(1:0, XTP::getSchema(`actualMarketData)['name'], XTP::getSchema(`actualMarketData)['typeString'])\nentrustSchema = table(1:0, XTP::getSchema(`entrust)['name'], XTP::getSchema(`entrust)['typeString'])\ntradeSchema = table(1:0, XTP::getSchema(`trade)['name'], XTP::getSchema(`trade)['typeString'])\nstateSchema = table(1:0, XTP::getSchema(`state)['name'], XTP::getSchema(`state)['typeString'])\norderBookSchema = table(1:0, XTP::getSchema(`orderBook)['name'], XTP::getSchema(`orderBook)['typeString'])\n// 创建分区表\nif (existsTable(\"dfs://XTP.actual\", \"actualMarketData\")) {\n    actualPt = loadTable(\"dfs://XTP.actual\", \"actualMarketData\")\n}\nelse {\n    actualPt = db.createPartitionedTable(actualSchema, \"actualMarketData\", `dataTime`ticker, {dataTime: \"delta\"}, `ticker`dataTime)\n}\nif (existsTable(\"dfs://XTP.actual\", \"entrust\")) {\n    entrustPt = loadTable(\"dfs://XTP.actual\", \"entrust\")\n}\nelse {\n    entrustPt = db.createPartitionedTable(entrustSchema, \"entrust\", `dataTime`ticker, {seq: \"delta\", dataTime: \"delta\", entrustSeq: \"delta\"}, `ticker`dataTime)\n}\nif (existsTable(\"dfs://XTP.actual\", \"trade\")) {\n    tradePt = loadTable(\"dfs://XTP.actual\", \"trade\")\n}\nelse {\n    tradePt = db.createPartitionedTable(tradeSchema, \"trade\", `dataTime`ticker, {seq: \"delta\", dataTime: \"delta\", tradeSeq: \"delta\"}, `ticker`dataTime)\n}\nif (existsTable(\"dfs://XTP.actual\", \"state\")) {\n    statePt = loadTable(\"dfs://XTP.actual\", \"state\")\n}\nelse {\n    statePt = db.createPartitionedTable(stateSchema, \"state\", `dataTime`ticker, {seq: \"delta\", dataTime: \"delta\"}, `ticker`dataTime)\n}\nif (existsTable(\"dfs://XTP.actual\", \"orderBook\")) {\n    orderBookPt = loadTable(\"dfs://XTP.actual\", \"orderBook\")\n}\nelse {\n    orderBookPt = db.createPartitionedTable(orderBookSchema, \"orderBook\", `dataTime`ticker, {dataTime: \"delta\"}, `ticker`dataTime)\n}\n\n// 指数快照\nif (!existsDatabase(\"dfs://XTP.index\")) {\n    dbVALUE = database(, VALUE, 2023.01.01..2023.01.02)\n    dbHASH = database(, HASH, [SYMBOL, 5])\n    db = database(\"dfs://XTP.index\", COMPO, [dbVALUE, dbHASH], , `TSDB)\n}\nelse {\n    db = database(\"dfs://XTP.index\")\n}\nindexSchema = table(1:0, XTP::getSchema(`indexMarketData)['name'], XTP::getSchema(`indexMarketData)['typeString'])\nif (existsTable(\"dfs://XTP.index\", \"indexMarketData\")) {\n    indexPt = loadTable(\"dfs://XTP.index\", \"indexMarketData\")\n}\nelse {\n    indexPt = db.createPartitionedTable(indexSchema, \"indexMarketData\", `dataTime`ticker, {dataTime: \"delta\"}, `ticker`dataTime)\n}\n\n// 期权快照\nif (!existsDatabase(\"dfs://XTP.option\")) {\n    dbVALUE = database(, VALUE, 2023.01.01..2023.01.02)\n    dbHASH = database(, HASH, [SYMBOL, 5])\n    db = database(\"dfs://XTP.option\", COMPO, [dbVALUE, dbHASH], , `TSDB)\n}\nelse {\n    db = database(\"dfs://XTP.option\")\n}\noptionSchema = table(1:0, XTP::getSchema(`optionMarketData)['name'], XTP::getSchema(`optionMarketData)['typeString'])\nif (existsTable(\"dfs://XTP.option\", \"optionMarketData\")) {\n    optionPt = loadTable(\"dfs://XTP.option\", \"optionMarketData\")\n}\nelse {\n    optionPt = db.createPartitionedTable(optionSchema, \"optionMarketData\", `dataTime`ticker, {dataTime: \"delta\"}, `ticker`dataTime)\n}\n\n// 债券快照\nif (!existsDatabase(\"dfs://XTP.bond\")) {\n    dbVALUE = database(, VALUE, 2023.01.01..2023.01.02)\n    dbHASH = database(, HASH, [SYMBOL, 20])\n    db = database(\"dfs://XTP.bond\", COMPO, [dbVALUE, dbHASH], , `TSDB)\n}\nelse {\n    db = database(\"dfs://XTP.bond\")\n}\nbondSchema = table(1:0, XTP::getSchema(`bondMarketData)['name'], XTP::getSchema(`bondMarketData)['typeString'])\nif (existsTable(\"dfs://XTP.bond\", \"bondMarketData\")) {\n    bondPt = loadTable(\"dfs://XTP.bond\", \"bondMarketData\")\n}\nelse {\n    bondPt = db.createPartitionedTable(bondSchema, \"bondMarketData\", `dataTime`ticker, {dataTime: \"delta\"}, `ticker`dataTime)\n}\n```\n\n----------------------------------------\n\nTITLE: Selecting Data from a Table in DolphinDB\nDESCRIPTION: This code snippet demonstrates a basic SQL select statement to retrieve all data from a table named 't' in DolphinDB.  Note that DolphinDB SQL is case-insensitive, but the tutorial specifies lowercase.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Single_Node_Tutorial.md#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect * from t\n```\n\n----------------------------------------\n\nTITLE: Declaring a DolphinDB Module\nDESCRIPTION: Defines the module name for the current script file. This statement must be the first line in a DolphinDB module file (.dos or .dom). The module name must match the file name (without the extension).\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/module_tutorial.md#_snippet_0\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nmodule fileLog\n```\n\n----------------------------------------\n\nTITLE: Retrieving Table Schemas from DolphinDB Databases in DolphinDB\nDESCRIPTION: This code defines a function to load table schemas from specified DolphinDB database paths, returning a minimal table with column names and data types. It extracts schemas for 'trade' and 'snapshot' tables for downstream processing.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/stock_market_replay/03.calTradeCost_lookUpJoin.txt#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef createSchemaTable(dbName, tableName){\n\tschema = loadTable(dbName, tableName).schema().colDefs\n\treturn table(1:0, schema.name, schema.typeString)\n}\ntradeSchema = createSchemaTable(\"dfs://trade\", \"trade\") \nsnapshotSchema = createSchemaTable(\"dfs://snapshot\", \"snapshot\") \n\n```\n\n----------------------------------------\n\nTITLE: Querying Top N Records (TSDB Engine)\nDESCRIPTION: This code snippet retrieves the latest 10 records for each SecurityID using the TSDB storage engine. The TSDB engine provides better performance for time-related queries due to its built-in sorting.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_sql_cases.md#_snippet_10\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntimer t2 = select * from loadTable(\"dfs://Level1_TSDB\", \"Snapshot\") where date(DateTime) = 2020.06.01 context by SecurityID csort DateTime limit -10 \n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication - DolphinDB\nDESCRIPTION: This code demonstrates matrix multiplication in DolphinDB using the `dot` or `**` operator. The number of columns in the first matrix must equal the number of rows in the second matrix. The code shows examples of matrix-vector and matrix-matrix multiplication.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/matrix.md#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\n>x=1..6$2:3;\n>y=1 2 3;\n>x;\n#0 #1 #2\n-- -- --\n1  3  5\n2  4  6\n>y;\n[1,2,3]\n>x dot y;\n#0\n--\n22\n28\n\n>y=6..1$3:2;\n>y;\n#0 #1\n-- --\n6  3\n5  2\n4  1\n\n>x**y;\n#0 #1\n-- --\n41 14\n56 20\n\n>y**x;\n#0 #1 #2\n-- -- --\n12 30 48\n9  23 37\n6  16 26\n```\n\n----------------------------------------\n\nTITLE: 计算年度平均收益率矩阵\nDESCRIPTION: 对每只基金的日收益率进行年度重采样，计算年度平均收益率，并按基金类型分组统计。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/public_fund_basic_analysis.md#_snippet_17\n\nLANGUAGE: DolphinDB\nCODE:\n```\nyearReturnsMatrix = ((returnsMatrix+1).resample(\"A\", prod)-1).nullFill(0).regroup(fundTypeMap[returnsMatrix.colNames()], mean, byRow=false)\n```\n\n----------------------------------------\n\nTITLE: Create Memory Table for Binary Import\nDESCRIPTION: This snippet creates a memory table to hold data imported from a binary file using `readRecord!`.  The schema of the table must match the structure of the binary data. This function does not support string types.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_data.md#_snippet_8\n\nLANGUAGE: txt\nCODE:\n```\ntb=table(1000:0, `id`date`time`last`volume`value`ask1`ask_size1`bid1`bid_size1, [INT,INT,INT,FLOAT,INT,FLOAT,FLOAT,INT,FLOAT,INT])\n```\n\n----------------------------------------\n\nTITLE: 实现降采样与滑动平均的时序数据分析 - DolphinDB 脚本\nDESCRIPTION: 本示例利用 DolphinDB 的 bar 函数实现固定时间间隔的降采样（如每30秒一条记录），并用 mavg 函数计算滑动平均，解析设备信号的局部均值。参数包括时间字段、采样周期和窗口大小，适用于降低数据复杂度、去噪处理及实时平滑分析，依赖 DolphinDB 内建函数支持。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/high_freq_data_storage_and_analysis.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\n/**** 数据分析 ****/\n\n//每 30 秒钟，取一条数据（降采样）\nselect ts,v1 from pt context by bar(ts,30s) limit -1        //limit -1 代表取最后一条记录\n\n//30 秒钟滑动平均计算\ntimer t=select ts,deviceid,mavg(v1,30) as val from pt where deviceid=deviceid , ts between 2022.01.01 00:00:00.000 : 2022.01.01 00:01:00.000 context by deviceid\n```\n\n----------------------------------------\n\nTITLE: Reading Binary Data File and Creating DolphinDB Table in Plugin C++\nDESCRIPTION: This function loads binary data from a file into DolphinDB's memory by creating a block file input stream limited to the specified range, then reading the data chunk by chunk. It manages reading buffers of fixed size and supports optional parameters to determine starting row and length. The snippet includes only partial reading logic and assumes further processing to parse and convert raw bytes into DolphinDB table columns. Proper handling of close, errors, or final data assembly is expected in full implementation.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_development_tutorial.md#_snippet_18\n\nLANGUAGE: cpp\nCODE:\n```\nConstantSP loadMyData(Heap *heap, vector<ConstantSP> &args) {\n\tConstantSP path = args[0];\n\tlong long fileLength = Util::getFileLength(path->getString());\n\tsize_t bytesPerRow = 32;\n\n\tint start = args.size()>= 2 ? args[1]->getInt() : 0;\n\tint length = args.size()>= 3 ? args[2]->getInt() : fileLength / bytesPerRow - start;\n\n\tDataInputStreamSP inputStream = Util::createBlockFileInputStream(path->getString(), 0, fileLength, Util::BUF_SIZE, start * bytesPerRow, length * bytesPerRow);\n\tchar buf[Util::BUF_SIZE];\n\tsize_t actualLength;\n\n\twhile (true) {\n\t\tinputStream->readBytes(buf, Util::BUF_SIZE, actualLength);\n\t\tif (actualLength <= 0)\n\t\t\tbreak;\n\t\t// ...\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: Getting recent jobs\nDESCRIPTION: This code retrieves the statuses of the three most recent batch processing jobs on the local node.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_59\n\nLANGUAGE: shell\nCODE:\n```\ngetRecentJobs(3);\n```\n\n----------------------------------------\n\nTITLE: Behavior Identifier String Format\nDESCRIPTION: Describes the format for behavior identifiers, included after the '报文指令长度' of the request message. This string provides server instructions, such as flags, cancellable status, priority, and parallelism, controlling the execution of scripts or functions. The example is \"/ 4_1_8_8__10000\\n\".\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/api_protocol.md#_snippet_10\n\nLANGUAGE: text\nCODE:\n```\n标识名 | 标识范围 | 说明 | 样本\n---|---|---|---\nflag | (0，1，2，4，8，16)任意数字组的和 | 一组开关量标识，按位取值， 参考flag表格 | 4\ncancellable |0,1 | 任务是否可以取消 | 1\npriority | 0~8  |指定本任务优先级 | 8\nparallelism | 0~64 | 指定本任务并行度| 8\nrootId | 整数 | 根任务编号，内部使用，API中固定为空| 12\nfetchSize | | 指定分块返回的块大小| 10000\noffset | | API中固定为空|\n```\n\n----------------------------------------\n\nTITLE: Checking Plugin Path Configuration\nDESCRIPTION: Command to verify the plugin path configuration in the Kafka Connect properties file to ensure plugins are loaded correctly.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Debezium_and_Kafka_data_sync.md#_snippet_14\n\nLANGUAGE: Bash\nCODE:\n```\ncat /KFDATA/kafka-connect/etc/kafka-connect.properties |grep plugin\n```\n\n----------------------------------------\n\nTITLE: Enabling and Restarting NTPD Service on Linux (console)\nDESCRIPTION: Enable NTPD to start on boot and immediately restart it using `systemctl`, ensuring time synchronization services are active after configuration. This operation must be repeated on every cluster node.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/deploy_dolphindb_on_new_server.md#_snippet_8\n\nLANGUAGE: console\nCODE:\n```\n# systemctl enable ntpd\n# systemctl restart ntpd\n\n```\n\n----------------------------------------\n\nTITLE: Granting TABLE_READ Permission to All Objects - DolphinDB\nDESCRIPTION: This snippet shows how to grant TABLE_READ permission to a user for all objects using the '*' wildcard.  It also demonstrates how to revoke this permission. Requires admin privileges to create user.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Permission_Management.md#_snippet_25\n\nLANGUAGE: DolphinDB\nCODE:\n```\ngrant(\"JoeFlacco\",TABLE_READ,\"*\")  \nrevoke(\"JoeFlacco\",TABLE_READ,\"*\")\n```\n\n----------------------------------------\n\nTITLE: Checking OLAP Cache Engine Memory Usage in DolphinDB\nDESCRIPTION: The getOLAPCacheEngineSize function returns the current memory usage of the OLAP cache engine, helping to monitor resource utilization.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/redoLog_cacheEngine.md#_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\ngetOLAPCacheEngineSize()\n```\n\n----------------------------------------\n\nTITLE: Validating Processed Results - DolphinDB\nDESCRIPTION: This snippet manually recalculates the expected timeout events based on the raw `doorRecord` data using `context by` and `deltas` to find time differences and `prev` to get previous values. It then filters for timeout conditions and compares the result to the contents of the `outputSt1` table using `eqObj` to verify the correctness of the stream processing pipeline.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/streaming_engine_anomaly_alerts/流计算引擎级联检测门禁超时数据.txt#_snippet_13\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//验证结果正确性\nt=select *,deltas(eventDate),prev(doorNum),prev(eventDate),prev(doorEventCode) from doorRecord context by doorNum \nresultTable=select prev_doorNum as doorNum,prev_eventDate+300 as eventDate,prev_doorEventCode  as doorEventCode from t where deltas_eventDate>= 300 and prev_doorEventCode in [11,12,56,60,65,67] and prev(doorEventCode)!=doorEventCode order by eventDate\neqObj(resultTable.values(),outputSt1.values())\n```\n\n----------------------------------------\n\nTITLE: 使用segment函数实现连续区间最值计算\nDESCRIPTION: 利用segment函数对大于等于目标值的连续区间进行分组，并使用context by和having子句筛选出每个区间内最大值的第一条记录。这是优化后的实现。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_SQL_Case_Tutorial.md#_snippet_27\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntimer(1000) res2 = select * from t \n\t\t\t\t   context by segment(value >= targetVal) \n\t\t\t\t   having value >= targetVal and value = max(value) limit 1\n```\n\n----------------------------------------\n\nTITLE: Loading DolphinDB ODBC Plugin\nDESCRIPTION: This snippet loads the ODBC plugin required to connect to external databases like Amazon Redshift.  The plugin is loaded from the specified file path.  The code utilizes the `loadPlugin` function with the path to the plugin's configuration file. Successful execution will make odbc functions available to DolphinDB.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/Migrate_data_from_Redshift_to_DolphinDB/Redshift2DDB.txt#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//加载插件\nloadPlugin(\"./plugins/odbc/PluginODBC.txt\")\n```\n\n----------------------------------------\n\nTITLE: Identifying midpoints using window function\nDESCRIPTION: This code uses the window function to identify midpoints in the 'id' column. A point is considered a midpoint if its value is the minimum value within a window of 5 values before and 5 values after the current value. The `iif` statement assigns 1 to midpoints and 0 to other points.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_54\n\nLANGUAGE: shell\nCODE:\n```\nselect *, iif(id==window(min, id, -4:4), 1, 0) as mid from t\n```\n\n----------------------------------------\n\nTITLE: DolphinDB Table-Level Deny over Global Grant\nDESCRIPTION: This example shows that a table-level `deny` overrides a global `grant`. User1 initially has global read access due to the `grant`, but after the table-level `deny` on `dfs://test/pt`, user1 is denied read access to that specific table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Permission_Management.md#_snippet_10\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(\"admin\", \"123456\")\ncreateUser(\"user1\",\"123456\")\ndbName = \"dfs://test\"\nt = table(1..10 as id , rand(100, 10) as val)\nif(existsDatabase(dbName)){\n        dropDatabase(dbName)\n}\ndb=database(dbName, VALUE, 1..10)\npt=  db.createPartitionedTable(t, \"pt\", \"id\")\npt.append!(t)\ngrant(\"user1\", TABLE_READ, \"*\")\ndeny(\"user1\", TABLE_READ, dbName+\"/pt\")\n\nlogin(\"user1\", \"123456\")\nselect * from loadTable(dbName, \"pt\")//user1被禁止读\"dfs://test/pt\"\n```\n\n----------------------------------------\n\nTITLE: Computing Weighted Skewness Factor with Array Vector in DolphinDB\nDESCRIPTION: Implements weighted covariance and skewness calculations for multi-level order book data. Demonstrates the efficiency gain of using array vectors instead of multiple columns for Level 2 market data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practice_for_factor_calculation.md#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef mathWghtCovar(x, y, w){\n\tv = (x - rowWavg(x, w)) * (y - rowWavg(y, w))\n\treturn rowWavg(v, w)\n}\n\n@state\ndef mathWghtSkew(x, w){\n\tx_var = mathWghtCovar(x, x, w)\n\tx_std = sqrt(x_var)\n\tx_1 = x - rowWavg(x, w)\n\tx_2 = x_1*x_1\n\tlen = size(w)\n\tadj = sqrt((len - 1) * len) \\ (len - 2)\n\tskew = rowWsum(x_2, x_1) \\ (x_var * x_std) * adj \\ len\n\treturn iif(x_std==0, 0, skew)\n}\n\n//weights:\nw = 10 9 8 7 6 5 4 3 2 1\n\n//权重偏度因子：\nresWeight =  select TradeTime, SecurityID, `mathWghtSkew as factorname, mathWghtSkew(BidPrice, w)  as val from loadTable(\"dfs://LEVEL2_Snapshot_ArrayVector\",\"Snap\")  where date(TradeTime) = 2020.01.02 map\nresWeight1 =  select TradeTime, SecurityID, `mathWghtSkew as factorname, mathWghtSkew(matrix(BidPrice0,BidPrice1,BidPrice2,BidPrice3,BidPrice4,BidPrice5,BidPrice6,BidPrice7,BidPrice8,BidPrice9), w)  as val from loadTable(\"dfs://snapshot_SH_L2_TSDB\", \"snapshot_SH_L2_TSDB\")  where date(TradeTime) = 2020.01.02 map\n```\n\n----------------------------------------\n\nTITLE: Calculating Maximum Drawdown in DolphinDB\nDESCRIPTION: Defines a DolphinDB function `getMaxDrawdown` to calculate the maximum drawdown from a peak. It takes a vector `value` (daily net values), finds the cumulative maximum, calculates drawdowns relative to the cumulative max, identifies the maximum drawdown point `i`, finds the preceding peak `j`, and computes the drawdown percentage. Returns 0 if no drawdown occurs.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/fund_factor_contrasted_by_py.md#_snippet_10\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef getMaxDrawdown(value){\n\ti = imax((cummax(value) - value) \\ cummax(value))\n\tif (i==0){\n\t\treturn 0\n\t}\n\tj = imax(value[:i])\n\treturn (value[j] - value[i]) \\ (value[j])\n}\n```\n\n----------------------------------------\n\nTITLE: Creating and Dropping Function Views in DolphinDB Script\nDESCRIPTION: Demonstrates how to create a function view (`addFunctionView`) using the previously defined `funRecodeData` function, enabling controlled access to data operations, mimicking stored procedure behavior. Also shows how to remove the function view (`dropFunctionView`).\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/high_freq_data_storage_and_analysis.md#_snippet_9\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\n/**** 存储过程的实现（函数视图） ****/\n\n//建立函数视图（存储过程），可通过外部 API 调用该函数\naddFunctionView(funRecodeData)\n\n//删除函数视图（存储过程）\ndropFunctionView(`funRecodeData)\n```\n\n----------------------------------------\n\nTITLE: Creating Reactive State Engine in DolphinDB\nDESCRIPTION: Creates a reactive state engine named `reactivEngine` to monitor sensor state changes. It uses `stream01` as the input table, `outputSt2` as the output table, groups data by the `tag` column, and filters results where the `value` is different from the previous `value`.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_streaming_application_in_IOT.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\nreactivEngine = createReactiveStateEngine(name=`reactivEngine, metrics=<[ts, value]>, dummyTable=stream01,\n      outputTable= outputSt2,keyColumn= \"tag\",filter=<value!=prev(value) && prev(value)!=NULL>)\n```\n\n----------------------------------------\n\nTITLE: Single-threaded Batch Writing of Time Series Data in DolphinDB Script\nDESCRIPTION: Defines a function for single-threaded data append operations to a DolphinDB partitioned table. Dependencies are prior creation of the target database and table as well as the data generation function. The function loops over days and machine partitions, generating and appending data in manageable batches. Inputs include IDs, range specs, and table/database names; output is the database increasingly populated by appended data. Assumes the table can efficiently absorb append operations without explicit locking.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/multipleValueModeWrite.txt#_snippet_2\n\nLANGUAGE: dolphindb\nCODE:\n```\ndef singleThreadWriting(id, startDay, days, freqPerDay, numMachinesPerPartition,numMetrics,dbName,tableName){\n\tt = loadTable(dbName,tableName)\n\tidSize=size(id)\n\n\tfor(d in 0:days){\n\t\tindex=0\n\t\tdo{\n\t\t\tidMax= numMachinesPerPartition - 1\n\t\t\tif(idSize - index <= 9) idMax = idSize - index - 1 \n\t\t\tt.append!(generate1DayData(startDay + d,id[index+0..idMax],freqPerDay,numMetrics))\n\t\t\tindex +=numMachinesPerPartition\n\t\t}while (index < idSize)\n\t}\n\n}\n```\n\n----------------------------------------\n\nTITLE: Querying and Grouping Data in DolphinDB\nDESCRIPTION: This code snippet queries the loaded table for the count of data points per day. The result is returned to the client and displayed.  It demonstrates a simple aggregate query grouped by date using an SQL select statement.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/single_machine_cluster_deploy.md#_snippet_9\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// SQL 返回数据量少的时候，可以直接取回客户端展示\nselect count(*) from pt group by date(DateTime) as Date\n```\n\n----------------------------------------\n\nTITLE: Evaluating XGBoost Predictions with RMSPE Metric in DolphinDB\nDESCRIPTION: Defines a custom RMSPE (Root Mean Squared Percentage Error) function for regression model evaluation. Loads a saved Xgboost model and predicts results on the test set, then prints the evaluation score. Dependencies: Xgboost plugin, existing saved model. Inputs: Test_y (labels), Test_x (features), model file path. Outputs: RMSPE score printed to console.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/metacode_derived_features/metacode_derived_features_buildmodel.txt#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef RMSPE(a,b)\n{\n\treturn sqrt( sum( ((a-b)\\a)*((a-b)\\a) ) \\a.size()  )\n}\nmodel = xgboost::loadModel(modelSavePath)\ny_pred = xgboost::predict(model_1 , Test_x)\nprint('RMSPE='+RMSPE(Test_y, y_pred))\n```\n\n----------------------------------------\n\nTITLE: Loading Data from MySQL to DolphinDB\nDESCRIPTION: Demonstrates how to load data from MySQL into DolphinDB using the MySQL plugin, which performs automatic data type conversion between MySQL and DolphinDB temporal types.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/timezone.md#_snippet_14\n\nLANGUAGE: SQL\nCODE:\n```\n// 加载插件\nloadPlugin(\"/DolphinDB/server/plugins/mysql/PluginMySQL.txt\");\n\n// 连接 MySQL Server\nconn = mysql::connect(`127.0.0.1, 3306, `root, \"root\", `testdb)\n\n// 执行导入\ntaqDdb = mysql::load(conn, \"SELECT * FROM taqTs\");\n\n// 查看数据\nselect * from taqDdb;\n```\n\n----------------------------------------\n\nTITLE: Loading and Preparing TAQ Data Schema with Name Normalization in DolphinDB\nDESCRIPTION: This snippet loads a sample TAQ tick data CSV file by first extracting its schema via extractTextSchema, then normalizing column names to lowercase to avoid conflicts with DolphinDB reserved keywords like SYMBOL, DATE, and TIME. It creates a schema table aligned with the extracted types and applies this schema to efficiently load the CSV data. The output is a structured table with adjusted schema for safe downstream processing.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs MongoDB/dolphindb_taq_partitioned.txt#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\nFP_TAQ = '/media/xllu/aa/TAQ/original/'device_\nFP_SAMPLE_TB = FP_TAQ + 'TAQ20070807.csv'\norig_tb_schema = extractTextSchema(FP_SAMPLE_TB)\n// 查看 orig_tb_schema\n// 将列名调整为小写避免与 DolphinDB 内置的 SYMBOL, DATE, TIME 等保留关键字产生冲突\ncols = lower(orig_tb_schema.name)\nschema = table(cols, orig_tb_schema.type)\ntimer sample_tb = loadText(FP_SAMPLE_TB, , schema)\n```\n\n----------------------------------------\n\nTITLE: 定义列表分区并创建分区表（DolphinDB, LIST）\nDESCRIPTION: 这段代码演示了如何以股票代码（ticker）的枚举值列表进行列表分区，创建对应的分区表，并支持数据追加，适用于股票等分类场景。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/database.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\nn=1000000\n ticker = rand(`MSFT`GOOG`FB`ORCL`IBM,n)\n x=rand(1.0, n)\nt=table(ticker, x)\n\ndb=database(\"dfs://listdb\", LIST, [`IBM`ORCL`MSFT, `GOOG`FB])\npt = db.createPartitionedTable(t, `pt, `ticker)\npt.append!(t)\n\npt=loadTable(db,`pt)\nselect count(x) from pt;\n```\n\n----------------------------------------\n\nTITLE: Subscribing to DolphinDB Stream Table with Python\nDESCRIPTION: This Python snippet demonstrates how to subscribe to a DolphinDB stream table using the DolphinDB Python client. Required dependencies include the DolphinDB Python API and network access to the specified DolphinDB host. Key parameters include 'host' and 'port' (for target DolphinDB server), 'handler' (callback for processing incoming messages), 'tableName' (the target stream table), 'actionName' (for identifying this subscription), 'offset' (starting point for consumption), 'batchSize', 'throttle', and 'msgAsTable'. The snippet enables real-time processing of data published to the specified stream table and supports batch consumption with flow control. Setting offset to 0 consumes all records from memory, while -1 only processes new data from the subscription start.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/K.md#_snippet_23\n\nLANGUAGE: Python\nCODE:\n```\ns.subscribe(host=\"192.198.1.39\",\n            port=8988,\n            handler=handlerTestPython,\n            tableName=\"mdlStockFundOHLC\",\n            actionName=\"testStream\",\n            offset=0,\n            batchSize=2,\n            throttle=0.1,\n            msgAsTable=True)\n```\n\n----------------------------------------\n\nTITLE: Calculating Maximum Drawdown in Python\nDESCRIPTION: Defines a Python function `getMaxDrawdown` using NumPy. It mirrors the DolphinDB logic for finding the maximum drawdown: calculates cumulative maximums, finds the index `i` of the maximum relative drawdown, identifies the preceding peak index `j`, and computes the drawdown percentage. Returns 0 if no drawdown occurs. Requires NumPy.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/fund_factor_contrasted_by_py.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\ndef getMaxDrawdown(value):\n    # Ensure value is a numpy array for calculations\n    value = np.asarray(value)\n    # Handle potential division by zero if initial value is zero or negative \n    # (though unlikely for fund NAV)\n    cummax_val = np.maximum.accumulate(value)\n    # Avoid division by zero if cummax is zero\n    drawdown = (cummax_val - value) / np.where(cummax_val == 0, 1, cummax_val)\n    i = np.argmax(drawdown)\n    if drawdown[i] == 0: # Check if max drawdown is actually 0\n        return 0\n    j = np.argmax(value[:i+1]) # Find max value up to and including index i\n    # Avoid division by zero if value[j] is zero\n    return (value[j] - value[i]) / value[j] if value[j] != 0 else 0\n```\n\n----------------------------------------\n\nTITLE: Reading Vector Data in DolphinDB C++ Plugin (getInt)\nDESCRIPTION: This snippet shows how to read data from a DolphinDB vector in a C++ plugin using the `getInt(int index)` method. This method retrieves an element at a specific index.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_advance.md#_snippet_9\n\nLANGUAGE: c++\nCODE:\n```\nVectorSP pVec = Util::createVector(DT_INT, 100000000);\nint tmp;\nfor(int i = 0; i < pVec->size(); ++i) {\n    tmp = pVec->getInt(i) ;\n}\n```\n\n----------------------------------------\n\nTITLE: Checking and Disabling Swap Space on Linux (console)\nDESCRIPTION: Display system memory and swap usage with `free -h` and temporarily disable swap using `swapoff -a` for better DolphinDB performance. Swap should be disabled if sufficient RAM is available to minimize disk I/O bottlenecks.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/deploy_dolphindb_on_new_server.md#_snippet_12\n\nLANGUAGE: console\nCODE:\n```\n# free -h\n              total        used        free      shared  buff/cache   available\nMem:           7.6G        378M        7.1G        8.5M        136M        7.1G\nSwap:          7.9G          0B        7.9G\n\n```\n\nLANGUAGE: console\nCODE:\n```\n# swapoff -a\n\n```\n\nLANGUAGE: console\nCODE:\n```\n# free -h\n              total        used        free      shared  buff/cache   available\nMem:           7.6G        378M        7.1G        8.5M        136M        7.1G\nSwap:            0B          0B          0B\n\n```\n\n----------------------------------------\n\nTITLE: Migrating and Appending DolphinDB Backup Data to New Database\nDESCRIPTION: This code snippet shows how to restore a distributed DolphinDB table by first migrating a single day's backup data to a new database and then iteratively migrating subsequent days to a temporary table for appending. It includes checking migration success, appending filtered data by date range into the new main table, and dropping temporary tables to manage storage. This method supports incremental data restoration and requires the migrate function, existing backup paths, and permission to create and modify tables.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/iot_examples.md#_snippet_36\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nmigrate(backupDir=\"/hdd/hdd1/backupDemo/20200901/\",backupDBPath=\"dfs://svmDemo\",backupTableName=\"sensors\",newDBPath=\"dfs://db1\",newTableName=\"sensors\")\nday=2020.09.02\nnewTable=loadTable(\"dfs://db1\",\"sensors\")\nfor(i in 2:31){\n\tpath=\"/hdd/hdd1/backupDemo/\"+temporalFormat(day, \"yyyyMMdd\") + \"/\";\n\tday=datetimeAdd(day,1,`d)\n\tif(!exists(path)) continue;\n\tprint \"restoring \" + path;\n\tt=migrate(backupDir=path,backupDBPath=\"dfs://svmDemo\",backupTableName=\"sensors\",newDBPath=\"dfs://db1\",newTableName=\"tmp\") \n\tif(t['success'][0]==false){\n\t\tprint t['errorMsg'][0]\n\t\tcontinue\n\t}\n\tnewTable.append!(select * from loadTable(\"dfs://db1\",\"tmp\") where datetime between datetime(day-1) : (day.datetime()-1) > )\n\tdatabase(\"dfs://db1\").dropTable(\"tmp\")\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Compression in DolphinDB Java API Connection - Java\nDESCRIPTION: This Java code snippet demonstrates how to enable compression when establishing a connection to a DolphinDB server via its API. By setting the third parameter of the DBConnection constructor to true, the client-server communication will use data compression, improving network efficiency for large query results. Required dependencies include the DolphinDB Java client libraries. The snippet shows initializing the connection, establishing it, and running a SQL query that loads a table. The output is a BasicTable object containing the query results. This snippet is intended for scenarios where query latency and network bandwidth are critical, and compression can significantly reduce transmitted data volume.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/thread_model_SQL.md#_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\n//API 建立 connection 的时候将第三个参数 compress 设置为 true 即可开启压缩                     \nDBConnection connection = new DBConnection(false, false, true);        \nconnection.connect(HOST, PORT, \"admin\", \"123456\");        \nBasicTable basicTable = (BasicTable) connection.run(\"select * from loadTable(\\\"dfs://database\\\", \\\"table\\\")\");\n```\n\n----------------------------------------\n\nTITLE: Filtering Elements in Fast Array Vector Variables by Condition - DolphinDB Script\nDESCRIPTION: This DolphinDB script demonstrates how to filter each element in a Fast Array Vector using a Boolean condition. The filter x[cond] returns an array where only elements satisfying the condition are retained in subarrays; other positions are filled with empty arrays. The code also shows how to apply this operation within a table SELECT to create new columns with filtered content. Requires DolphinDB 1.30.21 or later and a Fast Array Vector variable. The input is the array and a filter expression; the output is a Fast Array Vector of filtered results.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Array_Vector.md#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\nx = array(INT[], 0).append!([1 2 3, 4 5, 6 7 8, 9 10])\nz = x[x>=5]\n/* z\n[,[5],[6,7,8],[9,10]]\n*/\n\nt = table(1 2 3 4 as id, x as x)\nnew_t = select *, x[x>=5] as newCol from t\n/* new_t\nid x       newCol \n-- ------- -------\n1  [1,2,3] []  \n2  [4,5]   [5]    \n3  [6,7,8] [6,7,8]\n4  [9,10]  [9,10] \n*/\n```\n\n----------------------------------------\n\nTITLE: Querying Data via DolphinDB Python API\nDESCRIPTION: This Python snippet demonstrates establishing a session with a DolphinDB server, running queries on distributed tables using the run method, and handling exceptions. It includes running both raw script queries and function view queries. The code shows how to select data with filtering on id and datetime columns. Dependencies include the dolphindb Python package and pandas for data handling. Inputs are connection details and DolphinDB query scripts; output is query result tables printed to console. Exception handling addresses connection failures and query execution errors. The run method automatically reconnects if no active connection exists.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/iot_examples.md#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\nimport pandas as pd\ns = ddb.session()\ntry:\n\ts.connect(\"127.0.0.1\",8848,\"admin\",\"123456\")\nexcept Exception as e:\n\tprint e\n\texit(1)\nscript=\"\"\"\nselect * from loadTable('dfs://svmDemo','sensors') \nwhere id=1, datetime between 2020.09.01T00:00:00 : 2020.09.03T23:59:59\n\"\"\"\ntry:\n\tt = s.run(script)\n\tprint(t)\n\n\tt = s.run(\"task1\",1)\n\tprint(t)\nexcept Exception as e:\n\tprint e\n```\n\n----------------------------------------\n\nTITLE: Downloading DolphinDB Linux JIT Server\nDESCRIPTION: This command downloads the JIT (Just-In-Time) version of the DolphinDB Linux server for version 2.00.11.3 using wget. It fetches the package from the DolphinDB website and stores it as dolphindb.zip. This allows to download a server with JIT enabled for optimized performance.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/standalone_server.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nwget https://www.dolphindb.cn/downloads/DolphinDB_Linux64_V2.00.11.3_JIT.zip -O dolphindb.zip\n```\n\n----------------------------------------\n\nTITLE: Creating Non-Partitioned In-Memory Table\nDESCRIPTION: Demonstrates creating a non-partitioned in-memory table using the `table` function with vectors as input. Each vector corresponds to a column in the table. Example shows creating a table with 'id' and 'x' columns.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/partitioned_in_memory_table.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\nid=`XOM`GS`AAPL\nx=102.1 33.4 73.6\ntable(id, x)\n\nid   x\n---- -----\nXOM  102.1\nGS   33.4\nAAPL 73.6\n\ntable(`XOM`GS`AAPL as id, 102.1 33.4 73.6 as x)\n\nid   x\n---- -----\nXOM  102.1\nGS   33.4\nAAPL 73.6\n```\n\n----------------------------------------\n\nTITLE: 向Table中添加数据\nDESCRIPTION: 展示如何向Table中添加数据，包括准备列数据、添加数据到列，以及使用append方法将列数据添加到表中并处理可能的错误。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/c++api.md#_snippet_7\n\nLANGUAGE: C++\nCODE:\n```\n// prepare data\nVectorSP col0 = Util::createVector(DT_INT, 0);\nVectorSP col1 = Util::createVector(DT_BOOL, 0);\n// ...\n\n// col0->append(...)\n// col1->append(...)\n// ...\n\nvector<ConstantSP> cols;\ncols.push_back(col0);\ncols.push_back(col1);\n// ...\n\nINDEX insertedRows;\nstring errorMsg;\nif(!c->append(cols, insertedRows, errorMsg)) {\n    // error handling\n}\n```\n\n----------------------------------------\n\nTITLE: Upgrading DolphinDB Server\nDESCRIPTION: This script upgrades the DolphinDB server. It offers both online and offline upgrade options, and displays interactive prompts to guide the user through the process. The user will be asked to confirm the upgrade and select the upgrade type.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/single_machine_cluster_deploy.md#_snippet_13\n\nLANGUAGE: Shell\nCODE:\n```\n./upgrade.sh\n```\n\n----------------------------------------\n\nTITLE: Getting DolphinDB Stream Table Persistence Metadata\nDESCRIPTION: Retrieves a dictionary containing detailed information about the persistence configuration and current state of the specified stream table, such as memory size, disk size, directories, and asynchronous write status.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_tutorial.md#_snippet_13\n\nLANGUAGE: DolphinDB\nCODE:\n```\ngetPersistenceMeta(pubTable);\n```\n\n----------------------------------------\n\nTITLE: Defining High Price Function for K-line Synthesis\nDESCRIPTION: This code defines a custom function `high` to calculate the high price for the 1-minute K-line based on the snapshot data. It considers the scenario where the highest price might have changed within the window, or the last price should be considered if the highest price remained unchanged. Dependencies: DolphinDB environment.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/K.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndefg high(DeltasHighPrice, HighPrice, LastPrice){\n\tif(sum(DeltasHighPrice)>0.000001){\n\t\treturn max(HighPrice)\n\t}\n\telse{\n\t\treturn max(LastPrice)\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: Nested Aggregation: Max Offer by Symbol within each Date using urllib3\nDESCRIPTION: This function demonstrates nested aggregations using `urllib3` to query Elasticsearch. It first groups documents by DATE, then within each date group, it groups by SYMBOL and calculates the maximum OFR price for each symbol. Note: Contains a potential syntax error (Chinese comma) in the inner aggregation definition.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_68\n\nLANGUAGE: python\nCODE:\n```\nimport urllib3\nimport json\n\ndef search_8():\n    http = urllib3.PoolManager()\n    data = json.dumps({\n        \"aggs\": {\n                    \"group_by_bid\": {\n                        \"terms\": {\n                            \"field\": \"DATE\",\n                            \"size\": 4\n                        },\n                        \"aggs\": {\n                            \"group_by_symbol\": {\n                                \"terms\": {\n                                    \"field\": \"SYMBOL\"\n                                }，\n\t\t\t\t\t\t\t\t\"aggs\": {\n                                \t\"max_ofr\": {\n                                    \t\"max\": {\"field\": \"OFR\"}\n                               \t \t}\n                            \t }\n                            }\n                        }\n                    }\n                },\n        \"_source\": [\"\"]\n    }).encode(\"utf-8\")\n    r = http.request(\"GET\", \"http://localhost:9200/hundred/_search\", body=data,\n                     headers={'Content-Type': 'application/json'})\n    print(r.status)\n    print(json.loads(r.data.decode()))\n```\n\n----------------------------------------\n\nTITLE: Create Distributed Database\nDESCRIPTION: This snippet creates a distributed database in DolphinDB. This is a prerequisite for using `loadTextEx`.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_data.md#_snippet_5\n\nLANGUAGE: txt\nCODE:\n```\ndb=database(\"dfs://dataImportCSVDB\",VALUE,2018.01.01..2018.01.31)\n```\n\n----------------------------------------\n\nTITLE: Time Multi-Record 20 Column Update\nDESCRIPTION: This code snippet measures the time taken to update multiple records, including 20 columns, within a loop. It updates data for multiple records based on `id` and `datetime` and the time is measured using the `timer` function.  The results are used to examine how update performance is affected by increasing the number of updated columns.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/dolphindb_update.md#_snippet_15\n\nLANGUAGE: DolphinDB\nCODE:\n```\nmachines = loadTable(\"dfs://olapDemo\", \"machines\")\ntimer{\nfor(i in 0..20)\n update machines set tag1=i,tag2=i,tag3=i,tag4=i,tag5=i,tag6=i,tag7=i,tag8=i,tag9=i,tag10=i,tag11=i,tag12=i,tag13=i,tag14=i,tag15=i,tag16=i,tag17=i,tag18=i,tag19=i,tag20=i where id in 1..5,date(datetime)=2020.09.01\n}\n```\n\n----------------------------------------\n\nTITLE: Elasticsearch Python Script for Range Filtering and Permno Aggregations\nDESCRIPTION: This Python code creates a JSON query to filter data based on VOL range, performs a terms aggregation on PERMNO with average ASK, sorts results ascending by PERMNO, and carries out a search request. It helps evaluate the performance of range filters and nested aggregations in Elasticsearch.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_58\n\nLANGUAGE: Python\nCODE:\n```\ndata = json.dumps({\n    \"query\": {\n        \"constant_score\": {\n            \"filter\": {\n                \"range\": {\n                    \"VOL\": {\n                        \"gte\": 800000,\n                        \"lte\": 1000000\n                    }\n                }\n            }\n        }\n    },\n    \"aggs\": {\n        \"group_by_permno\": {\n            \"terms\": {\n                \"field\": \"PERMNO\",\n                \"size\": 13314\n            },\n            \"aggs\": {\n                \"avg_ask\": {\n                    \"avg\": {\"field\": \"ASK\"}\n                }\n            }\n        }\n    },\n    \"sort\": [\n        {\n            \"PERMNO\": {\"order\": \"asc\"}\n        }\n    ],\n    \"_source\": [\"\"]\n}).encode(\"utf-8\")\n\nr = http.request(\"GET\", \"http://localhost:9200/uscsv/_search\", body=data,\n                 headers={'Content-Type': 'application/json'})\nprint(r.status)\nprint(json.loads(r.data.decode()))\n```\n\n----------------------------------------\n\nTITLE: C++ API: Creating Table with Array Vector Column\nDESCRIPTION: Creates a table object in C++ with a column defined as an Array Vector (DT_INT_ARRAY). This snippet demonstrates how to specify the data type and column names for a DolphinDB table using the C++ API.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Array_Vector.md#_snippet_18\n\nLANGUAGE: C++\nCODE:\n```\nint rowNum = 3;\nint colNum = 2;\nvector<string> colNames = {\"id\",\"value\"};\nvector<DATA_TYPE> colTypes = {DT_INT, DT_INT_ARRAY};\nConstantSP table = Util::createTable(colNames, colTypes, rowNum, rowNum+1);\nvector<VectorSP> columnVecs;\ncolumnVecs.reserve(colNum);\nfor(int i = 0; i < colNum; ++i) {\n    columnVecs.emplace_back(table->getColumn(i));\n}\n```\n\n----------------------------------------\n\nTITLE: Checking DolphinDB Backup Integrity (Database Level)\nDESCRIPTION: Example of using `checkBackup` to verify the integrity of all partitions within a full database backup located in `backupDir` for the database `dbPath`. It compares checksums and returns a table listing any partitions that fail the check. An empty result indicates the backup is likely intact.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/backup-restore-new.md#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndbPath=\"dfs://testdb\"\nbackupDir=\"/home/$USER/backupDB\"\ncheckBackup(backupDir,dbPath)\n```\n\n----------------------------------------\n\nTITLE: 创建行情表和用户订单表的列名映射字典\nDESCRIPTION: 定义模拟撮合引擎所需的行情表和用户订单表结构，并创建相应的列名映射字典，用于在撮合过程中正确识别和处理各个字段。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/matching_engine_simulator.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndummyQuotationTable = table(1:0, `symbol`symbolSource`time`sourceType`orderType`price`qty`buyNo`sellNo`BSFlag`seqNum, \n                                [STRING, STRING, TIMESTAMP,INT, INT, DOUBLE, LONG, LONG, LONG, INT, LONG])\nquotationColMap = dict( `symbol`symbolSource`time`sourceType`orderType`price`qty`buyNo`sellNo`direction`seqNum, \n                        `symbol`symbolSource`time`sourceType`orderType`price`qty`buyNo`sellNo`BSFlag`seqNum)\n\ndummyUserOrderTable = table(1:0, `symbol`time`orderType`price`qty`BSFlag`orderID,  \n                                  [STRING, TIMESTAMP, INT, DOUBLE, LONG, INT, LONG])\nuserOrderColMap = dict( `symbol`time`orderType`price`qty`direction`orderID, \n                        `symbol`time`orderType`price`qty`BSFlag`orderID)\n```\n\n----------------------------------------\n\nTITLE: Load Json Configuration\nDESCRIPTION: This code snippet demonstrates how to load a JSON configuration file for deploying a calculation service in DolphinDB. The `loadJsonConfig` function is called with the path to the JSON configuration file (`testConfig.dos`) and the degree of parallelism. After execution, the corresponding input and output table variables of the stream computing will appear in the variable bar in the lower right corner of the DolphinDB GUI.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Level2_Snapshot_Factor_Calculation.md#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\nuse DolphinDBModules::SnapshotFactorCalculationPlatform::JsonConfig::JsonConfigLoad\n\n/**\n计算服务部署传参\ntestConfig.dos 是示例 Json 配置文件\nparallel 指定计算的并行度\n */\njsonPath = \"./modules/DolphinDBModules/SnapshotFactorCalculationPlatform/testConfig.dos\"\nparallel = 2\n// 执行计算服务部署函数\nloadJsonConfig(jsonPath, parallel)\n```\n\n----------------------------------------\n\nTITLE: Testing MySQL ODBC Connection with isql (File Not Found)\nDESCRIPTION: This command attempts to connect to a MySQL database via ODBC, but fails because the specified driver library file (`/usr/lib64/libmyodbc8w.so`) cannot be found. This indicates a missing or incorrectly installed ODBC driver.  Ensure the driver is installed and accessible to the system.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ODBC_plugin_user_guide.md#_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\nisql -v -k \"Driver=MySQL ODBC 8.0 Unicode Driver;Server=192.168.1.38;Port=3306;Uid=root;Pwd=123456;Database=Test\"\n[01000][unixODBC][Driver Manager]Can't open lib '/usr/lib64/libmyodbc8w.so' : file not found\n[ISQL]ERROR: Could not SQLDriverConnect\n```\n\n----------------------------------------\n\nTITLE: Converting Tick Data to Minute-level Data using Map-Reduce in DolphinDB\nDESCRIPTION: Shows how to use the map-reduce (mr) function to efficiently convert high-frequency tick data to minute-level aggregated data. This approach provides better performance for large datasets compared to standard SQL queries.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_63\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(`admin, `123456)\ndb = database(\"dfs://TAQ\")\nquotes = db.loadTable(\"quotes\")\n\n//create a new table quotes_minute\nmodel=select  top 1 symbol,date, minute(time) as minute,bid,ofr from quotes where date=2007.08.01,symbol=`EBAY\nif(existsTable(\"dfs://TAQ\", \"quotes_minute\"))\ndb.dropTable(\"quotes_minute\")\ndb.createPartitionedTable(model, \"quotes_minute\", `date`symbol)\n\n//populate data for table quotes_minute\ndef saveMinuteQuote(t){\nminuteQuotes=select avg(bid) as bid, avg(ofr) as ofr from t group by symbol,date,minute(time) as minute\nloadTable(\"dfs://TAQ\", \"quotes_minute\").append!(minuteQuotes)\nreturn minuteQuotes.size()\n}\n\nds = sqlDS(<select symbol,date,time,bid,ofr from quotes where date between 2007.08.01 : 2007.08.31>)\ntimer mr(ds, saveMinuteQuote, +)\n```\n\n----------------------------------------\n\nTITLE: Filtering with IN Keyword Optimization in DolphinDB SQL\nDESCRIPTION: Demonstrates filtering stock snapshot data by industry using the IN keyword to optimize performance vs join-based filtering. The original approach performs a left join between snapshot data (t1) and industry info table (t2) filtering by industry afterward. The optimized approach extracts SecurityIDs from the industry table using WHERE Industry=\"Edu\" and filters the snapshot table with SecurityID in SecurityIDs. This reduces query execution time from 336 ms to 72 ms by avoiding costly join operations. Key functions used include loadTable, exec, select, and timer for performance measurement. The input tables must exist in the database, and resulting datasets are shown to be equivalent via element-wise comparison.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_SQL_Case_Tutorial.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\nt1 = loadTable(\"dfs://Level1\", \"Snapshot\")\nSecurityIDs = exec distinct SecurityID from t1 where date(DateTime) = 2020.06.01\nt2 = table(SecurityIDs as SecurityID, \n           take(`Mul`IoT`Eco`Csm`Edu`Food, SecurityIDs.size()) as Industry)\n\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntimer res1 = select SecurityID, DateTime \n\t\t\t from lj(t1, t2, `SecurityID) \n\t\t\t where date(DateTime) = 2020.06.01, Industry=`Edu\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nSecurityIDs = exec SecurityID from t2 where Industry=\"Edu\"\ntimer res2 = select SecurityID, DateTime \n\t\t\t from t1 \n\t\t\t where date(DateTime) = 2020.06.01, SecurityID in SecurityIDs\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\neach(eqObj, res1.values(), res2.values()) // true\n```\n\n----------------------------------------\n\nTITLE: Querying TAQ Data - DolphinDB\nDESCRIPTION: These DolphinDB scripts demonstrate various queries against TAQ data, including selection, filtering, aggregation (count, sum, average, max, min), and grouping.  The `timer` function measures execution time. Dependencies: DolphinDB installation, TAQ table populated in DolphinDB.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_Druid_appendix.md#_snippet_9\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//根据股票代码和日期查找并取前1000条\ntimer select top 1000 * from TAQ where symbol = 'IBM' , date = 2007.08.10;\n\n//根据部分股票代码和时间、报价范围计数\ntimer select count(*) from TAQ where date = 2007.08.10, symbol in ('GOOG', 'THOO', 'IBM'), bid>0, ofr>bid;\n\n//按股票代码分组并按照卖出与买入价格差排序\ntimer select sum(ofr-bid) as spread from TAQ where date = 2007.08.27 group by symbol order by spread;\n\n//按时间和报价范围过滤，按小时分组并计算均值\ntimer select avg((ofr-bid)/(ofr+bid)) as spread from TAQ where date = 2007.08.01, bid > 0, ofr > bid group by hour(time);\n\n//按股票代码分组并计算最大卖出与最小买入价之差\ntimer select max(ofr)-min(bid) as gap from TAQ where date = 2007.08.03, bid > 0, ofr > bid group by symbol;\n\n//对最大买入价按股票代码、小时分组并排序\ntimer select max(bid) as mx from TAQ where date = 2007.08.27 group by symbol, date having sum(bidsiz)>0 order by symbol, date;\n\n//对买入与卖出价均值的最大值按股票代码、小时分组\ntimer select max(ofr+bid)/2.0 as mx from TAQ where date = 2007.08.01, group by symbol, date;\n```\n\n----------------------------------------\n\nTITLE: Using the Time-Series Aggregation Engine in DolphinDB Script\nDESCRIPTION: Sets up a time-series aggregation engine (`createTimeSeriesEngine`) named 'tsAggr_iot' to calculate real-time metrics (average of v1, average of v2, max of v3) over a 10-second sliding window (`windowSize`) with a 1-second step (`step`). It subscribes (`subscribeTable`) the engine to an input stream table (`replayStream`) and outputs results to a shared stream table (`calc_result`). Includes querying and plotting sample results from the output table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/high_freq_data_storage_and_analysis.md#_snippet_11\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\n/**** 时间序列聚合引擎 ****/\n\n//定义保存计算结果的流表\nshare streamTable(1000:0, `ts`deviceid`v1`v2`v3,[TIMESTAMP,SYMBOL,FLOAT,FLOAT,FLOAT]) as calc_result\n\n//计算引擎，实时计算均值指标（窗口 10s，步长 1s）\ntsAggr_iot = createTimeSeriesEngine(name=\"tsAggr_iot\",  windowSize=10*1000, step=1*1000, metrics=<[avg(v1),avg(v2),max(v3)]>, dummyTable=replayStream, outputTable=calc_result, timeColumn=`ts, keyColumn=`deviceid)\nsubscribeTable(tableName=\"replayStream\", actionName=\"act_tsAggr_iot\", offset=0, handler=append!{tsAggr_iot}, msgAsTable=true, batchSize=50, throttle=1, hash=0)\n\nt=select top 100 * from calc_result order by ts desc\ntitle=select min(ts),max(ts) from t\nplot(t.v1,t.ts,title.min_ts[0]+\"  ~  \"+title.max_ts[0])ame=\"collect\", diskUsage=true,top=0})\n```\n\n----------------------------------------\n\nTITLE: 全表记录数统计性能测试\nDESCRIPTION: 统计整个表的记录总数，用于验证大规模数据统计查询在合并前后的性能变化。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/tsdb_explained.md#_snippet_6\n\nLANGUAGE: SQL\nCODE:\n```\nselect count(*) from loadTable(dbName, tbName)\n```\n\n----------------------------------------\n\nTITLE: Grouping and calculating sum and last time\nDESCRIPTION: This code snippet selects the sum of volume and the last time for each group, grouping based on the calculated start time. It uses the iif(accumulate(...), time, NULL).ffill() expression as the grouping key, effectively grouping data points until the accumulated volume approaches the target volume.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_51\n\nLANGUAGE: shell\nCODE:\n```\noutput = select sum(volume) as sum_volume, last(time) as endTime from t group by iif(accumulate(caclCumVol{1500000}, volume) ==volume, time, NULL).ffill() as startTime\n```\n\n----------------------------------------\n\nTITLE: Executing Multiprocessing Pool to Compute VWAP Over Stock Trade Files in Python\nDESCRIPTION: This code initializes multiprocessing with a user-defined number of processes, lists stock trade files from a directory, splits them for parallel processing, and uses a multiprocessing pool to concurrently compute VWAP for stock ticks using 'pool_func'. It prints progress, computes elapsed time before and after processing, concatenates all results, and ensures resource cleanup. The code assumes correct file paths and sufficient system resources for multiprocessing.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/DolphinDB_Python_Parser_Intro_for_Quantitative_Finance/因子实现_Python版本/委托量加权平均委托价格.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nn_use = 24\n# 路径修改为存放数据路径\ntrade_path = r\"/ssd/ssd3/data/oneStock_oneFile_TL/20230201/entrust\"\nstock_pool = os.listdir(trade_path)\nprocesses_decided = multi_task_split(stock_pool, n_use).num_of_jobs()\nprint(\"进程数：\", processes_decided)\nsplit_args_to_process = list(multi_task_split(stock_pool, n_use).split_args())\nargs = [(split_args_to_process[i], trade_path) for i in range(len(split_args_to_process))]\nprint(\"#\" * 50 + \"Multiprocessing Start\" + \"#\" * 50)\nt0 = time.time()\nwith multiprocessing.Pool(processes=processes_decided) as pool:\n    res = tqdm(pool.starmap(pool_func, args))\n    print(\"cal time: \", time.time() - t0, \"s\")\n    res_combined = pd.concat(res, axis=0)\n    pool.close()\n    print(\"cal time: \", time.time() - t0, \"s\")\nprint(res_combined)\n```\n\n----------------------------------------\n\nTITLE: Configuring single-table replay using JSON\nDESCRIPTION: This JSON configuration is used to specify the parameters for replaying data from a single table.  It includes details like the database and table names, device column, time column, device ID, replay rate, sampling rate, and job name.  The replayIoT function uses this configuration to execute the data replay task.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/faultAnalysis.md#_snippet_2\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    \"dbName\":目标库，必选,  \n    \"tbName\":目标表，必选,\n    \"deviceColumn\":设备名称列，必选, \n    \"dateColumn\":数据时间列，可选,\n    \"deviceId\":需要回放的设备，可选,\n    \"selectColumn\":需要回放的设备，可选\n    \"timeBegin\":回放开始时间，可选,\n    \"timeEnd\":回放结束时间，可选,\n    \"replayRate\":回放倍率，可选,\n    \"sampleRate\":采样频率，可选,\n    \"jobName\":回放任务名称，必选\n}\n```\n\n----------------------------------------\n\nTITLE: Elasticsearch Connection and Search, Scroll\nDESCRIPTION: This Python code searches Elasticsearch using the `elasticsearch` library. It sets up a connection to an Elasticsearch instance and defines a search query with filtering based on a date range. The query uses a scroll operation to retrieve large result sets in batches. It prints the scroll ID and scroll size, then iterates through the results, retrieving the next batch with each scroll operation. The script iterates and continues until the scroll size is zero. Requires an Elasticsearch instance to be running and the \"uscsv\" index to be populated.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_47\n\nLANGUAGE: Python\nCODE:\n```\ndef search_1():\n    es = Elasticsearch(['http://localhost:9200/'])\n    page = es.search(\n        index='uscsv',\n        doc_type='type',\n        scroll='2m',\n        size=10000,\n        body={\n            \"query\": {\n                \"constant_score\": {\n                    \"filter\": {\n                        \"range\": {\n                            \"date\": {\n                                \"gte\": \"2008/08/08\",\n                                \"lte\": \"2010/08/08\"\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    )\n    sid = page['_scroll_id']\n    scroll_size = page['hits']['total']\n\n    print(sid)\n    print(scroll_size)\n    while (scroll_size > 0):\n        print(\"Scrolling...\")\n        page = es.scroll(scroll_id=sid, scroll='2m')\n        sid = page['_scroll_id']\n        scroll_size = len(page['hits']['hits'])\n        print(\"scroll size: \" + str(scroll_size))\n```\n\n----------------------------------------\n\nTITLE: 1.1 将整数时间转化为 TIME 类型并导入 CSV 数据\nDESCRIPTION: 演示如何使用 extractTextSchema 和自定义时间转换函数，将 CSV 文件中的整数时间字段转换为 TIME 类型，并导入到分布式表中以便查询分析，需要依赖 loadTextEx 等函数。适用于处理时间字段的格式转换与导入。实现包括 schema 提取、更新数据类型、数据转换和导入的完整步骤。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\nschemaTB=extractTextSchema(dataFilePath)\nupdate schemaTB set type=\"TIME\" where name=\"time\"\ntb=table(1:0,schemaTB.name,schemaTB.type)\ntb=db.createPartitionedTable(tb,`tb1,`date);\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef i2t(mutable t){\n    return t.replaceColumn!(`time,t.time.format(\"000000000\").temporalParse(\"HHmmssSSS\"))\n}\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntmpTB=loadTextEx(dbHandle=db,tableName=`tb1,partitionColumns=`date,filename=dataFilePath,transform=i2t);\n```\n\n----------------------------------------\n\nTITLE: Applying PIP Downsampling with `rolling` Function in DolphinDB Script\nDESCRIPTION: Demonstrates how to apply the custom `PIP` aggregate function to the simulated data `X` and `Y` using DolphinDB's built-in `rolling` function. It calculates the PIP downsampling over a sliding window of size 1000, stepping by 999 points. The result `pipResult` is a vector of strings, each containing the downsampled points for one window. It then calls the `strToTable` function to parse the results into a table `samplesPIP`. The `timer` command measures execution time.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/PIP_in_DolphinDB.md#_snippet_4\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\n// 调用 rolling 函数进行滑动窗口计算，每 1000 个点滑动 1 次，降采样到 10 个点\ntimer{pipResult = rolling(PIP, [X, Y], 1000, 999)}\n\n// 解析降采样结果输出一张表\nsamplesPIP = strToTable(pipResult)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Database and Table for Pre-processing Import in DolphinDB\nDESCRIPTION: This script prepares for importing data with pre-processing. It logs in, defines a database path, creates a VALUE-partitioned database (`dfs://DolphinDBdatabase`), extracts the schema from the source CSV, updates the schema to change the 'time' column's type to TIME, creates an empty table definition with the modified schema, and finally creates the partitioned table `tb1` in the database, partitioned by the 'date' column.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_csv.md#_snippet_12\n\nLANGUAGE: dolphindb\nCODE:\n```\nlogin(`admin,`123456)\ndataFilePath=\"/home/data/candle_201801.csv\"\ndbPath=\"dfs://DolphinDBdatabase\"\ndb=database(dbPath,VALUE,2018.01.02..2018.01.30)\nschemaTB=extractTextSchema(dataFilePath)\nupdate schemaTB set type=\"TIME\" where name=\"time\"\ntb=table(1:0,schemaTB.name,schemaTB.type)\ntb=db.createPartitionedTable(tb,`tb1,`date);\n```\n\n----------------------------------------\n\nTITLE: Installing dolphinindb wheel package on Windows offline\nDESCRIPTION: Performs pip installation of pre-downloaded wheel files in an offline environment, ensuring dependencies are satisfied through transferred wheel packages. Uses command prompt or PowerShell.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/python_api_install_offline.md#_snippet_8\n\nLANGUAGE: Shell Script\nCODE:\n```\npip install dolphindb-1.30.19.2-cp38-cp38-win_amd64.whl\n```\n\n----------------------------------------\n\nTITLE: Defining Function to Create OHLC DFS Table in DolphinDB Script\nDESCRIPTION: Defines the `createStockFundOHLCDfsTB` function in DolphinDB script. This function checks if the specified DFS database (e.g., \"dfs://stockFundOHLC\") and table (e.g., \"stockFundOHLC\") exist. If not, it creates the database partitioned by VALUE based on date ranges and then creates the partitioned table with a predefined schema suitable for storing 1-minute OHLC data, using `TradeTime` as the partitioning column.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/K.md#_snippet_5\n\nLANGUAGE: dolphindb\nCODE:\n```\ndef createStockFundOHLCDfsTB(dbName=\"dfs://stockFundOHLC\", tbName=\"stockFundOHLC\"){\n\tif(existsDatabase(dbUrl=dbName)){\n\t\tprint(dbName + \" has been created !\")\n\t\tprint(tbName + \" has been created !\")\n\t}\n\telse{\n\t\tdb = database(dbName, VALUE, 2021.01.01..2021.12.31)\n\t\tprint(dbName + \" created successfully.\")\n\t\tcolNames = `SecurityID`TradeTime`OpenPrice`HighPrice`LowPrice`ClosePrice`Volume`Turnover`TradesCount`PreClosePrice`PreCloseIOPV`IOPV`UpLimitPx`DownLimitPx`ChangeRate\n\t\tcolTypes = [SYMBOL, TIMESTAMP, DOUBLE, DOUBLE, DOUBLE, DOUBLE, LONG, DOUBLE, INT, DOUBLE, DOUBLE, DOUBLE, DOUBLE, DOUBLE, DOUBLE]\n\t\tschemaTable = table(1:0, colNames, colTypes)\n\t\tdb.createPartitionedTable(table=schemaTable, tableName=tbName, partitionColumns=`TradeTime)\n\t\tprint(tbName + \" created successfully.\")\n\t}\n\treturn loadTable(dbName, tbName).schema().colDefs\n}\n```\n\n----------------------------------------\n\nTITLE: Setting URL Parameter for WeChat Work API\nDESCRIPTION: Code snippet for setting the URL parameter to access the WeChat Work token API endpoint. This is the first step in establishing a connection with the WeChat Work API.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/send_messages_external_systems.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\nurl = 'https://qyapi.weixin.qq.com/cgi-bin/gettoken';\n```\n\n----------------------------------------\n\nTITLE: Listing Directory After Cleanup (keepDuplicates=ALL)\nDESCRIPTION: This snippet shows the file structure after updates and subsequent cleaning operations in a TSDB table configured with `keepDuplicates=ALL`.  The `tree` command shows that only the latest version is retained after periodic cleaning has occurred.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/dolphindb_update.md#_snippet_12\n\nLANGUAGE: console\nCODE:\n```\n$ tree \n.\n├── chunk.dict\n└── machines_2_219\n    ├── 0_00000615\n    ├── 0_00000616\n    ├── 0_00000617\n    ├── 0_00000618\n    └── 0_00000619\n\n1 directory, 6 files\n```\n\n----------------------------------------\n\nTITLE: Importing Part Columns using partCol\nDESCRIPTION: This code snippet shows how to select and import only a subset of columns from the source data during the loading process using the `partCol` function. The `select` statement within the function specifies the desired column names, allowing selective data import.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/LoadDataForPoc.md#_snippet_9\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef partCol(mutable memTable)\n{\n\treturn select [需要的部分列名] from memTable\n}\n```\n\n----------------------------------------\n\nTITLE: Creating and Sharing Stream Table DolphinDB Script\nDESCRIPTION: Creates a stream table named `sensorTemp` with a capacity of 1,000,000 rows and a predefined schema. It then enables sharing and persistence for this table, making it accessible across sessions and ensuring its data can be written to disk asynchronously.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/iot_demo_script.txt#_snippet_1\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nst=streamTable(1000000:0,`hardwareId`ts`temp1`temp2`temp3,[INT,TIMESTAMP,DOUBLE,DOUBLE,DOUBLE])\nenableTableShareAndPersistence(table=st, tableName=`sensorTemp, asynWrite=true, compress=false, cacheSize=1000000)\ngo\n```\n\n----------------------------------------\n\nTITLE: Aggregating Average Price and Sum of Size by Symbol using urllib3 in Python\nDESCRIPTION: This function uses `urllib3` to query Elasticsearch. It filters by DATE and aggregates results by SYMBOL, calculating both the average OFR price and the sum of BIDSIZ for each symbol. The request is sent via HTTP GET with a JSON body.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_63\n\nLANGUAGE: python\nCODE:\n```\nimport urllib3\nimport json\n\ndef search_3():\n    http = urllib3.PoolManager()\n    data = json.dumps({\n        \"query\": {\n            \"constant_score\": {\n                \"filter\": {\n                    \"term\": {\n                        \"DATE\": \"20070809\"\n                    }\n                }\n            }\n        },\n        \"aggs\": {\n            \"group_by_symbol\": {\n                \"terms\": {\n                    \"field\": \"SYMBOL\",\n                    \"size\": 8374\n                },\n                \"aggs\": {\n                    \"avg_price\": {\n                        \"avg\": {\"field\": \"OFR\"}\n                    },\n                    \"sum_bidsiz\": {\n                        \"sum\": {\"field\": \"BIDSIZ\"}\n                    }\n                }\n            }\n        },\n        \"_source\": [\"\"]\n    }).encode(\"utf-8\")\n    r = http.request(\"GET\", \"http://localhost:9200/hundred/_search\", body=data,\n                     headers={'Content-Type': 'application/json'})\n    print(r.status)\n    print(json.loads(r.data.decode()))\n```\n\n----------------------------------------\n\nTITLE: Editing User File Limits via limits.conf for DolphinDB (console)\nDESCRIPTION: Set the maximum number of open files per user to 102400 by editing `/etc/security/limits.conf`. This configuration is essential for preventing 'Too many open files' errors when running DolphinDB clusters under heavy workloads. Requires root or sudo privileges.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/deploy_dolphindb_on_new_server.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\n# vi /etc/security/limits.conf\n* soft nofile 102400\n* hard nofile 102400\n\n```\n\n----------------------------------------\n\nTITLE: Performance Testing: Total Record Count\nDESCRIPTION: Measures the total number of records in 'taq' where date is before or on August 7, 2007.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs InfluxDB/test_dolphindb_big.txt#_snippet_8\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// ---------------- 查询性能测试\n\n// 查询总记录数\ntimer\nselect count(*) from taq\nwhere date <= 2007.08.07\n```\n\n----------------------------------------\n\nTITLE: Updating Single Integer Element with setInt C++\nDESCRIPTION: Provides an example of updating individual integer elements in a DolphinDB Vector using the `setInt(index, val)` method. This method is less efficient for large-scale updates due to repeated virtual function calls.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_advance.md#_snippet_14\n\nLANGUAGE: C++\nCODE:\n```\nconst int size = 100000000;\nVectorSP pVec = Util::createVector(DT_INT, size);\nfor(int i = 0; i < size; ++i) {\n    pVec->setInt(i, i);\n}\n```\n\n----------------------------------------\n\nTITLE: Replaying Data into Stream Table in DolphinDB\nDESCRIPTION: Loads data from a CSV file and replays it into the `inputSt` stream table. This is an alternative to using MQTT for testing and debugging purposes, allowing direct injection of data into the stream processing pipeline.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_streaming_application_in_IOT.md#_snippet_10\n\nLANGUAGE: DolphinDB\nCODE:\n```\nt=loadText(getHomeDir()+\"/deviceState.csv\")\nreplay(t, inputSt, `ts, `ts)\n```\n\n----------------------------------------\n\nTITLE: Creating Partitioned Table for Daily K-Line Data in DolphinDB\nDESCRIPTION: Creates a distributed database table for storing daily K-line (candlestick) data. The table uses yearly range partitioning spanning 50 years and includes essential price and volume columns. It's optimized for time-series queries by security ID and date, suitable for long-term market analysis.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Best_Practices_for_DolphinDB_and_Python_AirFlow.md#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\nmodule Factor::createFactorDaily\n\n//创建日 K 线储存表\ndef createFactorDaily(dbName, tbName){\n\tif(existsDatabase(dbName)){\n\t\tdropDatabase(dbName)\n\t}\n\t//按年分区\n\tdb = database(dbName, RANGE, datetimeAdd(2000.01M,(0..50)*12, \"M\"),engine = `TSDB)\n\tcolName = `TradeDate`SecurityID`Open`High`Low`Close`Volume`Amount`Vwap\n\tcolType =[DATE, SYMBOL, DOUBLE, DOUBLE, DOUBLE, DOUBLE, LONG, DOUBLE, DOUBLE]\n\ttbSchema = table(1:0, colName, colType)\n  \tdb.createPartitionedTable(table=tbSchema,tableName=tbName,partitionColumns=`TradeDate,sortColumns=`SecurityID`TradeDate,keepDuplicates=ALL)\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Metrics for Time-Series Engine (DolphinDB Script)\nDESCRIPTION: Defines the metrics definition for the time-series stream engine. It specifies that the `featureEngineering` function should be applied to the relevant input columns, and the output columns should be named according to the generated `metaCodeColName` with suffixes for different time windows.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/metacode_derived_features/metacode_derived_features_streaming.txt#_snippet_11\n\nLANGUAGE: DolphinDB\nCODE:\n```\nmetrics=sqlColAlias(<featureEngineering(DateTime,\n\t\tmatrix(BidPrice0,BidPrice1,BidPrice2,BidPrice3,BidPrice4,BidPrice5,BidPrice6,BidPrice7,BidPrice8,BidPrice9),\n\t\tmatrix(BidOrderQty0,BidOrderQty1,BidOrderQty2,BidOrderQty3,BidOrderQty4,BidOrderQty5,BidOrderQty6,BidOrderQty7,BidOrderQty8,BidOrderQty9),\n\t\tmatrix(OfferPrice0,OfferPrice1,OfferPrice2,OfferPrice3,OfferPrice4,OfferPrice5,OfferPrice6,OfferPrice7,OfferPrice8,OfferPrice9),\n\t\tmatrix(OfferOrderQty0,OfferOrderQty1,OfferOrderQty2,OfferOrderQty3,OfferOrderQty4,OfferOrderQty5,OfferOrderQty6,OfferOrderQty7,OfferOrderQty8,OfferOrderQty9), aggMetaCode)>, metaCodeColName <- (metaCodeColName+\"_150\") <- (metaCodeColName+\"_300\") <- (metaCodeColName+\"_450\"))\n```\n\n----------------------------------------\n\nTITLE: Create Shared Memory Table in DolphinDB with Java API\nDESCRIPTION: This Java snippet creates a shared memory table in DolphinDB using the Java API.  The table schema includes columns for symbol, datetime, bid, ofr, bidsize, ofrsize, mode, ex, and mmid. The `share` command makes the table accessible to other DolphinDB processes.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/timezone.md#_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nconn.run(\"t = table(100:0,`symbol`datetime`bid`ofr`bidsize`ofrsize`mode`ex`mmid,[SYMBOL,DATETIME,DOUBLE,DOUBLE,LONG,LONG,INT,CHAR,SYMBOL])\\n\" +\n\"share t as timeTest;\");\n```\n\n----------------------------------------\n\nTITLE: 实现历史数据回放模拟实时流式注入 - DolphinDB 脚本\nDESCRIPTION: 本示例通过新建内存流数据表作为回放目标，并利用 replay 函数以设定速率将数据库中选定时间段内的历史数据批量注入至目标流表。支持多列回放及后台异步提交回放任务，便于异常诊断和试验故障排查。关键参数包含回放倍速、时间区间及回放表结构定义。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/high_freq_data_storage_and_analysis.md#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\n/**** 历史回放功能（可用于试验故障排查） ****/\n\n//1. 新建流数据表，容纳回放数据\nshare streamTable(1000:0,`ts`deviceid,[TIMESTAMP,SYMBOL]) as replayStream\nfor(i in 1..3){  //取测点v1,v2,v3用于演示\n    addColumn(replayStream,`v+string(i),DOUBLE)\n}\n\n//2. 数据回放（10 分钟）\nrate=1                                  //回放倍速\nbegintime=2022.01.01 00:00:00.000       //数据开始时间\nendtime  =2022.01.01 00:10:00.000       //数据结束时间\nt=select ts,deviceid,v1,v2,v3 from pt where ts between begintime:endtime order by ts,deviceid\n\n//3. 批处理调用 replay 函数，后台执行回放\nsubmitJob(\"replay_output\", \"replay_output_stream\", replay,  t,replayStream, `ts, `ts, rate,false)\n\n//4. 展示回放数据（top 1000）\nt=select * from replayStream limit 1000\nplot(t.v1,t.ts,\"top 1000 value 1\")\n```\n\n----------------------------------------\n\nTITLE: Synchronizing Oracle Tick Data to DolphinDB TSDB\nDESCRIPTION: Defines a function `syncData` that orchestrates the data synchronization from Oracle to DolphinDB. It takes the ODBC connection object (`conn`), target DolphinDB database name (`dbName`), table name (`tbName`), and an optional date (`dt`) as parameters. It constructs a SQL query for the Oracle `ticksh` table, optionally filtering by `TradeTime` if `dt` is provided. It then uses `odbc::query` to execute the SQL, retrieve data in batches of 100,000 rows, apply the `transForm` function to each batch for type conversion, and load the results into the specified DolphinDB table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/Oracle_to_DolphinDB/迁移.txt#_snippet_1\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\ndef syncData(conn, dbName, tbName, dt){\n\tsql = \"select SecurityID, TradeTime, TradePrice, TradeQty, TradeAmount, BuyNo, SellNo, ChannelNo, TradeIndex, TradeBSFlag, BizIndex from ticksh\"\n\tif(!isNull(dt)) {\n\t\tsql = sql + \" WHERE trunc(TradeTime) = TO_DATE('\"+dt+\"', 'yyyy.MM.dd')\"\n\t}\n    odbc::query(conn,sql, loadTable(dbName,tbName), 100000, transForm)\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Partition Policy for New Data in DolphinDB\nDESCRIPTION: This snippet specifies how DolphinDB should handle new data that falls outside the defined partition scheme for VALUE domains or VALUE domains within COMPO domains. The newValuePartitionPolicy parameter can be set to \"skip\", \"fail\", or \"add\".  \"skip\" ignores new data, \"fail\" throws an exception, and \"add\" creates new partitions.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/ha_cluster_deployment/P1/config/config-specification.txt#_snippet_2\n\nLANGUAGE: properties\nCODE:\n```\nnewValuePartitionPolicy=add\n```\n\n----------------------------------------\n\nTITLE: Saving a Trained Model to Disk\nDESCRIPTION: This code saves the trained machine learning model to disk using the `saveModel` function.  The model is saved as a binary file named 'wineModel.bin' in the 'D:/model/' directory. The path must be accessible and writable by the DolphinDB process.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/machine_learning.md#_snippet_5\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nmodel.saveModel(\"D:/model/wineModel.bin\")\n```\n\n----------------------------------------\n\nTITLE: Querying 5-Minute Price Change Rank for Grafana in DolphinDB\nDESCRIPTION: DolphinDB SQL query designed for use in Grafana. Selects the SecurityID, DateTime, and the 5-minute price change factor (`factor_5min`) from the `changeCrossSectionalTable`. Results are ordered by the pre-calculated 5-minute rank (`rank_5min`) for visualization.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Real-Time_Stock_Price_Increase_Calculation.md#_snippet_8\n\nLANGUAGE: DolphinDB (SQL)\nCODE:\n```\nselect SecurityID, DateTime, factor_5min from changeCrossSectionalTable order by rank_5min\n```\n\n----------------------------------------\n\nTITLE: Aggregating with single-column macro variable and alias\nDESCRIPTION: This snippet demonstrates the use of a single-column macro variable within an aggregate SQL query, alongside a macro variable alias. The SQL query computes the sum of a column (`name`) from a table (`t`), grouped by another column (`grp`). The sum will be aliased with name defined by the macro variable `alias`.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/macro_var_based_metaprogramming.md#_snippet_12\n\nLANGUAGE: DolphinDB\nCODE:\n```\nname=\"val\"\ngrp=\"sym\"\nalias=\"sum_val\"\n<select sum(_$name) as _$alias from t group by _$grp>.eval()\n```\n\n----------------------------------------\n\nTITLE: DolphinDB Table-Level Revoke over Global Deny\nDESCRIPTION: This example demonstrates that a table-level `revoke` does not lift a global `deny`. Despite revoking read access to `dfs://test/pt`, user1 remains denied read access to all tables due to the initial global `deny`.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Permission_Management.md#_snippet_12\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(\"admin\", \"123456\")\ncreateUser(\"user1\",\"123456\")\ndbName = \"dfs://test\"\nif(existsDatabase(dbName)){\n        dropDatabase(dbName)\n}\nt = table(1..10 as id , rand(100, 10) as val)\ndb=database(dbName, VALUE, 1..10)\npt=  db.createPartitionedTable(t, \"pt\", \"id\")\npt.append!(t)\n\ndeny(\"user1\", TABLE_READ, \"*\")\nrevoke(\"user1\", TABLE_READ, dbName+\"/pt\")\n\ngetUserAccess(\"user1\")//TABLE_READ 依旧是 deny\n```\n\n----------------------------------------\n\nTITLE: Querying 1-Minute Price Change Rank for Grafana in DolphinDB\nDESCRIPTION: DolphinDB SQL query designed for use in Grafana. Selects the SecurityID, DateTime, and the 1-minute price change factor (`factor_1min`) from the `changeCrossSectionalTable`. Results are ordered by the pre-calculated 1-minute rank (`rank_1min`) to display the top gainers/losers.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Real-Time_Stock_Price_Increase_Calculation.md#_snippet_7\n\nLANGUAGE: DolphinDB (SQL)\nCODE:\n```\nselect SecurityID, DateTime, factor_1min from changeCrossSectionalTable order by rank_1min\n```\n\n----------------------------------------\n\nTITLE: Creating Distributed Tables and Dimension Tables\nDESCRIPTION: This snippet creates two distributed tables (`Factor10MinSH`, `Factor10MinSZ`) and a dimension table (`MdSecurity`).  It populates these tables with sample data.  The distributed tables store stock factor data, and the dimension table stores stock information.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_sql_cases.md#_snippet_35\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndates = 2020.01.01..2021.10.31\ntime = join(09:30:00 + 1..12 * 60 * 10, 13:00:00 + 1..12 * 60 * 10)\n\nsyms = format(1..2000, \"000000\") + \".SH\"\ntmp = cj(cj(table(dates), table(time)), table(syms))\nt = table(tmp.syms as SecurityID, tmp.dates as Date, tmp.time as Time, take([\"Factor01\"], tmp.size()) as FactorID, rand(100.0, tmp.size()) as FactorValue)\n\ncreateDBAndTable(\"dfs://Factor10MinSH\", \"Factor10MinSH\").append!(t)\n\nsyms = format(2001..4000, \"000000\") + \".SZ\"\ntmp = cj(cj(table(dates), table(time)), table(syms))\nt = table(tmp.syms as SecurityID, tmp.dates as Date, tmp.time as Time, take([\"Factor01\"], tmp.size()) as FactorID, rand(100.0, tmp.size()) as FactorValue)\ncreateDBAndTable(\"dfs://Factor10MinSZ\", \"Factor10MinSZ\").append!(t)\n\ndb = database(\"dfs://infodb\", VALUE, 1 2 3)\nmodel = table(1:0, `SecurityID`Info, [SYMBOL, STRING])\nif(!existsTable(\"dfs://infodb\", \"MdSecurity\")) createTable(db, model, \"MdSecurity\")\nloadTable(\"dfs://infodb\", \"MdSecurity\").append!(\n\t table(join(format(1..2000, \"000000\") + \".SH\", format(2001..4000, \"000000\") + \".SZ\") as SecurityID, \n\t \t \t take(string(NULL), 4000) as Info))\n\nsetMaxMemSize(32)\n```\n\n----------------------------------------\n\nTITLE: Creating End-of-Data Markers - DolphinDB\nDESCRIPTION: The function `createEnd` creates and appends end-of-data markers to a partitioned table. It first checks if the database `dfs://End` exists; if not, it creates one with daily value partitions for 2023.04.03 and 2023.04.04. It defines a table schema for the end-of-data markers and appends an initial marker to the table. Then, it creates a replay data source from the end-of-data table and replays to the designated `tabName`. The `sortColumn` argument determines whether the replay includes sorting, and the function uses `replay` with appropriate configurations for parallel processing. This function requires that the database and table names be correctly set.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/appendices_market_replay_bp/replay.txt#_snippet_1\n\nLANGUAGE: dolphindb\nCODE:\n```\ndef createEnd(tabName, sortColumn)\n{\n    dbName = \"dfs://End\"\n    tbName = \"endline\"\n    if(not existsDatabase(dbName))\n    {\n        db = database(directory=dbName, partitionType=VALUE, partitionScheme=2023.04.03..2023.04.04)\n        endTb = table(2200.01.01T23:59:59.000 as DateTime, `END as point, long(0) as ApplSeqNum)\n        endLine = db.createPartitionedTable(table=endTb, tableName=tbName, partitionColumns=`DateTime)\n        endLine.append!(endTb)\n    }\n     \n    ds = replayDS(sqlObj=<select * from loadTable(dbName, tbName)>, dateColumn='DateTime', timeColumn='DateTime')\n    \n    inputEnd = dict([\"end\"], [ds])\n    dateEnd = dict([\"end\"], [`DateTime])\n    timeEnd = dict([\"end\"], [`DateTime])\n    if(sortColumn == \"NULL\")\n    {\n        replay(inputTables=inputEnd, outputTables=objByName(tabName), dateColumn=dateEnd, timeColumn=timeEnd, replayRate=-1, absoluteRate=false, parallelLevel=1)\n    }\n    else\n    {\n        replay(inputTables=inputEnd, outputTables=objByName(tabName), dateColumn=dateEnd, timeColumn=timeEnd, replayRate=-1, absoluteRate=false, parallelLevel=1, sortColumns=sortColumn)\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Importing DolphinDB Module with use and Calling Function Directly\nDESCRIPTION: Demonstrates importing the fileLog module using the `use` keyword. After import, functions within the module, such as `appendLog`, can be called directly without specifying the module namespace, provided there are no naming conflicts with other imported modules or local definitions. This import method is session-local.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/module_tutorial.md#_snippet_4\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nuse fileLog\nappendLog(\"mylog.txt\", \"test my log\")\n```\n\n----------------------------------------\n\nTITLE: 处理 Fast Vector - C++\nDESCRIPTION: 此代码片段演示了在 DolphinDB 插件编写时如何处理向量，通过使用 `getDoubleConst` 获取只读数据缓冲区，并从缓冲区中提取数据进行计算。 这种方法比循环使用 `getDouble` 或 `getInt` 更有效率。 每次获得长度为 `Util::BUF_SIZE` 的数据。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_development_tutorial.md#_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\ndouble buf[Util::BUF_SIZE];\n\nINDEX start = 0;\nwhile (start < size) {\n    int len = std::min(Util::BUF_SIZE, size - start);\n    const double *p = X->getDoubleConst(start, len, buf);\n    for (int i = 0; i < len; i++) {\n        double val = p[i];\n        // ...\n    }\n    start += len;\n}\n```\n\n----------------------------------------\n\nTITLE: 使用Pivot By子句重排列股票行情数据\nDESCRIPTION: 通过pivot by子句将股票行情表中的数据按时间和股票代码两个维度重新排列，每行代表一个时间点，每列代表一只股票的报价。这种排列方式便于后续计算投资组合价值。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/quant_finance_examples.md#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndb = database(\"dfs://level2\")\nquotes = loadTable(db, `quotes)\nsyms = `600000`600300`600400`600500`600600`600800`600900\n\npv = select last from quotes where date=2020.06.01, symbol in syms, time between 09:30:00.000 : 15:00:00.000 pivot by time, symbol\nselect top 10 * from pv;\n```\n\n----------------------------------------\n\nTITLE: Appending Incoming Data into Reactive State Engines\nDESCRIPTION: Feeds new data into each reactive engine by appending datasets, enabling real-time processing and updating of metrics in response to streaming data inputs.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Streaming_computing_of_financial_quantifiers.md#_snippet_30\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// Append data streams to each reactive engine for real-time processing\ntimer rse1.append!(data)\ntimer rse2.append!(dataArrayVector)\ntimer rse3.append!(data)\ntimer rse4.append!(dataArrayVector)\n```\n\n----------------------------------------\n\nTITLE: Testing Oracle ODBC Connection with isql - Shell\nDESCRIPTION: Uses the isql utility with verbose output to attempt a connection to the configured Oracle ODBC DSN ('orac'). Successful output confirms connection establishment and shows available isql commands. Replace 'orac' with your configured DSN name.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Oracle_to_DolphinDB.md#_snippet_8\n\nLANGUAGE: Shell\nCODE:\n```\nisql -v orac\n\n/**********output********/\n+---------------------------------------+\n| Connected!                            |\n|                                       |\n| sql-statement                         |\n| help [tablename]                      |\n| quit                                  |\n|                                       |\n+---------------------------------------+\nSQL> \n\n```\n\n----------------------------------------\n\nTITLE: Loading and Creating Regions Table in DolphinDB\nDESCRIPTION: Loads regional data from CSV, creates the 'regions' table with REGION_ID and REGION_NAME, then appends data to the table for regional analysis.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/Standard_SQL_in_DolphinDB/create_db_table_sql.txt#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\nregions_tmp=loadText(dir+\"REGIONS.csv\")\ncreate table \"dfs://hr\".\"regions\" (\n\tREGION_ID INT,\n\tREGION_NAME STRING\n)\nregions = loadTable(\"dfs://hr\", \"regions\")\nregions.append!(regions_tmp)\nselect * from regions\n```\n\n----------------------------------------\n\nTITLE: Using a Plugin Module Namespace in DolphinDB Script (C++)\nDESCRIPTION: Illustrates how to reference a plugin's namespace in DolphinDB script after loading, either with 'use' to bring all exported functions into scope or with explicit module prefix. Input: module name (e.g., demo), desired function call. Limitation: The plugin must be loaded and the namespace must match and be made visible in the context. Useful in demonstration scripts accompanying C++ plugin code.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_development_tutorial.md#_snippet_28\n\nLANGUAGE: C++\nCODE:\n```\nuse demo;\n```\n\nLANGUAGE: C++\nCODE:\n```\ndemo::f1();\n```\n\n----------------------------------------\n\nTITLE: Time-Interval Based Grouped Aggregation Using Interval Function in DolphinDB SQL\nDESCRIPTION: Shows how to perform sliding window aggregations with window length as multiples of step length using the interval function coupled with group by in DolphinDB. It calculates the sum of 'vol' over 10-second windows sliding every 5 seconds, given irregular timestamp data. Recommended for versions 1.30.14 and 2.00.2 or newer. The output groups and sums volume by interval windows of specified length and slide period. Suitable for time-series data where windows are based on fixed time intervals rather than row counts.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/window_cal.md#_snippet_9\n\nLANGUAGE: DolphinDB\nCODE:\n```\nt=table(2021.11.01T10:00:00+0 3 5 6 7 8 15 18 20 29 as time, 1..10 as vol)\nselect sum(vol) from t group by interval(time, 10s, \"null\", 5s)\n\n # output\n\ninterval_time       sum_vol\n------------------- -------\n2021.11.01T10:00:00 21     \n2021.11.01T10:00:05 18     \n2021.11.01T10:00:10 15       \n2021.11.01T10:00:15 24     \n2021.11.01T10:00:20 19     \n2021.11.01T10:00:25 10    \n```\n\n----------------------------------------\n\nTITLE: State Data Replay Configuration (JSON)\nDESCRIPTION: This JSON configures the replay of machine state data. It specifies the database and table name ('state'), the device column ('deviceId'), the date column ('ts'), the device ID ('device0001'), a time range, a replay rate of 50, a sampling rate of 60 seconds, and the job name ('stateStream'). This configuration is used to replay the machine's temperature, hydraulic pressure, and amplitude data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/faultAnalysis.md#_snippet_5\n\nLANGUAGE: JSON\nCODE:\n```\nargs2 = '\n{\n    \"dbName\":\"dfs://test_state\",  \n    \"tbName\":\"state\",\n    \"deviceColumn\":\"deviceId\", \n    \"dateColumn\":\"ts\",\n    \"deviceId\":[\"device0001\"],\n    \"timeBegin\":2024.01.01T00:00:00.000,\n    \"timeEnd\":2024.01.01T02:00:00.000,\n    \"replayRate\":50,\n    \"sampleRate\":60s,\n    \"jobName\":\"stateStream\"\n}\n'\nreplayIoT(args2)\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid - _common\nDESCRIPTION: This configuration file (e.g., `_common`) provides settings common to various Druid components, including Zookeeper connection details, metadata storage settings (MySQL), and deep storage configurations. Dependencies: Druid and MySQL setup, Zookeeper.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_Druid_appendix.md#_snippet_4\n\nLANGUAGE: Druid\nCODE:\n```\n# Zookeeper\ndruid.zk.service.host=zk.host.ip\ndruid.zk.paths.base=/druid\n# Metadata storage\ndruid.metadata.storage.type=mysql\ndruid.metadata.storage.connector.connectURI=jdbc:mysql://db.example.com:3306/druid\n# Deep storage\ndruid.storage.type=local\ndruid.storage.storageDirectory=var/druid/segments\n# Indexing service logs\ndruid.indexer.logs.type=file\ndruid.indexer.logs.directory=var/druid/indexing-logs\n```\n\n----------------------------------------\n\nTITLE: Creating Volume Matrix with Custom Labels using `panel` in DolphinDB Script\nDESCRIPTION: Demonstrates creating a volume matrix using `panel` with explicitly defined row labels (`rowLabel` covering a specific time range) and column labels (`colLabel` specifying symbols 'C' and 'MS'). Only data matching these labels from the original table 't' is included in the resulting matrix.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/panel_data.md#_snippet_11\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nrowLabel = 09:34:59..09:35:02;\ncolLabel = [\"C\", \"MS\"];\nvolume = panel(t.timestamp, t.sym, t.volume, rowLabel, colLabel);\nvolume;\n```\n\n----------------------------------------\n\nTITLE: Real-Time Volatility Prediction Visualization in DolphinDB\nDESCRIPTION: This code selects a random stock from the test set and plots the actual versus predicted volatility over several days. It uses DolphinDB's plotting functions to visually compare model predictions with real data, aiding in performance assessment.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/machine_learning_volatility.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\nstock_id = (select distinct(SecurityID) from Test)[rand(50,1)[0]].distinct_SecurityID\nplot((select targetRV, predict from Test where SecurityID=stock_id, date(TradeTime) between 2020.10.19 : 2020.10.23), title=\"The realized volatility of\" + stock_id, extras={multiYAxes: false})\n```\n\n----------------------------------------\n\nTITLE: Using each with eval and makeCall to apply multiple functions\nDESCRIPTION: This code snippet utilizes 'each', 'eval', and 'makeCall' for applying functions ('sin' and 'log') to a vector (1..3). It constructs the function calls as metadata using 'makeCall' and then evaluates them using 'eval'.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_47\n\nLANGUAGE: shell\nCODE:\n```\neach(eval, each(makeCall{,1..3},(sin,log)))\n```\n\n----------------------------------------\n\nTITLE: Finding DolphinDB Process Windows\nDESCRIPTION: This command checks for the DolphinDB process on a Windows system.  It utilizes the `tasklist` command to list all running processes and pipes the output to `findstr` which searches for \"dolphindb\" in the list. This allows you to verify the server's operation.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/standalone_server.md#_snippet_18\n\nLANGUAGE: sh\nCODE:\n```\ntasklist|findstr \"dolphindb\"\n```\n\n----------------------------------------\n\nTITLE: Loading Data from Disk Partitioned Table with loadTableBySQL\nDESCRIPTION: Loads data from a disk-based partitioned table into memory using `loadTableBySQL` in conjunction with `loadTable`.  This allows for selective loading of rows and columns using SQL queries, providing the most flexible way to create in-memory partitioned tables.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/partitioned_in_memory_table.md#_snippet_11\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndb = database(workDir+\"/tradeDB\")\ntrades=loadTable(db, `trades);\n\nsample=loadTableBySQL(<select * from trades where date between 2000.03.01 : 2000.05.01>);\n\nsample=loadTableBySQL(<select sym, date, price1, qty1 from trades where date between 2000.03.01 : 2000.05.01>);\n\ndates = 2000.01.16 2000.02.14 2000.08.01\nst = sql(<select sym, date, price1, qty1>, trades, expr(<date>, in, dates))\nsample = loadTableBySQL(st);\n\ncolNames =`sym`date`qty2`price2\nst= sql(sqlCol(colNames), trades)\nsample = loadTableBySQL(st);\n```\n\n----------------------------------------\n\nTITLE: Performing Feature Engineering per Group\nDESCRIPTION: Defines a global aggregate function `featureEngineering` that operates on data within each group (e.g., each 10-minute bar for a security). It calculates various derived features (like WAP, spreads, volume imbalances, log returns), creates a temporary table, and then runs four separate SQL aggregations on this temporary table using the provided `aggMetaCode`, filtering data based on time offsets (0s, 150s, 300s, 450s) from the bar start time. The results from the four aggregations are concatenated into a single matrix.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/metacode_derived_features/metacode_derived_features.txt#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndefg featureEngineering(DateTime, BidPrice, BidOrderQty, OfferPrice, OfferOrderQty, aggMetaCode){\n\twap = (BidPrice * OfferOrderQty + BidOrderQty * OfferPrice) \\ (BidOrderQty + OfferOrderQty)\n\twapBalance = abs(wap[0] - wap[1])\n\tpriceSpread = (OfferPrice[0] - BidPrice[0]) \\ ((OfferPrice[0] + BidPrice[0]) \\ 2)\n\tBidSpread = BidPrice[0] - BidPrice[1]\n\tOfferSpread = OfferPrice[0] - OfferPrice[1]\n\ttotalVolume = OfferOrderQty.rowSum() + BidOrderQty.rowSum()\n\tvolumeImbalance = abs(OfferOrderQty.rowSum() - BidOrderQty.rowSum())\n\tLogReturnWap = logReturn(wap)\n\tLogReturnOffer = logReturn(OfferPrice)\n\tLogReturnBid = logReturn(BidPrice)\n\tsubTable = table(DateTime as `DateTime, BidPrice, BidOrderQty, OfferPrice, OfferOrderQty, wap, wapBalance, priceSpread, BidSpread, OfferSpread, totalVolume, volumeImbalance, LogReturnWap, LogReturnOffer, LogReturnBid)\n\tcolNum = 0..9$STRING\n\tcolName = `DateTime <- (`BidPrice + colNum) <- (`BidOrderQty + colNum) <- (`OfferPrice + colNum) <- (`OfferOrderQty + colNum) <- (`Wap + colNum) <- `WapBalance`PriceSpread`BidSpread`OfferSpread`TotalVolume`VolumeImbalance <- (`LogReturn + colNum) <- (`LogReturnOffer + colNum) <- (`LogReturnBid + colNum)\n\tsubTable.rename!(colName)\n\tsubTable['BarDateTime'] = bar(subTable['DateTime'], 10m)\n\tresult = sql(select = aggMetaCode, from = subTable).eval().matrix()\n\tresult150 = sql(select = aggMetaCode, from = subTable, where = <time(DateTime) >= (time(BarDateTime) + 150*1000) >).eval().matrix()\n\tresult300 = sql(select = aggMetaCode, from = subTable, where = <time(DateTime) >= (time(BarDateTime) + 300*1000) >).eval().matrix()\n\tresult450 = sql(select = aggMetaCode, from = subTable, where = <time(DateTime) >= (time(BarDateTime) + 450*1000) >).eval().matrix()\n\treturn concatMatrix([result, result150, result300, result450])\n}\n```\n\n----------------------------------------\n\nTITLE: Evaluating Per-Row Calculations Based on Stored String Expressions Using DolphinDB\nDESCRIPTION: This snippet performs row-wise computations where the calculation formula is stored as a string in one column (v), and input variables (a, b) are in other columns of the same table. It uses DolphinDB's parseExpr function with a variable dictionary to parse and evaluate string expressions dynamically per row. The each function iterates over all rows, evaluating the expression for each row's variables by removing the formula column to form the variable dictionary. Output is a vector of evaluated results per row. This approach enables executing arbitrary string-based formulas stored in data tables, supporting complex dynamic computations within DolphinDB.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/macro_var_based_metaprogramming.md#_snippet_29\n\nLANGUAGE: DolphinDB\nCODE:\n```\neach(def(mutable d)->parseExpr(d.v, d.erase!(`v)).eval(), t)\n```\n\n----------------------------------------\n\nTITLE: Defining main Function\nDESCRIPTION: This is the main function that orchestrates the entire data ingestion and processing pipeline.  It calls `clearEnv` to clean up previous runs, `createInOutTable` to create stream tables, `consume` to set up the reactive and session window engines, and `writeStreamTable` to subscribe to the MQTT topic. It then publishes the content of deviceState.csv to the configured topic using a JSON formatter.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/data/DolphinDB_streaming_application_in_IOT/deviceState.txt#_snippet_9\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef main(){\n\tclearEnv()\n\t\n\tcreateInOutTable()\n\tconsume()\n\t\n\thost=\"127.0.0.1\"\n\tport=1883\n\ttopic=\"sensor/test\"\n\twriteStreamTable(host, port, topic)\n\n\tt=loadText(getHomeDir()+\"/deviceState.csv\")\n\t//myFormat=take(\"\", 3)   f= mqtt::createCsvFormatter(myFormat, ',', ';')\n\tf = createJsonFormatter()\n\tbatchsize=100\n\tsubmitJob(\"submit_pub1\", \"submit_p1\", publishTableData{host,topic,f, batchsize,t})\n}\n```\n\n----------------------------------------\n\nTITLE: Submitting Parallel Jobs for Yearly Data Loading in DolphinDB\nDESCRIPTION: Defines `loopLoadOneYearFiles` which finds all subdirectories within a given base `filePath`. It iterates through these directories (each assumed to represent data for one day) and submits a separate background job (`submitJob`) for each directory. Each job executes the `loadOneDayFiles` function with the appropriate parameters, enabling parallel processing of daily data loads across the entire year.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/csvImportDemo/importNewData.txt#_snippet_3\n\nLANGUAGE: dolphindb\nCODE:\n```\ndef loopLoadOneYearFiles(dbName,tableName, filePath,schema1){\n\tdirs = exec filename from files(filePath) where isDir=true\n\tfor (path in dirs){\n\t\tsubmitJob(\"new\"+path,\"loadOrderDir\"+path,loadOneDayFiles{dbName,tableName,filePath+\"/\"+path,schema1})\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Database Tables in DolphinDB - DolphinDB Script\nDESCRIPTION: This snippet demonstrates commands to create essential DolphinDB database tables for ETL tasks, such as entrust, snapshot, trade, and stock minute factor tables. It includes invoking initialization scripts from stockData and minFactor modules and calls to dedicated database creation functions with configurable database and table names. These tables serve as storage for various processed financial data required by ETL workflows.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/dolphinscheduler_integration.md#_snippet_17\n\nLANGUAGE: DolphinDB\nCODE:\n```\nuse stockData::createStockTable\nuse minFactor::createMinFactorTable\n\n// 创建数据库表，库表名可以根据实际需要修改\ncreateEntrust(\"dfs://stockData\", \"entrust\")\ncreateSnapshot(\"dfs://stockData\", \"snapshot\")\ncreateTrade(\"dfs://stockData\", \"trade\")\ncreateMinuteFactor(\"dfs://factorData\", \"stockMinFactor\")\n```\n\n----------------------------------------\n\nTITLE: Implementing a Node-RED Functional Node for DolphinDB - Python Syntax\nDESCRIPTION: This code defines a Node-RED node type 'DolphinDB,' implementing a node that references the above configuration node and initializes a client object for DolphinDB operations. The constructor uses the RED.nodes.createNode pattern and retrieves the DolphinDB configuration node from the Node-RED system. Requires Node-RED API and prior configuration node registration.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/node_red_tutorial_iot.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n//构造函数\nfunction DolphinDBNode(config) {\n    RED.nodes.createNode(this,config);\n    // 获取 DolphinDB 配置结点\n    this.dolphindb = n.dolphindb;\n    this.dolphindbConfig = RED.nodes.getNode(this.dolphindb);\n    var client = this.dolphindbConfig.client;\n    \n    //功能语句\n}\n//注册节点\nRED.nodes.registerType(\"DolphinDB\",DolphinDBNode);\n```\n\n----------------------------------------\n\nTITLE: Permanently Disabling Swap in /etc/fstab (config)\nDESCRIPTION: Comment out the swap entry in `/etc/fstab` to turn off swap space permanently on reboot. This step is essential for performance-sensitive DolphinDB deployments. Requires editing with root privileges.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/deploy_dolphindb_on_new_server.md#_snippet_13\n\nLANGUAGE: config\nCODE:\n```\n# vi /etc/fstab\n...\n# /dev/mapper/centos-swap swap                    swap    defaults        0 0\n...\n\n```\n\n----------------------------------------\n\nTITLE: Calling Module Function Using Namespace (Naming Conflict Resolution)\nDESCRIPTION: Demonstrates how to call the `myfunc` function specifically from the imported `sys` module by using its namespace (`sys::`). This method resolves the naming conflict with the locally defined `myfunc` or function view, ensuring the function from the module is executed.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/module_tutorial.md#_snippet_16\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nsys::myfunc()\n```\n\n----------------------------------------\n\nTITLE: Creating Partitioned Table for Raw Snapshot Data in DolphinDB\nDESCRIPTION: Creates a distributed database table for storing raw market snapshot data. The table uses composite partitioning with first-level partition by date and second-level partition by security ID (HASH with 50 buckets). The schema includes comprehensive market data fields including prices, order book data, and trading statistics.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Best_Practices_for_DolphinDB_and_Python_AirFlow.md#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\nmodule loadSnapshot::createSnapshotTable\n\n//创建 snapshot 原始数据存储库表\ndef createSnapshot(dbName, tbName){\n\tlogin(\"admin\", \"123456\")\n\tif(existsDatabase(dbName)){\n\t\tdropDatabase(dbName)\n\t}\n\tdb1 = database(, VALUE, 2020.01.01..2021.01.01)\n\tdb2 = database(, HASH, [SYMBOL, 50])\n\t//按天和股票代码组合分区\n\tdb = database(dbName,COMPO,[db1,db2],engine='TSDB')\n\tcolName = [\"SecurityID\",\"DateTime\",\"PreClosePx\",\"OpenPx\",\"HighPx\",\"LowPx\",\"LastPx\",\"TotalVolumeTrade\",\"TotalValueTrade\",\"InstrumentStatus\",\"BidPrice0\",\"BidPrice1\",\"BidPrice2\",\"BidPrice3\",\"BidPrice4\",\"BidPrice5\",\"BidPrice6\",\"BidPrice7\",\"BidPrice8\",\"BidPrice9\",\"BidOrderQty0\",\"BidOrderQty1\",\"BidOrderQty2\",\"BidOrderQty3\",\"BidOrderQty4\",\"BidOrderQty5\",\"BidOrderQty6\",\"BidOrderQty7\",\"BidOrderQty8\",\"BidOrderQty9\",\"BidNumOrders0\",\"BidNumOrders1\",\"BidNumOrders2\",\"BidNumOrders3\",\"BidNumOrders4\",\"BidNumOrders5\",\"BidNumOrders6\",\"BidNumOrders7\",\"BidNumOrders8\",\"BidNumOrders9\",\"BidOrders0\",\"BidOrders1\",\"BidOrders2\",\"BidOrders3\",\"BidOrders4\",\"BidOrders5\",\"BidOrders6\",\"BidOrders7\",\"BidOrders8\",\"BidOrders9\",\"BidOrders10\",\"BidOrders11\",\"BidOrders12\",\"BidOrders13\",\"BidOrders14\",\"BidOrders15\",\"BidOrders16\",\"BidOrders17\",\"BidOrders18\",\"BidOrders19\",\"BidOrders20\",\"BidOrders21\",\"BidOrders22\",\"BidOrders23\",\"BidOrders24\",\"BidOrders25\",\"BidOrders26\",\"BidOrders27\",\"BidOrders28\",\"BidOrders29\",\"BidOrders30\",\"BidOrders31\",\"BidOrders32\",\"BidOrders33\",\"BidOrders34\",\"BidOrders35\",\"BidOrders36\",\"BidOrders37\",\"BidOrders38\",\"BidOrders39\",\"BidOrders40\",\"BidOrders41\",\"BidOrders42\",\"BidOrders43\",\"BidOrders44\",\"BidOrders45\",\"BidOrders46\",\"BidOrders47\",\"BidOrders48\",\"BidOrders49\",\"OfferPrice0\",\"OfferPrice1\",\"OfferPrice2\",\"OfferPrice3\",\"OfferPrice4\",\"OfferPrice5\",\"OfferPrice6\",\"OfferPrice7\",\"OfferPrice8\",\"OfferPrice9\",\"OfferOrderQty0\",\"OfferOrderQty1\",\"OfferOrderQty2\",\"OfferOrderQty3\",\"OfferOrderQty4\",\"OfferOrderQty5\",\"OfferOrderQty6\",\"OfferOrderQty7\",\"OfferOrderQty8\",\"OfferOrderQty9\",\"OfferNumOrders0\",\"OfferNumOrders1\",\"OfferNumOrders2\",\"OfferNumOrders3\",\"OfferNumOrders4\",\"OfferNumOrders5\",\"OfferNumOrders6\",\"OfferNumOrders7\",\"OfferNumOrders8\",\"OfferNumOrders9\",\"OfferOrders0\",\"OfferOrders1\",\"OfferOrders2\",\"OfferOrders3\",\"OfferOrders4\",\"OfferOrders5\",\"OfferOrders6\",\"OfferOrders7\",\"OfferOrders8\",\"OfferOrders9\",\"OfferOrders10\",\"OfferOrders11\",\"OfferOrders12\",\"OfferOrders13\",\"OfferOrders14\",\"OfferOrders15\",\"OfferOrders16\",\"OfferOrders17\",\"OfferOrders18\",\"OfferOrders19\",\"OfferOrders20\",\"OfferOrders21\",\"OfferOrders22\",\"OfferOrders23\",\"OfferOrders24\",\"OfferOrders25\",\"OfferOrders26\",\"OfferOrders27\",\"OfferOrders28\",\"OfferOrders29\",\"OfferOrders30\",\"OfferOrders31\",\"OfferOrders32\",\"OfferOrders33\",\"OfferOrders34\",\"OfferOrders35\",\"OfferOrders36\",\"OfferOrders37\",\"OfferOrders38\",\"OfferOrders39\",\"OfferOrders40\",\"OfferOrders41\",\"OfferOrders42\",\"OfferOrders43\",\"OfferOrders44\",\"OfferOrders45\",\"OfferOrders46\",\"OfferOrders47\",\"OfferOrders48\",\"OfferOrders49\",\"NumTrades\",\"IOPV\",\"TotalBidQty\",\"TotalOfferQty\",\"WeightedAvgBidPx\",\"WeightedAvgOfferPx\",\"TotalBidNumber\",\"TotalOfferNumber\",\"BidTradeMaxDuration\",\"OfferTradeMaxDuration\",\"NumBidOrders\",\"NumOfferOrders\",\"WithdrawBuyNumber\",\"WithdrawBuyAmount\",\"WithdrawBuyMoney\",\"WithdrawSellNumber\",\"WithdrawSellAmount\",\"WithdrawSellMoney\",\"ETFBuyNumber\",\"ETFBuyAmount\",\"ETFBuyMoney\",\"ETFSellNumber\",\"ETFSellAmount\",\"ETFSellMoney\"]\n\tcolType = [\"SYMBOL\",\"TIMESTAMP\",\"DOUBLE\",\"DOUBLE\",\"DOUBLE\",\"DOUBLE\",\"DOUBLE\",\"INT\",\"DOUBLE\",\"SYMBOL\",\"DOUBLE\",\"DOUBLE\",\"DOUBLE\",\"DOUBLE\",\"DOUBLE\",\"DOUBLE\",\"DOUBLE\",\"DOUBLE\",\"DOUBLE\",\"DOUBLE\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"DOUBLE\",\"DOUBLE\",\"DOUBLE\",\"DOUBLE\",\"DOUBLE\",\"DOUBLE\",\"DOUBLE\",\"DOUBLE\",\"DOUBLE\",\"DOUBLE\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"DOUBLE\",\"DOUBLE\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"DOUBLE\",\"INT\",\"INT\",\"DOUBLE\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\"]\n\tschemaTable = table(1:0,colName, colType)\n\t\n\tdb.createPartitionedTable(table=schemaTable, tableName=tbName, partitionColumns=`DateTime`SecurityID, compressMethods={DateTime:\"delta\"}, sortColumns=`SecurityID`DateTime, keepDuplicates=ALL)\n}\n```\n\n----------------------------------------\n\nTITLE: Sending Email to Single Recipient in DolphinDB\nDESCRIPTION: Sends an email to a single recipient using the httpClient::sendEmail function, specifying the sender credentials, recipient, subject, and message text. Verifies successful sending by checking the response code.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/send_messages_external_systems.md#_snippet_18\n\nLANGUAGE: dolphindb\nCODE:\n```\nres=httpClient::sendEmail(user,psw,recipient,'This is a subject','It is a text');\nassert  res[`responseCode]==250;\n```\n\n----------------------------------------\n\nTITLE: Calculating Hourly Total Production - DolphinDB\nDESCRIPTION: This script calculates the total production amount per hour for a given device and measurement point. It uses the `bar` function to group data by hour, then sums the 'propertyValue' within each group. It also defines a function view for easy querying.  It requires a table named 'collect' with columns 'ts', 'deviceCode', 'logicalPositionId', 'physicalPositionId', 'propertyCode', 'propertyValue'.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Iot_intelligent_O&M.md#_snippet_4\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\n//案例 4：实时计算生料磨每小时的汇总产量\ndevice=\"361RP88\"      //设备编号\npoint=\"361RP88001\"    //测点编号,记录产量的测点，得到的 propertyValue 表示产量值\n\npt=loadTable(\"dfs://db_test\",`collect)\ntimer t=select deviceCode,logicalPositionId,physicalPositionId,sum(propertyValue) as production_amount_hour,ts from pt where ts between 2022.01.01 00:00:00 : 2022.01.01 02:59:59 and deviceCode=device and propertyCode=point group by bar(ts,1H) as ts,deviceCode,logicalPositionId,physicalPositionId\nt\n\n//自定义函数视图，供应用端调用：\ndef production_amount_hour(device,point,begintime,endtime){\n    //业务判断，合法性校验\n    if (datetime(endtime)-datetime(begintime)>24*3600*30){\n        return(toStdJson([-1,\"查询时间不能超过30天\"]))\n    }\n    t=select deviceCode,logicalPositionId,physicalPositionId,sum(propertyValue) as production_amount_hour,ts from loadTable(\"dfs://db_test\",`collect) where ts between begintime : endtime and deviceCode=device and propertyCode=point group by bar(ts,1H) as ts,deviceCode,logicalPositionId,physicalPositionId\n    return t\n}\n//添加函数视图\naddFunctionView(production_amount_hour)\n\nproduction_amount_hour(device,point,2022.01.01 00:00:00,2022.03.01 00:00:00)    //业务判断\nproduction_amount_hour(device,point,2022.01.01 00:00:00,2022.01.01 23:59:59)    //应用端正常查询，并输出结果\n```\n\n----------------------------------------\n\nTITLE: Creating DECIMAL32 Scalars in DolphinDB Using Various Input Types - DolphinDB\nDESCRIPTION: Creates DECIMAL32 scalar values by converting integer, floating-point, or string inputs using the decimal32() function with a specified scale parameter that indicates the number of decimal places to preserve. Demonstrates how input values are rounded or truncated based on the scale. Requires DolphinDB version supporting DECIMAL types.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DECIMAL.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\na=decimal32(142, 2)\n//output：142.00\n\nb=decimal32(1.23456, 3)\n//output: 1.234\n\nc=decimal32(`1.23456, 3)\n//output: 1.234 \n```\n\n----------------------------------------\n\nTITLE: Main Job Orchestration - Dolphindb\nDESCRIPTION: Defines the main function to orchestrate the data generation and writing process. It first checks if the target database exists and drops it if necessary, then creates the database and table. Finally, it submits either the single-threaded or multi-threaded writing job based on the 'threads' parameter.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/singleValueModeWrite.txt#_snippet_4\n\nLANGUAGE: Dolphindb\nCODE:\n```\ndef mainJob(id, startDay, days, ps1, ps2, freqPerDay, numIdPerPartition, threads){\n    if(existsDatabase(\"dfs://svmDemo\"))\n\t\tdropDatabase(\"dfs://svmDemo\")\n    createDatabase(\"dfs://svmDemo\",\"sensors\", ps1, ps2)\n\n    if(threads == 1)\n    \tsubmitJob(\"submit_singleThreadWriting\", \"write data\", singleThreadWriting{id, startDay, days, freqPerDay, numIdPerPartition})\n    else\n    \tsubmitJob(\"submit_multipleThreadWriting\", \"write data\", multipleThreadWriting{id, startDay, days, freqPerDay, numIdPerPartition, threads})\n}\n```\n\n----------------------------------------\n\nTITLE: Calculating Device Uptime in DolphinDB Script\nDESCRIPTION: Generates sample device status data (timestamp, device ID, status) and calculates the total operating time for each device. It utilizes the `deltas` function within a `context by` clause to find time differences between consecutive status entries for each device and aggregates the duration where `status=1` (assuming 1 means 'on'), converting the result to hours.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/high_freq_data_storage_and_analysis.md#_snippet_10\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\n/**** 统计设备使用时长 ****/\n//1. 生成数据\nn=100\ntimes=datetime(2022.01.01)+1..864000\nts=sort(rand(times,n))\nstatus=rand(1..3,n)\ndeviceid=rand(\"D\"+lpad(string(1..9),2,\"00\"),n)\nt=table(ts,deviceid, status)\n\n//2. 汇总设备状态时长\ndt = select *, deltas(ts) as duration from t context by deviceid\ndt=select sum(duration)/3600 as totalhour from dt where status=1 group by deviceid, status \n\n//3. 返回使用时长\nselect deviceid,string(totalhour)+\"小时\" as totalUse from dt order by totalhour desc\n```\n\n----------------------------------------\n\nTITLE: Generating Fee Histograms by Fund Type in DolphinDB\nDESCRIPTION: Creates histogram visualizations of fee distributions for different fund types, including REITs,保本型 (principal-protected), 债券型 (bond), and various other fund categories, with 100 bins for detailed distribution analysis.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/public_fund_basic_analysis/basic_analysis_of_public_fund_open_market_data.txt#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// Type=\"REITs\"\n(exec Fee from fundFee where Type=\"REITs\").plotHist(binNum=100)\n// Type=\"保本型\"\n(exec Fee from fundFee where Type=\"保本型\").plotHist(binNum=100)\n// Type=\"债券型\"\n(exec Fee from fundFee where Type=\"债券型\").plotHist(binNum=100)\n// Type=\"另类投资型\"\n(exec Fee from fundFee where Type=\"另类投资型\").plotHist(binNum=100)\n// Type=\"商品型\"\n(exec Fee from fundFee where Type=\"商品型\").plotHist(binNum=100)\n// Type=\"混合型\"\n(exec Fee from fundFee where Type=\"混合型\").plotHist(binNum=100)\n// Type=\"股票型\"\n(exec Fee from fundFee where Type=\"股票型\").plotHist(binNum=100)\n// Type=\"货币市场型\"\n(exec Fee from fundFee where Type=\"货币市场型\").plotHist(binNum=100)\n```\n\n----------------------------------------\n\nTITLE: Establishing Remote Connection for Remote Procedure Calls in DolphinDB - DolphinDB\nDESCRIPTION: Creates a remote database connection object 'h' to a DolphinDB node running at 'localhost' on port 8081 using 'xdb' function. This connection serves as the basis for executing remote procedure calls and running scripts remotely. The connection object encapsulates node address and port, enabling further remote executions with 'remoteRun' or calling the connection directly as a function.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/hybrid_programming_paradigms.md#_snippet_10\n\nLANGUAGE: DolphinDB\nCODE:\n```\nh = xdb(\"localhost\", 8081);\n```\n\n----------------------------------------\n\nTITLE: Data Import Script - Druid\nDESCRIPTION: This JSON configuration defines a Druid ingestion task to load data from CSV files.  It specifies the data source, parser details (CSV format, dimensions, timestamp column), metrics, granularity, IO configuration (local firehose), and tuning parameters. Dependencies: Druid installation, TAQ.csv file in the specified location.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_Druid_appendix.md#_snippet_12\n\nLANGUAGE: JSON\nCODE:\n```\n{\n\"type\" : \"index\",\n\"spec\" : {\n\"dataSchema\" : {\n\"dataSource\" : \"TAQ\",\n\"parser\" : {\n\"type\" : \"string\",\n\"parseSpec\" : {\n\"format\" : \"csv\",\n\"dimensionsSpec\" : {\n\"dimensions\" : [\n\"TIME\",\n\"SYMBOL\",\n{\"name\":\"BID\", \"type\" : \"double\"},\n{\"name\":\"OFR\", \"type\" : \"double\"},\n{\"name\":\"BIDSIZ\", \"type\" : \"int\"},\n{\"name\":\"OFRSIZ\", \"type\" : \"int\"},\n\"MODE\",\n\"EX\",\n\"MMID\"\n]\n},\n\"timestampSpec\": {\n\"column\": \"DATE\",\n\"format\": \"yyyyMMdd\"\n},\n\"columns\" : [\"SYMBOL\",\n\"DATE\",\n\"TIME\",\n\"BID\",\n\"OFR\",\n\"BIDSIZ\",\n\"OFRSIZ\",\n\"MODE\",\n\"EX\",\n\"MMID\"]\n}\n},\n\"metricsSpec\" : [],\n\"granularitySpec\" : {\n\"type\" : \"uniform\",\n\"segmentGranularity\" : \"day\",\n\"queryGranularity\" : \"none\",\n\"intervals\" : [\"2007-08-01/2007-09-01\"],\n\"rollup\" : false\n}\n},\n\"ioConfig\" : {\n\"type\" : \"index\",\n\"firehose\" : {\n\"type\" : \"local\",\n\"baseDir\" : \"/data/data/\",\n\"filter\" : \"TAQ.csv\"\n},\n\"appendToExisting\" : false\n},\n\"tuningConfig\" : {\n\"type\" : \"index\",\n\"targetPartitionSize\" : 5000000,\n\"maxRowsInMemory\" : 25000,\n\"forceExtendableShardSpecs\" : true\n}\n}\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring DolphinDB Single Mode Deployment with Stream Computing - Shell\nDESCRIPTION: This shell snippet shows the configuration parameters in dolphindb.cfg needed for single-node deployment with stream computing auto-subscription enabled. It defines network parameters, memory limits, worker numbers, persistence directory, startup script path, number of subscription executors, subscription port, throttle settings, and persistence worker count. These settings ensure the DolphinDB server runs in single mode with appropriate resources and points to the startup.dos script for auto-subscription initialization.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_auto_sub.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nlocalSite=localhost:8848:local8848\nmode=single\nmaxMemSize=128\nmaxConnections=512\nworkerNum=8\nmaxConnectionPerSite=15\nnewValuePartitionPolicy=add\nwebWorkerNum=8\ndataSync=1\nchunkCacheEngineMemSize=8\npersistenceDir=/opt/DolphinDB/server/local8848/persistenceDir\nstartup=/opt/DolphinDB/server/startup.dos\nmaxPubConnections=64\nsubExecutors=7\nsubPort=8849\nsubThrottle=1\npersistenceWorkerNum=1\nlanCluster=0\n```\n\n----------------------------------------\n\nTITLE: Airflow Core Commands for Initialization and Startup in Bash\nDESCRIPTION: This snippet contains shell commands for initializing the Airflow database, creating an admin user, and starting Airflow webserver and scheduler as daemon processes. The commands include: 'airflow db init' to initialize metadata, 'airflow users create' with user details for admin creation, 'airflow webserver' to start the UI service on port 8080 in the background, and 'airflow scheduler' to start the scheduler daemon. These commands must be run in an environment where Airflow is installed and configured, and are prerequisites for running Airflow DAGs including those integrating DolphinDB. The snippet also aids validation by checking running processes with 'ps -aux|grep airflow'.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Best_Practices_for_DolphinDB_and_Python_AirFlow.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n#初始化数据库\nairflow db init\n\n#创建用户\nairflow users create  --username admin  --firstname Peter  --lastname Parker  --role Admin  --email spiderman@superhero.org  --password admin\n\n# 守护进程运行 webserver\nairflow webserver --port 8080 -D\n\n# 守护进程运行 scheduler\nairflow scheduler -D\n```\n\n----------------------------------------\n\nTITLE: Elasticsearch Delete Scroll Context\nDESCRIPTION: This Python code defines a function, `delete_scroll()`, that issues an HTTP DELETE request to Elasticsearch to delete all active scroll contexts.  This is performed to release resources after performing queries with scrolling functionality. It requires `urllib3` and relies on a specific Elasticsearch endpoint.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_54\n\nLANGUAGE: Python\nCODE:\n```\ndef delete_scroll():\n    http = urllib3.PoolManager()\n    r = http.request(\"DELETE\", \"http://localhost:9200/_search/scroll/_all\")\n    print(r.status)\n    print(r.data)\n```\n\n----------------------------------------\n\nTITLE: Replacing License Linux\nDESCRIPTION: This snippet shows the path for replacing the license file on Linux. This file, `dolphindb.lic`, is replaced with the newer license. This applies to updating a trial or paid enterprise license.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/standalone_server.md#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\n/DolphinDB/server/dolphindb.lic\n```\n\n----------------------------------------\n\nTITLE: LU Decomposition for Non-Square Matrix - DolphinDB\nDESCRIPTION: This code demonstrates LU decomposition for a non-square matrix in DolphinDB.  The lu function works with non-square matrices. It shows how to obtain the permutation matrix (P), lower triangular matrix (L), and upper triangular matrix (U). The code showcases both the standard LU decomposition and a variation where PL is returned instead of P and L separately.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/matrix.md#_snippet_13\n\nLANGUAGE: DolphinDB\nCODE:\n```\n>m=matrix([2 5 5, 5 5 4, 8 6 4, 7 6 8]);\n>m;\n#0 #1 #2 #3\n-- -- -- --\n2  5  8  7\n5  5  6  6\n5  4  4  8\n\n>p,l,u=lu(m);\n>p;\n#0 #1 #2\n-- -- --\n0  1  0\n1  0  0\n0  0  1\n>l;\n#0  #1        #2\n--- --------- --\n1   0         0\n0.4 1         0\n1   -0.333333 1\n>u;\n#0 #1 #2        #3\n-- -- --------- --------\n5  5  6         6\n0  3  5.6       4.6\n0  0  -0.133333 3.533333\n\n>pl,u=lu(m,true);\n>pl;\n#0  #1        #2\n--- --------- --\n0.4 1         0\n1   0         0\n1   -0.333333 1\n>u;\n#0 #1 #2        #3\n-- -- --------- --------\n5  5  6         6\n0  3  5.6       4.6\n0  0  -0.133333 3.533333\n```\n\n----------------------------------------\n\nTITLE: DolphinDB Server Configuration for Data Persistence\nDESCRIPTION: Configuration snippet for DolphinDB server settings that specify directories for volumes, redo logs, chunk metadata, TSDB redo logs, persistence, and stream log persistence offsets. These parameters set the paths where DolphinDB stores persistent and streaming data files, essential for durable storage and recovery support. Dependencies: proper directory path substitutions as per installation environment. Inputs: custom storage paths. Outputs: enable DolphinDB persistence and recovery features.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_Telegraf_Grafana.md#_snippet_4\n\nLANGUAGE: Config\nCODE:\n```\nvolumes=<DolphinDBDir>/volumes \nredoLogDir=<DolphinDBDir>/redoLog\nchunkMetaDir=<DolphinDBDir>/chunkMeta\nTSDBRedoLogDir=<DolphinDBDir>/TSDBRedoLog\npersistenceDir<DolphinDBDir>/persistence\npersistenceOffsetDir=<DolphinDBDir>/streamlog\n```\n\n----------------------------------------\n\nTITLE: 1.2 将纳秒时间戳文本导入为 NANOTIMESTAMP\nDESCRIPTION: 演示如何从字符分隔的文本文件中，将存储为整数的纳秒时间戳字段转换为 NANOTIMESTAMP 类型，然后导入到分布式表中，适用于高精度时间数据加载。所用函数包括 nanotimestamp，数据结构包括组合分区，利用 transform 实现字段转换。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndbSendingTimeInNano=database(, VALUE, 2020.01.20..2020.02.22);\ndbSecurityIDRange=database(, RANGE, 0..10001);\ndb=database(\"dfs://testdb\", COMPO, [dbSendingTimeInNano,dbSecurityIDRange])\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nnameCol=`SendingTimeInNano`securityID`origSendingTimeInNano`bidSize\n typeCol=[`NANOTIMESTAMP,`INT,`NANOTIMESTAMP,`INT]\nschemaTb=table(1:0,nameCol,typeCol)\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndb=database(\"dfs://testdb\")\nx= db.createPartitionedTable(schemaTb, `nx, `SendingTimeInNano`securityID)\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef dataTransform(mutable t){\n  return t.replaceColumn!(`SendingTimeInNano, nanotimestamp(t.SendingTimeInNano)).replaceColumn!(`origSendingTimeInNano, nanotimestamp(t.origSendingTimeInNano))\n}\n\npt=loadTextEx(dbHandle=db,tableName=`nx , partitionColumns=`SendingTimeInNano`securityID,filename=\"nx.txt\",delimiter='#',transform=dataTransform);\n```\n\n----------------------------------------\n\nTITLE: Calculating Stock Portfolio Value (Optimized)\nDESCRIPTION: This snippet shows an optimized way to calculate the stock portfolio value using the `pivot by` clause and `ffill` function. It pivots the `ETF` table by `Time` and `Symbol`, filling NULL values using `ffill`, and then calculates the row sum of the filled values.  This is much faster and simpler than the previous method.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_sql_cases.md#_snippet_28\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntimer t2 = select rowSum(ffill(last(weightedPrice))) from ETF pivot by Time, Symbol\n```\n\n----------------------------------------\n\nTITLE: Constructing URL for Sending WeChat Work Application Messages\nDESCRIPTION: Code for constructing the complete URL needed to send messages via WeChat Work application. The access token obtained in the previous step is appended to the base URL.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/send_messages_external_systems.md#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\nurl = 'https://qyapi.weixin.qq.com/cgi-bin/message/send?';\nACCESS_TOKEN='xxxxx';\nurl+='access_token='+ACCESS_TOKEN;\n```\n\n----------------------------------------\n\nTITLE: Register and Subscribe Stream Data Tables in DolphinDB\nDESCRIPTION: This code snippet registers the stream calculation engines and subscribes to the stream data table. It defines the parallel degree and calls functions to process sell orders, buy orders, and capital flow. The `parallel` parameter controls the degree of parallelism for stream calculations.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_capital_flow_daily.md#_snippet_8\n\nLANGUAGE: DolphinDB\nCODE:\n```\nparallel = 3\nprocessCapitalFlowFunc(parallel)\ngo\nprocessSellOrderFunc(parallel)\ngo\nprocessBuyOrderFunc(parallel)\nprocessCapitalFlow60minFunc()\n```\n\n----------------------------------------\n\nTITLE: Copying DFS Tables with sqlDS in DolphinDB\nDESCRIPTION: Demonstrates creating a backup table with the same structure and copying data using sqlDS with map-reduce, which is more efficient than repartitionDS when maintaining the same partition scheme. Both approaches are shown for comparison.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/database.md#_snippet_13\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//创建tb1_bak\ndb=database(\"dfs://db1\")\nt=table(1:0,`timestamp`sym`qty`price,[TIMESTAMP,SYMBOL,DOUBLE,DOUBLE])\ndb.createPartitionedTable(t,`tb1_bak,`timestamp`sym)\n\n//把表tb1的内容写入到表tb1_bak中\ndef writeDataTo(dbPath, tbName, mutable tbdata){\n\tloadTable(dbPath,tbName).append!(tbdata)\n}\n\ndatasrc=sqlDS(<select * from tb1>)\nmr(ds=datasrc, mapFunc=writeDataTo{\"dfs://db1\",\"tb1_bak\"}, parallel=true)\n\ndatasrc=repartitionDS(<select * from tb1>,`date,VALUE)\nmr(ds=datasrc, mapFunc=writeDataTo{\"dfs://db1\",\"tb1_bak\"}, parallel=true)\n```\n\n----------------------------------------\n\nTITLE: Sending a message to DingTalk group chat with HTTP POST\nDESCRIPTION: This code demonstrates how to send a text message to an existing DingTalk chat by using DolphinDB's httpClient::httpPost function. It formats the message as JSON, sets the Content-Type header, and posts the message to DingTalk's chat send API endpoint.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/send_messages_external_systems.md#_snippet_15\n\nLANGUAGE: DolphinDB\nCODE:\n```\nurl = \"https://oapi.dingtalk.com/chat/send?\";\nACCESS_TOKEN='xxxxx';\nurl+='access_token='+ACCESS_TOKEN;\nheaders=map(string,string);\nheaders['Content-Type']='application/json';\nparam='{\"chatid\" : \"xxxxx\",\"msg\" : {\"msgtype\" : \"text\",\"text\" : {\"content\" : \"这是一条测试信息\"}}}';\nret=httpClient::httpPost(url,param,1000,headers);\nprint ret['text'];\nbody = parseExpr(ret.text).eval();\nERRCODE=body.errcode;\n```\n\n----------------------------------------\n\nTITLE: DolphinDB Time-Based Filtering\nDESCRIPTION: This DolphinDB script filters data from the 'trades' table based on a date range. It uses the `timer()` function to measure the execution time of the query, which selects all columns (`*`) from the 'trades' table where the 'date' column falls within the specified range. The script executes the query 10 times for performance testing.  Requires the `trades` table to be accessible in the DolphinDB environment.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_37\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\n//根据时间过滤\ntimer(10) select * from trades where date >= 2008.08.08 and date <= 2010.08.08\n```\n\n----------------------------------------\n\nTITLE: DolphinDB Table-Level Revoke over Global Grant\nDESCRIPTION: This example illustrates that a table-level `revoke` does not override a global `grant`. Even after revoking read access to `dfs://test/pt`, user1 retains read access to all tables due to the initial global `grant`.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Permission_Management.md#_snippet_11\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(\"admin\", \"123456\")\ncreateUser(\"user1\",\"123456\")\ndbName = \"dfs://test\"\nt = table(1..10 as id , rand(100, 10) as val)\nif(existsDatabase(dbName)){\n        dropDatabase(dbName)\n}\ndb=database(dbName, VALUE, 1..10)\npt=  db.createPartitionedTable(t, \"pt\", \"id\")\npt.append!(t)\ngrant(\"user1\", TABLE_READ, \"*\")\nrevoke(\"user1\", TABLE_READ, dbName+\"/pt\")\n\nlogin(\"user1\", \"123456\")\nselect * from loadTable(dbName, \"pt\")//user1 有读 \"dfs://test/pt\" 的权限\n```\n\n----------------------------------------\n\nTITLE: Defining a Node-RED Configuration Node for DolphinDB - Python Syntax\nDESCRIPTION: This snippet implements a Node-RED configuration node named 'dolphindb.' It initializes a DolphinDB JavaScript client with user credentials, and registers the credentials as part of the node definition. The configuration node is referenced by other DolphinDB nodes. Requires Node-RED module API and DolphinDB JavaScript library.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/node_red_tutorial_iot.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfunction DolphinConfigNode(n) {\n        RED.nodes.createNode(this, n);\n        this.url = n.url\n        this.name = n.name\n        this.client = new DDB(this.url, {\n            autologin: true,\n            username: this.credentials.username,\n            password: this.credentials.password,\n            python: false,\n            streaming: undefined\n        })\n    }\n    RED.nodes.registerType(\"dolphindb\", DolphinConfigNode, {\n        credentials: {\n            username: { type: \"text\" },\n            password: { type: \"password\" }\n        }\n    });\n```\n\n----------------------------------------\n\nTITLE: Starting DolphinDB Cluster Controller Nodes via Shell Commands (console)\nDESCRIPTION: Navigate to the DolphinDB cluster directory and execute the `startController.sh` script to launch controller nodes. Paths may differ based on your deployment. This step must be performed on each controller host with the appropriate configuration files.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/deploy_dolphindb_on_new_server.md#_snippet_17\n\nLANGUAGE: console\nCODE:\n```\n$ cd DolphinDB/clusterDemo/\n$ ./startController.sh\n\n```\n\n----------------------------------------\n\nTITLE: Submitting Parallel Job 2 (Multiple Users) in DolphinDB\nDESCRIPTION: This code submits `parJob2` as a multi-user job.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_factor.txt#_snippet_19\n\nLANGUAGE: DolphinDB\nCODE:\n```\n/**\n * 提交5个job（多用户）\n */\nfor(i in 0..4){\n\tsubmitJob(\"parallJob5\", \"parallJob_multi_ten\", parJob2)\n}\n```\n\n----------------------------------------\n\nTITLE: Replaying data with replay function\nDESCRIPTION: The `replay` function replays data in time order, simulating real-time data writing. It supports various replay modes based on the `replayRate` and `absoluteRate` parameters, including specifying the number of records per second and accelerating replay based on timestamps.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/faultAnalysis.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\nreplay(inputTables, outputTables, [dateColumn], [timeColumn], [replayRate], [absoluteRate=true], [parallelLevel=1])\n```\n\n----------------------------------------\n\nTITLE: Starting DolphinScheduler Standalone Server - Shell\nDESCRIPTION: This shell command starts the DolphinScheduler standalone server daemon after configuration steps are complete. It is fundamental for launching the scheduler service to enable workflow and task execution. Proper startup confirms with commands like `jps` and status checks. Port conflicts must be resolved prior to execution for a successful launch.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/dolphinscheduler_integration.md#_snippet_4\n\nLANGUAGE: Shell\nCODE:\n```\nbash ./bin/dolphinscheduler-daemon.sh start standalone-server\n```\n\n----------------------------------------\n\nTITLE: Concurrent Partitioned Table Writing Example in DolphinDB Script\nDESCRIPTION: This snippet demonstrates concurrent writing to different partitions of a regular partitioned in-memory table. It defines a write function and launches asynchronous jobs, each writing to a separate partition. Key dependencies are submitJob, append!, and table creation functions. The example ensures thread safety by assigning distinct ID ranges per job.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/in_memory_table.md#_snippet_6\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nt=table(1:0,`id`val,[INT,INT])\ndb=database(\"\",RANGE,1 101 201 301)\npt=db.createPartitionedTable(t,`pt,`id)\n\ndef writeData(mutable t,id,batchSize,n){\n\tfor(i in 1..n){\n\t\tidv=take(id,batchSize)\n\t\tvalv=rand(100,batchSize)\n\t\ttmp=table(idv,valv)\n\t\tt.append!(tmp)\n\t}\n}\n\njob1=submitJob(\"write1\",\"\",writeData,pt,1..100,1000,1000)\njob2=submitJob(\"write2\",\"\",writeData,pt,101..200,1000,1000)\njob3=submitJob(\"write3\",\"\",writeData,pt,201..300,1000,1000)\n```\n\n----------------------------------------\n\nTITLE: Aligning indexedMatrix Objects with Binary Operations in DolphinDB\nDESCRIPTION: This DolphinDB code aligns two indexedMatrix objects with potentially different row and column labels before performing element-wise addition. The alignment follows the same logic as with indexedSeries, using outer joins over both dimensions; missing elements result in NULLs. Dependencies: matrix constructor, rename!, setIndexedMatrix! methods. Inputs: matrices with different but overlapping row and column labels. Output: aligned matrix with summed values where both label sets overlap; absent entries are NULL. Supported natively from v1.30.20/2.00.8 and above.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/panel_data.md#_snippet_19\n\nLANGUAGE: DolphinDB\nCODE:\n```\nid1 = 2020.11.01..2020.11.06;\nm1 = matrix(1..6, 7..12, 13..18).rename!(id1, `a`b`d)\nm1.setIndexedMatrix!()\n\nid2 = 2020.11.04..2020.11.09;\nm2 = matrix(4..9, 10..15, 16..21).rename!(id2, `a`b`c)\nm2.setIndexedMatrix!()\n\nm1+m2;\n```\n\n----------------------------------------\n\nTITLE: Rebalancing Chunks Within a Data Node in DolphinDB\nDESCRIPTION: This DolphinDB script initiates a rebalance of data chunks within a specified data node. It uses `rebalanceChunksWithinDataNode`, a function likely provided by DolphinDB's controller.  The function takes the data node's alias and a boolean indicating whether to delete the source data after moving the chunks. The script is executed twice with different parameters for optimal rebalancing.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/data_move_rebalance.md#_snippet_10\n\nLANGUAGE: DolphinDB\nCODE:\n```\nrpc(getControllerAlias(), rebalanceChunksWithinDataNode{ \"P1-dn1\", false })\nrpc(getControllerAlias(), rebalanceChunksWithinDataNode{ \"P1-dn1\", true })\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nrpc(getControllerAlias(), rebalanceChunksWithinDataNode{ \"P2-dn1\", false })\nrpc(getControllerAlias(), rebalanceChunksWithinDataNode{ \"P2-dn1\", true })\n```\n\n----------------------------------------\n\nTITLE: 创建 DolphinDB 日频因子存储库表示例 - Python\nDESCRIPTION: 示例展示如何使用 DolphinDB Python API 创建一个采用 TSDB 引擎的日频因子分区表，采用时间(按年范围)与因子名作为分区键，实现多维度复合分区。表结构设计为窄表模式，以适应多因子数据同时存储，具有更优的因子运维性能。演示数据库的创建、分区表的定义、压缩算法及排序索引配置，并通过 loadTable 查看最终表结构。依赖 dolphindb Python 包，底层需 DolphinDB Server 支持相关功能。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_Python_Parser_Intro_for_Quantitative_Finance.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport dolphindb as ddb\n\ndbName, tbName = \"dfs://dayFactorDB\", \"dayFactorTB\"\n\n# 数据库如果存在，删除该数据库\nif existsDatabase(dbName):\n    dropDatabase(dbName)\n\n# 创建数据库：时间维度按年 RANGE 分区 + 因子名维度 VALUE 分区\ndb1 = database(\"\", ddb.RANGE, date(datetimeAdd(1980.01M,seq(0,80)*12,'M')))\ndb2 = database(\"\", ddb.VALUE, [\"f1\",\"f2\"].toddb())\ndb = database(dbName, ddb.COMPO, [db1, db2].toddb(), engine='TSDB', atomic='CHUNK')\n\n# 创建分区表\nschemaTB = table(array(ddb.DATE, 0) as tradetime, \n                array(ddb.SYMBOL, 0) as securityid, \n                array(ddb.SYMBOL, 0) as factorname, \n                array(ddb.DOUBLE, 0) as value)\n\ndb.createPartitionedTable(schemaTB, tbName, partitionColumns=[\"tradetime\", \"factorname\"].toddb(), compressMethods={\"tradetime\":\"delta\"}.toddb(), \n                        sortColumns=[\"securityid\", \"tradetime\"].toddb(), keepDuplicates=ddb.ALL, sortKeyMappingFunction=[lambda x:hashBucket(x, 500)].toddb())\n\n# 查看分区表结构\npt = loadTable(dbName, tbName)\npt.schema()\n```\n\n----------------------------------------\n\nTITLE: Cleaning Up DolphinDB Environment and Resources\nDESCRIPTION: This function cancels active jobs, unregisters tables, drops stream engines, and undefines shared table variables to reset the environment to a clean state. Dependencies include the DolphinDB environment and existing stream-related variables. It ensures that any previous data streams or tables are properly terminated and cleaned up before new setup.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/Real-Time_Stock_Price_Increase_Calculation/02.清理环境并创建相关流数据表.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef cleanEnvironment(){\n\tcancelJobEx()\n\ttry{ unsubscribeTable(tableName=`snapshotStreamTable, actionName=\"snapshotFilter\") } catch(ex){ print(ex) }\n\ttry{ dropStreamEngine(\"calChange\")} catch(ex){ print(ex) }\n\ttry{ dropStreamEngine(\"crossSectionalEngine\") } catch(ex){ print(ex) }\n\ttry{ undef(\"snapshotStreamTable\", SHARED) } catch(ex){ print(ex) }\n\ttry{ undef(\"changeCrossSectionalTable\", SHARED) } catch(ex){ print(ex) }\n}\n```\n\n----------------------------------------\n\nTITLE: Adjusting Time Variables with Operators in DolphinDB Shell\nDESCRIPTION: This snippet demonstrates how to adjust time variables in DolphinDB by adding or subtracting values using the \"+\" and \"-\" operators. The example shows adding and subtracting from the MONTH, DATE, and TIME types.  It also demonstrates modulo arithmetic for MINUTE and TIME types.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/date_time.md#_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\n>2016.02M-13;\n2015.01M\n>2018.02.17+100;\n2018.05.28\n>01:20:15+200;\n01:23:35\n>23:59m+10;\n00:09m\n>00:00:01-2;\n23:59:59\n>23:59:59.900+200;\n00:00:00.100\n```\n\n----------------------------------------\n\nTITLE: 生成测试数据计算N股VWAP\nDESCRIPTION: 创建模拟交易数据，用于计算每只股票最近1000股交易的VWAP。包含股票代码、价格和交易量三个字段。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_SQL_Case_Tutorial.md#_snippet_20\n\nLANGUAGE: DolphinDB\nCODE:\n```\nn = 500000\nt = table(rand(string(1..4000), n) as sym, rand(10.0, n) as price, rand(500, n) as vol)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Recent Jobs in DolphinDB\nDESCRIPTION: This line calls the `getRecentJobs()` function to retrieve information about recently submitted jobs. It doesn't take any arguments and its output provides information about jobs, including their status and progress. This function is used to monitor the execution of submitted jobs in DolphinDB.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/streaming_capital_flow_order_by_order/06.历史数据回放.txt#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\ngetRecentJobs()\n```\n\n----------------------------------------\n\nTITLE: Row Correlation Coefficient Calculation\nDESCRIPTION: Calculates the row-wise correlation coefficient between two Array Vectors and Columnar Tuples in DolphinDB using the `rowCorr` function. Demonstrates its application on pairs of Array Vectors and Columnar Tuples.  Also shows its usage within a table selection context.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Array_Vector.md#_snippet_13\n\nLANGUAGE: DolphinDB\nCODE:\n```\nx = array(INT[], 0).append!([1 2 3, 4 5, 6 7 8, 9 10])\nxx = array(INT[], 0).append!([3 1 2, 3 1, 1 2 3, 0 1])\nz = rowCorr(x, xx)\n/* z\n[-0.5,-1,1,1]\n*/\n\ny = [1 2 3, 4 5, 6 7 8, 9 10].setColumnarTuple!()\nyy = [3 1 2, 3 1, 1 2 3, 0 1].setColumnarTuple!()\nz = rowCorr(y, yy)\n/* z\n[-0.5,-1,1,1]\n*/\n\nt = table(1 2 3 4 as id, x as x, xx as xx, y as y, yy as yy)\nnew_t = select *, rowCorr(x, xx) as new_x, rowCorr(y, yy) as new_y from t\n/* new_t\nid x       xx      y       yy      new_x new_y\n-- ------- ------- ------- ------- ----- -----\n1  [1,2,3] [3,1,2] [1,2,3] [3,1,2] -0.5  -0.5 \n2  [4,5]   [3,1]   [4,5]   [3,1]   -1    -1   \n3  [6,7,8] [1,2,3] [6,7,8] [1,2,3] 1     1    \n4  [9,10]  [0,1]   [9,10]  [0,1]   1     1       \n*/\n```\n\n----------------------------------------\n\nTITLE: Calculating Cumulative Rank of Price per Symbol using Matrix Operations in DolphinDB Script\nDESCRIPTION: Computes the cumulative rank (`cumrank`) of prices directly on the price matrix (assuming `price` matrix is already created). The `cumrank(price)` function operates column-wise, assigning ranks based on the order of appearance within each symbol's price series.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/panel_data.md#_snippet_15\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\ncumrank(price);\n```\n\n----------------------------------------\n\nTITLE: Setting Up Distributed and Streaming Tables with Subscription Routing - DolphinDB Script\nDESCRIPTION: This snippet defines the setup of distributed and streaming tables in DolphinDB for aggregating and persisting third-party streaming data. It creates a partitioned distributed table and three memory-based stream tables with persistence. It ensures data from multiple sources is consolidated via an intermediary stream before writing to the distributed table, preventing write conflicts. Key dependencies: DolphinDB server with persistence enabled. Important parameters: column types, partitioning by date, handler functions for forwarding messages between tables. This approach is required when multiple sources write to overlapping partitions, as direct concurrent writing would fail.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_tutorial.md#_snippet_24\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin('admin','123456')\n// 定义表结构\nn=20000000\ncolNames =`Code`Date`DiffAskVol`DiffAskVolSum`DiffBidVol`DiffBidVolSum`FirstDerivedAskPrice`FirstDerivedAskVolume`FirstDerivedBidPrice`FirstDerivedBidVolume\ncolTypes = [SYMBOL,DATE,INT,INT,INT,INT,FLOAT,INT,FLOAT,INT]\n// 创建数据库与分布式表\ndbPath= \"dfs://ticks\"\nif(existsDatabase(dbPath))\n   dropDatabase(dbPath)\ndb=database(dbPath,VALUE, 2000.01.01..2030.12.31)\ndfsTB=db.createPartitionedTable(table(n:0, colNames, colTypes),`tick,`Date)\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 定义mem_tb_d表,并开启流数据持久化，将共享表命名为mem_stream_d\nmem_tb_d=streamTable(n:0, colNames, colTypes)\nenableTableShareAndPersistence(mem_tb_d,'mem_stream_d',false,true,n)\n\n// 定义mem_tb_f表,并开启流数据持久化，将共享表命名为mem_stream_f\nmem_tb_f=streamTable(n:0,colNames, colTypes)\nenableTableShareAndPersistence(mem_tb_f,'mem_stream_f',false,true,n)\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 定义ftb表，并开启流数据持久化，将共享表命名为ticks_stream\nftb=streamTable(n:0, colNames, colTypes)\nenableTableShareAndPersistence(ftb,'ticks_stream',false,true,n)\ngo\n\n// ticks_stream订阅mem_stream_d表的数据\ndef saveToTicksStreamd(mutable TB, msg): TB.append!(select * from msg)\nsubscribeTable(, 'mem_stream_d', 'action_to_ticksStream_tde', 0, saveToTicksStreamd{ticks_stream}, true, 100)\n\n// ticks_stream同时订阅mem_stream_f表的数据\ndef saveToTicksStreamf(mutable TB, msg): TB.append!(select * from msg)\nsubscribeTable(, 'mem_stream_f', 'action_to_ticksStream_tfe', 0, saveToTicksStreamf{ticks_stream}, true, 100)\n\n// dfsTB订阅ticks_stream表的数据\ndef saveToDFS(mutable TB, msg): TB.append!(select * from msg)\nsubscribeTable(, 'ticks_stream', 'action_to_dfsTB', 0, saveToDFS{dfsTB}, true, 100, 5)\n```\n\n----------------------------------------\n\nTITLE: Creating a Database for Replication\nDESCRIPTION: This DolphinDB script connects to the database as admin, creates a new database with a specified name and schema, and checks whether the database already exists, dropping if it does.  It sets the database for cluster replication. Requires admin privileges and the DolphinDB environment.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Asynchronous_Replication.md#_snippet_8\n\nLANGUAGE: dolphindb\nCODE:\n```\n// 创建存储的数据库\nlogin(\"admin\", \"123456\")\ndbName = \"dfs://testDB\"\nif(existsDatabase(dbName)){\n  dropDatabase(dbName)\n}\ndb = database(dbName, VALUE, 2023.01.01..2023.12.31)\n```\n\n----------------------------------------\n\nTITLE: Generating SQL Aggregation Metacode\nDESCRIPTION: Defines a function `createAggMetaCode` that takes a dictionary `aggDict` (mapping column names to lists of function names) and generates a list of `sqlCol` objects (metacode) for a SQL `select` statement. This allows for dynamic creation of aggregation expressions with descriptive aliases. It also returns a vector of the generated column names.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/metacode_derived_features/metacode_derived_features.txt#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef createAggMetaCode(aggDict){\n\tmetaCode = []\n\tmetaCodeColName = []\n\tfor(colName in aggDict.keys()){\n\t\tfor(funcName in aggDict[colName])\n\t\t{\n\t\t\tmetaCode.append!(sqlCol(colName, funcByName(funcName), colName + `_ + funcName$STRING))\n\t\t\tmetaCodeColName.append!(colName + `_ + funcName$STRING)\n\t\t}\n\t}\n\treturn metaCode, metaCodeColName$STRING\n}\n```\n\n----------------------------------------\n\nTITLE: Element-wise Calculations Between Two Array Vectors - DolphinDB Script\nDESCRIPTION: This code showcases how to perform element-wise mathematical operations (e.g., pow) between two Array Vectors or Columnar Tuples of the same size. Each subelement of the Array Vector at the same position is combined accordingly. Direct calculation between Fast Array Vector and Columnar Tuple is not supported in DolphinDB, enforcing type matching. Inputs are matched array-type variables and outputs are combined Array Vectors.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Array_Vector.md#_snippet_10\n\nLANGUAGE: DolphinDB\nCODE:\n```\nx = array(INT[], 0).append!([1 2 3, 4 5, 6 7 8, 9 10])\nxx = array(INT[], 0).append!([3 1 2, 3 1, 1 2 3, 0 1])\nz = pow(x, xx)\n/* z\n[[1,2,9],[64,5],[6,49,512],[1,10]]\n*/\n\ny = [1 2 3, 4 5, 6 7 8, 9 10].setColumnarTuple!()\nyy = [3 1 2, 3 1, 1 2 3, 0 1].setColumnarTuple!()\nz = pow(y, yy)\n/* z\n([1,2,9],[64,5],[6,49,512],[1,10])\n*/\nt = table(1 2 3 4 as id, x as x, xx as xx, y as y, yy as yy)\nnew_t = select *, pow(x, xx) as new_x, pow(y, yy) as new_y from t\n/* new_t\nid x       xx      y       yy      new_x      new_y     \n-- ------- ------- ------- ------- ---------- ----------\n1  [1,2,3] [3,1,2] [1,2,3] [3,1,2] [1,2,9]    [1,2,9]   \n2  [4,5]   [3,1]   [4,5]   [3,1]   [64,5]     [64,5]    \n3  [6,7,8] [1,2,3] [6,7,8] [1,2,3] [6,49,512] [6,49,512]\n4  [9,10]  [0,1]   [9,10]  [0,1]   [1,10]     [1,10]     \n*/\n```\n\n----------------------------------------\n\nTITLE: Writing Data (Single Thread) - Dolphindb\nDESCRIPTION: Defines a function to write data to a Dolphindb partitioned table using a single thread. It iterates through the specified number of days and appends data generated by `generate1DayData` for subsets of IDs to the target table in batches determined by `numIdPerPartition`.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/singleValueModeWrite.txt#_snippet_2\n\nLANGUAGE: Dolphindb\nCODE:\n```\ndef singleThreadWriting(id, startDay, days, freqPerDay, numIdPerPartition){\n\tt = loadTable(\"dfs://svmDemo\",\"sensors\")\n\tfor(d in 0:days){\n\t\tindex=0\n\t\tdo{\n\t\t\tt.append!(generate1DayData(startDay + d, id[index+0..(numIdPerPartition-1)], freqPerDay))\n\t\t\tindex += numIdPerPartition\n\t\t}while (index < size(id))\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: Ridge Regression Model Training\nDESCRIPTION: This snippet demonstrates training a lasso regression model using the `lasso` function. It uses a SQL data source, specifies the dependent variable `y` and independent variables `x0` and `x1`, and sets the regularization parameter `alpha` to 0.5. The trained model is stored in the `model` variable. Assumes the table `t` exists.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/machine_learning.md#_snippet_16\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nmodel = lasso(sqlDS(<select * from t>), `y, `x0`x1, alpha=0.5)\n```\n\n----------------------------------------\n\nTITLE: Trend Query in DolphinDB for Time Series Data\nDESCRIPTION: SQL query that retrieves time series data for specified devices and calculates statistics for each 2-minute interval. This demonstrates a common trend analysis pattern in IoT applications.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/iot_examples.md#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect first(value), last(value), max(value), min(value)\nfrom sensors \nwhere id in [1,2,51,52,101,102,151,152,201,202] and datetime between 2020.09.01T00:00:00 : 2020.09.07T23:59:59  \ngroup by id, bar(datetime,120)\n```\n\n----------------------------------------\n\nTITLE: Verifying Data After Time Transformation Import in DolphinDB\nDESCRIPTION: This script queries the first 5 rows of the table `tb1` (loaded using `loadTextEx` with a transformation function) from the specified database path. The purpose is to verify that the 'time' column now correctly stores data in the TIME data type as a result of the transformation applied during import.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_csv.md#_snippet_15\n\nLANGUAGE: dolphindb\nCODE:\n```\nselect top 5 * from loadTable(dbPath,`tb1);\n```\n\n----------------------------------------\n\nTITLE: Calculating Amount using Macro Variable Metaprogramming\nDESCRIPTION: This snippet uses macro variable metaprogramming to calculate amounts. It uses a macro variable to represent a range of column names and generate expressions of the form `priceK * qtyK as amountK` within a SELECT statement.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/macro_var_based_metaprogramming.md#_snippet_23\n\nLANGUAGE: DolphinDB\nCODE:\n```\npriceCols = \"price\" + string(1..50)\nqtyCols = \"qty\" + string(1..50)\namountCols=\"amount\"+string(1..50)\n<select _$$priceCols * _$$qtyCols as _$$amountCols from t>.eval()\n```\n\n----------------------------------------\n\nTITLE: 分布式列均值Map - C++\nDESCRIPTION: 这个C++代码片段定义了一个map函数 `columnAvgMap`，用于在DolphinDB分布式表中计算指定列的平均值。它接收一个`TableSP`和列名的集合作为输入。在每个分区内，它遍历指定的列，计算总和和非空元素的数量，并将结果作为包含总和和计数的元组返回，以便后续的reduce和final操作。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_development_tutorial.md#_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\nConstantSP columnAvgMap(Heap *heap, vector<ConstantSP> &args) {\n    TableSP table = args[0];\n    ConstantSP colNames = args[1];\n    double sum = 0.0;\n    int count = 0;\n\n    for (int i = 0; i < colNames->size(); i++) {\n        string colName = colNames->getString(i);\n        VectorSP col = table->getColumn(colName);\n        sum += col->sum()->getDouble();\n        count += col->count();\n    }\n\n    ConstantSP result = Util::createVector(DT_ANY, 2);\n    result->set(0, new Double(sum));\n    result->set(1, new Int(count));\n    return result;\n}\n```\n\n----------------------------------------\n\nTITLE: Synchronizing Databases Offline via Backup, rsync, and Restore - DolphinDB Script\nDESCRIPTION: This full script encapsulates the offline synchronization workflow by connecting remotely to a backup node, triggering a selective backup of data from DFS, running an rsync command over ssh to transfer backup files to a restore server, and restoring the backed up data on the local cluster. The function syncDataBases requires parameters defining backup node IP/port, backup directory, restore server IP, SSH user, and restore directory. It performs login authentication and executes DolphinDB remote procedures and shell commands. Users must prepare SSH keyless login and ensure empty directories exist for backup and restore paths. It efficiently automates offline sync with scheduling capability.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/data_synchronization_between_clusters.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef syncDataBases(backupNodeIP,backupNodePort,backupDir,restoreServerIP, userName,restoreDir){\n\tconn = xdb(backupNodeIP,backupNodePort)\n\tconn(login{`admin,`123456})\n\tconn(backup{backupDir,<select * from loadTable(\"dfs://db1\",\"mt\") where Timestamp > timestamp(date(now())) and Timestamp < now()>})\n\tcmd = \"rsync -av  \" + backupDir + \"/*  \" + userName + \"@\" + restoreServerIP + \":\" + restoreDir \n\tconn(shell{cmd})\n\trestore(restoreDir,\"dfs://db1\",\"mt\",\"%\",true,loadTable(\"dfs://db1\",\"mt\"))\n}\n\nlogin(`admin,`123456)\n//配置备份节点的 IP 地址，端口，以及备份机器上的目录（空目录）。\nbackupNodeIP = '115.239.209.234' \nbackupNodePort = 18846\nbackupDir = \"/home/myselfTest/backupDir\"\n//配置恢复数据节点的 IP 地址，由备份机器到恢复机器的 ssh 登录用户名（机器间应配置好 ssh 免密登录），以及恢复节点上的目录（空目录）。\nrestoreServerIP = '115.239.209.234'\nuserName = 'user1'\nrestoreDir = \"/home/myselfTest/backupDir\"\n```\n\n----------------------------------------\n\nTITLE: Loading and Creating Job History Table in DolphinDB\nDESCRIPTION: Loads job history data from CSV, defines the 'job_history' table with appropriate columns, and partitions it by EMPLOYEE_ID to optimize queries involving employee-based data. Loaded data is appended for further processing.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/Standard_SQL_in_DolphinDB/create_db_table_sql.txt#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\njob_history_tmp=loadText(dir+\"JOB_HISTORY.csv\")\ncreate table \"dfs://hr\".\"job_history\" (\n\tEMPLOYEE_ID INT,\n\tSTART_DATE DATE,\n\tEND_DATE DATE,\n\tJOB_ID SYMBOL,\n\tDEPARTMENT_ID INT\n)\npartitioned by EMPLOYEE_ID\n\njob_history = loadTable(\"dfs://hr\", \"job_history\")\njob_history.append!(job_history_tmp)\nselect * from job_history\n```\n\n----------------------------------------\n\nTITLE: Pushing Data to MQTT Server in DolphinDB\nDESCRIPTION: Loads data from a CSV file, formats it as JSON, and publishes it to an MQTT server in batches. This simulates sending sensor data to the MQTT server.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_streaming_application_in_IOT.md#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\n\tt=loadText(getHomeDir()+\"/deviceState.csv\")// 加载样本文件到内存，目录需根据实际情况修改\n\tf = createJsonFormatter()//json 格式打包函数\n\tbatchsize=100 // 每批打包 100 行记录发往 MQTT 服务器\n\tsubmitJob(\"submit_pub1\", \"submit_p1\", publishTableData{host,topic,f, batchsize,t})\n```\n\n----------------------------------------\n\nTITLE: Systemd Service Configuration for DolphinDB Agent\nDESCRIPTION: Systemd unit file for managing DolphinDB agent nodes, with parameters similar to the controller service. Also requires adjusting `WorkingDirectory` to match deployment path.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/multi_machine_cluster_deployment.md#_snippet_15\n\nLANGUAGE: Java\nCODE:\n```\n[Unit]\nDescription=ddbagent\nDocumentation=https://www.dolphindb.com/\n\n[Service]\nType=forking\nWorkingDirectory=/home/DolphinDB/server/clusterDemo\nExecStart=/bin/sh agent.sh start\nExecStop=/bin/sh agent.sh stop\nExecReload=/bin/sh agent.sh restart\nRestart=always\nRestartSec=10s\nLimitNOFILE=infinity\nLimitNPROC=infinity\nLimitCORE=infinity\n\n[Install]\nWantedBy=multi-user.target\n```\n\n----------------------------------------\n\nTITLE: Creating Non-Partitioned Table with Capacity and Schema\nDESCRIPTION: Shows how to create a non-partitioned in-memory table with a specified capacity, column names, and data types.  Demonstrates initializing an empty table and a table filled with default values.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/partitioned_in_memory_table.md#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntable(200:0, `name`id`value, [STRING,INT,DOUBLE])\n\nname id value\n---- -- -----\n\n\n\ntable(200:10, `name`id`value, [STRING,INT,DOUBLE])\n\nname id value\n---- -- -----\n     0  0\n     0  0\n     0  0\n     0  0\n     0  0\n     0  0\n     0  0\n     0  0\n     0  0\n     0  0\n\n\n```\n\n----------------------------------------\n\nTITLE: Filtering Trades by Stock Code - DolphinDB\nDESCRIPTION: This DolphinDB script filters the \"trades\" table by the \"ts_code\" column. It selects all records where the stock code is less than or equal to '002308.SZ'. The script uses `timer(10)` to measure the execution time over 10 iterations. `clearAllCache()` is used before each execution to avoid caching effects.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_16\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//根据股票代码过滤\ntimer(10) select * from trades where ts_code <= `002308.SZ\n```\n\n----------------------------------------\n\nTITLE: Complete Echarts Implementation with DolphinDB\nDESCRIPTION: Complete HTML page implementation showing how to integrate DolphinDB data with Echarts. Includes necessary script imports, DOM structure, and JavaScript code for data retrieval and visualization.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/web_chart_integration.md#_snippet_4\n\nLANGUAGE: HTML\nCODE:\n```\n<!DOCTYPE html>\n<html>\n<head>\n\t<meta charset=\"utf-8\">\n\t<script src=\"jquery-1.9.1.min.js\"></script>\n\t<script src=\"DBConnection.js\"></script>\n\t<script src=\"DolphinDBEntity.js\"></script>\n\t<script src=\"http://echarts.baidu.com/gallery/vendors/echarts/echarts-all-3.js\"></script>\n</head>\n<body>\n\t<div id=\"main\" style=\"width: 600px;height:400px;\"></div>\n\n\t<script type=\"text/javascript\">\n\t\tvar myChart = echarts.init(document.getElementById('main'));\n\t\tvar conn = new DolphinDBConnection('http://localhost:8848');\n\t\t//向DolphinDB发送查询脚本，并获取返回的数据\n\t\tconn.run(\"select avg(ec) as ec from iotTable group by second(time)\", function(re){\n\t\tif(re.resultCode&&re.resultCode==\"1\"){\n\t\t\talert(re.msg);\n\t\t} else {\n\t\t\tjobj = new DolphinDBEntity(re).toVector();//将返回结果转换成列数据\n\t\t\tvar time = jobj[0].value;\n\t\t\tvar ecdata = jobj[1].value;\n\t\t\tvar option = {\n\t\t\t\ttitle: {\n\t\t\t\t\ttext: '设备温度'\n\t\t\t\t},\n\t\t\t\txAxis: {\n\t\t\t\t\tdata: time\n\t\t\t\t},\n\t\t\t\tyAxis: {},\n\t\t\t\tseries: [{\n\t\t\t\t\tname: '温度',\n\t\t\t\t\ttype: 'line',\n\t\t\t\t\tdata: ecdata\n\t\t\t\t}]\n\t\t\t};\n\t\t\tmyChart.setOption(option);\n\t\t}\n\t});\n\t\t\n\t</script>\n</body>\n</html>\n```\n\n----------------------------------------\n\nTITLE: Memory Optimization for GUI Client Queries\nDESCRIPTION: Improved SQL query approach that stores results in a variable or limits the returned rows to reduce memory consumption in the GUI client.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/handling_oom.md#_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nt = select * from xx\nselect top 1000 * from xx\n```\n\n----------------------------------------\n\nTITLE: Generating Unique ID - Python\nDESCRIPTION: This Python code defines a function `uuid` to generate a unique identifier (UUID) of a specified length. It uses the process ID (`os.getpid()`) and random number generation (`random.randint()`) to create a unique string consisting of uppercase letters, lowercase letters, and numbers. The code is designed to create distinct names for replay tasks, preventing conflicts when multiple users perform replays simultaneously. The prerequisite is to have the `os` and `random` modules imported.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/appendices_market_replay_bp.md#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\ndef uuid(length):\n    str=\"\"\n    for i in range(length):\n        if(i % 3 == 0):\n            str += chr(ord('A') + random.randint(0, os.getpid() + 1) % 26)\n        elif(i % 3 == 1):\n            str += chr(ord('a') + random.randint(0, os.getpid() + 1) % 26)\n        else:\n            str += chr(ord('0') + random.randint(0, os.getpid() + 1) % 10)\n    return str\n\nuuidStr = uuid(16)\n```\n\n----------------------------------------\n\nTITLE: Iterative State Calculation with Null Handling Using genericStateIterate - DolphinDB\nDESCRIPTION: This snippet presents revised implementations of stateful factor calculation using genericStateIterate for effective null handling, supporting both window sizes of 2 and 1 (with future compatibility). It includes the definition of a process function and the main state function (iterateTestFunc), filling nulls using prior factor values and returning incremented results. The second version (for window=1) is future-facing. Dependencies are genericStateIterate and iif. Key parameters: tradePrice series and window. Outputs are robustly filled factors for each timestamp.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Streaming_computing_of_financial_quantifiers.md#_snippet_16\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 当前要求 window >= 2，所以回看上一个数据也需要 window=2\ndef processFunc(historyFactor, change){\n\tlastFactor = last(historyFactor)\n\tfactor = iif(change != NULL, change, lastFactor)\n\treturn factor+1\n}\n@state\ndef iterateTestFunc(tradePrice){\n\t// 计算交易价格涨跌幅\n\tchange = tradePrice \\ prev(tradePrice) - 1\n\t// 如果计算结果是空值，则用上一个因子值填充，返回 factor+1 作为最终因子值\n\tfactor = genericStateIterate(X=[change], initial=change, window=2, func=processFunc)\n\treturn factor\n}\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\n/*\ndef processFunc(lastFactor, change){\n\tfactor = iif(change != NULL, change, lastFactor)\n\treturn factor+1\n}\n@state\ndef iterateTestFunc(tradePrice){\n\t// 计算交易价格涨跌幅\n\tchange = tradePrice \\ prev(tradePrice) - 1\n\t// 如果计算结果是空值，则用上一个因子值填充，返回 factor+1 作为最终因子值\n\tfactor = genericStateIterate(X=[change], initial=change, window=1, func=processFunc)\n\treturn factor\n}\n*/\n```\n\n----------------------------------------\n\nTITLE: Stopping DolphinScheduler Standalone Server (Bash)\nDESCRIPTION: This command stops the DolphinScheduler standalone server process. It is recommended to run this command from the scheduler deployment directory before rebooting the server if DolphinScheduler is configured for auto-start, to prevent potential startup issues.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/dolphinscheduler_integration.md#_snippet_24\n\nLANGUAGE: Bash\nCODE:\n```\nbash ./bin/dolphinscheduler-daemon.sh stop standalone-server\n```\n\n----------------------------------------\n\nTITLE: Creating a Distributed Database for Historical Data Replay in DolphinDB\nDESCRIPTION: Prepares a distributed database for historical data replay. Creates a composite database with date and symbol partitions, defines the table schema, and imports historical data that will be used for replay functionality.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/quant_finance_examples.md#_snippet_20\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndbDate = database(\"\", VALUE, 2020.01.01..2020.12.31)\ndbSymbol=database(\"\", HASH, [SYMBOL, 10])\ndb = database(\"dfs://level2Replay\", COMPO, [dbDate, dbSymbol])\nmodal = table(1:0, `symbol`datetime`last`askPrice1`bidPrice1`askVolume1`bidVolume1`volume, [SYMBOL,DATETIME,DOUBLE,DOUBLE,DOUBLE,INT,INT,INT])\ndb.createPartitionedTable(modal,`quotes, `datetime`symbol)\ndata = select symbol, datetime(datetime(date(date))+second(time)) as datetime, last, askPrice1, bidPrice1, askVolume1, bidVolume1, curVol as volume from loadTable(\"dfs://level2\",\"quotes\")\nloadTable(\"dfs://level2Replay\",\"quotes\").append!(data)\n```\n\n----------------------------------------\n\nTITLE: Aligning indexedSeries Objects with Binary Operations in DolphinDB\nDESCRIPTION: This snippet demonstrates how to align and perform binary operations on two indexedSeries in DolphinDB. The system automatically aligns series by their labels (indexes) using an outer join before computation. Dependencies include DolphinDB's indexedSeries constructor and support for date indexes. Inputs are two indexedSeries defined over different overlapping date ranges; the operation aligns by the index and adds values where dates overlap. Output is an indexedSeries with missing values where there is no overlap. Both series must have strictly increasing indexes in older DolphinDB versions; from v1.30.20/2.00.8, further align functionality is available.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/panel_data.md#_snippet_18\n\nLANGUAGE: DolphinDB\nCODE:\n```\nindex1 = 2020.11.01..2020.11.06;\nvalue1 = 1..6;\ns1 = indexedSeries(index1, value1);\n\nindex2 = 2020.11.04..2020.11.09;\nvalue2 =4..9;\ns2 = indexedSeries(index2, value2);\n\ns1+s2;\n```\n\n----------------------------------------\n\nTITLE: Calculating Real-time Change Rate - DolphinDB\nDESCRIPTION: This code calculates the real-time change rate of a specified measurable indicator and records the instances where the change rate goes beyond a specified value. It uses the `deltas` function to calculate the difference between consecutive values of `propertyValue` for a given device and property, effectively computing the change in the indicator. It then calculates and formats the percentage change (rate) and filters for rates exceeding the specified threshold (1 in this case), effectively identifying significant fluctuations.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Iot_intelligent_O&M.md#_snippet_5\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\n//案例 5：根据可加指标实时计算变化率,超出指定值进行记录\n//数据准备\ndevice=\"361RP88\"      //设备编号\npoint=\"361RP88006\"    //测点编号，记录电流的测点，得到的 propertyValue 表示电流值\nt=select * from pt where deviceCode=device and propertyCode=point and ts between 2022.01.01 00:00:00 : 2022.01.01 01:59:59\n//统计变化率\nt = select *,propertyValue-deltas(propertyValue) as propertyValue_before,propertyValue as propertyValue_now from t  \nselect ts,deviceCode,logicalPositionId,physicalPositionId,propertyCode,propertyValue_before,propertyValue_now, string(format(abs(propertyValue_now-propertyValue_before)*100.0/propertyValue_before,\"0.00\"))+\"%\" as rate from t where abs(propertyValue_now-propertyValue_before)/propertyValue_before>1\n```\n\n----------------------------------------\n\nTITLE: Creating Stream Filter and Dispatch Engine in DolphinDB (Python)\nDESCRIPTION: This snippet creates a stream filter engine in DolphinDB to process heterogeneous data from the `messageStream`. It defines handler functions for 'trade' and 'snapshot' data types. The `appendLeftStream` function filters trade data and inserts it into the asof join engine's left table. The snapshot data is directly inserted into the asof join engine's right table. The `streamFilter` function handles deserialization based on the message schema.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/stock_market_replay.md#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\ndef appendLeftStream(msg){\n\ttempMsg = select * from msg where Price > 0 and Time>=09:30:00.000\n\tgetLeftStream(getStreamEngine(`tradeJoinSnapshot)).tableInsert(tempMsg)\n}\nfilter1 = dict(STRING,ANY)\nfilter1[\"condition\"] = \"trade\"\nfilter1[\"handler\"] = appendLeftStream\nfilter2 = dict(STRING,ANY)\nfilter2[\"condition\"] = \"snapshot\"\nfilter2[\"handler\"] = getRightStream(getStreamEngine(`tradeJoinSnapshot))\nschema = dict([\"trade\", \"snapshot\"], [tradeSchema, snapshotSchema])\nengine = streamFilter(name=\"streamFilter\", dummyTable=messageStream, filter=[filter1, filter2], msgSchema=schema)\n```\n\n----------------------------------------\n\nTITLE: Defining Benchmark Vector - DolphinDB\nDESCRIPTION: Defines a vector `benchX` used as the independent variable in the OLS regression. This vector represents the benchmark against which residuals are calculated.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/func_progr_cases.md#_snippet_22\n\nLANGUAGE: DolphinDB\nCODE:\n```\nbenchX = 10 15 7 8 9 1 2.0\n```\n\n----------------------------------------\n\nTITLE: Applying Functions Column-wise - DolphinDB\nDESCRIPTION: This code demonstrates applying a binary operator column-wise to two matrices using the `each` high-order function. The `each` function allows applying other functions, especially ones that do not support matrix operation. In this case, `**` is used for multiplication.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/matrix.md#_snippet_11\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//两个矩阵按列进行矩阵相乘运算\n>x=1..6$2:3;\n>y=6..1$2:3;\n>x;\n#0 #1 #2\n-- -- --\n1  3  5\n2  4  6\n\n>y;\n#0 #1 #2\n-- -- --\n6  4  2\n5  3  1\n\n>each(**, x, y);\n[16,24,16] //比如，24=3*4+4*3\n```\n\n----------------------------------------\n\nTITLE: DolphinDB Grouping by Date (Single Column)\nDESCRIPTION: This DolphinDB script aggregates data by date, calculating the sum of the 'VOL' column for each unique date. The results are grouped by the 'date' column. The `timer()` function measures performance over 10 runs. This tests how quickly the system can aggregate and group by date.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_43\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\n//按时间分组（单列）\ntimer(10) select sum(VOL) from trades group by date\n```\n\n----------------------------------------\n\nTITLE: Append Table Runnable Class in C++\nDESCRIPTION: Defines the `appendTable` class, derived from `Runnable`, which appends data to a DolphinDB table in a separate thread. The constructor copies the current session and sets the user and output for the new session. The `run` method periodically appends data from one table to another, or calls a function with the table as input. The class maintains a session to ensure permissions are correct. Requires Heap, TableSP, ConstantSP, and Client as inputs.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_advance.md#_snippet_24\n\nLANGUAGE: c++\nCODE:\n```\n    class Client;\n    class appendTable : public Runnable {\n    public:\n        appendTable(Heap *heap, TableSP table, ConstantSP handle, Client* client) \n        : heap_(heap), table_(table), handle_(handle), client_(client) {\n            session_ = heap->currentSession()->copy();//创建一个新的会话\n            session_->setUser(heap->currentSession()->getUser());//设置权限\n            session_->setOutput(new DummyOutput);//设置输出\n        }\n        ~appendTable() {}\n        void run() override {\n            while(true) {\n                Util::-sleep(1000);\n                if(handle_->isTable()) {\n                    TableSP result = handle_;\n                    std::vector<ConstantSP> dataToAppend = {result, table_};\n                    session_->getFunctionDef(\"append!\")->call(session_->getHeap().get(), dataToAppend);\n                }\n                else{\n                    std::vector<ConstantSP> dataToAppend = {table_};\n                    ((FunctionDefSP)handle_)->call(session_->getHeap().get(), dataToAppend);\n                }\n            }\n        }\n    private:\n        Heap* heap_;\n        SessionSP session_;\n        Client * client_;\n        ConstantSP handle_;\n        TableSP table_;\n    };\n```\n\n----------------------------------------\n\nTITLE: Calculating ln using moving average\nDESCRIPTION: This code calculates a new column 'ln' based on the 'bidPrice' column. It calculates the moving average of the 'bidPrice' over a window of size 3, then divides the current 'bidPrice' by the previous moving average, and finally takes the natural logarithm of the result. The function `prev` retrieves the value of previous row.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_28\n\nLANGUAGE: shell\nCODE:\n```\nt2 = select *, log(bidPrice / prev(moving(avg, bidPrice,3))) as ln from t\n```\n\n----------------------------------------\n\nTITLE: Creating and using a Field Series\nDESCRIPTION: The script constructs a field series using the expression `col000...col999` which selects multiple fields from the `t` table. This demonstrates a concise way to dynamically specify a range of column names for selection.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/macro_var_based_metaprogramming.md#_snippet_16\n\nLANGUAGE: DolphinDB\nCODE:\n```\ncols=\"col\" + lpad(string(0..999), 3, \"0\")\n<select _$$cols from t>\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\n<select col000...col999 from t>\n```\n\n----------------------------------------\n\nTITLE: Subscribing to Snapshot Stream (DolphinDB Script)\nDESCRIPTION: Subscribes the `aggrFeatures10min` time-series stream engine to the `snapshotStream` table. Incoming data batches from `snapshotStream` will be processed by the engine's defined metrics. `offset=-1` subscribes from the current end of the table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/metacode_derived_features/metacode_derived_features_streaming.txt#_snippet_13\n\nLANGUAGE: DolphinDB\nCODE:\n```\nsubscribeTable(tableName=\"snapshotStream\", actionName=\"aggrFeatures10min\", offset=-1, handler=getStreamEngine(\"aggrFeatures10min\"), msgAsTable=true, batchSize=2000, throttle=1, hash=0, reconnect=true)\n```\n\n----------------------------------------\n\nTITLE: Creating a table with sym, time, price, and volume columns\nDESCRIPTION: This code creates a table `t1` with columns `sym`, `time`, `price`, and `volume`. The sym column contains stock symbols. The time column represents trading times. The price column contains prices, and the volume column contains trading volumes.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_41\n\nLANGUAGE: shell\nCODE:\n```\nsyms=`600300`600400`600500$SYMBOL\nsym=syms[0 0 0 0 0 0 0 1 1 1 1 1 1 1 2 2 2 2 2 2 2]\ntime=09:40:00+1 30 65 90 130 185 195 10 40 90 140 160 190 200 5 45 80 140 170 190 210\nprice=172.12 170.32 172.25 172.55 175.1 174.85 174.5 36.45 36.15 36.3 35.9 36.5 37.15 36.9 40.1 40.2 40.25 40.15 40.1 40.05 39.95\nvolume=100 * 10 3 7 8 25 6 10 4 5 1 2 8 6 10 2 2 5 5 4 4 3\nt1=table(sym, time, price, volume);\nt1;\n```\n\n----------------------------------------\n\nTITLE: Accessing Elements in Array Vector Columns in Tables - DolphinDB Script\nDESCRIPTION: This snippet showcases how to extract elements from Array Vector columns within a DolphinDB table using subscript and range indexing within a SELECT statement. Out-of-bounds indices yield nulls or partial data per row. Prerequisites include a table structure with array columns, and result columns reflect the extracted or subarray data with possible nulls for missing indices.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Array_Vector.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\nx = array(INT[], 0).append!([1 2 3, 4 5, 6 7 8, 9 10])\ny = [1 2 3, 4 5, 6 7 8, 9 10].setColumnarTuple!()\nt = table(1 2 3 4 as id, x as x, y as y)\nnew_t = select *, x[2] as x_newCol1, x[1:3] as x_newCol2, y[2] as y_newCol1, y[1:3] as y_newCol2 from t\n/* new_t\nid x       y       x_newCol1 x_newCol2 y_newCol1 y_newCol2\n-- ------- ------- --------- --------- --------- ---------\n1  [1,2,3] [1,2,3] 3         [2,3]     3         [2,3]    \n2  [4,5]   [4,5]             [5,]                [5,]     \n3  [6,7,8] [6,7,8] 8         [7,8]     8         [7,8]    \n4  [9,10]  [9,10]            [10,]               [10,]    \n*/\n```\n\n----------------------------------------\n\nTITLE: Calculating Annual Kurtosis in DolphinDB\nDESCRIPTION: This function calculates the annualized kurtosis of a given value series. It takes a value series as input and returns the kurtosis. It calculates the kurtosis of the percentage change in the value series.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_factor.txt#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\n/**\n * 因子4：峰度\n峰度：指将净值峰度年化处理。计算方式为峰度* sqrt（N） 。 （日净值N=250，周净值N=52，月净值N=12）\n */\ndefg getAnnualKur(value){\n\treturn kurtosis(deltas(value)\\prev(value)) \n}\n```\n\n----------------------------------------\n\nTITLE: Modifying Controller Cluster Configuration in agent.cfg with sed\nDESCRIPTION: Appends additional controller nodes' IPs and ports to 'agent.cfg' file's 'sites' line, integrating new controllers into the cluster. Requires sed for in-place modification.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/service_deployment_and_migration.md#_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nsed -i '/^sites/s/$/,172.0.0.4:9912:controller4:controller,172.0.0.5:9913:controller5:controller/' agent.cfg\n```\n\n----------------------------------------\n\nTITLE: DolphinDB User and Permission Management Example 2\nDESCRIPTION: This example demonstrates how denying global TABLE_READ permission overrides a previously granted table-level TABLE_READ permission for a user in DolphinDB. It involves creating a user, a database, a partitioned table, granting table-level read access, denying global read access, and then verifying the user's lack of read access.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Permission_Management.md#_snippet_8\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(\"admin\", \"123456\")\ncreateUser(\"user2\",\"123456\")\ndbName = \"dfs://test\"\nif(existsDatabase(dbName)){\n        dropDatabase(dbName)\n}\nt = table(1..10 as id , rand(100, 10) as val)\ndb=database(dbName, VALUE, 1..10)\npt=  db.createPartitionedTable(t, \"pt\", \"id\")\npt.append!(t)\n\ngrant(\"user2\", TABLE_READ, dbName+\"/pt\")\ndeny(\"user2\", TABLE_READ, \"*\")\n\nlogin(\"user2\", \"123456\")\nselect * from loadTable(dbName, \"pt\")//user2 被禁止读 \"dfs://test\"\n```\n\n----------------------------------------\n\nTITLE: Creating OLAP Partitioned Database in DolphinDB\nDESCRIPTION: This code demonstrates how to create a partitioned database using the OLAP storage engine. It creates a composite database partitioned by date values and security ID hash values.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/sql_performance_optimization_wap_di_rv.md#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndbName = \"dfs://snapshot_SH_L2_OLAP\"\ndbTime = database(, VALUE, 2020.01.01..2020.12.31)\ndbSymbol = database(, HASH, [SYMBOL, 20])\ndb = database(dbName, COMPO, [dbTime, dbSymbol])\n```\n\n----------------------------------------\n\nTITLE: Backing Up a DolphinDB Table\nDESCRIPTION: Shows how to submit a background job to back up a specific table (`tbName`) within a DolphinDB database (`dbPath`) to a directory (`backupDir`) using the `backupTable` function. Similar to `backupDB`, it supports incremental backup features for the specified table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/backup-restore-new.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndbPath=\"dfs://testdb\"\ntbName=`quotes_2\nbackupDir=\"/home/$USER/backupTb\"\nsubmitJob(\"backupTable\",\"backup quotes_2 in testdb\",backupTable,backupDir,dbPath,tbName)\n```\n\n----------------------------------------\n\nTITLE: Calculating Average Holding Cost with JIT in DolphinDB Script\nDESCRIPTION: Defines the JIT-compiled function `holdingCost_JIT` to calculate the average holding cost for a series of trades. It uses the `@jit` decorator to accelerate the loop that iterates through price and amount vectors, updates holdings and cost, and computes the average cost. This implementation is functionally identical to `holdingCost_no_JIT` but offers significantly better performance due to JIT compilation, especially for large datasets.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/jit.md#_snippet_4\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\n@jit\ndef holdingCost_JIT(price, amount){\n\tholding = 0.0\n\tcost = 0.0\n\tavgPrice = 0.0\n\tn = size(price)\n\tavgPrices = array(DOUBLE, n, n, 0)\n\tfor (i in 0..n){\n\t\tholding += amount[i]\n\t\tif (amount[i] > 0){\n\t\t\tcost += amount[i] * price[i]\n\t\t\tavgPrice = cost/holding\n\t\t}\n\t\telse{\n\t\t\tcost += amount[i] * avgPrice\n\t\t}\n\t\tavgPrices[i]=avgPrice\n\t}\n\treturn avgPrices\n}\n```\n\n----------------------------------------\n\nTITLE: 通过 Grafana 进行可视化数据展示的示例脚本（DolphinDB 流式数据拼装与查询）\nDESCRIPTION: 示范如何在 Grafana 查询并显示功率谱密度和 RMS 相关指标。先计算信号的PSD并存入共享表psd，然后通过简单查询即可在Grafana中动态展示振动特征。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Random_Vibration_Signal_Analysis_Solution.md#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndata = select * from signal where source = `channel1  and timestamp >= 2023.01.29 03:54:21.652 and timestamp <= 2023.01.29 03:56:21.652\nnose_ = data[`signalnose]\ntemp_= nose_/sensitivity/gain * 9.8\ntemp_=temp_-mean(temp_)\nres_=pwelch(temp_, window, noverlap, nfft, fs)\nshare table(res_[1] as f, res_[0] as psdvalue) as psd\n//在Grafana中查询以下内容\n//select f, psdvalue from psd and f >= 0 and f <= 512\n```\n\n----------------------------------------\n\nTITLE: 创建Vector并添加数据\nDESCRIPTION: 演示如何创建整数类型的Vector，并通过四种不同的方法添加数据：添加单个数据点、通过原生类型添加、添加多个相同数据点和批量添加数据。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/c++api.md#_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\nVectorSP v = Util::createVector(DT_INT, 0); // 创建一个类型为int的空Vector\n\n// 添加单个数据点\nv->append(Util::createInt(1));        // v = [1]\n\n//添加单个数据点，也可以通过原生数据类型来实现\nint tmp = 1;\nv->appendInt(&tmp, 1);                // v = [1 1]\n\n// 一次性添加多个相同数据点\nv->append(Util::createInt(2), 2);     // v = [1 1 2 2]\n\n// 批量添加\nvector<int> v2{1, 2, 3, 4, 5};\nv->appendInt(v2.data(), v2.size());   // v = [1 1 2 2 1 2 3 4 5]\n```\n\n----------------------------------------\n\nTITLE: Configuring multi-table replay using JSON\nDESCRIPTION: This JSON configuration is designed for multi-table replay scenarios, defining parameters for both the left and right tables, including database and table names, device columns, date columns, and time ranges.  It also specifies the matching column for joining the tables, the replay rate, sampling rate, and the job name for the replay task.  The replayIoT function consumes this JSON to orchestrate the replay process.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/faultAnalysis.md#_snippet_3\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    \"leftTable\":{左表参数\n        \"dbName\":目标库，必选,\n        \"tbName\":目标表，必选,\n        \"deviceColumn\":设备名称列，必选,\n        \"dateColumn\":数据时间列，可选,\n        \"deviceId\":需要回放的设备，可选,\n        \"timeBegin\":回放开始时间，可选,\n        \"timeEnd\":回放结束时间，可选\n    },\n    \"rightTable\":{右表参数\n        \"dbName\":目标库，必选,\n        \"tbName\":目标表，必选,\n        \"deviceColumn\":设备名称列，必选,\n        \"dateColumn\":数据时间列，可选,\n        \"deviceId\":需要回放的设备，可选,\n        \"timeBegin\":回放开始时间，可选,\n        \"timeEnd\":回放结束时间，可选\n    },\n    \"matchColumn\": 左表右表连接列，必选,\n    \"replayRate\":回放倍率，可选,\n    \"sampleRate\":采样频率，可选,\n    \"jobName\":回放任务名称，必选\n}\n```\n\n----------------------------------------\n\nTITLE: String replacement (row-wise using each)\nDESCRIPTION: This DolphinDB script performs a string replacement operation on each element of the 'str' column in a table using the `each` function. It splits the string by '_', reverses the order, and concatenates them back with '_'. Demonstrates row-wise operation, less performant.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_18\n\nLANGUAGE: shell\nCODE:\n```\neach(x -> split(x, '_').reverse().concat('_'), t[`str])\n```\n\n----------------------------------------\n\nTITLE: Triggering Offline Synchronization Manually or with Scheduled Job - DolphinDB Script\nDESCRIPTION: This snippet shows how to invoke the previously defined syncDataBases function with configured parameters, either immediately or by scheduling it to run daily at 22:30. The scheduleJob function defines a named job with a time specification and date range, automating offline synchronization at a fixed daily time to keep data consistent across clusters. Parameters must match those used in syncDataBases. Relies on DolphinDB's internal scheduler and time format support.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/data_synchronization_between_clusters.md#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\nsyncDataBases(backupNodeIP=backupNodeIP,backupNodePort=backupNodePort,backupDir=backupDir,restoreServerIP=restoreServerIP, userName=userName,restoreDir=restoreDir)\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nscheduleJob(\"syncDB\",\"syncDB\",syncDataBases{backupNodeIP,backupNodePort,backupDir,restoreServerIP, userName,restoreDir},22:30m,2019.01.01,2030.12.31,'D')\n```\n\n----------------------------------------\n\nTITLE: Cosine Calculation on Matrix Elements - DolphinDB\nDESCRIPTION: This code demonstrates how to calculate the cosine of each element in a matrix. The cos() function, when applied to a matrix, operates element-wise.  The result is a new matrix with the cosines of the original elements.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/matrix.md#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\n>m=1..6$2:3;\n>m;\n#0 #1 #2\n-- -- --\n1  3  5\n2  4  6\n\n// 每个元素的余弦值\n>cos(m);\n#0        #1        #2\n--------- --------- --------\n0.540302  -0.989992 0.283662\n-0.416147 -0.653644 0.96017\n```\n\n----------------------------------------\n\nTITLE: Creating and Populating Partitioned MVCC In-Memory Tables in DolphinDB Script\nDESCRIPTION: This snippet creates MVCC (multi-version concurrency control) tables, partitions them, then inserts data for each partition, finally querying the unified partitioned table. The code ensures that insertions and reads are isolated across partitions. It requires MVCC table support and utilizes append! for data insertion. Each partition is handled by a separate MVCC table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/in_memory_table.md#_snippet_3\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nmt1=mvccTable(1:0,`id`val,[INT,INT])\nmt2=mvccTable(1:0,`id`val,[INT,INT])\nmt3=mvccTable(1:0,`id`val,[INT,INT])\ndb=database(\"\",RANGE,1 101 201 301)\npmt=db.createPartitionedTable([mt1,mt2,mt3],`pst,`id)\n\nmt1.append!(table(1..100 as id,rand(100,100) as val))\nmt2.append!(table(101..200 as id,rand(100,100) as val))\nmt3.append!(table(201..300 as id,rand(100,100) as val))\n\nselect * from pmt\n```\n\n----------------------------------------\n\nTITLE: Convert String to LocalDateTime in Java\nDESCRIPTION: This Java snippet defines a method `getTime` that converts a String representation of a datetime (in the format \"yyyy.MM.dd H:mm:ss\") to a LocalDateTime object. It then calculates the number of seconds since the epoch for the LocalDateTime and returns it as an integer.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/timezone.md#_snippet_5\n\nLANGUAGE: Java\nCODE:\n```\npublic static int getTime(String timeStr){\n    DateTimeFormatter df = DateTimeFormatter.ofPattern(\"yyyy.MM.dd H:mm:ss\");\n    LocalDateTime ldt = LocalDateTime.parse(timeStr,df);\n    return Utils.countSeconds(ldt);\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Table Schema in DolphinDB for Tick Data\nDESCRIPTION: DolphinDB script for creating a table with appropriate schema to store tick data migrated from OceanBase. The script creates a TSDB database with value-hash composite partitioning by date and symbol.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/OceanBase_to_DolphinDB.md#_snippet_0\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\ndef createTick(dbName, tbName){\n\tif(existsDatabase(dbName)){\n\t\tdropDatabase(dbName)\n\t}\n\tdb1 = database(, VALUE, 2020.01.01..2021.01.01)\n\tdb2 = database(, HASH, [SYMBOL, 10])\n\tdb = database(dbName, COMPO, [db1, db2], , \"TSDB\")\n\tdb = database(dbName)\n\tname = `SecurityID`TradeTime`TradePrice`TradeQty`TradeAmount`BuyNo`SellNo`ChannelNo`TradeIndex`TradeBSFlag`BizIndex\n\ttype = `SYMBOL`TIMESTAMP`DOUBLE`INT`DOUBLE`INT`INT`INT`INT`SYMBOL`INT\n\tschemaTable = table(1:0, name, type)\n\tdb.createPartitionedTable(table=schemaTable, tableName=tbName, partitionColumns=`TradeTime`SecurityID, compressMethods={TradeTime:\"delta\"}, sortColumns=`SecurityID`TradeTime, keepDuplicates=ALL)\n}\n\ndbName=\"dfs://TSDB_tick\"\ntbName=\"tick\"\ncreateTick(dbName, tbName)\n```\n\n----------------------------------------\n\nTITLE: Using moving High-Order Function for Sliding Window Interval Checks in DolphinDB\nDESCRIPTION: Defines a custom function 'rangeTest' that checks if at least 75% of the previous 20 'close' prices per row fall within an interval defined by 'DownAvgPrice' and 'UpAvgPrice'. Uses the 'moving' function with window size 21 (previous 20 + current) on relevant columns of table 't' to compute a boolean 'signal' column indicating this condition. Illustrates usage of DolphinDB's sliding window computations and vectorized 'between' function. Assumes table 't' contains necessary price fields and timestamps. Includes an example data creation snippet simulating financial data for testing.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/func_progr_cases.md#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndefg rangeTest(close, downlimit, uplimit){\n  size = close.size() - 1\n  return between(close.subarray(0:size), downlimit.last():uplimit.last()).sum() >= size*0.75\n}\n\nupdate t set signal = moving(rangeTest, [close, downAvgPrice, upAvgPrice], 21)\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nt=table(rand(\"d\"+string(1..n),n) as ts_code, nanotimestamp(2008.01.10+1..n) as trade_date, rand(n,n) as open, rand(n,n) as high, rand(n,n) as low, rand(n,n) as close, rand(n,n) as pre_close, rand(n,n) as change, rand(n,n) as pct_chg, rand(n,n) as vol, rand(n,n) as amount, rand(n,n) as downAvgPrice, rand(n,n) as upAvgPrice, rand(1 0,n) as singna)\n```\n\n----------------------------------------\n\nTITLE: Querying - Group by, Aggregation (Precision) DolphinDB\nDESCRIPTION: This snippet calculates the difference between the maximum and minimum `mem_used` values for each device, grouped by 5-minute intervals.  It uses `bar` to group the time data into 5-minute bins and demonstrates a common aggregation task of finding a range within groups.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs TimescaleDB/test_dolphindb_small.txt#_snippet_15\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 6. 精度查询：查询各设备在每 5 min 内的内存使用量最大、最小值之差\ntimer\nselect max(mem_used) - min(mem_used)\nfrom readings\ngroup by bar(time, 60 * 5)\n```\n\n----------------------------------------\n\nTITLE: Checking Linux System Resource Limits\nDESCRIPTION: Command to check the system-level resource limitations that might affect DolphinDB's memory usage.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/handling_oom.md#_snippet_7\n\nLANGUAGE: Bash\nCODE:\n```\nulimit -a\n```\n\n----------------------------------------\n\nTITLE: Configuring DolphinDB Controller Node via controller.cfg (config)\nDESCRIPTION: Sample settings for a DolphinDB controller's site address and distributed file system metadata location. The `dataSync=1` parameter is essential when cache engine is enabled in the cluster, ensuring proper data synchronization. Edit IPs and paths per controller machine in your cluster.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/deploy_dolphindb_on_new_server.md#_snippet_15\n\nLANGUAGE: config\nCODE:\n```\nlocalSite=175.178.100.3:8990:controller1\ndfsMetaDir=/ssd1/dolphindb/metaDir/dfsMeta/controller1\ndfsMetaDir=/ssd1/dolphindb/metaDir/dfsMeta/controller1\ndataSync=1\n...\n\n```\n\n----------------------------------------\n\nTITLE: Adding Controller Nodes to cluster.nodes File\nDESCRIPTION: Appends new controller node entries to 'cluster.nodes', specifying their IP addresses and roles, enabling the cluster to recognize additional nodes.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/service_deployment_and_migration.md#_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\necho 172.0.0.4:9912:controller4,controller >> cluster.nodes\necho 172.0.0.5:9913:controller5,controller >> cluster.nodes\n```\n\n----------------------------------------\n\nTITLE: Checking Java version\nDESCRIPTION: This command line instruction checks the installed Java version. It is used to verify that the installed Java version is compatible with DolphinDB GUI, which requires Java 8 64-bit or higher.  The expected output format includes the version number, runtime environment, and VM details.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/client_tool_tutorial.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\njava -version\n```\n\n----------------------------------------\n\nTITLE: Configuring Controller.cfg for Master Cluster\nDESCRIPTION: This snippet modifies the controller.cfg file in the master cluster to specify the number of slave clusters allowed.  It is used to set the upper limit of slave clusters to one. Prerequisites include access to the server and the DolphinDB installation directory.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Asynchronous_Replication.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nvim ./controller.cfg\n```\n\n----------------------------------------\n\nTITLE: Verifying Partition Count Post-Rebalancing in DolphinDB\nDESCRIPTION: Executes the `getAllChunks` function on all data nodes via `pnodeRun` to retrieve information about all partitions (chunks). It then uses SQL to count the total number of partitions, grouped by the data node alias (`site`). This command is used after a rebalancing operation to verify the final distribution of partitions across the data nodes.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/data_move_rebalance.md#_snippet_5\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nselect count(*) from pnodeRun(getAllChunks) group by site\n```\n\n----------------------------------------\n\nTITLE: Loading Partitioned Table and Querying Data in DolphinDB Compute Node (DolphinDB Script)\nDESCRIPTION: This DolphinDB script loads the metadata of the partitioned table 'testTB' from database 'dfs://testDB' without loading full data to achieve fast response times. It then performs two queries: 1) Counts the number of records grouped by date to check daily data volumes, useful for small result sets returned directly to the client; 2) Computes OHLC (Open, High, Low, Close) prices for each stock per day, storing the result in a server-side variable to avoid client overload and enabling paged result access via the web interface. This demonstrates querying and computation on a compute node in the cluster environment.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ha_cluster_deployment.md#_snippet_13\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 加载分区表对象\npt = loadTable(\"dfs://testDB\", \"testTB\")\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// SQL 返回数据量少的时候，可以直接取回客户端展示\nselect count(*) from pt group by date(DateTime) as Date\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// SQL 返回数据量较大时，可以赋值给变量，占用 server 端内存，客户端分页取回展示\nresult = select first(LastPx) as Open, max(LastPx) as High, min(LastPx) as Low, last(LastPx) as Close from pt group by date(DateTime) as Date, SecurityID\n```\n\n----------------------------------------\n\nTITLE: 筛选高性能基金并计算年度收益\nDESCRIPTION: 筛选满足特定指标条件的基金（如低风险、正收益），并提取最高夏普比率的50只基金，用于年度收益率分析。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/public_fund_basic_analysis.md#_snippet_23\n\nLANGUAGE: DolphinDB\nCODE:\n```\nfilterTB = select * from perf where exp<40, vol<40, sharpe>0 context by Type csort sharpe desc limit 50\nreturnsMatrix50 = returnsMatrix.loc(2015.01.01:, returnsMatrix.colNames() in filterTB[\"SecurityID\"])\nyearReturnsMatrix50 = transpose((returnsMatrix50 .setIndexedMatrix!()+1).resample(\"A\", prod)-1).nullFill(0)\n// 查看某类型基金的年度收益\nyearReturnsMatrix50.loc(fundTypeMap[yearReturnsMatrix50.rowNames()] == \"股票型\", )\n```\n\n----------------------------------------\n\nTITLE: Sliding, Cumulative Window Calculation with Reactive State Engine in DolphinDB\nDESCRIPTION: This code demonstrates the use of the `createReactiveStateEngine` to perform sliding, cumulative and time-based window calculations on streaming data. It calculates `msum(volume,2,1)`, `cumsum(volume)`, and `tmsum(time,volume,2m)` and outputs the results to `output2`. The engine subscribes to the `trades` stream table and applies the specified metrics.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/window_cal.md#_snippet_23\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//1.30.14，2.00.2以上版本支持tmsum函数。\nshare streamTable(1000:0, `time`sym`volume, [TIMESTAMP, SYMBOL, INT]) as trades\noutput2 = table(10000:0, `sym`time`Volume`msumVolume`cumsumVolume`tmsumVolume, [ SYMBOL,TIMESTAMP,INT, INT,INT,INT])\nreactiveState1= createReactiveStateEngine(name=\"reactiveState1\", metrics=[<time>,<Volume>,<msum(volume,2,1)>,<cumsum(volume)>,<tmsum(time,volume,2m)>], dummyTable=trades, outputTable=output2, keyColumn=\"sym\")\nsubscribeTable(tableName=\"trades\", actionName=\"reactiveState1\", offset=0, handler=append!{reactiveState1}, msgAsTable=true);\n\ninsert into trades values(2018.10.08T01:01:01.785,`A,10)\ninsert into trades values(2018.10.08T01:01:02.125,`B,26)\ninsert into trades values(2018.10.08T01:01:10.263,`B,14)\ninsert into trades values(2018.10.08T01:01:12.457,`A,28)\ninsert into trades values(2018.10.08T01:02:10.789,`A,15)\ninsert into trades values(2018.10.08T01:02:12.005,`B,9)\ninsert into trades values(2018.10.08T01:02:30.021,`A,10)\ninsert into trades values(2018.10.08T01:04:02.236,`A,29)\ninsert into trades values(2018.10.08T01:04:04.412,`B,32)\ninsert into trades values(2018.10.08T01:04:05.152,`B,23)\n\nsleep(10)\n\nselect * from output2\n\n//to drop the reactive state engine\n\ndropAggregator(`reactiveState1)\nunsubscribeTable(tableName=\"trades\", actionName=\"reactiveState1\")\nundef(\"trades\",SHARED)\n```\n\n----------------------------------------\n\nTITLE: Cholesky Decomposition in DolphinDB\nDESCRIPTION: Demonstrates Cholesky decomposition of a matrix in DolphinDB using the `cholesky` function. The function decomposes a symmetric positive-definite matrix X into L * L^T, where L is a lower triangular matrix. This shows using lower=true by default.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/matrix.md#_snippet_25\n\nLANGUAGE: DolphinDB\nCODE:\n```\n>m=[1, 0, 1, 0, 2, 0, 1, 0, 3]$3:3;\n>m;\n#0 #1 #2\n-- -- --\n1  0  1\n0  2  0\n1  0  3\n\n>L=cholesky(m);\n>L;\n#0 #1       #2      \n-- -------- --------\n1  0        0      \n0  1.414214 0      \n1  0        1.414214\n\n>L**transpose(L);\n#0 #1 #2\n-- -- --\n1  0  1\n0  2  0\n1  0  3\n\n```\n\n----------------------------------------\n\nTITLE: Training and Predicting with XGBoost in DolphinDB Script\nDESCRIPTION: This snippet defines parameters for the XGBoost model, trains the model using the `xgboost::train` function with training features (`train`) and target variable (`ytrain`), and then uses the trained model (`xgbModel`) to predict outcomes on a validation set (`valid`) using `xgboost::predict`. Requires the DolphinDB XGBoost plugin to be installed and loaded.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Forecast_of_Taxi_Trip_Duration.md#_snippet_0\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nxgb_pars = {'min_child_weight': 50, 'eta': 0.3, 'colsample_bytree': 0.3, 'max_depth': 10,\n            'subsample': 0.8, 'lambda': 1., 'nthread': 4, 'booster' : 'gbtree', 'silent': 1,\n            'eval_metric': 'rmse', 'objective': 'reg:linear', 'nthread': 48} // xgb 参数设置\nxgbModel = xgboost::train(ytrain, train, xgb_pars, 60) // 训练模型\nyvalid_ = xgboost::predict(xgbModel, valid) // 使用模型进行预测\n```\n\n----------------------------------------\n\nTITLE: Creating Reactive State Engine with ArrayVector and JIT Compression\nDESCRIPTION: Configures 'reactiveDemo4' for pressure calculations with combined arrayvector and JIT techniques, aiming at maximizing processing efficiency for streaming financial data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Streaming_computing_of_financial_quantifiers.md#_snippet_29\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// Drop existing engine if exists\ntry{ dropStreamEngine(\"reactiveDemo4\")} catch(ex){ print(ex) }\n// Define metrics with combined arrayvector + JIT\nmetrics4 = <[dateTime, averagePressArrayJIT(bidPrice, bidOrderQty, offerPrice, offerOrderQty, 60)]>\n// Create the reactive engine\nrse4 = createReactiveStateEngine(name=\"reactiveDemo4\", metrics=metrics4, dummyTable=inputTableArrayVector, outputTable=resultTable4, keyColumn=\"securityID\", keepOrder=true)\n```\n\n----------------------------------------\n\nTITLE: Testing the Price Sensitivity Function with a Stock Data CSV File\nDESCRIPTION: This code snippet reads a CSV file containing stock data into a pandas DataFrame, measures the execution time to compute the price sensitivity metric using the previously defined function, and prints the result along with the elapsed time. It assumes the CSV file path is valid and properly formatted.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/DolphinDB_Python_Parser_Intro_for_Quantitative_Finance/因子实现_Python版本/价格变动与一档量差的回归系数.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ndf = pd.read_csv(\"/ssd/ssd3/data/oneStock_oneFile_TL/20230201/snapshot/000001.csv\")\nt0 = time.time()\nres = priceSensitivityOrderFlowImbalance(df)\nprint(\"cal time: \", time.time() - t0, \"s\")\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Creating Non-Partitioned In-Memory Table with Mixed Inputs\nDESCRIPTION: Demonstrates creating a non-partitioned in-memory table with vectors, matrices, and tuples as inputs to the `table` function.  Ensures all inputs have compatible lengths.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/partitioned_in_memory_table.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\nx=1..6\ny=11..22$6:2\nz=(101..106, 201..206)\ntable(x,y,z)\n\nx C1 C2 C3  C4 \n- -- -- --- ---\n1 11 17 101 201\n2 12 18 102 202\n3 13 19 103 203\n4 14 20 104 204\n5 15 21 105 205\n6 16 22 106 206\n```\n\n----------------------------------------\n\nTITLE: Cache Eviction Triggered by Variable Allocation (OLAP) (DolphinDB Script)\nDESCRIPTION: First checks memory usage after caching several partitions (likely near `warningMemSize` from previous examples). Then, it attempts to allocate a large session variable (4GB). This demonstrates that if variable allocation requires more memory than available and cache usage is high, DolphinDB triggers LRU cache eviction to free up memory for the user request.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/memory_management.md#_snippet_11\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nmem().allocatedBytes - mem().freeBytes\n```\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nv = 1..1000000000\nmem().allocatedBytes - mem().freeBytes\n```\n\n----------------------------------------\n\nTITLE: Querying Writer Status and Handling Write Errors in DolphinDB C++ API\nDESCRIPTION: 本片段介绍如何利用 MultithreadedTableWriter::Status 查询当前写入状态并捕获错误。status.hasError() 用于判定是否发生写入异常，若有则输出详细错误信息。推荐用于写入任务后期的健康监测与自动恢复场景。需依赖 DolphinDB 的 C++ API 类定义。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_cpp_api_connector.md#_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\nMultithreadedTableWriter::Status status;\nwriter.getStatus(status);\nif (status.hasError()) {\n\tcout << \"error in writing: \" << status.errorInfo << endl;\n}\n```\n\n----------------------------------------\n\nTITLE: Inserting Data into Value-Partitioned Table with Invalid Characters\nDESCRIPTION: This code snippet demonstrates a failed data insertion into a value-partitioned table because the partition column contains special characters (spaces). It highlights a constraint in DolphinDB where value-partitioned columns must not contain certain special characters.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/database_and_table_creation_details.md#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\nn=1000\nID=take([\"1\",\"2\",\"3\"], n);\nx=rand(1.0, n);\nt=table(ID, x);\n//将 ID 列作为值分区\ndb=database(directory=\"dfs://valuedb\", partitionType=VALUE, partitionScheme=[\"1\"])\npt = db.createPartitionedTable(t, `pt, `ID);\npt.append!(t);\nn=1000\nID=take([\"1 12\",\"2.12\",\"3 \t 12\"], n);\nx=rand(1.0, n);\nt=table(ID, x);\npt.append!(t);\n```\n\n----------------------------------------\n\nTITLE: Inserting Rows into Tables with Array Vector Columns using tableInsert and append! - DolphinDB Script\nDESCRIPTION: This code demonstrates how to use tableInsert and append! to add rows to a DolphinDB table with Array Vector and Columnar Tuple columns. Both scalar and array rows can be inserted, and appending another table is also supported. Prerequisites are correct table and column initialization, and results are extended table objects reflecting newly inserted rows and columns.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Array_Vector.md#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\nx = array(INT[], 0).append!([1 2 3, 4 5, 6 7 8, 9 10])\ny = [1 2 3, 4 5, 6 7 8, 9 10].setColumnarTuple!()\nt = table(1 2 3 4 as id, x as col1, y as col2)\nt.tableInsert(5, 11, 11)\nt.tableInsert(6, [12 13 14], [12 13 14])\nt.append!(table(7 8 as id, [15 16, 17] as col1, [15 16, 17] as col2))\n/* t\nid col1       col2      \n-- ---------- ----------\n1  [1,2,3]    [1,2,3]   \n2  [4,5]      [4,5]     \n3  [6,7,8]    [6,7,8]   \n4  [9,10]     [9,10]    \n5  [11]       11        \n6  [12,13,14] [12,13,14]\n7  [15,16]    [15,16]   \n8  [17]       17         \n*/\n```\n\n----------------------------------------\n\nTITLE: Configuring DolphinDB Streaming, Raft HA, and Network\nDESCRIPTION: Sets various DolphinDB configuration parameters. `streamingHADir` (commented out) specifies the directory for streaming Raft logs. `streamingHAMode` (commented out) enables HA for streaming using 'raft'. `streamingRaftGroups` (commented out) defines Raft groups with node aliases. `lanCluster=0` actively configures the cluster to use TCP for heartbeats (lanCluster=false), appropriate for non-LAN or cloud deployments, overriding the default value of true (UDP).\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/ha_cluster_deployment/P2/config/config-specification.txt#_snippet_1\n\nLANGUAGE: Configuration\nCODE:\n```\n# The directory to keep streaming Raft log files. The default value is <HOME>/log/streamLog. Each data node should be configured with different streamingHADir.\n#streamingHADir=/home/DolphinDB/Data/NODE1/log/streamLog\n#\n# Enable high-availability for streaming.\n#streamingHAMode=raft\n#\n# nformation about Raft groups. Each Raft group is represented by group ID and aliases of data nodes in the group, separated with colon (:). \n# Raft group ID must be an integer greater than 1. Each Raft group has at least 3 data nodes. Use comma (,) to seperate multiple Raft groups.\n#streamingRaftGroups=2:NODE1:NODE2:NODE3,3:NODE3:NODE4:NODE5\n#\n# Whether the cluster is within a LAN (local area network). lanCluster=true: use UDP for heartbeats; lanCluster=false: use TCP for heartbeats. Set lanCluster=false if the system is deployed in the cloud.The default value is true.\nlanCluster=0\n```\n\n----------------------------------------\n\nTITLE: Executing the main Function\nDESCRIPTION: This line calls the `main` function, initiating the data ingestion and processing pipeline defined within it.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/data/DolphinDB_streaming_application_in_IOT/deviceState.txt#_snippet_10\n\nLANGUAGE: DolphinDB\nCODE:\n```\nmain()\n```\n\n----------------------------------------\n\nTITLE: Defining and Executing Online Synchronization with Socket Connection - DolphinDB Script\nDESCRIPTION: This script implements an online synchronization method where data is read from a source cluster's DFS table and appended directly to a remote cluster's table in memory via live socket connection. It defines a writeData function that appends data chunks to a remote table, connects to the remote server via xdb, authenticates, and pushes today's data filtered by Timestamp. The assumptions include sufficient memory at the restore site to hold the entire day's data and the availability of proper access credentials. Outputs are immediate data append operations with no intermediate file storage.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/data_synchronization_between_clusters.md#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef writeData(dbName,tableName,t) : loadTable(dbName,tableName).append!(t)\ndef synDataBaseOnline(restoreServerIP,restoreServerPort,writeData=writeData){\n\tt = select * from loadTable(\"dfs://db1\",\"mt\") where Timestamp > timestamp(date(now())) and Timestamp < now()\n\tconn = xdb(restoreServerIP,restoreServerPort)\n\tconn(login{`admin,`123456})\n\tconn(writeData{\"dfs://db1\",\"mt\",t})\n}\nlogin(`admin,`123456)\nrestoreServerIP = '115.239.209.234'\nrestoreServerPort = 18848\nsynDataBaseOnline(restoreServerIP=restoreServerIP,restoreServerPort=restoreServerPort)\n```\n\n----------------------------------------\n\nTITLE: Generating Simulated Trade Data with Various Order Types in DolphinDB\nDESCRIPTION: This code snippet creates a simulated transaction table 't' comprising 1,000,000 rows, with random dates, times, symbols, volumes, prices, and trading sides. The generated data enables performance and correctness testing for grouping and aggregation scenarios discussed further in the document. Key parameters include the number of rows N and controlled sampling for each column, ensuring diversity across trading dates, stock symbols, order sizes, and trade sides.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_sql_cases.md#_snippet_38\n\nLANGUAGE: DolphinDB\nCODE:\n```\nN = 1000000\nt = table(take(2021.11.01..2021.11.15, N) as date, \n          take([09:30:00, 09:35:00, 09:40:00, 09:45:00, 09:47:00, 09:49:00, 09:50:00, 09:55:00, 09:56:00, 10:00:00], N) as time, \n          take(`AAPL`FB`MSFT$SYMBOL, N) as symbol, \n          take([10000, 30000, 50000, 80000, 100000], N) as volume, \n          rand(100.0, N) as price, \n          take(`BUY`SELL$SYMBOL, N) as side)\n```\n\n----------------------------------------\n\nTITLE: Data Import Script - DolphinDB\nDESCRIPTION: This DolphinDB script imports data from a CSV file (`TAQ20070801.csv`) into a partitioned table in DolphinDB.  It creates a database, loads data, creates partitions based on a symbol, and submits jobs to load data into the table. Dependencies: DolphinDB installation, CSV file in the specified path.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_Druid_appendix.md#_snippet_11\n\nLANGUAGE: DolphinDB\nCODE:\n```\nif (existsDatabase(\"dfs://TAQ\"))\ndropDatabase(\"dfs://TAQ\")\n\ndb = database(\"/Druid/table\", SEQ, 4)\nt=loadTextEx(db, 'table', ,\"/data/data/TAQ/TAQ20070801.csv\")\nt=select count(*) as ct from t group by symbol\nbuckets = cutPoints(exec symbol from t, 128)\nbuckets[size(buckets)-1]=`ZZZZZ\nt1=table(buckets as bucket)\nt1.saveText(\"/data/data/TAQ/buckets.txt\")\n\ndb1 = database(\"\", VALUE, 2007.08.01..2007.09.01)\npartition = loadText(\"/data/data/buckets.txt\")\npartitions = exec * from partition\ndb2 = database(\"\", RANGE, partitions)\ndb = database(\"dfs://TAQ\", HIER, [db1, db2])\ndb.createPartitionedTable(table(100:0, `symbol`date`time`bid`ofr`bidsiz`ofrsiz`mode`ex`mmid, [SYMBOL, DATE, SECOND, DOUBLE, DOUBLE, INT, INT, INT, CHAR, SYMBOL]), `quotes, `date`symbol)\n\ndef loadJob() {\nfilenames = exec filename from files('/data/data/TAQ')\ndb = database(\"dfs://TAQ\")\nfiledir = '/data/data/TAQ'\nfor(fname in filenames){\njobId = fname.strReplace(\".csv\", \"\")\njobName = jobId \nsubmitJob(jobId,jobName, loadTextEx{db, \"quotes\", `date`symbol,filedir+'/'+fname})\n}\n}\nloadJob()\nselect * from getRecentJobs()\nTAQ = loadTable(\"dfs://TAQ\",\"quotes\");\n```\n\n----------------------------------------\n\nTITLE: Backing Up Data in DolphinDB Using backup Function - DolphindDB Script\nDESCRIPTION: This snippet demonstrates how to use DolphinDB's backup function to save selected data from a DFS database to a directory on the disk. It includes examples for backing up entire tables, recent time-partitioned data, and selective columns. The backupDir variable specifies the disk directory to store backups. The code relies on SQL-like querying within meta code brackets to specify the data to backup, supporting filtering on partition columns such as TradingDay. Users must have write permissions to the backup directory. Output consists of physical backup files that can be transferred and later restored.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/data_synchronization_between_clusters.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\nbackupDir = \"/hdd/hdd1/backDir\"\nbackup(backupDir,<select * from loadTable(\"dfs://db1\",\"mt\")>)\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nbackupDir = \"/hdd/hdd1/backDir\"\t\nbackup(backupDir,<select * from loadTable(\"dfs://db1\",\"mt\") where TradingDay > date(now()) - 7 and  TradingDay <= date(now())>)\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nbackupDir = \"/hdd/hdd1/backDir\"\nbackup(backupDir,<select col1,col2,col3 from loadTable(\"dfs://db1\",\"mt\")>)\n```\n\n----------------------------------------\n\nTITLE: Loading Daily CSV Files into DolphinDB Table\nDESCRIPTION: Defines `loadOneDayFiles` to load all CSV files from a specific directory (`path`) into a DolphinDB distributed table specified by `dbName` and `tableName`, using the provided `schema1`. It lists all '.csv' files in the directory, processes them in batches of 100 using `cut`, loads each file via `loadOneFile`, appends data to a temporary in-memory table (`bigTable`), and includes error handling (`try`/`catch`) for individual file loading failures. Finally, it appends the entire batch's data to the target distributed table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/csvImportDemo/importNewData.txt#_snippet_2\n\nLANGUAGE: dolphindb\nCODE:\n```\ndef loadOneDayFiles(dbName,tableName,path,schema1){\n\ttb = loadTable(dbName,tableName)\n\tfileList = exec filename from files(path, \"%.csv\")\n\tfs= fileList.cut(100)\n\tfor(i in 0:fs.size()){\n\t\tbigTable=table(500000:0,tb.schema().colDefs[`name],tb.schema().colDefs[`typeString])\n\t\tfor(f in fs[i])\t{\n\t\t\ttry\t{\n\t\t\t\tbigTable.append!(loadOneFile(path+\"/\"+f,bigTable,schema1))\n\t\t\t}\n\t\t\tcatch(ex){\n\t\t\t\tprint f + \": \"+ex\n\t\t\t}\n\n\t\t}\n\t\ttb.append!(bigTable)\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Large Random Stock Tick Data and Pivot Table Creation in DolphinDB - DolphinDB\nDESCRIPTION: Simulates 10 million stock tick data points generating random stock symbols, times, and prices to mimic high-frequency trading data. Uses 'rand' function to sample symbols and times, and to produce corresponding price data. Then uses the 'pivot' higher-order function to aggregate average prices by minute and symbol, resulting in a price pivot matrix where each column is a stock and each row corresponds to a time interval. Demonstrates data simulation and pivot aggregation for time-series financial data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/hybrid_programming_paradigms.md#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\nn=10000000\nsyms = rand(`FB`GOOG`MSFT`AMZN`IBM, n)\ntime = 09:30:00.000 + rand(21600000, n)\nprice = 500.0 + rand(500.0, n)\n\npriceMatrix = pivot(avg, price, time.minute(), syms)\n```\n\n----------------------------------------\n\nTITLE: DB_OWNER Granting Permissions to Created Databases - DolphinDB\nDESCRIPTION: This code demonstrates how a user with DB_OWNER permission can grant permissions to other users on databases they create. Requires admin privileges to create user.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Permission_Management.md#_snippet_26\n\nLANGUAGE: DolphinDB\nCODE:\n```\ncreateUser(`CliffLee, \"GH456$%\" )\ncreateUser(`MitchTrubisky, \"JI3564^\")\ngrant(`MitchTrubisky,DB_OWNER);\nlogin(`MitchTrubisky, \"JI3564^\");\ndb = database(\"dfs://dbMT\", VALUE, 1..10)\nt=table(1..1000 as id, rand(100, 1000) as x)\ndt = db.createTable(t, \"dt\").append!(t)\ngrant(`CliffLee, TABLE_READ, \"dfs://dbMT/dt\");\n```\n\n----------------------------------------\n\nTITLE: QR Decomposition with r mode in DolphinDB\nDESCRIPTION: Demonstrates QR decomposition of a matrix in DolphinDB using the `qr` function with `mode='r'` and `pivoting=false`.  The function returns the R matrix resulting from the QR decomposition with full mode.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/matrix.md#_snippet_18\n\nLANGUAGE: DolphinDB\nCODE:\n```\n>m=matrix([2 5 5, 5 5 4, 8 6 4, 7 6 8]);\n>m;\n#0 #1 #2 #3\n-- -- -- --\n2  5  8  7 \n5  5  6  6 \n5  4  4  8 \n\n>r=qr(m,mode='r',pivoting=false);\n>r;\n#0        #1        #2        #3        \n--------- --------- --------- ----------\n-7.348469 -7.484552 -8.981462 -11.430952\n0         3.159348  5.943561  3.622407  \n0         0         -0.086146 2.282872\n\n```\n\n----------------------------------------\n\nTITLE: Loading and Creating Employees Table in DolphinDB\nDESCRIPTION: Loads employee data from CSV, defines the 'employees' table with detailed employee information, and appends the data for HR analysis purposes.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/Standard_SQL_in_DolphinDB/create_db_table_sql.txt#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\nemployees_tmp=loadText(dir+\"EMPLOYEES.csv\")\ncreate table \"dfs://hr\".\"employees\" (\n\tEMPLOYEE_ID INT,\n\tFIRST_NAME STRING,\n\tLAST_NAME STRING,\n\tEMAIL STRING,\n\tPHONE_NUMBER STRING,\n\tHIRE_DATE DATE,\n\tJOB_ID SYMBOL,\n\tSALARY INT,\n\tCOMMISSION_PCT DOUBLE,\n\tMANAGER_ID INT,\n\tDEPARTMENT_ID INT\n)\nemployees = loadTable(\"dfs://hr\", \"employees\")\nemployees.append!(employees_tmp)\nselect * from employees\n```\n\n----------------------------------------\n\nTITLE: Stock Factor Alignment (Optimized)\nDESCRIPTION: This snippet presents an optimized way to align stock factors. It first retrieves the list of securities from dimension table. It then executes factor values from the two tables using the filtered list of securities using the \"in\" keyword. Last, it pivots by date and SecurityID, and merges the two result set.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_sql_cases.md#_snippet_37\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntimer {\n sec = exec SecurityID from loadTable(\"dfs://infodb\", \"MdSecurity\") where substr(SecurityID, 0, 3) in [\"001\", \"003\", \"005\", \"007\"]\n schema(loadTable(\"dfs://Factor10MinSH\", \"Factor10MinSH\"))\n nt1 = exec FactorValue from loadTable(\"dfs://Factor10MinSH\", \"Factor10MinSH\") where Date between 2019.01.01 : 2020.10.31, SecurityID in sec, FactorID = \"Factor01\" pivot by concatDateTime(Date, Time), SecurityID\n nt2 = exec FactorValue from loadTable(\"dfs://Factor10MinSZ\", \"Factor10MinSZ\") where Date between 2019.01.01 : 2020.10.31, SecurityID in sec, FactorID = \"Factor01\" pivot by concatDateTime(Date, Time), SecurityID\n\n res3 = merge(nt1.setIndexedMatrix!(), nt2.setIndexedMatrix!(), 'left')\n}\n```\n\n----------------------------------------\n\nTITLE: Executing data query from Redshift in DolphinDB script\nDESCRIPTION: Sample DolphinDB script to perform SQL query on Redshift via ODBC connection and store data into a DolphinDB partitioned table for migration.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Migrate_data_from_Redshift_to_DolphinDB.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\nodbc::query(conn, \"select * from trades_sz\", pt)\n```\n\n----------------------------------------\n\nTITLE: Creating Agent Startup Script for DolphinDB Cluster (Shell)\nDESCRIPTION: This shell script manages the launch and termination of a DolphinDB cluster agent node, similarly supporting start, stop, and restart via command-line arguments. It initializes environment variables and background execution for the agent process. The agent expects access to agent.cfg and other required resources in prescribed locations. Inputs: start|stop|restart. Outputs: Starts/stops/restarts the agent process; suppresses output to log files. Requires appropriate directory permissions. Similar limitations as the controller script.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ha_cluster_deployment.md#_snippet_15\n\nLANGUAGE: Shell\nCODE:\n```\n#!/bin/bash\n#agent.sh\n\nworkDir=$PWD\n\nstart(){\n    cd ${workDir} && export LD_LIBRARY_PATH=$(dirname \"$workDir\"):$LD_LIBRARY_PATH\n    nohup ./../dolphindb -console 0 -mode agent -home data -script dolphindb.dos -config config/agent.cfg -logFile log/agent.log  > agent.nohup 2>&1 &\n}\n\nstop(){\n    ps -o ruser=userForLongName -e -o pid,ppid,c,time,cmd |grep dolphindb|grep -v grep|grep $USER|grep agent| awk '{print $2}'| xargs kill -TERM\n}\n\ncase $1 in\n    start)\n        start\n        ;;\n    stop)\n        stop\n        ;;\n    restart)\n        stop\n        start\n        ;;\nesac\n```\n\n----------------------------------------\n\nTITLE: Defining DolphinDB Plugin Functions for MyData Format\nDESCRIPTION: Configuration that maps DolphinDB functions to implementations in the libPluginLoadMyData.so shared library. Defines functions for schema extraction, data loading, and data source creation with their respective parameters.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin/LoadMyData/PluginLoadMyData.txt#_snippet_0\n\nLANGUAGE: dolphindb\nCODE:\n```\nloadMyData,libPluginLoadMyData.so\nextractMyDataSchema,extractMyDataSchema,operator,0,0,0\nloadMyData,loadMyData,system,1,3,0\nloadMyDataEx,loadMyDataEx,system,4,6,0\nmyDataDS,myDataDS,1,3,0\n```\n\n----------------------------------------\n\nTITLE: 创建模拟撮合引擎\nDESCRIPTION: 使用MatchingEngineSimulator::createMatchEngine函数创建一个模拟撮合引擎，配置引擎参数、输入表结构和输出表。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/matching_engine_simulator.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\nengine = MatchingEngineSimulator::createMatchEngine(name, exchange, config, dummyQuotationTable, quotationColMap, dummyUserOrderTable, userOrderColMap, tradeOutputTable, , snapshotOutputTable)\n```\n\n----------------------------------------\n\nTITLE: Defining process Function\nDESCRIPTION: This function appends incoming messages (`msg`) to both reactive (`engine1`) and session window (`engine2`) engines. It assumes that `engine1` and `engine2` are already initialized and that the `msg` is compatible with their expected input format.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/data/DolphinDB_streaming_application_in_IOT/deviceState.txt#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef process(mutable  engine1,mutable  engine2,msg){\n\tengine1.append!(msg) \n\tengine2.append!(msg)\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring DolphinDB Sink Connector for Kafka\nDESCRIPTION: JSON configuration for the JDBC Sink Connector that connects Kafka to DolphinDB. Specifies connection details, topics to synchronize, and transformation settings for the data synchronization process.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Debezium_and_Kafka_data_sync.md#_snippet_21\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    \"name\": \"ddb-sink\",\n    \"config\": {\n        \"connector.class\": \"io.confluent.connect.jdbc.JdbcSinkConnector\",\n        \"tasks.max\": \"2\",\n        \"topics\": \"mysqlserver.basicinfo.index_components,mysqlserver.basicinfo.stock_basic\",\n        \"connection.url\": \"jdbc:dolphindb://192.168.189.130:8848?user=admin&password=123456\",\n        \"transforms\": \"unwrap\",\n        \"transforms.unwrap.type\": \"io.debezium.transforms.ExtractNewRecordState\",\n        \"transforms.unwrap.drop.tombstones\": \"false\",\n        \"auto.evolve\": \"false\",\n        \"insert.mode\": \"insert\",\n        \"delete.enabled\": \"true\",\n        \"batch.size\":\"10000\",\n        \"pk.mode\": \"record_key\",\n        \"ddbsync.config.table\":\"dfs://ddb_sync_config,sync_config\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Alternative JVM Memory Settings for DataX Execution\nDESCRIPTION: Provides an alternative command to set JVM heap size for DataX, enhancing memory allocation for large data transfers. Dependencies include a Linux environment with Python and DataX. Inputs are JVM options and config file path; output is initiation of the data migration process.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Oracle_to_DolphinDB.md#_snippet_17\n\nLANGUAGE: Shell\nCODE:\n```\npython datax.py --jvm=\"-Xms1g -Xmx8g\" ../../datax-writer-master/ddb_script/oracleddb.json\n```\n\n----------------------------------------\n\nTITLE: Checking License Expiry\nDESCRIPTION: This code snippet retrieves and displays the license expiration date using the 'ops' namespace functions.  This is done before and after license updates to verify expiry and the success of the license renewal process.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/single_machine_cluster_deploy.md#_snippet_17\n\nLANGUAGE: DolphinDB\nCODE:\n```\nuse ops\ngetAllLicenses()\n```\n\n----------------------------------------\n\nTITLE: Searching for ICU Library Packages\nDESCRIPTION: These commands use the `yum search` (for CentOS/RHEL) and `apt-cache search` (for Debian/Ubuntu) package managers to search for packages related to the ICU (International Components for Unicode) library. This is helpful when a missing ICU library dependency is identified.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ODBC_plugin_user_guide.md#_snippet_6\n\nLANGUAGE: Shell\nCODE:\n```\nyum search icu #CentOS 搜索 icu 库\n```\n\nLANGUAGE: Shell\nCODE:\n```\napt-cache search # libicu 搜索 icu 库\n```\n\n----------------------------------------\n\nTITLE: Creating a Partitioned Table\nDESCRIPTION: This DolphinDB script creates a partitioned table within a database. It defines the column names and types, then creates a table schema. Finally, it creates the partitioned table using the `createPartitionedTable` function with partition columns `DateTime`. Prerequisites include an existing database connection.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Asynchronous_Replication.md#_snippet_12\n\nLANGUAGE: dolphindb\nCODE:\n```\n// 创建分区表\ntbName = \"testTB\"\ncolNames = `SecurityID`DateTime`PreClosePx`OpenPx`HighPx`LowPx`LastPx`Volume`Amount\ncolTypes = [SYMBOL, DATETIME, DOUBLE, DOUBLE, DOUBLE, DOUBLE, DOUBLE, INT, DOUBLE]\nschemaTable = table(1:0, colNames, colTypes)\ndb.createPartitionedTable(table=schemaTable, tableName=tbName, partitionColumns=`DateTime)\n```\n\n----------------------------------------\n\nTITLE: Querying Recent Slave Replication Info\nDESCRIPTION: This DolphinDB script queries for the recent replication information from the slaves by using the `getRecentSlaveReplicationInfo` function through an rpc call. This function returns details of the latest task statuses from the slave clusters connected to the master cluster. A DolphinDB master cluster must be set up for this. This requires that the cluster is already running with the replication configured and enabled.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Asynchronous_Replication.md#_snippet_18\n\nLANGUAGE: dolphindb\nCODE:\n```\nrpc(getControllerAlias(), getRecentSlaveReplicationInfo)\n```\n\n----------------------------------------\n\nTITLE: Starting DolphinDB Cluster Nodes Using Shell Scripts\nDESCRIPTION: This snippet provides shell commands to start controller and agent nodes on servers P1, P2, and P3. It first sets execution permissions for the dolphindb binary, then runs startController.sh and startagent.sh scripts within the clusterDemo directory. Success can be verified by inspecting running dolphindb processes using ps and grep. Each server runs a control node and an agent node as per deployment requirements.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ha_cluster_deployment.md#_snippet_10\n\nLANGUAGE: Shell\nCODE:\n```\nchmod +x dolphindb\n```\n\nLANGUAGE: Shell\nCODE:\n```\nsh startController.sh\n```\n\nLANGUAGE: Shell\nCODE:\n```\nps aux|grep dolphindb\n```\n\nLANGUAGE: Shell\nCODE:\n```\nsh startagent.sh\n```\n\nLANGUAGE: Shell\nCODE:\n```\nps aux|grep dolphindb\n```\n\n----------------------------------------\n\nTITLE: Handling MTW Errors and Data Recovery in C++\nDESCRIPTION: This code snippet demonstrates how to handle errors and recover unwritten data when using the MultithreadedTableWriter (MTW). It retrieves the writer's status, gets the unwritten data, creates a new MTW instance, inserts the unwritten data using `insertUnwrittenData`, waits for the thread completion, and checks for any errors during the re-write process. This is a crucial step for ensuring data integrity in case of unexpected MTW termination.  It uses the `ErrorCodeInfo` struct to retrieve detailed error information.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_cpp_api_connector.md#_snippet_11\n\nLANGUAGE: C++\nCODE:\n```\n// 检查写入完成后 MTW 状态\nwriter.getStatus(status);\n\n// 获取未写入的数据\nstd::vector<std::vector<ConstantSP>*> unwrittenData;\nwriter.getUnwrittenData(unwrittenData);\ncout << \"Unwritten data length \" << unwrittenData.size() << endl;\n\n// 重新写入这些数据，原有的 MTW 因为异常退出已经不能用了，需要创建新的 MTW\nMultithreadedTableWriter newWriter(\"192.168.0.61\", 8848, \"admin\", \"123456\", \"dfs://test_MultithreadedTableWriter\", \"collect\", NULL,false,NULL,10000,1,10,\"deviceid\", &compress);\nErrorCodeInfo errorInfo;\n// 插入获取到的未写入数据  \nif (newWriter.insertUnwrittenData(unwrittenData, errorInfo)) {\n\t// 等待写入完成后检查状态\n\tnewWriter.waitForThreadCompletion();\n\tnewWriter.getStatus(status);\n\tif (status.hasError()) {\n\t\tcout << \"error in write again: \" << status.errorInfo << endl;\n\t}\n}\nelse {\n\tcout << \"error in write again: \" << errorInfo.errorInfo << endl;\n}\n```\n\n----------------------------------------\n\nTITLE: Matrix Null Value Handling - DolphinDB\nDESCRIPTION: This code shows how DolphinDB handles null values in matrix arithmetic.  If a cell contains a NULL value during arithmetic operations, the resulting cell will also be NULL. Several methods for handling null values in matrices include `bfill`, `ffill`, `lfill`, `fill!`, `nullFill`, `dropna`, and `withNullFill`. The example demonstrates matrix addition with nulls.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/matrix.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\n>m1 = 3 -2 1 NULL -5 6$2:3\ncol1\tcol2\tcol3\n3\t1\t(5)\n(2)\t\t6\n\n>m2 = 5 NULL 2 4 -5 9$2:3\ncol1\tcol2\tcol3\n5\t2\t(5)\n\t4\t9\n\n>m1 + m2\ncol1\tcol2\tcol3\n8\t3\t(10)\n\t\t15\n```\n\n----------------------------------------\n\nTITLE: 计算每日复权净值的收益率矩阵\nDESCRIPTION: 对构建的面板数据进行前向填充（最大窗口10天），然后计算每天的收益变化，结果为基金的日收益率矩阵。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/public_fund_basic_analysis.md#_snippet_10\n\nLANGUAGE: DolphinDB\nCODE:\n```\nreturnsMatrix = panelData.ffill(10).percentChange()\n```\n\n----------------------------------------\n\nTITLE: Handling Nulls Using conditionalIterate in a Stateful Function - DolphinDB\nDESCRIPTION: This code defines a stateful DolphinDB function (iterateTestFunc) to compute price change rates and fill nulls with the last available non-null value using conditionalIterate. It shows table setup, engine initialization, data insertion, and result querying for the streaming scenario. It depends on conditionalIterate and cumlastNot, and key parameters include input time series and trade prices. The function returns factor+1 as the final value. Notably, conditionalIterate tracks the variable state within a function, not the return value, which is an important limitation.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Streaming_computing_of_financial_quantifiers.md#_snippet_15\n\nLANGUAGE: DolphinDB\nCODE:\n```\n@state\ndef iterateTestFunc(tradePrice){\n\t// 计算交易价格涨跌幅\n\tchange = tradePrice \\ prev(tradePrice) - 1\n\t// 如果计算结果是空值，则用上一个非空因子值填充\n\tfactor = conditionalIterate(change != NULL, change, cumlastNot)\n\t// 返回 factor+1 作为最终因子值\n\treturn factor + 1\n}\n\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 定义输入输出的表结构\ninputTable = table(1:0, `securityID`tradeTime`tradePrice`tradeQty`tradeAmount`buyNo`sellNo`tradeBSFlag`tradeIndex`channelNo, [SYMBOL,DATETIME,DOUBLE,INT,DOUBLE,LONG,LONG,SYMBOL,INT,INT])\nresultTable = table(10000:0, [\"securityID\", \"tradeTime\", \"factor\"], [SYMBOL, TIMESTAMP, DOUBLE])\n\n// 使用 createReactiveStateEngine 创建响应式状态引擎\ntry{ dropStreamEngine(\"reactiveDemo\")} catch(ex){ print(ex) }\nmetrics = <[tradeTime, iterateTestFunc(tradePrice)]>\nrse = createReactiveStateEngine(name=\"reactiveDemo\", metrics =metrics, dummyTable=inputTable, outputTable=resultTable, keyColumn=\"securityID\")\n\n// 输入数据\ninsert into rse values(`000155, 2020.01.01T09:30:00, 30.85, 100, 3085, 4951, 0, `B, 1, 1)\ninsert into rse values(`000155, 2020.01.01T09:30:01, 30.86, 100, 3086, 4951, 1, `B, 2, 1)\ninsert into rse values(`000155, 2020.01.01T09:30:02, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL)\ninsert into rse values(`000155, 2020.01.01T09:30:03, 30.80, 200, 6160, 5501, 5600, `S, 3, 1)\n\n// 查看结果\nselect * from resultTable\n/*\nsecurityID tradeTime               factor          \n---------- ----------------------- ----------------\n000155     2020.01.01T09:30:00.000                 \n000155     2020.01.01T09:30:01.000 1.0003\n000155     2020.01.01T09:30:02.000 1.0003\n000155     2020.01.01T09:30:03.000 1.0003\n*/\n```\n\n----------------------------------------\n\nTITLE: Defining Guotai Junan 001 Factor Function in DolphinDB Script\nDESCRIPTION: This snippet implements the Guotai Junan 001 alpha factor in DolphinDB via the gtjaAlpha1 function. It takes open, close, and volume arguments, computes the logarithmic delta of volume, and the relative open-close price change, ranks both series, and calculates their rolling correlation over six periods. Output is scaled by -1. Dependencies are DolphinDB time series and correlation functions such as deltas, rowRank, mcorr, and basic math. Input requirements are vectors of open, close, and volume; output is a vector of computed factor values. Intended for integration into a streaming pipeline.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Streaming_computing_of_financial_quantifiers.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef gtjaAlpha1(open, close, vol){\n\tdelta = deltas(log(vol)) \n    return -1 * (mcorr(rowRank(delta, percent=true), rowRank((close - open) \\ open, percent=true), 6))\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Prevailing Quotes Output Table in DolphinDB (Python)\nDESCRIPTION: This snippet creates a shared stream table named `prevailingQuotes` in DolphinDB to store the results of real-time calculations. The table is configured for asynchronous writing, compression, caching, and retention.  It defines the table columns, data types, and various storage parameters for efficient streaming data processing. The final line sets `prevailingQuotesTemp` to NULL to free up the reference.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/stock_market_replay.md#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\ncolName = `TradeTime`SecurityID`Price`TradeQty`BidPX1`OfferPX1`Spread`SnapshotTime\ncolType = [TIME, SYMBOL, DOUBLE, INT, DOUBLE, DOUBLE, DOUBLE, TIME]\nprevailingQuotesTemp = streamTable(1000000:0, colName, colType)\nenableTableShareAndPersistence(table=prevailingQuotesTemp, tableName=\"prevailingQuotes\", asynWrite=true, compress=true, cacheSize=1000000, retentionMinutes=1440, flushMode=0, preCache=10000)\nprevailingQuotesTemp = NULL\n```\n\n----------------------------------------\n\nTITLE: Matrix Matrix Arithmetic - DolphinDB\nDESCRIPTION: This code demonstrates element-wise arithmetic operations between two matrices in DolphinDB. The matrices must have the same dimensions.  If the dimensions do not match, an error will occur.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/matrix.md#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\n>m=1..10$5:2;\n>n=3..12$5:2;\n>m;\n#0 #1\n-- --\n1  6\n2  7\n3  8\n4  9\n5  10\n>n;\n#0 #1\n-- --\n3  8\n4  9\n5  10\n6  11\n7  12\n\n>m*n;\n#0 #1\n-- ---\n3  48\n8  63\n15 80\n24 99\n35 120\n\n>m\\n;\n#0       #1\n-------- --------\n0.333333 0.75\n0.5      0.777778\n0.6      0.8\n0.666667 0.818182\n0.714286 0.833333\n\n>m+n;\n#0 #1\n-- --\n4  14\n6  16\n8  18\n10 20\n12 22\n\n>n=3..10$4:2\n>m+n;\nIncompatible vector size\n```\n\n----------------------------------------\n\nTITLE: Query DolphinDB Table with Java API\nDESCRIPTION: This Java snippet demonstrates how to query a DolphinDB table using the Java API. It executes a simple `select * from timeTest` query and prints the result to the console.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/timezone.md#_snippet_7\n\nLANGUAGE: Java\nCODE:\n```\nBasicTable res = (BasicTable) conn.run(\"select * from timeTest\");\nSystem.out.println(res.getString());\n```\n\n----------------------------------------\n\nTITLE: Loading Text Data and Basic Querying in DolphinDB\nDESCRIPTION: This script demonstrates loading a CSV file into a DolphinDB in-memory table using the `loadText` function. It then queries the table to get the total row count and displays the first 5 rows.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_csv.md#_snippet_1\n\nLANGUAGE: dolphindb\nCODE:\n```\ndataFilePath=\"/home/data/candle_201801.csv\"\ntmpTB=loadText(filename=dataFilePath)\nselect count(*) from tmpTB;\n\nselect top 5 * from tmpTB;\n```\n\n----------------------------------------\n\nTITLE: Disabling Database Replication\nDESCRIPTION: This DolphinDB script disables asynchronous replication for a specified database by setting the `setDatabaseForClusterReplication` function to false. The database object and a boolean value of `false` are passed as parameters. It assumes an existing database that has been set up for replication.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Asynchronous_Replication.md#_snippet_22\n\nLANGUAGE: dolphindb\nCODE:\n```\nsetDatabaseForClusterReplication(db, false)\n```\n\n----------------------------------------\n\nTITLE: Matrix Scalar Arithmetic - DolphinDB\nDESCRIPTION: This code shows how to perform arithmetic operations between a matrix and a scalar in DolphinDB. The operations are applied element-wise.  If the scalar is NULL, the resulting matrix will contain NULL values in all cells.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/matrix.md#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\n>m=1..10$5:2;\n>m;\n#0 #1\n-- --\n1  6\n2  7\n3  8\n4  9\n5  10\n\n>2.1*m;\n#0   #1\n---- ----\n2.1  12.6\n4.2  14.7\n6.3  16.8\n8.4  18.9\n10.5 21\n\n>m\\2;\n#0  #1\n--- ---\n0.5 3\n1   3.5\n1.5 4\n2   4.5\n2.5 5\n\n>m+1.1;\n#0  #1\n--- ----\n2.1 7.1\n3.1 8.1\n4.1 9.1\n5.1 10.1\n6.1 11.1\n\n>m*NULL;\n#0 #1\n--\n```\n\n----------------------------------------\n\nTITLE: Executing Aggregated Feature Engineering Query\nDESCRIPTION: Defines filtering conditions based on date, stock list, and trading hours. It then constructs and executes the main SQL query using `sql` and `groupBy` on the loaded `snapshot` table. The `featureEngineering` function is applied as a grouped aggregate function, and `sqlColAlias` is used to name the output columns generated by the function, including suffixes for the different time offset aggregations. The query is wrapped with `timer` to measure execution performance.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/metacode_derived_features/metacode_derived_features.txt#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\nwhereConditions = [<date(DateTime) between 2021.01.03 : 2021.12.31>, <SecurityID in stockList>, <(time(DateTime) between 09:30:00.000 : 11:29:59.999) or (time(DateTime) between 13:00:00.000 : 14:56:59.999)>]\n\ntimer result = sql(select = sqlColAlias(<featureEngineering(DateTime,\n\t\tmatrix(BidPrice0,BidPrice1,BidPrice2,BidPrice3,BidPrice4,BidPrice5,BidPrice6,BidPrice7,BidPrice8,BidPrice9),\n\t\tmatrix(BidOrderQty0,BidOrderQty1,BidOrderQty2,BidOrderQty3,BidOrderQty4,BidOrderQty5,BidOrderQty6,BidOrderQty7,BidOrderQty8,BidOrderQty9),\n\t\tmatrix(OfferPrice0,OfferPrice1,OfferPrice2,OfferPrice3,OfferPrice4,OfferPrice5,OfferPrice6,OfferPrice7,OfferPrice8,OfferPrice9),\n\t\tmatrix(OfferOrderQty0,OfferOrderQty1,OfferOrderQty2,OfferOrderQty3,OfferOrderQty4,OfferOrderQty5,OfferOrderQty6,OfferOrderQty7,OfferOrderQty8,OfferOrderQty9), aggMetaCode)>, metaCodeColName <- (metaCodeColName+\"_150\") <- (metaCodeColName+\"_300\") <- (metaCodeColName+\"_450\")), from = snapshot, where = whereConditions, groupBy = [<SecurityID>, <bar(DateTime, 10m) as DateTime>]).eval()\n```\n\n----------------------------------------\n\nTITLE: Checking Connector Status\nDESCRIPTION: Commands to check the status of the Debezium connector and view the list of connectors using the REST API tools.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Debezium_and_Kafka_data_sync.md#_snippet_11\n\nLANGUAGE: Bash\nCODE:\n```\n./rest.sh list\n./rest.sh showall\n```\n\n----------------------------------------\n\nTITLE: Creating and Populating a Persistent MVCC In-Memory Table in DolphinDB Script\nDESCRIPTION: This snippet creates a persistent MVCC in-memory table, populates it with random data, and demonstrates how to reload it from disk after a restart. It uses the mvccTable, append!, and loadMvccTable functions. The table is created with a specified disk path, making it robust to node restarts.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/in_memory_table.md#_snippet_9\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nt=mvccTable(1:0,`id`val,[INT,INT],\"/home/user/DolphinDB/mvccTable\")\nt.append!(table(1..10 as id,rand(100,10) as val))\n```\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nt=loadMvccTable(\"/home/user/DolphinDB/mvccTable\",\"t\")\n```\n\n----------------------------------------\n\nTITLE: C++ Code for IPC Table Subscription\nDESCRIPTION: This C++ code demonstrates subscribing to the IPC table created by DolphinDB. It defines the necessary data structures, creates a table with the same schema as the IPC table, sets up a `IPCInMemoryStreamClient` and calls subscribe function to receive data from the IPC table and process data. Relies on the DolphinDB C++ API and the table schema must match the IPC table schema exactly.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_implementation_for_AMD_Plugin.md#_snippet_14\n\nLANGUAGE: C++\nCODE:\n```\nIPCInMemoryStreamClient memTableClient;\n// 创建一个存储数据的 table，要求和 createIPCInMemoryTable 中列的类型和名称一一对应\nvector<string> colNames = {\"marketType\", \"securityCode\", \"execTime\", \"channelNo\", \"applSeqNum\", \"execPrice\", \"execVolume\", \"valueTrade\", \"bidAppSeqNum\", \"offerApplSeqNum\", \"side\", \"execType\", \"mdStreamId\", \"bizIndex\", \"varietyCategory\", \"receivedTime\", \"dailyIndex\", \"perPenetrationTime\"};\nvector<DATA_TYPE> colTypes = {DT_INT, DT_SYMBOL, DT_TIMESTAMP, DT_INT, DT_LONG, DT_LONG, DT_LONG, DT_LONG, DT_LONG, DT_LONG, DT_CHAR, DT_CHAR, DT_STRING, DT_LONG, DT_CHAR, DT_NANOTIMESTAMP, DT_INT, DT_NANOTIME};\nint rowNum = 0, indexCapacity=10000;\n// 创建一个和共享内存表结构相同的表\nTableSP outputTable = Util::createTable(colNames, colTypes, rowNum, indexCapacity); \n    \n// 是否覆盖前面旧的数据\nbool overwrite = true;\nThreadSP thread = memTableClient.subscribe(tableName, dumpData, outputTable, overwrite);\nthread->join();\n```\n\n----------------------------------------\n\nTITLE: Creating an Alertmanager Wechat Notification Template (Go Template)\nDESCRIPTION: This Go template defines the format for alert messages sent to Wechat via Alertmanager. It structures messages differently for firing and resolved alerts, including details like status, alert name, instance, summary, description, start time, and end time, formatted appropriately. The template uses Go's template syntax and accesses alert data provided by Alertmanager.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/cluster_monitor.md#_snippet_16\n\nLANGUAGE: gotemplate\nCODE:\n```\n{{ define \"wechat.default.message\" }}\n{{- if gt (len .Alerts.Firing) 0 -}}\n{{- range $index, $alert := .Alerts -}}\n{{- if eq $index 0 }}\n========= 监控报警 =========\n告警状态：{{   .Status }}\n告警类型：{{ $alert.Labels.alertname }}\n故障主机: {{ $alert.Labels.instance }}\n告警主题: {{ $alert.Annotations.summary }}\n告警详情: {{ $alert.Annotations.message }}{{ $alert.Annotations.description}}\n故障时间: {{ ($alert.StartsAt.Add 28800e9).Format \"2006-01-02 15:04:05\" }}\n========= = end =  =========\n{{- end }}\n{{- end }}\n{{- end }}\n{{- if gt (len .Alerts.Resolved) 0 -}}\n{{- range $index, $alert := .Alerts -}}\n{{- if eq $index 0 }}\n========= 异常恢复 =========\n告警类型：{{ .Labels.alertname }}\n告警状态：{{   .Status }}\n告警主题: {{ $alert.Annotations.summary }}\n告警详情: {{ $alert.Annotations.message }}{{ $alert.Annotations.description}}\n故障时间: {{ ($alert.StartsAt.Add 28800e9).Format \"2006-01-02 15:04:05\" }}\n恢复时间: {{ ($alert.EndsAt.Add 28800e9).Format \"2006-01-02 15:04:05\" }}\n{{- if gt (len $alert.Labels.instance) 0 }}\n实例信息: {{ $alert.Labels.instance }}\n{{- end }}\n========= = end =  =========\n{{- end }}\n{{- end }}\n{{- end }}\n{{- end }}\n```\n\n----------------------------------------\n\nTITLE: Scheduling a Job with Access Control - DolphinDB\nDESCRIPTION: This snippet demonstrates scheduling a job that reads a table. The job creation doesn't require table access permissions, but the job execution does. Uses `scheduleJob` and `loadTable`.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Permission_Management.md#_snippet_39\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(\"NickFoles\",\"AB123!@\") \ndef readTable(){\n\tread_t1=loadTable(\"dfs://db1\",\"t1\") \n\treturn exec count(*) from read_t1 \n}\nscheduleJob(\"readTableJob\",\"read DFS table\",readTable,minute(now()),date(now()),date(now())+1,'D');\n```\n\n----------------------------------------\n\nTITLE: Column-wise Aggregation of Matrices Using Built-in Functions in DolphinDB\nDESCRIPTION: Shows how to compute column-wise aggregations on a matrix in DolphinDB using the sum function. The computation is performed independently for each column. Input: matrix with numeric data. Output: vector where each value is the sum of a column. No external dependencies; matrix must be rectangular. Directly applies to statistical or financial data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/panel_data.md#_snippet_24\n\nLANGUAGE: DolphinDB\nCODE:\n```\nm = rand(10, 20)$10:2\nsum(m)\n```\n\n----------------------------------------\n\nTITLE: Restoring Backup to New DolphinDB Database Using migrate Function\nDESCRIPTION: Performs data restoration to a new DolphinDB database and table using migrate. Firstly, migrates the first day's backup directly to the new database and table. For subsequent days, migrates data to a temporary table, appends filtered data into the new table within a date range, and drops the temporary table afterward. Error handling is demonstrated for migration failures with error messages. This method requires familiarity with migrate function behavior, temporary tables, and table appending operations.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/restore-backup.md#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\nmigrate(\"/hdd/hdd1/backup/20200101/\",\"dfs://ddb\",\"windTurbine\",\"dfs://db1\",\"equip\")\nday=2020.01.02\nnewTable=loadTable(\"dfs://db1\",\"equip\")\nfor(i in 2:31){\n\tpath=\"/hdd/hdd1/backup/\"+temporalFormat(day, \"yyyyMMdd\") + \"/\";\n\tday=datetimeAdd(day,1,`d)\n\tif(!exists(path)) continue;\n\tprint \"restoring \" + path;\n\tt=migrate(path,\"dfs://ddb\",\"windTurbine\",\"dfs://db1\",\"tmp\") \n\tif(t['success'][0]==false){\n\t\tprint t['errorMsg'][0]\n\t\tcontinue\n\t}\n\tnewTable.append!(select * from loadTable(\"dfs://db1\",\"tmp\") where tm between datetime(day-1) : (day.datetime()-1) > )\n\tdatabase(\"dfs://db1\").dropTable(\"tmp\")\n}\n```\n\n----------------------------------------\n\nTITLE: Checking DolphinDB Version After Upgrade Windows\nDESCRIPTION: This code snippet enables users to confirm the DolphinDB version after a Windows upgrade.  The version() function is called through the Web UI to display the software version.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/standalone_server.md#_snippet_19\n\nLANGUAGE: sh\nCODE:\n```\nversion()\n```\n\n----------------------------------------\n\nTITLE: Setting New Value Partition Policy DolphinDB\nDESCRIPTION: This configuration specifies how the system should handle new data that falls outside the existing partition scheme for a VALUE domain (or VALUE domain in a COMPO domain). The available options are \"skip\" (ignore the data), \"fail\" (throw an error), and \"add\" (create a new partition).\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/ha_cluster_deployment/P3/config/config-specification.txt#_snippet_2\n\nLANGUAGE: properties\nCODE:\n```\nnewValuePartitionPolicy=add\n```\n\n----------------------------------------\n\nTITLE: Loading Table in Slave Cluster\nDESCRIPTION: This DolphinDB script loads a table into the slave cluster, verifying the successful replication of schema information. It uses `loadTable` function with the database name and table name as parameters. The database and table should already exist on the master cluster, and the slave should be configured to replicate from the master. A prerequisite is to have configured the clusters for replication.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Asynchronous_Replication.md#_snippet_13\n\nLANGUAGE: dolphindb\nCODE:\n```\nloadTable(\"dfs://testDB\", \"testTB\")\n```\n\n----------------------------------------\n\nTITLE: Checking Database Replication Status\nDESCRIPTION: This DolphinDB script checks whether asynchronous replication is enabled for a database.  It uses the schema method of the database object `db` and checks the `clusterReplicationEnabled` property. It expects a boolean return value: `true` or `false`. Prerequisites include having created a database object.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Asynchronous_Replication.md#_snippet_10\n\nLANGUAGE: dolphindb\nCODE:\n```\nschema(db).clusterReplicationEnabled\n```\n\n----------------------------------------\n\nTITLE: Extracting and Converting CSV Schema in DolphinDB Script\nDESCRIPTION: Defines the function getSchema that extracts the schema from a given CSV file by reading its header with extractTextSchema, converts column names from GBK encoding to UTF-8, and maps the original Chinese column names to standardized English column names. It updates the schema metadata accordingly, assigns a column index to each column, and removes an irrelevant 'Direction' field. This schema is later used for data loading consistency.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/csvImportDemo/importOldData.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef getSchema(csv){\n\tschema1=extractTextSchema(csv)\n\tupdate schema1 set name=convertEncode(name,\"gbk\",\"utf-8\") \n\tnewName=`Symbol`Market`DateTime`Price`TickCount`Volume`Amount`AskPrice1`AskPrice2`AskPrice3`AskPrice4`AskPrice5`BidPrice1`BidPrice2`BidPrice3`BidPrice4`BidPrice5`AskVolume1`AskVolume2`AskVolume3`AskVolume4`AskVolume5`BidVolume1`BidVolume2`BidVolume3`BidVolume4`BidVolume5\n\toldName=`优能代码`市场代码`时间`最新价`成交笔数`成交量`成交额`卖一价`卖二价`卖三价`卖四价`卖五价`买一价`买二价`买三价`买四价`买五价`卖一量`卖二量`卖三量`卖四量`卖五量`买一量`买二量`买三量`买四量`买五量\n\tdict1=dict(oldName,newName)\n\tupdate schema1 set name=dict1[name] where name in dict1.keys()\n\n\tupdate schema1 set col = rowNo(name)\n\tdelete from schema1 where name in [`方向]\n\t\n\treturn schema1\n}\n```\n\n----------------------------------------\n\nTITLE: Training and Saving an XGBoost Model in DolphinDB\nDESCRIPTION: The code sets up hyperparameters for XGBoost regression and trains a model on the training data. The trained model is then saved to disk using xgboost::saveModel. Dependencies: Xgboost plugin must be loaded, training data must be available. Inputs: Train_x, Train_y, parameters dictionary. Outputs: Model file at modelSavePath.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/metacode_derived_features/metacode_derived_features_buildmodel.txt#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\nparams = {\n\tobjective: 'reg:squarederror',\n\tcolsample_bytree: 0.8,\n\tsubsample: 0.8,\n\tmin_child_weight: 1,\n\tmax_leaves:128,\n\teta: 0.1,\n\tmax_depth:10,\n\teval_metric : 'rmse'\n\t}\nmodel_1 = xgboost::train(Train_y ,Train_x, params, 500)\nxgboost::saveModel(model_1, modelSavePath)\n```\n\n----------------------------------------\n\nTITLE: 生成不同聚合标签测试数据\nDESCRIPTION: 创建模拟数据用于测试不同标签的不同聚合方式，包含标签、时间和值三个字段。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_SQL_Case_Tutorial.md#_snippet_29\n\nLANGUAGE: DolphinDB\nCODE:\n```\nN = 1000000\nt = table(\"code\" + string(take(1..3, N)) as tag, \n          sort(take([2021.06.28T00:00:00, 2021.06.28T00:10:00, 2021.06.28T00:20:00], N)) as time, \n          take([1.0, 2.0, 9.1, 2.0, 3.0, 9.1, 9.1, 2.0, 3.0], N) as value)\n```\n\n----------------------------------------\n\nTITLE: Running DolphinDB Foreground Linux\nDESCRIPTION: This command starts the DolphinDB server in the foreground.  It executes the `dolphindb` binary directly.  This allows you to monitor the server's output in the terminal.  The default port is 8848.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/standalone_server.md#_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\n./dolphindb\n```\n\n----------------------------------------\n\nTITLE: Configuring dataX Synchronization Job with JSON\nDESCRIPTION: This JSON configuration file defines a full historical data synchronization job from SQL Server to a partitioned DolphinDB table using dataX. The 'reader' section specifies SQL Server connection details (host, port, credentials, and target table), while the 'writer' section configures the DolphinDB instance (host, port, credentials, schema mapping for each column, batch size) for data ingestion. Required dependencies include dataX 3.0+, JDK 1.8, Maven, and Python 2.x, as well as prior creation of the target table in DolphinDB. Input is a configured SQL Server and DolphinDB environment, and output is a successful bulk import into DolphinDB; limitation: the configuration as shown only supports full sync, not incremental.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Python_Celery.md#_snippet_1\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    \"job\": {\n        \"content\": [\n            {\n                \"writer\": {\n                    \"parameter\": {\n                        \"dbPath\": \"dfs://tick_close\",\n                        \"tableName\": \"tick_close\",\n                        \"batchSize\": 100,\n                        \"userId\": \"admin\",\n                        \"pwd\": \"123456\",\n                        \"host\": \"127.0.0.1\",\n                        \"table\": [\n                            {\n                                \"type\": \"DT_SYMBOL\",\n                                \"name\": \"SecurityID\"\n                            },\n                            {   \"type\": \"DT_DATE\",\n                                \"name\": \"TradeDate\"\n                            },\n                            {\n                                \"type\": \"DT_DOUBLE\",\n                                \"name\": \"Value\"\n                            }\n],\n                        \"port\": 8848\n                    },\n                    \"name\": \"dolphindbwriter\"\n                },\n                \"reader\": {\n                    \"name\": \"sqlserverreader\",\n                    \"parameter\": {\n                        \"username\": \"SA\",\n                        \"password\": \"Sa123456\",\n                        \"column\": [\n                            \"*\"\n                        ],\n                        \"connection\": [\n                            {\n                                \"table\": [\n                                    \"tick_close\"\n                                ],\n                                \"jdbcUrl\": [\n                                \"jdbc:sqlserver://127.0.0.1:1234;DatabaseName=tick_close\"\n                                ]\n                                \n                            }\n                        ]\n                    }\n                }\n            }\n        ],\n        \"setting\": {\n            \"speed\": {\n                \"channel\": 1\n            }\n        }\n    }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Registering Market Snapshot Processing Reactive State Engine in DolphinDB Script\nDESCRIPTION: This snippet registers a reactive state streaming engine named \"mdlSnapshotProcessEngine\" which processes incoming raw snapshot data. It applies previously defined metric calculations and filters records within specified trading time intervals. The engine supports stateful incremental computations and maintains input order. Input and output tables are referenced by their object names to enable streaming processing.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/K.md#_snippet_13\n\nLANGUAGE: DolphinDB\nCODE:\n```\nmdlSnapshotProcessEngineName = \"mdlSnapshotProcessEngine\"\ncreateReactiveStateEngine(\n\tname=mdlSnapshotProcessEngineName,\n\tmetrics =convert,\n\tdummyTable=objByName(mdlSnapshotTBName),\n\toutputTable=objByName(mdlSnapshotProcessTBName),\n\tkeyColumn=\"SecurityID\",\n\tfilter=<TradeTime.time() between 09:25:00.000:11:31:00.000 or TradeTime.time() between 13:00:00.000:14:57:00.000 or TradeTime.time()>=15:00:00.000>,\n\tkeepOrder = true)\n```\n\n----------------------------------------\n\nTITLE: Implementing flow Factor in DolphinDB\nDESCRIPTION: This function calculates the flow factor using DolphinDB's mavg function for moving averages. It processes buy and sell volumes along with price data to compute a ratio relative to the moving average of the spread.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Python+HDF5_vs_DolphinDB.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef flow(buy_vol, sell_vol, askPrice1, bidPrice1){\n\tbuy_vol_ma = round(mavg(buy_vol, 60), 5)\n\tsell_vol_ma = round(mavg(sell_vol, 60), 5)\n\tbuy_prop = iif(abs(buy_vol_ma+sell_vol_ma) < 0, 0.5 , buy_vol_ma/ (buy_vol_ma+sell_vol_ma))\n\tspd_tmp = askPrice1 - bidPrice1\n\tspd = iif(spd_tmp  < 0, 0, spd_tmp)\n\tspd_ma = round(mavg(spd, 60), 5)\n\treturn iif(spd_ma == 0, 0, buy_prop / spd_ma)\n}\n```\n\n----------------------------------------\n\nTITLE: Executing Parallel Data Processing with Multiprocessing - Python\nDESCRIPTION: This snippet orchestrates the parallel processing of multiple stock data files. It specifies the number of processes, the data directory, lists the files to be processed, and uses the `multi_task_split` class to determine the actual number of processes and split the file list into chunks. It then creates a `multiprocessing.Pool`, uses `starmap` to apply the `pool_func` to the file chunks in parallel, combines the results, and measures the total execution time.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/DolphinDB_Python_Parser_Intro_for_Quantitative_Finance/因子实现_Python版本/十档委买增额.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nn_use = 24\n# 路径修改为存放数据路径\nsnapshot_path =\"/ssd/ssd3/data/oneStock_oneFile_TL/20230201/snapshot/\"\nstock_pool = os.listdir(snapshot_path)\nprocesses_decided = multi_task_split(stock_pool, n_use).num_of_jobs()\nprint(\"进程数：\", processes_decided)\nsplit_args_to_process = list(multi_task_split(stock_pool, n_use).split_args())\nargs = [(split_args_to_process[i], snapshot_path) for i in range(len(split_args_to_process))]\nprint(\"#\" * 50 + \"Multiprocessing Start\" + \"#\" * 50)\nt0 = time.time()\nwith multiprocessing.Pool(processes=processes_decided) as pool:\n    res = tqdm(pool.starmap(pool_func, args))\n    print(\"cal time: \", time.time() - t0, \"s\")\n    res_combined = pd.concat(res, axis=0)\n    pool.close()\n    print(\"cal time: \", time.time() - t0, \"s\")\nprint(res_combined)\n```\n\n----------------------------------------\n\nTITLE: Connecting to Redshift via ODBC in DolphinDB\nDESCRIPTION: This snippet establishes a connection to the Redshift database using the ODBC plugin. It utilizes the `odbc::connect` function, providing the necessary connection string including the driver, server address, database name, user credentials, which are specific for the Redshift database. Successful execution returns a connection object which will be used for querying data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/Migrate_data_from_Redshift_to_DolphinDB/Redshift2DDB.txt#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//连接RS数据库\nconn = odbc::connect(\"Driver={Amazon Redshift (x64)}; Server=ddb-redshift-test.cpsycpmgsvcy.us-west-2.redshift.amazonaws.com; Database=ddb-test;User=ddbadmin;Password=DolphinDB123;\")\n```\n\n----------------------------------------\n\nTITLE: Simulating Data and Writing to Table\nDESCRIPTION: Simulates data for a set of trains and appends it to the distributed table. The `simulateData` function generates random data for specified train IDs. The `simulate` function writes this data to the table in batches with a short delay.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/snapshot_engine.md#_snippet_6\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\n//模拟数据产生\ndef simulateData(trainVector){\n\tnum = size(trainVector)\n\treturn table(take(trainVector,num) as trainID, take(now(),num) as ts, rand(20..41,num) as tag0001, rand(30..71,num) as tag0002, rand(70..151,num) as tag0003)\n}\n//将数据写入数据表。写入次数为batches。\ndef simulate(host,port,trainVector,batches){\n\th = xdb(host,port,\"admin\",\"123456\")\n\tfor(i in 0:batches){\n\t\tt=simulateData(trainVector)\n\t\th(\"append!{loadTable('dfs://testDB', 'trainInfoTable')}\", t)\n\t\tsleep(200)\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Simulated Trade Data in DolphinDB Script\nDESCRIPTION: Creates a simulated trading table with 1,000,000 rows, including columns for date, time, symbol, volume, price, and side by repeating or randomizing typical values. Useful as baseline test data for subsequent trade analysis queries. Requires the DolphinDB environment and assumes access to core vector functions like take and rand.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_SQL_Case_Tutorial.md#_snippet_33\n\nLANGUAGE: DolphinDB\nCODE:\n```\nN = 1000000\nt = table(take(2021.11.01..2021.11.15, N) as date, \n          take([09:30:00, 09:35:00, 09:40:00, 09:45:00, 09:47:00, 09:49:00, 09:50:00, 09:55:00, 09:56:00, 10:00:00], N) as time, \n          take(`AAPL`FB`MSFT$SYMBOL, N) as symbol, \n          take([10000, 30000, 50000, 80000, 100000], N) as volume, \n          rand(100.0, N) as price, \n          take(`BUY`SELL$SYMBOL, N) as side)\n```\n\n----------------------------------------\n\nTITLE: Create Schema Table for Data Types\nDESCRIPTION: This snippet creates a schema table to define data types for each column when importing data using `loadText`. It's used when automatic data type inference is incorrect.  The `extractTextSchema` function can be used to generate a base schema table, which can then be modified.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_data.md#_snippet_1\n\nLANGUAGE: txt\nCODE:\n```\nnameCol = `symbol`exchange`cycle`tradingDay`date`time`open`high`low`close`volume`turnover`unixTime\ntypeCol = `SYMBOL`SYMBOL`INT`DATE`DATE`INT`DOUBLE`DOUBLE`DOUBLE`DOUBLE`INT`DOUBLE`LONG\nschemaTb = table(nameCol as name,typeCol as type);\n```\n\n----------------------------------------\n\nTITLE: Calculating Sharpe Ratio Using DolphinDB\nDESCRIPTION: Defines getSharp function to compute the Sharpe ratio, which evaluates risk-adjusted return. It subtracts a fixed risk-free rate of 3% (0.03) from the annual return and divides by annual volatility. This metric indicates reward per unit of risk. Inputs are price/value series, outputs a numeric Sharpe ratio.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_factor_by_ddb/fund_factor_ten.txt#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndefg getSharp(value){\n\treturn (getAnnualReturn(value) - 0.03)\\getAnnualVolatility(value) as sharpeRat\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Parameters for Sending Group Chat Message\nDESCRIPTION: Code for preparing URL and JSON payload to send a message to a WeChat Work group chat. The payload includes the chat ID, message type, and content.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/send_messages_external_systems.md#_snippet_10\n\nLANGUAGE: DolphinDB\nCODE:\n```\nurl = 'https://qyapi.weixin.qq.com/cgi-bin/appchat/send?';\nACCESS_TOKEN='xxxxx';\nurl+='access_token='+ACCESS_TOKEN;\nparam='{\"chatid\" : \"CHATID\",\"msgtype\" : \"text\",\"text\" : {\"content\" : \"这是一条测试信息\"}}';\n```\n\n----------------------------------------\n\nTITLE: Getting Streaming Statistics\nDESCRIPTION: These lines retrieve streaming statistics related to published tables, connections, subscriber workers, and MQTT subscriber information. These statistics provide insights into the real-time data flow and connectivity within the DolphinDB system and the MQTT integration.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/data/DolphinDB_streaming_application_in_IOT/deviceState.txt#_snippet_12\n\nLANGUAGE: DolphinDB\nCODE:\n```\ngetStreamingStat().pubTables\ngetStreamingStat().pubConns\ngetStreamingStat().subWorkers\nmqtt::getSubscriberStat()  \ngetRecentJobs()\n```\n\n----------------------------------------\n\nTITLE: Creating and Importing Data into DolphinDB Public Fund Dimension Table\nDESCRIPTION: Defines the schema and loads CSV data into a DolphinDB dimension table stored in a database. Utilizes schema creation, database, and table creation, and data loading functions. Ensures appropriate schema and storage optimization for small to medium-sized datasets.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/public_fund_basic_analysis.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\ncsvDataPath = \"/ssd/ssd2/data/fundData/publicFundData.csv\"\ndbName = \"dfs://publicFundDB\"\ntbName = \"publicFundData\"\n// create database and one-partition table\nif(existsDatabase(dbName)){\n\tdropDatabase(dbName)\n}\ntimeRange = 1990.01.01 join sort(distinct(yearBegin(2016.01.01..2050.01.01)))\ndb = database(dbName, RANGE, timeRange, engine = 'TSDB')\nnames = `SecurityID`FullName`Name`Management`Type`Custodian`IssueShare`InceptDate`MFee`CFee`SFee`Closed`Status\ntypes = `SYMBOL`STRING`STRING`SYMBOL`SYMBOL`SYMBOL`DOUBLE`DATE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`INT\nschemaTB = table(1:0, names, types)\ndb.createTable(table=schemaTB, tableName=tbName, sortColumns=`InceptDate)\n// load CSV data\ntmp = ploadText(filename=csvDataPath, schema=table(names, types))\nloadTable(dbName, tbName).append!(tmp)\n```\n\n----------------------------------------\n\nTITLE: Updating Existing Columns with Assignment\nDESCRIPTION: Demonstrates updating existing columns in a memory table using direct assignment.  Includes updating all rows and updating rows based on a boolean expression.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/partitioned_in_memory_table.md#_snippet_20\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntrades[`qty1] = <qty1+10>;\n\ntrades[`qty1, <sym=`IBM>] = <qty1+10>;\n```\n\n----------------------------------------\n\nTITLE: Applying cleanFun using eachPre\nDESCRIPTION: This code snippet applies the 'cleanFun' function to the 'ln' column of the 't2' table using the 'eachPre' higher-order function. The 'eachPre' function applies 'cleanFun' to each pair of adjacent elements in the 'ln' column.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_31\n\nLANGUAGE: shell\nCODE:\n```\nt2[`clean] = eachPre(cleanFun{F}, t2[`ln])\n```\n\n----------------------------------------\n\nTITLE: Define Order Tagging Function (DolphinDB)\nDESCRIPTION: Defines a stateful function `tagFunc` using the `@state` decorator. This function categorizes order quantity (`qty`) into three size bins (0: small, 1: medium, 2: large) based on specified thresholds (<=20000, 20000-200000, >200000) using nested `iif` statements. It's intended for use within stream processing engines.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/streaming_capital_flow_daily/02.createEngineSub.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\n/* \n * Label small, medium and large order\n * small : 0\n * medium : 1\n * large : 2\n */\n@state\ndef tagFunc(qty){\n    return iif(qty <= 20000, 0, iif(qty <= 200000 and qty > 20000, 1, 2))\n}\n```\n\n----------------------------------------\n\nTITLE: Creating and Managing a Long-Running Background Thread with Resource Management C++\nDESCRIPTION: Shows how to create a long-running background thread in a DolphinDB plugin that is managed by the system's resource mechanism. It defines a `Runnable` (`DemoRun2`), a manager class (`Demo`) holding the thread, and a cleanup callback (`demoOnClose`) linked to a resource created by `Util::createResource`. This pattern allows the thread to run in the background without blocking the main plugin execution and ensures proper cleanup when the resource is released.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_advance.md#_snippet_22\n\nLANGUAGE: C++\nCODE:\n```\nclass DemoRun2 : public Runnable {\npublic:\n    DemoRun2(ConstantSP data) : data_(data) {}\n    ~DemoRun2() {}\n    void run() override {\n        size_t size = data_->size();\n        for (size_t i = 0; i < size; ++i) {\n            std::cout << data_->getInt(i) << std::endl;\n            Util::sleep(2000);\n        }\n    }\nprivate:\n    ConstantSP data_; \n};\n\nclass Demo {\npublic:\n    Demo() {}\n    ~Demo() {}\n    void createAndRun(ConstantSP data) {\n        SmartPointer<DemoRun2> demoRun = new DemoRun2(data);\n        thread_ = new Thread(demoRun);\n        if (!thread_->isStarted()) {\n            thread_->start();\n        }\n    }\nprivate:\n    ThreadSP thread_;\n};\n\nstatic void demoOnClose(Heap* heap, vector<ConstantSP>& args) {\n    Demo* pDemo = (Demo*)(args[0]->getLong());\n    if(pDemo != nullptr) {\n        delete pDemo;\n        args[0]->setLong(0);\n    }\n}\n\nConstantSP createThread2(Heap *heap, vector<ConstantSP> &arguments) {\n    if (!(arguments[0]->isVector() && arguments[0]->getType() == DT_INT)) {\n        throw IllegalArgumentException(\"createThread\", \"argument must be an integral vector\");\n    }\n    Demo * pDemo = new Demo();\n    pDemo->createAndRun(arguments[0]);\n    FunctionDefSP onClose(Util::createSystemProcedure(\"demo onClose()\", demoOnClose, 1, 1));\n    return Util::createResource(reinterpret_cast<long long>(pDemo), \"Demo\", onClose, heap->currentSession());\n}\n```\n\n----------------------------------------\n\nTITLE: 筛选符合条件的基金计算夏普比率\nDESCRIPTION: 基于收益率数据筛选发布超过1000个交易日，且最近30天内有数据的基金，剔除货币市场型和REITs，计算年化收益、波动和夏普比率。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/public_fund_basic_analysis.md#_snippet_19\n\nLANGUAGE: DolphinDB\nCODE:\n```\nuReturnsMatrix = returnsMatrix.loc(,(each(count, returnsMatrix) > 1000 && returnsMatrix.ilastNot() >=  returnsMatrix.rows() - 30)&&!(fundTypeMap[returnsMatrix.colNames()] in [\"货币市场型\", \"REITs\"]))\n```\n\n----------------------------------------\n\nTITLE: Batch Data Loading with Job Submission to Partitioned Tables - DolphinDB Script\nDESCRIPTION: Implements a function ('load_data') that iterates over a range of dates, submitting jobs to asynchronously load daily data into specified partitioned tables. Inputs are the list of dates, target database path, table name, and the source data table. Outputs include job submissions identified by descriptive names; dependencies include 'load_one_day_data' and configured job scheduling. This function supports parallel, automated loading and logging of ingestion progress.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/buildData.txt#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\nsmallDates=dates[0:100]\n\ndef load_data(dates, db_path, table_name,tmp_table) {\n    for (date in dates) {\n        job_name = \"load_data_\" + table_name + \"_\" + year(date)+\"_\" + monthOfYear(date)+\"_\"+dayOfMonth(date)\n        job_desc = \"TO: \" + db_path\n        submitJob(job_name,job_desc,load_one_day_data,date,db_path,table_name,tmp_table)\n        print(job_name + \" is submitted...\")\n    }\n}\n\nload_data(smallDates,dbPath,`quotes,tmp)\nload_data(smallDates,dbPath,`quotes_2,tmp)\nload_data(smallDates,dbPathTSDB,`quotes_tsdb,tmp)\n```\n\n----------------------------------------\n\nTITLE: Configuring ODBC DSN in odbc.ini\nDESCRIPTION: This snippet demonstrates how to configure the ODBC Data Source Name (DSN) in the /etc/odbc.ini file. It defines the connection details for both ANSI and Unicode drivers, including server, database, UID, PWD, Port, and Proto. It is important to replace the default values with the actual ClickHouse configuration.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ClickHouse_to_DolphinDB.md#_snippet_2\n\nLANGUAGE: ini\nCODE:\n```\n[ODBC Data Sources] \nClickHouse DSN (ANSI)=ClickHouse ODBC Driver (ANSI) \nClickHouse DSN (Unicode)=ClickHouse ODBC Driver (Unicode) \n\n[ClickHouseAnsi] \nDriver=ClickHouse ODBC Driver (ANSI) \nDescription=DSN (localhost) for ClickHouse ODBC Driver (ANSI) \nServer = localhost\nDatabase = default\nUID = default\nPWD = 123456\nPort = 8123\nProto = http\n\n[ClickHouseUni] \nDriver=ClickHouse ODBC Driver (Unicode) \nDescription=DSN (localhost) for ClickHouse ODBC Driver (Unicode) \nServer = localhost\nDatabase = default\nUID = default\nPWD = 123456\nPort = 8123\nProto = http\n```\n\n----------------------------------------\n\nTITLE: LU Decomposition - DolphinDB\nDESCRIPTION: This code demonstrates LU decomposition of a matrix in DolphinDB using the `lu` function.  It shows how to obtain the permutation matrix (P), lower triangular matrix (L), and upper triangular matrix (U). The code showcases both the standard LU decomposition and a variation where PL is returned instead of P and L separately.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/matrix.md#_snippet_12\n\nLANGUAGE: DolphinDB\nCODE:\n```\n>m=matrix([2 5 7 5, 5 2 5 4, 8 2 6 4, 7 8 6 8]);\n>m;\n#0 #1 #2 #3\n-- -- -- --\n2  5  8  7\n5  2  2  8\n8  2  6  4\n7  8  6  8\n\n>p,l,u=lu(m);\n>p;\n#0 #1 #2 #3\n-- -- -- --\n0  1  0  0\n0  0  0  1\n1  0  0  0\n0  0  1  0\n>l;\n#0       #1    #2        #3\n-------- ----- --------- --\n1        0     0         0\n0.285714 1     0         0\n0.714286 0.12  1         0\n0.714286 -0.44 -0.461538 1\n>u;\n#0 #1       #2       #3\n-- -------- -------- --------\n7  5        6        6\n0  3.571429 6.285714 5.285714\n0  0        -1.04    3.08\n0  0        0        7.461538\n\n>pl,u=lu(m,true);\n>pl;\n#0       #1    #2        #3\n-------- ----- --------- --\n0.285714 1     0         0\n0.714286 -0.44 -0.461538 1\n1        0     0         0\n0.714286 0.12  1         0\n>u;\n#0 #1       #2       #3\n-- -------- -------- --------\n7  5        6        6\n0  3.571429 6.285714 5.285714\n0  0        -1.04    3.08\n0  0        0        7.461538\n```\n\n----------------------------------------\n\nTITLE: Historical IOPV Calculation Using Panel Data in DolphinDB\nDESCRIPTION: Calculates historical IOPV values by creating a panel data structure from tick-by-tick trade data, with time series on rows and constituent stocks on columns, then applying ffill and rowSum operations.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_IOPV.md#_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\ntimeSeriesValue = select tradetime, SecurityID, price * portfolio[SecurityID]/1000 as constituentValue from loadTable(\"dfs://LEVEL2_SZ\",\"Trade\") where SecurityID in portfolio.keys(), tradedate = 2020.12.01, price > 0\niopvHist = select rowSum(ffill(constituentValue)) as IOPV from timeSeriesValue pivot by tradetime, SecurityID\n```\n\n----------------------------------------\n\nTITLE: Matrix Transposition - DolphinDB\nDESCRIPTION: This code demonstrates how to transpose a matrix in DolphinDB using the `transpose` function. Transposition swaps the rows and columns of the matrix.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/matrix.md#_snippet_9\n\nLANGUAGE: DolphinDB\nCODE:\n```\n>x=1..6 $ 3:2;\n>x;\n#0 #1\n-- --\n1  4\n2  5\n3  6\n\n>transpose x;\n#0 #1 #2\n-- -- --\n1  2  3\n4  5  6\n```\n\n----------------------------------------\n\nTITLE: QR Decomposition with economic mode in DolphinDB\nDESCRIPTION: Demonstrates QR decomposition of a matrix in DolphinDB using the `qr` function with `mode='economic'` and `pivoting=false`.  The function decomposes a matrix X into an orthogonal matrix Q and an upper triangular matrix R, returning only the essential parts. It shows the result of Q and R.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/matrix.md#_snippet_16\n\nLANGUAGE: DolphinDB\nCODE:\n```\n>m=matrix([2 5 5, 5 5 4, 8 6 4, 7 6 8]);\n>m;\n#0 #1 #2 #3\n-- -- -- --\n2  5  8  7 \n5  5  6  6 \n5  4  4  8 \n\n>q,r=qr(m,mode='economic',pivoting=false);\n>q;\n#0        #1        #2       \n--------- --------- ---------\n-0.272166 0.93784   0.215365 \n-0.680414 -0.029307 -0.732242\n-0.680414 -0.345828 0.646096 \n>r;\n#0        #1        #2        #3        \n--------- --------- --------- ----------\n-7.348469 -7.484552 -8.981462 -11.430952\n0         3.159348  5.943561  3.622407  \n0         0         -0.086146 2.282872\n\n```\n\n----------------------------------------\n\nTITLE: 生成连续区间测试数据\nDESCRIPTION: 创建模拟数据用于测试连续区间最值计算，包含日期和值两个字段。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_SQL_Case_Tutorial.md#_snippet_25\n\nLANGUAGE: DolphinDB\nCODE:\n```\nt = table(2021.09.29 + 0..15 as date, \n          0 0 0.3 0.3 0 0.5 0.3 0.5 0 0 0.3 0 0.4 0.6 0.6 0 as value)\ntargetVal = 0.3\n```\n\n----------------------------------------\n\nTITLE: Querying Distributed Table Access Statistics in DolphinDB\nDESCRIPTION: Complex SQL query to analyze distributed table access records, including memory usage and row counts across all queries. The script retrieves data from all nodes, joins sql records with their associated usage statistics, and formats the results.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/user_level_resource_tracking.md#_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nlogin(\"admin\", \"123456\")\n// Get data node aliases\ndataNodeAlias = exec name from rpc(getControllerAlias(), getClusterPerf) where mode in [0,3]\n// Calculate distributed table query data usage\ndataUsage = pnodeRun(getUserTableAccessRecords{2023.12.13}, dataNodeAlias)\nquery = select distinct script, rootQueryId, timestamp from dataUsage where type=\"sql\"\nstats = select sum(value)\nfrom dataUsage d inner join query q on d.rootQueryId = q.rootQueryId where type!=\"sql\" pivot by q.timestamp, userId, database, table, q.script, type\nif(count(stats)==0){\n\tresult = stats\n\t}else{\n\tresult =stats.reorderColumns!(`timestamp`userId`database`table`memUsage`rowCount`script)\n\t}\nresult\n```\n\n----------------------------------------\n\nTITLE: Creating and Updating a Scheduled Job Function in DolphinDB\nDESCRIPTION: This example demonstrates how updating a function definition after scheduling a job doesn't affect the scheduled job execution, which continues to use the serialized old function.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/scheduledJob.md#_snippet_8\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef f(){\n\tprint \"The old function is called \" \n}\nscheduleJob(`test, \"f\", f, 11:05m, today(), today(), 'D');\ngo\ndef f(){\n\tprint \"The new function is called \" \n}\n```\n\n----------------------------------------\n\nTITLE: 跳过失败任务并重启异步复制 - DolphinDB\nDESCRIPTION: 当异步复制任务由于异常而中止时，可以使用这段代码跳过失败的任务并重启异步复制。首先通过skipClusterReplicationTask跳过特定ID的失败任务，然后调用startClusterReplication重启异步复制流程。跳过的任务将被标记为完成状态。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Asynchronous_Replication.md#_snippet_23\n\nLANGUAGE: DolphinDB\nCODE:\n```\nrpc(getControllerAlias(), skipClusterReplicationTask, 938)\nrpc(getControllerAlias(), startClusterReplication)\n```\n\n----------------------------------------\n\nTITLE: Updating Data in TSDB Table\nDESCRIPTION: This script is designed to update data within a TSDB table. It loads the 'machines' table, and then iterates to update the `tag1` and `tag5` columns, filtering rows by `id` and `datetime`. This helps to simulate data changes and is used to test different update parameters.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/dolphindb_update.md#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\nmachines = loadTable(\"dfs://tsdbDemo\", \"machines\")\nfor(i in 0..20)\n update machines set tag1=i,tag5=i where id in 1..5,date(datetime)=2020.09.01\n```\n\n----------------------------------------\n\nTITLE: Editing Data and Compute Node Configuration Using Shell\nDESCRIPTION: This snippet modifies the cluster.cfg file on data and compute nodes to set hardware resource parameters including memory size, connection limits, worker threads, cache engine sizes, and partition policies. Parameters should be tuned based on server hardware. Additionally, publicName parameters may be added for external web interface access by specifying IP addresses associated with each node alias. This unified configuration applies to all data and compute nodes in the cluster.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ha_cluster_deployment.md#_snippet_8\n\nLANGUAGE: Shell\nCODE:\n```\nvim ./cluster.cfg\n```\n\nLANGUAGE: Shell\nCODE:\n```\nmaxMemSize=32\nmaxConnections=512\nworkerNum=4\nmaxBatchJobWorker=4\nOLAPCacheEngineSize=2\nTSDBCacheEngineSize=2\nnewValuePartitionPolicy=add\nmaxPubConnections=64\nsubExecutors=4\nlanCluster=0\nenableChunkGranularityConfig=true\n```\n\n----------------------------------------\n\nTITLE: Diagnosing Port Binding Failure on DolphinDB Node Startup Using Error Log Example\nDESCRIPTION: This snippet provides an example of an error message logged when a DolphinDB node fails to bind to a port due to the port being already in use (code 98). Troubleshooting involves checking DolphinDB log files and changing node port configurations to unused ports to resolve the startup failure.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/multi_machine_cluster_deployment.md#_snippet_10\n\nLANGUAGE: Log\nCODE:\n```\n<ERROR> :Failed to bind the socket on port 8900 with error code 98\n```\n\n----------------------------------------\n\nTITLE: Calculating Bid Withdraws Factor in Python\nDESCRIPTION: Python implementation of a high-frequency factor (`bid_withdraws`) that calculates the withdrawn bid volume based on level 2 order book snapshots. It uses nested loops to compare current and previous book states (`_bid_withdraws_volume` function) and iterates over time steps. This serves as a reference for the DolphinDB implementation.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Streaming_computing_of_financial_quantifiers.md#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndef _bid_withdraws_volume(l, n, levels=10):\n    withdraws = 0\n    for price_index in range(0,4*levels, 4):\n        now_p = n[price_index]\n        for price_last_index in range(0,4*levels,4):\n            if l[price_last_index] == now_p:\n                withdraws -= min(n[price_index+1] - l[price_last_index + 1], 0)     \n    return withdraws\n\ndef bid_withdraws(depth, trade):\n    ob_values = depth.values\n    flows = np.zeros(len(ob_values))\n    for i in range(1, len(ob_values)):\n        flows[i] = _bid_withdraws_volume(ob_values[i-1], ob_values[i])\n    return pd.Series(flows)\n```\n\n----------------------------------------\n\nTITLE: Replaying Historical Data for Simulation in DolphinDB Script\nDESCRIPTION: Selects historical L2 snapshot data for a specific list of stocks (`stockList`) on a particular date (2020.10.19) during trading hours from a distributed TSDB table (`dfs://snapshot_SH_L2_TSDB_ArrayVector`). It then uses the `replay` function submitted as a background job (`replay_trade`) to feed this historical data into the `snapshotStream` table, simulating a live data feed for testing the streaming pipeline.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/machine_learning_volatility/05.streamComputingArrayVector.txt#_snippet_6\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\n//replay history data, the day is 2020.10.19\nstockList=`601318`600519`600036`600276`601166`600030`600887`600016`601328`601288`600000`600585`601398`600031`601668`600048`601888`600837`601601`601012`603259`601688`600309`601988`601211`600009`600104`600690`601818`600703`600028`601088`600050`601628`601857`601186`600547`601989`601336`600196`603993`601138`601066`601236`601319`603160`600588`601816`601658`600745\ndata = select SecurityID, TradeTime, PreClosePx, OpenPx, HighPx, LowPx, LastPx, TotalVolumeTrade, TotalValueTrade, BidPrice, BidOrderQty, OfferPrice, OfferOrderQty\nfrom loadTable(dbName, tableName) where date(TradeTime)=2020.10.19, SecurityID in stockList, (time(TradeTime) between 09:30:00.000 : 11:29:59.999) || (time(TradeTime) between 13:00:00.000 : 14:56:59.999) \norder by TradeTime, SecurityID\nsubmitJob(\"replay_trade\", \"trade\",  replay{data, snapshotStream, `TradeTime, `TradeTime, 20000, true, 1})\ndata = NULL\ngetRecentJobs()\n```\n\n----------------------------------------\n\nTITLE: Creating Tables and Performing JOIN\nDESCRIPTION: This code prepares two tables in different databases and performs a join operation.  The code sets up the data and then demonstrates a join query, showing how the execution plan reflects the join operation's details.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_Explain.md#_snippet_3\n\nLANGUAGE: DolphinDB SQL\nCODE:\n```\nif(existsDatabase(\"dfs://valuedb\")) dropDatabase(\"dfs://valuedb\");\nif(existsDatabase(\"dfs://valuedb1\")) dropDatabase(\"dfs://valuedb1\");\n// valuedb 数据库\nn=1000000\nmonth=take(2000.01M..2016.12M, n)\nx=rand(1.0, n)\nid = rand(1..9,n)\nt1=table(month, x, id)\ndb1=database(\"dfs://valuedb\", VALUE, 2000.01M..2016.12M)\n\npt1 = db1.createPartitionedTable(t1, `pt, `month).append!(t1);\n\n// valuedb1 数据库\nid = rand(1..10,n)\ntimes = now()+ 1..n\nvals = take(1..20,n) + rand(1.0,n)\nt2 = table(id,times,vals)\ndb2=database(\"dfs://valuedb1\",VALUE, 1..10);\npt2=db2.createPartitionedTable(t2, `pt, `id).append!(t2);\n```\n\nLANGUAGE: DolphinDB SQL\nCODE:\n```\n// join 查询\nselect [HINT_EXPLAIN] times,vals,month from lsj(loadTable(\"dfs://valuedb\",`pt),loadTable(\"dfs://valuedb1\",`pt),`id)\n```\n\n----------------------------------------\n\nTITLE: Checking DolphinDB Version After Upgrade Linux\nDESCRIPTION: This code snippet allows the user to verify the DolphinDB version after an upgrade is complete. It accesses the version() function via the Web UI's interactive programming interface.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/standalone_server.md#_snippet_17\n\nLANGUAGE: sh\nCODE:\n```\nversion()\n```\n\n----------------------------------------\n\nTITLE: Creating Partitioned Transaction Table in DolphinDB\nDESCRIPTION: This function creates a composite partitioned database for transaction data with temporal and hash partitioning. It defines a schema with financial transaction fields and creates a partitioned table with specified sort columns and compression methods for date/time fields.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/appendices_market_replay_bp/transac_create.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef transacCreate(dbName, tbName)\n{\n    if(existsDatabase(dbName))\n    {\n\t    dropDatabase(dbName)\n    }\n\n    db1 = database(, VALUE, 2021.12.01..2021.12.31)\n    db2 = database(, HASH, [SYMBOL, 25])\n    db = database(dbName, COMPO, [db1, db2], , 'TSDB')\n\n    schemaTable = table(\n        array(SYMBOL, 0) as SecurityID,\n        array(DATE, 0) as MDDate,\n        array(TIME, 0) as MDTime,\n        array(TIMESTAMP, 0) as DataTimestamp,\n        array(SYMBOL, 0) as SecurityIDSource,\n        array(SYMBOL, 0) as SecurityType,\n        array(LONG, 0) as TradeIndex,\n        array(LONG, 0) as TradeBuyNo,\n        array(LONG, 0) as TradeSellNo,\n        array(INT, 0) as TradeType,\n        array(INT, 0) as TradeBSFlag,\n        array(LONG, 0) as TradePrice,\n        array(LONG, 0) as TradeQty,\n        array(LONG, 0) as TradeMoney,\n        array(LONG, 0) as ApplSeqNum,\n        array(INT, 0) as ChannelNo,\n        array(DATE, 0) as ExchangeDate,\n        array(TIME, 0) as Exchanime\n    )\n\n    db.createPartitionedTable(table=schemaTable, tableName=tbName, partitionColumns=`MDDate`SecurityID, sortColumns=`SecurityID`MDTime, keepDuplicates=ALL,compressMethods={MDDate:\"delta\", MDTime:\"delta\",DataTimestamp:\"delta\",ExchangeDate:\"delta\",Exchanime:\"delta\"})\n}\n```\n\n----------------------------------------\n\nTITLE: Querying - Range Query (Partition & Non-Partition) DolphinDB\nDESCRIPTION: This snippet performs a range query that uses both partitioned and non-partitioned dimensions. It selects records within a specified time range, for specific device IDs, and with filtering based on battery level and status. This demonstrates querying across multiple conditions and data partitions.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs TimescaleDB/test_dolphindb_small.txt#_snippet_14\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 5. 范围查询.分区及非分区维度：查询某时间段内某些设备的特定记录\ntimer\nselect *\nfrom readings\nwhere\n\ttime between 2016.11.15 20:00:00 : 2016.11.16 22:30:00,\n\tdevice_id in ['demo000001', 'demo000010', 'demo000100', 'demo001000'],\n    battery_level <= 10,\n    battery_status = 'discharging'\n```\n\n----------------------------------------\n\nTITLE: Creating Partitioned Table with Single Value Partitioning by TradeDate in DolphinDB Script\nDESCRIPTION: Defines a database partitioned only by date range, then creates a partitioned table with specified stock columns partitioned by TradeDate. This configuration leads to large partition sizes which can decrease concurrency and system utilization efficiency as demonstrated by the example in section 2.5.1. Dependencies require DolphinDB and defining a valid date range. Input is stock market columns and date partitioning; output is a large single partition per day table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/database_and_table_creation_details.md#_snippet_9\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//按交易日期分区\ndb= database(\"dfs://testDB1\", VALUE, 2020.01.01..2021.01.01)\ncolName = `SecurityID`TradeDate`TradeTime`TradePrice`TradeQty`TradeAmount`BuyNo`SellNo\ncolType = `SYMBOL`DATE`TIME`DOUBLE`INT`DOUBLE`INT`INT\ntbSchema = table(1:0, colName, colType)\n//每天只有 1 个分区，假设每天数据量为 3GB（内存中），每个分区大小约为 3GB，分区粒度过大\ndb.createPartitionedTable(table=tbSchema, tableName=`pt, partitionColumns=`TradeDate)\n```\n\n----------------------------------------\n\nTITLE: Performing a Scrolled Search in Elasticsearch using elasticsearch-py\nDESCRIPTION: This function demonstrates how to perform a scrolled search using the `elasticsearch` Python client. It searches the 'hundred' index, filtering documents by a specific DATE and TIME range, retrieves results in batches using the scroll API, and prints the scroll ID and size for each batch.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_61\n\nLANGUAGE: python\nCODE:\n```\nfrom elasticsearch import Elasticsearch\n\ndef search_1():\n    es = Elasticsearch(['http://localhost:9200/'])\n    page = es.search(\n        index='hundred',\n        doc_type='type',\n        scroll='2m',\n        size=10000,\n        body={\n            \"query\": {\n                \"constant_score\": {\n                    \"filter\": {\n                        \"bool\": {\n                            \"must\": [\n                                {\n                                    \"term\": {\n                                        \"DATE\": \"20070806\"\n                                    }\n\n                                },\n                                {\n                                    \"range\": {\n                                        \"TIME\": {\n                                            \"gte\": \"09:45:01\",\n                                            \"lte\": \"09:45:59\"\n                                        }\n                                    }\n                                }\n                            ]\n                        }\n                    }\n                }\n            }\n        }\n    )\n    sid = page['_scroll_id']\n    scroll_size = page['hits']['total']\n\n    print(sid)\n    print(scroll_size)\n    # Start scrolling\n    while (scroll_size > 0):\n        print(\"Scrolling...\")\n        page = es.scroll(scroll_id=sid, scroll='2m')\n        # Update the scroll ID\n        sid = page['_scroll_id']\n        # Get the number of results that we returned in the last scroll\n        scroll_size = len(page['hits']['hits'])\n        print(\"scroll size: \" + str(scroll_size))\n```\n\n----------------------------------------\n\nTITLE: Setting the LD_LIBRARY_PATH Environment Variable\nDESCRIPTION: This command sets the `LD_LIBRARY_PATH` environment variable, which specifies the directories where the system should search for shared libraries at runtime.  It's used to resolve 'not found' errors for shared libraries. `${LIB_PATH}` should be replaced with the actual path to the directory containing the missing library.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ODBC_plugin_user_guide.md#_snippet_5\n\nLANGUAGE: Shell\nCODE:\n```\nexport LD_LIBRARY_PATH=${LIB_PATH}:$LD_LIBRARY_PATH //${LIB_PATH}修改为依赖库所在的文件夹路径\n```\n\n----------------------------------------\n\nTITLE: Batch Write to DolphinDB with Java API\nDESCRIPTION: This Java snippet demonstrates batch writing data to a DolphinDB table using the Java API. It constructs `BasicVector` objects for each column (symbol, datetime, bid, ofr, bidSize, ofrSize, mode, ex, mmid) and then uses `conn.run(\"tableInsert{\\\"timeTest\\\"}\", data)` to insert the data into the table. Assumes variables `symbolList`, `dtList`, `bidList` etc. are already populated with data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/timezone.md#_snippet_6\n\nLANGUAGE: Java\nCODE:\n```\npublic static void write() throws IOException {\n    // 构建list，省略部分列\n    LinkedList<String> symbolList = new LinkedList<>(); \n    try (Reader reader = Files.newBufferedReader(Paths.get(CSV));\n        // 读取csv，插入到list中\n    } catch (IOException | CsvValidationException ex) {\n        ex.printStackTrace();\n    }\n\n    List<Entity> data = Arrays.asList(\n        new BasicSymbolVector(symbolList),\n        new BasicDateTimeVector(dtList),\n        new BasicDoubleVector(bidList),\n        new BasicDoubleVector(ofrList),\n        new BasicLongVector(bidSizeList),\n        new BasicLongVector(ofrSizeList),\n        new BasicIntVector(modeList),\n        new BasicByteVector(exList),\n        new BasicSymbolVector(mmidList)\n    );\n    conn.run(\"tableInsert{\\\"timeTest\\\"}\", data);\n}\n```\n\n----------------------------------------\n\nTITLE: Creating and Persisting DolphinDB Stream Table for CPU Metrics in DolphinDB Language\nDESCRIPTION: DolphinDB script to log in as admin, clear cache, and create a streaming table named 'cpu_stream' for CPU metrics with fields timestamp (TIMESTAMP), cpu (STRING), and usage_idle (DOUBLE). The streaming table is enabled for sharing and persistence with a specified cache size, supporting efficient stream data processing and durable storage. Dependencies: DolphinDB server running and accessible, admin credentials. Inputs: None. Outputs: persistent stream table ready to ingest CPU metric data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_Telegraf_Grafana.md#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//登录\nlogin(`admin,`123456)\n\n//清理缓存\nundef(all)\nclearAllCache()\n\n//持久化流表 cpu_stream\ncpuColnames = `timestamp`cpu`usage_idle\ncpuColtypes =[TIMESTAMP,STRING,DOUBLE]\nenableTableShareAndPersistence(table = streamTable(1000:0,cpuColnames,cpuColtypes), tableName=`cpu_stream, cacheSize = 5000000)  \n```\n\n----------------------------------------\n\nTITLE: Parallel Job Definition and Data Loading in DolphinDB\nDESCRIPTION: Defines 'parJob1' as a parallel job function that loads data tables from distributed file system, performs joins and data cleaning, splits symbol list to chunks of 250 symbols, and launches parallel execution of factor calculations using 'getFactor'. It includes timer to measure execution time. This function orchestrates data preparation and parallel computation of factors for large datasets.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_factor_by_ddb/fund_factor_nine.txt#_snippet_11\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef parJob1(){\n\ttimer{fund_OLAP=select * from loadTable(\"dfs://fund_OLAP\", \"fund_OLAP\")\n\t\t  fund_hs_OLAP=select * from loadTable(\"dfs://fund_OLAP\", \"fund_hs_OLAP\")\n\t\t  ajResult = select Tradedate, fundNum, value, fund_hs_OLAP.Tradedate as hsTradedate, fund_hs_OLAP.value as price from aj(fund_OLAP, fund_hs_OLAP, `Tradedate)\n\t\t  result2 = select Tradedate, fundNum, iif(isNull(value), ffill!(value), value) as value,price from ajResult where Tradedate == hsTradedate\n\t\t  symList = exec distinct(fundNum) as fundNum from result2 order by fundNum\n\t\t  symList2 = symList.cut(250)//此处，将任务切分，按每次250个不同关键数据进行计算\n\t\t  ploop(getFactor{result2}, symList2)}\n}\n```\n\n----------------------------------------\n\nTITLE: Elasticsearch Grouping by Stock Code (Multiple Aggregations) - Python\nDESCRIPTION: This Python function performs an aggregation query on Elasticsearch, grouping documents by 'ts_code' and calculating both the average 'low' price and the sum of 'high' prices for each group. It uses `urllib3` to make a GET request to the Elasticsearch API with a JSON payload defining the aggregations. The function outputs the HTTP status code and the parsed JSON response.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_31\n\nLANGUAGE: Python\nCODE:\n```\ndef search_6():\n    http = urllib3.PoolManager()\n    data = json.dumps({\n        \"aggs\": {\n            \"group_by_ts_code\": {\n                \"terms\": {\n                    \"field\": \"ts_code\",\n                    \"size\": 5000  # 跟这个size有关，是否精确\n                },\n                \"aggs\": {\n                    \"avg_low\": {\n                        \"avg\": {\"field\": \"low\"}\n                    },\n                    \"sum_high\": {\n                        \"sum\": {\"field\": \"high\"}\n                    }\n                }\n            }\n        },\n        \"_source\": [\"\"]\n    }).encode(\"utf-8\")\n    r = http.request(\"GET\", \"http://localhost:9200/elastic/_search\", body=data,\n                     headers={'Content-Type': 'application/json'})\n    print(r.status)\n    print(json.loads(r.data.decode()))\n```\n\n----------------------------------------\n\nTITLE: Creating Partitioned Tables in Composite and TSDB Databases - DolphinDB Script\nDESCRIPTION: This code defines a table schema for quotes and creates three partitioned tables (two in COMPO, one in TSDB) using the previously defined databases. Dependencies include initialized databases created above. Key parameters include the partition columns (`symbol`, `source`, `time`) and table schema. No explicit input is required at this step; output consists of partitioned tables available to store data. The TSDB version includes a secondary time-based ordering on partition.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/buildData.txt#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//Build partitioned tables in testdb and testdb_tsdb\nt=table(100:0, `time`symbol`price`volume`side`source, [TIMESTAMP,SYMBOL,FLOAT,FLOAT,SYMBOL,SYMBOL])\ndb.createPartitionedTable(t,`quotes,`symbol`source`time)\ndb.createPartitionedTable(t,`quotes_2,`symbol`source`time)\ndbTSDB.createPartitionedTable(t,`quotes_tsdb,`symbol`source`time, ,`symbol`time)\n```\n\n----------------------------------------\n\nTITLE: Downloading DolphinDB Server Using Shell on Linux\nDESCRIPTION: These shell commands demonstrate different ways to download DolphinDB server packages via wget on Linux. The '${release}' variable is replaced by the target version number, supporting standard, ABI, and JIT variants. Extraction of the downloaded zip file is also shown using unzip, specifying a destination directory. The code requires a Linux environment with wget and unzip installed, and the downloaded file is saved as dolphindb.zip and extracted to a target path. Note that the installation path must not contain spaces or Chinese characters to avoid failures when starting data nodes.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/single_machine_cluster_deploy.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\nwget https://www.dolphindb.cn/downloads/DolphinDB_Linux64_V${release}.zip -O dolphindb.zip\n```\n\nLANGUAGE: Shell\nCODE:\n```\nwget https://www.dolphindb.cn/downloads/DolphinDB_Linux64_V2.00.11.3.zip -O dolphindb.zip\n```\n\nLANGUAGE: Shell\nCODE:\n```\nwget https://www.dolphindb.cn/downloads/DolphinDB_Linux64_V2.00.11.3_ABI.zip -O dolphindb.zip\n```\n\nLANGUAGE: Shell\nCODE:\n```\nwget https://www.dolphindb.cn/downloads/DolphinDB_Linux64_V2.00.11.3_JIT.zip -O dolphindb.zip\n```\n\nLANGUAGE: Shell\nCODE:\n```\nunzip dolphindb.zip -d </path/to/directory>\n```\n\n----------------------------------------\n\nTITLE: Loading Tables (Metadata and Data) DolphinDB\nDESCRIPTION: This set of snippets loads the `readings` table in different ways. First it loads the metadata only using `loadTable`, and then it loads the data to memory and times each operation to measure loading time.  This allows for an understanding of how long it takes to load just the schema vs the full table contents.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs TimescaleDB/test_dolphindb_small.txt#_snippet_8\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// ----------------- 加载 readings 表\n// 加载磁盘分区表（仅加载元数据）\ntimer readings = loadTable(FP_DB, `readings)\n// 106 ms\n\n\n// 加载为内存表\ntimer readings = loadTable(FP_DB, `readings, , true)\n// 13.8 s\n```\n\n----------------------------------------\n\nTITLE: Parse CSV File with OpenCSV in Java\nDESCRIPTION: This Java snippet uses the OpenCSV library to parse a CSV file. It skips the first line (header) and iterates through each record, extracting the values from each field. Assumes OpenCSV dependency is added.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/timezone.md#_snippet_4\n\nLANGUAGE: Java\nCODE:\n```\nCSVReader csvReader = new CSVReader(reader) \nString[] record;\ncsvReader.readNext();// skip first line\nwhile ((record = csvReader.readNext()) != null) {\n    // record[i]对应某一个行的某一个字段，此处完整代码请参考附录\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Single Recipient Email in DolphinDB\nDESCRIPTION: Defines a single recipient email address as a string variable that will be used as the destination for the email.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/send_messages_external_systems.md#_snippet_17\n\nLANGUAGE: dolphindb\nCODE:\n```\nrecipient='Maildestination@xxx.com';\n```\n\n----------------------------------------\n\nTITLE: Finding Minimum Price per Timestamp using Matrix Row Aggregation in DolphinDB Script\nDESCRIPTION: Uses the `rowMin()` function on the price matrix (assuming `price` matrix is already created). This performs a row-wise aggregation, returning a vector containing the minimum price found in each row (i.e., the minimum price across all symbols for each timestamp).\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/panel_data.md#_snippet_17\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nrowMin(price);\n```\n\n----------------------------------------\n\nTITLE: Creating Tables in DolphinDB C++ Plugin\nDESCRIPTION: This snippet demonstrates two methods for creating tables in a DolphinDB C++ plugin using the `Util::createTable` function: one using column names and types, and the other using column names and vectors.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_advance.md#_snippet_5\n\nLANGUAGE: c++\nCODE:\n```\n//方法一\nstd::vector<std::string> colNames{\"col1\", \"col2\", \"col3\"};         // 存放列名\nstd::vector<DATA_TYPE> colTypes{DT_INT, DT_BOOL, DT_STRING};       // 存放列类型\n//创建一张包含三列的表，列名为col1, col2, col3, 列类型分别为int, bool, string的0行、容量为100的空表\nTableSP t1= Util::createTable(colNames, colTypes, 0, 100);   \n\n//方法二\nVectorSP v1 = Util::createIndexVector(0, 5);                       // 相当于 0..4\nVectorSP v2 = Util::createRepeatingVector(new String(\"Demo\"), 5);  // 创建一个长度为5，所有数据为\"Demo\"的向量\nVectorSP v3 = Util::createRepeatingVector(new Double(2.5), 5);     // 创建一个长度为5，所有数据为2.5的向量\nstd::vector<ConstantSP> columns;                                   // 存放列向量\n//添加列向量\ncolumns.emplace_back(v1);\ncolumns.emplace_back(v2);\ncolumns.emplace_back(v3);\n//用上述创建的列名vector和列向量vector来创建table对象\nTableSP t2 = Util::createTable(colNames, columns;           \n```\n\n----------------------------------------\n\nTITLE: Querying 10-Minute Price Change Rank for Grafana in DolphinDB\nDESCRIPTION: DolphinDB SQL query designed for use in Grafana. Selects the SecurityID, DateTime, and the 10-minute price change factor (`factor_10min`) from the `changeCrossSectionalTable`. Results are ordered by the pre-calculated 10-minute rank (`rank_10min`) for visualization.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Real-Time_Stock_Price_Increase_Calculation.md#_snippet_9\n\nLANGUAGE: DolphinDB (SQL)\nCODE:\n```\nselect SecurityID, DateTime, factor_10min from changeCrossSectionalTable order by rank_10min\n```\n\n----------------------------------------\n\nTITLE: Starting Controller Services on All Cluster Nodes\nDESCRIPTION: Runs shell script 'startController.sh' on each cluster machine to initiate controller processes, enabling the cluster to operate in managed mode.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/service_deployment_and_migration.md#_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\nsh startController.sh\n```\n\n----------------------------------------\n\nTITLE: Querying Factor Results - DolphinDB\nDESCRIPTION: Queries the `factors` table to retrieve the top 10 rows where the factor is valid. This allows verification of the calculated factors.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/func_progr_cases.md#_snippet_16\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect top 10 * from factors where isValid(factor)\n```\n\n----------------------------------------\n\nTITLE: Comparing Performance of moving(sum) and Optimized msum Functions in DolphinDB\nDESCRIPTION: Compares the execution time of 'moving(sum)' versus the optimized cumulative sum function 'msum' on a large numeric vector of 1 million elements in DolphinDB. Results show 'msum' is 50 to 200 times faster due to incremental calculation and superior memory management over 'moving(sum)', which recalculates entire windows and incurs frequent memory allocations and deallocations. Highlights best practice to use DolphinDB's specialized moving aggregation operators (msum, mavg, etc.) when possible for significant performance gains.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/func_progr_cases.md#_snippet_8\n\nLANGUAGE: DolphinDB\nCODE:\n```\nx=1..1000000\ntimer moving(sum, x, 10)\ntimer msum(x, 10)\n```\n\n----------------------------------------\n\nTITLE: Filtering Columns for Import in DolphinDB\nDESCRIPTION: This code snippet first assigns a unique column number to each field name in the schema table. Then, it removes the '方向' (direction) column from the schema table. This column is not required for import into the distributed table. The `rowNo` function generates the column numbers, and `delete from` removes the specified row.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/stockdata_csv_import_demo.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\n\tupdate schema1 set col = rowNo(name)\n\tdelete from schema1 where name in [`方向]\n```\n\n----------------------------------------\n\nTITLE: Querying data using DolphinDB Python API\nDESCRIPTION: Python script that connects to DolphinDB server and retrieves data using SQL queries through the API, with performance time measurement for both Level 1 and Level 2 datasets.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_pickle_comparison.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\nimport pandas as pd\nimport time\n\ns = ddb.session()\ns.connect(\"192.168.1.13\",22172, \"admin\", \"123456\")\n\n＃读取Level 1数据集一天的数据\nst1 = time.time()\nquotes =s.run('''\nselect * from loadTable(\"dfs://TAQ\", \"quotes\") where TradeDate = 2007.08.23\n''')\net1 = time.time()\nprint(et1 - st1)\n\n＃读取Level 2数据集一天的数据\nst = time.time()\ntick = s.run('''\nselect * from loadTable(\"dfs://DataYesDB\", \"tick\") where TradeDate = 2019.09.10\n''')\net = time.time()\nprint(et-st)\n```\n\n----------------------------------------\n\nTITLE: Elasticsearch Query, Filtering by Stock Code\nDESCRIPTION: This Python code defines an Elasticsearch search query with filtering by a range of 'PERMNO' values. The query utilizes the `elasticsearch` library, sets up a scroll operation for retrieving results, and prints the scroll ID and scroll size. It fetches results in batches, continuing until the entire result set is retrieved. Requires the uscsv index to be present and populated in Elasticsearch.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_48\n\nLANGUAGE: Python\nCODE:\n```\ndef search_2():\n    es = Elasticsearch(['http://localhost:9200/'])\n    page = es.search(\n        index='uscsv',\n        doc_type='type',\n        scroll='2m',\n        size=10000,\n        body={\n            \"query\": {\n                \"constant_score\": {\n                    \"filter\": {\n                        \"range\": {\n                            \"PERMNO\": {\n                                \"gte\": 23230,\n                                \"lte\": 30000\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    )\n    sid = page['_scroll_id']\n    scroll_size = page['hits']['total']\n\n    print(sid)\n    print(scroll_size)\n    while (scroll_size > 0):\n        print(\"Scrolling...\")\n        page = es.scroll(scroll_id=sid, scroll='2m')\n        sid = page['_scroll_id']\n        scroll_size = len(page['hits']['hits'])\n        print(\"scroll size: \" + str(scroll_size))\n```\n\n----------------------------------------\n\nTITLE: Parsing and Formatting Time in DolphinDB Shell\nDESCRIPTION: These examples demonstrate how to parse strings into DolphinDB time series data and format time series data into strings. The `temporalParse` function converts string representations of dates and times into DolphinDB's internal time series types, while `temporalFormat` converts DolphinDB's time series types into formatted strings.  They rely on format strings to handle input and output formats.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/date_time.md#_snippet_6\n\nLANGUAGE: Shell\nCODE:\n```\n>temporalParse(\"14-02-2018\",\"dd-MM-yyyy\");\n2018.02.14\n>temporalParse(\"14-02-18\",\"d-M-y\");\n2018.02.14\n>temporalParse(\"2018/2/6 02:33:01 PM\",\"y/M/d h:m:s a\");\n2018.02.06T14:33:01\n>temporalFormat(2018.02.14,\"dd-MM-yyyy\");\n14-02-2018\n>temporalFormat(2018.02.14,\"dd-MMM-yy\");\n14-FEB-18\n>temporalFormat(2018.02.06T13:30:10.001, \"y-M-d-H-m-s-SSS\");\n2018-2-6-13-30-10-001\n```\n\n----------------------------------------\n\nTITLE: Setting System-wide File Descriptor Limits via sysctl.conf (console)\nDESCRIPTION: Increase the global system file descriptor limit by editing `/etc/sysctl.conf`. This is required if the default is below the DolphinDB recommended threshold (102400). It ensures that the system can handle large concurrent workloads across all applications.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/deploy_dolphindb_on_new_server.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\n# vi /etc/sysctl.conf\nfs.file-max = 102400\n\n```\n\n----------------------------------------\n\nTITLE: Custom Function Application with byRow\nDESCRIPTION: Applies custom functions to each row of a Fast Array Vector using the `byRow` function in DolphinDB. The example defines two custom functions (`foo1` and `foo2`) that return a scalar and a vector, respectively.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Array_Vector.md#_snippet_16\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 返回值是标量\ndefg foo1(v){\n    return last(v)-first(v)\n}\n\n// 返回值是和 v 等长的向量\ndef foo2(v){\n    return v \\ prev(v) - 1\n}\n\nx = array(INT[], 0).append!([1 2 3, 4 5, 6 7 8, 9 10])\nx1 = byRow(foo1, x)\nx2 = byRow(foo2, x)\n/*\nx1: [2,1,2,1]\nx2: [[,1,0.5],[,0.25],[,0.166666666666667,0.142857142857143],[,0.111111111111111]]\n*/\n\nt = table(1 2 3 4 as id, x as x)\nnew_t = select *, byRow(foo1, x) as x1, byRow(foo2, x) as x2 from t\n/* new_t\nid x       x1 x2                                    \n-- ------- -- --------------------------------------\n1  [1,2,3] 2  [,1,0.5]                              \n2  [4,5]   1  [,0.25]                               \n3  [6,7,8] 2  [,0.166666666666667,0.142857142857143]\n4  [9,10]  1  [,0.111111111111111]                  \n*/\n```\n\n----------------------------------------\n\nTITLE: Replaying Historical Snapshot Data in DolphinDB\nDESCRIPTION: Loads historical snapshot data for a specific date (2021.12.01) from the specified DolphinDB table (`dfs://snapshot`, `snapshot`), sorts it by `DateTime`, and submits a background job (`submitJob`) to replay this data (`t`) into the `snapshotStreamTable`. The replay uses the original `DateTime` for both time columns and injects data at a rate of 1000 messages per second. `getRecentJobs()` is called to check the status of submitted jobs.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Real-Time_Stock_Price_Increase_Calculation.md#_snippet_10\n\nLANGUAGE: DolphinDB\nCODE:\n```\nt = select * from loadTable(\"dfs://snapshot\", \"snapshot\") where date(DateTime)=2021.12.01 order by DateTime\nsubmitJob(\"replay_snapshot\", \"snapshot\",  replay{t, snapshotStreamTable, `DateTime, `DateTime, 1000, true, 1})\ngetRecentJobs()\n```\n\n----------------------------------------\n\nTITLE: Submitting Historical Data Replay Job (DolphinDB Script)\nDESCRIPTION: Submits a background job to replay the historical data loaded into `testSnapshot` into the `snapshotStream` table. This simulates real-time data arrival, triggering the downstream streaming engines and handlers. Replay speed is set to 20000 times real speed.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/metacode_derived_features/metacode_derived_features_streaming.txt#_snippet_17\n\nLANGUAGE: DolphinDB\nCODE:\n```\nsubmitJob(\"replay\", \"replay\",  replay{testSnapshot, snapshotStream, `DateTime, `DateTime, 20000, true, 1})\n```\n\n----------------------------------------\n\nTITLE: Listing Directory Contents After Updates (keepDuplicates=LAST)\nDESCRIPTION: This snippet displays the file structure of a TSDB table after updates where keepDuplicates is set to LAST. The output demonstrates the addition of new files within the directory after updates. Specifically, the example showcases the changes in data storage and the append-only characteristics of this configuration.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/dolphindb_update.md#_snippet_6\n\nLANGUAGE: console\nCODE:\n```\n$ tree\n.\n├── chunk.dict\n└── machines_2\n    ├── 0_00000010\n    ├── 0_00000011\n    ├── 0_00000012\n    ├── 0_00000013\n    ├── 0_00000014\n    ├── 0_00000241\n    ├── 0_00000243\n     ...\n    ├── 1_00000002\n    ├── 1_00000050\n    └── 1_00000051\n\n1 directory, 21 files\n```\n\n----------------------------------------\n\nTITLE: Transforming Data Types in DolphinDB Mutable Table\nDESCRIPTION: Defines a function `transForm` that accepts a mutable DolphinDB table (`msg`). It modifies the table in-place by converting the `TradeQty`, `BuyNo`, `SellNo`, `ChannelNo`, `TradeIndex`, and `BizIndex` columns to the INT data type using the `replaceColumn!` function. The function returns the modified table. This is typically used as a callback function during data ingestion processes.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/Oracle_to_DolphinDB/迁移.txt#_snippet_0\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\ndef transForm(mutable msg){\n\tmsg.replaceColumn!(`TradeQty, int(msg[`TradeQty]))\n\tmsg.replaceColumn!(`BuyNo, int(msg[`BuyNo]))\n\tmsg.replaceColumn!(`SellNo, int(msg[`SellNo]))\n\tmsg.replaceColumn!(`ChannelNo, int(msg[`ChannelNo]))\n\tmsg.replaceColumn!(`TradeIndex, int(msg[`TradeIndex]))\n\tmsg.replaceColumn!(`BizIndex, int(msg[`BizIndex]))\n\treturn msg\n}\n```\n\n----------------------------------------\n\nTITLE: Importing Data via ODBC in DolphinDB\nDESCRIPTION: Illustrates how to import data from an external database (MySQL) into DolphinDB using the ODBC plugin. The code establishes a connection to MySQL, executes a query, and appends the retrieved data to a DolphinDB partitioned table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/database.md#_snippet_9\n\nLANGUAGE: DolphinDB\nCODE:\n```\nloadPlugin(\"plugins/odbc/odbc.cfg\")\nconn=odbc::connect(\"Driver=MySQL;Data Source = mysql-stock;server=127.0.0.1;uid=[xxx];pwd=[xxx];database=stockDB\")\nt=odbc::query(conn,\"select * from quotes\")\nloadTable(\"dfs://stockDB\", \"quotes\").append!(t)\n```\n\n----------------------------------------\n\nTITLE: Creating access token via HTTP GET request in DolphinDB\nDESCRIPTION: This snippet shows how to use DolphinDB's httpClient::httpGet function to request an access_token from DingTalk API, providing appkey and appsecret as parameters. It retrieves the token, parses the response, and makes it available for subsequent API calls.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/send_messages_external_systems.md#_snippet_11\n\nLANGUAGE: DolphinDB\nCODE:\n```\nurl = 'https://oapi.dingtalk.com/gettoken';\nparam=dict(string,string);\nkey='xxxxx';\nsecret='xxxxx';\nparam['appkey']=key;\nparam['appsecret']=secret;\nret=httpClient::httpGet(url,param,1000);\nprint ret['text'];\nbody = parseExpr(ret.text).eval();\nACCESS_TOKEN=body.access_token;\nERRCODE=body.errcode;\n```\n\n----------------------------------------\n\nTITLE: Adding a Column using addCol function\nDESCRIPTION: The `addCol` function is designed to add a new column to the memory table during the data loading process. It updates the table by setting the new column's value to the provided `datePara`, which can be used to insert missing columns. The function returns the modified table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/LoadDataForPoc.md#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef addCol(mutable memTable,datePara)\n{\n\tupdate memTable set date = datePara\n\treturn memTable\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Option Close Price Matrix using Panel (DolphinDB Script)\nDESCRIPTION: Transforms the narrow table `data` into a wide matrix `closPriceWideMatrix` using the `panel` function. Rows are indexed by option contract codes (`data.codes`), columns by trade dates (`data.tradeDate`), and the matrix cells contain the corresponding closing prices (`data.closePrice`). This prepares the data for time-series or cross-sectional analysis.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/IV_Greeks_Calculation_for_ETF_Options_Using_JIT.md#_snippet_2\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nclosPriceWideMatrix = panel(data.codes, data.tradeDate, data.closePrice)\n```\n\n----------------------------------------\n\nTITLE: Defining Door Record Stream Table Schema - DolphinDB\nDESCRIPTION: This function defines the schema for the input stream table, `doorRecord`, which will store door access event data. It includes columns for event type, code, timestamp, direction, device SN, door number, and card ID, along with Chinese comments describing their purpose and potential values.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/streaming_engine_anomaly_alerts/流计算引擎级联检测门禁超时数据.txt#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef createDoorRecordStreamTable(tableName){\n\tst=streamTable(\n\t\tarray(INT,0) as recordType,//记录类型  0:读卡记录,1:按鋕记录，2：门磁记录 3: 软件记录 4: 报警记录 5: 系统记录\n        array(INT,0) as doorEventCode,\n        //事件码 11:合法开门 12:密码开门 56 :  按钮开门 60: 开门 61关门 64: 门未关好 65: 软件开门 66:软件关门 67:软件常开 78: 门磁报警 81:匪警报警 84:消防报警 90: 胁迫报警 94: 烟雾报警 97:防盗报警100: 黑名单报警103: 开门超时报警\n       \tarray(DATETIME,0) as eventDate , //事件时间 \n    \tarray(BOOL,0) as readerType   , //进出类型 1:入 0:出\n   \t\tarray(SYMBOL,0) as sn, //设备SN maxLength: 30\n       \tarray(INT,0) as doorNum, //门号 0-4\n    \tarray(SYMBOL,0) as card //卡号maxLength 20                \n\t)\n\tenableTableShareAndPersistence(st,tableName, false, true, 100000,100,0);\n}\n```\n\n----------------------------------------\n\nTITLE: Loading Backup Data for a Specific Partition Using loadBackup Function\nDESCRIPTION: Loads data from a backup of a specified distributed table partition using the loadBackup function. It returns the data rows contained in the backup files, allowing examination or further processing of the backed up data before full restore. Inputs include the backup directory, database path, partition specification, and the table name. Used primarily for backup validation, partial recovery, or data analysis of backup snapshots.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/restore-backup.md#_snippet_8\n\nLANGUAGE: DolphinDB\nCODE:\n```\nloadBackup(\"/hdd/hdd1/backup/\",\"dfs://ddb\",\"/20200103/10\",\"windTurbine\")\n```\n\n----------------------------------------\n\nTITLE: Data Loading and Setup for Option Pricing\nDESCRIPTION: This snippet loads ETF data, option prices, contract information, and trading dates from DolphinDB tables and text files. It prepares datasets required for subsequent calculations and performance tests.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/IV_Greeks_Calculation_for_ETF_Options_Using_JIT/calculation_scripts.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndata = select * from loadTable(\"dfs://optionPrice\", \"optionPrice\") where sym =`510050\n\nclosPriceWideMatrix = panel(data.codes, data.tradeDate, data.closePrice)\n\netfPriceWideMatrix = panel(data.codes, data.tradeDate, data.etfprice)\n\ncontractInfo = select * from loadTable(\"dfs://optionInfo\", \"optionInfo\") where sym =`510050\n\ntradingDatesAbsoluteFilename = \"/hdd/hdd9/tutorials/jitAccelerated/tradedate.csv\"\nstartDate = 2015.02.01\nendDate = 2022.03.01\nallTradingDates = loadText(tradingDatesAbsoluteFilename)\ntradingDates = exec tradedate from allTradingDates where tradedate<endDate and tradedate >startDate\n```\n\n----------------------------------------\n\nTITLE: Loading Text Data with Specified Schema\nDESCRIPTION: This code loads text data from a CSV file while providing a schema that explicitly defines the data types of each column. The `schemaTB` table specifies that `price1` should be an INT and `price2` should be a DOUBLE, which influences how DolphinDB parses the data during loading, ignoring non-numeric characters and converting non-numeric values to NULL.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_csv.md#_snippet_39\n\nLANGUAGE: DolphinDB\nCODE:\n```\nschemaTB=table(`id`price1`price2`total as name, `INT`INT`DOUBLE`DOUBLE as type) \ntmpTB=loadText(dataFilePath,,schemaTB)\ntmpTB;\n```\n\n----------------------------------------\n\nTITLE: Querying Data and Warming Up Stream Engine in DolphinDB\nDESCRIPTION: This snippet first queries data from a specified DolphinDB table, filtering it based on date, SecurityID, and time.  It then warms up a stream engine called 'calChange' by feeding it data where the time is before 09:30:00.000. The query selects all columns from the table `t` which matches the criteria, and the `warmupStreamEngine` function is used to initialize the engine with the `warmupData` for potential future operations. Dependencies include an existing DolphinDB database and table with a DateTime and SecurityID column.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/Real-Time_Stock_Price_Increase_Calculation/05.性能测试.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndbName, tbName = \"dfs://SH_TSDB_snapshot_ArrayVector\", \"snapshot\"\nt = select * from loadTable(dbName, tbName) where date(DateTime)=2021.12.01 and left(SecurityID, 2)=\"60\" and time(DateTime)>=09:25:00.000 order by DateTime\n\n// warm up\nwarmupData = select * from t where time(DateTime) < 09:30:00.000\nwarmupStreamEngine(getStreamEngine(\"calChange\"), warmupData)\n```\n\n----------------------------------------\n\nTITLE: Sliding Window Aggregation with Rolling Function in DolphinDB SQL\nDESCRIPTION: Uses the rolling high-order function to compute aggregation with a window length of 6 rows and a step size of 3 rows. It creates a table with irregular timestamp steps and volume data and calculates the last timestamp and sum of volume in the rolling window. Unlike interval functions, rolling does not interpolate missing data and omits windows with fewer elements than the window size. Outputs results only for windows that contain full data. Useful for batch step-slide window operations on time series data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/window_cal.md#_snippet_8\n\nLANGUAGE: DolphinDB\nCODE:\n```\nt=table(2021.11.01T10:00:00+0 3 5 6 7 8 15 18 20 29 as time, 1..10 as vol)\nselect rolling(last,time,6,3) as last_time, rolling(sum,vol,6,3) as sum_vol from t\n\n # output\n\nlast_time           sum_vol\n------------------- -------\n2021.11.01T10:00:08 21     \n2021.11.01T10:00:20 39\n```\n\n----------------------------------------\n\nTITLE: Real-time High-Frequency Factor Calculation with Data Replay in DolphinDB\nDESCRIPTION: Implements a high-frequency trading factor that compares current ask price with the ask price from 30 quotes ago. Uses dictionary to maintain historical state and data replay to simulate real-time streaming calculations.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_64\n\nLANGUAGE: DolphinDB\nCODE:\n```\nquotesData = loadText(\"/data/ddb/data/sampleQuotes.csv\")\n\ndefg factorAskPriceRatio(x){\n\tcnt = x.size()\n\tif(cnt < 31) return double()\n\telse return x[cnt - 1]/x[cnt - 31]\n}\ndef factorHandler(mutable historyDict, mutable factors, msg){\n\thistoryDict.dictUpdate!(function=append!, keys=msg.symbol, parameters=msg.askPrice1, initFunc=x->array(x.type(), 0, 512).append!(x))\n\tsyms = msg.symbol.distinct()\n\tcnt = syms.size()\n\tv = array(DOUBLE, cnt)\n\tfor(i in 0:cnt){\n\t    v[i] = factorAskPriceRatio(historyDict[syms[i]])\n\t}\n\tfactors.tableInsert([take(now(), cnt), syms, v])\n}\n\nx=quotesData.schema().colDefs\nshare streamTable(100:0, x.name, x.typeString) as quotes1\nhistory = dict(STRING, ANY)\nshare streamTable(100000:0, `timestamp`symbol`factor, [TIMESTAMP,SYMBOL,DOUBLE]) as factors\nsubscribeTable(tableName = \"quotes1\", offset=0, handler=factorHandler{history, factors}, msgAsTable=true, batchSize=3000, throttle=0.005)\n\nreplay(inputTables=quotesData, outputTables=quotes1, dateColumn=`date, timeColumn=`time)\n```\n\n----------------------------------------\n\nTITLE: Cholesky Decomposition with lower=false in DolphinDB\nDESCRIPTION: Demonstrates Cholesky decomposition of a matrix in DolphinDB using the `cholesky` function with `lower=false`. If lower is false, the upper triangle of the matrix is used to calculate the decomposition.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/matrix.md#_snippet_26\n\nLANGUAGE: DolphinDB\nCODE:\n```\n>cholesky(m, false);\n#0 #1       #2      \n-- -------- --------\n1  0        1      \n0  1.414214 0      \n0  0        1.414214\n\n```\n\n----------------------------------------\n\nTITLE: Parallel Stream Processing with Multiple DolphinDB Engines and Subscriptions\nDESCRIPTION: Shows how to create multiple stream engines and use hash-based filtering in `subscribeTable` to distribute incoming data across these engines for parallel processing. Requires a shared output table for results.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_tutorial.md#_snippet_20\n\nLANGUAGE: DolphinDB\nCODE:\n```\nshare streamTable(1:0, `sym`price, [STRING,DOUBLE]) as tickStream\nsetStreamTableFilterColumn(tickStream, `sym)\nshare streamTable(1000:0, `sym`factor1, [STRING,DOUBLE]) as resultStream\n\nfor(i in 0..3){\n    rse = createReactiveStateEngine(name=\"reactiveDemo\"+string(i), metrics =<cumsum(price)>, dummyTable=tickStream, outputTable=resultStream, keyColumn=\"sym\")\n    subscribeTable(tableName=`tickStream, actionName=\"sub\"+string(i), handler=tableInsert{rse}, msgAsTable = true, hash = i, filter = (4,i))\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Data from DolphinDB (Level 2 Dataset)\nDESCRIPTION: This code snippet queries data from a DolphinDB database table named 'tick' which stores Level 2 market data. The query filters for a specific date and selects all columns.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/pickle_comparison.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\n＃读取Level 2数据集一天的数据\ntimer t2 = select * from loadTable(\"dfs://DataYesDB\", \"tick\") where TradeDate = 2019.09.10\n```\n\n----------------------------------------\n\nTITLE: 堆栈追踪显示在 Alpine 3.15使用FreeTDS连接时的崩溃\nDESCRIPTION: 该堆栈描述了在Alpine系统通过`apk add freetds`安装的FreeTDS库连接SQL Server时发生的崩溃，原因涉及OpenSSL 1.1.0的依赖性导致的兼容性问题。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ODBC_plugin_user_guide.md#_snippet_10\n\nLANGUAGE: C++\nCODE:\n```\n#0  0x00007ffff47d9434 in pthread_mutex_lock () from /usr/glibc-compat/lib/libpthread.so.0\n#1  0x00007fffc12f57d3 in ?? () from /usr/lib/libtdsodbc.so.0.0.0\n# ...\n#13  0x00007fffc12e8fe6 in SQLDriverConnectW () from /usr/lib/libtdsodbc.so.0.0.0\n# ...\n```\n\n----------------------------------------\n\nTITLE: Enabling Streaming and Subscribing to DolphinDB Tables - Python\nDESCRIPTION: This code snippet uses the DolphinDB Python API to enable streaming functionality on the client by specifying a local port and then subscribes to a DolphinDB streaming table ('trades'). It defines a simple message handler that prints received streaming messages and shows how to unsubscribe when done. Requires the 'dolphindb' Python package and network access between client and server. Key parameters: host, port, handler function, table and subscription names. Expects streaming rows as input and prints them as output. Ensure that network ports are open and the DolphinDB table is available.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_tutorial.md#_snippet_23\n\nLANGUAGE: Python\nCODE:\n```\nimport dolphindb as ddb\nconn = ddb.session()\nconn.enableStreaming(8000)\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nshare streamTable(1:0,`id`price`qty,[INT,DOUBLE,INT]) as trades\ntrades.append!(table(1..10 as id,rand(10.0,10) as price,rand(10,10) as qty))\n```\n\nLANGUAGE: Python\nCODE:\n```\ndef printMsg(msg):\n    print(msg)\n\nconn.subscribe(\"192.168.1.103\", 8941, printMsg, \"trades\", \"sub_trades\", 0)\n\n[1, 0.47664969926699996, 8]\n[2, 5.543625105638057, 4]\n[3, 8.10016839299351, 4]\n[4, 5.821204076055437, 9]\n[5, 9.768875930458307, 0]\n[6, 3.7460641632787883, 7]\n[7, 2.4479272053577006, 6]\n[8, 9.394394161645323, 5]\n[9, 5.966209815815091, 6]\n[10, 0.03534660907462239, 2]\n```\n\nLANGUAGE: Python\nCODE:\n```\nconn.unsubscribe(\"192.168.1.103\", 8941,\"trades\",\"sub_trades\")\n```\n\n----------------------------------------\n\nTITLE: Joining Device Readings with Device Info and Filtering by Time, Device, and Status in MongoDB - JavaScript\nDESCRIPTION: Through an aggregation pipeline, this snippet joins 'device_readings' and 'device_info' collections using $lookup, matching documents by 'device_id', and applies additional time, device ID, and battery status filters. Requires the existence of both collections and matching 'device_id' fields. Returns joined and filtered records based on given constraints.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs MongoDB/mongodb_readings_query.txt#_snippet_8\n\nLANGUAGE: JavaScript\nCODE:\n```\ndb.device_readings.explain(\"executionStats\").aggregate([{$lookup:{from:\"device_info\",localField:\"device_id\", foreignField:\"device_id\",as:\"device_result\"}},{$match:{time:{\"$gte\":ISODate(\"2016-11-15 09:00:00.000Z\"),\"$lte\":ISODate(\"2016-11-15 12:00:00.000Z\")},device_id:{\"$eq\":\"demo000100\"},battery_status:\"discharging\"}}])\n```\n\n----------------------------------------\n\nTITLE: DolphinDB LAN Cluster Configuration\nDESCRIPTION: This line configures whether the DolphinDB cluster is within a LAN. Setting it to 0 (false) means TCP will be used for heartbeats, indicating a cloud deployment. The default value is true (1).\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/ha_cluster_deployment/P3/config/config-specification.txt#_snippet_3\n\nLANGUAGE: properties\nCODE:\n```\nlanCluster=0\n```\n\n----------------------------------------\n\nTITLE: Defining Alpha1 Factor and Metrics\nDESCRIPTION: This code snippet shows the definition of the alpha1 factor using DolphinDB functions and defines the necessary schema and output table. It uses a `streamEngineParser` to create the streaming engine with specified metrics, dummy table, output table, and key/time columns. This example highlights the input and output of the parser, showing how it transforms a batch computation factor into a stream computation setup.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/StreamEngineParser.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef alpha1(close){\n    ts = mimax(pow(iif(ratios(close) - 1 < 0, mstd(ratios(close) - 1, 20), close), 2.0), 5)\n    return rowRank(X=ts, percent=true) - 0.5\n}\n\ninputSchemaT = table(1:0, [\"SecurityID\",\"TradeTime\",\"close\"], [SYMBOL,TIMESTAMP,DOUBLE])\nresultStream = table(10000:0, [\"TradeTime\",\"SecurityID\", \"factor\"], [TIMESTAMP,SYMBOL,DOUBLE])\nmetrics = <[SecurityID, alpha1(close)]>\nstreamEngine = streamEngineParser(name=\"alpha1ParserT\", metrics=metrics, dummyTable=inputSchemaT, outputTable=resultStream, keyColumn=\"SecurityID\", timeColumn=`tradetime, triggeringPattern='perBatch', triggeringInterval=4000)\n```\n\n----------------------------------------\n\nTITLE: Starting MongoDB Mongos Process with Custom Configuration in Bash\nDESCRIPTION: This shell command launches a MongoDB mongos process with specific configuration servers, network settings, logging options, and performance parameters. It requires a MongoDB installation and appropriate permissions to run. Key parameters include --configdb for specifying config servers, --bind_ip for network interfaces, --port for the listening port, --logpath for log output location, and various --setParameter options to fine-tune sharding task execution. The command runs in the background and is intended for setting up a sharded cluster node.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs MongoDB/master_mongos.txt#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnohup ./mongos --configdb conrep/localhost: 20000,localhost:30001,localhost:30002 --bind_ip localhost --port 40000 --logpath /media/xllu/aa/localhost_shard_jiqun/mongos/mongos40000.log --logappend --setParameter ShardingTaskExecutorPoolHostTimeoutMS=30000000 --setParameter taskExecutorPoolSize=6 --setParameter ShardingTaskExecutorPoolMaxConnecting=12 &\n```\n\n----------------------------------------\n\nTITLE: Calculating 10-Level Net Buy Order Amount Differential in DolphinDB\nDESCRIPTION: This function calculates a high-frequency factor based on all 10 levels of the order book, comparing current prices and quantities with previous tick data. It uses row alignment and vector operations to efficiently process array vector data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Array_Vector.md#_snippet_35\n\nLANGUAGE: DolphinDB\nCODE:\n```\n@state\ndef level10_Diff(price, qty, buy){\n        prevPrice = price.prev()\n        left, right = rowAlign(price, prevPrice, how=iif(buy, \"bid\", \"ask\"))\n        qtyDiff = (qty.rowAt(left).nullFill(0) - qty.prev().rowAt(right).nullFill(0)) \n        amtDiff = rowSum(nullFill(price.rowAt(left), prevPrice.rowAt(right)) * qtyDiff)\n        return amtDiff\n}\n\nsnapshot = loadTable(\"dfs://SH_TSDB_snapshot_ArrayVector\", \"snapshot\")\nres = select SecurityID, DateTime, level10_Diff(BidPrice, BidOrderQty, true) as level10_Diff from snapshot context by SecurityID csort DateTime\n```\n\n----------------------------------------\n\nTITLE: Verifying Data After Date to Month Transformation in DolphinDB\nDESCRIPTION: This script queries the first 5 rows of the table `tb1` from the specified database path after it has been loaded using `loadTextEx` with the `d2m` transformation function. The goal is to confirm that the 'tradingDay' column now correctly stores data in the MONTH data type, reflecting the transformation applied during the import.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_csv.md#_snippet_17\n\nLANGUAGE: dolphindb\nCODE:\n```\nselect top 5 * from loadTable(dbPath,`tb1);\n```\n\n----------------------------------------\n\nTITLE: Persisting Data to Distributed File System (DFS) in DolphinDB\nDESCRIPTION: This DolphinDB code persists both raw and aggregated streaming data to a distributed file system (DFS) for long-term storage and analysis. It defines database and table schemas, creates partitioned tables, and subscribes to stream tables to append data into DFS tables. The function `createConsumerDataBase` and `createAggregateDataBase` are for creating database and related partitioned table. The `subscribeTable` function is to subscribe the streaming data and persist it into dfs table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/knn_iot.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\n/*\n  * 将dataTable数据写入分布式表 \n\n\t数据建模：\n\t1）每小时记录数：360,000,000\n\t2）每条记录大小：46字节\n\t3）每小时空间占用（压缩前）：15.42G\n\t4）建议以“Id”值和“小时”进行组合分区，每分区≈157.93M\n\t5）分区索引为“时间戳”+“设备号”\n\n  */\n def createConsumerDataBase(dbname,tbname,col_names,col_types){\n    dbTime = database(\"\",VALUE,datehour(2023.01.01T00:00:00)..datehour(2023.01.01T23:00:00))\n    Ids = 1..100\n    dbId = database(\"\",VALUE,Ids)\n    db = database(directory=dbname, partitionType=COMPO, partitionScheme=[dbTime,dbId],engine=\"TSDB\")\n    factor_partition = db.createPartitionedTable(table=table(1:0,col_names,col_types),tableName = tbname,partitionColumns = [\"time\",\"deviceCode\"],sortColumns =[\"deviceCode\",\"time\"],compressMethods={time:\"delta\"},keepDuplicates=LAST)\n}\ndataTable_dbname,dataTable_tbname = \"dfs://Data\",\"data\"\ncreateConsumerDataBase(dataTable_dbname,dataTable_tbname,orgColNames,orgColTypes)\nsubscribeTable(tableName=\"dataTable\", actionName=\"append_data_into_dfs\", offset=0, handler=loadTable(dataTable_dbname,dataTable_tbname), msgAsTable=true,batchSize=100000, throttle=1, reconnect=true)\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\n/*\n  * 将聚合计算结果写入分布式表\n\t数据建模：\n\t\t1）每小时记录数：36,000,000\n\t\t2）每条记录大小：46字节\n\t\t3）每小时空间占用（压缩前）：1.54G\n\t\t4）建议以“id”和“天”进行值分区，每分区≈ 379.03M\n\t\t5）分区索引为“时间戳”+“设备号”\n\n  */\ndef createAggregateDataBase(dbname,tbname,col_names,col_types){\n\tif(existsDatabase(dbname)){dropDatabase(dbname)}\n\tIds = 1..100\n    dbId = database(\"\",VALUE,Ids)\n    dbTime = database(\"\",VALUE,date(2023.01.01T00:00:00)..date(2023.12.31T20:00:00))\n\tdb = database(directory=dbname, partitionType=COMPO, partitionScheme=[dbTime,dbId],engine=\"TSDB\")\n\tfactor_partition = db.createPartitionedTable(table=table(1:0,col_names,col_types),tableName = tbname,partitionColumns =[\"time\",\"deviceCode\"],sortColumns =[\"deviceCode\",\"time\"],compressMethods={time:\"delta\"},keepDuplicates=LAST)\n}\naggr_dbname,aggr_tbname = \"dfs://Aggregate\",\"data\"\ncreateAggregateDataBase(aggr_dbname,aggr_tbname,orgColNames,orgColTypes)\nsubscribeTable(tableName=\"aggrTable\", actionName=\"append_Aggregator_into_dfs\", offset=0, handler=loadTable(aggr_dbname,aggr_tbname), msgAsTable=true,batchSize=100000, throttle=1, reconnect=true)\n```\n\n----------------------------------------\n\nTITLE: Triggering Datanode to Report Chunk Meta Status Update in DolphinDB Scripting Language\nDESCRIPTION: This snippet manually triggers a specified Datanode to report its chunk metadata status to the Controller. It is used when a Datanode's chunk is stuck in a non-final state (state=3) and requires re-reporting of chunk data after invoking forceCorrectVersionByReplica or manual intervention. The triggerNodeReport function is available starting from DolphinDB version 2.00.8 and replaces the need for restarting the datanode process to update status.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/repair_chunk_status.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntriggerNodeReport(\"datanode3\")\n```\n\n----------------------------------------\n\nTITLE: DolphinDB Upgrade Script\nDESCRIPTION: Shell command to execute the DolphinDB upgrade process, replacing the current package with a new version.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/gray_scale_upgrade_ha.md#_snippet_10\n\nLANGUAGE: Shell\nCODE:\n```\ncd /home/zjchen/HA/server/clusterDemo/\nsh upgrade.sh\n```\n\n----------------------------------------\n\nTITLE: Query Streaming Subscription Consumption Information in DolphinDB\nDESCRIPTION: Retrieves data regarding the consumption of streaming data by subscribers using getStreamingStat().subWorkers. This provides insights into active sub-worker processes and their statuses for performance and reliability assessment.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/machine_learning_volatility/07.streamStateQuery.txt#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\ngetStreamingStat().subWorkers\n```\n\n----------------------------------------\n\nTITLE: C++ API - Subscribing Replay Service\nDESCRIPTION: This C++ code subscribes to a stream table with the replay data. The code defines a listening port (`listenport`), constructs the stream table name (`tableNameUuid`), and uses `ThreadedClient::subscribe()` to subscribe. The subscription handles the data replay by using the `myHandler` function (not defined in the code snippet) to process the incoming data. The `StreamDeserializerSP sdsp`  is used for deserialization. Prerequisites include a configured DolphinDB C++ API environment with a `ThreadedClient` object and the stream tables set up from replay.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/appendices_market_replay_bp.md#_snippet_11\n\nLANGUAGE: C++\nCODE:\n```\nint listenport = 10260;\nThreadedClient threadedClient(listenport);\nstring tableNameUuid = \"replay_\" + uuidStr;\nauto thread = threadedClient.subscribe(hostName, port, myHandler, tableNameUuid, \"stkReplay\", 0, true, nullptr, true, 500000, 0.001, false, \"admin\", \"123456\", sdsp);\nstd::cout << \"Successed to subscribe \" + tableNameUuid << endl;\nthread->join();\n```\n\n----------------------------------------\n\nTITLE: Accessing Array Vector Variables by Row and Column Indices - DolphinDB Script\nDESCRIPTION: This set of snippets demonstrates how to retrieve elements from Array Vector and Columnar Tuple structures using explicit row and column indices or combinations thereof. It covers both direct (x[r, c], y[c, r]) and chained indexing patterns, and handles out-of-bounds indices by returning null. Key requirements are proper index bounds, and the variables must be initialized arrays or columnar tuples. The result is a single value, vector, or sub-array depending on the slice specified.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Array_Vector.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\nr, c = 2, 1\nx = array(INT[], 0).append!([1 2 3, 4 5, 6 7 8, 9 10])\nx[r, c]\n/*\n[7]\n*/\n\ny = [[1,2,3],[4,5],[6,7,8],[9,10]].setColumnarTuple!()\ny[c, r]\n/*\n7\n*/\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 用函数先定位某一行，再定位某一列\nx = array(INT[], 0).append!([1 2 3, 4 5, 6 7 8, 9 10])\nx.row(r).at(c)\n/*\n7\n*/\n\ny = [1 2 3, 4 5, 6 7 8, 9 10].setColumnarTuple!()\ny.row(r).at(c)\n/*\n7\n*/\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 用下标先定位某一行，再定位某一列\nx = array(INT[], 0).append!([1 2 3, 4 5, 6 7 8, 9 10])\nx[[r]][c]\n/*\n[7]\n*/\n\ny = [1 2 3, 4 5, 6 7 8, 9 10].setColumnarTuple!()\ny[[r]][c]\n/*\n[7]\n*/\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 用下标先定位某几行，再定位某几列\nx = array(INT[], 0).append!([1 2 3, 4 5, 6 7 8, 9 10])\nx[rows][cols]\n/*\n[[4,5],[6,7],[9,10]]\n*/\n\ny = [1 2 3, 4 5, 6 7 8, 9 10].setColumnarTuple!()\ny[rows][cols]\n/*\n[[4,5],[6,7],[9,10]]\n*/\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 当 index 越界时，空值填充\nx = array(INT[], 0).append!([1 2 3, 4 5, 6 7 8, 9 10])\nx[2 3 4][1:3]\n/*\n[[7,8],[10,],[,]]\n*/\n\ny = [1 2 3, 4 5, 6 7 8, 9 10].setColumnarTuple!()\ny[2 3 4][1:3]\n/*\n[[7,8],[10,],[,]]\n*/\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Aggregation Function for Grouped Percentile - DolphinDB Script\nDESCRIPTION: Defines a user function 'percentile_40' to compute the mean of group values below the 40th percentile. Used within a grouped select statement, this script illustrates custom aggregation to circumvent limitations in direct percentile filtering in DolphinDB group-bys. Requires the 'percentile' and 'iif' built-in functions. Input: numeric vector 'x'; Output: scalar mean of the lower 40% values or NULL if none. Designed for scenarios where percentile-based filtering is needed on grouped data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_TopN.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndefg percentile_40(x){\n\tret = NULL\n\ty = percentile(x,40)\n\tcc = sum(x<y) \n\tif (cc > 0){\n\t\tret = sum(iif( x<y,x,0))\\cc\n\t}\n\treturn ret\n}\nselect percentile_40(value) as factor_value from tb group by trade_date,secu_code\n```\n\n----------------------------------------\n\nTITLE: Creating DolphinDB Partitioned Table for Option Information (DolphinDB Script)\nDESCRIPTION: Connects to DolphinDB, checks for and drops an existing database named 'dfs://optionInfo', then creates a new partitioned database and table ('optionInfo') to store option contract details. The table includes fields like contract code, type, strike price, dates, and underlying symbol. It is partitioned by the underlying symbol (`sym`) using a value partition scheme for symbols `510050` and `510300`.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/IV_Greeks_Calculation_for_ETF_Options_Using_JIT.md#_snippet_4\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nlogin(\"admin\", \"123456\")\ndbName = \"dfs://optionInfo\"\ntbName = \"optionInfo\"\nif(existsDatabase(dbName)){\n\tdropDatabase(dbName)\n}\ndb = database(dbName, VALUE, `510050`510300)\ncolNames = `code`name`exemode`exeprice`startdate`lastdate`sym`exeratio`exeprice2`dividenddate`tradecode\ncolTypes = [STRING, STRING, INT, DOUBLE, DATE, DATE, SYMBOL, DOUBLE, DOUBLE, DATE, STRING]\nschemaTable = table(1:0, colNames, colTypes)\ndb.createPartitionedTable(table=schemaTable, tableName=tbName, partitionColumns=`sym)\n```\n\n----------------------------------------\n\nTITLE: Constructing Consumption Callback with DolphinDB C++ API\nDESCRIPTION: This C++ snippet defines a lambda function 'myHandler' that processes batches of streamed messages from a heterogenous DolphinDB data stream, printing message contents and checking for an end symbol to unsubscribe. It accumulates the count of data items received and computes callback speed in nanoseconds per item. This requires DolphinDB C++ API and utilities like Util::getNanoEpochTime for timing. It handles inputs as vectors of Message objects and outputs to std::cout. Unsubscription is triggered when a message with symbol 'end' is detected, ensuring proper termination of data consumption.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/appendices_market_replay_bp.md#_snippet_12\n\nLANGUAGE: C++\nCODE:\n```\nlong sumcount = 0;\nlong long starttime = Util::getNanoEpochTime();\nauto myHandler = [&](vector<Message> msgs) \n{\n    for (auto& msg : msgs) \n    {\n        std::cout << msg.getSymbol() << \" : \" << msg->getString() << endl;\n        if(msg.getSymbol() == \"end\")\n        {\n            threadedClient.unsubscribe(hostName, port, tableNameUuid,\"stkReplay\");\n        }\n        sumcount += msg->get(0)->size();\n    }\n    long long speed = (Util::getNanoEpochTime() - starttime) / sumcount;\n    std::cout << \"callback speed: \" << speed << \"ns\" << endl;\n};\n```\n\n----------------------------------------\n\nTITLE: Restart DolphinDB After Upgrade - Shell\nDESCRIPTION: Command to restart DolphinDB single node in the background using the `startSingle.sh` script after upgrade procedures. This enables the server to operate with updated binaries but retain existing configurations and licenses.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ARM_standalone_deploy.md#_snippet_6\n\nLANGUAGE: Shell\nCODE:\n```\nsh startSingle.sh\n```\n\n----------------------------------------\n\nTITLE: Calculating Annual Skewness in DolphinDB\nDESCRIPTION: Defines a DolphinDB function `getAnnualSkew` to calculate the skewness of daily returns. It takes a vector `value` (daily net values), computes daily returns, and then uses the built-in `skew` function to measure the asymmetry of the return distribution.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/fund_factor_contrasted_by_py.md#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndefg getAnnualSkew(value){\n\treturn skew(deltas(value)\\prev(value))\n}\n```\n\n----------------------------------------\n\nTITLE: Loading Data from Different Sources - DolphinDB\nDESCRIPTION: This function, `dsTb`, loads data from different tables based on the `replayName` parameter. It supports loading from 'snapshot', 'order', and 'transaction' tables located in the distributed file system (DFS). It uses `loadTable` to load the data and returns NULL if the `replayName` is not recognized. Finally, it uses `replayDS` function to create a replay data source based on the provided table, date, time and stock list, and time repartition schema.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/appendices_market_replay_bp/replay.txt#_snippet_0\n\nLANGUAGE: dolphindb\nCODE:\n```\ndef dsTb(timeRS, startDate, endDate, stkList, replayName)\n{\n    if(replayName == \"snapshot\"){\n        tab = loadTable(\"dfs://Test_snapshot\", \"snapshot\")\n\t}\n\telse if(replayName == \"order\") {\n\t\ttab = loadTable(\"dfs://Test_order\", \"order\")\n\t}\n\telse if(replayName == \"transaction\") {\n\t\ttab = loadTable(\"dfs://Test_transaction\", \"transaction\")\n\t}\n\telse {\n\t\treturn NULL\n\t}\n    ds = replayDS(sqlObj=<select * from tab where MDDate>=startDate and MDDate<endDate and HTSCSecurityID in stkList>, dateColumn='MDDate', timeColumn='MDTime', timeRepartitionSchema=timeRS)\n    return ds\n}\n```\n\n----------------------------------------\n\nTITLE: Using Momentum Indicators in DolphinDB\nDESCRIPTION: Collection of momentum indicator functions available in the DolphinDB ta module. These functions analyze price movements to identify market strength, trend direction, and potential reversal points by comparing current prices to historical values.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ta.md#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\nadx(high, low, close, timePeriod)\nadxr(high, low, close, timePeriod)\napo(close, fastPeriod, slowPeriod, maType)\naroon(high, low, timePeriod)\naroonOsc(high, low, timePeriod)\nbop(open, high, low, close)\ncci(high, low, close, timePeriod)\ncmo(close, timePeriod)\ndx(high, low, close, timePeriod)\nmacd(close, fastPeriod, slowPeriod, signalPeriod)\nmacdExt(close, fastPeriod, fastMaType, slowPeriod, slowMaType, signalPeriod, signalMaType)\nmacdFix(close, signalPeriod)\nmfi(high, low, close, volume, timePeriod)\nminus_di(high, low, close, timePeriod)\nminus_dm(high, low, timePeriod)\nmom(close, timePeriod)\nplus_di(high, low, close, timePeriod)\nplus_dm(high, low, timePeriod)\nppo(close, fastPeriod, slowPeriod, maType)\nroc(close, timePeriod)\nrocp(close, timePeriod)\nrocr(close, timePeriod)\nrocr100(close, timeperiod)\nrsi(close, timePeriod)\nstoch(high, low, close, fastkPeriod, slowkPeriod, slowkMatype, slowdPeriod, slowdMatype)\nstochf(high, low, close, fastkPeriod, fastdPeriod, fastdMatype)\nstochRsi(real, timePeriod, fastkPeriod, fastdPeriod, fastdMatype)\ntrix(close, timePeriod)\nultOsc(high, low, close, timePeriod1, timePeriod2, timePeriod3)\nwillr(high, low, close, timePeriod)\n```\n\n----------------------------------------\n\nTITLE: Importing DolphinDB Module for Naming Conflict Example\nDESCRIPTION: Imports the previously defined `sys` module using the `use` keyword. This makes the `myfunc` function from the `sys` module available in the current session, setting up the scenario for a naming conflict with the locally defined `myfunc` or function view.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/module_tutorial.md#_snippet_15\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nuse sys\n```\n\n----------------------------------------\n\nTITLE: Implementing flow Factor in Python\nDESCRIPTION: This function calculates the flow factor using pandas rolling window operations and NumPy for array manipulations. It computes the same metric as the DolphinDB version but uses Python's data handling paradigms.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Python+HDF5_vs_DolphinDB.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ndef flow(df):\n    buy_vol_ma = np.round(df['BidSize1'].rolling(60).mean(), decimals=5)\n    sell_vol_ma = np.round((df['OfferSize1']).rolling(60).mean(), decimals=5)\n    buy_prop = np.where(abs(buy_vol_ma + sell_vol_ma) < 0, 0.5, buy_vol_ma / (buy_vol_ma + sell_vol_ma))\n    spd = df['OfferPX1'].values - df['BidPX1'].values\n    spd = np.where(spd < 0, 0, spd)\n    spd = pd.DataFrame(spd)\n    spd_ma = np.round((spd).rolling(60).mean(), decimals=5)\n    return np.where(spd_ma == 0, 0, pd.DataFrame(buy_prop) / spd_ma)\n```\n\n----------------------------------------\n\nTITLE: Connecting and Authenticating Redis Server in DolphinDB\nDESCRIPTION: Establishes a connection to a Redis server at a specific IP address and port, and authenticates with the given password. This function returns a connection handle for subsequent Redis commands used within the DolphinDB environment. Dependencies include the Redis plugin being installed and loaded.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/redis_plugin_tutorial/example3-kafka.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\nconn=redis::connect(192.168.0.75, 6379)\nredis::run(conn, \"AUTH\", \"password\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Tables with Composite Columns in DolphinDB SQL - DolphinDB\nDESCRIPTION: This snippet demonstrates how to create a DolphinDB table with multiple numeric factors and identifiers to be used in in-database analytics. The table 't' consists of columns 'id', 'y', 'factor1', and 'factor2'. The factors are provided as numeric vectors, and the table is constructed using vectorized expressions and basic DolphinDB table creation syntax. Dependencies include installed DolphinDB environment and familiarity with basic table creation syntax.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/hybrid_programming_paradigms.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\nfactor1=3.2 1.2 5.9 6.9 11.1 9.6 1.4 7.3 2.0 0.1 6.1 2.9 6.3 8.4 5.6\nfactor2=1.7 1.3 4.2 6.8 9.2 1.3 1.4 7.8 7.9 9.9 9.3 4.6 7.8 2.4 8.7\nt=table(take(1 2 3, 15).sort() as id, 1..15 as y, factor1, factor2);\n```\n\n----------------------------------------\n\nTITLE: Configuring JSON Data Source to Connect DolphinDB via Redash\nDESCRIPTION: This snippet demonstrates the YAML configuration required for Redash's JSON data source to connect to DolphinDB. It includes the essential parameters such as URL, HTTP method, and JSON payload with required keys ('client' and 'queries'), enabling data retrieval through POST requests.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/data_interface_for_redash.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nurl: http://115.239.209.224:18531\nmethod: \"post\"\njson: { 'client':'redash', 'queries':'select * from typeTable' }\n```\n\n----------------------------------------\n\nTITLE: Stopping All DolphinDB Cluster Nodes Using Shell Script\nDESCRIPTION: This shell command script stops all cluster nodes across multiple servers (P1, P2, and P3) by running the provided stop script within the clusterDemo directory. It ensures a clean shutdown before backup or upgrade operations. The command assumes the user has the necessary permissions to execute the script on each server.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/multi_machine_cluster_deployment.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\n./stopAllNode.sh\n```\n\n----------------------------------------\n\nTITLE: Setting up Real-time Streaming for Factor Calculation in DolphinDB\nDESCRIPTION: DolphinDB script demonstrating the setup for real-time factor calculation using the reactive state engine. It defines input table schema (`inputTable`) mimicking market data and output table schema (`resultTable`), creates a reactive state engine (`createReactiveStateEngine`) named 'reactiveDemo' using the previously defined `bidWithdrawsVolume` factor, generates sample data (`testdata`), inserts it into the engine, and finally queries the results.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Streaming_computing_of_financial_quantifiers.md#_snippet_19\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 定义输入输出的表结构\ncolName = [\"securityID\",\"dateTime\",\"preClosePx\",\"openPx\",\"highPx\",\"lowPx\",\"lastPx\",\"totalVolumeTrade\",\"totalValueTrade\",\"instrumentStatus\"] <- flatten(eachLeft(+, [\"bidPrice\",\"bidOrderQty\",\"bidNumOrders\"], string(0..9))) <- (\"bidOrders\"+string(0..49)) <- flatten(eachLeft(+, [\"offerPrice\",\"offerOrderQty\",\"offerNumOrders\"], string(0..9))) <- (\"offerOrders\"+string(0..49)) <- [\"numTrades\",\"iopv\",\"totalBidQty\",\"totalOfferQty\",\"weightedAvgBidPx\",\"weightedAvgOfferPx\",\"totalBidNumber\",\"totalOfferNumber\",\"bidTradeMaxDuration\",\"offerTradeMaxDuration\",\"numBidOrders\",\"numOfferOrders\",\"withdrawBuyNumber\",\"withdrawBuyAmount\",\"withdrawBuyMoney\",\"withdrawSellNumber\",\"withdrawSellAmount\",\"withdrawSellMoney\",\"etfBuyNumber\",\"etfBuyAmount\",\"etfBuyMoney\",\"etfSellNumber\",\"etfSellAmount\",\"etfSellMoney\"]\ncolType = [\"SYMBOL\",\"TIMESTAMP\",\"DOUBLE\",\"DOUBLE\",\"DOUBLE\",\"DOUBLE\",\"DOUBLE\",\"INT\",\"DOUBLE\",\"SYMBOL\"] <- take(\"DOUBLE\", 10) <- take(\"INT\", 70)<- take(\"DOUBLE\", 10) <- take(\"INT\", 70) <- [\"INT\",\"INT\",\"INT\",\"INT\",\"DOUBLE\",\"DOUBLE\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"DOUBLE\",\"INT\",\"INT\",\"DOUBLE\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\"]\ninputTable = table(1:0, colName, colType)\nresultTable = table(10000:0, [\"securityID\", \"dateTime\", \"factor\"], [SYMBOL, TIMESTAMP, DOUBLE])\n\n// 使用 createReactiveStateEngine 创建响应式状态引擎\ntry{ dropStreamEngine(\"reactiveDemo\")} catch(ex){ print(ex) }\nmetrics = <[dateTime, bidWithdrawsVolume(bidPrice0, bidPrice1, bidPrice2, bidPrice3, bidPrice4, bidPrice5, bidPrice6, bidPrice7, bidPrice8, bidPrice9,bidOrderQty0, bidOrderQty1, bidOrderQty2, bidOrderQty3, bidOrderQty4, bidOrderQty5, bidOrderQty6, bidOrderQty7, bidOrderQty8, bidOrderQty9, levels=3)]>\nrse = createReactiveStateEngine(name=\"reactiveDemo\", metrics =metrics, dummyTable=inputTable, outputTable=resultTable, keyColumn=\"securityID\")\n\n// 构造数据\nsetRandomSeed(9)\nn = 5\nsecurityID = take(`000001, n)\ndateTime = 2023.01.01T09:30:00.000 + 1..n*3*1000\nbidPrice0 = rand(10, n) \\ 100 + 19.5\nbidPrice1, bidPrice2 = bidPrice0+0.01, bidPrice0+0.02\nbidOrderQty0, bidOrderQty1, bidOrderQty2 = rand(200, n), rand(200, n), rand(200, n)\nofferPrice0 = rand(10, n) \\ 100 + 19.5\nofferPrice1, offerPrice2 = offerPrice0+0.01, offerPrice0+0.02\nofferOrderQty0, offerOrderQty1, offerOrderQty2 = rand(200, n), rand(200, n), rand(200, n)\ntestdata = table(securityID, dateTime, bidPrice0, bidPrice1, bidPrice2, bidOrderQty0, bidOrderQty1, bidOrderQty2, offerPrice0, offerPrice1, offerPrice2, offerOrderQty0, offerOrderQty1, offerOrderQty2)\n// 输入数据\ntableInsert(rse, testdata.flip())\n\n// 查看结果\nselect * from resultTable\n/*\nsecurityID dateTime                factor\n---------- ----------------------- ------\n000001     2023.01.01T09:30:03.000       \n000001     2023.01.01T09:30:06.000       \n000001     2023.01.01T09:30:09.000 0     \n000001     2023.01.01T09:30:12.000 36    \n000001     2023.01.01T09:30:15.000 26    \n*/\n```\n\n----------------------------------------\n\nTITLE: DolphinDB Grant DB_MANAGE requires existing database\nDESCRIPTION: Starting with versions 1.30.21 and 2.00.9, when using `grant`, `deny`, or `revoke` commands with `accessType=DB_MANAGE`, the specified databases in `objs` must exist. Otherwise, an error will be thrown.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Permission_Management.md#_snippet_15\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(\"admin\", \"123456\")\ncreateUser(\"user1\",\"123456\")\ngrant(\"user1\", DB_MANAGE,\"dfs://test0\") => The database [dfs://test0]  does not exist\ngrant(\"user1\", DB_MANAGE, [\"dfs://db1\",\"dfs://db2\"]) => The database [dfs://db1]  does not exist\n```\n\n----------------------------------------\n\nTITLE: Custom Function Application with loop\nDESCRIPTION: Applies a custom function to each row of an Array Vector and Columnar Tuple using the `loop` function in DolphinDB.  The example defines a function (`foo3`) that returns a vector with a different length than the input row.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Array_Vector.md#_snippet_17\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 返回值可以是和 v 不等长的向量\ndef foo3(v){\n    return [last(v)-first(v), last(v)+first(v)]\n} \n\nx = array(INT[], 0).append!([1 2 3, 4 5, 6 7 8, 9 10])\nz = loop(foo3, x)\n/* z\n([2,4],[1,9],[2,14],[1,19])\n*/\n\ny = [1 2 3, 4 5, 6 7 8, 9 10].setColumnarTuple!()\nz = loop(foo3, y)\n/* z\n([2,4],[1,9],[2,14],[1,19])\n*/\n\nt = table(1 2 3 4 as id, x as x, y as y)\nnew_t = select *, loop(foo3, x) as new_x, loop(foo3, y) as new_y from t\n/* new_t\nid x       y       new_x  new_y \n-- ------- ------- ------ ------\n1  [1,2,3] [1,2,3] [2,4]  [2,4] \n2  [4,5]   [4,5]   [1,9]  [1,9] \n3  [6,7,8] [6,7,8] [2,14] [2,14]\n4  [9,10]  [9,10]  [1,19] [1,19]\n*/\n```\n\n----------------------------------------\n\nTITLE: Loading 'Trades' Data into Elasticsearch (Python)\nDESCRIPTION: Python script to load trade data from a CSV file into Elasticsearch. It defines functions to create an index template with specific mappings and settings (shards, replicas, refresh interval), delete an existing index, create a new index, read the CSV in chunks, format data for the bulk API, and send bulk requests to Elasticsearch. The main function orchestrates these steps and measures the total import time. Requires `urllib3` library.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport urllib3\nimport json\nimport csv\nimport time\n\ndef main():\n    create_index_template()\n    delete_index()  \n    create_index()\n    http = urllib3.PoolManager()\n    t1 = time.time()\n    for tmp in read_lines():\n        # 分段生成小文件来加载\n        data = '\\n'.join(bulk_import_lines(tmp))\n        data += '\\n'\n        response = http.request('PUT', 'http://localhost:9200/_bulk', body=data.encode('utf-8'), headers={'Content-Type': 'application/json'})\n        print(response.status)\n        print(response.data)\n    t2 = time.time()\n    print(\"导入数据耗时(ms):\", (t2-t1)*1000)\n\n\ndef bulk_import_lines(lines):\n    for line in lines:\n        yield json.dumps({'index': {'_index': 'elastic', '_type': 'type'}})\n        yield json.dumps(line)\n\n\n# 读每一行转成json\ndef read_lines():\n    with open('/home/revenant/data/tushare_daily_data.csv') as f:\n        f.readline()\n        field_name = ['ts_code', 'trade_date', 'open', 'high', 'low', 'close', 'pre_close', 'change', 'pct_change', 'vol', 'amount']\n        symbols = csv.DictReader(f, fieldnames=field_name)\n        cnt = 0\n        temp = []\n        for symbol in symbols:\n            symbol.pop(None, None)\n            try:\n                cnt = cnt+1\n                temp.append(symbol)\n            except:\n                pass\n            if(cnt%100000==0 and cnt!=0):\n                print(cnt)\n                yield temp\n                temp = []\n        if(len(temp) > 0):\n            yield temp\n\n\ndef create_index():\n    http = urllib3.PoolManager()\n    try:\n        response = http.request('PUT', 'http://localhost:9200/elastic')\n        print(response.status)\n        print(response.data)\n    except urllib3.exceptions:\n        print('Connection failed.')\n\n\ndef delete_index():\n    http = urllib3.PoolManager()\n    try:\n        response = http.request('DELETE', 'http://localhost:9200/elastic')\n        print(response.status)\n        print(response.data)\n    except urllib3.exceptions:\n        pass\n\n\ndef create_index_template():\n    http = urllib3.PoolManager()\n    data = json.dumps({\n        'template': 'elastic',\n        'settings': {\n            'number_of_shards': 4,\n            'number_of_replicas': 1,\n            \"index.refresh_interval\": -1\n        },\n        'mappings': {\n            'type': {\n                '_source': {'enabled': True},\n                'properties': {\n                    'ts_code': {'type': 'keyword'},\n                    'trade_date': {'type': 'date', \"format\": \"yyyy.MM.dd\"},\n                    'open': {'type': 'double'},\n                    'high': {'type': 'double'},\n                    'low': {'type': 'double'},\n                    'close': {'type': 'double'},\n                    'pre_close': {'type': 'double'},\n                    'change': {'type': 'double'},\n                    'pct_change': {'type': 'double'},\n                    'vol': {'type': 'double'},\n                    'amount': {'type': 'double'}\n                }\n            }\n        }\n    }).encode('utf-8')\n    r = http.request('PUT', 'http://localhost:9200/_template/elastic', body=data, headers={'Content-Type': 'application/json'})\n    print(r.status)\n    print(r.data)\n\nmain()\n```\n\n----------------------------------------\n\nTITLE: 使用 updateChunkVersionOnMaster 函数修改控制节点上的chunk版本\nDESCRIPTION: 此片段展示在控制节点上调用 updateChunkVersionOnMaster 函数，通过提供chunk的唯一ID和目标版本，有效修改并持久化目标chunk的元数据版本。操作需在控制节点执行，修改后确保版本一致，但注意版本高低可能影响dataload过程。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/repair_chunk_status.md#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\nupdateChunkVersionOnMaster(\"deb91fa2-f05a-3096-5941-b80feda42562\", 270)\n```\n\n----------------------------------------\n\nTITLE: Cleaning the Environment in DolphinDB Script - DolphinDB\nDESCRIPTION: This function cleans up the session by unsubscribing from DolphinDB topic tables, dropping stream engines and tables, and releasing all user-defined objects. It uses exception handling to avoid errors if any component does not exist. This prepares the environment for a fresh run with no dependency on previous state, ensuring isolated execution for the real-time pipeline.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/machine_learning_volatility/04.streamComputing.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef cleanEnvironment(){\n\ttry{ unsubscribeTable(tableName=`snapshotStream, actionName=\"aggrFeatures10min\") } catch(ex){ print(ex) }\n\ttry{ unsubscribeTable(tableName=`aggrFeatures10min, actionName=\"predictRV\") } catch(ex){ print(ex) }\n\ttry{ dropStreamEngine(\"aggrFeatures10min\") } catch(ex){ print(ex) }\n\ttry{ dropStreamTable(`snapshotStream) } catch(ex){ print(ex) }\n\ttry{ dropStreamTable(`aggrFeatures10min) } catch(ex){ print(ex) }\n\ttry{ dropStreamTable(`result1min) } catch(ex){ print(ex) }\n\tundef all\n}\ncleanEnvironment()\ngo\n\n```\n\n----------------------------------------\n\nTITLE: Creating and Persisting 'tradeProcessStream' Table - DolphinDB\nDESCRIPTION: Defines and persists a stream table 'tradeProcessStream' for processed trade data, including trade numbers, quantities, amounts, and buy/sell flags. Table is set up with compression, asynchronous writing, and key performance parameters for cache and retention. Filter column is set for 'SecurityID'. Requires a running DolphinDB environment with rights to enable persistence.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/streaming_capital_flow_order_by_order/03.清理环境并创建相关流数据表.txt#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//create stream table: tradeProcessStream\ncolName = `SecurityID`TradeTime`Num`TradeQty`TradeAmount`BSFlag\ncolType = `SYMBOL`TIMESTAMP`INT`INT`DOUBLE`SYMBOL\ntradeProcessStreamTemp = streamTable(1000000:0, colName, colType)\ntry{ enableTableShareAndPersistence(table=tradeProcessStreamTemp, tableName=\"tradeProcessStream\", asynWrite=true, compress=true, cacheSize=1000000, retentionMinutes=1440, flushMode=0, preCache=10000) } catch(ex){ print(ex) }\nundef(\"tradeProcessStreamTemp\")\ngo\nsetStreamTableFilterColumn(tradeProcessStream, `SecurityID)\n```\n\n----------------------------------------\n\nTITLE: Configuring DolphinDB Cluster Nodes List in CSV Format Using Java-Style Syntax\nDESCRIPTION: This snippet shows the configuration of the cluster.nodes file, where each line lists a node’s local endpoint and its role within the DolphinDB cluster. The format uses a CSV structure, with the first column containing the IP address, port, and alias separated by colons, and the second column specifying the node mode such as 'agent', 'datanode', or 'computenode'. This configuration controls the membership of the cluster and requires that node aliases are unique and case-sensitive.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/single_machine_cluster_deploy.md#_snippet_20\n\nLANGUAGE: Java\nCODE:\n```\nlocalSite,mode\nlocalhost:8901:agent1,agent\nlocalhost:8902:dnode1,datanode\nlocalhost:8903:cnode1,computenode\n```\n\n----------------------------------------\n\nTITLE: Processing Sell Orders with Reactive State Engine in DolphinDB\nDESCRIPTION: This function `processSellOrderFunc` creates reactive state engines to process sell orders incrementally. It builds upon the output of the buy order processing.  It defines metrics for calculations like cumulative trade amount and order flags based on trade quantity, incorporating data from the buy order processing.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_capital_flow_daily.md#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef processSellOrderFunc(parallel){\n\tcolName = `SecurityID`BuyNum`TradeTime`SellNum`TradeAmount`TradeQty`TotalBuyAmount`BuyOrderFlag`PrevTotalBuyAmount`PrevBuyOrderFlag\n\tcolType =  [SYMBOL, INT, TIMESTAMP, INT, DOUBLE, INT, DOUBLE, INT, DOUBLE, INT]\n\tprocessBuyOrder = table(1:0, colName, colType)\n\tmetricsSell = [\n\t\t<TradeTime>,\n\t\t<TradeAmount>,\n\t\t<cumsum(TradeAmount)>,\n\t\t<tagFunc(cumsum(TradeQty))>,\n\t\t<prev(cumsum(TradeAmount))>,\n\t\t<prev(tagFunc(cumsum(TradeQty)))>,\n\t\t<BuyNum>,\n\t\t<TotalBuyAmount>,\n\t\t<BuyOrderFlag>,\n\t\t<PrevTotalBuyAmount>,\n\t\t<PrevBuyOrderFlag>]\n\tfor(i in 1..parallel){\n\t\tcreateReactiveStateEngine(name=\"processSellOrder\"+string(i), metrics=metricsSell, dummyTable=processBuyOrder, outputTable=getStreamEngine(\"processCapitalFlow\"+string(i)), keyColumn=`SecurityID`SellNum, keepOrder=true)\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: Single-Day Option Greeks Performance Test Function in DolphinDB\nDESCRIPTION: This function measures the computational performance of calculating implied volatility, delta, gamma, vega, and theta for options on a specific trading day. It uses key inputs like option close prices, ETF prices, and contract info, timing the calculation block for efficiency assessment.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/IV_Greeks_Calculation_for_ETF_Options_Using_JIT/calculation_scripts.txt#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef testOneDayPerformance(closPriceWideMatrix, etfPriceWideMatrix, contractInfo, targetDate){\n\ttargetDate_vec = [targetDate]\n\tr = 0\n\toptionTodayClose = getTargetDayOptionClose(closPriceWideMatrix, targetDate_vec)\n\tvalidContractsToday = optionTodayClose.columnNames()\n\tetfTodayPrice = getTargetDayEtfPrice(etfPriceWideMatrix, targetDate_vec)\n\tKPrice, dayRatio, CPMode = getTargetDayContractInfo(contractInfo, validContractsToday, targetDate_vec)\n\ttimer{\n\t\timpvMatrix = calculateImpv(optionTodayClose, etfTodayPrice, KPrice, r, dayRatio, CPMode)\n\t\t deltaMatrix = calculateDelta(etfTodayPrice, KPrice, r, dayRatio, impvMatrix, CPMode)(etfTodayPrice*0.01)\n\t\t gammaMatrix = calculateGamma(etfTodayPrice, KPrice, r, dayRatio, impvMatrix)(pow(etfTodayPrice, 2) * 0.0001)\n\t\t vegaMatrix = calculateVega(etfTodayPrice, KPrice, r, dayRatio, impvMatrix)\n\t\t thetaMatrix = calculateTheta(etfTodayPrice, KPrice, r, dayRatio, impvMatrix, CPMode)\n\t}\n\ttodayTable = table(validContractsToday as optionID, impvMatrix.reshape() as impv, deltaMatrix.reshape() as delta, gammaMatrix.reshape() as gamma, vegaMatrix.reshape() as vega, thetaMatrix.reshape() as theta)\n\ttodayTable[\"tradingDate\"] = targetDate\n\ttodayTable.reorderColumns!([\"optionID\", \"tradingDate\"])\n\treturn todayTable\n}\n```\n\n----------------------------------------\n\nTITLE: Modifying Table Schema for Array Vector Columns - DolphinDB Script\nDESCRIPTION: This short snippet demonstrates extracting the schema from a CSV file and updating the column types for 'bid' and 'ask' to use DOUBLE[] (array vector) types in DolphinDB. It uses the 'extractTextSchema' function, performs an in-place update using SQL-style syntax, and prepares the schema for advanced ingestion. It requires the file at the specified 'path' to be present and accessible, and should be run before data import to ensure array vector compatibility.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_csv.md#_snippet_20\n\nLANGUAGE: DolphinDB\nCODE:\n```\npath = \"/home/data/t.csv\"\nschema=extractTextSchema(path);\nupdate schema set type = \"DOUBLE[]\" where name=\"bid\" or name =\"ask\"\n```\n\n----------------------------------------\n\nTITLE: 3. 高阶函数—— cross 使用案例\nDESCRIPTION: 介绍如何使用 cross 高阶函数，将两个向量或矩阵两两组合，作为参数调用目标函数，适用于元素逐一配对的场景，比如两个时间序列的相关性计算。示例展示如何用 cross 来两两组合两个向量。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_4\n\n\n\n----------------------------------------\n\nTITLE: Simulating Real-Time MiniSeed Data Writing with DolphinDB Job Submission DolphinDB Script\nDESCRIPTION: Defines a DolphinDB function 'writeMseed' to simulate a seismic station continuously generating MiniSeed data packets by writing sampled data every 3 seconds to a MiniSeed file. Uses the MiniSeed plugin's write interface with parameters for path, station ID, current time, sample rate, and randomized sample values. The function executes 40 iterations, simulating continuous real-time data generation. It is submitted as a background job with submitJob for asynchronous execution, with a comment on how to cancel the job if needed.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Seismic_waveform_data_storage.md#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef writeMseed(){\n\t/*\n\t * Description：\n\t * \t此函数用于模拟实际生产环境中，每3s向台网中心发送 MiniSeed 数据包的过程，即每隔3s向 aimPath 路径上的 MiniSeed 文件写数据\n\t */\n\tsid = \"XFDSN:ZJ_A0001_40_E_I_E\"\n\taimPath = \"../streamMiniSeed/ZJ_A0001_40_E_I_E.20000101.mseed\"\n\tsampleRate = 100\n\tcnt = 0\n\tdo{\n\t\tstartTime = now()\n\t\tmseed::write(aimPath, sid, startTime, sampleRate, rand(-3000..500,300))\n\t\tsleep(3000)\n\t\tcnt += 1\n\t}while(cnt < 40) \n}\n\t\n\nmseedStreamJobid = submitJob(\"wrtieMseedStream\",\"wrtieMseedStream\",witeMseed) //提交后台任务\n/*\n * 使用以下方式取消实时写入任务\n * cancelJob(mseedStreamJobid)\n */\n```\n\n----------------------------------------\n\nTITLE: Extracting CSV Schema using extractTextSchema\nDESCRIPTION: This code snippet uses the `extractTextSchema` function in DolphinDB to extract the schema (column names and data types) from a CSV file. It takes the file path and a skipRows parameter as input and returns a table containing the schema information.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/LoadDataForPoc.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\nfilePath = \"/home/ychan/data/loadForPoc/SH/Order/20210104/Entrust.csv\"\nextractTextSchema(filename = filePath, skipRows = 1)\n```\n\n----------------------------------------\n\nTITLE: Query Registered Subscription Tables in DolphinDB Stream Computing\nDESCRIPTION: Retrieves information regarding subscription registration tables for stream processing, used to monitor which tables are subscribed to by streaming clients. It is essential for managing stream subscriptions. Requires DolphinDB environment.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/streaming_capital_flow_daily/04.streamStateQuery.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\ngetStreamingStat().pubTables\n```\n\n----------------------------------------\n\nTITLE: 开发聚合函数logSum - C++\nDESCRIPTION: 此代码片段展示了如何开发DolphinDB的聚合函数，特别是处理常规数组和大数组。它定义了`logSum`函数，用于计算数据的对数和。通过`isFastMode` 函数判断数据存储方式，如果是常规数组，使用`getDataArray` 函数获取数据指针；如果是大数组，使用`getDataSegment` 函数获取分块存储的数据。NULL 值判断通过 `DBL_NMIN` 实现。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_development_tutorial.md#_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\nConstantSP logSum(const ConstantSP &x, const ConstantSP &placeholder) {\n    if (((VectorSP) x)->isFastMode()) {\n        int size = x->size();\n        double *data = (double *) x->getDataArray();\n\n        double logSum = 0;\n        for (int i = 0; i < size; i++) {\n            if (data[i] != DBL_NMIN)    // is not NULL\n                logSum += std::log(data[i]);\n        }\n        return new Double(logSum);\n    }\n    else {\n        int size = x->size();\n        int segmentSize = x->getSegmentSize();\n        double **segments = (double **) x->getDataSegment();\n        INDEX start = 0;\n        int segmentId = 0;\n        double logSum = 0;\n\n        while (start < size) {\n            double *block = segments[segmentId];\n            int blockSize = std::min(segmentSize, size - start);\n            for (int i = 0; i < blockSize; i++) {\n                if (block[i] != DBL_NMIN)    // is not NULL\n                    logSum += std::log(block[i]);\n            }\n            start += blockSize;\n            segmentId++;\n        }\n        return new Double(logSum);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Dolphindb Cluster Node Role Assignments in cluster.nodes\nDESCRIPTION: This snippet lists cluster nodes and their roles using a simple CSV format mapping local sites with respective roles such as controller, agent, or datanode. It provides the foundation for the DolphinDB cluster node topology and deployment plan, necessary for ensuring correct traffic routing and service distribution between various cluster components.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/iot_examples.md#_snippet_31\n\nLANGUAGE: cfg\nCODE:\n```\nlocalSite,mode\n192.168.1.12:22216:controller1,controller\n192.168.1.13:22216:controller2,controller\n192.168.1.14:22216:controller3,controller\n192.168.1.12:22215:agent1,agent\n192.168.1.13:22215:agent2,agent\n192.168.1.14:22215:agent3,agent\n192.168.1.12:22217:NODE1,datanode\n192.168.1.12:22218:NODE2,datanode\n192.168.1.13:22217:NODE2241,datanode\n192.168.1.13:22218:NODE2242,datanode\n192.168.1.14:22217:NODE2281,datanode\n192.168.1.14:22218:NODE2282,datanode\n~\n```\n\n----------------------------------------\n\nTITLE: Creating ESP Trade Table with TSDB Engine in DolphinDB\nDESCRIPTION: This DolphinDB code creates a database and table designed for storing ESP trade data using the TSDB storage engine with daily partitions. The table schema includes trading time, security identifiers, execution details, prices, quantities, trading method, and timestamps. The table partitions by createDate and sorts by securityID and createTime, which enables efficient time-series queries for ESP trade data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practices_for_partitioned_storage.md#_snippet_18\n\nLANGUAGE: DolphinDB\nCODE:\n```\ncreate database \"dfs://ESP\"\npartitioned by VALUE(2023.01.01..2023.12.31)\nengine='TSDB'\n\ncreate table \"dfs://ESP\".\"ESPTradetable\"(\n    createDate DATE[comment=\"创建日期\", compress=\"delta\"]  \n    createTime TIME[comment=\"创建时间\", compress=\"delta\"]\n    securityID SYMBOL\n    execId STRING\n    execType STRING\n    lastQty LONG\n    marketIndicator LONG\n    messageSource STRING\n    msgSeqNum LONG\n    msgType STRING\n    price DOUBLE\n    senderCompID STRING\n    stipulationType STRING\n    stipulationValue DOUBLE\n    symbol STRING\n    tradeDate DATE\n    tradeMethod LONG\n    tradeTime TIME\n    tradeType LONG\n    transactTime TIMESTAMP\n)\npartitioned by createDate,\nsortColumns=[`securityID, `createTime]\n```\n\n----------------------------------------\n\nTITLE: Distributed Linear Regression Computation Using DolphinDB SQLDS (DolphinDB script)\nDESCRIPTION: Constructs a distributed dataset via 'sqlDS' from the 'quotes' table, computing two derived variables 'spread' and 'quoteSize' with filtering conditions on date, time, and quote values. The regression model is computed using the 'olsEx' function which performs distributed linear regression on the dataset to analyze the relationship between 'spread' and 'quoteSize'. The last parameters indicate the regression includes an intercept and uses 2 threads or partitions.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Cluster_scale_out_performance_test.md#_snippet_2\n\nLANGUAGE: DolphinDB script\nCODE:\n```\nds = sqlDS(<select ofr/bid-1 as spread, (bidsiz+ofrsiz)*(ofr+bid)\\2 as quoteSize from quotes where date between 2007.08.01 : 2007.08.05, time between 09:30:00 : 15:59:59, ofr>bid, ofr>0, bid>0, ofr/bid<1.2>)\nolsEx(ds, `spread, `quoteSize, true, 2)\n```\n\n----------------------------------------\n\nTITLE: Joining and Aggregating Hourly Average Battery Temperature by Device with MongoDB - JavaScript\nDESCRIPTION: This snippet joins 'device_readings' with 'device_info', filters documents by device ID and time range, applies CPU usage constraints, groups results by device ID and hour to compute average battery temperature and sample standard deviation of CPU, and sorts by average temperature. Inputs include time range, device ID range, and required CPU average. Requires proper indexing and field setup.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs MongoDB/mongodb_readings_query.txt#_snippet_9\n\nLANGUAGE: JavaScript\nCODE:\n```\ndb.device_readings.explain(\"executionStats\").aggregate([{$lookup:{from:\"device_info\",localField:\"device_id\",foreignField:\"device_id\",as:\"device_result\"}},{$match:{device_id:{\"$gte\":\"demo000100\",\"$lte\":\"demo000150\"},time:{\"$gte\":ISODate(\"2016-11-15 12:00:00.000Z\"),\"$lte\":ISODate(\"2016-11-16 12:00:00.000Z\")},cpu_avg_15min:{\"$gte\":5}}},{$group:{_id:{hour_new:{$hour:\"$time\"},device_id:\"$device_id\"},avg_battery_temperature:{$avg:\"$battery_temperature\"},std_cpu_avg_15min:{$stdDevSamp:\"$cpu_avg_15min\"}}},{$sort:{\"avg_battery_temperature\":-1,\"device_id\":1}}])\n```\n\n----------------------------------------\n\nTITLE: Displaying System CPU Information (Bash Output)\nDESCRIPTION: Shows the output of a command (likely `lscpu` or similar) detailing the system's CPU architecture, core count, threading, cache sizes, and other hardware specifics. This information provides context for the performance test environment.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nArchitecture:          x86_64\nCPU 运行模式：    32-bit, 64-bit\nByte Order:            Little Endian\nCPU(s):                12\nOn-line CPU(s) list:   0-11\n每个核的线程数：2\n每个座的核数：  6\nSocket(s):             1\nNUMA 节点：         1\n厂商 ID：           GenuineIntel\nCPU 系列：          6\n型号：              158\nModel name:            Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz\n步进：              10\nCPU MHz：             899.990\nCPU max MHz:           4600.0000\nCPU min MHz:           800.0000\nBogoMIPS:              6384.00\n虚拟化：           VT-x\nL1d 缓存：          32K\nL1i 缓存：          32K\nL2 缓存：           256K\nL3 缓存：           12288K\nNUMA node0 CPU(s):     0-11\n```\n\n----------------------------------------\n\nTITLE: C++ API - Submit Replay Task\nDESCRIPTION: This C++ code snippet demonstrates submitting a replay task via the DolphinDB C++ API using the `stkReplay` function. It initializes the arguments and calls the `run` method of the `DBConnection` object (`conn`) to execute the function, retrieving the result in a dictionary. Error checking is then performed by checking the `errorCode` returned in the result. It also generates a unique identifier for each replay task. Prerequisites are a working C++ DolphinDB API setup and the `stkReplay` function available within the DolphinDB server.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/appendices_market_replay_bp.md#_snippet_6\n\nLANGUAGE: C++\nCODE:\n```\nDictionarySP result = conn.run(\"stkReplay\", args);\nstring errorCode = result->get(Util::createString(\"errorCode\"))->getString();\nif (errorCode != \"1\") \n{\n    std::cout << result->getString() << endl;\n    return -1;\n}\n```\n\n----------------------------------------\n\nTITLE: 使用OLAP引擎创建宽表示例\nDESCRIPTION: 使用OLAP引擎创建一个包含203列的宽表，其中包含ID、日期、时间和200个因子列。OLAP为默认引擎，不适合存储列数过多的宽表。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/database_and_table_creation_details.md#_snippet_20\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndb= database(\"dfs://testDB1\", VALUE, 2020.01.01..2021.01.01)\n//202行的宽表，使用OLAP引擎（默认为OLAP）\ncolName = `ID`Date`Time\ncolName.append!(take(\"factor\"+string(0..200),200))\ncolType = `SYMBOL`DATE\ncolType.append!(take([\"DOUBLE\"],200))\ntbSchema = table(1:0, colName, colType)\ndb.createPartitionedTable(table=tbSchema, tableName=`pt, partitionColumns=`Date)\n```\n\n----------------------------------------\n\nTITLE: Calculating Drawdown Ratio (Calmar Ratio) in DolphinDB\nDESCRIPTION: Defines a DolphinDB function `getDrawdownRatio` to compute the Drawdown Ratio (often Calmar Ratio). It takes a vector `value` (daily net values) and calculates the ratio of annualized return to maximum drawdown, using the previously defined `getAnnualReturn` and `getMaxDrawdown` functions.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/fund_factor_contrasted_by_py.md#_snippet_12\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef getDrawdownRatio(value){\n\treturn getAnnualReturn(value) \\ getMaxDrawdown(value)\n}\n```\n\n----------------------------------------\n\nTITLE: Replaying Historical Stock Market Data to Stream Table - DolphinDB Script\nDESCRIPTION: Implements logic to replay historical 'order', 'trade', and 'snapshot' records by loading from DFS tables, partitioning by time intervals, and submitting each to the streaming workflow for simulated real-time processing. Inputs include data queries, time partitioning schema, and output stream table; depends on replayDS, submitJob, and prior table definitions. Outputs are injected to the stream for downstream filtering and Kafka publishing.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/stock_market_replay/04.publishToKafka.txt#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef replayStockMarketData(){\n\ttimeRS = cutPoints(09:15:00.000..15:00:00.000, 100)\n\torderDS = replayDS(sqlObj=<select * from loadTable(\"dfs://order\", \"order\") where Date = 2020.12.31>, dateColumn=`Date, timeColumn=`Time, timeRepartitionSchema=timeRS)\n\ttradeDS = replayDS(sqlObj=<select * from loadTable(\"dfs://trade\", \"trade\") where Date = 2020.12.31>, dateColumn=`Date, timeColumn=`Time, timeRepartitionSchema=timeRS)\n\tsnapshotDS = replayDS(sqlObj=<select * from loadTable(\"dfs://snapshot\", \"snapshot\") where Date =2020.12.31>, dateColumn=`Date, timeColumn=`Time, timeRepartitionSchema=timeRS)\n\tinputDict = dict([\"order\", \"trade\", \"snapshot\"], [orderDS, tradeDS, snapshotDS])\n\t\n\tsubmitJob(\"replay\", \"replay stock market\", replay, inputDict, messageStream, `Date, `Time, 20000, true, 3)\n}\nreplayStockMarketData()\n```\n\n----------------------------------------\n\nTITLE: Helper Function for Simulating Duplicate Data - DolphinDB\nDESCRIPTION: This helper function is used by `genData` to simulate the generation of multiple records with the same `doorEventCode` at slightly different times. It appends `num` rows to a mutable stream table `st`, setting the `doorEventCode` and `eventDate` based on input parameters, while randomizing other fields.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/streaming_engine_anomaly_alerts/流计算引擎级联检测门禁超时数据.txt#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 模拟写入验证\n// 更新产生数据情况 需求 按照轮询时间记录 每条数据的时间都不一样 一轮引擎去掉重复数据 二轮引擎检测超时数据 三轮去掉关门告警\ndef duplicateData(mutable st, num, doorCode, time){\n    for(i in 0:num){\n        eventTime = time\n        st.append!(table(rand(0..5,1) as recordType, doorCode as doorEventCode, eventTime as eventDate, rand([true,false],1) as readerType, rand(`a+string(1000..1010),1) as sn, 1 as doorNum, rand(`ic+string(100000..100000),1) as card))\n        eventTime = datetimeAdd(eventTime, 5, `s)\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring DataX Job with DolphinDB Writer and OceanBase Reader in JSON\nDESCRIPTION: This JSON snippet defines a DataX job configuration for migrating data from OceanBase to DolphinDB. The writer segment specifies connection parameters to DolphinDB, including database path, target table, user credentials, batch size, and detailed schema with DolphinDB data types. The reader segment uses the OceanBasev10Reader plugin configured with connection URL and authentication details to read data from OceanBase 'tick' table. Key parameters include 'batchSize', 'column' selection, and the JDBC URL for database access. It expects this JSON to be placed in a custom directory for execution.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/OceanBase_to_DolphinDB.md#_snippet_4\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    \"job\": {\n        \"setting\": {\n            \"speed\": {\n                \"channel\":1\n            }\n        },\n        \"content\": [\n            {\n                \"writer\": {\n                    \"parameter\": {\n                        \"dbPath\": \"dfs://TSDB_tick\",\n                        \"tableName\": \"tick\",\n                        \"userId\": \"admin\",\n                        \"pwd\": \"123456\",\n                        \"host\": \"127.0.0.1\",\n                        \"batchSize\": 200000,\n                        \"table\": [\n                            {\n                                \"type\": \"DT_SYMBOL\",\n                                \"name\": \"SecurityID\"\n                            },\n                            {\n                                \"type\": \"DT_TIMESTAMP\",\n                                \"name\": \"TradeTime\"\n                            },\n                            {\n                                \"type\": \"DT_DOUBLE\",\n                                \"name\": \"TradePrice\"\n                            },\n                            {\n                                \"type\": \"DT_INT\",\n                                \"name\": \"TradeQty\"\n                            },\n                            {\n                                \"type\": \"DT_DOUBLE\",\n                                \"name\": \"TradeAmount\"\n                            },\n                            {\n                                \"type\": \"DT_INT\",\n                                \"name\": \"BuyNo\"\n                            },\n                            {\n                                \"type\": \"DT_INT\",\n                                \"name\": \"SellNo\"\n                            },\n                            {\n                                \"type\": \"DT_INT\",\n                                \"name\": \"TradeIndex\"\n                            },\n                            {\n                                \"type\": \"DT_INT\",\n                                \"name\": \"ChannelNo\"\n                            },\n                            {\n                                \"type\": \"DT_SYMBOL\",\n                                \"name\": \"TradeBSFlag\"\n                            },\n                            {\n                                \"type\": \"DT_INT\",\n                                \"name\": \"BizIndex\"\n                            }\n                        ],\n                        \"port\": 8800\n                    },\n                    \"name\": \"dolphindbwriter\"\n                },\n                \"reader\": {\n                    \"name\": \"oceanbasev10reader\",\n                    \"parameter\": {\n                        \"username\": \"root\",\n                        \"password\": \"123456\",\n                        \"batchSize\":10000,\n                        \"column\": [\n                            \"*\"\n                        ],\n                        \"connection\": [\n                            {\n                                \"table\": [\n                                    \"tick\"\n                                ],\n                                \"jdbcUrl\": [\n                                    \"jdbc:oceanbase://127.0.0.1:2883/db1\"\n                                ]\n                            }\n                        ]\n                    }\n                }\n            }\n        ]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Partial Functions in DolphinDB C++ Plugin\nDESCRIPTION: This snippet demonstrates how to create partial functions in a DolphinDB C++ plugin using `Util::createPartialFunction`. It involves creating a system function and then creating a partial application by fixing some of its arguments.  It also provides an example of how to use `createPartialFunction` with DolphinDB built-in function `append!`.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_advance.md#_snippet_6\n\nLANGUAGE: c++\nCODE:\n```\nConstantSP myFunc1(Heap* heap, vector<ConstantSP>& arguments) {\n    if (arguments[0]->getType() != DT_INT || arguments[1]->getType() != DT_INT) {\n        throw IllegalArgumentException(\"myFunc1\", \"argument must be two integral scalars!\");\n    }\n    int a = arguments[0]->getInt();\n    int b = arguments[1]->getInt();\n    int result = a * b - (a + b);\n    return new Int(result);\n}\nFunctionDefSP myFunc2(Heap* heap, vector<ConstantSP>& arguments) {\n    FunctionDefSP temFunc = Util::createSystemFunction(\"temFunc\", myFunc1, 2, 2, false);\n    ConstantSP a = new Int(10);   \n    vector<ConstantSP> args = {a}; \n    return Util::createPartialFunction(temFunc, args); //固定第一个参数为10\n}\n\n```\n\nLANGUAGE: c++\nCODE:\n```\nConstantSP subscribeTag(Heap *heap, vector<ConstantSP> &arguments) {\n    std::string usage = \"Usage: subscribe(conn,Tag,handler). \";\n\n    OPCClient *conn;\n\n    //skipped...\n\n    if (!arguments[2]->isTable() && (arguments[2]->getType() != DT_FUNCTIONDEF)) {\n        throw IllegalArgumentException(__FUNCTION__, usage + \"handler must be a  table or a unary function.\");\n    }else if (arguments[2]->getType() == DT_FUNCTIONDEF) {\n        if (FunctionDefSP(arguments[2])->getMaxParamCount() < 1 || FunctionDefSP(arguments[2])->getMinParamCount() > 1)\n            throw IllegalArgumentException(__FUNCTION__, usage + \"handler must be a table or a unary function.\");\n    }\n\n    FunctionDefSP handler;\n    //skipped...\n\n\n    if (arguments[2]->getType() == DT_FUNCTIONDEF) {\n        handler = FunctionDefSP(arguments[2]);\n    } else {\n        TableSP table = arguments[2];\n        FunctionDefSP func = conn->session->getFunctionDef(\"append!\");\n        vector<ConstantSP> params(1, table);\n        handler = Util::createPartialFunction(func, params);\n    }\n\n    //skipped...\n\n    return new Void();\n}\n```\n\n----------------------------------------\n\nTITLE: Querying OrderBook Data with Timer in DolphinDB\nDESCRIPTION: Loads a Level2 order book table 'entrust' from a DFS database and performs a select query applying the openBidVolDvdAskVol function to compute the logarithmic order quantity ratio. The data is filtered for trades on 2023-02-01 between 09:30 and 10:30. Aggregations are grouped by trading date and SecurityID. All operations are wrapped inside a DolphinDB timer block, enabling periodic execution of this query logic.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/DolphinDB_Python_Parser_Intro_for_Quantitative_Finance/因子实现_DolphinDB版本/早盘买卖单大小比.txt#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntimer{\n\torderTB = loadTable(\"dfs://TL_Level2\", \"entrust\")\n\tres = select openBidVolDvdAskVol(OrderQty, Side) as openBidVolDvdAskVol\n\t\tfrom orderTB\n\t\twhere date(TradeTime)=2023.02.01 and time(TradeTime)>=09:30:00.000 and time(TradeTime)<=10:30:00.000\n\t\tgroup by date(TradeTime) as TradeTime, SecurityID\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Large Test CSV File in DolphinDB\nDESCRIPTION: This script generates a large CSV file (~4GB) for performance testing purposes. It creates an in-memory table with random data across different types (INT, STRING, DATE, FLOAT, TIME) and then saves it to the specified file path using `saveText`.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_csv.md#_snippet_4\n\nLANGUAGE: dolphindb\nCODE:\n```\nfilePath=\"/home/data/testFile.csv\"\nappendRows=100000000\nt=table(rand(100,appendRows) as int,take(string('A'..'Z'),appendRows) as symbol,take(2010.01.01..2018.12.30,appendRows) as date,rand(float(100),appendRows) as float,00:00:00.000 + rand(86400000,appendRows) as time)\nt.saveText(filePath);\n```\n\n----------------------------------------\n\nTITLE: Standard Group By Query in DolphinDB SQL\nDESCRIPTION: A standard group by query without the map keyword, which performs both partition-level and global aggregation calculations.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_SQL_Case_Tutorial.md#_snippet_12\n\nLANGUAGE: SQL\nCODE:\n```\ntimer result = select count(*) from snapshot group by SecurityID, bar(DateTime, 60)\n```\n\n----------------------------------------\n\nTITLE: Verifying successful DolphinDB Python API installation\nDESCRIPTION: Test script to import dolphindb, start a session, and confirm correct setup by creating a session object, indicating successful installation. Dependencies include Python with dolphindb package installed.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/python_api_install_offline.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nimport dolphindb as ddb\ns = ddb.session()\nprint(s)\n```\n\n----------------------------------------\n\nTITLE: Deleting a User via DolphinDB Admin Script - DolphinDB\nDESCRIPTION: This snippet depicts user deletion performed by an administrator in DolphinDB using 'login' and 'deleteUser'. Inputs are admin credentials and the username to be deleted. Attempting login as the deleted user confirms successful removal. Only admins can execute deletion, and deleted users cannot be restored.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Permission_Management.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(\"admin1\",\"123456\")//登陆管理员用户\ndeleteUser(\"user1\")//删除用户\nlogin(\"user1\",\"123456\")//用户删除成功\n=> The user name or password is incorrect\n```\n\n----------------------------------------\n\nTITLE: Partition Pruning Examples (Cannot Optimize)\nDESCRIPTION: These code snippets illustrate various scenarios where partition pruning cannot be effectively applied. These include using arithmetic operations on partition fields, chained comparisons, non-partition field filtering, and comparisons with other columns.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_sql_cases.md#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect count(*) from snapshot where date(DateTime) + 1 > 2020.06.01\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect count(*) from snapshot where 2020.06.01 < date(DateTime) < 2020.06.03\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect count(*) from snapshot where Volume < 500\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect count(*) from snapshot where date(DateTime) < AnnouncementDate - 3\n```\n\n----------------------------------------\n\nTITLE: Filtering by Bid Range and Aggregating Average Bid Size by Symbol using urllib3\nDESCRIPTION: This function uses `urllib3` to query Elasticsearch. It filters documents where the BID field is within a specific range (35-40), aggregates by SYMBOL to calculate the average BIDSIZ, and sorts the resulting aggregation buckets by SYMBOL ascending.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_66\n\nLANGUAGE: python\nCODE:\n```\nimport urllib3\nimport json\n\ndef search_6():\n    http = urllib3.PoolManager()\n    data = json.dumps({\n        \"query\": {\n            \"constant_score\": {\n                \"filter\": {\n                    \"range\": {\n                        \"BID\": {\n                            \"gte\": 35,\n                            \"lte\": 40\n                        }\n                    }\n                }\n            }\n        },\n        \"aggs\": {\n            \"group_by_symbol\": {\n                \"terms\": {\n                    \"field\": \"SYMBOL\",\n                    \"size\": 8374\n                },\n                \"aggs\": {\n                    \"avg_BIDSIZ\": {\n                        \"avg\": {\"field\": \"BIDSIZ\"}\n                    }\n\n                }\n            }\n        },\n        \"sort\": [\n            {\n                \"SYMBOL\": {\"order\": \"asc\"}\n            }\n        ],\n        \"_source\": [\"\"]\n    }).encode(\"utf-8\")\n    r = http.request(\"GET\", \"http://localhost:9200/hundred/_search\", body=data,\n                     headers={'Content-Type': 'application/json'})\n    print(r.status)\n    print(json.loads(r.data.decode()))\n```\n\n----------------------------------------\n\nTITLE: 分析SQL查询执行计划\nDESCRIPTION: 此SQL示例演示分布式查询的执行流程，包括parse、map、merge、reduce四个阶段，说明计算节点在merge和reduce中的作用。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Compute_Node.md#_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\npt1 = loadTable(database(\"dfs://szstock\"),\"sztrade\")\nselect [HINT_EXPLAIN]\n    securityID, min(price) as LowPx, max(price) as HighPx, sum(tradeQty) as vol\n    , first(price) as OpenPx, last(price) as LastPx, sum(tradeQty * price) as val\nfrom pt\nwhere execType=\"F\"\ngroup by tradedate,securityID,minute(temporalParse(tradetime$STRING, \"yyyyMMddHHmmssSSS\")) as minute\norder by tradedate,securityID, minute\n```\n\n----------------------------------------\n\nTITLE: Aggregating TopN Using Built-in aggrTopN Function - DolphinDB Script\nDESCRIPTION: Illustrates the built-in aggrTopN function to compute the grouped average of values below a dynamic TopN threshold. Groups by trade_date and secu_code on table 'tb', sorts and selects the lowest 40% of 'value', and applies 'avg'. Parameters: funcArgs (target column), sortingCol (column to sort/filter), top (fractional TopN), ascending (order direction). Outputs a grouped table with calculated averages. This replaces custom user aggregation for percentile-based stock analytics.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_TopN.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect aggrTopN(avg, funcArgs=value, sortingCol=value, top=0.4, ascending=true) as factor_value from tb group by trade_date,secu_code\n```\n\n----------------------------------------\n\nTITLE: Creating Reactive State Engine with JIT-Optimized Multi-Column Metrics\nDESCRIPTION: Initializes 'reactiveDemo3' to perform pressure calculations with just-in-time (JIT) compilation optimizations for multiple bid and offer columns, enhancing performance for complex real-time computations.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Streaming_computing_of_financial_quantifiers.md#_snippet_28\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// Drop existing engine if exists\ntry{ dropStreamEngine(\"reactiveDemo3\")} catch(ex){ print(ex) }\n// Define JIT-optimized metrics\nmetrics3 = <[dateTime, averagePressJIT(bidPrice0, bidPrice1, bidPrice2, bidPrice3, bidPrice4, bidPrice5, bidPrice6, bidPrice7, bidPrice8, bidPrice9, bidOrderQty0, bidOrderQty1, bidOrderQty2, bidOrderQty3, bidOrderQty4, bidOrderQty5, bidOrderQty6, bidOrderQty7, bidOrderQty8, bidOrderQty9, offerPrice0, offerPrice1, offerPrice2, offerPrice3, offerPrice4, offerPrice5, offerPrice6, offerPrice7, offerPrice8, offerPrice9, offerOrderQty0, offerOrderQty1, offerOrderQty2, offerOrderQty3, offerOrderQty4, offerOrderQty5, offerOrderQty6, offerOrderQty7, offerOrderQty8, offerOrderQty9, 60)]>\n// Instantiate engine\nrse3 = createReactiveStateEngine(name=\"reactiveDemo3\", metrics=metrics3, dummyTable=inputTable, outputTable=resultTable3, keyColumn=\"securityID\", keepOrder=true)\n```\n\n----------------------------------------\n\nTITLE: Define level10Diff UDF in DolphinDB\nDESCRIPTION: Defines a user-defined function `level10Diff` in DolphinDB to calculate the difference in price and quantity between the current and previous values, considering bid or ask sides. It uses `rowAlign` to align the price series, calculates the quantity difference (`qtyDiff`), computes the amount difference (`amtDiff`) using `rowSum` and `nullFill`, and then calculates the moving sum (`msum`) over a specified lag. The function takes `price`, `qty`, `buy` (boolean), and `lag` (integer) as input.  It relies on `prev()`, `rowAlign()`, `rowAt()`, `nullFill()`, `rowSum()`, and `msum()` DolphinDB functions.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/DolphinDB_Python_Parser_Intro_for_Quantitative_Finance/因子实现_DolphinDB版本/十档净委买增额.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\n@state\ndef level10Diff(price, qty, buy=true, lag=20){\n        prevPrice = price.prev()\n        left, right = rowAlign(price, prevPrice, how=iif(buy, \"bid\", \"ask\"))\n        qtyDiff = (qty.rowAt(left).nullFill(0) - qty.prev().rowAt(right).nullFill(0)) \n        amtDiff = rowSum(nullFill(price.rowAt(left), prevPrice.rowAt(right)) * qtyDiff)\n        return msum(amtDiff, lag, 1).nullFill(0)\n}\n```\n\n----------------------------------------\n\nTITLE: Submitting Parallel Job 2 (Single User) in DolphinDB\nDESCRIPTION: This code submits `parJob2` as a single-user job.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_factor.txt#_snippet_18\n\nLANGUAGE: DolphinDB\nCODE:\n```\n/**\n * 提交1个job（单用户）\n */\nsubmitJob(\"parallJob1\", \"parallJob_single_ten\", parJob2)\n```\n\n----------------------------------------\n\nTITLE: Defining Prediction Handler and Subscribing in DolphinDB\nDESCRIPTION: Defines a handler function `predictRV` that takes the aggregated features table (`msg`) and the pre-loaded `model` as input. It measures the prediction latency, uses the model's `predict` method to generate `PredictRV`, selects relevant columns, and appends the results to the `result1min` stream table. Subsequently, it subscribes this handler to the `aggrFeatures10min` table using `subscribeTable`, ensuring predictions are made as soon as new features are calculated.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/machine_learning_volatility/06.streamComputingReproduction.txt#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//define handler\ndef predictRV(mutable result1min, model, msg){\n\tstartTime = now()\n\tpredicted = model.predict(msg)\n\ttemp = select TradeTime, SecurityID, predicted as PredictRV, (now()-startTime) as CostTime from msg\n\tresult1min.append!(temp)\n}\n//subscribe data\nsubscribeTable(tableName=\"aggrFeatures10min\", actionName=\"predictRV\", offset=-1, handler=predictRV{result1min, model}, msgAsTable=true, hash=1, reconnect=true)\ngo\n```\n\n----------------------------------------\n\nTITLE: Defining systemd Service for DolphinDB Agent Node (systemd)\nDESCRIPTION: This systemd unit file defines the DolphinDB agent node as a backgrounded Linux system service, managed via the agent.sh script's start, stop, and restart actions. The service will automatically restart on failure, supports reload, and enforces resource usage limits. Inputs: requires properly set WorkingDirectory and a valid agent.sh file. Outputs: Exposes standard service lifecycle via systemctl. Limitations: Modify WorkingDirectory to match deployment path.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ha_cluster_deployment.md#_snippet_17\n\nLANGUAGE: systemd\nCODE:\n```\n[Unit]\nDescription=ddbagent\nDocumentation=https://www.dolphindb.com/\n\n[Service]\nType=forking\nWorkingDirectory=/home/DolphinDB/server/clusterDemo\nExecStart=/bin/sh agent.sh start\nExecStop=/bin/sh agent.sh stop\nExecReload=/bin/sh agent.sh restart\nRestart=always\nRestartSec=10s\nLimitNOFILE=infinity\nLimitNPROC=infinity\nLimitCORE=infinity\n\n[Install]\nWantedBy=multi-user.target\n```\n\n----------------------------------------\n\nTITLE: Aligning Matrices by Row - DolphinDB\nDESCRIPTION: This code snippet demonstrates how to align two matrices (m1 and m2) by row labels using the align function in DolphinDB. It renames the row labels and uses the 'fj' (full join) alignment type. The last parameter `false` seems to control null value replacement. The code shows how to access the aligned matrices using indexing.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/matrix.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\n>x1 = [09:00:00, 09:00:01, 09:00:03]\n>x2 = [09:00:00, 09:00:02, 09:00:03, 09:00:04]\n>m1 = matrix(1 2 3, 2 3 4, 3 4 5).rename!(x1)\n>m2 = matrix(1 2 3, 2 3 4, 3 4 5, 4 5 6).rename!(x2)\n>m = align(m1, m3, 'fj', false);\n>m[0];\n09:00:00\t09:00:01\t09:00:03\t09:00:04\n1\t2\t3\t\n2\t3\t4\t\n3\t4\t5\t\n\n>m[1];\n09:00:00\t09:00:01\t09:00:03\t09:00:04\n1\t2\t\t3\n2\t3\t\t4\n3\t4\t\t5\n4\t5\t\t6\n```\n\n----------------------------------------\n\nTITLE: Defining consume Function\nDESCRIPTION: This function sets up the reactive state engine and session window engine, and subscribes the `inputSt` table to the `process` handler.  The reactive engine filters for changes in the 'value' column, outputting to `outputSt1`. The session window engine calculates the last value within 30-second sessions, outputting to `outputSt2`. The handler function receives data from the input stream and appends it to both engines for real-time processing.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/data/DolphinDB_streaming_application_in_IOT/deviceState.txt#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef consume(){\n\treactivEngine = createReactiveStateEngine(name=`reactivEngine, metrics=<[ts, value]>, dummyTable=objByName(`inputSt),outputTable= objByName(`outputSt1),keyColumn= \"tag\",filter=<value!=prev(value) && prev(value)!=NULL>)\n\t\n\tswEngine = createSessionWindowEngine(name = \"swEngine\", sessionGap = 30000, metrics = < last(value)>, dummyTable = objByName(`inputSt), outputTable = objByName(`outputSt2), timeColumn = `ts, keyColumn=`tag,useSessionStartTime=false)\n\t\n\tsubscribeTable(tableName=\"inputSt\", actionName=\"monitor\", offset=0, \n\t\t\thandler=process{swEngine,reactivEngine}, msgAsTable=true)\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Reactive State Engine with Snapshot Mechanism\nDESCRIPTION: This code demonstrates how to enable the snapshot mechanism for the Reactive State Engine. It sets up the engine with `snapshotDir` and `snapshotIntervalInMsgCount` parameters and then retrieves the last processed message ID and subscribes to the stream table with `offset`. The mechanism allows for resuming operations from a saved state, improving reliability.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/reactive_state_engine.md#_snippet_8\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\ndef sum_diff(x, y){\n    return (x-y)/(x+y)\n}\nfactor1 = <ema(1000 * sum_diff(ema(price, 20), ema(price, 40)),10) -  ema(1000 * sum_diff(ema(price, 20), ema(price, 40)), 20)>\n\nshare streamTable(1:0, `sym`price, [STRING,DOUBLE]) as tickStream\nresult = table(1000:0, `sym`factor1, [STRING,DOUBLE])\nrse = createReactiveStateEngine(name=\"reactiveDemo\", metrics =factor1, dummyTable=tickStream, outputTable=result, keyColumn=\"sym\", snapshotDir= \"/home/data/snapshot\", snapshotIntervalInMsgCount=400000)\nmsgId = getSnapshotMsgId(rse)\nif(msgId >= 0) msgId += 1\nsubscribeTable(tableName=`tickStream, actionName=\"factors\", offset=msgId, handler=appendMsg{rse}, handlerNeedMsgId=true)\n```\n\n----------------------------------------\n\nTITLE: Creating TSDB database and table schema\nDESCRIPTION: This code snippet demonstrates how to create a database and a partitioned table using the TSDB storage engine in DolphinDB.  It defines the database partitions based on time and stock ID, creates a table schema with Timestamp, StockID, and Bid columns, and specifies `StockID` and `Timestamp` as the sorting columns for efficient querying.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/tsdb_engine.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndbTime = database(\"\", VALUE, 2021.08.01..2021.09.01)\ndbStockID = database(\"\", HASH, [SYMBOL, 100])\n\ndb = database(directory=\"dfs://stock\",partitionType=COMPO,partitionScheme=[dbTime,dbStockID],engine=\"TSDB\")\n\nschema = table(1:0, `Timestamp`StockID`bid, [TIMESTAMP, SYMBOL, DOUBLE])\nstocks = db.createPartitionedTable(table=schema, tableName=`stocks, partitionColumns=`Timestamp`StockID, sortColumns=`StockID`Timestamp)\n```\n\n----------------------------------------\n\nTITLE: Backing Up DolphinDB Partitioned Table Data with Optional Filtering\nDESCRIPTION: Backs up all data or filtered subsets from the partitioned table \"pt\" in the composite database \"dfs://compoDB\" to a specified backup directory. Uses the built-in 'backup' function with parameters for backup path, data source (a select query on the table), and an overwrite flag. Filtering can be applied using SQL where clauses (e.g., date > 2017.08.10). The expected input is the data in \"pt\" or its filtered subsets, and output files are stored under the given backup directory. Dependencies include access to the DolphinDB database and backup APIs. This snippet facilitates data preservation and selective backup operations.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/restore-backup.md#_snippet_10\n\nLANGUAGE: DolphinDB\nCODE:\n```\nbackup(\"/home/DolphinDB/backup\",<select * from loadTable(\"dfs://compoDB\",\"pt\")>,true);\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nbackup(\"/home/DolphinDB/backup\",<select * from loadTable(\"dfs://compoDB\",\"pt\") where date>2017.08.10>,true);\n```\n\n----------------------------------------\n\nTITLE: Simulating High-Frequency Financial Data Table - DolphinDB Script\nDESCRIPTION: Presents script to generate synthetic minute-level trading table 't' for multiple securities. Assigns datetime, security codes, open/high/low/close prices, and volume using columnwise operations and vectorized joins across securities. Used as a basis for illustrating windowed TopN financial calculations. Outputs a structured DolphinDB table. Dependencies: core DolphinDB core vector and table operators.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_TopN.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\nn = 5*121\ntimeVector = 2023.04.30T09:30:00.000 + 0..120 * 60000\ntradingTime = take(timeVector,n)\nwindCode = stretch(format(600001..600005, \"000000\") + \".SH\", n)\nopen = (20.00+0.01*0..120) join (30.00-0.01*0..120) join (40.00+0.01*0..120) join (50.00-0.01*0..120) join (60.00+0.01*0..120)\nhigh = (20.50+0.01*0..120) join (31.00-0.01*0..120) join (40.80+0.01*0..120) join (50.90-0.01*0..120) join (60.70+0.01*0..120)\nlow = (19.50+0.01*0..120) join (29.00-0.01*0..120) join (39.00+0.01*0..120) join (48.00-0.01*0..120) join (59.00+0.01*0..120)\nclose = (20.00+0.01*0..120) join (30.00-0.01*0..120) join (40.00+0.01*0..120) join (50.00-0.01*0..120) join (60.00+0.01*0..120)\nvolume= 10000+ take(-100..100,n)\nt = table(tradingTime, windCode, open, high, low, close, volume)\n```\n\n----------------------------------------\n\nTITLE: Generating Stock and Futures Data\nDESCRIPTION: This code generates a dataset containing both stock and futures data and saves it to a CSV file. It creates a table with random data for various columns like type (stock/futures), symbol, date, price, and quantity. The table is then saved to the specified file path.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_csv.md#_snippet_25\n\nLANGUAGE: DolphinDB\nCODE:\n```\nn=10000000\ndataFilePath=\"/home/data/chunkText.csv\"\ntrades=table(rand(`stock`futures,n) as type, rand(`IBM`MSFT`GM`C`FB`GOOG`V`F`XOM`AMZN`TSLA`PG`S,n) as sym,take(2000.01.01..2000.06.30,n) as date,10.0+rand(2.0,n) as price1,100.0+rand(20.0,n) as price2,1000.0+rand(200.0,n) as price3,10000.0+rand(2000.0,n) as price4,10000.0+rand(3000.0,n) as price5,10000.0+rand(4000.0,n) as price6,rand(10,n) as qty1,rand(100,n) as qty2,rand(1000,n) as qty3,rand(10000,n) as qty4,rand(10000,n) as qty5,rand(10000,n) as qty6)\ntrades.saveText(dataFilePath);\n```\n\n----------------------------------------\n\nTITLE: Creating Sets in DolphinDB C++ Plugin\nDESCRIPTION: This snippet demonstrates how to create sets in a DolphinDB C++ plugin using the `Util::creatSet` function.  The function takes a data type, a SymbolBaseSP, and an initial capacity as arguments. SymbolBaseSP is relevant for symbol types; otherwise, it can be set to nullptr.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_advance.md#_snippet_3\n\nLANGUAGE: c++\nCODE:\n```\n//创建一个初始容量为0的float类型的集合，第二个参数为SymbolBaseSP，与symbol类型相关，常设置为nullptr\nSetSP s = Util::createSet(DT_FLOAT, nullptr, 0);   \ns->append(new Float(2.5));                         //相当于s.append!(2.5)\n```\n\n----------------------------------------\n\nTITLE: Checking DolphinDB Backup Integrity (Table Level)\nDESCRIPTION: Demonstrates using `checkBackup` to verify the integrity of a specific table (`quotes_2`) within a database backup located in `backupDir`. This targets the check to a single table, potentially reducing execution time compared to checking the entire database.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/backup-restore-new.md#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\ncheckBackup(backupDir,dbPath,`quotes_2)\n```\n\n----------------------------------------\n\nTITLE: Creating a scheduled job with email notification in DolphinDB\nDESCRIPTION: Example showing how to set up a job with email notification upon completion using the onComplete callback parameter and HttpClient plugin.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/scheduledJob.md#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef sendEmail(jobId, jobDesc, success, result){\ndesc = \"jobId=\" + jobId + \" jobDesc=\" + jobDesc\nif(success){\ndesc += \" successful \" + result\nres = httpClient::sendEmail('patrick.mahomes@dolphindb.com','password','andy.reid@dolphindb.com','This is a subject',desc)\n}\nelse{\ndesc += \" with error: \" + result\nres = httpClient::sendEmail('patrick.mahomes@dolphindb.com','password','andy.reid@dolphindb.com','This is a subject',desc)\n}\n}\nscheduleJob(jobId=`PnL, jobDesc=\"Calculate Profit & Loss\", jobFunc=run{\"PnL.dos\"}, scheduleTime=[12:00m, 02\n```\n\n----------------------------------------\n\nTITLE: Defining and Sharing Stream Tables for Real-Time Processing - DolphinDB\nDESCRIPTION: Creates three stream tables in DolphinDB: \"snapshotStream\" for real-time market data, \"aggrFeatures10min\" for storing extracted features, and \"result1min\" for storing model predictions. Each table is created with a detailed schema specifying column names and types required by downstream processing functions.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/machine_learning_volatility/04.streamComputing.txt#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\nshare streamTable(100000:0, name, type) as snapshotStream\nshare streamTable(100000:0 , `TradeTime`SecurityID`BAS`DI0`DI1`DI2`DI3`DI4`DI5`DI6`DI7`DI8`DI9`Press`RV,`TIMESTAMP`SYMBOL`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE) as aggrFeatures10min\nshare streamTable(100000:0 , `TradeTime`SecurityID`PredictRV`CostTime, `TIMESTAMP`SYMBOL`DOUBLE`INT) as result1min\ngo\n\n```\n\n----------------------------------------\n\nTITLE: 循环实现N股VWAP计算\nDESCRIPTION: 通过自定义聚合函数lastVolPx1使用循环方式，计算每只股票最近1000股相关交易的VWAP。这是未优化版本的实现。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_SQL_Case_Tutorial.md#_snippet_21\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndefg lastVolPx1(price, vol, bound) {\n\tsize = price.size()\n\tcumSum = 0\n\tfor(i in 0:size) {\n\t\tcumSum = cumSum + vol[size - 1 - i]\n\t\tif(cumSum >= bound) {\n\t\t\tprice_tmp = price.subarray(size - 1 - i :)\n\t\t\tvol_tmp = vol.subarray(size - 1 - i :)\n\t\t\treturn wavg(price_tmp, vol_tmp)\n\t\t}\n\t\tif(i == size - 1 && cumSum < bound) {\n\t\t\treturn wavg(price, vol)\n\t\t}\n\t}\n}\n\ntimer lastVolPx_t1 = select lastVolPx1(price, vol, 1000) as lastVolPx from t group by sym\n```\n\n----------------------------------------\n\nTITLE: 使用window join计算历史交易量\nDESCRIPTION: 通过window join函数计算每条记录之前一分钟内的总交易量。window join可以为时间序列数据的每个记录定义一个时间窗口，并在该窗口内进行聚合计算。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/quant_finance_examples.md#_snippet_12\n\nLANGUAGE: DolphinDB\nCODE:\n```\nt=select symbol, date, time, curVol, askPrice1, bidPrice1 from quotes where date=2020.06.01, symbol>=`600000, time between 09:30:00.000 : 15:00:00.000 order by symbol, date, time\nt1 = wj(t, t, -60000:-1, <sum(curVol) as sumVolumePrev1m>, `symbol`date`time)\n```\n\n----------------------------------------\n\nTITLE: Applying byRow High-Order Function to Compute Row-Wise Maximum Index in DolphinDB Matrix\nDESCRIPTION: Generates a matrix 'm' with four rows and three columns using given vectors. Demonstrates three methods to compute the index of the maximum value per row: transposing the matrix and applying 'imax' column-wise, applying the 'byRow' high-order function with 'imax' for row-wise computation, and directly calling the specialized 'rowImax' function. These methods produce the same result, with the 'byRow' function avoiding an explicit transpose operation for efficiency and clarity. Requires DolphinDB environment and knowledge of matrix operations.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/func_progr_cases.md#_snippet_10\n\nLANGUAGE: DolphinDB\nCODE:\n```\na1=2 3 4\na2=1 2 3\na3=1 4 5\na4=5 3 2\nm = matrix(a1,a2,a3,a4)\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nimax(m.transpose())\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nbyRow(imax, m)\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nprint rowImax(m)\n```\n\n----------------------------------------\n\nTITLE: Setting Email Credentials in DolphinDB\nDESCRIPTION: Initializes email sender credentials including the sender's email address and password (or authorization code) required for authentication with the email server.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/send_messages_external_systems.md#_snippet_16\n\nLANGUAGE: dolphindb\nCODE:\n```\nuser='MailFrom@xxx.com';\npsw='xxxxx';\n```\n\n----------------------------------------\n\nTITLE: Creating X-Bond Trade Table with TSDB Engine in DolphinDB\nDESCRIPTION: This DolphinDB snippet creates a database and table for storing X-Bond trade data using the TSDB engine, partitioned daily for one year. The table includes fields for date, time, bond security identifiers, price and yield statistics, message attributes and trade details. Data is partitioned by createDate and sorted by securityID and createTime to optimize queries on bond codes and timestamps.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practices_for_partitioned_storage.md#_snippet_16\n\nLANGUAGE: DolphinDB\nCODE:\n```\ncreate database \"dfs://XBond\"\npartitioned by VALUE(2023.01.01..2023.12.31)\nengine='TSDB'\n\ncreate table \"dfs://XBond\".\"XBondTradetable\"(\n    createDate DATE[comment=\"创建日期\", compress=\"delta\"]  \n    createTime TIME[comment=\"创建时间\", compress=\"delta\"]\n    securityID SYMBOL\n    beforeClosingPrice DOUBLE\n    beforeClosingYield DOUBLE\n    beforeWeightedAveragePrice DOUBLE\n    beforeWeightedAverageYield DOUBLE\n    fillSide LONG\n    highestPrice DOUBLE\n    highestYield DOUBLE\n    lowestPrice DOUBLE\n    lowestYield DOUBLE\n    marketIndicator LONG\n    mdSubType LONG\n    mdType LONG\n    messageId LONG\n    messageSource STRING\n    msgSeqNum LONG\n    msgType STRING\n    openingValence DOUBLE\n    openingYield DOUBLE\n    priceRiseFallAmplitude DOUBLE\n    senderCompID STRING\n    sendingTime TIMESTAMP\n    settlType LONG\n    symbol STRING\n    tradeMethod LONG\n    transactTime TIMESTAMP\n    transactionNumber LONG\n    uniqueOutputKey LONG\n    upToDatePrice DOUBLE\n    upToDateYield DOUBLE\n    weightedAveragePrice DOUBLE\n    weightedAverageYield DOUBLE\n    yieldRiseFall DOUBLE\n)\npartitioned by createDate,\nsortColumns=[`securityID, `createTime]\n```\n\n----------------------------------------\n\nTITLE: Configuring DolphinDB Agent Node (agent.cfg)\nDESCRIPTION: Configuration file for a DolphinDB agent node. Defines the number of worker threads, local executors, maximum memory usage (4GB), the agent's local site address/alias, and the location of the controller node it connects to.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_6\n\nLANGUAGE: ini\nCODE:\n```\nworkerNum=3\nlocalExecutors=2\nmaxMemSize=4\nlocalSite=localhost:7910:agent\ncontrollerSite=localhost:6920:ctl6920\n```\n\n----------------------------------------\n\nTITLE: Installing dolphinindb wheel package offline with pip\nDESCRIPTION: Performs local installation of a .whl package using pip in an offline environment, assuming the package files are pre-downloaded. No dependencies beyond pip and Python are required.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/python_api_install_offline.md#_snippet_3\n\nLANGUAGE: Shell Script\nCODE:\n```\npip install dolphindb-1.30.19.2-cp38-cp38-manylinux2010_x86_64.whl\n```\n\n----------------------------------------\n\nTITLE: Loading the ODBC Plugin in DolphinDB\nDESCRIPTION: This snippet shows how to load the ODBC plugin in DolphinDB. The ServerPath should be replaced with the actual path of the PluginODBC.txt file.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ClickHouse_to_DolphinDB.md#_snippet_3\n\nLANGUAGE: dolphindb\nCODE:\n```\nloadPlugin(\"ServerPath/plugins/odbc/PluginODBC.txt\")\n```\n\n----------------------------------------\n\nTITLE: Loading the MQTT Plugin\nDESCRIPTION: This snippet attempts to load the MQTT plugin. It uses a try-catch block to handle potential errors during plugin loading, printing any exceptions that occur.  The plugin is expected to be located in the `plugins/mqtt/PluginMQTTClient.txt` directory relative to the DolphinDB home directory.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/data/DolphinDB_streaming_application_in_IOT/deviceState.txt#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntry{loadPlugin(getHomeDir()+\"/plugins/mqtt/PluginMQTTClient.txt\")} catch(ex) {print(ex)}\n```\n\n----------------------------------------\n\nTITLE: Inserting Test Data into 'index_components'\nDESCRIPTION: SQL command to insert sample data into the 'index_components' table for testing the synchronization pipeline.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Debezium_and_Kafka_data_sync.md#_snippet_5\n\nLANGUAGE: SQL\nCODE:\n```\ninsert into index_components (trade_date,code,effdate,indexShortName,indexCode,secShortName,exchangeCD,weight,timestamp,flag)\nvalues('2006-11-30','000759','2018-06-30 03:48:05','中证500','000905','中百集团','XSHE',0.0044,'2018-06-30 05:43:05',1),\n('2006-11-30','000759','2018-06-30 04:47:05','中证500','000906','中百集团','XSHE',0.0011,'2018-06-30 05:48:06',1),\n('2006-11-30','600031','2018-06-30 05:48:05','上证180','000010','三一重工','XSHG',0.0043,'2018-06-30 05:48:05',1),\n('2006-11-30','600031','2018-06-30 06:48:02','沪深300','000300','三一重工','XSHG',0.0029,'2018-06-30 05:48:05',1);\n```\n\n----------------------------------------\n\nTITLE: Loading DolphinDB ODBC Plugin - DolphinDB Script\nDESCRIPTION: DolphinDB command to load the ODBC plugin required to perform database connectivity operations from within DolphinDB scripting environment. The plugin must be loaded prior to establishing connections or querying SQL Server data via ODBC.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/SQLServer_to_DolphinDB.md#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\nloadPlugin(\"/home/Linux64_V2.00.8/server/plugins/odbc/PluginODBC.txt\")\n```\n\n----------------------------------------\n\nTITLE: Submitting a replay snapshot job in DolphinDB\nDESCRIPTION: This snippet submits a replay job named \"replay_snapshot\" to process the filtered snapshot data. The 'replay' function is configured with parameters including the data table, a target stream table, a list of relevant columns, a batch size, and other options. It enables replaying snapshot data into a stream table for further analysis.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/Real-Time_Stock_Price_Increase_Calculation/04.历史数据回放.txt#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\nsubmitJob(\"replay_snapshot\", \"snapshot\",  replay{t, snapshotStreamTable, `DateTime, `DateTime, 100, true, 1})\n```\n\n----------------------------------------\n\nTITLE: Calculating Rebalancing Task Duration in DolphinDB\nDESCRIPTION: Retrieves the detailed status of all recovery (rebalancing) tasks from the controller node using `rpc(getControllerAlias(), getRecoveryTaskStatus)`. It then processes the resulting table using SQL to calculate the maximum difference between the `FinishTime` and `StartTime` for all tasks. This provides the overall time taken for the entire rebalancing operation to complete.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/data_move_rebalance.md#_snippet_6\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nselect max(FinishTime - StartTime) as maxDiffTime from rpc(getControllerAlias(), getRecoveryTaskStatus)\n```\n\n----------------------------------------\n\nTITLE: Generating Simulated Data and Partitioned Keyed Table for Concurrency Testing in DolphinDB Script\nDESCRIPTION: This snippet generates a large simulated dataset (5 million rows) and creates both keyed and partitioned-keyed in-memory tables to demonstrate concurrency in queries. It establishes partition cuts, creates and appends data to the partitioned table, and registers both as shared tables. Main dependencies are DolphinDB's sequence, table, keyedTable, and database functions. It enables robust benchmarking for read query performance.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/in_memory_table.md#_snippet_4\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nn=5000000\nid=shuffle(1..n)\nqty=rand(1000,n)\nprice=rand(1000.0,n)\nkt=keyedTable(`id,id,qty,price)\nshare kt as skt\n\nid_range=cutPoints(1..n,20)\ndb=database(\"\",RANGE,id_range)\npkt=db.createPartitionedTable(kt,`pkt,`id).append!(kt)\nshare pkt as spkt\n```\n\n----------------------------------------\n\nTITLE: Retrieving department list from DingTalk API\nDESCRIPTION: This snippet demonstrates how to fetch the list of departments using Dolphindb's httpClient::httpGet function by sending a GET request with an access_token. It parses the JSON response to extract department IDs and details necessary for further department-specific operations.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/send_messages_external_systems.md#_snippet_12\n\nLANGUAGE: DolphinDB\nCODE:\n```\nurl = 'https://oapi.dingtalk.com/department/list';\nparam=dict(string,string);\nACCESS_TOKEN='xxxxx';\nparam['access_token']=ACCESS_TOKEN;\nret=httpClient::httpGet(url,param,1000);\nprint ret['text'];\nbody = parseExpr(ret.text).eval();\nDEPTID=string(body[0].id);\nERRCODE=body.errcode;\n```\n\n----------------------------------------\n\nTITLE: Handling Overflow in DECIMAL Data Creation and Computation - DolphinDB\nDESCRIPTION: Explains overflow checks when creating DECIMAL32/DECIMAL64 values or performing arithmetic and comparison operations. Shows that scale outside valid ranges or values exceeding max digit count cause errors during creation. Arithmetic operations that exceed value range trigger overflow errors, and comparison operators converted internally to DECIMAL types can also cause overflow errors in older DolphinDB versions. Examples illustrate errors like \"Scale is out of bounds\" and \"Decimal math overflow\" upon invalid input or operations.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DECIMAL.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndecimal32(1.2, 10)\ndecimal64(1.2, 19)\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndecimal32(1000000000, 1)\ndecimal32(`1000000000, 1)\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\n6*decimal32(4.2, 8)\n6*decimal32(100000000, 1)\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndecimal32(1, 8) < 100\n//output: Decimal math overflow\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndecimal32(1, 8) < 100\n//output: true\n```\n\n----------------------------------------\n\nTITLE: Loading a Plugin Using loadPlugin in DolphinDB Script (C++)\nDESCRIPTION: Demonstrates how to load a plugin into DolphinDB server by calling loadPlugin with the path to the plugin description file. This code is written in DolphinDB script (with C++ context), requiring the server to have proper plugin search path setup. Input: file path to PluginODBC.txt; output: dynamic loading of the plugin into the server. Useful for automating plugin deployment steps.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_development_tutorial.md#_snippet_27\n\nLANGUAGE: C++\nCODE:\n```\nloadPlugin(\"/<YOUR_SERVER_PATH>/plugins/odbc/PluginODBC.txt\");\n```\n\n----------------------------------------\n\nTITLE: Creating a Time Series Engine and Adding Access Control - DolphinDB\nDESCRIPTION: This code creates a time series engine ('agg1') and adds access control to it, restricting access to the engine.  It uses the `createTimeSeriesEngine` and `addAccessControl` functions. Requires admin privileges to create user.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Permission_Management.md#_snippet_32\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(`admin, `123456)\ncreateUser(`u1, \"111111\");\ncreateUser(`u2, \"222222\");\nlogin(`u1, \"111111\")\nshare streamTable(1000:0, `time`sym`volume, [TIMESTAMP, SYMBOL, INT]) as trades\noutput1 = table(10000:0, `time`sym`sumVolume, [TIMESTAMP, SYMBOL, INT])\nagg1 = createTimeSeriesEngine(name=\"agg1\", windowSize=600, step=600, metrics=<[sum(volume)]>, dummyTable=trades, outputTable=output1, timeColumn=`time, useSystemTime=false, keyColumn=`sym, garbageSize=50, useWindowStartTime=false)\naddAccessControl(agg1)\n```\n\n----------------------------------------\n\nTITLE: Verifying Record Count After Import in DolphinDB\nDESCRIPTION: This script queries the target distributed table (`dfs://DolphinDBdatabase`, `tb`) using `loadTable` to retrieve the total number of records imported, verifying that all data from the source files has been loaded successfully.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_csv.md#_snippet_11\n\nLANGUAGE: dolphindb\nCODE:\n```\nselect count(*) from loadTable(\"dfs://DolphinDBdatabase\", `tb);\n```\n\n----------------------------------------\n\nTITLE: Initializing Dimension Table Data for Seismic Network DolphinDB Script\nDESCRIPTION: Populates the dimension table 'tagInfo' with sample metadata including network, station, location, channel, and a generated tagid string. Uses array and table manipulation functions (stretch, take, append!) to build a mapping of 150 entries simulating seismic sensor metadata. Requires the 'tagInfo' table to exist and be accessible via loadTable. The purpose is to provide foundational reference data for subsequent streaming and storage operations.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Seismic_waveform_data_storage.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 导入台网、台站、位置、通道等基础信息的维度表信息数据\nnet = [\"ZJ\",\"YN\",\"XZ\",\"XJ\",\"TJ\"]\nsta = [\"A0001\",\"A0002\",\"A0003\",\"A0004\",\"A0005\",\"A0006\",\"B0001\",\"B0002\",\"B0003\",\"C0001\"]\ntmp = `EIE`EIN`EIZ\nnetList = stretch(net,150)\nstaList = take(stretch(sta,30),150)\nlocList = take(`40,150)\nchn = take(tmp,150)\ncolt =   array(STRING)\nfor(i in 0..(chn.size()-1)){\n\tcolt.append!( chn[i].split()[0] + \"_\" + chn[i].split()[1] + \"_\" +chn[i].split()[2] )\n}\ntagid = \"XFDSN:\"+netList+\"_\"+staList+\"_\"+locList+\"_\"+colt\nt = table(1..150 as id,netList as net,staList as sta,locList as loc,chn,tagid)\npt = loadTable(\"dfs://real\",\"tagInfo\")\npt.append!(t)\n```\n\n----------------------------------------\n\nTITLE: Filtering Stream Data and Registering Stream Computing Engine - DolphinDB Script\nDESCRIPTION: Configures stream filters for 'order', 'trade', and 'snapshot' message types and registers a processing engine to handle each case via corresponding handler functions. Links the filtered stream to the Kafka publishing function and subscribes to the underlying stream table for asynchronous processing. Relies on stream tables, filters, message schemas, and Kafka producer initialization.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/stock_market_replay/04.publishToKafka.txt#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef filterAndParseStreamFunc(producer){\n\tfilter1 = dict(STRING,ANY)\n\tfilter1[\"condition\"] =  \"order\"\n\tfilter1[\"handler\"] = sendMsgToKafkaFunc{\"order\", producer}\n\tfilter2 = dict(STRING,ANY)\n\tfilter2[\"condition\"] = \"trade\"\n\tfilter2[\"handler\"] = sendMsgToKafkaFunc{\"trade\", producer}\n\tfilter3 = dict(STRING,ANY)\n\tfilter3[\"condition\"] = \"snapshot\"\n\tfilter3[\"handler\"] = sendMsgToKafkaFunc{\"snapshot\", producer}\n\tschema = dict([\"order\",\"trade\", \"snapshot\"], [loadTable(\"dfs://order\", \"order\"), loadTable(\"dfs://trade\", \"trade\"), loadTable(\"dfs://snapshot\", \"snapshot\")])\n\tengine = streamFilter(name=\"streamFilter\", dummyTable=messageStream, filter=[filter1, filter2, filter3], msgSchema=schema)\n\t\n\tsubscribeTable(tableName=\"messageStream\", actionName=\"sendMsgToKafka\", offset=-1, handler=engine, msgAsTable=true, reconnect=true)\n}\nfilterAndParseStreamFunc(producer)\n```\n\n----------------------------------------\n\nTITLE: Row-wise Application of Custom Functions via byRow in DolphinDB\nDESCRIPTION: Shows how to apply a user-defined function to each row of a matrix using byRow in DolphinDB. Inputs: matrix and a custom function (mfunc) accepting each row. Output: vector of computed results for each row. It's necessary that the function is written to handle a single row input. Facilitates complex row-level computations beyond built-in row functions.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/panel_data.md#_snippet_27\n\nLANGUAGE: DolphinDB\nCODE:\n```\nbyRow(mfunc{, 0}, m)\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus Scrape Target for Node Exporter - YAML\nDESCRIPTION: This snippet shows how to add a scrape configuration in prometheus.yml to collect metrics from a Node Exporter instance. It defines a job name and specifies the target IP and port where Node Exporter is running.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/cluster_monitor.md#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\n  # 用于抓取服务器资源使用信息 对应第一套方案          \n  - job_name: \"服务器名称\"\n    static_configs:\n      - targets: [\"xxxxxxxx:xxx\"] #IP:NodeExporter端口\n```\n\n----------------------------------------\n\nTITLE: Applying Custom Functions Column-wise with each in DolphinDB\nDESCRIPTION: Exhibits the use of a custom function (mfunc) to apply different aggregations (sum or avg) to each column of a matrix via the each high-order function. Inputs: matrix and an integer vector determining which function to apply to each column. Outputs: vector containing the result of applying mfunc to each column. Demonstrates dependency on each and user-defined functions. The matrix should be appropriately sized per the input vector.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/panel_data.md#_snippet_25\n\nLANGUAGE: DolphinDB\nCODE:\n```\nm = rand(10, 20)$10:2\nm\n\ndef mfunc(x, flag){if(flag==1) return sum(x); else return avg(x)}\neach(mfunc, m, 0 1)\n```\n\n----------------------------------------\n\nTITLE: Creating a Stream Table in DolphinDB\nDESCRIPTION: Creates and shares a stream table named 'level2' with columns for symbol, datetime, price data and volume. The table is initialized with capacity for 100 rows and designed to store level 2 market data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/quant_finance_examples.md#_snippet_18\n\nLANGUAGE: DolphinDB\nCODE:\n```\nshare streamTable(100:0, `symbol`datetime`last`askPrice1`bidPrice1`askVolume1`bidVolume1`volume, [SYMBOL,DATETIME,DOUBLE,DOUBLE,DOUBLE,INT,INT,INT]) as level2\n```\n\n----------------------------------------\n\nTITLE: Revoking Table-Level Permissions on Table Recreation in DolphinDB Script\nDESCRIPTION: This snippet demonstrates the process of granting TABLE_READ permission on a partitioned table to a user, then deleting and recreating the table, resulting in the automatic revocation of the user's TABLE_READ permission. It shows login, database creation, table creation, user creation, permission granting, and verification steps. The expected input includes a user ID, database and table names, and the initial data. The output confirms whether access is allowed or revoked after table recreation, illustrating DolphinDB's behavior regarding table-level permission revocation.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Permission_Management.md#_snippet_16\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(\"admin\", \"123456\")\ndbName = \"dfs://valuedb\"\nt = table(1..10 as id , rand(100, 10) as val)\nif(existsDatabase(dbName)){\n\t dropDatabase(dbName)\n} \ndb=database(dbName, VALUE, 1..10)\npt=  db.createPartitionedTable(t, \"pt\", \"id\").append!(t)\n\ncreateUser(\"user1\",\"123456\")\ngrant(\"user1\", TABLE_READ,\"dfs://valuedb/pt\")\nlogin(\"user1\", \"123456\")\nselect * from loadTable( \"dfs://valuedb\",`pt)//此时user1 拥有\"dfs://valuedb/pt\"表的TABLE_READ权限\nlogin(\"admin\", \"123456\")\ndropTable(db,`pt)\npt=  db.createPartitionedTable(t, \"pt\", \"id\").append!(t)\nlogin(\"user1\", \"123456\")\nselect * from loadTable( \"dfs://valuedb\",`pt)//user1用户的TABLE_READ权限被回收\n```\n\n----------------------------------------\n\nTITLE: QR Decomposition with full mode in DolphinDB\nDESCRIPTION: Demonstrates QR decomposition of a matrix in DolphinDB using the `qr` function with `mode='full'` and `pivoting=false`.  The function decomposes a matrix X into an orthogonal matrix Q and an upper triangular matrix R. It shows the result of Q and R.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/matrix.md#_snippet_14\n\nLANGUAGE: DolphinDB\nCODE:\n```\n>m=matrix([2 5 5, 5 5 4, 8 6 4, 7 6 8]);\n>m;\n#0 #1 #2 #3\n-- -- -- --\n2  5  8  7 \n5  5  6  6 \n5  4  4  8 \n\n>q,r=qr(m,mode='full',pivoting=false);\n>q;\n#0        #1        #2       \n--------- --------- ---------\n-0.272166 0.93784   0.215365 \n-0.680414 -0.029307 -0.732242\n-0.680414 -0.345828 0.646096 \n>r;\n#0        #1        #2        #3        \n--------- --------- --------- ----------\n-7.348469 -7.484552 -8.981462 -11.430952\n0         3.159348  5.943561  3.622407  \n0         0         -0.086146 2.282872\n\n```\n\n----------------------------------------\n\nTITLE: Grouping Trades by Date (Single Column) - DolphinDB\nDESCRIPTION: This DolphinDB script groups the \"trades\" table by \"trade_date\" and calculates the maximum open price for each group.  The `timer(10)` function measures the execution time of the query, repeated 10 times. `clearAllCache()` is called before each iteration to ensure consistent results.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_21\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//按时间分组（单列）\ntimer(10) select max(open) from trades group by trade_date\n```\n\n----------------------------------------\n\nTITLE: Creating a DingTalk group chat via HTTP POST request\nDESCRIPTION: This snippet shows how to send a POST request to DingTalk API using DolphinDB's httpClient::httpPost to create a group chat. It constructs the JSON payload including group name, owner, and user ID list, and sets the appropriate Content-Type header to specify JSON data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/send_messages_external_systems.md#_snippet_14\n\nLANGUAGE: DolphinDB\nCODE:\n```\nurl = 'https://oapi.dingtalk.com/chat/create?';\nACCESS_TOKEN='xxxxx';\nurl+='access_token='+ACCESS_TOKEN;\nparam='{\"name\" : \"groupName\",\"owner\" : \"zhangsan\",\"useridlist\" : [\"zxxxx\",\"lxxxx\"]}';\nheader='Content-Type: application/json';\nret=httpClient::httpPost(url,param,1000,header);\nprint ret['text'];\nbody = parseExpr(ret.text).eval();\nERRCODE=body.errcode;\n```\n\n----------------------------------------\n\nTITLE: Adding Columns to Memory Table with update!\nDESCRIPTION: Demonstrates adding new columns to a memory table using the `update!` function. It takes column names and corresponding expressions as arguments.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/partitioned_in_memory_table.md#_snippet_16\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntrades.update!(`logPrice1`newQty1, <[log(price1), double(qty1)]>);\n```\n\n----------------------------------------\n\nTITLE: Importing Data as DECIMAL Types with saveText and loadText - DolphinDB\nDESCRIPTION: Demonstrates importing data from CSV by first saving a DolphinDB table to a CSV file using saveText, then loading the file with loadText while specifying a schema table to convert specific columns into DECIMAL32 and DECIMAL64 types with defined scales. Requires setting a working directory and having DolphinDB functions saveText and loadText available.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DECIMAL.md#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\nWORK_DIR=\"/hdd/hdd1/test_decimal/\"\nn=1000\nt=table(rand(1..100, n) as id, rand(2022.11.23T12:39:56+1..100, n) as datetimev, rand(`AAPL`ARS`BSA, n) as sym, rand(rand(100.0, 10) join take(00f, 10), n) as val1, rand(rand(100.0, 10) join take(00f, 10), n) as val2)\nsaveText(t, WORK_DIR+\"test_decimal.csv\")\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nshemaTable=table(`id`timev`sym`val1`val2 as name, [`INT, `DATETIME, `SYMBOL, \"DECIMAL32(4)\", \"DECIMAL64(5)\"] as type)\nre=loadText(filePath, , shemaTable)\n```\n\n----------------------------------------\n\nTITLE: Factorial Function Implementation in DolphinDB Plugin\nDESCRIPTION: This code snippet demonstrates the implementation of a `factorial` plugin function in C++ for DolphinDB. It calculates the factorial of a non-negative integer input `n`. It includes argument validation to ensure that `n` is an integral scalar within the range of 0 to 25, throwing an `IllegalArgumentException` if the input is invalid. It returns a `Long` type variable representing the factorial result.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_development_tutorial.md#_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\n#include \"CoreConcept.h\"\n#include \"Exceptions.h\"\n#include \"ScalarImp.h\"\n\nConstantSP factorial(const ConstantSP &n, const ConstantSP &placeholder) {\n    string syntax = \"Usage: factorial(n).\";\n    if (!n->isScalar() || n->getCategory() != INTEGRAL)\n        throw IllegalArgumentException(\"factorial\", syntax + \"n must be an integral scalar.\");\n    int nValue = n->getInt();\n    if (nValue < 0 || nValue> 25)\n        throw IllegalArgumentException(\"factorial\", syntax + \"n must be a non-negative integer less than 26.\");\n\n    long long fact = 1;\n    for (int i = nValue; i> 0; i--)\n        fact *= i;\n    return new Long(fact);\n}\n```\n\n----------------------------------------\n\nTITLE: Querying TopN Mean in Moving Window by Volume - DolphinDB Script\nDESCRIPTION: This script applies mavgTopN to compute the mean of the 'close' price for each stock, considering the top 3 'volume' values in the last 5 records. Uses the 'context by' clause for partitioned calculation by 'windCode'. Requires a table 't' with 'close' and 'volume' columns. Parameters: close (target), volume (sorting key), 5 (window size), 3 (TopN), false (descending sort). Output: per-stock time series with Top3 volume-mean close.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_TopN.md#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect windCode, tradingTime, mavgTopN(close, volume, 5, 3, false) as mavgTop3Close from t context by windCode\n```\n\n----------------------------------------\n\nTITLE: Downloading DolphinDB Server Using Wget Shell Command\nDESCRIPTION: Shows how to download DolphinDB server packages using wget commands in shell. The snippet demonstrates downloading different DolphinDB Linux server versions including standard, ABI, and JIT variants by specifying version and variant in the URL. Requires wget utility installed. Inputs are version release strings, outputs are zip files downloaded locally.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ha_cluster_deployment.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nwget https://www.dolphindb.cn/downloads/DolphinDB_Linux64_V${release}.zip -O dolphindb.zip\n```\n\nLANGUAGE: sh\nCODE:\n```\nwget https://www.dolphindb.cn/downloads/DolphinDB_Linux64_V2.00.11.3.zip -O dolphindb.zip\n```\n\nLANGUAGE: sh\nCODE:\n```\nwget https://www.dolphindb.cn/downloads/DolphinDB_Linux64_V2.00.11.3_ABI.zip -O dolphindb.zip\n```\n\nLANGUAGE: sh\nCODE:\n```\nwget https://www.dolphindb.cn/downloads/DolphinDB_Linux64_V2.00.11.3_JIT.zip -O dolphindb.zip\n```\n\n----------------------------------------\n\nTITLE: Elasticsearch Grouping by Trade Date (Single Aggregation) - Python\nDESCRIPTION: This Python function executes an aggregation query on Elasticsearch, grouping documents by 'trade_date' and calculating the maximum 'open' price for each date. It utilizes the `urllib3` library to send a GET request to the Elasticsearch API with a JSON payload defining the aggregation. The function prints the HTTP status code and the parsed JSON response.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_32\n\nLANGUAGE: Python\nCODE:\n```\ndef search_7():\n    http = urllib3.PoolManager()\n    data = json.dumps({\n        \"aggs\": {\n            \"group_by_trade_date\": {\n                \"terms\": {\n                    \"field\": \"trade_date\",\n                    \"size\": 5000\n                },\n                \"aggs\": {\n                    \"max_open\": {\n                        \"max\": {\"field\": \"open\"}\n                    }\n                }\n            }\n        },\n        \"_source\": [\"\"]\n    }).encode(\"utf-8\")\n    r = http.request(\"GET\", \"http://localhost:9200/elastic/_search\", body=data,\n                     headers={'Content-Type': 'application/json'})\n    print(r.status)\n    print(json.loads(r.data.decode()))\n```\n\n----------------------------------------\n\nTITLE: Row Alignment and Data Extraction\nDESCRIPTION: Demonstrates data alignment between two Array Vectors using `rowAlign` and extracts data based on the aligned indices using `rowAt` in DolphinDB. The example aligns bid prices at different timestamps and extracts corresponding values.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Array_Vector.md#_snippet_14\n\nLANGUAGE: DolphinDB\nCODE:\n```\nleft = array(DOUBLE[], 0).append!([9.00 8.98 8.97 8.96 8.95, 8.99 8.97 8.95 8.93 8.91])\nright = prev(left)\n/* \nleft:[[9,8.98,8.97,8.96,8.949999999999999],[8.99,8.97,8.949999999999999,8.929999999999999,8.91]]\nright:[,[9,8.98,8.97,8.96,8.949999999999999]]\n*/\n\nleftIndex, rightIndex = rowAlign(left, right, how=\"bid\")\n/*\nleftIndex:[[0,1,2,3,4],[-1,0,-1,1,-1,2]]\nrightIndex:[[-1,-1,-1,-1,-1],[0,-1,1,2,3,4]]\n*/\n\nleftResult = rowAt(left, leftIndex)\nrightResult = rowAt(right, rightIndex)\n/*\nleftResult:[[9,8.98,8.97,8.96,8.949999999999999],[,8.99,,8.97,,8.949999999999999]]\nrightResult:[[,,,,],[9,,8.98,8.97,8.96,8.949999999999999]]\n*/\n```\n\n----------------------------------------\n\nTITLE: Defining Aggregation Features and Metacode\nDESCRIPTION: Initializes a dictionary `features` specifying which columns should be aggregated using which functions (both built-in and custom). It then calls the `createAggMetaCode` function with this dictionary to generate the actual metacode (`aggMetaCode`) and the list of resulting column names (`metaCodeColName`) to be used in the main query.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/metacode_derived_features/metacode_derived_features.txt#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\nfeatures = {\n\t\"DateTime\":[`count]\n}\nfor( i in 0..9)\n{\n\tfeatures[\"Wap\"+i] = [`sum, `mean, `std]\n\tfeatures[\"LogReturn\"+i] = [`sum, `realizedVolatility, `mean, `std]\n\tfeatures[\"LogReturnOffer\"+i] = [`sum, `realizedVolatility, `mean, `std]\n\tfeatures[\"LogReturnBid\"+i] = [`sum, `realizedVolatility, `mean, `std]\n}\nfeatures[\"WapBalance\"] = [`sum, `mean, `std]\nfeatures[\"PriceSpread\"] = [`sum, `mean, `std]\nfeatures[\"BidSpread\"] = [`sum, `mean, `std]\nfeatures[\"OfferSpread\"] = [`sum, `mean, `std]\nfeatures[\"TotalVolume\"] = [`sum, `mean, `std]\nfeatures[\"VolumeImbalance\"] = [`sum, `mean, `std]\naggMetaCode, metaCodeColName = createAggMetaCode(features)\n```\n\n----------------------------------------\n\nTITLE: Loading and Calling Plugin Functions in DolphinDB\nDESCRIPTION: This snippet demonstrates how to load a DolphinDB plugin and call its functions in DolphinDB.  It shows how to use the `loadPlugin` function to load the plugin and then call the exposed functions using the plugin's namespace.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_advance.md#_snippet_8\n\nLANGUAGE: dolphindb\nCODE:\n```\nloadPlugin(\"Path_to_PluginTest.txt/PluginTest.txt\");  // 加载插件\nre1 = test::myFunc1(10, 5);  // re1值为35\nnewFunc= test::myFunc2();    // 获得一个只需一个参数的新函数\nre2 = newFunc(5);            // 调用新函数，re2值为35\n```\n\n----------------------------------------\n\nTITLE: Defining Stateful Function for Active Volume Percentage in DolphinDB\nDESCRIPTION: Defines a DolphinDB stateful function 'actVolumePercent' which calculates the ratio of the buy volume to the total traded volume over a specified sliding window. It sums the trade quantities where buy order sequence number exceeds sell order sequence number, normalized by the total trade quantity sums. Prerequisites include DolphinDB environment and data variables: tradeQty (trade quantity), buyNo (buy order sequence number), sellNo (sell order sequence number), and window (integer defining sliding window size). The output is a floating-point ratio representing the active volume percentage.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/DolphinDB_Python_Parser_Intro_for_Quantitative_Finance/因子实现_DolphinDB版本/主动成交量占比.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\n@state\ndef actVolumePercent(tradeQty, buyNo, sellNo, window){\n\treturn msum(iif(buyNo > sellNo, tradeQty, 0), window) \\\\ msum(tradeQty, window)\n}\n```\n\n----------------------------------------\n\nTITLE: 用 Left Semi Join 引擎补充逐笔成交数据的委托信息脚本\nDESCRIPTION: 该脚本创建成交表和委托订单表，通过两个 Left Semi Join 引擎级联实现逐笔成交与对应委托单的关联。左边引擎关联卖单号，右边引擎关联买单号，最终输出含有详细委托信息的成交数据。订阅流数据并将实时数据注入引擎，实现逐笔关联和补充。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming-real-time-correlation-processing.md#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// create table\nshare streamTable(1:0, `Sym`BuyNo`SellNo`TradePrice`TradeQty`TradeTime, [SYMBOL, LONG, LONG, DOUBLE, LONG, TIME]) as trades\nshare streamTable(1:0, `Sym`OrderNo`Side`OrderQty`OrderPrice`OrderTime, [SYMBOL, LONG, INT, LONG, DOUBLE, TIME]) as orders\nshare streamTable(1:0, `Sym`SellNo`BuyNo`TradePrice`TradeQty`TradeTime`BuyOrderQty`BuyOrderPrice`BuyOrderTime, [SYMBOL, LONG, LONG, DOUBLE, LONG, TIME, LONG, DOUBLE, TIME]) as outputTemp\nshare streamTable(1:0, `Sym`BuyNo`SellNo`TradePrice`TradeQty`TradeTime`BuyOrderQty`BuyOrderPrice`BuyOrderTime`SellOrderQty`SellOrderPrice`SellOrderTime, [SYMBOL, LONG, LONG, DOUBLE, LONG, TIME, LONG, DOUBLE, TIME, LONG, DOUBLE, TIME]) as output\n\n// create engine: left join buy order\nljEngineBuy=createLeftSemiJoinEngine(name=\"leftJoinBuy\", leftTable=outputTemp, rightTable=orders, outputTable=output,  metrics=<[SellNo, TradePrice, TradeQty, TradeTime, BuyOrderQty, BuyOrderPrice, BuyOrderTime, OrderQty, OrderPrice, OrderTime]>, matchingColumn=[`Sym`BuyNo, `Sym`OrderNo])\n\n//  create engine: left join sell order \nljEngineSell=createLeftSemiJoinEngine(name=\"leftJoinSell\", leftTable=trades, rightTable=orders, outputTable=getLeftStream(ljEngineBuy),  metrics=<[BuyNo, TradePrice, TradeQty, TradeTime, OrderQty, OrderPrice, OrderTime]>, matchingColumn=[`Sym`SellNo, `Sym`OrderNo])\n\n// subscribe topic\nsubscribeTable(tableName=\"trades\", actionName=\"appendLeftStream\", handler=getLeftStream(ljEngineSell), msgAsTable=true, offset=-1)\nsuscribeTable(tableName=\"orders\", actionName=\"appendRightStreamForSell\", handler=getRightStream(ljEngineSell), msgAsTable=true, offset=-1)\nsuscribeTable(tableName=\"orders\", actionName=\"appendRightStreamForBuy\", handler=getRightStream(ljEngineBuy), msgAsTable=true, offset=-1)\n```\n\n----------------------------------------\n\nTITLE: Creating Matrices in DolphinDB C++ Plugin\nDESCRIPTION: This snippet demonstrates how to create matrices in a DolphinDB C++ plugin using the `Util::createMatrix` and `Util::createDoubleMatrix` functions. It shows how to initialize matrices with specific data types and dimensions.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_advance.md#_snippet_2\n\nLANGUAGE: c++\nCODE:\n```\nConstantSP m = Util::createMatrix(DT_INT, 3, 10, 3); // 创建一个10行3列，列容量为3的int类型矩阵\nConstantSP seq = Util::createIndexVector(1, 10);     // 相当于 1..10\nm->setColumn(0, seq);                                // 相当于m[0]=seq\n\nConstantSP dm = Util::createDoubleMatrix(3, 5);      // 创建一个5行3列的double类型矩阵\n```\n\n----------------------------------------\n\nTITLE: Implementing No-Data Detection for Sensor Devices in DolphinDB\nDESCRIPTION: Creates a system to monitor for device inactivity. If no data is received from a device for 5 minutes, it generates a warning entry. The function maintains a keyed table with the latest timestamp for each device.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/alarm.txt#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\nt=keyedTable(`deviceID,100:0, `deviceID`time, [INT,DATETIME])\ndeviceNum = 3\ninsert into t values(1..deviceNum, take(now().datetime(), deviceNum))\ndef checkNoData (mutable keyedTable, mutable outputTable, msg) {\n\tkeyedTable.append!(select deviceID, ts from msg)\n\twarning = select now().datetime(), deviceID, 1 as anomalyType, \"\" as anomalyString from keyedTable where time < datetimeAdd(now().datetime(), -5, \"m\")\n\toutputTable.append!(warning)\n}\nsubscribeTable(tableName=\"sensor\", actionName=\"noData\", offset=0, handler=checkNoData{t, warningTable}, msgAsTable=true, batchSize=1000000, throttle=1)\n```\n\n----------------------------------------\n\nTITLE: Configuring DolphinDB Cluster-Wide Settings (cluster.cfg)\nDESCRIPTION: Sets cluster-level configurations for DolphinDB. This includes the maximum number of connections, maximum memory size per node (7GB), number of worker threads, local executors, and web worker threads.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_5\n\nLANGUAGE: ini\nCODE:\n```\nmaxConnections=128\nmaxMemSize=7\nworkerNum=4\nlocalExecutors=3\nwebWorkerNum=2\n```\n\n----------------------------------------\n\nTITLE: Executing dataX Import Command in Bash Shell\nDESCRIPTION: This Bash command executes the dataX import process to synchronize table data from SQL Server to DolphinDB by running the datax.py script with a given configuration file. It must be run from within the dataX bin directory and expects the tick_close.json configuration file to be present at the specified relative path. Prerequisites include a working Python 2.x environment, proper installation of dataX and its dependencies, and prior setup of the DolphinDB schema referred to by the config. Input is the path to the dataX job configuration file, and output is console logs indicating job status and success or failure. Limitation: Assumes paths and environment variables are set up appropriately.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Python_Celery.md#_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\n$ python datax.py ../conf/tick_close.json\n```\n\n----------------------------------------\n\nTITLE: Calculating Alpha98 Factor Using Vectorized Matrix Functions in DolphinDB\nDESCRIPTION: This pair of DolphinDB functions computes the Alpha98 factor for panel data using vectorized matrix operations, prioritizing performance. The code first defines a ranking function and then performs moving averages, rolling correlations, and various sliding-window matrix operations to compute the factor across a panel. Input: vwap, open, and vol as matrices (created via panel function from a table). Output: matrix (or vector) of Alpha98 factors per instrument and time step. Dependencies: matrix operations, mcorr, msum, mavg, mrank, mimin. Requires that panel data be arranged appropriately.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/panel_data.md#_snippet_31\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef myrank(x){\n\treturn rowRank(x)\\x.columns()\n}\n\ndef alphaPanel98(vwap, open, vol){\n\treturn myrank(mavg(mcorr(vwap, msum(mavg(vol, 5), 26), 5), 1..7)) - myrank(mavg(mrank(9 - mimin(mcorr(myrank(open), myrank(mavg(vol, 15)), 21), 9), true, 7), 1..8))\n}\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nt = select * from loadTable(\"dfs://k_day_level\",\"k_day\")\ntimer vwap, open, vol = panel(t.tradetime, t.securityid, [t.vwap, t.open, t.vol])\ntimer res = alphaPanel98(vwap, open, vol)\n```\n\n----------------------------------------\n\nTITLE: Listing Intermediate Directories (keepDuplicates=ALL)\nDESCRIPTION: This code snippet displays the directory structure during an update operation where keepDuplicates is configured to ALL. The tree command is used to list the files showing the intermediate directory created during the update process, characterized by names containing 'tid'.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/dolphindb_update.md#_snippet_9\n\nLANGUAGE: console\nCODE:\n```\n$ tree\n.\n├── chunk.dict\n├── machines_2\n│   ├── 0_00000273\n│   ├── 0_00000275\n│   ├── 0_00000277\n│   ├── 0_00000278\n│   ├── 0_00000279\n│   └── 1_00000054\n└── machines_2_tid_199\n    ├── 0_00000515\n    ├── 0_00000516\n    ├── 0_00000517\n    ├── 0_00000518\n    └── 0_00000519\n```\n\n----------------------------------------\n\nTITLE: Initializing Streaming Environment and Creating Stream Tables (DolphinDB Script)\nDESCRIPTION: Sets up the environment for stream processing. It defines a function `cleanEnvironment` to remove existing stream tables, subscriptions, and engines based on a parallelism parameter. It then creates three key shared, asynchronous, persistent stream tables (`tradeOriginalStream`, `tradeProcessStream`, `capitalFlowStream`) with specified schemas, cache sizes, and retention periods using `enableTableShareAndPersistence`. Finally, it sets `SecurityID` as the filter column for each stream table using `setStreamTableFilterColumn` to enable efficient filtering during subscription.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_capital_flow_order_by_order.md#_snippet_5\n\nLANGUAGE: dolphindb\nCODE:\n```\n// clean up environment\ndef cleanEnvironment(parallel){\n\tfor(i in 1..parallel){\n\t\ttry{ unsubscribeTable(tableName=`tradeOriginalStream, actionName=\"tradeProcess\"+string(i)) } catch(ex){ print(ex) }\n\t\ttry{ unsubscribeTable(tableName=`tradeProcessStream, actionName=\"tradeTSAggr\"+string(i)) } catch(ex){ print(ex) }\n\t\ttry{ dropStreamEngine(\"tradeProcess\"+string(i)) } catch(ex){ print(ex) }\n\t\ttry{ dropStreamEngine(\"tradeTSAggr\"+string(i)) } catch(ex){ print(ex) }\n\t}\n\ttry{ unsubscribeTable(tableName=`tradeOriginalStream, actionName=\"tradeToDatabase\") } catch(ex){ print(ex) }\n\ttry{ dropStreamTable(`tradeOriginalStream) } catch(ex){ print(ex) }\n\ttry{ dropStreamTable(`tradeProcessStream) } catch(ex){ print(ex) }\n\ttry{ dropStreamTable(`capitalFlowStream) } catch(ex){ print(ex) }\n\tundef all\n}\n//calculation parallel, developers need to modify according to the development environment\nparallel = 3\ncleanEnvironment(parallel)\ngo\n//create stream table: tradeOriginalStream\ncolName = `SecurityID`Market`TradeTime`TradePrice`TradeQty`TradeAmount`BuyNum`SellNum\ncolType = `SYMBOL`SYMBOL`TIMESTAMP`DOUBLE`INT`DOUBLE`INT`INT\ntradeOriginalStreamTemp = streamTable(1000000:0, colName, colType)\ntry{ enableTableShareAndPersistence(table=tradeOriginalStreamTemp, tableName=\"tradeOriginalStream\", asynWrite=true, compress=true, cacheSize=1000000, retentionMinutes=1440, flushMode=0, preCache=10000) } catch(ex){ print(ex) }\nundef(\"tradeOriginalStreamTemp\")\ngo\nsetStreamTableFilterColumn(tradeOriginalStream, `SecurityID)\n//create stream table: tradeProcessStream\ncolName = `SecurityID`TradeTime`Num`TradeQty`TradeAmount`BSFlag\ncolType = `SYMBOL`TIMESTAMP`INT`INT`DOUBLE`SYMBOL\ntradeProcessStreamTemp = streamTable(1000000:0, colName, colType)\ntry{ enableTableShareAndPersistence(table=tradeProcessStreamTemp, tableName=\"tradeProcessStream\", asynWrite=true, compress=true, cacheSize=1000000, retentionMinutes=1440, flushMode=0, preCache=10000) } catch(ex){ print(ex) }\nundef(\"tradeProcessStreamTemp\")\ngo\nsetStreamTableFilterColumn(tradeProcessStream, `SecurityID)\n//create stream table: capitalFlow\ncolName = `TradeTime`SecurityID`BuySmallAmount`BuyBigAmount`SellSmallAmount`SellBigAmount\ncolType =  `TIMESTAMP`SYMBOL`DOUBLE`DOUBLE`DOUBLE`DOUBLE\ncapitalFlowStreamTemp = streamTable(1000000:0, colName, colType)\ntry{ enableTableShareAndPersistence(table=capitalFlowStreamTemp, tableName=\"capitalFlowStream\", asynWrite=true, compress=true, cacheSize=1000000, retentionMinutes=1440, flushMode=0, preCache=10000) } catch(ex){ print(ex) }\nundef(\"capitalFlowStreamTemp\")\ngo\nsetStreamTableFilterColumn(capitalFlowStream, `SecurityID)\n```\n\n----------------------------------------\n\nTITLE: Checking NTP Synchronization Status via ntpstat (console)\nDESCRIPTION: Inspect the NTPD synchronization status on both the server and client nodes with `ntpstat`. The output details the upstream server, time accuracy, and polling interval. It helps administrators verify time consistency before launching DolphinDB clusters.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/deploy_dolphindb_on_new_server.md#_snippet_9\n\nLANGUAGE: console\nCODE:\n```\n# ntpstat\nsynchronised to NTP server (202.112.31.197) at stratum 2\n   time correct to within 41 ms\n   polling server every 64 s\n\n```\n\nLANGUAGE: console\nCODE:\n```\n# ntpstat\nsynchronised to NTP server (192.168.0.30) at stratum 3\n   time correct to within 243 ms\n   polling server every 64 s\n\n```\n\n----------------------------------------\n\nTITLE: Parallel Execution Setup in Python\nDESCRIPTION: Code for setting up parallel processing of securities using Python's Joblib library. It splits the list of security IDs based on the desired parallelism and executes the calculation function in parallel.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Python+HDF5_vs_DolphinDB.md#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\n# 股票 HDF5 文件地址\npathDir=/data/stockdata\n# 并行度\nn = 1\n# SecurityIDs 为要计算的全部股票列表\nSecurityIDs_splits = np.array_split(SecurityIDs, n)\nParallel(n_jobs=n)(delayed(ParallelBySymbol)(SecurityIDs,pathDir) for SecurityIDs in SecurityIDs_splits)\n```\n\n----------------------------------------\n\nTITLE: Restoring Specific DolphinDB Partitions\nDESCRIPTION: Demonstrates restoring specific partitions from a backup directory (`backupDir`) to a target table (`tbName` in `dbPath`) using the `restore` function within a loop. Each partition path listed in the `pars` vector is restored individually. Note that the `restore` function's partition parameter takes a single path (which can include wildcards), necessitating a loop for multiple specific, non-wildcard paths.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/backup-restore-new.md#_snippet_10\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndbPath=\"dfs://testdb\"\nbackupDir=\"/home/$USER/backupPar\"\ntbName=`quotes_2\npars=[\"/testdb/Key3/tp/20120101\",\"/testdb/Key4/tp/20120101\"]\nfor (par in pars){\n\trestore(backupDir,dbPath,tbName,par,false,,true,true)\n}\n```\n\n----------------------------------------\n\nTITLE: Function Command Request Format\nDESCRIPTION: This describes the format for the 'function' command request. It includes the request type, session ID, message length, the function name, the number of parameters, endianness, and the function parameters which data format references the third section. This format allows the client to invoke server-side functions.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/api_protocol.md#_snippet_6\n\nLANGUAGE: text\nCODE:\n```\n长度(Byte) | 报文 | 说明 | 样本\n---|---|---|---\n3| 请求类型 | API | API\n1| 空格| char(0x20) |\n不固定|SESSIONID | 长度不固定，到空格为止 | 2247761467，638252939\n1| 空格| char(0x20) |\n2| 报文指令长度| 包含从“function\"到大小端标志为止的长度，如\"function\\nsum\\n1\\n1\" | 16\n1| 换行符 | char(0x10) |\n8| 指令 | function | \"function\"\n1| 换行符 | char(0x10) |\n不固定| 函数名称 | 长度到下一个换行符为止| sum\n1| 换行符 | char(0x10) |\n1| 参数数量 | 传递到函数的参数个数 | 1\n1| 换行符 | char(0x10) |\n1| 大小端标志 | 1-小端，0-大端 | 1\n不固定| 参数数据 | 数据格式参考第3节 |\n```\n\n----------------------------------------\n\nTITLE: Creating and Sharing Stream Tables in DolphinDB\nDESCRIPTION: This function loads a table schema from an existing database, then creates a stream table with specified size and schema, and shares it under a shared variable name. It also creates and shares a keyed table for cross-sectional analysis, with key 'SecurityID' and defined columns. Dependencies include the database and table names, and existing schemas. The created tables facilitate streaming data analysis and are shared across sessions.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/Real-Time_Stock_Price_Increase_Calculation/02.清理环境并创建相关流数据表.txt#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef createStreamTable(dbName, tbName){\n\tschemaTB = loadTable(dbName, tbName).schema().colDefs\n\tshare(streamTable(40000:0, schemaTB.name, schemaTB.typeString), `snapshotStreamTable)\n\tshare(keyedTable(`SecurityID, 50:0, `DateTime`SecurityID`factor_1min`rank_1min`factor_5min`rank_5min`factor_10min`rank_10min, [TIMESTAMP, SYMBOL, DOUBLE,  INT, DOUBLE, INT, DOUBLE, INT]), `changeCrossSectionalTable)\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring DolphinDB Parallelism\nDESCRIPTION: Configuration parameter to control the number of worker threads in DolphinDB, which determines the parallelism of factor calculations and resource utilization.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Python+HDF5_vs_DolphinDB.md#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\nworkerNum=20\n```\n\n----------------------------------------\n\nTITLE: Accessing Columns and Elements in Array Vector and Columnar Tuple - DolphinDB Script\nDESCRIPTION: These DolphinDB script snippets illustrate how to access single or multiple columns in an Array Vector or Columnar Tuple variable using different indexing methods. They also demonstrate out-of-bounds handling, which involves filling with nulls. Required dependency is DolphinDB, and applicable objects include those created with 'array' and '.setColumnarTuple!'. The expected inputs are array variables and integer indices; outputs are vectors or Array Vectors as per the operation, with constraints on indices (e.g., start >= 0, start < end).\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Array_Vector.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\nx = array(INT[], 0).append!([1 2 3, 4 5, 6 7 8, 9 10])\nx[1]\n/*\n[2,5,7,10]\n*/\n\ny = [1 2 3, 4 5, 6 7 8, 9 10].setColumnarTuple!()\ny[1]\n/*\n[2,5,7,10]\n*/\n\n// 当 index 越界时，空值填充\nx = array(INT[], 0).append!([1 2 3, 4 5, 6 7 8, 9 10])\nx[10]\n/*\n[,,,]\n*/\n\ny = [1 2 3, 4 5, 6 7 8, 9 10].setColumnarTuple!()\ny[10]\n/*\n[,,,]\n*/\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// end 不为空\nx = array(INT[], 0).append!([1 2 3, 4 5, 6 7 8, 9 10])\nx[0:2]\n/*\n[[1,2],[4,5],[6,7],[9,10]]\n*/\n\ny = [1 2 3, 4 5, 6 7 8, 9 10].setColumnarTuple!()\ny[0:2]\n/*\n[[1,2],[4,5],[6,7],[9,10]]\n*/\n\n// end 为空\nx = array(INT[], 0).append!([1 2 3, 4 5, 6 7 8, 9 10])\nx[1:]\n/*\n[[2,3],[5],[7,8],[10]]\n*/\n\ny = [1 2 3, 4 5, 6 7 8, 9 10].setColumnarTuple!()\ny[1:]\n/*\n([2,3],[5],[7,8],[10])\n*/\n\n// 当 index 越界时，空值填充\nx = array(INT[], 0).append!([1 2 3, 4 5, 6 7 8, 9 10])\nx[1:4]\n/*\n[[2,3,],[5,,],[7,8,],[10,,]]\n*/\ny = [1 2 3, 4 5, 6 7 8, 9 10].setColumnarTuple!()\ny[1:4]\n/*\n[[2,3,],[5,,],[7,8,],[10,,]]\n*/\n```\n\n----------------------------------------\n\nTITLE: Import Binary Data into Memory Table\nDESCRIPTION: This snippet imports data from a binary file into a memory table using `readRecord!`. It requires the file to be opened using the `file` function. This approach is efficient for numerical data without strings.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_data.md#_snippet_9\n\nLANGUAGE: txt\nCODE:\n```\ndataFilePath=\"/home/data/binSample.bin\"\nf=file(dataFilePath)\nf.readRecord!(tb);\n```\n\n----------------------------------------\n\nTITLE: Managing Users with DolphinDB Scripting - DolphinDB\nDESCRIPTION: This snippet demonstrates how to log in as an administrator, create users with varying privilege levels, and manage user authentication. The commands utilize the built-in 'login' and 'createUser' functions, requiring admin credentials. Key parameters include user names, passwords, and an optional administrator flag ('isAdmin'). Input: admin/user credentials; Output: user creation and session authentication. The script assumes a running DolphinDB server and sufficient admin rights.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Permission_Management.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(`admin, \"123456\");//超级管理员登录\ncreateUser(\"admin1\",\"123456\",,true)//创建普通管理员\nlogin(\"admin1\",\"123456\")//普通管理员登录\ncreateUser(\"user1\",\"123456\",,false)//普通管理员创建普通用户\ncreateUser(\"user2\",\"123456\",,true)//普通管理员创建普通管理员\n```\n\n----------------------------------------\n\nTITLE: Elasticsearch Grouping by Ticker Aggregation\nDESCRIPTION: This Python code performs an aggregation in Elasticsearch to group by the 'TICKER' field and calculate the average of the 'VOL' field for each group. It sends an HTTP request to the Elasticsearch server to execute the query. The code utilizes `urllib3` to make the HTTP request and `json` to construct the request body. The `search_5` function is designed to test aggregation performance in Elasticsearch.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_51\n\nLANGUAGE: Python\nCODE:\n```\ndef search_5():\n    http = urllib3.PoolManager()\n    data = json.dumps({\n        \"aggs\": {\n            \"group_by_ticker\": {\n                \"terms\": {\n                    \"field\": \"TICKER\",\n                    \"size\": 23934  \n                },\n                \"aggs\": {\n                    \"avg_price\": {\n                        \"avg\": {\"field\": \"VOL\"}\n                    }\n                }\n            }\n        },\n        \"_source\": [\"\"]\n    }).encode(\"utf-8\")\n    r = http.request(\"GET\", \"http://localhost:9200/uscsv/_search\", body=data,\n                     headers={'Content-Type': 'application/json'})\n    print(r.status)\n    print(json.loads(r.data.decode()))\n```\n\n----------------------------------------\n\nTITLE: Counting NULL Values in Table Columns Using each High-Order Function in DolphinDB\nDESCRIPTION: Calculates the number of NULL entries per column in table 't' by applying a lambda function using the high-order function 'each'. It computes the difference between total size and non-NULL count per column vector, where 't.values()' returns a tuple of all columns' vectors. This provides a fast and vectorized approach to identify missing data per column in DolphinDB tables. Dependency: table 't' must be defined. Input: table columns; Output: vector with NULL counts per column.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/func_progr_cases.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\neach(x->x.size() - x.count(), t.values())\n```\n\n----------------------------------------\n\nTITLE: Adding Function Views - DolphinDB\nDESCRIPTION: This DolphinDB code utilizes the `addFunctionView` command to register the defined functions (`dsTb`, `createEnd`, `replayJob`, and `stkReplay`) as function views.  Function views enable API access to these functions. This means these functions can be called through API calls for a user.  It provides a mechanism for the functions to be accessible from the API layer. It is crucial for allowing external applications to invoke the data replay operations.  Dependencies include a running DolphinDB server and having the functions (`dsTb`, `createEnd`, `replayJob`, and `stkReplay`) defined.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/appendices_market_replay_bp.md#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\naddFunctionView(dsTb)\naddFunctionView(createEnd)\naddFunctionView(replayJob)\naddFunctionView(stkReplay)\n```\n\n----------------------------------------\n\nTITLE: 基于DolphinDB脚本的系统初始化与执行流程说明\nDESCRIPTION: 该片段为DolphinDB脚本片段，描述了用户执行地震数据相关解决方案时的初始化步骤和函数调用顺序，包括插件加载、分布式表创建、实时流模拟、历史数据加载、磁盘使用统计及多种查询和导出操作。关键依赖有MiniSeed插件、相应目录结构，以及对应的txt脚本文件按顺序执行。该脚本确保环境准备充分，适用于批量复现和测试完整的地震数据存储与处理流程。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Seismic_waveform_data_storage.md#_snippet_12\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(`admin,`123456)\nundef(all);\nclearAllCache();\ngo;\n\n/***\n * ==============================\n * 在运行此脚本次之前，需要做以下事情：\n * \t1.建立与server同级的目录  miniSeed\n * \t2.建立与server同级的目录  streamMiniSeed\n * \t3.建立与server同级的目录  outputMiniSeed\n * \t4.在./server/plugins/目录下建立mseed目录，并将libPluginMseed.so文件和PluginMseed.txt文件放置在放到来目录下\n * \t5.　运行脚本1-8\n * ==============================\n */\n\n\nnetAim,staAim,locAim = `ZJ,`A0003, `40  \t//查询case的查询条件\n\n\n  //step1：加载插件、创建分布存储表、流数据表持久化\nmainPrepare()\n\n  //step2：MiniSeed实时流数据模拟与解析\nmainStreamSimulate()\n\n  //step3：实时流入库、时延计算及异常告警\nmainBuidSubscribe()\n\n  //step4：MiniSeed历史数据模拟(耗时1min左右，该步骤后台务处理完成2内创建此步骤)\nmainHistoryMiniSeedSimulate()\n\n\n  //step5：MiniSeed历史数据入库(耗时5min左右，该步骤后台务处理完成后可以执行step6和step7)\nmainHistoryMiniSeedParse()\n\n\n  //step6：磁盘空间占用统计(单位：GB)\nmainDiskUsage()\n\n  //step7：查询case1-case4\nqueryRecord =  mainQueryTest(netAim,staAim,locAim)\n\n  //step8：查询case5\noutputRecord = mainOutputMiniSeed(netAim,staAim)\n```\n\n----------------------------------------\n\nTITLE: Testing pickle file loading and comparing with DolphinDB\nDESCRIPTION: Python script that first creates pickle files from DolphinDB data, then measures the time required to load them using pickle.load(), for benchmarking against DolphinDB query performance.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_pickle_comparison.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport dolphindb as ddb\nimport pandas as pd\nimport time\nimport pickle\n\ns = ddb.session()\ns.connect(\"192.168.1.13\", 22172, \"admin\", \"123456\")\ntick = s.run('''\nselect * from loadTable(\"dfs://DataYesDB\", \"tick\") where TradeDate = 2019.09.10\n''')\nquotes =s.run('''\nselect * from loadTable(\"dfs://TAQ\", \"quotes\")\n''')\n\n#将数据集1的Level 1的数据转换为pkl文件\nquotes.to_pickle(\"taq.pkl\")\n\n#将数据集2的Level 2一天的数据转换为pkl文件\ntick.to_pickle(\"level2.pkl\")\n\n#使用pickle模块读取数据集1的Level 1一天的数据\nst1 = time.time()\nf = open('taq.pkl', 'rb')\nc = pickle.load(f)\net1 = time.time()\nprint(et1 - st1)\nf.close()\n\n#使用pickle模块读取数据集2的Level 2一天的数据\nf = open('level2.pkl', 'rb')\nst = time.time()\nc = pickle.load(f)\net = time.time()\nprint(et - st)\nf.close()\n```\n\n----------------------------------------\n\nTITLE: Initialize Active Volume Ratio Calculation in Python with DolphinDB and Pandas\nDESCRIPTION: Imports the necessary `pandas` and `dolphindb` libraries to begin the process of calculating the active volume ratio. This snippet sets up the environment but does not contain the actual calculation logic, which would typically involve analyzing Level 2 tick-by-tick trade data to determine the proportion of actively initiated trades within a given window.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_Python_Parser_Intro_for_Quantitative_Finance.md#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nimport pandas as pd\nimport dolphindb as ddb\n```\n\n----------------------------------------\n\nTITLE: Defining DolphinDB Cluster Nodes (cluster.nodes)\nDESCRIPTION: Specifies the nodes participating in the DolphinDB cluster using CSV format. It lists the site address, port, alias, and mode (agent or datanode) for each node in the cluster.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_4\n\nLANGUAGE: csv\nCODE:\n```\nlocalSite,mode\nlocalhost:6910:agent,agent\nlocalhost:6921:DFS_NODE1,datanode\nlocalhost:6922:DFS_NODE2,datanode\nlocalhost:6923:DFS_NODE3,datanode\nlocalhost:6924:DFS_NODE4,datanode\n```\n\n----------------------------------------\n\nTITLE: Loading 'taq' Table from Database\nDESCRIPTION: Reopens the database and loads the 'taq' table into memory for analysis or querying.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs InfluxDB/test_dolphindb_big.txt#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// --------------------- 加载数据库\ndb = database(FP_DB)\n\n// load 'taq' table from database\n\ntaq = db.loadTable(`taq)\n```\n\n----------------------------------------\n\nTITLE: Executing DataX Task from Linux Shell Using Python\nDESCRIPTION: This shell command sequence illustrates how to run the DataX data migration task defined by the JSON configuration file. The user navigates into the DataX binary directory and invokes the datax.py Python script with the relative path to the JSON configuration. This execution triggers the migration job and produces synchronization output.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/OceanBase_to_DolphinDB.md#_snippet_5\n\nLANGUAGE: Shell\nCODE:\n```\ncd ./dataX/bin/\npython datax.py ../../datax-writer-master/ddb_script/ocean.json\n```\n\n----------------------------------------\n\nTITLE: Configuring ODBC Driver in odbcinst.ini\nDESCRIPTION: This snippet shows how to configure the ClickHouse ODBC driver in the /etc/odbcinst.ini file. It defines the driver's name, description, and path to the driver and setup libraries for both ANSI and Unicode versions. The '<savedir>' placeholder must be replaced with the actual installation directory.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ClickHouse_to_DolphinDB.md#_snippet_1\n\nLANGUAGE: ini\nCODE:\n```\n[ODBC Drivers] \nClickHouse ODBC Driver (ANSI)    = Installed \nClickHouse ODBC Driver (Unicode) = Installed \n\n[ClickHouse ODBC Driver (ANSI)] \nDescription = ODBC Driver (ANSI) for ClickHouse \nDriver      = <savedir>/clickhouse-odbc-1.2.1-Linux/lib64/libclickhouseodbc.so \nSetup       = <savedir>/clickhouse-odbc-1.2.1-Linux/lib64/libclickhouseodbc.so \nUsageCount  = 1 \n\n[ClickHouse ODBC Driver (Unicode)] \nDescription = ODBC Driver (Unicode) for ClickHouse \nDriver      =<savedir>/clickhouse-odbc-1.2.1-Linux/lib64/libclickhouseodbcw.so \nSetup       =<savedir>/clickhouse-odbc-1.2.1-Linux/lib64/libclickhouseodbcw.so \nUsageCount  = 1\n```\n\n----------------------------------------\n\nTITLE: Segmented Value Range Counting using Grouped Aggregation and asof in DolphinDB\nDESCRIPTION: Generates a table with simulated values, then counts records falling within user-defined ranges per date and code. The asof function is leveraged for efficient interval join and group by for aggregation, greatly simplifying and speeding up the process over loop-and-reduce alternatives. Requires definitions of 't' and 'range' as shown, and assumes the objective is per-range stat aggregation.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_sql_cases.md#_snippet_42\n\nLANGUAGE: DolphinDB\nCODE:\n```\nN = 100000\nt = table(take(2021.11.01, N) as date, \n          take(`AAPL, N) as code, \n          rand([-5, 5, 10, 15, 20, 25, 100], N) as value)\nrange = [-9999, 0, 10, 30, 9999]\n\ntimer res2 = select count(*) from t \n\t\t\tgroup by date, code, asof(range, value) as grp\n```\n\n----------------------------------------\n\nTITLE: 遍历Table并转换为自定义结构体\nDESCRIPTION: 展示如何高效地遍历Table对象，获取多列数据并转换为自定义的结构体对象，使用批量读取方法以提高性能。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/c++api.md#_snippet_8\n\nLANGUAGE: C++\nCODE:\n```\nstruct Line {\n    string id;\n    long long value;\n    long long time;\n    explicit Line(string id, long long value, long long time) : id(std::move(id)), value(value), time(time) {}\n};\nvector<Line> v;\n\nDBConnection conn;\nTableSP t = conn.run(\"select * from loadTable('dfs://testdb', 'testTbl')\");\n\nVectorSP col_id = t->getColumn(\"id\");\nVectorSP col_val = t->getColumn(\"val\");\nVectorSP col_time = t->getColumn(\"t\");\n\nconst int BUF_SIZE = 1024;\nlong long buf_val[BUF_SIZE];\nlong long buf_time[BUF_SIZE];\nchar* buf_id[BUF_SIZE];\n\nint start = 0;\nint N = t->rows();\nwhile (start < N) {\n    int len = std::min(N - start, BUF_SIZE);\n    char** pId = col_id->getStringConst(start, len, buf_id);\n    const long long* pVal = col_val->getLongConst(start, len, buf_val);\n    const long long* pTime = col_time->getLongConst(start, len, buf_time);\n\n    for (int i = 0; i < len; ++i) {\n        v.emplace_back(pId[i], pVal[i], pTime[i]);\n    }\n    start += len;\n}\n```\n\n----------------------------------------\n\nTITLE: Loading Data into a Table using loadTextEx\nDESCRIPTION: This snippet demonstrates loading data from a CSV file into a DolphinDB table using `loadTextEx`.  It uses the `dbHandle`, `tableName`, `partitionColumns`, `filename`, and `skipRows` parameters. It highlights the importance of data type compatibility and shows how to deal with potential errors due to type mismatches during the load process.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/LoadDataForPoc.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndb = database(\"dfs://sh_entrust\")\nfilePath = \"/home/ychan/data/loadForPoc/SH/Order/20210104/Entrust.csv\"\nloadTextEx(dbHandle = db, tableName = `entrust, partitionColumns = `col1`col0, filename = filePath, skipRows = 1)\n```\n\n----------------------------------------\n\nTITLE: Parsing JSON Response from WeChat Work API\nDESCRIPTION: Code snippet for parsing the JSON response from WeChat Work API to extract error code, error message, and access token. Uses parseExpr and eval functions to convert the JSON string to DolphinDB objects.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/send_messages_external_systems.md#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\nbody = parseExpr(ret.text).eval();\nERRCODE=body.errcode;\nERRMSG=body.errmsg;\nACCESS_TOKEN=body.access_token;\n```\n\n----------------------------------------\n\nTITLE: Interpolation using interval\nDESCRIPTION: This code uses the interval function with the group by clause to handle missing values. It calculates the average value per minute for an ID. The 'linear' method is used for interpolation, that means missing values are replaced by linearly interpolated values within each minute. If the column to be interpolated is not numeric, interpolation will revert to the 'prev' method (i.e., fill with the previous value).\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/iot_examples.md#_snippet_17\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect avg(value)  as value \nfrom sensors \nwhere id = 1 and date(datetime)=2020.09.01 group by id, interval(datetime, 1m, \"linear\")\n```\n\n----------------------------------------\n\nTITLE: Loading 50 ETF Option Information from DolphinDB Table (DolphinDB Script)\nDESCRIPTION: Loads all contract information data for the symbol `510050` from the `optionInfo` table within the `dfs://optionInfo` distributed database into a table variable named `contractInfo`.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/IV_Greeks_Calculation_for_ETF_Options_Using_JIT.md#_snippet_5\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\ncontractInfo = select * from loadTable(\"dfs://optionInfo\", \"optionInfo\") where sym =`510050\n```\n\n----------------------------------------\n\nTITLE: Deploying DataX and DataX-DolphinDBWriter for Data Migration - Instruction\nDESCRIPTION: Instructions for deploying DataX, a data synchronization framework, beginning with downloading the compressed package from a specified URL and extracting it to a custom directory. It also describes how to deploy the DolphinDB Writer plugin by copying its files from the GitHub repository to the DataX plugin writer folder, enabling data migrate from SQL Server to DolphinDB using Java SDK.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/SQLServer_to_DolphinDB.md#_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\n\n```\n\n----------------------------------------\n\nTITLE: Fetching and Displaying Entrust Data Load Runtime Info in DolphinScheduler Logs - DolphinDB Script\nDESCRIPTION: This snippet shows how to execute the entrust data import function view with parameters inside DolphinScheduler's SQL query task node, simultaneously capturing and formatting the returned info table into a readable log string. It uses string concatenation with a prefix '[DOLPHINDB INFO]' for each line to clearly distinguish runtime messages in task logs. This method facilitates viewing detailed ETL execution status directly within DolphinScheduler's task log interface.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/dolphinscheduler_integration.md#_snippet_22\n\nLANGUAGE: DolphinDB\nCODE:\n```\n\"\\n[DOLPHINDB INFO] \" + concat(exec * from loadEntrustFV(startDate=${startDate},endDate=${endDate},loadType=\"batch\"),\"\\n[DOLPHINDB INFO] \");\n```\n\n----------------------------------------\n\nTITLE: Parallel Factor Calculation by Symbol in Python\nDESCRIPTION: Function that processes data for each security ID, loading relevant HDF5 files, concatenating the data, and calculating both flow and mathWghtSkew factors using the Python implementations.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Python+HDF5_vs_DolphinDB.md#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\ndef ParallelBySymbol(SecurityIDs,dirPath):\n    for SecurityID in SecurityIDs:\n        sec_data_list=[]\n        for date in os.listdir(dirPath):\n            filepath = os.path.join(dirPath,str(date),\"data_\"+str(date) + \"_\" + str(SecurityID) + \".h5\")\n            sec_data_list.append(loadData(filepath))\n        df=pd.concat(sec_data_list)\n        # 计算 flow 因子\n        df_flow_res = flow(df)\n        # 计算 mathWghtSkew 因子\n        w = np.array([10, 9, 8, 7, 6, 5, 4, 3, 2, 1])\n        pxs = np.array(df[[\"BidPX1\",\"BidPX2\",\"BidPX3\",\"BidPX4\",\"BidPX5\",\"BidPX6\",\"BidPX7\",\"BidPX8\",\"BidPX9\",\"BidPX10\"]])\n        np.seterr(divide='ignore', invalid='ignore')\n        df_skew_res = mathWghtSkew(pxs,w)\n```\n\n----------------------------------------\n\nTITLE: Establishing Connection Request\nDESCRIPTION: This snippet details the structure of a 'connect' request sent by a client to the DolphinDB server. The request is a fixed-length message including an API identifier, session ID (initialized to 0), and a fixed string 'connect\\n'. This initiates the connection handshake, leading to session ID assignment.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/api_protocol.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n长度(Byte) | 报文 | 说明\n---|---|---\n3| API | 请求类型\n1| 空格| char(0x20)\n1|0 | SESSIONID\n1| 空格|  char(0x20)\n2| 8| 报文指令长度，固定字符串connect\\n\n1|  换行符(LF)| char(0x10)\n8| \"connect\\n\"  | 固定字符串\n```\n\n----------------------------------------\n\nTITLE: 计算持有区间的累计收益率\nDESCRIPTION: 模拟从2010年至2020年持有基金一年的收益情况，计算区间内基金的累计收益率，并筛选出表现良好的基金。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/public_fund_basic_analysis.md#_snippet_25\n\nLANGUAGE: DolphinDB\nCODE:\n```\nfilterPanelData = panelData.loc(2010.01.01..2020.12.31, view=true)\ndayIndex = panelData.rowNames().temporalAdd(-1,'y')\nworkdays = select * from loadTable(\"dfs://publicFundDB\", \"workday\")\nworkeDayIndex = each(def(dayIndex){return exec last(Day) from workdays where Day <= dayIndex}, dayIndex)\nfilterPanelDataTmp = panelData.loc(workeDayIndex>=panelData.rowNames()[0]&&workeDayIndex<=2020.12.31, ).rename!(workeDayIndex[workeDayIndex>=panelData.rowNames()[0]&&workeDayIndex<=2020.12.31], panelData.colNames())\n// 计算累计收益\nfilterCumulativeReturn = ((filterPanelDataTmp - filterPanelData) / filterPanelData)\nselect SecurityID, mean from table(filterCumulativeReturn[x->count(x) > 1000]) order by mean desc\n// 计算一年后收益大于0.2的基金比例\nresult = each(count, cumulativeReturn[cumulativeReturn>0.2]) \\ cumulativeReturn.rows()\n(select SecurityID, prop from table(cumulativeReturn.colNames() as SecurityID, result as prop) order by prop desc).head(30)\n```\n\n----------------------------------------\n\nTITLE: Registering Asof Join Stream Computing Engine (DolphinDB Script)\nDESCRIPTION: Instantiates and registers an asof join engine for real-time stream computation. It synchronizes trade data to prevailing snapshot quotes based on security identifier and time, and computes derived metrics like price cost differentials. Key parameters include leftTable, rightTable, outputTable, metrics (output columns), and time alignment columns. The join engine enables low-latency joining of two streams for intraday cost calculation.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/stock_market_replay/02.calTradeCost_asofJoin.txt#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\njoinEngine=createAsofJoinEngine(name=\"tradeJoinSnapshot\", leftTable=tradeSchema, rightTable=snapshotSchema, outputTable=prevailingQuotes, metrics=<[Price, TradeQty, BidPX1, OfferPX1, abs(Price-(BidPX1+OfferPX1)/2), snapshotSchema.Time]>, matchingColumn=`SecurityID, timeColumn=`Time, useSystemTime=false, delayedTime=1)\n```\n\n----------------------------------------\n\nTITLE: Filtering Trades by Multiple Conditions - DolphinDB\nDESCRIPTION: This DolphinDB script filters the \"trades\" table using a combination of conditions on \"ts_code\", \"trade_date\", and \"higb\" columns.  It selects records where the stock code is '002308.SZ', the trade date is May 12, 2015, or the high price is greater than or equal to 15. The `timer(10)` function is used for performance measurement, and `clearAllCache()` clears the cache beforehand.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_17\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//根据股票代码、浮点型和时间过滤\ntimer(10) select * from trades where ts_code =  `002308.SZ and trade_date = 2015.05.12 or higb >= 15\n```\n\n----------------------------------------\n\nTITLE: Custom Transform Function for AMD Snapshot Data Subscription in DolphinDB\nDESCRIPTION: This transformation function modifies incoming AMD snapshot market data tables by adding a tradeDate column derived from the timestamp, suffixing security codes based on market type to distinguish Shanghai and Shenzhen markets, and normalizing price fields by dividing all price-related columns by 100. It reorders columns per the given reorderedColNames parameter to ensure consistency with the destination schema. This function should be partially applied to conform to the required one-parameter signature for subscription transforms.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_implementation_for_AMD_Plugin.md#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 赋阅处理方法，以 snapshot 为例\ndef handleSnapshotSubs(mutable msg, reorderedColNames) {\n    // 增加一个日期字段 tradeDate，其值为对应的时间戳字段的日期部分\n    update msg set tradeDate = date(origTime)\n    // 股票代码字段增加后缀，上海市场的后缀为\".SH\"，深圳市场的后缀为\".SZ\"\n    update msg set securityCode = securityCode + \".SZ\" where marketType=102\n    update msg set securityCode = securityCode + \".SH\" where marketType=101\n    // 所有价格字段值除以100\n    update msg set lastPrice = lastPrice / 100\n    update msg set openPrice = openPrice / 100\n    update msg set highPrice = highPrice / 100\n    update msg set lowPrice = lowPrice / 100\n    update msg set preClosePrice = preClosePrice / 100\n    update msg set offerPrice1 = offerPrice1 / 100\n    update msg set offerPrice2 = offerPrice2 / 100\n    update msg set offerPrice3 = offerPrice3 / 100\n    update msg set offerPrice4 = offerPrice4 / 100\n    update msg set offerPrice5 = offerPrice5 / 100\n    update msg set offerPrice6 = offerPrice6 / 100\n    update msg set offerPrice7 = offerPrice7 / 100\n    update msg set offerPrice8 = offerPrice8 / 100\n    update msg set offerPrice9 = offerPrice9 / 100\n    update msg set offerPrice10 = offerPrice10 / 100\n    update msg set bidPrice1 = bidPrice1 / 100\n    update msg set bidPrice2 = bidPrice2 / 100\n    update msg set bidPrice3 = bidPrice3 / 100\n    update msg set bidPrice4 = bidPrice4 / 100\n    update msg set bidPrice5 = bidPrice5 / 100\n    update msg set bidPrice6 = bidPrice6 / 100\n    update msg set bidPrice7 = bidPrice7 / 100\n    update msg set bidPrice8 = bidPrice8 / 100\n    update msg set bidPrice9 = bidPrice9 / 100\n    update msg set bidPrice10 = bidPrice10 / 100\n    update msg set weightedAvgOfferPrice = weightedAvgOfferPrice / 100\n    update msg set weightedAvgBidPrice = weightedAvgBidPrice / 100\n    update msg set highLimited = highLimited / 100\n    update msg set lowLimited = lowLimited / 100\n\n    // 调整列顺序为与流表、分布存储表一致\n    reorderColumns!(msg, reorderedColNames)\n\n    return msg\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Few Rows Loads Full Partition (OLAP) (DolphinDB Script)\nDESCRIPTION: Selects the top 100 rows from a specific partition ('2022.01.01') of an OLAP table. This example illustrates that even when retrieving a small subset of rows, DolphinDB loads all columns for the entire partition into memory, as the partition-column is the minimum unit for loading.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/memory_management.md#_snippet_6\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nselect top 100 * from loadTable(dbName,tableName) where day = 2022.01.01\nmem().allocatedBytes - mem().freeBytes\n```\n\n----------------------------------------\n\nTITLE: Schur Decomposition with sorting (lhp) in DolphinDB\nDESCRIPTION: Demonstrates Schur decomposition of a matrix in DolphinDB using the `schur` function with the `'lhp'` sort option (left half plane). The eigenvalues are sorted based on being in the left half plane (e < 0). sdim shows the number of eigenvalues satisfying the sort criteria.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/matrix.md#_snippet_28\n\nLANGUAGE: DolphinDB\nCODE:\n```\n>t,u, sdim=schur(m,'lhp') //'lhp':(e < 0.0), sdim 为符合sort条件的特征值数量\n>t;\n#0        #1        #2        #3       \n--------- --------- --------- ---------\n-4.306007 -1.390041 -1.099778 -1.857969\n0         -0.995651 -0.414519 2.960681 \n0         0         21.16354  -4.29753 \n0         0         0         2.138117 \n>u;\n#0       #1        #2       #3       \n-------- --------- -------- ---------\n-0.8395  -0.207229 0.483426 0.136364 \n0.444339 -0.793483 0.405703 0.091394 \n0.296182 0.529453  0.591475 0.531143 \n0.100391 0.217073  0.501858 -0.831228\n>sdim;\n2\n\n```\n\n----------------------------------------\n\nTITLE: 模拟单设备振动传感器数据的脚本（DolphinDB Python风格）\nDESCRIPTION: 此脚本定义了一个函数fakedata，模拟生成一台设备上16个振动传感器的时间序列数据，生成数据后通过tableInsert存储到传入的表中。支持不断模拟大量数据，适合实时测试。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Random_Vibration_Signal_Analysis_Solution.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef fakedata(t){\n    source=`channel+string(1..16) //16台传感器编号\n    num=source.shape()[0]\n    n=1000*60*10 \n    timestamp=now()\n    for (i in 1..n){\n        timestamp = timestamp+1\n        time1=take(timestamp,num)\n        flag = rand([-1, 1], num)\n        signalnose =rand(1.,num)\n        signalnose = signalnose*flag\n        Table = table(time1 as timestamp, source, signalnose).sortBy!(`source`timestamp)\n        tableInsert(t, Table)\n    }\n}\nsubmitJob(\"fakedataplay\",\"fakedataplayjob\",fakedata,t)\ngetRecentJobs(1)\n```\n\n----------------------------------------\n\nTITLE: Performing Row Sum using Multi-column Macro Variable\nDESCRIPTION: This query computes the sum of multiple columns using the `rowSum` function and a multi-column macro variable (`_$$names`). The macro variable references the names of columns to sum. The aggregate result is then given an alias using a macro variable too.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/macro_var_based_metaprogramming.md#_snippet_14\n\nLANGUAGE: DolphinDB\nCODE:\n```\nalias = \"rs_val\"\n<select rowSum(_$$names) as _$alias from t>.eval()\n```\n\n----------------------------------------\n\nTITLE: 从Vector中读取数据的三种方法\nDESCRIPTION: 展示从Vector读取数据的三种方法：使用getInt等方法获取单个元素、批量将数据复制到指定buffer以及获取只读buffer以提高读取效率。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/c++api.md#_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\n// 方法1：逐个元素获取\nfor(int i = 0; i < v->size(); ++i) {\n  cout << v->getInt(i) << ' ';\n}\n\n// 方法2：批量复制到buffer\nconst int BUF_SIZE = 1024;\nint buf[BUF_SIZE];\nint start = 0;\nint N = v->size();\nwhile (start < N) {\n    int len = std::min(N - start, BUF_SIZE);\n    v->getInt(start, len, buf);\n    for (int i = 0; i < len; ++i) {\n        cout << buf[i << ' ';\n    }\n    start += len;\n}\ncout << endl;\n\n// 方法3：获取只读buffer\nconst int BUF_SIZE = 1024;\nint buf[BUF_SIZE];\nint start = 0;\nint N = v->size();\nwhile (start < N) {\n    int len = std::min(N - start, BUF_SIZE);\n    const int* p = v->getIntConst(start, len, buf);\n    for (int i = 0; i < len; ++i) {\n        cout << p[i] << ' ';\n    }\n    start += len;\n}\ncout << endl;\n```\n\n----------------------------------------\n\nTITLE: Monitoring DolphinDB Server Logs for Stream Subscription Success - Shell\nDESCRIPTION: This shell command uses tail to continuously follow the latest 1000 lines of the dolphindb.log server log file. It is used to verify that the user startup script executed successfully during DolphinDB server startup, by checking for specific INFO logs indicating stream tables were created, enabled for persistence, and subscribed to streaming topics. Observation of successful subscription logs confirms that the streaming auto-subscription deployment has been completed without errors.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_auto_sub.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ntail -1000f /opt/DolphinDB/server/dolphindb.log\n```\n\n----------------------------------------\n\nTITLE: Elasticsearch Python Script for Date Filter and Average Volume Aggregation\nDESCRIPTION: This Python snippet constructs a JSON query filtering data by date greater than a specified date, applies a terms aggregation on PERMNO with average VOL, then executes the search request. It measures response status and fetches aggregation results for performance analysis.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_59\n\nLANGUAGE: Python\nCODE:\n```\ndata = json.dumps({\n    \"query\": {\n        \"constant_score\": {\n            \"filter\": {\n                 \"range\": {\n                    \"date\": {\n                        \"gt\": \"2014/01/12\",\n                    }\n                }\n            }\n        }\n    },\n    \"aggs\": {\n        \"group_by_permno\": {\n            \"terms\": {\n                \"field\": \"PERMNO\",\n                \"size\": 8373\n            },\n            \"aggs\": {\n                \"avg_vol\": {\n                    \"avg\": {\"field\": \"VOL\"}\n                }\n            }\n        }\n    },\n    \"_source\": [\"\"]\n}).encode(\"utf-8\")\n\nr = http.request(\"GET\", \"http://localhost:9200/uscsv/_search\", body=data,\n                 headers={'Content-Type': 'application/json'})\nprint(r.status)\nprint(json.loads(r.data.decode()))\n```\n\n----------------------------------------\n\nTITLE: Creating Workday Table in DolphinDB\nDESCRIPTION: Defines paths and names, checks if the 'workday' table exists in the 'dfs://publicFundDB' database, drops it if necessary, creates a new single-partition table schema with 'Day' (DATE) and 'Market' (SYMBOL) columns, creates the table sorted by 'Day', loads data from a specified CSV file ('workday.csv'), and appends it to the newly created table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/public_fund_basic_analysis/basic_analysis_of_public_fund_net_value.txt#_snippet_0\n\nLANGUAGE: dolphindb\nCODE:\n```\ncsvDataPath = \"/ssd/ssd2/data/fundData/workday.csv\"\ndbName = \"dfs://publicFundDB\"\ntbName = \"workday\"\n// create one-partition table in datase(\"dfs://publicFundDB\")\nif(existsTable(dbName, tbName)){\n\tdropTable(database(dbName), tbName)\n}\nnames = `Day`Market\ntypes = `DATE`SYMBOL\nschemaTB = table(1:0, names, types)\ndb = database(dbName)\ndb.createTable(table=schemaTB, tableName=tbName, sortColumns=`Day)\n// load CSV data\ntmp = ploadText(filename=csvDataPath, schema=table(names, types))\nloadTable(dbName, tbName).append!(tmp)\n```\n\n----------------------------------------\n\nTITLE: Creating Hash-Partitioned Table by SecurityID Causing Scaling Issues for Increasing Data Volume in DolphinDB Script\nDESCRIPTION: Creates a hash-partitioned database and partitioned table by symbol only. This design causes partition size to grow indefinitely with daily inserts, slowing queries filtering by date and making partitions too large (2.9.1 error). Requires DolphinDB hash partitioning. Input is stock trade fields; output is hash-partitioned table susceptible to large partition growth with time.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/database_and_table_creation_details.md#_snippet_17\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndb = database(\"dfs://testDB1\",HASH, [SYMBOL, 25])\ncolName = `SecurityID`TradeDate`TradeTime`TradePrice`TradeQty`TradeAmount`BuyNo`SellNo\ncolType = `SYMBOL`DATE`TIME`DOUBLE`INT`DOUBLE`INT`INT\ntbSchema = table(1:0, colName, colType)\ndb.createPartitionedTable(table=tbSchema, tableName=`pt, partitionColumns=`SecurityID)\n```\n\n----------------------------------------\n\nTITLE: Loading Trade Data and Querying Active Volume Percentage with DolphinDB\nDESCRIPTION: This snippet loads trade data from a distributed DolphinDB table 'trade' located in 'dfs://TL_Level2', then executes a query within a timer block to calculate the active volume percentage using the defined 'actVolumePercent' function. The query filters data for trades on 2023-02-01 and groups results by trade date and SecurityID, outputting SecurityID, TradeTime, and computed active volume percentage. Dependencies include the previously defined 'actVolumePercent' function and access to the specified distributed table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/DolphinDB_Python_Parser_Intro_for_Quantitative_Finance/因子实现_DolphinDB版本/主动成交量占比.txt#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntimer{\n\ttradeTB = loadTable(\"dfs://TL_Level2\", \"trade\")\n\tres = select SecurityID, TradeTime, actVolumePercent(TradeQty, BidApplSeqNum, OfferApplSeqNum, 60) as actVolumePercent from tradeTB where date(TradeTime)=2023.02.01 context by date(TradeTime), SecurityID\n}\n```\n\n----------------------------------------\n\nTITLE: Aggregating Total CPU Load for Specific Devices and Sorting by Hour with MongoDB - JavaScript\nDESCRIPTION: This aggregation pipeline filters by both time range and a set of device IDs, then groups by the hour component of time, calculates the total 15-minute average CPU load, and sorts results in descending order. Inputs include an array of device IDs and the time range. Output consists of hourly aggregate load values.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs MongoDB/mongodb_readings_query.txt#_snippet_12\n\nLANGUAGE: JavaScript\nCODE:\n```\ndb.device_readings.explain(\"executionStats\").aggregate([{$match:{time:{\"$gte\":ISODate(\"2016-11-15 12:00:00.000Z\"),\"$lte\":ISODate(\"2016-11-16 12:00:00.000Z\")},      device_id:{\"$in\":[\"demo000001\",\"demo000010\",\"demo000100\",\"demo001000\"]}}},{$group:{_id:{hour_new:{$hour:\"$time\"}},sum_15min:{$sum:\"$cpu_avg_15min\"}}},{$sort:{\"sum_15min\":-1}}])\n```\n\n----------------------------------------\n\nTITLE: Querying Backup Metadata for DolphinDB Partitioned Tables\nDESCRIPTION: Retrieves backup information about the partitioned table \"pt\" stored in the backup directory, including listing all backups and detailed metadata for specific partitions. The functions 'getBackupList' and 'getBackupMeta' require the backup path, database path, partition path, and table name as parameters. These provide insights into backup chunk paths and metadata useful for verifying backup status. This snippet depends on the existing backups being present at the target directory and is intended for auditing and validation of backup data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/restore-backup.md#_snippet_11\n\nLANGUAGE: DolphinDB\nCODE:\n```\ngetBackupList(\"/home/DolphinDB/backup\",\"dfs://compoDB\",\"pt\");\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\ngetBackupMeta(\"/home/DolphinDB/backup\",\"dfs://compoDB\",\"/20170810/0_50\",\"pt\");\n```\n\n----------------------------------------\n\nTITLE: Saving Time-Series Data as Redis Hashes in DolphinDB\nDESCRIPTION: Defines a function that extracts data from a DolphinDB table, converts multiple columns to string format using the string() function, and writes them to Redis hashes in batch based on provided IDs. It requires an active Redis connection handle and a DolphinDB table with specified columns (time, dwMileage, speed, longitude, latitude, elevation). It outputs no direct result but stores the data in Redis hashes with IDs as keys.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/redis_plugin_tutorial/example3-kafka.txt#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef saveHashDataToRedis(conn, tb){\n    id = exec id from tb\n    // 通过 string 函数将数据转为 string 类型\n    data = select string(time) as time,string(dwMileage) as dwMileage,\n        string(speed) as speed, string(longitude) as longitude, string(latitude) as latitude,\n        string(elevation) as elevation from tb\n    redis::batchHashSet(conn, id, data)\n}\n```\n\n----------------------------------------\n\nTITLE: Using RPC Function to Execute Remote Functions Within DolphinDB Cluster - DolphinDB\nDESCRIPTION: Illustrates using the 'rpc' function to execute a user-defined function remotely on a named cluster node 'nodeB'. The example calls 'salesSum' function with parameters on the remote node and returns the result. This syntax is designed to reuse existing network connections within a DolphinDB cluster, optimizing resources. It requires the user to have a running DolphinDB cluster with node aliases properly configured. The snippet also shows how to use partial application to form zero-parameter function calls for enhanced readability.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/hybrid_programming_paradigms.md#_snippet_15\n\nLANGUAGE: DolphinDB\nCODE:\n```\nrpc(\"nodeB\", salesSum, \"sales\",2018.07.02);\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nrpc(\"nodeB\", salesSum{\"sales\", 2018.07.02});\n```\n\n----------------------------------------\n\nTITLE: Python脚本中的性能测试流程（基金收益率计算和时间统计）\nDESCRIPTION: 脚本通过连接DolphinDB数据库，加载基金日净值和对应指数数据，进行数据合并和时间序列对齐，之后调用之前定义的 `getLog()` 和 `main()` 进行因子计算，最后统计全过程耗时。需要安装dolphindb-python客户端库，和numpy、pandas。输出为总耗时，可以评估Python处理性能。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/fund_factor_contrasted_by_py.md#_snippet_31\n\nLANGUAGE: Python\nCODE:\n```\ns = ddb.session()\ns.connect(\"127.0.0.1\", 8848, \"admin\", \"123456\")\nstart = time.time()\nfund_OLAP = s.loadTable(dbPath=\"dfs://fund_OLAP\", tableName=\"fund_OLAP\").select(\"*\").toDF().sort_values(['tradingDate'])\nfund_hs_OLAP = s.loadTable(dbPath=\"dfs://fund_OLAP\", tableName=\"fund_hs_OLAP\").select(\"*\").toDF()\nfund_hs_OLAP.rename(columns={'tradingDate': 'hstradingDate'}, inplace=True)\nfund_hs_OLAP = fund_hs_OLAP.sort_values(['hstradingDate'])\nfund_dui_OLAP = pd.merge_asof(fund_OLAP, fund_hs_OLAP, left_on=\"tradingDate\", right_on=\"hstradingDate\").sort_values(['fundNum_x', 'tradingDate'])\nfund_dui_OLAP = fund_dui_OLAP[fund_dui_OLAP['tradingDate'] == fund_dui_OLAP['hstradingDate']]\nfund_dui_OLAP.reset_index(drop=True, inplace=True)\nfund_dui_OLAP.drop(columns=['fundNum_y', 'hstradingDate'], inplace=True)\nfund_dui_OLAP.columns = ['tradingDate', 'fundNum', 'value', 'price']\nfund_dui_OLAP[\"log\"] = pd.Series(getLog(fund_dui_OLAP[\"value\"]))\nlist = fund_dui_OLAP[(fund_dui_OLAP['tradingDate'] >= datetime(2018, 5, 24)) & (fund_dui_OLAP['tradingDate'] <= datetime(2021, 5, 27))].groupby('fundNum')\nParallel(n_jobs=1)(delayed(main)(i) for _,i in list)\nend = time.time()\nprint(end-start)\n```\n\n----------------------------------------\n\nTITLE: Scheduled Job to Query and Calculate\nDESCRIPTION: This code defines a scheduled job in DolphinDB that loads a table named 'snapshot' from a distributed file system (DFS) and performs a query to calculate the `amtDiff` using the `level10Diff` UDF. The query selects `SecurityID`, `TradeTime`, and the calculated `amtDiff`. It filters data for a specific date (2023.02.01), applies a context-by clause (`context by SecurityID`) for grouped calculations, and sorts the results by `TradeTime`. Requires table \"dfs://TL_Level2\".snapshot to exist.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/DolphinDB_Python_Parser_Intro_for_Quantitative_Finance/因子实现_DolphinDB版本/十档净委买增额.txt#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntimer{\n\tsnapshotTB = loadTable(\"dfs://TL_Level2\", \"snapshot\")\n\tres = select SecurityID, TradeTime, level10Diff(BidPrice, BidOrderQty, true, 20) as amtDiff from snapshotTB where date(TradeTime)=2023.02.01 context by SecurityID csort TradeTime\n}\n```\n\n----------------------------------------\n\nTITLE: Applying Price Sensitivity Calculation in DolphinDB Timer\nDESCRIPTION: This `timer` block demonstrates loading Level 2 snapshot data for a specific date (2023.02.01) from the DFS table \"dfs://TL_Level2/snapshot\" using `loadTable`. It then applies the previously defined `priceSensitivityOrderFlowImbalance` function within a `select` statement, grouping the results by `TradeTime` (as date) and `SecurityID`, and stores the calculated sensitivities in the `res` variable.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/DolphinDB_Python_Parser_Intro_for_Quantitative_Finance/因子实现_DolphinDB版本/价格变动与一档量差的回归系数.txt#_snippet_1\n\nLANGUAGE: dolphindb script\nCODE:\n```\ntimer{\n\tsnapshotTB = loadTable(\"dfs://TL_Level2\", \"snapshot\")\n\tres = select priceSensitivityOrderFlowImbalance(LastPrice, BidOrderQty, OfferOrderQty) as priceSensitivityOrderFlowImbalance\n\t\tfrom snapshotTB\n\t\twhere date(TradeTime)=2023.02.01\n\t\tgroup by date(TradeTime) as TradeTime, SecurityID\n}\n```\n\n----------------------------------------\n\nTITLE: Script Command Request Format\nDESCRIPTION: Defines the format of a request for the 'script' command. It specifies the structure, including request type, session ID, message length, the command itself ('script'), and the script content to be executed. The script content follows, separated by newline characters. The 'script' command allows users to send script strings to execute in DolphinDB.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/api_protocol.md#_snippet_4\n\nLANGUAGE: text\nCODE:\n```\n长度(Byte) | 报文 | 说明 | 样本\n---|---|---|---\n3 或 4| 请求类型 | API，API2 | API\n1| 空格| char(0x20) |\n不固定|SESSIONID | 长度不固定，到空格为止  | 2247761467\n1| 空格| char(0x20) |\n2| 报文指令长度| 包含从“script\"到脚本内容结束为止的长度，如\"script\\n1+1\"  | 11\n1| 换行符 | char(0x10) |\n7| 指令 | script | \"script\"\n1| 换行符 | char(0x10) |\n不固定| 脚本内容 | 长度到下一个换行符为止| select * from loadTable('dfs://db','tb1') 或 sum(1..100) + avg(1..100)\n```\n\n----------------------------------------\n\nTITLE: Configuring DolphinDB Server for Stream Processing (Shell)\nDESCRIPTION: Configuration snippet for the DolphinDB server (`dolphindb.cfg`) tailored for a single-node deployment focused on stream processing. It sets parameters like memory limits (`maxMemSize`), worker threads (`workerNum`), web workers (`webWorkerNum`), persistence directory (`persistenceDir`), subscriber executors (`subExecutors`), and subscriber port (`subPort`) essential for handling streaming data efficiently.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_capital_flow_order_by_order.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nlocalSite=localhost:8848:local8848\nmode=single\nmaxMemSize=64\nmaxConnections=512\nworkerNum=8\nmaxConnectionPerSite=15\nnewValuePartitionPolicy=add\nwebWorkerNum=2\ndataSync=1\npersistenceDir=/opt/DolphinDB/server/local8848/persistenceDir\nmaxPubConnections=64\nsubExecutors=16\nsubPort=8849\nsubThrottle=1\npersistenceWorkerNum=1\nlanCluster=0\n```\n\n----------------------------------------\n\nTITLE: Query Streaming Registration Subscription Information in DolphinDB\nDESCRIPTION: Retrieves registration details for active streaming subscriptions, specifically the published tables. Utilizes getStreamingStat().pubTables to access subscription metadata for monitoring data stream publishing activities.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/machine_learning_volatility/07.streamStateQuery.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\ngetStreamingStat().pubTables\n```\n\n----------------------------------------\n\nTITLE: Creating MySQL User for Data Synchronization\nDESCRIPTION: SQL commands to create a dedicated MySQL user with necessary permissions for Debezium to capture changes. The user 'datasyn' is created and granted appropriate privileges for CDC operations.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Debezium_and_Kafka_data_sync.md#_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE USER 'datasyn'@'%' IDENTIFIED BY '1234';\n```\n\nLANGUAGE: SQL\nCODE:\n```\nGRANT SELECT, RELOAD, SHOW DATABASES, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'datasyn';\n```\n\nLANGUAGE: SQL\nCODE:\n```\nFLUSH PRIVILEGES;\n```\n\n----------------------------------------\n\nTITLE: Continuous Interval Max Value (Unoptimized - Custom Function)\nDESCRIPTION: This code snippet demonstrates an unoptimized approach to finding the maximum value within continuous intervals using a custom function. The function generates group IDs based on a target value, which are then used in context by.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_sql_cases.md#_snippet_20\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef generateGrp(targetVal, val) {\n\tarr = array(INT, val.size())\n\tn = 1\n\tfor(i in 0 : val.size()) {\n\t\tif(val[i] >= targetVal) {\n\t\t\tarr[i] = n\n\t\t\tif(val[i + 1] < targetVal) n = n + 1\n\t\t}\n\t}\n\treturn arr\n}\n\ntimer(1000) {\n\ttmp = select date, value, generateGrp(targetVal, value) as grp from t\n\tres1 = select date, value from tmp where grp != 0 \n\t\t   context by grp \n\t\t   having value = max(value) limit 1\n}\n```\n\n----------------------------------------\n\nTITLE: Updating MySQL Table Data with SQL\nDESCRIPTION: This SQL snippet demonstrates how to modify data in the MySQL 'config' table by setting new values for the columns frequency, maxvoltage, and maxec. This update triggers data changes that will be reflected when DolphinDB cache tables synchronize their contents. It requires access to the MySQL server with appropriate permissions to perform update operations. The input is the specified new data values, and the output is the updated rows in the MySQL database reflected in subsequent DolphinDB synchronizations.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/cachedtable.md#_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nupdate configDB.config set frequency=10,maxvoltage=250,maxec=30;\n```\n\n----------------------------------------\n\nTITLE: Equal Join in DolphinDB\nDESCRIPTION: This snippet performs an equal join (`ej`) between two tables: `sensors` (a distributed table - assumed) and `tagInfo` (the dimension table created in the previous example). It joins these tables using the `id` column in `sensors` and the `tagId` column in `tagInfo`. The `where` clause filters the results based on `machineId` and `datetime`. The output selects specific columns from both tables. This allows the retrieval of tag descriptions using the tagId from the sensors table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/iot_examples.md#_snippet_9\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect tagId,datetime,value,machineId,tagName,organization from ej(sensors,tagInfo,`id,`tagid) where machineId in [1,2], datetime between 2020.09.01T00:00:00 : 2020.09.01T00:59:59 order by datetime\n```\n\n----------------------------------------\n\nTITLE: Stock Factor Alignment (Inefficient)\nDESCRIPTION: This snippet shows an inefficient way to align stock factors.  It loads data from two distributed tables, merges them, and then joins with a dimension table to filter the data. Finally, it uses `pivot by` to reshape the data. This approach is slow due to the unnecessary table joins.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_sql_cases.md#_snippet_36\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntimer {\n nt1 = select concatDateTime(Date, Time) as TradeTime, SecurityID, FactorValue from loadTable(\"dfs://Factor10MinSH\", \"Factor10MinSH\") where Date between 2019.01.01 : 2020.10.31, FactorID = \"Factor01\"\n nt2 = select concatDateTime(Date, Time) as TradeTime, SecurityID, FactorValue from loadTable(\"dfs://Factor10MinSZ\", \"Factor10MinSZ\") where Date between 2019.01.01 : 2020.10.31, FactorID = \"Factor01\"\n unt = unionAll(nt1, nt2)\n \n sec = select SecurityID from loadTable(\"dfs://infodb\", \"MdSecurity\") where substr(SecurityID, 0, 3) in [\"001\", \"003\", \"005\", \"007\"]\n res = select * from lj(sec, unt, `SecurityID)\n\n res1 = select FactorValue from res pivot by TradeTime, SecurityID\n}\n```\n\n----------------------------------------\n\nTITLE: Disabling Firewall on Linux via systemctl (console)\nDESCRIPTION: Stop and permanently disable the firewall service (`firewalld`) if no firewall is required for DolphinDB. Note: Disabling the firewall may expose the system to security risks. Ensure this step aligns with your security policy.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/deploy_dolphindb_on_new_server.md#_snippet_11\n\nLANGUAGE: console\nCODE:\n```\n# systemctl stop firewalld\n# systemctl disable firewalld.service\n\n```\n\n----------------------------------------\n\nTITLE: Incremental Training of XGBoost Model\nDESCRIPTION: This code shows how to perform incremental training of an existing XGBoost model. By providing the `model` as the `xgbModel` parameter to the `xgboost::train` function, the training process will start from the existing model instead of training a new one from scratch. This is useful for updating a model with new data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/machine_learning.md#_snippet_23\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nmodel = xgboost::train(Y, X, params, , model)\n```\n\n----------------------------------------\n\nTITLE: Installing GitHub CLI on Ubuntu\nDESCRIPTION: These commands install the GitHub CLI tool on an Ubuntu system. The CLI is used to check out pull requests directly from the command line for testing and review.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_Open_Source_Project_Contribution.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ntype -p curl >/dev/null || (sudo apt update && sudo apt install curl -y)\ncurl -fsSL https://cli.github.com/packages/githubcli-archive-keyring.gpg | sudo dd of=/usr/share/keyrings/githubcli-archive-keyring.gpg \\\n&& sudo chmod go+r /usr/share/keyrings/githubcli-archive-keyring.gpg \\\n&& echo \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/githubcli-archive-keyring.gpg] https://cli.github.com/packages stable main\" | sudo tee /etc/apt/sources.list.d/github-cli.list > /dev/null \\\n&& sudo apt update \\\n&& sudo apt install gh -y\n```\n\n----------------------------------------\n\nTITLE: N-to-N Multi-Table Replay with replayDS and replay in DolphinDB Script\nDESCRIPTION: Streams several corresponding tables simultaneously by dividing each table with replayDS and relaying each source to its respective output stream table using the replay function in an N-to-N mode. All tables must have matching schemas and date/time partitioning. Replay is parallelized with specified concurrency for maximized performance.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/historical_data_replay.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\nds1 = replayDS(<select * from input1>, `date, `time, 08:00:00.000 + (1..10) * 3600000)\nds2 = replayDS(<select * from input2>, `date, `time, 08:00:00.000 + (1..10) * 3600000)\nds3 = replayDS(<select * from input3>, `date, `time, 08:00:00.000 + (1..10) * 3600000)\nreplay([ds1, ds2, ds3], [out1, out2, out3], `date, `time, 1000, true, 2)\n```\n\n----------------------------------------\n\nTITLE: 过滤工作日日期范围\nDESCRIPTION: 通过提取唯一交易日期，找到数据中的最早和最晚日期范围，随后筛选对应的工作日数据，用于时间序列分析。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/public_fund_basic_analysis.md#_snippet_8\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndateRange = exec distinct(TradeDate) from fundNetValue\nfirstDate = min(dateRange)\nlastDate = max(dateRange)\nworkdays = exec day from loadTable(\"dfs://publicFundDB\", \"workday\") where market=\"SSE\", day between firstDate : lastDate, day in dateRange\n```\n\n----------------------------------------\n\nTITLE: Configuring Data and Compute Nodes in cluster.cfg File\nDESCRIPTION: Defines configuration settings for data and compute nodes in cluster.cfg. Controls memory size limits, worker thread counts, job worker limits, cache engine sizes for OLAP and TSDB, partition policies, connection limits, and chunk granularity enabling. Users should tailor these parameters according to hardware specs and cluster workloads. Also includes example settings for publicName parameters enabling web interface access per node.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ha_cluster_deployment.md#_snippet_4\n\nLANGUAGE: config\nCODE:\n```\nmaxMemSize=32\nmaxConnections=512\nworkerNum=4\nmaxBatchJobWorker=4\nOLAPCacheEngineSize=2\nTSDBCacheEngineSize=2\nnewValuePartitionPolicy=add\nmaxPubConnections=64\nsubExecutors=4\nlanCluster=0\nenableChunkGranularityConfig=true\n```\n\nLANGUAGE: config\nCODE:\n```\ndatanode1.publicName=19.56.128.21\ncomputenode1.publicName=19.56.128.21\ndatanode2.publicName=19.56.128.22\ncomputenode2.publicName=19.56.128.22\ndatanode3.publicName=19.56.128.23\ncomputenode3.publicName=19.56.128.23\n```\n\n----------------------------------------\n\nTITLE: C++ API - Deserialize Stream Data - Constructing Schema\nDESCRIPTION: This C++ code constructs the schema for the tables used in the replay, including 'snapshot', 'order', 'transaction', and the 'end' signal. This involves querying the existing tables in DolphinDB and retrieving their schema using the `conn.run()` method and saving to the `snap_full_schema`, `order_full_schema`, `transac_full_schema`, and `end_full_schema`. The retrieved schema is then stored in an `unordered_map` called `sym2schema`, which maps the table name to its corresponding schema. Finally, it creates a `StreamDeserializer` object to deserialize the incoming streaming data. Prerequisites: Establish C++ API connection and know the table schemas.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/appendices_market_replay_bp.md#_snippet_10\n\nLANGUAGE: C++\nCODE:\n```\nDictionarySP snap_full_schema = conn.run(\"loadTable(\\\"dfs://Test_snapshot\\\", \\\"snapshot\\\").schema()\");\nDictionarySP order_full_schema = conn.run(\"loadTable(\\\"dfs://Test_order\\\", \\\"order\\\").schema()\");\nDictionarySP transac_full_schema = conn.run(\"loadTable(\\\"dfs://Test_transaction\\\", \\\"transaction\\\").schema()\");\nDictionarySP end_full_schema = conn.run(\"loadTable(\\\"dfs://End\\\", \\\"endline\\\").schema()\");\n\nunordered_map<string, DictionarySP> sym2schema;\nsym2schema[\"snapshot\"] = snap_full_schema;\nsym2schema[\"order\"] = order_full_schema;\nsym2schema[\"transaction\"] = transac_full_schema;\nsym2schema[\"end\"] = end_full_schema;\nStreamDeserializerSP sdsp = new StreamDeserializer(sym2schema);\n```\n\n----------------------------------------\n\nTITLE: Query Narrow Table Data in DolphinDB\nDESCRIPTION: This function queries factor data stored in a narrow table format within a specified time range for a given set of factors.  It loads the table and then filters based on the time range and the factors, using `pivot by` to restructure the data for output. The dependencies are `loadTable`, `tradetime`, `symbol`, and `factorname` columns.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practices_for_multi_factor.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 窄表模式查询随机1000因子\ndef querySingleModel(dbname,tbname,start_time,end_time,aim_factor){\n\treturn select value from loadTable(dbname,tbname) where tradetime>=start_time and tradetime<= end_time and  factorname in aim_factor pivot by tradetime,symbol,factorname\n}\n```\n\n----------------------------------------\n\nTITLE: Defining a Function for Calculating Capital Flow (DolphinDB)\nDESCRIPTION: This DolphinDB function, `calCapitalFlow`, calculates capital flow based on trade data, categorizing trades into 'Buy Small', 'Buy Big', 'Sell Small', and 'Sell Big' amounts based on a threshold (`smallBigBoundary`). It takes `Num`, `BSFlag`, `TradeQty`, and `TradeAmount` as input, aggregates trade data, and returns an array of the calculated capital flow amounts, filling null values with 0.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_capital_flow_order_by_order.md#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//real time calculation of minute index\ndefg calCapitalFlow(Num, BSFlag, TradeQty, TradeAmount){\n\t// You can define the smallBigBoundary by yourself\n\tsmallBigBoundary = 50000\n\ttempTable1 = table(Num as `Num, BSFlag as `BSFlag, TradeQty as `TradeQty, TradeAmount as `TradeAmount)\n\ttempTable2 = select sum(TradeQty) as TradeQty, sum(TradeAmount) as TradeAmount from tempTable1 group by Num, BSFlag\n\tBuySmallAmount = exec sum(TradeAmount) from  tempTable2 where TradeQty<=smallBigBoundary && BSFlag==`B\n\tBuyBigAmount = exec sum(TradeAmount) from tempTable2 where TradeQty>smallBigBoundary && BSFlag==`B\n\tSellSmallAmount = exec sum(TradeAmount) from  tempTable2 where TradeQty<=smallBigBoundary && BSFlag==`S\n\tSellBigAmount = exec sum(TradeAmount) from tempTable2 where TradeQty>smallBigBoundary && BSFlag==`S\n\treturn nullFill([BuySmallAmount, BuyBigAmount, SellSmallAmount, SellBigAmount], 0)\n}\n```\n\n----------------------------------------\n\nTITLE: Add New Factor in Narrow Table (DolphinDB)\nDESCRIPTION: This function adds a new factor to a narrow table. It creates a new table with the new factor's data, then appends it to the existing table using `append!`.  It uses `createColnameAndColtype` (not defined here) to create column names and types. Dependencies: `getTimeList`, `stretch`, `take`, `rand`, `loadTable`, and `append!`.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practices_for_multi_factor.md#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//窄表模式新增1个因子\ndef singleModelAddNewFactor(dbname,tbname,start_date,end_date,symbols,factor_names,new_factor){\n\ttime_list = getTimeList(start_date,end_date).flatten()\n\tnum_row = symbols.size()*time_list.size()\n\tcol_names,col_types = createColnameAndColtype(\"single\",factor_names)\n\tt = table(num_row:num_row,col_names,col_types)\n\tt[\"tradetime\"] = stretch(time_list,num_row)\n\tt[\"symbol\"] = take(symbols,num_row)\n\tt[\"factorname\"] = take(new_factor,num_row)\n\tt[\"value\"] = rand(100.0,num_row)\n\tpt = loadTable(dbname,tbname)\n\tpt.append!(t)\t\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Volume Weighted Average Price Function Using pandas in Python\nDESCRIPTION: Defines a function 'volumeWeightedAvgPrice' that computes the volume weighted average price over a rolling window defined by 'lag'. It takes a dataframe 'df' containing trade data with columns 'TradeTime', 'SecurityID', 'OrderQty', and 'Price'. The function calculates rolling sums of total traded amount and volume to compute VWAP and returns a dataframe including 'TradeTime', 'SecurityID', and the VWAP column. It requires pandas and assumes input dataframe columns are correctly formatted.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/DolphinDB_Python_Parser_Intro_for_Quantitative_Finance/因子实现_Python版本/委托量加权平均委托价格.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef volumeWeightedAvgPrice(df, lag):\n    res = df[[\"TradeTime\", \"SecurityID\"]]\n    totalAmount = (df[\"OrderQty\"]*df[\"Price\"]).rolling(lag).sum()\n    totalVolume = df[\"OrderQty\"].rolling(lag).sum()\n    res[\"volumeWeightedAvgPrice\"] = totalAmount / totalVolume\n    return res\n```\n\n----------------------------------------\n\nTITLE: 实现 columnAvg 函数 - C++\nDESCRIPTION: 这段C++代码定义了 `columnAvg` 函数，它使用 DolphinDB 的 `mr` 函数实现对分布式表中多个指定列的平均值的计算。它首先通过 `heap->currentSession()->getFunctionDef` 获取必要的 map, reduce, 和 final 函数，以及 `mr` 函数本身。 然后，它使用 `Util::createPartialFunction` 对 map 函数进行部分应用，以满足 `mr` 函数的参数需求。 最后，它调用 `mr` 函数来执行分布式计算，并返回结果。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_development_tutorial.md#_snippet_11\n\nLANGUAGE: cpp\nCODE:\n```\nConstantSP columnAvg(Heap *heap, vector<ConstantSP> &args) {\n    ConstantSP ds = args[0];\n    ConstantSP colNames = args[1];\n\n    FunctionDefSP mapFunc = heap->currentSession()->getFunctionDef(\"columnAvg::columnAvgMap\");\n    vector<ConstantSP> mapWithColNamesArgs = {new Void(), colNames};\n    FunctionDefSP mapWithColNames = Util::createPartialFunction(mapFunc, mapWithColNamesArgs);    // columnAvgMap{, colNames}\n    FunctionDefSP reduceFunc = heap->currentSession()->getFunctionDef(\"add\");\n    FunctionDefSP finalFunc = heap->currentSession()->getFunctionDef(\"columnAvg::columnAvgFinal\");\n\n    FunctionDefSP mr = heap->currentSession()->getFunctionDef(\"mr\");    // mr(ds, columnAvgMap{, colNames}, add, columnAvgFinal)\n    vector<ConstantSP> mrArgs = {ds, mapWithColNames, reduceFunc, finalFunc};\n    return mr->call(heap, mrArgs);\n}\n```\n\n----------------------------------------\n\nTITLE: Specifying Data Volumes in DolphinDB Cluster Configuration (Shell)\nDESCRIPTION: Examples show how to configure data volume paths for DolphinDB data nodes using cluster config files. Volumes can be assigned per node, via wildcard patterns, or by explicit alias expansion. Prerequisite: cluster.cfg accessible and writable. Inputs: configuration lines for the DolphinDB config file. Outputs: Sets disk paths for distributed storage. Limitation: Only absolute paths are supported; avoid NAS/NFS mounts without proper root permissions.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ha_cluster_deployment.md#_snippet_19\n\nLANGUAGE: Shell\nCODE:\n```\nP1-datanode.volumes=/DFS/P1-datanode\nP2-datanode.volumes=/DFS/P2-datanode\n```\n\nLANGUAGE: Shell\nCODE:\n```\n%-datanode.volumes=/VOL1\n```\n\nLANGUAGE: Shell\nCODE:\n```\nvolumes=/VOL1/<ALIAS>,/VOL2/<ALIAS>\n```\n\n----------------------------------------\n\nTITLE: Setup Parallel Sell Order Processing Engines (DolphinDB)\nDESCRIPTION: Defines a function `processSellOrderFunc` to create parallel `reactiveStateEngine` instances for processing sell trade data. It defines the schema for a dummy table representing expected input from the previous stage and the metrics to calculate. It iterates `parallel` times, creating an engine instance for each parallel partition and chaining its output to the corresponding `processCapitalFlow` engine instance. This stage receives data from the `processBuyOrder` engines.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/streaming_capital_flow_daily/02.createEngineSub.txt#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef processSellOrderFunc(parallel){\n\tcolName = `SecurityID`BuyNum`TradeTime`SellNum`TradeAmount`TradeQty`TotalBuyAmount`BuyOrderFlag`PrevTotalBuyAmount`PrevBuyOrderFlag\n\tcolType =  [SYMBOL, INT, TIMESTAMP, INT, DOUBLE, INT, DOUBLE, INT, DOUBLE, INT]\n\tprocessBuyOrder = table(1:0, colName, colType)\n\tmetricsSell = [\n\t\t<TradeTime>,\n\t\t<TradeAmount>,\n\t\t<cumsum(TradeAmount)>,\n\t\t<tagFunc(cumsum(TradeQty))>,\n\t\t<prev(cumsum(TradeAmount))>,\n\t\t<prev(tagFunc(cumsum(TradeQty)))>,\n\t\t<BuyNum>,\n\t\t<TotalBuyAmount>,\n\t\t<BuyOrderFlag>,\n\t\t<PrevTotalBuyAmount>,\n\t\t<PrevBuyOrderFlag>]\n\tfor(i in 1..parallel){\n\t\tcreateReactiveStateEngine(name = \"processSellOrder\"+string(i), metrics = metricsSell, dummyTable = processBuyOrder, outputTable = getStreamEngine(\"processCapitalFlow\"+string(i)), keyColumn = `SecurityID`SellNum, keepOrder = true)\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: Simulating Stock Data Generation and Inserting into Partitioned Table on DolphinDB Data Node (DolphinDB script)\nDESCRIPTION: This DolphinDB script simulates the generation of intraday stock data for 5000 stocks over one day with 1-minute intervals and inserts the generated data into a previously created partitioned table. Random price and volume data arrays are created, stock IDs and timestamps constructed, and combined into a table matching the schema. The generated table is then appended to the partitioned table in the distributed database. This example demonstrates data synthesis, table manipulation, and distributed data insertion. It requires the distributed database and partitioned table to exist and be accessible on the data node.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/single_machine_cluster_deploy.md#_snippet_3\n\nLANGUAGE: DolphinDB script\nCODE:\n```\n// 模拟数据并写入分区表\nn = 1210000\nrandPrice = round(10+rand(1.0, 100), 2)\nrandVolume = 100+rand(100, 100)\nSecurityID = lpad(string(take(0..4999, 5000)), 6, `0)\nDateTime = (2023.01.08T09:30:00 + take(0..120, 121)*60).join(2023.01.08T13:00:00 + take(0..120, 121)*60)\nPreClosePx = rand(randPrice, n)\nOpenPx = rand(randPrice, n)\nHighPx = rand(randPrice, n)\nLowPx = rand(randPrice, n)\nLastPx = rand(randPrice, n)\nVolume = int(rand(randVolume, n))\nAmount = round(LastPx*Volume, 2)\ntmp = cj(table(SecurityID), table(DateTime))\nt = tmp.join!(table(PreClosePx, OpenPx, HighPx, LowPx, LastPx, Volume, Amount))\ndbName = \"dfs://testDB\"\ntbName = \"testTB\"\nloadTable(dbName, tbName).append!(t)\n```\n\n----------------------------------------\n\nTITLE: Accessing DolphinDB Web Console via HTTP\nDESCRIPTION: URL to open the DolphinDB web console for login and interactive metric query execution. The console typically listens on port 8848 on localhost. Dependencies: DolphinDB is running and listening on port 8848. Input: none. Output: web interface for performing database operations.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_Telegraf_Grafana.md#_snippet_8\n\nLANGUAGE: HTTP\nCODE:\n```\nhttp://localhost:8848/\n```\n\n----------------------------------------\n\nTITLE: Define and Share Keyed Stream Table\nDESCRIPTION: This code snippet shows how to create a keyed stream table using the `keyedStreamTable` function. The table `pubTable` is defined with a primary key (`timestamp`), initial capacity, columns, and data types.  Duplicate keys are rejected to ensure uniqueness.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_tutorial.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\nshare keyedStreamTable(`timestamp, 10000:0,`timestamp`temperature, [TIMESTAMP,DOUBLE]) as pubTable\n```\n\n----------------------------------------\n\nTITLE: Restoring DolphinDB Table Partitions with restore Function\nDESCRIPTION: Restores data for specified partitions of a DolphinDB distributed table using the restore function. The example iterates over 31 days, dynamically constructing backup directory paths, checks for existence, and restores partitions using wildcard (%) to restore all partitions. The force parameter ensures data overwrite if necessary. This requires the target database and table to be pre-created with matching names and partitioning structure. It uses DolphinDB commands and functions for date and path handling.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/restore-backup.md#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\nday=2020.01.01\nfor(i in 1..31){\n\tpath=\"/hdd/hdd1/backup/\"+temporalFormat(day, \"yyyyMMdd\") + \"/\";\n\tday=datetimeAdd(day,1,`d)\n\tif(!exists(path)) continue;\n\tprint \"restoring \" + path;\n\trestore(backupDir=path,dbPath=\"dfs://ddb\",tableName=\"windTurbine\",partition=\"%\",force=true);\n}\n```\n\n----------------------------------------\n\nTITLE: Write Narrow Table Data in DolphinDB\nDESCRIPTION: This function writes factor data in a narrow table format to a DolphinDB database. It iterates through a given time range and factor names, submitting jobs to write data for each combination to a node in the cluster. The function utilizes the `singleModelPartitionData` function (not defined in this snippet) to perform the actual data writing for a specific partition.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practices_for_multi_factor.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 窄表模式写入某个时间范围数据\ndef writeSingleModelData(dbname,tbname,start_date,end_date,symbols,factor_names){\n\ttotal_time_range = getTimeList(start_date,end_date)\n\tnodes = exec value from pnodeRun(getNodeAlias)\n\tfor(j in 0..(total_time_range.size()-1)){\n\t\tfor(i in 0..(factor_names.size()-1)){\n\t\t\trpc(nodes[i%(nodes.size())],submitJob,\"singleModel\"+j+\"and\"+i,dbname,singleModelPartitionData,dbname,tbname,total_time_range[j],symbols,factor_names,factor_names[i])\n\t\t}\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Distributed DFS Table Statistics with summary - DolphinDB Script\nDESCRIPTION: This DolphinDB snippet showcases how to generate summary statistics for a distributed DFS table. It involves loading a DFS database, then computes summary statistics using the summary function with partition sampling to speed up calculations. Requires an existing DFS database and table; DolphinDB server cluster is assumed ready. Parameters specify interpolation, statistical characteristics, percentiles, precision, and partitionSampling rate. The snippet is optimized for large-scale, partitioned data where full scans are costly.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/generate_large_scale_statistics_with_summary.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndb = database(dbName,VALUE,`A`B`C`D)\n...\npt = loadTable(dbName, tableName)\n// 生成 DFS 表统计信息，同时指定分区采样率为 0.5\nre = summary(pt,`linear,`avg`std, [25,50], 0.001, 0.5)\n```\n\n----------------------------------------\n\nTITLE: Calculating Annual Average Returns by Fund Type in DolphinDB\nDESCRIPTION: Loads fund metadata including type from 'publicFundData' table, creates a dictionary mapping SecurityID to fund Type for funds present in `returnsMatrix`. Reorders the columns of `returnsMatrix` based on the fund type order and renames columns to include type information. Calculates the annual average rate of return for each fund type by resampling daily returns annually (using product for compounding, filling nulls with 0), then grouping by type and calculating the mean. Plots the results for Bond, Stock, and Hybrid types from 2014 onwards as a bar chart. Depends on 'publicFundData' table, `returnsMatrix`, and `fundTypeMap`.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/public_fund_basic_analysis/basic_analysis_of_public_fund_net_value.txt#_snippet_6\n\nLANGUAGE: dolphindb\nCODE:\n```\n// Build a one-to-one dictionary of fund names and fund types\nfundData = loadTable(\"dfs://publicFundDB\", \"publicFundData\")\nfundType = select SecurityID, Type from fundData where SecurityID in returnsMatrix.colNames() order by Type\nfundTypeMap = dict(fundType[\"SecurityID\"], fundType[\"Type\"])\n// Change the column sorting and column name for display\ntReturnsMatrix = returnsMatrix[fundType[\"SecurityID\"]]\nnewNames = fundType[\"Type\"] + \"_\" + fundType[\"SecurityID\"].strReplace(\".\", \"_\").strReplace(\"!\", \"1\")\ntReturnsMatrix.rename!(newNames)\ntReturnsMatrix[0:3]\n// calculate the annual average rate of return of each type of fund\nyearReturnsMatrix = ((returnsMatrix+1).resample(\"A\", prod)-1).nullFill(0).regroup(fundTypeMap[returnsMatrix.colNames()], mean, byRow=false)\n// plot partial results\nyearReturnsMatrix = yearReturnsMatrix.loc( , [\"债券型\", \"股票型\", \"混合型\"])\nyearReturnsMatrix.loc(year(yearReturnsMatrix.rowNames())>=2014, ).plot(chartType=BAR)\n```\n\n----------------------------------------\n\nTITLE: Creating Scalar Constants in DolphinDB C++ Plugin\nDESCRIPTION: This snippet demonstrates how to create scalar constants in a DolphinDB C++ plugin.  It illustrates the use of both direct instantiation and the `Util::createConstant` method. The direct instantiation method is recommended for its simplicity.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_advance.md#_snippet_0\n\nLANGUAGE: c++\nCODE:\n```\nConstantSP i = new Int(1);                 // 相当于1i\nConstantSP i1 = Util::createConstant(DT_INT);\ni1->setInt(1);                             // 也相当于1i    \n\nConstantSP s = new String(\"DolphinDB\");    // 相当于\"DolphinDB\"\nConstantSP s1 = Util::createConstant(DT_STRING);\ns1->setString(\"DolphinDB\");                // 也相当于\"DolphinDB\"\n\nConstantSP d = new Date(2020, 11, 11);     // 相当于2020.11.11\nConstantSP d1 = Util::createConstant(DT_DATE);\n//由于日期类型没有对应的set方法，而date在dolphindb中存储为int，为从1970.01.01开始经过的天数，所以可以通过换算，并用setInt进行赋值\nd1->setInt(18577);                         // 也相当于2020.11.11  \n\nConstantSP voidConstant = new Void();      // 创建一个void类型变量，常用于表示空的函数参数\n```\n\n----------------------------------------\n\nTITLE: Analyzing the WHERE Clause in DolphinDB Execution Plan\nDESCRIPTION: This JSON snippet illustrates the 'where' section of a DolphinDB execution plan. It typically shows the number of rows remaining after the filter ('rows') and the time cost ('cost' in microseconds) of applying the filter condition. In this example, the filtering reduced the data to 9 rows and took 13 microseconds.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_Explain.md#_snippet_6\n\nLANGUAGE: JSON\nCODE:\n```\n    \"where\": {\n      \"rows\": 9,\n      \"cost\": 13\n    },\n```\n\n----------------------------------------\n\nTITLE: DolphinDB Server Configuration File (cluster.cfg) in INI Format\nDESCRIPTION: This configuration file sets parameters for a DolphinDB single-server cluster deployment. It specifies memory limits, concurrency and worker thread counts, log settings, volume paths for multiple nodes, redo log dirs, meta dirs, persistence directories, sub-port numbers, streaming high availability settings including raft mode and groups, and cache sizes. It requires user adaptation of volume and directory paths to the target environment. Proper configuration impacts server performance, data reliability, and streaming stability.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/appendices_market_replay_bp.md#_snippet_16\n\nLANGUAGE: INI\nCODE:\n```\nmaxMemSize=128\nmaxConnections=5000\nworkerNum=24\nwebWorkerNum=2\nchunkCacheEngineMemSize=16\nnewValuePartitionPolicy=add\nlogLevel=INFO\nmaxLogSize=512\nnode1.volumes=/ssd/ssd3/pocTSDB/volumes/node1,/ssd/ssd4/pocTSDB/volumes/node1\nnode2.volumes=/ssd/ssd5/pocTSDB/volumes/node2,/ssd/ssd6/pocTSDB/volumes/node2\nnode3.volumes=/ssd/ssd7/pocTSDB/volumes/node3,/ssd/ssd8/pocTSDB/volumes/node3\ndiskIOConcurrencyLevel=0\nnode1.redoLogDir=/ssd/ssd3/pocTSDB/redoLog/node1\nnode2.redoLogDir=/ssd/ssd4/pocTSDB/redoLog/node2\nnode3.redoLogDir=/ssd/ssd5/pocTSDB/redoLog/node3\nnode1.chunkMetaDir=/ssd/ssd3/pocTSDB/metaDir/chunkMeta/node1\nnode2.chunkMetaDir=/ssd/ssd4/pocTSDB/metaDir/chunkMeta/node2\nnode3.chunkMetaDir=/ssd/ssd5/pocTSDB/metaDir/chunkMeta/node3\nnode1.persistenceDir=/ssd/ssd6/pocTSDB/persistenceDir/node1\nnode2.persistenceDir=/ssd/ssd7/pocTSDB/persistenceDir/node2\nnode3.persistenceDir=/ssd/ssd8/pocTSDB/persistenceDir/node3\nmaxPubConnections=128\nsubExecutors=24\nsubThrottle=1\npersistenceWorkerNum=1\nnode1.subPort=8825\nnode2.subPort=8826\nnode3.subPort=8827\nmaxPartitionNumPerQuery=200000\nstreamingHAMode=raft\nstreamingRaftGroups=2:node1:node2:node3\nnode1.streamingHADir=/ssd/ssd6/pocTSDB/streamingHADir/node1\nnode2.streamingHADir=/ssd/ssd7/pocTSDB/streamingHADir/node2\nnode3.streamingHADir=/ssd/ssd8/pocTSDB/streamingHADir/node3\nTSDBCacheEngineSize=16\nTSDBCacheEngineCompression=false\nnode1.TSDBRedoLogDir=/ssd/ssd3/pocTSDB/TSDBRedoLogDir/node1\nnode2.TSDBRedoLogDir=/ssd/ssd4/pocTSDB/TSDBRedoLogDir/node2\nnode3.TSDBRedoLogDir=/ssd/ssd5/pocTSDB/TSDBRedoLogDir/node3\nTSDBLevelFileIndexCacheSize=20\nTSDBLevelFileIndexCacheInvalidPercent=0.6\nlanCluster=0\nenableChunkGranularityConfig=true\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid - overload\nDESCRIPTION: Configuration settings for the Druid overload service including memory allocation (Xms, Xmx). Dependencies: Druid setup.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_Druid_appendix.md#_snippet_8\n\nLANGUAGE: Druid\nCODE:\n```\noverload:\nXms3g\nXmx3g\n```\n\n----------------------------------------\n\nTITLE: Defining clearEnv Function\nDESCRIPTION: This function clears the environment by unsubscribing from tables, dropping stream tables, dropping aggregators (reactive and session window engines), and unsubscribing from MQTT topics. It uses try-catch blocks to handle potential errors during the cleanup process. It iterates through the subscription IDs to unsubscribe from each topic.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/data/DolphinDB_streaming_application_in_IOT/deviceState.txt#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef  clearEnv(){\n\ttry{\n\t\tunsubscribeTable( tableName=`inputSt, actionName=\"monitor\")\n\t\tif(objs(true).name.find('inputSt')!=-1) \n\t\t\tdropStreamTable(`inputSt)\n\t\tif(objs(true).name.find('outputSt1')!=-1) \n\t\t\tdropStreamTable(`outputSt1)\n\t\tif(objs(true).name.find('outputSt2')!=-1) \n\t\t\tdropStreamTable(`outputSt2)\n\t\tif (getAggregatorStat().ReactiveStreamEngine[`name].find(`reactivEngine)!=-1)\n\t\t\tdropAggregator(`reactivEngine)\n\t\tif (getAggregatorStat().SessionWindowEngine[`name].find(`swEngine)!=-1)\n\t\t\tdropAggregator(`swEngine)\n\t\tfor (id in mqtt::getSubscriberStat()[`subscriptionId]) \n\t\t\tmqtt::unsubscribe(id)\n\t}catch(ex){\n\t\tprint(ex)\n\t}\n\t\n}\n```\n\n----------------------------------------\n\nTITLE: Starting DolphinDB Agent Nodes Using Shell Script on CentOS (console)\nDESCRIPTION: This snippet includes shell commands to launch DolphinDB agent nodes on centos-1, centos-2, and centos-3 machines by executing the startAgent.sh script located in the DolphinDB cluster demo directory. It requires a Linux environment with DolphinDB pre-installed. The expected input is the command line invocation, and output is the initialization of agent nodes necessary for the distributed cluster operation.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/deploy_dolphindb_on_new_server.md#_snippet_18\n\nLANGUAGE: console\nCODE:\n```\n$ cd DolphinDB/clusterDemo/\n$ ./startAgent.sh\n```\n\n----------------------------------------\n\nTITLE: Calculating Price Sensitivity and Order Flow Imbalance Function in Python\nDESCRIPTION: This function computes the price sensitivity and order flow imbalance based on the provided stock data DataFrame. It calculates the price difference, bid/ask quantities, and applies a linear regression formula to evaluate market imbalance indicators. It requires pandas and numpy for data manipulation and mathematical operations.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/DolphinDB_Python_Parser_Intro_for_Quantitative_Finance/因子实现_Python版本/价格变动与一档量差的回归系数.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ndef priceSensitivityOrderFlowImbalance(df):  \n    deltaP = 10000*df[\"LastPrice\"].diff().fillna(0)\n    bidQty1 = df[\"BidOrderQty1\"].fillna(0)\n    askQty1 = df[\"OfferOrderQty1\"].fillna(0)\n    NVOL = bidQty1 - askQty1\n\n    # 线性回归公式 linearRegression(y, x): (len(x)*sum(x*y) - sum(x)*sum(y)) / (len(x)*sum(x*x) - sum(x)*sum(x))\n    res = (len(NVOL)*sum(NVOL*deltaP) - sum(NVOL)*sum(deltaP)) / (len(NVOL)*sum(NVOL*NVOL)- sum(NVOL)*sum(NVOL))\n    return res\n```\n\n----------------------------------------\n\nTITLE: Define Schema for Binary Import with Strings\nDESCRIPTION: This snippet defines the schema for importing binary data containing strings using `loadRecord`. The schema is defined as a tuple of tuples, where each inner tuple specifies the column name, data type, and string length (if applicable). String lengths must be fixed.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_data.md#_snippet_11\n\nLANGUAGE: txt\nCODE:\n```\nschema = [(\"code\", SYMBOL, 32),(\"date\", INT),(\"time\", INT),(\"last\", FLOAT),(\"volume\", INT),(\"value\", FLOAT),(\"ask1\", FLOAT),(\"ask2\", FLOAT),(\"ask3\", FLOAT),(\"ask4\", FLOAT),(\"ask5\", FLOAT),(\"ask6\", FLOAT),(\"ask7\", FLOAT),(\"ask8\", FLOAT),(\"ask9\", FLOAT),(\"ask10\", FLOAT),(\"ask_size1\", INT),(\"ask_size2\", INT),(\"ask_size3\", INT),(\"ask_size4\", INT),(\"ask_size5\", INT),(\"ask_size6\", INT),(\"ask_size7\", INT),(\"ask_size8\", INT),(\"ask_size9\", INT),(\"ask_size10\", INT),(\"bid1\", FLOAT),(\"bid2\", FLOAT),(\"bid3\", FLOAT),(\"bid4\", FLOAT),(\"bid5\", FLOAT),(\"bid6\", FLOAT),(\"bid7\", FLOAT),(\"bid8\", FLOAT),(\"bid9\", FLOAT),(\"bid10\", FLOAT),(\"bid_size1\", INT),(\"bid_size2\", INT),(\"bid_size3\", INT),(\"bid_size4\", INT),(\"bid_size5\", INT),(\"bid_size6\", INT),(\"bid_size7\", INT),(\"bid_size8\", INT),(\"bid_size9\", INT),(\"bid_size10\", INT)]\n```\n\n----------------------------------------\n\nTITLE: Allocating Memory and Displaying Usage\nDESCRIPTION: This snippet demonstrates how to create a vector of integers and a table, then checks the memory allocated to them using the `mem()` function.  It highlights the approximate memory usage for the created data structures.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/memory_management.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\nv = 1..100000000\nmem().allocatedBytes - mem().freeBytes\n```\n\n----------------------------------------\n\nTITLE: Implementing Bid Withdraws Factor with `each` in DolphinDB\nDESCRIPTION: DolphinDB script defining functions `withdrawsVolumeTmp` (inner loop logic vectorized) and `withdrawsVolume` (outer loop logic using `each`) to calculate the bid withdraws factor, mimicking the Python example. It also defines a state function `bidWithdrawsVolume` using the `@state` decorator for the reactive state engine, employing `fixedLengthArrayVector` to structure input arrays and the `moving` function to apply the calculation over a sliding window.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Streaming_computing_of_financial_quantifiers.md#_snippet_18\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 对应内层循环\ndef withdrawsVolumeTmp(lastPrices, lastVolumes, nowPrice, nowVolume){ \n\twithdraws = lastVolumes[lastPrices == nowPrice] - nowVolume\n\treturn sum(withdraws * (withdraws > 0))\n}\n\n// 对应外层循环\ndefg withdrawsVolume(prices, Volumes){ \n\tlastPrices, nowPrices = prices[0], prices[1]\n\tlastVolumes, nowVolumes = Volumes[0], Volumes[1]\n\n\twithdraws = each(withdrawsVolumeTmp{lastPrices, lastVolumes}, nowPrices, nowVolumes)\n\treturn sum(withdraws)\n}\n\n\n@state\ndef bidWithdrawsVolume(bidPrice0, bidPrice1, bidPrice2, bidPrice3, bidPrice4, bidPrice5, bidPrice6, bidPrice7, bidPrice8, bidPrice9,bidOrderQty0, bidOrderQty1, bidOrderQty2, bidOrderQty3, bidOrderQty4, bidOrderQty5, bidOrderQty6, bidOrderQty7, bidOrderQty8, bidOrderQty9, levels=10){\n\tbidPrice = fixedLengthArrayVector(bidPrice0, bidPrice1, bidPrice2, bidPrice3, bidPrice4, bidPrice5, bidPrice6, bidPrice7, bidPrice8, bidPrice9)\n\tbidOrderQty = fixedLengthArrayVector(bidOrderQty0, bidOrderQty1, bidOrderQty2, bidOrderQty3, bidOrderQty4, bidOrderQty5, bidOrderQty6, bidOrderQty7, bidOrderQty8, bidOrderQty9)\n\treturn moving(withdrawsVolume, [bidPrice[0:levels], bidOrderQty[0:levels]], 2)\n}\n```\n\n----------------------------------------\n\nTITLE: Recovering Cluster Metadata and Data Node State After Upgrade Failure (Shell)\nDESCRIPTION: These shell commands are used to restore backed-up DolphinDB metadata and data node chunk metadata after a failed upgrade. The commands copy backup directories into operational locations to revert DolphinDB to a previous working state. Prerequisite: backup/raft, backup/dfsMeta, and dataBackup/CHUNK_METADATA must exist. Inputs: None (run in appropriate directories). Outputs: Old metadata replaces current files. Limitation: Only valid if no new data has been written since upgrade.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ha_cluster_deployment.md#_snippet_18\n\nLANGUAGE: Shell\nCODE:\n```\ncp -r backup/raft ./\n```\n\nLANGUAGE: Shell\nCODE:\n```\ncp -r backup/dfsMeta ./\n```\n\nLANGUAGE: Shell\nCODE:\n```\ncp -r dataBackup/CHUNK_METADATA ./\n```\n\n----------------------------------------\n\nTITLE: Query Registered Stream Computing Engine Status in DolphinDB\nDESCRIPTION: Retrieves general status information about all registered stream computing engines in DolphinDB, useful for system health monitoring and management of streaming engines. Requires DolphinDB environment.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/streaming_capital_flow_daily/04.streamStateQuery.txt#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\ngetStreamEngineStat()\n```\n\n----------------------------------------\n\nTITLE: Calling DolphinDB Function View with Parameters from DolphinScheduler - DolphinDB Script\nDESCRIPTION: This snippet exemplifies how to call a globally registered DolphinDB function with parameters from DolphinScheduler's SQL task node, allowing dynamic passing of parameters such as database and table names. This method supports both local and global parameter passing.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/dolphinscheduler_integration.md#_snippet_9\n\nLANGUAGE: DolphinDB script\nCODE:\n```\ncreateTable(\"dfs://testDb\", \"testTb\");\n```\n\n----------------------------------------\n\nTITLE: Querying with Snapshot Engine (Performance Test)\nDESCRIPTION: Registers the snapshot engine and then queries the latest data. The `timer` keyword measures the execution time of the query. This is used as a baseline for performance comparison.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/snapshot_engine.md#_snippet_7\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nregisterSnapshotEngine(\"dfs://testDB\", \"trainInfoTable\", `trainID)\ntimer select [64] * from loadTable(\"dfs://testDB\", \"trainInfoTable\")\n```\n\n----------------------------------------\n\nTITLE: 实时流数据可视化 Grafana 查询语句 示例\nDESCRIPTION: 提供Grafana面板中用于接收和展示地震波形及时延实时流数据的DolphinDB查询语句示例。波形数据查询通过对流数据取平均并按时间窗口和id分组实现，再与标签信息表联合后做pivot操作以按tagid转置值。时延数据查询则做pivot操作直接展示delay值。依赖Grafana配置连接DolphinDB数据源。输入为流数据流和标签表，输出为可供Grafana可视化的结构化数据。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Seismic_waveform_data_storage.md#_snippet_11\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//波形实时流数据可视化展示\nreal_db_name,dt_name = \"dfs://real\",\"tagInfo\"\nt = select avg(data)  as value from dataStream group by bar(ts,6000) as ts, id\nselect t.value from t left join loadTable(realDbName,dtName) rt on t.id = rt.id  pivot by t.ts,rt.tagid\n\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//时延流数据可视化展示\nselect delay from delayStream pivot by startTime,tagid\n\n```\n\n----------------------------------------\n\nTITLE: SVD Decomposition with fullMatrices=false in DolphinDB\nDESCRIPTION: Demonstrates Singular Value Decomposition (SVD) of a matrix in DolphinDB using the `svd` function with `fullMatrices=false`. The function decomposes a matrix X into U, s, and Vh. This returns U and Vh with reduced dimensions.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/matrix.md#_snippet_23\n\nLANGUAGE: DolphinDB\nCODE:\n```\n>u,s,vh = svd(m, fullMatrices=false); //computeUV=true\n>u;\n#0        #1        #2       \n--------- --------- ---------\n-0.604057 -0.734356 0.309574 \n-0.570062 0.126705  -0.811773\n-0.556906 0.666834  0.495165 \n>s;\n[19.202978,3.644527,1.721349]\n>vh;\n#0        #1        #2        #3      \n--------- --------- --------- --------\n-0.356349 -0.421717 -0.545772 -0.63032\n0.68568   -0.101776 -0.671497 0.261873\n-0.559964 -0.308094 -0.240155 0.730646\n\n```\n\n----------------------------------------\n\nTITLE: 流表订阅与数据落库脚本（DolphinDB 流式处理）\nDESCRIPTION: 订阅信号表，实时将数据传输到聚合引擎；再订阅 srms 表，将处理结果落盘到数据库中；同时订阅 warn 表，存储异常告警信息到独立数据库。采用批量操作和 throttle 控制写入频率。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Random_Vibration_Signal_Analysis_Solution.md#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\nsubscribeTable(tableName=\"signal\", actionName=\"act_tsAggr1\", offset=0, handler=append!{tsAggr1}, msgAsTable=true);\n\ndb = database(\"dfs://rmsDB\", VALUE, 2022.01.01..2022.12.31)\n\nm = table(1:0,`datetime`source`rmsAcc`rmsVel`rmsDis,[TIMESTAMP,SYMBOL,DOUBLE,DOUBLE,DOUBLE]) \ndb.createPartitionedTable(m, \"rms\", [\"datetime\"])\npt_rms = loadTable(\"dfs://rmsDB\", \"rms\")\ndef saveSrmsToDFS(mutable dfsRMS, msg){\n    dfsRMS.append!(select datetime, source, rmsAcc/10000 as rmsAcc, rmsVel/10000 as rmsVel, rmsDis/10000 as rmsDis from msg)\n}\nsubscribeTable(tableName=\"srms\", actionName=\"act_saveDfs1\", offset=-1, handler=saveSrmsToDFS{pt_rms}, msgAsTable=true, batchSize=10000, throttle=1);\n\n//订阅 srms 表，将检测后数据落盘到 warnDB\nsubscribeTable(tableName=\"srms\", actionName=\"act_tsAggr2\", offset=0, handler=append!{tsAggr2}, msgAsTable=true);\n//创建warnrms分布式表，并订阅了落库\ndef saveWarnToDFS(mutable dfswarnrms, msg){\n    dfswarnrms.append!(select datetime, source, type, metric from msg)\n}\nif(existsDatabase(\"dfs://warnDB\")){ dropDatabase(\"dfs://warnDB\")}\n\n\n\n\n```\n\n----------------------------------------\n\nTITLE: Example SQL Query Splitting for Repartitioning Data in DolphinDB (SQL)\nDESCRIPTION: Illustrates splitting a query involving a pivot operation into multiple subqueries based on specified partition column and scheme values. This repartitioning addresses the problem of large data volumes in memory by segmenting data into smaller partitions, each handled separately. Implemented by applying a where clause on repartitionCol equal to each repartitionScheme value.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_AI_DataLoader_for_Deep_Learning.md#_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nselect val from loadTable(dbName, tableName) pivot by datetime, stockID, factorName where date(datetime) = 2023.09.05\nselect val from loadTable(dbName, tableName) pivot by datetime, stockID, factorName where date(datetime) = 2023.09.06\nselect val from loadTable(dbName, tableName) pivot by datetime, stockID, factorName where date(datetime) = 2023.09.07\n```\n\n----------------------------------------\n\nTITLE: Aggregating Data with Time-Series Engine in DolphinDB\nDESCRIPTION: This DolphinDB code creates a time-series engine to aggregate data from the 'dataTable' stream table. It defines a window size and step, specifies the metrics to calculate (average of wind, humidity, air_pressure, temperature, life, and propertyValue), and outputs the aggregated results to the 'aggrTable' stream table. The `createTimeSeriesEngine` function is central to this process.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/knn_iot.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\nenableTableShareAndPersistence(table = streamTable(100000:0,orgColNames,orgColTypes), tableName=`aggrTable, cacheSize = 5000000) \n\ntradesAggregator = createTimeSeriesEngine(name=\"streamAggr\", windowSize=10, step=10, metrics=<[avg(wind),avg(humidity),avg(air_pressure),avg(temperature),avg(life),avg(propertyValue)]>, dummyTable=dataTable, outputTable=aggrTable, timeColumn=`time, useSystemTime=false, keyColumn=`deviceCode, garbageSize=1000000)\n\nsubscribeTable(tableName=\"dataTable\", actionName=\"Aggregator\", offset=0, handler=append!{tradesAggregator}, msgAsTable=true)\n```\n\n----------------------------------------\n\nTITLE: Loading DolphinDB ODBC Plugin - DolphinDB\nDESCRIPTION: Loads the ODBC plugin for DolphinDB, which must be located at the specified ServerPath. This enables the use of ODBC::connect and ODBC::query functions within DolphinDB scripts.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Oracle_to_DolphinDB.md#_snippet_11\n\nLANGUAGE: DolphinDB\nCODE:\n```\nloadPlugin(\"ServerPath/plugins/odbc/PluginODBC.txt\")\n\n```\n\n----------------------------------------\n\nTITLE: Calculating Average Holding Cost (Non-JIT) in DolphinDB Script\nDESCRIPTION: Defines the function `holdingCost_no_JIT` to calculate the average holding cost for a series of trades using a standard DolphinDB script loop. It takes price and amount vectors as input, iterates through them, updates the total cost and holding quantity, and calculates the running average cost. This function serves as a baseline for performance comparison against the JIT version.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/jit.md#_snippet_3\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\ndef holdingCost_no_JIT(price, amount){\n\tholding = 0.0\n\tcost = 0.0\n\tavgPrice = 0.0\n\tn = size(price)\n\tavgPrices = array(DOUBLE, n, n, 0)\n\tfor (i in 0:n){\n\t\tholding += amount[i]\n\t\tif (amount[i] > 0){\n\t\t\tcost += amount[i] * price[i]\n\t\t\tavgPrice = cost/holding\n\t\t}\n\t\telse{\n\t\t\tcost += amount[i] * avgPrice\n\t\t}\n\t    avgPrices[i] = avgPrice\n\t}\n\treturn avgPrices\n}\n```\n\n----------------------------------------\n\nTITLE: Segmented Count Using Custom Range Grouping Function in DolphinDB Script\nDESCRIPTION: Implements a custom function generateGrp which assigns a group label based on which interval a value falls into, using a loop and left-closed, right-open intervals. Reduces all group labels per value before group by to count records per segment. Shows array and reduce usage on vectorized data. Performance is lower than asof-based grouping.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_SQL_Case_Tutorial.md#_snippet_39\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef generateGrp(range, val) {\n\tres = array(ANY, range.size()-1)\n\tfor(i in 0 : (range.size()-1)) {\n\t\tcond = val >= range[i] && val < range[i+1]\n\t\tres[i] = iif(cond, strReplace(range[i] + \"_\" + range[i+1], \"-\", \"\"), string(NULL))\n\t}\n\treturn reduce(add, res)\n}\n\ntimer res1 = select count(*) from t group by date, code, generateGrp(range, value) as grp\n```\n\n----------------------------------------\n\nTITLE: Calculating Number of Start/Stop Events - DolphinDB\nDESCRIPTION: This script calculates the number of start/stop events for a device within a specified time range. It uses the `deltas` function to identify state transitions in the 'propertyValue' column (0 for off, 1 for on). A difference of 1 or -1 indicates a state change. The script requires a table named 'dt' with columns 'ts', 'deviceCode', 'logicalPositionId', 'physicalPositionId', 'propertyValue'.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Iot_intelligent_O&M.md#_snippet_2\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\n//案例 2：根据指定时间段内状态突变次数计算累计开停机次数\nt= select *,deltas(propertyValue) from dt where ts between 2022.01.01 00:00:00 : 2022.01.01 01:59:59 \n//输出\nselect count(*) from t where deltas_propertyValue in (1,-1) group by deviceCode,logicalPositionId,physicalPositionId,bar(ts,1H),deltas_propertyValue\n```\n\n----------------------------------------\n\nTITLE: Define Close Price Factor (Incremental Calculation)\nDESCRIPTION: Defines a function named `Close` that returns the closing price. It utilizes the `last` aggregate function to retrieve the latest trade price (`LastPx`) within a one-minute window. The platform parses the string \"last(LastPx)\" as meta code and passes it to the time-series engine. This example illustrates how to express the incremental calculation of the closing price using the last trade price.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Level2_Snapshot_Factor_Calculation.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef Close(){\n\treturn \"last(LastPx)\"\n}\n```\n\n----------------------------------------\n\nTITLE: Invoking Entrust Data Import Task Function in DolphinScheduler Task Node - DolphinDB Script\nDESCRIPTION: These snippets illustrate the SQL commands used in DolphinScheduler task nodes to invoke the previously defined ETL function view for entrust data import. For the daily scheduled task, the function is called without parameters relying on defaults, while for batch historical tasks, explicit start date, end date, and 'batch' loadType parameters are passed using DolphinScheduler's local or global parameter syntax. This facilitates automated and parameterized task execution.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/dolphinscheduler_integration.md#_snippet_19\n\nLANGUAGE: DolphinDB\nCODE:\n```\nloadEntrustFV();\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nloadEntrustFV(startDate=${startDate},endDate=${endDate},loadType=\"batch\");\n```\n\n----------------------------------------\n\nTITLE: Configuring ODBC Driver Settings in odbcinst.ini - Configuration\nDESCRIPTION: Configuration snippet for odbcinst.ini specifying the SQL Server ODBC driver setup. Defines driver path, description, setup executable, and file usage flags needed by unixODBC to use FreeTDS driver to connect to SQL Server. Separate snippets provided for Ubuntu and CentOS.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/SQLServer_to_DolphinDB.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n[SQLServer]\nDescription = ODBC for SQLServer\nDriver = /usr/lib/x86_64-linux-gnu/odbc/libtdsodbc.so\nSetup = /usr/lib/x86_64-linux-gnu/odbc/libtdsodbc.so\nFileUsage = 1\n```\n\n----------------------------------------\n\nTITLE: Defining Input and Result Tables with Schema for Processing Results\nDESCRIPTION: Creates empty input tables with specified schemas to hold incoming data and processing results, including various result tables for storing computed metrics like stock factors. These tables are foundational for reactive engine operations.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Streaming_computing_of_financial_quantifiers.md#_snippet_25\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// Define input table schemas\ninputTable = table(1:0, data.schema().colDefs.name, data.schema().colDefs.typeString)\ninputTableArrayVector = table(1:0, dataArrayVector.schema().colDefs.name, dataArrayVector.schema().colDefs.typeString)\n// Define result tables for storing output metrics\nresultTable1 = table(10000:0, [\"securityID\", \"dateTime\", \"factor\"], [SYMBOL, TIMESTAMP, DOUBLE])\nresultTable2 = table(10000:0, [\"securityID\", \"dateTime\", \"factor\"], [SYMBOL, TIMESTAMP, DOUBLE])\nresultTable3 = table(10000:0, [\"securityID\", \"dateTime\", \"factor\"], [SYMBOL, TIMESTAMP, DOUBLE])\nresultTable4 = table(10000:0, [\"securityID\", \"dateTime\", \"factor\"], [SYMBOL, TIMESTAMP, DOUBLE])\n```\n\n----------------------------------------\n\nTITLE: Loading Distributed Quotes Table and Calculating Average Bid-Ask Spread per Minute in DolphinDB Script\nDESCRIPTION: This snippet loads the distributed database and the quotes table into memory enabling SQL-style query operations on Level 2 quotes data. It calculates the per-minute maximum bid-ask spread, defined as a normalized difference between askPrice1 and bidPrice1, for a specific stock symbol (`600600) on a single trading day (2020-06-01) and within defined trading hours. The computed average spread per minute is then visualized using a plot function. The snippet relies on the distributed quotes table having correctly typed price columns. Inputs include filtering parameters date, symbol, time bounds. Output is a visualization of average bid-ask spread per minute on the GUI.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/quant_finance_examples.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndb = database(\"dfs://level2\")\nquotes = loadTable(db, `quotes);\n\navgSpread = select max((askPrice1-bidPrice1)/(askPrice1+bidPrice1)*2) as avgSpread from quotes where date=2020.06.01, symbol=`600600, time between 09:30:00.000 : 15:00:00.000, askPrice1>=bidPrice1 group by minute(time) as minute\nplot(avgSpread.avgSpread, avgSpread.minute, \"Average bid-ask spread per minute\")\n```\n\n----------------------------------------\n\nTITLE: Replacing License Windows\nDESCRIPTION: This snippet shows the path for replacing the license file on Windows. It provides the full file path for the `dolphindb.lic` file to be replaced within the server directory. This applies to updating a trial or paid enterprise license.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/standalone_server.md#_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\nC:\\DolphinDB\\server\\dolphindb.lic\n```\n\n----------------------------------------\n\nTITLE: Add New Factor in Wide Table (DolphinDB)\nDESCRIPTION: This function adds a new factor to a wide table. It first adds a new column using `addColumn`, and then updates the column for each time partition and symbol using `wideModelSinglePartitionUpdate` (not defined here). It can perform the update either sequentially or in parallel using `ploop`. Dependencies: `getTimeList`, `addColumn`, `loadTable`, `wideModelSinglePartitionUpdate`, and `ploop`.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practices_for_multi_factor.md#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//宽表模型新增一个因子\ndef wideModelAddNewFactor(dbname,tbname,start_date,end_date,symbols,new_factor,parallel = true){   //parallel=true表示并行,=false表示串行\n\tpt = loadTable(dbname,tbname)\n\taddColumn(pt,[new_factor],[DOUBLE])\n\ttime_list = getTimeList(start_date,end_date)\n\tstart_time_list,end_time_list = [],[] \n\tfor(i in 0..(time_list.size()-1)){\n\t\tstart_time_list.append!(time_list[i][0])\n\t\tidx = time_list[i].size()-1\n\t\tend_time_list.append!(time_list[i][idx])\n\t}\n\tif(!parallel){\n\t\tfor(i in 0..(start_time_list.size()-1)){\n\t\t\tfor(j in 0..(symbols.size()-1)){\n\t\t\t\twideModelSinglePartitionUpdate(dbname,tbname,start_time_list[i],end_time_list[i],new_factor,symbols[j])\n\t\t\t}\n\t\t}\n\t}else{\n\t\tfor(i in 0..(start_time_list.size()-1)){\n\t\t\tploop(wideModelSinglePartitionUpdate{dbname,tbname,start_time_list[i],end_time_list[i],new_factor,},symbols)\n\t\t}\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: Calculating Volume-Based Time Windows\nDESCRIPTION: This snippet calculates volume-based time windows. It uses the `accumulate` function with the custom `caclCumVol` function to iteratively calculate cumulative volumes and determine the start time of each window.  Then, it groups by the start time and aggregates the data, calculating the total volume and end time for each window.  The `iif` and `ffill` functions are used to determine the start time and fill in NULL values.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_sql_cases.md#_snippet_33\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntimer result = select first(wind_code) as wind_code, first(date) as date, sum(volume) as sum_volume, last(time) as endTime \n\t\t\t   from t \n\t\t\t   group by iif(accumulate(caclCumVol{1500000}, volume) == volume, time, NULL).ffill() as startTime\n```\n\n----------------------------------------\n\nTITLE: Creating a Heterogeneous Stream Table in Python\nDESCRIPTION: This script creates a shared, asynchronous, persistent, heterogeneous stream table named `messageStream` for receiving replayed data. It defines the table structure with columns `msgTime`, `msgType`, and `msgBody`, representing the timestamp, source type (e.g., 'order', 'trade', 'snapshot'), and the data content, respectively.  The table is configured for sharing, persistence with a cache size, and data compression.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/stock_market_replay.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ncolName = `msgTime`msgType`msgBody\ncolType = [TIMESTAMP,SYMBOL, BLOB]\nmessageTemp = streamTable(1000000:0, colName, colType)\nenableTableShareAndPersistence(table=messageTemp, tableName=\"messageStream\", asynWrite=true, compress=true, cacheSize=1000000, retentionMinutes=1440, flushMode=0, preCache=10000)\nmessageTemp = NULL\n```\n\n----------------------------------------\n\nTITLE: DolphinDB服务器监控指标计算公式\nDESCRIPTION: 这组计算公式用于基于DolphinDB Server数据源的第二套监控方案，包含CPU使用率、内存占用、磁盘空间与IO、网络IO以及高可用集群性能与状态等指标的计算方法。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/cluster_monitor.md#_snippet_9\n\nLANGUAGE: PromQL\nCODE:\n```\n#--------------------CPU-----------------------------#\n# CPU 使用率(%)\ncpuUsage\n\n#--------------------内存-----------------------------#\n#节点内存占用(MB)\nmemoryUsed/1024/1024\n\n#--------------------磁盘-----------------------------#\n#节点所在磁盘空间使用率(%)\n(1-diskFreeSpace/diskCapacity)*100\n\n#节点所在磁盘读速率(MB/s)\ndiskReadRate/1024/1024\n\n#节点所在磁盘写速率(MB/s)\ndiskWriteRate/1024/1024\n\n#--------------------网络-----------------------------#\n#节点网络发送速率(MB/s)\nnetworkSendRate/1024/1024\n\n#节点网络接收速率(MB/s)\nnetworkRecvRate/1024、1024\n\n#--------------------高可用集群性能-----------------------------#\n#节点前10个完成的查询执行所耗费时间的中间值（ms）\nmaxLast10QueryTime/1000000\n\n#节点当前正在执行的查询的最大耗时(ms)\nmaxRunningQueryTime/1000000\n\n#节点正在执行 Job 数\nrunningJobs\n\n#节点等待执行 Job 数\nqueuedJobs\n\n#节点 CPU 平均负载(%)\navgLoad\n\n#节点作业负载(%)\njobLoad\n\n#--------------------高可用集群状态-----------------------------#\n#节点与 Prometheus 的连接状态\nup{=~\"[^0-9].*?[0-9]}\n```\n\n----------------------------------------\n\nTITLE: Calculating Level 10 Bid Amount Difference - Python\nDESCRIPTION: Defines a function `level10Diff` that calculates a custom 'amount difference' (`amtDiff`) metric for stock snapshot data. It processes a pandas DataFrame containing bid price and quantity information for 10 levels, calculates the weighted difference in total bid amount within a price range defined by the minimum/maximum of current and previous bid prices, and applies a rolling sum over a specified lag window. Requires columns like 'TradeTime', 'SecurityID', 'BidPrice1-10', and 'BidOrderQty1-10'.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/DolphinDB_Python_Parser_Intro_for_Quantitative_Finance/因子实现_Python版本/十档委买增额.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef level10Diff(df, lag=20):\n    temp = df[[\"TradeTime\", \"SecurityID\"]]\n\n    for i in range(10):\n        index = str(i+1)\n        temp[\"bid\"+index] = df[\"BidPrice\"+index].fillna(0)\n        temp[\"bidAmt\"+index] = df[\"BidOrderQty\"+index].fillna(0) * df[\"BidPrice\"+index].fillna(0)\n        temp[\"prevbid\"+index] = temp[\"bid\"+index].shift(1).fillna(0)\n        temp[\"prevbidAmt\"+index] = temp[\"bidAmt\"+index].shift(1).fillna(0)\n\n    temp[\"bidMin\"] = temp[[\"bid\"+str(i+1) for i in range(10)]].min(axis=1)\n    temp[\"bidMax\"] = temp[[\"bid\"+str(i+1) for i in range(10)]].max(axis=1)\n    temp[\"prevbidMin\"] = temp[[\"prevbid\"+str(i+1) for i in range(10)]].min(axis=1)\n    temp[\"prevbidMax\"] = temp[[\"prevbid\"+str(i+1) for i in range(10)]].max(axis=1)\n    temp[\"pmin\"] = temp[[\"bidMin\", \"prevbidMin\"]].max(axis=1)\n    temp[\"pmax\"] = temp[[\"bidMax\", \"prevbidMax\"]].max(axis=1)\n\n    temp[\"amtDiff\"] = 0.0\n    for i in range(10):\n        index = str(i+1)\n        temp[\"amtDiff\"] += temp[\"bidAmt\"+index]*((temp[\"bid\"+index] >= temp[\"pmin\"])&(temp[\"bid\"+index] <= temp[\"pmax\"])).astype(int) - \\\n                        temp[\"prevbidAmt\"+index]*((temp[\"prevbid\"+index] >= temp[\"pmin\"])&(temp[\"prevbid\"+index] <= temp[\"pmax\"])).astype(int)\n    temp[\"amtDiff\"] = temp[\"amtDiff\"].rolling(lag, 1).sum()\n    return temp[[\"TradeTime\", \"SecurityID\", \"amtDiff\"]].fillna(0)\n```\n\n----------------------------------------\n\nTITLE: Calculating Cumulative On/Off Time - DolphinDB\nDESCRIPTION: This code calculates the cumulative on/off time for a device based on its state. It uses the `deltas` function to calculate the time difference between consecutive readings, then groups the data by hour and device state to sum the durations. It requires a table named 'pt' with columns 'ts', 'deviceCode', 'logicalPositionId', 'physicalPositionId', 'propertyValue'.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Iot_intelligent_O&M.md#_snippet_1\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\n//案例 1：根据状态指标计算设备累计开停机时长\ndt=select * from pt where deviceCode=\"361RP88\" and propertyCode=\"361RP88002\"\n//处理累计时长 \nt = select *,deltas(ts) as duration from dt\n//输出（分钟）\nselect sum(duration)/60 as duration from t group by bar(ts,1H) as bar_ts,deviceCode,logicalPositionId,physicalPositionId,propertyValue\n```\n\n----------------------------------------\n\nTITLE: Testing MySQL ODBC Connection with isql (Alternative)\nDESCRIPTION: This command attempts to connect to a MySQL database using `isql`. The connection fails with a different error code (110), suggesting a potential network configuration issue or DNS resolution problem that prevents reaching the specified IP address.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ODBC_plugin_user_guide.md#_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nisql -v -k \"Driver=MySQL ODBC 8.0 Unicode Driver;Server=192.168.2.38;Port=3306;Uid=root;Pwd=123456;Database=Test\"\n[S1000][unixODBC][MySQL][ODBC 8.0(w) Driver]Can't connect to MySQL server on '192.168.2.38' (110)\n[ISQL]ERROR: Could not SQLDriverConnect\n```\n\n----------------------------------------\n\nTITLE: Validation of Consistency Across Computed Result Tables\nDESCRIPTION: Performs assertions to ensure all result tables produce the same 'factor' values, verifying the accuracy and consistency of the different reactive processing methods.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Streaming_computing_of_financial_quantifiers.md#_snippet_31\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// Validate that all result tables have matching 'factor' values\nassert each(eqObj, resultTable1.factor, resultTable2.factor).all()\nassert each(eqObj, resultTable1.factor, resultTable3.factor).all()\nassert each(eqObj, resultTable1.factor, resultTable4.factor).all()\n```\n\n----------------------------------------\n\nTITLE: 使用Gurobi插件求解最大化期望收益率问题\nDESCRIPTION: 通过Gurobi插件解决最大化期望收益率的投资组合优化问题。设置了相同的行业暴露限制和单个股票权重上限约束，使用DolphinDB的Gurobi插件进行求解。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/MVO.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\nloadPlugin(\"plugins/gurobi/PluginGurobi.txt\")\nmodel = gurobi::model()\n\n/// 增加变量\nnumOfVars = 10\nlb = take(0, numOfVars)\nub = take(0.15, numOfVars)\nstocks = [\"A001\", \"A002\", \"A003\", \"A004\", \"A005\", \"A006\", \"A007\", \"A008\", \"A009\", \"A010\"]\nvars = gurobi::addVars(model, lb, ub, , , stocks)\n\n/// 增加线性约束\nf = [0.1,0.02,0.01,0.05,0.17,0.01,0.07,0.08,0.09,0.10]\nA = ([1,1,1,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,1,1,1,1,-1,-1,-1,0,0,0,0,0,0,0,0,0,0,-1,-1,-1,0,0,0,0,0,0,0,0,0,0,-1,-1,-1,-1]$10:6)\nrhs = 0.38 0.48 0.38 -0.22 -0.32 -0.22\n\nfor (i in 0:6) {\n  lhsExpr = gurobi::linExpr(model, A[i], stocks)\n  gurobi::addConstr(model, lhsExpr, '<', rhs[i])\n}\n\nlhsExpr = gurobi::linExpr(model, take(1, numOfVars), stocks)\ngurobi::addConstr(model, lhsExpr, '=', 1)\n\n// 设置目标函数\nobj = gurobi::linExpr(model, f, vars)\ngurobi::setObjective(model, obj, -1)\n\n// 执行优化\ngurobi::optimize(model) \n\n// 获取优化结果\nresult = gurobi::getResult(model)\n//股票权重：result = [0.15,0.15,0,0.15,0.15,0.02,0,0.08,0.15,0.15]\nobj = gurobi::getObjective(model)\n//目标值：obj = 0.0861\n```\n\n----------------------------------------\n\nTITLE: Calculating Log Return\nDESCRIPTION: Defines a simple function `logReturn` to calculate the logarithmic return of a time series `s`. It computes the difference between the natural logarithm of the current value and the previous value.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/metacode_derived_features/metacode_derived_features.txt#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef logReturn(s){\n\treturn log(s)-log(prev(s))\n}\n```\n\n----------------------------------------\n\nTITLE: 使用quadprog函数求解最大化效用函数问题\nDESCRIPTION: 通过二次规划解决最大化效用函数的投资组合优化问题。使用DolphinDB的quadprog函数求解，目标是在风险与收益之间取得平衡，利用大型股票组合(2457支股票)和单位协方差矩阵。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/MVO.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\nbaseDir = \"/home/data/\"\nf = dropna(flatten(float(matrix(select col1, col2, col3, col4, col5, col6, col7, col8, col9, col10 from loadText(baseDir + \"C.csv\") where rowNo(col1) > 0)).transpose()))\n\nN = f.size()\nH = eye(N) //协方差矩阵\nA = matrix(select * from loadText(baseDir + \"A_ub.csv\"))\nb = \n[0.025876723,\t0.092515275,\t0.035133942,\t0.053184884,\t0.067410565,\t0.009709433,\t0.04668745,\t0.00636804,\t0.022258664,\t0.11027537,\n0.018488302,\t0.027417204,\t0.028585,\t0.017228214,\t0.008055527,\t0.015727843,\t0.026132369,\t0.013646113,\t0.066000808,\t0.043606587,\n0.048325258,\t0.033868626,\t0.010790603,\t0.017737391,\t0.03252374,\t0.039329965,\t0.040665779,\t0.010868773,\t0.006819891,\t0.015879314,\n0.008882335,\t-0.025876723,\t-0.092515275,\t-0.035133942,\t-0.053184884,\t-0.067410565,\t-0.009709433,\t-0.04668745,\t-0.00636804,\t-0.022258664,\n-0.110275379,\t-0.018488302,\t-0.027417204,\t-0.028585,\t-0.017228214,\t-0.008055527,\t-0.015727843,\t-0.026132369,\t-0.013646113,\t-0.066000808,\n-0.043606587,\t-0.048325258,\t-0.033868626,\t-0.010790603,\t-0.017737391,\t-0.03252374,\t-0.039329965,\t-0.040665779,\t-0.010868773,\t-0.006819891,\n-0.015879314,\t-0.008882335]\n\n//线性约束\nAeq = matrix(take(1, N)).transpose()\nbeq = array(DOUBLE).append!(1)\nres2 = quadprog(H, f, A, b, Aeq, beq)\n```\n\n----------------------------------------\n\nTITLE: Calculating R/S Statistic (Partial Hurst) in DolphinDB\nDESCRIPTION: Defines a DolphinDB function `calAllRs2` which appears to be a helper for calculating the Hurst Exponent. It calculates the log of the average Rescaled Range (R/S) statistic for a *specific* time scale `k` across multiple return series (`mret` indexed by `symList`). This is one step in the R/S analysis needed for the Hurst Exponent.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/fund_factor_contrasted_by_py.md#_snippet_18\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef calAllRs2(mret, symList, k){\n        rowCount = mret.rows()/k * k\n        demeanCum = rolling(cumsum, mret[0:rowCount,] - each(stretch{, rowCount}, rolling(avg, mret, k, k)), k, k)\n        a = rolling(max, demeanCum, k, k) - rolling(min, demeanCum, k, k)\n        RS = nullFill!(a/rolling(stdp, mret, k, k), 1.0).mean().log()\n        return table(symList as fundNum, take(log(k), symList.size()) as knum, RS as factor1)\n}\n```\n\n----------------------------------------\n\nTITLE: Transforming matrix data to long-format tables in DolphinDB\nDESCRIPTION: Processes both NAV and HS300 data by converting wide-format CSV data into long-format tables using unpivot operations. The resulting tables have tradingdate, fundNum, and value columns suitable for time-series analysis.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_data_load.txt#_snippet_3\n\nLANGUAGE: dolphindb\nCODE:\n```\nallSymbols = readColumnsFromWideCSV(csvPath)$STRING\ndataMatrix = readIndexedMatrixFromWideCSV(csvPath)\nfundTable = table(dataMatrix.rowNames() as tradingDate, dataMatrix)\nresult = fundTable.unpivot(`tradingdate, allSymbols).rename!(`tradingdate`fundNum`value)\nallSymbols1 = readColumnsFromWideCSV(csvPath1)$STRING\ndataMatrix1 = readIndexedMatrixFromWideCSV(csvPath1)\nfundTable1 = table(dataMatrix1.rowNames() as tradingDate, dataMatrix1)\nresult1 = fundTable1.unpivot(`tradingdate, allSymbols1).rename!(`tradingdate`fundNum`value)\nresult1\n```\n\n----------------------------------------\n\nTITLE: 通过会话窗口引擎检测门禁状态超时数据 - DolphinDB\nDESCRIPTION: 创建会话窗口引擎用于检测门禁状态连续超时情况，设计会话窗口的时间间隙(sessionGap)为300秒，表示若在300秒内无新数据则终止当前会话窗口。设置分组键为门号doorNum，时间列为eventDate，指标为该窗口最后一次事件码，利用createSessionWindowEngine函数关联输入输出，其中输入为响应式状态引擎输出，输出作为下级引擎输入，保证多级级联监测流程。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_engine_anomaly_alerts.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\nswOut2 = table(1:0,`doorNum`eventDate`doorEventCode,[INT,DATETIME,INT])\nswEngine = createSessionWindowEngine(name=\"swEngine\",sessionGap = 300,metrics=<last(doorEventCode)>,\n    dummyTable = objByName(`doorRecord), outputTable = getStreamEngine(\"reactivEngine\"), \n    timeColumn = `eventDate, keyColumn =`doorNum, useSessionStartTime = false)\n```\n\n----------------------------------------\n\nTITLE: Defining Feature Aggregation Dictionary (DolphinDB Script)\nDESCRIPTION: Defines a dictionary `features` specifying which aggregation functions (`sum`, `mean`, `std`, `realizedVolatility`, `count`) should be applied to which input columns (`DateTime`, `WapX`, `LogReturnX`, `WapBalance`, etc.) during the feature engineering process.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/metacode_derived_features/metacode_derived_features_streaming.txt#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\nfeatures = {\n\t\"DateTime\":[`count]\n}\nfor( i in 0..9)\n{\n\tfeatures[\"Wap\"+i] = [`sum, `mean, `std]\n\tfeatures[\"LogReturn\"+i] = [`sum, `realizedVolatility, `mean, `std]\n\tfeatures[\"LogReturnOffer\"+i] = [`sum, `realizedVolatility, `mean, `std]\n\tfeatures[\"LogReturnBid\"+i] = [`sum, `realizedVolatility, `mean, `std]\n}\nfeatures[\"WapBalance\"] = [`sum, `mean, `std]\nfeatures[\"PriceSpread\"] = [`sum, `mean, `std]\nfeatures[\"BidSpread\"] = [`sum, `mean, `std]\nfeatures[\"OfferSpread\"] = [`sum, `mean, `std]\nfeatures[\"TotalVolume\"] = [`sum, `mean, `std]\nfeatures[\"VolumeImbalance\"] = [`sum, `mean, `std]\n```\n\n----------------------------------------\n\nTITLE: Loading and Filtering Trading Dates from CSV (DolphinDB Script)\nDESCRIPTION: Loads trading dates from a specified CSV file path (`tradingDatesAbsoluteFilename`) using the `loadText` function into a table `allTradingDates`. It then filters this table using an `exec` statement to create a date vector `tradingDates` containing only the dates between `startDate` (2015.02.01) and `endDate` (2022.03.01).\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/IV_Greeks_Calculation_for_ETF_Options_Using_JIT.md#_snippet_6\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\n//交易日历csv文件路径\ntradingDatesAbsoluteFilename = \"/hdd/hdd9/tutorials/jitAccelerated/tradedate.csv\"\nstartDate = 2015.02.01\nendDate = 2022.03.01\n//读取csv文件\nallTradingDates = loadText(tradingDatesAbsoluteFilename)\n//生成交易日向量\ntradingDates = exec tradedate from allTradingDates where tradedate<endDate and tradedate >startDate\n```\n\n----------------------------------------\n\nTITLE: Calculating Indicators with Different Aggregation Methods\nDESCRIPTION: This snippet calculates indicators using different aggregation methods based on tags.  It defines a dictionary `codes` mapping tags to aggregation functions (max, min, avg). A custom function `func` applies the appropriate aggregation function based on the tag. The `group by` clause groups data by tag and 10-minute intervals, calling the custom function. Finally, the `pivot by` clause transforms the data into a wide table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_sql_cases.md#_snippet_23\n\nLANGUAGE: DolphinDB\nCODE:\n```\ncodes = dict(`code1`code2`code3, [max, min, avg])\n\ndefg func(tag, value, codes) : codes[tag.first()](value)\n \ntimer {\n\tt_tmp = select func(tag, value, codes) as value from t \n\t\t\tgroup by tag, interval(time, 10m, \"null\") as time\n\tt_result = select value from t_tmp pivot by time, tag\n}\n```\n\n----------------------------------------\n\nTITLE: Calculating TopN Correlation Using mcorrTopN Function - DolphinDB Script\nDESCRIPTION: Uses mcorrTopN to calculate the correlation between 'low' and a compound factor (close*volume, log(ratios(close))) for each stock within a rolling 5-record window, keeping only the top 3 low values. Employs 'context by' on windCode and trading date. Input columns: low, close, volume; Output: rolling correlation metric for advanced signal analysis.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_TopN.md#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect windCode, tradingTime, mcorrTopN(low, close * volume,  log(ratios(close)), 5, 3, false) as mcorrTop3CloseVol from t context by windCode, date(tradingTime)\n```\n\n----------------------------------------\n\nTITLE: Creating Sample Orders Table - DolphinDB\nDESCRIPTION: Creates a sample table `orders` containing stock information (SecID), Value, and Vol. This table is used for demonstration purposes in subsequent examples.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/func_progr_cases.md#_snippet_17\n\nLANGUAGE: DolphinDB\nCODE:\n```\norders = table(`IBM`IBM`IBM`GOOG as SecID, 1 2 3 4 as Value, 4 5 6 7 as Vol)\n```\n\n----------------------------------------\n\nTITLE: Define Time Series Aggregator - DolphinDB\nDESCRIPTION: Defines a time series aggregator named `tsAggr` to calculate OHLC data from the `Trade` stream table. It specifies a window size of 60 seconds, a step size of 60 seconds, and uses `updateTime=1` to trigger calculations more frequently than the window end. The output is appended to the `OHLC` stream table. The `useWindowStartTime=true` is used to make the first column of output table as data window start time.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/OHLC.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntsAggr = createTimeSeriesAggregator(name=\"tsAggr\", windowSize=60, step=60, metrics=<[first(Price),max(Price),min(Price),last(Price),sum(volume)]>, dummyTable=Trade, outputTable=OHLC, timeColumn=`Datetime, keyColumn=`Symbol, updateTime=1, useWindowStartTime=true)\n```\n\n----------------------------------------\n\nTITLE: Loading and Querying DolphinDB Partitioned Table\nDESCRIPTION: Loads the partitioned table \"orderBook\" from the \"dfs://stocks_orderbook\" database into the variable `orderBook`. It then executes two SQL queries: one to retrieve the first 10 rows from the loaded table variable and another to count the total number of rows in the table by reloading it directly within the query.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/csvImportDemo/createDB.txt#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\norderBook=loadTable(\"dfs://stocks_orderbook\",`orderBook)\nselect top 10 * from orderBook\nselect count(*) from loadTable(\"dfs://stocks_orderbook\",`orderBook)\n```\n\n----------------------------------------\n\nTITLE: Creating Distributed Table in DolphinDB\nDESCRIPTION: This code block creates a distributed table in DolphinDB based on Redshift data. It first defines the database using the `database` function, specifying a hash partitioning scheme on the INT column. Then, the code defines the table schema `dt`, including data types for each column. Finally it creates a partitioned table with the specified database, table schema, and partition column.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/Migrate_data_from_Redshift_to_DolphinDB/Redshift2DDB.txt#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//在DolphinDB中根据RedShift的数据创建分布式表\ndb=database(\"dfs://redshift\",HASH,[INT,10]);\ndt=table(300000000:0,[\"channelno\",\"applseqnum\",\"mdstreamid\",\"bidapplseqnum\",\"offerapplseqnum\",\"securityid\",\"securityidsource\",\"lastpx\",\"lastqty\",\"exectype\",\"transacttime\",\"localtime\",\"seqno\",\"transactiondate\"],[INT,INT,SYMBOL,INT,INT,SYMBOL,SYMBOL,DOUBLE,DOUBLE,INT,TIME,TIME,INT,DATE]);\npt=createPartitionedTable(db,dt,\"dfstable\",\"channelno\");\n```\n\n----------------------------------------\n\nTITLE: Counting Persisted Snapshot Data Entries in DolphinDB\nDESCRIPTION: This snippet executes a count aggregation query to determine the total number of records stored in a DolphinDB persistent database table (\"snapshot\" in dfs://snapshot). It helps confirm whether data ingested during replay has been successfully persisted without loss, providing a quantitative validation of data integrity.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_auto_sub.md#_snippet_9\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect count(*) from loadTable(\"dfs://snapshot\", \"snapshot\")\n```\n\n----------------------------------------\n\nTITLE: Parallel Execution on Multiple Data Nodes with pnodeRun in DolphinDB - DolphinDB\nDESCRIPTION: Executes the 'getRecentJobs' and 'getCompletedQueries' functions on one or multiple data nodes concurrently using 'pnodeRun', collecting and merging the results. This is useful for cluster management and data monitoring. Also demonstrates cache clearing commands run concurrently on all data nodes with 'clearAllCache'. This functionality supports distributed parallel administration and task querying in a cluster environment. Inputs specify number of queried entries or node lists where applicable.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/hybrid_programming_paradigms.md#_snippet_17\n\nLANGUAGE: DolphinDB\nCODE:\n```\npnodeRun(getRecentJobs{10});\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\npnodeRun(getCompletedQueries{10}, `nodeA`nodeB);\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\npnodeRun(clearAllCache);\n```\n\n----------------------------------------\n\nTITLE: Create sample table with NULL values\nDESCRIPTION: This DolphinDB script creates a sample table `t` with NULL values. It uses `take` and `join` to generate data for the columns `sym`, `id`, and `id2`, including some NULL values in the `id` and `id2` columns.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\nsym = take(`a`b`c, 110)\nid = 1..100 join take(int(),10)\nid2 =  take(int(),10) join 1..100\nt = table(sym, id,id2)\n```\n\n----------------------------------------\n\nTITLE: Starting DolphinDB Agent Nodes Using Shell Script\nDESCRIPTION: This command starts the agent nodes across all cluster servers (P1, P2, and P3) by running the startagent.sh script. Agent nodes act as intermediaries facilitating communication between control and data/compute nodes. The command must be run on each server hosting an agent node.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/multi_machine_cluster_deployment.md#_snippet_6\n\nLANGUAGE: Shell\nCODE:\n```\nsh startagent.sh\n```\n\n----------------------------------------\n\nTITLE: Registering Time-Series Engine and Subscribing in DolphinDB Script\nDESCRIPTION: Creates a time-series streaming engine named `aggrFeatures10min` using the `featureEngine` function to calculate metrics over a 10-minute sliding window (windowSize=600000ms) with a 1-minute step (step=60000ms). It then subscribes this engine to the `snapshotStream` table to process incoming raw data and output results to the `aggrFeatures10min` stream table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/machine_learning_volatility/05.streamComputingArrayVector.txt#_snippet_4\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nmetrics=<featureEngine(BidPrice,BidOrderQty,OfferPrice,OfferOrderQty) as `BAS`DI0`DI1`DI2`DI3`DI4`DI5`DI6`DI7`DI8`DI9`Press`RV>\n//register stream computing engine\ncreateTimeSeriesEngine(name=\"aggrFeatures10min\", windowSize=600000, step=60000, metrics=metrics, dummyTable=snapshotStream, outputTable=aggrFeatures10min, timeColumn=`TradeTime, useWindowStartTime=true, keyColumn=`SecurityID)\n//subscribe data\nsubscribeTable(tableName=\"snapshotStream\", actionName=\"aggrFeatures10min\", offset=-1, handler=getStreamEngine(\"aggrFeatures10min\"), msgAsTable=true, batchSize=2000, throttle=1, hash=0, reconnect=true)\n```\n\n----------------------------------------\n\nTITLE: Analyzing Data Loss in DolphinDB\nDESCRIPTION: Loads data from a CSV file and identifies instances where the time difference between consecutive sensor readings for the same tag exceeds 30 seconds. This helps verify that the session window engine correctly detects data loss.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_streaming_application_in_IOT.md#_snippet_9\n\nLANGUAGE: DolphinDB\nCODE:\n```\n\tt=loadText(getHomeDir()+\"/deviceState.csv\")\n\tselect tag,ts,prev(ts),deltas(ts) from t context by tag  having deltas(ts)>30000\n```\n\n----------------------------------------\n\nTITLE: Publishing IOPV Results to External Systems with ZMQ in DolphinDB\nDESCRIPTION: Sets up a ZMQ publisher to stream the calculated IOPV values to downstream systems for consumption by trading strategies or other applications.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_IOPV.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\ntry{\n    formatter = zmq::createJSONFormatter()\n    socket = zmq::socket(\"ZMQ_PUB\", formatter)\n    zmq::bind(socket, \"tcp://*:20414\")\n}catch(ex){}\n\nsubscribeTable(tableName=\"IOPVStreamResult\", actionName=\"IOPV_mq_read\", offset=0, handler=zmq::send{socket}, msgAsTable=true)\n```\n\n----------------------------------------\n\nTITLE: Publishing Messages to Kafka Topic - DolphinDB Script\nDESCRIPTION: Defines a function to send data messages to a Kafka topic via the initialized producer. Implements error handling and logging for successful or failed sends. Depends on the Kafka plugin and producer; accepts data type string, producer, and message payload (likely a table or array), returns none, and writes operational logs to the system.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/stock_market_replay/04.publishToKafka.txt#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef sendMsgToKafkaFunc(dataType, producer, msg){\n\tstartTime = now()\n\ttry {\n\t\tkafka::produce(producer, \"topic-message\", 1, msg, true) \n\t\tcost = now() - startTime\n\t\twriteLog(\"[Kafka Plugin] Successed to send \" + dataType + \" : \" + msg.size() + \" rows, \" + cost + \" ms.\")\n\t} \n\tcatch(ex) {writeLog(\"[Kafka Plugin] Failed to send msg to kafka with error: \" +ex)}\n}\n```\n\n----------------------------------------\n\nTITLE: Calculating VaR in DolphinDB\nDESCRIPTION: This function calculates the Value at Risk (VaR) of a given value series at a 95% confidence level. It calculates the 5th percentile of the percentage changes in the value series and returns its negative value.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_factor.txt#_snippet_8\n\nLANGUAGE: DolphinDB\nCODE:\n```\n/**\n * 因子8：var\n * var 置信度为0.95\n */\ndefg getVar(value){\n\treturn -percentile(deltas(value)\\prev(value), 5)\n}\n```\n\n----------------------------------------\n\nTITLE: Executing DataX Job via Linux Shell Command\nDESCRIPTION: This snippet demonstrates how to run the DataX synchronization job using shell commands in a Linux terminal. It shows navigating into the DataX binary directory and executing the Python-based DataX script with the path to the JSON configuration file as an argument. The snippet includes example output metrics such as start/end time, total duration, throughput, write speed, total records read, and error counts. This approach assumes Python and DataX dependencies are installed and accessible in the environment.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/SQLServer_to_DolphinDB.md#_snippet_11\n\nLANGUAGE: Shell\nCODE:\n```\ncd ./DataX/bin/\npython DataX.py ../job/synchronization.json\n```\n\n----------------------------------------\n\nTITLE: Time Single Record 2 Column Update\nDESCRIPTION: This is DolphinDB script to measure the execution time of the update operation on 2 columns for a single record. The `timer` function is used to measure the execution time of the statement. It provides a basis for comparing update performance across different configurations.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/dolphindb_update.md#_snippet_13\n\nLANGUAGE: DolphinDB\nCODE:\n```\nmachines = loadTable(\"dfs://olapDemo\", \"machines\")\ntimer update machines set tag1=1, tag5=5 where id=1 and datetime=2020.09.01T00:00:00\n```\n\n----------------------------------------\n\nTITLE: Loading DolphinDB Module as Built-in with loadModule\nDESCRIPTION: Uses the `loadModule` function to load the specified module (fileLog) into the system's built-in function set. This makes the module's functions available globally to all sessions without needing the use keyword. This function can only be executed within the system's initialization script (e.g., dolphindb.dos). Functions loaded this way must be called using their namespace.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/module_tutorial.md#_snippet_6\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nloadModule(\"fileLog\")\n```\n\n----------------------------------------\n\nTITLE: Flattening and Converting Array Vector to Vectors and Matrices - DolphinDB Script\nDESCRIPTION: This section shows how to use the flatten function to convert an Array Vector or Columnar Tuple to a one-dimensional vector, and the matrix function to convert an Array Vector with equal-length subarrays to a matrix. Input is a nested array or columnar tuple; output is a flattened vector or matrix. All functions require the relevant DolphinDB array variables, and errors occur if subarray lengths are inconsistent during matrix conversion.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Array_Vector.md#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\nx = array(INT[], 0).append!([1 2 3, 4 5 6])\nz = flatten(x)\n/* z\n[1,2,3,4,5,6]\n*/\n\ny = [1 2 3, 4 5 6].setColumnarTuple!()\nz = flatten(y)\n/* z\n[1,2,3,4,5,6]\n*/\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nx = array(INT[], 0).append!([1 2 3, 4 5 6])\nz = matrix(x)\n/* z\n#0 #1 #2\n-- -- --\n1  2  3 \n4  5  6 \n*/\n\ny = [1 2 3, 4 5 6].setColumnarTuple!()\nz = matrix(y)\n/* z\n#0 #1\n-- --\n1  4 \n2  5 \n3  6 \n*/\n```\n\n----------------------------------------\n\nTITLE: Defining Multiprocessing Helpers for File Processing - Python\nDESCRIPTION: This snippet defines helper components for parallel processing using Python's `multiprocessing`. The `pool_func` function is designed to be executed by a pool process; it takes a list of filenames and a directory path, reads each file, applies the `level10Diff` function, handles potential errors, and concatenates the results. The `multi_task_split` class assists in dividing a list of tasks (filenames) among a specified number of processes, considering the available CPU count.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/DolphinDB_Python_Parser_Intro_for_Quantitative_Finance/因子实现_Python版本/十档委买增额.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef pool_func(tick_obj, snapshot_path_obj):\n    single_tick_res = []\n\n    for tick in tqdm(tick_obj):\n        try:\n            df = pd.read_csv(os.path.join(snapshot_path_obj, tick))\n            single_tick_res.append(level10Diff(df, lag=20))\n        except Exception as error:\n            single_tick_res = pd.DataFrame(columns=[\"UpdateTime\", \"SecurityID\", \"amtDiff\"])\n            continue\n\n    return pd.concat(single_tick_res, axis=0)\n\n\nclass multi_task_split:\n\n    def __init__(self, data, processes_to_use):\n        self.data = data\n        self.processes_to_use = processes_to_use\n\n    def num_of_jobs(self):\n        return min(len(self.data), self.processes_to_use, multiprocessing.cpu_count())\n\n    def split_args(self):\n        q, r = divmod(len(self.data), self.num_of_jobs())\n        return (self.data[i * q + min(i, r): (i + 1) * q + min(i + 1, r)] for i in range(self.num_of_jobs()))\n```\n\n----------------------------------------\n\nTITLE: 订阅XTP实时行情并写入分布式表的配置脚本（DolphinDB脚本）\nDESCRIPTION: 该脚本配置XTP账号信息、建立连接，并订阅快照、逐笔和订单簿行情数据，写入流式分布式表。使用前须修改账户配置参数，确保XTP插件正确接入全市场行情，并自动存储到持久化流表中。依赖XTP SDK和DolphinDB的订阅接口，实现实时行情流入。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/xtp.md#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 设置XTP配置\ntry { XTP::setGlobalConfig(1, \"./plugins/xtp/\", 3) } catch(ex) { print(ex) }\ngo\n// 创建连接\nxtpConn = XTP::createXTPConnection(\"xtpConn\")\n// 账户信息配置\nxtpConfig = dict(STRING, ANY);\nxtpConfig[\"ip\"] = \"111.111.111.111\";\nxtpConfig[\"port\"] = 1111;\nxtpConfig[\"user\"] = \"11111111111\";\nxtpConfig[\"password\"] = \"11111111111\";\nxtpConfig[\"protocalType\"] = 1;\nxtpConfig[\"heartBeatInterval\"] = 60;\ngo\n// 登录\nXTP::login(xtpConn, xtpConfig)\n// 接入快照行情\ntableDict = dict(STRING, ANY);\ntableDict[\"indexTable\"] = indexMarketDataStream\n\ntableDict[\"optionTable\"] = optionMarketDataStream\n\ntableDict[\"actualTable\"] = actualMarketDataStream\n\ntableDict[\"bondTable\"] = bondMarketDataStream\ngo\nXTP::subscribe(xtpConn, 1, 4, , tableDict)\n// 接入逐笔行情\n\n\n\ntableDict = dict(STRING, ANY);\ntableDict[\"entrustTable\"] = entrustStream\n\ntableDict[\"tradeTable\"] = tradeStream\n\ntableDict[\"statusTable\"] = stateStream\ngo\nXTP::subscribe(xtpConn, 2, 4, , tableDict)\n// 接入订单簿\n\n\ntableDict = dict(STRING, ANY);\ntableDict[\"orderBookTable\"] = orderBookStream\ngo\nXTP::subscribe(xtpConn, 3, 4, , tableDict)\n\n```\n\n----------------------------------------\n\nTITLE: QR Decomposition with default mode in DolphinDB\nDESCRIPTION: Demonstrates QR decomposition of a matrix in DolphinDB using the `qr` function with default parameters (`mode='full',pivoting=false`). The function decomposes a matrix X into an orthogonal matrix Q and an upper triangular matrix R. It shows the result of Q and R.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/matrix.md#_snippet_15\n\nLANGUAGE: DolphinDB\nCODE:\n```\n>m=matrix([2 5 7 5, 5 2 5 4, 8 2 6 4]);\n>m;\n#0 #1 #2\n-- -- --\n2  5  8 \n5  2  2 \n7  5  6 \n5  4  4  \n\n>q,r=qr(m); //mode='full',pivoting=false\n>q;\n#0        #1        #2        #3       \n--------- --------- --------- ---------\n-0.197066 0.903357  0.300275  0.234404 \n-0.492665 -0.418267 0.459245  0.609449 \n-0.68973  -0.02475  0.170745  -0.703211\n-0.492665 0.091573  -0.818398 0.281284 \n>r;\n#0         #1       #2       \n---------- -------- ---------\n-10.148892 -7.38997 -8.670898\n0          3.922799 6.608121 \n0          0        1.071571 \n0          0        0  \n\n```\n\n----------------------------------------\n\nTITLE: Cleaning Up DolphinDB Streaming Tables Environment - DolphinDB Script\nDESCRIPTION: This snippet provides commands to unsubscribe tables, drop streaming tables, and create shared and persistent streaming tables in DolphinDB. It ensures a clean environment by deleting previous stream subscriptions and tables before running simulations or anomaly detection. The snippet also shows the creation of a distributed database and a dimension table with metadata for seismic stations. It highlights usage of enableTableShareAndPersistence(), unsubscribeTable(), dropStreamTable(), and database/table creation syntax in DolphinDB.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Earthquake_Prediction_with_DolphinDB_and_Machine_Learning.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\nunsubscribeTable(tableName = \"dataStream\", actionName=\"filterPickerPredict\");go\nunsubscribeTable(tableName = \"pickerStream\",actionName=\"tensorFlowPredict\");go\ntry{dropStreamTable(\"dataStream\")}catch(ex){print(ex)}\ntry{dropStreamTable(\"pickerStream\")}catch(ex){print(ex)}\ntry{dropStreamTable(\"tensorStream\")}catch(ex){print(ex)}\n\n  //dataStream，接收波形实时流数据\nenableTableShareAndPersistence(table=streamTable(1000000:0, `tagid`ts`data, [INT,TIMESTAMP, INT]), tableName=`dataStream, cacheSize=1000000);\n  //pickerStream\nenableTableShareAndPersistence(table=streamTable(100000:0, `ts`id, [TIMESTAMP,INT]), tableName=`pickerStream, cacheSize=1000000);\n  //tensorStream\nenableTableShareAndPersistence(table=streamTable(100000:0, `ts`id, [TIMESTAMP, INT]), tableName=`tensorStream, cacheSize=1000000);\n\nif(existsDatabase(\"dfs://seis01\")) dropDatabase(\"dfs://seis01\");\n  //创建分布式数据库\ncreate database \"dfs://seis01\" partitioned by VALUE(1..10), engine='TSDB'\n  //创建维度表\ncreate table \"dfs://seis01\".\"tagInfo\"(\n\txfdsn SYMBOL,\n\tnet SYMBOL,\n\tsta SYMBOL,\n\tloc SYMBOL,\n\tchn SYMBOL,\n\tid INT[compress=\"delta\"]\n)\nsortColumns=[`id];\n  //向维度表插入数据\nnet = [\"ZJ\",\"YN\",\"XZ\",\"XJ\",\"TJ\"]\nsta = [\"A0001\",\"A0002\",\"A0003\",\"A0004\",\"A0005\",\"A0006\",\"B0001\",\"B0002\",\"B0003\",\"C0001\"]\ntmp = `EIE`EIN`EIZ\nnetList = stretch(net,150)\nstaList = take(stretch(sta,30),150)\nlocList = take(`40,150)\nchn = take(tmp,150)\ncolt =   array(STRING)\nfor(i in 0..(chn.size()-1)){\n\tcolt.append!( chn[i].split()[0] + \"_\" + chn[i].split()[1] + \"_\" +chn[i].split()[2] )\n}\nxfdsn = \"XFDSN:\"+netList+\"_\"+staList+\"_\"+locList+\"_\"+colt\nt = table(xfdsn,netList as net,staList as sta,locList as loc,chn,1..150 as id)\nloadTable( \"dfs://seis01\",\"tagInfo\").append!(t);\n```\n\n----------------------------------------\n\nTITLE: Adding Access Control to Shared Table - DolphinDB\nDESCRIPTION: This snippet demonstrates adding access control to a shared table using the `addAccessControl` function. This restricts access to the table to the creator and administrators.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Permission_Management.md#_snippet_31\n\nLANGUAGE: DolphinDB\nCODE:\n```\nshare table(1:0, `time`sym`volume, [TIMESTAMP, SYMBOL, INT]) as st2\naddAccessControl(st2)\n```\n\n----------------------------------------\n\nTITLE: Extracting Table Schema for Binary Data Source in DolphinDB Plugin C++\nDESCRIPTION: This function defines a static schema extractor that returns the structure of a data source as a DolphinDB table with column names and their corresponding types. It uses utility functions to create string vectors for names and types, and composes them into a table with headers 'name' and 'type'. This example assumes a fixed schema and does not require reading from actual files, but the method can be extended to parse headers or metadata for dynamic schemas.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_development_tutorial.md#_snippet_17\n\nLANGUAGE: cpp\nCODE:\n```\nConstantSP extractMyDataSchema(const ConstantSP &placeholderA, const ConstantSP &placeholderB) {\n\tConstantSP colNames = Util::createVector(DT_STRING, 4);\n\tConstantSP colTypes = Util::createVector(DT_STRING, 4);\n\tstring names[] = {\"id\", \"symbol\", \"date\", \"value\"};\n\tstring types[] = {\"LONG\", \"SYMBOL\", \"DATE\", \"DOUBLE\"};\n\tcolNames->setString(0, 4, names);\n\tcolTypes->setString(0, 4, types);\n\n\tvector<ConstantSP> schema = {colNames, colTypes};\n\tvector<string> header = {\"name\", \"type\"};\n\n\treturn Util::createTable(header, schema);\n}\n```\n\n----------------------------------------\n\nTITLE: Selecting a single field using single-column macro variable\nDESCRIPTION: The code retrieves a single column specified by the macro variable `name` from the table `t`. `name` is a string, and `_$name` is the single-column macro variable representing the column name in the SQL query.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/macro_var_based_metaprogramming.md#_snippet_11\n\nLANGUAGE: DolphinDB\nCODE:\n```\nt = table(`a`a`b as sym, 1 2 3 as val)\nname=\"sym\"\n<select _$name from t>.eval()\n```\n\n----------------------------------------\n\nTITLE: Result Equivalence Checking for Aggregated Trade Groupings in DolphinDB Script\nDESCRIPTION: Checks the equality of values between trade aggregation results using eqObj and each, ensuring that different grouping/logical strategies yield identical data summaries. This is used for validation after optimization refactoring.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_SQL_Case_Tutorial.md#_snippet_37\n\nLANGUAGE: DolphinDB\nCODE:\n```\neach(eqObj, (select date, symbol, side, type, volume_sum, amount_sum \n             from res1 order by date, symbol, side, type).values(), res2.values()) // true\neach(eqObj, res2.values(), res3.values()) // true\n```\n\n----------------------------------------\n\nTITLE: Creating Vectors in DolphinDB C++ Plugin\nDESCRIPTION: This snippet demonstrates how to create different types of vectors in a DolphinDB C++ plugin using the `Util::createVector`, `Util::createRepeatingVector`, and `Util::createIndexVector` functions. It shows how to initialize vectors with specific data types, repeating values, and sequential numbers.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_advance.md#_snippet_1\n\nLANGUAGE: c++\nCODE:\n```\nVectorSP v = Util::createVector(DT_INT, 10);     // 创建一个初始长度为10的int类型向量\nv->setInt(0, 60);                                // 相当于v[0] = 60\n\nVectorSP t = Util::createVector(DT_ANY, 0);      // 创建一个初始长度为0的any类型向量（元组）\nt->append(new Int(3));                           // 相当于t.append!(3)\nt->get(0)->setInt(4);                            // 相当于t[0] = 4\n// 这里不能用t->setInt(0, 4)，因为t是一个元组，setInt(0, 4)只对int类型的向量有效\n\nConstantSP tem = new Double(2.1);                // 相当于2.1    \nVectorSP v1 = Util::createRepeatingVector(tem, 10); // 创建一个初始长度为10，所有数据为2.1的向量\n\nVectorSP seq = Util::createIndexVector(5, 10);   // 创建一个长度为10，起始值为5的向量，相当于5..14\nint seq0 = seq->getInt(0);                       // 相当于seq[0]\n```\n\n----------------------------------------\n\nTITLE: Aggregating Memory Usage of High-Load, High-Temperature Devices within a Time Range in MongoDB - JavaScript\nDESCRIPTION: This aggregation pipeline filters documents in 'device_readings' for a given time cutoff, minimum battery temperature, and minimum CPU load, then groups by timestamp and device ID to find the maximum free memory. Inputs are the time, temperature, and CPU load thresholds. Outputs grouped maximums. Appropriate indexes on filter fields are required for efficiency.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs MongoDB/mongodb_readings_query.txt#_snippet_10\n\nLANGUAGE: JavaScript\nCODE:\n```\ndb.device_readings.explain(\"executionStats\").aggregate([{$match:{time:{\"$lte\":ISODate(\"2016-11-18 21:00:00.000Z\")},battery_temperature:{\"$gte\":90},cpu_avg_1min:{\"$gte\":90}}},{$group:{_id:{time:\"$time\",device_id:\"$device_id\"},max_mem_free:{$max:\"$mem_free\"}}}])\n```\n\n----------------------------------------\n\nTITLE: Loading 'US_Trades' Data into DolphinDB (DolphinDB Script)\nDESCRIPTION: DolphinDB script to load US trade data from a CSV file (`US.csv`) into a distributed DolphinDB database. It sets the file path, defines the DFS database path (`dfs://rangedb_us`), creates a ranged partitioned database based on year (starting Jan 1990, partitioned annually), measures execution time, and loads the CSV data into the 'trades' table, partitioning by the 'date' column.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_9\n\nLANGUAGE: dolphindb\nCODE:\n```\nfilepath = \"/home/revenant/data/US.csv\"\ndbpath = \"dfs://rangedb_us\"\ndb = database(dbpath, RANGE, 1990.01M+(0..27)*12)\ntimer(1)\ntrades = db.loadTextEx(`trades, `date, filepath)\n```\n\n----------------------------------------\n\nTITLE: Creating Partitioned Table with Single Hash Partitioning by SecurityID in DolphinDB Script\nDESCRIPTION: Creates a hash-partitioned database and partitioned table by stock symbol only, without including trade date as partition column. This leads to scanning all partitions when queried by date and continuous growth of partitions (2.7.1 mistake). Dependencies: DolphinDB with hash partition support. Inputs are symbol partitioning parameters and stock fields; output is a partitioned table partitioned only by symbol.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/database_and_table_creation_details.md#_snippet_13\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndb = database(\"dfs://testDB1\",HASH, [SYMBOL, 25])\ncolName = `SecurityID`TradeDate`TradeTime`TradePrice`TradeQty`TradeAmount`BuyNo`SellNo\ncolType = `SYMBOL`DATE`TIME`DOUBLE`INT`DOUBLE`INT`INT\ntbSchema = table(1:0, colName, colType)\ndb.createPartitionedTable(table=tbSchema, tableName=`pt, partitionColumns=`SecurityID)\n```\n\n----------------------------------------\n\nTITLE: Querying with Multiple Filters: Time, Device IDs, and Battery Criteria in MongoDB - JavaScript\nDESCRIPTION: This snippet combines time range selection, device ID filtering using '$in', battery level, and battery status filters in the 'device_readings' collection. Inputs are time bounds, a device ID array, a maximum battery level, and specific status. Requires MongoDB with relevant fields and appropriate indexes. Returns matching records with all specified constraints.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs MongoDB/mongodb_readings_query.txt#_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\ndb.device_readings.explain(\"executionStats\").find({time:{\"$gte\":ISODate(\"2016-11-15 20:00:00.000Z\"),\"$lte\":ISODate(\"2016-11-16 22:30:00.000Z\")},device_id:{\"$in\":[\"demo000002\",\"demo000020\",\"demo000200\",\"demo002000\"]},battery_level:{\"$lte\":50},battery_status:\"discharging\"},{})\n```\n\n----------------------------------------\n\nTITLE: Executing Parallel Processing on Stock Data Files\nDESCRIPTION: Sets the number of processes, defines the data directory, and splits stock file list into chunks for multiprocessing. Then uses Pool and tqdm to process chunks in parallel, and combines results into a single DataFrame.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/DolphinDB_Python_Parser_Intro_for_Quantitative_Finance/因子实现_Python版本/主动成交量占比.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nn_use = 24\n# 路径修改为存放数据路径\ntrade_path = r\"/ssd/ssd3/data/oneStock_oneFile_TL/20230201/trade\"\nstock_pool = os.listdir(trade_path)\nprocesses_decided = multi_task_split(stock_pool, n_use).num_of_jobs()\nprint(\"进程数：\", processes_decided)\nsplit_args_to_process = list(multi_task_split(stock_pool, n_use).split_args())\nargs = [(split_args_to_process[i], trade_path) for i in range(len(split_args_to_process))]\nprint(\"#\" * 50 + \"Multiprocessing Start\" + \"#\" * 50)\nt0 = time.time()\nwith multiprocessing.Pool(processes=processes_decided) as pool:\n    res = tqdm(pool.starmap(pool_func, args))\n    print(\"cal time: \", time.time() - t0, \"s\")\n    res_combined = pd.concat(res, axis=0)\n    pool.close()\n    print(\"cal time: \", time.time() - t0, \"s\")\nprint(res_combined)\n```\n\n----------------------------------------\n\nTITLE: Sending HTTP Request for ElasticSearch\nDESCRIPTION: This Python snippet sends an HTTP GET request to an ElasticSearch instance to perform a search. It uses `urllib3` to manage the connection pool and `json` to serialize the request body. It then prints the HTTP status code and the decoded JSON response from the server. This snippet acts as a basic request template for various search queries.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_35\n\nLANGUAGE: Python\nCODE:\n```\nr = http.request(\"GET\", \"http://localhost:9200/elastic/_search\", body=data,\n                     headers={'Content-Type': 'application/json'})\n    print(r.status)\n    print(json.loads(r.data.decode()))\n```\n\n----------------------------------------\n\nTITLE: Counting Partitions per Data Node in DolphinDB\nDESCRIPTION: Executes the `getAllChunks` function on all data nodes using `pnodeRun` to retrieve partition (chunk) information for databases whose paths start with '/Level1'. It then aggregates the results using SQL to count the number of partitions, grouped by the data node alias (`site`) and partition type (`type`, where 0=File Chunk, 1=Tablet Chunk). This helps verify the initial distribution of data partitions across the cluster.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/data_move_rebalance.md#_snippet_1\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nselect count(*) from pnodeRun(getAllChunks) where dfsPath like \"/Level1%\" group by site, type\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Financial Functions and Metaprogramming for Aggregation in DolphinDB\nDESCRIPTION: Defines functions for logarithmic return and realized volatility calculations used in financial time series analysis. Additionally, implements a meta-programming function to generate aggregation code dynamically based on a dictionary of columns and associated aggregation functions, mimicking pandas' groupby-agg.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/metacode_derived_features/metacode_derived_features_arrayVector.txt#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef logReturn(s){\n\treturn log(s)-log(prev(s))\n}\n\ndef realizedVolatility(s){\n\treturn sqrt(sum2(s))\n}\n\n//与pandas中的group by agg功能相同，传入字典（key为列名，value为functions），批量生成元编程代码\ndef createAggMetaCode(aggDict){\n\tmetaCode = []\n\tmetaCodeColName = []\n\tfor(colName in aggDict.keys()){\n\t\tfor(funcName in aggDict[colName])\n\t\t{\n\t\t\tmetaCode.append!(sqlCol(colName, funcByName(funcName), colName + `_ + funcName$STRING))\n\t\t\tmetaCodeColName.append!(colName + `_ + funcName$STRING)\n\t\t}\n\t}\n\treturn metaCode, metaCodeColName$STRING\n}\n```\n\n----------------------------------------\n\nTITLE: Querying TopN Correlation in Time-based Window - DolphinDB Script\nDESCRIPTION: Utilizes tmcorrTopN to compute correlation between 'close' and 'volume' over a 5-minute window for each stock, selecting the top 3 records by volume. Script demonstrates aggregation in a real-time or streaming context. Input: tradingTime, close, volume (twice for sorting and as secondary metric), 5m, 3, false; Output: time-indexed per-stock rolling Top3 correlation.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_TopN.md#_snippet_8\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect windCode, tradingTime, tmcorrTopN(tradingTime, close, volume, volume, 5m, 3, false) as tmavgTop3CorrCloseVolume from t context by windCode, date(tradingTime)\n```\n\n----------------------------------------\n\nTITLE: Loading Historical Data for Replay (DolphinDB Script)\nDESCRIPTION: Specifies the path to a historical snapshot data CSV file and loads its content into a temporary in-memory table `testSnapshot`, ensuring it conforms to the schema of the `snapshotStream` table. This data will be used to simulate a stream for testing.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/metacode_derived_features/metacode_derived_features_streaming.txt#_snippet_16\n\nLANGUAGE: DolphinDB\nCODE:\n```\ncsvFilePath = \"/hdd/hdd9/machineLearning/testSnapshot.csv\"\ntestSnapshot = loadText(filename=csvFilePath, schema=table(snapshotStream.schema().colDefs.name, snapshotStream.schema().colDefs.typeString))\n```\n\n----------------------------------------\n\nTITLE: Starting DolphinDB Cluster Nodes on Windows Using Batch and VBScript\nDESCRIPTION: These Windows commands show how to start DolphinDB cluster nodes on a Windows environment. Users can start the control and agent nodes in the foreground by double-clicking batch files 'startController.bat' and 'startAgent.bat' located in the 'C:\\DolphinDB\\server\\clusterDemo' directory. Alternatively, the nodes can be started in the background by running the VBScript files 'backgroundStartController.vbs' and 'backgroundStartAgent.vbs' in the same directory. The installation path must exclude spaces and non-ASCII characters to avoid failures when starting data nodes.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/single_machine_cluster_deploy.md#_snippet_5\n\nLANGUAGE: Windows Batch\nCODE:\n```\nstartController.bat\n```\n\nLANGUAGE: Windows Batch\nCODE:\n```\nstartAgent.bat\n```\n\nLANGUAGE: VBScript\nCODE:\n```\nbackgroundStartController.vbs\n```\n\nLANGUAGE: VBScript\nCODE:\n```\nbackgroundStartAgent.vbs\n```\n\n----------------------------------------\n\nTITLE: 使用loadText导入csv文件并查看结构（DolphinDB脚本）\nDESCRIPTION: 该代码示例演示如何用loadText函数导入CSV格式文件到内存表，并通过查询前五行数据和schema函数，查看导入数据的内容和结构信息。importPath为数据文件路径，输出包含列名及数据类型的表结构信息。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_csv.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndataFilePath=\"/home/data/candle_201801.csv\"\ntmpTB=loadText(filename=dataFilePath);\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect top 5 * from tmpTB;\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntmpTB.schema().colDefs;\n```\n\n----------------------------------------\n\nTITLE: Running Recent Jobs in DolphinDB\nDESCRIPTION: This snippet runs the `getRecentJobs` function, likely a built-in function in the DolphinDB environment for retrieving information about recent job executions.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_factor.txt#_snippet_9\n\nLANGUAGE: DolphinDB\nCODE:\n```\npnodeRun(getRecentJobs)\n```\n\n----------------------------------------\n\nTITLE: C++ API: Adding Data to Array Vector Column\nDESCRIPTION: Adds data row by row to a C++ table object, specifically targeting an Array Vector column.  This snippet shows how to create a `DdbVector`, populate it with data, and then set it as a row in the Array Vector column.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Array_Vector.md#_snippet_19\n\nLANGUAGE: C++\nCODE:\n```\nfor(int i = 0; i < rowNum; ++i) {\n    // 构造 Array Vector 的一行\n    DdbVector<int> intVec(0, 10);\n    for(int j = 0; j < 3; ++j) {\n        intVec.add(i*3+j);\n    }\n    VectorSP value_row = intVec.createVector(DT_INT);\n    columnVecs[0]->setInt(i, i);        // id 列\n    columnVecs[1]->set(i, value_row);    // value 列\n}\n```\n\n----------------------------------------\n\nTITLE: Disabling DolphinDB Stream Table Persistence\nDESCRIPTION: Stops the persistence process for the specified stream table. Data already persisted on disk is not removed by this function (use `clearTablePersistence` for that).\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_tutorial.md#_snippet_12\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndisableTablePersistence(pubTable)\n```\n\n----------------------------------------\n\nTITLE: Calculating OHLC values in DolphinDB\nDESCRIPTION: This code snippet calculates the Open, High, Low, and Close (OHLC) values for each stock per day.  The result is assigned to the 'result' variable on the server side and is not returned to the client immediately for display.  This approach minimizes client-side memory usage when dealing with potentially large result sets.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/single_machine_cluster_deploy.md#_snippet_10\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// SQL 返回数据量较大时，可以赋值给变量，占用 server 端内存，客户端分页取回展示\nresult = select first(LastPx) as Open, max(LastPx) as High, min(LastPx) as Low, last(LastPx) as Close from pt group by date(DateTime) as Date, SecurityID\n```\n\n----------------------------------------\n\nTITLE: Thread-Unsafe Concurrent Partition Writing Example in DolphinDB Script\nDESCRIPTION: This code attempts to concurrently write to all partitions from multiple jobs, leading to possible race conditions and system instability. It submits two jobs with overlapping partition ID ranges, which DolphinDB cannot guarantee to serialize safely unless the partitioned table is shared explicitly. The snippet illustrates unsafe usage that should be avoided.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/in_memory_table.md#_snippet_7\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\njob1=submitJob(\"write1\",\"\",writeData,pt,1..300,1000,1000)\njob2=submitJob(\"write2\",\"\",writeData,pt,1..300,1000,1000)\n```\n\n----------------------------------------\n\nTITLE: Configuring Maximum Number of Connections in DolphinDB\nDESCRIPTION: This snippet sets the maximum number of connections allowed to a DolphinDB data node from various sources (GUI, API, other nodes).  The maxConnections parameter controls the concurrency level of the data node, limiting the number of simultaneous connections to prevent resource exhaustion.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/ha_cluster_deployment/P1/config/config-specification.txt#_snippet_1\n\nLANGUAGE: properties\nCODE:\n```\nmaxConnections=512\n```\n\n----------------------------------------\n\nTITLE: AlertManager Configuration for WeChat Enterprise Alerting\nDESCRIPTION: YAML configuration for Prometheus AlertManager to send notifications to WeChat Enterprise. Includes global settings, notification templates, routing rules, and receiver configuration.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/cluster_monitor.md#_snippet_13\n\nLANGUAGE: YAML\nCODE:\n```\nglobal: \n  resolve_timeout: 1m\n\ntemplates:\n  - '/home/appadmin/monitor/alertmanager/alertmanager-0.24.0.linux-amd64/template/wechat.tmpl'  #企业微信告警消息模板地址\n\nroute:\n  receiver: 'wechat'  #下面 receivers 设置的接收者名称\n  group_by: ['alertname'] \n  group_wait: 30s  # 这三个的概念和前面 Grafana 里的概念一样\n  group_interval: 1m \n  repeat_interval: 10m \n  \n\nreceivers:\n- name: 'wechat' \n  wechat_configs: \n    - corp_id: 'ww68d468fe12810853' #企业 ID\n      to_party: '38'   #部门 ID\n      agent_id: '1000015'   #应用 ID\n      api_secret: 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx' #Secret\n      send_resolved: true  #是否发送告警恢复\n```\n\n----------------------------------------\n\nTITLE: Saving In-Memory Table to a Distributed Database in DolphinDB\nDESCRIPTION: This code snippet shows how to save an in-memory table to a distributed database in DolphinDB. It creates a partitioned table from the in-memory table and appends the in-memory data to it. Requires that `t1` is already defined as an in-memory table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Single_Node_Tutorial.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndb = database(\"dfs://DolphinDB\",VALUE,1..10)\npt=createPartitionedTable(db,t1,`pt,`id).append!(t1)\n```\n\n----------------------------------------\n\nTITLE: Listing Directory Contents in Console\nDESCRIPTION: This snippet shows how to list the contents of a directory using the `tree` command in the console. It's used to examine the file structure before and after updates to observe changes in data storage.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/dolphindb_update.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n$ tree\n.\n└── 2\n    └── machines_2\n        ├── datetime.col\n        ├── id.col\n        ├── tag10.col\n        ├── tag11.col\n         ...\n        └── tag9.col\n```\n\n----------------------------------------\n\nTITLE: Plotting Fee Distributions by Fund Type\nDESCRIPTION: Generates histograms for fund fee distributions across different fund types. Uses embedded DolphinDB syntax to filter data by fund type and plot histograms with specified bin numbers. Aids in visual analysis of fee variance and distribution shape per category.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/public_fund_basic_analysis.md#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\n/**plot fees histogram*/\n// Type=\"REITs\"\n(exec Fee from fundFee where Type=\"REITs\").plotHist(binNum=100)\n// Type=\"保本型\"\n(exec Fee from fundFee where Type=\"保本型\").plotHist(binNum=100)\n// Type=\"债券型\"\n(exec Fee from fundFee where Type=\"债券型\").plotHist(binNum=100)\n// Type=\"另类投资型\"\n(exec Fee from fundFee where Type=\"另类投资型\").plotHist(binNum=100)\n// Type=\"商品型\"\n(exec Fee from fundFee where Type=\"商品型\").plotHist(binNum=100)\n// Type=\"混合型\"\n(exec Fee from fundFee where Type=\"混合型\").plotHist(binNum=100)\n// Type=\"股票型\"\n(exec Fee from fundFee where Type=\"股票型\").plotHist(binNum=100)\n// Type=\"货币市场型\"\n(exec Fee from fundFee where Type=\"货币市场型\").plotHist(binNum=100)\n```\n\n----------------------------------------\n\nTITLE: Loading Level 2 Snapshot Data and Defining Stock List in DolphinDB\nDESCRIPTION: Specifies the database (`dbName`) and table (`tableName`) containing the source level 2 snapshot data. It loads this table into the `snapshot` variable. A list of specific stock security IDs (`stockList`), likely components of the SZ50 index, is defined for filtering the processing.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/machine_learning_volatility/01.dataProcess.txt#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndbName = \"dfs://snapshot_SH_L2_OLAP\"\ntableName = \"snapshot_SH_L2_OLAP\"\nsnapshot = loadTable(dbName, tableName)\nstockList=`601318`600519`600036`600276`601166`600030`600887`600016`601328`601288`600000`600585`601398`600031`601668`600048`601888`600837`601601`601012`603259`601688`600309`601988`601211`600009`600104`600690`601818`600703`600028`601088`600050`601628`601857`601186`600547`601989`601336`600196`603993`601138`601066`601236`601319`603160`600588`601816`601658`600745\n```\n\n----------------------------------------\n\nTITLE: Timezone Conversions in DolphinDB Shell\nDESCRIPTION: This code demonstrates the use of the `localtime`, `gmtime` and `convertTZ` functions for time zone conversion in DolphinDB. `localtime` converts Greenwich Mean Time to local time, `gmtime` converts local time to Greenwich Mean Time, and `convertTZ` converts between arbitrary timezones.  The example shows conversions based on Eastern Standard Time (EST).\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/date_time.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\n>localtime(2018.01.22T15:20:26);\n2018.01.22T10:20:26\n>localtime(2017.12.16T18:30:10.001);\n2017.12.16T13:30:10.001\n>gmtime(2018.01.22 10:20:26);\ngmtime(2018.01.22 10:20:26);\n>gmtime(2017.12.16T13:30:10.008);\n2017.12.16T18:30:10.008\n>convertTZ(2016.04.25T08:25:45,\"US/Eastern\",\"Asia/Shanghai\");\n2016.04.25T20:25:45\n```\n\n----------------------------------------\n\nTITLE: Reading data matrix from wide CSV files in DolphinDB\nDESCRIPTION: Defines a function to read data from a wide-format CSV file into a matrix. The function sets the first column as DATE type for timestamps and converts all other data to DOUBLE type, then creates a matrix with appropriate row and column names.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_data_load.txt#_snippet_2\n\nLANGUAGE: dolphindb\nCODE:\n```\ndef readIndexedMatrixFromWideCSV(absoluteFilename){\n        contracts = readColumnsFromWideCSV(absoluteFilename)\n        dataZoneSchema = extractTextSchema(absoluteFilename, skipRows = 1)\n        update dataZoneSchema set type = \"DOUBLE\" where name != \"col0\"//所有行除第一行外全部改成double\n        update dataZoneSchema set type = \"DATE\" where name = \"col0\"//所有行除第一行外全部改成DATE\n        dataZoneWithIndexColumn = loadText(absoluteFilename, skipRows = 1, schema = dataZoneSchema)\n        indexVector = exec col0 from dataZoneWithIndexColumn\n        dataZoneWithoutIndex = dataZoneWithIndexColumn[:, 1:]\n        dataMatrix = matrix(dataZoneWithoutIndex)\n        dataMatrix.rename!(indexVector, contracts)\n        return dataMatrix\n}\n```\n\n----------------------------------------\n\nTITLE: Calculating Amount using Function Based Metaprogramming\nDESCRIPTION: This snippet uses function-based metaprogramming, specifically the `binaryExpr` function, to calculate amounts. It constructs SQL with the `sqlCol` function to generate individual column references. It employs nested calls to `sqlColAlias` to assign aliases.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/macro_var_based_metaprogramming.md#_snippet_22\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 模拟数据脚本\nsym = [\"a\" + string(1..10)]\ndate = [take(2022.01.02, 10)]\nprice = table(rand(10.0, 500) $ 10:50).values()\nqty = table(rand(1000, 500) $ 10:50).values()\ndata = (sym).appendTuple!(date).appendTuple!(price).appendTuple!(qty)\nt=table(1:0, [`sym, `date] join priceCols join amountCols, [SYMBOL, DATE] join take(DOUBLE, 50) join take(INT, 50))\nt.tableInsert(data)\n\n// 批计算 SQL 脚本示意（中间省略部分）\nselect price1*qty1 as amount1, price2*qty2 as amount2 ... , price50*qty50 as amount50 from t\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\npriceCols = \"price\" + string(1..50)\nqtyCols = \"qty\" + string(1..50)\namountCols=\"amount\"+string(1..50)\nslt=sqlColAlias(binaryExpr(sqlCol(priceCols), sqlCol(qtyCols), *), amountCols)\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nsql(select=slt, from=t).eval()\n```\n\n----------------------------------------\n\nTITLE: Create sample table for moving example\nDESCRIPTION: This DolphinDB script creates a sample table `t` with random data for demonstrating the `moving` function. The table includes columns like `ts_code`, `trade_date`, `open`, `high`, `low`, `close`, `downAvgPrice`, `upAvgPrice`, and `singna` (signal). The size of each column is determined by 'n'.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_25\n\nLANGUAGE: shell\nCODE:\n```\nt=table(rand(\"d\"+string(1..n),n) as ts_code, nanotimestamp(2008.01.10+1..n) as trade_date, rand(n,n) as open, rand(n,n) as high, rand(n,n) as low, rand(n,n) as close, rand(n,n) as pre_close, rand(n,n) as change, rand(n,n) as pct_chg, rand(n,n) as vol, rand(n,n) as amount, rand(n,n) as downAvgPrice, rand(n,n) as upAvgPrice, rand(1 0,n) as singna)\n```\n\n----------------------------------------\n\nTITLE: Upgrading DolphinDB Cluster Nodes Online and Offline Using Shell Script\nDESCRIPTION: This snippet shows the upgrade workflow by executing the upgrade.sh script. For online upgrade, users input 'y' to confirm and select option '1' to specify the version to update to. For offline upgrade, after uploading the installation package to all servers, users run the script, confirm with 'y', select option '2' for offline upgrade, then specify the version number. Proper execution results in upgrading all cluster nodes to the desired version.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/multi_machine_cluster_deployment.md#_snippet_4\n\nLANGUAGE: Shell\nCODE:\n```\n./upgrade.sh\n```\n\n----------------------------------------\n\nTITLE: Elasticsearch Grouping by Date Aggregation\nDESCRIPTION: This Python code defines an Elasticsearch aggregation to group data by the 'date' field and calculate the sum of the 'VOL' field for each date. It prepares the JSON request body using `json.dumps()`, encodes it to UTF-8, and sends an HTTP GET request using `urllib3`. This function retrieves and prints the status code and the JSON response from Elasticsearch. Requires the `uscsv` index to be present and populated.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_53\n\nLANGUAGE: Python\nCODE:\n```\ndef search_7():\n    http = urllib3.PoolManager()\n    data = json.dumps({\n        \"aggs\": {\n            \"group_by_ts_code\": {\n                \"terms\": {\n                    \"field\": \"date\",\n                    \"size\": 6828  \n                },\n                \"aggs\": {\n                    \"sum_vol\": {\n                        \"sum\": {\"field\": \"VOL\"}\n                    }\n                }\n            }\n        },\n        \"_source\": [\"\"]\n    }).encode(\"utf-8\")\n    r = http.request(\"GET\", \"http://localhost:9200/uscsv/_search\", body=data,\n                     headers={'Content-Type': 'application/json'})\n    print(r.status)\n    print(json.loads(r.data.decode()))\n```\n\n----------------------------------------\n\nTITLE: Publishing Side Data Filtering for Stream Tables in DolphinDB\nDESCRIPTION: Shows how to filter published stream data on the server side by specifying a filter vector, range, or hash bucket using setStreamTableFilterColumn and filter parameter in subscribeTable. Examples include filtering by specific symbol values, price ranges, and hashed buckets to limit the data published to subscribers. Requires setting the filtering column on the stream table and providing the correct filter expressions. Inputs are published stream table data; outputs are filtered data delivered to subscribers. This enhances data delivery efficiency by publishing only relevant content.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_tutorial.md#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\nshare streamTable(10000:0,`time`symbol`price, [TIMESTAMP,SYMBOL,INT]) as trades\nsetStreamTableFilterColumn(trades, `symbol)\ntrades_1=table(10000:0,`time`symbol`price, [TIMESTAMP,SYMBOL,INT])\n\nfilter=symbol(`IBM`GOOG)\n\nsubscribeTable(tableName=\"trades\", actionName=\"trades_1\", handler=append!{trades_1}, msgAsTable=true, filter=filter)\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nshare streamTable(10000:0,`time`symbol`price, [TIMESTAMP,SYMBOL,INT]) as trades\nsetStreamTableFilterColumn(trades, `price)\ntrades_1=table(10000:0,`time`symbol`price, [TIMESTAMP,SYMBOL,INT])\n\nsubscribeTable(tableName=\"trades\", actionName=\"trades_1\", handler=append!{trades_1}, msgAsTable=true, filter=1:100)\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nshare streamTable(10000:0,`time`symbol`price, [TIMESTAMP,SYMBOL,INT]) as trades\nsetStreamTableFilterColumn(trades, `symbol)\ntrades_1=table(10000:0,`time`symbol`price, [TIMESTAMP,SYMBOL,INT])\n\nsubscribeTable(tableName=\"trades\", actionName=\"trades_1\", handler=append!{trades_1}, msgAsTable=true, filter=(10,1:5))\n```\n\n----------------------------------------\n\nTITLE: Handler for Realized Volatility Prediction - DolphinDB\nDESCRIPTION: Defines and subscribes the 'predictRV' handler to receive aggregated features and run inference with the loaded model, then writes predictions to the \"result1min\" table. The handler measures prediction latency ('CostTime') and outputs a table with prediction results per batch. The subscription ensures continuous real-time inference as new data is processed.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/machine_learning_volatility/04.streamComputing.txt#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//define handler\ndef predictRV(mutable result1min, model, msg){\n\tstartTime = now()\n\tpredicted = model.predict(msg)\n\ttemp = select TradeTime, SecurityID, predicted as PredictRV, (now()-startTime) as CostTime from msg\n\tresult1min.append!(temp)\n}\n//subscribe data\nsubscribeTable(tableName=\"aggrFeatures10min\", actionName=\"predictRV\", offset=-1, handler=predictRV{result1min, model}, msgAsTable=true, hash=1, reconnect=true)\ngo\n\n```\n\n----------------------------------------\n\nTITLE: Aggregating Max Offer and Average Bid by Date using urllib3 in Python\nDESCRIPTION: This function queries Elasticsearch via `urllib3`. It aggregates results by DATE, calculating both the maximum OFR price and the average BID price for each date. The source is suppressed.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_65\n\nLANGUAGE: python\nCODE:\n```\nimport urllib3\nimport json\n\ndef search_5():\n    http = urllib3.PoolManager()\n    data = json.dumps({\n        \"aggs\": {\n            \"group_by_date\": {\n                \"terms\": {\n                    \"field\": \"DATE\",\n                    \"size\": 4\n                },\n                \"aggs\": {\n                    \"max_ofr\": {\n                        \"max\": {\"field\": \"OFR\"}\n                    },\n                    \"avg_bid\": {\n                        \"avg\": {\"field\": \"BID\"}\n                    }\n                }\n            }\n        },\n        \"_source\": [\"\"]\n    }).encode(\"utf-8\")\n    r = http.request(\"GET\", \"http://localhost:9200/hundred/_search\", body=data,\n                     headers={'Content-Type': 'application/json'})\n    print(r.status)\n    print(json.loads(r.data.decode()))\n```\n\n----------------------------------------\n\nTITLE: Elasticsearch Filtering by Stock Code - Python\nDESCRIPTION: This Python function searches an Elasticsearch index named 'elastic' to retrieve data filtered by stock code ('ts_code'). It uses the `elasticsearch` library to construct and execute a search query with a range filter on the 'ts_code' field. The function employs scrolling to process large datasets, printing the scroll ID and total hits. It then iterates through the scroll results and outputs the scroll size for each batch.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_27\n\nLANGUAGE: Python\nCODE:\n```\ndef search_2():\n    es = Elasticsearch(['http://localhost:9200/'])\n    page = es.search(\n        index='elastic',\n        doc_type='type',\n        scroll='2m',\n        size=10000,\n        body={\n            \"query\": {\n                \"constant_score\": {\n                    \"filter\": {\n                        \"range\": {\n                            \"ts_code\": {\n                                \"lte\": \"002308.SZ\"\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    )\n    sid = page['_scroll_id']\n    scroll_size = page['hits']['total']\n\n    print(sid)\n    print(scroll_size)\n    # Start scrolling\n    while (scroll_size > 0):\n        print(\"Scrolling...\")\n        page = es.scroll(scroll_id=sid, scroll='2m')\n        # Update the scroll ID\n        sid = page['_scroll_id']\n        # Get the number of results that we returned in the last scroll\n        scroll_size = len(page['hits']['hits'])\n        print(\"scroll size: \" + str(scroll_size))\n```\n\n----------------------------------------\n\nTITLE: Main Function for Orchestrating Setup and Execution - DolphinDB\nDESCRIPTION: This `main` function orchestrates the entire process. It calls `clearEnv` for cleanup, `createInOutTable` to set up stream tables, `consume` to configure engines and subscriptions, and `writeStreamTable` to start the MQTT data ingestion. Finally, it calls `genData` to create simulation data and `publishTableData` via a submitted job to feed the data into the system.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/streaming_engine_anomaly_alerts/流计算引擎级联检测门禁超时数据.txt#_snippet_9\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef main(mutable st){\n\tclearEnv()\n\tcreateInOutTable()\n\tconsume()\n\t\nhost=\"127.0.0.1\"\n\tport=1883\n\ttopic=\"sensor/test\"\n\twriteStreamTable(host, port, topic)\n\t\n\t//产生并推送表数据到mqtt服务器\n\tgenData(st)\n\tf = createJsonFormatter()\n\tbatchsize=100\n\tsubmitJob(\"submit_pub1\", \"submit_p1\", publishTableData{host,topic,f, batchsize,st})\n}\n```\n\n----------------------------------------\n\nTITLE: Defining a table for segment example\nDESCRIPTION: This code defines a DolphinDB table 't' with two columns: 'date' and 'v'. The 'date' column contains a sequence of dates from 2021.09.01 to 2021.09.12. The 'v' column contains a sequence of floating-point numbers.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_38\n\nLANGUAGE: shell\nCODE:\n```\ndated = 2021.09.01..2021.09.12\nv = 0 0 0.3 0.3 0 0.5 0.3 0.7 0 0 0.3 0\nt = table(dated as date, v)\n```\n\n----------------------------------------\n\nTITLE: Script Command Response Format\nDESCRIPTION: Describes the response format for the 'script' command. Includes optional MSG section (for API2 and scripts that use print), session ID, byte order, and the execution result ('OK' on success) along with the result data which data format references the third section.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/api_protocol.md#_snippet_5\n\nLANGUAGE: text\nCODE:\n```\n长度(Byte) | 报文 | 说明 | 样本\n---|---|---|---\n不固定|MSG| 如果请求类型为API2, 并且脚本中间有print等输出脚本,在返回报文包含MSG段 | MSG<br>\"this is output message1\"<br>MSG<br>\"this is output message2\"\n不固定| SESSIONID | 长度不固定，到空格为止  | 2247761467\n1| 空格| char(0x20) |\n1|大小端 | 1-小端，0-大端 | 1\n1| 换行符(LF) | char(0x10) |\n1| 执行成功否| 返回文本OK表示执行成功 | \"OK\"\n1| 换行符(LF) | char(0x10) |\n不固定| 返回结果 | 数据格式参考第3节 |\n```\n\n----------------------------------------\n\nTITLE: Logging In and Clearing Environment in DolphinDB\nDESCRIPTION: Logs into the DolphinDB server using the username 'admin' and password '123456', clears all cached items from memory, and undefines all variables in the current session to ensure a clean state before script execution.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/machine_learning_volatility/01.dataProcess.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(\"admin\", \"123456\")\nclearAllCache()\nundef(all)\ngo\n```\n\n----------------------------------------\n\nTITLE: 确认DolphinDB使用的OpenSSL版本（Shell命令）\nDESCRIPTION: 该代码段展示了如何通过`strings`命令检出DolphinDB的共享库`libDolphinDB.so`中OpenSSL的版本信息，以验证OpenSSL版本是否与ODBC驱动的版本冲突，从而判断崩溃原因。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ODBC_plugin_user_guide.md#_snippet_8\n\nLANGUAGE: Shell\nCODE:\n```\nstrings libDolphinDB.so | grep -i \"openssl 1.\"\n```\n\n----------------------------------------\n\nTITLE: Self-contained Script File for Scheduled Jobs in DolphinDB\nDESCRIPTION: This example shows a properly self-contained script file that includes all necessary function definitions, allowing it to run correctly in a scheduled job.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/scheduledJob.md#_snippet_16\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef foo(){\n\tprint (\"Hello world!\")\n}\nfoo()\n```\n\n----------------------------------------\n\nTITLE: Loading Text Files and Replaying Market Data in DolphinDB (Commented)\nDESCRIPTION: A commented-out alternative function for loading market data from CSV files into memory tables and replaying them. This could be used instead of reading from database tables.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/stock_market_replay/01.stockMarketReplay.txt#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//def loadTextAndReplay(){\n//\torderTable= select * from loadText(\"/yourDataPath/replayData/order.csv\") order by Time\n//\ttradeTable = select * from loadText(\"/yourDataPath/replayData/trade.csv\") order by Time\n//\tsnapshotTable = select * from loadText(\"/yourDataPath/replayData/snapshot.csv\") order by Time\n//\tinputDict = dict([\"order\", \"trade\", \"snapshot\"], [orderTable, tradeTable, snapshotTable])\n//\t\n//\tsubmitJob(\"replay\", \"replay memory table\", replay, inputDict, messageStream, `Date, `Time, , , 1)\n//}\n//loadCSVAndReplay()\n```\n\n----------------------------------------\n\nTITLE: Defining Aggregation Metadata Function (DolphinDB Script)\nDESCRIPTION: Defines a DolphinDB function `createAggMetaCode` that generates SQL aggregation metadata and corresponding output column names based on a dictionary mapping column names to a list of function names. This metadata is used to define the aggregation metrics for stream engines.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/metacode_derived_features/metacode_derived_features_streaming.txt#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef createAggMetaCode(aggDict){\n\tmetaCode = []\n\tmetaCodeColName = []\n\tfor(colName in aggDict.keys()){\n\t\tfor(funcName in aggDict[colName])\n\t\t{\n\t\t\tmetaCode.append!(sqlCol(colName, funcByName(funcName), colName + `_ + funcName$STRING))\n\t\t\tmetaCodeColName.append!(colName + `_ + funcName$STRING)\n\t\t}\n\t}\n\treturn metaCode, metaCodeColName$STRING\n}\n```\n\n----------------------------------------\n\nTITLE: 计算振动信号的均方根（RMS）值的函数实现（DolphinDB 脚本）\nDESCRIPTION: 此函数用于计算振动信号的加速度、速度和位移的 RMS值。输入信号经过归一化、多普勒修正后，利用pwelch函数得到功率谱密度，再计算对应的有效值（RMS），适合振动监测分析。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Random_Vibration_Signal_Analysis_Solution.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndefg rms(nose,N,sensitivity,gain,window, noverlap, nfft, fs,bandwidthL,bandwidthH){\n    if (size(nose)<N){\n        return 0.0,0.0,0.0\n    }\n    temp= nose/sensitivity/gain * 9.8\n    temp=temp-mean(temp)\n    res=pwelch(temp, window, noverlap, nfft, fs)\n    psdAcc=double(res[0])\n    f=double(res[1]) \n    powAcc = psdAcc * f[1]\n    powVel = powAcc / square(2 *pi * f)\n    powDis = powVel / square(2 * pi * f)\n    resolution = fs*1.0 / nfft; \n    bandwidthLidx = int(bandwidthL / resolution) + 1;\n    bandwidthHidx = int(bandwidthH / resolution) + 1;\n    rmsAcc = sqrt(sum(powAcc[(int(bandwidthLidx) - 1):bandwidthHidx]))\n    rmsVel = sqrt(sum(powVel[(int(bandwidthLidx) - 1):bandwidthHidx]))*1000\n    rmsDis = sqrt(sum(powDis[(int(bandwidthLidx) - 1):bandwidthHidx]))*1000000\n    return rmsAcc, rmsVel, rmsDis\n}\n```\n\n----------------------------------------\n\nTITLE: Registering bundleQuery as a Function View in DolphinDB Script\nDESCRIPTION: Ensures the bundleQuery function is registered as a 'function view' in DolphinDB for persistence, allowing pre-defined callable meta-programming routines to survive node restarts for cluster use. Should be executed as admin for cluster-wide effect.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_SQL_Case_Tutorial.md#_snippet_47\n\nLANGUAGE: DolphinDB\nCODE:\n```\naddFunctionView(bundleQuery)\n```\n\n----------------------------------------\n\nTITLE: Aggregating Average Price per Minute/Symbol using `exec pivot by` (Matrix Output) in DolphinDB Script\nDESCRIPTION: Calculates the average price per minute for symbols 'C' and 'IBM', similar to the previous example, but uses `exec` with `pivot by`. This combination returns the pivoted result as a matrix (`resM`) instead of a table, with minutes as rows and symbols as columns.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/panel_data.md#_snippet_7\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nresM = exec avg(price) from t where sym in `C`IBM pivot by minute(timestamp) as minute, sym;\nresM\n```\n\n----------------------------------------\n\nTITLE: Querying Subscription Consumption Information in DolphinDB\nDESCRIPTION: This snippet queries subscription consumption information in DolphinDB. It uses the `getStreamingStat()` function to retrieve streaming statistics and then accesses the `subWorkers` property to obtain information about subscription consumption.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/streaming_capital_flow_order_by_order/07.流计算状态监控函数.txt#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\ngetStreamingStat().subWorkers\n```\n\n----------------------------------------\n\nTITLE: Structured Meta-Programming SQL Generation with DolphinDB Built-in Functions\nDESCRIPTION: Constructs a group-by SQL query using DolphinDB meta-programming helpers such as sqlCol, makeCall (for interval creation), and sqlColAlias, then generates and executes the statement via eval. This approach enables safer, more maintainable dynamic query construction. All dependencies are built-in for DolphinDB scripting.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_SQL_Case_Tutorial.md#_snippet_43\n\nLANGUAGE: DolphinDB\nCODE:\n```\ngroupingCols = [sqlColAlias(makeCall(bar, sqlCol(\"TradeTime\"), duration(min_num.string() + \"m\")), \"minute_TradTime\"), sqlCol(\"SecurityID\"), sqlCol(\"DataDate\")]\nres = sql(select = sqlCol(\"cal_variable\", funcByName(\"avg\"), \"FactorValue\"), \n          from = t, groupBy = groupingCols, groupFlag = GROUPBY).eval()\n```\n\n----------------------------------------\n\nTITLE: Vector-wise Calculations with Array Vectors - DolphinDB Script\nDESCRIPTION: In these examples, a vector is broadcast to each row of an Array Vector or Columnar Tuple, performing element-wise calculations. The length of the vector must match the number of rows of the Array Vector. These arithmetic operations are also available in DolphinDB table expressions. Input constraints require matching lengths, and outputs are Array Vectors with rows calculated element-wise.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Array_Vector.md#_snippet_9\n\nLANGUAGE: DolphinDB\nCODE:\n```\nx = array(INT[], 0).append!([1 2 3, 4 5, 6 7 8, 9 10])\nz = x * [1, 2, 3, 4]\n/* z\n[[1,2,3],[8,10],[18,21,24],[36,40]]\n*/\n\ny = [1 2 3, 4 5, 6 7 8, 9 10].setColumnarTuple!()\nz = y * [1, 2, 3, 4]\n/* z\n([1,2,3],[8,10],[18,21,24],[36,40])\n*/\n\nt = table(1 2 3 4 as id, x as x, y as y)\nnew_t = select *, x*id as new_x, y*id as new_y from t\n/* new_t\nid x       y       new_x      new_y     \n-- ------- ------- ---------- ----------\n1  [1,2,3] [1,2,3] [1,2,3]    [1,2,3]   \n2  [4,5]   [4,5]   [8,10]     [8,10]    \n3  [6,7,8] [6,7,8] [18,21,24] [18,21,24]\n4  [9,10]  [9,10]  [36,40]    [36,40]  \n*/\n```\n\n----------------------------------------\n\nTITLE: Submitting Parallel Job 1 (Multiple Users) in DolphinDB\nDESCRIPTION: This code submits the `parJob1` function as a job named \"parallJob_multi_nine\" under the job group \"parallJob5\" five times in a loop. This is designed for multi-user execution to simulate concurrent job submissions.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_factor.txt#_snippet_15\n\nLANGUAGE: DolphinDB\nCODE:\n```\n/**\n * 提交5个job（多用户）\n */\nfor(i in 0..4){\n\tsubmitJob(\"parallJob5\", \"parallJob_multi_nine\", parJob1)\n}\n```\n\n----------------------------------------\n\nTITLE: Retrieving Reactive State Engine Status - DolphinDB\nDESCRIPTION: This DolphinDB code fragment queries the status of the Reactive State Engine (used for incremental computations on stream data). The getStreamEngineStat().ReactiveStreamEngine API call returns metrics and operational state information, providing insight into the stream processing engine's health and activity.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_auto_sub.md#_snippet_5\n\nLANGUAGE: dolphindb\nCODE:\n```\ngetStreamEngineStat().ReactiveStreamEngine\n```\n\n----------------------------------------\n\nTITLE: Adding a Table to Heap for SQL Reference in DolphinDB Plugin C++\nDESCRIPTION: This snippet demonstrates how to programmatically create a table and register it with the plugin heap for later use in SQL statements within DolphinDB plugin C++ code. It sets up column names and types, instantiates a new table via Util::createTable, and then uses heap->addItem to associate it with a given name. Requires access to Util and heap API, with column names and types passed as vectors. Output is registration of a TableSP under a given name, such as 't', making it accessible in subsequent SQL queries.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_development_tutorial.md#_snippet_23\n\nLANGUAGE: C++\nCODE:\n```\nvector<string> colNames {\"id\", \"x\"};\nvector<DATA_TYPE> colTypes {DT_SYMBOL, DT_DOUBLE};\nTableSP t = Util::createTable(colNames, colTypes, 0, 8);\nheap->addItem(\"t\", t);\n```\n\n----------------------------------------\n\nTITLE: Parsing and Executing SQL Expressions in DolphinDB Plugin C++\nDESCRIPTION: Illustrates executing a SQL statement from within a DolphinDB plugin in C++ by parsing it to bytecode using parseExpr and then evaluating it with eval. The example sets up the SQL string, wraps it in a String object, prepares argument vectors, and invokes the function definitions fetched from the session. Dependencies: ScalarImp.h for string type and proper heap/session setup. Key input: SQL string and properly initialized heap with target table; output: result ConstantSP from SQL execution. Care should be taken to ensure the table referenced in the SQL exists in heap and function names are correct.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_development_tutorial.md#_snippet_24\n\nLANGUAGE: C++\nCODE:\n```\nstring sql = \"select * from t\";\nConstantSP sqlArg =  new String(DolphinString(sql));\nvector<ConstantSP> args{sqlArg};\nObjectSP sqlObj = heap->currentSession()->getFunctionDef(\"parseExpr\")->call(heap, args);\nvector<ConstantSP> evalArgs{sqlObj};\nConstantSP ret = heap->currentSession()->getFunctionDef(\"eval\")->call(heap, evalArgs);\n```\n\n----------------------------------------\n\nTITLE: 生成股票收益率测试数据\nDESCRIPTION: 创建过去十年的股票日收益率模拟数据，用于计算月度波动率。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_SQL_Case_Tutorial.md#_snippet_31\n\nLANGUAGE: DolphinDB\nCODE:\n```\nN = 3653\nt = table(2011.11.01..2021.10.31 as date, \n          take(`AAPL, N) as code, \n          rand([0.0573, -0.0231, 0.0765, 0.0174, -0.0025, 0.0267, 0.0304, -0.0143, -0.0256, 0.0412, 0.0810, -0.0159, 0.0058, -0.0107, -0.0090, 0.0209, -0.0053, 0.0317, -0.0117, 0.0123], N) as rate)\n```\n\n----------------------------------------\n\nTITLE: Write Wide Table Data in DolphinDB\nDESCRIPTION: This function writes factor data in a wide table format to a DolphinDB database. It iterates through a given time range and symbols, submitting jobs to write data for each combination to a node in the cluster. The `wideModelPartitionData` function (not defined in this snippet) is responsible for the actual data writing for a specific partition.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practices_for_multi_factor.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 宽表模式写入某个时间范围数据\ndef writeWideModelData(dbname,tbname,start_date,end_date,symbols,factor_names){\n\ttotal_time_range = getTimeList(start_date,end_date)\n\tnodes = exec value from pnodeRun(getNodeAlias)\n\tfor(j in 0..(total_time_range.size()-1)){\n\t\tfor(i in 0..(symbols.size()-1)){\n\t\t\trpc(nodes[i%(nodes.size())],submitJob,\"wideModel\"+j+\"and\"+i,dbname,wideModelPartitionData,dbname,tbname,total_time_range[j],factor_names,symbols[i])\n\t\t}\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: ElasticSearch Aggregation Query\nDESCRIPTION: This Python code defines an ElasticSearch query that aggregates data. It constructs a JSON payload to group data by 'ts_code' and calculates the average of the 'low' field for each group. It uses `urllib3` for the HTTP request and `json` to encode the data. The output includes the HTTP status and the parsed JSON response with aggregated data. It requires the 'uscsv' index and relevant data present.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_36\n\nLANGUAGE: Python\nCODE:\n```\ndef search_10():\n    http = urllib3.PoolManager()\n    data = json.dumps({\n        \"query\": {\n            \"constant_score\": {\n                \"filter\": {\n                     \"range\": {\n                        \"trade_date\": {\n                            \"gt\": \"2014.01.12\",\n                        }\n                    }\n                }\n            }\n        },\n        \"aggs\": {\n            \"group_by_ts_code\": {\n                \"terms\": {\n                    \"field\": \"ts_code\",\n                    \"size\": 5000\n                },\n                \"aggs\": {\n                    \"avg_price\": {\n                        \"avg\": {\"field\": \"low\"}\n                    }\n                }\n            }\n        },\n        \"_source\": [\"\"]\n    }).encode(\"utf-8\")\n    r = http.request(\"GET\", \"http://localhost:9200/elastic/_search\", body=data,\n                     headers={'Content-Type': 'application/json'})\n    print(r.status)\n    print(json.loads(r.data.decode()))\n```\n\n----------------------------------------\n\nTITLE: Calculating Fund Return Correlations in DolphinDB\nDESCRIPTION: Calculates the pairwise correlation matrix for the daily returns of the previously selected top 50 funds (`returnsMatrix50`) using the `pcross` function with the `corr` aggregator. It then displays the first 3 rows of the resulting correlation matrix and shows how to view the correlation sub-matrix specifically for funds belonging to the 'Stock' type. Depends on `returnsMatrix50` and `fundTypeMap`.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/public_fund_basic_analysis/basic_analysis_of_public_fund_net_value.txt#_snippet_9\n\nLANGUAGE: dolphindb\nCODE:\n```\ncorrMatrix = pcross(corr, returnsMatrix50)\n// view partial results\ncorrMatrix[0:3]\n// view correlation between specified types\ncorrMatrix.loc(fundTypeMap[corrMatrix.rowNames()]==\"股票型\", fundTypeMap[corrMatrix.rowNames()]==\"股票型\")\n```\n\n----------------------------------------\n\nTITLE: DolphinDB Filtering, Grouping, Sorting\nDESCRIPTION: This DolphinDB script demonstrates a more complex query, including filtering, grouping, and sorting. It filters records based on the 'VOL' column. It then groups by 'PERMNO' (stock code) and orders by `PERMNO` in ascending order.  The `timer()` function tests execution over 10 runs. This test evaluates combined operations in DolphinDB.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_45\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\n//按浮点型过滤，按股票代码分组，按股票代码升序排序\ntimer(10) select avg(ASK) from trades where VOL between 800000:1000000 group by PERMNO order by PERMNO asc\n```\n\n----------------------------------------\n\nTITLE: Table Window Calculation with twindow/window in DolphinDB\nDESCRIPTION: Demonstrates how to use `twindow` and `window` functions to perform window calculations on a table, grouping by the 'sym' column and calculating the average bid price within a time window. The example creates a sample table and then applies the window functions to calculate the average bid.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/window_cal.md#_snippet_15\n\nLANGUAGE: DolphinDB\nCODE:\n```\nt2 = table(take(1,10) join take(2,10) as sym, take(09:56:00+1..10,20) as time, (10+(1..10)\\10-0.05) join (20+(1..10)\\10-0.05) as bid, (10+(1..10)\\10+0.05) join (20+(1..10)\\10+0.05) as offer, take(100 300 800 200 600, 20) as volume);\n\n//twindow\nselect *, twindow(avg,t2.bid,t2.time,-6s:1s) from t2 context by sym\n\n//window\nselect *, window(avg, t2.time.indexedSeries(t2.bid), -6s:1s) from t2 context by sym\n```\n\n----------------------------------------\n\nTITLE: Creating Time-Series Stream Engine (DolphinDB Script)\nDESCRIPTION: Creates a time-series stream engine named 'aggrFeatures10min'. It aggregates data over a 10-minute window (`windowSize=600000`, `step=600000`), grouping by `SecurityID`. It uses `snapshotStream` as a dummy table and outputs results to `aggrFeatures10min` using the defined `metrics`. `forceTriggerTime=5` ensures triggers every 5 seconds even with sparse data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/metacode_derived_features/metacode_derived_features_streaming.txt#_snippet_12\n\nLANGUAGE: DolphinDB\nCODE:\n```\ncreateTimeSeriesEngine(name=\"aggrFeatures10min\", windowSize=600000, step=600000, metrics=metrics, dummyTable=snapshotStream, outputTable=aggrFeatures10min, timeColumn=`DateTime, useWindowStartTime=true, keyColumn=`SecurityID, forceTriggerTime = 5)\n```\n\n----------------------------------------\n\nTITLE: Classifying Temperature Data into Levels in DolphinDB Script\nDESCRIPTION: This script demonstrates secondary calculation by classifying temperature data into levels (A, B, C) based on predefined rules. It retrieves temperature data, optionally updates it with random values (commented out in the original but shown here), filters data into three temporary tables based on temperature ranges, adds a 'temperature_level' column to each, assigns the corresponding level, and finally merges them back into a single table 't'.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Iot_intelligent_O&M.md#_snippet_8\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\n//案例 8：指标根据表达式的二次计算\ndevice=\"361RP17\"       //设备编号\npoint=\"361RP17009\"     //测点编号，记录温度的测点，得到的 propertyValue 表示温度\ndt=select * from pt where deviceCode=device and propertyCode=point \nupdate!(dt,`propertyValue,rand(100,dt.size()))\n//取值\na=select * from dt where propertyValue <=60\nb=select * from dt where propertyValue >60 and propertyValue<80\nc=select * from dt where propertyValue >=80\n//分类\na.addColumn(`temperature_level,SYMBOL)\nb.addColumn(`temperature_level,SYMBOL)\nc.addColumn(`temperature_level,SYMBOL)\na[`temperature_level]=\"A\"\nb[`temperature_level]=\"B\"\nc[`temperature_level]=\"C\"\n//合并数据\nt=a.append!(b).append!(c)\n//输出\nselect * from t order by ts\n```\n\n----------------------------------------\n\nTITLE: Remote Creation of Shared Table on DolphinDB Node Using RPC - DolphinDB\nDESCRIPTION: Runs a script on the remote node via the connection object 'h' to create a shared table named 'sales' with date, quantity, and price columns. This enables sharing data seamlessly across nodes in a cluster with the table accessible to other remote calls and distributed computations. The syntax features keyword-based column naming and constant vector assignment.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/hybrid_programming_paradigms.md#_snippet_13\n\nLANGUAGE: DolphinDB\nCODE:\n```\nh(\"share table(2018.07.02 2018.07.02 2018.07.03 as date, 1 2 3 as qty, 10 15 7 as price) as sales\");\n```\n\n----------------------------------------\n\nTITLE: Function View Returning Runtime Logging Table for Entrust Data Load - DolphinDB Script\nDESCRIPTION: This function view 'loadEntrustFV' defines an ETL entrypoint that initializes an empty in-memory table for logging task execution info. It passes this table to the underlying 'loadEntrust' function which populates it during execution. After processing a specified date range based on task type, it returns the info table, enabling visibility of detailed runtime logs in calling environments like DolphinScheduler. This design enables integration of task status and info output directly into task logs.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/dolphinscheduler_integration.md#_snippet_21\n\nLANGUAGE: DolphinDB\nCODE:\n```\nuse stockData::stockDataLoad\ndef loadEntrustFV(userName=\"admin\" , userPassword=\"123456\", startDate = 2023.02.01, endDate = 2023.02.01, dbName = \"dfs://stockData\", tbName = \"entrust\", filePath = \"/hdd/hdd8/ymchen\", loadType = \"daily\")\n{\n    // 定义运行信息表\n    infoTb = table(1:0,[\"info\"] ,[STRING])\n    if(loadType == \"daily\")\n    {\n        sDate = today()\n        eDate = today()\n        // 将运行信息表作为参数传入数据导入函数中，将每次需要输出的信息写入该表\n        loadEntrust(userName, userPassword, sDate, eDate, dbName, tbName, filePath, loadType,infoTb)\n    }\n    else if(loadType == \"batch\")\n    {\n        loadEntrust(userName, userPassword, date(startDate), date(endDate), dbName, tbName, filePath, loadType,infoTb)\n    }\n    // 返回运行信息表\n    return infoTb\n}\n```\n\n----------------------------------------\n\nTITLE: JDBC Connection String with Oracle SQL Dialect - Configuration\nDESCRIPTION: This snippet shows the configuration for a JDBC connection to DolphinDB, specifying the Oracle SQL dialect through the `sqlStd` parameter in the connection URL. It also includes the database path, username, password, and driver class name.  This allows JDBC clients to interact with DolphinDB using Oracle-compatible SQL syntax.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Standard_SQL_in_DolphinDB.md#_snippet_10\n\nLANGUAGE: properties\nCODE:\n```\nspring.datasource.url=jdbc:dolphindb://192.168.1.206:11702?databasePath=dfs://hr&sqlStd=Oracle\nspring.datasource.username=admin\nspring.datasource.password=123456\nspring.datasource.driver-class-name=com.dolphindb.jdbc.Driver\n```\n\n----------------------------------------\n\nTITLE: Creating Partitioned Database DolphinDB\nDESCRIPTION: This code creates a partitioned database named `db` at the specified path (`FP_DB`).  It defines range-based partitioning for the `time` and `id` columns using predefined ranges (`TIME_RANGE` and `ID_RANGE`).  The `database` function with `COMPO` to combine multiple partitions is used, allowing for efficient data storage and retrieval.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs TimescaleDB/test_dolphindb_small.txt#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// ----------------- 创建 readings 分区数据库并定义分区方式\nTIME_RANGE \t= 2016.11.15T00:00:00 + 86400 * 0..4\nID_RANGE \t= ('demo' + lpad((0..10 * 300)$STRING, 6, \"0\"))$SYMBOL\n\ntime_schema   = database('', RANGE, TIME_RANGE)\nid_schema     = database('', RANGE, ID_RANGE)\n\ndb = database(FP_DB, COMPO, [time_schema, id_schema])\n```\n\n----------------------------------------\n\nTITLE: Loading 50 ETF Option Price Data from DolphinDB Table (DolphinDB Script)\nDESCRIPTION: Loads all data for the symbol `510050` from the `optionPrice` table within the `dfs://optionPrice` distributed database into a table variable named `data`. This retrieves the historical daily price data for the specified ETF options.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/IV_Greeks_Calculation_for_ETF_Options_Using_JIT.md#_snippet_1\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\ndata = select * from loadTable(\"dfs://optionPrice\", \"optionPrice\") where sym =`510050\n```\n\n----------------------------------------\n\nTITLE: Dropping DolphinDB Stream Calculation Engine\nDESCRIPTION: Shuts down and removes a previously created stream calculation engine by its name, releasing the resources it was using.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_tutorial.md#_snippet_17\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndropStreamEngine(\"reactiveDemo\")\n```\n\n----------------------------------------\n\nTITLE: Loading Data into Partitioned Memory Table with loadTextEx\nDESCRIPTION: Loads data into a partitioned in-memory table with a specified partitioning scheme using `loadTextEx`.  This method is suitable for frequent sorting and group-by operations on the partition column.  Requires an empty string for the directory and table name in `database` and `loadTextEx` respectively.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/partitioned_in_memory_table.md#_snippet_9\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndb = database(\"\", VALUE, `IBM`MSFT`GM`C`FB`GOOG`V`F`XOM`AMZN`TSLA`PG`S)\ntrades = db.loadTextEx(\"\", `sym, workDir + \"/trades.txt\");\n\ntrades.sortBy!(`qty1);\n\ntrades.sortBy!(`date`qty1, false true);\n\ntrades.sortBy!(<qty1 * price1>, false);\n```\n\n----------------------------------------\n\nTITLE: Editing Controller Node Configuration using Shell and Text Editing\nDESCRIPTION: This snippet shows how to modify the controller.cfg file on a controller node server using Vim via shell commands. The configuration sets the node mode to \"controller\", specifies the local site with IP, port, and alias, and configures high availability features including Raft mode and replication factor. Users must set the localSite parameter according to their environment. Optional publicName can be added for external web access. This configuration is crucial for defining the control node's role and communication parameters within the DolphinDB cluster.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ha_cluster_deployment.md#_snippet_6\n\nLANGUAGE: Shell\nCODE:\n```\nvim ./controller.cfg\n```\n\nLANGUAGE: Shell\nCODE:\n```\nmode=controller\nlocalSite=10.0.0.82:8800:controller3\ndfsReplicationFactor=2\ndfsReplicaReliabilityLevel=1\ndataSync=1\nworkerNum=4\nmaxConnections=512\nmaxMemSize=8\ndfsHAMode=Raft\nlanCluster=0\n```\n\n----------------------------------------\n\nTITLE: Subscribing to Remote Stream Table with Access Control - DolphinDB\nDESCRIPTION: This snippet shows how to subscribe to a stream table on a remote node (NODE2) when access control is enabled. The permission object must be in the format 'nodeAlias:tableName'. Uses `rpc`, `subscribeTable` and `grant`. Requires admin privileges to create user.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Permission_Management.md#_snippet_35\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(`admin, `123456)\ndef pubTable(){\n    share (streamTable(1000:0, `time`sym`volume, [TIMESTAMP, SYMBOL, INT]), `trades)\n    addAccessControl(`trades)\n}\nrpc(`NODE2,pubTable)\nshare streamTable(10000:0, `time`sym`sumVolume, [TIMESTAMP, SYMBOL, INT]) as output1\ncreateUser(`u1, \"111111\");\ngrant(\"u1\", TABLE_READ, \"NODE2:trades\")\nlogin(`u1, \"111111\")\nsubscribeTable(server=`NODE2,tableName=\"trades\", actionName=\"agg1\", offset=0, handler=append!{output1}, msgAsTable=true);\n```\n\n----------------------------------------\n\nTITLE: Replaying Historical Stock Market Data in DolphinDB\nDESCRIPTION: Defines and executes a function to replay historical stock market data from order, trade, and snapshot tables. It creates replay data sources with time repartitioning and submits a job to replay the data into the messageStream.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/stock_market_replay/01.stockMarketReplay.txt#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef replayStockMarketData(){\n\ttimeRS = cutPoints(09:15:00.000..15:00:00.000, 100)\n\torderDS = replayDS(sqlObj=<select * from loadTable(\"dfs://order\", \"order\") where Date = 2020.12.31>, dateColumn=`Date, timeColumn=`Time, timeRepartitionSchema=timeRS)\n\ttradeDS = replayDS(sqlObj=<select * from loadTable(\"dfs://trade\", \"trade\") where Date = 2020.12.31>, dateColumn=`Date, timeColumn=`Time, timeRepartitionSchema=timeRS)\n\tsnapshotDS = replayDS(sqlObj=<select * from loadTable(\"dfs://snapshot\", \"snapshot\") where Date =2020.12.31>, dateColumn=`Date, timeColumn=`Time, timeRepartitionSchema=timeRS)\n\tinputDict = dict([\"order\", \"trade\", \"snapshot\"], [orderDS, tradeDS, snapshotDS])\n\t\n\tsubmitJob(\"replay\", \"replay stock market\", replay, inputDict, messageStream, `Date, `Time, , , 3)\n}\nreplayStockMarketData()\n```\n\n----------------------------------------\n\nTITLE: Elasticsearch Combined Filters\nDESCRIPTION: This Python script defines an Elasticsearch query that uses a boolean filter to combine multiple range conditions.  It filters based on a date range, PERMNO range, and an OR condition applied to a volume range.  The search uses the scroll parameter to retrieve results in batches. The code depends on the `elasticsearch` library. It requires the 'uscsv' index to be present in Elasticsearch.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_49\n\nLANGUAGE: Python\nCODE:\n```\ndef search_3():\n    es = Elasticsearch(['http://localhost:9200/'])\n    page = es.search(\n        index='uscsv',\n        doc_type='type',\n        scroll='2m',\n        size=10000,\n        body={\n            \"query\": {\n                \"constant_score\": {\n                    \"filter\": {\n                        \"bool\": {\n                            \"should\": [\n                                {\n                                    \"bool\": {\n                                        \"must\": [\n                                            {\n                                                \"range\": {\n                                                    \"date\": {\n                                                        \"gte\": \"2015/05/12\",\n                                                        \"lte\": \"2016/05/12\"\n                                                    }\n                                                }\n                                            },\n                                            {\n                                                \"range\": {\n                                                    \"PERMNO\": {\n                                                        \"gte\": \"23240\",\n                                                        \"lte\": \"30000\"\n                                                    }\n                                                }\n                                            }\n                                        ]\n                                    }\n                                },\n                                {\n                                    \"range\": {\n                                        \"VOL\": {\n                                            \"gte\": 1500000,\n                                            \"lte\": 2000000\n                                        }\n                                    }\n                                }\n                            ]\n\n                        }\n                    }\n                }\n            }\n        }\n    )\n    sid = page['_scroll_id']\n    scroll_size = page['hits']['total']\n\n    print(sid)\n    print(scroll_size)\n    while (scroll_size > 0):\n        print(\"Scrolling...\")\n        page = es.scroll(scroll_id=sid, scroll='2m')\n        sid = page['_scroll_id']\n        scroll_size = len(page['hits']['hits'])\n        print(\"scroll size: \" + str(scroll_size))\n```\n\n----------------------------------------\n\nTITLE: Enabling and Managing DolphinDB Systemd Services (Shell)\nDESCRIPTION: These shell commands enable, start, stop, and inspect the status of DolphinDB controller and agent systemd services. 'systemctl' manages these services as background daemons, facilitating automated startup and clean shutdowns. Prerequisites: Service unit files must be installed and reloaded. Inputs: Service management commands. Outputs: Alters or reports service lifecycle state.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ha_cluster_deployment.md#_snippet_20\n\nLANGUAGE: Shell\nCODE:\n```\nsystemctl enable ddbcontroller.service   #配置自启\nsystemctl start ddbcontroller.service  #启动\nsystemctl stop  ddbcontroller.service   #停止服务\nsystemctl status  ddbcontroller.service  #检测状态\n```\n\nLANGUAGE: Shell\nCODE:\n```\nsystemctl enable ddbagent.service   #配置自启\nsystemctl start ddbagent.service  #启动\nsystemctl stop  ddbagent.service   #停止服务\nsystemctl status  ddbagent.service  #检测状态\n```\n\n----------------------------------------\n\nTITLE: Creating and Enabling Persistence for DolphinDB Stream Table\nDESCRIPTION: Creates an in-memory stream table and then shares it globally under a specified name, simultaneously configuring it for disk persistence. `cacheSize` defines the total rows before triggering persistence, and `preCache` specifies the number of most recent rows kept in memory after persistence.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_tutorial.md#_snippet_10\n\nLANGUAGE: DolphinDB\nCODE:\n```\npubTable=streamTable(10000:0,`timestamp`temperature, [TIMESTAMP,DOUBLE])\nenableTableShareAndPersistence(table=pubTable, tableName=`sharedPubTable, cacheSize=1000000, preCache=500000)\n```\n\n----------------------------------------\n\nTITLE: Executing OHLC Calculation for One Day in DolphinDB Script\nDESCRIPTION: Initializes variables for the start date (`calStartDate`), end date (`calEndDate`), database name (`dbName`), and table name (`tbName`). It then calls the previously defined `calOHLCBaseOnSnapshot` function with these parameters to perform the 1-minute OHLC calculation for the specified single day (2023.02.01) and stores the result in the `oneDayResult` variable.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/K.md#_snippet_4\n\nLANGUAGE: dolphindb\nCODE:\n```\ncalStartDate = 2023.02.01\ncalEndDate = 2023.02.01\ndbName = \"dfs://snapshotDB\"\ntbName = \"snapshotTB\"\noneDayResult = calOHLCBaseOnSnapshot(calStartDate, calEndDate, dbName, tbName)\n```\n\n----------------------------------------\n\nTITLE: Database and table definition for stock data\nDESCRIPTION: This DolphinDB script defines a distributed database and a partitioned table for storing stock data from Tushare.  It specifies the database path, partition scheme (RANGE), and table schema including column names and data types. Requires `login` and setting `dbPath` and `yearRange` variables.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nlogin(\"admin\",\"123456\")\ndbPath=\"dfs://tushare\"\nyearRange=date(2008.01M + 12*0..22)\nif(existsDatabase(dbPath)){\n\tdropDatabase(dbPath)\n}\ncolumns1=`ts_code`trade_date`open`high`low`close`pre_close`change`pct_chg`vol`amount\ntype1=`SYMBOL`NANOTIMESTAMP`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE`DOUBLE\ndb=database(dbPath,RANGE,yearRange)\nhushen_daily_line=db.createPartitionedTable(table(100000000:0,columns1,type1),`hushen_daily_line,`trade_date)\n```\n\n----------------------------------------\n\nTITLE: Generating SQL Query with sql function\nDESCRIPTION: This code uses the `sql` function to generate a SQL query based on the parameters initialized in the previous snippet. The `sql` function constructs a SELECT statement by taking parameters that correspond to different clauses such as `select`, `from`, `where`, `groupby`, `csort`, and `limit`.  The output shows the generated SQL query that incorporates these parameters.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/macro_var_based_metaprogramming.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\nsql(select=sel, from=fm, where=wre, groupby=ctxBy, groupFlag=0, csort=cs, limit=lim)\n// output:\n< select cumsum(price) as cum_price from objByName(\"t\") where time between pair(09:00:00, 15:00:00) context by securityID csort time asc limit -1 >\n```\n\n----------------------------------------\n\nTITLE: Migrating DolphinDB Disk Data Between Servers Using scp (Shell)\nDESCRIPTION: Uses 'scp' to recursively copy disk directories with DolphinDB data (such as under /ssd/ssd1/) from one node to another, as part of cluster migration. Ensures continuous data availability after physical relocation. Requires source path existence and write access on the remote target server.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/service_deployment_and_migration.md#_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nscp -r /ssd/ssd1/dolphindb_2 root@172.0.0.2:/ssd/ssd1/;\n\nscp -r /ssd/ssd1/dolphindb_3 root@172.0.0.3:/ssd/ssd1/;\n```\n\n----------------------------------------\n\nTITLE: Loading Stock Data CSV and Measuring Execution Time in Python\nDESCRIPTION: Loads a CSV file containing trading data for a specific stock symbol into a pandas dataframe, then computes the opening bid/ask volume log ratio using the previously defined function. It measures and prints the execution time of the calculation. The code assumes the file path is valid and the CSV follows expected formatting.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/DolphinDB_Python_Parser_Intro_for_Quantitative_Finance/因子实现_Python版本/早盘买卖单大小比.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_csv(\"/ssd/ssd3/data/oneStock_oneFile_TL/20230201/entrust/000001.csv\")\nt0 = time.time()\nres = openBidVolDvdAskVol(df)\nprint(\"cal time: \", time.time() - t0, \"s\")\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Incremental Calculation using Reactive State Engine and cumsum in DolphinDB\nDESCRIPTION: This code snippet shows how to perform incremental calculation using DolphinDB's built-in cumulative aggregation function `cumsum` within a reactive state engine. It calculates the cumulative sum of `deltaValue`, which represents the changes in IOPV. The result is stored in the `IOPVResult` table. `tradeResultDummy` is the dummy table, `BasketID` is the key column, and `keepOrder=true` is set to maintain the order.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_IOPV.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nmetricsResult = [\n    <tradetime>,\n    <cumsum(deltaValue)>]\ncreateReactiveStateEngine(name=\"IOPVResult\", metrics=metricsResult, dummyTable=tradeResultDummy, outputTable=IOPVResult, keyColumn=`BasketID, keepOrder=true)\n```\n\n----------------------------------------\n\nTITLE: Reading Data from Redis and Storing in Table\nDESCRIPTION: This code creates an empty in-memory table `t`. It then loops `n` times, and in each iteration, it uses `redis::run` to execute the Redis `GET` command for a specific key generated from the loop counter. The result from Redis is inserted along with the key into the table `t`, effectively reading back the data previously written by the handler.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/redis_plugin_tutorial/example2.txt#_snippet_5\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nt = table(n:0, [`id, `val], [`string, `string])\nfor(x in 0:n){\n\tinsert into t values(\"key\" + x, redis::run(conn, \"GET\", \"key\" + x))\n}\n```\n\n----------------------------------------\n\nTITLE: Elasticsearch Filtering by Multiple Conditions - Python\nDESCRIPTION: This Python function queries an Elasticsearch index ('elastic') with multiple filter conditions. It uses the `elasticsearch` library to construct a boolean query combining term and range filters on 'ts_code', 'trade_date', and 'high' fields. The function uses scrolling to retrieve results in batches and prints the scroll ID, total hits, and the scroll size for each iteration.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_28\n\nLANGUAGE: Python\nCODE:\n```\ndef search_3():\n    es = Elasticsearch(['http://localhost:9200/'])\n    page = es.search(\n        index='elastic',\n        doc_type='type',\n        scroll='2m',\n        size=10000,\n        body={\n            \"query\": {\n                \"constant_score\": {\n                    \"filter\": {\n                        \"bool\": {\n                            \"should\": [\n                                {\n                                    \"bool\": {\n                                        \"must\": [\n                                            {\"term\": {\"ts_code\": \"002308.SZ\"}},\n                                            {\"term\": {\"trade_date\": \"2015.05.12\"}}\n                                        ]\n                                    }\n                                },\n                                {\n                                    \"range\": {\n                                        \"high\": {\n                                            \"gte\": 15\n                                        }\n                                    }\n                                }\n                            ]\n\n                        }\n                    }\n                }\n            }\n        }\n    )\n    sid = page['_scroll_id']\n    scroll_size = page['hits']['total']\n\n    print(sid)\n    print(scroll_size)\n    # Start scrolling\n    while (scroll_size > 0):\n        print(\"Scrolling...\")\n        page = es.scroll(scroll_id=sid, scroll='2m')\n        # Update the scroll ID\n        sid = page['_scroll_id']\n        # Get the number of results that we returned in the last scroll\n        scroll_size = len(page['hits']['hits'])\n        print(\"scroll size: \" + str(scroll_size))\n```\n\n----------------------------------------\n\nTITLE: Transferring DolphinDB Installation to New Node Using scp (Shell)\nDESCRIPTION: Securely copies the installation directory of DolphinDB (such as dolphindb_3) from an existing node to a new node. The transfer is crucial for expanding the cluster or replacing a failed server, and requires SSH key-based authentication or interactive password entry.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/service_deployment_and_migration.md#_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\nscp /home/dolphindb_3 root@172.0.0.3:/home/dolphindb_3;\n```\n\n----------------------------------------\n\nTITLE: Synchronizing Backup Files Using Shell Commands - DolphinDB Script\nDESCRIPTION: This snippet illustrates executing system shell commands from DolphinDB to synchronize backup directories between machines using rsync over SSH. It constructs a command string concatenating directory paths and SSH login details, then calls shell() to run it. Prerequisites include configured SSH key-based authentication for passwordless login, and availability of rsync on both source and target Linux systems. This method efficiently copies only changed files, enabling offline transmission of backup data for restoration on remote clusters.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/data_synchronization_between_clusters.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\ncmd = \"rsync -av  \" + backupDir + \"/*  \" + userName + \"@\" + restoreIP + \":\" + restoreDir \nshell(cmd)\n```\n\n----------------------------------------\n\nTITLE: Generating Mock Quote Data and In-Memory Table - DolphinDB Script\nDESCRIPTION: This code snippet creates in-memory quote data for various tickers and trading days, simulating prices, volumes, sides, and data sources. Dependencies include the standard DolphinDB environment; key parameters are the number of symbols, dates, and records. The generated temporary table ('tmp') serves as a data source for later database insertion. Inputs are simulated; outputs include an in-memory table with realistic fields. The script assumes sufficient computational resources to handle the generated record size.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/buildData.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//Create a in-memory table\nn=80000\nsymbols=symbol(string(1..500))\ndates=date(datetimeAdd(2020.01.01,0..366,'d'))\nyears=date(datetimeAdd(2020.01M,0..30*12,'M'))\ndate=dates[0]\ntime=rand(timestamp(date)+0..(1000*60*60*23),n*500)\nsyms=take(symbols,n*500)\nprice=randNormal(100,5,n*500)\nvolume=randNormal(1000,5,n*500)\nside=`B`B\nside=symbol(side)\nside=take(side,n*500)\nsources=`tp`tp\nsources=symbol(sources)\nsources=take(sources,n*500)\ntmp=table(time, syms as symbol,price, volume,side,sources as source)\n```\n\n----------------------------------------\n\nTITLE: Migrating Chunks Between Nodes in DolphinDB\nDESCRIPTION: This DolphinDB script defines a function `moveChunks` that migrates all chunk replicas from a specified source node to other nodes in the cluster. It uses `moveReplicas` to perform the actual migration. The script iterates through chunks on the source node, determines a destination node, and initiates the migration via RPC.  The script retrieves all available nodes in the cluster to find a destination for the chunk.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/data_move_rebalance.md#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef moveChunks(srcNode) {\n    chunks = exec chunkId from pnodeRun(getAllChunks) where site = srcNode\n    allNodes = pnodeRun(getNodeAlias).node\n    for(chunk in chunks) {\n        destNode = allNodes[at(not allNodes in (exec site from pnodeRun(getAllChunks) where chunkId = chunk))].rand(1)[0]\n        print(\"From \" + srcNode + \", to \" + destNode + \", moving \" + chunk)\n        rpc(getControllerAlias(), moveReplicas, srcNode, destNode, chunk)\n    }\n}\n\nsrcNode = \"P3-dn1\"\nmoveChunks(srcNode)\n```\n\n----------------------------------------\n\nTITLE: Creating Sample Data for Different Aggregation\nDESCRIPTION: This snippet creates a sample table `t` with columns `tag`, `time`, and `value`. The `tag` column contains labels 'code1', 'code2', and 'code3'. The `time` column contains sorted timestamps. The `value` column contains numerical data. This table is used in the next snippet.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_sql_cases.md#_snippet_22\n\nLANGUAGE: DolphinDB\nCODE:\n```\nN = 1000000\nt = table(\"code\" + string(take(1..3, N)) as tag, \n          sort(take([2021.06.28T00:00:00, 2021.06.28T00:10:00, 2021.06.28T00:20:00], N)) as time, \n          take([1.0, 2.0, 9.1, 2.0, 3.0, 9.1, 9.1, 2.0, 3.0], N) as value)\n```\n\n----------------------------------------\n\nTITLE: 不同去重策略下的存储空间和文件数变化分析\nDESCRIPTION: 对比了在不同去重策略（ALL、FIRST、LAST）条件下，合并前后磁盘占用和 Level File 数量的变化，为性能优化提供依据。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/tsdb_explained.md#_snippet_1\n\n\n\n----------------------------------------\n\nTITLE: Disable DolphinDB Resource Tracking\nDESCRIPTION: This snippet shows how to disable resource tracking on a DolphinDB data node using the `disableResourceTracking()` function. This function can only be used when resource tracking is already enabled (i.e., *resourceSamplingInterval* is a positive integer). It should be executed by an administrator on a data node. This function clears any unsaved SQL query records from memory before disabling the tracking.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/user_level_resource_tracking.md#_snippet_9\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndisableResourceTracking()\n```\n\n----------------------------------------\n\nTITLE: Querying Registered Stream Computing Engine in DolphinDB\nDESCRIPTION: This snippet retrieves information about registered stream computing engines in DolphinDB. It uses the `getStreamEngineStat()` function to access stream engine statistics.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/streaming_capital_flow_order_by_order/07.流计算状态监控函数.txt#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\ngetStreamEngineStat()\n```\n\n----------------------------------------\n\nTITLE: Dynamic SQL Generation with parseExpr and eval in DolphinDB Script\nDESCRIPTION: Forms a SQL query for group-by aggregation as a string, parses it into an expression using parseExpr, and executes it with eval. Demonstrates the classic meta-programming approach for flexible query construction, though without leverage of DolphinDB's higher-level meta functions.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_SQL_Case_Tutorial.md#_snippet_42\n\nLANGUAGE: DolphinDB\nCODE:\n```\nres = parseExpr(\"select \" + avg + \"(cal_variable) as FactorValue from t group by bar(TradeTime, \" + min_num + \"m) as minute_TradeTime, SecurityID, DataDate\").eval()\n```\n\n----------------------------------------\n\nTITLE: Logging into DolphinDB\nDESCRIPTION: This snippet logs into the DolphinDB server using the specified username and password.  It is a prerequisite for executing subsequent commands that require authentication.  The username is 'admin' and the password is '123456'.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/Migrate_data_from_Redshift_to_DolphinDB/Redshift2DDB.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(\"admin\", \"123456\")\n```\n\n----------------------------------------\n\nTITLE: Opening TCP Ports for DolphinDB Cluster in Linux Firewall (console)\nDESCRIPTION: Add firewall rules to allow traffic on TCP ports 8900–8903 (required by DolphinDB nodes). Reload firewall and restart `firewalld` to apply these settings. Validate port requirements based on your cluster topology. Requires root access.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/deploy_dolphindb_on_new_server.md#_snippet_10\n\nLANGUAGE: console\nCODE:\n```\n# firewall-cmd --add-port=8900-8903/tcp --permanent\n# firewall-cmd --reload\n# systemctl restart firewalld\n\n```\n\n----------------------------------------\n\nTITLE: Subscribing Convertible Bonds - DolphinDB\nDESCRIPTION: This code defines a function, `subscribeConvertibleBond`, to subscribe to convertible bond snapshot data.  It first gets the available codes by calling  `amdQuote::getCodeList()`, filters the codes based on market type and security code prefixes to identify convertible bonds, then subscribes to the snapshot stream with the filtered code list. Requires a connection handle and reordered column names.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_implementation_for_AMD_Plugin.md#_snippet_8\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef subscribeConvertibleBond(handle, reorderedColNames) {\n    // 通过代码规律过滤得到可转债代码表\n    codeList = amdQuote::getCodeList()\n    convertibleBondCodeSh = exec securityCode from codeList where marketType = 101 and (securityCode like \"110%\" or securityCode like \"111%\" or securityCode like \"113%\" or securityCode like \"118%\" or securityCode like \"1320%\")\n    convertibleBondCodeSz = exec securityCode from codeList where marketType = 102 and (securityCode like \"123%\" or securityCode like \"127%\" or securityCode like \"128%\")\n    \n    amdQuote::subscribe(handle, `bondSnapshot, objByName(\"snapshot\"), 101, convertibleBondCodeSh, handleSnapshotSubs{reorderedColNames=reorderedColNames})\n    amdQuote::subscribe(handle, `bondSnapshot, objByName(\"snapshot\"), 102, convertibleBondCodeSz, handleSnapshotSubs{reorderedColNames=reorderedColNames})\n}\n```\n\n----------------------------------------\n\nTITLE: Multi-Process Splitting of Stock Data Files into Tasks in Python\nDESCRIPTION: This class manages dividing a list of stock data files into multiple chunks based on available processes or CPU count, facilitating parallel processing. It provides methods to determine the number of jobs and generate argument splits for multiprocessing.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/DolphinDB_Python_Parser_Intro_for_Quantitative_Finance/因子实现_Python版本/价格变动与一档量差的回归系数.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nclass multi_task_split:\n\n    def __init__(self, data, processes_to_use):\n        self.data = data\n        self.processes_to_use = processes_to_use\n\n    def num_of_jobs(self):\n        return min(len(self.data), self.processes_to_use, multiprocessing.cpu_count())\n\n    def split_args(self):\n        q, r = divmod(len(self.data), self.num_of_jobs())\n        return (self.data[i * q + min(i, r): (i + 1) * q + min(i + 1, r)] for i in range(self.num_of_jobs()))\n```\n\n----------------------------------------\n\nTITLE: Cumulative Trade Volume and Amount Aggregation by Order Type - Baseline Approach in DolphinDB\nDESCRIPTION: This snippet groups trades by date, symbol, and trade side, then applies four separate select queries to calculate cumulative volume and amount for small, medium, large, and extra-large orders. Order type thresholds are hardcoded using volume * price filters. Resulting tables are appended to produce a unified summary. The approach is straightforward but contains repeated code per class and can be slower due to multiple queries. Prerequisite: data table 't' as structured above.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_sql_cases.md#_snippet_39\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntimer {\n\t// 小单\n\tresS = select sum(volume) as volume_sum, sum(volume * price) as amount_sum, 0 as type \n\t\t\tfrom t \n\t\t\twhere time <= 10:00:00, volume * price < 40000 \n\t\t\tgroup by date, symbol, side\n\t// 中单\n\tresM = select sum(volume) as volume_sum, sum(volume * price) as amount_sum, 1 as type \n\t\t\tfrom t \n\t\t\twhere time <= 10:00:00, 40000 <= volume * price < 200000 \n\t\t\tgroup by date, symbol, side\n\t// 大单\n\tresB = select sum(volume) as volume_sum, sum(volume * price) as amount_sum, 2 as type \n\t\t\tfrom t \n\t\t\twhere time <= 10:00:00, 200000 <= volume * price < 1000000 \n\t\t\tgroup by date, symbol, side\n\t// 特大单\n\tresX = select sum(volume) as volume_sum, sum(volume * price) as amount_sum, 3 as type \n\t\t\tfrom t \n\t\t\twhere time <= 10:00:00, volume * price >= 1000000 \n\t\t\tgroup by date, symbol, side\n\t\n\tres1 = table(N:0, `date`symbol`side`volume_sum`amount_sum`type, [DATE, SYMBOL, SYMBOL, LONG, DOUBLE, INT])\n\tres1.append!(resS).append!(resM).append!(resB).append!(resX)\n}\n```\n\n----------------------------------------\n\nTITLE: Complete HighCharts Implementation with DolphinDB\nDESCRIPTION: Complete HTML page implementation showing how to integrate DolphinDB data with HighCharts. Shows the alternative implementation using HighCharts library with the same DolphinDB data source.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/web_chart_integration.md#_snippet_5\n\nLANGUAGE: HTML\nCODE:\n```\n<!DOCTYPE html>\n<html>\n<head>\n\t<meta charset=\"utf-8\">\n\t<script src=\"http://code.jquery.com/jquery-1.12.4.min.js\" integrity=\"sha256-ZosEbRLbNQzLpnKIkEdrPv7lOy9C27hHQ+Xp8a4MxAQ=\" crossorigin=\"anonymous\"></script>\n\t<script src=\"DolphinDBConnection.js\"></script>\n\t<script src=\"DolphinDBEntity.js\"></script>\n    <script src=\"http://cdn.highcharts.com.cn/highcharts/highcharts.js\"></script>\n    <script src=\"https://code.highcharts.com.cn/highcharts-plugins/highcharts-zh_CN.js\"></script>\n</head>\n<body>\n\t<div id=\"main\" style=\"width: 600px;height:400px;\"></div>\n\n    <script type=\"text/javascript\">\n    \tvar conn = new DolphinDBConnection('http://localhost:8848');\n\t\t//向DolphinDB发送查询脚本，并获取返回的数据\n\t\tconn.run(\"select avg(ec) as ec from iotTable group by second(time)\", function(re){\n\t\tif(re.resultCode&&re.resultCode==\"1\"){\n\t\t\talert(re.msg);\n\t\t} else {\n\t\t\tjobj = new DolphinDBEntity(re).toVector();//将返回结果转换成列数据\n\t\t\tvar time = jobj[0].value;\n\t\t\tvar ecdata = jobj[1].value;\n\t\t\tvar option = {\n                    chart: {\n                    type: 'line'\n                    },\n                    title: {\n                        text: '温度'\n                    },\n                    xAxis: {\n                        categories: time\n                    },\n                yAxis: {\n                    title: {\n                        text: '温度'\n                    }\n                },\n                 series: [{\n                    name: '平均温度',\n                    data: ecdata\n                }]\n            };\n\t\t\tvar chart = Highcharts.chart('main', option);\n\n\t\t}\n\t});\n    \n\t</script>\n</body>\n</html>\n```\n\n----------------------------------------\n\nTITLE: Configuring AMD Daily Subscription - DolphinDB\nDESCRIPTION: This function, `subscribeAmdDaily`, sets up the daily subscription for convertible bonds and ETF funds. It establishes a connection to the AMD service using `amdQuote::connect`, loads the reordered column names for the snapshot table and then calls the `subscribeConvertibleBond` and `subscribeEtfFund` functions to perform the subscriptions. This uses a connection handle and reordered column names.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_implementation_for_AMD_Plugin.md#_snippet_10\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef subscribeAmdDaily() {\n    option = dict(STRING, ANY)\n    option[`ReceivedTime] = true\n    option[`DailyIndex] = true\n    handle = amdQuote::connect(\n        \"myUsername\", \n        \"myPassword\", \n        [\"110.110.10.10\"], \n        [7890], \n        option\n    )\n\n    reorderedColNames = loadTable(\"dfs://amd\", \"snapshot\").schema().colDefs.name\n    subscribeConvertibleBond(handle, reorderedColNames)\n    subscribeEtfFund(handle, reorderedColNames)\n}\n```\n\n----------------------------------------\n\nTITLE: Login, Configuration, and Model Loading in DolphinDB Script\nDESCRIPTION: Logs into the DolphinDB server using credentials, defines configuration variables for the model path and database/table names, and loads a pre-trained machine learning model from the specified path. These steps prepare the environment for the subsequent stream processing.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/machine_learning_volatility/05.streamComputingArrayVector.txt#_snippet_1\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\n//login account\nlogin(\"admin\", \"123456\")\n\n/**\nmodified location 1: modelSavePath, dbName and tbName\n*/\nmodelSavePath = \"/hdd/hdd9/machineLearning/realizedVolatilityModel_2.00.6.bin\"\ndbName = \"dfs://snapshot_SH_L2_TSDB_ArrayVector\"\ntableName = \"snapshot_SH_L2_TSDB_ArrayVector\"\n\n//load model\nmodel = loadModel(modelSavePath)\n```\n\n----------------------------------------\n\nTITLE: Generating Sample Data for Moving Weighted Average Calculation\nDESCRIPTION: Creates a sample dataset with 3000 stocks and 10000 records per stock for testing moving weighted average calculation methods.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_SQL_Case_Tutorial.md#_snippet_16\n\nLANGUAGE: SQL\nCODE:\n```\nsyms = format(1..3000, \"SH000000\")\nN = 10000\nt = cj(table(syms as symbol), table(rand(100.0, N) as price, rand(10000, N) as volume))\n```\n\n----------------------------------------\n\nTITLE: Cleaning the DolphinDB Stream Processing Environment - DolphinDB\nDESCRIPTION: Defines the cleanEnvironment function to unsubscribe and drop previously created stream tables, stream engines, and associated actions to ensure a clean state before table creation. Accepts a 'parallel' parameter for batch cleanup. Removes any data or processes that may interfere with new stream publishing workflows. Assumes access to tables, engines, and utilities defined in DolphinDB server environment.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/streaming_capital_flow_order_by_order/03.清理环境并创建相关流数据表.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef cleanEnvironment(parallel){\n\tfor(i in 1..parallel){\n\t\ttry{ unsubscribeTable(tableName=`tradeOriginalStream, actionName=\"tradeProcess\"+string(i)) } catch(ex){ print(ex) }\n\t\ttry{ unsubscribeTable(tableName=`tradeProcessStream, actionName=\"tradeTSAggr\"+string(i)) } catch(ex){ print(ex) }\n\t\ttry{ dropStreamEngine(\"tradeProcess\"+string(i)) } catch(ex){ print(ex) }\n\t\ttry{ dropStreamEngine(\"tradeTSAggr\"+string(i)) } catch(ex){ print(ex) }\n\t}\n\ttry{ unsubscribeTable(tableName=`tradeOriginalStream, actionName=\"tradeToDatabase\") } catch(ex){ print(ex) }\n\ttry{ dropStreamTable(`tradeOriginalStream) } catch(ex){ print(ex) }\n\ttry{ dropStreamTable(`tradeProcessStream) } catch(ex){ print(ex) }\n\ttry{ dropStreamTable(`capitalFlowStream) } catch(ex){ print(ex) }\n\tundef all\n}\n//calculation parallel, developers need to modify according to the development environment\nparallel = 3\ncleanEnvironment(parallel)\ngo\n```\n\n----------------------------------------\n\nTITLE: Creating Synchronization Configuration Table in DolphinDB\nDESCRIPTION: Creates a configuration table that maps Kafka topics to DolphinDB distributed tables. This table is essential for the connector to know where to store data from each topic.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Debezium_and_Kafka_data_sync.md#_snippet_19\n\nLANGUAGE: SQL\nCODE:\n```\ndbName = \"dfs://ddb_sync_config\"\nif(existsDatabase(dbName)){\n    dropDatabase(dbName)\n}\ndb=database(dbName, HASH, [SYMBOL, 5])\n\nif(existsTable(dbName, \"sync_config\"))\n    db.dropTable(\"sync_config\")\nmtable=table(100:0, `connector_name`topic_name`target_db`target_tab, [SYMBOL,SYMBOL,SYMBOL,SYMBOL]);\ndb.createTable(table=mtable, tableName=\"sync_config\")\n```\n\n----------------------------------------\n\nTITLE: Data Import/Export and Grouping in pandas and DolphinDB\nDESCRIPTION: This snippet illustrates functions for reading and writing data formats like CSV and JSON, as well as grouping and aggregation functions. It facilitates data I/O and summarization routines in pandas, with DolphinDB equivalents ensuring seamless data ingestion and analysis.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/function_mapping_py.md#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\npandas.read_csv  # Load CSV files\npandas.to_csv  # Save DataFrame to CSV\npandas.read_json  # Load JSON data\npandas.DataFrame.to_json  # Export DataFrame as JSON\npandas.DataFrame.groupby.aggFunc  # Perform aggregation operations\n```\n\n----------------------------------------\n\nTITLE: Creating agent.sh Script for DolphinDB Agent Node\nDESCRIPTION: Shell script to manage DolphinDB agent mode, including starting and stopping the process, setting environment variables, and redirecting output logs. Dependencies include the DolphinDB executable and configuration files. It uses similar process management as `controller.sh`.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/multi_machine_cluster_deployment.md#_snippet_13\n\nLANGUAGE: Bash\nCODE:\n```\n#!/bin/bash\n#agent.sh\n\nworkDir=$PWD\n\nstart(){\n    cd ${workDir} && export LD_LIBRARY_PATH=$(dirname \"$workDir\"):$LD_LIBRARY_PATH\n    nohup ./../dolphindb -console 0 -mode agent -home data -script dolphindb.dos -config config/agent.cfg -logFile log/agent.log  > agent.nohup 2>&1 &\n}\n\nstop(){\n    ps -o ruser=userForLongName -e -o pid,ppid,c,time,cmd |grep dolphindb|grep -v grep|grep $USER|grep agent| awk '{print $2}'| xargs kill -TERM\n}\n\ncase $1 in\n    start)\n        start\n        ;;\n    stop)\n        stop\n        ;;\n    restart)\n        stop\n        start\n        ;;\nesac\n```\n\n----------------------------------------\n\nTITLE: Defining a DolphinDB Module with Conflicting Function Name\nDESCRIPTION: Defines a module named `sys` containing a function also named `myfunc`, which returns 3. This module is used to demonstrate the naming conflict that occurs when a module containing a function with the same name as a locally defined function or function view is imported.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/module_tutorial.md#_snippet_14\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nmodule sys\ndef myfunc(){\n return 3\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Expression with Aggregate Function and sqlCol\nDESCRIPTION: The snippet shows an SQL expression where the `sqlCol` function is combined with an aggregate function (`sum`) and an alias.  The `sqlCol` function is used to specify the column to apply the sum function to along with a new alias. The resulting SQL expression would represent an aggregate operation on a single column and provides an alias for the result.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/macro_var_based_metaprogramming.md#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// sqlCol(\"col\", func=sum, alias=\"newCol\") --> <sum(col) as newCol>\n```\n\n----------------------------------------\n\nTITLE: Subscribing AMD Data - DolphinDB\nDESCRIPTION: This code snippet demonstrates subscribing to AMD real-time snapshot data using the `amdQuote::subscribe` function. It sets up the subscription to the `snapshot` stream table, specifies the market types (101 for Shanghai, 102 for Shenzhen), and uses partial application for `handleSnapshotSubs` function with reordered column names.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_implementation_for_AMD_Plugin.md#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// AMD 订阅，以 snapshot为例\nreorderedColNames = loadTable(dbName, snapshotTbName).schema().colDefs.name\namdQuote::subscribe(handle, `snapshot, snapshot, 101, , handleSnapshotSubs{reorderedColNames=reorderedColNames})\namdQuote::subscribe(handle, `snapshot, snapshot, 102, , handleSnapshotSubs{reorderedColNames=reorderedColNames})\n```\n\n----------------------------------------\n\nTITLE: Checking DolphinDB Backup/Restore Job Status\nDESCRIPTION: Demonstrates how an administrator, after logging in, can use `getBackupStatus` to view the progress and status of ongoing or recently completed backup and restore tasks across all users. The output includes details like involved databases/tables, partition counts, completion percentage, and estimated/actual end times.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/backup-restore-new.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(`admin, `123456)\ngetBackupStatus()\n```\n\n----------------------------------------\n\nTITLE: Executing Transaction Table Creation in DolphinDB\nDESCRIPTION: This snippet demonstrates how to use the transacCreate function to create a partitioned database. It defines the database and table names and calls the function to create the schema.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/appendices_market_replay_bp/transac_create.txt#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndbName = \"dfs://Test_transaction\"\ntbName = \"transaction\"\ntransacCreate(dbName, tbName)\n```\n\n----------------------------------------\n\nTITLE: Configuring DolphinDB Controller Node in controller.cfg File\nDESCRIPTION: Illustrates the configuration parameters for a DolphinDB cluster control node within the controller.cfg file. Key configurations include mode, localSite (IP, port, alias), DFS replication and high availability mode (Raft), worker thread number, max connections/memory size, and LAN cluster flag. Parameters must be tailored to the actual server environment. Also describes the optional publicName parameter for enabling external web management access.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ha_cluster_deployment.md#_snippet_2\n\nLANGUAGE: config\nCODE:\n```\nmode=controller\nlocalSite=10.0.0.80:8800:controller1\ndfsReplicationFactor=2\ndfsReplicaReliabilityLevel=1\ndataSync=1\nworkerNum=4\nmaxConnections=512\nmaxMemSize=8\ndfsHAMode=Raft\nlanCluster=0\n```\n\nLANGUAGE: config\nCODE:\n```\npublicName=19.56.128.21\n```\n\n----------------------------------------\n\nTITLE: QR Decomposition with economic mode (m<=n) in DolphinDB\nDESCRIPTION: Demonstrates QR decomposition of a matrix in DolphinDB using the `qr` function with `mode='economic'` and `pivoting=false`.  The function decomposes a matrix X into an orthogonal matrix Q and an upper triangular matrix R. It shows the result of Q and R, and if m <= n, the result is the same as 'full' mode.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/matrix.md#_snippet_17\n\nLANGUAGE: DolphinDB\nCODE:\n```\n>m=matrix([2 5 7 5, 5 2 5 4, 8 2 6 4]);\n>m;\n#0 #1 #2\n-- -- --\n2  5  8 \n5  2  2 \n7  5  6 \n5  4  4  \n\n>q,r=qr(m,mode='economic'); //pivoting=false\n>q;\n#0        #1        #2       \n--------- --------- ---------\n-0.197066 0.903357  0.300275 \n-0.492665 -0.418267 0.459245 \n-0.68973  -0.02475  0.170745 \n-0.492665 0.091573  -0.818398\n>r;\n#0         #1       #2       \n---------- -------- ---------\n-10.148892 -7.38997 -8.670898\n0          3.922799 6.608121 \n0          0        1.071571 \n\n```\n\n----------------------------------------\n\nTITLE: Creating Vectors and Matrices in DolphinDB Plugin\nDESCRIPTION: Illustrates how to create vectors and matrices of various types within a DolphinDB plugin using the utility functions provided in `Util.h`.  It demonstrates creating an integer vector, an ANY-type vector (tuple), an index vector, and a double matrix, and how to manipulate their elements.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_development_tutorial.md#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nVectorSP v = Util::createVector(DT_INT, 10);     // 创建一个初始长度为 10 的 INT 类型向量\nv->setInt(0, 60);                                // 相当于 v[0] = 60\n\nVectorSP t = Util::createVector(DT_ANY, 0);      // 创建一个初始长度为 0 的 ANY 类型向量（元组）\nt->append(new Int(3));                           // 相当于 t.append!(3)\nt->get(0)->setInt(4);                            // 相当于 t[0] = 4\n// 这里不能用 t->setInt(0, 4)，因为 t 是一个元组，setInt(0, 4) 只对 INT 类型的向量有效\n\nConstantSP seq = Util::createIndexVector(5, 10); // 相当于 5..14\nint seq0 = seq->getInt(0);                       // 相当于 seq[0]\n\nConstantSP mat = Util::createDoubleMatrix(5, 10);// 创建一个 10 行 5 列的 DOUBLE 类型矩阵\nmat->setColumn(3, seq);                          // 相当于 mat[3] = seq\n```\n\n----------------------------------------\n\nTITLE: 分布式列均值Final - C++\nDESCRIPTION: 此C++代码片段定义了`columnAvgFinal`函数，它是`columnAvg` 分布式计算的 final 函数。 它接受 `reduce` 函数的计算结果（数据总和和非空元素个数）作为输入，然后计算平均值（总和除以计数）。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_development_tutorial.md#_snippet_10\n\nLANGUAGE: cpp\nCODE:\n```\nConstantSP columnAvgFinal(const ConstantSP &result, const ConstantSP &placeholder) {\n    double sum = result->get(0)->getDouble();\n    int count = result->get(1)->getInt();\n\n    return new Double(sum / count);\n}\n```\n\n----------------------------------------\n\nTITLE: Populating Synchronization Configuration in DolphinDB\nDESCRIPTION: Inserts configuration data that maps MySQL tables' corresponding Kafka topics to specific DolphinDB distributed tables, establishing the relationship for data synchronization.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Debezium_and_Kafka_data_sync.md#_snippet_20\n\nLANGUAGE: SQL\nCODE:\n```\nsync_config=loadTable(\"dfs://ddb_sync_config\",\"sync_config\");\ntmp_tab=table(100:0,`connector_name`topic_name`target_db`target_tab, [SYMBOL,SYMBOL,SYMBOL,SYMBOL]);\ninsert into tmp_tab (connector_name,topic_name,target_db,target_tab) values (\"ddb-sink\",\"mysqlserver.basicinfo.index_components\",\"dfs://index_data\",\"index_components\");\ninsert into tmp_tab (connector_name,topic_name,target_db,target_tab) values (\"ddb-sink\",\"mysqlserver.basicinfo.stock_basic\",\"dfs://wddb\",\"stock_basic\");\nsync_config.append!(tmp_tab);\n```\n\n----------------------------------------\n\nTITLE: Querying All Fields in Vertical Table\nDESCRIPTION: This query retrieves all fields for a specific stock (symbol) from a vertical table named `tsdb_min_factor`.  The `*` wildcard is used to select all available columns, demonstrating a general approach to data retrieval in DolphinDB.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practice_for_factor_calculation.md#_snippet_23\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//纵表查询sql, 查询全部字段，使用通配符*\ntsdb_symbol_all=select  * from tsdb_min_factor where symbol=`sz000056\n```\n\n----------------------------------------\n\nTITLE: Backup Data Node Metadata\nDESCRIPTION: Backup command for data node metadata directory, safeguarding essential data before upgrading.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/gray_scale_upgrade_ha.md#_snippet_9\n\nLANGUAGE: Shell\nCODE:\n```\ncd /home/zjchen/HA/server/clusterDemo/data/HActl44/\ncp -r dfsMeta/ dfsMeta_bak/\n```\n\n----------------------------------------\n\nTITLE: Creating a Compound Index in MongoDB\nDESCRIPTION: Switches to the 'device_pt' database and creates a compound ascending index on the 'date' and 'symbol' fields within the 'device_readings' collection to optimize queries.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs MongoDB/mongodb_taq_partitioned.txt#_snippet_1\n\nLANGUAGE: mongodb\nCODE:\n```\nUse device_pt\ndb.device_readings.createIndex({date:1,symbol:1})\n```\n\n----------------------------------------\n\nTITLE: Dynamically Adjusting OLAP Cache Engine Size in DolphinDB\nDESCRIPTION: The setOLAPCacheEngineSize function allows changing the OLAP cache engine capacity at runtime without restarting the server, making it useful for performance tuning.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/redoLog_cacheEngine.md#_snippet_4\n\nLANGUAGE: SQL\nCODE:\n```\nsetOLAPCacheEngineSize(newSizeInGB)\n```\n\n----------------------------------------\n\nTITLE: Testing MySQL ODBC Connection with isql\nDESCRIPTION: This command attempts to connect to a MySQL database using the `isql` utility with the specified driver, server address, port, user ID, password, and database name.  It's used to verify basic ODBC connectivity. A failed connection indicates network or authentication problems.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ODBC_plugin_user_guide.md#_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nisql -v -k \"Driver=MySQL ODBC 8.0 Unicode Driver;Server=192.168.1.38;Port=3305;Uid=root;Pwd=123456;Database=Test\"\n[S1000][unixODBC][MySQL][ODBC 8.0(w) Driver]Can't connect to MySQL server on '192.168.1.38' (111)\n[ISQL]ERROR: Could not SQLDriverConnect\n```\n\n----------------------------------------\n\nTITLE: Querying TopN Cumulative Sum over Return-ranked Records - DolphinDB Script\nDESCRIPTION: Demonstrates cumsumTopN to produce a running total of volume for each stock, considering only the top 3 records (so far) by return (ratios(close)). Output is cumulative, showing historical maximums per window. Requires appropriate input columns in table t. Output is a per-stock cumulative Top3 volume sum for reporting and analytics.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_TopN.md#_snippet_9\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect windCode, tradingTime, cumsumTopN(volume, ratios(close), 3, false) as cumsumTop3Volume from t context by windCode\n```\n\n----------------------------------------\n\nTITLE: Main Execution: Setup and Start Loading Order Book Data in DolphinDB\nDESCRIPTION: The script sets variables for the database name, table name, file path, and a sample CSV file path, then derives the schema by calling getSchema on the sample CSV. It finally triggers the recursive loading process for the whole year’s data via loopLoadOneYearFiles, initializing the complete data import workflow.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/csvImportDemo/importOldData.txt#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndbName = \"dfs://stocks_orderbook\"\ntableName = \"orderBook\"\nfilePath=\"/hdd/hdd9/data/quotes/2013\" \ncsv=\"/hdd/hdd9/data/quotes/2013/20130104/SH000001_20130104.csv\" \nschema1 =  getSchema(csv)\nloopLoadOneYearFiles(dbName,tableName, filePath,schema1)\n```\n\n----------------------------------------\n\nTITLE: Calculating Closing Period Trading Volume Percentage in DolphinDB\nDESCRIPTION: This code defines a function to calculate the percentage of trading volume that occurs during the closing period (14:30-15:00) of a trading day, then applies it to trade data using a timer function. The script loads data from a distributed table and groups results by trading date and security ID.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/DolphinDB_Python_Parser_Intro_for_Quantitative_Finance/因子实现_DolphinDB版本/当日尾盘成交占比.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef beforeClosingVolumePercent(TradeTime, TradeQty){\n\treturn sum(TradeQty*(time(TradeTime) between 14:30:00.000 : 15:00:00.000)) \\ sum(TradeQty)\n}\n\ntimer{\n\ttradeTB = loadTable(\"dfs://TL_Level2\", \"trade\")\n\tres = select beforeClosingVolumePercent(TradeTime, TradeQty) as BCVP\n\t\tfrom tradeTB\n\t\twhere date(TradeTime)=2023.02.01\n\t\tgroup by date(TradeTime) as TradeDate, SecurityID\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Expression with nullFill function, sqlCol and makeCall\nDESCRIPTION: This example uses `sqlColAlias`, `makeCall`, and `sqlCol` functions to construct the SQL expression. The primary function here is `nullFill`, and it takes two arguments: the column name (`colName`) and the result of `quantile` function. The column name itself is dynamically provided and the result of the `quantile` function determines the value to use for null replacement.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/macro_var_based_metaprogramming.md#_snippet_9\n\nLANGUAGE: DolphinDB\nCODE:\n```\ncolName=`price\nsqlColAlias(makeCall(nullFill, sqlCol(colName), makeUnifiedCall(quantile, (sqlCol(colName), 0.5))), colName)\n```\n\n----------------------------------------\n\nTITLE: Merging Matrices with 'asof' and 'outer' joins - DolphinDB\nDESCRIPTION: This code demonstrates how to merge two matrices (m1 and m2) using the 'asof' and 'outer' join methods in DolphinDB. The merge function combines the matrices based on their row labels, similar to table joins. 'asof' join returns the nearest previous match, while 'outer' join returns all rows from both matrices.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/matrix.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\n>m1 = matrix([1.2, 7.8, 4.6, 5.1, 9.5], [0.15, 1.26, 0.45, 1.02, 0.33]).rename!([2012.01.01, 2015.02.01, 2015.03.01, 2015.04.01, 2015.05.01], `x1`x2).setIndexedMatrix!()\n>m2 = matrix([1.0, 2.0, 3.0, 4.0], [0.14, 0.26, 0.35, 0.48]).rename!([2015.02.01, 2015.02.16, 2015.05.01, 2015.05.02], `y1`y2).setIndexedMatrix!()\n\n>m1;\nlabel\tx1\tx2\ty1\ty2\n2012.01.01\t1.2\t0.15\t\t\n2015.02.01\t7.8\t1.26\t1\t0.14\n2015.03.01\t4.6\t0.45\t2\t0.26\n2015.04.01\t5.1\t1.02\t2\t0.26\n2015.05.01\t9.5\t0.33\t3\t0.35\n\n>m2;\nlabel\ty1\ty2\n2015.02.01\t1\t0.14\n2015.02.16\t2\t0.26\n2015.05.01\t3\t0.35\n2015.05.02\t4\t0.48\n\n>merge(m1, m2, 'asof');\nlabel\tx1\tx2\ty1\ty2\n2012.01.01\t1.2\t0.15\t\t\n2015.02.01\t7.8\t1.26\t1\t0.14\n2015.03.01\t4.6\t0.45\t2\t0.26\n2015.04.01\t5.1\t1.02\t2\t0.26\n2015.05.01\t9.5\t0.33\t3\t0.35\n\n>merge(m1, m2, 'outer');\nlabel\tx1\tx2\ty1\ty2\n2012.01.01\t1.2\t0.15\t\t\n2015.02.01\t7.8\t1.26\t1\t0.14\n2015.02.16\t\t\t2\t0.26\n2015.03.01\t4.6\t0.45\t\t\n2015.04.01\t5.1\t1.02\t\t\n2015.05.01\t9.5\t0.33\t3\t0.35\n2015.05.02\t\t\t4\t0.48\n```\n\n----------------------------------------\n\nTITLE: Dropping DolphinDB Stream Table and Persistence Data\nDESCRIPTION: Deletes the specified stream table entirely from the system. This operation removes the in-memory table definition and all corresponding persisted data from disk.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_tutorial.md#_snippet_15\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndropStreamTable(`pubTable);\n```\n\n----------------------------------------\n\nTITLE: Transferring DolphinDB Installation Packages Between Servers (Shell)\nDESCRIPTION: Transmits DolphinDB installation directories from the current host to remote servers via 'scp', facilitating cluster expansion or migration. Requires SSH access to target machines and ensures the receiving directories exist and have appropriate write permissions. Inputs are source/target paths; outputs are remote copies of the installation packages.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/service_deployment_and_migration.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nscp /home/dolphindb_2 root@172.0.0.2:/home/dolphindb_2;\n\nscp /home/dolphindb_3 root@172.0.0.3:/home/dolphindb_3;\n```\n\n----------------------------------------\n\nTITLE: Deleting Data from a Table in DolphinDB\nDESCRIPTION: This code snippet demonstrates how to delete data from a table named 't' in DolphinDB using the SQL delete statement. It deletes rows where the value of column 'id' is 3.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Single_Node_Tutorial.md#_snippet_11\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndelete from t where id=3\n```\n\n----------------------------------------\n\nTITLE: Offline package collection and transfer on Windows\nDESCRIPTION: Uses conda to create a package cache with specific packages, then compresses and transfers the package directory to an offline environment for later setup.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/python_api_install_offline.md#_snippet_7\n\nLANGUAGE: Shell Script\nCODE:\n```\nconda create -n test38 numpy=1.22.3 future pandas python=3.8.13 --download-only\n# Transfer pkgs directory to offline machine\n# Verify integrity with md5sum\nmd5sum pkgs.tar.gz.md5\n# Extract in offline environment\ntar -zxvf pkgs.tar.gz\n```\n\n----------------------------------------\n\nTITLE: N-to-One Heterogeneous Multi-Table Replay in DolphinDB Script\nDESCRIPTION: Streams multiple tables with differing schemas into a single heterogeneous output stream table, preserving strict time order across all input sources. Output table must be a heterogeneous stream table, and the example shows slicing input tables with replayDS and merging them into a unified replay via the replay function with parallelization enabled. Suits use cases like multi-asset event replay for consolidated analytics.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/historical_data_replay.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\nds1 = replayDS(<select * from input1>, `date, `time, 08:00:00.000 + (1..10) * 3600000)\nds2 = replayDS(<select * from input2>, `date, `time, 08:00:00.000 + (1..10) * 3600000)\nds3 = replayDS(<select * from input3>, `date, `time, 08:00:00.000 + (1..10) * 3600000)\nreplay([ds1, ds2, ds3], outputTable, `date, `time, 1000, true, 2)\n```\n\n----------------------------------------\n\nTITLE: Resetting Another User's Password in DolphinDB - DolphinDB\nDESCRIPTION: This snippet shows a DolphinDB admin performing a password reset for another user. Usage of 'login' and 'resetPwd' functions requires administrator privileges. Parameters are the admin credentials, target username, and the new password. Upon reset, the target user can log in with the modified password.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Permission_Management.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(\"admin1\",\"123456\")//登陆管理员用户\nresetPwd(\"user1\",\"123456\")//修改其他用户密码\nlogin(\"user1\",\"123456\")//修改密码成功\n```\n\n----------------------------------------\n\nTITLE: Starting DolphinDB Controller Node with HTTPS Enabled (Bash)\nDESCRIPTION: This snippet demonstrates how to start a DolphinDB controller node with HTTPS enabled and specifies the public domain name, home directory, operation mode, and log file destination using command line options. Dependencies include a valid DolphinDB installation and network access to the specified address. Key parameters: -enableHTTPS (enables HTTPS), -home (specifies the master directory), -publicName (sets the domain name for the certificate), -mode (sets to controller mode), -localSite (binds the local network address and ports), and -logFile (defines the log output location). The command must be executed on the intended control node server and assumes required directories and permissions.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ACL_and_Security.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./dolphindb -enableHTTPS true -home master -publicName www.ABCD.com -mode controller -localSite 192.168.1.30:8500:rh8500 -logFile  ./log/master.log\n```\n\n----------------------------------------\n\nTITLE: Implementing Alpha98 Factor Calculation Using pandas in Python\nDESCRIPTION: This Python code implements Alpha98 factor computation on panel data using pandas DataFrames. It includes helper functions for row-wise ranking, minimum index selection, and a layered computation employing pandas' rolling, mean, corr, and apply capabilities. Input: vwap, open, vol as pandas DataFrames indexed by instrument and time. Output: DataFrame or array of Alpha98 values. Dependencies: pandas, numpy. Limitations: much slower than DolphinDB for large datasets. Code is best run in an environment with sufficient memory for large DataFrames.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/panel_data.md#_snippet_32\n\nLANGUAGE: Python\nCODE:\n```\ndef myrank(x):\n    return ((x.rank(axis=1,method='min'))-1)/x.shape[1]\n\ndef imin(x):\n    return np.where(x==min(x))[0][0]\n\n\ndef rank(x):\n    s = pd.Series(x)\n    return (s.rank(ascending=True, method=\"min\")[len(s)-1])-1\n\n\ndef alpha98(vwap, open, vol):\n    return myrank(vwap.rolling(5).corr(vol.rolling(5).mean().rolling(26).sum()).rolling(7).apply(lambda x: np.sum(np.arange(1, 8)*x)/np.sum(np.arange(1, 8)))) - myrank((9 - myrank(open).rolling(21).corr(myrank(vol.rolling(15).mean())).rolling(9).apply(imin)).rolling(7).apply(rank).rolling(8).apply(lambda x: np.sum(np.arange(1, 9)*x)/np.sum(np.arange(1, 9))))\n```\n\nLANGUAGE: Python\nCODE:\n```\nstart_time = time.time()\nre=alpha98(vwap, open, vol)\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\n```\n\n----------------------------------------\n\nTITLE: Using a Field Series in SQL with select\nDESCRIPTION: This snippet demonstrates the use of a field series (e.g., `col1...coln`) in a `select` statement. It shows how to select a range of columns from the table. It also demonstrates the use of an alias for the columns.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/macro_var_based_metaprogramming.md#_snippet_17\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect col1 ... coln from t\nselect col1...col3 as nm1 ... nm3 from t\n```\n\n----------------------------------------\n\nTITLE: Define Open Price Factor (Incremental Calculation)\nDESCRIPTION: Defines a function named `Open` that returns the opening price.  It utilizes the `first` aggregate function to retrieve the first trade price (`LastPx`) within a one-minute window.  The platform parses the string \"first(LastPx)\" as meta code and passes it to the time-series engine. This example illustrates how to express the incremental calculation of the open price using the first trade price.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Level2_Snapshot_Factor_Calculation.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef Open(){\n\treturn \"first(LastPx)\"\n}\n```\n\n----------------------------------------\n\nTITLE: Simulating Continuous Data Insertion and Engine Recreation with Snapshot in DolphinDB\nDESCRIPTION: This snippet simulates data ingestion, engine unsubscription, and recreation from a snapshot offset. It inserts new data with timestamps after the last processed message, unsubscribes and deletes the engine, then recreates the engine and resumes subscription starting from the saved message ID offset to show that data processing continues seamlessly with snapshots.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/stream_aggregator.md#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\nn=500\ntimev=timestamp(501..1000) + 2021.03.12T15:00:00.000\nsymv = take(`abc`def, n)\npricev = int(1..n)\nid = take(-1, n)\ninsert into trades values(timev, symv, pricev, id)\n```\n\n----------------------------------------\n\nTITLE: Define Low Price Factor (Incremental Calculation)\nDESCRIPTION: Defines a function named `Low` that returns the lowest price. It utilizes the `min` aggregate function to retrieve the minimum trade price (`LastPx`) within a one-minute window. The platform parses the string \"min(LastPx)\" as meta code and passes it to the time-series engine. This example illustrates how to express the incremental calculation of the lowest price using the minimum trade price.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Level2_Snapshot_Factor_Calculation.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef Low(){\n\treturn \"min(LastPx)\"\n}\n```\n\n----------------------------------------\n\nTITLE: Filtering Trades by Date - DolphinDB\nDESCRIPTION: This DolphinDB script filters the \"trades\" table based on the \"trade_date\" column.  It selects all records where the trade date is less than or equal to January 11, 2014. The `timer(10)` function executes the query 10 times and measures the execution time. The `clearAllCache()` function clears the cache before each execution.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_15\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//根据时间过滤\ntimer(10) select * from trades where trade_date <= 2014.01.11\n```\n\n----------------------------------------\n\nTITLE: Find most correlated stocks\nDESCRIPTION: This DolphinDB script finds the 10 most correlated stocks for each stock based on the correlation matrix.  It transforms the correlation matrix into a table, renames the columns, unpivots the table, and then uses `context by` and `rank` to find the top 10 correlated stocks. It requires `corrMatrix` and `daily_line` tables to be defined.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\nsyms=(exec count(*) from daily_line group by ts_code).ts_code\nsyms=\"C\"+strReplace(syms, \".\", \"_\")\nmostCorrelated=select * from table(corrMatrix.columnNames() as ts_code, corrMatrix).rename!([`ts_code].append!(syms)).unpivot(`ts_code, syms).rename!(`ts_code`corr_ts_code`corr) context by ts_code having rank(corr,false) between 1:10\n```\n\n----------------------------------------\n\nTITLE: Cleaning DolphinDB Environment (DolphinDB Script)\nDESCRIPTION: Unsubscribes from specific stream tables and drops named stream engines and tables to ensure a clean state before starting the script execution. Uses `try...catch` blocks to prevent script termination if an object does not exist.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/metacode_derived_features/metacode_derived_features_streaming.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef cleanEnvironment(){\n\ttry{ unsubscribeTable(tableName=`snapshotStream, actionName=\"aggrFeatures10min\") } catch(ex){ print(ex) }\n\ttry{ unsubscribeTable(tableName=`aggrFeatures10min, actionName=\"predictRV\") } catch(ex){ print(ex) }\n\ttry{ dropStreamEngine(\"aggrFeatures10min\") } catch(ex){ print(ex) }\n\ttry{ dropStreamTable(`snapshotStream) } catch(ex){ print(ex) }\n\ttry{ dropStreamTable(`aggrFeatures10min) } catch(ex){ print(ex) }\n\ttry{ dropStreamTable(`result10min) } catch(ex){ print(ex) }\n\tundef all\n}\ncleanEnvironment()\ngo\n```\n\n----------------------------------------\n\nTITLE: Defining Task Dependencies for Full Load (Airflow DAG)\nDESCRIPTION: This snippet defines the execution order for the previously defined full data processing tasks within an Apache Airflow DAG. It specifies a linear dependency chain where tasks run sequentially: creating a parameter table, populating it, loading snapshot data, processing snapshot data, calculating minute factors, and finally calculating daily factors.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Best_Practices_for_DolphinDB_and_Python_AirFlow.md#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nstart_task >> create_parameter_table >> given_param >> loadSnapshot >> processSnapshot >> calMinuteFactor >> calDailyFactor\n```\n\n----------------------------------------\n\nTITLE: Creating Partitioned Table in DolphinDB with SecurityID as Sort Column\nDESCRIPTION: This code snippet shows creating a partitioned table in DolphinDB with SecurityID as the sort column. This is a solution to the problem of excessive index keys when using time-related fields. It offers a better distribution of data per index key compared to using TradeTime alone.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/database_and_table_creation_details.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndb=database(\"dfs://testDB1\",VALUE, 2020.01.01..2021.01.01,engine=\"TSDB\")\ncolName = `SecurityID`TradeDate`TradeTime`TradePrice`TradeQty`TradeAmount`BuyNo`SellNo\ncolType = `SYMBOL`DATE`TIME`DOUBLE`INT`DOUBLE`INT`INT\ntbSchema = table(1:0, colName, colType)\ndb.createPartitionedTable(table=tbSchema, tableName=`pt3, partitionColumns=`TradeDate, sortColumns=`SecurityID, keepDuplicates=ALL)\n```\n\n----------------------------------------\n\nTITLE: Generating SQL Query using Macro Variables\nDESCRIPTION: This is an example of how to use macro variables within a DolphinDB SQL query. `col`, `cxtByCol`, and `csCol` are the macro variables that will be replaced by the actual column names. `a` and `b` are variables that defines the value used within `between` condition. This provides a more intuitive and cleaner approach to constructing SQL queries.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/macro_var_based_metaprogramming.md#_snippet_10\n\nLANGUAGE: DolphinDB\nCODE:\n```\ncol = \"price\"\ncxtByCol = \"SecurityID\"\ncsCol = \"time\"\na = 09:00:00\nb = 15:00:00\n<select cumsum(_$col) from t where _$csCol between a and b context by _$cxtByCol csort _$csCol limit -1>\n```\n\n----------------------------------------\n\nTITLE: Defining Schema for Tables DolphinDB\nDESCRIPTION: This code defines the schema for two tables: `device_info` and `readings`.  It uses backticks to create column definitions and specifies the data types for each column. This is essential for creating and importing data into the tables in the proper format.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs TimescaleDB/test_dolphindb_small.txt#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// ----------------- 创建两张表的 schema\nCOLS_INFO \t\t= `device_id`api_version`manufacturer`model`os_name\nCOLS_READINGS \t= `time`device_id`battery_level`battery_status`battery_temperature`bssid`cpu_avg_1min`cpu_avg_5min`cpu_avg_15min`mem_free`mem_used`rssi`ssid\n\nTYPES_INFO\t\t= `SYMBOL`SYMBOL`SYMBOL`SYMBOL`SYMBOL\nTYPES_READINGS  = `DATETIME`SYMBOL`INT`SYMBOL`DOUBLE`SYMBOL`DOUBLE`DOUBLE`DOUBLE`LONG`LONG`SHORT`SYMBOL\n\nschema_info \t\t= table(COLS_INFO, TYPES_INFO)\nschema_readings = table(COLS_READINGS, TYPES_READINGS)\n```\n\n----------------------------------------\n\nTITLE: Counting Chunk Partitions by Disk After Rebalance in DolphinDB\nDESCRIPTION: This DolphinDB script queries the cluster to count the number of partitions per data node per disk. It uses the `getDiskNo` function to determine the disk path from the chunk path. It runs the `getAllChunks` function on each node, groups the result by site (data node), and the disk path. The output is a table showing the distribution of chunks after a disk rebalance, verifying successful migration.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/data_move_rebalance.md#_snippet_12\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect count(*) from pnodeRun(getAllChunks) group by site, getDiskNo(path) as disk\n```\n\n----------------------------------------\n\nTITLE: Querying by Time Range and Device List in MongoDB - JavaScript\nDESCRIPTION: This snippet retrieves documents for a given time range and a list of device IDs using '$in' and time comparisons in MongoDB. Inputs include the start and end ISODate and an array of device IDs. Suitable indices are required for performance. Returns all records matching both criteria.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs MongoDB/mongodb_readings_query.txt#_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\ndb.device_readings.explain(\"executionStats\").find({time:{\"$gte\":ISODate(\"2016-11-16 09:30:00.000Z\"),\"$lte\":ISODate(\"2016-11-17 09:30:00.000Z\")},device_id:{\"$in\":[\"demo000001\",\"demo000010\",\"demo000100\",\"demo001000\"]}},{})\n```\n\n----------------------------------------\n\nTITLE: Replaying Historical Data to a Stream Table in DolphinDB\nDESCRIPTION: Utilizes the replay function to feed historical data into the level2 stream table at 10x speed. This simulates real-time data flow for testing purposes, enabling experimentation with DolphinDB's real-time processing features without connecting to a live data source.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/quant_finance_examples.md#_snippet_21\n\nLANGUAGE: DolphinDB\nCODE:\n```\nquotes = loadTable(\"dfs://level2Replay\",\"quotes\")\n\n//设置每次提取到内存数据量=1小时\nrepartitionSchema = time(cutPoints(08:00:00..18:00:00,10))\ninputDS = replayDS(<select * from quotes>, `datetime, `datetime,  repartitionSchema)\nsubmitJob(\"replay_quotes\", \"replay_quotes_stream\",  replay,  [inputDS],  [`level2], `datetime, `datetime, 10, false, 2)\n```\n\n----------------------------------------\n\nTITLE: 使用window join查找未来报价数据\nDESCRIPTION: 通过window join函数查找每条记录一分钟之后的第一笔报价信息。可以指定未来时间范围，用于分析价格变动趋势和预测模型开发。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/quant_finance_examples.md#_snippet_13\n\nLANGUAGE: DolphinDB\nCODE:\n```\nt=select symbol, date, time, askPrice1, bidPrice1 from quotes where date=2020.06.01, symbol>=`600000, time between 09:30:00.000 : 15:00:00.000 order by symbol, date, time\nt1 = wj(t, t, 60000:70000, <[first(askPrice1) as firstAskPrice1In1m, first(bidPrice1) as firstBidPrice1In1m]>, `symbol`date`time)\n```\n\n----------------------------------------\n\nTITLE: Counting Records in head_tail_tb\nDESCRIPTION: This code counts the number of records in the `head_tail_tb` table after loading the first and last chunks of a data file.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_csv.md#_snippet_35\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect count(*) from head_tail_tb;\n```\n\n----------------------------------------\n\nTITLE: Viewing Kafka Topics\nDESCRIPTION: Commands to view the Kafka topics created by Debezium and check the data in those topics using kafka-tools scripts.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Debezium_and_Kafka_data_sync.md#_snippet_12\n\nLANGUAGE: Bash\nCODE:\n```\ncd /KFDATA/kafka-tools/bin\n./kafka.sh tplist|grep mysqlserver\n```\n\nLANGUAGE: Bash\nCODE:\n```\n./kafka.sh get_offsets mysqlserver.basicinfo.index_components\n```\n\n----------------------------------------\n\nTITLE: Splitting Dataset into Training and Test Sets in DolphinDB\nDESCRIPTION: Defines a function trainTestSplit to partition a dataset into training and test portions based on a specified ratio. It is used to split both features and labels. Dependencies: None outside basic DolphinDB functionality. Inputs: Table/array x, testRatio. Outputs: Two partitions (train/test) of the input data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/metacode_derived_features/metacode_derived_features_buildmodel.txt#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef trainTestSplit(x, testRatio) {\n    xSize = x.size()\n    testSize =( xSize * (1-testRatio))$INT\n    return x[0: testSize], x[testSize:xSize]\n}\nTrain_x, Test_x = trainTestSplit(result_input, 0.3)\nTrain_y, Test_y = trainTestSplit(label, 0.3)\n```\n\n----------------------------------------\n\nTITLE: Skipping Header Rows During Text Import in DolphinDB\nDESCRIPTION: This script uses the `loadText` function with the `skipRows` parameter set to 1000 to skip the first 1000 lines of the CSV file during import. It then verifies the reduced row count and displays the first 5 rows of the imported data, noting that the header row (if present in the original file) is also skipped, resulting in default column names.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_csv.md#_snippet_2\n\nLANGUAGE: dolphindb\nCODE:\n```\ntmpTB=loadText(filename=dataFilePath,skipRows=1000)\nselect count(*) from tmpTB;\n\nselect top 5 * from tmpTB;\n```\n\n----------------------------------------\n\nTITLE: Generating Panel Data (3 factors) from Vertical Tables\nDESCRIPTION: This query shows SQL for getting panel data for three factors from vertical tables. It uses pivot by tradetime, symbol, factorcode to transform the data. The where clause selects specified factorcodes.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practice_for_factor_calculation.md#_snippet_26\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//纵表模式取面板数据sql\nolap_factor_year_pivot=select val from olap_min_factor where factorcode in ('f0001','f0002','f0003') pivot by tradetime,symbol ,factorcode\n```\n\n----------------------------------------\n\nTITLE: DataX JSON Configuration for ClickHouse to DolphinDB Migration\nDESCRIPTION: This JSON configuration file is used by DataX to migrate data from ClickHouse to DolphinDB. It specifies the reader (ClickHouse) and writer (DolphinDB) configurations, including connection details, table names, column mappings, and data types. The configuration also includes connection details and authentication for both ClickHouse and DolphinDB, as well as transformation functions for data types.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ClickHouse_to_DolphinDB.md#_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n        \"job\": {\n                \"content\": [{\n                        \"writer\": {\n                                \"parameter\": {\n                                        \"dbPath\": \"dfs://TSDB_tick\",\n                                        \"userId\": \"admin\",\n                                        \"tableName\": \"tick\",\n                                        \"host\": \"127.0.0.1\",\n                                        \"pwd\": \"123456\",\n                                        \"table\": [\n                                            {\n                                                    \"type\": \"DT_SYMBOL\",\n                                                    \"name\": \"SecurityID\"\n                                            },\n                                            {\n                                                \"type\": \"DT_TIMESTAMP\",\n                                                \"name\": \"TradeTime\"\n                                            },\n                                            {\n                                                \"type\": \"DT_DOUBLE\",\n                                                \"name\": \"TradePrice\"\n                                            },\n                                            {\n                                                \"type\": \"DT_INT\",\n                                                \"name\": \"TradeQty\"\n                                            },\n                                            {\n                                                \"type\": \"DT_DOUBLE\",\n                                                \"name\": \"TradeAmount\"\n                                            },\n                                            {\n                                                \"type\": \"DT_INT\",\n                                                \"name\": \"BuyNo\"\n                                            },\n                                            {\n                                                \"type\": \"DT_INT\",\n                                                \"name\": \"SellNo\"\n                                            },\n                                            {\n                                                \"type\": \"DT_INT\",\n                                                \"name\": \"TradeIndex\"\n                                            },\n                                            {\n                                                \"type\": \"DT_INT\",\n                                                \"name\": \"ChannelNo\"\n                                            },\n                                            {\n                                                \"type\": \"DT_SYMBOL\",\n                                                \"name\": \"TradeBSFlag\"\n                                            },\n                                            {\n                                                \"type\": \"DT_INT\",\n                                                \"name\": \"BizIndex\"\n                                            }\n                                        ],\n                                        \"port\": 10001\n                                },\n                                \"name\": \"dolphindbwriter\"\n                        },\n                        \"reader\": {\n                                \"parameter\": {\n                                        \"username\": \"default\",\n                                        \"column\": [\"SecurityID\", \"toString(TradeTime)\", \"TradePrice\", \"TradeQty\", \"TradeAmount\", \"BuyNo\", \"SellNo\", \"ChannelNo\", \"TradeIndex\", \"TradeBSFlag\", \"BizIndex\"],\n                                        \"connection\": [{\n                                                \"table\": [\"ticksh\"],\n                                                \"jdbcUrl\": [\"jdbc:clickhouse://127.0.0.1:8123/migrate\"]\n                                        }],\n                                        \"password\": \"123456\",\n                                        \"where\": \"\"\n                                },\n                                \"name\": \"clickhousereader\"\n                        }\n                }],\n                \"setting\": {\n                        \"speed\": {\n                                \"channel\": 1\n                        }\n                }\n        }\n}\n```\n\n----------------------------------------\n\nTITLE: Subscribing to a Stream Table and Replaying Data in DolphinDB\nDESCRIPTION: This DolphinDB script subscribes the reactive state engine to a stream table (`snapshotStreamTable`) and then replays historical data using `replayDS` to simulate real-time data flow.  For live data,行情数据直接写入snapshotStreamTable流表. Requires `SecurityID`, `TradeTime`, `LastPx`.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practice_for_factor_calculation.md#_snippet_17\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//demo_engine订阅snapshotStreamTable流表\nsubscribeTable(tableName=snapshotSharedTableName, actionName=actionName, handler=append!{demoEngine}, msgAsTable=true)\n\n//创建播放数据源供replay函数历史回放；盘中的时候，改为行情数据直接写入snapshotStreamTable流表\ninputDS = replayDS(<select SecurityID, TradeTime, LastPx from tableHandle where date(TradeTime)<2020.07.01>, `TradeTime, `TradeTime)\n```\n\n----------------------------------------\n\nTITLE: Aligning indexedSeries and indexedMatrix Objects in DolphinDB\nDESCRIPTION: This code illustrates performing binary operations between an indexedMatrix and an indexedSeries in DolphinDB, automatically aligning along shared row labels. The indexedSeries is broadcast across each column of the matrix where labels overlap. Dependencies: matrix and indexedSeries functions, setIndexedMatrix!. Inputs: an indexedMatrix with date row labels and an indexedSeries with potentially overlapping dates. Output: matrix where each overlapped row is summed with the indexedSeries entry, with NULLs in rows not present in both.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/panel_data.md#_snippet_20\n\nLANGUAGE: DolphinDB\nCODE:\n```\nm1=matrix(1..6, 11..16);\nm1.rename!(2020.11.04..2020.11.09, `A`B);\nm1.setIndexedMatrix!();\nm1;\n\ns1;\n\nm1 + s1;\n```\n\n----------------------------------------\n\nTITLE: Calculating wma with Timer in DolphinDB\nDESCRIPTION: This snippet measures the execution time of the `wma` function applied to a large vector of closing prices.  It first defines a `close` vector and then applies the `wma` function, using the `timer` command to measure the time taken to execute this calculation. This is used to compare the performance of the DolphinDB implementation against TA-Lib.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ta.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\nuse ta\nclose = 7.2 6.97 7.08 6.74 6.49 5.9 6.26 5.9 5.35 5.63\nclose = take(close, 1000000)\ntimer x = wma(close, 5);\n```\n\n----------------------------------------\n\nTITLE: AdaBoost 机器学习模型训练脚本（Lua/ DolphinDB）\nDESCRIPTION: 该Lua脚本用于在 DolphinDB 中的计算节点进行 AdaBoost 回归模型训练，定义数据预处理、模型训练、保存流程。依赖于 DolphinDB 内置的机器学习函数如 adaBoostRegressor，输入为训练数据，模型参数包括树个数、深度和损失类型，输出模型文件路径。适合在资源充足的计算节点部署，提升训练效率。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Compute_Node.md#_snippet_6\n\nLANGUAGE: Lua\nCODE:\n```\ndef tranAdaBoost(TrainData){\n\tdb = database(,HASH, [SYMBOL, 10])\n\tp10TranData = db.createPartitionedTable(table=TrainData, partitionColumns=`SecurityID)\n\tp10TranData.append!(TrainData)\n\tmodel = adaBoostRegressor(sqlDS(<select * from p10TranData>), yColName=`targetRV, xColNames=`BAS`DI0`DI1`DI2`DI3`DI4`Press`RV, numTrees=30, maxDepth=16, loss=`square)\n\tsaveModel(model, \"/hdd/data/finance/model/adaBoost.txt\")\n}\n\njobId=\"adaBoost\" \njobDesc=\"adaBoost train snap\"\nsubmitJob(jobId, jobDesc, tranAdaBoost, Train)\n```\n\n----------------------------------------\n\nTITLE: Basic SQL Query Example for DDBDataLoader\nDESCRIPTION: Example of a simple SQL query that can be used with DDBDataLoader to load data from a DolphinDB table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ai_dataloader_ml.md#_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nselect * from loadTable(dbName, tableName)\n```\n\n----------------------------------------\n\nTITLE: Querying Data - Python\nDESCRIPTION: This code snippet queries data from the DolphinDB table named 'myTable' and prints the result.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Array_Vector.md#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nt = s.run(\"select * from myTable\")\nprint(t)\n```\n\n----------------------------------------\n\nTITLE: Grouping Trades by Stock Code (Single Column) - DolphinDB\nDESCRIPTION: This DolphinDB script groups the \"trades\" table by \"ts_code\" and calculates the average low price for each group. The `timer(10)` function is used to measure the performance of the query over 10 iterations. `clearAllCache()` clears the cache before each iteration to avoid caching effects.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_19\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//按股票代码分组（单列）\ntimer(10) select avg(low) from trades group by ts_code\n```\n\n----------------------------------------\n\nTITLE: Filtering Trades by Floating-Point Values - DolphinDB\nDESCRIPTION: This DolphinDB script filters the \"trades\" table based on a range of \"pre_close\" values. It selects records where the previous close price is greater than 25 and less than 35.  The `timer(10)` function measures the execution time over 10 iterations, and `clearAllCache()` is called before each iteration.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_18\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//根据浮点型过滤\ntimer(10) select * from trades where pre_close > 25 and pre_close < 35\n```\n\n----------------------------------------\n\nTITLE: Creating Partitioned Table with Hash Reduction on SecurityID Sort Column\nDESCRIPTION: This code snippet demonstrates creating a partitioned table and applying hash reduction on the SecurityID sort column using the `sortKeyMappingFunction` parameter. This technique reduces the number of index keys, which improves query performance and reduces memory consumption.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/database_and_table_creation_details.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndb=database(\"dfs://testDB1\",VALUE, 2020.01.01..2021.01.01,engine=\"TSDB\")\ncolName = `SecurityID`TradeDate`TradeTime`TradePrice`TradeQty`TradeAmount`BuyNo`SellNo\ncolType = `SYMBOL`DATE`TIME`DOUBLE`INT`DOUBLE`INT`INT\ntbSchema = table(1:0, colName, colType)\ndb.createPartitionedTable(table=tbSchema, tableName=`pt4, partitionColumns=`TradeDate, sortColumns=`SecurityID keepDuplicates=ALL,sortKeyMappingFunction=[hashBucket{,500}])\n```\n\n----------------------------------------\n\nTITLE: Using Statistical Functions in DolphinDB for Technical Analysis\nDESCRIPTION: Statistical functions in the DolphinDB ta module for technical analysis. These functions apply statistical methods to price data to identify trends, correlations, and potential future price movements.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ta.md#_snippet_11\n\nLANGUAGE: DolphinDB\nCODE:\n```\nbeta(high, low, timePeriod)\ncorrel(high, low, timePeriod)\nlinearreg(close, timePeriod)\nlinearreg_angle(close, timePeriod)\nlinearreg_intercept(close, timePeriod)\nlinearreg_slope(close, timePeriod)\nstdDev(close, timePeriod, nbdev)\ntsf(close, timePeriod)\nvar(close, timePeriod, nbdev)\n```\n\n----------------------------------------\n\nTITLE: Querying Predicted Arrival Times in DolphinDB for Grafana\nDESCRIPTION: This DolphinDB SQL-like query retrieves the trip ID, pickup time, calculated arrival time (pickup time plus predicted duration, assuming duration is log-transformed and needs exponentiation), and predicted duration in minutes from the `predictTable`. It filters for records matching the latest date in the table, designed for visualization in Grafana.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Forecast_of_Taxi_Trip_Duration.md#_snippet_2\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nselect id as ID, pickup_datetime as pickup_time, (pickup_datetime+int((exp(duration)-1))) as arrival_time,  (exp(duration)-1)/60 as duration from predictTable \nwhere date(predictTable.pickup_datetime) == date(select max(pickup_datetime) from predictTable)\n```\n\n----------------------------------------\n\nTITLE: Parallel Processing Function for Multi-File Volume Calculation\nDESCRIPTION: Defines 'pool_func' to process each tick file in a directory by reading CSVs, computing active volume percentage, and aggregating results. Uses tqdm for progress and handles exceptions gracefully.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/DolphinDB_Python_Parser_Intro_for_Quantitative_Finance/因子实现_Python版本/主动成交量占比.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ndef pool_func(tick_obj, trade_path_obj):\n    single_tick_res = []\n    tmp_date = trade_path_obj.split('/')[-2]\n    # print(tmp_date)\n    tmp_date = tmp_date[0:4] + \"-\" + tmp_date[4:6] + \"-\" + tmp_date[6:8]\n    # print(tmp_date)\n    for tick in tqdm(tick_obj):\n        try:\n            df = pd.read_csv(os.path.join(trade_path_obj, tick))\n            Indicator = actVolumePercent(df, 60)\n            single_tick_res.append(Indicator)\n            # print(Indicator)\n            # print(\"开盘后大单净买入占比:\", Indicator)\n        except Exception as error:\n            continue\n\n    return pd.concat(single_tick_res)\n```\n\n----------------------------------------\n\nTITLE: Subscribing to Stream Data Using subscribeTable in DolphinDB\nDESCRIPTION: Demonstrates different ways to call the subscribeTable function to subscribe to stream data tables in DolphinDB. It covers local, cluster-different-node, and remote-node subscription scenarios using parameters server, tableName, actionName, offset, and handler. The snippet shows how to instantiate subscribers under different network topologies with required parameters. It requires a published shared streaming table. Expected inputs include server identifier or connection handle, streaming table name, subscription task name, data start offset, and the handler to process data. The output is a subscription topic string identifying the subscription uniquely. This snippet assumes the subscribeTable function of DolphinDB is available.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_tutorial.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\nsubscribeTable(tableName=\"pubTable\", actionName=\"act1\", offset=0, handler=subTable, msgAsTable=true)\n\nsubscribeTable(server=\"NODE2\", tableName=\"pubTable\", actionName=\"act1\", offset=0, handler=subTable, msgAsTable=true)\n\npubNodeHandler=xdb(\"192.168.1.13\",8891)\nsubscribeTable(server=pubNodeHandler, tableName=\"pubTable\", actionName=\"act1\", offset=0, handler=subTable, msgAsTable=true)\n```\n\n----------------------------------------\n\nTITLE: Importing XGBoost Plugin in DolphinDB Script\nDESCRIPTION: This snippet attempts to dynamically load the XGBoost plugin for DolphinDB using loadPlugin. It employs exception handling to catch and print errors if the plugin fails to load. Dependency: Xgboost plugin must be pre-installed at the specified directory. Inputs: Plugin path constructed via getHomeDir(). Outputs: Xgboost module available for model training.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/metacode_derived_features/metacode_derived_features_buildmodel.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntry{\n\tloadPlugin(getHomeDir()+\"/plugins/xgboost/PluginXgboost.txt\")\n}\ncatch(ex){\n\tprint(ex)\n}\n```\n\n----------------------------------------\n\nTITLE: Querying with atImax() function and group by\nDESCRIPTION: Queries the latest data using the `atImax()` function with `group by`. `atImax` finds the index of the maximum timestamp and returns the corresponding values from other columns.  A time filter is used to limit the scope of the query. The `timer` keyword is used to measure the execution time.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/snapshot_engine.md#_snippet_9\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\ntimer select atImax(ts,tag0001 ) as tag0001, atImax(ts,tag0002 ) as tag0002, atImax(ts,tag0003 ) as tag0003\nfrom loadTable('dfs://testDB', 'trainInfoTable') where ts >datetimeAdd(now(),-1,`h) group by trainID\n```\n\n----------------------------------------\n\nTITLE: Reading Trade Data File and Computing VWAP Using Python\nDESCRIPTION: This snippet reads a CSV file containing trade information into a pandas dataframe and computes the volume weighted average price using the previously defined function. It also measures and prints the computation time. The input file path is hard-coded and must exist for successful execution.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/DolphinDB_Python_Parser_Intro_for_Quantitative_Finance/因子实现_Python版本/委托量加权平均委托价格.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_csv(\"/ssd/ssd3/data/oneStock_oneFile_TL/20230201/entrust/000001.csv\")\nt0 = time.time()\nres = volumeWeightedAvgPrice(df, 60)\nprint(\"cal time: \", time.time() - t0, \"s\")\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Delete Factor in Narrow Table (DolphinDB)\nDESCRIPTION: This function deletes a specific factor from a narrow table within a given time range. It loads the table and then uses a `delete` statement to remove the rows that match the specified factor name and time range. Dependencies: `loadTable`, `tradetime`, `factorname` columns.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practices_for_multi_factor.md#_snippet_8\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 单值模型删除一个因子\ndef singleModelDeleteFactor(dbname,tbname,start_date,end_date,delete_factor){\n\tpt = loadTable(dbname,tbname)\n\ttime_list = getTimeList(start_date,end_date).flatten()\n\tstart_time,end_time = time_list[0],time_list[time_list.size()-1]\n\tdelete  from pt where tradetime >= start_time and tradetime <= end_time and factorname = delete_factor\n}\n```\n\n----------------------------------------\n\nTITLE: Calculate Minute Averages\nDESCRIPTION: This snippet calculates the average `value` for specific IDs within one day, grouping the data by `minute(datetime)`. This query is an example of data aggregation using `group by` clause, where the data is first filtered by `id` and `datetime`, then aggregated by the minute of the datetime to compute the average value.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/iot_examples.md#_snippet_13\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect avg(value) as avg \nfrom sensors \nwhere id in [1,51,101,151,201], datetime between 2020.09.01T00:00:00 : 2020.09.01T23:59:59  \ngroup by id, minute(datetime)\n```\n\n----------------------------------------\n\nTITLE: Incorrect Use of <ALIAS> Macro Variable in DolphinDB Cluster Configuration Causing Startup Failure\nDESCRIPTION: This example shows incorrect and correct usage of the <ALIAS> macro in cluster.cfg for node-specific configuration paths. Using <ALIAS> directly under explicit node alias keys causes node startup failures. Instead, avoid using <ALIAS> under explicit nodes, or place configuration keys globally using <ALIAS> for all nodes collectively.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/multi_machine_cluster_deployment.md#_snippet_11\n\nLANGUAGE: Configuration\nCODE:\n```\nP1-datanode.persistenceDir = /hdd/hdd1/streamCache/<ALIAS>\n```\n\nLANGUAGE: Configuration\nCODE:\n```\nP1-datanode.persistenceDir = /hdd/hdd1/streamCache/P1-datanode\n```\n\nLANGUAGE: Configuration\nCODE:\n```\npersistenceDir = /hdd/hdd1/streamCache/<ALIAS>\n```\n\n----------------------------------------\n\nTITLE: Convert Local Time to UTC with gmtime in DolphinDB\nDESCRIPTION: This snippet demonstrates how to convert a local datetime to UTC time (GMT) using the `gmtime` function in DolphinDB.  It then uses the `long` function to get the Unix timestamp. This is useful for storing time data in a consistent time zone.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/timezone.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\n$ gmtime(2022.05.20 11:30:05)\n2022.05.20 03:30:05\n$ long(gmtime(2022.05.20 11:30:05))\n1653017405\n```\n\n----------------------------------------\n\nTITLE: Initializing History Dictionary (String, Any) - DolphinDB\nDESCRIPTION: Declares a dictionary named `historyDict` to store the history. The keys are of type STRING and the values are of type ANY.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/func_progr_cases.md#_snippet_18\n\nLANGUAGE: DolphinDB\nCODE:\n```\nhistoryDict = dict(STRING, ANY)\n```\n\n----------------------------------------\n\nTITLE: Define 60-Minute Capital Flow Aggregation Function in DolphinDB\nDESCRIPTION: The `processCapitalFlow60minFunc` function creates a daily time series engine to aggregate capital flow metrics over a 60-minute rolling window. It uses the `last` function to retrieve the latest values of the metrics within each window and outputs the result to the `capitalFlowStream60min` table. The input is the capitalFlowStream table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_capital_flow_daily.md#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef processCapitalFlow60minFunc(){\n\taggrMetrics = <[\n\t\tlast(TotalAmount),\n\t\tlast(SellSmallAmount),\n\t\tlast(SellMediumAmount),\n\t\tlast(SellBigAmount),\n\t\tlast(SellSmallCount),\n\t\tlast(SellMediumCount),\n\t\tlast(SellBigCount),\n\t\tlast(BuySmallAmount),\n\t\tlast(BuyMediumAmount),\n\t\tlast(BuyBigAmount),\n\t\tlast(BuySmallCount),\n\t\tlast(BuyMediumCount),\n\t\tlast(BuyBigCount)]>\n\tcreateDailyTimeSeriesEngine(name=\"processCapitalFlow60min\", windowSize=60000*60, step=60000*60, metrics=aggrMetrics, dummyTable=capitalFlowStream, outputTable=capitalFlowStream60min, timeColumn=\"TradeTime\", useSystemTime=false, keyColumn=`SecurityID, useWindowStartTime=false)\n\tsubscribeTable(tableName=\"capitalFlowStream\", actionName=\"processCapitalFlow60min\", offset=-1, handler=getStreamEngine(\"processCapitalFlow60min\"), msgAsTable=true, batchSize=10000, throttle=1, hash=0)\n}\n```\n\n----------------------------------------\n\nTITLE: Aggregating Maximum Battery Temperature per Device and Hour in MongoDB - JavaScript\nDESCRIPTION: This snippet uses an aggregation pipeline to calculate the maximum battery temperature per device and per hour. It groups documents by both 'device_id' and the extracted hour, then finds the max battery_temperature in each group. Requires 'device_id', 'time', and 'battery_temperature' fields. Outputs grouped max values by device and hour.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs MongoDB/mongodb_readings_query.txt#_snippet_6\n\nLANGUAGE: JavaScript\nCODE:\n```\ndb.device_readings.explain(\"executionStats\").aggregate([{$group:{_id:{device_id:\"$device_id\",hour_new:{$hour:\"$time\"}},max_temperature:{$max:\"$battery_temperature\"}}}])\n```\n\n----------------------------------------\n\nTITLE: Granting SCRIPT_EXEC Permission - DolphinDB\nDESCRIPTION: This code grants a user SCRIPT_EXEC permission, allowing them to execute scripts using the `run` function. Requires admin privileges to create user.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Permission_Management.md#_snippet_40\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(`admin, `123456)\ncreateUser(`user1, \"123456\");\ngrant(`user1,SCRIPT_EXEC)\nlogin(`user1, \"123456\")\nrun(\"test.txt\")\n```\n\n----------------------------------------\n\nTITLE: 构建基金类型字典映射\nDESCRIPTION: 加载基金基础信息表，建立基金ID到基金类型的映射关系，为后续按类型分析提供依据。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/public_fund_basic_analysis.md#_snippet_15\n\nLANGUAGE: DolphinDB\nCODE:\n```\nfundData = loadTable(\"dfs://publicFundDB\", \"publicFundData\")\nfundType = select SecurityID, Type from fundData where SecurityID in returnsMatrix.colNames() order by Type\nfundTypeMap = dict(fundType[\"SecurityID\"], fundType[\"Type\"])\n```\n\n----------------------------------------\n\nTITLE: Performance Testing: Daily and Minute-based Average Price\nDESCRIPTION: Calculates the average of the mid-price (average of bid and offer) over a date and time range, grouped by date and minute, to analyze price movements.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs InfluxDB/test_dolphindb_big.txt#_snippet_15\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 7. 经典查询：按 [日期、时间范围、卖出买入价格条件、股票代码] 过滤，查询 (各个股票 每分钟) [平均变化幅度]\ntimer\nselect avg( (ofr - bid) / (ofr + bid) ) * 2 as spread \nfrom taq \nwhere \n\tdate = 2007.08.01,\n\ttime between 09:30:00 : 16:00:00,\n\tbid > 0,\n\tofr > bid\n\ngroup by symbol, minute(time) as minute\n```\n\n----------------------------------------\n\nTITLE: Updating Controller and Agent Configuration Files on Server 5\nDESCRIPTION: Uses sed to replace existing controller and agent entries in respective configuration files on server 5, integrating additional controllers into the cluster setup.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/service_deployment_and_migration.md#_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\nsed -i.bak -E -e 's/([0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+):([0-9]+):controller1/172.0.0.5:9913:controller5/' controller.cfg\nsed -i.bak -E -e 's/([0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+):([0-9]+):agent1/172.0.0.5:9911:agent5/' agent.cfg\nsed -i '/^sites/s/$/,172.0.0.4:9912:controller4:controller,172.0.0.5:9913:controller5:controller/' agent.cfg\n\necho 172.0.0.4:9912:controller4,controller >> cluster.nodes\necho 172.0.0.5:9913:controller5,controller >> cluster.nodes\n```\n\n----------------------------------------\n\nTITLE: Inserting Data into Stream Table\nDESCRIPTION: This snippet defines a variable `n` and then uses a loop to insert `n` records into the `table1` stream table. Each record consists of a generated 'key' and 'value' string. As data is inserted into this table, the subscribed handler `myHandle` will be triggered to write these key-value pairs to Redis.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/redis_plugin_tutorial/example2.txt#_snippet_4\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nn = 1000000\nfor(x in 0:n){\n\tinsert into table1 values(\"key\" + x, \"value\" + x)\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Standard Upper Triangular Matrix Creation in DolphinDB\nDESCRIPTION: This snippet defines a standard DolphinDB function `upperTriangularMatrix` without the `@jit` annotation. It performs the same task as the JIT version, creating an upper triangular matrix, but uses standard script execution with nested loops and `flatten` to access matrix elements. It serves as a non-JIT comparison for performance.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/matrix.md#_snippet_39\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef upperTriangularMatrix(m, rowNum,colNum){\n\tupperM=m\n  for( i in 0:colNum){\n    col=flatten(m[i])\n\t\tfor(j in 0:rowNum){\n\t\t\tif(i<j){\n\t\t\t  col[j]=0\n\t\t\t}\n\t\t}\n\t\tupperM[i]=col\n\t}\n\treturn upperM\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Real-time IOPV Calculation with CrossSectionalEngine in DolphinDB\nDESCRIPTION: Creates a cross-sectional engine to calculate IOPV in real-time as new trades arrive, processing only the ETF component stocks and triggering recalculation on each new tick.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_IOPV.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nshare streamTable(1000:0, `tradetime`tradedate`IOPV, [TIMESTAMP,DATE,DOUBLE]) as IOPVStreamResult\nIOPV_engine = createCrossSectionalEngine(name=\"IOPV_calculator\", metrics=[<last(tradedate)>, <sum(ffill(price) * portfolio[SecurityID]/1000)>],  dummyTable=TradeStreamData, outputTable=IOPVStreamResult, keyColumn=`SecurityID, triggeringPattern='perRow',  timeColumn=`tradetime, useSystemTime=false)\nsetStreamTableFilterColumn(TradeStreamData, `SecurityID)\nsubscribeTable(tableName=\"TradeStreamData\", actionName=\"trade_subscribe\", offset=0, handler=append!{IOPV_engine}, msgAsTable=true, batchSize=10000, throttle=1, hash=0, filter=portfolio.keys());\n```\n\n----------------------------------------\n\nTITLE: Setting Up TSDB Distributed Table for Cache Examples (DolphinDB Script)\nDESCRIPTION: Creates a distributed database ('dfs://TSDB_db1') and a partitioned table ('pt1') using the TSDB engine. It defines the schema, specifies partitioning and sorting columns, and is used for demonstrating TSDB-specific caching behavior.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/memory_management.md#_snippet_12\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nn=10000\nID=rand(100, n)\ndates=2021.08.07..2021.08.11\ndate=rand(dates, n)\nvol=rand(1..10 join int(), n)\nt=table(ID, date, vol)\nif(existsDatabase(\"dfs://TSDB_db1\")){\ndropDatabase(\"dfs://TSDB_db1\")\n}\ndb=database(directory=\"dfs://TSDB_db1\", partitionType=VALUE, partitionScheme=2021.08.07..2021.08.11, engine=\"TSDB\")\npt1=db.createPartitionedTable(table=t, tableName=`pt1, partitionColumns=`date, sortColumns=`ID)\n```\n\n----------------------------------------\n\nTITLE: Managing Groups (Create, Add/Remove Members, Delete) in DolphinDB - DolphinDB\nDESCRIPTION: These snippets collectively manage user groups in DolphinDB, including creation, adding/removing users, and deletion. All operations require admin privileges and leverage functions like 'createGroup', 'addGroupMember', 'deleteGroupMember', and 'deleteGroup'. Inputs include group names and user lists; outputs are modified group structures and affected user permissions.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Permission_Management.md#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\ncreateGroup(\"group1\",[\"admin1\"])\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\naddGroupMember([\"user2\"],\"group1\")\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndeleteGroupMember([\"user2\"],\"group1\")\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndeleteGroup(\"group1\")\n```\n\n----------------------------------------\n\nTITLE: Computing and Sorting Average and Standard Deviation of Battery Level by Network in MongoDB - JavaScript\nDESCRIPTION: This snippet aggregates 'device_readings' by network SSID, computes the average and sample standard deviation of battery level for each group, and sorts results by descending average battery level. Inputs require a 'ssid' and 'battery_level' field. Outputs grouped statistics per network connection.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs MongoDB/mongodb_readings_query.txt#_snippet_11\n\nLANGUAGE: JavaScript\nCODE:\n```\ndb.device_readings.explain(\"executionStats\").aggregate([{$group:{_id:{ssid:\"$ssid\"},std_battery_level:{$stdDevSamp:\"$battery_level\"},avg_battery_level:{$avg:\"$battery_level\"}}},{$sort:{\"avg_battery_level\":-1}}])\n```\n\n----------------------------------------\n\nTITLE: DolphinDB Grouping by Stock Code (Single Column)\nDESCRIPTION: This DolphinDB script aggregates data using the `group by` clause. It calculates the average of the `VOL` column and groups the results by the `TICKER` column, representing stock codes.  The `timer()` function times the execution across 10 runs.  The test focuses on the performance of grouping on a single column.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_41\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\n//按股票代码分组（单列）\ntimer(10) select avg(VOL) from trades group by TICKER\n```\n\n----------------------------------------\n\nTITLE: Submitting Parallel Jobs for Factor Calculation in DolphinDB\nDESCRIPTION: Shows usage examples to submit job(s) in DolphinDB using 'submitJob'. The first submission runs a single job identified by 'parallJob_single_nine' and the second submits 5 jobs in a loop named 'parallJob_multi_nine' for multi-user parallelism. This allows asynchronous execution of the parallel job function 'parJob1'.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_factor_by_ddb/fund_factor_nine.txt#_snippet_12\n\nLANGUAGE: DolphinDB\nCODE:\n```\nsubmitJob(\"parallJob1\", \"parallJob_single_nine\", parJob1)\n\nfor(i in 0..4){\n\tsubmitJob(\"parallJob5\", \"parallJob_multi_nine\", parJob1)\n}\n```\n\n----------------------------------------\n\nTITLE: Training and Evaluating AdaBoost Regressor Model in DolphinDB\nDESCRIPTION: This snippet trains an adaBoost regressor model using DolphinDB's machine learning functions, evaluates its performance with RMSPE, and outputs the prediction error. It demonstrates model training, prediction, and performance measurement procedures.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/machine_learning_volatility.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef RMSPE(a,b) {\n    return sqrt(sum(((a - b) / a) * ((a - b) / a)) / a.size())\n}\nmodel = adaBoostRegressor(sqlDS(<select * from Train>), yColName=`targetRV, xColNames=`BAS`DI0`DI1`DI2`DI3`DI4`Press`RV, numTrees=30, maxDepth=16, loss=`square)\npredicted = model.predict(Test)\nTest[`predict]=predicted\nprint(\"RMSPE=\"+RMSPE(Test.targetRV, predicted))\n```\n\n----------------------------------------\n\nTITLE: Configuring DolphinDB Cluster Parameters\nDESCRIPTION: This snippet shows the configuration parameters for a DolphinDB cluster, including memory size, number of connections, worker threads, cache size, partition policy, and IO concurrency settings. These parameters affect the performance of the DolphinDB cluster, including backup and restore operations. The configurations are set in cluster.cfg file.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/backup-restore-new.md#_snippet_12\n\nLANGUAGE: DolphinDB\nCODE:\n```\nmaxMemSize=256\nmaxConnections=512\nworkerNum=8\nchunkCacheEngineMemSize=16\nnewValuePartitionPolicy=add\nmaxPubConnections=64\nsubExecutors=4\nsubPort=8893\nlanCluster=0\nenableChunkGranularityConfig=true\ndiskIOConcurrencyLevel=0\n```\n\n----------------------------------------\n\nTITLE: Defining Disk Number by Path in DolphinDB\nDESCRIPTION: This DolphinDB script defines a function `getDiskNo` that extracts the disk identifier from a given file path.  It takes an array of paths as input, and it uses string manipulation (`split`, `concat`) to generate the disk identifier (e.g., `/ssd/ssd0/jtwang/chunkData`).  The result is an array of disk identifiers.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/data_move_rebalance.md#_snippet_11\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef getDiskNo(path) {\n    size = path.size()\n    result = array(STRING, 0, size)\n    for(i in 0 : size) { append!(result, concat(split(path[i], \"/\")[0:5], \"/\")) }\n    return result\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Database Parameters and Creating Database\nDESCRIPTION: Sets up database and table names, then calls the function to create the time-series database for storing market snapshot data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/Real-Time_Stock_Price_Increase_Calculation/01.创建存储快照数据的库表并导入数据.txt#_snippet_2\n\nLANGUAGE: dolphindb\nCODE:\n```\ndbName, tbName = \"dfs://SH_TSDB_snapshot_ArrayVector\", \"snapshot\"\ncreateDfsTb(dbName, tbName)\n```\n\n----------------------------------------\n\nTITLE: Elasticsearch Filtering & Grouping by Stock Code - Python\nDESCRIPTION: This Python function performs a filtered aggregation query on Elasticsearch. It first filters documents where the 'amount' field is between 5000 and 13000. Then, it groups the filtered documents by 'ts_code' and calculates the average 'high' price for each group. It sends the request to the Elasticsearch API using `urllib3` and prints the status and JSON decoded response.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_34\n\nLANGUAGE: Python\nCODE:\n```\ndef search_9():\n    http = urllib3.PoolManager()\n    data = json.dumps({\n        \"query\": {\n            \"constant_score\": {\n                \"filter\": {\n                    \"range\": {\n                        \"amount\": {\n                            \"gte\": 5000,\n                            \"lte\": 13000\n                        }\n                    }\n                }\n            }\n        },\n        \"aggs\": {\n            \"group_by_ts_code\": {\n                \"terms\": {\n                    \"field\": \"ts_code\",\n                    \"size\": 5000\n                },\n                \"aggs\": {\n                    \"avg_high\": {\n                        \"avg\": {\"field\": \"high\"}\n                    }\n                }\n            }\n        },\n        \"_source\": [\"\"]\n    }).encode(\"utf-8\")\n```\n\n----------------------------------------\n\nTITLE: View DataX Synchronization Results\nDESCRIPTION: Displays the completion and performance metrics of the data migration task, including timestamps, total duration, throughput, record count, and errors. Dependencies are the executed DataX process logs. Inputs are reading from log outputs; outputs are human-readable status and performance data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Oracle_to_DolphinDB.md#_snippet_18\n\nLANGUAGE: Shell\nCODE:\n```\n任务启动时刻                    : 2023-06-06 15:41:34\n任务结束时刻                    : 2023-06-06 15:45:49\n任务总计耗时                    :                254s\n任务平均流量                    :            6.44MB/s\n记录写入速度                    :         107133rec/s\n读出记录总数                    :            27211975\n读写失败总数                    :                   0\n```\n\n----------------------------------------\n\nTITLE: Implementing Stateful Stream Processing with Cumulative Average in DolphinDB\nDESCRIPTION: Demonstrates how to create a stateful message handler function for calculating the cumulative average of a price field in a stream table. The example uses partial application to maintain state across incoming messages.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_62\n\nLANGUAGE: DolphinDB\nCODE:\n```\nshare streamTable(10000:0,`time`symbol`price, [TIMESTAMP,SYMBOL,DOUBLE]) as trades\navgT=table(10000:0,[`avg_price],[DOUBLE])\n\ndef cumulativeAverage(mutable avgTable, mutable stat, trade){\n   newVals = exec price from trade;\n\n   for(val in newVals) {\n      stat[0] = (stat[0] * stat[1] + val )/(stat[1] + 1)\n      stat[1] += 1\n      insert into avgTable values(stat[0])\n   }\n}\n\nsubscribeTable(tableName=\"trades\", actionName=\"action30\", handler=cumulativeAverage{avgT,0.0 0.0}, msgAsTable=true)\n```\n\n----------------------------------------\n\nTITLE: 较短时间范围内多条件查询性能测试\nDESCRIPTION: 在时间范围内限制的多条件查询中，比对合并前后的耗时表现，验证合并优化的效果。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/tsdb_explained.md#_snippet_4\n\nLANGUAGE: SQL\nCODE:\n```\nselect * from loadTable(dbName, tbName_all) where machineId=999 and datetime=2023.07.10 and datetime between 2023.07.10 09:00:01.000 and 2023.07.10 09:00:03.000\n```\n\n----------------------------------------\n\nTITLE: Replay from CSV Files in Python\nDESCRIPTION: This Python code demonstrates how to replay data from CSV files when database tables are unavailable. It loads data from CSV files using `loadText`, sorts it by time, and then performs the heterogeneous replay. This approach is useful for quick testing and demonstration with local data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/stock_market_replay.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\norderDS = select * from loadText(\"/yourDataPath/replayData/order.csv\") order by Time\ntradeDS = select * from loadText(\"/yourDataPath/replayData/trade.csv\") order by Time\nsnapshotDS = select * from loadText(\"/yourDataPath/replayData/snapshot.csv\") order by Time\ninputDict = dict([\"order\", \"trade\", \"snapshot\"], [orderDS, tradeDS, snapshotDS])\n\nsubmitJob(\"replay\", \"replay text\", replay, inputDict, messageStream, `Date, `Time, , , 1)\n```\n\n----------------------------------------\n\nTITLE: Querying Master Replication Status\nDESCRIPTION: This DolphinDB script queries the master cluster's replication status by calling getMasterReplicationStatus using the rpc function. Requires calling this function from a control node. The function will return the state of the replication tasks in the master cluster, including task information such as creation time, progress, and status. Requires a proper setup of a DolphinDB master cluster.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Asynchronous_Replication.md#_snippet_16\n\nLANGUAGE: dolphindb\nCODE:\n```\nrpc(getControllerAlias(), getMasterReplicationStatus)\n```\n\n----------------------------------------\n\nTITLE: Running Prometheus Server - Shell\nDESCRIPTION: This command starts the Prometheus server in the background. The `--config.file` flag specifies the path to the Prometheus configuration file (prometheus.yml), which defines scrape targets and other settings.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/cluster_monitor.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\nnohup ./prometheus --config.file=prometheus.yml &\n```\n\n----------------------------------------\n\nTITLE: Simulated Data and Range Setup for Value Segment Counting in DolphinDB Script\nDESCRIPTION: Generates a table for a single date, code, and a random 'value' column to allow testing of counts of how many values fall into given numeric ranges using asof or custom functions. Sets up a range vector for segment definition. No external dependencies.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_SQL_Case_Tutorial.md#_snippet_38\n\nLANGUAGE: DolphinDB\nCODE:\n```\nN = 100000\nt = table(take(2021.11.01, N) as date, \n          take(`AAPL, N) as code, \n          rand([-5, 5, 10, 15, 20, 25, 100], N) as value)\nrange = [-9999, 0, 10, 30, 9999]\n```\n\n----------------------------------------\n\nTITLE: Defining systemd Service for DolphinDB Controller Node (systemd)\nDESCRIPTION: This systemd unit file configures the DolphinDB cluster controller as a managed system service. It sets up startup, stop, and reload hooks linked to the controller.sh script, customizes process restart policy, and applies resource limits. The WorkingDirectory must point to DolphinDB's cluster workspace. Required: controller.sh script with execute permissions and accessible by systemd. Outputs: Controls the DolphinDB controller as a system service via systemctl. Limit: Paths must be adjusted for actual installation.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ha_cluster_deployment.md#_snippet_16\n\nLANGUAGE: systemd\nCODE:\n```\n[Unit]\nDescription=ddbcontroller\nDocumentation=https://www.dolphindb.com/\n\n[Service]\nType=forking\nWorkingDirectory=/home/DolphinDB/server/clusterDemo\nExecStart=/bin/sh controller.sh start\nExecStop=/bin/sh controller.sh stop\nExecReload=/bin/sh controller.sh restart\nRestart=always\nRestartSec=10s\nLimitNOFILE=infinity\nLimitNPROC=infinity\nLimitCORE=infinity\n\n[Install]\nWantedBy=multi-user.target\n```\n\n----------------------------------------\n\nTITLE: Unoptimized Group By Query with Intermediate Table in DolphinDB\nDESCRIPTION: A non-optimized approach using an intermediate memory table before performing group by calculations, which reduces parallel processing opportunities.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_SQL_Case_Tutorial.md#_snippet_10\n\nLANGUAGE: SQL\nCODE:\n```\ntimer {\n\ttmp_t = select *, iif(LastPx > OpenPx, 1, 0) as Flag \n\t\t\tfrom snapshot \n\t\t\twhere date(DateTime) = 2020.06.01, second(DateTime) >= 09:30:00\n\tt1 = select iif(max(OfferPrice1) - min(BidPrice1) == 0, 0, 1) as Price1Diff, count(OfferPrice1) as OfferPrice1Count, sum(Volume) as Volumes \n\t\t\tfrom tmp_t \n\t\t\tgroup by SecurityID, date(DateTime) as Date, Flag\n}\n```\n\n----------------------------------------\n\nTITLE: Executing dsTb Function Example - DolphinDB\nDESCRIPTION: This DolphinDB code demonstrates how to call the `dsTb` function. It defines parameters like `timeRS` (time repartition schema), `startDate`, `endDate`, `stkList` (stock list), and `replayName`. `timeRS` is created using `cutPoints` for time-based partitioning. It sets up date range, stock filter, and replay name, then calls `dsTb` to generate the data source.  The output `ds` will be a vector consisting of several DataSource, generated from the `replayDS` function. The prerequisite is having `cutPoints` and `dsTb` defined and a valid data setup in DFS.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/appendices_market_replay_bp.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntimeRS = cutPoints(09:30:00.000..15:00:00.000, 3)\nstartDate = 2021.12.01\nendDate = 2021.12.02\nstkList = ['000616.SZ']\nreplayName = [\"order\"]\nds = dsTb(timeRS, startDate, endDate, stkList, replayName)\n```\n\n----------------------------------------\n\nTITLE: Enable DolphinDB Resource Tracking\nDESCRIPTION: This snippet shows how to enable resource tracking on a DolphinDB data node using the `enableResourceTracking()` function. This function can only be used when resource tracking is already enabled (i.e., *resourceSamplingInterval* is a positive integer). It should be executed by an administrator on a data node.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/user_level_resource_tracking.md#_snippet_10\n\nLANGUAGE: DolphinDB\nCODE:\n```\nenableResourceTracking()\n```\n\n----------------------------------------\n\nTITLE: SVD Decomposition in DolphinDB\nDESCRIPTION: Demonstrates Singular Value Decomposition (SVD) of a matrix in DolphinDB using the `svd` function.  The function decomposes a matrix X into U, s, and Vh, where X = U * diag(s) * Vh. This example shows the full matrices mode.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/matrix.md#_snippet_22\n\nLANGUAGE: DolphinDB\nCODE:\n```\n>m=matrix([2 5 5, 5 5 4, 8 6 4, 7 6 8]);\n>m;\n#0 #1 #2 #3\n-- -- -- --\n2  5  8  7 \n5  5  6  6 \n5  4  4  8\n\n>u,s,vh = svd(m); //fullMatrices=true,computeUV=true\n>u;\n#0        #1        #2       \n--------- --------- ---------\n-0.604057 -0.734356 0.309574 \n-0.570062 0.126705  -0.811773\n-0.556906 0.666834  0.495165 \n>s;\n[19.202978,3.644527,1.721349]\n>vh;\n#0        #1        #2        #3       \n--------- --------- --------- ---------\n-0.356349 -0.421717 -0.545772 -0.63032 \n0.68568   -0.101776 -0.671497 0.261873 \n-0.559964 -0.308094 -0.240155 0.730646 \n-0.29883  0.846684  -0.439944 -0.016602\n\n```\n\n----------------------------------------\n\nTITLE: Testing Stream Engine with Continuous Data Feed in DolphinDB\nDESCRIPTION: This part of the code tests the stream engine's performance by feeding all the data from the `t` table continuously into the stream engine 'calChange'. The timer function is used to measure the time taken for the append operation of the whole table.  This tests the ability of the stream engine to handle a large volume of data in a single operation. Requires that 'calChange' stream engine has been initialized.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/Real-Time_Stock_Price_Increase_Calculation/05.性能测试.txt#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// test continuous\ntimer getStreamEngine(\"calChange\").append!(t)\n```\n\n----------------------------------------\n\nTITLE: Module Function Serialization in DolphinDB Scheduled Jobs\nDESCRIPTION: This example demonstrates how functions from a module are serialized in scheduled jobs, and updating the module doesn't affect already scheduled jobs.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/scheduledJob.md#_snippet_10\n\nLANGUAGE: DolphinDB\nCODE:\n```\nmodule printLog\ndef printLogs(logText){\n\twriteLog(string(now()) + \" : \" + logText)\n\tprint \"The old function is called\"\n}\n```\n\n----------------------------------------\n\nTITLE: Calculating Act Volume Percentages Using Pandas\nDESCRIPTION: Defines a function 'actVolumePercent' that calculates the proportion of active trading volume over a specified lag period for each security in trade data. It uses rolling sums over 'TradeQty' filtered by bid-ask sequence comparisons.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/DolphinDB_Python_Parser_Intro_for_Quantitative_Finance/因子实现_Python版本/主动成交量占比.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ndef actVolumePercent(trade, lag):   \n    res = trade[[\"TradeTime\", \"SecurityID\"]]\n    actVolume = (trade[\"TradeQty\"]*(trade['BidApplSeqNum'] > trade['OfferApplSeqNum'])).rolling(lag).sum()\n    totalVolume = trade[\"TradeQty\"].rolling(lag).sum()\n    res[\"actVolumePercent\"] = actVolume/totalVolume\n    return res\n```\n\n----------------------------------------\n\nTITLE: Retrieving Backup Partition Metadata with getBackupMeta Function\nDESCRIPTION: Fetches detailed metadata for a specific partition backup of a DolphinDB table using getBackupMeta. Returns a dictionary containing the schema with column names, types, and comments, along with the dfs path, chunkID, and partition identifier (cid). Required parameters include backup directory, database path, partition path string, and table name. This function facilitates inspecting backup structure and verifying backup content validity.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/restore-backup.md#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\ngetBackupMeta(\"/hdd/hdd1/backup/\",\"dfs://ddb\",\"/20200103/10\",\"windTurbine\")\n```\n\n----------------------------------------\n\nTITLE: Clearing DolphinDB Module Cache\nDESCRIPTION: Executes the `clearCachedModules` command, which removes all modules currently cached in the session or system memory. This forces DolphinDB to reload modules from their .dos files the next time they are used via the `use` keyword or accessed, allowing developers to quickly test changes made to module source code without restarting the DolphinDB server. This command requires admin privileges.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/module_tutorial.md#_snippet_18\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nclearCachedModules\n```\n\n----------------------------------------\n\nTITLE: Creating controller.sh Script for DolphinDB Controller Node\nDESCRIPTION: Shell script to start and stop the DolphinDB controller mode. It sets environment variables, launches the DolphinDB process with appropriate parameters, and handles process termination using `ps` and `kill`. Dependencies include the DolphinDB executable and configuration files located relative to the script.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/multi_machine_cluster_deployment.md#_snippet_12\n\nLANGUAGE: Bash\nCODE:\n```\n#!/bin/bash\n#controller.sh\nworkDir=$PWD\n\nstart(){\n    cd ${workDir} && export LD_LIBRARY_PATH=$(dirname \"$workDir\"):$LD_LIBRARY_PATH\n    nohup ./../dolphindb -console 0 -mode controller -home data -script dolphindb.dos -config config/controller.cfg -logFile log/controller.log -nodesFile config/cluster.nodes -clusterConfig config/cluster.cfg > controller.nohup 2>&1 &\n}\n\nstop(){\n    ps -o ruser=userForLongName -e -o pid,ppid,c,time,cmd |grep dolphindb|grep -v grep|grep $USER|grep controller| awk '{print $2}'| xargs kill -TERM\n}\n\ncase $1 in\n    start)\n        start\n        ;;\n    stop)\n        stop\n        ;;\n    restart)\n        stop\n        start\n        ;;\nesac\n```\n\n----------------------------------------\n\nTITLE: Inserting Test Data into 'stock_basic'\nDESCRIPTION: SQL command to insert sample data into the 'stock_basic' table for testing the synchronization pipeline with multiple tables.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Debezium_and_Kafka_data_sync.md#_snippet_7\n\nLANGUAGE: SQL\nCODE:\n```\ninsert into stock_basic(id,ts_code,symbol,name,area,industry,list_date)\nvalues (1,'000001.SZ','000001','平安银行','深圳','银行','1991-04-03'),\n(2,'000002.SZ','000002','万科A','深圳','地产','1991-01-29'),\n(3,'000004.SZ','000004','ST国华','深圳','软件服务','1991-01-14')\n```\n\n----------------------------------------\n\nTITLE: Processing Multiple Tick Stock Files with Exception Handling and Progress Bar in Python\nDESCRIPTION: Implements a multiprocessing-compatible function pool_func that processes multiple stock tick files within a given directory. For each tick file, it reads the CSV, computes the open bid to ask volume log ratio, and stores the result in a dataframe indexed by ticker code. The date is parsed from the directory path. The function handles exceptions by assigning NaN in case of errors and uses tqdm to display processing progress. It requires os, pandas, numpy, tqdm, and the openBidVolDvdAskVol function.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/DolphinDB_Python_Parser_Intro_for_Quantitative_Finance/因子实现_Python版本/早盘买卖单大小比.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef pool_func(tick_obj, trade_path_obj):\n    single_tick_res = pd.DataFrame(columns=[\"DATE\",\"openBidVolDvdAskVol\"])\n    tmp_date = trade_path_obj.split('/')[-2]\n    tmp_date = tmp_date[0:4] + \"-\" + tmp_date[4:6] + \"-\" + tmp_date[6:8]\n    for tick in tqdm(tick_obj):\n        single_tick_res.at[tick[:6], \"DATE\"] = tmp_date\n        try:\n            df = pd.read_csv(os.path.join(trade_path_obj, tick))\n\n            Indicator = openBidVolDvdAskVol(df)\n            single_tick_res.at[tick[:6], \"openBidVolDvdAskVol\"] = Indicator\n\n        except Exception as error:\n            single_tick_res.at[tick[:6], \"openBidVolDvdAskVol\"] = np.nan\n            continue\n\n    return single_tick_res\n\n\nclass multi_task_split:\n\n    def __init__(self, data, processes_to_use):\n        self.data = data\n        self.processes_to_use = processes_to_use\n\n    def num_of_jobs(self):\n        return min(len(self.data), self.processes_to_use, multiprocessing.cpu_count())\n\n    def split_args(self):\n        q, r = divmod(len(self.data), self.num_of_jobs())\n        return (self.data[i * q + min(i, r): (i + 1) * q + min(i + 1, r)] for i in range(self.num_of_jobs()))\n```\n\n----------------------------------------\n\nTITLE: Implementing Standard Diagonal Matrix Creation in DolphinDB\nDESCRIPTION: This snippet defines a standard DolphinDB function `diagonalMatrix` without the `@jit` annotation. It implements the same logic as the JIT version to create a diagonal matrix, using a nested loop structure to access and set matrix elements. This function serves as a baseline for performance comparison with its JIT-compiled counterpart.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/matrix.md#_snippet_36\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef diagonalMatrix(data, m){\n\tn=data.size()\n\tres=m\n\tfor( i in 0:n){\n\t \tx=m[i]\n\t \tfor(j in 0:n){\n\t \t\tif(i==j){\n\t \t\t\tx[j]=data[i]\n\t \t\t}\n\t \t}\n\t\tres[i]=x\n\t}\n\treturn res\n}\n```\n\n----------------------------------------\n\nTITLE: Clearing DolphinDB Stream Table Persistence Data\nDESCRIPTION: Removes the data files previously persisted to disk for the specified stream table. This function does not affect the in-memory data or disable persistence itself.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_tutorial.md#_snippet_11\n\nLANGUAGE: DolphinDB\nCODE:\n```\nclearTablePersistence(pubTable)\n```\n\n----------------------------------------\n\nTITLE: Partitioning Data for Replay with cutPoints and replayDS in DolphinDB Script\nDESCRIPTION: Demonstrates how to partition a large historical data set by time using cutPoints to create replay schema, then dividing the main quote data table with replayDS for memory-efficient segmented replay. Output is a vector of replay data sources, suitable for streaming or parallelized processing.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/historical_data_replay.md#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntrs = cutPoints(09:30:00.000..16:00:00.000, 60)\nrds = replayDS(<select * from quotes where date=2007.08.17>, `date, `time,  trs);\n```\n\n----------------------------------------\n\nTITLE: Logging In to DolphinDB Platform Using DolphinDB Script\nDESCRIPTION: This snippet performs the login operation to a DolphinDB session using administrator credentials. It requires valid DolphinDB credentials for access. No parameters besides username and password are required, and it does not produce a direct output but initializes the session for subsequent operations. This step is prerequisite before accessing or manipulating DolphinDB resources.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs MongoDB/dolphindb_taq_partitioned.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(\"admin\",\"123456\")\n```\n\n----------------------------------------\n\nTITLE: DolphinDB RPC Usage Examples for Cluster User Management and Inline Function Execution - DolphinDB\nDESCRIPTION: Shows RPC remote calls to control node aliases like 'master' to create a user with username and password, and executing a reduction sum function inline on a remote node 'nodeB'. This highlights DolphinDB RPC's flexibility in executing both registered and inline anonymous functions for administrative and computational tasks inside the cluster nodes.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/hybrid_programming_paradigms.md#_snippet_16\n\nLANGUAGE: DolphinDB\nCODE:\n```\nrpc(\"master\", createUser{\"jerry\", \"123456\"});\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nrpc(\"nodeB\", reduce{+, 1 2 3 4 5});\n```\n\n----------------------------------------\n\nTITLE: Streaming Data Injection for Real-Time Join Tables - DolphinDB\nDESCRIPTION: Illustrates how to emulate real-time data inflow by generating batch data and appending it to streaming tables already registered in DolphinDB. This is typically used for test data population or simulating live trade and snapshot feeds. It presumes prior creation and sharing of the required stream tables. Input: temporal and symbol data matching the output join structure. Output: data appears in streaming tables, which in turn triggers the associated join engine for incremental processing. Suitable for both functional validation and demo environments.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming-real-time-correlation-processing.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// generate data\nt1 = table(take(`A, 4) as Sym, 10:10:03.000+(10 2100 2890 6030) as Time, 12.5 12.5 12.6 12.6 as Price)\nt2 = table(take(`A, 4) as Sym, 10:10:00.000+(0 3000 6000 9000) as Time, 12.6 12.5 12.7 12.6 as BidPrice)\n// input data\nsnapshot.append!(t2)\ntrade.append!(t1)\n```\n\n----------------------------------------\n\nTITLE: Calculating Beta Coefficient Using DolphinDB\nDESCRIPTION: Defines getBeta function calculating beta, measuring sensitivity of the fund's returns relative to a benchmark price series. Computes covariance between daily returns of fund and benchmark divided by the standard deviation of the benchmark returns. Inputs include two numeric value vectors (portfolio and benchmark), output is beta numeric scalar.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_factor_by_ddb/fund_factor_ten.txt#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef getBeta(value, price){\n\treturn covar(deltas(value)\\prev(value), deltas(price)\\prev(price)) \\ std(deltas(price)\\prev(price))\n}\n```\n\n----------------------------------------\n\nTITLE: Loading CSV Data into DolphinDB with Schema Definition\nDESCRIPTION: Loads stock snapshot data from a CSV file into DolphinDB using a predefined schema with various data types, and orders the data by date and security ID. This setup prepares the dataset for further analysis and processing.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Streaming_computing_of_financial_quantifiers.md#_snippet_24\n\nLANGUAGE: DolphinDB\nCODE:\n```\ncolType = [\"SYMBOL\",\"TIMESTAMP\",\"DOUBLE\",\"DOUBLE\",\"DOUBLE\",\"DOUBLE\",\"DOUBLE\",\"INT\",\"DOUBLE\",\"SYMBOL\",\"DOUBLE[]\",\"INT[]\",\"INT[]\",\"INT[]\",\"DOUBLE[]\",\"INT[]\",\"INT[]\",\"INT[]\",\"INT\",\"INT\",\"INT\",\"INT\",\"DOUBLE\",\"DOUBLE\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"DOUBLE\",\"INT\",\"INT\",\"DOUBLE\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\",\"INT\"]\ndataArrayVector = select * from loadText(csvPath + \"snapshot_100stocks_arrayvector.csv\", schema=table(colName, colType)) order by dateTime, securityID\n```\n\n----------------------------------------\n\nTITLE: Generating Multiple Expressions with Aggregate Functions and sqlCol\nDESCRIPTION: This snippet demonstrates the generation of multiple aggregate expressions using `sqlCol`. It applies a function (`sum`) to multiple columns, each with its own alias. It showcases how `sqlCol` can be used to generate multiple aggregate expressions dynamically based on the input column names and aliases.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/macro_var_based_metaprogramming.md#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// sqlCol([\"col0\",\"col1\",\"col2\"], func=sum, alias=[\"newCol0\",\"newCol1\",\"newCol2\"])\n// --> [<sum(col0) as newCol0>, <sum(col1) as newCol1>, <sum(col2) as newCol2>]\n```\n\n----------------------------------------\n\nTITLE: DolphinDB Grouping by Date (Multiple Columns)\nDESCRIPTION: This DolphinDB script aggregates data and groups by the date. It calculates the average of the 'ASK' column and the maximum of the 'BID' column, grouping the results by 'date'. The `timer()` function measures the execution time over 10 runs. This test focuses on the performance of multiple aggregations on a date grouping.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_44\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\n//按时间分组（多列）\ntimer(10) select avg(ASK), max(BID) from trades group by date\n```\n\n----------------------------------------\n\nTITLE: Calculating Annual Skewness Using DolphinDB\nDESCRIPTION: Defines getAnnualSkew function to evaluate the skewness of the series return distribution. Uses the deltas/prev ratio to compute daily returns and then calculates their skewness. This metric helps to understand asymmetry in return distribution, useful in risk analysis. The function accepts a numerical vector and returns the skewness as a numeric scalar.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_factor_by_ddb/fund_factor_ten.txt#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndefg getAnnualSkew(value){\n\treturn skew(deltas(value)\\prev(value))\n}\n```\n\n----------------------------------------\n\nTITLE: Row-wise Aggregation of Matrices Using rowCount in DolphinDB\nDESCRIPTION: Illustrates the use of DolphinDB's rowCount function to count non-NULL elements across each row of a matrix. Inputs: matrix with possible NULLs. Output: vector where each element is the count of non-NULL entries in that row. Dependencies: matrix, rowCount. Useful for data validation and missing data analysis.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/panel_data.md#_snippet_26\n\nLANGUAGE: DolphinDB\nCODE:\n```\nm=matrix([4.5 NULL 1.5, 1.5 4.8 5.9, 4.9 2.0 NULL]);\nrowCount(m);\n```\n\n----------------------------------------\n\nTITLE: Inserting Time-Series Data using Line Protocol in DolphinDB\nDESCRIPTION: Inserts multiple data points into the 'readings' measurement using the InfluxDB Line Protocol format. Each line represents a single point containing the measurement name ('readings'), comma-separated tags (key=value pairs like device_id, battery_status), a space, comma-separated fields (key=value pairs like battery_level, cpu_avg_1min), another space, and the timestamp in nanoseconds since the epoch. The context comments indicate these writes target the 'test' database and 'one_day' retention policy.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs InfluxDB/readings.lineprotocol.txt#_snippet_1\n\nLANGUAGE: InfluxDB Line Protocol\nCODE:\n```\n# CONTEXT-DATABASE:test\n# CONTEXT-RETENTION-POLICY:one_day\nreadings,device_id=demo000000,battery_status=discharging,bssid=A0:B1:C5:D2:E0:F3,ssid=stealth-net battery_level=96,battery_temperature=91.7,cpu_avg_1min=5.26,cpu_avg_5min=6.172,cpu_avg_15min=6.51066666666667,mem_free=650609585,mem_used=349390415,rssi=-42 1479211200\nreadings,device_id=demo000001,battery_status=discharging,bssid=A0:B1:C5:D2:E0:F3,ssid=stealth-net battery_level=36,battery_temperature=87.9,cpu_avg_1min=7.23,cpu_avg_5min=6.886,cpu_avg_15min=7.01533333333333,mem_free=530709408,mem_used=469290592,rssi=-62 1479211200\nreadings,device_id=demo000002,battery_status=discharging,bssid=A0:B1:C5:D2:E0:F3,ssid=stealth-net battery_level=94,battery_temperature=88.3,cpu_avg_1min=5.17,cpu_avg_5min=7.034,cpu_avg_15min=7.53133333333333,mem_free=469434537,mem_used=530565463,rssi=-59 1479211200\nreadings,device_id=demo000003,battery_status=discharging,bssid=A0:B1:C5:D2:E0:F3,ssid=stealth-net battery_level=30,battery_temperature=92,cpu_avg_1min=6.48,cpu_avg_5min=8.416,cpu_avg_15min=8.92533333333333,mem_free=470670134,mem_used=529329866,rssi=-49 1479211200\nreadings,device_id=demo000004,battery_status=discharging,bssid=01:02:03:04:05:06,ssid=demo-net battery_level=88,battery_temperature=89,cpu_avg_1min=20.44,cpu_avg_5min=10.968,cpu_avg_15min=9.576,mem_free=470145855,mem_used=529854145,rssi=-48 1479211200\nreadings,device_id=demo000005,battery_status=discharging,bssid=01:02:03:04:05:06,ssid=demo-net battery_level=91,battery_temperature=92.7,cpu_avg_1min=8.98,cpu_avg_5min=8.596,cpu_avg_15min=8.71866666666667,mem_free=420020456,mem_used=579979544,rssi=-56 1479211200\nreadings,device_id=demo000006,battery_status=discharging,bssid=22:32:A2:B3:05:98,ssid=demo-5ghz battery_level=46,battery_temperature=93,cpu_avg_1min=29.27,cpu_avg_5min=10.574,cpu_avg_15min=7.64466666666667,mem_free=579480118,mem_used=420519882,rssi=-61 1479211200\nreadings,device_id=demo000007,battery_status=discharging,bssid=01:02:03:04:05:06,ssid=demo-net battery_level=34,battery_temperature=91.3,cpu_avg_1min=25.88,cpu_avg_5min=9.816,cpu_avg_15min=7.32533333333333,mem_free=449941315,mem_used=550058685,rssi=-61 1479211200\nreadings,device_id=demo000008,battery_status=discharging,bssid=A0:B1:C5:D2:E0:F3,ssid=stealth-net battery_level=41,battery_temperature=89.8,cpu_avg_1min=30.03,cpu_avg_5min=11.126,cpu_avg_15min=8.162,mem_free=460768013,mem_used=539231987,rssi=-54 1479211200\nreadings,device_id=demo000009,battery_status=discharging,bssid=A0:B1:C5:D2:E0:F3,ssid=stealth-net battery_level=99,battery_temperature=88.5,cpu_avg_1min=34.74,cpu_avg_5min=11.508,cpu_avg_15min=7.82266666666667,mem_free=429419192,mem_used=570580808,rssi=-51 1479211200\nreadings,device_id=demo000010,battery_status=discharging,bssid=22:32:A2:B3:05:98,ssid=demo-5ghz battery_level=47,battery_temperature=89.1,cpu_avg_1min=6.55,cpu_avg_5min=6.75,cpu_avg_15min=6.97,mem_free=670268240,mem_used=329731760,rssi=-48 1479211200\nreadings,device_id=demo000011,battery_status=discharging,bssid=01:02:03:04:05:06,ssid=demo-net battery_level=97,battery_temperature=91.5,cpu_avg_1min=27.47,cpu_avg_5min=9.574,cpu_avg_15min=6.778,mem_free=699451778,mem_used=300548222,rssi=-59 1479211200\nreadings,device_id=demo000012,battery_status=discharging,bssid=22:32:A2:B3:05:98,ssid=demo-5ghz battery_level=87,battery_temperature=89.4,cpu_avg_1min=33.9,cpu_avg_5min=13.26,cpu_avg_15min=10.0066666666667,mem_free=669831516,mem_used=330168484,rssi=-65 1479211200\nreadings,device_id=demo000013,battery_status=discharging,bssid=22:32:A2:B3:05:98,ssid=demo-5ghz battery_level=36,battery_temperature=91.1,cpu_avg_1min=4.11,cpu_avg_5min=5.862,cpu_avg_15min=6.34066666666667,mem_free=419244196,mem_used=580755804,rssi=-54 1479211200\nreadings,device_id=demo000014,battery_status=discharging,bssid=01:02:03:04:05:06,ssid=demo-net battery_level=34,battery_temperature=90.6,cpu_avg_1min=4.56,cpu_avg_5min=6.992,cpu_avg_15min=7.584,mem_free=599480631,mem_used=400519369,rssi=-43 1479211200\nreadings,device_id=demo000015,battery_status=discharging,bssid=01:02:03:04:05:06,ssid=demo-net battery_level=38,battery_temperature=87.8,cpu_avg_1min=29.51,cpu_avg_5min=12.702,cpu_avg_15min=10.0873333333333,mem_free=650690016,mem_used=349309984,rssi=-51 1479211200\nreadings,device_id=demo000016,battery_status=discharging,bssid=A0:B1:C5:D2:E0:F3,ssid=stealth-net battery_level=53,battery_temperature=92.9,cpu_avg_1min=4.99,cpu_avg_5min=5.078,cpu_avg_15min=5.27933333333333,mem_free=579023757,mem_used=420976243,rssi=-54 1479211200\nreadings,device_id=demo000017,battery_status=discharging,bssid=A0:B1:C5:D2:E0:F3,ssid=stealth-net battery_level=66,battery_temperature=87.4,cpu_avg_1min=23.01,cpu_avg_5min=9.722,cpu_avg_15min=7.694,mem_free=689457803,mem_used=310542197,rssi=-57 1479211200\nreadings,device_id=demo000018,battery_status=discharging,bssid=22:32:A2:B3:05:98,ssid=demo-5ghz battery_level=81,battery_temperature=90.9,cpu_avg_1min=8.59,cpu_avg_5min=5.798,cpu_avg_15min=5.51933333333333,mem_free=449526435,mem_used=550473565,rssi=-48 1479211200\nreadings,device_id=demo000019,battery_status=discharging,bssid=01:02:03:04:05:06,ssid=demo-net battery_level=72,battery_temperature=92.8,cpu_avg_1min=6.62,cpu_avg_5min=6.764,cpu_avg_15min=6.97466666666667,mem_free=469387324,mem_used=530612676,rssi=-48 1479211200\n```\n\n----------------------------------------\n\nTITLE: Granting Task Group Memory Limit - DolphinDB\nDESCRIPTION: This snippet shows how to limit the memory used by task groups (batch subqueries) for a specific user in DolphinDB. It uses the `grant` function with `TASK_GROUP_MEM_LIMIT` to set the limit for the 'AlexSmith' user to 4GB. This limits the memory that a batch of subqueries initiated by this user can consume.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Permission_Management.md#_snippet_43\n\nLANGUAGE: DolphinDB\nCODE:\n```\ngrant(\"AlexSmith\", TASK_GROUP_MEM_LIMIT, 4)\n```\n\n----------------------------------------\n\nTITLE: 更新Vector中的数据\nDESCRIPTION: 介绍更新Vector数据的多种方法，包括更新单个数据点、批量更新连续数据点及获取buffer后批量更新，并解释不同方法在性能上的差异。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/c++api.md#_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\n// 更新单个数据点\nVectorSP v = Util::createVector(DT_INT, 5, 5);\nfor(int i = 0; i < 5; ++i) {\n    v->setInt(i, i);\n}  // v = [0 1 2 3 4]\n\n// 批量更新连续数据点\nvector<int> tmp{5,4,3,2,1};\nv->setInt(0, 5, tmp.data());    // v = [5 4 3 2 1]\n\n// 批量更新数据点，不做类型检查\nvector<int> tmp{11, 22, 33, 44, 55};\nv->setData(0, 5, tmp.data());   // v = [11 22 33 44 55]\n\n// 获取buffer后批量更新（最高效方法）\nint buf[1024];\nint* p = v->getIntBuffer(0, 1024, buf);\n// p[0] = ...\nv->setInt(0, 1024, p);\n```\n\n----------------------------------------\n\nTITLE: 使用 Lookup Join 引擎将实时行情与历史指标关联的完整脚本\nDESCRIPTION: 该脚本创建两个表：实时行情快照表和历史日频指标表，并用 Lookup Join 引擎实现一对一实时关联。引擎会周期性刷新右表缓存，确保关联数据的实时性。在订阅实时数据后，关联结果会立即输出到输出表，适用于需要实时结合历史数据的场景。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming-real-time-correlation-processing.md#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// create table\nshare streamTable(1:0, `Sym`Time`Open`High`Low`Close, [SYMBOL, TIME, DOUBLE, DOUBLE, DOUBLE, DOUBLE]) as snapshot\nhistoricalData = table(`A`B as Sym, (0.8 0.2) as PreWeight, (3.1 7.6) as PreClose)\nshare table(1:0, `Sym`Time`Open`High`Low`Close`PreWeight`PreClose, [SYMBOL, TIME, DOUBLE, DOUBLE, DOUBLE, DOUBLE, DOUBLE, DOUBLE]) as output\n\n// create engine\nlookupJoinEngine = createLookupJoinEngine(name=\"lookupJoin\", leftTable=snapshot, rightTable=historicalData, outputTable=output, metrics=<[Time, Open, High, Low, Close, PreWeight, PreClose]>, matchingColumn=`Sym, checkTimes=10s)\n\n// subscribe topic\nsubscribeTable(tableName=\"snapshot\", actionName=\"appendLeftStream\", handler=getLeftStream(lookupJoinEngine), msgAsTable=true, offset=-1)\n```\n\n----------------------------------------\n\nTITLE: Creating Single-Value Model Database in DolphinDB\nDESCRIPTION: Function to create a database using the single-value model, with temporal and metric-based partitioning. This model stores each metric as a separate row with timestamp and value.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/iot_examples.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\nnumMachines=100\nnumMetrics=50\nnumMachinesPerPartition=2\nps1=2020.09.01..2020.12.31\nps2=(numMetrics*numMachinesPerPartition)*(0..(numMachines/numMachinesPerPartition))+1\n\ndef createDatabase(dbName,tableName, ps1, ps2){\n\ttableSchema = table(1:0,`id`datetime`value,[INT,DATETIME,FLOAT]);\n\tdb1 = database(\"\", VALUE, ps1)\n\tdb2 = database(\"\", RANGE, ps2)\n\tdb = database(dbName,COMPO,[db1,db2])\n\tdfsTable = db.createPartitionedTable(tableSchema,tableName,`datetime`id)\n}\ncreateDatabase(\"dfs://svmDemo\",\"sensors\", ps1, ps2)\n```\n\n----------------------------------------\n\nTITLE: Creating Database for Daily K-line Stock Data with OLAP Engine\nDESCRIPTION: Creates a database for storing daily K-line stock data using monthly range partitioning over a 30-year period. The OLAP engine is used instead of TSDB, and partitioning is done solely by tradetime without security partitioning.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practices_for_partitioned_storage.md#_snippet_9\n\nLANGUAGE: SQL\nCODE:\n```\ncreate database \"dfs://k_day_level\"\npartitioned by RANGE(2000.01M + (0..30)*12)\nengine='OLAP'\n\ncreate table \"dfs://k_day_level\".\"k_day\"(\n\tsecurityid SYMBOL  \n\ttradetime TIMESTAMP\n\topen DOUBLE        \n\tclose DOUBLE       \n\thigh DOUBLE        \n\tlow DOUBLE\n\tvol INT\n\tval DOUBLE\n\tvwap DOUBLE\n)\npartitioned by tradetime\n```\n\n----------------------------------------\n\nTITLE: Cluster Nodes Configuration File\nDESCRIPTION: Defines the cluster topology, specifying each node's network address, port, and role (controller, agent, data node) to establish the cluster layout.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/gray_scale_upgrade_ha.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\nlocalSite,mode\n10.10.11.1:8920:HActl43,controller\n10.10.11.2:8920:HActl44,controller\n10.10.11.3:8920:HActl45,controller\n10.10.11.1:8921:agent43,agent\n10.10.11.2:8921:agent44,agent\n10.10.11.3:8921:agent45,agent\n10.10.11.1:8922:node43,datanode\n10.10.11.2:8922:node44,datanode\n10.10.11.3:8922:node45,datanode\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Accumulation Function for Volume-Based Time Windows\nDESCRIPTION: This snippet defines a custom accumulation function `caclCumVol` that determines how to group stock data based on volume. It decides whether to include the next volume in the current group based on a target volume. The function returns the new cumulative volume if the next volume should be included; otherwise, it returns the next volume to start a new group.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_sql_cases.md#_snippet_32\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef caclCumVol(target, cumVol, nextVol) {\n\tnewVal = cumVol + nextVol\n\tif(newVal < target) return newVal\n\telse if(newVal - target > target - cumVol) return nextVol\n\telse return newVal\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring DolphinDB Controller Node (controller.cfg)\nDESCRIPTION: Configuration file for the DolphinDB controller node. It defines the local site address and alias, the number of local executors, maximum connections, maximum memory usage (4GB), web worker threads, general worker threads, and DFS replication settings (factor=1, reliability=0).\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_3\n\nLANGUAGE: ini\nCODE:\n```\nlocalSite=localhost:6920:ctl6920\nlocalExecutors=3\nmaxConnections=128\nmaxMemSize=4\nwebWorkerNum=4\nworkerNum=4\ndfsReplicationFactor=1\ndfsReplicaReliabilityLevel=0\n```\n\n----------------------------------------\n\nTITLE: Calculating Beta Coefficient in DolphinDB\nDESCRIPTION: Defines a DolphinDB function `getBeta` to calculate the Beta coefficient. It takes two vectors: `value` (fund daily net values) and `price` (benchmark daily values). It computes the covariance between their daily returns and divides by the standard deviation (variance is more standard, check formula context) of the benchmark's daily returns.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/fund_factor_contrasted_by_py.md#_snippet_14\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef getBeta(value, price){\n\treturn covar(deltas(value)\\prev(value), deltas(price)\\prev(price)) \\ std(deltas(price)\\prev(price))\n}\n```\n\n----------------------------------------\n\nTITLE: Analyzing Low-Fee Bond Funds in DolphinDB\nDESCRIPTION: Calculates the total fee for each fund by summing management fee, custodian fee, and service fee. Then identifies the bond funds with the lowest total fees, with and without index funds.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/public_fund_basic_analysis/basic_analysis_of_public_fund_open_market_data.txt#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// step1: calculate the total fees and assign it to the variable openFundFee \nfundFee = select *, (MFee + CFee + SFee) as Fee from fundData\n// step2: Query the top 50 funds with the lowest fees in bond type\nselect top 50 * from fundFee where Type == \"债券型\" order by Fee\n// step3: Query the top 50 funds with the lowest fees in bond type, excluding index type\nselect top 50 * from  fundFee where Type == \"债券型\", not(FullName like \"%指数%\") order by Fee\n```\n\n----------------------------------------\n\nTITLE: Subscribing ETF Funds - DolphinDB\nDESCRIPTION: This function, `subscribeEtfFund`, subscribes to ETF fund snapshot data.  It uses `amdQuote::getETFCodeList()` to fetch ETF codes, then subscribes to the snapshot stream with the ETF code list.  Requires a connection handle and reordered column names.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_implementation_for_AMD_Plugin.md#_snippet_9\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef subscribeEtfFund(handle, reorderedColNames) {\n    // 获取ETF代码表\n    codeList = exec securityCode from amdQuote::getETFCodeList()\n\n    amdQuote::subscribe(handle, `fundSnapshot, objByName(\"snapshot\"), 101, codeList, handleSnapshotSubs{reorderedColNames=reorderedColNames})\n    amdQuote::subscribe(handle, `fundSnapshot, objByName(\"snapshot\"), 102, codeList, handleSnapshotSubs{reorderedColNames=reorderedColNames})\n}\n```\n\n----------------------------------------\n\nTITLE: Calculating Alpha Coefficient Using DolphinDB\nDESCRIPTION: Defines getAlpha to calculate alpha, representing excess return of a portfolio compared to expected return predicted by beta and benchmark returns. It adjusts annual return by subtracting risk-free rate and beta-adjusted benchmark return. Inputs are fund value and benchmark price series, output numeric alpha scalar.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_factor_by_ddb/fund_factor_ten.txt#_snippet_8\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef getAlpha(value, price){\n\treturn getAnnualReturn(value) - 0.03 - getBeta(value, price) * (getAnnualReturn(price) - 0.03)\n}\n```\n\n----------------------------------------\n\nTITLE: Calculating Maximum Drawdown from Asset Value Series in DolphinDB\nDESCRIPTION: Defines 'getMaxDrawdown' which computes the maximum drawdown, the largest peak-to-trough decline, from a time series of asset values. It identifies indices of maximum cumulative values and their declines, returning zero if no drawdown exists. Input is a numeric series of values and output is the maximum drawdown ratio.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_factor_by_ddb/fund_factor_nine.txt#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef getMaxDrawdown(value){\n\ti = imax((cummax(value) - value) \\ cummax(value))\n\tif (i==0){\n\t\treturn 0\n\t}\n\tj = imax(value[:i])\n\treturn (value[j] - value[i]) \\ (value[j])\n}\n```\n\n----------------------------------------\n\nTITLE: Calculating Quarterly Average Fund Returns in DolphinDB\nDESCRIPTION: Converts the daily returns matrix (`returnsMatrix`) into an indexed matrix suitable for time-based operations. It then resamples the daily returns into quarterly intervals, calculating the average (mean) return for each fund within each quarter. Finally, it plots the quarterly average returns for a specific fund ('160211.SZ') as a line chart. Depends on the 'returnsMatrix' variable.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/public_fund_basic_analysis/basic_analysis_of_public_fund_net_value.txt#_snippet_5\n\nLANGUAGE: dolphindb\nCODE:\n```\nqavgReturns = returnsMatrix.setIndexedMatrix!().resample(\"Q\", mean)\t\n// plot partial results\nplot(qavgReturns[\"160211.SZ\"], chartType=LINE)\n```\n\n----------------------------------------\n\nTITLE: Systemd Service Configuration for DolphinDB Controller\nDESCRIPTION: Systemd service unit file to manage DolphinDB controller nodes, specifying working directory, start/stop commands, automatic restart policies, and resource limits. Requires modification of `WorkingDirectory` to the cluster directory.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/multi_machine_cluster_deployment.md#_snippet_14\n\nLANGUAGE: Java\nCODE:\n```\n[Unit]\nDescription=ddbcontroller\nDocumentation=https://www.dolphindb.com/\n\n[Service]\nType=forking\nWorkingDirectory=/home/DolphinDB/server/clusterDemo\nExecStart=/bin/sh controller.sh start\nExecStop=/bin/sh controller.sh stop\nExecReload=/bin/sh controller.sh restart\nRestart=always\nRestartSec=10s\nLimitNOFILE=infinity\nLimitNPROC=infinity\nLimitCORE=infinity\n\n[Install]\nWantedBy=multi-user.target\n```\n\n----------------------------------------\n\nTITLE: Querying TopN Moving Average Returns over Long Window - DolphinDB Script\nDESCRIPTION: Computes the moving average of returns (ratios of close) for each stock, using the top 10 largest volume observations in the past 100 records, partitioned by stock and date. Utilizes mavgTopN in a sliding window setup, highlighting usage for longer windows and aggregate behavioral analysis in high-frequency data. Output: windowed per-stock, per-date Top10 volume-mean return.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_TopN.md#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect windCode, tradingTime, mavgTopN(ratios(close), volume, 100, 10, false) as mavgTop10RatioClose from t context by windCode, date(tradingTime)\n```\n\n----------------------------------------\n\nTITLE: Creating Stream Data Persistence Database and Table (DolphinDB Script)\nDESCRIPTION: This script mirrors the structure of the historical data table creation but defines a separate database (`dfs://trade_stream`) and table (`trade`) intended for persisting incoming real-time stream data before or after processing. It uses the same schema, composite partitioning strategy (VALUE on `TradeTime`, HASH on `SecurityID`), and compression settings as the historical table, ensuring consistency.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_capital_flow_order_by_order.md#_snippet_4\n\nLANGUAGE: dolphindb\nCODE:\n```\n//login account\nlogin(\"admin\", \"123456\")\n//create database and table\ndbName = \"dfs://trade_stream\"\ntbName = \"trade\"\nif(existsDatabase(dbName)){\n\tdropDatabase(dbName)\n}\ndb1 = database(, VALUE, 2020.01.01..2022.01.01)\ndb2 = database(, HASH, [SYMBOL, 5])\ndb = database(dbName, COMPO, [db1, db2])\nschemaTable = table(\n\tarray(SYMBOL, 0) as SecurityID,\n\tarray(SYMBOL, 0) as Market,\n\tarray(TIMESTAMP, 0) as TradeTime,\n\tarray(DOUBLE, 0) as TradePrice,\n\tarray(INT, 0) as TradeQty,\n\tarray(DOUBLE, 0) as TradeAmount,\n\tarray(INT, 0) as BuyNum,\n\tarray(INT, 0) as SellNum\n)\ndb.createPartitionedTable(table=schemaTable, tableName=tbName, partitionColumns=`TradeTime`SecurityID, compressMethods={TradeTime:\"delta\"})\n```\n\n----------------------------------------\n\nTITLE: Applying Multi-argument Functions and Assigning Alias with sqlColAlias\nDESCRIPTION: The snippet demonstrates `sqlColAlias` with `makeCall` used for multi-argument functions and also aliasing. The expression is a generic multi-argument function applied to multiple columns. The function, column arguments, and the alias are all dynamically generated using the `makeCall` function to form more complex expressions.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/macro_var_based_metaprogramming.md#_snippet_8\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// sqlColAlias(makeCall(corr, sqlCol(\"col0\"), sqlCol(\"col1\")), \"newCol\")\n// --> <func(col1, col2, …, colN) as newCol>\n```\n\n----------------------------------------\n\nTITLE: Calculating Annualized Volatility in Python\nDESCRIPTION: Defines a Python function `getAnnualVolatility` using NumPy. It computes daily returns from the input `value` array, calculates their sample standard deviation (`np.std` with `ddof=1`), and annualizes by multiplying with the square root of 252. Requires the NumPy library.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/fund_factor_contrasted_by_py.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\ndef getAnnualVolatility(value):\n    diff_value = np.diff(value)\n    rolling_value = np.roll(value, 1)\n    rolling_value = np.delete(rolling_value, [0])\n    return np.std(np.true_divide(diff_value, rolling_value), ddof=1) * np.sqrt(252)\n```\n\n----------------------------------------\n\nTITLE: Batch Updating Integer Vector Data with setInt (buffer) C++\nDESCRIPTION: Demonstrates how to efficiently update a range of integer elements in a DolphinDB Vector using the `setInt(start, len, buf)` method. Data is prepared in a local buffer and then copied into the vector in batches.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_advance.md#_snippet_15\n\nLANGUAGE: C++\nCODE:\n```\nconst int size = 100000000;\nconst int BUF_SIZE = 1024;\nint tmp[1024];\nVectorSP pVec = Util::createVector(DT_INT, size);\nint start = 0;\nwhile(start < size) {\n    int len =  std::min(size - start, BUF_SIZE);\n    for(int i = 0; i < len; ++i) {\n        tmp[i] = i;\n    }\n    pVec->setInt(start, len, tmp);\n    start += len;\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring DolphinDB - controller.cfg\nDESCRIPTION: This DolphinDB configuration file (`controller.cfg`) defines the settings for the controller node, which manages the cluster.  It specifies parameters like the local site address, number of executors, maximum connections, memory size, and replication settings. Dependencies: DolphinDB installation.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_Druid_appendix.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlocalSite=localhost:9919:ctl9919\nlocalExecutors=3\nmaxConnections=128\nmaxMemSize=4\nwebWorkerNum=4\nworkerNum=4\ndfsReplicationFactor=1\ndfsReplicaReliabilityLevel=0\nenableDFS=1\nenableHTTPS=0\n```\n\n----------------------------------------\n\nTITLE: Row Sum Calculation for Array Vector\nDESCRIPTION: Calculates the sum of each row in an Array Vector and Columnar Tuple in DolphinDB using the `rowSum` function. The example demonstrates its use with direct Array Vectors, Columnar Tuples, and within a table query. It showcases the function's ability to sum elements row by row.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Array_Vector.md#_snippet_11\n\nLANGUAGE: DolphinDB\nCODE:\n```\nx = array(INT[], 0).append!([1 2 3, 4 5, 6 7 8, 9 10])\nz = rowSum(x)\n/* z\n[6,9,21,19]\n*/\n\ny = [1 2 3, 4 5, 6 7 8, 9 10].setColumnarTuple!()\nz = rowSum(y)\n/* z\n[6,9,21,19]\n*/\n\nt = table(1 2 3 4 as id, x as x, y as y)\nnew_t = select *, rowSum(x) as new_x, rowSum(y) as new_y from t\n/* new_t\nid x       y       new_x new_y\n-- ------- ------- ----- -----\n1  [1,2,3] [1,2,3] 6     6    \n2  [4,5]   [4,5]   9     9    \n3  [6,7,8] [6,7,8] 21    21   \n4  [9,10]  [9,10]  19    19   \n*/\n```\n\n----------------------------------------\n\nTITLE: Partial Application to Compute Correlations Between Vector and Matrix Columns in DolphinDB - DolphinDB\nDESCRIPTION: Shows usage of partial application to create new functions by fixing some arguments. The example computes correlation between a fixed vector 'a' and each column of matrix 'm' applying 'each' with partially applied 'corr{a}'. It contrasts with an imperative approach using a for loop to calculate correlations column-wise, which is verbose and less performant. Demonstrates how partial application simplifies higher-order function usage by encapsulating fixed parameters elegantly.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/hybrid_programming_paradigms.md#_snippet_8\n\nLANGUAGE: DolphinDB\nCODE:\n```\na = 12 14 18\nm = matrix(5 6 7, 1 3 2, 8 7 11)\n\neach(corr{a}, m)\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\ncols = m.columns()\nc = array(DOUBLE, cols)\nfor(i in 0:cols)\n    c[i] = corr(a, m[i])\n```\n\n----------------------------------------\n\nTITLE: Calculating Weighted Moving Average (wma) in DolphinDB\nDESCRIPTION: This snippet demonstrates the direct application of the `wma` function from the ta module to a vector of closing prices. It calculates the weighted moving average over a specified period (5 in this case). It requires the ta module to be loaded in DolphinDB and the `close` variable should contain numerical data. The output is a vector of the weighted moving average values.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ta.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\nuse ta\nclose = 7.2 6.97 7.08 6.74 6.49 5.9 6.26 5.9 5.35 5.63\nx = wma(close, 5);\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Data Type Limitations in DolphinDB Reactive State Engine\nDESCRIPTION: This example demonstrates data type limitations when passing parameters between state and non-state functions in DolphinDB's reactive state engine. It shows that scalars, vectors, and array vectors are supported, while ANY vectors (tuples) cannot be properly passed between functions.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Streaming_computing_of_financial_quantifiers.md#_snippet_11\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 因子实现\ndef typeTestNonStateFunc(scalar, vector, arrayVector, anyVector){\n\tprint(\"---------------------------------------\")\n\tprint(typestr(scalar))\n\tprint(scalar)\n\tprint(typestr(vector))\n\tprint(vector)\n\tprint(typestr(arrayVector))\n\tprint(arrayVector)\n\tprint(typestr(anyVector))\n\tprint(anyVector)\n\treturn fixedLengthArrayVector(rowSum(arrayVector), rowAvg(arrayVector))\n}\n\n@state\ndef typeTestStateFunc(price1, price2, price3, lag){\n\tscalar = lag\n\tvector = price1\n\tarrayVector = fixedLengthArrayVector(price1, price2, price3)\n\tanyVector = [price1, price2, price3]\n\tres = typeTestNonStateFunc(scalar, vector, arrayVector, anyVector)\n\tsumRes = res[0]\n\tavgRes = res[1]\n\treturn sumRes, avgRes, res, anyVector\n}\n```\n\n----------------------------------------\n\nTITLE: Using bBands Function and Context by SQL in DolphinDB\nDESCRIPTION: This example showcases how to use the `bBands` function, which returns multiple columns (low, mid, high), within a DolphinDB SQL query. The `context by symbol` clause is used to perform calculations separately for each stock in the `t` table. The `select *, bBands(close, 5, 2, 2, 2) as `high`mid`low from t context by symbol` statement demonstrates the structured output, including the original columns and the Bollinger Band values.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ta.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\nclose = 7.2 6.97 7.08 6.74 6.49 5.9 6.26 5.9 5.35 5.63 3.81 3.935 4.04 3.74 3.7 3.33 3.64 3.31 2.69 2.72\ndate = (2020.03.02 + 0..4 join 7..11).take(20)\nsymbol = take(`F,10) join take(`GPRO,10)\nt = table(symbol, date, close) \nselect *, bBands(close, 5, 2, 2, 2) as `high`mid`low from t context by symbol\n```\n\n----------------------------------------\n\nTITLE: Simplifying Pipeline Creation with DolphinDB streamEngineParser\nDESCRIPTION: Demonstrates using `streamEngineParser` to automatically build a stream engine pipeline from a complex metrics expression containing both time-series and cross-sectional operations, simplifying engine setup compared to manual chaining.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_tutorial.md#_snippet_19\n\nLANGUAGE: DolphinDB\nCODE:\n```\n@state\ndef wqAlpha1TS(close){\n    ret = ratios(close) - 1\n    v = iif(ret < 0, mstd(ret, 20), close)\n    return mimax(signum(v)*v*v, 5)\n}\n\n//构建计算因子\nmetrics=<[sym, rowRank(wqAlpha1TS(close), percent=true)- 0.5]>\n\nstreamEngine=streamEngineParser(name=`alpha1_parser, metrics=metrics, dummyTable=input, outputTable=resultTable, keyColumn=`sym, timeColumn=`time, triggeringPattern='keyCount', triggeringInterval=3000)\n```\n\n----------------------------------------\n\nTITLE: Configuring DolphinDB Module Preload\nDESCRIPTION: Sets the `preloadModules` configuration parameter to specify modules that should be loaded as system-built-in functions upon DolphinDB startup. Multiple modules can be listed separated by commas. This configuration ensures the module functions are globally accessible to all sessions from the beginning, similar to using `loadModule`. Functions loaded via this method must be called using their namespace.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/module_tutorial.md#_snippet_7\n\nLANGUAGE: Configuration\nCODE:\n```\npreloadModules=fileLog\n```\n\n----------------------------------------\n\nTITLE: Querying Cluster Replication Metrics\nDESCRIPTION: This DolphinDB script retrieves cluster replication metrics from the slave cluster within a specified time interval, using the `getClusterReplicationMetrics` function via `rpc`. This function can provide metrics like the average execution time. It needs a running DolphinDB slave cluster that has been configured for replication and a configured time interval as a parameter.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Asynchronous_Replication.md#_snippet_19\n\nLANGUAGE: dolphindb\nCODE:\n```\nrpc(getControllerAlias(), getClusterReplicationMetrics, 30)\n```\n\n----------------------------------------\n\nTITLE: Backing Up DolphinDB Cluster Metadata Files Using Shell Commands\nDESCRIPTION: This sequence of shell commands creates a backup directory and copies critical cluster metadata files from the control node and data node storage directories into the backup folder. The backup ensures data integrity before upgrading the cluster. Paths and file names pertain to default storage configurations but may vary according to custom dfsMetaDir and chunkMetaDir parameters.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/multi_machine_cluster_deployment.md#_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\nmkdir backup\ncp -r DFSMetaLog.0 backup\ncp -r DFSMasterMetaCheckpoint.0 backup\n```\n\nLANGUAGE: Shell\nCODE:\n```\ncp -r CHUNK_METADATA ../../backup\n```\n\n----------------------------------------\n\nTITLE: QR Decomposition with raw mode in DolphinDB\nDESCRIPTION: Demonstrates QR decomposition of a matrix in DolphinDB using the `qr` function with `mode=`raw` and `pivoting=false`. The function returns the transformation matrix h, the transformation factor tau, and the R matrix.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/matrix.md#_snippet_20\n\nLANGUAGE: DolphinDB\nCODE:\n```\n>m=matrix([2 5 5, 5 5 4, 8 6 4, 7 6 8]);\n>m;\n#0 #1 #2 #3\n-- -- -- --\n2  5  8  7 \n5  5  6  6 \n5  4  4  8 \n\n>h,tau,r=qr(m,mode=`raw,pivoting=false);\n>h;\n#0        #1        #2        #3        \n--------- --------- --------- ----------\n-7.348469 -7.484552 -8.981462 -11.430952\n0.534847  3.159348  5.943561  3.622407  \n0.534847  0.553547  -0.086146 2.282872  \n>tau;\n[1.272166,1.530908,0]\n>r;\n#0        #1        #2        #3        \n--------- --------- --------- ----------\n-7.348469 -7.484552 -8.981462 -11.430952\n0         3.159348  5.943561  3.622407  \n0         0         -0.086146 2.282872\n\n```\n\n----------------------------------------\n\nTITLE: Querying a Stock's Factors (Wide Table)\nDESCRIPTION: This code snippet demonstrates querying a wide table to retrieve data for a specific stock, showing the selection of specific columns. The query aims to fetch data for all factors related to a given stock (symbol) within the `tsdb_wide_min_factor` table, showing `mtime` and `factorname` columns.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practice_for_factor_calculation.md#_snippet_22\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect mtime,factorname,sz000001 from tsdb_wide_min_factor\n```\n\n----------------------------------------\n\nTITLE: Creating a test table with random ids\nDESCRIPTION: This code creates a DolphinDB table named `t` with a single column named `id`. The `id` column contains 20 random integers between 1 and 100 inclusive.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_53\n\nLANGUAGE: shell\nCODE:\n```\nt = table(rand(1..100,20) as id)\n```\n\n----------------------------------------\n\nTITLE: DDBDataLoader Class Interface Definition\nDESCRIPTION: Complete interface definition for the DDBDataLoader class, showing all parameters including required and optional ones for customizing data loading behavior.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ai_dataloader_ml.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nDDBDataLoader(\n    ddbSession: Session,\n    sql: str,\n    targetCol: List[str],\n    batchSize: int = 1,\n    shuffle: bool = True,\n    windowSize: Union[List[int], int, None] = None,\n    windowStride: Union[List[int], int, None] = None,\n    *,\n    inputCol: Optional[List[str]] = None,\n    excludeCol: Optional[List[str]] = None,\n    repartitionCol: str = None,\n    repartitionScheme: List[str] = None,\n    groupCol: str = None,   \n    groupScheme: List[str] = None,\n    seed: Optional[int] = None,\n    dropLast: bool = False,\n    offset: int = None,\n    device: str = \"cpu\",\n    prefetchBatch: int = 1,\n    prepartitionNum: int = 2,\n    groupPoolSize: int = 3,\n    **kwargs\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid - coordinator\nDESCRIPTION: Configuration settings for the Druid coordinator service, focusing on memory allocation (Xms, Xmx), HTTP server thread count, processing threads and buffers, segment cache locations, and server sizes. Dependencies: Druid setup.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_Druid_appendix.md#_snippet_6\n\nLANGUAGE: Druid\nCODE:\n```\nXms3g\nXmx3g\n\nhistorical:\nXms8g\nXmx8g\n\n# HTTP server threads\ndruid.server.http.numThreads=25\n\n# Processing threads and buffers\ndruid.processing.buffer.sizeBytes=2147483648\ndruid.processing.numThreads=7\n\n# Segment storage\ndruid.segmentCache.locations=[{\"path\":\"var/druid/segment-cache\",\"maxSize\":0}]\ndruid.server.maxSize=130000000000\n\ndruid.historical.cache.useCache=false\ndruid.historical.cache.populateCache=false\n```\n\n----------------------------------------\n\nTITLE: Filtering Rows with Multiple Dynamic Conditions Using DolphinDB Function-Based Metaprogramming\nDESCRIPTION: This example creates a dynamic WHERE clause filtering a table's rows where the flagName column matches any of multiple patterns using DolphinDB's function-based metaprogramming. It uses higher-order functions like eachRight to apply the like function for pattern matching on the column. Multiple filters are combined with rowOr to form a compound predicate. The resulting condition is passed to sql's where argument to perform dynamic filtering of records. The snippet requires no external libraries beyond DolphinDB's built-in functions and supports multiple dynamic patterns for flexible filtering.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/macro_var_based_metaprogramming.md#_snippet_27\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef(col, pattern){return rowOr(like:R(col,pattern))}\n\npattern = [\"%新能源%\", \"%光伏%\"]\ncol= \"flagName\"\nwhereCond=makeCall(def(col, pattern){return rowOr(like:R(col,pattern))}, sqlCol(col), pattern)\nsql(select=sqlCol(\"val\"), from=t, where=whereCond).eval()\n```\n\n----------------------------------------\n\nTITLE: Complete code for calculating ln and clean columns\nDESCRIPTION: This code combines the previous snippets to calculate the 'ln' and 'clean' columns. It creates a table, calculates 'ln' using 'mavg' and 'prev', defines the 'cleanFun' function, and applies it to the 'ln' column using 'eachPre'.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_32\n\nLANGUAGE: shell\nCODE:\n```\nF = 0.02\nt = table(take(`a`b`c`d`e ,100) as sym, rand(100.0,100) as bidPrice)\nt2 = select *, log(bidPrice / prev(mavg(bidPrice,3))) as ln from t\ndef cleanFun(F,x,y) : iif(abs(x) > F, y,x)\nt2[`clean] = eachPre(cleanFun{F}, t2[`ln])\n```\n\n----------------------------------------\n\nTITLE: 读取CSV文件定义的函数\nDESCRIPTION: 定义两个函数来读取CSV文件：一个用于获取列名，另一个用于读取数据内容并转换为矩阵格式。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/fund_factor_contrasted_by_py.md#_snippet_20\n\nLANGUAGE: dolphindb\nCODE:\n```\n//获取 CSV 数据列名\ndef readColumnsFromWideCSV(absoluteFilename){\n        schema1 = extractTextSchema(absoluteFilename)\n        update schema1 set type = `STRING \n        allSymbol = loadText(absoluteFilename,,schema1)[0, 1:]\n        titleSchema = extractTextSchema(absoluteFilename, skipRows = 0);\n        for(x in allSymbol){\n                testValue = exec x[name] from titleSchema\n                testValue = testValue[1:]\n        }\n        return testValue\n}\n//获取 CSV 文件数据内容\ndef readIndexedMatrixFromWideCSV(absoluteFilename){\n        contracts = readColumnsFromWideCSV(absoluteFilename)\n        dataZoneSchema = extractTextSchema(absoluteFilename, skipRows = 1)\n        update dataZoneSchema set type = \"DOUBLE\" where name != \"col0\"//所有行除第一行外全部改成double\n        update dataZoneSchema set type = \"DATE\" where name = \"col0\"//所有行除第一行外全部改成DATE\n        dataZoneWithIndexColumn = loadText(absoluteFilename, skipRows = 1, schema = dataZoneSchema)\n        indexVector = exec col0 from dataZoneWithIndexColumn\n        dataZoneWithoutIndex = dataZoneWithIndexColumn[:, 1:]\n        dataMatrix = matrix(dataZoneWithoutIndex)\n        dataMatrix.rename!(indexVector, contracts)\n        return dataMatrix\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Trip Durations Over Time in DolphinDB for Grafana\nDESCRIPTION: This DolphinDB SQL-like query retrieves the pickup timestamp and the predicted trip duration in minutes (assuming the stored `duration` requires exponentiation and conversion from seconds) from `predictTable`. It filters for trips on the latest date, intended for plotting trip duration trends over time in Grafana.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Forecast_of_Taxi_Trip_Duration.md#_snippet_5\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nselect pickup_datetime, (exp(duration)-1)/60 as duration from predictTable \nwhere date(predictTable.pickup_datetime) == date(select max(pickup_datetime) from predictTable)\n```\n\n----------------------------------------\n\nTITLE: Creating Database for Minute K-line Stock Data with OLAP Engine\nDESCRIPTION: Creates a database for storing minute K-line stock data using daily value partitioning. The OLAP engine is used with tradetime as the partitioning column. Each partition contains minute K data for all stocks on a given day.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practices_for_partitioned_storage.md#_snippet_10\n\nLANGUAGE: SQL\nCODE:\n```\ncreate database \"dfs://k_minute_level\"\npartitioned by VALUE(2020.01.01..2021.01.01)\nengine='OLAP'\n\ncreate table \"dfs://k_minute_level\".\"k_minute\"(\n\tsecurityid SYMBOL  \n\ttradetime TIMESTAMP\n\topen DOUBLE        \n\tclose DOUBLE       \n\thigh DOUBLE        \n\tlow DOUBLE\n\tvol INT\n\tval DOUBLE\n\tvwap DOUBLE\n)\npartitioned by tradetime\n```\n\n----------------------------------------\n\nTITLE: Scheduling a Repeated Redis Data Synchronization Job in DolphinDB\nDESCRIPTION: Implements a repetitive job that invokes the redisjob function every 500 milliseconds indefinitely, effectively synchronizing the latest data from DolphinDB to Redis with high frequency. The job is submitted asynchronously under the name 'submitByHalfSecond' to run persistently in the DolphinDB environment.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/redis_plugin_tutorial/example3-kafka.txt#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef submitByHalfSecond(){\n    do {\n        redisjob()\n        sleep(500)\n    } while(true)\n}\nsubmitJob(\"submitByHalfSecond\", \"submitByHalfSecond\", submitByHalfSecond)\n```\n\n----------------------------------------\n\nTITLE: Group By Parallel Query (Optimized - Direct Query)\nDESCRIPTION: This code snippet illustrates an optimized approach to grouping and aggregating data by directly querying the distributed table. This avoids creating intermediate in-memory tables, improving performance by leveraging parallel computation.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_sql_cases.md#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntimer t2 = select iif(max(OfferPrice1) - min(BidPrice1) == 0, 0, 1) as Price1Diff, count(OfferPrice1) as OfferPrice1Count, sum(Volume) as Volumes \n\t\t\tfrom snapshot \n\t\t\twhere date(DateTime) = 2020.06.01, second(DateTime) >= 09:30:00 \n\t\t\tgroup by SecurityID, date(DateTime) as Date, iif(LastPx > OpenPx, 1, 0) as Flag\n```\n\n----------------------------------------\n\nTITLE: Defining a function to clean the ln column\nDESCRIPTION: This code defines a function named 'cleanFun' that takes three parameters: 'F' (a threshold), 'x' (the current value of 'ln'), and 'y' (the previous value of 'ln'). It calculates the absolute value of 'x'. If the absolute value of 'x' is greater than 'F', it returns 'y'; otherwise, it returns 'x'.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_30\n\nLANGUAGE: shell\nCODE:\n```\nF = 0.02\ndef cleanFun(F, x, y): iif(abs(x) > F, y, x)\n```\n\n----------------------------------------\n\nTITLE: Computing Sharpe Ratio with 3% Risk-Free Rate in DolphinDB\nDESCRIPTION: Defines 'getSharp' to calculate the Sharpe ratio, a key risk-adjusted return metric, by taking the annualized return minus a fixed 3% risk-free rate, divided by the annualized volatility of an asset. It depends on previously defined annual return and volatility functions and outputs the Sharpe ratio as a numeric scalar.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_factor_by_ddb/fund_factor_nine.txt#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndefg getSharp(value){\n\treturn (getAnnualReturn(value) - 0.03)\\getAnnualVolatility(value) as sharpeRat\n}\n```\n\n----------------------------------------\n\nTITLE: Making Predictions with the Trained Model\nDESCRIPTION: This code snippet uses the trained random forest model (`model`) to predict the labels for the test dataset (`wineTest`). The `predict` method is called on the model, and the predicted labels are stored in the `predicted` variable. It calculates the accuracy of the predictions by comparing them with the actual labels in `wineTest.Label`.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/machine_learning.md#_snippet_4\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\npredicted = model.predict(wineTest)\n> sum(predicted == wineTest.Label) \\ wineTest.size();\n\n0.925926\n```\n\n----------------------------------------\n\nTITLE: Applying Functions and Assigning Alias with sqlColAlias\nDESCRIPTION: This snippet combines `sqlColAlias` with `makeCall` to apply a function (e.g., sum) to a column and assign an alias.  The generated SQL expression will include the function applied to the column and the assigned alias. This pattern is useful for complex calculations that require aliasing.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/macro_var_based_metaprogramming.md#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// sqlColAlias(makeCall(sum, sqlCol(\"col\")), \"newCol\") \n// --> <func(col) as newCol>\n```\n\n----------------------------------------\n\nTITLE: Indexed Matrix Creation in DolphinDB\nDESCRIPTION: Creates an indexed matrix `m` with specified row and column names using `matrix`, `rename!`, and `setIndexedMatrix!` functions. The row names are dates from 2020.01.01 to 2020.01.04, and 2020.01.06, and column names are `A` and `B`.  This indexed matrix is later used for time-series based window calculations.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/window_cal.md#_snippet_16\n\nLANGUAGE: DolphinDB\nCODE:\n```\nm=matrix(1..4 join 6, 11..13 join 8..9)\nm.rename!(2020.01.01..2020.01.04 join 2020.01.06,`A`B)\nm.setIndexedMatrix!();\n```\n\n----------------------------------------\n\nTITLE: Querying by Device ID Equality and Counting Results with MongoDB - JavaScript\nDESCRIPTION: This snippet retrieves and counts documents matching a specific 'device_id' in the 'device_readings' collection using MongoDB's .find() method followed by .count(). Input is the device ID to filter. Requires MongoDB and the target collection. Returns the number of matches for the specified ID.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs MongoDB/mongodb_readings_query.txt#_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\ndb.device_readings.explain(\"executionStats\").find({device_id:{\"$eq\":\"demo000101\"}},{}).count()\n```\n\n----------------------------------------\n\nTITLE: Checking DolphinDB Process Linux\nDESCRIPTION: This command checks if the DolphinDB server process is running by using `ps aux` and `grep`. It searches for the string \"dolphindb\" in the process list, to confirm if the server has been launched successfully.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/standalone_server.md#_snippet_11\n\nLANGUAGE: sh\nCODE:\n```\nps aux|grep dolphindb\n```\n\n----------------------------------------\n\nTITLE: Subscribing to Aggregated Features Stream (DolphinDB Script)\nDESCRIPTION: Subscribes the custom `predictRV` handler function to the `aggrFeatures10min` table. This sets up the real-time prediction step, where aggregated features are fed into the handler for scoring using the loaded XGBoost model and appending results to `result10min`.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/metacode_derived_features/metacode_derived_features_streaming.txt#_snippet_15\n\nLANGUAGE: DolphinDB\nCODE:\n```\nsubscribeTable(tableName=\"aggrFeatures10min\", actionName=\"predictRV\", offset=-1, handler=predictRV{result10min, model}, msgAsTable=true, hash=1, reconnect=true)\n```\n\n----------------------------------------\n\nTITLE: Loading Another TSDB Partition and Checking Cache (DolphinDB Script)\nDESCRIPTION: Loads data from a second partition ('2021.08.08') of the TSDB table and checks cache usage again. This shows the cumulative effect on the index cache (usage increases) while confirming that TSDB primarily caches indices, not large amounts of raw data, as overall memory usage changes minimally.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/memory_management.md#_snippet_15\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nselect * from loadTable(\"dfs://TSDB_db1\",`pt1) where date=2021.08.08\ngetLevelFileIndexCacheStatus().usage  \n//输出结果为：78256\nmem().allocatedBytes - mem().freeBytes\n//输出结果为：28561912\n```\n\n----------------------------------------\n\nTITLE: Loading Data Across Partitions on Different Nodes (OLAP) (DolphinDB Script)\nDESCRIPTION: Selects the top 100 rows from two partitions ('2022.01.02', '2022.01.03') located on different data nodes. This shows that each node loads the entire partition data it owns into its memory, and results are aggregated.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/memory_management.md#_snippet_8\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nselect top 100 * from loadTable(dbName,tableName) where day in [2022.01.02,2022.01.03]\nmem().allocatedBytes - mem().freeBytes\n```\n\n----------------------------------------\n\nTITLE: Creating/Ensuring OHLC DFS Table Existence in DolphinDB Script\nDESCRIPTION: Sets the `dbName` variable to \"dfs://stockFundOHLC\" and `tbName` to \"stockFundOHLC\". It then calls the `createStockFundOHLCDfsTB` function with these names to ensure that the necessary distributed database and partitioned table exist before attempting to save OHLC data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/K.md#_snippet_6\n\nLANGUAGE: dolphindb\nCODE:\n```\ndbName = \"dfs://stockFundOHLC\"\ntbName = \"stockFundOHLC\"\ncreateStockFundOHLCDfsTB(dbName, tbName)\n```\n\n----------------------------------------\n\nTITLE: Receiving Data from MQTT Server in DolphinDB\nDESCRIPTION: Loads the MQTT plugin and subscribes to data from an MQTT server. It uses `mqtt::createJsonParser` to parse JSON data and `mqtt::subscribe` to subscribe to the specified topic, host, and port, writing the data to the `inputSt` stream table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_streaming_application_in_IOT.md#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\nloadPlugin(getHomeDir()+\"/plugins/mqtt/PluginMQTTClient.txt\")// 也可用 preloadModules=plugins::mqtt 自动加载\ngo\nsp = mqtt::createJsonParser([SYMBOL,TIMESTAMP, INT], `tag`ts`value)\nmqtt::subscribe(host, port, topic, sp, inputSt)\n```\n\n----------------------------------------\n\nTITLE: Finding DolphinScheduler Process IDs (Bash)\nDESCRIPTION: This command lists all running processes (`ps aux`) and filters the output (`grep dolphinscheduler`) to find processes related to DolphinScheduler. It's used in troubleshooting scenarios to identify the Process IDs (PIDs) needed to manually terminate stuck processes if DolphinScheduler fails to start correctly.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/dolphinscheduler_integration.md#_snippet_25\n\nLANGUAGE: Bash\nCODE:\n```\nps aux | grep dolphinscheduler\n```\n\n----------------------------------------\n\nTITLE: Generating Panel Data from Vertical Tables\nDESCRIPTION: These SQL queries demonstrate how to generate panel data from a vertical table. It uses the pivot by tradetime, symbol command to transform vertical data into a panel data format suitable for further analysis, selecting the val and using the factorcode in the where clause.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practice_for_factor_calculation.md#_snippet_24\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//纵表模式取面板数据sql\nolap_factor_year_pivot_1=select val from olap_min_factor where factorcode=`f0002 pivot by tradetime,symbol \n```\n\n----------------------------------------\n\nTITLE: Extracting Explained Variance Ratio\nDESCRIPTION: This line of code extracts the 'explainedVarianceRatio' vector from the `pcaRes` dictionary, which contains the proportion of variance explained by each principal component. This information is useful for determining how many components to retain for dimensionality reduction.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/machine_learning.md#_snippet_12\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\n> pcaRes.explainedVarianceRatio;\n[0.209316,0.201225,0.121788,0.088709,0.077805,0.075314,0.058028,0.045604,0.038463,0.031485,0.021256,0.018073,0.012934]\n```\n\n----------------------------------------\n\nTITLE: Creating Price and Volume Matrices using `panel` in DolphinDB Script\nDESCRIPTION: Uses the `panel` function to convert both the 'price' and 'volume' columns from table 't' into separate matrices simultaneously. `t.timestamp` defines the rows, `t.sym` defines the columns, and `[t.price, t.volume]` specifies the data columns to convert. The result is a tuple containing the two matrices.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/panel_data.md#_snippet_10\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nprice, volume = panel(t.timestamp, t.sym, [t.price, t.volume]);\n```\n\n----------------------------------------\n\nTITLE: Defining Custom State Function factor1 (simplified)\nDESCRIPTION: This DolphinDB script defines a simplified version of the `factor1` function using the `@state` decorator. This shows how to define a state function and demonstrates how the Reactive State Engine processes it. It takes price as input and returns the calculated factor.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/reactive_state_engine.md#_snippet_6\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\n@state\ndef factor1(price) {\n    a = ema(price, 20)\n    b = ema(price, 40)\n    c = 1000 * sum_diff(a, b)\n    return  ema(c, 10) - ema(c, 20)\n}\n```\n\n----------------------------------------\n\nTITLE: AutoFitTableAppender Usage in C++\nDESCRIPTION: This code snippet shows how to use AutoFitTableAppender (AFTA) to write data to a DolphinDB table. It creates an AFTA object and appends data. AFTA automatically handles data type conversions, simplifying the writing process. This example depends on an existing connection object 'conn' and a table object 'bt'.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_cpp_api_connector.md#_snippet_5\n\nLANGUAGE: C++\nCODE:\n```\nAutoFitTableAppender appender(\"dfs://test_AutoFitTableAppender\", \"collect\", conn);\nappender.append(bt);\n```\n\n----------------------------------------\n\nTITLE: Granting DB_READ and DB_WRITE Permissions - DolphinDB\nDESCRIPTION: This code grants a user both DB_READ and DB_WRITE permissions to a specific database. This allows the user to perform all CRUD operations on tables within that database. It uses the `login`, `createUser`, and `grant` functions. Requires admin privileges to create user.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Permission_Management.md#_snippet_22\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(\"admin\",\"123456\")\ncreateUser(\"user1\",\"123456\")\ngrant(\"user1\",DB_READ,\"dfs://valuedb\")\ngrant(\"user1\",DB_WRITE,\"dfs://valuedb\")\n```\n\n----------------------------------------\n\nTITLE: Subscribe to Stream Table - DolphinDB\nDESCRIPTION: Subscribes to the `Trade` stream table and appends incoming data to the `tsAggr` time series aggregator. The `msgAsTable=true` parameter ensures that the data is passed as a table. `offset=0` starts the subscription from the latest messages. `actionName` is set to \"act_tsaggr\".\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/OHLC.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\nsubscribeTable(tableName=\"Trade\", actionName=\"act_tsaggr\", offset=0, handler=append!{tsAggr}, msgAsTable=true);\n```\n\n----------------------------------------\n\nTITLE: Calculating factorial using reduce and accumulate\nDESCRIPTION: This code snippet calculates the factorial of 10 using both the `reduce` and `accumulate` functions. `reduce` returns the final result, while `accumulate` returns all intermediate results.  In this case, `accumulate` result at index 9 corresponds to the factorial of 10.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_55\n\nLANGUAGE: shell\nCODE:\n```\nr1 = reduce(mul, 1..10);\nr2 = accumulate(mul, 1..10)[9];\n```\n\n----------------------------------------\n\nTITLE: Segmenting data based on a threshold\nDESCRIPTION: This code snippet uses the 'segment' function to group data in the 'v' column of table 't' based on whether the value is greater than or equal to a threshold 'minV'. Consecutive values that satisfy the condition are grouped together.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_39\n\nLANGUAGE: shell\nCODE:\n```\nsegment(v>= minV)\n```\n\n----------------------------------------\n\nTITLE: Register Persistent Function View in DolphinDB Metaprogramming\nDESCRIPTION: Executes addFunctionView(bundleQuery) to register the bundleQuery function as a persistent function view, ensuring availability after system or node restarts. This enables cluster-wide use of complex dynamic querying logic for long-term reproducibility and integration into larger workflows. Dependency: bundleQuery function defined in the running environment with expected arguments.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_sql_cases.md#_snippet_46\n\nLANGUAGE: DolphinDB\nCODE:\n```\naddFunctionView(bundleQuery)\n```\n\n----------------------------------------\n\nTITLE: 点查性能测试（命中 sortKey）\nDESCRIPTION: 执行针对特定 machineId 的点查操作，测试不同去重策略在合并前后对查询时间的影响，帮助理解合并对点查性能的作用。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/tsdb_explained.md#_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nselect * from loadTable(dbName, tbName_all) where machineId=999\n```\n\n----------------------------------------\n\nTITLE: Executing DolphinDB Script File using run Function in DolphinScheduler - DolphinDB Script\nDESCRIPTION: This DolphinDB statement invokes the run function to execute a DolphinDB script saved on disk. It instructs DolphinScheduler's SQL node to execute the entire script located at '/data/script.dos'. This approach is simple but does not support parameter passing and is suitable for fixed or single-use scripts.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/dolphinscheduler_integration.md#_snippet_6\n\nLANGUAGE: DolphinDB script\nCODE:\n```\nrun(\"/data/script.dos\");\n```\n\n----------------------------------------\n\nTITLE: Measuring Rebalance Time in DolphinDB\nDESCRIPTION: This DolphinDB script queries the recovery task status to calculate the maximum time taken for a rebalance operation. It calls `getRecoveryTaskStatus` and subtracts the `StartTime` from `FinishTime` to determine the duration of each task and retrieves the maximum. The output represents the longest duration among all the rebalancing tasks.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/data_move_rebalance.md#_snippet_13\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect max(FinishTime - StartTime) as maxDiffTime from rpc(getControllerAlias(), getRecoveryTaskStatus)\n```\n\n----------------------------------------\n\nTITLE: Querying System File Limits on Linux via Shell Commands (console)\nDESCRIPTION: This snippet shows how to retrieve user-level and system-level maximum file descriptor (open file) limits using shell commands. It requires standard Linux permissions to execute. `cat /proc/sys/fs/file-max` fetches the system-wide limit, while `ulimit -n` (not directly shown here) fetches the user limit. These values inform how to set proper limits for DolphinDB stability.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/deploy_dolphindb_on_new_server.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n# cat /proc/sys/fs/file-max # 系统级\n763964\n\n```\n\n----------------------------------------\n\nTITLE: 计算移动窗口和 - C++\nDESCRIPTION: 这个代码片段实现了一个`msum`函数，用于计算X中长度为`windowSize`的移动窗口的和。它使用临时变量`tmpSum`来记录当前窗口的和，通过增加新窗口尾部的值，减去旧窗口头部的值来更新`tmpSum`。它使用`getDoubleBuffer`获取可读写的缓冲区，并通过`setDouble`将计算值写回`result`，从而避免数据拷贝，提高性能。 需要注意判断 DOUBLE 类型的 NULL 值，该值在 DolphinDB 中用 `DBL_NMIN` 定义。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_development_tutorial.md#_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\nConstantSP msum(const ConstantSP &X, const ConstantSP &window) {\n    INDEX size = X->size();\n    int windowSize = window->getInt();\n    ConstantSP result = Util::createVector(DT_DOUBLE, size);\n\n    double buf[Util::BUF_SIZE];\n    double windowHeadBuf[Util::BUF_SIZE];\n    double resultBuf[Util::BUF_SIZE];\n    double tmpSum = 0.0;\n\n    INDEX start = 0;\n    while (start < windowSize) {\n        int len = std::min(Util::BUF_SIZE, windowSize - start);\n        const double *p = X->getDoubleConst(start, len, buf);\n        double *r = result->getDoubleBuffer(start, len, resultBuf);\n        for (int i = 0; i < len; i++) {\n            if (p[i] != DBL_NMIN)    // p[i] is not NULL\n                tmpSum += p[i];\n            r[i] = DBL_NMIN;\n        }\n        result->setDouble(start, len, r);\n        start += len;\n    }\n\n    result->setDouble(windowSize - 1, tmpSum);    // 上一个循环多设置了一个 NULL，填充为 tmpSum\n\n    while (start < size) {\n        int len = std::min(Util::BUF_SIZE, size - start);\n        const double *p = X->getDoubleConst(start, len, buf);\n        const double *q = X->getDoubleConst(start - windowSize, len, windowHeadBuf);\n        double *r = result->getDoubleBuffer(start, len, resultBuf);\n        for (int i = 0; i < len; i++) {\n            if (p[i] != DBL_NMIN)\n                tmpSum += p[i];\n            if (q[i] != DBL_NMIN)\n                tmpSum -= q[i];\n            r[i] = tmpSum;\n        }\n        result->setDouble(start, len, r);\n        start += len;\n    }\n\n    return result;\n}\n```\n\n----------------------------------------\n\nTITLE: Creating and Batch Writing Data Table in DolphinDB using C++ API\nDESCRIPTION: This C++ snippet demonstrates table creation with specified schema and batch inserting data into DolphinDB using C++ API. It defines a function to create a TableSP with typed columns, populates rows with sample data, connects to DolphinDB server, and uploads the table batch via the tableInsert function. Exception handling captures connection and execution failures. Dependencies include DolphinDB C++ API headers and utilities. Inputs encompass database connection details and table size; output is the insertion of a populated table into distributed storage.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/iot_examples.md#_snippet_27\n\nLANGUAGE: c++\nCODE:\n```\nTableSP createDemoTable(long rows) {\n  vector<string> colNames = {\"id\",\"time\",\"value\"};\n  vector<DATA_TYPE> colTypes = {DT_INT, DT_DATETIME, DT_FLOAT};\n  int colNum = 3, rowNum = rows, indexCapacity = rows;\n  ConstantSP table =\n      Util::createTable(colNames, colTypes, rowNum, indexCapacity);\n  vector<VectorSP> columnVecs;\n  for (int i = 0; i < colNum; i++)\n    columnVecs.push_back(table->getColumn(i));\n  \n  for ( int i = 0; i < rowNum; i++) {\n    columnVecs[0]->setInt(i, i % 500 +1);\n    columnVecs[1]->setInt(i, Util::getEpochTime()/1000);\n    columnVecs[2]->setFloat(i, 1.0);\n  }\n  return table;\n}\n\nDBConnection conn;\ntry {\n\tbool ret = conn.connect(hString, pLong, \"admin\", \"123456\");\n\tif (!ret) {\n\t\tcout << \"Failed to connect to the server\" << endl;\n\t\treturn 0;\n\t}\n} catch (exception &ex) {\n\tcout << \"Failed to  connect  with error: \" << ex.what();\n\treturn -1;\n}\n\nTableSP table = createDemoTable(cLong, sLong, 1);\nvector<ConstantSP> args;\nargs.push_back(table);\ntry {\n\tconn.run(\"tableInsert{loadTable('dfs://svmDemo', `sensors)}\", args);\n} catch (exception &ex) {\n\tcout << \"Failed to  run  with error: \" << ex.what();\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Function View with Parameters - DolphinDB\nDESCRIPTION: This code creates a function view ('getTrades') that takes parameters. It grants VIEW_EXEC permission to a user, allowing them to execute the view with specific parameters without needing direct access to the underlying tables. Uses `addFunctionView` and `grant`. Requires admin privileges to create user.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Permission_Management.md#_snippet_38\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef getTrades(s, d){\n\treturn select * from loadTable(\"dfs://TAQ\",\"Trades\") where sym=s, date=d\n}\naddFunctionView(getTrades)\ngrant(\"NickFoles\",VIEW_EXEC,\"getTrades\")  \n```\n\n----------------------------------------\n\nTITLE: Defining Metrics for Sell Order Processing in DolphinDB\nDESCRIPTION: This code defines the `metricsSell` variable, which contains the expressions used within the reactive state engine for processing sell orders. It includes calculations such as cumulative sum of trade amount and a tag function based on cumulative trade quantity, and incorporates values from the processed buy orders.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_capital_flow_daily.md#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\nmetricsSell = [\n  \t<TradeTime>,\n  \t<TradeAmount>,\n  \t<cumsum(TradeAmount)>,\n  \t<tagFunc(cumsum(TradeQty))>,\n  \t<prev(cumsum(TradeAmount))>,\n  \t<prev(tagFunc(cumsum(TradeQty)))>,\n  \t<BuyNum>,\n  \t<TotalBuyAmount>,\n  \t<BuyOrderFlag>,\n  \t<PrevTotalBuyAmount>,\n  \t<PrevBuyOrderFlag>]\n```\n\n----------------------------------------\n\nTITLE: 创建和使用整数标量类型\nDESCRIPTION: 展示如何使用Util::createInt创建整数标量，并通过Constant接口访问值和验证其类型属性。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/c++api.md#_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\nConstantSP val = Util::createInt(47);\nassert(val->getInt() == 47); // 47\nassert(val->isScalar() && val->getForm() == DF_SCALAR && val->getType() == DT_INT);\n```\n\n----------------------------------------\n\nTITLE: Loading a Table from a Database in DolphinDB\nDESCRIPTION: This code snippet shows how to load an existing table from a DolphinDB database using the `loadTable` function. It first obtains a handle to the database and then loads the table named 'pt'.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Single_Node_Tutorial.md#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndb = database(\"dfs://DolphinDB\");\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nt = loadTable(db, \"pt\");\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntypestr(t);\n```\n\n----------------------------------------\n\nTITLE: Installing NTPD for Time Synchronization on Linux (console)\nDESCRIPTION: Install the NTP Daemon (NTPD) using the `yum` package manager to synchronize system clocks across all cluster nodes. Accurate timekeeping is critical for proper transaction ordering in DolphinDB clusters. Ensure network access to NTP servers.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/deploy_dolphindb_on_new_server.md#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\n# yum install ntp\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Incremental Data Synchronization in DataX JSON\nDESCRIPTION: This JSON fragment illustrates modifying the DataX reader configuration to include a SQL WHERE clause for incremental data synchronization based on a trade date filter. The clause filters data to be synchronized by selecting records only from the previous day. This reduces the data volume processed during each run, enabling efficient delta updates. The snippet must be integrated within the 'reader' parameter section inside the main DataX JSON configuration file. Dependencies include proper SQL Server syntax compatibility and accurate field naming.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/SQLServer_to_DolphinDB.md#_snippet_12\n\nLANGUAGE: JSON\nCODE:\n```\n\"reader\": {\n    \"name\": \"sqlserverreader\",\n    \"parameter\": {\n        \"username\": \"sa\",\n        \"password\": \"DolphinDB123\",\n        \"column\": [\n            \"ChannelNo\",\"ApplSeqNum\",\"MDStreamID\",\"SecurityID\",\"SecurityIDSource\",\"Price\",\"OrderQty\",\"Side\",\"TransactTime\",\"OrderType\",\"LocalTime\"\n        ],\n        \"connection\": [\n            {\n                \"table\": [\n                    \"data\"\n                ],\n                \"jdbcUrl\": [\n                    \"jdbc:sqlserver://127.0.0.1:1433;databasename=historyData\"\n                ]\n            }\n        ],\n        \"where\":\"TradeDate=(select CONVERT ( varchar ( 12) , dateadd(d,-1,getdate()), 102))\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Monitoring Rebalancing Task Status in DolphinDB\nDESCRIPTION: Executes three commands to monitor the data rebalancing process: `rpc(getControllerAlias(), getRecoveryTaskStatus)` retrieves the status (e.g., In-Progress, Finished) and details of each rebalancing task from the controller. `rpc(getControllerAlias(), getConfigure{ `dfsRebalanceConcurrency })` checks the configuration parameter defining the maximum number of concurrent rebalancing tasks initiated by the controller. `pnodeRun(getRecoveryWorkerNum)` queries the number of worker threads dedicated to recovery/rebalancing tasks on each data node.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/data_move_rebalance.md#_snippet_4\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nrpc(getControllerAlias(), getRecoveryTaskStatus)\nrpc(getControllerAlias(), getConfigure{ `dfsRebalanceConcurrency })\npnodeRun(getRecoveryWorkerNum)\n```\n\n----------------------------------------\n\nTITLE: Defining Function to Send Messages to Kafka Topic (Python)\nDESCRIPTION: This snippet defines a function to send messages to a Kafka topic. The `kafka::produce` function sends the message in JSON format to the specified Kafka topic. The `writeLog` function logs the success or failure of the message sending operation, including the number of rows sent and the time taken.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/stock_market_replay.md#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\ndef sendMsgToKafkaFunc(dataType, producer, msg){\n\tstartTime = now()\n\ttry {\n\t\tkafka::produce(producer, \"topic-message\", 1, msg, true)\n\t\tcost = now() - startTime\n\t\twriteLog(\"[Kafka Plugin] Successed to send\" + dataType + \":\" + msg.size() + \"rows,\" + cost + \"ms.\")\n\t}\n\tcatch(ex) {writeLog(\"[Kafka Plugin] Failed to send msg to kafka with error:\" +ex)}\n}\n```\n\n----------------------------------------\n\nTITLE: Calculating Annualized Return in Python\nDESCRIPTION: Defines a Python function `getAnnualReturn` using basic list/array indexing. It calculates the annualized return from a sequence of daily net values (`value`), mirroring the DolphinDB logic by annualizing based on a 252/730 day ratio.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/fund_factor_contrasted_by_py.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef getAnnualReturn(value):\n    return pow(1 + ((value[-1] - value[0])/value[0]), 252/730)-1\n```\n\n----------------------------------------\n\nTITLE: Querying - Order by, Aggregation DolphinDB\nDESCRIPTION: This snippet calculates the maximum, average, and minimum battery level, grouped by SSID, and then orders the results by average battery level in descending order.  It illustrates ordering aggregated results and common statistical computations.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs TimescaleDB/test_dolphindb_small.txt#_snippet_24\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 15. 经典查询：统计连接不同网络的设备的平均电量和最大、最小电量，并按平均电量降序排列\ntimer\nselect\n    max(battery_level) as max_battery,\n    avg(battery_level) as avg_battery,\n    min(battery_level) as min_battery\nfrom readings\ngroup by ssid\norder by avg_battery desc\n```\n\n----------------------------------------\n\nTITLE: Generating Batch Derived Feature Aggregation Meta Code in DolphinDB Script\nDESCRIPTION: This DolphinDB function creates meta code for batch aggregation of multiple features, similar to pandas groupby.agg functionality. Given a dictionary mapping feature column names to lists of aggregation functions, it generates corresponding SQL-style aggregation expressions using DolphinDB's meta programming capabilities via the sql function and funcByName. It returns a vector of meta code expressions and corresponding column names. This approach significantly reduces manual code when aggregating many features over time windows. Dependencies include DolphinDB's meta programming functions sql and funcByName. Inputs are a feature-to-function dictionary, and outputs are meta code and output column name vectors.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/metacode_derived_features.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef createAggMetaCode(aggDict){\\n\\tmetaCode = []\\n\\tmetaCodeColName = []\\n\\tfor(colName in aggDict.keys()){\\n\\t\\tfor(funcName in aggDict[colName])\\n\\t\\t{\\n\\t\\t\\tmetaCode.append!(sqlCol(colName, funcByName(funcName), colName + `_ + funcName$STRING))\\n\\t\\t\\tmetaCodeColName.append!(colName + `_ + funcName$STRING)\\n\\t\\t}\\n\\t}\\n\\treturn metaCode, metaCodeColName$STRING\\n}\\n\\nfeatures = {\\n\\t\"DateTime\":[`count]\\n}\\nfor(i in 0..9)\\n{\\n\\tfeatures[\"Wap\"+i] = [`sum, `mean, `std]\\n\\tfeatures[\"LogReturn\"+i] = [`sum, `realizedVolatility, `mean, `std]\\n\\tfeatures[\"LogReturnOffer\"+i] = [`sum, `realizedVolatility, `mean, `std]\\n\\tfeatures[\"LogReturnBid\"+i] = [`sum, `realizedVolatility, `mean, `std]\\n}\\nfeatures[\"WapBalance\"] = [`sum, `mean, `std]\\nfeatures[\"PriceSpread\"] = [`sum, `mean, `std]\\nfeatures[\"BidSpread\"] = [`sum, `mean, `std]\\nfeatures[\"OfferSpread\"] = [`sum, `mean, `std]\\nfeatures[\"TotalVolume\"] = [`sum, `mean, `std]\\nfeatures[\"VolumeImbalance\"] = [`sum, `mean, `std]\\naggMetaCode, metaCodeColName = createAggMetaCode(features)\n```\n\n----------------------------------------\n\nTITLE: Calculating Beta Coefficient Against Market Price in DolphinDB\nDESCRIPTION: Defines 'getBeta' to compute the beta coefficient by calculating covariance between asset and market returns divided by the standard deviation of market returns. Inputs are two numeric series for asset value and market price, and output is the beta scalar indicating asset sensitivity to market movements.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_factor_by_ddb/fund_factor_nine.txt#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef getBeta(value, price){\n\treturn covar(deltas(value)\\prev(value), deltas(price)\\prev(price)) \\ std(deltas(price)\\prev(price))\n}\n```\n\n----------------------------------------\n\nTITLE: Defining writeStreamTable Function\nDESCRIPTION: This function subscribes to an MQTT topic and writes the received data to the `inputSt` stream table. It creates a JSON parser to convert the MQTT message payload into a format suitable for the stream table. It depends on the MQTT plugin functions such as `mqtt::subscribe`.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/data/DolphinDB_streaming_application_in_IOT/deviceState.txt#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef writeStreamTable(host, port, topic){\n\t//sp = mqtt::createCsvParser([SYMBOL,TIMESTAMP,INT])\n\tsp = createJsonParser([SYMBOL,TIMESTAMP, INT], `tag`ts`value)\n\tmqtt::subscribe(host, port, topic, sp, objByName(`inputSt))\n}\n```\n\n----------------------------------------\n\nTITLE: Elasticsearch Scroll Deletion - Python\nDESCRIPTION: This Python function deletes all active scroll contexts in Elasticsearch. It uses the `urllib3` library to send a DELETE request to the `/_search/scroll/_all` endpoint. The function prints the HTTP status code and the response data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_25\n\nLANGUAGE: Python\nCODE:\n```\ndef delete_scroll():\n    http = urllib3.PoolManager()\n    r = http.request(\"DELETE\", \"http://localhost:9200/_search/scroll/_all\")\n    print(r.status)\n    print(r.data)\n```\n\n----------------------------------------\n\nTITLE: Performing Grouped Linear Regression Evaluation with OLS in DolphinDB SQL - DolphinDB\nDESCRIPTION: Executes ordinary least squares (OLS) regression grouped by 'id' on table 't', fitting model y = alpha + beta1*factor1 + beta2*factor2. The 'ols' function returns regression parameters as composite columns 'alpha', 'beta1', and 'beta2' without needing manual unpacking. The snippet highlights DolphinDB’s ability to output multiple regression coefficients into a single SQL query grouped by an identifier. Inputs are numeric vectors in table 't', output is a table of regression coefficients per id. Requires DolphinDB environment with ols function available.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/hybrid_programming_paradigms.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect ols(y, [factor1,factor2], true, 0) as `alpha`beta1`beta2 from t group by id;\n```\n\n----------------------------------------\n\nTITLE: Trade Order Type Aggregation using Range Partitioning with asof in DolphinDB\nDESCRIPTION: Utilizes the asof function with predefined numerical ranges to determine the order type based on trade amount, replacing explicit comparison logic. The aggregation groups by date, symbol, side, and computed type, enabling fast, concise partitioning without explicit conditional logic. The approach expects that 'range' is defined with correct thresholds and that t is the populated trades table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_sql_cases.md#_snippet_41\n\nLANGUAGE: DolphinDB\nCODE:\n```\nrange = [0.0, 40000.0, 200000.0, 1000000.0, 100000000.0]\n\ntimer res3 = select sum(volume) as volume_sum, sum(volume*price) as amount_sum \n\t\t\t\tfrom t \n\t\t\t\twhere time <= 10:00:00 \n\t\t\t\tgroup by date, symbol, side, asof(range, volume*price) as type\n```\n\n----------------------------------------\n\nTITLE: Tracking Historical Fund Count in DolphinDB\nDESCRIPTION: Creates a single-column matrix representing the count of active funds (non-null values in the returnsMatrix) for each date. It renames the row index (dates) and the column to 'count'. Finally, it plots the historical change in the number of public funds over time using a line chart. Depends on the 'returnsMatrix' variable created previously.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/public_fund_basic_analysis/basic_analysis_of_public_fund_net_value.txt#_snippet_4\n\nLANGUAGE: dolphindb\nCODE:\n```\nfundNum = matrix(rowCount(returnsMatrix)).rename!(returnsMatrix.rowNames(), [\"count\"])\nplot(fundNum.loc( ,`count), fundNum.rowNames(), '公募基金在历史上的数量变化', LINE)\n```\n\n----------------------------------------\n\nTITLE: Loading First and Last Chunks with MR\nDESCRIPTION: This code loads the first and last chunks of a data file using the `mr` function. It retrieves the head and tail data sources, applies a simple map function (x->x) to load each chunk, and then combines the results using `unionAll` with `false` to preserve the order.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_csv.md#_snippet_34\n\nLANGUAGE: DolphinDB\nCODE:\n```\nhead_tail_tb = mr(ds=[ds.head(), ds.tail()], mapFunc=x->x, finalFunc=unionAll{,false});\n```\n\n----------------------------------------\n\nTITLE: Installing DolphinDB Airflow Provider Plugin Using pip\nDESCRIPTION: This snippet illustrates the command to install the Airflow provider plugin for DolphinDB from PyPI using pip. This plugin facilitates the integration of Airflow with DolphinDB by providing the DolphinDBOperator and supporting infrastructure. It is a prerequisite for using DolphinDBOperator in Airflow DAGs. The environment requires Python with pip package manager accessible and permissions to install Python packages.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Best_Practices_for_DolphinDB_and_Python_AirFlow.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install airflow-provider-dolphindb\n```\n\n----------------------------------------\n\nTITLE: Creating Scalar Variables in DolphinDB Plugin\nDESCRIPTION: Demonstrates how to create scalar variables of different types (integer, date, string, void) within a DolphinDB plugin using C++. It uses the `new` keyword with types defined in `ScalarImp.h` and assigns the created objects to a `ConstantSP`, a smart pointer that manages memory automatically.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_development_tutorial.md#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nConstantSP i = new Int(1);                 // 相当于 1i\nConstantSP d = new Date(2019, 3, 14);      // 相当于 2019.03.14\nConstantSP s = new String(\"DolphinDB\");    // 相当于 \"DolphinDB\"\nConstantSP voidConstant = new Void();      // 创建一个 void 类型变量，常用于表示空的函数参数\n```\n\n----------------------------------------\n\nTITLE: Setting DolphinDB Stream Persistence Directory\nDESCRIPTION: Configures the directory path on disk where DolphinDB will store persisted stream data. This is a prerequisite for enabling persistence on stream tables.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_tutorial.md#_snippet_9\n\nLANGUAGE: DolphinDB\nCODE:\n```\npersistenceDir = /data/streamCache\n```\n\n----------------------------------------\n\nTITLE: Verifying Data Replication\nDESCRIPTION: This DolphinDB script counts the rows in a table in the slave cluster to verify successful data synchronization.  It uses the `loadTable` function to load the table and the `count` aggregate function. It checks whether the number of rows matches what was inserted in the master cluster. The prerequisite is an existing table replicated from the master cluster.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Asynchronous_Replication.md#_snippet_15\n\nLANGUAGE: dolphindb\nCODE:\n```\nselect count(*) from loadTable(\"dfs://testDB\", \"testTB\")\n```\n\n----------------------------------------\n\nTITLE: Calculating Sharpe Ratio in Python\nDESCRIPTION: Defines a Python function `getSharp`. It computes the Sharpe Ratio using the Python versions of `getAnnualReturn` and `getAnnualVolatility`. Includes a check to prevent division by zero if volatility is zero. Assumes a risk-free rate of 3% (0.03). Requires the helper functions `getAnnualReturn` and `getAnnualVolatility`.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/fund_factor_contrasted_by_py.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef getSharp(value):\n    annual_volatility = getAnnualVolatility(value)\n    return (getAnnualReturn(value) - 0.03)/annual_volatility if annual_volatility != 0 else 0\n```\n\n----------------------------------------\n\nTITLE: Querying - Pivot Table DolphinDB\nDESCRIPTION: This snippet calculates the average battery level, pivoted by the hour of the day and device ID.  It calculates the hourly average battery level for each device and allows for side-by-side comparison. This shows a common data analysis technique.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs TimescaleDB/test_dolphindb_small.txt#_snippet_18\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 9. 对比查询：对比 10 个设备 24 小时中每个小时平均电量变化情况\ntimer\nselect avg(battery_level)\nfrom readings\nwhere\n\ttime between 2016.11.15 07:00:00 : 2016.11.16 06:00:00,\n\tdevice_id < 'demo000010'\npivot by time.hour(), device_id\n```\n\n----------------------------------------\n\nTITLE: Class for Splitting Data for Multiprocessing\nDESCRIPTION: Defines 'multi_task_split' class to partition a list of dataset files into chunks suitable for multiprocessing based on specified process count, ensuring balanced workload distribution.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/DolphinDB_Python_Parser_Intro_for_Quantitative_Finance/因子实现_Python版本/主动成交量占比.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nclass multi_task_split:\n\n    def __init__(self, data, processes_to_use):\n        self.data = data\n        self.processes_to_use = processes_to_use\n\n    def num_of_jobs(self):\n        return min(len(self.data), self.processes_to_use, multiprocessing.cpu_count())\n\n    def split_args(self):\n        q, r = divmod(len(self.data), self.num_of_jobs())\n        return (self.data[i * q + min(i, r): (i + 1) * q + min(i + 1, r)] for i in range(self.num_of_jobs()))\n```\n\n----------------------------------------\n\nTITLE: 流式数据的订阅和处理（DolphinDB 流引擎定义与订阅）\nDESCRIPTION: 创建时序聚合引擎（tsAggr1）实现2分钟窗口内RMS值的计算（rms函数作为聚合规则），并订阅s信号表实时数据；同时定义异常检测引擎（tsAggr2）利用阈值检测故障，将结果输出到warn表。最后，将计算结果和告警信息实时落库。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Random_Vibration_Signal_Analysis_Solution.md#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\nmetrics=<[rms(signalnose,N,sensitivity,gain,window, noverlap, nfft, fs,bandwidthL,bandwidthH) as `rmsAcc`rmsVel`rmsDis]>\ntsAggr1 = createTimeSeriesAggregator(name=\"tsAggr1\",  windowSize=2*60*1000, step=2*60*1000, metrics=metrics, dummyTable=signal, outputTable=srms, timeColumn=`timestamp, keyColumn=`source)\n\ntsAggr2 = createAnomalyDetectionEngine(name=\"tsAggr2\", metrics=<[rmsAcc > 0.055, rmsVel >0.32, rmsDis > 34.5]>, dummyTable=srms, outputTable=warn, timeColumn=`datetime, keyColumn=`source, windowSize=2*60*1000, step=2*60*1000)\n```\n\n----------------------------------------\n\nTITLE: Loading Trade Data and Computing Volume Percentages\nDESCRIPTION: Loads stock trade data from a CSV file and measures the time to compute active volume percentage over a 60-period rolling window using the 'actVolumePercent' function. Prints the elapsed time and result frame.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/DolphinDB_Python_Parser_Intro_for_Quantitative_Finance/因子实现_Python版本/主动成交量占比.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ndf = pd.read_csv(\"/ssd/ssd3/data/oneStock_oneFile_TL/20230201/trade/000001.csv\")\nt0 = time.time()\nres = actVolumePercent(df, 60)\nprint(\"cal time: \", time.time() - t0, \"s\")\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: Setting Chunk Cache Engine Memory Size in DolphinDB\nDESCRIPTION: This snippet sets the capacity of the cache engine for a DolphinDB data node. Setting chunkCacheEngineMemSize to a value greater than 0 (e.g., 2) enables the cache engine.  The value indicates the cache capacity in GB. Enabling the cache engine also requires setting dataSync=1 for data to be written to disk after exceeding 30% of the cache size.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/ha_cluster_deployment/P1/config/config-specification.txt#_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\nchunkCacheEngineMemSize=2\n```\n\n----------------------------------------\n\nTITLE: Querying - Full Join DolphinDB\nDESCRIPTION: This snippet performs a full join between the `readings` and `device_info` tables, joining based on the 'device_id'.  It selects all columns from the resulting merged table. This shows how to combine all records from both tables, even when there isn't a matching device_id.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs TimescaleDB/test_dolphindb_small.txt#_snippet_22\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 13. 关联查询.全连接（full join）\ntimer\nselect *\nfrom fj((select * from readings), device_info, 'device_id')\n```\n\n----------------------------------------\n\nTITLE: Replacing License File for DolphinDB - Shell\nDESCRIPTION: This command indicates the location where the DolphinDB license file (`dolphindb.lic`) should be placed for license updates. Users with enterprise trial licenses replace the file at `/DolphinDB/server/dolphindb.lic`. The snippet assumes manual file replacement without automated commands.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ARM_standalone_deploy.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\n/DolphinDB/server/dolphindb.lic\n```\n\n----------------------------------------\n\nTITLE: Creating, Deleting, and Managing Indices in ElasticSearch (Python)\nDESCRIPTION: These functions manage ElasticSearch indices using the urllib3 HTTP client: creating a new index, deleting an existing index, and configuring an index template with detailed mappings (fields, types, and settings) for efficient data storage and retrieval. Dependencies include urllib3 and json. Input/output is through HTTP requests to a running ElasticSearch instance (e.g., localhost:9200). Error handling is present for connection failures. Field mappings must match the structure of the expected CSV data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef create_index():\n    http = urllib3.PoolManager()\n    try:\n        response = http.request('PUT', 'http://localhost:9200/uscsv')\n        print(response.status)\n        print(response.data)\n    except urllib3.exceptions:\n        print('Connection failed.')\n\ndef delete_index():\n    http = urllib3.PoolManager()\n    try:\n        response = http.request('DELETE', 'http://localhost:9200/uscsv')\n        print(response.status)\n        print(response.data)\n    except urllib3.exceptions:\n        pass\n\ndef create_index_template():\n    http = urllib3.PoolManager()\n    data = json.dumps({\n        'template': 'uscsv',\n        'settings': {\n            'number_of_shards': 4,\n            'number_of_replicas': 1,\n            \"index.refresh_interval\": -1\n        },\n        'mappings': {\n            'type': {\n                '_source': {'enabled': True},\n                'properties': { \n                    'PERMNO': {'type': 'integer'},\n                    'date': {'type': 'date', \"format\": \"yyyy/MM/dd\"},\n                    'SHRCD': {'type': 'integer'},\n                    'TICKER': {'type': 'keyword'},\n                    'TRDSTAT': {'type': 'keyword'},\n                    'PERMCO': {'type': 'keyword'},\n                    'HSICCD': {'type': 'keyword'},\n                    'CUSIP': {'type': 'keyword'},\n                    'DLSTCD': {'type': 'keyword'},\n                    'DLPRC': {'type': 'keyword'},\n                    'DLRET': {'type': 'keyword'},\n                    'BIDLO': {'type': 'double'},\n                    'ASKHI': {'type': 'double'},\n                    'PRC': {'type': 'double'},\n                    'VOL': {'type': 'integer'},\n                    'RET': {'type': 'keyword'},\n                    'BID': {'type': 'double'},\n                    'ASK': {'type': 'double'},\n                    'SHROUT': {'type': 'keyword'},\n                    'CFACPR': {'type': 'double'},\n                    'CFACSHR': {'type': 'double'},\n                    'OPENPRC': {'type': 'double'}\n                }\n            }\n        }\n    }).encode('utf-8')\n    r = http.request('PUT', 'http://localhost:9200/_template/uscsv', body=data, headers={'Content-Type': 'application/json'})\n    print(r.status)\n    print(r.data)\n\n```\n\n----------------------------------------\n\nTITLE: 使用MapReduce计算全量分钟级K线并存储\nDESCRIPTION: 通过MapReduce方法计算全部数据的分钟级K线，并将结果保存到分布式数据库中。该方法能够有效处理超过内存容量的大数据集，并充分利用系统资源进行并行计算。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/quant_finance_examples.md#_snippet_9\n\nLANGUAGE: DolphinDB\nCODE:\n```\nmodel=select top 1 symbol,date, minute(time) as minute, open, high, low, last, curVol as volume from quotes where date=2020.06.01,symbol=`600000\nif(existsTable(\"dfs://level2\", \"minuteBar\"))\n\tdb.dropTable(\"minuteBar\")\ndb.createPartitionedTable(model, \"minuteBar\", `date`symbol)\n\ndef saveMinuteBar(t){\n\tminuteBar=select first(last) as open, max(last) as high, min(last) as low, last(last) as last, sum(curVol) as volume from t where symbol>=`600000, time between 09:30:00.000 : 15:00:00.000 group by symbol, date, minute(time) as minute\n\tloadTable(\"dfs://level2\", \"minuteBar\").append!(minuteBar)\n\treturn minuteBar.size()\n}\n\nds = sqlDS(<select symbol, date, time, last, curVol from quotes>)\nmr(ds,saveMinuteBar,+)\n```\n\n----------------------------------------\n\nTITLE: Configuring ODBC Driver Settings in odbcinst.ini for CentOS - Configuration\nDESCRIPTION: Configuration snippet for odbcinst.ini tailored for CentOS environment, defining SQL Server ODBC driver with paths specific to CentOS filesystem. Enables unixODBC to recognize and load the FreeTDS SQL Server ODBC driver.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/SQLServer_to_DolphinDB.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n[SQLServer]\nDescription = ODBC for SQLServer\nDriver = /usr/lib64/libtdsodbc.so.0.0.0\nSetup = /usr/lib64/libtdsodbc.so.0.0.0\nFileUsage = 1\n```\n\n----------------------------------------\n\nTITLE: Manually Loading NSQ Plugin (DolphinDB Script)\nDESCRIPTION: Loads the DolphinDB NSQ plugin from the specified path using the `loadPlugin` function. This needs to be executed in a DolphinDB session (e.g., GUI, console, API) after the server starts. Replace `/DolphinDB/server/plugins/nsq/PluginNsq.txt` with the actual path to the plugin descriptor file.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_implementation_for_NSQ_Plugin.md#_snippet_2\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nloadPlugin(\"/DolphinDB/server/plugins/nsq/PluginNsq.txt\")\n```\n\n----------------------------------------\n\nTITLE: Adding DolphinDB Function as Function View for Global Access - DolphinDB Script\nDESCRIPTION: This DolphinDB command registers the previously defined function 'createTable' as a function view, making it globally accessible within the DolphinDB environment. This enables DolphinScheduler to invoke the function with parameters in SQL task nodes, allowing parameterized and flexible ETL job execution.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/dolphinscheduler_integration.md#_snippet_8\n\nLANGUAGE: DolphinDB script\nCODE:\n```\n// 添加为函数视图\naddFunctionView(createTable)\n```\n\n----------------------------------------\n\nTITLE: Granting TABLE_READ Permission at Table Level - DolphinDB\nDESCRIPTION: This code snippet shows how to grant a user TABLE_READ permission for a specific table in a distributed file system (DFS). It uses the `grant` function with the table's full path. Requires a DolphinDB environment with a DFS database.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Permission_Management.md#_snippet_20\n\nLANGUAGE: DolphinDB\nCODE:\n```\ngrant(`user1, TABLE_READ,\"dfs://valuedb/pt\")\n```\n\n----------------------------------------\n\nTITLE: Loading Kafka Plugin - DolphinDB Script\nDESCRIPTION: Loads the Kafka Plugin to extend DolphinDB's streaming capabilities and enable interaction with Kafka clusters. Requires the correct plugin path; no input parameters except the file path string. Successful loading allows further use of Kafka producer and messaging functions.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/stock_market_replay/04.publishToKafka.txt#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\nloadPlugin(\"/yourPluginsPath/kafka/PluginKafka.txt\")\ngo\n```\n\n----------------------------------------\n\nTITLE: Configuring DolphinDB - agent.cfg\nDESCRIPTION: This configuration file (`agent.cfg`) specifies the configuration for a DolphinDB agent, including the number of workers and executors, the maximum memory size, the agent's local site, and the controller site address. Dependencies: DolphinDB cluster setup.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_Druid_appendix.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\nworkerNum=3\nlocalExecutors=2\nmaxMemSize=4\nlocalSite=localhost:9910:agent\ncontrollerSite=localhost:9919:ctl9919\n```\n\n----------------------------------------\n\nTITLE: Getting All Databases Replication Status\nDESCRIPTION: This DolphinDB function retrieves the replication status of all databases. The function getDatabaseClusterReplicationStatus() retrieves this information. This command retrieves the database name and the status of replication. Requires a database instance to exist in the cluster.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Asynchronous_Replication.md#_snippet_11\n\nLANGUAGE: dolphindb\nCODE:\n```\ngetDatabaseClusterReplicationStatus()\n```\n\n----------------------------------------\n\nTITLE: Renaming Columns in Memory Table\nDESCRIPTION: Demonstrates renaming a column in a memory table using the `rename!` function. Specify the old and new column names as strings.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/partitioned_in_memory_table.md#_snippet_24\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntrades.rename!(\"qty2\", \"qty2New\");\n```\n\n----------------------------------------\n\nTITLE: Transforming and Loading SSE Snapshot Data in DolphinDB\nDESCRIPTION: Defines a DolphinDB function `transform` to restructure input data, padding the SecurityID and converting multiple bid/offer columns into array vectors. Uses the `loadTextEx` function to load data from a specified CSV file (`csvDataPath`) into a DolphinDB distributed table (`dbName`, `tbName`), applying the `transform` function during ingestion. The data is partitioned by DateTime and SecurityID.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Real-Time_Stock_Price_Increase_Calculation.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef transform(t){\n\ttemp = select lpad(string(SecurityID), 6, \"0\") as SecurityID, DateTime, PreClosePx, OpenPx, HighPx, LowPx, LastPx, TotalVolumeTrade, TotalValueTrade, InstrumentStatus,\n\t\t\tfixedLengthArrayVector(BidPrice0, BidPrice1, BidPrice2, BidPrice3,  BidPrice4, BidPrice5, BidPrice6, BidPrice7, BidPrice8, BidPrice9) as BidPrice,\n\t\t\tfixedLengthArrayVector(BidOrderQty0, BidOrderQty1, BidOrderQty2, BidOrderQty3,  BidOrderQty4, BidOrderQty5, BidOrderQty6, BidOrderQty7, BidOrderQty8, BidOrderQty9) as BidOrderQty,\n\t\t\tfixedLengthArrayVector(BidNumOrders0, BidNumOrders1, BidNumOrders2, BidNumOrders3,  BidNumOrders4, BidNumOrders5, BidNumOrders6, BidNumOrders7, BidNumOrders8, BidNumOrders9) as BidNumOrders,\n\t\t\tfixedLengthArrayVector(BidOrders0, BidOrders1, BidOrders2, BidOrders3,  BidOrders4, BidOrders5, BidOrders6, BidOrders7, BidOrders8, BidOrders9, BidOrders10, BidOrders11, BidOrders12, BidOrders13,  BidOrders14, BidOrders15, BidOrders16, BidOrders17, BidOrders18, BidOrders19, BidOrders20, BidOrders21, BidOrders22, BidOrders23,  BidOrders24, BidOrders25, BidOrders26, BidOrders27, BidOrders28, BidOrders29, BidOrders30, BidOrders31, BidOrders32, BidOrders33,  BidOrders34, BidOrders35, BidOrders36, BidOrders37, BidOrders38, BidOrders39, BidOrders40, BidOrders41, BidOrders42, BidOrders43,  BidOrders44, BidOrders45, BidOrders46, BidOrders47, BidOrders48, BidOrders49) as BidOrders,\n\t\t\tfixedLengthArrayVector(OfferPrice0, OfferPrice1, OfferPrice2, OfferPrice3,  OfferPrice4, OfferPrice5, OfferPrice6, OfferPrice7, OfferPrice8, OfferPrice9) as OfferPrice,\n\t\t\tfixedLengthArrayVector(OfferOrderQty0, OfferOrderQty1, OfferOrderQty2, OfferOrderQty3,  OfferOrderQty4, OfferOrderQty5, OfferOrderQty6, OfferOrderQty7, OfferOrderQty8, OfferOrderQty9) as OfferOrderQty,\n\t\t\tfixedLengthArrayVector(OfferNumOrders0, OfferNumOrders1, OfferNumOrders2, OfferNumOrders3,  OfferNumOrders4, OfferNumOrders5, OfferNumOrders6, OfferNumOrders7, OfferNumOrders8, OfferNumOrders9) as OfferNumOrders,\n\t\t\tfixedLengthArrayVector(OfferOrders0, OfferOrders1, OfferOrders2, OfferOrders3,  OfferOrders4, OfferOrders5, OfferOrders6, OfferOrders7, OfferOrders8, OfferOrders9, OfferOrders10, OfferOrders11, OfferOrders12, OfferOrders13,  OfferOrders14, OfferOrders15, OfferOrders16, OfferOrders17, OfferOrders18, OfferOrders19, OfferOrders20, OfferOrders21, OfferOrders22, OfferOrders23,  OfferOrders24, OfferOrders25, OfferOrders26, OfferOrders27, OfferOrders28, OfferOrders29, OfferOrders30, OfferOrders31, OfferOrders32, OfferOrders33,  OfferOrders34, OfferOrders35, OfferOrders36, OfferOrders37, OfferOrders38, OfferOrders39, OfferOrders40, OfferOrders41, OfferOrders42, OfferOrders43,  OfferOrders44, OfferOrders45, OfferOrders46, OfferOrders47, OfferOrders48, OfferOrders49) as OfferOrders,\n\t\t\tNumTrades, IOPV, TotalBidQty, TotalOfferQty, WeightedAvgBidPx, WeightedAvgOfferPx, TotalBidNumber, TotalOfferNumber, BidTradeMaxDuration, OfferTradeMaxDuration, \n\t\t\tNumBidOrders, NumOfferOrders, WithdrawBuyNumber, WithdrawBuyAmount, WithdrawBuyMoney,WithdrawSellNumber, WithdrawSellAmount, WithdrawSellMoney, ETFBuyNumber, ETFBuyAmount, \n\t\t\tETFBuyMoney, ETFSellNumber, ETFSellAmount, ETFSellMoney\n\t\t\tfrom t\n\treturn temp\n}\n\ncsvDataPath = \"/home/v2/下载/data/20211201snapshot_30stocks.csv\"\nloadTextEx(dbHandle=database(dbName), tableName=tbName, partitionColumns=`DateTime`SecurityID, filename=csvDataPath, transform=transform)\n```\n\n----------------------------------------\n\nTITLE: Installing ODBC Driver Components on CentOS - Shell\nDESCRIPTION: Shell commands to install the freeTDS and unixODBC libraries on CentOS Linux to enable ODBC connectivity to SQL Server. Provides prerequisites for configuring FreeTDS and unixODBC with corresponding driver files.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/SQLServer_to_DolphinDB.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n# 安装 freeTDS\nyum install -y freetds\n\n# 安装 unixODBC 库\nyum install unixODBC-devel\n```\n\n----------------------------------------\n\nTITLE: Querying Pickup Locations in DolphinDB for Grafana\nDESCRIPTION: This DolphinDB SQL-like query selects the pickup latitude and longitude from the `predictTable` for all trips that occurred on the latest date present in the table. This data is suitable for visualizing pickup locations on a map within a Grafana dashboard.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Forecast_of_Taxi_Trip_Duration.md#_snippet_4\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nselect pickup_latitude as latitude, pickup_longitude as longitude from predictTable \nwhere date(predictTable.pickup_datetime) == date(select max(pickup_datetime) from predictTable)\n```\n\n----------------------------------------\n\nTITLE: Calculating Moving VWAP (Unoptimized - Loop)\nDESCRIPTION: This code snippet demonstrates an unoptimized approach to calculating moving weighted average (mwavg) using a loop. It iterates through each symbol, extracts the price and volume vectors, calculates mwavg, and joins the results.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_sql_cases.md#_snippet_12\n\nLANGUAGE: DolphinDB\nCODE:\n```\narr = array(ANY, syms.size())\n\ntimer {\n\tfor(i in 0 : syms.size()) {\n\t\tprice_vec = exec price from t where symbol = syms[i]\n\t\tvolume_vec = exec volume from t where symbol = syms[i]\n\t\tarr[i] = mwavg(price_vec, volume_vec, 4)\n\t}\n\tres1 = reduce(join, arr)\n}\n```\n\n----------------------------------------\n\nTITLE: Writing Date Data from Java to DolphinDB\nDESCRIPTION: Demonstrates writing date data from Java to DolphinDB using the Java API. The method creates a Date object, converts it to timestamp, and inserts it into a DolphinDB table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/timezone.md#_snippet_10\n\nLANGUAGE: Java\nCODE:\n```\npublic static void write() throws IOException {\n    DBConnection conn = new DBConnection();\n    conn.connect(\"localhost\", 8848, \"admin\", \"123456\");\n    LinkedList<Long> dtList = new LinkedList<>();// ts\n    LinkedList<Integer> valueList = new LinkedList<>();// value\n    Date date = new Date();\n    System.out.println(date);\n    dtList.add(date.getTime());\n    valueList.add(1);\n    List<Entity> data = Arrays.asList(\n    new BasicTimestampVector(dtList),\n    new BasicIntVector(valueList)\n    );\n    conn.run(\"tableInsert{\\\"testJava\\\"}\", data);\n}\n```\n\n----------------------------------------\n\nTITLE: Inserting Data into a Table in DolphinDB\nDESCRIPTION: This code snippet demonstrates how to insert a single row of data into a table named 't' in DolphinDB using the SQL insert statement.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Single_Node_Tutorial.md#_snippet_8\n\nLANGUAGE: DolphinDB\nCODE:\n```\ninsert into t values (5, 6, 2.5)\n```\n\n----------------------------------------\n\nTITLE: Installing ODBC Driver Components on Ubuntu - Shell\nDESCRIPTION: Shell commands to install essential libraries and drivers required for ODBC connectivity with SQL Server on an Ubuntu 22.04 system. Includes installation of freeTDS, unixODBC, and Microsoft SQL Server ODBC driver packages. Configuration files referenced separately.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/SQLServer_to_DolphinDB.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n# 安装 freeTDS\napt install -y freetds\n\n# 安装 unixODBC 库\napt-get install unixodbc unixodbc-dev\n\n# 安装 SQL Server ODBC 驱动\napt-get install tdsodbc\n```\n\n----------------------------------------\n\nTITLE: Configuring DataX JSON for Oracle to DolphinDB Data Migration\nDESCRIPTION: Defines the JSON configuration file for DataX to perform data transfer from Oracle to DolphinDB, specifying source and target parameters, connection details, and batch settings. Dependencies include DataX environment and appropriate JDBC drivers. Inputs include table schemas, database credentials, and connection URLs; outputs are logs and data transfer results.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Oracle_to_DolphinDB.md#_snippet_15\n\nLANGUAGE: JSON\nCODE:\n```\n{\n\t\"job\": {\n\t\t\"content\": [{\n\t\t\t\"writer\": {\n\t\t\t\t\"parameter\": {\n\t\t\t\t\t\"dbPath\": \"dfs://TSDB_tick\",\n\t\t\t\t\t\"userId\": \"admin\",\n\t\t\t\t\t\"tableName\": \"tick\",\n\t\t\t\t\t\"host\": \"127.0.0.1\",\n\t\t\t\t\t\"pwd\": \"123456\",\n\t\t\t\t\t\"table\": [\n\t\t\t\t\t\t{\"type\": \"DT_SYMBOL\", \"name\": \"SecurityID\"},\n\t\t\t\t\t\t{\"type\": \"DT_TIMESTAMP\", \"name\": \"TradeTime\"},\n\t\t\t\t\t\t{\"type\": \"DT_DOUBLE\", \"name\": \"TradePrice\"},\n\t\t\t\t\t\t{\"type\": \"DT_INT\", \"name\": \"TradeQty\"},\n\t\t\t\t\t\t{\"type\": \"DT_DOUBLE\", \"name\": \"TradeAmount\"},\n\t\t\t\t\t\t{\"type\": \"DT_INT\", \"name\": \"BuyNo\"},\n\t\t\t\t\t\t{\"type\": \"DT_INT\", \"name\": \"SellNo\"},\n\t\t\t\t\t\t{\"type\": \"DT_INT\", \"name\": \"TradeIndex\"},\n\t\t\t\t\t\t{\"type\": \"DT_INT\", \"name\": \"ChannelNo\"},\n\t\t\t\t\t\t{\"type\": \"DT_SYMBOL\", \"name\": \"TradeBSFlag\"},\n\t\t\t\t\t\t{\"type\": \"DT_INT\", \"name\": \"BizIndex\"}\n\t\t\t\t\t],\n\t\t\t\t\t\"port\": 8858\n\t\t\t\t},\n\t\t\t\t\"name\": \"dolphindbwriter\"\n\t\t\t\t},\n\t\t\t\t\"reader\": {\n\t\t\t\t\t\"parameter\": {\n\t\t\t\t\t\t\"username\": \"system\",\n\t\t\t\t\t\t\"column\": [\"SecurityID\", \"TradeTime\", \"TradePrice\", \"TradeQty\", \"TradeAmount\", \"BuyNo\", \"SellNo\", \"ChannelNo\", \"TradeIndex\", \"TradeBSFlag\", \"BizIndex\"],\n\t\t\t\t\t\t\"connection\": [{\n\t\t\t\t\t\t\t\"table\": [\"ticksh\"],\n\t\t\t\t\t\t\t\"jdbcUrl\": [\"jdbc:oracle:thin:@127.0.0.1:1521:ora21c\"]\n\t\t\t\t\t\t}],\n\t\t\t\t\t\t\"password\": \"dolphindb123\",\n\t\t\t\t\t\t\"where\": \"\"\n\t\t\t\t\t},\n\t\t\t\t\t\"name\": \"oraclereader\"\n\t\t\t\t}\n\t\t\t}],\n\t\t\"setting\": {\n\t\t\t\"speed\": {\n\t\t\t\t\"channel\": 1\n\t\t\t}\n\t\t}\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: Installing unixODBC Libraries on Linux - Shell\nDESCRIPTION: Installs the core unixODBC and development libraries using the apt-get package manager. Required for enabling ODBC connectivity on Linux prior to Oracle Instant Client installation. No parameters are required; run with appropriate user privileges on Debian/Ubuntu systems.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Oracle_to_DolphinDB.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\napt-get install unixodbc unixodbc-dev\n\n```\n\n----------------------------------------\n\nTITLE: Function for Simulating Door Event Data - DolphinDB\nDESCRIPTION: This function `genData` populates a mutable stream table (`st`) with simulated door access events designed to test the stream processing logic. It generates sequences of different `doorEventCode` values with timing patterns that should trigger the timeout detection and filtering engines.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/streaming_engine_anomaly_alerts/流计算引擎级联检测门禁超时数据.txt#_snippet_8\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef genData(mutable st){\n    startEventDate = 2022.12.01T00:00:00\n    // 先生成75条代码为11的相同数据，即出现了五分钟开门超时数据 需要过滤代码重复数据才能检测出\n    duplicateData(st, 75, 11, startEventDate)\n    startEventDate=datetimeAdd(startEventDate , 375, `s)\n    // 再生成25条代码为56的相同数据\n    duplicateData(st, 25, 5 EventDate)\n    startEventDate=datetimeAdd(startEventDate , 125, `s)\n    // 生成100条代码为61的重复数据，出现了五分钟超时的关门数据，需要后续过滤掉\n    duplicateData(st, 100, 61, startEventDate)\n    startEventDate=datetimeAdd(startEventDate , 500, `s)\n    // 生成25条代码为66的重复数据\n    duplicateData(st, 25, 66, startEventDate)\n    startEventDate=datetimeAdd(startEventDate , 125, `s)\n    // 生成70条代码为12的相同数据，出现超时开门数据\n    duplicateData(st, 70, 12, startEventDate)\n    startEventDate=datetimeAdd(startEventDate , 350, `s)\n    // 生成30条代码为60的开门数据\n    duplicateData(st, 30, 60, startEventDate)\n    startEventDate=datetimeAdd(startEventDate , 150, `s)\n    // 生成25条代码为67的开门数据\n    duplicateData(st, 25, 67, startEventDate)\n    startEventDate=datetimeAdd(startEventDate , 125, `s)\n}\n```\n\n----------------------------------------\n\nTITLE: Revoking Database-Level Permissions on Database Recreation in DolphinDB Script\nDESCRIPTION: This code snippet illustrates granting DB_MANAGE permission to a user on a specific database, then deleting and recreating the database, which results in the revocation of the user's DB_MANAGE permission. It highlights login, database management, partitioned table creation, user creation, permission granting, and permission status checks using getUserAccess. The snippet outputs the user's permissions before and after database deletion to demonstrate permission revocation behavior at the database level.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Permission_Management.md#_snippet_17\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(\"admin\", \"123456\")\ndbName = \"dfs://valuedb\"\nt = table(1..10 as id , rand(100, 10) as val)\nif(existsDatabase(dbName)){\n\t dropDatabase(dbName)\n}\ndb=database(dbName, VALUE, 1..10)\npt=  db.createPartitionedTable(t, \"pt\", \"id\").append!(t)\n\ncreateUser(\"user1\",\"123456\")\ngrant(\"user1\", DB_MANAGE,\"dfs://valuedb\")\ngetUserAccess(\"user1\")\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(\"admin\", \"123456\")\nif(existsDatabase(dbName)){\n\t dropDatabase(dbName)\n}\ndb=database(dbName, VALUE, 1..10)\npt=  db.createPartitionedTable(t, \"pt\", \"id\").append!(t)\ngetUserAccess(\"user1\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Paths and Loading Model in DolphinDB\nDESCRIPTION: Sets the file paths for the pre-trained machine learning model (`modelSavePath`) and the historical CSV data (`csvDataPath`). It then loads the specified model using the `loadModel` function, making it available for prediction tasks later in the script.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/machine_learning_volatility/06.streamComputingReproduction.txt#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\n/**\nmodified location 1: modelSavePath, csvDataPath\n*/\nmodelSavePath = \"/hdd/hdd9/machineLearning/realizedVolatilityModel_1.30.18.bin\"\n//modelSavePath = \"/hdd/hdd9/machineLearning/realizedVolatilityModel_2.00.6.bin\"\ncsvDataPath = \"/hdd/hdd9/machineLearning/testSnapshot.csv\"\n\n//load modle\nmodel = loadModel(modelSavePath)\n```\n\n----------------------------------------\n\nTITLE: Checking Current License Expiration Dates Using DolphinDB ops Module\nDESCRIPTION: This snippet uses DolphinDB's ops module to retrieve all active license expiration information from any node in the cluster. It helps administrators verify current license validity before performing updates.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/multi_machine_cluster_deployment.md#_snippet_8\n\nLANGUAGE: DolphinDB\nCODE:\n```\nuse ops\ngetAllLicenses()\n```\n\n----------------------------------------\n\nTITLE: Updated Module Definition in DolphinDB\nDESCRIPTION: This code shows the updated module definition that won't affect previously scheduled jobs as they use the serialized version of functions.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/scheduledJob.md#_snippet_12\n\nLANGUAGE: DolphinDB\nCODE:\n```\nmodule printLog\ndef printLogs(logText){\n\twriteLog(string(now()) + \" : \" + logText)\n\tprint \"The new function is called\"\n}\n```\n\n----------------------------------------\n\nTITLE: Exploring DolphinDB Storage Directory Structure\nDESCRIPTION: Command showing the directory structure of a DolphinDB database after data is stored. This illustrates how data is physically organized according to the partitioning scheme.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/iot_examples.md#_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\n[root@188 CHUNKS]# tree ./demo/\n./demo/\n├── 20200101\n│   ├── 1_11\n│   │   └── sensor\n│   │       ├── data.col\n│   │       ├── tagid.col\n│   │       └── ts.col\n│   ├── 11_21\n│   │   └── sensor\n│   │       ├── data.col\n│   │       ├── tagid.col\n│   │       └── ts.col\n│   └── 21_31\n│       └── sensor\n│           ├── data.col\n│           ├── tagid.col\n│           └── ts.col\n├── 20200102\n│   ├── 1_11\n...\n├── dolphindb.lock\n├── domain\n└── sensor.tbl\n```\n\n----------------------------------------\n\nTITLE: Calculating Hurst Exponent in Python\nDESCRIPTION: Defines a Python function `calHurst` using NumPy to calculate the Hurst Exponent for a single time series `value_list`. It iterates through different time scales `k` (from `min_k` to `n/2`), calculates the average Rescaled Range (R/S) for each scale, performs a linear regression on the log-log plot of R/S vs `k`, and returns the slope, which is the Hurst Exponent. Includes basic error handling for the regression. Requires NumPy.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/fund_factor_contrasted_by_py.md#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\ndef calHurst(value_list, min_k):\n    n = len(value_list)\n    max_k = int(np.floor(n / 2))\n    r_s_dict = []\n    for k in range(min_k, max_k +1):\n        subset_list = [value_list[i: i+k] for i in range(0, n, k)]\n        if np.mod(n, k) > 0:\n            subset_list.pop() # Remove incomplete subset at the end\n        # Check if any subsets remain after pop\n        if not subset_list:\n            continue \n        df_subset = np.array(subset_list)\n        df_mean = df_subset.mean(axis=1).reshape(-1,1)\n        df_cusum = (df_subset - df_mean).cumsum(axis=1)\n        r = df_cusum.max(axis=1) - df_cusum.min(axis=1) + np.spacing(1) # Add spacing for stability\n        s = df_subset.std(axis=1, ddof=0) + np.spacing(1) # Add spacing for stability\n        # Avoid division by zero or invalid S values\n        valid_s = s > np.spacing(1)\n        if np.any(valid_s):\n            r_s_mean = (r[valid_s] / s[valid_s]).mean()\n            r_s_dict.append({'R_S': r_s_mean, 'N': k})\n        else:\n             continue # Skip scale k if no valid std deviations\n\n    # Check if enough points for regression\n    if len(r_s_dict) < 2:\n        return None # Not enough points to fit a line\n\n    log_r_s=[]\n    log_n=[]\n    for i in range(len(r_s_dict)):\n        log_r_s.append(np.log(r_s_dict[i]['R_S']))\n        log_n.append(np.log(r_s_dict[i]['N']))\n    try:\n        res = np.polyfit(log_n, log_r_s, 1)[0]\n    except np.linalg.LinAlgError: # Catch potential errors in polyfit\n        res = None\n    except ValueError: # Catch other potential errors (e.g., NaN/inf)\n        res = None\n    return res\n```\n\n----------------------------------------\n\nTITLE: Defining Cleanup Function - DolphinDB\nDESCRIPTION: This function, `clearEnv`, is designed to clean up existing stream tables, stream processing engines (SessionWindowEngine, ReactiveStreamEngine), and MQTT subscriptions. It uses `try-catch` to gracefully handle cases where objects might not exist.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/streaming_engine_anomaly_alerts/流计算引擎级联检测门禁超时数据.txt#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef  clearEnv(){\n\ttry{\n\t\tunsubscribeTable( tableName=`doorRecord, actionName=\"monitor\")\n\t\tif(objs(true).name.find('doorRecord')!=-1) \n\t\t\tdropStreamTable(`doorRecord)\n\t\tif(objs(true).name.find('outputSt1')!=-1) \n\t\t\tdropStreamTable(`outputSt1)\n\t\tif(objs(true).name.find('outputSt2')!=-1) \n\t\t\tdropStreamTable(`outputSt2)\n\n\t\tif (getAggregatorStat().SessionWindowEngine[`name].find(`swEngine)!=-1)\n\t\t\tdropAggregator(`swEngine)\n\t\tif (getAggregatorStat().ReactiveStreamEngine[`name].find(`reactivEngine)!=-1)\n\t\t\tdropAggregator(`reactivEngine)\n        if (getAggregatorStat().ReactiveStreamEngine[`name].find(`reactivEngine1)!=-1)\n        \tdropAggregator(`reactivEngine1)\n         if (getAggregatorStat().ReactiveStreamEngine[`name].find(`reactivEngine2)!=-1)\n        \tdropAggregator(`reactivEngine2)\n\t\tfor (id in mqtt::getSubscriberStat()[`subscriptionId]) \n\t\t\tmqtt::unsubscribe(id)\n\t}catch(ex){\n\t\tprint(ex)\n\t}\n\t\n}\n```\n\n----------------------------------------\n\nTITLE: Reordering Columns in Memory Table\nDESCRIPTION: Demonstrates how to reorder columns in a memory table using the `reorderColumns!` function. Only non-partitioned tables support this operation. Specify the desired column order as a list of column names.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/partitioned_in_memory_table.md#_snippet_29\n\nLANGUAGE: DolphinDB\nCODE:\n```\nreorderColumns!(trades,`sym`date`price6`price5`price4`price3`price2`price1`qty6`qty5`qty4`qty3`qty2`qty1)\n```\n\n----------------------------------------\n\nTITLE: Adding Incremental Data Synchronization Condition in DataX JSON Reader Configuration\nDESCRIPTION: This JSON reader snippet shows how to enhance the DataX reader configuration to filter incremental data by adding a SQL 'where' condition. The example filters data where the 'TradeTime' date equals the previous day, enabling selective synchronization of only new or modified data since the last run. Parameters like username, password, batch size, columns, and connection details remain consistent with the full data read configuration.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/OceanBase_to_DolphinDB.md#_snippet_7\n\nLANGUAGE: JSON\nCODE:\n```\n\"reader\": {\n    \"name\": \"oceanbasev10reader\",\n    \"parameter\": {\n        \"username\": \"root\",\n        \"password\": \"123456\",\n        \"batchSize\":10000,\n        \"column\": [\n            \"*\"\n        ],\n        \"connection\": [\n            {\n                \"table\": [\n                    \"tick\"\n                ],\n                \"jdbcUrl\": [\n                    \"jdbc:oceanbase://127.0.0.1:2883/db1\"\n                ]\n            }\n        ],\n        \"where\":\"date(TradeTime) = (SELECT DATE_ADD(CURDATE(), INTERVAL -1 DAY))\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining and Running External Function in DolphinDB GUI\nDESCRIPTION: This example shows defining a function in the GUI and running a script file that depends on it, which works in the same session but not in scheduled jobs.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/scheduledJob.md#_snippet_14\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef foo(){\n\tprint (\"Hello world!\")\n}\nrun \"/home/user/testjob.dos\"\n```\n\n----------------------------------------\n\nTITLE: Checking Redo Log Garbage Collection Status in DolphinDB\nDESCRIPTION: The getRedoLogGCStat function allows you to monitor the garbage collection status of the redo log system, providing information about what transactions have been processed and which are pending.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/redoLog_cacheEngine.md#_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\ngetRedoLogGCStat()\n```\n\n----------------------------------------\n\nTITLE: Configuring SQL Server Connection in freetds.conf - Configuration\nDESCRIPTION: Configuration snippet to add SQL Server server details to the freetds.conf file. Specifies server IP address and port to enable the ODBC FreeTDS driver to communicate with SQL Server instance. Required for correct network connection establishment through ODBC.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/SQLServer_to_DolphinDB.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n[sqlserver]\nhost = 127.0.0.1\nport = 1433\n```\n\n----------------------------------------\n\nTITLE: Filtering Data using fliterData function\nDESCRIPTION: The `fliterData` function filters data within the `transform` parameter during data import in DolphinDB. It uses the `select` statement to select only those records from the `memTable` where the 'price' is greater than 0, effectively removing invalid data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/LoadDataForPoc.md#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef fliterData(mutable memTable)\n{\n\treturn select * from memTable where price > 0\n}\n```\n\n----------------------------------------\n\nTITLE: Plotting Downsampled Data using DolphinDB `plot` Function\nDESCRIPTION: Uses the built-in `plot` function in DolphinDB to generate a line chart visualizing the first 100 points of the downsampled data stored in the `samplesPIP` table (column y vs column x). Requires the `samplesPIP` table to be created by parsing the output of the `rolling(PIP, ...)` operation.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/PIP_in_DolphinDB.md#_snippet_6\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nplot(samplesPIP.y[0:100], samplesPIP.x[0:100])\n```\n\n----------------------------------------\n\nTITLE: Calculating Annual Kurtosis Using DolphinDB\nDESCRIPTION: Defines getAnnualKur that computes the kurtosis of daily returns, measuring the 'tailedness' of the distribution. Returns a numeric value that quantifies the propensity of extreme values. Input is a numeric vector representing sequential values, output is the kurtosis statistic.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_factor_by_ddb/fund_factor_ten.txt#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndefg getAnnualKur(value){\n\treturn kurtosis(deltas(value)\\prev(value)) \n}\n```\n\n----------------------------------------\n\nTITLE: Logging into DolphinDB and Clearing Cache in DolphinDB Script\nDESCRIPTION: This snippet logs into the DolphinDB server with administrator credentials, clears all caches, undefines all symbols, and proceeds with execution. It prepares the environment for subsequent data operations and ensures no stale data remains in-memory.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/metacode_derived_features/metacode_derived_features_arrayVector.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(\"admin\", \"123456\")\nclearAllCache()\nundef(all)\ngo\n```\n\n----------------------------------------\n\nTITLE: Scheduling AMD Subscription - DolphinDB\nDESCRIPTION: This code schedules the `subscribeAmdDaily` function to run daily using the `scheduleJob` function.  The job is set to run at 09:11, with a start date of 2022.12.13 and an end date of 2032.12.13.  The frequency is set to daily ('D').\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_implementation_for_AMD_Plugin.md#_snippet_11\n\nLANGUAGE: DolphinDB\nCODE:\n```\nscheduleJob(\"subscribeAmdDaily\", \"subscribeAmdDaily\", subscribeAmdDaily, 09:11m, startDate=2022.12.13, endDate=2032.12.13, frequency='D')\n```\n\n----------------------------------------\n\nTITLE: Updating Data in OLAP Table\nDESCRIPTION: This script updates data in the 'machines' table within the OLAP storage engine. It sets the values of `tag1` and `tag5` columns based on the loop counter and filters the data by `id` and `datetime`. This is used to simulate and test the update functionality.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/dolphindb_update.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\nmachines = loadTable(\"dfs://olapDemo\", \"machines\")\nfor(i in 0..20)\n update machines set tag1=i,tag5=i where id in 1..5,date(datetime)=2020.09.01\n```\n\n----------------------------------------\n\nTITLE: Inserting Data into Memory Table with SQL\nDESCRIPTION: Demonstrates inserting data into a memory table using SQL INSERT statements.  It covers inserting data into specific columns, leaving others null, and inserting data into all columns.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/partitioned_in_memory_table.md#_snippet_12\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//往指定列插入数据，其他列为空\ninsert into trades(sym,date) values(`S,2000.12.31)\n\n//往所有列插入数据\ninsert into trades values(`S`IBM,[2000.12.31,2000.12.30],[10.0,20.0],[10.0,20.0],[10.0,20.0],[10.0,20.0],[10.0,20.0],[10.0,20.0],[10,20],[10,20],[10,20],[10,20],[10,20],[10,20])\n\n```\n\n----------------------------------------\n\nTITLE: Querying CPU Usage Percentage - PromQL\nDESCRIPTION: This PromQL query calculates the percentage of CPU time spent in non-idle modes, effectively giving the CPU utilization percentage from Node Exporter metrics. It uses rate and sum aggregation functions over a 1-minute time window.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/cluster_monitor.md#_snippet_4\n\nLANGUAGE: PromQL\nCODE:\n```\n(1-sum(rate(node_cpu_seconds_total{mode=\"idle\"}[1m])) by (job) / sum(rate(node_cpu_seconds_total[1m])) by (job)) *100\n```\n\n----------------------------------------\n\nTITLE: Initializing Mutable Stream Table - DolphinDB\nDESCRIPTION: This snippet initializes a mutable stream table variable named `st` with the same schema used for the `doorRecord` table. This table will be used by the `genData` function to hold the simulated data before it is published.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/streaming_engine_anomaly_alerts/流计算引擎级联检测门禁超时数据.txt#_snippet_10\n\nLANGUAGE: DolphinDB\nCODE:\n```\nst=streamTable(\n\t\tarray(INT,0) as recordType,\n        array(INT,0) as doorEventCode,\n       \tarray(DATETIME,0) eventDate, \n       \tarray(BOOL,0) as readerType,\n       \tarray(SYMBOL,0) as sn,\n       \tarray(INT,0) as doorNum,\n       \tarray(SYMBOL,0) as card\n\t)\n```\n\n----------------------------------------\n\nTITLE: Loading and Querying Fund Net Value Data in DolphinDB\nDESCRIPTION: Loads the metadata of the distributed table 'publicFundNetValue' from the 'dfs://publicFundDB' database into a variable. It then performs basic SQL queries to retrieve the top 10 records and the total number of records from the table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/public_fund_basic_analysis/basic_analysis_of_public_fund_net_value.txt#_snippet_2\n\nLANGUAGE: dolphindb\nCODE:\n```\n// load metadata\nfundNetValue = loadTable(\"dfs://publicFundDB\", \"publicFundNetValue\")\n// query the top ten records\nselect top 10 * from fundNetValue\n// query the total records\nselect count(*) from fundNetValue\n```\n\n----------------------------------------\n\nTITLE: Preparing Data for XGBoost Training\nDESCRIPTION: This code prepares the data for training an XGBoost model. It extracts the 'Label' column from the 'wineTrain' table as the target variable (Y) and selects the remaining columns as the feature variables (X).\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/machine_learning.md#_snippet_19\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nY = exec Label from wineTrain\nX = select Alcohol, MalicAcid, Ash, AlcalinityOfAsh, Magnesium, TotalPhenols, Flavanoids, NonflavanoidPhenols, Proanthocyanins, ColorIntensity, Hue, OD280_OD315, Proline from wineTrain\n```\n\n----------------------------------------\n\nTITLE: Average Price per Stock over Multiple Days\nDESCRIPTION: Calculates daily average mid-price for all stocks within a date range, filtered by trading hours, grouped by symbol and date.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs InfluxDB/test_dolphindb_big.txt#_snippet_18\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 10. 经典查询：按 [日期段、时间段] 过滤, 查询 (每股票，每天) 均价\n\ntimer\nselect avg(ofr + bid) / 2.0 as avg_price\nfrom taq \nwhere\n\tdate between 2007.08.05 : 2007.08.07,\n\ttime between 09:30:00 : 16:00:00\n\ngroup by symbol, date\n```\n\n----------------------------------------\n\nTITLE: Generating Sample IoT Data with DolphinDB Script\nDESCRIPTION: Python-like script that creates a sample IoT dataset in DolphinDB with device IDs, timestamps, and temperature readings. It generates 10,000 records distributed across 10 devices and shares the table as 'iotTable'.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/web_chart_integration.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ndata = table(100000:0, `devId`time`ec,[INT,TIMESTAMP,DOUBLE]);\ndata.tableInsert(take(1..10,10000),add((1..10000),now()) , norm(1,0.5,10000))\nshare data as iotTable\n```\n\n----------------------------------------\n\nTITLE: Downloading DolphinDB Linux ABI Server\nDESCRIPTION: Downloads the ABI version of the DolphinDB server for Linux, version 2.00.11.3.  Uses `wget` to retrieve the package from the official DolphinDB website, specifying the ABI suffix. The downloaded file is saved as `dolphindb.zip`.  This command targets a specific build variant.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/standalone_server.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nwget https://www.dolphindb.cn/downloads/DolphinDB_Linux64_V2.00.11.3_ABI.zip -O dolphindb.zip\n```\n\n----------------------------------------\n\nTITLE: Calculating Drawdown Ratio (Calmar Ratio) in Python\nDESCRIPTION: Defines a Python function `getDrawdownRatio`. It calculates the ratio of annualized return to maximum drawdown using the Python versions of `getAnnualReturn` and `getMaxDrawdown`. Includes a check to prevent division by zero if the maximum drawdown is zero. Requires the helper functions.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/fund_factor_contrasted_by_py.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef getDrawdownRatio(value):\n    max_drawdown = getMaxDrawdown(value)\n    return getAnnualReturn(value) / max_drawdown if max_drawdown != 0 else 0\n```\n\n----------------------------------------\n\nTITLE: 列出CentOS 8中SQL Server官方ODBC驱动崩溃的调用堆栈\nDESCRIPTION: 此堆栈追踪描述了在使用Microsoft的SQL Server官方ODBC驱动时，OpenSSL 1.1.0版本引起的崩溃，显示了在`libssl.so.1.1`相关函数中的段错误，帮助定位兼容性问题。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ODBC_plugin_user_guide.md#_snippet_9\n\nLANGUAGE: C++\nCODE:\n```\n# Thread 71 \"dolphindb\" received signal SIGSEGV, Segmentation fault.\n# [Switching to Thread 0x7fffd17d4700 (LWP 988437)]\n# __strcmp_avx2 () at ../sysdeps/x86_64/multiarch/strcmp-avx2.S:101\n# 101     ../sysdeps/x86_64/multiarch/strcmp-avx2.S: 没有那个文件或目录.\n# (gdb) bt\n# ...\n# 0  __strcmp_avx2 () at ../sysdeps/x86_64/multiarch/strcmp-avx2.S:101\n# 1  0x00007ffff64705f5 in lh_insert () from ./libDolphinDB.so\n# ...\n```\n\n----------------------------------------\n\nTITLE: Loading and Creating Locations Table in DolphinDB\nDESCRIPTION: Reads location data from CSV and creates the 'locations' table with fields like LOCATION_ID, STREET_ADDRESS, and others. Data is then loaded into the table to support location-based queries.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/Standard_SQL_in_DolphinDB/create_db_table_sql.txt#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlocations_tmp=loadText(dir+\"LOCATIONS.csv\")\ncreate table \"dfs://hr\".\"locations\" (\n\tLOCATION_ID INT,\n\tSTREET_ADDRESS STRING,\n\tPOSTAL_CODE LONG,\n\tCITY STRING,\n\tSTATE_PROVINCE STRING,\n\tCOUNTRY_ID SYMBOL\n)\nlocations = loadTable(\"dfs://hr\", \"locations\")\nlocations.append!(locations_tmp)\nselect * from locations\n```\n\n----------------------------------------\n\nTITLE: Creating Database and Retention Policy in DolphinDB\nDESCRIPTION: Creates a DolphinDB database named 'test' and defines a retention policy named 'one_day' on it. The policy keeps data indefinitely ('DURATION INF'), uses a replication factor of 1, shards data by day ('SHARD DURATION 1d'), and is set as the default policy for the 'test' database.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs InfluxDB/readings.lineprotocol.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\nCREATE DATABASE test\nCREATE RETENTION POLICY one_day ON test DURATION INF REPLICATION 1 SHARD DURATION 1d DEFAULT\n```\n\n----------------------------------------\n\nTITLE: Starting Debezium MySQL Connector\nDESCRIPTION: Curl command to start the Debezium MySQL connector by posting the configuration to the Kafka Connect REST API.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Debezium_and_Kafka_data_sync.md#_snippet_10\n\nLANGUAGE: Bash\nCODE:\n```\ncurl -i -X POST -H \"Accept:application/json\" -H  \"Content-Type:application/json\" http://192.168.189.130:8083/connectors/ -d @/KFDATA/datasyn-config/source-mysql.json\n```\n\nLANGUAGE: Bash\nCODE:\n```\ncd /KFDATA/kafka-tools/bin\n./rest.sh create @/KFDATA/datasyn-config/source-mysql.json\n```\n\n----------------------------------------\n\nTITLE: Authenticating and Loading MQTT Plugin - DolphinDB\nDESCRIPTION: This snippet performs initial setup by logging into the DolphinDB server as 'admin' with password '123456' and attempting to load the MQTT plugin from the home directory. A try-catch block is used to handle potential errors during plugin loading.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/streaming_engine_anomaly_alerts/流计算引擎级联检测门禁超时数据.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(`admin,`123456)\ntry{loadPlugin(getHomeDir()+\"/plugins/mqtt/PluginMQTTClient.txt\")} catch(ex) {print(ex)}\ngo\nuse mqtt;\n```\n\n----------------------------------------\n\nTITLE: Querying - Inner Join DolphinDB\nDESCRIPTION: This snippet performs an inner join between the `readings` and `device_info` tables, linking based on `device_id`.  It then selects distinct model values, filtering by a specific SSID.  This demonstrates how to combine data from multiple tables.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs TimescaleDB/test_dolphindb_small.txt#_snippet_19\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 10. 关联查询.等值连接：查询连接某个 WiFi 的所有设备的型号\ntimer\nselect distinct model\nfrom ej(readings, device_info, 'device_id')\nwhere ssid = 'demo-net'\n```\n\n----------------------------------------\n\nTITLE: Updating Licenses Online\nDESCRIPTION: This code snippet attempts to update the license key online. It requires that the customer name, node count, memory size, and CPU core count on the new license are the same or greater than those on the old license.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/single_machine_cluster_deploy.md#_snippet_18\n\nLANGUAGE: DolphinDB\nCODE:\n```\nuse ops\nupdateAllLicenses()\n```\n\n----------------------------------------\n\nTITLE: Calculating Daily Return Skewness with Minute-Level Data in DolphinDB\nDESCRIPTION: Creates a custom function for calculating daily return skewness using minute-level data. The function computes skewness of price ratios and applies it across securities grouped by date.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practice_for_factor_calculation.md#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndefg dayReturnSkew(close){\n\treturn skew(ratios(close))\t\n}\n\nminReturn = select `dayReturnSkew as factorname, dayReturnSkew(close) as val from loadTable(\"dfs://k_minute_level\", \"k_minute\") where date(tradetime) between 2020.01.02 : 2020.01.31 group by date(tradetime) as tradetime, securityid\n```\n\n----------------------------------------\n\nTITLE: Granting DB_OWNER Permission to User\nDESCRIPTION: This snippet demonstrates granting the `DB_OWNER` permission to a user, allowing them to create databases and manage the databases they create. This is suitable for development teams that need full control over their databases.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practice_for_factor_calculation.md#_snippet_33\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(\"admin\", \"123456\");\ncreateUser(\"user1\", \"passwd1\")\ngrant(\"user1\", DB_OWNER)\n```\n\n----------------------------------------\n\nTITLE: Partition Pruning Optimization (Temporal Format)\nDESCRIPTION: This code snippet demonstrates an unoptimized query using `temporalFormat` on the DateTime column in the where clause, hindering partition pruning. The query calculates the count of records grouped by SecurityID within a specific date range.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_sql_cases.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntimer t1 = select count(*) from snapshot \n\t\t   where temporalFormat(DateTime, \"yyyy.MM.dd\") >= \"2020.06.01\" and temporalFormat(DateTime, \"yyyy.MM.dd\") <= \"2020.06.02\" \n\t\t   group by SecurityID \n```\n\n----------------------------------------\n\nTITLE: Calculate Max Offer - Min Bid per Minute for Each Stock\nDESCRIPTION: Computes the maximum offer price minus the minimum bid price for each symbol over each minute of a specific day, revealing price gaps.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs InfluxDB/test_dolphindb_big.txt#_snippet_16\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 8. 经典查询：计算 某天 (每个股票 每分钟) 最大卖出与最小买入价之差\n\ntimer\nselect max(ofr) - min(bid) as gap \nfrom taq \nwhere \n\tdate = 2007.08.03, \n\tbid > 0, \n\tofr > bid\n\ngroup by symbol, minute(time) as minute\n```\n\n----------------------------------------\n\nTITLE: Calculating Information Ratio in DolphinDB\nDESCRIPTION: This function calculates the information ratio of a fund relative to a benchmark.  It takes the value series of the fund and the price series of the benchmark as inputs. It uses the previously defined `getTrackError` and `getIndexFundAnReturn` functions to calculate the information ratio.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_factor.txt#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndefg getInforRatio(value, price){\n\treturn (pow(1 + ((last(value) - first(value))\\first(value)), 252\\487) - 1 - getIndexFundAnReturn(price)) \\ (getTrackError(value, price) * sqrt(252))\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Stateful Functions for Alpha #1 Factor Calculation in DolphinDB\nDESCRIPTION: This DolphinDB script defines stateful functions `alpha1TS` and `alpha1Panel` to calculate the Alpha #1 factor. `alpha1TS` performs time series processing, and `alpha1Panel` performs cross-sectional ranking.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practice_for_factor_calculation.md#_snippet_14\n\nLANGUAGE: DolphinDB\nCODE:\n```\n@state\ndef alpha1TS(close){\n\treturn mimax(pow(iif(ratios(close) - 1 < 0, mstd(ratios(close) - 1, 20),close), 2.0), 5)\n}\n\ndef alpha1Panel(close){\n\treturn rowRank(X=alpha1TS(close), percent=true) - 0.5\n}\n```\n\n----------------------------------------\n\nTITLE: Grafana Query for Warning Table - DolphinDB\nDESCRIPTION: This code snippet shows the Grafana query used to fetch data from the `warningTable` for visualization. It is a simple `select *` query that retrieves all columns and rows from the table. This query is executed by the Grafana DolphinDB DataSource plugin to display anomaly rates and warning statuses.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/knn_iot.md#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect * from warningTable\n```\n\n----------------------------------------\n\nTITLE: Configuring Chunk Cache Engine Memory Size DolphinDB\nDESCRIPTION: This parameter sets the capacity of the chunk cache engine in GB. After the cache engine is enabled, data is not written to disk until data in the cache exceeds 30% of hunkCacheEngineMemSize. To enable the cache engine, chunkCacheEngineMemSize must be greater than 0 and dataSync must be set to 1.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/ha_cluster_deployment/P3/config/config-specification.txt#_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\nchunkCacheEngineMemSize=2\n```\n\n----------------------------------------\n\nTITLE: Implementing `msum` Function in DolphinDB Plugin\nDESCRIPTION: This code snippet outlines the structure for implementing a `msum` (moving sum) function as a DolphinDB plugin in C++. It initializes the result vector with the same size and data type as the input vector, preparing for the subsequent calculations within the `msum` function.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_development_tutorial.md#_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\nint size = X->size();\nint windowSize = window->getInt();\nConstantSP result = Util::createVector(DT_DOUBLE, size);\n```\n\n----------------------------------------\n\nTITLE: Limiting Database Creation with DB_OWNER - DolphinDB\nDESCRIPTION: This example restricts a user from creating databases with a specific prefix using the DB_OWNER permission. It grants DB_OWNER permission for databases starting with 'dfs://db0'.  Requires admin privileges to create user.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Permission_Management.md#_snippet_23\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(\"admin\",\"123456\")\ncreateUser(\"AlexSmith\",\"123456\")\ngrant(\"AlexSmith\", DB_OWNER, \"dfs://db0*\")\n```\n\n----------------------------------------\n\nTITLE: High-Frequency Factor Calculation Expression\nDESCRIPTION: This DolphinDB script demonstrates a complex high-frequency factor calculation, using nested `ema` (exponential moving average) and the `sum_diff` function. This expression serves as a starting point to explain the challenges of calculating factors.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/reactive_state_engine.md#_snippet_1\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nema(1000 * sum_diff(ema(price, 20), ema(price, 40)),10) -  ema(1000 * sum_diff(ema(price, 20), ema(price, 40)), 20)\n```\n\n----------------------------------------\n\nTITLE: DolphinDB Data Integration with Echarts\nDESCRIPTION: JavaScript function that connects to DolphinDB, retrieves data using SQL-like query, and processes the response for display in an Echarts chart. Uses DolphinDBConnection and DolphinDBEntity for data fetching and transformation.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/web_chart_integration.md#_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\nvar conn = new DolphinDBConnection('http://localhost:8848');\n//向DolphinDB发送查询脚本，并获取返回的数据\nconn.run(\"select avg(ec) as ec from iotTable group by second(time)\", function(re){\n\tif(re.resultCode&&re.resultCode==\"1\"){\n\t\talert(re.msg);\n\t} else {\n\t\tjobj = new DolphinDBEntity(re).toVector();//将返回结果转换成列数据\n\t\tvar time = jobj[0].value;\n\t\tvar ecdata = jobj[1].value;\n\t\tvar option = {\n\t\t\ttitle: {\n\t\t\t\ttext: '设备温度'\n\t\t\t},\n\t\t\txAxis: {\n\t\t\t\tdata: time\n\t\t\t},\n\t\t\tyAxis: {},\n\t\t\tseries: [{\n\t\t\t\tname: '温度',\n\t\t\t\ttype: 'line',\n\t\t\t\tdata: ecdata\n\t\t\t}]\n\t\t};\n\t\tmyChart.setOption(option);\n\t}\n});\n```\n\n----------------------------------------\n\nTITLE: Extracting Unique Symbols and MMID Values\nDESCRIPTION: Counts occurrences by symbol and mmid, then saves symbol list to a text file and prepares MMID enum values for TimescaleDB.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs InfluxDB/test_dolphindb_big.txt#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// --------------------- 将股票代码和 mmid 的所有取值导出，以此创建 TimescaleDB 的 enum type 的可能取值\nselect count(*) from taq\n\nsymbols_tb =\n\tselect count(*)\n\tfrom taq\n\tgroup by symbol\n\nsaveText(symbols_tb.symbol, FP_TAQ + 'symbols.txt')\n\n// FLOW, EDGX, EDGA, NASD  ->  作为 TimescaleDB 的 Mmid enum type 的可能值\n\nmmids_tb =\n\tselect count(*)\n\tfrom taq\n\tgroup by mmid\n```\n\n----------------------------------------\n\nTITLE: Calculating Index Fund Annual Return in DolphinDB\nDESCRIPTION: This function calculates the annualized return of the index fund. It uses the first and last values of the price to calculate the return. It is used within information ratio calculation.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_factor.txt#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\n/**\n * 因子7：信息比率\n * 基金年化收益率 - 指数年化收益率\\跟踪误差\n */\n \ndefg getIndexFundAnReturn(price){\n\treturn pow(1 + ((last(price) - first(price))\\first(price)), 252\\487) - 1\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a scheduled job with permission constraints in DolphinDB\nDESCRIPTION: Example showing a scheduled job created by a user without proper permissions, which will fail when trying to access a distributed table due to insufficient privileges.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/scheduledJob.md#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef foo1(){\n\tprint \"Test scheduled job \"+ now()\n\tcnt=exec count(*) from loadTable(\"dfs://FuturesContract\",\"tb\")\n\tprint \"The count of table is \"+cnt\n\treturn cnt\n}\nlogin(\"guestUser1\",\"123456\")\nscheduleJob(`guestGetDfsjob, \"dfs read\", foo1, [12:00m, 21:03m, 21:45m], 2020.01.01, 2021.12.31, \"D\");\n```\n\n----------------------------------------\n\nTITLE: Pivoting, Reshaping, and Merging Data in pandas and DolphinDB\nDESCRIPTION: This snippet covers functions for reshaping data (pivot, melt) and merging or joining DataFrames in pandas, with their DolphinDB equivalents. It supports data transformation workflows including pivoting, unpivoting, and combining multiple datasets.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/function_mapping_py.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\npandas.DataFrame.pivot  # Reshape data based on unique values\npandas.DataFrame.melt  # Convert wide to long format\npandas.DataFrame.merge / pandas.DataFrame.join  # Combine DataFrames based on shared keys\n```\n\n----------------------------------------\n\nTITLE: Checking Prediction Results (DolphinDB Script)\nDESCRIPTION: Pauses script execution for 1 second using `sleep(1000)` to allow the streaming jobs to process some data, and then queries the `result10min` table to display the first few rows of generated real-time prediction results.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/metacode_derived_features/metacode_derived_features_streaming.txt#_snippet_18\n\nLANGUAGE: DolphinDB\nCODE:\n```\nsleep(1000)\nselect * from result10min\n```\n\n----------------------------------------\n\nTITLE: Querying Stream Publication Tables via DolphinDB API - DolphinDB\nDESCRIPTION: This snippet queries the current stream publication tables to retrieve all registered subscriptions information for the node. The getStreamingStat().pubTables function returns metadata on published stream tables, allowing the user to verify active stream subscriptions after startup.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_auto_sub.md#_snippet_4\n\nLANGUAGE: dolphindb\nCODE:\n```\ngetStreamingStat().pubTables\n```\n\n----------------------------------------\n\nTITLE: Granting Table Read Permission to User and Group\nDESCRIPTION: This code demonstrates granting `TABLE_READ` permission to a user and a group. This setup is appropriate for scenarios where operations staff manages the database, and analysts only require read access to the tables.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practice_for_factor_calculation.md#_snippet_34\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//以用户的方式进行授权\ncreateUser(\"user1\", \"passwd1\")\ngrant(\"user1\", TABLE_READ, \"dfs://db1/pt1\")\n\n//以group的方式进行授权\ncreateGroup(\"group1name\", \"user1\")\ngrant(\"group1name\", TABLE_READ, \"dfs://db1/pt1\")\n```\n\n----------------------------------------\n\nTITLE: Import Binary Data with Strings\nDESCRIPTION: This snippet imports binary data with string fields using `loadRecord`. A schema must be defined specifying the data types and string lengths. The imported data is then optionally transformed.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_data.md#_snippet_12\n\nLANGUAGE: txt\nCODE:\n```\ndataFilePath=\"/home/data/binStringSample.bin\"\ntmp=loadRecord(dataFilePath, schema)\ntb=select code,date,time,last,volume,value,ask1,ask_size1,bid1,bid_size1 from tmp;\n```\n\n----------------------------------------\n\nTITLE: Creating a Database in DolphinDB\nDESCRIPTION: This code snippet demonstrates how to create a database in DolphinDB. If the specified directory does not exist, it will be created. If the directory exists and contains only DolphinDB-created tables and related files, the database will be opened. If it contains non-DolphinDB files, the database creation will fail.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Single_Node_Tutorial.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndb = database(\"dfs://DolphinDB\");\n```\n\n----------------------------------------\n\nTITLE: Simulating Data Ingestion with MTW in C++\nDESCRIPTION: This code snippet simulates data ingestion from a message queue using the MultithreadedTableWriter. It iterates through the data (simulated by 'bt'), inserts each row into the MTW, and controls the data ingestion rate using `sleep_for`. It simulates receiving data and writing to the DolphinDB stream table using MTW.  Assumes `datas` is an array of `ConstantSP` representing the data to be written.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_cpp_api_connector.md#_snippet_10\n\nLANGUAGE: C++\nCODE:\n```\n// 模拟接受批量数据，创建单线程写入数据\n// bt 模拟接收消息中间件发送的数据，按设备（每台设备1000条数据）遍历采集数据\nfor(int i=0;i < (bt->rows())/1000;i++){\n\tsystem_clock::duration begin = system_clock::now().time_since_epoch();\n\tmilliseconds milbegin = duration_cast<milliseconds>(begin);\n\t// 每台数据共1000个测点，写入1000行\n\tfor(int j=i*1000;j<(i+1)*1000;j++){\n\t\tErrorCodeInfo pErrorInfo;\n\t\t// 模拟对单条数据6个字段的写入\n\t\twriter.insert(pErrorInfo,\n\t\t\tdatas[i*6+0], datas[i*6+1], datas[i*6+2], datas[i*6+3], datas[i*6+4], datas[i*6+5]\n\t\t)\n\t}\n\tsystem_clock::duration end = system_clock::now().time_since_epoch();\n\tmilliseconds milend = duration_cast<milliseconds>(end);\n\tif((milend.count()-milbegin.count())<5000){\n\t\t// 控制模拟写入的频率\n\t\tsleep_for(std::chrono::milliseconds(5000-(milend.count()-milbegin.count())));\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: Calculating Price Deltas per Symbol using Matrix Operations in DolphinDB Script\nDESCRIPTION: First, creates a price matrix using `panel`. Then, applies the `deltas()` sequence function directly to this matrix. The function calculates the difference between consecutive prices column-wise (i.e., for each stock symbol independently).\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/panel_data.md#_snippet_13\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nprice = panel(t.timestamp, t.sym, t.price);\ndeltas(price);\n```\n\n----------------------------------------\n\nTITLE: Loading DolphinDB Redis Plugin\nDESCRIPTION: This snippet attempts to load the DolphinDB Redis plugin from a specified file path. It uses a try-catch block to handle potential errors during plugin loading, printing the exception details if loading fails. This is a necessary prerequisite for using any `redis::` functions.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/redis_plugin_tutorial/example2.txt#_snippet_0\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\ntry{loadPlugin(\"./path/PluginRedis.txt\")}catch(ex){print ex};\n```\n\n----------------------------------------\n\nTITLE: Replacing Column Data Type\nDESCRIPTION: Demonstrates how to modify the data type of a column using the `replaceColumn!` function. Only non-partitioned tables support this operation. The example changes the data type of price1 from DOUBLE to FLOAT.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/partitioned_in_memory_table.md#_snippet_28\n\nLANGUAGE: DolphinDB\nCODE:\n```\nNewPrice1=float(exec price1 from trades)\nreplaceColumn!(trades,`price1,NewPrice1)\nschema(trades)\n```\n\n----------------------------------------\n\nTITLE: Querying the Output Stream Table - DolphinDB\nDESCRIPTION: This simple query selects all columns and rows from the `outputSt1` stream table. This allows users to view the final processed results produced by the cascaded stream processing engines.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/streaming_engine_anomaly_alerts/流计算引擎级联检测门禁超时数据.txt#_snippet_12\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect * from outputSt1\n```\n\n----------------------------------------\n\nTITLE: Subscribing Stream Table to DFS Table DolphinDB Script\nDESCRIPTION: Sets up a subscription from the `sensorTemp` stream table to the partitioned `dfsTable`. The `handler` function `append!{dfsTable}` ensures that new data arriving in the stream table is automatically appended to the persistent DFS table for long-term storage.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/iot_demo_script.txt#_snippet_10\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nsubscribeTable(tableName=\"sensorTemp\", actionName=\"sensorTemp\", offset=-1, handler=append!{dfsTable}, msgAsTable=true, batchSize=1000000, throttle=10)\n```\n\n----------------------------------------\n\nTITLE: Restarting Cluster Replication\nDESCRIPTION: This DolphinDB script restarts cluster replication by using the `startClusterReplication` function, which can be called via `rpc` function from a control node. Calling this will transition the cluster's replication status to \"ENABLED.\" Requires a running DolphinDB cluster with replication configured and the function can only be called from a control node.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Asynchronous_Replication.md#_snippet_21\n\nLANGUAGE: dolphindb\nCODE:\n```\nrpc(getControllerAlias(), startClusterReplication)\n```\n\n----------------------------------------\n\nTITLE: Configuring odbcinst.ini for Oracle 21c Driver - Shell\nDESCRIPTION: Sample snippet to be added to /etc/odbcinst.ini defines an Oracle 21c ODBC driver entry, specifying its description and the absolute path to the driver library. The Driver path must be adjusted to reflect the actual extracted client location.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Oracle_to_DolphinDB.md#_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\n[ORAC21c]\nDescription     = Oracle ODBC driver for Oracle 21c\nDriver          = /usr/local/oracle/instantclient_21_7/libsqora.so.21.1\n\n```\n\n----------------------------------------\n\nTITLE: Post-Upgrade Node Verification\nDESCRIPTION: Query to confirm that nodes have been upgraded successfully to version 2.00.8.12 and all are synchronized to the same build date.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/gray_scale_upgrade_ha.md#_snippet_11\n\nLANGUAGE: Shell\nCODE:\n```\nnode\tvalue\n0\tnode44\t2.00.8.12 2023.01.10\n1\tnode45\t2.00.8.12 2023.01.10\n2\tnode43\t2.00.8.12 2023.01.10\n```\n\n----------------------------------------\n\nTITLE: Configuring Elasticsearch JVM Heap Size (jvm.options)\nDESCRIPTION: Specifies the initial (-Xms) and maximum (-Xmx) Java Virtual Machine heap size for the Elasticsearch process. Both are set to 6 gigabytes to allocate dedicated memory resources.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_1\n\nLANGUAGE: properties\nCODE:\n```\n-Xms6g\n-Xmx6g\n```\n\n----------------------------------------\n\nTITLE: Using each with call and funcByName to apply multiple functions\nDESCRIPTION: This snippet uses 'each', 'call', and 'funcByName' to apply the 'sin' and 'log' functions to a vector (1..3). It gets the function references by name and then uses 'call' to create a partial application.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_46\n\nLANGUAGE: shell\nCODE:\n```\neach(call{, 1..3},(funcByName('sin'),funcByName('log')));\n```\n\n----------------------------------------\n\nTITLE: Querying data with TSDB engine\nDESCRIPTION: This code snippet shows an example query that is suitable for the TSDB storage engine.  It retrieves all columns from a table where the StockID is 'AAPL' and the timestamp is within a specified range.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/tsdb_engine.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect * from table where StockID='AAPL', Timestamp > 2021.08.05T09:30:00, Timestamp < 2021.08.05T09:35:00\n```\n\n----------------------------------------\n\nTITLE: Closing and Dropping Database DolphinDB\nDESCRIPTION: This code closes the connection to the database `db` and sets the `readings` variable to NULL, which removes the in-memory table. Subsequently, it drops the database identified by the file path `FP_DB`.  This clears the database and its related data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs TimescaleDB/test_dolphindb_small.txt#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// ----------------- 关闭数据库\nclose(db)\nreadings = NULL\n\n\n// ----------------- 删除数据库\ndropDatabase(FP_DB)\n```\n\n----------------------------------------\n\nTITLE: Initializing Pandas Display Options and Warnings\nDESCRIPTION: Sets global pandas display options for better readability and disables warning messages to suppress clutter in output. No data processing occurs here; it's configuration code.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/DolphinDB_Python_Parser_Intro_for_Quantitative_Finance/因子实现_Python版本/主动成交量占比.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport pandas as pd\nimport numpy as np\nimport itertools\nfrom functools import reduce\nimport os\nimport multiprocessing\nimport time\nimport warnings\nfrom tqdm import tqdm\nimport datetime\n\nwarnings.filterwarnings(\"ignore\")\n\npd.options.display.width = 1200\npd.options.display.max_colwidth = 100\npd.options.display.max_columns = 10\npd.options.mode.chained_assignment = None\n```\n\n----------------------------------------\n\nTITLE: Serializing a DolphinDB Module (Overwriting Existing)\nDESCRIPTION: Calls the `saveModule` function to serialize the fileLog module. The third parameter true indicates that any existing .dom file for this module should be overwritten, ensuring the .dom file is updated after changes to the source .dos file.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/module_tutorial.md#_snippet_3\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nsaveModule(\"fileLog\" , , true)\n```\n\n----------------------------------------\n\nTITLE: Restore Backup Metadata on Downgrade - Shell\nDESCRIPTION: Commands to restore backed-up metadata files during a downgrade or rollback after a failed upgrade. It copies previously saved `dfsMeta` and `CHUNK_METADATA` directories back to their original locations to maintain data consistency.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ARM_standalone_deploy.md#_snippet_7\n\nLANGUAGE: Shell\nCODE:\n```\ncp -r backup/dfsMeta/ local8848/dfsMeta\n```\n\nLANGUAGE: Shell\nCODE:\n```\ncp -r backup/CHUNK_METADATA/ local8848/storage/CHUNK_METADATA\n```\n\n----------------------------------------\n\nTITLE: Dropping DFS Database if Exists DolphinDB Script\nDESCRIPTION: Checks for the existence of a distributed file system (DFS) database named `dfs://iotDemoDB`. If the database is found, it is dropped, ensuring a clean state before attempting to create a new database instance in the subsequent steps.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/iot_demo_script.txt#_snippet_6\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nif(exists(\"dfs://iotDemoDB\")){\n\tdropDatabase(\"dfs://iotDemoDB\")\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling HTTPS via Command Line - DolphinDB\nDESCRIPTION: This snippet demonstrates enabling HTTPS for a DolphinDB controller node through a command-line argument.  It shows the use of the `-enableHTTPS true` flag along with other startup parameters like `-home`, `-publicName`, `-mode`, `-localSite`, and `-logFile`. The `publicName` parameter is important for certificate validation.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Permission_Management.md#_snippet_44\n\nLANGUAGE: Shell\nCODE:\n```\n./dolphindb -enableHTTPS true -home master -publicName www.psui.com -mode controller -localSite 192.168.1.30:8500:rh8500 -logFile ./log/master.log\n```\n\n----------------------------------------\n\nTITLE: Granting DBOBJ_CREATE Permission at Database Level - DolphinDB\nDESCRIPTION: This snippet grants a user DBOBJ_CREATE permission for a specific database in a distributed file system (DFS). It uses the `grant` function with the database's full path. A DolphinDB environment with a DFS database is required.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Permission_Management.md#_snippet_21\n\nLANGUAGE: DolphinDB\nCODE:\n```\ngrant(`user1, DBOBJ_CREATE,\"dfs://valuedb\")\n```\n\n----------------------------------------\n\nTITLE: Configuring DolphinDB - cluster.nodes\nDESCRIPTION: This file (`cluster.nodes`) defines the nodes within a DolphinDB cluster. It specifies the local site and the mode (agent or datanode) for each node. Dependencies: DolphinDB cluster setup.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_Druid_appendix.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlocalSite,mode\nlocalhost:9910:agent,agent\nlocalhost:9921:DFS_NODE1,datanode\nlocalhost:9922:DFS_NODE2,datanode\nlocalhost:9923:DFS_NODE3,datanode\nlocalhost:9924:DFS_NODE4,datanode\n```\n\n----------------------------------------\n\nTITLE: Execute Single-Day Performance Test for Specific Date\nDESCRIPTION: This snippet runs the previously defined performance testing function for a specific date (February 28, 2022), generating a table of calculated option Greeks and implied volatilities, with timing information included.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/IV_Greeks_Calculation_for_ETF_Options_Using_JIT/calculation_scripts.txt#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\noneDay = testOneDayPerformance(closPriceWideMatrix, etfPriceWideMatrix, contractInfo, 2022.02.28)\n```\n\n----------------------------------------\n\nTITLE: Sliding Window Calculation on Matrix with msum in DolphinDB\nDESCRIPTION: Applies the `msum` function to calculate the sliding sum on the indexed matrix `m`. The window size is 3 rows, and the step is 1 row. This example shows how to perform a sliding window calculation on each column of the matrix. Requires a pre-existing indexed matrix named 'm'.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/window_cal.md#_snippet_17\n\nLANGUAGE: DolphinDB\nCODE:\n```\nmsum(m,3,1)\n```\n\n----------------------------------------\n\nTITLE: Loading and Creating Departments Table in DolphinDB\nDESCRIPTION: Reads department data from CSV, creates a 'departments' table, and loads the data for departmental analysis. Columns include department ID, name, manager, and location.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/Standard_SQL_in_DolphinDB/create_db_table_sql.txt#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndepartments_tmp=loadText(dir+\"DEPARTMENTS.csv\")\ncreate table \"dfs://hr\".\"departments\" (\n\tDEPARTMENT_ID INT,\n\tDEPARTMENT_NAME STRING,\n\tMANAGER_ID INT,\n\tLOCATION_ID INT\n)\ndepartments = loadTable(\"dfs://hr\", \"departments\")\ndepartments.append!(departments_tmp)\nselect * from departments\n```\n\n----------------------------------------\n\nTITLE: Creating a Reactive State Engine for Double EMA Factor Calculation in DolphinDB\nDESCRIPTION: This DolphinDB script creates a reactive state engine for calculating the double EMA factor using the `factorDoubleEMA` function.  Requires `inputDummyTable` and populates the `resultTable`. \nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practice_for_factor_calculation.md#_snippet_16\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//创建流引擎，并传入因子算法factorDoubleEMA\nfactors = <[TradeTime, factorDoubleEMA(close)]>\ndemoEngine = createReactiveStateEngine(name=engineName, metrics=factors, dummyTable=inputDummyTable, outputTable=resultTable, keyColumn=\"SecurityID\")\n```\n\n----------------------------------------\n\nTITLE: Loading and Querying Public Fund Data in DolphinDB\nDESCRIPTION: Loads the public fund data from the database and performs basic queries to explore the data, including fetching the top 10 records, all records, and counting the total number of records.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/public_fund_basic_analysis/basic_analysis_of_public_fund_open_market_data.txt#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// load metadata\nfundData = loadTable(\"dfs://publicFundDB\", \"publicFundData\")\n// query the top ten records\nselect top 10 * from fundData\n// query all records\npublicFundData = select * from fundData\n// query the total records\nselect count(*) from fundData\n```\n\n----------------------------------------\n\nTITLE: Creating Database for Shenzhen Stock Exchange Tick-by-Tick Trade Data with TSDB Engine\nDESCRIPTION: Creates a database for storing Shenzhen Stock Exchange tick-by-tick trade data using a combined partitioning strategy with date value and HASH on symbols. The TSDB engine is used with symbol and trade time as sorting columns.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practices_for_partitioned_storage.md#_snippet_4\n\nLANGUAGE: SQL\nCODE:\n```\ncreate database \"dfs://split_SZ_TB\"\npartitioned by VALUE(2020.01.01..2021.01.01), HASH([SYMBOL, 25])\nengine='TSDB'\n\ncreate table \"dfs://split_SZ_TB\".\"split_SZ_tradeTB\"(\n    ChannelNo INT\n    ApplSeqNum LONG\n    MDStreamID SYMBOL\n    BidApplSeqNum LONG\n    OfferApplSeqNum LONG\n    SecurityID SYMBOL\n    SecurityIDSource SYMBOL\n    TradePrice DOUBLE\n    TradeQty INT\n    ExecType SYMBOL\n    TradeDate DATE[comment=\"交易日期\", compress=\"delta\"]   \n    TradeTime TIME[comment=\"交易时间\", compress=\"delta\"]   \n    LocalTime TIME\n    SeqNo INT\n    OrderKind SYMBOL\n)\npartitioned by TradeDate, SecurityID,\nsortColumns=[`SecurityID,`TradeTime],\nkeepDuplicates=ALL\n```\n\n----------------------------------------\n\nTITLE: Subscribing to Snapshot Stream Data in DolphinDB\nDESCRIPTION: Subscribes to real-time incremental data in the snapshotStream table and processes it through the time series aggregation engine. This handles the input data for the volatility prediction workflow.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/machine_learning_volatility.md#_snippet_5\n\nLANGUAGE: dolphindb\nCODE:\n```\nsubscribeTable(tableName=\"snapshotStream\", actionName=\"aggrFeatures10min\", offset=-1, handler=getStreamEngine(\"aggrFeatures10min\"), msgAsTable=true, batchSize=2000, throttle=1, hash=0, reconnect=true)\n```\n\n----------------------------------------\n\nTITLE: Comparative Query with Pivot in DolphinDB\nDESCRIPTION: SQL query using DolphinDB's pivot by clause to reorganize data for easier comparison between devices. This transforms time series data into a matrix format for side-by-side comparison.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/iot_examples.md#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect avg(value) from sensors \nwhere id in [1,51,101], datetime between 2020.09.01T00:00:00 : 2020.09.01T23:59:59   \npivot by datetime.minute() as time, id\n```\n\n----------------------------------------\n\nTITLE: Sending Bulk Email to Multiple Recipients in DolphinDB\nDESCRIPTION: Sends an email to multiple recipients simultaneously using the httpClient::sendEmail function with a collection of email addresses, along with the sender credentials, subject, and message text. Verifies successful sending.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/send_messages_external_systems.md#_snippet_20\n\nLANGUAGE: dolphindb\nCODE:\n```\nres=httpClient::sendEmail(user,psw,recipientCollection,'This is a subject','It is a text');\nassert  res[`responseCode]==250;\n```\n\n----------------------------------------\n\nTITLE: Calculating Drawdown Ratio Using DolphinDB\nDESCRIPTION: Defines getDrawdownRatio function to compute the ratio of annual return to maximum drawdown, reflecting return per unit of drawdown risk. Inputs are value series ending in numeric scalar outputs.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_factor_by_ddb/fund_factor_ten.txt#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef getDrawdownRatio(value){\n\treturn getAnnualReturn(value) \\ getMaxDrawdown(value)\n}\n```\n\n----------------------------------------\n\nTITLE: Running DolphinDB Background Linux\nDESCRIPTION: This command starts the DolphinDB server in the background using the `startSingle.sh` script.  This script likely handles the process of starting the server as a daemon, allowing it to run independently of the current terminal session. The server will start with the default port (8848).\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/standalone_server.md#_snippet_10\n\nLANGUAGE: sh\nCODE:\n```\nsh startSingle.sh\n```\n\n----------------------------------------\n\nTITLE: Extracting and Configuring DolphinScheduler Environment - Shell\nDESCRIPTION: This snippet includes shell commands to extract the DolphinScheduler binary package and navigate into its directory. It is a fundamental step in preparing the DolphinScheduler environment before configuration and startup.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/dolphinscheduler_integration.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\ntar -xvzf apache-dolphinscheduler-3.1.7-bin.tar.gz\ncd apache-dolphinscheduler-3.1.7-bin\n```\n\n----------------------------------------\n\nTITLE: Subscribing to Heterogeneous Stream Data for Kafka Publishing (Python)\nDESCRIPTION: This snippet subscribes to the `messageStream` table and sends the incoming data to the previously defined filter `engine` which forwards it to Kafka. The `offset` parameter set to -1 means subscription will start from the current end of the stream table, ensuring that newly incoming data is consumed. `msgAsTable=true` indicates that messages are treated as tables and `reconnect=true` facilitates automatic reconnection.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/stock_market_replay.md#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\nsubscribeTable(tableName=\"messageStream\", actionName=\"sendMsgToKafka\", offset=-1, handler=engine, msgAsTable=true, reconnect=true)\n```\n\n----------------------------------------\n\nTITLE: Stopping DolphinDB Node Linux\nDESCRIPTION: This command shuts down all nodes in the cluster. This is achieved by executing the `stopAllNode.sh` script located in the */DolphinDB/server/clusterDemo* directory.  This is a prerequisite for upgrades.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/standalone_server.md#_snippet_12\n\nLANGUAGE: sh\nCODE:\n```\n./stopAllNode.sh\n```\n\n----------------------------------------\n\nTITLE: Load Text File with Specified Schema\nDESCRIPTION: This snippet demonstrates how to load a text file into a DolphinDB table using `loadText` with a specified schema. It first extracts a schema, updates a specific column's data type, and then loads the data using the modified schema.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_data.md#_snippet_2\n\nLANGUAGE: txt\nCODE:\n```\ndataFilePath = \"/home/data/candle_201801.csv\"\nschemaTb=extractTextSchema(dataFilePath)\nupdate schemaTb set type=`LONG where name=`volume        tt=loadText(dataFilePath,,schemaTb);\n```\n\n----------------------------------------\n\nTITLE: Calculating Moving Average Price per Symbol using `context by` in DolphinDB Script\nDESCRIPTION: Illustrates calculating a 3-period moving average (`mavg`) of the price for each stock symbol (`sym`). The `context by sym` clause groups the data by symbol before applying the `mavg(price, 3)` window function to each group.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/panel_data.md#_snippet_2\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nselect *, mavg(price,3) from t context by sym;\n```\n\n----------------------------------------\n\nTITLE: Dropping a Table in DolphinDB\nDESCRIPTION: This code snippet demonstrates how to drop a table from a DolphinDB database using the `dropTable` function.  The function takes the database object and table name as parameters.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Single_Node_Tutorial.md#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndb = database(\"dfs://DolphinDB\")\ndropTable(db, \"tableName\"); \n```\n\n----------------------------------------\n\nTITLE: Verifying Reactive Engine Results in DolphinDB\nDESCRIPTION: Loads data from a CSV file and compares the results from the reactive engine (`outputSt1`) with the expected output calculated directly from the CSV data. This verifies that the reactive engine correctly identifies state changes.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_streaming_application_in_IOT.md#_snippet_8\n\nLANGUAGE: DolphinDB\nCODE:\n```\n\tt=loadText(getHomeDir()+\"/deviceState.csv\")\n\tt1=select tag,ts,value from t context by tag  having deltas(value)!=0 and prev(value)!=NULL\n\tt2=select * from outputSt1 order by tag\n\tassert eqObj(t1.values(),t2.values())==true\n```\n\n----------------------------------------\n\nTITLE: Performance Testing: Top 1000 Records Sorted by Price Difference\nDESCRIPTION: Fetches top 1000 entries filtered by symbols, date, and time with price spread positive, sorted by spread in descending order.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs InfluxDB/test_dolphindb_big.txt#_snippet_11\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 3. top 1000 + 排序: 按 [股票代码、日期] 过滤，按 [卖出与买入价格差] 降序 排序\n\ntimer \nselect top 1000 *\nfrom taq \nwhere\n\tsymbol in ('IBM', 'MSFT', 'GOOG', 'YHOO'),\n\tdate == 2007.08.07,\n\ttime >= 07:36:37,\n\tofr > bid\norder by (ofr - bid) desc\n```\n\n----------------------------------------\n\nTITLE: Backing Up Metadata\nDESCRIPTION: These commands back up metadata files before upgrading the DolphinDB server. This step is crucial for data recovery in case of upgrade failures. Backups are created for controller and data nodes.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/single_machine_cluster_deploy.md#_snippet_12\n\nLANGUAGE: Shell\nCODE:\n```\nmkdir backup\ncp -r DFSMetaLog.0 backup\ncp -r DFSMasterMetaCheckpoint.0 backup\n```\n\nLANGUAGE: Shell\nCODE:\n```\ncp -r CHUNK_METADATA ../../backup\n```\n\n----------------------------------------\n\nTITLE: Listing Jupyter Notebook kernels\nDESCRIPTION: This shell command lists the available Jupyter Notebook kernels.  The output displays the names and paths of the installed kernels, including the DolphinDB kernel. Used for verifying the DolphinDB kernel's installation path.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/client_tool_tutorial.md#_snippet_5\n\nLANGUAGE: Shell\nCODE:\n```\n>jupyter kernelspec list\nAvailable kernels:\n    dolphindb   /home/admin/.local/share/jupyter/kernels/dolphindb\n    python3       /home/admin/.local/share/jupyter/kernels/python3\n```\n\nLANGUAGE: Shell\nCODE:\n```\n>jupyter kernelspec list\nAvailable kernels:\n    dolphindb   C:\\Users\\admin\\appdata\\local\\programs\\python3\\python37\\share\\jupyter\\kernels\\dolphindb\n    python3       C:\\Users\\admin\\appdata\\local\\programs\\python3\\python37\\share\\jupyter\\kernels\\python3\n```\n\n----------------------------------------\n\nTITLE: Querying HTTP Request Rate with PromQL\nDESCRIPTION: Examples of using PromQL functions to analyze HTTP request metrics, including rate calculation, top values selection, and instantaneous rate calculation.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/cluster_monitor.md#_snippet_11\n\nLANGUAGE: PromQL\nCODE:\n```\nrate(http_requests_total[2h])，获取2小时内，该指标下各时间序列上的 http 总请求数的增长速率\n\ntopk(3,http_requests_total)，获取该指标下http请求总数排名前3的时间序列\n\nirate(http_requests_total[2h])，高灵敏度函数，用于计算指标的瞬时速率\n```\n\n----------------------------------------\n\nTITLE: Cleanup Operations for Stream Processing in DolphinDB\nDESCRIPTION: Provides cleanup commands to terminate the anomaly detection process. It drops the aggregator engine, unsubscribes from both monitoring actions, removes the stream table, and undefines the stream table variable.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/alarm.txt#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndropAggregator(\"engine1\")\nunsubscribeTable(, \"sensor\", \"sensorAnomalyDetection\")\nunsubscribeTable(, \"sensor\", \"noData\")\ndropStreamTable(`sensor)\nundef(`st, VAR)\n```\n\n----------------------------------------\n\nTITLE: Calculating Tracking Error in DolphinDB\nDESCRIPTION: This function calculates the tracking error between a value series and a price series. It takes two series as input: the value series and the price series. It returns the standard deviation of the difference between the percentage changes in the two series.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_factor.txt#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\n/**\n * 因子6：跟踪误差(Tracking Error)\n */\ndefg getTrackError(value, price){\n\treturn std(deltas(value)\\prev(value) - deltas(price)\\prev(price))\n}\n```\n\n----------------------------------------\n\nTITLE: SQL执行计划的详细资源消耗分析\nDESCRIPTION: 此JSON结构展示了分布式SQL在计算节点和数据节点的资源消耗，包括每个阶段的时间和处理节点，有助于优化性能。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Compute_Node.md#_snippet_4\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    \"measurement\": \"microsecond\",\n    \"explain\": {\n        \"from\": {\n            \"cost\": 2\n        },\n        \"map\": {\n            \"partitions\": {\n                \"local\": 0,\n                \"remote\": 320\n            },\n            \"cost\": 5189927,\n            \"detail\": {...}\n        },\n        \"merge\": {\n            \"cost\": 446588,\n            \"rows\": 8522241,\n            \"detail\": {...}\n        },\n        \"reduce\": {\n            \"sql\": \"select [98307] * from 702a9832b17f0000 order by tradedate asc,securityID asc,minute asc\",\n            \"explain\": {\n                \"sort\": {\n                    \"cost\": 296783\n                },\n                \"rows\": 8522241,\n                \"cost\": 746374\n            }\n        },\n        \"rows\": 8522241,\n        \"cost\": 6691112\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Using Function Views with Dependencies in DolphinDB Scheduled Jobs\nDESCRIPTION: This example shows how a scheduled job using a function view retains the original implementation even when dependencies are updated and the function view is regenerated.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/scheduledJob.md#_snippet_9\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef foo(){\n\tprint \"The old function is called \" \n}\ndef fv(){\n\tfoo()\n}\naddFunctionView(fv)  \n\nscheduleJob(`testFvJob, \"fv\", fv, 11:36m, today(), today(), 'D');\ngo\ndef foo(){\n\tprint \"The new function is called \" \n}\ndropFunctionView(`fv)\naddFunctionView(fv)\n```\n\n----------------------------------------\n\nTITLE: Plotting Original Data using DolphinDB `plot` Function\nDESCRIPTION: Uses the built-in `plot` function in DolphinDB to generate a line chart visualizing the first 10,000 points of the original simulated sine wave data (Y vs X). Requires the data vectors X and Y to be defined.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/PIP_in_DolphinDB.md#_snippet_5\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nplot(Y[0:10000], X[0:10000])\n```\n\n----------------------------------------\n\nTITLE: Querying - Count DolphinDB\nDESCRIPTION: This snippet performs a simple count query on the `readings` table to determine the total number of records. The `timer` function measures the time it takes to execute the query, which is useful for performance benchmarking.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs TimescaleDB/test_dolphindb_small.txt#_snippet_10\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// --------------------- 查询性能测试\n\n// 1. 查询总记录数\ntimer select count(*) from readings\n```\n\n----------------------------------------\n\nTITLE: MySQL Server Configuration for CDC\nDESCRIPTION: Configuration parameters for MySQL server to enable binary logging and other settings required for Change Data Capture. These settings enable row-level replication and GTID consistency.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Debezium_and_Kafka_data_sync.md#_snippet_1\n\nLANGUAGE: INI\nCODE:\n```\n[mysqld]\nserver-id = 1\n\nlog_bin=mysql-bin\nbinlog_format=ROW\nbinlog_row_image=FULL\nbinlog_row_value_options=\"\"\n\ngtid_mode=ON\nenforce_gtid_consistency=ON\n\nexpire_logs_days=3\n```\n\n----------------------------------------\n\nTITLE: Verifying Data Import into DolphinDB\nDESCRIPTION: This snippet verifies the data import by selecting the top 10 rows from the partitioned table 'pt'. It confirms the successful import of data from Redshift into DolphinDB and displays a small sample of the data to verify the schema and contents are correct.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/Migrate_data_from_Redshift_to_DolphinDB/Redshift2DDB.txt#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//验证查询的结果\nselect top 10 * from pt\n```\n\n----------------------------------------\n\nTITLE: Generating Multiple Field Expression with sqlCol\nDESCRIPTION: This snippet uses the `sqlCol` function with an array of column names.  It creates an SQL expression that represents multiple columns, enclosed in square brackets. This approach allows the function to dynamically generate expressions for multiple columns at once.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/macro_var_based_metaprogramming.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// sqlCol([\"col0\",\"col1\",\"col2\"]) --> [<col0>, <col1>, …, <colN>]\n```\n\n----------------------------------------\n\nTITLE: Sharding a MongoDB Collection\nDESCRIPTION: Executes the 'sh.shardCollection' command in the MongoDB shell to shard the 'device_readings' collection within the 'device_pt' database. The shard key is a compound key consisting of the 'time' and 'device_id' fields, both in ascending order.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs MongoDB/mongodb_taq_partitioned.txt#_snippet_3\n\nLANGUAGE: mongodb\nCODE:\n```\nsh.shardCollection(\"device_pt.device_readings\",{\"time\":1,\"device_id\":1})\n```\n\n----------------------------------------\n\nTITLE: Generating Sample Data for Continuous Interval Calculation\nDESCRIPTION: This code snippet generates sample data for demonstrating calculations within continuous intervals. It creates a table with date and value columns.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_sql_cases.md#_snippet_19\n\nLANGUAGE: DolphinDB\nCODE:\n```\nt = table(2021.09.29 + 0..15 as date, \n          0 0 0.3 0.3 0 0.5 0.3 0.5 0 0 0.3 0 0.4 0.6 0.6 0 as value)\ntargetVal = 0.3\n```\n\n----------------------------------------\n\nTITLE: Defining a Local Custom Function\nDESCRIPTION: Defines a simple custom function named myfunc that returns the integer 1. This example serves as a baseline to demonstrate how name conflicts arise when a module containing a function with the same name is introduced.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/module_tutorial.md#_snippet_12\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nlogin(\"admin\",\"123456\")\ndef myfunc(){\n return 1\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Sample Data for VWAP Calculation\nDESCRIPTION: This code snippet generates sample data for demonstrating the calculation of moving weighted average (mwavg). It creates a table with symbol, price, and volume columns.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_sql_cases.md#_snippet_11\n\nLANGUAGE: DolphinDB\nCODE:\n```\nsyms = format(1..3000, \"SH000000\")\nN = 10000\nt = cj(table(syms as symbol), table(rand(100.0, N) as price, rand(10000, N) as volume))\n```\n\n----------------------------------------\n\nTITLE: Installing DolphinDB AI DataLoader Python Package\nDESCRIPTION: Provides the command to install the required Python package 'dolphindb-tools' that includes the DDBDataLoader class and related utilities. This package installation is prerequisite to use the AI DataLoader functionality in Python with DolphinDB.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_AI_DataLoader_for_Deep_Learning.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install dolphindb-tools\n```\n\n----------------------------------------\n\nTITLE: Query Publishing Queue Information in DolphinDB Stream Computing\nDESCRIPTION: Fetches data about publishing connections in DolphinDB stream processing, allowing users to monitor active publishers and their connection statuses. Useful for diagnosing or managing data streams. Requires DolphinDB environment.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/streaming_capital_flow_daily/04.streamStateQuery.txt#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\ngetStreamingStat().pubConns\n```\n\n----------------------------------------\n\nTITLE: ffill function\nDESCRIPTION: The code demonstrates how to use the ffill function to fill the null value. First, it retrieves the information from sensors data within a particular time range and updates the `value` column to `NULL` based on the specified datetime conditions, which results in a time period with missing values. Then, it fills the null values with the ffill method, which fills missing values with the previous valid value.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/iot_examples.md#_snippet_16\n\nLANGUAGE: DolphinDB\nCODE:\n```\nt = select * from sensors where id=1, datetime between 2020.09.01T00:00:00 : 2020.09.01T00:59:59 \nupdate t set value=NULL where datetime between 2020.09.01T00:08:00 : 2020.09.01T00:09:00\nupdate t set value=value.ffill()\n```\n\n----------------------------------------\n\nTITLE: Querying - Point Query DolphinDB\nDESCRIPTION: This snippet performs a point query to count the number of records for a specific device ID (`'demo000101'`).  The `timer` function is used to measure query execution time, demonstrating a targeted lookup.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs TimescaleDB/test_dolphindb_small.txt#_snippet_11\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 2. 点查询：按设备 ID 查询记录数\ntimer\nselect count(*)\nfrom readings\nwhere device_id = 'demo000101'\n```\n\n----------------------------------------\n\nTITLE: Retrieving Row and Column Labels from a Matrix in DolphinDB Script\nDESCRIPTION: Shows how to use the `rowNames()` and `colNames()` methods to retrieve the row labels (timestamps) and column labels (symbols), respectively, from the `volume` matrix created by the `panel` function in the previous step.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/panel_data.md#_snippet_12\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nvolume.rowNames();\nvolume.colNames();\n```\n\n----------------------------------------\n\nTITLE: Inserting Data into Dimension Table - Java\nDESCRIPTION: This code snippet inserts data into a pre-created dimension table in DolphinDB using the `tableInsert` method. It uploads the table data to the DolphinDB dimension table loadTable ('dfs://testDB','test').\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Array_Vector.md#_snippet_24\n\nLANGUAGE: java\nCODE:\n```\nList<Entity> tbArg = new ArrayList<>(1);\ntbArg.add(tb);\nconn.run(\"tableInsert{loadTable('dfs://testDB','test')}\", tbArg);\n```\n\n----------------------------------------\n\nTITLE: Gracefully Stop DolphinDB Single Node - Shell\nDESCRIPTION: Shell command to stop all DolphinDB nodes by executing the provided script `stopAllNode.sh` located in the cluster demo directory. This command ensures clean shutdown before maintenance or upgrades.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ARM_standalone_deploy.md#_snippet_4\n\nLANGUAGE: Shell\nCODE:\n```\n./stopAllNode.sh\n```\n\n----------------------------------------\n\nTITLE: SQL Query with Pivot and Repartition for DDBDataLoader\nDESCRIPTION: Examples showing how DDBDataLoader handles complex pivot operations with repartitioning to manage memory usage efficiently.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ai_dataloader_ml.md#_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\nselect val from loadTable(dbName, tableName) pivot by datetime, stockID, factorName where date(datetime) = 2023.09.05\nselect val from loadTable(dbName, tableName) pivot by datetime, stockID, factorName where date(datetime) = 2023.09.06\nselect val from loadTable(dbName, tableName) pivot by datetime, stockID, factorName where date(datetime) = 2023.09.07\n```\n\n----------------------------------------\n\nTITLE: Remove rows with NULL values (row-wise - method 2)\nDESCRIPTION: This DolphinDB script removes rows containing NULL values from a table by checking each row using `isValid`. It applies a lambda function and the `each` function to iterate through the table rows and filters out rows where any value in the row is NULL. Less performant for large number of columns.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\nt[each(x -> all(isValid(x.values())), t)]\n```\n\n----------------------------------------\n\nTITLE: Calculating Stock Portfolio Value (Inefficient)\nDESCRIPTION: This snippet shows an inefficient way to calculate the stock portfolio value. It iterates through the `ETF` table, populating `colAAPL` and `colFB` arrays with the weighted prices of AAPL and FB respectively, and filling the other with NULL. Then it forward-fills NULL values, and calculates the row sum.  This method is slow due to the explicit loop.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_sql_cases.md#_snippet_27\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntimer {\n\tcolAAPL = array(DOUBLE, ETF.Time.size())\n\tcolFB = array(DOUBLE, ETF.Time.size())\n\t\n\tfor(i in 0:ETF.Time.size()) {\n\t\tif(ETF.Symbol[i] == `AAPL) {\n\t\t\tcolAAPL[i] = ETF.weightedPrice[i]\n\t\t\tcolFB[i] = NULL\n\t\t}\n\t\tif(ETF.Symbol[i] == `FB) {\n\t\t\tcolAAPL[i] = NULL\n\t\t\tcolFB[i] = ETF.weightedPrice[i]\n\t\t}\n\t}\n\t\n\tETF_TMP1 = table(ETF.Time, ETF.Symbol, colAAPL, colFB)\n\tETF_TMP2 = select last(colAAPL) as colAAPL, last(colFB) as colFB from ETF_TMP1 group by time, Symbol\n\tETF_TMP3 = ETF_TMP2.ffill()\n\t\n\tt1 = select Time, rowSum(colAAPL, colFB) as rowSum from ETF_TMP3\n}\n```\n\n----------------------------------------\n\nTITLE: Handling Input Events in a Node-RED Node - Python Syntax\nDESCRIPTION: This snippet sets up an input event handler for the custom Node-RED node, allowing it to process incoming messages from the Node-RED flow. When a message is received, custom logic is executed for the node's purpose. Requires the Node-RED node context and should be used within a Node-RED node constructor function.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/node_red_tutorial_iot.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nthis.on('input', function(msg, send, done) {\n    //执行功能\n});\n```\n\n----------------------------------------\n\nTITLE: Testing Stream Engine with Limited, Contextually Sorted Data in DolphinDB\nDESCRIPTION: This snippet tests the stream engine with a single data point selected from `t` after 09:30:00.000. The data is selected using the context by SecurityID csort DateTime clause, ensuring data is sorted and the limit restricts the data set to one row. It then uses the timer function to measure the time taken for appending the `testData` to the stream engine. The dependency is the stream engine `calChange` and requires that `t` be populated with data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/Real-Time_Stock_Price_Increase_Calculation/05.性能测试.txt#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// test once 1650\ntestData = select * from t where time(DateTime) > 09:30:00.000 context by SecurityID csort DateTime limit 1\ntimer(10){getStreamEngine(\"calChange\").append!(testData)}\n```\n\n----------------------------------------\n\nTITLE: Aggregating Average Battery Temperature per Device in MongoDB - JavaScript\nDESCRIPTION: This snippet computes the average battery temperature for each device using MongoDB's aggregation framework. It groups documents by 'device_id' and averages 'battery_temperature'. Dependencies are fields: 'device_id' and 'battery_temperature'. Output provides the average per device. Ensure proper indexing for performance.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs MongoDB/mongodb_readings_query.txt#_snippet_7\n\nLANGUAGE: JavaScript\nCODE:\n```\ndb.device_readings.explain(\"executionStats\").aggregate([{$group:{_id:{device_id:\"$device_id\",},avg_temperature:{$avg:\"$battery_temperature\"}}}])\n```\n\n----------------------------------------\n\nTITLE: Using Volatility Indicators in DolphinDB\nDESCRIPTION: Volatility indicator functions in the DolphinDB ta module. These functions measure the rate and magnitude of price changes to evaluate market volatility, which helps in risk assessment and identifying potential breakouts.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ta.md#_snippet_9\n\nLANGUAGE: DolphinDB\nCODE:\n```\natr(high, low, close, timePeriod)\nnatr(high, low, close, timePeriod)\ntrange(high, low, close)\n```\n\n----------------------------------------\n\nTITLE: Granting Query Result Memory Limit - DolphinDB\nDESCRIPTION: This snippet demonstrates how to limit the query result memory for a specific user in DolphinDB.  It uses the `grant` function with `QUERY_RESULT_MEM_LIMIT` to set the limit for the 'AlexSmith' user to 4GB.  The `grant` function only affects the specified user.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Permission_Management.md#_snippet_42\n\nLANGUAGE: DolphinDB\nCODE:\n```\ngrant(\"AlexSmith\", QUERY_RESULT_MEM_LIMIT, 4)\n```\n\n----------------------------------------\n\nTITLE: 时间戳条件下的点查性能测试\nDESCRIPTION: 在特定时间点筛选数据，评估合并后在时间粒度查询中的性能变化，为时序数据优化提供参考。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/tsdb_explained.md#_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\nselect * from loadTable(dbName, tbName_all) where machineId=999 and datetime=2023.07.10\n```\n\n----------------------------------------\n\nTITLE: Retrieving Recent Jobs in DolphinDB\nDESCRIPTION: This snippet retrieves and displays the list of recently submitted jobs in DolphinDB. It helps monitor ongoing or completed data replay jobs, facilitating job management and troubleshooting. It relies on the getRecentJobs function.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/streaming_capital_flow_daily/03.replay.txt#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\ngetRecentJobs()\n```\n\n----------------------------------------\n\nTITLE: Adding Cluster Replication Configuration - Master\nDESCRIPTION: This snippet adds the configuration item `clusterReplicationSlaveNum=1` to the controller.cfg file in the master cluster. This configuration parameter specifies the maximum number of slave clusters allowed.  The shell command `vim` is used to edit the configuration file.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Asynchronous_Replication.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nclusterReplicationSlaveNum=1\n```\n\n----------------------------------------\n\nTITLE: Custom Data Type Conversion with transType\nDESCRIPTION: This code defines the `transType` function, which is used to customize data type conversions within the data loading process. It leverages the `replaceColumn!` function to transform specific columns in a mutable memory table to the correct data types before importing. It provides examples for converting several data types, like strings, dates, and numeric data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/LoadDataForPoc.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef transType(mutable memTable)\n{\n   return memTable.replaceColumn!(`col0,string(memTable.col0)).replaceColumn!(`col1,datetimeParse(string(memTable.col1),\"yyyyMMddHHmmssSSS\")).replaceColumn!(`col5,string(memTable.col5)).replaceColumn!(`col6,string(memTable.col6))\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Chunk Size and Enabling Sharding in MongoDB\nDESCRIPTION: Connects to the 'config' database to check chunk information and set the chunk size to 1024MB. Then connects to the 'admin' database to enable sharding for the 'taq_pt_db' database.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs MongoDB/mongodb_taq_partitioned.txt#_snippet_0\n\nLANGUAGE: mongodb\nCODE:\n```\nuse config\ndb.chunks.find()\ndb.settings.save({\"_id\":\"chunksize\",\"value\":1024}) \nuse admin\nsh.enableSharding(\"taq_pt_db\")\n```\n\n----------------------------------------\n\nTITLE: Retrieving recent DolphinDB job statuses in DolphinDB\nDESCRIPTION: This snippet retrieves and lists the statuses of recent jobs managed within DolphinDB. This helps users monitor the progress and completion of submitted jobs like data replays or other scheduled tasks.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/Real-Time_Stock_Price_Increase_Calculation/04.历史数据回放.txt#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\ngetRecentJobs()\n```\n\n----------------------------------------\n\nTITLE: Creating a scheduled job with plugin dependency in DolphinDB\nDESCRIPTION: Example showing a scheduled job that uses a plugin function (ODBC) that must be pre-loaded before the job can be deserialized when the system restarts.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/scheduledJob.md#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\nuse odbc\ndef jobDemo(){\n\tconn = odbc::connect(\"dsn=mysql_factorDBURL\");\n}\nscheduleJob(\"job demo\",\"example of init\",jobDemo,15:48m, 2019.01.01, 2020.12.31, 'D')\n```\n\n----------------------------------------\n\nTITLE: Verifying Equality of Results\nDESCRIPTION: This code snippet validates that the results from the optimized and unoptimized queries are equivalent. It uses the `each` function with `eqObj` to compare the values returned by the two queries.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_sql_cases.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\neach(eqObj, t1.values(), t2.values()) // true\n```\n\n----------------------------------------\n\nTITLE: Checking Initial TSDB Index Cache Usage (DolphinDB Script)\nDESCRIPTION: Retrieves the current usage of the TSDB level file index cache using `getLevelFileIndexCacheStatus()` function, typically executed after a server restart to observe the initial state (expected to be zero).\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/memory_management.md#_snippet_13\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\ngetLevelFileIndexCacheStatus().usage\n//输出结果为：0\n```\n\n----------------------------------------\n\nTITLE: Examples of Queries that Prevent Partition Pruning in DolphinDB\nDESCRIPTION: Four examples of SQL queries that prevent effective partition pruning, leading to full table scans and reduced performance.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_SQL_Case_Tutorial.md#_snippet_9\n\nLANGUAGE: SQL\nCODE:\n```\nselect count(*) from snapshot where date(DateTime) + 1 > 2020.06.01\n\nselect count(*) from snapshot where 2020.06.01 < date(DateTime) < 2020.06.03\n\nselect count(*) from snapshot where Volume < 500\n\nselect count(*) from snapshot where date(DateTime) < AnnouncementDate - 3\n```\n\n----------------------------------------\n\nTITLE: Splitting Text File into Chunks\nDESCRIPTION: This code uses the `textChunkDS` function to split a text file into smaller chunks of approximately 300MB each. The function returns a data source object that can be used for parallel processing.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_csv.md#_snippet_28\n\nLANGUAGE: DolphinDB\nCODE:\n```\nds=textChunkDS(dataFilePath,300)\nds;\n```\n\n----------------------------------------\n\nTITLE: Path Configuration for Data Files in DolphinDB\nDESCRIPTION: Defines file paths for raw data, sample CSV files, and database storage locations, establishing the environment for data processing.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs InfluxDB/test_dolphindb_big.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// ----------------- 路径配置\nFP_TAQ = '/data/TAQ/'\nFP_SAMPLE_TB = FP_TAQ + 'csv/TAQ20070801.csv'\nFP_DB = FP_TAQ + 'db/'\n```\n\n----------------------------------------\n\nTITLE: Unoptimized Date Filtering with temporalFormat in DolphinDB SQL\nDESCRIPTION: A non-optimized query using temporalFormat function on a partition column, which prevents partition pruning and results in poor performance.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_SQL_Case_Tutorial.md#_snippet_7\n\nLANGUAGE: SQL\nCODE:\n```\ntimer t1 = select count(*) from snapshot \n\t\t   where temporalFormat(DateTime, \"yyyy.MM.dd\") >= \"2020.06.01\" and temporalFormat(DateTime, \"yyyy.MM.dd\") <= \"2020.06.02\" \n\t\t   group by SecurityID\n```\n\n----------------------------------------\n\nTITLE: 数据处理和对齐\nDESCRIPTION: 从两个表中取出数据，使用aj(as-of join)函数按日期对齐基金净值与沪深300指数数据，填充空缺值并计算基金的日收益率。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/fund_factor_contrasted_by_py.md#_snippet_24\n\nLANGUAGE: dolphindb\nCODE:\n```\nfund_OLAP=select * from loadTable(\"dfs://fund_OLAP\", \"fund_OLAP\")\nfund_hs_OLAP=select * from loadTable(\"dfs://fund_OLAP\", \"fund_hs_OLAP\")\najResult=select tradingDate, fundNum, value, fund_hs_OLAP.tradingDate as hstradingDate, fund_hs_OLAP.value as price from aj(fund_OLAP, fund_hs_OLAP, `tradingDate)\nresult2=select tradingDate, fundNum, iif(isNull(value), ffill!(value), value) as value, price from ajResult where tradingDate == hstradingDate\n```\n\n----------------------------------------\n\nTITLE: Synchronizing Data from OceanBase to DolphinDB\nDESCRIPTION: Command to synchronize full data from an OceanBase table to a DolphinDB table using the MySQL plugin. This loads data while respecting the partitioning scheme defined by TradeTime and SecurityID columns.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/OceanBase_to_DolphinDB.md#_snippet_2\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nmysql::loadEx(conn, database('dfs://TSDB_tick'), `tick, `TradeTime`SecurityID,\"tick\")\n```\n\n----------------------------------------\n\nTITLE: Calculating Realized Volatility\nDESCRIPTION: Defines a function `realizedVolatility` that calculates the realized volatility of a series `s` (typically log returns). It computes the square root of the sum of the squared values in the series.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/metacode_derived_features/metacode_derived_features.txt#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef realizedVolatility(s){\n\treturn sqrt(sum2(s))\n}\n```\n\n----------------------------------------\n\nTITLE: Loading Data with Double Quotes\nDESCRIPTION: This code loads data from a CSV file where numerical values are enclosed in double quotes. DolphinDB automatically removes these double quotes during the loading process, treating the values as regular numerical data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_csv.md#_snippet_41\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntmpTB=loadText(dataFilePath)\ntmpTB;\n```\n\n----------------------------------------\n\nTITLE: Query Streaming Publish Queue Details in DolphinDB\nDESCRIPTION: Retrieves information about the publish queue for streaming data, using getStreamingStat().pubConns. This helps in monitoring data propagation and identifying potential bottlenecks or failures in publishing streams.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/machine_learning_volatility/07.streamStateQuery.txt#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\ngetStreamingStat().pubConns\n```\n\n----------------------------------------\n\nTITLE: Installing unixODBC Library on Centos\nDESCRIPTION: This command installs the unixODBC library and its development package on a Centos system. unixODBC is required before installing PostgreSQL's ODBC driver for data migration to DolphinDB.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/migrate_data_from_Postgre_and_Greenplum_to_DolphinDB.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# 安装 unixODBC 库\nyum install unixODBC unixODBC-devel\n```\n\n----------------------------------------\n\nTITLE: Creating a Warning Table for Anomaly Detection in DolphinDB\nDESCRIPTION: Creates and shares a stream table to store anomaly detection results. The table includes columns for the time of detection, deviceID, type of anomaly, and a description string.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/alarm.txt#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\nshare streamTable(1000:0, `time`deviceID`anomalyType`anomalyString, [DATETIME,INT,INT, SYMBOL]) as warningTable\n```\n\n----------------------------------------\n\nTITLE: Splitting Text File into Smaller Chunks (10MB)\nDESCRIPTION: This code splits a text file into smaller chunks, each approximately 10MB in size, using the `textChunkDS` function. It creates a data source for reading the file in segments.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_csv.md#_snippet_33\n\nLANGUAGE: DolphinDB\nCODE:\n```\nds=textChunkDS(dataFilePath, 10);\n```\n\n----------------------------------------\n\nTITLE: Creating Scheduled Jobs in postStart Script for DolphinDB\nDESCRIPTION: This example shows how to conditionally create a scheduled job in the postStart script to ensure it exists after system startup.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/scheduledJob.md#_snippet_17\n\nLANGUAGE: DolphinDB\nCODE:\n```\nif(getScheduledJobs().jobDesc.find(\"daily resub\") == -1){\n\tscheduleJob(jobId=`daily, jobDesc=\"daily resub\", jobFunc=run{\"/home/appadmin/server/resubJob.dos\"}, scheduleTime=08:30m, startDate=2021.08.30, endDate=2023.12.01, frequency='D')\n}\n```\n\n----------------------------------------\n\nTITLE: Comparing Query Performance Using Non-Partition vs. Partition Key in DolphinDB\nDESCRIPTION: This DolphinDB script sets up a partitioned table 'pt' with similar date columns, 'datea' (non-partition key) and 'dateb' (partition key). It then runs a query filtering by the non-partition key 'datea' using HINT_EXPLAIN to demonstrate the lack of optimization and partition pruning.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_Explain.md#_snippet_11\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nif( existsDatabase(\"dfs://valuedb2\") ) dropDatabase(\"dfs://valuedb2\");\nn=1000000\ndatea=take(2000.01.01..2000.01.02, n)\ndateb=take(2000.01.01..2000.01.02, n)\nx=rand(1.0, n)\nt=table(datea,dateb, x)\ndb=database(\"dfs://valuedb2\", VALUE, 2000.01.01..2000.01.02)\npt = db.createPartitionedTable(t, `pt, `dateb)\npt.append!(t)\n\nselect [HINT_EXPLAIN] * from pt where datea = 2000.01.01;\n```\n\n----------------------------------------\n\nTITLE: Verifying Plugin Loading\nDESCRIPTION: Command to check the Kafka Connect logs to verify that the JdbcSinkConnector plugin has been loaded successfully.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Debezium_and_Kafka_data_sync.md#_snippet_16\n\nLANGUAGE: Bash\nCODE:\n```\ncat /KFDATA/kafka-connect/logs/connect.log|grep JdbcSinkConnector\n```\n\n----------------------------------------\n\nTITLE: Using Stream Table with No Snapshot for Data Continuity in DolphinDB\nDESCRIPTION: This code demonstrates that without enabling snapshots, continuing subscription after a disconnection results in discontinuities in the output data. It shows creating the stream table, subscribing at offset 0, inserting data, then unsubscribing and recreating the engine without snapshot parameters, re-subscribing at a specific offset, and observing that the results are inconsistent or incomplete compared to interrupted but resumed processes.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/stream_aggregator.md#_snippet_8\n\nLANGUAGE: DolphinDB\nCODE:\n```\nshare streamTable(10000:0,`time`sym`price`id, [TIMESTAMP,SYMBOL,INT,INT]) as trades\noutput1 =table(10000:0, `time`sumprice, [TIMESTAMP,INT])\nAgg1 = createTimeSeriesEngine(name=`Agg1, windowSize=100, step=50, metrics=<sum(price)>, dummyTable=trades, outputTable=output1, timeColumn=`time)\nsubscribeTable(server=\"\", tableName=\"trades\", actionName=\"Agg1\",offset= 0, handler=append!{Agg1}, msgAsTable=true)\n\nn=500\ntimev=timestamp(1..n) + 2021.03.12T15:00:00.000\nsymv = take(`abc`def, n)\npricev = int(1..n)\nid = take(-1, n)\ninsert into trades values(timev, symv, pricev, id)\n\nunsubscribeTable(, \"trades\", \"Agg1\")\ndropStreamEngine(\"Agg1\")\nAgg1=NULL\n\nn=500\ntimev=timestamp(501..1000) + 2021.03.12T15:00:00.000\nsymv = take(`abc`def, n)\npricev = int(1..n)\nid = take(-1, n)\ninsert into trades values(timev, symv, pricev, id)\n\nAgg1 = createTimeSeriesEngine(name=`Agg1, windowSize=100, step=50, metrics=<sum(price)>, dummyTable=trades, outputTable=output1, timeColumn=`time)\nsubscribeTable(server=\"\", tableName=\"trades\", actionName=\"Agg1\",offset= 500, handler=append!{Agg1}, msgAsTable=true)\n\nselect * from output1\n```\n\n----------------------------------------\n\nTITLE: Listing Partitions in a DolphinDB Table Backup\nDESCRIPTION: Shows how to use `getBackupList` to retrieve metadata about the partitions included in a specific table backup. It requires the backup directory (`backupDir`), the original database path (`dbPath`), and the table name (`quotes_2`). The function returns a table listing partition details like chunk ID, path, version, row count, and last update timestamp.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/backup-restore-new.md#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndbPath=\"dfs://testdb\"\nbackupDir=\"/home/$USER/backupDB\"\ngetBackupList(backupDir,dbPath,`quotes_2)\n```\n\n----------------------------------------\n\nTITLE: Performance Testing: Point Query on Symbol and Date\nDESCRIPTION: Performs a single record lookup for symbol 'IBM' on a specific date, measuring query duration.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs InfluxDB/test_dolphindb_big.txt#_snippet_9\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// 1. 点查询：按股票代码、时间查询\n\ntimer\nselect *\nfrom taq\nwhere\n\tsymbol = 'IBM', \n\tdate == 2007.08.07\n```\n\n----------------------------------------\n\nTITLE: Load Table Metadata into Memory\nDESCRIPTION: This snippet loads the metadata of a distributed table into memory using `loadTable`. This is necessary before querying the distributed table. The actual data is loaded on demand during query execution.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_data.md#_snippet_7\n\nLANGUAGE: txt\nCODE:\n```\ntb = database(\"dfs://dataImportCSVDB\").loadTable(\"cycle\")\n```\n\n----------------------------------------\n\nTITLE: Query DailyTimeSeriesEngine Type Status in DolphinDB\nDESCRIPTION: Obtains status details for the DailyTimeSeriesEngine, enabling monitoring of this specific engine type’s activity within DolphinDB's streaming framework. Requires DolphinDB environment.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/streaming_capital_flow_daily/04.streamStateQuery.txt#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\ngetStreamEngineStat().DailyTimeSeriesEngine\n```\n\n----------------------------------------\n\nTITLE: Connecting to ClickHouse via ODBC in DolphinDB\nDESCRIPTION: This snippet shows how to establish a connection to ClickHouse using the ODBC plugin in DolphinDB.  The Dsn name should match the DSN configured in the odbc.ini file.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ClickHouse_to_DolphinDB.md#_snippet_4\n\nLANGUAGE: dolphindb\nCODE:\n```\nconn = odbc::connect(\"Dsn=ClickHouseAnsi\", `ClickHouse)\n```\n\n----------------------------------------\n\nTITLE: Authenticating User (DolphinDB Script)\nDESCRIPTION: Logs in to the DolphinDB server using the specified username and password. This is required for operations that need administrative or specific user privileges.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/metacode_derived_features/metacode_derived_features_streaming.txt#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(\"admin\", \"123456\")\n```\n\n----------------------------------------\n\nTITLE: Using fixedLengthArrayVector function with a Field Series\nDESCRIPTION: This snippet demonstrates the usage of `fixedLengthArrayVector` function along with a field series (`ask1...ask10`) to combine multiple columns into an array vector.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/macro_var_based_metaprogramming.md#_snippet_19\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect fixedLengthArrayVector(ask1...ask10) as askArray from t\n```\n\n----------------------------------------\n\nTITLE: Getting Aggregator Statistics\nDESCRIPTION: These lines retrieve and display statistics for the reactive stream engine and session window engine.  These statistics can be used to monitor the performance and behavior of the engines.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/data/DolphinDB_streaming_application_in_IOT/deviceState.txt#_snippet_11\n\nLANGUAGE: DolphinDB\nCODE:\n```\ngetAggregatorStat().ReactiveStreamEngine\ngetAggregatorStat().SessionWindowEngine\n```\n\n----------------------------------------\n\nTITLE: Finding the index of the maximum value in each row using byRow\nDESCRIPTION: This code snippet finds the index of the maximum value in each row of the matrix 'm' using the 'byRow' higher-order function. It avoids the need to transpose the matrix.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_35\n\nLANGUAGE: shell\nCODE:\n```\nbyRow(imax, m)\n```\n\n----------------------------------------\n\nTITLE: Obtaining SQL Execution Plan\nDESCRIPTION: This snippet demonstrates how to retrieve the execution plan for a SQL query in DolphinDB. It requires adding the `[HINT_EXPLAIN]` keyword immediately after `select` or `exec` to enable the execution plan output.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_Explain.md#_snippet_0\n\nLANGUAGE: DolphinDB SQL\nCODE:\n```\nselect [HINT_EXPLAIN] * from pt;\n```\n\n----------------------------------------\n\nTITLE: DolphinDB Grant DB_OWNER permissions with prefixes\nDESCRIPTION: From versions 1.30.21 and 2.00.9 onwards, when assigning DB_OWNER permissions, the `objs` parameter in `grant`, `deny`, or `revoke` commands must end with \"*\" if applied to databases. This indicates a prefix range for the specified dbName.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Permission_Management.md#_snippet_14\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(\"admin\", \"123456\")\ncreateUser(\"user1\",\"123456\")\ngrant(\"user1\", DB_OWNER,\"*\")//用户拥有所有数据库的 DB_OWNER 权限\ngrant(\"user1\", DB_OWNER,\"dfs://test0*\")//用户拥有以 \"dfs://test0\" 为前缀的数据库 DB_OWNER 权限\n```\n\n----------------------------------------\n\nTITLE: Configuring Controller.cfg for Slave Cluster\nDESCRIPTION: This snippet modifies the controller.cfg file in the slave cluster to specify the master cluster's control node. The configuration involves accessing the server and modifying the DolphinDB installation files.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Asynchronous_Replication.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nvim ./controller.cfg\n```\n\n----------------------------------------\n\nTITLE: Query ReactiveStreamEngine Type Status in DolphinDB\nDESCRIPTION: Fetches status information specifically related to the ReactiveStreamEngine component, helping to monitor this engine type's operational state within DolphinDB stream processing. Requires DolphinDB environment.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/streaming_capital_flow_daily/04.streamStateQuery.txt#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\ngetStreamEngineStat().ReactiveStreamEngine\n```\n\n----------------------------------------\n\nTITLE: Using Volume Indicators in DolphinDB\nDESCRIPTION: Volume indicator functions in the DolphinDB ta module. These functions analyze trading volume data alongside price movements to identify market strength and potential trend reversals.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ta.md#_snippet_8\n\nLANGUAGE: DolphinDB\nCODE:\n```\nad(high, low, close, volume)\nobv(close, volume)\n```\n\n----------------------------------------\n\nTITLE: Creating Partitioned In-Memory Table\nDESCRIPTION: Demonstrates creating a partitioned in-memory table using `createPartitionedTable`. It first creates a non-partitioned table, then a partitioned database in memory, and finally appends the data to the partitioned table. SQL is needed to access the partitioned table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/partitioned_in_memory_table.md#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\nn=10000\nt=table(rand(1..10,n) as id,rand(10.0,n) as val)\ndb=database(\"\",VALUE,1..10)\npt=db.createPartitionedTable(t,`pt,`id).append!(t)\ntypestr(pt)\n\nSEGMENTED IN-MEMORY TABLE\n```\n\n----------------------------------------\n\nTITLE: Checking TSDB Cache Engine Size in DolphinDB\nDESCRIPTION: The getTSDBCacheEngineSize function returns the maximum memory limit configured for the TSDB cache engine, helping administrators verify the current configuration.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/redoLog_cacheEngine.md#_snippet_7\n\nLANGUAGE: SQL\nCODE:\n```\ngetTSDBCacheEngineSize()\n```\n\n----------------------------------------\n\nTITLE: Simulating Sine Wave Data in DolphinDB Script\nDESCRIPTION: Generates 10 million data points representing a sine wave in DolphinDB. It creates a vector `X` representing the sequence axis (0 to 9,999,999 scaled by pi/1000) and a vector `Y` containing the corresponding sine values (`sin(X)`).\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/PIP_in_DolphinDB.md#_snippet_3\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\n// 模拟 1 千万条正弦数据，大小为 153 MB\nX = (double(0..9999999)*pi/1000)\nY = sin(X)\n```\n\n----------------------------------------\n\nTITLE: Defining Stream Table Handler for Redis Batch Set\nDESCRIPTION: This snippet defines a user-defined function `myHandle` that will serve as a subscriber handler for the stream table. The function takes a Redis connection object (`conn`) and a message (`msg`) as input. It uses the `redis::batchSet` function to write the key-value pair from the message (assuming `msg` is a table with at least two columns) to Redis.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/redis_plugin_tutorial/example2.txt#_snippet_2\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\ndef myHandle(conn, msg) {\n\tredis::batchSet(conn, msg[0], msg[1])\n}\n```\n\n----------------------------------------\n\nTITLE: Unregistering Example Snapshot Engine\nDESCRIPTION: Unregisters the snapshot engine previously registered for the example distributed table. This snippet removes the snapshot engine associated with the specified database and table, releasing the associated memory resources.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/snapshot_engine.md#_snippet_4\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nunregisterSnapshotEngine(\"dfs://compoDB\",\"pt\")\n```\n\n----------------------------------------\n\nTITLE: Loading NSQ Plugin with Error Handling (DolphinDB Script)\nDESCRIPTION: Attempts to load the DolphinDB NSQ plugin using `loadPlugin`. It includes a try-catch block to gracefully handle the potential error if the plugin is already loaded, preventing script interruption.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_implementation_for_NSQ_Plugin.md#_snippet_3\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\ntry{loadPlugin(\"/DolphinDB/server/plugins/nsq/PluginNsq.txt\")} catch(ex){print(ex)}\n```\n\n----------------------------------------\n\nTITLE: Execution Plan When Filtering by Non-Partition Key in DolphinDB\nDESCRIPTION: This JSON object shows the execution plan when filtering a partitioned table using a non-partition key ('datea'). The 'map.partitions.remote' value of 2 indicates that both partitions were scanned, even though the data might only exist in one, demonstrating inefficiency compared to filtering by the partition key.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_Explain.md#_snippet_12\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"measurement\": \"microsecond\",\n  \"explain\": {\n    \"from\": {\n      \"cost\": 1\n    },\n    \"map\": {\n      \"partitions\": {\n        \"local\": 0,\n        \"remote\": 2\n      },\n      \"cost\": 10010,\n      \"detail\": {\n        \"most\": {\n          \"sql\": \"select [114699] datea,dateb,x from pt where datea == 2000.01.01 [partition = /valuedb2/20000101/OD]\",\n          \"explain\": {\n            \"where\": {\n              \"rows\": 500000,\n              \"cost\": 3973\n            },\n            \"rows\": 500000,\n            \"cost\": 9783\n          }\n        },\n        \"least\": {\n          \"sql\": \"select [114695] datea,dateb,x from pt where datea == 2000.01.01 [partition = /valuedb2/20000102/OD]\",\n          \"explain\": {\n            \"from\": {\n              \"cost\": 8\n            },\n            \"where\": {\n              \"rows\": 0,\n              \"cost\": 2392\n            },\n            \"rows\": 0,\n            \"cost\": 2516\n          }\n        }\n      }\n    },\n    \"merge\": {\n      \"cost\": 2079,\n      \"rows\": 500000,\n      \"detail\": {\n        \"most\": {\n          \"sql\": \"select [114699] datea,dateb,x from pt where datea == 2000.01.01 [partition = /valuedb2/20000101/OD]\",\n          \"explain\": {\n            \"where\": {\n              \"rows\": 500000,\n              \"cost\": 3973\n            },\n            \"rows\": 500000,\n            \"cost\": 9783\n          }\n        },\n        \"least\": {\n          \"sql\": \"select [114695] datea,dateb,x from pt where datea == 2000.01.01 [partition = /valuedb2/20000102/OD]\",\n          \"explain\": {\n            \"from\": {\n              \"cost\": 8\n            },\n            \"where\": {\n              \"rows\": 0,\n              \"cost\": 2392\n            },\n            \"rows\": 0,\n            \"cost\": 2516\n          }\n        }\n      }\n    },\n    \"rows\": 500000,\n    \"cost\": 13019\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Shutting Down the Cluster\nDESCRIPTION: This script stops all nodes in the cluster. It's a necessary first step before upgrading the DolphinDB server.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/single_machine_cluster_deploy.md#_snippet_11\n\nLANGUAGE: Shell\nCODE:\n```\n./stopAllNode.sh\n```\n\n----------------------------------------\n\nTITLE: Validating Calculation Results in DolphinDB\nDESCRIPTION: DolphinDB script to verify that the results from both calculation methods are identical. It loads result tables and performs a comparison, with an empty array indicating complete agreement between results.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Python+HDF5_vs_DolphinDB.md#_snippet_9\n\nLANGUAGE: DolphinDB\nCODE:\n```\nresTb = select * from loadTable(\"dfs://tempResultDB\",\"result_cyc_test\")\nresTb2 = select * from loadTable(\"dfs://tempResultDB\",\"result_cyc_test2\")\nresTb.eq(resTb2).matrix().rowAnd().not().at()\n```\n\n----------------------------------------\n\nTITLE: Setting XGBoost Parameters\nDESCRIPTION: This code defines a dictionary `params` containing the parameters for the XGBoost training process.  It sets the objective function to 'multi:softmax' for multi-class classification, specifies the number of classes, and sets values for `max_depth`, `eta`, and `subsample`. These parameters control the behavior of the XGBoost algorithm and can be tuned to optimize performance.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/machine_learning.md#_snippet_20\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nparams = {\n    objective: \"multi:softmax\",\n    num_class: 3,\n    max_depth: 5,\n    eta: 0.1,\n    subsample: 0.9\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing Table Columns with getColumn C++\nDESCRIPTION: Illustrates how to retrieve individual columns from a DolphinDB Table object (`TableSP`) using the `getColumn` method. Each column is returned as a `ConstantSP` which can then be processed as a Vector using the methods described previously.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_advance.md#_snippet_18\n\nLANGUAGE: C++\nCODE:\n```\nTableSP ddbTbl = input;\nfor (size_t i = 0; i < columnSize; ++i) {\n    ConstantSP col = input->getColumn(i);\n// ...\n}\n```\n\n----------------------------------------\n\nTITLE: Calling Built-in DolphinDB Functions from Plugin\nDESCRIPTION: This snippet demonstrates how to call built-in DolphinDB functions from within a C++ plugin. It shows two ways to invoke functions: first, by calling methods directly on `VectorSP` objects for commonly used functions like `avg` and `sort`, and second, by retrieving a `FunctionDefSP` object for a specific function (e.g., `cumsum`) and invoking it using the `call` method.  The `setTemporary(false)` method is used to prevent the input variable from being modified during the built-in function call.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_development_tutorial.md#_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nVectorSP v = Util::createIndexVector(1, 100);\nConstantSP avg = v->avg();     // 相当于 avg(v)\nConstantSP sum2 = v->sum2();   // 相当于 sum2(v)\nv->sort(false);                // 相当于 sort(v, false)\n```\n\nLANGUAGE: cpp\nCODE:\n```\nConstantSP v = Util::createIndexVector(1, 100);\nv->setTemporary(false);                                   //v 的值可能在内置函数调用时被修改。如果不希望它被修改，应先调用 setTemporary(false)\nFunctionDefSP cumsum = heap->currentSession()->getFunctionDef(\"cumsum\");\nConstantSP result = cumsum->call(heap, v, new Void());\n// 相当于 cumsum(v)，这里的 new Void() 是一个占位符，没有实际用途\n```\n\n----------------------------------------\n\nTITLE: Configuring Redshift ODBC driver on Linux\nDESCRIPTION: Step-by-step commands to download, install, and set up Amazon Redshift ODBC driver, including copying configuration files and editing odbc.ini for connection.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Migrate_data_from_Redshift_to_DolphinDB.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\n# Ubuntu\n\n dpkg -i ./AmazonRedshiftODBC-1.x.x.xxxx-x.x86_64.deb\n\n# CentOS\nyum --nogpgcheck localinstall AmazonRedshiftODBC-64-bit-1.x.xx.xxxx-x.x86_64.rpm\n\n#复制配置文件\ncp /opt/amazon/redshiftodbc/Setup/odbcinst.ini /etc/\ncp /opt/amazon/redshiftodbc/Setup/odbc.ini /etc/\n\n#编辑 odbc.ini 添加连接\n[Redshift]\nDriver=Amazon Redshift (x64); \nServer=Your server; \nDatabase=Your database\n```\n\n----------------------------------------\n\nTITLE: Generating Single Field Expression with sqlCol\nDESCRIPTION: This snippet demonstrates the use of `sqlCol` to represent a single column in a SQL expression. `sqlCol` is used to dynamically specify the column name 'col'.  This generates a simple SQL expression representing the column itself.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/macro_var_based_metaprogramming.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// sqlCol(\"col\") --> <col>\n```\n\n----------------------------------------\n\nTITLE: Configuring Single-Node DolphinDB Server for Resource Tracking\nDESCRIPTION: Shell commands to modify the dolphindb.cfg configuration file to enable resource tracking functionality on a single-node server, including setting sampling interval, maximum log size, and log retention time.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/user_level_resource_tracking.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ncd <DolphinDBInstallDir>/server\nvim ./dolphindb.cfg\n```\n\nLANGUAGE: shell\nCODE:\n```\nresourceSamplingInterval=30\nresourceSamplingMaxLogSize=1024\nresourceSamplingLogRetentionTime=-1\n```\n\n----------------------------------------\n\nTITLE: Summarizing User Resource Usage for Cost Allocation\nDESCRIPTION: Simple SQL query to aggregate memory usage and row count statistics by user, useful for allocating costs among multiple users based on their resource consumption.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/user_level_resource_tracking.md#_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nselect sum(memUsage) as memUsage, sum(rowCount) as rowCount from result group by userId\n```\n\n----------------------------------------\n\nTITLE: Querying Data Count by Trade Date\nDESCRIPTION: Retrieves and aggregates the count of records grouped by trading date from the loaded snapshot data to verify the data loading process.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/Real-Time_Stock_Price_Increase_Calculation/01.创建存储快照数据的库表并导入数据.txt#_snippet_4\n\nLANGUAGE: dolphindb\nCODE:\n```\nselect count(*) from loadTable(dbName, tbName) group by date(DateTime) as TradeDate\n```\n\n----------------------------------------\n\nTITLE: Changing Own Password via DolphinDB Script - DolphinDB\nDESCRIPTION: This code enables a user to change their own password within DolphinDB. The script uses the 'login', 'changePwd', and 'logout' functions with appropriate credentials. Required parameters are the username, current password, and new password. Successful execution results in password change confirmation.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Permission_Management.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(\"user1\",\"123456\")//登陆用户\nchangePwd(\"123456\",\"123456@\")//修改自己的密码\nlogout(\"user1\")\nlogin(\"user1\",\"123456@\")//修改密码成功\n```\n\n----------------------------------------\n\nTITLE: 使用DDBDataLoader进行模型训练\nDESCRIPTION: 展示如何使用配置好的DDBDataLoader实例来训练一个简单的CNN神经网络模型。代码包含模型定义、损失函数和优化器设置，以及基于DDBDataLoader迭代训练数据的训练循环。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ai_dataloader_ml.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmodel = SimpleNet()\nmodel = model.to(\"cuda\")\nloss_fn = nn.MSELoss()\nloss_fn = loss_fn.to(\"cuda\")\noptimizer = torch.optim.Adam(model.parameters(), lr=0.05)\nnum_epochs = 10\n\nmodel.train()\nfor epoch in range(num_epochs):\n    for X, y in dataloader:\n        X = X.to(\"cuda\")\n        y = y.to(\"cuda\")\n        y_pred = model(X)\n        loss = loss_fn(y_pred, y)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n```\n\n----------------------------------------\n\nTITLE: Measuring Execution Time of Parallel Job 2 in DolphinDB\nDESCRIPTION: This code measures the execution time of parallel job 2 for both single and multiple users.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_factor.txt#_snippet_20\n\nLANGUAGE: DolphinDB\nCODE:\n```\n//获取单个用户的运行时间\nselect max(endTime) - min(startTime) from getRecentJobs() where jobDesc = \"parallJob_single_ten\"\n//获取多个用户的运行时间\nselect max(endTime) - min(startTime) from getRecentJobs() where jobDesc = \"parallJob_multi_ten\"\n```\n\n----------------------------------------\n\nTITLE: Instantiating Java Date Class\nDESCRIPTION: Shows how to create a Date object in Java which stores Unix timestamp internally but displays the time in server's timezone when printed.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/timezone.md#_snippet_8\n\nLANGUAGE: Java\nCODE:\n```\nDate date = new Date();\nSystem.out.println(date);\n// Mon May 23 22:22:10 CST 2022\n```\n\n----------------------------------------\n\nTITLE: Backup Controller Metadata\nDESCRIPTION: Backup command for the control node's chunk metadata directory, ensuring data safety prior to upgrade.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/gray_scale_upgrade_ha.md#_snippet_8\n\nLANGUAGE: Shell\nCODE:\n```\ncd /home/zjchen/HA/server/clusterDemo/data/node43/storage/\ncp -r CHUNK_METADATA/ CHUNK_METADATA_BAK/\n```\n\n----------------------------------------\n\nTITLE: 根据标签实现不同聚合计算\nDESCRIPTION: 使用字典将标签映射到不同的聚合函数，结合自定义聚合函数和pivot功能，实现对不同标签数据应用不同的聚合方式并转为宽表格式。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_SQL_Case_Tutorial.md#_snippet_30\n\nLANGUAGE: DolphinDB\nCODE:\n```\ncodes = dict(`code1`code2`code3, [max, min, avg])\n\ndefg func(tag, value, codes) : codes[tag.first()](value)\n \ntimer {\n\tt_tmp = select func(tag, value, codes) as value from t \n\t\t\tgroup by tag, interval(time, 10m, \"null\") as time\n\tt_result = select value from t_tmp pivot by time, tag\n}\n```\n\n----------------------------------------\n\nTITLE: Query TimeSeriesEngine Stream Computing Engine in DolphinDB\nDESCRIPTION: Retrieves status details specifically for the TimeSeriesEngine type using getStreamEngineStat().TimeSeriesEngine, facilitating targeted monitoring of this specialized stream processing engine.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/machine_learning_volatility/07.streamStateQuery.txt#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\ngetStreamEngineStat().TimeSeriesEngine\n```\n\n----------------------------------------\n\nTITLE: Starting DolphinDB Background Linux After Upgrade\nDESCRIPTION: This command starts the DolphinDB server in the background, typically after an upgrade. This is achieved using the startSingle.sh script.  This brings the upgraded DolphinDB instance online.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/standalone_server.md#_snippet_16\n\nLANGUAGE: sh\nCODE:\n```\nsh startSingle.sh\n```\n\n----------------------------------------\n\nTITLE: Authentication in DolphinDB\nDESCRIPTION: Logs into the DolphinDB server with administrator credentials. This is typically the first step in any DolphinDB script that requires access to protected resources.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/alarm.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(\"admin\", \"123456\")\n```\n\n----------------------------------------\n\nTITLE: Running Script Files with External Dependencies in DolphinDB\nDESCRIPTION: This example demonstrates a script file that depends on an external function, which will fail when used in a scheduled job because dependencies aren't included.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/scheduledJob.md#_snippet_13\n\nLANGUAGE: DolphinDB\nCODE:\n```\nfoo()\n```\n\n----------------------------------------\n\nTITLE: Releasing Shared Variable Memory with undef\nDESCRIPTION: This snippet shows how to release shared variable's memory using `undef`. It specifies the `SHARED` parameter, indicating that the shared variable's memory will be freed.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/memory_management.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\nundef(\"sharedTable\", SHARED)\n```\n\n----------------------------------------\n\nTITLE: Creating a matrix\nDESCRIPTION: This code snippet creates a matrix named 'm' using vectors a1, a2, a3, and a4.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_33\n\nLANGUAGE: shell\nCODE:\n```\na1=2 3 4\na2=1 2 3\na3=1 4 5\na4=5 3 2\nm = matrix(a1,a2,a3,a4)\n```\n\n----------------------------------------\n\nTITLE: Generating Sample Data for Tables\nDESCRIPTION: Generates a sample dataset as a table and saves it as a text file.  It creates a table named `trades` with various columns and random data, then saves it to a specified directory.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/partitioned_in_memory_table.md#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\nn=30000000\nworkDir = \"C:/DolphinDB/Data\"\nif(!exists(workDir)) mkdir(workDir)\ntrades=table(rand(`IBM`MSFT`GM`C`FB`GOOG`V`F`XOM`AMZN`TSLA`PG`S,n) as sym, 2000.01.01+rand(365,n) as date, 10.0+rand(2.0,n) as price1, 100.0+rand(20.0,n) as price2, 1000.0+rand(200.0,n) as price3, 10000.0+rand(2000.0,n) as price4, 10000.0+rand(3000.0,n) as price5, 10000.0+rand(4000.0,n) as price6, rand(10,n) as qty1, rand(100,n) as qty2, rand(1000,n) as qty3, rand(10000,n) as qty4, rand(10000,n) as qty5, rand(10000,n) as qty6)\ntrades.saveText(workDir + \"/trades.txt\");\n```\n\n----------------------------------------\n\nTITLE: Creating Non-Partitioned In-Memory Table from Tuple\nDESCRIPTION: Illustrates creating a non-partitioned in-memory table from a tuple using the `table` function. Each element in the tuple corresponds to a column in the table.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/partitioned_in_memory_table.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\nx=([\"a\",\"b\",\"c\"],[4,5,6])\ntable(x)\n\nC0 C1\n-- --\na  4 \nb  5 \nc  6 \n\n```\n\n----------------------------------------\n\nTITLE: Inserting data into DolphinDB table\nDESCRIPTION: Inserts a row of data into the 'config' table with values (1, 245, 12) for frequency, maxvoltage, and maxec respectively.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/cachedTable/mysql_data.txt#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\ninsert into config (frequency,maxvoltage,maxec) values(1,245,12);\n```\n\n----------------------------------------\n\nTITLE: Adding Columns to Memory Table with Assignment\nDESCRIPTION: Demonstrates adding new columns to a memory table using direct assignment. This is a concise way to add columns based on expressions.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/partitioned_in_memory_table.md#_snippet_17\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntrades[`logPrice1`newQty1] = <[log(price1), double(qty1)]>;\n```\n\n----------------------------------------\n\nTITLE: Query Registered Stream Computing Engine in DolphinDB\nDESCRIPTION: Fetches overall status of registered stream computing engines via getStreamEngineStat(), enabling users to monitor the operational state of stream processing components.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/machine_learning_volatility/07.streamStateQuery.txt#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\ngetStreamEngineStat()\n```\n\n----------------------------------------\n\nTITLE: Server Response to Connection Request\nDESCRIPTION: This describes the format of the server's response to a 'connect' request. The response includes the assigned session ID, an indicator for the byte order (1 for little-endian, 0 for big-endian), and the execution result ('OK' upon successful connection). The session ID is crucial for subsequent communication.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/api_protocol.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n长度(Byte) | 报文 | 说明\n---|---|---\n不固定| 例如'1195587396' | SESSIONID  \n1| 空格| char(0x20)\n1|0 | 返回对象数量\n1| 空格|  char(0x20)\n1| 1| 大小端，1-小端，0-大端\n1|  换行符(LF)| char(0x10)\n不固定| 执行结果  | \"OK\"\n```\n\n----------------------------------------\n\nTITLE: Connecting to Redis and Subscribing Handler\nDESCRIPTION: This code establishes a connection to a Redis server at the specified host and port using the `redis::connect` function. The connection object is stored in the variable `conn`. It then uses `subscribeTable` to subscribe the previously defined `myHandle` function, along with the `conn` object passed as a partial application argument, to the stream table `table1`.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/redis_plugin_tutorial/example2.txt#_snippet_3\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nconn = redis::connect(host, port)\nsubscribeTable(tableName=\"table1\", handler=myHandle{conn})\n```\n\n----------------------------------------\n\nTITLE: Viewing Table Schema\nDESCRIPTION: Shows how to view the schema of a table, including column names, data types, and partition information, using the `schema` function.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/partitioned_in_memory_table.md#_snippet_25\n\nLANGUAGE: DolphinDB\nCODE:\n```\nschema(trades)\n```\n\n----------------------------------------\n\nTITLE: Deleting a Database in DolphinDB\nDESCRIPTION: This code snippet demonstrates how to delete a database in DolphinDB using the `dropDatabase` function.  The function takes the database path as a parameter.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Single_Node_Tutorial.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndropDatabase(\"dfs://DolphinDB\");\n```\n\n----------------------------------------\n\nTITLE: DolphinDB SQL 查询：获取某设备最新100条记录\nDESCRIPTION: 此脚本快速查询指定日期、租户和设备ID的最新100条采集记录，通过分区剪枝定位数据，利用索引加快检索，适用于实时监控场景，查询耗时一般为2毫秒左右。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/iot_query_case.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nloadTable(database(\"dfs://NoiseDB\"),'noise')\nselect * from noise \nwhere date=2022.01.01 and tenantId=1055 and deviceId=10067\norder by ts desc\nlimit 100\n```\n\n----------------------------------------\n\nTITLE: 查询租户所有设备的最新状态\nDESCRIPTION: 此脚本通过条件过滤和分区定位，结合 context by 及 csort，将每个设备的最新状态快速获取，适合实时监控多个设备的最新状态信息，单次查询耗时约25毫秒。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/iot_query_case.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nloadTable(database(\"dfs://NoiseDB\"),'noise')\nselect * from noise \nwhere date=2022.01.01 and tenantId=1055\ncontext by deviceId\ncsort ts desc\nlimit 1\n```\n\n----------------------------------------\n\nTITLE: Deleting Rows from Memory Table with SQL\nDESCRIPTION: Demonstrates deleting rows from a memory table using a SQL DELETE statement with a WHERE clause to specify the rows to delete.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/partitioned_in_memory_table.md#_snippet_21\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndelete from trades where qty3<20;\n```\n\n----------------------------------------\n\nTITLE: Granting TABLE_READ Permission Globally - DolphinDB\nDESCRIPTION: This code snippet demonstrates how to grant a user global TABLE_READ permission, allowing them to read any table in the system.  It utilizes the `grant` function. No specific dependencies are required beyond a DolphinDB environment.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Permission_Management.md#_snippet_19\n\nLANGUAGE: DolphinDB\nCODE:\n```\ngrant(`user1, TABLE_READ, \"*\")\n```\n\n----------------------------------------\n\nTITLE: Starting DolphinDB Control Node Using Shell Script\nDESCRIPTION: This command is executed on the server hosting the control node (typically P1) within the clusterDemo directory. It launches the control node process that manages the cluster's metadata and operations. This start operation is critical after upgrades or shutdowns.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/multi_machine_cluster_deployment.md#_snippet_5\n\nLANGUAGE: Shell\nCODE:\n```\nsh startController.sh\n```\n\n----------------------------------------\n\nTITLE: Configuring Authentication Parameters for WeChat Work API\nDESCRIPTION: Code for setting up the authentication parameters needed to obtain an access token from WeChat Work. Requires the corporation ID and secret which should be replaced with actual values.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/send_messages_external_systems.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\nparam=dict(string,string);\nID='xxxxx';\nSECRET='xxxxx';\nparam['corpid']=ID;\nparam['corpsecret']=SECRET;\n```\n\n----------------------------------------\n\nTITLE: Calling getRecentJobs on a remote node using rpc\nDESCRIPTION: This code snippet shows how to use the 'rpc' function to call 'getRecentJobs' on a remote node named 'P1-node1'. This allows you to obtain the job information from other nodes in the DolphinDB cluster.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_60\n\nLANGUAGE: shell\nCODE:\n```\nrpc(\"P1-node1\",getRecentJobs)\n```\n\n----------------------------------------\n\nTITLE: Viewing migrated data in DolphinDB\nDESCRIPTION: Sample DolphinDB command to retrieve top records from the migrated data table for verification purposes.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Migrate_data_from_Redshift_to_DolphinDB.md#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect top 10 * from pt\n```\n\n----------------------------------------\n\nTITLE: Installing DolphinDB Tools for AI DataLoader\nDESCRIPTION: Command to install the Python package that includes DDBDataLoader functionality, which is available from DolphinDB Python API version 1.30.22.2 onwards.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ai_dataloader_ml.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\npip install dolphindb-tools\n```\n\n----------------------------------------\n\nTITLE: 绘制基金季度平均收益率曲线\nDESCRIPTION: 针对某个基金，绘制其季度平均收益率变化趋势。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/public_fund_basic_analysis.md#_snippet_14\n\nLANGUAGE: DolphinDB\nCODE:\n```\nplot(qavgReturns[\"160211.SZ\"], chartType=LINE)\n```\n\n----------------------------------------\n\nTITLE: Subscribe Function for DolphinDB Plugin in C++\nDESCRIPTION: Defines the `subscribe` function, which acts as the user interface to create a new thread to append data. It checks that the arguments are of the correct type (TableSP or FunctionDefSP). Then, it creates a Client object and calls createAndRun. It also sets up a resource with an onClose callback to delete the client object. It takes Heap and a vector of ConstantSP as parameters.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_advance.md#_snippet_27\n\nLANGUAGE: c++\nCODE:\n```\n    ConstantSP subscribe(Heap *heap, vector<ConstantSP> &arguments) {\n        if (!arguments[0]->isTable()) {\n            throw IllegalArgumentException(\"subscribe\", \"First argument must be a table!\");\n        }\n        if (!arguments[1]->isTable() && (arguments[1]->getType() != DT_FUNCTIONDEF)){\n            throw IllegalArgumentException(\"subscribe\", \"Second argument must be a table or function!\");\n        }\n        Client * pClient = new Client();\n        pClient->createAndRun(heap, arguments[0], arguments[1]);\n        FunctionDefSP onClose(Util::createSystemProcedure(\"client onClose()\", clientOnClose, 1, 1));\n        return Util::createResource(reinterpret_cast<long long>(pClient), \"client\", onClose, heap->currentSession());\n    }\n```\n\n----------------------------------------\n\nTITLE: Loading 'US_Trades' Data into Elasticsearch (Python - Partial)\nDESCRIPTION: Partial Python script intended to load US trade data from 'US.csv' into the 'uscsv' Elasticsearch index. It shows the main execution block which orchestrates index setup (assuming helper functions like `create_index_template`, `delete_index`, `create_index` are defined elsewhere, likely similar to the 'Trades' script), reads data in chunks (assuming a `read_lines` function), formats data for the bulk API using `bulk_import_lines`, sends bulk requests, and measures import time. Requires `urllib3` library.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport urllib3\nimport json\nimport csv\nimport time\n\ndef main():\n    create_index_template()\n    delete_index()  \n    create_index()\n    http = urllib3.PoolManager()\n    t1 = time.time()\n    for tmp in read_lines():\n        # 分段生成小文件来加载\n        data = '\\n'.join(bulk_import_lines(tmp))\n        data += '\\n'\n        response = http.request('PUT', 'http://localhost:9200/_bulk', body=data.encode('utf-8'), headers={'Content-Type': 'application/json'})\n        print(response.status)\n        print('\\n')\n        print(response.data)\n        print('\\n')\n    t2 = time.time()\n    print(\"导入数据耗时(ms):\", (t2-t1)*1000)\n\n\ndef bulk_import_lines(lines):\n    for line in lines:\n        yield json.dumps({'index': {'_index': 'uscsv', '_type': 'type'}})\n        yield json.dumps(line)\n```\n\n----------------------------------------\n\nTITLE: 查看数据概览\nDESCRIPTION: 通过select语句查询加载的表中的前10条记录，用于验证数据是否正确导入。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/fund_factor_contrasted_by_py.md#_snippet_23\n\nLANGUAGE: dolphindb\nCODE:\n```\nselect top 10 * from loadTable(\"dfs://fund_OLAP\", \"fund_OLAP\")\n```\n\nLANGUAGE: dolphindb\nCODE:\n```\nselect top 10 * from loadTable(\"dfs://fund_OLAP\", \"fund_hs_OLAP\")\n```\n\n----------------------------------------\n\nTITLE: 定义主计算流程的 `main()` 函数（Python）\nDESCRIPTION: 该函数接受含有价值、价格和日志序列的字典，利用numpy数组执行多项因子指标的计算（调用如 `getAnnualReturn` 等函数，假定已定义或导入），以及对数收益率的计算。目标是框架化因子计算流程，方便在多资产数据上批量处理。依赖numpy和pandas库。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/fund_factor_contrasted_by_py.md#_snippet_30\n\nLANGUAGE: Python\nCODE:\n```\ndef main(li):\n    value = np.array(li[\"value\"])\n    price = np.array(li[\"price\"])\n    log = np.array(li[\"log\"])\n    getAnnualReturn(value)\n    getAnnualVolatility(value)\n    getAnnualSkew(value)\n    getAnnualKur(value)\n    getSharp(value)\n    getMaxDrawdown(value)\n    getDrawdownRatio(value)\n    getBeta(value, price)\n    getAlpha(value, price)\n    calHurst(log, 2)\n```\n\n----------------------------------------\n\nTITLE: 计算股票价格分段变化率\nDESCRIPTION: 使用segment函数对股票数据按照OfferPrice1连续相同的部分进行分组，计算每段的价格变化率。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_SQL_Case_Tutorial.md#_snippet_24\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntimer t = select last(OfferPrice1) \\ first(OfferPrice1) - 1 \n\t\t  from loadTable(\"dfs://Level1\", \"Snapshot\") \n\t\t  where date(DateTime) = 2020.06.01 \n\t\t  group by SecurityID, segment(OfferPrice1, false) \n```\n\n----------------------------------------\n\nTITLE: 使用引用赋值提高变量交换效率(DolphinDB)\nDESCRIPTION: 展示了在DolphinDB中使用引用赋值(&)与普通值交换的性能对比。通过timer函数测量，引用赋值方法在交换大型向量时明显比直接交换更高效。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/kdb_to_dolphindb.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\nn=20000000;\nx=rand(200000.0, n);\ny=rand(200000.0, n);\n\ntimer x, y = y, x;        // Time elapsed: 1240.119 ms\ntimer {&t=x;&x=y;&y=t;}      // Time elapsed: 0.004 ms\n```\n\n----------------------------------------\n\nTITLE: 定义哈希分区并创建分区表（DolphinDB, HASH）\nDESCRIPTION: 该代码展示了如何在DolphinDB中利用哈希算法（HASH）对数据进行水平分割，指定分区数并建立分区表，然后加载和查询。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/database.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\nn=1000000\nID=rand(10, n)\n x=rand(1.0, n)\nt=table(ID, x)\ndb=database(\"dfs://hashdb\", HASH,  [INT, 2])\n\npt = db.createPartitionedTable(t, `pt, `ID)\npt.append!(t)\n\npt=loadTable(db,`pt)\nselect count(x) from pt;\n```\n\n----------------------------------------\n\nTITLE: Creating Directory for Debezium Configuration\nDESCRIPTION: Bash commands to create a directory for storing Debezium connector configuration files.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Debezium_and_Kafka_data_sync.md#_snippet_8\n\nLANGUAGE: Bash\nCODE:\n```\nmkdir /KFDATA/datasyn-config\ncd /KFDATA/datasyn-config\nvim source-mysql.json\n```\n\n----------------------------------------\n\nTITLE: Checking DolphinDB License Memory Limits\nDESCRIPTION: DolphinDB script to check the maximum memory per node allowed by the current license.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/handling_oom.md#_snippet_4\n\nLANGUAGE: SQL\nCODE:\n```\nlicense().maxMemoryPerNode\n//8\n```\n\n----------------------------------------\n\nTITLE: Checking DolphinDB Configuration Memory Limits\nDESCRIPTION: DolphinDB script to check the maximum memory size configured for the database node.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/handling_oom.md#_snippet_5\n\nLANGUAGE: SQL\nCODE:\n```\ngetConfig(`maxMemSize)\n//16\n```\n\n----------------------------------------\n\nTITLE: Hamming窗函数（生成窗函数系数矩阵，DolphinDB 脚本实现）\nDESCRIPTION: 此函数生成长度为N的Hamming窗系数，用于信号处理中的窗函数，减少频谱泄漏。参数N为窗长度。该函数可被集成到PSD计算中的窗函数应用中。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Random_Vibration_Signal_Analysis_Solution.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef hamming(N){\n    return 0.54-0.46*cos(2*pi*(0..(N-1))/(N-1))\n}\n```\n\n----------------------------------------\n\nTITLE: Complete code for calculating cumulative volume with dynamic grouping\nDESCRIPTION: This code includes the complete example, defining data, the custom volume aggregation logic, and the query that groups and calculates the cumulative volume for each segment.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_52\n\nLANGUAGE: shell\nCODE:\n```\ntimex = 13:03:00+(0..27)*60\nvolume = 288658 234804 182714 371986 265882 174778 153657 201388 175937 138388 169086 203013 261230 398871 692212 494300 581400 348160 250354 220064 218116 458865 673619 477386 454563 622870 458177 880992\nt = table(timex as time, volume)\n\ndef caclCumVol(target, preResult, x){\n result = preResult + x\n if(result - target> target - preResult) return x\n else return result\n}\noutput = select sum(volume) as sum_volume, last(time) as endTime from t group by iif(accumulate(caclCumVol{1500000}, volume)==volume, time, NULL).ffill() as startTime\n```\n\n----------------------------------------\n\nTITLE: 测量任务执行时间\nDESCRIPTION: 通过查询getRecentJobs()获取单用户和多用户任务的执行时间，用于比较不同配置下的性能。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/fund_factor_contrasted_by_py.md#_snippet_28\n\nLANGUAGE: dolphindb\nCODE:\n```\n//获取单个用户的运行时间\nselect max(endTime) - min(startTime) from getRecentJobs() where jobDesc = \"parallJob_single_ten\"\n//获取多个用户的运行时间\nselect max(endTime) - min(startTime) from getRecentJobs() where jobDesc = \"parallJob_multi_ten\"\n```\n\n----------------------------------------\n\nTITLE: 加载数据表计算累积VWAP\nDESCRIPTION: 加载DolphinDB中的股票快照数据表，使用group by和cgroup by语法计算每只股票每分钟的累积交易量加权平均价格(VWAP)。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_SQL_Case_Tutorial.md#_snippet_19\n\nLANGUAGE: DolphinDB\nCODE:\n```\nsnapshot = loadTable(\"dfs://Level1\", \"Snapshot\")\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntimer result = select wavg(LastPx, Volume) as vwap \n\t\t\t   from snapshot \n\t\t\t   group by SecurityID \n\t\t\t   cgroup by minute(DateTime) as Minute \n\t\t\t   order by SecurityID, Minute\n```\n\n----------------------------------------\n\nTITLE: SQL Query with Group Filtering for DDBDataLoader\nDESCRIPTION: Examples showing how DDBDataLoader splits a query into multiple group-specific queries based on groupCol and groupScheme parameters.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ai_dataloader_ml.md#_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nselect * from loadTable(dbName, tableName) where stockID = \"apple\"\nselect * from loadTable(dbName, tableName) where stockID = \"google\"\nselect * from loadTable(dbName, tableName) where stockID = \"amazon\"\n```\n\n----------------------------------------\n\nTITLE: Setting Oracle ODBC Environment Variables - Shell\nDESCRIPTION: Adds LD_LIBRARY_PATH and TNS_ADMIN export statements to the user's shell profile to make instant client libraries and network configuration available to ODBC applications. Users must reload or source their profile for changes to take effect.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Oracle_to_DolphinDB.md#_snippet_6\n\nLANGUAGE: Shell\nCODE:\n```\nvi ~/.bashrc\n\nexport LD_LIBRARY_PATH=/usr/local/oracle/instantclient_21_7:$LD_LIBRARY_PATH\nexport TNS_ADMIN=/etc/oracle\n\n```\n\n----------------------------------------\n\nTITLE: Character Encoding Conversion with addCol\nDESCRIPTION: This code snippet is a part of `addCol` function and is used to convert the character encoding of a specific column (custname) from GBK to UTF-8 using the `toUTF8` function. It ensures proper display of data with different character encodings.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/LoadDataForPoc.md#_snippet_8\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef addCol(mutable memTable)\n{\n\treturn mutable.replaceColumn!(`custname,toUTF8(mutable.custname,`gbk))\n}\n```\n\n----------------------------------------\n\nTITLE: Defining a custom function for cumulative volume calculation\nDESCRIPTION: This code defines a function `caclCumVol` that calculates the cumulative volume with a grouping logic.  If adding the current volume (x) makes the cumulative result further from the target than the previous result, then current value starts a new group, return current value. The purpose is to group data points until the accumulated volume is close to the target.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_48\n\nLANGUAGE: shell\nCODE:\n```\ndef caclCumVol(target, preResult, x){\n result = preResult + x\n if(result - target> target - preResult) return x\n else return result\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Druid - middleManager\nDESCRIPTION: Configuration settings for the Druid middleManager service, including memory allocation, number of tasks per middleManager, HTTP server thread count, and settings for processing threads and buffers on Peons. Dependencies: Druid setup.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_Druid_appendix.md#_snippet_7\n\nLANGUAGE: Druid\nCODE:\n```\nmiddleManager:\nXms64m\nXmx64m\n\n# Number of tasks per middleManager\ndruid.worker.capacity=3\n\n# HTTP server threads\ndruid.server.http.numThreads=25\n\n# Processing threads and buffers on Peons\ndruid.indexer.fork.property.druid.processing.buffer.sizeBytes=4147483648\ndruid.indexer.fork.property.druid.processing.numThreads=2\n```\n\n----------------------------------------\n\nTITLE: Checking Job Status in DolphinDB\nDESCRIPTION: Retrieves information about recently submitted jobs to monitor the replay process.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/stock_market_replay/01.stockMarketReplay.txt#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\ngetRecentJobs()\n```\n\n----------------------------------------\n\nTITLE: 计算股票月度收益波动率\nDESCRIPTION: 使用interval函数对日期按月分组，并计算每月内收益率的标准差(波动率)。使用'prev'填充方式处理缺失值。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_SQL_Case_Tutorial.md#_snippet_32\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntimer res = select std(rate) from t group by code, interval(date(date), 1, \"prev\")\n```\n\n----------------------------------------\n\nTITLE: Downloading Oracle Instant Client ODBC Packages - Shell\nDESCRIPTION: Uses wget to retrieve Oracle Instant Client Basic and ODBC packages for Linux x86-64 from the official Oracle website. These files must match your Oracle server version and Linux architecture. Substitute URLs as needed for your version.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Oracle_to_DolphinDB.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\nwget https://download.oracle.com/otn_software/linux/instantclient/217000/instantclient-basic-linux.x64-21.7.0.0.0dbru.zip\nwget https://download.oracle.com/otn_software/linux/instantclient/217000/instantclient-odbc-linux.x64-21.7.0.0.0dbru.zip\n\n```\n\n----------------------------------------\n\nTITLE: Remove rows with NULL values using reduce\nDESCRIPTION: This DolphinDB script removes rows containing NULL values from a table using the `reduce` function and a custom function. It iterates through the table's columns and checks if a row contains NULL values.  This method avoids memory issues with large datasets. Assumes a boolean result of `true` if the table is empty.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/functional_programming_cases.md#_snippet_17\n\nLANGUAGE: shell\nCODE:\n```\nt[reduce(def(x,y) -> x and isValid(y), t.values(), true)]\n```\n\n----------------------------------------\n\nTITLE: 获取Table中的数据\nDESCRIPTION: 演示如何获取Table中的列数据，可以通过列索引或列名获取特定列的Vector对象。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/c++api.md#_snippet_6\n\nLANGUAGE: C++\nCODE:\n```\nConstantSP ret = conn.run(\"select ...\");\nVectorSP col0 = ret->getColumn(0); // 获得第0列\nVectorSP col1 = ret->getColumn(\"id\"); // 获取id列\n```\n\n----------------------------------------\n\nTITLE: Authentication in DolphinDB\nDESCRIPTION: Logs in to the DolphinDB server using admin credentials.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/stock_market_replay/01.stockMarketReplay.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(\"admin\", \"123456\")\n```\n\n----------------------------------------\n\nTITLE: 计算季度平均收益率\nDESCRIPTION: 将日收益率矩阵重采样为季度频率，计算每季度的平均收益率，用于季度表现分析。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/public_fund_basic_analysis.md#_snippet_13\n\nLANGUAGE: DolphinDB\nCODE:\n```\nqavgReturns = returnsMatrix.setIndexedMatrix!().resample(\"Q\", mean)\n```\n\n----------------------------------------\n\nTITLE: 创建 DolphinDB TSDB 数据库及表结构\nDESCRIPTION: 该脚本在 DolphinDB 中创建一个包含物联网噪声数据的 TSDB 类型数据库，支持分区和排序优化，以提高查询效率，包括设定分区字段 tenantId 与 date，以及汇总索引 deviceId 和 ts。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/iot_query_case.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ndb1 = database(,VALUE,1000..2000) \ndb2  = database(, VALUE, 2022.01.01..2022.12.30) \n\n// TSDB for iot \ndbNoise = database(\"dfs://NoiseDB\",COMPO,[db1,db2], engine=\"TSDB\") \n\ncreate table \"dfs://NoiseDB\".\"noise\"(\n    tenantId INT,\n    deviceId INT,\n    soundPressureLevel INT,\n    soundPowerLevel DOUBLE,\n    ts TIMESTAMP,\n    date DATE\n)\npartitioned by tenantId, date\nsortColumns=[`deviceId,`ts]\n```\n\n----------------------------------------\n\nTITLE: 解除订阅并清理流数据引擎资源\nDESCRIPTION: 在重复执行时序引擎示例前，需要解除订阅关系并删除相关资源，包括流数据表和时序引擎。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/stream_aggregator.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\nunsubscribeTable(tableName=\"trades\", actionName=\"append_tradesAggregator\")\nundef(`trades, SHARED)\ndropStreamEngine(\"streamAggr1\")\n```\n\n----------------------------------------\n\nTITLE: Editing Agent Node Configuration Using Shell\nDESCRIPTION: This snippet shows editing the agent.cfg file to configure a proxy (agent) node's mode, local site, controller site, and list of cluster sites including all controllers and the agent itself. localSite specifies the IP, port and alias for this agent; controllerSite must point to a controller's localSite for initial communication; sites includes all relevant nodes with their roles. Parameters workerNum, maxMemSize, and lanCluster can be adjusted. Consistency with controller node configurations is essential when localSite aliases or IPs change across the cluster.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ha_cluster_deployment.md#_snippet_9\n\nLANGUAGE: Shell\nCODE:\n```\nvim ./agent.cfg\n```\n\nLANGUAGE: Shell\nCODE:\n```\nmode=agent\nlocalSite=10.0.0.82:8801:agent3\ncontrollerSite=10.0.0.80:8800:controller1\nsites=10.0.0.82:8801:agent3:agent,10.0.0.80:8800:controller1:controller,10.0.0.81:8800:controller2:controller,10.0.0.82:8800:controller3:controller\nworkerNum=4\nmaxMemSize=4\nlanCluster=0\n```\n\n----------------------------------------\n\nTITLE: Loading Data into Partitioned Memory Table with ploadText\nDESCRIPTION: Loads data from a text file into a partitioned in-memory table using the `ploadText` function.  This method is simple but limited, requiring the file to fit in memory and lacking advanced sorting capabilities.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/partitioned_in_memory_table.md#_snippet_8\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntrades = ploadText(workDir + \"/trades.txt\");\n```\n\n----------------------------------------\n\nTITLE: Modify Permissions and Edit Configuration - Shell\nDESCRIPTION: This group of shell commands is used to enable execution permission on the DolphinDB server binary and open the configuration file (`dolphindb.cfg`) for edits to optimize memory and port settings specific to embedded ARM environments. It includes permission modification, usage of vim editor, and examples of critical configuration parameters like `localSite`, `maxMemSize`, `regularArrayMemoryLimit`, and `maxLogSize` with their recommended value formats.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ARM_standalone_deploy.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\nchmod +x dolphindb\n```\n\nLANGUAGE: Shell\nCODE:\n```\nvim dolphindb.cfg\n```\n\nLANGUAGE: Shell\nCODE:\n```\nlocalSite=localhost:8900:local8900\n```\n\nLANGUAGE: Shell\nCODE:\n```\nmaxMemSize=0.8\n```\n\nLANGUAGE: Shell\nCODE:\n```\nregularArrayMemoryLimit=64\n```\n\nLANGUAGE: Shell\nCODE:\n```\nmaxLogSize=100\n```\n\n----------------------------------------\n\nTITLE: Defining Stage Data Structure\nDESCRIPTION: This code snippet defines the `Process` class, which represents a stage in the StreamEngineParser's processing pipeline. Each stage includes an engine type, the input table's schema, a list of metrics to be calculated, and the schema for the output table. The `EngineType` enum specifies the supported engine types, like REACTIVE_STATE_ENGINE, CROSS_SECTIONAL_ENGINE, and TIME_SERIES_ENGINE. This structure is fundamental to how the parser organizes and executes calculations across stages.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/StreamEngineParser.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\nclass Process {\n  private:\n    EngineType engineType_;\n    TableSP inputDummy_;\n    vector<ObjectSP> metrics_;\n    TableSP outputTableDummy_;\n}\n\nenum EngineType {\n    REACTIVE_STATE_ENGINE,\n    CROSS_SECTIONAL_ENGINE,\n    TIME_SERIES_ENGINE，\n    NONE_ENGINE\n}\n```\n\n----------------------------------------\n\nTITLE: Unregistering Snapshot Engine\nDESCRIPTION: Unregisters the snapshot engine for a distributed table.  It requires the database name and table name as input. This action releases the memory used by the snapshot engine, which is beneficial when the engine is no longer needed and memory resources are limited.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/snapshot_engine.md#_snippet_3\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nunregisterSnapshotEngine(dbName, tableName)\n```\n\n----------------------------------------\n\nTITLE: 生成绩效统计数据表\nDESCRIPTION: 整理基金ID、类型、年化收益率、波动率和夏普比率，为后续可视化提供数据支撑。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/public_fund_basic_analysis.md#_snippet_21\n\nLANGUAGE: DolphinDB\nCODE:\n```\nperf = table(uReturnsMatrix.colNames() as SecurityID, fundTypeMap[uReturnsMatrix.colNames()] as Type, exp*100 as exp, vol*100 as vol, sharpe)\n```\n\n----------------------------------------\n\nTITLE: Setting Maximum Connections for DolphinDB Node\nDESCRIPTION: This parameter defines the maximum number of connections allowed to the local DolphinDB node from various sources, including GUI clients, APIs, and other nodes. Increasing this value allows for more concurrent connections, potentially improving performance under heavy load.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/ha_cluster_deployment/P3/config/config-specification.txt#_snippet_1\n\nLANGUAGE: properties\nCODE:\n```\nmaxConnections=512\n```\n\n----------------------------------------\n\nTITLE: Appending Data to a Table with append C++\nDESCRIPTION: Provides a function example for appending data from one DolphinDB Table (`t2`) to another (`t1`) using the `TableSP::append` method. It collects columns from the source table into a vector of `ConstantSP` and passes it to `append`. Includes basic error handling for column type mismatch.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_advance.md#_snippet_19\n\nLANGUAGE: C++\nCODE:\n```\nConstantSP append(Heap *heap, vector<ConstantSP> &arguments) {\n    if (!(arguments[0]->isTable() && arguments[1]->isTable())) {\n        throw IllegalArgumentException(\"append\", \"arguments need two tables\");\n    }\n    TableSP t1 = arguments[0];\n    TableSP t2 = arguments[1];\n    size_t columnSize = t2->columns();\n    std::vector<ConstantSP> dataToAppend;\n    for (size_t i = 0; i < columnSize; i++) {\n        dataToAppend.emplace_back(t2->getColumn(i));\n    }\n    INDEX insertedRows;\n    std::string errMsg;\n    bool success = t1->append(dataToAppend, insertedRows, errMsg);\n    if (!success)\n        std::cerr << errMsg << std::endl;\n    return new Void();\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring DolphinDB PluginHandler Build with CMake\nDESCRIPTION: CMake script that sets up the build environment for the PluginHandler project. It configures C++11 compatibility, platform-specific definitions, include paths, and library dependencies required to build the PluginHandler shared library for DolphinDB.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin/Handler/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.0)\n\nproject(PluginHandler)\n\nadd_definitions(-D_GLIBCXX_USE_CXX11_ABI=0)\n\nLINK_DIRECTORIES(\"../libs\")\nINCLUDE_DIRECTORIES(\"../include\")\nif(WIN32)\n    add_definitions(-DWINDOWS)\nelseif(UNIX)\n    add_definitions(-DLINUX)\nendif()\n\nadd_compile_options( \"-std=c++11\" \"-fPIC\" \"-Wall\" \"-Werror\")\n\nadd_library(PluginHandler SHARED\n    \"./src/Handler.cpp\"\n)\ntarget_link_libraries(PluginHandler DolphinDB)\n```\n\n----------------------------------------\n\nTITLE: Copying DolphinDB Files Between Servers Using SCP\nDESCRIPTION: Uses 'scp' command to recursively copy DolphinDB installation directories from one server to another, facilitating migration or setup of new nodes.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/service_deployment_and_migration.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nscp -r /home/dolphindb_1 root@172.0.0.4:/home/dolphindb_4\nscp -r /home/dolphindb_1 root@172.0.0.5:/home/dolphindb_5\n```\n\n----------------------------------------\n\nTITLE: Executing DataX Data Migration Command in Linux Terminal\nDESCRIPTION: Runs DataX job via command line, setting JVM memory options and specifying the JSON configuration file. Dependencies include Linux shell environment with Python and DataX installed. Inputs are the directory path and configuration filename; output is the start of data migration process and logs.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Oracle_to_DolphinDB.md#_snippet_16\n\nLANGUAGE: Shell\nCODE:\n```\ncd ./dataX/bin/\npython datax.py --jvm=-Xmx8g ../../datax-writer-master/ddb_script/oracleddb.json\n```\n\n----------------------------------------\n\nTITLE: Data Type and Data Form Codes\nDESCRIPTION: This is a key-value list providing the numeric codes used to represent different data types (DT_*) and data forms (DF_*) within the API protocol. It is useful for interpreting the raw byte streams in the messages, to understand the structure of the data being transmitted.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/api_protocol.md#_snippet_12\n\nLANGUAGE: text\nCODE:\n```\n数据类型| 报文值\n---|---\nDT_VOID|0\nDT_BOOL|1\nDT_BYTE|2\nDT_SHORT|3\nDT_INT|4\nDT_LONG|5\nDT_DATE|6\nDT_MONTH|7\nDT_TIME|8\nDT_MINUTE|9\nDT_SECOND|10\nDT_DATETIME|11\nDT_TIMESTAMP|12\nDT_NANOTIME|13\nDT_NANOTIMESTAMP|14\nDT_FLOAT|15\nDT_DOUBLE|16\nDT_SYMBOL|17\nDT_STRING|18\nDT_UUID|19\nDT_FUNCTIONDEF|20\nDT_HANDLE|21\nDT_CODE|22\nDT_DATASOURCE|23\nDT_RESOURCE|24\nDT_ANY|25\nDT_DICTIONARY|26\nDT_OBJECT|27\n```\n\nLANGUAGE: text\nCODE:\n```\n数据形式|报文值\n---|---\nDF_SCALAR|0\nDF_VECTOR|1\nDF_PAIR|2\nDF_MATRIX|3\nDF_SET|4\nDF_DICTIONARY|5\nDF_TABLE|6\nDF_CHART|7\nDF_CHUNK|8\n```\n\n----------------------------------------\n\nTITLE: Logging In DolphinDB Script\nDESCRIPTION: Authenticates the user with the Dolphin DolphinDB server using the provided username and password. This is typically a necessary first step to gain appropriate permissions for subsequent operations like creating tables, databases, or submitting jobs.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/iot_demo_script.txt#_snippet_0\n\nLANGUAGE: DolphinDB Script\nCODE:\n```\nlogin(\"admin\",\"123456\")\n```\n\n----------------------------------------\n\nTITLE: Identifying Port Binding Failure from DolphinDB Log Error\nDESCRIPTION: This shell console log error message indicates that the DolphinDB node failed to bind to the specified port (e.g., 8900) because the port is already in use by another process, evidenced by error code 98. This information guides users to change the port configuration to an available port in order to successfully start the DolphinDB cluster node.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/single_machine_cluster_deploy.md#_snippet_22\n\nLANGUAGE: Shell\nCODE:\n```\n<ERROR> :Failed to bind the socket on port 8900 with error code 98\n```\n\n----------------------------------------\n\nTITLE: Setting up the environment and configuring input files in DolphinDB\nDESCRIPTION: Initializes the DolphinDB environment by clearing cache, logging in as admin, and defining file paths for the input CSV data files containing NAV and HS300 data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_data_load.txt#_snippet_0\n\nLANGUAGE: dolphindb\nCODE:\n```\nundef all\nclearAllCache()\ngo\nlogin(\"admin\", \"123456\")\n\ncsvPath = \"datafile/nav.csv\"\ncsvPath1 = \"datafile/hs300.csv\"\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Describe Function on Large CSV - Python Script\nDESCRIPTION: This Python snippet loads a large CSV with selective columns and converters using numpy, constructs a pandas DataFrame, then executes describe to compute summary statistics, measuring performance. Dependencies: pandas, numpy, and sufficient memory for the expected data size. Key parameters include columns to load, converters for type casting, and timer setup for benchmarking. Expected input is a CSV; output is printed descriptive statistics and duration. Limitations: pandas requires data to fit in memory, restricting scalability for very large files.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/generate_large_scale_statistics_with_summary.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nimport pandas as pd\nimport numpy as np\nimport time\n\ndef to_int(s):\n    return int(s[0])\n\ndata = np.loadtxt('/path/to/TradesData.csv', delimiter=',', skiprows=1, usecols=(3, 4, 5, 6, 8), converters={8: to_int})\ndf = pd.DataFrame(data, columns=['price', 'size', 'g127', 'corr', 'ex'])\nstartTime = time.time()\ndf.describe()\nendTime = time.time()\nprint(\"duration\", endTime - startTime)\n```\n\n----------------------------------------\n\nTITLE: 查看计算节点的存储卷配置\nDESCRIPTION: 此SQL查询用于验证数据节点的存储卷配置，确认它们管理着事务日志和数据块文件，而计算节点没有存储数据。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Compute_Node.md#_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nselect * from pnodeRun(getConfig{`volumes})\n```\n\n----------------------------------------\n\nTITLE: 查看计算节点data目录\nDESCRIPTION: 此操作系统命令检查计算节点的默认数据目录内容，确认其不存储任何的分布式数据文件。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Compute_Node.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\n[appadmin@compute volumes]$ lsof -p pidOfComputenode\n```\n\n----------------------------------------\n\nTITLE: Checking Recovery Task Status in DolphinDB\nDESCRIPTION: This DolphinDB script uses the `rpc` function to call `getRecoveryTaskStatus` on the controller.  This command checks the status of ongoing recovery tasks, which is used to monitor the progress of data migrations or rebalancing operations. It doesn't take any parameters.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/data_move_rebalance.md#_snippet_8\n\nLANGUAGE: DolphinDB\nCODE:\n```\nrpc(getControllerAlias(), getRecoveryTaskStatus)\n```\n\n----------------------------------------\n\nTITLE: Deleting All Elasticsearch Scrolls using urllib3 in Python\nDESCRIPTION: This function uses the `urllib3` library to send a DELETE request to the Elasticsearch `_search/scroll/_all` endpoint, effectively clearing all active search scrolls on the cluster. It prints the HTTP status code and response data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_60\n\nLANGUAGE: python\nCODE:\n```\nimport urllib3\n\ndef delete_scroll():\n    http = urllib3.PoolManager()\n    r = http.request(\"DELETE\", \"http://localhost:9200/_search/scroll/_all\")\n    print(r.status)\n    print(r.data)\n```\n\n----------------------------------------\n\nTITLE: Creating a DolphinDB database\nDESCRIPTION: Creates a database named configDB to store configuration settings.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/cachedTable/mysql_data.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\ncreate database configDB;\n```\n\n----------------------------------------\n\nTITLE: Creating Stream Table for Market Data Messages in DolphinDB\nDESCRIPTION: Defines and executes a function to create a persistent stream table named 'messageStream' with columns for message time, type, and body. The table is configured with sharing, persistence, and caching options.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/stock_market_replay/01.stockMarketReplay.txt#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef createStreamTableFunc(){\n\tcolName = `msgTime`msgType`msgBody\n\tcolType = [TIMESTAMP,SYMBOL, BLOB]\n\tmessageTemp = streamTable(1000000:0, colName, colType)\n\tenableTableShareAndPersistence(table=messageTemp, tableName=\"messageStream\", asynWrite=true, compress=true, cacheSize=1000000, retentionMinutes=1440, flushMode=0, preCache=10000)\n\tmessageTemp = NULL\n}\ncreateStreamTableFunc()\ngo\n```\n\n----------------------------------------\n\nTITLE: Using a DolphinDB database\nDESCRIPTION: Switches the active database context to configDB.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/cachedTable/mysql_data.txt#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\nuse configDB;\n```\n\n----------------------------------------\n\nTITLE: Executing DataX Task via Command Line\nDESCRIPTION: This snippet shows how to execute a DataX task using the python datax.py command. The path to the DataX JSON configuration file should be provided as an argument.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ClickHouse_to_DolphinDB.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython datax.py ../../datax-writer-master/ddb_script/ClickHouseToDDB.json\n```\n\n----------------------------------------\n\nTITLE: Tuning DolphinDB Cluster Node Storage and Caching Parameters in cluster.cfg (config)\nDESCRIPTION: This configuration file example sets disk I/O concurrency, multiple SSD volumes for data, redo log locations, metadata directories, and caching sizes for chunk and TSDB operations. Proper adjustment ensures high performance and resilience in production-grade DolphinDB clusters. Update node paths and values according to your actual disk layout and RAM size.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/deploy_dolphindb_on_new_server.md#_snippet_14\n\nLANGUAGE: config\nCODE:\n```\ndiskIOConcurrencyLevel=0\nnode1.volumes=/ssd1/dolphindb/volumes/node1,/ssd2/dolphindb/volumes/node1 \nnode1.redoLogDir=/ssd1/dolphindb/redoLog/node1 \nnode1.chunkMetaDir=/ssd1/dolphindb/metaDir/chunkMeta/node1 \nnode1.TSDBRedoLogDir=/ssd1/dolphindb/tsdb/node1/redoLog\nchunkCacheEngineMemSize=2\nTSDBCacheEngineSize=2\n...\n\n```\n\n----------------------------------------\n\nTITLE: Transforming and Loading Financial Market Data with Array Vectors\nDESCRIPTION: Defines a transform function to convert flattened CSV data into structured table data with array vectors for order book levels, then loads the data from a CSV file into the database with the transform applied.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/Real-Time_Stock_Price_Increase_Calculation/01.创建存储快照数据的库表并导入数据.txt#_snippet_3\n\nLANGUAGE: dolphindb\nCODE:\n```\ndef transform(t){\n\ttemp = select lpad(string(SecurityID), 6, \"0\") as SecurityID, DateTime, PreClosePx, OpenPx, HighPx, LowPx, LastPx, TotalVolumeTrade, TotalValueTrade, InstrumentStatus,\n\t\t\tfixedLengthArrayVector(BidPrice0, BidPrice1, BidPrice2, BidPrice3,  BidPrice4, BidPrice5, BidPrice6, BidPrice7, BidPrice8, BidPrice9) as BidPrice,\n\t\t\tfixedLengthArrayVector(BidOrderQty0, BidOrderQty1, BidOrderQty2, BidOrderQty3,  BidOrderQty4, BidOrderQty5, BidOrderQty6, BidOrderQty7, BidOrderQty8, BidOrderQty9) as BidOrderQty,\n\t\t\tfixedLengthArrayVector(BidNumOrders0, BidNumOrders1, BidNumOrders2, BidNumOrders3,  BidNumOrders4, BidNumOrders5, BidNumOrders6, BidNumOrders7, BidNumOrders8, BidNumOrders9) as BidNumOrders,\n\t\t\tfixedLengthArrayVector(BidOrders0, BidOrders1, BidOrders2, BidOrders3,  BidOrders4, BidOrders5, BidOrders6, BidOrders7, BidOrders8, BidOrders9, BidOrders10, BidOrders11, BidOrders12, BidOrders13,  BidOrders14, BidOrders15, BidOrders16, BidOrders17, BidOrders18, BidOrders19, BidOrders20, BidOrders21, BidOrders22, BidOrders23,  BidOrders24, BidOrders25, BidOrders26, BidOrders27, BidOrders28, BidOrders29, BidOrders30, BidOrders31, BidOrders32, BidOrders33,  BidOrders34, BidOrders35, BidOrders36, BidOrders37, BidOrders38, BidOrders39, BidOrders40, BidOrders41, BidOrders42, BidOrders43,  BidOrders44, BidOrders45, BidOrders46, BidOrders47, BidOrders48, BidOrders49) as BidOrders,\n\t\t\tfixedLengthArrayVector(OfferPrice0, OfferPrice1, OfferPrice2, OfferPrice3,  OfferPrice4, OfferPrice5, OfferPrice6, OfferPrice7, OfferPrice8, OfferPrice9) as OfferPrice,\n\t\t\tfixedLengthArrayVector(OfferOrderQty0, OfferOrderQty1, OfferOrderQty2, OfferOrderQty3,  OfferOrderQty4, OfferOrderQty5, OfferOrderQty6, OfferOrderQty7, OfferOrderQty8, OfferOrderQty9) as OfferOrderQty,\n\t\t\tfixedLengthArrayVector(OfferNumOrders0, OfferNumOrders1, OfferNumOrders2, OfferNumOrders3,  OfferNumOrders4, OfferNumOrders5, OfferNumOrders6, OfferNumOrders7, OfferNumOrders8, OfferNumOrders9) as OfferNumOrders,\n\t\t\tfixedLengthArrayVector(OfferOrders0, OfferOrders1, OfferOrders2, OfferOrders3,  OfferOrders4, OfferOrders5, OfferOrders6, OfferOrders7, OfferOrders8, OfferOrders9, OfferOrders10, OfferOrders11, OfferOrders12, OfferOrders13,  OfferOrders14, OfferOrders15, OfferOrders16, OfferOrders17, OfferOrders18, OfferOrders19, OfferOrders20, OfferOrders21, OfferOrders22, OfferOrders23,  OfferOrders24, OfferOrders25, OfferOrders26, OfferOrders27, OfferOrders28, OfferOrders29, OfferOrders30, OfferOrders31, OfferOrders32, OfferOrders33,  OfferOrders34, OfferOrders35, OfferOrders36, OfferOrders37, OfferOrders38, OfferOrders39, OfferOrders40, OfferOrders41, OfferOrders42, OfferOrders43,  OfferOrders44, OfferOrders45, OfferOrders46, OfferOrders47, OfferOrders48, OfferOrders49) as OfferOrders,\n\t\t\tNumTrades, IOPV, TotalBidQty, TotalOfferQty, WeightedAvgBidPx, WeightedAvgOfferPx, TotalBidNumber, TotalOfferNumber, BidTradeMaxDuration, OfferTradeMaxDuration, \n\t\t\tNumBidOrders, NumOfferOrders, WithdrawBuyNumber, WithdrawBuyAmount, WithdrawBuyMoney,WithdrawSellNumber, WithdrawSellAmount, WithdrawSellMoney, ETFBuyNumber, ETFBuyAmount, \n\t\t\tETFBuyMoney, ETFSellNumber, ETFSellAmount, ETFSellMoney\n\t\t\tfrom t\n\treturn temp\n}\n\ncsvDataPath = \"/home/v2/下载/data/20211201snapshot_30stocks.csv\"\nloadTextEx(dbHandle=database(dbName), tableName=tbName, partitionColumns=`DateTime`SecurityID, filename=csvDataPath, transform=transform)\n```\n\n----------------------------------------\n\nTITLE: Authenticating with DolphinDB server\nDESCRIPTION: Logs into the DolphinDB server with admin credentials for database operations.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/Real-Time_Stock_Price_Increase_Calculation/01.创建存储快照数据的库表并导入数据.txt#_snippet_0\n\nLANGUAGE: dolphindb\nCODE:\n```\nlogin(\"admin\", \"123456\")\n```\n\n----------------------------------------\n\nTITLE: Generating Unique ID - C++\nDESCRIPTION: This C++ code defines a function `uuid` to generate a unique identifier (UUID) of a specified length. It uses the system process ID (`getpid()`) and random number generation (`rand()`) to create a unique string consisting of uppercase letters, lowercase letters, and numbers.  The generated UUID is used to identify different replay jobs, preventing naming conflicts and ensuring each replay has a distinct identifier.  The prerequisites involve including necessary header files for memory allocation and standard library functionalities.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/appendices_market_replay_bp.md#_snippet_7\n\nLANGUAGE: C++\nCODE:\n```\nstring uuid(int len)\n{\n    char* str = (char*)malloc(len + 1);\n    srand(getpid());\n    for (int i = 0; i < len; ++i)\n    {\n        switch (i % 3)\n        {\n        case 0:\n            str[i] = 'A' + std::rand() % 26;\n            break;\n        case 1:\n            str[i] = 'a' + std::rand() % 26;\n            break;\n        default:\n            str[i] = '0' + std::rand() % 10;\n            break;\n        }\n    }\n    str[len] = '\\0';\n    std::string rst = str;\n    free(str);\n    return rst;\n}\n```\n\n----------------------------------------\n\nTITLE: 使用Gurobi插件求解最大化效用函数问题\nDESCRIPTION: 通过Gurobi插件解决同样的最大化效用函数投资组合优化问题。在效用函数中同时考虑收益最大化和风险最小化，使用DolphinDB的Gurobi插件进行求解。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/MVO.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\nbaseDir=\"/home/data/\"\nf = dropna(flatten(double(matrix(select col1, col2, col3, col4, col5, col6, col7, col8, col9, col10 from loadText(baseDir + \"C.csv\") where rowNo(col1) > 0)).transpose()))\n\nN = f.size()\nH = eye(N)\nA = matrix(select * from loadText(baseDir + \"A_ub.csv\")).transpose()\nb = \n[0.025876723,\t0.092515275,\t0.035133942,\t0.053184884,\t0.067410565,\t0.009709433,\t0.04668745,\t0.00636804,\t0.022258664,\t0.11027537,\n0.018488302,\t0.027417204,\t0.028585,\t0.017228214,\t0.008055527,\t0.015727843,\t0.026132369,\t0.013646113,\t0.066000808,\t0.043606587,\n0.048325258,\t0.033868626,\t0.010790603,\t0.017737391,\t0.03252374,\t0.039329965,\t0.040665779,\t0.010868773,\t0.006819891,\t0.015879314,\n0.008882335,\t-0.025876723,\t-0.092515275,\t-0.035133942,\t-0.053184884,\t-0.067410565,\t-0.009709433,\t-0.04668745,\t-0.00636804,\t-0.022258664,\n-0.110275379,\t-0.018488302,\t-0.027417204,\t-0.028585,\t-0.017228214,\t-0.008055527,\t-0.015727843,\t-0.026132369,\t-0.013646113,\t-0.066000808,\n-0.043606587,\t-0.048325258,\t-0.033868626,\t-0.010790603,\t-0.017737391,\t-0.03252374,\t-0.039329965,\t-0.040665779,\t-0.010868773,\t-0.006819891,\n-0.015879314,\t-0.008882335]\n\nmodel = gurobi::model()\n/// 增加变量\nlb = take(-10, N)\nub = take(10, N)\nvarName = \"v\" + string(1..N)\nvars = gurobi::addVars(model, lb, ub, , ,varName)\n\n//增加线性约束\nfor (i in 0:N) {\n    lhsExpr = gurobi::linExpr(model, A[i], varName)\n    gurobi::addConstr(model, lhsExpr, '<', b[i])\n}\nlhsExpr = gurobi::linExpr(model, take(1, N), varName)\ngurobi::addConstr(model, lhsExpr, '=', 1)\n\n// 增加目标值\nlinExpr = gurobi::linExpr(model, f, vars)\nquadExpr = gurobi::quadExpr(model, H, varName, linExpr)\ngurobi::setObjective(model, quadExpr, 1)\n\ntimer status = gurobi::optimize(model) \n\n// 获取优化结果\nresult = gurobi::getResult(model)\nobj = gurobi::getObjective(model)\n```\n\n----------------------------------------\n\nTITLE: Query Subscription Worker Consumption Details in DolphinDB Stream Computing\nDESCRIPTION: Obtains details on subscription worker consumption, which helps track how data is being consumed by subscribers in the streaming system. Facilitates monitoring of stream data consumption. Requires DolphinDB environment.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/streaming_capital_flow_daily/04.streamStateQuery.txt#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\ngetStreamingStat().subWorkers\n```\n\n----------------------------------------\n\nTITLE: Client Class for Thread Management in C++\nDESCRIPTION: Defines a `Client` class that manages the creation and cancellation of a thread running the `appendTable` runnable. The `createAndRun` method creates a new `appendTable` instance and starts it in a new thread. The `cancelThread` method stops the thread.  It depends on the appendTable class and Thread class.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_advance.md#_snippet_25\n\nLANGUAGE: c++\nCODE:\n```\n    class Client {\n    public:\n        Client() {}\n        ~Client() {}\n        void createAndRun(Heap *heap, ConstantSP table, ConstantSP handle) { \n            SmartPointer<appendTable> append = new appendTable(heap, table, handle, this);\n            thread_ = new Thread(append);\n            if (!thread_->isStarted()) {\n                thread_->start();\n            }\n        }\n        void cancelThread(){\n            thread_->cancel();\n        }\n        \n    private:\n        ThreadSP thread_;\n    };\n```\n\n----------------------------------------\n\nTITLE: Implementing Multiprocessing Batch Processing for VWAP Calculation in Python\nDESCRIPTION: This snippet includes a function 'pool_func' which reads multiple trade CSV files for stock ticks, calculates the VWAP for each file, handles exceptions silently, and aggregates the results. It also defines a class 'multi_task_split' to split datasets into chunks based on the number of processes and CPU count for efficient multiprocessing. The components enable parallel processing of many files, improving performance on large datasets.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/DolphinDB_Python_Parser_Intro_for_Quantitative_Finance/因子实现_Python版本/委托量加权平均委托价格.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef pool_func(tick_obj, trade_path_obj):\n    single_tick_res = []\n    tmp_date = trade_path_obj.split('/')[-2]\n    # print(tmp_date)\n    tmp_date = tmp_date[0:4] + \"-\" + tmp_date[4:6] + \"-\" + tmp_date[6:8]\n    # print(tmp_date)\n    for tick in tqdm(tick_obj):\n        try:\n            df = pd.read_csv(os.path.join(trade_path_obj, tick))\n            Indicator = volumeWeightedAvgPrice(df, 60)\n            single_tick_res.append(Indicator)\n            # print(Indicator)\n            # print(\"开盘后大单净买入占比:\", Indicator)\n        except Exception as error:\n            continue\n\n    return pd.concat(single_tick_res)\n\n\nclass multi_task_split:\n\n    def __init__(self, data, processes_to_use):\n        self.data = data\n        self.processes_to_use = processes_to_use\n\n    def num_of_jobs(self):\n        return min(len(self.data), self.processes_to_use, multiprocessing.cpu_count())\n\n    def split_args(self):\n        q, r = divmod(len(self.data), self.num_of_jobs())\n        return (self.data[i * q + min(i, r): (i + 1) * q + min(i + 1, r)] for i in range(self.num_of_jobs()))\n```\n\n----------------------------------------\n\nTITLE: Creating Table Schema in PostgreSQL\nDESCRIPTION: This SQL code defines the schema for the `ticksh` table in PostgreSQL, specifying the data types for each column, such as SecurityID (varchar), TradeTime (TIMESTAMP), TradePrice (NUMERIC), and others. It is essential to create this table before migrating data from PostgreSQL to DolphinDB.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/migrate_data_from_Postgre_and_Greenplum_to_DolphinDB.md#_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\ncreate table ticksh(\n  SecurityID         varchar(20),\n  TradeTime       \t TIMESTAMP,\n  TradePrice         NUMERIC(38,4),\n  TradeQty \t         NUMERIC(38),\n  TradeAmount        NUMERIC(38,4),\n  BuyNo              NUMERIC(38),\n  SellNo             NUMERIC(38),\n  TradeIndex         NUMERIC(38),\n  ChannelNo          NUMERIC(38),\n  TradeBSFlag        varchar(10),\n  BizIndex           integer\n);\n```\n\n----------------------------------------\n\nTITLE: Loading XTP Plugin in DolphinDB\nDESCRIPTION: Code to load the XTP plugin in DolphinDB environment, with error handling to catch potential 'module already in use' exceptions when plugin is already loaded.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/xtp.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\nloadPlugin(\"xtp\")\n\n// Alternative with error handling\ntry{loadPlugin(\"xtp\")}catch(ex){print(ex)}\n```\n\n----------------------------------------\n\nTITLE: Initializing Example Financial Data Table - DolphinDB Script\nDESCRIPTION: Demonstrates creation of a sample DolphinDB table for financial data analysis. Constructs a table 'tb' with columns for trade date, security code, and value, using column-wise assignment and the 'table' function. No external dependencies are required; key parameters include trade_date (dates), secu_code (stock identifiers), and value (numeric values). Output is a DolphinDB in-memory table used for subsequent analytical queries.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_TopN.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntrade_date=sort(take(2017.01.11..2017.01.12,20))\nsecu_code=take(`600570`600000,20)\nvalue=1..20\ntb=table(trade_date,secu_code,value)\n```\n\n----------------------------------------\n\nTITLE: Historical Percentile Calculation on Matrix with mrank in DolphinDB\nDESCRIPTION: Calculates historical percentiles for each element in the indexed matrix `m` using the `mrank` function. The calculation considers a 10-year window (`10y`) and returns the percentile rank (percent=true). This example shows how to perform historical percentile calculations efficiently with matrix operations.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/window_cal.md#_snippet_20\n\nLANGUAGE: DolphinDB\nCODE:\n```\nmrank(m, true, 10y, percent=true)\n```\n\n----------------------------------------\n\nTITLE: Listing Directory Contents for TSDB (keepDuplicates=ALL)\nDESCRIPTION: This snippet presents the file structure using the `tree` command within a TSDB table when `keepDuplicates=ALL` is set. The output demonstrates the directory structure prior to updates, which informs about how the system organizes its data files.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/dolphindb_update.md#_snippet_8\n\nLANGUAGE: console\nCODE:\n```\n$ tree\n.\n├── chunk.dict\n└── machines_2\n    ├── 0_00000273\n    ├── 0_00000275\n    ├── 0_00000277\n    ├── 0_00000278\n    ├── 0_00000279\n    └── 1_00000054\n\n1 directory, 7 files\n```\n\n----------------------------------------\n\nTITLE: Aggregating Average Offer Price by Date using urllib3 in Python\nDESCRIPTION: This function sends an Elasticsearch query using `urllib3`. It performs a terms aggregation on the DATE field and calculates the average OFR price for each date. It also specifies returning the DATE, BID, and OFR fields in the source.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_64\n\nLANGUAGE: python\nCODE:\n```\nimport urllib3\nimport json\n\ndef search_4():\n    http = urllib3.PoolManager()\n    data = json.dumps({\n        \"aggs\": {\n            \"group_by_date\": {\n                \"terms\": {\n                    \"field\": \"DATE\",\n                    \"size\": 4\n                },\n                \"aggs\": {\n                    \"avg_ofr\": {\n                        \"avg\": {\"field\": \"OFR\"}\n                    }\n                }\n            }\n        },\n        \"_source\": [\"DATE\", \"BID\", \"OFR\"]\n    }).encode(\"utf-8\")\n    r = http.request(\"GET\", \"http://localhost:9200/hundred/_search\", body=data,\n                     headers={'Content-Type': 'application/json'})\n    print(r.status)\n    print(json.loads(r.data.decode()))\n```\n\n----------------------------------------\n\nTITLE: Getting Subscription Status - DolphinDB\nDESCRIPTION: This snippet retrieves and displays the subscription status using `amdQuote::getStatus(handle)`.  It is useful for confirming that subscriptions have been created correctly.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_implementation_for_AMD_Plugin.md#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\namdQuote::getStatus(handle)\n```\n\n----------------------------------------\n\nTITLE: DolphinDB Configuration File for Controller\nDESCRIPTION: Configuration settings for the DolphinDB controller node, including mode, network sites, replication, and resource limits, used to initialize and manage cluster behavior.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/gray_scale_upgrade_ha.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\nmode=controller\nlocalSite=10.10.11.2:8920:HActl44\ndfsHAMode=Raft\n dfsReplicationFactor=2\ndfsReplicaReliabilityLevel=1\ndataSync=1\nworkerNum=4\nmaxConnections=512\nmaxMemSize=8\nlanCluster=0\ndfsRecoveryWaitTime=30000\n```\n\n----------------------------------------\n\nTITLE: Cancel Thread Function for DolphinDB Plugin in C++\nDESCRIPTION: Defines the `cancelThread` function, which acts as the user interface to cancel/stop the created thread. It takes a DolphinDB resource object as input, retrieves the `Client` object, and calls the `cancelThread` method on it to stop the thread. It takes Heap and a vector of ConstantSP as parameters.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_advance.md#_snippet_28\n\nLANGUAGE: c++\nCODE:\n```\n    ConstantSP cancelThread(Heap *heap, vector<ConstantSP> &arguments) {\n        if (arguments[0]->getType() != DT_RESOURCE) {\n            throw IllegalArgumentException(\"stopRun\", \"Argument must be a resource!\");\n        }\n        Client* pClient = (Client*)(arguments[0]->getLong());\n        pClient->cancelThread();\n        return new Void();\n    }\n```\n\n----------------------------------------\n\nTITLE: Calculating N-Share VWAP (Unoptimized - Loop)\nDESCRIPTION: This code snippet demonstrates an unoptimized approach to calculating N-share VWAP using a loop within a custom aggregation function. This involves iterative calculations and can be less efficient than vectorized approaches.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ddb_sql_cases.md#_snippet_16\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndefg lastVolPx1(price, vol, bound) {\n\tsize = price.size()\n\tcumSum = 0\n\tfor(i in 0:size) {\n\t\tcumSum = cumSum + vol[size - 1 - i]\n\t\tif(cumSum >= bound) {\n\t\t\tprice_tmp = price.subarray(size - 1 - i :)\n\t\t\tvol_tmp = vol.subarray(size - 1 - i :)\n\t\t\treturn wavg(price_tmp, vol_tmp)\n\t\t}\n\t\tif(i == size - 1 && cumSum < bound) {\n\t\t\treturn wavg(price, vol)\n\t\t}\n\t}\n}\n\ntimer lastVolPx_t1 = select lastVolPx1(price, vol, 1000) as lastVolPx from t group by sym\n```\n\n----------------------------------------\n\nTITLE: Querying Results Table after Streaming Analytics in DolphinDB Script\nDESCRIPTION: Provides a simple query to view the first 15 rows of the output table where ETF values have been aggregated during the replay process. Allows users to validate streaming calculation results and check time stamps for each aggregation batch.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/historical_data_replay.md#_snippet_11\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect top 15 * from outputTable;\n```\n\n----------------------------------------\n\nTITLE: Unzipping DolphinDB Installation Package in Shell\nDESCRIPTION: Demonstrates unzipping the downloaded DolphinDB zip package to a specified directory on Linux using the unzip shell command. Important to note that the target directory path should not contain spaces or non-ASCII characters to avoid startup failures of data nodes.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ha_cluster_deployment.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nunzip dolphindb.zip -d </path/to/directory>\n```\n\n----------------------------------------\n\nTITLE: Update License Online via DolphinDB Script - Shell\nDESCRIPTION: Demonstrates how to trigger an online license update by executing the DolphinDB function `updateLicense()` in the web management interface's interactive programming console. Preconditions and constraints on license consistency are detailed separately.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ARM_standalone_deploy.md#_snippet_9\n\nLANGUAGE: Shell\nCODE:\n```\nupdateLicense()\n```\n\n----------------------------------------\n\nTITLE: Orchestrating Multiprocessing for Parallel Stock Indicator Computation in Python\nDESCRIPTION: Sets up and executes a multiprocessing pool to concurrently process stock tick files and compute the opening bid/ask volume log ratio indicator. It dynamically determines the number of processes to use based on configuration and system capabilities, splits the stock pool list accordingly, and tracks computation time. The code uses tqdm for progress visualization during multiprocessing and concatenates the results into a single dataframe. Dependencies include os, multiprocessing, tqdm, pandas, and the user-defined pool_func and multi_task_split classes.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/DolphinDB_Python_Parser_Intro_for_Quantitative_Finance/因子实现_Python版本/早盘买卖单大小比.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nn_use = 24\ntrade_path = r\"/ssd/ssd3/data/oneStock_oneFile_TL/20230201/entrust\"\nstock_pool = os.listdir(trade_path)\nprocesses_decided = multi_task_split(stock_pool, n_use).num_of_jobs()\nprint(\"进程数：\", processes_decided)\nsplit_args_to_process = list(multi_task_split(stock_pool, n_use).split_args())\nargs = [(split_args_to_process[i], trade_path) for i in range(len(split_args_to_process))]\nprint(\"#\" * 50 + \"Multiprocessing Start\" + \"#\" * 50)\nt0 = time.time()\nwith multiprocessing.Pool(processes=processes_decided) as pool:\n    res = tqdm(pool.starmap(pool_func, args))\n    print(\"cal time: \", time.time() - t0, \"s\")\n    res_combined = pd.concat(res, axis=0)\n    pool.close()\n    print(\"cal time: \", time.time() - t0, \"s\")\nprint(res_combined)\n```\n\n----------------------------------------\n\nTITLE: 查询和重置模拟撮合引擎\nDESCRIPTION: 展示如何获取未成交订单信息并重置模拟撮合引擎，为下一轮撮合做准备。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/matching_engine_simulator.md#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\nopentable = MatchingEngineSimulator::getOpenOrders(engine)\nMatchingEngineSimulator::resetMatchEngine(engine)\n```\n\n----------------------------------------\n\nTITLE: Defining File Paths DolphinDB\nDESCRIPTION: This snippet defines file paths for device data, including information and readings, as well as a path for the database.  It sets up variables for commonly accessed CSV and database paths used later in the script. It's a prerequisite for loading and manipulating data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs TimescaleDB/test_dolphindb_small.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// ----------------- 路径配置\nFP_DEVICES \t= '/data/devices/'\n\nFP_INFO \t\t= FP_DEVICES + 'csv/devices_big_device_info.csv'\nFP_READINGS \t= FP_DEVICES + 'csv/devices_big_readings.csv'\n\nFP_DB \t\t\t= FP_DEVICES + 'db/'\n```\n\n----------------------------------------\n\nTITLE: Installing ICU Library Development Package\nDESCRIPTION: This command uses the `yum install` package manager (for CentOS/RHEL) to install the ICU library development package. This is done to resolve missing dependencies related to the ICU library, enabling the proper functioning of ODBC drivers that rely on it.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ODBC_plugin_user_guide.md#_snippet_7\n\nLANGUAGE: Shell\nCODE:\n```\nyum install libicu-devel\n```\n\n----------------------------------------\n\nTITLE: Generating In-Memory Table Statistics with summary - DolphinDB Script\nDESCRIPTION: This DolphinDB snippet demonstrates how to create a simple in-memory table with columns data, value, and name, and then computes its summary statistics using the summary function. Dependencies include DolphinDB server running and necessary permissions. Key parameters: t is the input table and precision controls percentile calculation. Inputs are a sample in-memory table; output is a table with statistical metrics such as min, max, avg, std, and percentile. The snippet operates fully in-memory and is suitable for small to medium-sized datasets.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/generate_large_scale_statistics_with_summary.md#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\nn=2022\ndata=1..2022\nvalue=take(1..3,n)\nname=take(`APPLE`IBM`INTEL,n)\nt=table(data,value,name);\nres = summary(t, precision=0.001);\n```\n\n----------------------------------------\n\nTITLE: Creating Database Partition Schemas\nDESCRIPTION: Defines date range and symbol ranges for database partitioning; sets up a DolphinDB database with partitioned schemas for time and symbol.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs InfluxDB/test_dolphindb_big.txt#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\n// ----------------- 创建数据库分区\nDATE_RANGE = 2007.01.01..2008.01.01\ndate_schema = database('', VALUE, DATE_RANGE)\nsymbol_schema = database('', RANGE, buckets)\n\ndb = database(FP_DB, COMPO, [date_schema, symbol_schema])\n```\n\n----------------------------------------\n\nTITLE: Fixing Chunk Version Mismatch by Forcing Controller Synchronization with a Replica in DolphinDB Scripting Language\nDESCRIPTION: The provided Python-style DolphinDB script iterates over a list of chunk IDs to fix version mismatches by identifying the data node replica with the highest version for each chunk, and then invoking the forceCorrectVersionByReplica function on the Controller to force synchronization to that replica's version. Dependencies include having chunkIDs variable defined as a list of target chunks and the forceCorrectVersionByReplica function available on the Controller node. The operations ensure consistency of version chains and synchronize all replicas to the authoritative version but should be used cautiously to avoid data loss.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/repair_chunk_status.md#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\nfor(chunk in chunkIDs){          \nnodes = exec  top 1 node from pnodeRun(getAllChunks) where chunkId=chunk order by version desc\nrpc(getControllerAlias(), forceCorrectVersionByReplica{chunk, nodes[0]})\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Stock Minute Processing Metrics from DolphinDB Table\nDESCRIPTION: This snippet performs a SELECT query to retrieve all records related to a specific stock symbol (\"000001.SZ\") from a processed snapshot aggregation table with 1-minute granularity. It is used for analyzing stock-specific minute-level metrics generated during replay or real-time processing, with inputs being table data and output being filtered records.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_auto_sub.md#_snippet_8\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect * from snapshotAggr1min where SecurityID=\"000001.SZ\"\n```\n\n----------------------------------------\n\nTITLE: Loading XGBoost Plugin (DolphinDB Script)\nDESCRIPTION: Attempts to load the DolphinDB XGBoost plugin from the specified path. This plugin provides functions for loading models and performing predictions. Requires the plugin file to be available on the server at the given path.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/metacode_derived_features/metacode_derived_features_streaming.txt#_snippet_7\n\nLANGUAGE: DolphinDB\nCODE:\n```\ntry{\n\tloadPlugin(getHomeDir()+\"/plugins/xgboost/PluginXgboost.txt\")\n}\ncatch(ex){\n\tprint(ex)\n}\n```\n\n----------------------------------------\n\nTITLE: Canceling Stream Data Subscriptions Using unsubscribeTable in DolphinDB\nDESCRIPTION: Describes how to cancel an existing stream data subscription uniquely identified by topic (tableName and actionName). Provides examples for unsubscribing from local and remote tables with optional retention of current offset for future resubscription continuity via removeOffset flag. Also shows removing stored offset state from subscriber memory explicitly with removeTopicOffset. Inputs are topic parameters (server alias, tableName, actionName) and removeOffset boolean. Outputs are cancellation of subscription and optionally offset data removal. Requires existing active subscription topics to operate correctly.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/streaming_tutorial.md#_snippet_8\n\nLANGUAGE: DolphinDB\nCODE:\n```\nunsubscribeTable(tableName=\"pubTable\", actionName=\"act1\")\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nunsubscribeTable(server=\"NODE_1\", tableName=\"pubTable\", actionName=\"act1\")\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nunsubscribeTable(tableName=\"pubTable\", actionName=\"act1\", removeOffset=false)\n```\n\nLANGUAGE: DolphinDB\nCODE:\n```\nremoveTopicOffset(topic)\n```\n\n----------------------------------------\n\nTITLE: Updating Controller and Agent Configuration Files on Server 4\nDESCRIPTION: Uses sed with extended regex to replace existing controller and agent node entries in 'controller.cfg' and 'agent.cfg' respectively on server 4, updating cluster topology for scaling.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/service_deployment_and_migration.md#_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nsed -i.bak -E -e 's/([0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+):([0-9]+):controller1/172.0.0.4:9912:controller4/' controller.cfg\nsed -i.bak -E -e 's/([0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+):([0-9]+):agent1/172.0.0.4:9910:agent4/' agent.cfg\nsed -i '/^sites/s/$/,172.0.0.4:9912:controller4:controller,172.0.0.5:9913:controller5:controller/' agent.cfg\n\necho 172.0.0.4:9912:controller4,controller >> cluster.nodes\necho 172.0.0.4:9913:controller5,controller >> cluster.nodes\n```\n\n----------------------------------------\n\nTITLE: Cumulative Sum with Specified Time-Length Step Using Bar and CGroup By in DolphinDB SQL\nDESCRIPTION: Implements cumulative aggregation with step size defined by a fixed time-length (5 seconds) by using bar function combined with cgroup by in SQL. Input data is a table with time and volume; the query sums volume grouped by 5-second bars ordered by time. This approach enables cumulative window calculation with non-row-based step intervals. Versions supporting bar and cgroup by functionality are required. Outputs sums of volume per time bar window.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/window_cal.md#_snippet_11\n\nLANGUAGE: DolphinDB\nCODE:\n```\nt=table(2021.11.01T10:00:00..2021.11.01T10:00:04 join 2021.11.01T10:00:06..2021.11.01T10:00:10 as time,1..10 as vol)\nselect sum(vol) from t cgroup by bar(time, 5s) as time order by time\n\n# output\n\ntime                sum_vol\n------------------- -------\n2021.11.01T10:00:00 15     \n2021.11.01T10:00:05 45     \n2021.11.01T10:00:10 55  \n```\n\n----------------------------------------\n\nTITLE: Creating QB Trade Table with TSDB Engine Partitioned by Day in DolphinDB\nDESCRIPTION: This DolphinDB snippet creates a database and table configured for QB trade data using the TSDB storage engine with daily partitions over October 2023. The table schema includes trade details such as prices, volumes, trade and contributor IDs, market times, yield, status and settlement speed. Partitioning by market data time and sorting by security ID supports efficient time and bond-based queries.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_practices_for_partitioned_storage.md#_snippet_20\n\nLANGUAGE: DolphinDB\nCODE:\n```\ncreate database \"dfs://QB_TRADE\"\npartitioned by VALUE(2023.10.01..2023.10.31)\nengine='TSDB'\n\ncreate table \"dfs://QB_TRADE\".\"lastTradeTable\"(\n\tSECURITYID SYMBOL\n\tBONDNAME SYMBOL\n\tSENDINGTIME TIMESTAMP\n\tCONTRIBUTORID SYMBOL\n\tMARKETDATATIME TIMESTAMP\n\tMODIFYTIME SECOND\n\tDISPLAYLISTEDMARKET SYMBOL\n\tEXECID STRING\n\tDEALSTATUS INT\n\tTRADEMETHOD INT\n\tYIELD DOUBLE\n\tTRADEPX DOUBLE\n\tPRICETYPE INT\n\tTRADEPRICE DOUBLE\n\tDIRTYPRICE DOUBLE\n\tSETTLSPEED STRING\n)\npartitioned by MARKETDATATIME,\nsortColumns=[`SECURITYID,`MARKETDATATIME]\n```\n\n----------------------------------------\n\nTITLE: Elasticsearch Filtering by Floating-Point Range - Python\nDESCRIPTION: This Python function queries an Elasticsearch index ('elastic') to retrieve documents where the 'pre_close' field falls within a specified range (greater than 25 and less than 35). The function uses the `elasticsearch` library and scrolling to handle potentially large result sets. It prints the scroll ID, the total number of hits, and the scroll size during each scroll iteration.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_29\n\nLANGUAGE: Python\nCODE:\n```\ndef search_4():\n    es = Elasticsearch(['http://localhost:9200/'])\n    page = es.search(\n        index='elastic',\n        doc_type='type',\n        scroll='2m',\n        size=10000,\n        body={\n            \"query\": {\n                \"constant_score\": {\n                    \"filter\": {\n                        \"range\": {\n                            \"pre_close\": {\n                                \"gt\": 25,\n                                \"lt\": 35\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    )\n    sid = page['_scroll_id']\n    scroll_size = page['hits']['total']\n\n    print(sid)\n    print(scroll_size)\n    # Start scrolling\n    while (scroll_size > 0):\n        print(\"Scrolling...\")\n        page = es.scroll(scroll_id=sid, scroll='2m')\n        # Update the scroll ID\n        sid = page['_scroll_id']\n        # Get the number of results that we returned in the last scroll\n        scroll_size = len(page['hits']['hits'])\n        print(\"scroll size: \" + str(scroll_size))\n```\n\n----------------------------------------\n\nTITLE: Connecting to Oracle via ODBC in DolphinDB - DolphinDB\nDESCRIPTION: Creates an ODBC connection to Oracle using the DSN 'orac', specifying the database type as Oracle. This connection object is then used with DolphinDB's ODBC integration functions. Adjust DSN and other parameters as appropriate for your configuration.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Oracle_to_DolphinDB.md#_snippet_12\n\nLANGUAGE: DolphinDB\nCODE:\n```\nconn = odbc::connect(\"Dsn=orac\", `Oracle)\n\n```\n\n----------------------------------------\n\nTITLE: Transforming and Synchronizing Data from Oracle to DolphinDB - DolphinDB\nDESCRIPTION: Defines transformation and synchronization functions to query tick-level data from Oracle through ODBC, cast columns to appropriate types, and insert results into a DolphinDB table. Both single and multi-day batch jobs are shown, utilizing DolphinDB's job submission for parallel processing. Requires the ODBC connection, target database/table name, and optionally a date for filtering.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Oracle_to_DolphinDB.md#_snippet_13\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndef transForm(mutable msg){\n\tmsg.replaceColumn!(`TradeQty, int(msg[`TradeQty]))\n\tmsg.replaceColumn!(`BuyNo, int(msg[`BuyNo]))\n\tmsg.replaceColumn!(`SellNo, int(msg[`SellNo]))\n\tmsg.replaceColumn!(`ChannelNo, int(msg[`ChannelNo]))\n\tmsg.replaceColumn!(`TradeIndex, int(msg[`TradeIndex]))\n\tmsg.replaceColumn!(`BizIndex, int(msg[`BizIndex]))\n\treturn msg\n}\n\ndef syncData(conn, dbName, tbName, dt){\n\tsql = \"select SecurityID, TradeTime, TradePrice, TradeQty, TradeAmount, BuyNo, SellNo, ChannelNo, TradeIndex, TradeBSFlag, BizIndex from ticksh\"\n\tif(!isNull(dt)) {\n\t\tsql = sql + \" WHERE trunc(TradeTime) = TO_DATE('\"+dt+\"', 'yyyy.MM.dd')\"\n\t}\n    odbc::query(conn,sql, loadTable(dbName,tbName), 100000, transForm)\n}\n\ndbName=\"dfs://TSDB_tick\"\ntbName=\"tick\"\nsyncData(conn, dbName, tbName, NULL)\n\n```\n\n----------------------------------------\n\nTITLE: Granting TEST_EXEC Permission - DolphinDB\nDESCRIPTION: This snippet grants a user TEST_EXEC permission, which is required to execute unit tests or use the `test` function. Requires admin privileges to create user.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Permission_Management.md#_snippet_41\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(`admin, `123456)\ncreateUser(`user1, \"123456\");\ngrant(`user1,TEST_EXEC)\nlogin(`user1, \"123456\")\ntest(\"test.txt\")\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting ClickHouse ODBC Driver (Linux)\nDESCRIPTION: This snippet demonstrates how to download and extract the ClickHouse ODBC driver on a Linux system. It creates a directory, navigates into it, downloads the ZIP file, and then extracts its contents.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ClickHouse_to_DolphinDB.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmkdir <savedir>\ncd <savedir>\nwget https://github.com/ClickHouse/clickhouse-odbc/releases/download/v1.2.1.20220905/clickhouse-odbc-linux.zip\nunzip clickhouse-odbc-linux.zip\n```\n\n----------------------------------------\n\nTITLE: Initializing DolphinDB Session\nDESCRIPTION: This snippet initializes the DolphinDB session by logging in with specified credentials, clearing the session cache, and undefining all user-defined variables. This ensures a clean and predictable environment for script execution.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/metacode_derived_features/metacode_derived_features.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(\"admin\", \"123456\")\nclearAllCache()\nundef(all)\ngo\n```\n\n----------------------------------------\n\nTITLE: 结果验证N股VWAP\nDESCRIPTION: 验证两种实现方式计算结果的一致性。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_SQL_Case_Tutorial.md#_snippet_23\n\nLANGUAGE: DolphinDB\nCODE:\n```\neach(eqObj, lastVolPx_t1.values(), lastVolPx_t2.values()) // true\n```\n\n----------------------------------------\n\nTITLE: Library Imports and Pandas Options Configuration\nDESCRIPTION: This snippet imports necessary libraries such as pandas, numpy, itertools, os, multiprocessing, time, warnings, tqdm, and datetime. It also configures pandas display options for better visualization of DataFrames and suppresses warnings to avoid clutter during execution.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/DolphinDB_Python_Parser_Intro_for_Quantitative_Finance/因子实现_Python版本/价格变动与一档量差的回归系数.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport pandas as pd\nimport numpy as np\nimport itertools\nfrom functools import reduce\nimport os\nimport multiprocessing\nimport time\nimport warnings\nfrom tqdm import tqdm\nimport datetime\n\nwarnings.filterwarnings(\"ignore\")\n\npd.options.display.width = 1200\npd.options.display.max_colwidth = 100\npd.options.display.max_columns = 10\npd.options.mode.chained_assignment = None\n```\n\n----------------------------------------\n\nTITLE: Editing Cluster Nodes Information File with Shell\nDESCRIPTION: This snippet edits the cluster.nodes configuration file using Vim on the server. It lists all nodes in the high-availability cluster including controller, agent, data, and compute nodes by defining their IP addresses, ports, aliases, and roles. The entries must be uniform and consistent across all servers (P1, P2, P3) to ensure proper cluster operation. Node aliases are case sensitive and unique within the cluster.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ha_cluster_deployment.md#_snippet_7\n\nLANGUAGE: Shell\nCODE:\n```\nvim ./cluster.nodes\n```\n\nLANGUAGE: Shell\nCODE:\n```\nlocalSite,mode\n10.0.0.80:8800:controller1,controller\n10.0.0.81:8800:controller2,controller\n10.0.0.82:8800:controller3,controller\n10.0.0.80:8801:agent1,agent\n10.0.0.80:8802:datanode1,datanode\n10.0.0.80:8803:computenode1,computenode\n10.0.0.81:8801:agent2,agent\n10.0.0.81:8802:datanode2,datanode\n10.0.0.81:8803:computenode2,computenode\n10.0.0.82:8801:agent3,agent\n10.0.0.82:8802:datanode3,datanode\n10.0.0.82:8803:computenode3,computenode\n```\n\n----------------------------------------\n\nTITLE: Response Message Format\nDESCRIPTION: This is a high-level structure that describes the generic format for response messages received from the DolphinDB server. It includes the session ID, the number of returned objects, byte order information, and the execution result. The specific data returned depends on the command executed.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/api_protocol.md#_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n| SESSIONID | 空格 | 返回对象数量 | 空格 | 大小端 | 换行符(LF)\n| 执行结果 | 换行符(LF)\n```\n\n----------------------------------------\n\nTITLE: Logging into DolphinDB Account - DolphinDB\nDESCRIPTION: Logs into a DolphinDB server using the provided username and password. This ensures all subsequent operations have the necessary permissions. The snippet requires the login credentials (here \"admin\" and \"123456\") to match the server's user configuration.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/machine_learning_volatility/04.streamComputing.txt#_snippet_1\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(\"admin\", \"123456\")\n\n```\n\n----------------------------------------\n\nTITLE: Creating MySQL Table with Different Time Types\nDESCRIPTION: Creates a MySQL table with DATE, TIME, and TIMESTAMP columns to demonstrate the different behaviors of these time-related data types with respect to time zones.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/timezone.md#_snippet_12\n\nLANGUAGE: SQL\nCODE:\n```\nmysql> USE testdb;\nmysql> CREATE TABLE testTable(\n    -> date DATE NOT NULL,\n    -> time TIME NOT NULL,\n    -> ts TIMESTAMP NOT NULL\n    -> );\nQuery OK, 0 rows affected (0.11 sec)\n```\n\n----------------------------------------\n\nTITLE: Retrieve DFS Metadata Directory Path\nDESCRIPTION: DolphinDB RPC call to obtain the directory path where DFS metadata is stored, important for backup and recovery during node upgrade.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/gray_scale_upgrade_ha.md#_snippet_6\n\nLANGUAGE: DolphinDB\nCODE:\n```\nrpc(getControllerAlias(),getConfig{`dfsMetaDir})\n```\n\n----------------------------------------\n\nTITLE: Dummy Output Class Definition in C++\nDESCRIPTION: Defines a `DummyOutput` class, derived from the `Output` class, to handle output within a DolphinDB plugin thread. This class overrides the virtual methods of the `Output` class with empty implementations or implementations that return `true` or `OK`. This is done as output is typically not handled by the plugin.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_advance.md#_snippet_23\n\nLANGUAGE: c++\nCODE:\n```\n    class DummyOutput: public Output{\n    public:\n        virtual bool timeElapsed(long long nanoSeconds){return true;}\n        virtual bool write(const ConstantSP& obj){return true;}\n        virtual bool message(const string& msg){return true;}\n        virtual void enableIntermediateMessage(bool enabled) {}\n        virtual IO_ERR done(){return OK;}\n        virtual IO_ERR done(const string& errMsg){return OK;}\n        virtual bool start(){return true;}\n        virtual bool start(const string& message){return true;}\n        virtual IO_ERR writeReady(){return OK;}\n        virtual ~DummyOutput(){}\n        virtual OUTPUT_TYPE getOutputType() const {return STDOUT;}\n        virtual void close() {}\n        virtual void setWindow(INDEX index,INDEX size){};\n        virtual IO_ERR flush() {return OK;}\n    };\n```\n\n----------------------------------------\n\nTITLE: 使用 imtUpdateChunkVersionOnDataNode 函数修改数据节点上的chunk版本\nDESCRIPTION: 此代码示例演示了如何在数据节点上调用 imtUpdateChunkVersionOnDataNode 函数，通过传入唯一逄块ID和目标版本号，直接修改目标chunk的元数据版本，实现版本同步。该函数不支持分布式操作，仅作用于单一数据节点，确保版本一致性。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/repair_chunk_status.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\nimtUpdateChunkVersionOnDataNode(\"530076e2-c6e9-cf97-8d49-9e5faac17325\", 1)\n```\n\n----------------------------------------\n\nTITLE: Retrieve Metadata Directory Path from Configuration\nDESCRIPTION: DolphinDB RPC call to get the directory path for storage metadata of data nodes, used for backup before upgrade.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/gray_scale_upgrade_ha.md#_snippet_5\n\nLANGUAGE: DolphinDB\nCODE:\n```\npnodeRun(getConfig{`chunkMetaDir})\n```\n\n----------------------------------------\n\nTITLE: Reloading Shell Profile to Activate Environment Variables - Shell\nDESCRIPTION: Runs the 'source' command to apply the updated environment settings from .bashrc to the current shell session, enabling ODBC drivers to use the configured paths and parameters. Should be run after modifying environment variable settings.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Oracle_to_DolphinDB.md#_snippet_7\n\nLANGUAGE: Shell\nCODE:\n```\nsource ~/.bashrc\n\n```\n\n----------------------------------------\n\nTITLE: Creating HR Database with Partitioning in DolphinDB\nDESCRIPTION: Establishes a new DolphinDB database named \"hr\" with hash partitioning on the first 10 integers. This forms the foundational storage space for subsequent HR data tables.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/Standard_SQL_in_DolphinDB/create_db_table_sql.txt#_snippet_0\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndrop database if exists \"dfs://hr\"\ncreate database \"dfs://hr\" partitioned by HASH([INT, 10])\n```\n\n----------------------------------------\n\nTITLE: Starting DolphinDB Controllers and Agents with Shell Scripts\nDESCRIPTION: Starts controller and agent processes for all cluster nodes using predefined shell scripts ('startController.sh' and 'startAgent.sh'). Typically executed from the clusterDemo directory on each server, assuming necessary permissions and environment variables. Output is cluster services running and ready to accept connections.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/service_deployment_and_migration.md#_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\nsh startController.sh\nsh startAgent.sh\n```\n\n----------------------------------------\n\nTITLE: Starting Controller and Agent on Dual Servers\nDESCRIPTION: Commands to start controller and agent processes on two servers for a dual-server pseudo-high availability DolphinDB cluster.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/service_deployment_and_migration.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# 在服务器一（172.0.0.1）下面执行如下命令\ncd dolphindb_1/server/clusterdemo;\nsh startController.sh\nsh startAgent.sh\n\ncd dolphindb_3/server/clusterdemo;\nsh startController.sh\n\n# 在服务器二（172.0.0.2）下面执行如下命令\ncd dolphindb_2/server/clusterdemo;\nsh startController.sh\nsh startAgent.sh\n```\n\n----------------------------------------\n\nTITLE: Configuring DolphinScheduler Environment Variables for MySQL - Shell Script\nDESCRIPTION: This configuration snippet sets environment variables necessary for DolphinScheduler to connect to a MySQL database. Key variables include DATABASE, SPRING_PROFILES_ACTIVE, and SPRING_DATASOURCE_* parameters, which define the database type, JDBC URL, username, and password. Users must replace placeholders {user} and {password} with actual MySQL credentials. This file is critical for enabling DolphinScheduler's metadata persistence and stable operation.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/dolphinscheduler_integration.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\nexport DATABASE=mysql\nexport SPRING_PROFILES_ACTIVE=${DATABASE}\nexport SPRING_DATASOURCE_URL=\"jdbc:mysql://127.0.0.1:3306/dolphinscheduler?useUnicode=true&characterEncoding=UTF-8&useSSL=false\"\nexport SPRING_DATASOURCE_USERNAME={user}\nexport SPRING_DATASOURCE_PASSWORD={password}\n```\n\n----------------------------------------\n\nTITLE: Variable Assignments and Swapping Values in DolphinDB - DolphinDB\nDESCRIPTION: Shows basic imperative programming concepts in DolphinDB with variable assignment, increment by adding a scalar, and swapping multiple variables’ values in one statement. Demonstrates support for vector assignment including simultaneous assignments and in-place operations. Input variables 'x' and 'y' are assigned numeric vectors with arithmetic operations and swaps illustrating vector manipulation.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/hybrid_programming_paradigms.md#_snippet_3\n\nLANGUAGE: DolphinDB\nCODE:\n```\nx = 1 2 3\ny = 4 5\ny += 2\nx, y = y, x //swap the value of x and y\nx, y =1 2 3, 4 5\n```\n\n----------------------------------------\n\nTITLE: Setting Up Model and Data Paths for Simulation in DolphinDB\nDESCRIPTION: Configures the file paths for the pre-trained volatility prediction model and test data to quickly reproduce the stream processing example. Users must modify these paths based on their server environment.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/machine_learning_volatility.md#_snippet_7\n\nLANGUAGE: dolphindb\nCODE:\n```\n/**\nmodified location 1: modelSavePath, csvDataPath\n*/\nmodelSavePath = \"/hdd/hdd9/machineLearning/realizedVolatilityModel_1.30.18.bin\"\n//modelSavePath = \"/hdd/hdd9/machineLearning/realizedVolatilityModel_2.00.6.bin\"\ncsvDataPath = \"/hdd/hdd9/machineLearning/testSnapshot.csv\"\n```\n\n----------------------------------------\n\nTITLE: Transferring DolphinDB Data Directory to New Node with scp (Shell)\nDESCRIPTION: Copies DolphinDB's disk data directory (such as /ssd/ssd1/dolphindb3) from one server to another using 'scp'. This step is performed during cluster scaling or node recovery and requires write access on both ends. Ensures the data structure remains intact on the target server.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/service_deployment_and_migration.md#_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\nscp /ssd/ssd1/dolphindb3 root@172.0.0.3:/ssd/ssd1/dolphindb3\n```\n\n----------------------------------------\n\nTITLE: Upgrading DolphinDB Linux Offline\nDESCRIPTION: This command executes the upgrade script in the specified directory, after the user has manually downloaded a DolphinDB installation package. This allows you to upgrade DolphinDB offline by downloading the installation package from the DolphinDB website.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/standalone_server.md#_snippet_15\n\nLANGUAGE: sh\nCODE:\n```\n./upgrade.sh\n```\n\n----------------------------------------\n\nTITLE: Node Version Check Script\nDESCRIPTION: Shell script to verify current version of each node in the cluster, used before upgrade to ensure all nodes are running the expected software version.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/gray_scale_upgrade_ha.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\nnode\tvalue\n0\tnode43\t2.00.7.4 2022.12.08\n1\tnode44\t2.00.7.4 2022.12.08\n2\tnode45\t2.00.7.4 2022.12.08\n```\n\n----------------------------------------\n\nTITLE: Viewing Hardware Usage Log Format in CSV\nDESCRIPTION: Example of the CSV format used for logging CPU and memory usage information by user in the hardware.log file. Each record includes timestamp, user ID, CPU usage (thread count), and memory usage (bytes).\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/user_level_resource_tracking.md#_snippet_0\n\nLANGUAGE: csv\nCODE:\n```\ntimestamp,userId,cpu,memory\n2023.12.15T08:38:17.219438396,guest,0,32\n2023.12.15T08:38:17.219438396,admin,0,16\n2023.12.15T08:38:18.219753330,guest,0,32\n...\n2023.12.15T08:38:21.220409418,guest,0,32\n```\n\n----------------------------------------\n\nTITLE: Login Operation in DolphinDB\nDESCRIPTION: Performs a login operation using hardcoded username 'admin' and password '123456'. This likely authenticates the user session at the start of the script execution. No dependencies or outputs beyond authentication state.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/fund_factor_contrasted_by_py/fund_factor_by_ddb/fund_factor_ten.txt#_snippet_13\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(\"admin\",\"123456\")\n```\n\n----------------------------------------\n\nTITLE: Creating MySQL Database for Testing\nDESCRIPTION: SQL command to create a test database named 'basicinfo' for demonstrating the data synchronization process.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Debezium_and_Kafka_data_sync.md#_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\ncreate database basicinfo;\n```\n\n----------------------------------------\n\nTITLE: Querying TAQ Data - Druid\nDESCRIPTION: These Druid SQL queries perform similar operations to the DolphinDB examples: selecting data, counting, aggregating (sum, average, max, min), grouping, and filtering on timestamp and other dimensions. Dependencies: Druid installation, TAQ data ingested into Druid.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_Druid_appendix.md#_snippet_10\n\nLANGUAGE: Druid\nCODE:\n```\n//根据股票代码和日期查找并取前1000条\nselect * from TAQ where SYMBOL = 'IBM' and __time = TIMESTAMP'2007-08-10 00:00:00' limit 1000;\n\n//根据部分股票代码和时间、报价范围计数\nselect count(*) from TAQ where __time = TIMESTAMP'2007-08-10 00:00:00' and SYMBOL in ('GOOG', 'THOO', 'IBM') and BID>0 and OFR>BID;\n\n//按股票代码分组并按照卖出与买入价格差排序\nselect sum(OFR-BID) as spread from TAQ where __time = TIMESTAMP'2007-08-27 00:00:00' group by SYMBOL order by spread;\n\n//按时间和报价范围过滤，按小时分组并计算均值\nselect avg((OFR-BID)/(OFR+BID)) as speard from TAQ where __time = TIMESTAMP'2007-08-01 00:00:00' and BID > 0 and OFR > BID group by hour(__time);\n\n//按股票代码分组并计算最大卖出与最小买入价之差\nselect max(OFR) - min(BID) as gap from TAQ where __time = TIMESTAMP'2007-08-03 00:00:00' and BID > 0 and OFR > BID group by SYMBOL;\n\n//对最大买入价按股票代码、小时分组并排序\nselect max(BID) as mx from TAQ where __time =TIMESTAMP'2007-08-27 00:00:00' group by SYMBOL, __time having sum(BIDSIZ)>0 order by SYMBOL, __time;\n\n//对买入与卖出价均值的最大值按股票代码、小时分组\nselect max(OFR+BID)/2.0 as mx from TAQ where __time = TIMESTAMP'2007-08-01 00:00:00' group by SYMBOL, __time;\n```\n\n----------------------------------------\n\nTITLE: Stopping DolphinScheduler Standalone Server - Shell\nDESCRIPTION: This shell command stops the DolphinScheduler standalone server daemon, useful for maintenance or reconfiguration. It gracefully shuts down the running server instance.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/dolphinscheduler_integration.md#_snippet_5\n\nLANGUAGE: Shell\nCODE:\n```\nbash ./bin/dolphinscheduler-daemon.sh stop standalone-server\n```\n\n----------------------------------------\n\nTITLE: Installing unixODBC on Linux for ODBC connectivity\nDESCRIPTION: Command-line instructions to install unixODBC and verify installation success, establishing the foundation for ODBC connections.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Migrate_data_from_Redshift_to_DolphinDB.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\n# Ubuntu 安装 ODBC\napt-get install unixodbc unixodbc-dev\n\n# CentOS 安装 ODBC\nyum install unixODBC  unixODBC-devel\n\n# 检查是否安装成功\nodbcinst -j\n```\n\n----------------------------------------\n\nTITLE: DataX Reader Configuration with WHERE Clause for Incremental Load\nDESCRIPTION: This snippet shows how to add a WHERE clause in the DataX JSON configuration file to filter data based on a date for incremental data loading. It selects data for the previous day.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ClickHouse_to_DolphinDB.md#_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n\"reader\": {\n    \"parameter\": {\n        \"username\": \"default\",\n        \"column\": [\"SecurityID\", \"toString(TradeTime)\", \"TradePrice\", \"TradeQty\", \"TradeAmount\", \"BuyNo\", \"SellNo\", \"ChannelNo\", \"TradeIndex\", \"TradeBSFlag\", \"BizIndex\"],\n        \"connection\": [{\n            \"table\": [\"ticksh\"],\n            \"jdbcUrl\": [\"jdbc:clickhouse://127.0.0.1:8123/migrate\"]\n        }],\n        \"password\": \"123456\",\n        \"where\": \"toDate(TradeTime) = date_sub(day,1,today())\"\n    },\n    \"name\": \"clickhousereader\",\n}\n```\n\n----------------------------------------\n\nTITLE: Executing Data Generation Workflow - Dolphindb\nDESCRIPTION: This block sets up the parameters required for the data generation and writing process, including login credentials, frequency, number of machines and metrics, partition schemes, start date, duration, and the number of threads. It then calls the `mainJob` function with these parameters to execute the workflow.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/singleValueModeWrite.txt#_snippet_5\n\nLANGUAGE: Dolphindb\nCODE:\n```\nlogin(\"admin\",\"123456\")\n\nfreqPerDay=86400\nnumMachines=100\nnumMetrics=50\nnumMachinesPerPartition=2\nnumIdPerPartition=numMachinesPerPartition*numMetrics\nps1=2020.09.01..2020.12.31\nps2=(numMetrics*numMachinesPerPartition)*(0..(numMachines/numMachinesPerPartition))+1\nid =1..(numMachines*numMetrics)\nstartDay=2020.09.01\n//写入多少天的数据\ndays = 5\n//多少个线程并行写入\nthreads = 20\n\nmainJob(id, startDay, days, ps1, ps2, freqPerDay, numIdPerPartition, threads)\n```\n\n----------------------------------------\n\nTITLE: Starting DolphinDB Server (Bash)\nDESCRIPTION: Starts the DolphinDB server process in the background using `nohup`, redirecting standard output and error to `single.nohup`. The `-console 0` argument disables the interactive console.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_implementation_for_NSQ_Plugin.md#_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\nnohup ./dolphindb -console 0 > single.nohup 2>&1 &\n```\n\n----------------------------------------\n\nTITLE: Allowing NTP Traffic via Linux Firewall (firewalld) (console)\nDESCRIPTION: Add permanent firewall rules for NTP traffic using `firewall-cmd` and reload the firewall configuration. This step ensures cluster nodes can synchronize time. Requires appropriate system privileges and the `firewalld` service running.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/deploy_dolphindb_on_new_server.md#_snippet_6\n\nLANGUAGE: console\nCODE:\n```\n# firewall-cmd --add-service=ntp --permanent\n# firewall-cmd --reload\n\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake build for DolphinDB PluginMsum shared library\nDESCRIPTION: This CMake script sets up the build environment for a DolphinDB plugin named PluginMsum. It configures C++11 compilation, platform-specific definitions, includes necessary headers, and links against the DolphinDB library.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin/Msum/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.0)\n\nproject(PluginMsum)\n\nadd_definitions(-D_GLIBCXX_USE_CXX11_ABI=0)\n\nLINK_DIRECTORIES(\"../libs\")\nINCLUDE_DIRECTORIES(\"../include\")\nif(WIN32)\n    add_definitions(-DWINDOWS)\nelseif(UNIX)\n    add_definitions(-DLINUX)\nendif()\n\nadd_compile_options( \"-std=c++11\" \"-fPIC\" \"-Wall\" \"-Werror\")\n\nadd_library(PluginMsum SHARED\n    \"./src/Msum.cpp\"\n)\ntarget_link_libraries(PluginMsum DolphinDB)\n```\n\n----------------------------------------\n\nTITLE: Setting LD_LIBRARY_PATH for NSQ Plugin (Bash)\nDESCRIPTION: Sets the LD_LIBRARY_PATH environment variable to include the directory containing the NSQ plugin's shared libraries. This allows the DolphinDB server to locate and load the plugin dependencies. Replace `/DolphinDB/server/plugins/nsq` with the actual installation path.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/best_implementation_for_NSQ_Plugin.md#_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\nexport LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:/DolphinDB/server/plugins/nsq\"\n```\n\n----------------------------------------\n\nTITLE: Schur Decomposition in DolphinDB\nDESCRIPTION: Demonstrates Schur decomposition of a matrix in DolphinDB using the `schur` function.  The function decomposes a matrix X into U * T * U^H, where U is a unitary matrix and T is an upper triangular matrix. This shows the basic schur decomposition.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/matrix.md#_snippet_27\n\nLANGUAGE: DolphinDB\nCODE:\n```\n>m=matrix([2 5 7 5, 5 2 5 4, 8 2 6 4, 7 8 6 8]);\n>m;\n#0 #1 #2 #3\n-- -- -- --\n2  5  8  7 \n5  2  2  8 \n7  5  6  6 \n5  4  4  8 \n\n>t,u=schur(m);\n>t;\n#0       #1        #2        #3       \n-------- --------- --------- ---------\n21.16354 -1.073588 -0.473548 -4.270044\n0        -4.306007 -1.391659 2.039609 \n0        0         -0.995651 -2.879786\n0        0         0         2.138117 \n>u;\n#0       #1        #2        #3       \n-------- --------- --------- ---------\n0.52214  0.818236  0.198151  0.136364 \n0.401387 -0.461653 0.785756  0.091394 \n0.568479 -0.320408 -0.540423 0.531143 \n0.493041 -0.121263 -0.226421 -0.831228\n\n>u**t**u.transpose();\n#0 #1 #2 #3\n-- -- -- --\n2  5  8  7 \n5  2  2  8 \n7  5  6  6 \n5  4  4  8 \n\n```\n\n----------------------------------------\n\nTITLE: DolphinDB Grant Specific Over Global Deny (Legacy)\nDESCRIPTION: This example shows that in DolphinDB versions prior to 1.30.21 and 2.00.9, a `grant` on a specific table could override a global `deny`.  However, newer versions throw an error. The code illustrates the behavior in older versions where user1 would have read access only to `dfs://test/pt` and no other tables.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Permission_Management.md#_snippet_13\n\nLANGUAGE: DolphinDB\nCODE:\n```\nlogin(\"admin\", \"123456\")\ncreateUser(\"user1\",\"123456\")\ndbName = \"dfs://test\"\nif(existsDatabase(dbName)){\n        dropDatabase(dbName)\n}\nt = table(1..10 as id , rand(100, 10) as val)\ndb=database(dbName, VALUE, 1..10)\npt=  db.createPartitionedTable(t, \"pt\", \"id\")\npt1=  db.createPartitionedTable(t, \"pt1\", \"id\")\npt.append!(t)\npt1.append!(t)\n\ndeny(\"user1\", TABLE_READ, \"*\")\n//新版本中执行grant报错 'Invalid grant: grant [dfs://test/pt] and [deny *] are in conflict'\ngrant(\"user1\", TABLE_READ, dbName+\"/pt\")\nlogin(\"user1\", \"123456\")\nselect * from loadTable(dbName, \"pt\")//老版本有读 dbName+\"/pt\" 的权限\nselect * from loadTable(dbName, \"pt1\")//老版本没有读其他表的权限\n```\n\n----------------------------------------\n\nTITLE: Downloading conda packages for offline use\nDESCRIPTION: Creates a reusable tar.gz archive of downloaded conda packages for an environment with specified packages, enabling offline installation. Dependencies include conda CLI and tar for archiving.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/python_api_install_offline.md#_snippet_2\n\nLANGUAGE: Shell Script\nCODE:\n```\nconda create -n test38 numpy=1.22.3 pandas python=3.8.13 --download-only\n# Compress the pkgs directory\ntar -zcvf pkgs.tar.gz pkgs/\nmd5sum pkgs.tar.gz > pkgs.tar.gz.md5\n```\n\n----------------------------------------\n\nTITLE: Schur Decomposition with sorting (rhp) in DolphinDB\nDESCRIPTION: Demonstrates Schur decomposition of a matrix in DolphinDB using the `schur` function with the `'rhp'` sort option (right half plane). The eigenvalues are sorted based on being in the right half plane (e > 0). sdim shows the number of eigenvalues satisfying the sort criteria.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/matrix.md#_snippet_29\n\nLANGUAGE: DolphinDB\nCODE:\n```\n>t,u, sdim=schur(m,'rhp') //'rhp':(e < 0.0)\n>t;\n#0       #1        #2        #3       \n-------- --------- --------- ---------\n21.16354 -3.020883 0.002732  3.237958 \n0        2.138117  2.443445  -2.818711\n0        0         -4.306007 -0.688714\n0        0         0         -0.995651\n>u;\n#0       #1        #2        #3       \n-------- --------- --------- ---------\n0.52214  0.258617  -0.777021 -0.238171\n0.401387 -0.597888 0.267022  -0.640405\n0.568479 0.594002  0.567898  0.03853  \n0.493041 -0.472026 -0.049293 0.729158 \n>sdim;\n2\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple Recipients for Bulk Email in DolphinDB\nDESCRIPTION: Creates a string vector containing multiple email addresses to be used as recipients for a bulk email operation.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/send_messages_external_systems.md#_snippet_19\n\nLANGUAGE: dolphindb\nCODE:\n```\nrecipientCollection='Maildestination@xxx.com''Maildestination2@xxx.com''Maildestination3@xxx.com';\n```\n\n----------------------------------------\n\nTITLE: 大数组随机访问 - C++\nDESCRIPTION: 此代码片段演示了如何对 DolphinDB 中的大数组进行随机访问。 通过`getSegmentSizeInBit` 函数获得块大小的二进制位数，然后使用位运算获得块的偏移量和块内偏移量，进而访问大数组中的特定元素。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_development_tutorial.md#_snippet_8\n\nLANGUAGE: cpp\nCODE:\n```\nint segmentSizeInBit = x->getSegmentSizeInBit();\nint segmentMask = (1 << segmentSizeInBit) - 1;\ndouble **segments = (double **) x->getDataSegment();\n\nint index = 3000000;    // 想要访问的下标\n\ndouble result = segments[index>> segmentSizeInBit][index & segmentMask];\n//                       ^ 块的偏移量                ^ 块内偏移量\n```\n\n----------------------------------------\n\nTITLE: Calculating Beta Coefficient in Python\nDESCRIPTION: Defines a Python function `getBeta` using NumPy. It calculates the Beta coefficient between a fund (`value`) and a benchmark (`price`). It computes daily returns for both, finds their covariance using `np.cov`, and divides by the sample standard deviation (`np.std` with `ddof=1`) of the benchmark returns (variance is more standard, check formula context). Requires NumPy.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/fund_factor_contrasted_by_py.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\ndef getBeta(value, price):\n    # Ensure inputs are numpy arrays\n    value = np.asarray(value)\n    price = np.asarray(price)\n    \n    diff_price = np.diff(price)\n    rolling_price = np.roll(price, 1)[1:] # Corrected rolling/diff alignment\n    price_returns = np.true_divide(diff_price, rolling_price, where=rolling_price!=0)\n    \n    diff_value = np.diff(value)\n    rolling_value = np.roll(value, 1)[1:] # Corrected rolling/diff alignment\n    value_returns = np.true_divide(diff_value, rolling_value, where=rolling_value!=0)\n\n    # Ensure returns arrays are same length (can happen if inputs differ by 1)\n    min_len = min(len(value_returns), len(price_returns))\n    value_returns = value_returns[:min_len]\n    price_returns = price_returns[:min_len]\n\n    # Calculate covariance matrix\n    cov_matrix = np.cov(value_returns, price_returns)\n    # Calculate variance (or std dev as per code) of benchmark returns\n    benchmark_std = np.std(price_returns, ddof=1)\n    \n    # Avoid division by zero\n    if benchmark_std == 0:\n        return 0 # Or None, or raise error depending on desired behavior\n    \n    # Beta is Cov(fund, benchmark) / Var(benchmark), but code uses Std Dev\n    # Using Std Dev as per the provided code snippet:\n    return cov_matrix[0][1] / benchmark_std \n    # If Var was intended: return cov_matrix[0][1] / (benchmark_std**2)\n\n```\n\n----------------------------------------\n\nTITLE: Building a DolphinDB Shared Library Plugin with CMake\nDESCRIPTION: This CMake script configures the build process for a shared library named 'PluginLoadMyData'. It sets the minimum required CMake version, defines the project name, adds compiler definitions (including ABI compatibility and OS-specific flags for Windows/Linux), specifies include and link directories, enforces C++11 standard with PIC and warning flags, and defines the shared library target using 'LoadMyData.cpp'. Finally, it links the target library against the 'DolphinDB' library.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin/LoadMyData/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.0)\n\nproject(PluginLoadMyData)\n\nadd_definitions(-D_GLIBCXX_USE_CXX11_ABI=0)\n\nLINK_DIRECTORIES(\"../libs\")\nINCLUDE_DIRECTORIES(\"../include\")\nif(WIN32)\n    add_definitions(-DWINDOWS)\nelseif(UNIX)\n    add_definitions(-DLINUX)\nendif()\n\nadd_compile_options( \"-std=c++11\" \"-fPIC\" \"-Wall\" \"-Werror\")\n\nadd_library(PluginLoadMyData SHARED\n    \"./src/LoadMyData.cpp\"\n)\ntarget_link_libraries(PluginLoadMyData DolphinDB)\n```\n\n----------------------------------------\n\nTITLE: Configuring PluginLogSum Build with CMake for C++\nDESCRIPTION: This CMake snippet configures the build process for the PluginLogSum project using CMake version 3.0 or higher. It defines project metadata, sets compiler options for C++11 with position-independent code, and enforces strict compilation warnings as errors. Platform-specific define macros (-DWINDOWS or -DLINUX) are set based on the operating system. The shared library PluginLogSum is built from the source file LogSum.cpp, with include directories and linked libraries specified. Dependencies include DolphinDB for linking and local include/link directories for headers and libraries.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin/GeometricMean/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.0)\n\nproject(PluginLogSum)\n\nadd_definitions(-D_GLIBCXX_USE_CXX11_ABI=0)\n\nLINK_DIRECTORIES(\"../libs\")\nINCLUDE_DIRECTORIES(\"../include\")\nif(WIN32)\n    add_definitions(-DWINDOWS)\nelif(UNIX)\n    add_definitions(-DLINUX)\nendif()\n\nadd_compile_options( \"-std=c++11\" \"-fPIC\" \"-Wall\" \"-Werror\")\n\nadd_library(PluginLogSum SHARED\n    \"./src/LogSum.cpp\"\n)\ntarget_link_libraries(PluginLogSum DolphinDB)\n```\n\n----------------------------------------\n\nTITLE: Extracting Kafka Tools Package\nDESCRIPTION: Bash commands to extract and set up the kafka-tools package, which contains useful scripts for managing Kafka operations and configurations.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Debezium_and_Kafka_data_sync.md#_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\ncd /KFDATA\nsudo tar -xvf kafka-tools.tar\nsudo chown kafka:kafka kafka-tools\nrm ./kafka-tools.tar\n```\n\n----------------------------------------\n\nTITLE: Automatic Multiprocessing Execution on Stock Files in Python\nDESCRIPTION: This script sets the number of processes, specifies the stock data directory, and employs the multi_task_split class for dividing tasks. It initializes a process pool to execute the pool_func across chunks of stock files asynchronously, collecting and merging the results, with timing metrics displayed for performance analysis.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/DolphinDB_Python_Parser_Intro_for_Quantitative_Finance/因子实现_Python版本/价格变动与一档量差的回归系数.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nn_use = 24\n# 路径修改为存放数据路径\ntrade_path = r\"/ssd/ssd3/data/oneStock_oneFile_TL/20230201/snapshot\"\nstock_pool = os.listdir(trade_path)\nprocesses_decided = multi_task_split(stock_pool, n_use).num_of_jobs()\nprint(\"进程数：\", processes_decided)\nsplit_args_to_process = list(multi_task_split(stock_pool, n_use).split_args())\nargs = [(split_args_to_process[i], trade_path) for i in range(len(split_args_to_process))]\nprint(\"#\" * 50 + \"Multiprocessing Start\" + \"#\" * 50)\nt0 = time.time()\nwith multiprocessing.Pool(processes=processes_decided) as pool:\n    res = tqdm(pool.starmap(pool_func, args))\n    print(\"cal time: \", time.time() - t0, \"s\")\n    res_combined = pd.concat(res, axis=0)\n    pool.close()\n    print(\"cal time: \", time.time() - t0, \"s\")\nprint(res_combined)\n```\n\n----------------------------------------\n\nTITLE: Extracting DolphinDB Package on Single Server\nDESCRIPTION: Command to extract the DolphinDB Linux installation package to a specific directory on a single server for pseudo-high availability setup.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/service_deployment_and_migration.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nunzip DolphinDB_Linux64_V2.00.10.5.zip -d /home/dolphindb_1;\n```\n\n----------------------------------------\n\nTITLE: Extracting DolphinDB Package on Dual Servers\nDESCRIPTION: Commands to extract the DolphinDB installation package on two separate servers for a dual-server pseudo-high availability setup.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/service_deployment_and_migration.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# 服务器一上执行\nunzip DolphinDB_Linux64_V2.00.10.5.zip -d /home/dolphindb_1;\n\n# 服务器二上执行\nunzip DolphinDB_Linux64_V2.00.10.5.zip -d /home/dolphindb_2;\n```\n\n----------------------------------------\n\nTITLE: Starting Telegraf Service Using Shell Commands\nDESCRIPTION: Shell commands to start the Telegraf monitoring agent with a specified configuration file. It includes using the environment variable $TelegrafConfig or directly specifying the config file location. Also provides a command to verify the telegraf service process is running. Required dependencies include Telegraf installed and the configuration file accessible in the specified path. Input: configuration file path. Output: Telegraf service running.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_Telegraf_Grafana.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\ntelegraf --config $TelegrafConfig\n```\n\nLANGUAGE: Shell\nCODE:\n```\ntelegraf --config TelegrafConfig\n```\n\nLANGUAGE: Shell\nCODE:\n```\nps -ef | grep telegraf\n```\n\n----------------------------------------\n\nTITLE: Unzipping DolphinDB Linux\nDESCRIPTION: This command unzips the DolphinDB installation package (dolphindb.zip) to a specified directory. The `-d` option specifies the destination directory. It unpacks the contents of the zip archive into the specified location. This operation is necessary to make the DolphinDB binaries accessible.  The destination path should not contain spaces or Chinese characters.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/standalone_server.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\nunzip dolphindb.zip -d </path/to/directory>\n```\n\n----------------------------------------\n\nTITLE: Extracting Group Chat ID from WeChat Work Response\nDESCRIPTION: Code for parsing the JSON response after creating a group chat to extract the chat ID and error code. This chat ID is needed for sending messages to the specific group.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/send_messages_external_systems.md#_snippet_9\n\nLANGUAGE: DolphinDB\nCODE:\n```\nbody = parseExpr(ret.text).eval();\nERRCODE=body.errcode;\nCHATID=body.access_token;\n```\n\n----------------------------------------\n\nTITLE: Applying Indicator to Single File - Python\nDESCRIPTION: Demonstrates a single-threaded application of the `beforeClosingVolumePercent` function. It reads a single stock trade data file from a specified path into a pandas DataFrame, measures the time taken to calculate the indicator using the previously defined function, and prints the execution time and the resulting indicator value.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/DolphinDB_Python_Parser_Intro_for_Quantitative_Finance/因子实现_Python版本/当日尾盘成交占比.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_csv(\"/ssd/ssd3/data/oneStock_oneFile_TL/20230201/trade/000001.csv\")\nt0 = time.time()\nres = beforeClosingVolumePercent(df)\nprint(\"cal time: \", time.time() - t0, \"s\")\nprint(res)\n```\n\n----------------------------------------\n\nTITLE: 查看数据节点volume目录\nDESCRIPTION: 此操作系统命令列出指定数据节点的存储目录内容，包括事务日志和数据块，验证存储状态。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Compute_Node.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\n[appadmin@support4 log]$ lsof -p pidOfDatanode\n```\n\n----------------------------------------\n\nTITLE: Setting Executable Permissions Linux\nDESCRIPTION: This command grants execute permissions to the `dolphindb` executable file. It uses `chmod +x` to set the executable bit. This step is required to execute the DolphinDB server on Linux.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/standalone_server.md#_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\nchmod +x dolphindb\n```\n\n----------------------------------------\n\nTITLE: Cloning a DolphinDB Plugin Repository using Git\nDESCRIPTION: This command clones the DolphinDBPlugin repository from a personal GitHub account to the local machine. This is a prerequisite for contributing code changes or bug fixes to the project.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_Open_Source_Project_Contribution.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/peeyee/DolphinDBPlugin.git\n```\n\n----------------------------------------\n\nTITLE: Querying data from DolphinDB table\nDESCRIPTION: Queries all columns and rows from the 'config' table within the 'configDB' database.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/cachedTable/mysql_data.txt#_snippet_4\n\nLANGUAGE: DolphinDB\nCODE:\n```\nselect * from configDB.config;\n```\n\n----------------------------------------\n\nTITLE: Elasticsearch Grouping by Stock Code (Single Aggregation) - Python\nDESCRIPTION: This Python function performs an aggregation query on an Elasticsearch index ('elastic'), grouping documents by 'ts_code' and calculating the average 'low' price for each group. It uses the `urllib3` library to send a GET request to the Elasticsearch API with a JSON payload defining the aggregation. The function prints the HTTP status code and the JSON response.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_30\n\nLANGUAGE: Python\nCODE:\n```\ndef search_5():\n    http = urllib3.PoolManager()\n    data = json.dumps({\n        \"aggs\": {\n            \"group_by_ts_code\": {\n                \"terms\": {\n                    \"field\": \"ts_code\",\n                    \"size\": 5000  # 跟这个size有关，是否精确\n                },\n                \"aggs\": {\n                    \"avg_price\": {\n                        \"avg\": {\"field\": \"low\"}\n                    }\n                }\n            }\n        },\n        \"_source\": [\"\"]\n    }).encode(\"utf-8\")\n    r = http.request(\"GET\", \"http://localhost:9200/elastic/_search\", body=data,\n                     headers={'Content-Type': 'application/json'})\n    print(r.status)\n    print(json.loads(r.data.decode()))\n```\n\n----------------------------------------\n\nTITLE: Request Message Format\nDESCRIPTION: This is a high-level structure representing the generic format for request messages sent to the DolphinDB server. It outlines the key components of a request, which include the request type, session ID, and the payload containing the instruction and its parameters. The format depends on the specific command being issued (script, function, variable).\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/api_protocol.md#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n| 请求类型 | 空格 | SESSIONID | 空格 | 报文指令长度 | 换行符(LF)\n| 指令类型 | 换行符(LF)\n| 指令参数 | 数据 |\n```\n\n----------------------------------------\n\nTITLE: Batch Importing CSV Data into MongoDB using Bash\nDESCRIPTION: A Bash script that iterates through all CSV files in '/media/xllu/aa/TAQ/mongo_split/'. For each file, it uses 'mongoimport' to load data into the 'taq_pt_col' collection within the 'taq_pt_db' database on the MongoDB instance running at localhost:40000. It specifies column types, handles parsing errors by skipping rows, and uses 12 insertion workers for parallel processing.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs MongoDB/mongodb_taq_partitioned.txt#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n#!/bin/bash\nfor f in /media/xllu/aa/TAQ/mongo_split/*.csv ; do\n        /usr/bin/mongoimport \\\n        -h localhost \\\n        --port 40000 \\\n        -d taq_pt_db \\\n        -c taq_pt_col \\\n        --type csv \\\n        --columnsHaveTypes \\\n --fields \"symbol.string(),date.date(20060102),time.date(15:04:05),bid.double(),ofr.double(),bidsiz.int32(),ofrsiz.int32(),mode.int32(),ex.string(),mmid.string()\" \\\n        --parseGrace skipRow \\\n        --numInsertionWorkers 12 \\\n        --file $f\n    echo \"文件 $f 导入完成\"这个\ndone\n```\n\n----------------------------------------\n\nTITLE: Installing Miniconda on Linux\nDESCRIPTION: Commands to download and install Miniconda on a Linux system, including environment activation and verification using conda commands. Dependencies include wget and shell script execution capability.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/python_api_install_offline.md#_snippet_0\n\nLANGUAGE: Shell Script\nCODE:\n```\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nsh Miniconda3-latest-Linux-x86_64.sh\n```\n\nLANGUAGE: Shell Script\nCODE:\n```\nsource ~/.bashrc\n# Verify installation\nconda -V\n```\n\n----------------------------------------\n\nTITLE: Configuring conda environment via .condarc\nDESCRIPTION: Creates and edits the conda configuration file ~/.condarc to set channel URLs, environment directories, and package storage directories. Dependencies include conda installation and text editor for file modification.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/python_api_install_offline.md#_snippet_1\n\nLANGUAGE: Shell Script\nCODE:\n```\nconda config\n# Manual editing of ~/.condarc to include:\nshow_channel_urls: true\nenvs_dirs:\n  - ~/envs\npkgs_dirs:\n  - ~/pkgs\n```\n\n----------------------------------------\n\nTITLE: Listing Intermediate Directories During Update\nDESCRIPTION: This console command `ll` (equivalent to `ls -l` on Unix-like systems) is used to display the contents of the current directory during an update operation. The output shows the creation of intermediate directories with names including 'tid' as the update process progresses.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/dolphindb_update.md#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\n$ ll\ntotal 20\ndrwxrwxr-x 2 dolphindb dolphindb 4096 Sep  7 05:26 machines_2_115\ndrwxrwxr-x 2 dolphindb dolphindb 4096 Sep  7 05:26 machines_2_116\ndrwxrwxr-x 2 dolphindb dolphindb 4096 Sep  7 05:26 machines_2_117\ndrwxrwxr-x 2 dolphindb dolphindb 4096 Sep  7 05:26 machines_2_118\ndrwxrwxr-x 2 dolphindb dolphindb 4096 Sep  7 05:26 machines_2_119\ndrwxrwxr-x 2 dolphindb dolphindb  120 Sep  7 05:26 machines_2_tid_120\n```\n\n----------------------------------------\n\nTITLE: Creating Sample Data with Mixed Number and Character Data\nDESCRIPTION: This code creates a sample dataset with columns containing numerical data with mixed characters (e.g., currency symbols, thousands separators) and saves it to a CSV file. The purpose is to demonstrate DolphinDB's handling of such data when loading.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/import_csv.md#_snippet_37\n\nLANGUAGE: DolphinDB\nCODE:\n```\ndataFilePath=\"/home/data/testSym.csv\"\nprices1=[\"2131\",\"\\$2,131\", \"N/A\"]\nprices2=[\"213.1\",\"\\$213.1\", \"N/A\"]\ntotals=[\"2.658E7\",\"-2.658e7\",\"2.658e-7\"]\ntt=table(1..3 as id, prices1 as price1, prices2 as price2, totals as total)\nsaveText(tt,dataFilePath);\n```\n\n----------------------------------------\n\nTITLE: 性能对比测试-环境参数设置\nDESCRIPTION: 定义用于性能测试的硬件环境和软件配置参数，确保 DolphinDB TSDB、OLAP 及 ClickHouse 在相似硬件环境下进行比较，测试参数包括内存限制、分区和排序设置。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/iot_query_case.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n# 测试环境参数配置省略（描述性文本）\n```\n\n----------------------------------------\n\nTITLE: Querying Data - Java\nDESCRIPTION: This code snippet queries data from the DolphinDB table and prints the results to the console.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Array_Vector.md#_snippet_25\n\nLANGUAGE: java\nCODE:\n```\nBasicTable t;\nt = (BasicTable)conn.run(\"select * from loadTable('dfs://testDB','test')\");\nSystem.out.println(t.getString());\n```\n\n----------------------------------------\n\nTITLE: Listing Updated File Contents\nDESCRIPTION: This console command `ll` is used to display the contents of a specific directory after an update operation, showing the structure of data files. It's particularly useful to observe the effects of the update on the storage of column data.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/dolphindb_update.md#_snippet_3\n\nLANGUAGE: console\nCODE:\n```\n$ ll machines_2_125/\ntotal 169632\n-rw-rw-r-- 5 dolphindb dolphindb 3469846 Sep  7 05:15 datetime.col\n-rw-rw-r-- 5 dolphindb dolphindb   14526 Sep  7 05:15 id.col\n-rw-rw-r-- 5 dolphindb dolphindb 3469845 Sep  7 05:15 tag10.col\n-rw-rw-r-- 5 dolphindb dolphindb 3469846 Sep  7 05:15 tag11.col\n...\n-rw-rw-r-- 1 dolphindb dolphindb 1742158 Sep  7 05:26 tag1.col\n...\n-rw-rw-r-- 1 dolphindb dolphindb 1742158 Sep  7 05:26 tag5.col\n...\n```\n\n----------------------------------------\n\nTITLE: Plugin Description File Example\nDESCRIPTION: This snippet shows an example of a DolphinDB plugin description file (PluginTest.txt), which defines the plugin name, library name, and the functions exposed by the plugin, including their names, type, and parameter counts.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/plugin_advance.md#_snippet_7\n\nLANGUAGE: text\nCODE:\n```\ntest,libPluginTest.so\nmyFunc1,myFunc1,system,2,2,0\nmyFunc2,myFunc2,system,0,0,0\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Configuring Pandas - Python\nDESCRIPTION: Imports necessary standard and third-party Python libraries for data manipulation (pandas, numpy), iteration (itertools, functools), file system operations (os), multiprocessing, timing (time), warnings, progress bars (tqdm), and datetime. It also configures pandas display options for wider output and more columns, suppresses specific warnings, and sets the chained assignment mode.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/DolphinDB_Python_Parser_Intro_for_Quantitative_Finance/因子实现_Python版本/当日尾盘成交占比.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport numpy as np\nimport itertools\nfrom functools import reduce\nimport os\nimport multiprocessing\nimport time\nimport warnings\nfrom tqdm import tqdm\nimport datetime\n\nwarnings.filterwarnings(\"ignore\")\n\npd.options.display.width = 1200\npd.options.display.max_colwidth = 100\npd.options.display.max_columns = 10\npd.options.mode.chained_assignment = None\n```\n\n----------------------------------------\n\nTITLE: Configuring Pandas Display and Warning Settings in Python\nDESCRIPTION: Initializes the Python environment by importing required libraries such as pandas, numpy, and tqdm, and configures pandas display options for better readability of dataframes. Warnings are suppressed to avoid cluttering output. This setup facilitates efficient data manipulation and progress visualization.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/DolphinDB_Python_Parser_Intro_for_Quantitative_Finance/因子实现_Python版本/早盘买卖单大小比.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport numpy as np\nimport itertools\nfrom functools import reduce\nimport os\nimport multiprocessing\nimport time\nimport warnings\nfrom tqdm import tqdm\nimport datetime\n\nwarnings.filterwarnings(\"ignore\")\n\npd.options.display.width = 1200\npd.options.display.max_colwidth = 100\npd.options.display.max_columns = 10\npd.options.mode.chained_assignment = None\n```\n\n----------------------------------------\n\nTITLE: Variable Command Response Format\nDESCRIPTION: Describes the response format for the 'variable' command. This format is simpler, containing the session ID, byte order, and an execution status ('OK' on success).\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/api_protocol.md#_snippet_9\n\nLANGUAGE: text\nCODE:\n```\n占位(Byte) | 报文 | 说明 | 样本\n---|---|---|---\n3| SESSIONID | 长度不定，到空格为止  | API\n1| 空格| char(0x20) |\n1|大小端 | 1-小端，0-大端 | 1\n1| 换行符(LF) | char(0x10) |\n1| 执行成功否| 返回文本OK表示执行成功 | \"OK\"\n```\n\n----------------------------------------\n\nTITLE: Creating MySQL Table 'stock_basic'\nDESCRIPTION: SQL command to create a second test table 'stock_basic' with a composite primary key for demonstrating multi-table synchronization.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Debezium_and_Kafka_data_sync.md#_snippet_6\n\nLANGUAGE: SQL\nCODE:\n```\nCREATE TABLE `stock_basic` (\n  `id` bigint NOT NULL ,\n  `ts_code` varchar(20) NOT NULL,\n  `symbol` varchar(20) DEFAULT NULL,\n  `name` varchar(20) DEFAULT NULL,\n  `area` varchar(20) DEFAULT NULL,\n  `industry` varchar(40) DEFAULT NULL,\n  `list_date` date DEFAULT NULL,\n  PRIMARY KEY (`id`,`ts_code`)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;\n```\n\n----------------------------------------\n\nTITLE: Elasticsearch Grouping by Ticker, Multiple Aggregations\nDESCRIPTION: This Python code defines an Elasticsearch aggregation query, using `urllib3` and `json`. It groups by the 'TICKER' field and calculates the average of the 'VOL' field and the maximum of the 'OPENPRC' field for each ticker group. The function `search_6()` sends this query to Elasticsearch and prints the HTTP status code and JSON response. This tests the performance of multiple aggregations with grouping by 'TICKER'.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_52\n\nLANGUAGE: Python\nCODE:\n```\ndef search_6():\n    http = urllib3.PoolManager()\n    data = json.dumps({\n        \"aggs\": {\n            \"group_by_ticker\": {\n                \"terms\": {\n                    \"field\": \"TICKER\",\n                    \"size\": 23934 \n                },\n                \"aggs\": {\n                    \"avg_price\": {\n                        \"avg\": {\"field\": \"VOL\"}\n                    },\n                    \"max_open\": {\n                        \"max\": {\"field\": \"OPENPRC\"}\n                    }\n                }\n            }\n        },\n        \"_source\": [\"\"]\n    }).encode(\"utf-8\")\n    r = http.request(\"GET\", \"http://localhost:9200/uscsv/_search\", body=data,\n                     headers={'Content-Type': 'application/json'})\n    print(r.status)\n    print(json.loads(r.data.decode()))\n```\n\n----------------------------------------\n\nTITLE: Checking Current User and System File Limits after Reboot (console)\nDESCRIPTION: Verify if the new file descriptor limits are in effect after a reboot using `ulimit -n` for the user-level and `cat /proc/sys/fs/file-max` for the system-wide setting. These validations help ensure system readiness for DolphinDB deployment.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/deploy_dolphindb_on_new_server.md#_snippet_3\n\nLANGUAGE: console\nCODE:\n```\n# ulimit -n # 用户级\n102400\n# cat /proc/sys/fs/file-max # 系统级\n763964\n\n```\n\n----------------------------------------\n\nTITLE: Elasticsearch Aggregation Query\nDESCRIPTION: This Python script constructs and executes an Elasticsearch aggregation query to group by 'date' and calculate the average of 'ASK' and maximum of 'BID'. It sends a request to an Elasticsearch instance using `urllib3`. It constructs the request using `json.dumps()`, and then prints the status code and decoded JSON. Requires a running Elasticsearch instance and the \"uscsv\" index with the appropriate fields.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_55\n\nLANGUAGE: Python\nCODE:\n```\ndef search_8():\n    http = urllib3.PoolManager()\n    data = json.dumps({\n        \"aggs\": {\n        }\n    }).encode(\"utf-8\")\n    r = http.request(\"GET\", \"http://localhost:9200/uscsv/_search\", body=data,\n                     headers={'Content-Type': 'application/json'})\n    print(r.status)\n    print(json.loads(r.data.decode()))\n```\n\n----------------------------------------\n\nTITLE: Downloading DolphinDB Linux Version\nDESCRIPTION: This command downloads a specific version (2.00.11.3) of the DolphinDB Linux server installation package. It utilizes the `wget` utility to retrieve the zip archive from the official website and saves it as `dolphindb.zip`.  This allows for downloading a known version of the software to a specific directory.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/standalone_server.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nwget https://www.dolphindb.cn/downloads/DolphinDB_Linux64_V2.00.11.3.zip -O dolphindb.zip\n```\n\n----------------------------------------\n\nTITLE: Check DolphinDB Version After Upgrade - Shell\nDESCRIPTION: Command executed through the DolphinDB web console to verify the current running version by calling the `version()` function. This step confirms a successful upgrade or startup of the DolphinDB server.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ARM_standalone_deploy.md#_snippet_10\n\nLANGUAGE: Shell\nCODE:\n```\nversion()\n```\n\n----------------------------------------\n\nTITLE: Running DolphinDB Foreground with Port Linux\nDESCRIPTION: This command launches DolphinDB in the foreground, specifying a custom port. The `-localSite` argument sets the local site configuration, including the IP address, port, and a local ID. This allows the server to listen on the specified port (8900 in the example).\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/standalone_server.md#_snippet_9\n\nLANGUAGE: sh\nCODE:\n```\n./dolphindb -localSite localhost:8900:local8900\n```\n\n----------------------------------------\n\nTITLE: Generating Sample Data in InfluxDB using Flux\nDESCRIPTION: A Flux script to import sample machine production data into InfluxDB. The script imports the InfluxDB sample data module and writes the machine production dataset to a demo bucket.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Migrate_data_from_InfluxDB_to_DolphinDB.md#_snippet_0\n\nLANGUAGE: flux\nCODE:\n```\nimport \"influxdata/influxdb/sample\"\n\nsample.data(set: \"machineProduction\")\n    |> to(bucket: \"demo-bucket\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Elasticsearch Network and Discovery (elasticsearch.yml)\nDESCRIPTION: Sets Elasticsearch configuration parameters using YAML format. It configures the network host binding, HTTP port, enables CORS for broader access, sets the transport protocol port and enables compression, defines unicast hosts for cluster discovery, and specifies the minimum number of master nodes required.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nnetwork.host: 127.0.0.1 \nhttp.port: 9200\nhttp.cors.enabled: true\nhttp.cors.allow-origin: \"*\"\ntransport.tcp.port: 9300\ntransport.tcp.compress: true\ndiscovery.zen.ping.unicast.hosts: [\"127.0.0.1:9300\", \"127.0.0.1:9301\", \"127.0.0.1:9302\", \"127.0.0.1:9303\"]\ndiscovery.zen.minimum_master_nodes: 1\n```\n\n----------------------------------------\n\nTITLE: HDF5 File Operations in Python\nDESCRIPTION: Functions for saving and loading data using pandas HDFStore. These utilities handle the storage and retrieval of dataframes to/from HDF5 files for the Python-based factor calculation approach.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Python+HDF5_vs_DolphinDB.md#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n# 保存 HDF5 文件\ndef saveByHDFStore(path,df):\n    store = pd.HDFStore(path)\n    store[\"Table\"] = df\n    store.close()\n\n# 读取单个 HDF5 文件\ndef loadData(path):\n    store = pd.HDFStore(path, mode='r')\n    data = store[\"Table\"]\n    store.close()\n    return data\n```\n\n----------------------------------------\n\nTITLE: Configuring pandas and Importing Required Libraries in Python\nDESCRIPTION: This snippet imports essential libraries like pandas, numpy, itertools, multiprocessing, and tqdm, and sets pandas options for better display formatting and disables warnings. These configurations ensure cleaner output during data analysis and handle large datasets smoothly.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/script/DolphinDB_Python_Parser_Intro_for_Quantitative_Finance/因子实现_Python版本/委托量加权平均委托价格.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport numpy as np\nimport itertools\nfrom functools import reduce\nimport os\nimport multiprocessing\nimport time\nimport warnings\nfrom tqdm import tqdm\nimport datetime\n\nwarnings.filterwarnings(\"ignore\")\n\npd.options.display.width = 1200\npd.options.display.max_colwidth = 100\npd.options.display.max_columns = 10\npd.options.mode.chained_assignment = None\n```\n\n----------------------------------------\n\nTITLE: LocalDateTime Usage in Java\nDESCRIPTION: This Java snippet demonstrates the usage of `LocalDateTime` for handling local date and time. It includes examples of creating `LocalDateTime` objects, performing time calculations (adding/subtracting days/hours), and specifying a time zone offset.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/timezone.md#_snippet_2\n\nLANGUAGE: Java\nCODE:\n```\nLocalDateTime now = LocalDateTime.now();//获取当前时间\nLocalDateTime time = LocalDateTime.of(2022,2,20,9,35,3);// of方法指定年月日时分秒\n\n// 时间计算相关\ntime.isAfter(now);\ntime.isBefore(now);\nSystem.out.println(time.plusDays(1L));\nSystem.out.println(time.minusHours(8));\n\n// 支持指定时区偏移量\nLocalDateTime.now(ZoneId.of(\"Australia/Sydney\"));\n```\n\n----------------------------------------\n\nTITLE: Listing Directory Contents After Updates (keepDuplicates=ALL)\nDESCRIPTION: This snippet demonstrates the file structure using the `tree` command after update operations when the `keepDuplicates` parameter is configured to ALL. The output depicts the directory structure which shows the persistent retention of multiple update versions prior to any periodic cleanups.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/dolphindb_update.md#_snippet_10\n\nLANGUAGE: console\nCODE:\n```\n$ tree\n.\n├── chunk.dict\n└── machines_2_215\n    ├── 0_00000595\n    ├── 0_00000596\n    ├── 0_00000597\n    ├── 0_00000598\n    └── 0_00000599\n├── machines_2_216\n│   ├── 0_00000600\n│   ├── 0_00000601\n│   ├── 0_00000602\n│   ├── 0_00000603\n│   └── 0_00000604\n├── machines_2_217\n│   ├── 0_00000605\n│   ├── 0_00000606\n│   ├── 0_00000607\n│   ├── 0_00000608\n│   └── 0_00000609\n├── machines_2_218\n│   ├── 0_00000610\n│   ├── 0_00000611\n│   ├── 0_00000612\n│   ├── 0_00000613\n│   └── 0_00000614\n└── machines_2_219\n    ├── 0_00000615\n    ├── 0_00000616\n    ├── 0_00000617\n    ├── 0_00000618\n    └── 0_00000619\n\n```\n\n----------------------------------------\n\nTITLE: MongoDB Config Server Configuration for Shard Cluster\nDESCRIPTION: Configuration file for a MongoDB config server in a sharded cluster environment. It specifies storage paths, network settings, replication parameters, and performance optimizations.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs MongoDB/slave_config.txt#_snippet_0\n\nLANGUAGE: ini\nCODE:\n```\npidfilepath = /media/xllu/aa/localhost_shard_jiqun/conf/conf_s1/conf_s1.pid\ndbpath = /media/xllu/aa/localhost_shard_jiqun/conf/conf_s1/conf_s1_data\nlogpath = /media/xllu/aa/localhost_shard_jiqun/conf/conf_s1/conf_s1.log\ndirectoryperdb=true\nlogappend = true\nbind_ip = localhost\nport = 30001\nfork = true\n#declare this is a config db of a cluster;\nconfigsvr = true\n#副本集名称\nreplSet=conrep\nnoprealloc=true\n#设置最大连接数\nmaxConns=128\nwiredTIgerCacheSizeGB=1\n```\n\n----------------------------------------\n\nTITLE: Backup Metadata Files Before Upgrade - Shell\nDESCRIPTION: Commands to create a backup directory and recursively copy DolphinDB local single node metadata files (`dfsMeta` and `CHUNK_METADATA`) into the backup folder. Users are reminded to verify custom metadata paths in configuration files if defaults are missing.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ARM_standalone_deploy.md#_snippet_5\n\nLANGUAGE: Shell\nCODE:\n```\nmkdir backup\n```\n\nLANGUAGE: Shell\nCODE:\n```\ncp -r local8848/dfsMeta/ backup/dfsMeta\n```\n\nLANGUAGE: Shell\nCODE:\n```\ncp -r local8848/storage/CHUNK_METADATA/ backup/CHUNK_METADATA\n```\n\n----------------------------------------\n\nTITLE: Opening firewall port\nDESCRIPTION: These commands open a specific port (8848) in the firewall. This is used to allow external access to the DolphinDB Notebook. The first command checks active zones and the interface associated with it, whereas the second command opens the specified port.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/client_tool_tutorial.md#_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\n>firewall-cmd --get-active-zones\n\npublic\ninterfaces: eth1\n\n>sudo firewall-cmd --zone=public --permanent --add-port=8848/tcp\n\nsuccess\n```\n\n----------------------------------------\n\nTITLE: Configuring DolphinDB - cluster.cfg\nDESCRIPTION: This configuration file (`cluster.cfg`) sets global cluster settings such as maximum connections, worker threads, local executors, web worker threads, and memory limits. Dependencies: DolphinDB cluster setup.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_Druid_appendix.md#_snippet_2\n\nLANGUAGE: DolphinDB\nCODE:\n```\nmaxConnection=128\nworkerNum=8\nlocalExecutors=7\nwebWorkerNum=2\nmaxMemSize=0\n```\n\n----------------------------------------\n\nTITLE: Level File 合并性能测试场景配置说明\nDESCRIPTION: 描述了测试中使用的表结构、分区方案、去重策略及其对存储空间和文件数的影响，便于理解实验背景。\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/tsdb_explained.md#_snippet_0\n\n\n\n----------------------------------------\n\nTITLE: Checking Linux System Memory Issues with dmesg\nDESCRIPTION: Command to inspect Linux system logs for memory-related issues, particularly OOM killer events that might have terminated DolphinDB processes.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/handling_oom.md#_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\ndmesg -T|grep memory\n```\n\n----------------------------------------\n\nTITLE: Variable Command Request Format\nDESCRIPTION: This outlines the format for the 'variable' command request. It includes the request type, session ID, message length, the command 'variable', the variable names (comma separated), the number of variables, endianness, and the variable data which data format references the third section. This command allows clients to upload local variables to the server.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/api_protocol.md#_snippet_8\n\nLANGUAGE: text\nCODE:\n```\n长度(Byte) | 报文 | 说明 | 样本\n---|---|---|---\n3| 请求类型 | API | API\n1| 空格| char(0x20) |\n不固定|SESSIONID | 长度不固定，到空格为止  | 2247761467\n1| 空格| char(0x20) |\n2| 报文指令长度| 包含从“variable\"到大小端标志为止的长度，如\"variable\\na,b\\n2\\n1\" | 16\n1| 换行符 | char(0x10) |\n8| 指令 | variable | \"variable\"\n1| 换行符 | char(0x10) |\n不固定| 变量名 | 多个变量通过\",\"号分隔，字符串 | a,b\n1| 换行符 | char(0x10) |\n1| 变量数量 | 传递到函数的变量个数 | 2\n1| 换行符 | char(0x10) |\n1| 大小端标志 | 1-小端，0-大端 | 1\n不固定| 变量数据 | 数据格式参考第3节 |\n```\n\n----------------------------------------\n\nTITLE: Testing FreeTDS ODBC Connection with isql\nDESCRIPTION: This command demonstrates connecting to a MSSQL database using the FreeTDS driver with `isql`. An incorrect port can lead to a segmentation fault, causing the process to crash.  This highlights the importance of correct driver configuration.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/ODBC_plugin_user_guide.md#_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nisql -v -k  \"Driver=FreeTDS;Servername=MSSQL;UID=SA;Pwd=Sa123456;\"\nSegmentation fault (core dumped)\n```\n\n----------------------------------------\n\nTITLE: Function Command Response Format\nDESCRIPTION: Defines the response format for the 'function' command. It contains the session ID, byte order, and the execution result ('OK' on success) with the result data which data format references the third section.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/api_protocol.md#_snippet_7\n\nLANGUAGE: text\nCODE:\n```\n长度(Byte) | 报文 | 说明 | 样本\n---|---|---|---\n3| SESSIONID | 长度不固定，到空格为止   | API\n1| 空格| char(0x20) |\n1|大小端 | 1-小端，0-大端 | 1\n1| 换行符(LF) | char(0x10) |\n1| 执行成功否| 返回文本OK表示执行成功 | \"OK\"\n1| 换行符(LF) | char(0x10) |\n不固定| 返回结果 | 数据格式参考第3节 |\n```\n\n----------------------------------------\n\nTITLE: Monitoring System Resource Usage\nDESCRIPTION: Command to monitor system resource usage including memory to identify processes consuming excessive resources.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/handling_oom.md#_snippet_8\n\nLANGUAGE: Bash\nCODE:\n```\ntop\n```\n\n----------------------------------------\n\nTITLE: Running Grafana Server - Shell\nDESCRIPTION: This command starts the Grafana server in the background. The `web` argument is part of the command structure for the grafana-server executable.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/cluster_monitor.md#_snippet_5\n\nLANGUAGE: Shell\nCODE:\n```\nnohup ./grafana-server web &\n```\n\n----------------------------------------\n\nTITLE: Adding Cluster Replication Configurations - Master\nDESCRIPTION: This snippet adds configuration parameters to the cluster.cfg file for the master cluster. `clusterReplicationMode=master` designates this cluster as the master.  `clusterReplicationWorkDir` specifies the directory for replication work files. `clusterReplicationSyncPersistence=false` determines whether to enable synchronous persistence; default is false which could lead to data loss in case of a master node failure. The configurations are set using shell commands such as `vim`.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Asynchronous_Replication.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nclusterReplicationMode=master\nclusterReplicationWorkDir=/dolphindb/server/cluster1/replication\nclusterReplicationSyncPersistence=false\n```\n\n----------------------------------------\n\nTITLE: Restarting Kafka Connect\nDESCRIPTION: Commands to restart the Kafka Connect service to load the newly installed Kafka-DolphinDB connector plugin.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Debezium_and_Kafka_data_sync.md#_snippet_15\n\nLANGUAGE: Bash\nCODE:\n```\nsudo systemctl stop kafka-connect\nsudo systemctl start kafka-connect\n```\n\n----------------------------------------\n\nTITLE: Elasticsearch Performance Testing\nDESCRIPTION: This Python code runs performance tests for a specific Elasticsearch query and measures the total execution time. It first defines a main function which loops 10 times and calls the `search_9()` function in each iteration, adding the execution time to a total. The `search_9()` function is not provided. After the loop, the `delete_scroll()` function is called to clean up the search scroll context, and the total execution time in milliseconds is printed. This structure allows for consistent performance comparisons.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB_vs_ElasticSearch_Benchmark_appendix.md#_snippet_56\n\nLANGUAGE: Python\nCODE:\n```\ndef main():\n    total = 0\n    for i in range(0, 10):\n        t1 = time.time()\n        search_9()\n        t2 = time.time()\n        total += t2-t1\n    delete_scroll()\n    print(total * 1000)\n```\n\n----------------------------------------\n\nTITLE: Committing and Pushing changes to a personal Git Repo\nDESCRIPTION: These commands commit changes with a descriptive message and then push the committed changes to the remote personal repository. This allows the contributor to upload their changes to their forked repository on GitHub.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/DolphinDB_Open_Source_Project_Contribution.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ngit commit -a -m \"add a document about xxx.\" && git push\n```\n\n----------------------------------------\n\nTITLE: Creating MySQL Table 'index_components'\nDESCRIPTION: SQL command to create a test table 'index_components' with a composite primary key. This table will be used to demonstrate CDC functionality.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/Debezium_and_Kafka_data_sync.md#_snippet_4\n\nLANGUAGE: SQL\nCODE:\n```\nuse basicinfo;\nCREATE TABLE `index_components` (\n  `trade_date` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,\n  `code` varchar(20) NOT NULL,\n  `effDate` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,\n  `indexShortName` varchar(20) CHARACTER SET utf8mb3 COLLATE utf8mb3_general_ci DEFAULT NULL,\n  `indexCode` varchar(20) NOT NULL,\n  `secShortName` varchar(20) CHARACTER SET utf8mb3 COLLATE utf8mb3_general_ci DEFAULT NULL,\n  `exchangeCD` varchar(4) CHARACTER SET utf8mb3 COLLATE utf8mb3_general_ci DEFAULT NULL,\n  `weight` decimal(26,6) DEFAULT NULL,\n  `timestamp` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,\n  `flag` int NOT NULL DEFAULT '1',\n  PRIMARY KEY `index_components_pkey` (`trade_date`,`code`,`indexCode`,`flag`)\n)ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;\n```\n\n----------------------------------------\n\nTITLE: Querying by Time Equality with MongoDB - JavaScript\nDESCRIPTION: This snippet demonstrates a point query on the 'device_readings' collection to find records with an exact timestamp match. It uses MongoDB's .find() method with ISODate for precision. Inputs are a specific ISO-formatted date-time. Requires MongoDB and appropriate collection setup. Output is the matching document(s).\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/benchmark/DolphinDB vs MongoDB/mongodb_readings_query.txt#_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\ndb.device_readings.explain(\"executionStats\").find({time:{\"$eq\":ISODate(\"2016-11-15 07:00:00.000Z\")}},{})\n```\n\n----------------------------------------\n\nTITLE: Configuring Grafana Unsigned Plugins - INI\nDESCRIPTION: This configuration line in Grafana's defaults.ini file allows loading the specified unsigned plugin, in this case, the dolphindb-datasource. This is necessary if the plugin is not officially signed by Grafana Labs.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/cluster_monitor.md#_snippet_6\n\nLANGUAGE: INI\nCODE:\n```\nallow_loading_unsigned_plugins = dolphindb-datasource\n```\n\n----------------------------------------\n\nTITLE: Checking MySQL Time Zone Variables\nDESCRIPTION: Shows how to check the current time zone settings in MySQL, displaying both the system_time_zone and the server time_zone variables.\nSOURCE: https://github.com/dolphindb/tutorials_cn/blob/master/timezone.md#_snippet_11\n\nLANGUAGE: SQL\nCODE:\n```\nmysql> show variables like '%time_zone%';\n+------------------+--------+\n| Variable_name    | Value  |\n+------------------+--------+\n| system_time_zone | CST    |\n| time_zone        | SYSTEM |\n+------------------+--------+\n```"
  }
]
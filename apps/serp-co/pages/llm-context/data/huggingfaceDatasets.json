[
  {
    "owner": "huggingface",
    "repo": "datasets",
    "content": "TITLE: Loading Dataset from Hugging Face Hub with load_dataset in Python\nDESCRIPTION: Loads a dataset from a repository on the Hugging Face Hub using the `load_dataset` function. It specifies the repository namespace and dataset name. The dataset repository contains CSV files.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/loading.mdx#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n>>> from datasets import load_dataset\n>>> dataset = load_dataset(\"lhoestq/demo1\")\n```\n\n----------------------------------------\n\nTITLE: Loading Dataset Split with Datasets\nDESCRIPTION: This code demonstrates how to load a specific split of a dataset using the `load_dataset` function and the `split` parameter. It requires the `datasets` library. The `split` parameter specifies which subset of the data (e.g., 'train', 'test', 'validation') to load. Returns a `Dataset` object representing the loaded data.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/load_hub.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\", split=\"train\")\n```\n\n----------------------------------------\n\nTITLE: Loading Remote WebDatasets via HTTP with load_dataset in Python\nDESCRIPTION: Loads remote WebDatasets accessible via HTTP URLs using the `load_dataset` function with the 'webdataset' data loading script. Streaming is enabled via `streaming=True`.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/loading.mdx#_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\n>>> from datasets import load_dataset\n>>>\n>>> base_url = \"https://huggingface.co/datasets/lhoestq/small-publaynet-wds/resolve/main/publaynet-train-{i:06d}.tar\"\n>>> urls = [base_url.format(i=i) for i in range(4)]\n>>> dataset = load_dataset(\"webdataset\", data_files={\"train\": urls}, split=\"train\", streaming=True)\n```\n\n----------------------------------------\n\nTITLE: Loading a CSV File with load_dataset in Python\nDESCRIPTION: Loads a dataset from a local or remote CSV file using the `load_dataset` function.  It requires the 'csv' data loading script to be specified.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/loading.mdx#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n>>> from datasets import load_dataset\n>>> dataset = load_dataset(\"csv\", data_files=\"my_file.csv\")\n```\n\n----------------------------------------\n\nTITLE: Splitting a Dataset into Train and Test Sets in Python\nDESCRIPTION: This code snippet demonstrates how to split a dataset into training and testing sets using the `train_test_split` method. It creates a test split that is 10% of the original dataset. The splits are shuffled by default.  It also shows the resulting schema and number of rows in each split.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/process.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset.train_test_split(test_size=0.1)\n{'train': Dataset(schema: {'sentence1': 'string', 'sentence2': 'string', 'label': 'int64', 'idx': 'int32'}, num_rows: 3301),\n'test': Dataset(schema: {'sentence1': 'string', 'sentence2': 'string', 'label': 'int64', 'idx': 'int32'}, num_rows: 367)}\n>>> 0.1 * len(dataset)\n366.8\n```\n\n----------------------------------------\n\nTITLE: Loading Remote Text File via HTTP\nDESCRIPTION: Loads a text dataset directly from a remote URL using the `load_dataset` function. The `data_files` parameter is set to the URL of the remote file.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/nlp_load.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset = load_dataset(\"text\", data_files=\"https://huggingface.co/datasets/lhoestq/test/resolve/main/some_text.txt\")\n```\n\n----------------------------------------\n\nTITLE: Converting a Dataset to IterableDataset (Py)\nDESCRIPTION: This snippet demonstrates how to convert an existing `Dataset` object to an `IterableDataset` using the `to_iterable_dataset` function. This approach is noted to be faster than loading a dataset directly in streaming mode. The code also includes a comparison showing the slower approach using `load_dataset` with `streaming=True`.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/stream.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n\n# faster ðŸ‡\n>>> dataset = load_dataset(\"ethz/food101\")\n>>> iterable_dataset = dataset.to_iterable_dataset()\n\n# slower ðŸ¢\n>>> iterable_dataset = load_dataset(\"ethz/food101\", streaming=True)\n```\n\n----------------------------------------\n\nTITLE: Loading Dataset from a Loading Script\nDESCRIPTION: Loads a dataset using a custom loading script. The script handles dataset downloading, generation, and metadata definition.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/image_dataset.mdx#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n>>> dataset = load_dataset(\"path/to/my_dataset\")\n```\n\n----------------------------------------\n\nTITLE: Multiprocessing for Dataset Preparation in Python\nDESCRIPTION: Uses multiprocessing to speed up dataset downloading and preparation using the `num_proc` parameter in the `load_dataset` function.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/loading.mdx#_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\nfrom datasets import load_dataset\n\nimagenet = load_dataset(\"timm/imagenet-1k-wds\", num_proc=8)\nml_librispeech_spanish = load_dataset(\"facebook/multilingual_librispeech\", \"spanish\", num_proc=8)\n```\n\n----------------------------------------\n\nTITLE: Preparing TensorFlow Dataset\nDESCRIPTION: Prepares the dataset to be compatible with TensorFlow using the `prepare_tf_dataset` method. This creates a `tf.data.Dataset` with collation and batching, which can be directly used with Keras' `fit()` method for training.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/quickstart.mdx#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n>>> import tensorflow as tf\n\n>>> tf_dataset = model.prepare_tf_dataset(\n...     dataset,\n...     batch_size=4,\n...     shuffle=True,\n... )\n```\n\n----------------------------------------\n\nTITLE: Loading Data with PyTorch DataLoader\nDESCRIPTION: This snippet demonstrates how to use a Hugging Face `Dataset` with a PyTorch `DataLoader`. It shows how to create a dataset, format it for PyTorch, and then load it using the `DataLoader` for batch processing during training.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_pytorch.mdx#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n>>> import numpy as np\n>>> from datasets import Dataset \n>>> from torch.utils.data import DataLoader\n>>> data = np.random.rand(16)\n>>> label = np.random.randint(0, 2, size=16)\n>>> ds = Dataset.from_dict({\"data\": data, \"label\": label}).with_format(\"torch\")\n>>> dataloader = DataLoader(ds, batch_size=4)\n>>> for batch in dataloader:\n...     print(batch)\n```\n\n----------------------------------------\n\nTITLE: Loading a Dataset with Hugging Face Datasets in Python\nDESCRIPTION: This code snippet demonstrates how to load a dataset using the `load_dataset` function from the `datasets` library. It loads the 'mrpc' configuration of the 'glue' dataset from the 'nyu-mll' namespace and assigns the 'train' split to the `dataset` variable.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/process.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n>>> dataset = load_dataset(\"nyu-mll/glue\", \"mrpc\", split=\"train\")\n```\n\n----------------------------------------\n\nTITLE: Loading Data as Tensors on GPU\nDESCRIPTION: This snippet demonstrates how to load data as PyTorch tensors directly onto a GPU using the `device` argument in `ds.with_format(\"torch\", device=device)`.  It checks for GPU availability and defaults to CPU if a GPU is not available.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_pytorch.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> import torch\n>>> device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n>>> ds = ds.with_format(\"torch\", device=device)\n>>> ds[0]\n{'data': tensor([1, 2], device='cuda:0')}\n```\n\n----------------------------------------\n\nTITLE: Load Slices with pct1_dropremainder Percentage Rounding Python\nDESCRIPTION: This snippet demonstrates how to load slices of a dataset using `pct1_dropremainder` rounding to ensure equal-sized splits.  It shows how to use the `datasets.ReadInstruction` API and string notation with `pct1_dropremainder`.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/loading.mdx#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\n# 18 records, from 450 (included) to 468 (excluded).\n>>> train_50_52pct1_ds = datasets.load_dataset(\"ajibawa-2023/General-Stories-Collection\", split=datasets.ReadInstruction(\"train\", from_=50, to=52, unit=\"%\", rounding=\"pct1_dropremainder\"))\n# 18 records, from 468 (included) to 486 (excluded).\n>>> train_52_54pct1_ds = datasets.load_dataset(\"ajibawa-2023/General-Stories-Collection\", split=datasets.ReadInstruction(\"train\",from_=52, to=54, unit=\"%\", rounding=\"pct1_dropremainder\"))\n# Or equivalently:\n>>> train_50_52pct1_ds = datasets.load_dataset(\"ajibawa-2023/General-Stories-Collection\", split=\"train[50%:52%](pct1_dropremainder)\")\n>>> train_52_54pct1_ds = datasets.load_dataset(\"ajibawa-2023/General-Stories-Collection\", split=\"train[52%:54%](pct1_dropremainder)\")\n```\n\n----------------------------------------\n\nTITLE: Load Local Loading Script Python\nDESCRIPTION: This snippet demonstrates how to load a dataset using a local loading script. Pass `trust_remote_code=True` to allow ðŸ¤— Datasets to execute the loading script.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/loading.mdx#_snippet_36\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset = load_dataset(\"path/to/local/loading_script/loading_script.py\", split=\"train\", trust_remote_code=True)\n>>> dataset = load_dataset(\"path/to/local/loading_script\", split=\"train\", trust_remote_code=True)  # equivalent because the file has the same name as the directory\n```\n\n----------------------------------------\n\nTITLE: Resampling the Entire Audio Dataset\nDESCRIPTION: This code defines a `preprocess_function` that extracts the audio array and applies the feature extractor to resample the audio to the desired sampling rate. The `map` function with `batched=True` is used to efficiently resample the entire dataset in batches.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_dataset.mdx#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n>>> def preprocess_function(examples):\n...     audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n...     inputs = feature_extractor(\n...         audio_arrays, sampling_rate=feature_extractor.sampling_rate, max_length=16000, truncation=True\n...     )\n...     return inputs\n\n>>> dataset = dataset.map(preprocess_function, batched=True)\n```\n\n----------------------------------------\n\nTITLE: Removing Columns from a Dataset in Python\nDESCRIPTION: This code snippet demonstrates how to remove columns from a dataset using the `remove_columns` method. It shows removing a single column ('label') and then removing multiple columns ('sentence1' and 'sentence2').\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/process.mdx#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset = dataset.remove_columns(\"label\")\n>>> dataset\nDataset({\n    features: ['sentence1', 'sentence2', 'idx'],\n    num_rows: 3668\n})\n>>> dataset = dataset.remove_columns([\"sentence1\", \"sentence2\"])\n>>> dataset\nDataset({\n    features: ['idx'],\n    num_rows: 3668\n})\n```\n\n----------------------------------------\n\nTITLE: Setting Dataset Format for PyTorch\nDESCRIPTION: Sets the dataset format to `torch` and specifies the columns to format. This prepares the dataset for use with PyTorch. A DataLoader is then created.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/quickstart.mdx#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\n>>> import torch\n\n>>> dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"labels\"])\n>>> dataloader = torch.utils.data.DataLoader(dataset, batch_size=32)\n```\n\n----------------------------------------\n\nTITLE: Casting Features in Datasets (Datasets Library, Python)\nDESCRIPTION: This code demonstrates how to cast the feature type of one or more columns in a Hugging Face Dataset. It uses the `cast` function, taking a dictionary of new `Features` as an argument.  The example shows changing the `ClassLabel` and `Value` features, converting 'label' to 'negative'/'positive' and 'idx' to int64.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/process.mdx#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset.features\n{'sentence1': Value(dtype='string', id=None),\n'sentence2': Value(dtype='string', id=None),\n'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n'idx': Value(dtype='int32', id=None)}\n\n>>> from datasets import ClassLabel, Value\n>>> new_features = dataset.features.copy()\n>>> new_features[\"label\"] = ClassLabel(names=[\"negative\", \"positive\"])\n>>> new_features[\"idx\"] = Value(\"int64\")\n>>> dataset = dataset.cast(new_features)\n>>> dataset.features\n{'sentence1': Value(dtype='string', id=None),\n'sentence2': Value(dtype='string', id=None),\n'label': ClassLabel(names=['negative', 'positive'], id=None),\n'idx': Value(dtype='int64', id=None)}\n```\n\n----------------------------------------\n\nTITLE: Loading a Dataset in Streaming Mode (Py)\nDESCRIPTION: This snippet demonstrates how to load a dataset in streaming mode using the `load_dataset` function from the `datasets` library. Setting `streaming=True` avoids downloading the entire dataset at once. It also shows how to iterate through the streamed dataset and access individual samples.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/stream.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n>>> dataset = load_dataset('HuggingFaceFW/fineweb', split='train', streaming=True)\n>>> print(next(iter(dataset)))\n{'text': \"How AP reported in all formats from tornado-stricken regionsMarch 8, 2012\\nWhen the first serious bout of tornadoes of 2012 blew through middle America in the middle of the night, they touched down in places hours from any AP bureau...\"\n```\n\n----------------------------------------\n\nTITLE: Applying Data Augmentation Transform\nDESCRIPTION: Defines a function to apply the specified data augmentation transform to each image in the dataset. Converts the image to RGB mode before applying the transform. The transformed pixel values are stored in the `pixel_values` field of each example.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/quickstart.mdx#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n>>> def transforms(examples):\n...     examples[\"pixel_values\"] = [jitter(image.convert(\"RGB\")) for image in examples[\"image\"]]\n...     return examples\n```\n\n----------------------------------------\n\nTITLE: Load Tokenizer from Transformers\nDESCRIPTION: This code snippet demonstrates how to load a pre-trained tokenizer from the Hugging Face Transformers library. It initializes the tokenizer using the `AutoTokenizer.from_pretrained` method, which loads the tokenizer configuration and vocabulary from the specified pre-trained model ('bert-base-cased' in this case).\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/nlp_process.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from transformers import AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n```\n\n----------------------------------------\n\nTITLE: Loading Dataset from the Hub in Python\nDESCRIPTION: This Python code snippet demonstrates how to load a dataset from the Hugging Face Hub using the `load_dataset` function from the `datasets` library. It imports the necessary function and then calls `load_dataset` with the dataset identifier (e.g., \"<username>/my_dataset\").\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/dataset_script.mdx#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n>>> load_dataset(\"<username>/my_dataset\")\n```\n\n----------------------------------------\n\nTITLE: Loading a Dataset from the Hub in Python\nDESCRIPTION: This code shows how to load a dataset from the Hugging Face Hub using the `load_dataset` function.  It requires the dataset name, which typically follows the format `<username>/my_dataset`.  This demonstrates how to access and utilize a dataset that has been uploaded to the Hub.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/image_dataset.mdx#_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\n>>> from datasets import load_dataset\n>>> load_dataset(\"<username>/my_dataset\")\n```\n\n----------------------------------------\n\nTITLE: Tokenize and Return NumPy Arrays\nDESCRIPTION: This code snippet demonstrates how to tokenize a dataset and explicitly return the tensors as NumPy arrays using the `return_tensors=\"np\"` parameter in the tokenizer. This can improve performance, as NumPy arrays are a natively supported PyArrow format, which is used by the Hugging Face Datasets library.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/nlp_process.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset = dataset.map(lambda examples: tokenizer(examples[\"text\"], return_tensors=\"np\"), batched=True)\n```\n\n----------------------------------------\n\nTITLE: Parallelizing Data Loading with Multiple Workers\nDESCRIPTION: This snippet shows how to parallelize data loading using the `num_workers` argument in the PyTorch `DataLoader`. It also saves the dataset to disk and loads it back to simulate a large dataset.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_pytorch.mdx#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n>>> import numpy as np\n>>> from datasets import Dataset, load_from_disk\n>>> from torch.utils.data import DataLoader\n>>> data = np.random.rand(10_000)\n>>> Dataset.from_dict({\"data\": data}).save_to_disk(\"my_dataset\")\n>>> ds = load_from_disk(\"my_dataset\").with_format(\"torch\")\n>>> dataloader = DataLoader(ds, batch_size=32, num_workers=4)\n```\n\n----------------------------------------\n\nTITLE: Converting Datasets to PyTorch Tensors\nDESCRIPTION: This snippet demonstrates how to convert a Hugging Face Dataset to a PyTorch tensor format using `ds.with_format(\"torch\")`. This allows direct access to the data as `torch.Tensor` objects.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_pytorch.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import Dataset\n>>> data = [[1, 2],[3, 4]]\n>>> ds = Dataset.from_dict({\"data\": data})\n>>> ds = ds.with_format(\"torch\")\n>>> ds[0]\n{'data': tensor([1, 2])}\n>>> ds[:2]\n{'data': tensor([[1, 2],\n         [3, 4]])}\n```\n\n----------------------------------------\n\nTITLE: Importing Data from Pandas to Dataset\nDESCRIPTION: This snippet shows how to import a Pandas DataFrame into a Hugging Face Dataset using `Dataset.from_pandas`. Requires a Pandas DataFrame `df` to be defined beforehand.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_pandas.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nds = Dataset.from_pandas(df)\n```\n\n----------------------------------------\n\nTITLE: Tokenize and Pad with Custom Transform (with_transform)\nDESCRIPTION: This snippet demonstrates how to use `with_transform` to apply a custom tokenization and padding function on-the-fly using the `transformers` library. The `encode` function tokenizes `sentence1` and `sentence2` columns of a dataset, adds padding, truncates the sequence to a maximum length of 512, and returns PyTorch tensors.  It requires the `transformers` library and a pre-trained tokenizer.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/process.mdx#_snippet_28\n\nLANGUAGE: Python\nCODE:\n```\n>>> from transformers import AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n>>> def encode(batch):\n...     return tokenizer(batch[\"sentence1\"], batch[\"sentence2\"], padding=\"longest\", truncation=True, max_length=512, return_tensors=\"pt\")\n>>> dataset = dataset.with_transform(encode)\n>>> dataset.format\n{'type': 'custom', 'format_kwargs': {'transform': <function __main__.encode(batch)>}, 'columns': ['idx', 'label', 'sentence1', 'sentence2'], 'output_all_columns': False}\n```\n\n----------------------------------------\n\nTITLE: Mapping a Function to a Dataset (Datasets Library, Python)\nDESCRIPTION: This code demonstrates how to use the `map` function to apply a processing function to each example in a dataset. It adds a prefix to the 'sentence1' column by defining a function `add_prefix` and applying it to the dataset.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/process.mdx#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n>>> def add_prefix(example):\n...     example[\"sentence1\"] = 'My sentence: ' + example[\"sentence1\"]\n...     return example\n\n>>> updated_dataset = small_dataset.map(add_prefix)\n>>> updated_dataset[\"sentence1\"][:5]\n['My sentence: Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n\"My sentence: Yucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion .\",\n'My sentence: They had published an advertisement on the Internet on June 10 , offering the cargo for sale , he added .',\n'My sentence: Around 0335 GMT , Tab shares were up 19 cents , or 4.4 % , at A $ 4.56 , having earlier set a record high of A $ 4.57 .',\n]\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading IterableDataset State\nDESCRIPTION: This snippet demonstrates how to save and load the state of an IterableDataset for resuming training from a checkpoint. It uses `state_dict` to capture the current shard and example index, and `load_state_dict` to restore the dataset to that point.  This avoids re-reading already processed shards.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/stream.mdx#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n>>> iterable_dataset = Dataset.from_dict({\"a\": range(6)}).to_iterable_dataset(num_shards=3)\n>>> for idx, example in enumerate(iterable_dataset):\n...     print(example)\n...     if idx == 2:\n...         state_dict = iterable_dataset.state_dict()\n...         print(\"checkpoint\")\n...         break\n>>> iterable_dataset.load_state_dict(state_dict)\n>>> print(f\"restart from checkpoint\")\n>>> for example in iterable_dataset:\n...     print(example)\n```\n\n----------------------------------------\n\nTITLE: Instantiate Dataset from PyArrow Table\nDESCRIPTION: This snippet shows how to create a `Dataset` directly from a PyArrow Table using the `Dataset()` constructor.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_pyarrow.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nds = Dataset(table)\n```\n\n----------------------------------------\n\nTITLE: Reshuffling an IterableDataset per Epoch (Py)\nDESCRIPTION: This snippet demonstrates how to reshuffle a dataset after each epoch during training. It uses the `set_epoch` function to update the epoch number, which is then incorporated into the shuffling seed.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/stream.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> for epoch in range(epochs):\n...     shuffled_dataset.set_epoch(epoch)\n...     for example in shuffled_dataset:\n...         ...\n```\n\n----------------------------------------\n\nTITLE: Lazy Data Processing with IterableDataset.map in Python\nDESCRIPTION: Demonstrates lazy processing of an IterableDataset using the map function, applying processing functions on-the-fly during iteration.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nmy_iterable_dataset = my_iterable_dataset.map(process_fn_1)\nmy_iterable_dataset = my_iterable_dataset.filter(filter_fn)\nmy_iterable_dataset = my_iterable_dataset.map(process_fn_2)\n\n# process_fn_1, filter_fn and process_fn_2 are applied on-the-fly when iterating over the dataset\nfor example in my_iterable_dataset:  \n    print(example)\n    break\n```\n\n----------------------------------------\n\nTITLE: Loading Dataset from Pandas DataFrame Python\nDESCRIPTION: This snippet demonstrates how to create a Hugging Face Dataset from a Pandas DataFrame using the `Dataset.from_pandas` method. It initializes a Pandas DataFrame and then converts it into a Dataset object.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/loading.mdx#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import Dataset\n>>> import pandas as pd\n>>> df = pd.DataFrame({\"a\": [1, 2, 3]})\n>>> dataset = Dataset.from_pandas(df)\n```\n\n----------------------------------------\n\nTITLE: Save FAISS Index\nDESCRIPTION: Saves the FAISS index to disk for later use. This allows for persisting the index and reusing it without re-computing the embeddings.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/faiss_es.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> ds_with_embeddings.save_faiss_index('embeddings', 'my_index.faiss')\n```\n\n----------------------------------------\n\nTITLE: Applying Transformation to Dataset\nDESCRIPTION: Applies the previously defined transform function to the dataset using the `with_transform` function.  This will apply the transform on the fly when the data is accessed.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/quickstart.mdx#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset = dataset.with_transform(transforms)\n```\n\n----------------------------------------\n\nTITLE: Accessing Audio Data and Decoding\nDESCRIPTION: This code accesses the first row of the dataset and retrieves the `audio` column.  The audio is automatically decoded and resampled upon access.  The output displays the audio array, path to the audio file, and the current sampling rate.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_dataset.mdx#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset[0][\"audio\"]\n{'array': array([ 0.        ,  0.00024414, -0.00024414, ..., -0.00024414,\n         0.        ,  0.        ], dtype=float32),\n 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav',\n 'sampling_rate': 8000}\n```\n\n----------------------------------------\n\nTITLE: Setting Dataset Format for PyTorch\nDESCRIPTION: Sets the dataset format to PyTorch tensors using the `set_format` function, specifying the columns to format (`input_values` and `labels`). Then it creates a `DataLoader` to handle batching and shuffling of the data during training.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/quickstart.mdx#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n>>> from torch.utils.data import DataLoader\n\n>>> dataset.set_format(type=\"torch\", columns=[\"input_values\", \"labels\"])\n>>> dataloader = DataLoader(dataset, batch_size=4)\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Audio Data with Dataset.map\nDESCRIPTION: This snippet shows how to preprocess audio data using the `~Dataset.map` function in Hugging Face Datasets. It defines a `prepare_dataset` function that processes the audio array, extracts input values, and prepares labels using the provided processor.  It requires a defined `processor` and assumes the dataset has an 'audio' and 'sentence' column.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/audio_process.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> def prepare_dataset(batch):\n...     audio = batch[\"audio\"]\n...     batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n...     batch[\"input_length\"] = len(batch[\"input_values\"])\n...     with processor.as_target_processor():\n...         batch[\"labels\"] = processor(batch[\"sentence\"]).input_ids\n...     return batch\n>>> dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names)\n```\n\n----------------------------------------\n\nTITLE: Loading a Dataset from CSV files with Hugging Face Datasets\nDESCRIPTION: This code snippet demonstrates how to load a dataset from one or more CSV files using the `load_dataset` function from the `datasets` library. It takes the \"csv\" format as the first argument and a list of CSV file paths as the `data_files` argument.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/create_dataset.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n>>> dataset = load_dataset(\"csv\", data_files=\"my_file.csv\")\n```\n\n----------------------------------------\n\nTITLE: Sharding an IterableDataset (Py)\nDESCRIPTION: This snippet demonstrates how to shard an `IterableDataset` using the `to_iterable_dataset` function with the `num_shards` parameter.  It also shows how to shuffle the sharded dataset and use it with a PyTorch DataLoader for parallel loading. This is useful for large datasets and enabling fast parallel processing.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/stream.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> import torch\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"ethz/food101\")\n>>> iterable_dataset = dataset.to_iterable_dataset(num_shards=64) # shard the dataset\n>>> iterable_dataset = iterable_dataset.shuffle(buffer_size=10_000)  # shuffles the shards order and use a shuffle buffer when you start iterating\ndataloader = torch.utils.data.DataLoader(iterable_dataset, num_workers=4)  # assigns 64 / 4 = 16 shards from the shuffled list of shards to each worker when you start iterating\n```\n\n----------------------------------------\n\nTITLE: Casting a Single Column in Datasets (Datasets Library, Python)\nDESCRIPTION: This code shows how to change the feature type of a single column using the `cast_column` function. It takes the column name and its new feature type as arguments. The example demonstrates changing the sampling rate of the 'audio' column.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/process.mdx#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset.features\n{'audio': Audio(sampling_rate=44100, mono=True, id=None)}\n\n>>> dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n>>> dataset.features\n{'audio': Audio(sampling_rate=16000, mono=True, id=None)}\n```\n\n----------------------------------------\n\nTITLE: Loading Audio Dataset from Local Files in Python\nDESCRIPTION: This snippet demonstrates how to load an audio dataset from local audio files using the `Dataset.from_dict` and `cast_column` functions. It requires the `datasets` library and the `Audio` feature. The code creates a dataset from a dictionary containing paths to audio files and casts the 'audio' column to the Audio feature.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/audio_load.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> audio_dataset = Dataset.from_dict({\"audio\": [\"path/to/audio_1\", \"path/to/audio_2\", ..., \"path/to/audio_n\"]}).cast_column(\"audio\", Audio())\n>>> audio_dataset[0][\"audio\"]\n{'array': array([ 0.        ,  0.00024414, -0.00024414, ..., -0.00024414,\n         0.        ,  0.        ], dtype=float32),\n 'path': 'path/to/audio_1',\n 'sampling_rate': 16000}\n```\n\n----------------------------------------\n\nTITLE: Loading Percentage of Split with ReadInstruction Python\nDESCRIPTION: This snippet demonstrates how to load a percentage (10%) of the 'train' split of a dataset using the `datasets.ReadInstruction` API with `datasets.load_dataset`. It loads the 'ajibawa-2023/General-Stories-Collection' dataset and specifies the split using `ReadInstruction` with percentage units.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/loading.mdx#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\n>>> train_10_20_ds = datasets.load_dataset(\"ajibawa-2023/General-Stories-Collection\", split=datasets.ReadInstruction(\"train\", to=10, unit=\"%\"))\n```\n\n----------------------------------------\n\nTITLE: Formatting Specific Columns with TensorFlow\nDESCRIPTION: This example illustrates how to format certain columns of a dataset with TensorFlow while leaving other columns unformatted. It formats only the 'data' column to TensorFlow, leaving the 'text' column as Python objects.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_tensorflow.mdx#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> ds = ds.with_format(\"tf\", columns=[\"data\"], output_all_columns=True)\n>>> ds[:2]\n{'data': <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 1])>,\n 'text': ['foo', 'bar']}\n```\n\n----------------------------------------\n\nTITLE: Loading ImageFolder Dataset\nDESCRIPTION: Loads an image dataset using the `ImageFolder` builder. It assumes the dataset is structured in a directory where subdirectories represent class labels.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/image_dataset.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"path/to/folder\")\n```\n\n----------------------------------------\n\nTITLE: Load Dataset with Custom Features Python\nDESCRIPTION: This snippet demonstrates how to load a dataset with custom features, which you can add custom labels with the `ClassLabel` feature. It specifies the `features` parameter in `load_dataset` with the features you just created.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/loading.mdx#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset = load_dataset('csv', data_files=file_dict, delimiter=';', column_names=['text', 'label'], features=emotion_features)\n```\n\n----------------------------------------\n\nTITLE: Concatenating Datasets with `concatenate_datasets`\nDESCRIPTION: This code snippet demonstrates how to concatenate two datasets using the `concatenate_datasets` function. It loads two datasets, 'ajibawa-2023/General-Stories-Collection' and 'wikimedia/wikipedia', removes all columns except 'text', and then concatenates them into a single dataset named `bert_dataset`.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/process.mdx#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import concatenate_datasets, load_dataset\n\n>>> stories = load_dataset(\"ajibawa-2023/General-Stories-Collection\", split=\"train\")\n>>> stories = stories.remove_columns([col for col in stories.column_names if col != \"text\"])  # only keep the 'text' column\n>>> wiki = load_dataset(\"wikimedia/wikipedia\", \"20220301.en\", split=\"train\")\n>>> wiki = wiki.remove_columns([col for col in wiki.column_names if col != \"text\"])  # only keep the 'text' column\n\n>>> assert stories.features.type == wiki.features.type\n>>> bert_dataset = concatenate_datasets([stories, wiki])\n```\n\n----------------------------------------\n\nTITLE: Loading Dataset from Hugging Face Hub in Python\nDESCRIPTION: This snippet demonstrates how to load a dataset from the Hugging Face Hub using the `load_dataset` function.  It assumes the `datasets` library is installed. The function takes the dataset name as a string parameter and returns a `Dataset` object.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/share.mdx#_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\ndataset = load_dataset(\"Wauplin/my-cool-dataset\")\n```\n\n----------------------------------------\n\nTITLE: Slicing a Dataset\nDESCRIPTION: This snippet shows how to extract a subset of a `Dataset` object using slicing. It demonstrates retrieving the first three rows and a range of rows using the `:` operator.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/access.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Get the first three rows\n>>> dataset[:3]\n{'label': [1, 1, 1],\n 'text': ['the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .',\n  'the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson\\'s expanded vision of j . r . r . tolkien\\'s middle-earth .',\n  'effective but too-tepid biopic']}\n```\n\nLANGUAGE: python\nCODE:\n```\n# Get rows between three and six\n>>> dataset[3:6]\n{'label': [1, 1, 1],\n 'text': ['if you sometimes like to go to the movies to have fun , wasabi is a good place to start .',\n  \"emerges as something rare , an issue movie that\\'s so honest and keenly observed that it doesn\\'t feel like one .\",\n  'the film provides some great insight into the neurotic mindset of all comics -- even those who have reached the absolute top of the game .']}\n```\n\n----------------------------------------\n\nTITLE: Loading CSV files with Hugging Face Datasets\nDESCRIPTION: This code snippet demonstrates how to load CSV files into a Hugging Face Dataset using the `load_dataset` function with the 'csv' dataset builder.  It covers loading a single CSV file, multiple CSV files, mapping files to train/test splits, loading remote files via URLs, and loading zipped files.  It requires the `datasets` library.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/tabular_load.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n>>> dataset = load_dataset(\"csv\", data_files=\"my_file.csv\")\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset = load_dataset(\"csv\", data_files=[\"my_file_1.csv\", \"my_file_2.csv\", \"my_file_3.csv\"])\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset = load_dataset(\"csv\", data_files={\"train\": [\"my_train_file_1.csv\", \"my_train_file_2.csv\"], \"test\": \"my_test_file.csv\"})\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> base_url = \"https://huggingface.co/datasets/lhoestq/demo1/resolve/main/data/\"\n>>> dataset = load_dataset('csv', data_files={\"train\": base_url + \"train.csv\", \"test\": base_url + \"test.csv\"})\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> url = \"https://domain.org/train_data.zip\"\n>>> data_files = {\"train\": url}\n>>> dataset = load_dataset(\"csv\", data_files=data_files)\n```\n\n----------------------------------------\n\nTITLE: Streaming Data with IterableDataset and DataLoader\nDESCRIPTION: This snippet demonstrates how to stream data from a remote dataset using an `IterableDataset` and a `DataLoader`. It uses `load_dataset` with `streaming=True` and then loads the iterable dataset using `DataLoader`.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_pytorch.mdx#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n>>> import numpy as np\n>>> from datasets import Dataset, load_dataset\n>>> from torch.utils.data import DataLoader\n>>> data = np.random.rand(10_000)\n>>> Dataset.from_dict({\"data\": data}).push_to_hub(\"<username>/my_dataset\")  # Upload to the Hugging Face Hub\n>>> my_iterable_dataset = load_dataset(\"<username>/my_dataset\", streaming=True, split=\"train\")\n>>> dataloader = DataLoader(my_iterable_dataset, batch_size=32)\n```\n\n----------------------------------------\n\nTITLE: Setting Dataset Format In-place\nDESCRIPTION: This code snippet demonstrates the use of `set_format` to modify the dataset format in place. Here it sets the format to PyTorch tensors.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/process.mdx#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset.set_format(type=\"torch\")\n```\n\n----------------------------------------\n\nTITLE: Loading Dataset with Remote Code and Datasets\nDESCRIPTION: This code demonstrates loading a dataset that requires executing remote code. The `trust_remote_code=True` argument is essential for allowing the execution of dataset loading scripts. Failure to include this argument will result in an error due to security restrictions. Requires `datasets` library.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/load_hub.mdx#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import get_dataset_config_names, get_dataset_split_names, load_dataset\n\n>>> c4 = load_dataset(\"c4\", \"en\", split=\"train\", trust_remote_code=True)\n>>> get_dataset_config_names(\"c4\", trust_remote_code=True)\n['en', 'realnewslike', 'en.noblocklist', 'en.noclean']\n>>> get_dataset_split_names(\"c4\", \"en\", trust_remote_code=True)\n['train', 'validation']\n```\n\n----------------------------------------\n\nTITLE: Defining Dataset Splits with Python\nDESCRIPTION: Implements the `_split_generators` method to download the dataset and define the train/test splits. It uses `dl_manager.download` to download the data and metadata, and `datasets.SplitGenerator` to define the splits.  `dl_manager.iter_archive` enables audio streaming from the archive.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/audio_dataset.mdx#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef _split_generators(self, dl_manager):\n    \"\"\"Returns SplitGenerators.\"\"\"\n    prompts_paths = dl_manager.download(_PROMPTS_URLS)\n    archive = dl_manager.download(_DATA_URL)\n    train_dir = \"vivos/train\"\n    test_dir = \"vivos/test\"\n\n    return [\n        datasets.SplitGenerator(\n            name=datasets.Split.TRAIN,\n            gen_kwargs={\n                \"prompts_path\": prompts_paths[\"train\"],\n                \"path_to_clips\": train_dir + \"/waves\",\n                \"audio_files\": dl_manager.iter_archive(archive),\n            },\n        ),\n        datasets.SplitGenerator(\n            name=datasets.Split.TEST,\n            gen_kwargs={\n                \"prompts_path\": prompts_paths[\"test\"],\n                \"path_to_clips\": test_dir + \"/waves\",\n                \"audio_files\": dl_manager.iter_archive(archive),\n            },\n        ),\n    ]\n```\n\n----------------------------------------\n\nTITLE: Preparing TensorFlow Dataset\nDESCRIPTION: Prepares the dataset for use with TensorFlow. The prepare_tf_dataset method from ðŸ¤— Transformers creates a tf.data.Dataset that is compatible with TensorFlow models.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/quickstart.mdx#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n>>> import tensorflow as tf\n\n>>> tf_dataset = model.prepare_tf_dataset(\n...     dataset,\n...     batch_size=4,\n...     shuffle=True,\n... )\n```\n\n----------------------------------------\n\nTITLE: Processing Data with Polars Functions\nDESCRIPTION: This snippet shows how to use Polars functions within `Dataset.map` and `Dataset.filter` to efficiently process data.  It demonstrates adding a new column and filtering based on column values using Polars expressions. Using `batched=True` is recommended for performance.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_polars.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> import polars as pl\n>>> from datasets import Dataset\n>>> data = {\"col_0\": [\"a\", \"b\", \"c\", \"d\"], \"col_1\": [0., 0., 1., 1.]}\n>>> ds = Dataset.from_dict(data)\n>>> ds = ds.with_format(\"polars\")\n>>> ds = ds.map(lambda df: df.with_columns(pl.col(\"col_1\").add(1).alias(\"col_2\")), batched=True)\n>>> ds[:2]\nshape: (2, 3)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ col_0 â”† col_1 â”† col_2 â”‚\nâ”‚ ---   â”† ---   â”† f64   â”‚\nâ•žâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•¡\nâ”‚ a     â”† 0.0   â”† 1.0   â”‚\nâ”‚ b     â”† 0.0   â”† 1.0   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n>>> ds = ds.filter(lambda df: df[\"col_0\"] == \"b\", batched=True)\n>>> ds[0]\nshape: (1, 3)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ col_0 â”† col_1 â”† col_2 â”‚\nâ”‚ ---   â”† ---   â”† f64   â”‚\nâ•žâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•¡\nâ”‚ b     â”† 0.0   â”† 1.0   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n----------------------------------------\n\nTITLE: Mapping with Indices (Datasets Library, Python)\nDESCRIPTION: This code demonstrates how to use the `map` function with indices by setting `with_indices=True`. It adds the index to the beginning of each sentence in the 'sentence2' column.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/process.mdx#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n>>> updated_dataset = dataset.map(lambda example, idx: {\"sentence2\": f\"{idx}: \" + example[\"sentence2\"]}, with_indices=True)\n>>> updated_dataset[\"sentence2\"][:5]\n['0: Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n \"1: Yucaipa bought Dominick 's in 1995 for $ 693 million and sold it to Safeway for $ 1.8 billion in 1998 .\",\n \"2: On June 10 , the ship 's owners had published an advertisement on the Internet , offering the explosives for sale .\",\n '3: Tab shares jumped 20 cents , or 4.6 % , to set a record closing high at A $ 4.57 .',\n '4: PG & E Corp. shares jumped $ 1.63 or 8 percent to $ 21.03 on the New York Stock Exchange on Friday .'\n]\n```\n\n----------------------------------------\n\nTITLE: Shuffling a Dataset in Python\nDESCRIPTION: This code snippet demonstrates how to shuffle a dataset using the `shuffle` method. It shuffles a sorted dataset with a seed for reproducibility and prints the first 10 labels of the shuffled dataset. It also demonstrates how to switch to an `IterableDataset` and use its fast approximate shuffling.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/process.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> shuffled_dataset = sorted_dataset.shuffle(seed=42)\n>>> shuffled_dataset[\"label\"][:10]\n[1, 1, 1, 0, 1, 1, 1, 1, 1, 0]\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> iterable_dataset = dataset.to_iterable_dataset(num_shards=128)\n>>> shuffled_iterable_dataset = iterable_dataset.shuffle(seed=42, buffer_size=1000)\n```\n\n----------------------------------------\n\nTITLE: Setting Dataset format to Polars\nDESCRIPTION: This snippet demonstrates how to set the format of a Hugging Face Dataset to 'polars'. This allows you to work with the data as Polars DataFrames and Series directly, enabling zero-copy operations since both `datasets` and Polars use Arrow under the hood.\n\nRequires `datasets` and `polars` to be installed.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_polars.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import Dataset\n>>> data = {\"col_0\": [\"a\", \"b\", \"c\", \"d\"], \"col_1\": [0., 0., 1., 1.]}\n>>> ds = Dataset.from_dict(data)\n>>> ds = ds.with_format(\"polars\")\n>>> ds[0]       # pl.DataFrame\nshape: (1, 2)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ col_0 â”† col_1 â”‚\nâ”‚ ---   â”† ---   â”‚\nâ”‚ str   â”† f64   â”‚\nâ•žâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•¡\nâ”‚ a     â”† 0.0   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n>>> ds[:2]      # pl.DataFrame\nshape: (2, 2)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ col_0 â”† col_1 â”‚\nâ”‚ ---   â”† ---   â”‚\nâ”‚ str   â”† f64   â”‚\nâ•žâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•¡\nâ”‚ a     â”† 0.0   â”‚\nâ”‚ b     â”† 0.0   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n>>> ds[\"data\"]  # pl.Series\nshape: (4,)\nSeries: 'col_0' [str]\n[\n        \"a\"\n        \"b\"\n        \"c\"\n        \"d\"\n]\n```\n\n----------------------------------------\n\nTITLE: Streaming a Dataset in Python\nDESCRIPTION: This code snippet shows how to stream a dataset using the `load_dataset` function with `streaming=True`. It iterates over the dataset without downloading the entire dataset to disk.\nSOURCE: https://github.com/huggingface/datasets/blob/main/README.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# If you want to use the dataset immediately and efficiently stream the data as you iterate over the dataset\nimage_dataset = load_dataset('timm/imagenet-1k-wds', streaming=True)\nfor example in image_dataset[\"train\"]:\n    break\n```\n\n----------------------------------------\n\nTITLE: Casting Audio Column with Resampling in Datasets\nDESCRIPTION: This snippet demonstrates how to cast an audio column in a Hugging Face Dataset to a new feature with a specified sampling rate using `dataset.cast_column` and the `Audio` feature. It resamples the audio data on-the-fly when accessed. It requires the `datasets` library.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/audio_process.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset, Audio\n\n>>> dataset = load_dataset(\"PolyAI/minds14\", \"en-US\", split=\"train\")\n>>> dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n```\n\n----------------------------------------\n\nTITLE: Loading Dataset from Python Generator Python\nDESCRIPTION: This snippet demonstrates how to create a Hugging Face Dataset from a Python generator using the `Dataset.from_generator` method. It defines a generator function that yields dictionaries and then converts it into a Dataset object. This method supports loading data larger than available memory.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/loading.mdx#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import Dataset\n>>> def my_gen():\n...     for i in range(1, 4):\n...         yield {\"a\": i}\n...\n>>> dataset = Dataset.from_generator(my_gen)\n```\n\n----------------------------------------\n\nTITLE: Sorting a Dataset by Label in Python\nDESCRIPTION: This code snippet demonstrates how to sort a dataset by the 'label' column using the `sort` method. It first prints the first 10 labels, then sorts the dataset, and prints the first and last 10 labels of the sorted dataset. This showcases the effect of sorting on the 'label' column.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/process.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset[\"label\"][:10]\n[1, 0, 1, 0, 1, 1, 0, 1, 0, 0]\n>>> sorted_dataset = dataset.sort(\"label\")\n>>> sorted_dataset[\"label\"][:10]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n>>> sorted_dataset[\"label\"][-10:]\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n```\n\n----------------------------------------\n\nTITLE: Loading a Dataset with Random Access in Python\nDESCRIPTION: Demonstrates how to load a standard Dataset, providing random access to rows, which requires downloading the entire dataset.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n\nimagenet = load_dataset(\"timm/imagenet-1k-wds\", split=\"train\")  # downloads the full dataset\nprint(imagenet[0])\n```\n\n----------------------------------------\n\nTITLE: Mapping a Function on a Pandas DataFrame Dataset\nDESCRIPTION: This code demonstrates how to use the `map` function with a Pandas DataFrame formatted dataset. It defines a lambda function to create a new column 'upper_text' by converting the 'text' column to uppercase.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/process.mdx#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\n>>> ds = ds.map(lambda df: df.assign(upper_text=df.text.str.upper()), batched=True)\n>>> ds[:2]\n  text  label upper_text\n0  foo      0        FOO\n1  bar      1        BAR\n```\n\n----------------------------------------\n\nTITLE: Loading data from SQLite database with SQL query\nDESCRIPTION: This code loads data from a SQLite database using a SQL query.  It connects to the database using a URI, executes the SQL query, and loads the results into a Hugging Face Dataset using `Dataset.from_sql`.  It also demonstrates filtering the dataset. Requires the `datasets` library.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/tabular_load.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import Dataset\n\n>>> uri = \"sqlite:///us_covid_data.db\"\n>>> ds = Dataset.from_sql('SELECT * FROM states WHERE state=\"California\";', uri)\n>>> ds\nDataset({\n    features: ['index', 'date', 'state', 'fips', 'cases', 'deaths'],\n    num_rows: 1019\n})\n>>> ds.filter(lambda x: x[\"cases\"] > 10000)\n```\n\n----------------------------------------\n\nTITLE: Disabling Audio Decoding in Python\nDESCRIPTION: This snippet demonstrates how to disable audio decoding to access the raw audio data (path/bytes) instead of NumPy arrays. It uses the `decode(False)` method on the dataset. This is available for streaming datasets only.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/audio_load.mdx#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset = dataset.decode(False)\n```\n\n----------------------------------------\n\nTITLE: Converting Entire Dataset to TensorFlow Tensors\nDESCRIPTION: This snippet shows how to convert an entire dataset to TensorFlow tensors by querying the full dataset using slicing (`ds[:]`). This is useful when you want to work with the entire dataset as a single tensor.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_tensorflow.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> ds[:]\n{'data': <tf.Tensor: shape=(2, 2), dtype=int64, numpy=\narray([[1, 2],\n       [3, 4]])>}\n```\n\n----------------------------------------\n\nTITLE: Using the tf.data.Dataset for Keras Model Training\nDESCRIPTION: This snippet shows how to use the converted `tf.data.Dataset` object directly with Keras' `model.fit()` method for training. It passes the `tf_ds` object to `model.fit()` without specifying the batch size.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_tensorflow.mdx#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n>>> model.fit(tf_ds, epochs=2)\n```\n\n----------------------------------------\n\nTITLE: Interleaving Datasets with Probabilities\nDESCRIPTION: This code demonstrates how to interleave three datasets using the `interleave_datasets` function with specified probabilities for each dataset. It creates three sample datasets `d1`, `d2`, and `d3`, and then interleaves them with probabilities `[0.3, 0.5, 0.2]` and a seed of 42.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/process.mdx#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import Dataset, interleave_datasets\n>>> seed = 42\n>>> probabilities = [0.3, 0.5, 0.2]\n>>> d1 = Dataset.from_dict({\"a\": [0, 1, 2]})\n>>> d2 = Dataset.from_dict({\"a\": [10, 11, 12, 13]})\n>>> d3 = Dataset.from_dict({\"a\": [20, 21, 22]})\n>>> dataset = interleave_datasets([d1, d2, d3], probabilities=probabilities, seed=seed)\n>>> dataset[\"a\"]\n[10, 11, 20, 12, 0, 21, 13]\n```\n\n----------------------------------------\n\nTITLE: Mapping a Data File to a Specific Split with load_dataset in Python\nDESCRIPTION: Maps a data file to a specific split using the `split` parameter along with the `data_files` parameter. This is useful for loading specific parts of a large dataset.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/loading.mdx#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n>>> data_files = {\"validation\": \"en/c4-validation.*.json.gz\"}\n>>> c4_validation = load_dataset(\"allenai/c4\", data_files=data_files, split=\"validation\")\n```\n\n----------------------------------------\n\nTITLE: Streaming an IterableDataset in Python\nDESCRIPTION: Shows how to load an IterableDataset using streaming, allowing access to large datasets without downloading them entirely to disk.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n\nimagenet = load_dataset(\"timm/imagenet-1k-wds\", split=\"train\", streaming=True)  # will start loading the data when iterated over\nfor example in imagenet:\n    print(example)\n    break\n```\n\n----------------------------------------\n\nTITLE: Filtering Rows in a Dataset by Condition in Python\nDESCRIPTION: This code snippet demonstrates how to filter rows in a dataset based on a condition using the `filter` method. It filters the dataset to include only rows where the 'sentence1' column starts with 'Ar'. It also showcases filtering by indices using `with_indices=True` to only keep even-indexed rows.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/process.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> start_with_ar = dataset.filter(lambda example: example[\"sentence1\"].startswith(\"Ar\"))\n>>> len(start_with_ar)\n6\n>>> start_with_ar[\"sentence1\"]\n['Around 0335 GMT , Tab shares were up 19 cents , or 4.4 % , at A $ 4.56 , having earlier set a record high of A $ 4.57 .',\n'Arison said Mann may have been one of the pioneers of the world music movement and he had a deep love of Brazilian music .',\n'Arts helped coach the youth on an eighth-grade football team at Lombardi Middle School in Green Bay .',\n'Around 9 : 00 a.m. EDT ( 1300 GMT ) , the euro was at $ 1.1566 against the dollar , up 0.07 percent on the day .',\n\"Arguing that the case was an isolated example , Canada has threatened a trade backlash if Tokyo 's ban is not justified on scientific grounds .\",\n'Artists are worried the plan would harm those who need help most - performers who have a difficult time lining up shows .'\n]\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> even_dataset = dataset.filter(lambda example, idx: idx % 2 == 0, with_indices=True)\n>>> len(even_dataset)\n1834\n>>> len(dataset) / 2\n1834.0\n```\n\n----------------------------------------\n\nTITLE: Sampling Text Data by Paragraph/Document\nDESCRIPTION: Loads text data and samples it by paragraph or document using the `sample_by` parameter in the `load_dataset` function. This allows loading larger text units instead of the default line-by-line sampling.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/nlp_load.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Sample by paragraph\n>>> dataset = load_dataset(\"text\", data_files={\"train\": \"my_train_file.txt\", \"test\": \"my_test_file.txt\"}, sample_by=\"paragraph\")\n\n# Sample by document\n>>> dataset = load_dataset(\"text\", data_files={\"train\": \"my_train_file.txt\", \"test\": \"my_test_file.txt\"}, sample_by=\"document\")\n```\n\n----------------------------------------\n\nTITLE: Setting Epoch for IterableDataset Shuffling (Python)\nDESCRIPTION: This code demonstrates how to set the epoch for an `IterableDataset` to control shuffling behavior across multiple epochs. The effective seed for shuffling is calculated as `seed + epoch`, allowing for reshuffling at each epoch. This helps in creating different shuffles for each training epoch.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfor epoch in range(n_epochs):\n    my_iterable_dataset.set_epoch(epoch)\n    for example in my_iterable_dataset:  # fast + reshuffled at each epoch using `effective_seed = seed + epoch`\n        pass\n```\n\n----------------------------------------\n\nTITLE: Speeding Up Audio Decoding with Multithreading in Python\nDESCRIPTION: This snippet demonstrates how to speed up audio decoding using multithreading in the Hugging Face Datasets library. It calculates the number of threads to use based on the CPU count and then calls the `decode` function on the dataset with the calculated `num_threads` parameter.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/audio_load.mdx#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> import os\n>>> num_threads = num_threads = min(32, (os.cpu_count() or 1) + 4)\n>>> dataset = dataset.decode(num_threads=num_threads)\n>>> for example in dataset:  # up to 20 times faster !\n...     ...\n```\n\n----------------------------------------\n\nTITLE: Loading Video Paths without Decoding\nDESCRIPTION: This snippet demonstrates how to load only the video file paths without decoding the video objects. By setting `decode=False` in the `Video` feature, the dataset will contain dictionaries with the file path instead of `VideoReader` objects. This can be useful to reduce memory usage or for deferred decoding.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/video_load.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset = dataset.cast_column(\"video\", Video(decode=False))\n>>> dataset[0][\"video\"]\n{'bytes': None,\n 'path': 'path/to/video/folder/video0.mp4'}\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Audio Data\nDESCRIPTION: Defines a `preprocess_function` to extract the audio arrays, apply the feature extractor with padding, truncation and specified sampling rate and then maps it to the dataset. This prepares the audio data for the model by extracting features and ensuring consistent input lengths.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/quickstart.mdx#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n>>> def preprocess_function(examples):\n...     audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n...     inputs = feature_extractor(\n...         audio_arrays,\n...         sampling_rate=16000,\n...         padding=True,\n...         max_length=100000,\n...         truncation=True,\n...     )\n...     return inputs\n\n>>> dataset = dataset.map(preprocess_function, batched=True)\n```\n\n----------------------------------------\n\nTITLE: Loading a Parquet File with load_dataset in Python\nDESCRIPTION: Loads a dataset from a local or remote Parquet file using the `load_dataset` function. The 'parquet' data loading script must be specified, and data_files must be a dictionary mapping splits to file paths.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/loading.mdx#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\n>>> from datasets import load_dataset\n>>> dataset = load_dataset(\"parquet\", data_files={'train': 'train.parquet', 'test': 'test.parquet'})\n```\n\n----------------------------------------\n\nTITLE: Loading Subset of C4 Dataset\nDESCRIPTION: Loads a subset of the C4 dataset using grep patterns to select specific files. The `data_files` parameter specifies the file pattern to match.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/nlp_load.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n>>> c4_subset = load_dataset(\"allenai/c4\", data_files=\"en/c4-train.0000*-of-01024.json.gz\")\n```\n\n----------------------------------------\n\nTITLE: Using Image Feature Type with Datasets\nDESCRIPTION: This snippet demonstrates how to use the `Image` feature type with `datasets`.  It shows that the image data is loaded as tensors. Note that the `vision` extra must be installed (`pip install datasets[vision]`).\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_pytorch.mdx#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import Dataset, Features, Audio, Image\n>>> images = [\"path/to/image.png\"] * 10\n>>> features = Features({\"image\": Image()})\n>>> ds = Dataset.from_dict({\"image\": images}, features=features) \n>>> ds = ds.with_format(\"torch\")\n>>> ds[0][\"image\"].shape\ntorch.Size([512, 512, 4])\n>>> ds[0]\n{'image': tensor([[[255, 215, 106, 255],\n         [255, 215, 106, 255],\n         ...,\n         [255, 255, 255, 255],\n         [255, 255, 255, 255]]], dtype=torch.uint8)}\n>>> ds[:2][\"image\"].shape\ntorch.Size([2, 512, 512, 4])\n>>> ds[:2]\n{'image': tensor([[[[255, 215, 106, 255],\n          [255, 215, 106, 255],\n          ...,\n          [255, 255, 255, 255],\n          [255, 255, 255, 255]]]], dtype=torch.uint8)}\n```\n\n----------------------------------------\n\nTITLE: Renaming the Label Column\nDESCRIPTION: Renames the `label` column to `labels` using the `map` function. This is necessary because the BertForSequenceClassification model expects the input label column to be named `labels`.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/quickstart.mdx#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset = dataset.map(lambda examples: {\"labels\": examples[\"label\"]}, batched=True)\n```\n\n----------------------------------------\n\nTITLE: Resizing images with Dataset.map in Python\nDESCRIPTION: This snippet defines a transformation function that resizes images to a specified size (100x100) and converts them to RGB format. It is then applied to a dataset using `dataset.map` with `batched=True` for efficiency. The 'image' column is removed after the transformation. The function expects an input dictionary `examples` containing image data under the \"image\" key and returns the modified dictionary with resized images stored under the 'pixel_values' key.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/image_process.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> def transforms(examples):\n...     examples[\"pixel_values\"] = [image.convert(\"RGB\").resize((100,100)) for image in examples[\"image\"]]\n...     return examples\n```\n\n----------------------------------------\n\nTITLE: Training Loop with IterableDataset in PyTorch\nDESCRIPTION: This snippet sets up a training loop using an IterableDataset in PyTorch. It initializes a DataLoader with a DataCollatorForLanguageModeling, loads a pre-trained DistilBERT model, defines an optimizer, and iterates through the dataset in epochs. The loss is calculated, backpropagated, and the optimizer updates the model parameters.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/stream.mdx#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n>>> import torch\n>>> from torch.utils.data import DataLoader\n>>> from transformers import AutoModelForMaskedLM, DataCollatorForLanguageModeling\n>>> from tqdm import tqdm\n>>> dataset = dataset.with_format(\"torch\")\n>>> dataloader = DataLoader(dataset, collate_fn=DataCollatorForLanguageModeling(tokenizer))\n>>> device = 'cuda' if torch.cuda.is_available() else 'cpu'\n>>> model = AutoModelForMaskedLM.from_pretrained(\"distilbert-base-uncased\")\n>>> model.train().to(device)\n>>> optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-5)\n>>> for epoch in range(3):\n...     dataset.set_epoch(epoch)\n...     for i, batch in enumerate(tqdm(dataloader, total=5)):\n...         if i == 5:\n...             break\n...         batch = {k: v.to(device) for k, v in batch.items()}\n...         outputs = model(**batch)\n...         loss = outputs[0]\n...         loss.backward()\n...         optimizer.step()\n...         optimizer.zero_grad()\n...         if i % 10 == 0:\n...             print(f\"loss: {loss}\")\n```\n\n----------------------------------------\n\nTITLE: Batching a Dataset\nDESCRIPTION: This code demonstrates how to use the `batch()` method to group samples from a dataset into batches of a specified size. It loads the 'cornell-movie-review-data/rotten_tomatoes' dataset and creates batches of size 4.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/process.mdx#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n>>> dataset = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\", split=\"train\")\n>>> batched_dataset = dataset.batch(batch_size=4)\n>>> batched_dataset[0]\n{'text': ['the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .',\n        'the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson\\'s expanded vision of j . r . r . tolkien\\'s middle-earth .',\n        'effective but too-tepid biopic',\n        'if you sometimes like to go to the movies to have fun , wasabi is a good place to start .'],\n'label': [1, 1, 1, 1]}\n```\n\n----------------------------------------\n\nTITLE: Define Label Mapping for NLI Datasets\nDESCRIPTION: This code snippet shows how to define a label mapping for Natural Language Inference (NLI) datasets.  It creates a dictionary `label2id` that maps the textual labels ('contradiction', 'neutral', 'entailment') to numerical IDs (0, 1, 2).  This is often necessary because different models or datasets may use different label mappings.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/nlp_process.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> label2id = {\"contradiction\": 0, \"neutral\": 1, \"entailment\": 2}\n```\n\n----------------------------------------\n\nTITLE: Iterating Audio Files and Yielding Examples in Python\nDESCRIPTION: This code snippet iterates through audio files, reads their content, and combines them with metadata from the `examples` dictionary to yield dataset examples. It checks if the current path is inside the `path_to_clips` directory, and if the path exists in the `examples` dictionary, it reads the audio data and yields it with associated metadata.  Requires `audio_files` to be an iterable of (path, file-like object) tuples.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/audio_dataset.mdx#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ninside_clips_dir = False\nid_ = 0\nfor path, f in audio_files:\n    if path.startswith(path_to_clips):\n        inside_clips_dir = True\n        if path in examples:\n            audio = {\"path\": path, \"bytes\": f.read()}\n            yield id_, {**examples[path], \"audio\": audio}\n            id_ += 1\n    elif inside_clips_dir:\n        break\n```\n\n----------------------------------------\n\nTITLE: Multiprocessing with Map (Datasets Library, Python)\nDESCRIPTION: This code demonstrates how to use multiprocessing to speed up the `map` function by setting the `num_proc` parameter. It maps a function that adds the index to 'sentence2', leveraging multiple CPU processes.  It also shows an example using rank to parallelize computation across GPUs, requiring setting `multiprocess.set_start_method(\"spawn\")`.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/process.mdx#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n>>> updated_dataset = dataset.map(lambda example, idx: {\"sentence2\": f\"{idx}: \" + example[\"sentence2\"]}, with_indices=True, num_proc=4)\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> import torch\n>>> from multiprocess import set_start_method\n>>> from transformers import AutoTokenizer, AutoModelForCausalLM \n>>> from datasets import load_dataset\n>>> \n>>> # Get an example dataset\n>>> dataset = load_dataset(\"fka/awesome-chatgpt-prompts\", split=\"train\")\n>>> \n>>> # Get an example model and its tokenizer \n>>> model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen1.5-0.5B-Chat\").eval()\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-0.5B-Chat\")\n>>>\n>>> def gpu_computation(batch, rank):\n...     # Move the model on the right GPU if it's not there already\n...     device = f\"cuda:{(rank or 0) % torch.cuda.device_count()}\"\n...     model.to(device)\n...     \n...     # Your big GPU call goes here, for example:\n...     chats = [[\n...         {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n...         {\"role\": \"user\", \"content\": prompt}\n...     ] for prompt in batch[\"prompt\"]]\n...     texts = [tokenizer.apply_chat_template(\n...         chat,\n...         tokenize=False,\n...         add_generation_prompt=True\n...     ) for chat in chats]\n...     model_inputs = tokenizer(texts, padding=True, return_tensors=\"pt\").to(device)\n...     with torch.no_grad():\n...         outputs = model.generate(**model_inputs, max_new_tokens=512)\n...     batch[\"output\"] = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n...     return batch\n>>>\n>>> if __name__ == \"__main__\":\n...     set_start_method(\"spawn\")\n...     updated_dataset = dataset.map(\n...         gpu_computation,\n...         batched=True,\n...         batch_size=16,\n...         with_rank=True,\n...         num_proc=torch.cuda.device_count(),  # one process per GPU\n...     )\n```\n\n----------------------------------------\n\nTITLE: Defining Dataset Metadata in Python\nDESCRIPTION: This code snippet shows how to define dataset metadata using the `datasets.DatasetInfo` class within the `_info` method.  It specifies the dataset description, features (including image and label types), supervised keys, homepage, citation, and license. The `datasets.Features` class defines the structure of each example in the dataset.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/image_dataset.mdx#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\ndef _info(self):\n    return datasets.DatasetInfo(\n        description=_DESCRIPTION,\n        features=datasets.Features(\n            {\n                \"image\": datasets.Image(),\n                \"label\": datasets.ClassLabel(names=_NAMES),\n            }\n        ),\n        supervised_keys=(\"image\", \"label\"),\n        homepage=_HOMEPAGE,\n        citation=_CITATION,\n        license=_LICENSE,\n\n    )\n```\n\n----------------------------------------\n\nTITLE: Defining image transformations with Albumentations\nDESCRIPTION: This code defines an image transformation pipeline using Albumentations. It resizes the image to 480x480, applies a horizontal flip with probability 1.0, and adjusts brightness and contrast with probability 1.0. It also specifies that bounding boxes should be in 'coco' format with 'category' as the label field.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/object_detection.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> import albumentations\n>>> import numpy as np\n\n>>> transform = albumentations.Compose([\n...     albumentations.Resize(480, 480),\n...     albumentations.HorizontalFlip(p=1.0),\n...     albumentations.RandomBrightnessContrast(p=1.0),\n... ], bbox_params=albumentations.BboxParams(format='coco',  label_fields=['category']))\n\n>>> image = np.array(example['image'])\n>>> out = transform(\n...     image=image,\n...     bboxes=example['objects']['bbox'],\n...     category=example['objects']['category'],\n... )\n```\n\n----------------------------------------\n\nTITLE: Data Augmentation with Albumentations for TensorFlow\nDESCRIPTION: Applies data augmentation to images using `albumentations` for TensorFlow. Requires `albumentations` and `numpy`. Defines a Compose of transformations including `RandomCrop`, `HorizontalFlip`, and `RandomBrightnessContrast`.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/quickstart.mdx#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n>>> import albumentations\n>>> import numpy as np\n\n>>> transform = albumentations.Compose([\n...     albumentations.RandomCrop(width=256, height=256),\n...     albumentations.HorizontalFlip(p=0.5),\n...     albumentations.RandomBrightnessContrast(p=0.2),\n... ])\n\n>>> def transforms(examples):\n...     examples[\"pixel_values\"] = [\n...         transform(image=np.array(image))[\"image\"] for image in examples[\"image\"]\n...     ]\n...     return examples\n\n>>> dataset.set_transform(transforms)\n>>> tf_dataset = model.prepare_tf_dataset(\n...     dataset,\n...     batch_size=4,\n...     shuffle=True,\n... )\n```\n\n----------------------------------------\n\nTITLE: Define Custom Labels with Features Python\nDESCRIPTION: This snippet shows how to define custom labels for a dataset using the `Features` class and `ClassLabel` feature. This allows you to specify the labels for classification tasks, ensuring the dataset aligns with your expectations.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/loading.mdx#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\n>>> class_names = [\"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"]\n>>> emotion_features = Features({'text': Value('string'), 'label': ClassLabel(names=class_names)})\n```\n\n----------------------------------------\n\nTITLE: Creating SQLite database and table\nDESCRIPTION: This snippet creates an SQLite database file `us_covid_data.db`, downloads COVID-19 data from a remote CSV, and inserts it into a table named `states`. It requires the `sqlite3` and `pandas` libraries.  The `if_exists='replace'` argument overwrites the table if it already exists.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/tabular_load.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> import sqlite3\n>>> import pandas as pd\n\n>>> conn = sqlite3.connect(\"us_covid_data.db\")\n>>> df = pd.read_csv(\"https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-states.csv\")\n>>> df.to_sql(\"states\", conn, if_exists=\"replace\")\n```\n\n----------------------------------------\n\nTITLE: Loading Data from SQL Database with Dataset.from_sql in Python\nDESCRIPTION: Loads data from an SQL database using the `Dataset.from_sql` method. It allows loading entire tables or data from a specific query.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/loading.mdx#_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\n>>> from datasets import Dataset\n# load entire table\n>>> dataset = Dataset.from_sql(\"data_table_name\", con=\"sqlite:///sqlite_file.db\")\n# load from query\n>>> dataset = Dataset.from_sql(\"SELECT text FROM table WHERE length(text) > 100 LIMIT 10\", con=\"sqlite:///sqlite_file.db\")\n```\n\n----------------------------------------\n\nTITLE: Loading a JSON File with a Nested Field with load_dataset in Python\nDESCRIPTION: Loads a dataset from a local or remote JSON file with a nested field, specifying the `field` argument in the `load_dataset` function.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/loading.mdx#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\n>>> from datasets import load_dataset\n>>> dataset = load_dataset(\"json\", data_files=\"my_file.json\", field=\"data\")\n```\n\n----------------------------------------\n\nTITLE: Setting Dataset Format to TensorFlow\nDESCRIPTION: This code snippet demonstrates how to set the format of a dataset to TensorFlow, which will cause the dataset to return `tf.Tensor` objects. It initializes a dataset from a dictionary and then uses the `with_format` method to change the output format to 'tf'.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_tensorflow.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import Dataset\n>>> data = [[1, 2],[3, 4]]\n>>> ds = Dataset.from_dict({\"data\": data})\n>>> ds = ds.with_format(\"tf\")\n>>> ds[0]\n{'data': <tf.Tensor: shape=(2,), dtype=int64, numpy=array([1, 2])>}\n>>> ds[:2]\n{'data': <tf.Tensor: shape=(2, 2), dtype=int64, numpy=\narray([[1, 2],\n       [3, 4]])>}\n```\n\n----------------------------------------\n\nTITLE: Selecting Specific Columns in a Dataset in Python\nDESCRIPTION: This code snippet demonstrates how to select specific columns from a dataset using the `select_columns` method. It shows selecting multiple columns ('sentence1', 'sentence2', 'idx') and then selecting a single column ('idx').\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/process.mdx#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset\nDataset({\n    features: ['sentence1', 'sentence2', 'label', 'idx'],\n    num_rows: 3668\n})\n>>> dataset = dataset.select_columns(['sentence1', 'sentence2', 'idx'])\n>>> dataset\nDataset({\n    features: ['sentence1', 'sentence2', 'idx'],\n    num_rows: 3668\n})\n>>> dataset = dataset.select_columns('idx')\n>>> dataset\nDataset({\n    features: ['idx'],\n    num_rows: 3668\n})\n```\n\n----------------------------------------\n\nTITLE: Applying transforms with Dataset.map in Python\nDESCRIPTION: This snippet showcases how to apply a defined `transforms` function to an image dataset using the `dataset.map` function from the Hugging Face Datasets library. It demonstrates resizing all the images to a fixed size. Setting `batched=True` speeds up the processing by applying the transform to batches of examples. The function expects an input dictionary `examples` containing image data under the \"image\" key and returns the modified dictionary with resized images stored under the 'pixel_values' key.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/image_process.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset = dataset.map(transforms, remove_columns=[\"image\"], batched=True)\n>>> dataset[0]\n{'label': 6,\n 'pixel_values': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=100x100 at 0x7F058237BB10>}\n```\n\n----------------------------------------\n\nTITLE: Loading AudioFolder Dataset with Filters in Python\nDESCRIPTION: This snippet shows how to load a subset of an AudioFolder dataset based on a condition on the label or metadata using the `filters` argument. It is recommended to use this argument with `streaming=True`.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/audio_load.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> filters = [(\"label\", \"=\", 0)]\n>>> dataset = load_dataset(\"username/dataset_name\", streaming=True, filters=filters)\n```\n\n----------------------------------------\n\nTITLE: Applying Transformations to Images\nDESCRIPTION: Defines a function to apply the defined Albumentations transformations to a batch of images and segmentation masks. It transforms the images and masks using the transform object and returns the transformed images and masks.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/semantic_segmentation.mdx#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> def transforms(examples):\n...     transformed_images, transformed_masks = [], []\n...\n...     for image, seg_mask in zip(examples[\"image\"], examples[\"annotation\"]):\n...         image, seg_mask = np.array(image), np.array(seg_mask)\n...         transformed = transform(image=image, mask=seg_mask)\n...         transformed_images.append(transformed[\"image\"])\n...         transformed_masks.append(transformed[\"mask\"])\n...\n...     examples[\"pixel_values\"] = transformed_images\n...     examples[\"label\"] = transformed_masks\n...     return examples\n```\n\n----------------------------------------\n\nTITLE: Loading Image Dataset and Accessing Image Feature in Py\nDESCRIPTION: This snippet demonstrates how to load the 'beans' image dataset using the `load_dataset` function and access the 'image' column. The `Image` feature automatically decodes the image file when accessed. The example shows how to access the 'image' feature for the first element in the dataset, resulting in a PIL.Image object.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/about_dataset_features.mdx#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset, Image\n\n>>> dataset = load_dataset(\"AI-Lab-Makerere/beans\", split=\"train\")\n>>> dataset[0][\"image\"]\n<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x500 at 0x125506CF8>\n```\n\n----------------------------------------\n\nTITLE: Add Elasticsearch Index\nDESCRIPTION: Creates an Elasticsearch index for a specified column in the dataset. It connects to a local Elasticsearch instance on port 9200.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/faiss_es.mdx#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n>>> squad.add_elasticsearch_index(\"context\", host=\"localhost\", port=\"9200\")\n```\n\n----------------------------------------\n\nTITLE: Creating Albumentations Transformations\nDESCRIPTION: Creates a series of image transformations using Albumentations. These transformations include resizing the image and adjusting its brightness and contrast.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/semantic_segmentation.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> import albumentations\n\n>>> transform = albumentations.Compose(\n...     [\n...         albumentations.Resize(256, 256),\n...         albumentations.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),\n...     ]\n... )\n```\n\n----------------------------------------\n\nTITLE: Using Audio Feature Type with TensorFlow\nDESCRIPTION: This example shows how to use the `Audio` feature type in datasets when formatted for TensorFlow. It requires installing the `audio` extra. It creates a dataset with audio paths and formats it to TensorFlow.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_tensorflow.mdx#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import Dataset, Features, Audio, Image\n>>> audio = [\"path/to/audio.wav\"] * 10\n>>> features = Features({\"audio\": Audio()})\n>>> ds = Dataset.from_dict({\"audio\": audio}, features=features)\n>>> ds = ds.with_format(\"tf\")\n>>> ds[0][\"audio\"][\"array\"]\n<tf.Tensor: shape=(202311,), dtype=float32, numpy=\narray([ 6.1035156e-05,  1.5258789e-05,  1.6784668e-04, ...,\n       -1.5258789e-05, -1.5258789e-05,  1.5258789e-05], dtype=float32)>\n>>> ds[0][\"audio\"][\"sampling_rate\"]\n<tf.Tensor: shape=(), dtype=int32, numpy=44100>\n```\n\n----------------------------------------\n\nTITLE: Loading BERT Model and Tokenizer (PyTorch)\nDESCRIPTION: Loads a pretrained BERT model and its corresponding tokenizer from the ðŸ¤— Transformers library for PyTorch. Requires the `transformers` library.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/quickstart.mdx#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n>>> from transformers import AutoModelForSequenceClassification, AutoTokenizer\n\n>>> model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n```\n\n----------------------------------------\n\nTITLE: Loading Specific Rows of Train Split with ReadInstruction Python\nDESCRIPTION: This snippet demonstrates how to load specific rows (10 to 20) of the 'train' split of a dataset using the `datasets.ReadInstruction` API with `datasets.load_dataset`.  It loads the 'bookcorpu' dataset and specifies the split using `ReadInstruction` with absolute units.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/loading.mdx#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\n>>> train_10_20_ds = datasets.load_dataset(\"bookcorpu\", split=datasets.ReadInstruction(\"train\", from_=10, to=20, unit=\"abs\"))\n```\n\n----------------------------------------\n\nTITLE: Instantiate Cloud Storage Filesystem with fsspec (Python)\nDESCRIPTION: This snippet demonstrates how to instantiate a cloud storage filesystem using `fsspec`. It imports the `fsspec` library, initializes a filesystem object with the appropriate storage type (e.g., s3, gcs, abfs), specifies the directory containing the data, defines a file pattern to filter files, and retrieves a list of matching files. The ellipse \"...\" should be replaced with the specific cloud storage type.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/filesystems.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> import fsspec\n>>> fs = fsspec.filesystem(\"...\")  # s3 / gcs / abfs / adl / oci / ...\n>>> data_dir = \"path/to/my/data/\"\n>>> pattern = \"*.parquet\"\n>>> data_files = fs.glob(data_dir + pattern)\n[\"path/to/my/data/0001.parquet\", \"path/to/my/data/0001.parquet\", ...]\n```\n\n----------------------------------------\n\nTITLE: Loading Pandas DataFrames into Hugging Face Datasets\nDESCRIPTION: This code demonstrates how to load a Pandas DataFrame into a Hugging Face Dataset using the `Dataset.from_pandas` method. It creates a Pandas DataFrame from a CSV file and then converts it into a Dataset. It also shows how to specify the split name for the resulting dataset. Requires `datasets` and `pandas` libraries.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/tabular_load.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import Dataset\n>>> import pandas as pd\n\n# create a Pandas DataFrame\n>>> df = pd.read_csv(\"https://huggingface.co/datasets/imodels/credit-card/raw/main/train.csv\")\n>>> df = pd.DataFrame(df)\n# load Dataset from Pandas DataFrame\n>>> dataset = Dataset.from_pandas(df)\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> train_ds = Dataset.from_pandas(train_df, split=\"train\")\n>>> test_ds = Dataset.from_pandas(test_df, split=\"test\")\n```\n\n----------------------------------------\n\nTITLE: Generating Examples for Dataset\nDESCRIPTION: This code defines the `_generate_examples` method for creating dataset examples. It reads metadata from a CSV file, associates it with audio files from an archive, and yields the combined data. It handles missing metadata fields and constructs the full path to the audio files. It requires `local_extracted_archive`, `audio_files`, `metadata_path`, and `path_to_clips` as input. The function expects `_info` to have defined `features`.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/audio_dataset.mdx#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndef _generate_examples(\n        self,\n        local_extracted_archive,\n        audio_files,\n        metadata_path,\n        path_to_clips,\n    ):\n        \"\"\"Yields examples.\"\"\"\n        data_fields = list(self._info().features.keys())\n        metadata = {}\n        with open(metadata_path, \"r\", encoding=\"utf-8\") as f:\n            reader = csv.DictReader(f)\n            for row in reader:\n                if self.config.name == \"all\" or self.config.name == row[\"language\"]:\n                    row[\"path\"] = os.path.join(path_to_clips, row[\"path\"])\n                    # if data is incomplete, fill with empty values\n                    for field in data_fields:\n                        if field not in row:\n                            row[field] = \"\"\n                    metadata[row[\"path\"]] = row\n        id_ = 0\n        for path, f in audio_files:\n            if path in metadata:\n                result = dict(metadata[path])\n                # set the audio feature and the path to the extracted file\n                path = os.path.join(local_extracted_archive, path) if local_extracted_archive else path\n                result[\"audio\"] = {\"path\": path, \"bytes\": f.read()}\n                result[\"path\"] = path\n                yield id_, result\n                id_ += 1\n```\n\n----------------------------------------\n\nTITLE: Tokenizing the Entire Dataset\nDESCRIPTION: This snippet defines a `tokenization` function to apply the tokenizer to the `text` column of the dataset.  It then uses the `map` function with `batched=True` to efficiently tokenize the entire dataset in batches, speeding up the process.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_dataset.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> def tokenization(example):\n...     return tokenizer(example[\"text\"])\n\n>>> dataset = dataset.map(tokenization, batched=True)\n```\n\n----------------------------------------\n\nTITLE: Renaming Column\nDESCRIPTION: Renames the `intent_class` column to `labels` using the `rename_column` function. This is done to match the expected input name for the target variable in Wav2Vec2ForSequenceClassification.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/quickstart.mdx#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset = dataset.rename_column(\"intent_class\", \"labels\")\n```\n\n----------------------------------------\n\nTITLE: Interleaving Datasets with all_exhausted strategy\nDESCRIPTION: This code demonstrates how to interleave three datasets using the `interleave_datasets` function with the 'all_exhausted' stopping strategy.  The resulting dataset continues until all samples from all datasets have been used at least once.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/process.mdx#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n>>> d1 = Dataset.from_dict({\"a\": [0, 1, 2]})\n>>> d2 = Dataset.from_dict({\"a\": [10, 11, 12, 13]})\n>>> d3 = Dataset.from_dict({\"a\": [20, 21, 22]})\n>>> dataset = interleave_datasets([d1, d2, d3], stopping_strategy=\"all_exhausted\")\n>>> dataset[\"a\"]\n[0, 10, 20, 1, 11, 21, 2, 12, 22, 0, 13, 20]\n```\n\n----------------------------------------\n\nTITLE: Loading a Dataset Split\nDESCRIPTION: This snippet demonstrates how to load a dataset split using the `load_dataset` function from the `datasets` library. It loads the `rotten_tomatoes` dataset and assigns the training split to the `dataset` variable.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/access.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\", split=\"train\")\n```\n\n----------------------------------------\n\nTITLE: Loading Pretrained Model and Feature Extractor\nDESCRIPTION: Loads a pretrained Wav2Vec2 model for audio classification and its corresponding feature extractor from the `transformers` library. The model checkpoint used is `facebook/wav2vec2-base`.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/quickstart.mdx#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> from transformers import AutoModelForAudioClassification, AutoFeatureExtractor\n\n>>> model = AutoModelForAudioClassification.from_pretrained(\"facebook/wav2vec2-base\")\n>>> feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n```\n\n----------------------------------------\n\nTITLE: JSONL metadata example: multiple input videos\nDESCRIPTION: Example of a JSONL metadata file showing how to define multiple videos, useful for tasks like input and output video pairs.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/video_dataset.mdx#_snippet_8\n\nLANGUAGE: jsonl\nCODE:\n```\n{\"input_file_name\": \"0001.mp4\", \"output_file_name\": \"0001_output.mp4\"}\n{\"input_file_name\": \"0002.mp4\", \"output_file_name\": \"0002_output.mp4\"}\n{\"input_file_name\": \"0003.mp4\", \"output_file_name\": \"0003_output.mp4\"}\n```\n\n----------------------------------------\n\nTITLE: Loading Private Dataset from Hub\nDESCRIPTION: This snippet demonstrates how to load a private dataset from the Hugging Face Hub using the `load_dataset` function with the `token` parameter set to `True`. It requires the `datasets` library to be installed.  The function takes the dataset name as input and requires an authentication token.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/upload_dataset.mdx#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n>>> from datasets import load_dataset\n\n# Load a private individual dataset\n>>> dataset = load_dataset(\"stevhliu/demo\", token=True)\n\n# Load a private organization dataset\n>>> dataset = load_dataset(\"organization/dataset_name\", token=True)\n```\n\n----------------------------------------\n\nTITLE: Verifying Datasets Installation\nDESCRIPTION: Verifies that the Datasets library has been installed correctly by loading a sample dataset and printing its first example.  This confirms that the library is accessible and can load data.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/installation.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython -c \"from datasets import load_dataset; print(load_dataset('rajpurkar/squad', split='train')[0])\"\n```\n\n----------------------------------------\n\nTITLE: Loading a Specific Dataset Configuration in Python\nDESCRIPTION: This code demonstrates how to load a specific configuration of a dataset using the `load_dataset` function from the `datasets` library.  It specifies the dataset name (\"ethz/food101\") and the configuration name (\"breakfast\") to load only the breakfast subset of the Food-101 dataset. The `split` parameter specifies which split to load.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/image_dataset.mdx#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"ethz/food101\", \"breakfast\", split=\"train\")\n```\n\n----------------------------------------\n\nTITLE: Converting Dataset to tf.data.Dataset for Keras Training\nDESCRIPTION: This code demonstrates how to convert a Hugging Face `Dataset` to a `tf.data.Dataset` using the `to_tf_dataset()` method for Keras training.  It creates a dataset and then converts it to a `tf.data.Dataset`, specifying columns, label columns, batch size, and shuffle.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_tensorflow.mdx#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import Dataset\n>>> data = {\"inputs\": [[1, 2],[3, 4]], \"labels\": [0, 1]}\n>>> ds = Dataset.from_dict(data)\n>>> tf_ds = ds.to_tf_dataset(\n            columns=[\"inputs\"],\n            label_cols=[\"labels\"],\n            batch_size=2,\n            shuffle=True\n            )\n```\n\n----------------------------------------\n\nTITLE: Indexing a Dataset\nDESCRIPTION: This snippet showcases how to access data within a `Dataset` object using indexing. It demonstrates accessing a row by index, including negative indexing to access rows from the end of the dataset, and retrieving an entire column by its name. It also shows how to combine row and column indexing.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/access.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Get the first row in the dataset\n>>> dataset[0]\n{'label': 1,\n 'text': 'the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'}\n```\n\nLANGUAGE: python\nCODE:\n```\n# Get the last row in the dataset\n>>> dataset[-1]\n{'label': 0,\n 'text': 'things really get weird , though not particularly scary : the movie is all portent and no content .'}\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset[\"text\"]\n['the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .',\n 'the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson\\'s expanded vision of j . r . r . tolkien\\'s middle-earth .',\n 'effective but too-tepid biopic',\n ...,\n 'things really get weird , though not particularly scary : the movie is all portent and no content .']\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset[0][\"text\"]\n'the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'\n```\n\n----------------------------------------\n\nTITLE: Upload Current Directory to Dataset\nDESCRIPTION: This command uploads the contents of the current directory to the root of the specified dataset repository. Replace `my-cool-dataset` with the actual repository name. The `. .` arguments specify the current directory as both the source and destination.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/share.mdx#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n>>> huggingface-cli upload my-cool-dataset . . --repo-type dataset\nhttps://huggingface.co/datasets/Wauplin/my-cool-dataset/tree/main/\n```\n\n----------------------------------------\n\nTITLE: Creating an IterableDataset from a Generator in Hugging Face Datasets\nDESCRIPTION: This code demonstrates creating an IterableDataset from a Python generator using `IterableDataset.from_generator`. The generator function `gen` yields dictionaries representing data samples. This type of dataset needs to be iterated over.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/create_dataset.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import IterableDataset\n>>> ds = IterableDataset.from_generator(gen)\n>>> for example in ds:\n...     print(example)\n{\"pokemon\": \"bulbasaur\", \"type\": \"grass\"}\n{\"pokemon\": \"squirtle\", \"type\": \"water\"}\n```\n\n----------------------------------------\n\nTITLE: Load Spark DataFrame into Dataset\nDESCRIPTION: This snippet shows how to load a Spark DataFrame into a Hugging Face Dataset object using `Dataset.from_spark`. The Spark workers write the dataset to disk as Arrow files, and the Dataset is loaded from there. The resulting Dataset is cached to avoid re-running the Spark job on subsequent calls with the same DataFrame.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_spark.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import Dataset\n>>> df = spark.createDataFrame(\n...     data=[[1, \"Elia\"], [2, \"Teo\"], [3, \"Fang\"]],\n...     columns=[\"id\", \"name\"],\n... )\n>>> ds = Dataset.from_spark(df)\n```\n\n----------------------------------------\n\nTITLE: Formatting Dataset as NumPy\nDESCRIPTION: This snippet demonstrates how to set the format of a dataset to 'numpy' so that it returns NumPy arrays. It initializes a dataset from a dictionary, sets the format, and then accesses elements to show the resulting NumPy arrays.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_numpy.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import Dataset\n>>> data = [[1, 2], [3, 4]]\n>>> ds = Dataset.from_dict({\"data\": data})\n>>> ds = ds.with_format(\"numpy\")\n>>> ds[0]\n{'data': array([1, 2])}\n>>> ds[:2]\n{'data': array([\n    [1, 2],\n    [3, 4]])}\n```\n\n----------------------------------------\n\nTITLE: Filtering Data with Pandas Functions using Dataset.filter\nDESCRIPTION: This snippet demonstrates how to filter data in a Hugging Face Dataset using Pandas functions within `Dataset.filter`. It filters the Dataset to only include rows where `col_0` is equal to \"b\", using `batched=True` for optimized performance.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_pandas.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> ds = ds.filter(lambda df: df.col_0 == \"b\", batched=True)\n>>> ds[0]\n  col_0  col_1  col_2\n0     b    0.0    1.0\n```\n\n----------------------------------------\n\nTITLE: Using Audio Feature Type with Datasets\nDESCRIPTION: This snippet demonstrates how to use the `Audio` feature type with `datasets`.  It shows that the audio data is loaded as tensors including the sampling rate. Note that the `audio` extra must be installed (`pip install datasets[audio]`).\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_pytorch.mdx#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import Dataset, Features, Audio, Image\n>>> audio = [\"path/to/audio.wav\"] * 10\n>>> features = Features({\"audio\": Audio()})\n>>> ds = Dataset.from_dict({\"audio\": audio}, features=features) \n>>> ds = ds.with_format(\"torch\")  \n>>> ds[0][\"audio\"][\"array\"]\ntensor([ 6.1035e-05,  1.5259e-05,  1.6785e-04,  ..., -1.5259e-05,\n        -1.5259e-05,  1.5259e-05])\n>>> ds[0][\"audio\"][\"sampling_rate\"]\ntensor(44100)\n```\n\n----------------------------------------\n\nTITLE: Defining ColorJitter transform with torchvision in Python\nDESCRIPTION: This snippet demonstrates how to define a data augmentation pipeline using `torchvision.transforms`. It creates a `Compose` object containing a `ColorJitter` transform (for random color property changes) followed by a `ToTensor` transform (for converting images to tensors). It requires the `torchvision` library to be installed. The jitter transform is applied to modify the brightness, contrast, saturation, and hue of the input images.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/image_process.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> from torchvision.transforms import Compose, ColorJitter, ToTensor\n\n>>> jitter = Compose(\n...     [\n...          ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.7),\n...          ToTensor(),\n...     ]\n... )\n```\n\n----------------------------------------\n\nTITLE: Generating Examples from Prompts and Audio Files in Python\nDESCRIPTION: This code snippet processes a prompts file to extract speaker IDs and sentences, then iterates through audio files, matching them with metadata to yield dataset examples.  It opens the prompts file, extracts relevant information and pairs it with the corresponding audio file data, associating metadata like speaker ID and sentence to each audio sample. It requires the `prompts_path`, `path_to_clips`, and `audio_files` as input.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/audio_dataset.mdx#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nexamples = {}\nwith open(prompts_path, encoding=\"utf-8\") as f:\n    for row in f:\n        data = row.strip().split(\" \", 1)\n        speaker_id = data[0].split(\"_\")[0]\n        audio_path = \"/\".join([path_to_clips, speaker_id, data[0] + \".wav\"])\n        examples[audio_path] = {\n            \"speaker_id\": speaker_id,\n            \"path\": audio_path,\n            \"sentence\": data[1],\n        }\n```\n\n----------------------------------------\n\nTITLE: Iterate through PyArrow Dataset\nDESCRIPTION: This code snippet shows how to iterate through an `IterableDataset` in PyArrow format, obtained using `load_dataset(..., streaming=True)`. It prints each batch of data as a PyArrow table.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_pyarrow.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> ds = ds.with_format(\"arrow\")\n>>> for table in ds.iter(batch_size=2):\n...     print(table)\n...     break\npyarrow.Table\ncol_0: string\ncol_1: double\n----\ncol_0: [[\"a\",\"b\"]]\ncol_1: [[0,0]]\n```\n\n----------------------------------------\n\nTITLE: Loading ImageFolder Without Labels with Hugging Face Datasets\nDESCRIPTION: This code snippet demonstrates how to load an `ImageFolder` dataset without automatically created labels using `drop_labels=True` in `load_dataset`. This creates a dataset with only an image column, which can be useful if labels are not needed or are handled separately.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/image_load.mdx#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"username/dataset_without_metadata\", drop_labels=True)\n```\n\n----------------------------------------\n\nTITLE: Shuffling an IterableDataset (Py)\nDESCRIPTION: This snippet demonstrates how to shuffle an `IterableDataset` using the `shuffle` function.  The `buffer_size` argument controls the size of the buffer to randomly sample examples from.  The `seed` argument ensures reproducibility.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/stream.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n>>> dataset = load_dataset('HuggingFaceFW/fineweb', split='train', streaming=True)\n>>> shuffled_dataset = dataset.shuffle(seed=42, buffer_size=10_000)\n```\n\n----------------------------------------\n\nTITLE: Generating Dataset Examples in Python\nDESCRIPTION: This complete `_generate_examples` method combines prompt processing and audio file iteration to yield dataset examples. It opens the prompts file, extracts metadata, iterates over audio files, and yields examples containing audio data and associated metadata. The function expects `prompts_path`, `path_to_clips`, and `audio_files` as inputs, and yields tuples of (key, example).\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/audio_dataset.mdx#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndef _generate_examples(self, prompts_path, path_to_clips, audio_files):\n    \"\"\"Yields examples as (key, example) tuples.\"\"\"\n    examples = {}\n    with open(prompts_path, encoding=\"utf-8\") as f:\n        for row in f:\n            data = row.strip().split(\" \", 1)\n            speaker_id = data[0].split(\"_\")[0]\n            audio_path = \"/\".join([path_to_clips, speaker_id, data[0] + \".wav\"])\n            examples[audio_path] = {\n                \"speaker_id\": speaker_id,\n                \"path\": audio_path,\n                \"sentence\": data[1],\n            }\n    inside_clips_dir = False\n    id_ = 0\n    for path, f in audio_files:\n        if path.startswith(path_to_clips):\n            inside_clips_dir = True\n            if path in examples:\n                audio = {\"path\": path, \"bytes\": f.read()}\n                yield id_, {**examples[path], \"audio\": audio}\n                id_ += 1\n        elif inside_clips_dir:\n            break\n```\n\n----------------------------------------\n\nTITLE: Loading Local Files into a Dataset in Python\nDESCRIPTION: Shows how to load local data files (e.g., CSV) into a Dataset using load_dataset, which requires a conversion to Arrow format.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndata_files = {\"train\": [\"path/to/data.csv\"]}\nmy_dataset = load_dataset(\"csv\", data_files=data_files, split=\"train\")\nprint(my_dataset[0])\n```\n\n----------------------------------------\n\nTITLE: Save Dataset to Disk\nDESCRIPTION: This snippet showcases how to save a Hugging Face Dataset to disk using the `save_to_disk` method.  The dataset will be stored in the specified directory for later use. It requires a pre-processed dataset to be saved.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/process.mdx#_snippet_30\n\nLANGUAGE: Python\nCODE:\n```\n>>> encoded_dataset.save_to_disk(\"path/of/my/dataset/directory\")\n```\n\n----------------------------------------\n\nTITLE: Loading and Inspecting MRPC Dataset Features in Py\nDESCRIPTION: This snippet demonstrates how to load the 'mrpc' dataset from the GLUE benchmark using the `load_dataset` function and then inspect its features using the `dataset.features` attribute.  The 'mrpc' dataset contains information about sentence pairs and their equivalence, so the features include 'idx' (an integer), 'label' (a ClassLabel indicating equivalence), and 'sentence1' and 'sentence2' (strings).\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/about_dataset_features.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n>>> dataset = load_dataset('nyu-mll/glue', 'mrpc', split='train')\n>>> dataset.features\n{'idx': Value(dtype='int32', id=None),\n 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n 'sentence1': Value(dtype='string', id=None),\n 'sentence2': Value(dtype='string', id=None),\n}\n```\n\n----------------------------------------\n\nTITLE: Apply Transform On-the-Fly in Python\nDESCRIPTION: Applies the defined transform to the dataset on-the-fly using the `set_transform` function.  When indexing into the image `pixel_values`, the transform is applied dynamically, rotating the image each time it is accessed.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_dataset.mdx#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset.set_transform(transforms)\n>>> dataset[0][\"pixel_values\"]\n```\n\n----------------------------------------\n\nTITLE: Loading Text Data from Files\nDESCRIPTION: Loads text data from specified text files for training and testing datasets using the `load_dataset` function from the `datasets` library. It demonstrates loading from a list of files for the 'train' split and a single file for the 'test' split.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/nlp_load.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n>>> dataset = load_dataset(\"text\", data_files={\"train\": [\"my_text_1.txt\", \"my_text_2.txt\"], \"test\": \"my_test_file.txt\"})\n\n# Load from a directory\n>>> dataset = load_dataset(\"text\", data_dir=\"path/to/text/dataset\")\n```\n\n----------------------------------------\n\nTITLE: Formatting Dataset to Torch Tensors\nDESCRIPTION: This code shows how to format a dataset to use PyTorch tensors. It creates a dataset with 'text' and 'tokens' columns, then formats it to PyTorch, converting the 'tokens' column to tensors.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/process.mdx#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\n>>> ds = Dataset.from_dict({\"text\": [\"foo\", \"bar\"], \"tokens\": [[0, 1, 2], [3, 4, 5]]})\n>>> ds = ds.with_format(\"torch\")\n>>> ds[0]\n{'text': 'foo', 'tokens': tensor([0, 1, 2])}\n>>> ds[:2]\n{'text': ['foo', 'bar'],\n 'tokens': tensor([[0, 1, 2],\n         [3, 4, 5]])}\n```\n\n----------------------------------------\n\nTITLE: Loading Image Classification Dataset using Hugging Face Datasets\nDESCRIPTION: Loads the `AI-Lab-Makerere/beans` dataset using the `load_dataset` function from the `datasets` library. This dataset contains images of bean plants and their corresponding labels, which can be used for training an image classification model to identify bean plant diseases. An example from the train dataset is then printed.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/image_classification.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"AI-Lab-Makerere/beans\")\n>>> dataset[\"train\"][10]\n{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x500 at 0x7F8D2F4D7A10>,\n 'image_file_path': '/root/.cache/huggingface/datasets/downloads/extracted/b0a21163f78769a2cf11f58dfc767fb458fc7cea5c05dccc0144a2c0f0bc1292/train/angular_leaf_spot/angular_leaf_spot_train.204.jpg',\n 'labels': 0}\n```\n\n----------------------------------------\n\nTITLE: Loading Dataset from Python List Python\nDESCRIPTION: This snippet demonstrates how to create a Hugging Face Dataset from a Python list of dictionaries using the `Dataset.from_list` method. It initializes a list of dictionaries and then converts it into a Dataset object.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/loading.mdx#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import Dataset\n>>> my_list = [{\"a\": 1}, {\"a\": 2}, {\"a\": 3}]\n>>> dataset = Dataset.from_list(my_list)\n```\n\n----------------------------------------\n\nTITLE: Multithreaded PDF decoding\nDESCRIPTION: This snippet demonstrates how to enable multithreaded decoding for PDF datasets to speed up processing. It calculates the number of threads to use based on the available CPU cores and calls the `decode` method on the dataset.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/document_load.mdx#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n>>> import os\n>>> num_threads = num_threads = min(32, (os.cpu_count() or 1) + 4)\n>>> dataset = dataset.decode(num_threads=num_threads)\n>>> for example in dataset:  # up to 20 times faster !\n...     ...\n```\n\n----------------------------------------\n\nTITLE: Loading a Video Dataset\nDESCRIPTION: This code snippet demonstrates how to load a video dataset from a local folder using `load_dataset`.  It assumes the video files are stored in a directory structure that the `datasets` library can understand. The resulting dataset will have a 'video' column containing `torchvision.io.video_reader.VideoReader` objects.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/video_load.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset, Video\n\n>>> dataset = load_dataset(\"path/to/video/folder\", split=\"train\")\n>>> dataset[0][\"video\"]\n<torchvision.io.video_reader.VideoReader at 0x1652284c0>\n```\n\n----------------------------------------\n\nTITLE: Loading Dataset with ReadInstruction API Python\nDESCRIPTION: This snippet demonstrates how to concatenate train and test splits using `datasets.ReadInstruction` with `datasets.load_dataset`. It loads the 'ajibawa-2023/General-Stories-Collection' dataset using a `ReadInstruction` object to specify the combined split.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/loading.mdx#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\n>>> ri = datasets.ReadInstruction(\"train\") + datasets.ReadInstruction(\"test\")\n>>> train_test_ds = datasets.load_dataset(\"ajibawa-2023/General-Stories-Collection\", split=ri)\n```\n\n----------------------------------------\n\nTITLE: Loading Cross-Validated Splits with ReadInstruction Python\nDESCRIPTION: This snippet demonstrates how to create 10-fold cross-validated splits using the `datasets.ReadInstruction` API with `datasets.load_dataset`. It loads the 'ajibawa-2023/General-Stories-Collection' dataset and specifies the split with a list comprehension using `ReadInstruction` objects to create validation and training splits.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/loading.mdx#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\n>>> val_ds = datasets.load_dataset(\"ajibawa-2023/General-Stories-Collection\", [datasets.ReadInstruction(\"train\", from_=k, to=k+10, unit=\"%\") for k in range(0, 100, 10)])\n>>> train_ds = datasets.load_dataset(\"ajibawa-2023/General-Stories-Collection\", [(datasets.ReadInstruction(\"train\", to=k, unit=\"%\") + datasets.ReadInstruction(\"train\", from_=k+10, unit=\"%\")) for k in range(0, 100, 10)])\n```\n\n----------------------------------------\n\nTITLE: Explicitly Specifying Array Feature Type and Shape\nDESCRIPTION: This snippet demonstrates how to explicitly use the `Array` feature type to specify the shape of tensors, which can avoid slow shape comparisons and data copies. It creates a dataset with `Array2D` feature and formats it to TensorFlow.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_tensorflow.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import Dataset, Features, Array2D\n>>> data = [[[1, 2],[3, 4]],[[5, 6],[7, 8]]]\n>>> features = Features({\"data\": Array2D(shape=(2, 2), dtype='int32')})\n>>> ds = Dataset.from_dict({\"data\": data}, features=features)\n>>> ds = ds.with_format(\"tf\")\n>>> ds[0]\n{'data': <tf.Tensor: shape=(2, 2), dtype=int64, numpy=\n array([[1, 2],\n        [3, 4]])>}\n>>> ds[:2]\n{'data': <tf.Tensor: shape=(2, 2, 2), dtype=int64, numpy=\n array([[[1, 2],\n         [3, 4]],\n \n        [[5, 6],\n         [7, 8]]])>}\n```\n\n----------------------------------------\n\nTITLE: Upload Single File to Specific Path in Dataset\nDESCRIPTION: This command uploads a single file to a specific directory within the specified dataset repository. Replace `Wauplin/my-cool-dataset` with the actual repository name, `./files/train.csv` with the path to the file you want to upload, and `/data/train.csv` with the desired destination path in the repository.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/share.mdx#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n>>> huggingface-cli upload Wauplin/my-cool-dataset ./files/train.csv /data/train.csv --repo-type dataset\nhttps://huggingface.co/datasetsWauplin/my-cool-dataset/blob/main/data/train.csv\n```\n\n----------------------------------------\n\nTITLE: Defining dataset features with datasets.Features - Python\nDESCRIPTION: This code snippet shows how to define the features of a dataset using the `datasets.Features` class. It specifies the data type and structure of each column in the dataset.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/dataset_script.mdx#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ndatasets.Features(\n    {\n        \"id\": datasets.Value(\"string\"),\n        \"title\": datasets.Value(\"string\"),\n        \"context\": datasets.Value(\"string\"),\n        \"question\": datasets.Value(\"string\"),\n        \"answers\": datasets.Sequence(\n            {\n                \"text\": datasets.Value(\"string\"),\n                \"answer_start\": datasets.Value(\"int32\"),\n            }\n        ),\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Streaming Data with Multiple Shards and Workers\nDESCRIPTION: This snippet extends the previous streaming example by demonstrating how to load a sharded dataset in parallel using multiple workers. It specifies `num_workers` in the `DataLoader` to distribute shard loading among workers.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_pytorch.mdx#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n>>> my_iterable_dataset = load_dataset(\"deepmind/code_contests\", streaming=True, split=\"train\")\n>>> my_iterable_dataset.num_shards\n39\n>>> dataloader = DataLoader(my_iterable_dataset, batch_size=32, num_workers=4)\n```\n\n----------------------------------------\n\nTITLE: Casting Image Column to RGB mode\nDESCRIPTION: Casts the 'image' column of the dataset to RGB mode using the `cast_column` function. Requires the `datasets` library. This is useful if the dataset contains images in a different color mode.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/quickstart.mdx#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset = dataset.cast_column(\"image\", Image(mode=\"RGB\"))\n```\n\n----------------------------------------\n\nTITLE: Eager Data Processing with Dataset.map in Python\nDESCRIPTION: Illustrates eager processing of a Dataset using the map function, applying the processing function to all examples at once.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmy_dataset = my_dataset.map(process_fn)  # process_fn is applied on all the examples of the dataset\nprint(my_dataset[0])\n```\n\n----------------------------------------\n\nTITLE: Tokenize Dataset with Batched Mapping\nDESCRIPTION: This code snippet demonstrates how to tokenize a dataset using the `map` function with the `batched` parameter set to `True`. It applies the tokenizer to batches of examples in the dataset, which significantly speeds up the tokenization process. The example shows tokenizing the 'text' column of the dataset and inspecting the first example.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/nlp_process.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset = dataset.map(lambda examples: tokenizer(examples[\"text\"]), batched=True)\n>>> dataset[0]\n{'text': 'the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .', \n 'label': 1, \n 'input_ids': [101, 1996, 2600, 2003, 16036, 2000, 2022, 1996, 7398, 2301, 1005, 1055, 2047, 1000, 16608, 1000, 1998, 2008, 2002, 1005, 1055, 2183, 2000, 2191, 1037, 17624, 2130, 3618, 2084, 7779, 29058, 8625, 13327, 1010, 3744, 1011, 18856, 19513, 3158, 5477, 4168, 2030, 7112, 16562, 2140, 1012, 102], \n 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \n 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n```\n\n----------------------------------------\n\nTITLE: Downloading and Defining Dataset Splits in Python\nDESCRIPTION: This code shows how to download the dataset and define the dataset splits using `datasets.SplitGenerator`. It downloads the data archive and metadata using `dl_manager.download`.  Then, it defines the train and validation splits using `datasets.SplitGenerator`, passing the downloaded data paths to the `gen_kwargs` for the `_generate_examples` method.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/image_dataset.mdx#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\ndef _split_generators(self, dl_manager):\n    archive_path = dl_manager.download(_BASE_URL)\n    split_metadata_paths = dl_manager.download(_METADATA_URLS)\n    return [\n        datasets.SplitGenerator(\n            name=datasets.Split.TRAIN,\n            gen_kwargs={\n                \"images\": dl_manager.iter_archive(archive_path),\n                \"metadata_path\": split_metadata_paths[\"train\"],\n            },\n        ),\n        datasets.SplitGenerator(\n            name=datasets.Split.VALIDATION,\n            gen_kwargs={\n                \"images\": dl_manager.iter_archive(archive_path),\n                \"metadata_path\": split_metadata_paths[\"test\"],\n            },\n        ),\n    ]\n```\n\n----------------------------------------\n\nTITLE: Using Image Feature Type\nDESCRIPTION: This snippet demonstrates how to use the Image feature type in datasets with NumPy.  It requires the `vision` extra to be installed (`pip install datasets[vision]`).  It shows how to load image paths, define an Image feature, create a dataset, format it as NumPy, and access the image array's shape and values.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_numpy.mdx#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import Dataset, Features, Image\n>>> images = [\"path/to/image.png\"] * 10\n>>> features = Features({\"image\": Image()})\n>>> ds = Dataset.from_dict({\"image\": images}, features=features)\n>>> ds = ds.with_format(\"numpy\")\n>>> ds[0][\"image\"].shape\n(512, 512, 3)\n>>> ds[0]\n{'image': array([[[ 255, 255, 255],\n              [ 255, 255, 255],\n              ...,\n              [ 255, 255, 255],\n              [ 255, 255, 255]]], dtype=uint8)}\n>>> ds[:2][\"image\"].shape\n(2, 512, 512, 3)\n>>> ds[:2]\n{'image': array([[[[ 255, 255, 255],\n              [ 255, 255, 255],\n              ...,\n              [ 255, 255, 255],\n              [ 255, 255, 255]]]], dtype=uint8)}\n```\n\n----------------------------------------\n\nTITLE: Sync Local Dataset with Hub using Include/Exclude\nDESCRIPTION: This command syncs a local dataset folder with the Hugging Face Hub, including only CSV files in the `/data` directory and deleting all other files from the repository. Replace `Wauplin/my-cool-dataset` with the actual repository name.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/share.mdx#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n# Sync local Space with Hub (upload new CSV files, delete removed files)\n>>> huggingface-cli upload Wauplin/my-cool-dataset --repo-type dataset --include=\"/data/*.csv\" --delete=\"*\" --commit-message=\"Sync local dataset with Hub\"\n...\n```\n\n----------------------------------------\n\nTITLE: Checkpointing and Resuming DataLoader with StatefulDataLoader\nDESCRIPTION: This snippet demonstrates how to use `StatefulDataLoader` from `torchdata` to enable checkpointing and resuming of a `DataLoader` during training.  It shows how to save and load the state of the dataloader.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_pytorch.mdx#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n>>> from torchdata.stateful_dataloader import StatefulDataLoader\n>>> my_iterable_dataset = load_dataset(\"deepmind/code_contests\", streaming=True, split=\"train\")\n>>> dataloader = StatefulDataLoader(my_iterable_dataset, batch_size=32, num_workers=4)\n>>> # save in the middle of training\n>>> state_dict = dataloader.state_dict()\n>>> # and resume later\n>>> dataloader.load_state_dict(state_dict)\n```\n\n----------------------------------------\n\nTITLE: Setting the transform using Dataset.set_transform in Python\nDESCRIPTION: This snippet demonstrates how to apply a transformation function (`transforms`) to a dataset using the `dataset.set_transform()` method provided by the Hugging Face Datasets library. This method sets the transformation function that will be applied to each example in the dataset when it is accessed.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/image_process.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset.set_transform(transforms)\n```\n\n----------------------------------------\n\nTITLE: Process Data with PyArrow Compute Functions\nDESCRIPTION: This snippet demonstrates how to use PyArrow compute functions within `Dataset.map` and `Dataset.filter` for efficient data processing.  It utilizes `pc.add` to add a new column and `pc.equal` to filter rows based on a condition.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_pyarrow.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> import pyarrow.compute as pc\n>>> from datasets import Dataset\n>>> data = {\"col_0\": [\"a\", \"b\", \"c\", \"d\"], \"col_1\": [0., 0., 1., 1.]}\n>>> ds = Dataset.from_dict(data)\n>>> ds = ds.with_format(\"arrow\")\n>>> ds = ds.map(lambda t: t.append_column(\"col_2\", pc.add(t[\"col_1\"], 1)), batched=True)\n>>> ds[:2]\npyarrow.Table\ncol_0: string\ncol_1: double\ncol_2: double\n----\ncol_0: [[\"a\",\"b\"]]\ncol_1: [[0,0]]\ncol_2: [[1,1]]\n>>> ds = ds.filter(lambda t: pc.equal(t[\"col_0\"], \"b\"), batched=True)\n>>> ds[0]\npyarrow.Table\ncol_0: string\ncol_1: double\ncol_2: double\n----\ncol_0: [[\"b\"]]\ncol_1: [[0]]\ncol_2: [[1]]\n```\n\n----------------------------------------\n\nTITLE: Loading a JSON File with load_dataset in Python\nDESCRIPTION: Loads a dataset from a local or remote JSON file using the `load_dataset` function. It requires the 'json' data loading script to be specified.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/loading.mdx#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n>>> from datasets import load_dataset\n>>> dataset = load_dataset(\"json\", data_files=\"my_file.json\")\n```\n\n----------------------------------------\n\nTITLE: Pushing Dataset to Hub\nDESCRIPTION: This snippet demonstrates how to push a dataset to the Hugging Face Hub using the `push_to_hub` function from the `datasets` library. It requires the `datasets` library to be installed and a dataset object to be created. The function takes the repository name as input.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/upload_dataset.mdx#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"stevhliu/demo\")\n# dataset = dataset.map(...)  # do all your processing here\n>>> dataset.push_to_hub(\"stevhliu/processed_demo\")\n```\n\n----------------------------------------\n\nTITLE: Create Transformation Function\nDESCRIPTION: Creates a function `transforms` that applies the defined Albumentations transformations to a batch of images.  It converts PIL images to NumPy arrays, applies the transformations using `transform(image=np.array(image))[\"image\"]`, and stores the transformed images in the `pixel_values` field of the input examples dictionary. It returns the modified examples dictionary.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/image_classification.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> def transforms(examples):\n...     examples[\"pixel_values\"] = [\n...         transform(image=np.array(image))[\"image\"] for image in examples[\"image\"]\n...     ]\n... \n...     return examples\n```\n\n----------------------------------------\n\nTITLE: Visualizing Transformed Image (Torchvision)\nDESCRIPTION: Verifies the applied Torchvision transformation by retrieving the transformed image and mask from the dataset and visualizing them using the visualize_seg_mask function.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/semantic_segmentation.mdx#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n>>> image = np.array(dataset[index][\"pixel_values\"])\n>>> mask = np.array(dataset[index][\"label\"])\n\n>>> visualize_seg_mask(image, mask)\n```\n\n----------------------------------------\n\nTITLE: Iterating over Batches with Polars Format\nDESCRIPTION: This code demonstrates how to iterate over an `IterableDataset` object using Polars format. The `with_format(\"polars\")` method sets the dataset to return Polars DataFrames, and the `iter` method allows iterating over batches of data.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_polars.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> ds = ds.with_format(\"polars\")\n>>> for df in ds.iter(batch_size=2):\n...     print(df)\n...     break\nshape: (2, 2)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ col_0 â”† col_1 â”‚\nâ”‚ ---   â”† ---   â”‚\nâ”‚ str   â”† f64   â”‚\nâ•žâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•¡\nâ”‚ a     â”† 0.0   â”‚\nâ”‚ b     â”† 0.0   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n----------------------------------------\n\nTITLE: Handling N-Dimensional Arrays with Fixed Shape\nDESCRIPTION: This snippet demonstrates how the `datasets` library automatically handles N-dimensional arrays with fixed shapes by converting them into tensors.  It shows that the resulting tensor retains the shape of the input data.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_pytorch.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import Dataset\n>>> data = [[[1, 2],[3, 4]],[[5, 6],[7, 8]]]  # fixed shape\n>>> ds = Dataset.from_dict({\"data\": data})\n>>> ds = ds.with_format(\"torch\")\n>>> ds[0]\n{'data': tensor([[1, 2],\n         [3, 4]])}\n```\n\n----------------------------------------\n\nTITLE: Load PDF dataset without labels\nDESCRIPTION: This code shows how to load a PDF dataset using the `PdfFolder` dataset builder and dropping the automatically created labels using `drop_labels=True`. This results in a dataset containing only the PDF column.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/document_load.mdx#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"username/dataset_without_metadata\", drop_labels=True)\n```\n\n----------------------------------------\n\nTITLE: Creating a Dataset from a Generator with Hugging Face Datasets\nDESCRIPTION: This code demonstrates creating a dataset from a Python generator using `Dataset.from_generator`. The generator function `gen` yields dictionaries representing data samples. This approach is memory-efficient for large datasets.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/create_dataset.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import Dataset\n>>> def gen():\n...     yield {\"pokemon\": \"bulbasaur\", \"type\": \"grass\"}\n...     yield {\"pokemon\": \"squirtle\", \"type\": \"water\"}\n>>> ds = Dataset.from_generator(gen)\n>>> ds[0]\n{\"pokemon\": \"bulbasaur\", \"type\": \"grass\"}\n```\n\n----------------------------------------\n\nTITLE: Creating a custom BuilderConfig subclass - Python\nDESCRIPTION: This code demonstrates how to create a custom `BuilderConfig` subclass, allowing you to define specific configurations for your dataset. It includes attributes like features, data URL, citation, and URL.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/dataset_script.mdx#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nclass SuperGlueConfig(datasets.BuilderConfig):\n    \"\"\"BuilderConfig for SuperGLUE.\"\"\"\n\n    def __init__(self, features, data_url, citation, url, label_classes=(\"False\", \"True\"), **kwargs):\n        \"\"\"BuilderConfig for SuperGLUE.\n\n        Args:\n        features: *list[string]*, list of the features that will appear in the\n            feature dict. Should not include \"label\".\n        data_url: *string*, url to download the zip file from.\n        citation: *string*, citation for the data set.\n        url: *string*, url for information about the data set.\n        label_classes: *list[string]*, the list of classes for the label if the\n            label is present as a string. Non-string labels will be cast to either\n            'False' or 'True'.\n        **kwargs: keyword arguments forwarded to super.\n        \"\"\"\n        # Version history:\n        # 1.0.2: Fixed non-nondeterminism in ReCoRD.\n        # 1.0.1: Change from the pre-release trial version of SuperGLUE (v1.9) to\n        #        the full release (v2.0).\n        # 1.0.0: S3 (new shuffling, sharding and slicing mechanism).\n        # 0.0.2: Initial version.\n        super().__init__(version=datasets.Version(\"1.0.2\"), **kwargs)\n        self.features = features\n        self.label_classes = label_classes\n        self.data_url = data_url\n        self.citation = citation\n        self.url = url\n```\n\n----------------------------------------\n\nTITLE: Query Elasticsearch Index\nDESCRIPTION: Queries the Elasticsearch index for the nearest examples based on a given query string. It retrieves the scores and corresponding examples.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/faiss_es.mdx#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n>>> query = \"machine\"\n>>> scores, retrieved_examples = squad.get_nearest_examples(\"context\", query, k=10)\n>>> retrieved_examples[\"title\"][0]\n'Computational_complexity_theory'\n```\n\n----------------------------------------\n\nTITLE: Load Hugging Face Dataset (Python)\nDESCRIPTION: This snippet shows how to load a Hugging Face Dataset using the `load_dataset` function from the `datasets` library. It imports the `load_dataset` function and then calls it with the dataset identifier (e.g., \"username/my-dataset\") to retrieve the dataset. The resulting dataset object (`ds`) can then be used for further analysis or training.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/filesystems.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"username/my-dataset\")\n```\n\n----------------------------------------\n\nTITLE: Setting Dataset Transform\nDESCRIPTION: This snippet sets the transformation function `apply_transforms` as the transform for the `train_dataset`. This means that the transformation will be applied on-the-fly whenever data is accessed from the dataset, saving disk space.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/depth_estimation.mdx#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n>>> train_dataset.set_transform(apply_transforms)\n```\n\n----------------------------------------\n\nTITLE: Loading Dataset Split (Returns Dataset Object)\nDESCRIPTION: This code loads a specific split of a dataset using `load_dataset` and prints the resulting `Dataset` object. It showcases the structure of the loaded data, including features and the number of rows. Requires the `datasets` library.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/load_hub.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\", split=\"train\")\n>>> dataset\nDataset({\n    features: ['text', 'label'],\n    num_rows: 8530\n})\n```\n\n----------------------------------------\n\nTITLE: Testing Dataset Loading Script in Python\nDESCRIPTION: This shell command line instruction is used to test a Hugging Face dataset loading script. The `datasets-cli test` command is called with the path to the dataset loading script, along with the `--save_info` flag to generate dataset metadata, and the `--all_configs` flag, presumably to test all dataset configurations.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/dataset_script.mdx#_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\ndatasets-cli test path/to/<your-dataset-loading-script> --save_info --all_configs\n```\n\n----------------------------------------\n\nTITLE: Sharding a Dataset into Multiple Chunks in Python\nDESCRIPTION: This code snippet demonstrates how to shard a dataset into a specified number of chunks using the `shard` method. It divides the 'stanfordnlp/imdb' dataset into four chunks and selects the first shard (index 0), showing that this shard contains 6250 examples.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/process.mdx#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n>>> dataset = load_dataset(\"stanfordnlp/imdb\", split=\"train\")\n>>> print(dataset)\nDataset({\n    features: ['text', 'label'],\n    num_rows: 25000\n})\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset.shard(num_shards=4, index=0)\nDataset({\n    features: ['text', 'label'],\n    num_rows: 6250\n})\n>>> print(25000/4)\n6250.0\n```\n\n----------------------------------------\n\nTITLE: Loading from a Python Dictionary with Dataset.from_dict in Python\nDESCRIPTION: Creates a Dataset from a Python dictionary using the `Dataset.from_dict` method.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/loading.mdx#_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\n>>> from datasets import Dataset\n>>> my_dict = {\"a\": [1, 2, 3]}\n>>> dataset = Dataset.from_dict(my_dict)\n```\n\n----------------------------------------\n\nTITLE: Hugging Face CLI Login\nDESCRIPTION: This command logs the user into their Hugging Face account, which is necessary for accessing private datasets or performing write operations on the Hugging Face Hub.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/cli.mdx#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n>>> huggingface-cli login\n```\n\n----------------------------------------\n\nTITLE: Load Dataset from Disk\nDESCRIPTION: This snippet shows how to load a Hugging Face Dataset from disk using the `load_from_disk` function.  The dataset must have been previously saved using `save_to_disk`. It requires the `datasets` library.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/process.mdx#_snippet_31\n\nLANGUAGE: Python\nCODE:\n```\n>>> from datasets import load_from_disk\n>>> reloaded_dataset = load_from_disk(\"path/of/my/dataset/directory\")\n```\n\n----------------------------------------\n\nTITLE: Concatenating Datasets\nDESCRIPTION: This code demonstrates how to concatenate two datasets horizontally with `axis=1`, adding the 'ids' column.  It uses `Dataset.from_dict` to create a dataset with ids and then concatenates it with the original dataset.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/process.mdx#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import Dataset\n>>> stories_ids = Dataset.from_dict({\"ids\": list(range(len(stories)))})\n>>> stories_with_ids = concatenate_datasets([stories, stories_ids], axis=1)\n```\n\n----------------------------------------\n\nTITLE: Creating a Builder Configuration with Python\nDESCRIPTION: This example shows how to create a custom builder configuration using `datasets.BuilderConfig`. It allows specifying various dataset properties like language, release date, and size. The configuration name and version are required parameters.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/audio_dataset.mdx#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass LibriVoxIndonesiaConfig(datasets.BuilderConfig):\n    \"\"\"BuilderConfig for LibriVoxIndonesia.\"\"\"\n\n    def __init__(self, name, version, **kwargs):\n        self.language = kwargs.pop(\"language\", None)\n        self.release_date = kwargs.pop(\"release_date\", None)\n        self.num_clips = kwargs.pop(\"num_clips\", None)\n        self.num_speakers = kwargs.pop(\"num_speakers\", None)\n        self.validated_hr = kwargs.pop(\"validated_hr\", None)\n        self.total_hr = kwargs.pop(\"total_hr\", None)\n        self.size_bytes = kwargs.pop(\"size_bytes\", None)\n        self.size_human = size_str(self.size_bytes)\n        description = (\n            f\"LibriVox-Indonesia speech to text dataset in {self.language} released on {self.release_date}. \"\n            f\"The dataset comprises {self.validated_hr} hours of transcribed speech data\"\n        )\n        super(LibriVoxIndonesiaConfig, self).__init__(\n            name=name,\n            version=datasets.Version(version),\n            description=description,\n            **kwargs,\n        )\n```\n\n----------------------------------------\n\nTITLE: Generating Data Splits with DownloadManager in Python\nDESCRIPTION: This Python code snippet demonstrates how to generate data splits using the DownloadManager and SplitGenerator classes from the datasets library. It downloads the data files specified in the _URLS dictionary and then creates SplitGenerator objects for the 'train' and 'validation' splits, providing the file paths to the downloaded data for each split.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/dataset_script.mdx#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef _split_generators(self, dl_manager: datasets.DownloadManager) -> List[datasets.SplitGenerator]:\n    urls_to_download = self._URLS\n    downloaded_files = dl_manager.download_and_extract(urls_to_download)\n\n    return [\n        datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={\"filepath\": downloaded_files[\"train\"]}),\n        datasets.SplitGenerator(name=datasets.Split.VALIDATION, gen_kwargs={\"filepath\": downloaded_files[\"dev\"]}),\n    ]\n```\n\n----------------------------------------\n\nTITLE: Dataset with Audio Feature Type\nDESCRIPTION: This snippet shows how to use the `Audio` feature type with JAX. The `audio` extra must be installed to use this feature. It demonstrates how to access the audio array and sampling rate.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_jax.mdx#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import Dataset, Features, Audio\n>>> audio = [\"path/to/audio.wav\"] * 10\n>>> features = Features({\"audio\": Audio()})\n>>> ds = Dataset.from_dict({\"audio\": audio}, features=features)\n>>> ds = ds.with_format(\"jax\")\n>>> ds[0][\"audio\"][\"array\"]\nDeviceArray([-0.059021  , -0.03894043, -0.00735474, ...,  0.0133667 ,\n              0.01809692,  0.00268555], dtype=float32)\n>>> ds[0][\"audio\"][\"sampling_rate\"]\nDeviceArray(44100, dtype=int32, weak_type=True)\n```\n\n----------------------------------------\n\nTITLE: Loading Image Datasets with PIL Images using Hugging Face Datasets\nDESCRIPTION: This code snippet demonstrates how to load an image dataset, specifically the 'beans' dataset, and access an image as a PIL Image object. It requires the `datasets` library and the `Image` feature.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/image_load.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset, Image\n\n>>> dataset = load_dataset(\"beans\", split=\"train\")\n>>> dataset[0][\"image\"]\n```\n\n----------------------------------------\n\nTITLE: Converting Dataset to IterableDataset\nDESCRIPTION: This snippet shows how to convert an existing `Dataset` object to an `IterableDataset` using the `to_iterable_dataset()` method.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/access.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\", split=\"train\")\n>>> iterable_dataset = dataset.to_iterable_dataset()\n```\n\n----------------------------------------\n\nTITLE: Convert PDF page to PIL Image\nDESCRIPTION: This snippet demonstrates how to convert a PDF page to a PIL image using `pdfplumber`. It first converts the page to an image object, then saves it to a buffer, and finally opens the image using PIL.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/document_load.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> import PIL.Image\n>>> import io\n>>> first_page.to_image()\n<pdfplumber.display.PageImage at 0x107d68dd0>\n>>> buffer = io.BytesIO()\n>>> first_page.to_image().save(buffer)\n>>> img = PIL.Image.open(buffer)\n>>> img\n<PIL.PngImagePlugin.PngImageFile image mode=P size=612x792>\n```\n\n----------------------------------------\n\nTITLE: Loading an Arrow File with load_dataset in Python\nDESCRIPTION: Loads a dataset from a local or remote Arrow file using the `load_dataset` function. The 'arrow' data loading script must be specified, and data_files must be a dictionary mapping splits to file paths.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/loading.mdx#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\n>>> from datasets import load_dataset\n>>> dataset = load_dataset(\"arrow\", data_files={'train': 'train.arrow', 'test': 'test.arrow'})\n```\n\n----------------------------------------\n\nTITLE: Loading an IterableDataset\nDESCRIPTION: This snippet demonstrates how to load a dataset as an `IterableDataset` using the `streaming=True` parameter in the `load_dataset` function. It then iterates through the dataset and prints the first example.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/access.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n\n>>> iterable_dataset = load_dataset(\"ethz/food101\", split=\"train\", streaming=True)\n>>> for example in iterable_dataset:\n...     print(example)\n...     break\n{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=384x512 at 0x7F0681F5C520>, 'label': 6}\n```\n\n----------------------------------------\n\nTITLE: Setting Dataset Format to PyTorch\nDESCRIPTION: This code sets the dataset format to be compatible with PyTorch. It uses the `set_format` function to specify the type as `torch` and lists the relevant columns for the model. This ensures the data is in the correct format for training.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_dataset.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"label\"])\n>>> dataset.format['type']\n'torch'\n```\n\n----------------------------------------\n\nTITLE: Importing Data from Polars to Datasets\nDESCRIPTION: Demonstrates importing a Polars DataFrame into a Hugging Face Dataset using `Dataset.from_polars`. This allows leveraging the features of the `datasets` library with existing Polars DataFrames.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_polars.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nds = Dataset.from_polars(df)\n```\n\n----------------------------------------\n\nTITLE: Load PDF dataset\nDESCRIPTION: This code snippet demonstrates how to load a PDF dataset from a directory using the `load_dataset` function. It assumes that the `pdfplumber` package is installed. After loading, it accesses the first PDF object in the dataset.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/document_load.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset, Pdf\n\n>>> dataset = load_dataset(\"path/to/pdf/folder\", split=\"train\")\n>>> dataset[0][\"pdf\"]\n<pdfplumber.pdf.PDF at 0x1075bc320>\n```\n\n----------------------------------------\n\nTITLE: Iterating Through JAX-Formatted Dataset\nDESCRIPTION: This snippet shows how to iterate through a JAX-formatted dataset in batches using the `Dataset.iter()` method, which is crucial for feeding data into JAX models during training.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_jax.mdx#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n>>> for epoch in range(epochs):\n...     for batch in ds[\"train\"].iter(batch_size=32):\n...         x, y = batch[\"image\"], batch[\"label\"]\n...         ...\n```\n\n----------------------------------------\n\nTITLE: Exporting Data from Datasets to Polars\nDESCRIPTION: Demonstrates exporting a Hugging Face Dataset to a Polars DataFrame using `Dataset.to_polars`.  This allows for further analysis and manipulation of the dataset using Polars.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_polars.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndf = Dataset.from_polars(ds)\n```\n\n----------------------------------------\n\nTITLE: Loading a dataset with a custom BuilderConfig - Python\nDESCRIPTION: This code shows how to load a dataset using `datasets.load_dataset` and overriding the default builder configuration by passing specific builder configuration arguments.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/dataset_script.mdx#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n>>> from datasets import load_dataset\n>>> dataset = load_dataset('super_glue', data_url=\"https://custom_url\")\n```\n\n----------------------------------------\n\nTITLE: Tokenizing the First Row of Text\nDESCRIPTION: This code snippet demonstrates how to tokenize the first row of the `text` column in the dataset using the loaded tokenizer. It shows the output, which includes `input_ids`, `token_type_ids`, and `attention_mask`.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_dataset.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> tokenizer(dataset[0][\"text\"])\n{'input_ids': [101, 1103, 2067, 1110, 17348, 1106, 1129, 1103, 6880, 1432, 112, 188, 1207, 107, 14255, 1389, 107, 1105, 1115, 1119, 112, 188, 1280, 1106, 1294, 170, 24194, 1256, 3407, 1190, 170, 11791, 5253, 188, 1732, 7200, 10947, 12606, 2895, 117, 179, 7766, 118, 172, 15554, 1181, 3498, 6961, 3263, 1137, 188, 1566, 7912, 14516, 6997, 119, 102], \n 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \n 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n```\n\n----------------------------------------\n\nTITLE: Add Elasticsearch Index with Name\nDESCRIPTION: Adds an Elasticsearch index for a specified column in the dataset, explicitly defining the index name to allow for later reuse.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/faiss_es.mdx#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n>>> squad = load_dataset('rajpurkar/squad', split='validation')\n>>> squad.add_elasticsearch_index(\"context\", host=\"localhost\", port=\"9200\", es_index_name=\"hf_squad_val_context\")\n>>> squad.get_index(\"context\").es_index_name\nhf_squad_val_context\n```\n\n----------------------------------------\n\nTITLE: Loading Combination of Percentages Python\nDESCRIPTION: This snippet demonstrates how to load a combination of percentages (first 10% and last 80%) of the 'train' split of a dataset using string slicing with `datasets.load_dataset`. It loads the 'ajibawa-2023/General-Stories-Collection' dataset and specifies the split using string notation.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/loading.mdx#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\n>>> train_10_80pct_ds = datasets.load_dataset(\"ajibawa-2023/General-Stories-Collection\", split=\"train[:10%]+train[-80%:]\")\n```\n\n----------------------------------------\n\nTITLE: Cleaning up dataset cache files in Python\nDESCRIPTION: This Python code snippet shows how to use the `cleanup_cache_files` method on a `Dataset` object to remove unused cache files from the cache directory. The function returns the number of files removed.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/cache.mdx#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n# Returns the number of removed cache files\n>>> dataset.cleanup_cache_files()\n2\n```\n\n----------------------------------------\n\nTITLE: Removing a Column with Mapping (Datasets Library, Python)\nDESCRIPTION: This example demonstrates removing a column using the `map` function and the `remove_columns` parameter. It creates a new column 'new_sentence' from the 'sentence1' column and then removes the original 'sentence1' column.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/process.mdx#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n>>> updated_dataset = dataset.map(lambda example: {\"new_sentence\": example[\"sentence1\"]}, remove_columns=[\"sentence1\"])\n>>> updated_dataset.column_names\n['sentence2', 'label', 'idx', 'new_sentence']\n```\n\n----------------------------------------\n\nTITLE: Loading Image Dataset with Decode=False in Py\nDESCRIPTION: This snippet demonstrates how to load the 'beans' image dataset and cast the 'image' column to the `Image` feature with `decode=False`. This prevents automatic decoding of the image file. Accessing the 'image' column for the first element in the dataset returns a dictionary containing the path to the image file.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/about_dataset_features.mdx#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset = load_dataset(\"AI-Lab-Makerere/beans\", split=\"train\").cast_column(\"image\", Image(decode=False))\n>>> dataset[0][\"image\"]\n{'bytes': None,\n 'path': '/Users/username/.cache/huggingface/datasets/downloads/extracted/772e7c1fba622cff102b85dd74bcce46e8168634df4eaade7bedd3b8d91d3cd7/train/healthy/healthy_train.265.jpg'}\n```\n\n----------------------------------------\n\nTITLE: Pushing Dataset to Hugging Face Hub\nDESCRIPTION: This code snippet demonstrates how to push a prepared audio dataset to the Hugging Face Hub using `push_to_hub`. This makes the dataset available for sharing and collaboration.  Requires the `datasets` library and authentication with the Hugging Face Hub.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/audio_dataset.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\naudio_dataset.push_to_hub(\"<username>/my_dataset\")\n```\n\n----------------------------------------\n\nTITLE: Load Dataset for Elasticsearch Indexing\nDESCRIPTION: Loads a dataset using the `load_dataset` function from the `datasets` library, preparing it for indexing with Elasticsearch.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/faiss_es.mdx#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n>>> squad = load_dataset('rajpurkar/squad', split='validation')\n```\n\n----------------------------------------\n\nTITLE: Restarting a Map-Style Dataset (Python)\nDESCRIPTION: This code shows how to restart the iteration of a map-style dataset from a specific index using the `select` method. It skips the initial examples up to `start_index` and creates a new dataset starting from that point. Note: This approach can be inefficient and is recommended to use when the state of the sampler can be resumed.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nmy_dataset = my_dataset.select(range(start_index, len(dataset)))\n```\n\n----------------------------------------\n\nTITLE: Setting Dataset Format to Pandas\nDESCRIPTION: This snippet demonstrates how to set the format of a Hugging Face Dataset to Pandas, allowing you to access data as Pandas DataFrames or Series. It initializes a Dataset from a dictionary and then sets the format to 'pandas' using `ds.with_format(\"pandas\")`.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_pandas.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import Dataset\n>>> data = {\"col_0\": [\"a\", \"b\", \"c\", \"d\"], \"col_1\": [0., 0., 1., 1.]}\n>>> ds = Dataset.from_dict(data)\n>>> ds = ds.with_format(\"pandas\")\n>>> ds[0]       # pd.DataFrame\n  col_0  col_1\n0     a    0.0\n>>> ds[:2]      # pd.DataFrame\n  col_0  col_1\n0     a    0.0\n1     b    0.0\n>>> ds[\"data\"]  # pd.Series\n0    a\n1    b\n2    c\n3    d\nName: col_0, dtype: object\n```\n\n----------------------------------------\n\nTITLE: Upload to Organization Dataset\nDESCRIPTION: This command uploads the contents of the current directory to the root of a dataset repository owned by an organization. Replace `MyCoolOrganization/my-cool-dataset` with the actual repository name. The `. .` arguments specify the current directory as both the source and destination.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/share.mdx#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n>>> huggingface-cli upload MyCoolOrganization/my-cool-dataset . . --repo-type dataset\nhttps://huggingface.co/datasetsMyCoolOrganization/my-cool-dataset/tree/main/\n```\n\n----------------------------------------\n\nTITLE: Loading Text Dataset with Hugging Face Datasets in Python\nDESCRIPTION: Loads the MRPC dataset (Microsoft Research Paraphrase Corpus) from Hugging Face Datasets library. Requires the `datasets` library. The dataset is loaded with a specified configuration and split.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/quickstart.mdx#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"nyu-mll/glue\", \"mrpc\", split=\"train\")\n```\n\n----------------------------------------\n\nTITLE: Loading VideoFolder Dataset with Filters\nDESCRIPTION: Shows how to filter a VideoFolder dataset based on a condition. It's especially useful with Parquet metadata. This example filters by the 'label' column. `streaming=True` is recommended to prevent fully downloading before filtering.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/video_load.mdx#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n>>> filters = [(\"label\", \"=\", 0)]\n>>> dataset = load_dataset(\"username/dataset_name\", streaming=True, filters=filters)\n```\n\n----------------------------------------\n\nTITLE: Upload Single File to Dataset Root\nDESCRIPTION: This command uploads a single file to the root of the specified dataset repository. Replace `Wauplin/my-cool-dataset` with the actual repository name and `./files/train.csv` with the path to the file you want to upload.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/share.mdx#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n>>> huggingface-cli upload Wauplin/my-cool-dataset ./files/train.csv --repo-type dataset\nhttps://huggingface.co/datasetsWauplin/my-cool-dataset/blob/main/train.csv\n```\n\n----------------------------------------\n\nTITLE: Loading ImageFolder with Filters and Hugging Face Datasets\nDESCRIPTION: This code snippet demonstrates how to load a subset of a dataset based on a condition applied to a label or metadata using the `filters` argument of the `load_dataset` function. It requires the `datasets` library, and it is recommended to use this with `streaming=True`.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/image_load.mdx#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> filters = [(\"label\", \"=\", 0)]\n>>> dataset = load_dataset(\"username/dataset_name\", streaming=True, filters=filters)\n```\n\n----------------------------------------\n\nTITLE: Loading PdfFolder Dataset Directly\nDESCRIPTION: Loads a dataset using the PdfFolder structure directly with the `load_dataset` function from the `datasets` library. It automatically infers class labels based on the directory structure. This requires the `datasets` library to be installed.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/document_dataset.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"path/to/folder\")\n```\n\n----------------------------------------\n\nTITLE: Accessing ImageFolder Images and Labels with Hugging Face Datasets\nDESCRIPTION: This snippet demonstrates how to access images and labels in an `ImageFolder` dataset.  It assumes the dataset has been loaded as shown in prior examples and that the images have been decoded as PIL objects.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/image_load.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset[\"train\"][0]\n{\"image\": <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=1200x215 at 0x15E6D7160>, \"label\": 0}\n\n>>> dataset[\"train\"][-1]\n{\"image\": <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=1200x215 at 0x15E8DAD30>, \"label\": 1}\n```\n\n----------------------------------------\n\nTITLE: Formatting Dataset to JAX with Device Specification\nDESCRIPTION: This example demonstrates how to format a `Dataset` to JAX and specify the device for the data. It converts the device object to a string before passing it to `with_format` to avoid serialization issues.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_jax.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> import jax\n>>> from datasets import Dataset\n>>> data = [[1, 2], [3, 4]]\n>>> ds = Dataset.from_dict({\"data\": data})\n>>> device = str(jax.devices()[0])  # Not casting to `str` before passing it to `with_format` will raise a `ValueError`\n>>> ds = ds.with_format(\"jax\", device=device)\n>>> ds[0]\n{'data': DeviceArray([1, 2], dtype=int32)}\n>>> ds[0][\"data\"].device()\nTFRT_CPU_0\n>>> assert ds[0][\"data\"].device() == jax.devices()[0]\nTrue\n```\n\n----------------------------------------\n\nTITLE: Accessing Video Frames\nDESCRIPTION: This snippet shows how to access individual frames from a `torchvision.io.video_reader.VideoReader` object. It uses `next()` to get the first frame and then shows how to access the frame data (image) and the timestamp. The `VideoReader` provides sequential access to frames.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/video_load.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> video = dataset[0][\"video\"]\n>>> first_frame = next(video)\n>>> first_frame[\"data\"].shape\n(3, 240, 320)\n>>> first_frame[\"pts\"]  # timestamp\n0.0\n```\n\n----------------------------------------\n\nTITLE: Loading VideoFolder dataset from directory - Python\nDESCRIPTION: Loads a video dataset using the VideoFolder dataset builder by specifying the path to the folder containing the video files. The class labels are automatically inferred from the directory structure. Requires the `datasets` library to be installed.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/video_dataset.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"path/to/folder\")\n```\n\n----------------------------------------\n\nTITLE: Defining Dataset Splits with SplitGenerator in Python\nDESCRIPTION: This code snippet demonstrates how to define dataset splits using the `SplitGenerator` class. It downloads audio and metadata files, extracts the audio archive if not in streaming mode, and then creates train and test splits. Each split's `gen_kwargs` contains paths to extracted archives, audio files (iterated using `dl_manager.iter_archive`), metadata files, and a path to clips directory. Requires `_AUDIO_URL` and `_METADATA_URL` to be defined.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/audio_dataset.mdx#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndef _split_generators(self, dl_manager):\n    \"\"\"Returns SplitGenerators.\"\"\"\n    audio_path = dl_manager.download(_AUDIO_URL)\n    local_extracted_archive = dl_manager.extract(audio_path) if not dl_manager.is_streaming else None\n    path_to_clips = \"librivox-indonesia\"\n\n    return [\n        datasets.SplitGenerator(\n            name=datasets.Split.TRAIN,\n            gen_kwargs={\n                \"local_extracted_archive\": local_extracted_archive,\n                \"audio_files\": dl_manager.iter_archive(audio_path),\n                \"metadata_path\": dl_manager.download_and_extract(_METADATA_URL + \"/metadata_train.csv.gz\"),\n                \"path_to_clips\": path_to_clips,\n            },\n        ),\n        datasets.SplitGenerator(\n            name=datasets.Split.TEST,\n            gen_kwargs={\n                \"local_extracted_archive\": local_extracted_archive,\n                \"audio_files\": dl_manager.iter_archive(audio_path),\n                \"metadata_path\": dl_manager.download_and_extract(_METADATA_URL + \"/metadata_test.csv.gz\"),\n                \"path_to_clips\": path_to_clips,\n            },\n        ),\n    ]\n```\n\n----------------------------------------\n\nTITLE: Tokenizing the Text Dataset\nDESCRIPTION: Tokenizes the text dataset using the loaded tokenizer. The tokenizer truncates and pads the text.  The function utilizes sentence1 and sentence2 keys to generate input_ids, token_type_ids, and attention_mask.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/quickstart.mdx#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n>>> def encode(examples):\n...     return tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True, padding=\"max_length\")\n\n>>> dataset = dataset.map(encode, batched=True)\n>>> dataset[0]\n{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n'label': 1,\n'idx': 0,\n'input_ids': array([  101,  7277,  2180,  5303,  4806,  1117,  1711,   117,  2292, 1119,  1270,   107,  1103,  7737,   107,   117,  1104,  9938, 4267, 12223, 21811,  1117,  2554,   119,   102, 11336,  6732, 3384,  1106,  1140,  1112,  1178,   107,  1103,  7737,   107,   117, 7277,  2180,  5303,  4806,  1117,  1711,  1104,  9938, 4267, 12223, 21811,  1117,  2554,   119,   102]),\n'token_type_ids': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n```\n\n----------------------------------------\n\nTITLE: Loading Image Captioning Dataset with ImageFolder\nDESCRIPTION: Loads an image dataset for image captioning tasks using `ImageFolder`. It reads the text captions from a metadata file and creates a `text` column.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/image_dataset.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset = load_dataset(\"imagefolder\", data_dir=\"/path/to/folder\", split=\"train\")\n>>> dataset[0][\"text\"]\n\"This is a golden retriever playing with a ball\"\n```\n\n----------------------------------------\n\nTITLE: Loading a dataset from a local path using datasets.load_dataset - Python\nDESCRIPTION: This code demonstrates how to load a dataset using the `datasets.load_dataset` function, specifying the path to the dataset loading script.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/dataset_script.mdx#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n>>> from datasets import load_dataset\n>>> load_dataset(\"path/to/my_dataset\")\n```\n\n----------------------------------------\n\nTITLE: YAML Configuration with Multiple Files per Split\nDESCRIPTION: This snippet showcases using a list of file paths within the YAML configuration to include multiple files in a single split. The 'train' split includes 'data/abc.csv' and 'data/def.csv', while the 'test' split includes 'holdout/ghi.csv'.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/repository_structure.mdx#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n---\nconfigs:\n- config_name: default\n  data_files:\n  - split: train\n    path:\n    - \"data/abc.csv\"\n    - \"data/def.csv\"\n  - split: test\n    path: \"holdout/ghi.csv\"\n---\n```\n\n----------------------------------------\n\nTITLE: Set Logging Verbosity to Info - Python\nDESCRIPTION: This code snippet demonstrates how to set the logging verbosity level to INFO using the `datasets.logging.set_verbosity_info()` method. This will cause the library to report errors, warnings, and basic information.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/package_reference/utilities.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport datasets\ndatasets.logging.set_verbosity_info()\n```\n\n----------------------------------------\n\nTITLE: Loading and Inspecting SQuAD Dataset Features in Py\nDESCRIPTION: This snippet demonstrates how to load the 'squad' dataset using the `load_dataset` function and then inspect its features using the `dataset.features` attribute. The 'squad' dataset is a question answering dataset, so the features include 'answers' (a Sequence containing 'text' and 'answer_start'), 'context' (a string), 'id' (a string), 'question' (a string), and 'title' (a string). The `Sequence` feature is used to represent lists of objects, in this case, the 'answers' field.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/about_dataset_features.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n>>> dataset = load_dataset('rajpurkar/squad', split='train')\n>>> dataset.features\n{'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None),\n'context': Value(dtype='string', id=None),\n'id': Value(dtype='string', id=None),\n'question': Value(dtype='string', id=None),\n'title': Value(dtype='string', id=None)}\n```\n\n----------------------------------------\n\nTITLE: Load PDF dataset with PdfFolder\nDESCRIPTION: This code demonstrates loading a PDF dataset using the `PdfFolder` dataset builder, which is ideal for datasets with a specific directory structure. It shows how to load the dataset from a local path or a Hugging Face Hub repository.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/document_load.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"username/dataset_name\")\n>>> # OR locally:\n>>> dataset = load_dataset(\"/path/to/folder\")\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset = load_dataset(\"pdffolder\", data_dir=\"/path/to/folder\")\n```\n\n----------------------------------------\n\nTITLE: Loading a Dataset from the Hub\nDESCRIPTION: This code snippet demonstrates how to load a dataset from the Hugging Face Hub using the `load_dataset` function. It requires the `datasets` library to be installed. The dataset is identified by the `<username>/my_dataset` identifier.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/audio_dataset.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"<username>/my_dataset\")\n```\n\n----------------------------------------\n\nTITLE: Accessing Annotation Data\nDESCRIPTION: Demonstrates how to retrieve the annotation (segmentation mask) data from the loaded dataset using the index and the 'annotation' key.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/semantic_segmentation.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset[index][\"annotation\"]\n```\n\n----------------------------------------\n\nTITLE: Loading WebDataset\nDESCRIPTION: Loads a dataset in the WebDataset format. The data is stored in TAR archives and each example is identified by a common prefix for its constituent files.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/image_dataset.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"webdataset\", data_dir=\"/path/to/folder\", split=\"train\")\n>>> dataset[0][\"json\"]\n{\"bbox\": [[302.0, 109.0, 73.0, 52.0]], \"categories\": [0]}\n```\n\n----------------------------------------\n\nTITLE: Load PDF paths without decoding\nDESCRIPTION: This snippet shows how to load the underlying paths of PDF files without decoding them into `pdfplumber` objects by setting `decode=False` in the `Pdf` feature. This can be useful when you only need the file paths and not the PDF content itself.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/document_load.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset = dataset.cast_column(\"pdf\", Pdf(decode=False))\n>>> dataset[0][\"pdf\"]\n{'bytes': None,\n 'path': 'path/to/pdf/folder/pdf0.pdf'}\n```\n\n----------------------------------------\n\nTITLE: Converting to IterableDataset (Python)\nDESCRIPTION: These snippets demonstrate how to convert a map-style `Dataset` to an `IterableDataset` using `to_iterable_dataset`. The first example shows a simple conversion. The second example demonstrates how to convert to a sharded `IterableDataset` specifying the number of shards which makes it suitable to use with `DataLoader` and PyTorch.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nmy_iterable_dataset = my_dataset.to_iterable_dataset()\n```\n\nLANGUAGE: python\nCODE:\n```\nmy_iterable_dataset = my_dataset.to_iterable_dataset(num_shards=1024)\nmy_iterable_dataset.num_shards  # 1024\n```\n\n----------------------------------------\n\nTITLE: Generating Examples from Data Files in Python\nDESCRIPTION: This Python function, _generate_examples, reads and parses the data files (e.g., JSON) to extract the dataset examples. It opens the file, loads the JSON data, iterates through the articles and paragraphs, and extracts the question, ID, context, and answers. It yields a tuple containing an ID and a dictionary representing a single example in the dataset.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/dataset_script.mdx#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef _generate_examples(self, filepath):\n    \"\"\"This function returns the examples in the raw (text) form.\"\"\"\n    logger.info(\"generating examples from = %s\", filepath)\n    with open(filepath) as f:\n        squad = json.load(f)\n        for article in squad[\"data\"]:\n            title = article.get(\"title\", \"\").strip()\n            for paragraph in article[\"paragraphs\"]:\n                context = paragraph[\"context\"].strip()\n                for qa in paragraph[\"qas\"]:\n                    question = qa[\"question\"].strip()\n                    id_ = qa[\"id\"]\n\n                    answer_starts = [answer[\"answer_start\"] for answer in qa[\"answers\"]]\n                    answers = [answer[\"text\"].strip() for answer in qa[\"answers\"]]\n\n                    # Features currently used are \"context\", \"question\", and \"answers\".\n                    # Others are extracted here for the ease of future expansions.\n                    yield id_, {\n                        \"title\": title,\n                        \"context\": context,\n                        \"question\": question,\n                        \"id\": id_,\n                        \"answers\": {\"answer_start\": answer_starts, \"text\": answers,},\n                    }\n```\n\n----------------------------------------\n\nTITLE: Determining libsndfile Version\nDESCRIPTION: Determines the version of the libsndfile library being used by the soundfile package, which is an audio dependency for Hugging Face Datasets. This is important for ensuring compatibility with MP3 decoding.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/installation.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython -c \"import soundfile; print(soundfile.__libsndfile_version__)\"\n```\n\n----------------------------------------\n\nTITLE: YAML Configuration with Glob Patterns\nDESCRIPTION: This snippet shows how to use glob patterns in the YAML configuration to automatically include all files matching a specific pattern within a split. The 'train' split includes all CSV files in the 'data' directory, and the 'test' split includes all CSV files in the 'holdout' directory.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/repository_structure.mdx#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n---\nconfigs:\n- config_name: default\n  data_files:\n  - split: train\n    path: \"data/*.csv\"\n  - split: test\n    path: \"holdout/*.csv\"\n---\n```\n\n----------------------------------------\n\nTITLE: YAML Configuration with Builder Parameters\nDESCRIPTION: This snippet demonstrates passing builder-specific parameters via YAML, specifically the separator ('sep') for CSV files. It defines two configurations: 'tab' using a tab separator and 'comma' using a comma separator.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/repository_structure.mdx#_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\n---\nconfigs:\n- config_name: tab\n  data_files: \"main_data.csv\"\n  sep: \"\\t\"\n- config_name: comma\n  data_files: \"additional_data.csv\"\n  sep: \",\"\n---\n```\n\n----------------------------------------\n\nTITLE: Handling Too Many Requests Error in push_to_hub()\nDESCRIPTION: This snippet shows the error message encountered when exceeding hourly quotas for commits when uploading datasets via `push_to_hub()`. It highlights the need to upgrade the `datasets` library to at least version `2.15.0` to resolve this issue.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/troubleshoot.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nHfHubHTTPError: 429 Client Error: Too Many Requests for url: ...\nYou have exceeded our hourly quotas for action: commit. We invite you to retry later.\n```\n\n----------------------------------------\n\nTITLE: Loading an Audio Dataset from a Folder with Hugging Face Datasets\nDESCRIPTION: This code snippet shows how to load an audio dataset from a directory structure using the `load_dataset` function with the \"audiofolder\" format. The `data_dir` argument specifies the path to the root directory containing the audio folders.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/create_dataset.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"audiofolder\", data_dir=\"/path/to/folder\")\n```\n\n----------------------------------------\n\nTITLE: Create and Upload Data to Hugging Face Dataset (Python)\nDESCRIPTION: This code demonstrates creating a Hugging Face Dataset repository and uploading data files from a cloud storage location to it. It imports necessary modules from `huggingface_hub` and `tqdm`, creates a repository, and iterates through a list of data files retrieved from cloud storage. Within the loop, it opens each file, constructs the path within the repository, and uploads the file using `upload_file` function, showing progress with `tqdm`.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/filesystems.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> from huggingface_hub import create_repo, upload_file\n>>> from tqdm.auto import tqdm\n>>> destination_dataset = \"username/my-dataset\"\n>>> create_repo(destination_dataset, repo_type=\"dataset\")\n>>> for data_file in tqdm(fs.glob(data_dir + pattern)):\n...     with fs.open(data_file) as fileobj:\n...         path_in_repo = data_file[len(data_dir):]\n...         upload_file(\n...             path_or_fileobj=fileobj,\n...             path_in_repo=path_in_repo,\n...             repo_id=destination_dataset,\n...             repo_type=\"dataset\",\n...         )\n```\n\n----------------------------------------\n\nTITLE: Loading VideoFolder for video captioning - Python\nDESCRIPTION: Loads a video captioning dataset using the VideoFolder, specifying the data directory and the split (e.g., train). Assumes a metadata file (e.g., metadata.csv) contains file_name and text columns for video filenames and captions respectively. Requires the `datasets` library.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/video_dataset.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset = load_dataset(\"videofolder\", data_dir=\"/path/to/folder\", split=\"train\")\n>>> dataset[0][\"text\"]\n\"This is a golden retriever playing with a ball\"\n```\n\n----------------------------------------\n\nTITLE: Creating Image Dataset from Numpy Arrays in Py\nDESCRIPTION: This snippet demonstrates how to create a dataset of images directly from numpy arrays using the `Dataset.from_dict` method and specifying the `Image` feature. This allows creating datasets from in-memory image data. The numpy array is automatically encoded as a PNG (or TIFF) image and stored in the dataset.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/about_dataset_features.mdx#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n>>> ds = Dataset.from_dict({\"i\": [np.zeros(shape=(16, 16, 3), dtype=np.uint8)]}, features=Features({\"i\": Image()}))\n```\n\n----------------------------------------\n\nTITLE: Align Labels with Custom Mapping\nDESCRIPTION: This code snippet demonstrates how to align the labels of a dataset with a custom label mapping using the `align_labels_with_mapping` function. It loads the 'mnli' dataset, defines a custom label mapping, and then aligns the dataset's labels with the custom mapping. The function requires specifying the label-to-ID mapping and the column containing the labels.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/nlp_process.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n\n>>> mnli = load_dataset(\"nyu-mll/glue\", \"mnli\", split=\"train\")\n>>> mnli_aligned = mnli.align_labels_with_mapping(label2id, \"label\")\n```\n\n----------------------------------------\n\nTITLE: Loading Audio Dataset\nDESCRIPTION: Loads the MInDS-14 audio dataset using the `load_dataset` function from the `datasets` library. It specifies the dataset name (`PolyAI/minds14`), the configuration (`en-US`), and the split (`train`).\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/quickstart.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset, Audio\n\n>>> dataset = load_dataset(\"PolyAI/minds14\", \"en-US\", split=\"train\")\n```\n\n----------------------------------------\n\nTITLE: Define Image Transformations with Albumentations\nDESCRIPTION: Defines a sequence of image transformations using the `albumentations` library. These transformations include random cropping, horizontal flipping, and random brightness/contrast adjustments.  The transformations are applied randomly with specified probabilities. Requires `cv2`, `albumentations`, and `numpy`.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/image_classification.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> import cv2\n>>> import albumentations\n>>> import numpy as np\n\n>>> transform = albumentations.Compose([\n...     albumentations.RandomCrop(width=256, height=256),\n...     albumentations.HorizontalFlip(p=0.5),\n...     albumentations.RandomBrightnessContrast(p=0.2),\n... ])\n```\n\n----------------------------------------\n\nTITLE: Accessing Dataset Metadata with Python\nDESCRIPTION: Demonstrates how to load a dataset builder and access its metadata (DatasetInfo) using `load_dataset_builder` and the `.info` attribute. This is a way to programmatically access the dataset's description, features, homepage, license and citation.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/audio_dataset.mdx#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset_builder\n>>> ds_builder = load_dataset_builder(\"vivos\")\n>>> ds_builder.info\n```\n\n----------------------------------------\n\nTITLE: Cloning Datasets Repository\nDESCRIPTION: Clones the Hugging Face Datasets repository from GitHub.  This is the first step for installing the library from source, allowing for code modifications.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/installation.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/huggingface/datasets.git\n```\n\n----------------------------------------\n\nTITLE: Loading ImageFolder Dataset with Explicit Builder\nDESCRIPTION: Loads an image dataset using the `ImageFolder` builder and specifies the data directory explicitly. This is an alternative to directly passing the path.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/image_dataset.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset = load_dataset(\"imagefolder\", data_dir=\"/path/to/folder\")\n```\n\n----------------------------------------\n\nTITLE: Visualizing Multiple Augmented Samples\nDESCRIPTION: This snippet iterates through a set of random indices, retrieves corresponding examples from the transformed dataset, merges the augmented images and depth maps using the `merge_into_row` function, and displays them using Matplotlib.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/depth_estimation.mdx#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n>>> plt.figure(figsize=(15, 6))\n\n>>> for i, idx in enumerate(random_indices):\n...     ax = plt.subplot(3, 3, i + 1)\n...     example = train_dataset[idx]\n...     image_viz = merge_into_row(\n...         example[\"pixel_values\"], example[\"labels\"]\n...     )\n...     plt.imshow(image_viz.astype(\"uint8\"))\n...     plt.axis(\"off\")\n```\n\n----------------------------------------\n\nTITLE: Load Dataset and Feature Extractor in Python\nDESCRIPTION: Loads the Beans dataset, the Image feature, and the feature extractor corresponding to a pretrained ViT model from Hugging Face Transformers and Datasets libraries. It initializes the feature extractor and loads the dataset's training split.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_dataset.mdx#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n>>> from transformers import AutoFeatureExtractor\n>>> from datasets import load_dataset, Image\n\n>>> feature_extractor = AutoFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n>>> dataset = load_dataset(\"AI-Lab-Makerere/beans\", split=\"train\")\n```\n\n----------------------------------------\n\nTITLE: Creating a Development Branch\nDESCRIPTION: This snippet shows how to create a new branch for development. It is recommended to create a new branch for each feature or bug fix, rather than working directly on the main branch. This helps to keep your changes isolated and makes it easier to submit pull requests.\nSOURCE: https://github.com/huggingface/datasets/blob/main/CONTRIBUTING.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit checkout -b a-descriptive-name-for-my-changes\n```\n\n----------------------------------------\n\nTITLE: Loading AudioFolder with data_dir in Python\nDESCRIPTION: This snippet demonstrates loading an AudioFolder dataset by explicitly passing 'audiofolder' as the dataset name and specifying the local directory using the `data_dir` argument. It uses the `load_dataset` function and is equivalent to directly passing the directory path.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/audio_load.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset = load_dataset(\"audiofolder\", data_dir=\"/path/to/folder\")\n```\n\n----------------------------------------\n\nTITLE: Load FAISS Index\nDESCRIPTION: Loads a previously saved FAISS index from disk. This allows for reusing the index without re-computing the embeddings, thus speeding up the search process.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/faiss_es.mdx#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> ds = load_dataset('crime_and_punish', split='train[:100]')\n>>> ds.load_faiss_index('embeddings', 'my_index.faiss')\n```\n\n----------------------------------------\n\nTITLE: Specify Commit Message and Description\nDESCRIPTION: This command uploads the contents of a specified folder to a dataset repository, including a custom commit message and description. Replace `Wauplin/my-cool-dataset` with the actual repository name, `./data` with the path to the folder you want to upload, and the commit message and description with your desired values.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/share.mdx#_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\n>>> huggingface-cli upload Wauplin/my-cool-dataset ./data . --repo-type dataset --commit-message=\"Version 2\" --commit-description=\"Train size: 4321. Check Dataset Viewer for more details.\"\n...\nhttps://huggingface.co/datasetsWauplin/my-cool-dataset/tree/main\n```\n\n----------------------------------------\n\nTITLE: Exporting Data from Dataset to Pandas\nDESCRIPTION: This snippet demonstrates how to export a Hugging Face Dataset to a Pandas DataFrame using `Dataset.to_pandas`. Requires a Dataset `ds` to be defined beforehand. It stores the result into a Pandas DataFrame named `df`.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_pandas.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndf = Dataset.to_pandas()\n```\n\n----------------------------------------\n\nTITLE: Loading an Image Dataset from a Folder with Hugging Face Datasets\nDESCRIPTION: This code snippet shows how to load an image dataset from a directory structure using the `load_dataset` function with the \"imagefolder\" format. The `data_dir` argument specifies the path to the root directory containing the image folders.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/create_dataset.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"imagefolder\", data_dir=\"/path/to/pokemon\")\n```\n\n----------------------------------------\n\nTITLE: Upload Files to a Specific Revision\nDESCRIPTION: This command uploads files to a specific revision (branch or PR) of the dataset repository. Replace `bigcode/the-stack` with the actual repository name and `refs/pr/104` with the desired revision.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/share.mdx#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\n# Upload files to a PR\nhuggingface-cli upload bigcode/the-stack . . --repo-type dataset --revision refs/pr/104\n...\n```\n\n----------------------------------------\n\nTITLE: Defining Dynamic Array3D Features in Py\nDESCRIPTION: This snippet demonstrates how to define a `Features` object with an `Array3D` feature that has a dynamic first dimension. The `Array3D` feature is used to represent arrays with three dimensions. Setting the first dimension to `None` allows for variable-length sequences. In this example, the 'a' column is defined as an `Array3D` with a shape of (None, 5, 2) and a data type of 'int32'.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/about_dataset_features.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> features = Features({'a': Array3D(shape=(None, 5, 2), dtype='int32')})\n```\n\n----------------------------------------\n\nTITLE: Creating an IterableDataset from a Generator in Python\nDESCRIPTION: Demonstrates the creation of an IterableDataset using a generator function that yields examples one at a time, providing a lazy way to load data.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef my_generator(n):\n    for i in range(n):\n        yield {\"col_1\": i}\n\nmy_iterable_dataset = IterableDataset.from_generator(my_generator, gen_kwargs={\"n\": 10})\nfor example in my_iterable_dataset:\n    print(example)\n    break\n```\n\n----------------------------------------\n\nTITLE: Installing Datasets Library\nDESCRIPTION: This command installs the core `datasets` library using pip, the Python package installer. It allows the user to load and work with various datasets from the Hugging Face Hub and other sources.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/quickstart.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install datasets\n```\n\n----------------------------------------\n\nTITLE: Mapping dataset with cache disabling in Python\nDESCRIPTION: This Python code snippet demonstrates how to map a function over a dataset using the `map` method, while disabling the cache using the `load_from_cache_file=False` argument.  This forces the function to be applied to all samples, even if they exist in the cache.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/cache.mdx#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n>>> updated_dataset = small_dataset.map(add_prefix, load_from_cache_file=False)\n```\n\n----------------------------------------\n\nTITLE: Loading a Dataset Configuration with Python\nDESCRIPTION: This example demonstrates loading a specific configuration ('bal') of the 'indonesian-nlp/librivox-indonesia' dataset using the `load_dataset` function from the `datasets` library. It assigns the loaded dataset to the `dataset` variable.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/audio_dataset.mdx#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n>>> dataset = load_dataset(\"indonesian-nlp/librivox-indonesia\", \"bal\", split=\"train\")\n```\n\n----------------------------------------\n\nTITLE: Loading Object Detection Dataset with ImageFolder\nDESCRIPTION: Loads an object detection dataset using `ImageFolder`. It reads bounding box and category information from a metadata file and creates an `objects` column.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/image_dataset.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset = load_dataset(\"imagefolder\", data_dir=\"/path/to/folder\", split=\"train\")\n>>> dataset[0][\"objects\"]\n{\"bbox\": [[302.0, 109.0, 73.0, 52.0]], \"categories\": [0]}\n```\n\n----------------------------------------\n\nTITLE: Create FAISS Index\nDESCRIPTION: Creates a FAISS index on a specified column of the dataset. The specified column contains the pre-computed embeddings.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/faiss_es.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> ds_with_embeddings.add_faiss_index(column='embeddings')\n```\n\n----------------------------------------\n\nTITLE: Casting a Column to Audio Feature\nDESCRIPTION: This code snippet shows how to load audio files from local paths and cast the 'audio' column to the Audio feature using `Dataset.from_dict` and `cast_column`. This prepares the data for audio processing within the datasets library. It requires the `datasets` library to be installed.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/audio_dataset.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\naudio_dataset = Dataset.from_dict({\"audio\": [\"path/to/audio_1\", \"path/to/audio_2\", ..., \"path/to/audio_n\"]}).cast_column(\"audio\", Audio())\naudio_dataset[0][\"audio\"]\n\n```\n\n----------------------------------------\n\nTITLE: Loading Sharded Dataset from Python Generator Python\nDESCRIPTION: This snippet demonstrates how to create a sharded Hugging Face Dataset from a Python generator using the `IterableDataset.from_generator` method. It reads shards of data from multiple text files and yields dictionaries. It uses `torch.utils.data.DataLoader` for parallel data loading.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/loading.mdx#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n>>> def gen(shards):\n...     for shard in shards:\n...         with open(shard) as f:\n...             for line in f:\n...                 yield {\"line\": line}\n...\n>>> shards = [f\"data{i}.txt\" for i in range(32)]\n>>> ds = IterableDataset.from_generator(gen, gen_kwargs={\"shards\": shards})\n>>> ds = ds.shuffle(seed=42, buffer_size=10_000)  # shuffles the shards order + uses a shuffle buffer\n>>> from torch.utils.data import DataLoader\n>>> dataloader = DataLoader(ds.with_format(\"torch\"), num_workers=4)  # give each worker a subset of 32/4=8 shards\n```\n\n----------------------------------------\n\nTITLE: Decoding Images with Multithreading using Hugging Face Datasets\nDESCRIPTION: This code snippet shows how to speed up image decoding by using multithreading. It calculates the number of threads to use based on the number of available CPUs and then calls the `decode` method on the dataset.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/image_load.mdx#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n>>> import os\n>>> num_threads = num_threads = min(32, (os.cpu_count() or 1) + 4)\n>>> dataset = dataset.decode(num_threads=num_threads)\n>>> for example in dataset:  # up to 20 times faster !\n...     ...\n```\n\n----------------------------------------\n\nTITLE: Loading PdfFolder Dataset with Explicit Arguments\nDESCRIPTION: Loads a dataset using the PdfFolder structure with explicit arguments for the dataset builder (`pdffolder`) and the data directory (`data_dir`). This method provides more control over how the dataset is loaded.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/document_dataset.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset = load_dataset(\"pdffolder\", data_dir=\"/path/to/folder\")\n```\n\n----------------------------------------\n\nTITLE: Using Audio Feature Type\nDESCRIPTION: This snippet demonstrates how to use the Audio feature type in datasets.  It requires the `audio` extra to be installed (`pip install datasets[audio]`). It loads audio file paths, defines an Audio feature, creates a dataset, formats it as NumPy, and shows how to access the audio array and sampling rate.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_numpy.mdx#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import Dataset, Features, Audio\n>>> audio = [\"path/to/audio.wav\"] * 10\n>>> features = Features({\"audio\": Audio()})\n>>> ds = Dataset.from_dict({\"audio\": audio}, features=features)\n>>> ds = ds.with_format(\"numpy\")\n>>> ds[0][\"audio\"][\"array\"]\narray([-0.059021  , -0.03894043, -0.00735474, ...,  0.0133667 ,\n              0.01809692,  0.00268555], dtype=float32)\n>>> ds[0][\"audio\"][\"sampling_rate\"]\narray(44100, weak_type=True)\n```\n\n----------------------------------------\n\nTITLE: Configuring Dataset Subsets in Python\nDESCRIPTION: This code demonstrates how to define dataset subsets within the `BUILDER_CONFIGS` list of a `GeneratorBasedBuilder` subclass.  Each `Food101Config` specifies a name, description, data URL, and metadata URLs for a specific subset (e.g., 'breakfast' and 'dinner').  This allows users to load specific configurations of the dataset.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/image_dataset.mdx#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nclass Food101(datasets.GeneratorBasedBuilder):\n    \"\"\"Food-101 Images dataset\"\"\"\n \n    BUILDER_CONFIGS = [\n        Food101Config(\n            name=\"breakfast\",\n            description=\"Food types commonly eaten during breakfast.\",\n            data_url=\"https://link-to-breakfast-foods.zip\",\n            metadata_urls={\n                \"train\": \"https://link-to-breakfast-foods-train.txt\", \n                \"validation\": \"https://link-to-breakfast-foods-validation.txt\"\n            },\n        ,\n        Food101Config(\n            name=\"dinner\",\n            description=\"Food types commonly eaten during dinner.\",\n            data_url=\"https://link-to-dinner-foods.zip\",\n            metadata_urls={\n                \"train\": \"https://link-to-dinner-foods-train.txt\", \n                \"validation\": \"https://link-to-dinner-foods-validation.txt\"\n            },\n        )...\n    ]\n```\n\n----------------------------------------\n\nTITLE: Applying ColorJitter with Dataset.set_transform in Python\nDESCRIPTION: This snippet demonstrates how to apply a color jitter transform to images using `Dataset.set_transform`. It defines a `transforms` function that applies the `jitter` transformation to each image in the dataset and then uses `dataset.set_transform` to apply this transformation on-the-fly. It utilizes the `torchvision` library for defining the color jitter transform. The function expects an input dictionary `examples` containing image data under the \"image\" key and returns the modified dictionary with color jittered images stored under the 'pixel_values' key.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/image_process.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> def transforms(examples):\n...     examples[\"pixel_values\"] = [jitter(image.convert(\"RGB\")) for image in examples[\"image\"]]\n...     return examples\n```\n\n----------------------------------------\n\nTITLE: YAML Configuration for Data Splits\nDESCRIPTION: This snippet demonstrates how to define data splits using a YAML configuration within the README.md file. It specifies the 'train' split using 'data.csv' and the 'test' split using 'holdout.csv'.  This is useful when filenames do not conform to the standard train/test/validation naming.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/repository_structure.mdx#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n---\nconfigs:\n- config_name: default\n  data_files:\n  - split: train\n    path: \"data.csv\"\n  - split: test\n    path: \"holdout.csv\"\n---\n```\n\n----------------------------------------\n\nTITLE: Loading AudioFolder Dataset\nDESCRIPTION: This code snippet shows how to load a dataset using the `AudioFolder` builder with `load_dataset`. The dataset is structured in a directory format recognizable by `AudioFolder`, where subdirectories represent classes. Requires the `datasets` library and that the dataset be structured in a specific directory format.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/audio_dataset.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"username/dataset_name\")\n```\n\n----------------------------------------\n\nTITLE: Loading OCR Dataset with PdfFolder and Accessing Text\nDESCRIPTION: Loads a dataset of PDF documents containing text (OCR data) using PdfFolder. The text content is then accessed from the loaded dataset.  Requires the `datasets` library to be installed.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/document_dataset.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset = load_dataset(\"pdffolder\", data_dir=\"/path/to/folder\", split=\"train\")\n>>> dataset[0][\"text\"]\n\"Invoice 1234 from 01/01/1970...\"\n```\n\n----------------------------------------\n\nTITLE: Defining a function for batch transformations\nDESCRIPTION: This code defines a function `transforms` that applies the previously defined Albumentations transformations to a batch of image examples. It iterates through the images and their corresponding object data, applies the transformation, and returns a dictionary containing transformed images, bounding boxes, and categories.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/object_detection.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> def transforms(examples):\n...     images, bboxes, categories = [], [], []\n...     for image, objects in zip(examples['image'], examples['objects']):\n...         image = np.array(image.convert(\"RGB\"))\n...         out = transform(\n...             image=image,\n...             bboxes=objects['bbox'],\n...             category=objects['category']\n...         )\n...         images.append(torch.tensor(out['image']).permute(2, 0, 1))\n...         bboxes.append(torch.tensor(out['bboxes']))\n...         categories.append(out['category'])\n...     return {'image': images, 'bbox': bboxes, 'category': categories}\n```\n\n----------------------------------------\n\nTITLE: Renaming Columns in a Dataset in Python\nDESCRIPTION: This code snippet demonstrates how to rename columns in a dataset using the `rename_column` method. It renames 'sentence1' to 'sentenceA' and 'sentence2' to 'sentenceB'.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/process.mdx#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset\nDataset({\n    features: ['sentence1', 'sentence2', 'label', 'idx'],\n    num_rows: 3668\n})\n>>> dataset = dataset.rename_column(\"sentence1\", \"sentenceA\")\n>>> dataset = dataset.rename_column(\"sentence2\", \"sentenceB\")\n>>> dataset\nDataset({\n    features: ['sentenceA', 'sentenceB', 'label', 'idx'],\n    num_rows: 3668\n})\n```\n\n----------------------------------------\n\nTITLE: Hugging Face CLI Upload Usage\nDESCRIPTION: This demonstrates the basic usage pattern for the `huggingface-cli upload` command.  It shows the positional arguments needed: the dataset repository ID, the local path to upload, and the path within the repository where the file(s) will be placed.  The `--repo-type dataset` flag specifies that you are uploading to a dataset repository.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/share.mdx#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Usage:  huggingface-cli upload [dataset_repo_id] [local_path] [path_in_repo] --repo-type dataset\n```\n\n----------------------------------------\n\nTITLE: Disabling Video Decoding\nDESCRIPTION: Shows how to disable video decoding entirely, which is useful if you only need access to the video paths or bytes without decoding them into `VideoReader` objects.  This reduces memory usage.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/video_load.mdx#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset = dataset.decode(False)\n```\n\n----------------------------------------\n\nTITLE: Installing Audio Feature for Datasets\nDESCRIPTION: This command installs the `audio` feature of the `datasets` library. This includes all the necessary dependencies and functionality for working with audio datasets, such as resampling and feature extraction.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/quickstart.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install datasets[audio]\n```\n\n----------------------------------------\n\nTITLE: Loading data from SQLite database into Hugging Face Dataset\nDESCRIPTION: This code loads data from a SQLite database table named `states` into a Hugging Face Dataset using `Dataset.from_sql`.  It uses a URI string to connect to the database and then loads the entire table into a Dataset.  It also demonstrates filtering the dataset. Requires the `datasets` library.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/tabular_load.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import Dataset\n\n>>> uri = \"sqlite:///us_covid_data.db\"\n>>> ds = Dataset.from_sql(\"states\", uri)\n>>> ds\nDataset({\n    features: ['index', 'date', 'state', 'fips', 'cases', 'deaths'],\n    num_rows: 54382\n})\n>>> ds.filter(lambda x: x[\"state\"] == \"California\")\n```\n\n----------------------------------------\n\nTITLE: Loading Dataset Builder with Datasets\nDESCRIPTION: This code snippet demonstrates how to use the `load_dataset_builder` function to load a dataset builder and inspect its attributes without downloading the dataset. It allows you to view dataset information, such as description and features, before committing to a full download. This is helpful for quickly assessing if a dataset meets your requirements.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/load_hub.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset_builder\n>>> ds_builder = load_dataset_builder(\"cornell-movie-review-data/rotten_tomatoes\")\n\n# Inspect dataset description\n>>> ds_builder.info.description\nMovie Review Dataset. This is a dataset of containing 5,331 positive and 5,331 negative processed sentences from Rotten Tomatoes movie reviews. This data was first used in Bo Pang and Lillian Lee, ``Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.'', Proceedings of the ACL, 2005.\n\n# Inspect dataset features\n>>> ds_builder.info.features\n{'label': ClassLabel(names=['neg', 'pos'], id=None),\n 'text': Value(dtype='string', id=None)}\n```\n\n----------------------------------------\n\nTITLE: Uploading Dataset to Hugging Face Hub\nDESCRIPTION: Loads a dataset and pushes it to the Hugging Face Hub. Requires the `huggingface_hub` library and login.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/image_dataset.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"imagefolder\", data_dir=\"/path/to/folder\", split=\"train\")\n>>> dataset.push_to_hub(\"stevhliu/my-image-captioning-dataset\")\n```\n\n----------------------------------------\n\nTITLE: Loading Dataset with Memory Mapping in Python\nDESCRIPTION: This code snippet demonstrates how to load a dataset using the Hugging Face Datasets library, leveraging Arrow's memory-mapping capabilities. It measures the RAM usage before and after loading the dataset to illustrate the efficiency of memory mapping, which avoids loading the entire dataset into memory.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/about_arrow.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> import os; import psutil; import timeit\n>>> from datasets import load_dataset\n\n# Process.memory_info is expressed in bytes, so convert to megabytes \n>>> mem_before = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)\n>>> wiki = load_dataset(\"wikimedia/wikipedia\", \"20220301.en\", split=\"train\")\n>>> mem_after = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)\n\n>>> print(f\"RAM memory used: {(mem_after - mem_before)} MB\")\n```\n\n----------------------------------------\n\nTITLE: Skipping the First N Examples from IterableDataset (Py)\nDESCRIPTION: This snippet demonstrates how to skip the first `n` examples from an `IterableDataset` using the `skip` function.  It returns a new `IterableDataset` starting from the (n+1)-th example.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/stream.mdx#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> train_dataset = shuffled_dataset.skip(1000)\n```\n\n----------------------------------------\n\nTITLE: Loading Remote JSON Files via HTTP with load_dataset in Python\nDESCRIPTION: Loads a dataset from remote JSON files accessible via HTTP URLs, specifying the URLs in the `data_files` parameter of the `load_dataset` function. The `field` argument is also used to specify the nested field.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/loading.mdx#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\n>>> base_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/\"\n>>> dataset = load_dataset(\"json\", data_files={\"train\": base_url + \"train-v1.1.json\", \"validation\": base_url + \"dev-v1.1.json\"}, field=\"data\")\n```\n\n----------------------------------------\n\nTITLE: Dataset with Image Feature Type\nDESCRIPTION: This example demonstrates how to use the `Image` feature type with JAX. The `vision` extra must be installed to use this feature. It shows how to access the shape and data of the image.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_jax.mdx#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import Dataset, Features, Image\n>>> images = [\"path/to/image.png\"] * 10\n>>> features = Features({\"image\": Image()})\n>>> ds = Dataset.from_dict({\"image\": images}, features=features)\n>>> ds = ds.with_format(\"jax\")\n>>> ds[0][\"image\"].shape\n(512, 512, 3)\n>>> ds[0]\n{'image': DeviceArray([[[ 255, 255, 255],\n              [ 255, 255, 255],\n              ...,\n              [ 255, 255, 255],\n              [ 255, 255, 255]]], dtype=uint8)}\n>>> ds[:2][\"image\"].shape\n(2, 512, 512, 3)\n>>> ds[:2]\n{'image': DeviceArray([[[[ 255, 255, 255],\n              [ 255, 255, 255],\n              ...,\n              [ 255, 255, 255],\n              [ 255, 255, 255]]]], dtype=uint8)}\n```\n\n----------------------------------------\n\nTITLE: Data Extraction with Polars vs Python\nDESCRIPTION: This example compares using a regular Python function versus a Polars function to extract data from a dataset. The Polars function leverages Rust-based operations for faster processing, resulting in a significant speed improvement.\nRequires `datasets` and `polars` to be installed and regular expression library.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_polars.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n\nds = load_dataset(\"ServiceNow-AI/R1-Distill-SFT\", \"v0\", split=\"train\")\n\n# Using a regular python function\npattern = re.compile(\"boxed\\\\{(.*)\\\\}\")\nresult_ds = ds.map(lambda x: {\"value_solution\": m.group(1) if (m:=pattern.search(x[\"solution\"])) else None})\n# Time: 10s\n\n# Using a Polars function\nexpr = pl.col(\"solution\").str.extract(\"boxed\\\\{(.*)\\\\}\").alias(\"value_solution\")\nresult_ds = ds.with_format(\"polars\").map(lambda df: df.with_columns(expr), batched=True)\n# Time: 2s\n```\n\n----------------------------------------\n\nTITLE: Loading the CPPE-5 dataset\nDESCRIPTION: This code snippet loads the 'cppe-5' dataset from the Hugging Face Datasets library and selects the first example from the training split. The example contains image data, bounding box information, and category labels.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/object_detection.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n\n>>> ds = load_dataset(\"cppe-5\")\n>>> example = ds['train'][0]\n>>> example\n{'height': 663,\n 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=943x663 at 0x7FC3DC756250>,\n 'image_id': 15,\n 'objects': {'area': [3796, 1596, 152768, 81002],\n  'bbox': [[302.0, 109.0, 73.0, 52.0],\n   [810.0, 100.0, 57.0, 28.0],\n   [160.0, 31.0, 248.0, 616.0],\n   [741.0, 68.0, 202.0, 401.0]],\n  'category': [4, 4, 0, 0],\n  'id': [114, 115, 116, 117]},\n 'width': 943}\n```\n\n----------------------------------------\n\nTITLE: Loading Image Dataset with Hugging Face Datasets in Python\nDESCRIPTION: Loads the Beans dataset from Hugging Face Datasets library. Requires the `datasets` library. The dataset is loaded with a specified split (e.g., 'train').\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/quickstart.mdx#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset, Image\n\n>>> dataset = load_dataset(\"AI-Lab-Makerere/beans\", split=\"train\")\n```\n\n----------------------------------------\n\nTITLE: Using Image Feature Type with TensorFlow\nDESCRIPTION: This snippet demonstrates how to use the `Image` feature type in datasets when formatted for TensorFlow.  It requires installing the `vision` extra. It creates a dataset with image paths and formats it to TensorFlow.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_tensorflow.mdx#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import Dataset, Features, Image\n>>> images = [\"path/to/image.png\"] * 10\n>>> features = Features({\"image\": Image()})\n>>> ds = Dataset.from_dict({\"image\": images}, features=features)\n>>> ds = ds.with_format(\"tf\")\n>>> ds[0]\n{'image': <tf.Tensor: shape=(512, 512, 4), dtype=uint8, numpy=\n array([[[255, 215, 106, 255],\n         [255, 215, 106, 255],\n         ...,\n         [255, 255, 255, 255],\n         [255, 255, 255, 255]]], dtype=uint8)>}\n>>> ds[:2]\n{'image': <tf.Tensor: shape=(2, 512, 512, 4), dtype=uint8, numpy=\n array([[[[255, 215, 106, 255],\n          [255, 215, 106, 255],\n          ...,\n          [255, 255, 255, 255],\n          [255, 255, 255, 255]]]], dtype=uint8)>}\n```\n\n----------------------------------------\n\nTITLE: Loading MNIST Dataset with JAX Formatting\nDESCRIPTION: This example demonstrates loading the MNIST dataset from the Hugging Face Hub and formatting it to JAX.  The formatted dataset is then accessed to display the `image` and `label` fields as `DeviceArray` objects.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_jax.mdx#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"mnist\")\n>>> ds = ds.with_format(\"jax\")\n>>> ds[\"train\"][0]\n{'image': DeviceArray([[  0,   0,   0, ...],\n                       [  0,   0,   0, ...],\n                       ...,\n                       [  0,   0,   0, ...],\n                       [  0,   0,   0, ...]], dtype=uint8),\n 'label': DeviceArray(5, dtype=int32)}\n```\n\n----------------------------------------\n\nTITLE: Defining Multiple Configurations with BuilderConfig in Python\nDESCRIPTION: This snippet shows how to create multiple configurations for a dataset using `datasets.BuilderConfig`.  The `Food101Config` class inherits from `BuilderConfig` and allows defining different subsets of the dataset with specific data and metadata URLs. The constructor takes arguments for the data URL and metadata URLs, enabling the creation of distinct dataset configurations.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/image_dataset.mdx#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nclass Food101Config(datasets.BuilderConfig):\n    \"\"\"Builder Config for Food-101\"\"\"\n \n    def __init__(self, data_url, metadata_urls, **kwargs):\n        \"\"\"BuilderConfig for Food-101.\n        Args:\n          data_url: `string`, url to download the zip file from.\n          metadata_urls: dictionary with keys 'train' and 'validation' containing the archive metadata URLs\n          **kwargs: keyword arguments forwarded to super.\n        \"\"\"\n        super(Food101Config, self).__init__(version=datasets.Version(\"1.0.0\"), **kwargs)\n        self.data_url = data_url\n        self.metadata_urls = metadata_urls\n```\n\n----------------------------------------\n\nTITLE: Handling N-dimensional Arrays with Varying Shape\nDESCRIPTION: This code shows how the `datasets` library handles N-dimensional arrays with varying shapes. When the dataset is formatted as 'torch' (although the description says 'tf', the code uses 'torch'), it outputs a `RaggedTensor` instead of a single tensor.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_tensorflow.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import Dataset\n>>> data = [[[1, 2],[3]],[[4, 5, 6],[7, 8]]]  # varying shape\n>>> ds = Dataset.from_dict({\"data\": data})\n>>> ds = ds.with_format(\"torch\")\n>>> ds[0]\n{'data': <tf.RaggedTensor [[1, 2], [3]]>}\n```\n\n----------------------------------------\n\nTITLE: Yielding Audio Files with Metadata\nDESCRIPTION: This code snippet shows how to iterate through audio files, associate them with metadata, and yield the combined result. It retrieves the full path to the audio file, reads the audio data, and combines it with the corresponding metadata to create a dataset example.  It relies on `audio_files`, `metadata`, and `local_extracted_archive` being previously defined.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/audio_dataset.mdx#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nfor path, f in audio_files:\n    if path in metadata:\n        result = dict(metadata[path])\n        # set the audio feature and the path to the extracted file\n        path = os.path.join(local_extracted_archive, path) if local_extracted_archive else path\n        result[\"audio\"] = {\"path\": path, \"bytes\": f.read()}\n        result[\"path\"] = path\n        yield id_, result\n        id_ += 1\n```\n\n----------------------------------------\n\nTITLE: Load Spark DataFrame with Feature Types\nDESCRIPTION: This snippet demonstrates how to load a Spark DataFrame into a Hugging Face Dataset object while specifying the feature types using the `features` argument in `Dataset.from_spark`. This is useful for datasets containing images, audio data, or N-dimensional arrays. It includes an example of specifying the `Image` feature type.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_spark.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import Dataset, Features, Image, Value\n>>> data = [(0, open(\"image.png\", \"rb\").read())]\n>>> df = spark.createDataFrame(data, \"idx: int, image: binary\")\n>>> # Also works if you have arrays\n>>> # data = [(0, np.zeros(shape=(32, 32, 3), dtype=np.int32).tolist())]\n>>> # df = spark.createDataFrame(data, \"idx: int, image: array<array<array<int>>>\")\n>>> features = Features({\"idx\": Value(\"int64\"), \"image\": Image()})\n>>> dataset = Dataset.from_spark(df, features=features)\n>>> dataset[0]\n{'idx': 0, 'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=32x32>}\n```\n\n----------------------------------------\n\nTITLE: Loading ImageFolder Datasets with Hugging Face Datasets\nDESCRIPTION: This code snippet demonstrates how to load a dataset structured as an ImageFolder using `load_dataset`. It can load datasets from a Hugging Face repository or from a local directory, requiring the `datasets` library.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/image_load.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"username/dataset_name\")\n>>> # OR locally:\n>>> dataset = load_dataset(\"/path/to/folder\")\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset = load_dataset(\"imagefolder\", data_dir=\"/path/to/folder\")\n```\n\n----------------------------------------\n\nTITLE: Compute Embeddings with DPR and Datasets (FAISS)\nDESCRIPTION: Loads a dataset and computes vector representations (embeddings) for each example using the DPR context encoder. It then adds the embeddings as a new column to the dataset.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/faiss_es.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n>>> ds = load_dataset('crime_and_punish', split='train[:100]')\n>>> ds_with_embeddings = ds.map(lambda example: {'embeddings': ctx_encoder(**ctx_tokenizer(example[\"line\"], return_tensors=\"pt\"))[0][0].numpy()})\n```\n\n----------------------------------------\n\nTITLE: Shuffling IterableDataset with Seed and Buffer\nDESCRIPTION: This snippet shuffles an IterableDataset using a specified seed and buffer size.  The seed ensures reproducibility while the buffer_size determines how many examples are loaded into the buffer for shuffling. This is crucial for introducing randomness into the training process when using streaming datasets.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/stream.mdx#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n>>> seed, buffer_size = 42, 10_000\n>>> dataset = dataset.shuffle(seed, buffer_size=buffer_size)\n```\n\n----------------------------------------\n\nTITLE: Loading Processor for Fine-Tuned Speech Recognition\nDESCRIPTION: This code snippet demonstrates how to load a pre-trained processor for fine-tuned speech recognition models using the Transformers library. It uses `AutoProcessor.from_pretrained` to load the processor. It requires the `transformers` library.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/audio_process.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> from transformers import AutoProcessor\n\n>>> processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n```\n\n----------------------------------------\n\nTITLE: Indexing Performance Comparison\nDESCRIPTION: This snippet compares the execution time of accessing a specific value in the dataset using two different indexing methods: `dataset[0][\"text\"]` and `dataset[\"text\"][0]`. It demonstrates that indexing by row first is faster for this particular dataset.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/access.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> import time\n\n>>> start_time = time.time()\n>>> text = dataset[0][\"text\"]\n>>> end_time = time.time()\n>>> print(f\"Elapsed time: {end_time - start_time:.4f} seconds\")\nElapsed time: 0.0031 seconds\n\n>>> start_time = time.time()\n>>> text = dataset[\"text\"][0]\n>>> end_time = time.time()\n>>> print(f\"Elapsed time: {end_time - start_time:.4f} seconds\")\nElapsed time: 0.0094 seconds\n```\n\n----------------------------------------\n\nTITLE: Loading ImageFolder without Metadata with Hugging Face Datasets\nDESCRIPTION: This code snippet demonstrates how to load an ImageFolder dataset and ignore a metadata file by setting `drop_metadata=True` when calling `load_dataset`. It requires the `datasets` library.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/image_load.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"username/dataset_with_metadata\", drop_metadata=True)\n```\n\n----------------------------------------\n\nTITLE: Loading dataset with custom cache directory in Python\nDESCRIPTION: This Python code snippet demonstrates how to load a dataset using the `load_dataset` function from the `datasets` library, specifying a custom cache directory using the `cache_dir` parameter. This overrides the default cache location.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/cache.mdx#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n>>> from datasets import load_dataset\n>>> dataset = load_dataset('username/dataset', cache_dir=\"/path/to/another/directory/datasets\")\n```\n\n----------------------------------------\n\nTITLE: Sharded Dataset Example in Python\nDESCRIPTION: This Python code demonstrates creating a sharded dataset for parallel processing in ðŸ¤— Datasets. It defines a custom DatasetBuilder (MyShardedDataset) that downloads data shards, specifies the filepaths as `gen_kwargs` for the SplitGenerator, and then processes those shards in parallel within the _generate_examples function.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/dataset_script.mdx#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nclass MyShardedDataset(datasets.GeneratorBasedBuilder):\n\n    def _split_generators(self, dl_manager: datasets.DownloadManager) -> List[datasets.SplitGenerator]:\n        downloaded_files = dl_manager.download([f\"data/shard_{i}.jsonl\" for i in range(1024)])\n        return [\n            datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={\"filepaths\": downloaded_files}),\n        ]\n\n    def _generate_examples(self, filepaths):\n        # Each worker can be given a slice of the original `filepaths` list defined in the `gen_kwargs`\n        # so that this code can run in parallel on several shards at the same time\n        for filepath in filepaths:\n            ...\n```\n\n----------------------------------------\n\nTITLE: Simple Dataset Repository Structure\nDESCRIPTION: This snippet shows the most basic structure for a dataset repository, including a README file for the dataset card, and train/test CSV files.  The data files will automatically be loaded as 'train' and 'test' splits respectively.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/repository_structure.mdx#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nmy_dataset_repository/\nâ”œâ”€â”€ README.md\nâ”œâ”€â”€ train.csv\nâ””â”€â”€ test.csv\n```\n\n----------------------------------------\n\nTITLE: Loading a Local Dataset in Streaming Mode (Py)\nDESCRIPTION: This snippet shows how to load a local dataset consisting of compressed JSONL files in streaming mode.  It uses the `load_dataset` function with the `json` builder and specifies the `data_files` argument to point to the local files. Setting `streaming=True` ensures data is streamed from local files as it's iterated over.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/stream.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n>>> data_files = {'train': 'path/to/OSCAR-2201/compressed/en_meta/*.jsonl.gz'}\n>>> dataset = load_dataset('json', data_files=data_files, split='train', streaming=True)\n>>> print(next(iter(dataset)))\n{'id': 0, 'text': 'Founded in 2015, Golden Bees is a leading programmatic recruitment platform dedicated to employers, HR agencies and job boards. The company has developed unique HR-custom technologies and predictive algorithms to identify and attract the best candidates for a job opportunity.', ...\n```\n\n----------------------------------------\n\nTITLE: Merging Image and Depth Map\nDESCRIPTION: This snippet defines a function to merge an input image and its corresponding colored depth map horizontally for visualization purposes. It converts the depth map to a colored representation using `colored_depthmap` and then concatenates it with the original image.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/depth_estimation.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> def merge_into_row(input_image, depth_target):\n...     if not isinstance(input_image, np.ndarray):\n...         input_image = np.array(input_image)\n...\n...     d_min = np.min(depth_target)\n...     d_max = np.max(depth_target)\n...     depth_target_col = colored_depthmap(depth_target, d_min, d_max)\n...     img_merge = np.hstack([input_image, depth_target_col])\n...\n...     return img_merge\n\n>>> random_indices = np.random.choice(len(train_dataset), 9).tolist()\n>>> plt.figure(figsize=(15, 6))\n>>> for i, idx in enumerate(random_indices):\n...     example = train_dataset[idx]\n...     ax = plt.subplot(3, 3, i + 1)\n...     image_viz = merge_into_row(\n...         example[\"image\"], example[\"depth_map\"]\n...     )\n...     plt.imshow(image_viz.astype(\"uint8\"))\n...     plt.axis(\"off\")\n```\n\n----------------------------------------\n\nTITLE: CSV metadata example\nDESCRIPTION: Example of a CSV metadata file for video captioning datasets. Each row represents a video file and its corresponding text caption, linked by the 'file_name' column.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/video_dataset.mdx#_snippet_5\n\nLANGUAGE: csv\nCODE:\n```\nfile_name,text\n0001.mp4,This is a golden retriever playing with a ball\n0002.mp4,A german shepherd\n0003.mp4,One chihuahua\n```\n\n----------------------------------------\n\nTITLE: Casting Audio Column\nDESCRIPTION: Casts the `audio` column of the dataset to a specified sampling rate (16000 Hz) using the `cast_column` function and `Audio` feature. This is necessary to match the sampling rate the Wav2Vec2 model was trained on.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/quickstart.mdx#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n>>> dataset[0][\"audio\"]\n{'array': array([ 2.3443763e-05,  2.1729663e-04,  2.2145823e-04, ...,\\n         3.8356509e-05, -7.3497440e-06, -2.1754686e-05], dtype=float32),\n 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav',\n 'sampling_rate': 16000}\n```\n\n----------------------------------------\n\nTITLE: Creating Torchvision Transformations\nDESCRIPTION: Creates image transformations using Torchvision. These transformations include resizing the image and adjusting color properties such as brightness, contrast, saturation, and hue.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/semantic_segmentation.mdx#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n>>> from torchvision.transforms import Resize, ColorJitter, Compose\n\n>>> transformation_chain = Compose([\n...     Resize((256, 256)),\n...     ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1)\n... ])\n>>> resize = Resize((256, 256))\n```\n\n----------------------------------------\n\nTITLE: Setting the transform for the dataset\nDESCRIPTION: This code applies the `transforms` function as the transformation for the 'train' split of the dataset using `set_transform`. This applies the transformations on-the-fly when accessing elements of the dataset.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/object_detection.mdx#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> ds['train'].set_transform(transforms)\n```\n\n----------------------------------------\n\nTITLE: Formatting DatasetDict as NumPy\nDESCRIPTION: This snippet illustrates how to format a DatasetDict object to 'numpy'. Setting the format of a DatasetDict to 'numpy' will format all the Dataset objects within the DatasetDict as 'numpy'.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_numpy.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import DatasetDict\n>>> data = {\"train\": {\"data\": [[1, 2], [3, 4]]}, \"test\": {\"data\": [[5, 6], [7, 8]]}}\n>>> dds = DatasetDict.from_dict(data)\n>>> dds = dds.with_format(\"numpy\")\n>>> dds[\"train\"][:2]\n{'data': array([\n    [1, 2],\n    [3, 4]])}\n```\n\n----------------------------------------\n\nTITLE: Handling N-dimensional Arrays with Fixed Shape\nDESCRIPTION: This example illustrates how the `datasets` library handles N-dimensional arrays with a fixed shape when formatted as TensorFlow tensors.  It creates a dataset with 3D data and formats it to TensorFlow, resulting in a single tensor.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_tensorflow.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import Dataset\n>>> data = [[[1, 2],[3, 4]],[[5, 6],[7, 8]]]  # fixed shape\n>>> ds = Dataset.from_dict({\"data\": data})\n>>> ds = ds.with_format(\"tf\")\n>>> ds[0]\n{'data': <tf.Tensor: shape=(2, 2), dtype=int64, numpy=\n array([[1, 2],\n        [3, 4]])>}\n```\n\n----------------------------------------\n\nTITLE: Installing Transformers library\nDESCRIPTION: This command installs the `transformers` library, which is used for working with pretrained models and tokenizers.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_dataset.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install transformers\n```\n\n----------------------------------------\n\nTITLE: Streaming Local Files into an IterableDataset in Python\nDESCRIPTION: Demonstrates how to load local data files into an IterableDataset using streaming, avoiding the need for conversion and saving disk space.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndata_files = {\"train\": [\"path/to/data.csv\"]}\nmy_iterable_dataset = load_dataset(\"csv\", data_files=data_files, split=\"train\", streaming=True)\nfor example in my_iterable_dataset:  # this reads the CSV file progressively as you iterate over the dataset\n    print(example)\n    break\n```\n\n----------------------------------------\n\nTITLE: Casting Audio Column to a New Sampling Rate\nDESCRIPTION: This snippet upsamples the audio signal to match the sampling rate of the pretrained model.  It uses the `cast_column` function with the `Audio` feature, setting the desired `sampling_rate`. The updated audio now has the specified sampling rate.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_dataset.mdx#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16_000))\n>>> dataset[0][\"audio\"]\n{'array': array([ 2.3443763e-05,  2.1729663e-04,  2.2145823e-04, ...,\n         3.8356509e-05, -7.3497440e-06, -2.1754686e-05], dtype=float32),\n 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav',\n 'sampling_rate': 16000}\n```\n\n----------------------------------------\n\nTITLE: Activating/Deactivating Virtual Environment\nDESCRIPTION: Activates and deactivates the created virtual environment.  Activating the environment makes the project's Python interpreter and installed packages available for use. Deactivating reverts to the system's default Python environment.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/installation.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Activate the virtual environment\nsource .env/bin/activate\n\n# Deactivate the virtual environment\nsource .env/bin/deactivate\n```\n\n----------------------------------------\n\nTITLE: Explicitly Using Array Feature Type\nDESCRIPTION: This snippet showcases how to use the `Array` feature type in datasets to specify the shape and dtype of tensors explicitly, avoiding slow shape comparisons and data copies when dealing with N-dimensional arrays. The shape is set to (2, 2) and the dtype to int32.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_numpy.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import Dataset, Features, Array2D\n>>> data = [[[1, 2],[3, 4]],[[5, 6],[7, 8]]]\n>>> features = Features({\"data\": Array2D(shape=(2, 2), dtype='int32')})\n>>> ds = Dataset.from_dict({\"data\": data}, features=features)\n>>> ds = ds.with_format(\"numpy\")\n>>> ds[0]\n{'data': array([[1, 2],\n        [3, 4]])}\n>>> ds[:2]\n{'data': array([[[1, 2],\n         [3, 4]],\n \n        [[5, 6],\n         [7, 8]]])}\n```\n\n----------------------------------------\n\nTITLE: Iterating Through IterableDataset\nDESCRIPTION: This snippet demonstrates how to iterate through an `IterableDataset` using `next(iter())` and a `for` loop to retrieve examples.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/access.mdx#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> next(iter(iterable_dataset))\n{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=384x512 at 0x7F0681F59B50>,\n 'label': 6}\n\n>>> for example in iterable_dataset:\n...     print(example)\n...     break\n{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=384x512 at 0x7F7479DE82B0>, 'label': 6}\n```\n\n----------------------------------------\n\nTITLE: Query FAISS Index\nDESCRIPTION: Loads the DPR question encoder and tokenizer, encodes a question, and then queries the FAISS index to retrieve the nearest examples from the dataset.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/faiss_es.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n>>> q_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n>>> q_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n\n>>> question = \"Is it serious ?\"\n>>> question_embedding = q_encoder(**q_tokenizer(question, return_tensors=\"pt\"))[0][0].numpy()\n>>> scores, retrieved_examples = ds_with_embeddings.get_nearest_examples('embeddings', question_embedding, k=10)\n>>> retrieved_examples[\"line\"][0]\n'_that_ serious? It is not serious at all. Itâ€™s simply a fantasy to amuse\\r\\n'\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading IterableDataset State (Python)\nDESCRIPTION: This code demonstrates how to save and load the state of an `IterableDataset` using `state_dict` and `load_state_dict`. This allows resuming iteration from a checkpoint, similar to how models and optimizers are handled. The `state_dict` captures the current shard and example index within the shard.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n>>> iterable_dataset = Dataset.from_dict({\"a\": range(6)}).to_iterable_dataset(num_shards=3)\n>>> # save in the middle of training\n>>> state_dict = iterable_dataset.state_dict()\n>>> # and resume later\n>>> iterable_dataset.load_state_dict(state_dict)\n```\n\n----------------------------------------\n\nTITLE: Creating a Colored Depth Map\nDESCRIPTION: This snippet defines functions to create and display colored depth maps. The `colored_depthmap` function normalizes the depth values and maps them to colors using a colormap. The `show_depthmap` function uses `colored_depthmap` and `matplotlib` to display the colored depth map.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/depth_estimation.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> import numpy as np\n>>> import matplotlib.pyplot as plt\n\n>>> cmap = plt.cm.viridis\n\n>>> def colored_depthmap(depth, d_min=None, d_max=None):\n...     if d_min is None:\n...         d_min = np.min(depth)\n...     if d_max is None:\n...         d_max = np.max(depth)\n...     depth_relative = (depth - d_min) / (d_max - d_min)\n...     return 255 * cmap(depth_relative)[:,:,:3]\n\n>>> def show_depthmap(depth_map):\n...    if not isinstance(depth_map, np.ndarray):\n...        depth_map = np.array(depth_map)\n...    if depth_map.ndim == 3:\n...        depth_map = depth_map.squeeze()\n\n...    d_min = np.min(depth_map)\n...    d_max = np.max(depth_map)\n...    depth_map = colored_depthmap(depth_map, d_min, d_max)\n\n...    plt.imshow(depth_map.astype(\"uint8\"))\n...    plt.axis(\"off\")\n...    plt.show()\n\n>>> show_depthmap(example[\"depth_map\"])\n```\n\n----------------------------------------\n\nTITLE: Visualizing transformed image with bounding boxes\nDESCRIPTION: This snippet visualizes the image after applying the defined Albumentations transformations. It converts the transformed image to a PyTorch tensor, converts bounding boxes to xyxy format, and uses `draw_bounding_boxes` to display the transformed image with the updated bounding boxes.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/object_detection.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> image = torch.tensor(out['image']).permute(2, 0, 1)\n>>> boxes_xywh = torch.stack([torch.tensor(x) for x in out['bboxes']])\n>>> boxes_xyxy = box_convert(boxes_xywh, 'xywh', 'xyxy')\n>>> labels = [categories.int2str(x) for x in out['category']]\n>>> to_pil_image(\n...     draw_bounding_boxes(\n...         image,\n...         boxes_xyxy,\n...         colors='red',\n...         labels=labels\n...     )\n... )\n```\n\n----------------------------------------\n\nTITLE: Loading Dataset from Hub\nDESCRIPTION: This snippet demonstrates how to load a dataset from the Hugging Face Hub using the `load_dataset` function from the `datasets` library. It requires the `datasets` library to be installed. The function takes the dataset name as input.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/upload_dataset.mdx#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"stevhliu/demo\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Dataset Builder Class in Python\nDESCRIPTION: This code snippet demonstrates the basic structure for creating a dataset builder class by subclassing `datasets.GeneratorBasedBuilder`.  It defines three essential methods: `_info`, `_split_generators`, and `_generate_examples`, which are placeholders for defining dataset metadata, splitting the dataset, and generating examples respectively.  This is the first step in creating a custom dataset loading script.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/image_dataset.mdx#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nclass Food101(datasets.GeneratorBasedBuilder):\n    \"\"\"Food-101 Images dataset\"\"\"\n\n    def _info(self):\n\n    def _split_generators(self, dl_manager):\n\n    def _generate_examples(self, images, metadata_path):\n```\n\n----------------------------------------\n\nTITLE: Listing Dataset Configuration Names with Datasets\nDESCRIPTION: This code snippet uses the `get_dataset_config_names` function to retrieve a list of available configurations (sub-datasets) for a given dataset. It helps discover the different subsets or variations of the dataset, such as language-specific versions. This relies on the `datasets` library.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/load_hub.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import get_dataset_config_names\n\n>>> configs = get_dataset_config_names(\"PolyAI/minds14\")\n>>> print(configs)\n['cs-CZ', 'de-DE', 'en-AU', 'en-GB', 'en-US', 'es-ES', 'fr-FR', 'it-IT', 'ko-KR', 'nl-NL', 'pl-PL', 'pt-PT', 'ru-RU', 'zh-CN', 'all']\n```\n\n----------------------------------------\n\nTITLE: IterableDataset with StatefulDataLoader\nDESCRIPTION: This snippet demonstrates the usage of IterableDataset with `torchdata`'s `StatefulDataLoader`. It shows how to create a checkpoint using `dataloader.state_dict()` which internally uses `iterable_dataset.state_dict()`, and how to resume from a checkpoint using `dataloader.load_state_dict()`. This ensures that the dataloader resumes from the correct position in the dataset.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/stream.mdx#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n>>> from torchdata.stateful_dataloader import StatefulDataLoader\n>>> iterable_dataset = load_dataset(\"deepmind/code_contests\", streaming=True, split=\"train\")\n>>> dataloader = StatefulDataLoader(iterable_dataset, batch_size=32, num_workers=4)\n>>> # checkpoint\n>>> state_dict = dataloader.state_dict()  # uses iterable_dataset.state_dict() under the hood\n>>> # resume from checkpoint\n>>> dataloader.load_state_dict(state_dict)  # uses iterable_dataset.load_state_dict() under the hood\n```\n\n----------------------------------------\n\nTITLE: Loading a Dataset from Video Paths\nDESCRIPTION: This code snippet shows how to create a dataset from a list of video file paths using `Dataset.from_dict` and `cast_column`. The `cast_column` function is used to cast the column of file paths to `torchvision` video objects. This is useful when you have a list of video paths and want to load them as a dataset.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/video_load.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import Dataset, Video\n\n>>> dataset = Dataset.from_dict({\"video\": [\"path/to/video_1\", \"path/to/video_2\", ..., \"path/to/video_n\"]}).cast_column(\"video\", Video())\n>>> dataset[0][\"video\"]\n<torchvision.io.video_reader.VideoReader at 0x1657d0280>\n```\n\n----------------------------------------\n\nTITLE: Installing Datasets with conda\nDESCRIPTION: This code snippet shows how to install the Hugging Face Datasets library using conda. It uses the huggingface and conda-forge channels.\nSOURCE: https://github.com/huggingface/datasets/blob/main/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nconda install -c huggingface -c conda-forge datasets\n```\n\n----------------------------------------\n\nTITLE: Loading and Processing a Text Dataset in Python\nDESCRIPTION: This code snippet demonstrates how to load a text dataset using the `load_dataset` function from the `datasets` library and perform basic data processing steps such as adding a column and tokenizing the text.\nSOURCE: https://github.com/huggingface/datasets/blob/main/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n\n# Print all the available datasets\nfrom huggingface_hub import list_datasets\nprint([dataset.id for dataset in list_datasets()])\n\n# Load a dataset and print the first example in the training set\nsquad_dataset = load_dataset('rajpurkar/squad')\nprint(squad_dataset['train'][0])\n\n# Process the dataset - add a column with the length of the context texts\ndataset_with_length = squad_dataset.map(lambda x: {\"length\": len(x[\"context\"])})\n\n# Process the dataset - tokenize the context texts (using a tokenizer from the ðŸ¤— Transformers library)\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n\ntokenized_dataset = squad_dataset.map(lambda x: tokenizer(x['context']), batched=True)\n```\n\n----------------------------------------\n\nTITLE: Interleaving Datasets (Py)\nDESCRIPTION: This snippet demonstrates how to combine multiple datasets into a single dataset using the `interleave_datasets` function.  It shows how to interleave two datasets and access the interleaved data. It also demonstrates how to use probabilities to control the sampling rate from each dataset.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/stream.mdx#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import interleave_datasets\n>>> es_dataset = load_dataset('allenai/c4', 'es', split='train', streaming=True)\n>>> fr_dataset = load_dataset('allenai/c4', 'fr', split='train', streaming=True)\n\n>>> multilingual_dataset = interleave_datasets([es_dataset, fr_dataset])\n>>> list(multilingual_dataset.take(2))\n[{'text': 'Comprar Zapatillas para niÃ±a en chancla con goma por...'}, {'text': 'Le sacre de philippe ier, 23 mai 1059 - Compte Rendu...'}]\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> multilingual_dataset_with_oversampling = interleave_datasets([es_dataset, fr_dataset], probabilities=[0.8, 0.2], seed=42)\n>>> list(multilingual_dataset_with_oversampling.take(2))\n[{'text': 'Comprar Zapatillas para niÃ±a en chancla con goma por...'}, {'text': 'Chevrolet Cavalier Usados en Bogota - Carros en Vent...'}]\n```\n\n----------------------------------------\n\nTITLE: Creating a Map-Style Dataset from a Dictionary in Python\nDESCRIPTION: Illustrates how to create a Dataset from a dictionary using the from_dict method, enabling random row access after the data is converted to Arrow format.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmy_dataset = Dataset.from_dict({\"col_1\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]})\nprint(my_dataset[0])\n```\n\n----------------------------------------\n\nTITLE: Iterating Over Memory-Mapped Dataset in Python\nDESCRIPTION: This snippet showcases iterating over a memory-mapped dataset loaded with Hugging Face Datasets and Apache Arrow.  It measures the time it takes to iterate over the entire dataset in batches and calculates the iteration speed in gigabits per second (Gb/s). The `timeit` module is used for accurate performance measurement. The statement `s` is defined for timeit.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/about_arrow.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> s = \"\"\"batch_size = 1000\n... for batch in wiki.iter(batch_size):\n...     ...\n... \"\"\"\n\n>>> elapsed_time = timeit.timeit(stmt=s, number=1, globals=globals())\n>>> print(f\"Time to iterate over the {wiki.dataset_size >> 30} GB dataset: {elapsed_time:.1f} sec, \"\n...       f\"ie. {float(wiki.dataset_size >> 27)/elapsed_time:.1f} Gb/s\")\n```\n\n----------------------------------------\n\nTITLE: Visualizing Augmented Image\nDESCRIPTION: This snippet retrieves an example from the transformed dataset and displays the augmented image using Matplotlib. The augmented image is accessed through the 'pixel_values' key.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/depth_estimation.mdx#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n>>> example = train_dataset[index]\n\n>>> plt.imshow(example[\"pixel_values\"])\n>>> plt.axis(\"off\")\n>>> plt.show()\n```\n\n----------------------------------------\n\nTITLE: Setting Dataset Format to Torch\nDESCRIPTION: This code shows how to set the format of a dataset to PyTorch tensors using the `with_format` function.  It applies the formatting on-the-fly.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/process.mdx#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset = dataset.with_format(type=\"torch\")\n```\n\n----------------------------------------\n\nTITLE: Testing Dataset Loading Script using CLI\nDESCRIPTION: This command line instruction utilizes the `datasets-cli test` command to test a dataset loading script. The `--save_info` flag ensures that dataset metadata is saved, and the `--all_configs` flag tests all configurations defined in the script.  This allows verification of the script's functionality and correctness.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/image_dataset.mdx#_snippet_14\n\nLANGUAGE: Bash\nCODE:\n```\ndatasets-cli test path/to/<your-dataset-loading-script> --save_info --all_configs\n```\n\n----------------------------------------\n\nTITLE: Defining Albumentations Transforms\nDESCRIPTION: This snippet defines a set of data augmentation transformations using the Albumentations library. The transformations include horizontal flipping, random cropping, random brightness and contrast adjustment, random gamma correction, and hue saturation value adjustment.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/depth_estimation.mdx#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> import albumentations as A\n\n>>> crop_size = (448, 576)\n>>> transforms = [\n...     A.HorizontalFlip(p=0.5),\n...     A.RandomCrop(crop_size[0], crop_size[1]),\n...     A.RandomBrightnessContrast(),\n...     A.RandomGamma(),\n...     A.HueSaturationValue()\n... ]\n```\n\n----------------------------------------\n\nTITLE: Example JSON Data Instance\nDESCRIPTION: This JSON snippet shows a template for how to provide an example of a typical data instance within your dataset. It includes an 'example_field' which should be replaced with actual fields and values relevant to your dataset.\nSOURCE: https://github.com/huggingface/datasets/blob/main/templates/README_guide.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  'example_field': ...,\n  ...\n}\n```\n\n----------------------------------------\n\nTITLE: Loading AudioFolder Dataset with Metadata and Python\nDESCRIPTION: This snippet showcases how to load an AudioFolder dataset while ignoring metadata by setting `drop_metadata=True` in the `load_dataset` function. It is necessary to import `load_dataset` from `datasets`.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/audio_load.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"username/dataset_with_metadata\", drop_metadata=True)\n```\n\n----------------------------------------\n\nTITLE: Loading Feature Extractor and Tokenizer for Speech Recognition\nDESCRIPTION: This snippet shows how to load a feature extractor and tokenizer for pretrained speech recognition models using the Transformers library. It demonstrates the instantiation of tokenizer object and loading feature extractor and processor using `AutoTokenizer`, `AutoFeatureExtractor`, and `AutoProcessor`. It requires the `transformers` library.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/audio_process.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> from transformers import AutoTokenizer, AutoFeatureExtractor, AutoProcessor\n\n>>> model_checkpoint = \"facebook/wav2vec2-large-xlsr-53\"\n# after defining a vocab.json file you can instantiate a tokenizer object:\n>>> tokenizer = AutoTokenizer(\"./vocab.json\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")\n>>> feature_extractor = AutoFeatureExtractor.from_pretrained(model_checkpoint)\n>>> processor = AutoProcessor.from_pretrained(feature_extractor=feature_extractor, tokenizer=tokenizer)\n```\n\n----------------------------------------\n\nTITLE: Pushing Changes to Fork\nDESCRIPTION: This snippet shows how to push the changes to your forked repository on GitHub. The `git push -u origin a-descriptive-name-for-my-changes` command pushes the current branch to the `origin` remote (your fork) and sets up tracking for future pushes.\nSOURCE: https://github.com/huggingface/datasets/blob/main/CONTRIBUTING.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ngit push -u origin a-descriptive-name-for-my-changes\n```\n\n----------------------------------------\n\nTITLE: Loading Dataset (Returns DatasetDict Object)\nDESCRIPTION: This code loads a dataset without specifying a split, resulting in a `DatasetDict` object. The `DatasetDict` contains all available splits of the dataset (train, validation, test in this example). It utilizes the `load_dataset` function from the `datasets` library.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/load_hub.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\")\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 8530\n    })\n    validation: Dataset({\n        features: ['text', 'label'],\n        num_rows: 1066\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 1066\n    })\n})\n```\n\n----------------------------------------\n\nTITLE: Setting Dataset Format to TensorFlow\nDESCRIPTION: This code snippet sets the dataset format to TensorFlow. It imports `DataCollatorWithPadding` from the `transformers` library.  The `to_tf_dataset` function converts the dataset to a TensorFlow dataset, using the specified columns, labels, batch size, data collator, and shuffle option.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_dataset.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> from transformers import DataCollatorWithPadding\n\n>>> data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n>>> tf_dataset = dataset.to_tf_dataset(\n...     columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\"],\n...     label_cols=[\"label\"],\n...     batch_size=2,\n...     collate_fn=data_collator,\n...     shuffle=True\n... )\n```\n\n----------------------------------------\n\nTITLE: Processing Data with Pandas Functions using Dataset.map\nDESCRIPTION: This snippet shows how to process data in a Hugging Face Dataset using Pandas functions within `Dataset.map`. It uses `batched=True` for faster processing and assigns a new column `col_2` by adding 1 to `col_1`.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_pandas.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import Dataset\n>>> data = {\"col_0\": [\"a\", \"b\", \"c\", \"d\"], \"col_1\": [0., 0., 1., 1.]}\n>>> ds = Dataset.from_dict(data)\n>>> ds = ds.with_format(\"pandas\")\n>>> ds = ds.map(lambda df: df.assign(col_2=df.col_1 + 1), batched=True)\n>>> ds[:2]\n  col_0  col_1  col_2\n0     a    0.0    1.0\n1     b    0.0    1.0\n```\n\n----------------------------------------\n\nTITLE: Loading MNIST Dataset and Formatting as NumPy\nDESCRIPTION: This snippet demonstrates how to load the MNIST dataset from the Hugging Face Hub and format it as NumPy. It uses `load_dataset` to load the MNIST dataset and then sets the format to 'numpy' so the images and labels are returned as NumPy arrays.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_numpy.mdx#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"mnist\")\n>>> ds = ds.with_format(\"numpy\")\n>>> ds[\"train\"][0]\n{'image': array([[  0,   0,   0, ...],\n                       [  0,   0,   0, ...],\n                       ...,\n                       [  0,   0,   0, ...],\n                       [  0,   0,   0, ...]], dtype=uint8),\n 'label': array(5)}\n```\n\n----------------------------------------\n\nTITLE: Taking a Subset of IterableDataset\nDESCRIPTION: This snippet demonstrates how to extract a subset of an `IterableDataset` with a specific number of examples using the `take()` method.  It returns a new `IterableDataset` object with the specified number of elements.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/access.mdx#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Get first three examples\n>>> list(iterable_dataset.take(3))\n[{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=384x512 at 0x7F7479DEE9D0>,\n  'label': 6},\n {'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x512 at 0x7F7479DE8190>,\n  'label': 6},\n {'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x383 at 0x7F7479DE8310>,\n  'label': 6}]\n```\n\n----------------------------------------\n\nTITLE: Visualizing Segmentation Mask Overlay\nDESCRIPTION: Defines a function to visualize the segmentation mask overlaid on the original image. It imports matplotlib and numpy to display the image with the segmentation map. The function takes the image and segmentation mask as inputs, applies a color palette to the mask, and overlays it on the image.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/semantic_segmentation.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> import matplotlib.pyplot as plt\n\n>>> def visualize_seg_mask(image: np.ndarray, mask: np.ndarray):\n...    color_seg = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)\n...    palette = np.array(create_ade20k_label_colormap())\n...    for label, color in enumerate(palette):\n...        color_seg[mask == label, :] = color\n...    color_seg = color_seg[..., ::-1]  # convert to BGR\n\n...    img = np.array(image) * 0.5 + color_seg * 0.5  # plot the image with the segmentation map\n...    img = img.astype(np.uint8)\n\n...    plt.figure(figsize=(15, 10))\n...    plt.imshow(img)\n...    plt.axis(\"off\")\n...    plt.show()\n\n\n>>> visualize_seg_mask(\n...     np.array(dataset[index][\"image\"]),\n...     np.array(dataset[index][\"annotation\"])\n... )\n```\n\n----------------------------------------\n\nTITLE: Explicitly Defining Array Feature Type and Shape\nDESCRIPTION: This snippet shows how to use the `Array` feature type to explicitly define the shape of tensors in the dataset. This avoids slow shape comparisons and data copies. It requires importing `Features` and `Array2D`.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_pytorch.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import Dataset, Features, Array2D\n>>> data = [[[1, 2],[3, 4]],[[5, 6],[7, 8]]]\n>>> features = Features({\"data\": Array2D(shape=(2, 2), dtype='int32')})\n>>> ds = Dataset.from_dict({\"data\": data}, features=features)\n>>> ds = ds.with_format(\"torch\")\n>>> ds[0]\n{'data': tensor([[1, 2],\n         [3, 4]])}\n>>> ds[:2]\n{'data': tensor([[[1, 2],\n          [3, 4]],\n \n         [[5, 6],\n          [7, 8]]])}\n```\n\n----------------------------------------\n\nTITLE: Flattening Nested Columns in Datasets (Datasets Library, Python)\nDESCRIPTION: This snippet demonstrates how to flatten nested columns in a Hugging Face Dataset using the `flatten` function.  It loads the SQuAD dataset and flattens the 'answers' field, creating new columns 'answers.text' and 'answers.answer_start'.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/process.mdx#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n>>> dataset = load_dataset(\"rajpurkar/squad\", split=\"train\")\n>>> dataset.features\n{'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None),\n'context': Value(dtype='string', id=None),\n'id': Value(dtype='string', id=None),\n'question': Value(dtype='string', id=None),\n'title': Value(dtype='string', id=None)}\n\n>>> flat_dataset = dataset.flatten()\n>>> flat_dataset\nDataset({\n    features: ['id', 'title', 'context', 'question', 'answers.text', 'answers.answer_start'],\n num_rows: 87599\n})\n```\n\n----------------------------------------\n\nTITLE: Iterating and Stacking Video Frames\nDESCRIPTION: This code demonstrates how to efficiently obtain multiple frames from a video by iterating over the `VideoReader`. It then stacks the frames into a PyTorch tensor for further processing. The `itertools.islice` function is used to limit the number of frames extracted.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/video_load.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> import torch\n>>> import itertools\n>>> frames = torch.stack([frame[\"data\"] for frame in itertools.islice(video, 5)])\n>>> frames.shape\n(5, 3, 240, 320)\n```\n\n----------------------------------------\n\nTITLE: Installing Vision Feature for Datasets\nDESCRIPTION: This command installs the `vision` feature of the `datasets` library. This provides the tools and dependencies needed to work with image datasets, including data augmentation and image processing functionalities.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/quickstart.mdx#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install datasets[vision]\n```\n\n----------------------------------------\n\nTITLE: Loading Dataset from Local Path\nDESCRIPTION: This code snippet demonstrates how to load a dataset from a local path using `load_dataset`.  This assumes that there is a dataset loading script at the specified path. Requires the `datasets` library and a valid dataset loading script.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/audio_dataset.mdx#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\ndataset = load_dataset(\"path/to/my_dataset\")\n```\n\n----------------------------------------\n\nTITLE: Defining Dataset Metadata with Python\nDESCRIPTION: Defines the dataset metadata within the `_info` method using the `datasets.DatasetInfo` class. Includes dataset description, feature specifications including audio sampling rate (16kHz), and metadata such as homepage, license, and citation variables.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/audio_dataset.mdx#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef _info(self):\n    return datasets.DatasetInfo(\n        description=_DESCRIPTION,\n        features=datasets.Features(\n            {\n                \"speaker_id\": datasets.Value(\"string\"),\n                \"path\": datasets.Value(\"string\"),\n                \"audio\": datasets.Audio(sampling_rate=16_000),\n                \"sentence\": datasets.Value(\"string\"),\n            }\n        ),\n        supervised_keys=None,\n        homepage=_HOMEPAGE,\n        license=_LICENSE,\n        citation=_CITATION,\n    )\n```\n\n----------------------------------------\n\nTITLE: Loading Audio Dataset and Accessing Audio Feature in Py\nDESCRIPTION: This snippet demonstrates how to load the 'minds14' audio dataset using the `load_dataset` function and access the 'audio' column. The `Audio` feature automatically decodes and resamples the audio file when accessed. The example shows how to access the 'audio' feature for the first element in the dataset, resulting in a dictionary containing the audio array, the path to the audio file, and the sampling rate.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/about_dataset_features.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset, Audio\n\n>>> dataset = load_dataset(\"PolyAI/minds14\", \"en-US\", split=\"train\")\n>>> dataset[0][\"audio\"]\n{'array': array([ 0.        ,  0.00024414, -0.00024414, ..., -0.00024414,\n         0.        ,  0.        ], dtype=float32),\n 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav',\n 'sampling_rate': 8000}\n```\n\n----------------------------------------\n\nTITLE: Listing Dataset Split Names with Datasets\nDESCRIPTION: This code snippet uses the `get_dataset_split_names` function to retrieve a list of available split names for a specified dataset. It helps determine the different subsets available within a dataset, such as 'train', 'validation', and 'test'. This function is part of the `datasets` library.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/load_hub.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import get_dataset_split_names\n\n>>> get_dataset_split_names(\"cornell-movie-review-data/rotten_tomatoes\")\n['train', 'validation', 'test']\n```\n\n----------------------------------------\n\nTITLE: Handling N-Dimensional Arrays with Varying Shape\nDESCRIPTION: This snippet demonstrates how the `datasets` library handles N-dimensional arrays with varying shapes.  It shows that when shapes are not consistent, the library creates a list of tensors, each representing a different shape.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_pytorch.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import Dataset\n>>> data = [[[1, 2],[3]],[[4, 5, 6],[7, 8]]]  # varying shape\n>>> ds = Dataset.from_dict({\"data\": data})\n>>> ds = ds.with_format(\"torch\")\n>>> ds[0]\n{'data': [tensor([1, 2]), tensor([3])]}\n```\n\n----------------------------------------\n\nTITLE: Loading a WebDataset\nDESCRIPTION: This snippet demonstrates how to load a WebDataset, which is a format suitable for large video datasets.  WebDatasets are typically loaded in streaming mode using `streaming=True`. The `data_dir` parameter specifies the path to the folder containing the TAR archives.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/video_load.mdx#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"webdataset\", data_dir=\"/path/to/folder\", streaming=True)\n```\n\n----------------------------------------\n\nTITLE: Loading WebDataset from directory - Python\nDESCRIPTION: Loads a dataset in WebDataset format, where data is stored in TAR archives. The `data_dir` parameter specifies the directory containing the TAR files. This will create one column per file suffix. Requires the `datasets` library.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/video_dataset.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"webdataset\", data_dir=\"/path/to/folder\", split=\"train\")\n>>> dataset[0][\"json\"]\n{\"bbox\": [[302.0, 109.0, 73.0, 52.0]], \"categories\": [0]}\n```\n\n----------------------------------------\n\nTITLE: Loading NYU Depth V2 Dataset\nDESCRIPTION: This snippet loads the 'train' split of the NYU Depth V2 dataset using the `load_dataset` function from the `datasets` library. It then accesses an example from the dataset at index 17, displaying the structure of the example which contains an image and a depth map.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/depth_estimation.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n\n>>> train_dataset = load_dataset(\"sayakpaul/nyu_depth_v2\", split=\"train\")\n>>> index = 17\n>>> example = train_dataset[index]\n>>> example\n{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=640x480>,\n 'depth_map': <PIL.TiffImagePlugin.TiffImageFile image mode=F size=640x480>}\n```\n\n----------------------------------------\n\nTITLE: Upload Folder to Specific Path in Dataset\nDESCRIPTION: This command uploads the contents of a specific folder to a specific path within the dataset repository. Replace `my-cool-dataset` with the actual repository name, `./path/to/curated/data` with the path to the folder you want to upload, and `/data/train` with the desired destination path in the repository.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/share.mdx#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n>>> huggingface-cli upload my-cool-dataset ./path/to/curated/data /data/train --repo-type dataset\nhttps://huggingface.co/datasetsWauplin/my-cool-dataset/tree/main/data/train\n```\n\n----------------------------------------\n\nTITLE: Access PDF pages and extract content\nDESCRIPTION: This example shows how to access individual pages from a loaded PDF object and extract text, images, and tables using `pdfplumber` functions. It retrieves the first page, extracts text and images, and attempts to extract tables.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/document_load.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> pdf = dataset[0][\"pdf\"]\n>>> first_page = pdf.pages[0]\n>>> first_page\n<Page:1>\n>>> first_page.extract_text()\nDocling Technical Report\nVersion1.0\nChristophAuer MaksymLysak AhmedNassar MicheleDolfi NikolaosLivathinos\nPanosVagenas CesarBerrospiRamis MatteoOmenetti FabianLindlbauer\nKasperDinkla LokeshMishra YusikKim ShubhamGupta RafaelTeixeiradeLima\nValeryWeber LucasMorin IngmarMeijer ViktorKuropiatnyk PeterW.J.Staar\nAI4KGroup,IBMResearch\nRuÂ¨schlikon,Switzerland\nAbstract\nThis technical report introduces Docling, an easy to use, self-contained, MIT-\nlicensed open-source package for PDF document conversion.\n...\n>>> first_page.images\nIn [24]: first_page.images\nOut[24]: \n[{'x0': 256.5,\n  'y0': 621.0,\n  'x1': 355.49519999999995,\n  'y1': 719.9952,\n  'width': 98.99519999999995,\n  'height': 98.99519999999995,\n  'name': 'Im1',\n  'stream': <PDFStream(44): raw=88980, {'Type': /'XObject', 'Subtype': /'Image', 'BitsPerComponent': 8, 'ColorSpace': /'DeviceRGB', 'Filter': /'DCTDecode', 'Height': 1024, 'Length': 88980, 'Width': 1024}>,\n  'srcsize': (1024, 1024),\n  'imagemask': None,\n  'bits': 8,\n  'colorspace': [/'DeviceRGB'],\n  'mcid': None,\n  'tag': None,\n  'object_type': 'image',\n  'page_number': 1,\n  'top': 72.00480000000005,\n  'bottom': 171.0,\n  'doctop': 72.00480000000005}]\n>>> first_page.extract_tables()\n[]\n```\n\n----------------------------------------\n\nTITLE: Setting Dataset Transformation\nDESCRIPTION: Sets the transformation function for the dataset. This allows the transformation to be applied on-the-fly when accessing the dataset.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/semantic_segmentation.mdx#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset.set_transform(transforms)\n```\n\n----------------------------------------\n\nTITLE: Creating an Audio Dataset from Dictionary and Casting the Audio Column\nDESCRIPTION: This code shows how to create an audio dataset from a dictionary containing file paths, and then cast the audio column to the Audio feature using `cast_column`. This prepares the dataset for audio processing.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/create_dataset.mdx#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> audio_dataset = Dataset.from_dict({\"audio\": [\"path/to/audio_1\", ..., \"path/to/audio_n\"]}).cast_column(\"audio\", Audio())\n```\n\n----------------------------------------\n\nTITLE: Arrow-Based Dataset Builder Example in Python\nDESCRIPTION: This Python code snippet shows how to create a dataset builder that yields Arrow tables instead of individual examples for improved performance. The builder inherits from `datasets.ArrowBasedBuilder` and implements the `_generate_tables` method, which processes data and yields Arrow tables.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/dataset_script.mdx#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow as pa\npa_table = pa.Table.from_pandas(df)\n\n```\n\nLANGUAGE: python\nCODE:\n```\nclass MySuperFastDataset(datasets.ArrowBasedBuilder):\n\n    def _generate_tables(self, filepaths):\n        idx = 0\n        for filepath in filepaths:\n            ...\n            yield idx, pa_table\n            idx += 1\n```\n\n----------------------------------------\n\nTITLE: Loading Audio Dataset with Decode=False in Py\nDESCRIPTION: This snippet demonstrates how to load the 'minds14' audio dataset and cast the 'audio' column to the `Audio` feature with `decode=False`. This prevents automatic decoding and resampling of the audio file. Accessing the 'audio' column for the first element in the dataset returns a dictionary containing the path to the audio file, without the audio array or sampling rate.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/about_dataset_features.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset = load_dataset(\"PolyAI/minds14\", \"en-US\", split=\"train\").cast_column(\"audio\", Audio(decode=False))\n>>> dataset[0]\n{'audio': {'bytes': None,\n  'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav'},\n 'english_transcription': 'I would like to set up a joint account with my partner',\n 'intent_class': 11,\n 'lang_id': 4,\n 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav',\n 'transcription': 'I would like to set up a joint account with my partner'}\n```\n\n----------------------------------------\n\nTITLE: Loading AudioFolder Dataset in Python\nDESCRIPTION: This snippet shows how to load an AudioFolder dataset using the `load_dataset` function from the `datasets` library.  It can load from a Hugging Face Hub repository or a local directory. When loading from a local directory, it automatically detects audio files and creates a dataset. No custom dataloader is needed.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/audio_load.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"username/dataset_name\")\n>>> # OR locally:\n>>> dataset = load_dataset(\"/path/to/folder\")\n```\n\n----------------------------------------\n\nTITLE: Run in Quiet Mode\nDESCRIPTION: This command uploads the contents of a specified folder to a dataset repository in quiet mode, suppressing most output except for the final URL. Replace `Wauplin/my-cool-dataset` with the actual repository name and `./data` with the path to the folder you want to upload.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/share.mdx#_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\n>>> huggingface-cli upload Wauplin/my-cool-dataset ./data . --repo-type dataset --quiet\nhttps://huggingface.co/datasets/Wauplin/my-cool-dataset/tree/main\n```\n\n----------------------------------------\n\nTITLE: Loading Combination of Percentages with ReadInstruction Python\nDESCRIPTION: This snippet demonstrates how to load a combination of percentages (first 10% and last 80%) of the 'train' split of a dataset using the `datasets.ReadInstruction` API with `datasets.load_dataset`. It loads the 'ajibawa-2023/General-Stories-Collection' dataset and specifies the split using `ReadInstruction` objects.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/loading.mdx#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\n>>> ri = (datasets.ReadInstruction(\"train\", to=10, unit=\"%\") + datasets.ReadInstruction(\"train\", from_=-80, unit=\"%\"))\n>>> train_10_80pct_ds = datasets.load_dataset(\"ajibawa-2023/General-Stories-Collection\", split=ri)\n```\n\n----------------------------------------\n\nTITLE: Loading Dataset and Tokenizer for Text\nDESCRIPTION: This snippet loads the `rotten_tomatoes` dataset and the tokenizer for the `bert-base-uncased` model. It imports necessary libraries from `transformers` and `datasets`. The tokenizer ensures that the text is split into tokens in the same way as the pretrained model expects.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_dataset.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> from transformers import AutoTokenizer\n>>> from datasets import load_dataset\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n>>> dataset = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\", split=\"train\")\n```\n\n----------------------------------------\n\nTITLE: Loading CSV Dataset with Custom Separator in Datasets\nDESCRIPTION: This code snippet demonstrates how to load a CSV dataset using the `load_dataset` function from the Hugging Face Datasets library, while specifying a custom separator. The `sep` parameter is used to define the [`~datasets.packaged_modules.csv.CsvConfig`] for loading the data. It requires the `datasets` library to be installed.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/package_reference/loading_methods.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nload_dataset(\"csv\", data_dir=\"path/to/data/dir\", sep=\"\\t\")\n```\n\n----------------------------------------\n\nTITLE: Iterating Over NumPy Dataset in Batches\nDESCRIPTION: This snippet shows how to iterate over a NumPy-formatted dataset in batches using the `Dataset.iter()` method.  It assumes the dataset `ds` has already been loaded and formatted as NumPy. It extracts the image and label data for each batch to be used in training.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_numpy.mdx#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n>>> for epoch in range(epochs):\n...     for batch in ds[\"train\"].iter(batch_size=32):\n...         x, y = batch[\"image\"], batch[\"label\"]\n...         ...\n```\n\n----------------------------------------\n\nTITLE: Load Spark DataFrame into IterableDataset\nDESCRIPTION: This snippet demonstrates how to load a Spark DataFrame into a Hugging Face IterableDataset object using `IterableDataset.from_spark`. This method avoids materialization by returning an IterableDataset. The snippet also shows how to iterate through the dataset to access individual elements.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_spark.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import IterableDataset\n>>> df = spark.createDataFrame(\n...     data=[[1, \"Elia\"], [2, \"Teo\"], [3, \"Fang\"]],\n...     columns=[\"id\", \"name\"],\n... )\n>>> ds = IterableDataset.from_spark(df)\n>>> print(next(iter(ds)))\n{\"id\": 1, \"name\": \"Elia\"}\n```\n\n----------------------------------------\n\nTITLE: Loading Dataset Configuration with Datasets\nDESCRIPTION: This code demonstrates how to load a specific configuration (subset) of a dataset using the `load_dataset` function and providing a configuration name. It requires the `datasets` library and the configuration name (e.g., \"fr-FR\" for French). This allows accessing specific variations of the dataset.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/load_hub.mdx#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n\n>>> mindsFR = load_dataset(\"PolyAI/minds14\", \"fr-FR\", split=\"train\")\n```\n\n----------------------------------------\n\nTITLE: Logging in to Hugging Face Account\nDESCRIPTION: This snippet demonstrates how to log in to your Hugging Face account using the huggingface-cli. This is required before you can push datasets to the hub programmatically. You will be prompted for your authentication token.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/upload_dataset.mdx#_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\nhuggingface-cli login\n```\n\n----------------------------------------\n\nTITLE: Applying Torchvision Transformations\nDESCRIPTION: Defines a function to apply the Torchvision transformations to a batch of images and segmentation masks. It transforms the images and masks using the transformation_chain and resize objects respectively, and returns the transformed images and masks.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/semantic_segmentation.mdx#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n>>> def train_transforms(example_batch):\n...     example_batch[\"pixel_values\"] = [transformation_chain(x) for x in example_batch[\"image\"]]\n...     example_batch[\"label\"] = [resize(x) for x in example_batch[\"annotation\"]]\n...     return example_batch\n```\n\n----------------------------------------\n\nTITLE: Load PDF dataset with filters\nDESCRIPTION: This example demonstrates loading a streaming PDF dataset and applying filters based on the label column. It loads only a subset of the dataset where the label is equal to 0. This is useful for processing large datasets selectively.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/document_load.mdx#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n>>> filters = [(\"label\", \"=\", 0)]\n>>> dataset = load_dataset(\"username/dataset_name\", streaming=True, filters=filters)\n```\n\n----------------------------------------\n\nTITLE: Loading Cross-Validated Splits Python\nDESCRIPTION: This snippet demonstrates how to create 10-fold cross-validated splits using string slicing with `datasets.load_dataset`. It loads the 'ajibawa-2023/General-Stories-Collection' dataset and specifies the split with a list comprehension using string notation to create validation and training splits.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/loading.mdx#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n>>> val_ds = datasets.load_dataset(\"ajibawa-2023/General-Stories-Collection\", split=[f\"train[{k}%:{k+10}%]\" for k in range(0, 100, 10)])\n>>> train_ds = datasets.load_dataset(\"ajibawa-2023/General-Stories-Collection\", split=[f\"train[:{k}%]+train[{k+10}%:]\" for k in range(0, 100, 10)])\n```\n\n----------------------------------------\n\nTITLE: Set Logging Verbosity via Environment Variable - Bash\nDESCRIPTION: This code snippet shows how to override the default logging verbosity by setting the `DATASETS_VERBOSITY` environment variable. The example sets the verbosity to 'error', causing only errors to be reported. Other valid values are 'debug', 'info', 'warning', and 'critical'.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/package_reference/utilities.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nDATASETS_VERBOSITY=error ./myprogram.py\n```\n\n----------------------------------------\n\nTITLE: Formatting a Dataset to JAX\nDESCRIPTION: This code snippet demonstrates how to format a `Dataset` object to output `jax.Array` objects instead of standard Python objects using the `with_format('jax')` method.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_jax.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import Dataset\n>>> data = [[1, 2], [3, 4]]\n>>> ds = Dataset.from_dict({\"data\": data})\n>>> ds = ds.with_format(\"jax\")\n>>> ds[0]\n{'data': DeviceArray([1, 2], dtype=int32)}\n>>> ds[:2]\n{'data': DeviceArray([\n    [1, 2],\n    [3, 4]], dtype=int32)}\n```\n\n----------------------------------------\n\nTITLE: Applying Albumentations Transforms\nDESCRIPTION: This snippet defines a function to apply the defined Albumentations transformations to the images and depth maps in a batch of examples. It iterates through the images and depth maps, applies the transformations, and then updates the examples with the transformed data.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/depth_estimation.mdx#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n>>> def apply_transforms(examples):\n...     transformed_images, transformed_maps = [], []\n...     for image, depth_map in zip(examples[\"image\"], examples[\"depth_map\"]):\n...         image, depth_map = np.array(image), np.array(depth_map)\n...         transformed = aug(image=image, depth=depth_map)\n...         transformed_images.append(transformed[\"image\"])\n...         transformed_maps.append(transformed[\"depth\"])\n...\n...     examples[\"pixel_values\"] = transformed_images\n...     examples[\"labels\"] = transformed_maps\n...     return examples\n```\n\n----------------------------------------\n\nTITLE: Apply Transformations to Dataset\nDESCRIPTION: Applies the defined image transformations to the dataset on-the-fly using the `set_transform` function.  This function modifies the dataset so that the transformations are applied whenever data is accessed, rather than pre-processing and storing the entire transformed dataset, which saves disk space.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/image_classification.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset.set_transform(transforms)\n```\n\n----------------------------------------\n\nTITLE: Multithreaded Video Decoding\nDESCRIPTION: This code shows how to enable multithreaded decoding to speed up video loading and processing.  The `decode` method is used to enable multithreading with `num_threads`.  `num_threads=0` can be faster for local data on fast disks.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/video_load.mdx#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n>>> import os\n>>> num_threads = num_threads = min(32, (os.cpu_count() or 1) + 4)\n>>> dataset = dataset.decode(num_threads=num_threads)\n>>> for example in dataset:  # up to 20 times faster !\n...     ...\n```\n\n----------------------------------------\n\nTITLE: Loading a VideoFolder Dataset\nDESCRIPTION: Demonstrates loading a video dataset using the VideoFolder structure. This dataset structure is ideal for vision tasks with thousands of videos. It automatically infers labels from the directory structure. The code shows loading both a local dataset and a dataset from a remote repository.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/video_load.mdx#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"username/dataset_name\")\n>>> # OR locally:\n>>> dataset = load_dataset(\"/path/to/folder\")\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset = load_dataset(\"videofolder\", data_dir=\"/path/to/folder\")\n```\n\n----------------------------------------\n\nTITLE: Converting ClassLabel Data to Tensors\nDESCRIPTION: This snippet demonstrates how `ClassLabel` data is properly converted to PyTorch tensors. It showcases the integration of categorical data into the tensor format using `Features` and `ClassLabel`.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_pytorch.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import Dataset, Features, ClassLabel\n>>> labels = [0, 0, 1]\n>>> features = Features({\"label\": ClassLabel(names=[\"negative\", \"positive\"])})\n>>> ds = Dataset.from_dict({\"label\": labels}, features=features) \n>>> ds = ds.with_format(\"torch\")  \n>>> ds[:3]\n{'label': tensor([0, 0, 1])}\n```\n\n----------------------------------------\n\nTITLE: Upload and Create a PR\nDESCRIPTION: This command uploads files to a specified revision and creates a pull request (PR) for the changes. Replace `bigcode/the-stack` with the actual repository name and `refs/pr/104` with the desired revision.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/share.mdx#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\n# Create a PR and upload the files to it\n>>> huggingface-cli upload bigcode/the-stack --repo-type dataset --revision refs/pr/104 --create-pr . .\nhttps://huggingface.co/datasets/bigcode/the-stack/blob/refs%2Fpr%2F104/\n```\n\n----------------------------------------\n\nTITLE: Handling N-Dimensional Arrays with Varying Shapes\nDESCRIPTION: This snippet demonstrates how datasets handle N-dimensional arrays with varying shapes when formatted as NumPy. It shows that datasets can represent arrays of different shapes, though this may come with performance costs.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_numpy.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import Dataset\n>>> data = [[[1, 2],[3]], [[4, 5, 6],[7, 8]]]  # varying shape\n>>> ds = Dataset.from_dict({\"data\": data})\n>>> ds = ds.with_format(\"numpy\")\n>>> ds[0]\n{'data': array([array([1, 2]), array([3])], dtype=object)}\n```\n\n----------------------------------------\n\nTITLE: Loading Image Datasets from Local Files with Hugging Face Datasets\nDESCRIPTION: This code snippet demonstrates how to load an image dataset from a list of image paths using the `Dataset.from_dict` function and casting the image column to the `Image` feature. This requires the `datasets` library.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/image_load.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import Dataset, Image\n\n>>> dataset = Dataset.from_dict({\"image\": [\"path/to/image_1\", \"path/to/image_2\", ..., \"path/to/image_n\"]}).cast_column(\"image\", Image())\n>>> dataset[0][\"image\"]\n```\n\n----------------------------------------\n\nTITLE: Create a Dataset Repository\nDESCRIPTION: This command creates a new dataset repository on the Hugging Face Hub. The `--type dataset` flag specifies that the repository is for a dataset. Replace `my-cool-dataset` with your desired dataset name.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/share.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhuggingface-cli repo create my-cool-dataset --type dataset\n```\n\n----------------------------------------\n\nTITLE: Uploading dataset to Hugging Face Hub - Python\nDESCRIPTION: Uploads a local dataset to the Hugging Face Hub using the `huggingface_hub` library.  It requires authentication to a Hugging Face account and uses the `upload_folder` function to upload the contents of a local directory to a specified repository. `folder_path` specifies the local path, `repo_id` is the Hugging Face repository ID, and `repo_type` is set to 'dataset'.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/video_dataset.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import HfApi\napi = HfApi()\n\napi.upload_folder(\n    folder_path=\"/path/to/local/dataset\",\n    repo_id=\"username/my-cool-dataset\",\n    repo_type=\"dataset\",\n)\n```\n\n----------------------------------------\n\nTITLE: Dataset with ClassLabel Feature Type\nDESCRIPTION: This snippet demonstrates how `ClassLabel` data is properly converted to arrays when formatting a dataset to JAX.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_jax.mdx#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import Dataset, Features, ClassLabel\n>>> labels = [0, 0, 1]\n>>> features = Features({\"label\": ClassLabel(names=[\"negative\", \"positive\"])})\n>>> ds = Dataset.from_dict({\"label\": labels}, features=features)\n>>> ds = ds.with_format(\"jax\")\n>>> ds[:3]\n{'label': DeviceArray([0, 0, 1], dtype=int32)}\n```\n\n----------------------------------------\n\nTITLE: Loading Remote Arrow Files via HTTP with load_dataset in Python\nDESCRIPTION: Loads a dataset from remote Arrow files accessible via HTTP URLs using the `load_dataset` function. The 'arrow' data loading script must be specified.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/loading.mdx#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\n>>> base_url = \"https://huggingface.co/datasets/croissantllm/croissant_dataset/resolve/main/english_660B_11/\"\n>>> data_files = {\"train\": base_url + \"train/data-00000-of-00080.arrow\"}\n>>> wiki = load_dataset(\"arrow\", data_files=data_files, split=\"train\")\n```\n\n----------------------------------------\n\nTITLE: Using Strings and Binary Objects with TensorFlow\nDESCRIPTION: This snippet demonstrates the support for strings and binary objects in datasets when formatted as TensorFlow tensors. It creates a dataset with text and data columns and formats it to TensorFlow.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_tensorflow.mdx#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import Dataset, Features\n>>> text = [\"foo\", \"bar\"]\n>>> data = [0, 1]\n>>> ds = Dataset.from_dict({\"text\": text, \"data\": data})\n>>> ds = ds.with_format(\"tf\")\n>>> ds[:2]\n{'text': <tf.Tensor: shape=(2,), dtype=string, numpy=array([b'foo', b'bar'], dtype=object)>,\n 'data': <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 1])>}\n```\n\n----------------------------------------\n\nTITLE: Preparing DataLoader for PyTorch\nDESCRIPTION: Prepares the dataset for PyTorch by creating a `DataLoader`. A collate function is defined to stack the images and labels into batches. Requires the `torch` library.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/quickstart.mdx#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n>>> from torch.utils.data import DataLoader\n\n>>> def collate_fn(examples):\n...     images = []\n...     labels = []\n...     for example in examples:\n...         images.append((example[\"pixel_values\"]))\n...         labels.append(example[\"labels\"])\n...         \n...     pixel_values = torch.stack(images)\n...     labels = torch.tensor(labels)\n...     return {\"pixel_values\": pixel_values, \"labels\": labels}\n>>> dataloader = DataLoader(dataset, collate_fn=collate_fn, batch_size=4)\n```\n\n----------------------------------------\n\nTITLE: Loading dataset with force redownload in Python\nDESCRIPTION: This Python snippet demonstrates loading a dataset using `load_dataset` and forcing a redownload of the data using the `download_mode` parameter set to `'force_redownload'`.  This ensures that the latest version of the dataset is used, regardless of what is currently cached.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/cache.mdx#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n>>> from datasets import load_dataset\n>>> dataset = load_dataset('rajpurkar/squad', download_mode='force_redownload')\n```\n\n----------------------------------------\n\nTITLE: Accessing Videos in VideoFolder Dataset\nDESCRIPTION: Demonstrates accessing a video from a loaded VideoFolder dataset. The 'video' column contains `torchvision.io.video_reader.VideoReader` objects, and the 'label' column contains the inferred label from the directory name.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/video_load.mdx#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset[\"train\"][0]\n{\"video\": <torchvision.io.video_reader.VideoReader at 0x161715e50>, \"label\": 0}\n\n>>> dataset[\"train\"][-1]\n{\"video\": <torchvision.io.video_reader.VideoReader at 0x16170bd90>, \"label\": 1}\n```\n\n----------------------------------------\n\nTITLE: Creating a Dataset from a Dictionary with Hugging Face Datasets\nDESCRIPTION: This code snippet illustrates creating a dataset from a Python dictionary using `Dataset.from_dict`. The dictionary's keys become the dataset's columns, and the values become the corresponding data.  Each list should be of the same length.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/create_dataset.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import Dataset\n>>> ds = Dataset.from_dict({\"pokemon\": [\"bulbasaur\", \"squirtle\"], \"type\": [\"grass\", \"water\"]})\n>>> ds[0]\n{\"pokemon\": \"bulbasaur\", \"type\": \"grass\"}\n```\n\n----------------------------------------\n\nTITLE: Setting a default configuration name - Python\nDESCRIPTION: This code demonstrates how to set a default configuration for a dataset. This configuration is automatically loaded if the user does not specify a configuration name when loading the dataset.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/dataset_script.mdx#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nclass NewDataset(datasets.GeneratorBasedBuilder):\n\nVERSION = datasets.Version(\"1.1.0\")\n\nBUILDER_CONFIGS = [\n    datasets.BuilderConfig(name=\"first_domain\", version=VERSION, description=\"This part of my dataset covers a first domain\"),\n    datasets.BuilderConfig(name=\"second_domain\", version=VERSION, description=\"This part of my dataset covers a second domain\"),\n]\n\nDEFAULT_CONFIG_NAME = \"first_domain\"\n```\n\n----------------------------------------\n\nTITLE: Access Image from Dataset in Python\nDESCRIPTION: Indexes into the first row of the dataset to access the image. When accessing the `image` column, the underlying PIL object is automatically decoded into an image object. This allows for direct manipulation of the image data.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_dataset.mdx#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset[0][\"image\"]\n<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x500 at 0x7FE5A047CC70>\n```\n\n----------------------------------------\n\nTITLE: Upload Data at Regular Intervals\nDESCRIPTION: This command uploads data from a specified directory to the dataset repository at regular intervals (every 10 minutes in this example). Replace `my-cool-dynamic-dataset` with the actual repository name and `data/` with the path to the directory containing the data to upload.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/share.mdx#_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\n# Upload new logs every 10 minutes\nhuggingface-cli upload my-cool-dynamic-dataset data/ --every=10\n```\n\n----------------------------------------\n\nTITLE: Converting ClassLabel Data to TensorFlow Tensors\nDESCRIPTION: This example shows how `ClassLabel` data is properly converted to tensors when the dataset is formatted to TensorFlow. It creates a dataset with class labels and formats it to TensorFlow.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_tensorflow.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import Dataset, Features, ClassLabel\n>>> labels = [0, 0, 1]\n>>> features = Features({\"label\": ClassLabel(names=[\"negative\", \"positive\"])})\n>>> ds = Dataset.from_dict({\"label\": labels}, features=features)\n>>> ds = ds.with_format(\"tf\")\n>>> ds[:3]\n{'label': <tf.Tensor: shape=(3,), dtype=int64, numpy=array([0, 0, 1])>}\n```\n\n----------------------------------------\n\nTITLE: Load Slices with Default Percentage Rounding Python\nDESCRIPTION: This snippet shows how to load slices of the dataset with default percentage rounding. Due to rounding, slices may contain different number of records.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/loading.mdx#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\n# 19 records, from 500 (included) to 519 (excluded).\n>>> train_50_52_ds = datasets.load_dataset(\"ajibawa-2023/General-Stories-Collection\", split=\"train[50%:52%]\")\n# 20 records, from 519 (included) to 539 (excluded).\n>>> train_52_54_ds = datasets.load_dataset(\"ajibawa-2023/General-Stories-Collection\", split=\"train[52%:54%]\")\n```\n\n----------------------------------------\n\nTITLE: Mapping Data Files to Splits with load_dataset in Python\nDESCRIPTION: Maps data files to specific splits like train, validation, and test using the `data_files` parameter when loading a dataset from the Hub. The `load_dataset` function will load only specified splits.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/loading.mdx#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n>>> data_files = {\"train\": \"train.csv\", \"test\": \"test.csv\"}\n>>> dataset = load_dataset(\"namespace/your_dataset_name\", data_files=data_files)\n```\n\n----------------------------------------\n\nTITLE: Iterating through an IterableDataset in Pandas Format\nDESCRIPTION: This snippet demonstrates how to iterate through an `IterableDataset` that has been set to the Pandas format. The `ds.iter(batch_size=2)` method yields Pandas DataFrames with a batch size of 2.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_pandas.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> ds = ds.with_format(\"pandas\")\n>>> for df in ds.iter(batch_size=2):\n...     print(df)\n...     break\n  col_0  col_1\n0     a    0.0\n1     b    0.0\n```\n\n----------------------------------------\n\nTITLE: Taking the First N Examples from IterableDataset (Py)\nDESCRIPTION: This snippet demonstrates how to extract the first `n` examples from an `IterableDataset` using the `take` function.  It shows how to create a new dataset containing only the first 2 examples.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/stream.mdx#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset = load_dataset('HuggingFaceFW/fineweb', split='train', streaming=True)\n>>> dataset_head = dataset.take(2)\n>>> list(dataset_head)\n[{'text': \"How AP reported in all formats from tor...}, {'text': 'Did you know you have two little yellow...'}]\n```\n\n----------------------------------------\n\nTITLE: Loading XML Data\nDESCRIPTION: Loads XML data using the \"xml\" loader from the `datasets` library.  Demonstrates loading from both individual files and a directory. The XML loader is equivalent to the \"text\" loader with `sample_by=\"document\"`.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/nlp_load.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n>>> dataset = load_dataset(\"xml\", data_files={\"train\": [\"my_xml_1.xml\", \"my_xml_2.xml\"], \"test\": \"my_xml_file.xml\"})\n\n# Load from a directory\n>>> dataset = load_dataset(\"xml\", data_dir=\"path/to/xml/dataset\")\n```\n\n----------------------------------------\n\nTITLE: Installing huggingface_hub library\nDESCRIPTION: This snippet demonstrates how to install the huggingface_hub library using pip. This is a prerequisite for programmatically interacting with the Hugging Face Hub from Python.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/upload_dataset.mdx#_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\npip install huggingface_hub\n```\n\n----------------------------------------\n\nTITLE: Loading Remote Parquet Files via HTTP with load_dataset in Python\nDESCRIPTION: Loads a dataset from remote Parquet files accessible via HTTP URLs using the `load_dataset` function. The 'parquet' data loading script must be specified.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/loading.mdx#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\n>>> base_url = \"https://huggingface.co/datasets/wikimedia/wikipedia/resolve/main/20231101.ab/\"\n>>> data_files = {\"train\": base_url + \"train-00000-of-00001.parquet\"}\n>>> wiki = load_dataset(\"parquet\", data_files=data_files, split=\"train\")\n```\n\n----------------------------------------\n\nTITLE: Generating Dataset Examples in Python\nDESCRIPTION: This code demonstrates how to generate examples for the dataset splits within the `_generate_examples` method. It reads metadata from the specified `metadata_path`, then iterates through the images. For each image, it extracts the label from the file path and yields a dictionary containing the image path, image bytes, and label.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/image_dataset.mdx#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\ndef _generate_examples(self, images, metadata_path):\n    \"\"\"Generate images and labels for splits.\"\"\"\n    with open(metadata_path, encoding=\"utf-8\") as f:\n        files_to_keep = set(f.read().split(\"\\n\"))\n    for file_path, file_obj in images:\n        if file_path.startswith(_IMAGES_DIR):\n            if file_path[len(_IMAGES_DIR) : -len(\".jpg\")] in files_to_keep:\n                label = file_path.split(\"/\")[2]\n                yield file_path, {\n                    \"image\": {\"path\": file_path, \"bytes\": file_obj.read()},\n                    \"label\": label,\n                }\n```\n\n----------------------------------------\n\nTITLE: Displaying an Image\nDESCRIPTION: This snippet accesses the 'image' field of the example, which is a PIL image object, and displays it.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/depth_estimation.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> example[\"image\"]\n```\n\n----------------------------------------\n\nTITLE: Selecting Rows from a Dataset in Python\nDESCRIPTION: This code snippet demonstrates how to select specific rows from a dataset using the `select` method. It selects rows at indices 0, 10, 20, 30, 40, and 50 and prints the length of the resulting dataset.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/process.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> small_dataset = dataset.select([0, 10, 20, 30, 40, 50])\n>>> len(small_dataset)\n6\n```\n\n----------------------------------------\n\nTITLE: Batch Mapping Example with Column Removal in Datasets\nDESCRIPTION: This code snippet demonstrates how to correctly use batch mapping to duplicate rows in a dataset by removing the original column ('a'). This resolves the error from the previous example, ensuring that the output dataset has a consistent number of rows for all columns.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/about_map_batch.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import Dataset\n>>> dataset = Dataset.from_dict({\"a\": [0, 1, 2]})\n>>> dataset_with_duplicates = dataset.map(lambda batch: {\"b\": batch[\"a\"] * 2}, remove_columns=[\"a\"], batched=True)\n>>> len(dataset_with_duplicates)\n6\n```\n\n----------------------------------------\n\nTITLE: Loading VideoFolder Dataset with Metadata Options\nDESCRIPTION: Shows how to load VideoFolder datasets and control metadata usage.  `drop_metadata=True` ignores the metadata file, and `drop_labels=True` removes automatically created labels, resulting in a dataset with only the video column.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/video_load.mdx#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"username/dataset_with_metadata\", drop_metadata=True)\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"username/dataset_without_metadata\", drop_labels=True)\n```\n\n----------------------------------------\n\nTITLE: Pickling Error with Generator Objects\nDESCRIPTION: This snippet demonstrates the `TypeError` encountered when attempting to pickle a generator object directly, as opposed to a generator function, when using `Dataset.from_generator`. This error occurs because generator objects are not picklable, while generator functions are.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/troubleshoot.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nTypeError: cannot pickle 'generator' object\n```\n\n----------------------------------------\n\nTITLE: Dataset with Varying Shape N-Dimensional Arrays\nDESCRIPTION: This example demonstrates how datasets handles n-dimensional arrays with varying shapes.  Each array withing the `data` field is converted to a `jax.Array`.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_jax.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import Dataset\n>>> data = [[[1, 2],[3]], [[4, 5, 6],[7, 8]]]  # varying shape\n>>> ds = Dataset.from_dict({\"data\": data})\n>>> ds = ds.with_format(\"jax\")\n>>> ds[0]\n{'data': [Array([1, 2], dtype=int32), Array([3], dtype=int32)]}\n```\n\n----------------------------------------\n\nTITLE: Sharding a Dataset (Py)\nDESCRIPTION: This snippet demonstrates how to shard a dataset into a specified number of chunks using the `shard` function.  It illustrates how to specify the shard index to access a specific chunk of the dataset.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/stream.mdx#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n>>> dataset = load_dataset(\"amazon_polarity\", split=\"train\", streaming=True)\n>>> print(dataset)\nIterableDataset({\n    features: ['label', 'title', 'content'],\n    num_shards: 4\n})\n\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset.shard(num_shards=2, index=0)\nIterableDataset({\n    features: ['label', 'title', 'content'],\n    num_shards: 2\n})\n```\n\n----------------------------------------\n\nTITLE: Loading AudioFolder Dataset without Labels in Python\nDESCRIPTION: This snippet demonstrates how to load an AudioFolder dataset and drop automatically created labels by setting `drop_labels=True` in the `load_dataset` function.  This results in a dataset containing only the audio column.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/audio_load.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"username/dataset_without_metadata\", drop_labels=True)\n```\n\n----------------------------------------\n\nTITLE: Loading Percentage of Split Python\nDESCRIPTION: This snippet demonstrates how to load a percentage (10%) of the 'train' split of a dataset using string slicing with `datasets.load_dataset`. It loads the 'ajibawa-2023/General-Stories-Collection' dataset and specifies the split using string notation.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/loading.mdx#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n>>> train_10pct_ds = datasets.load_dataset(\"ajibawa-2023/General-Stories-Collection\", split=\"train[:10%]\")\n```\n\n----------------------------------------\n\nTITLE: Defining Additional Target\nDESCRIPTION: This code defines an additional target for Albumentations to map the 'depth' key to 'mask'. This allows applying transformations consistently to both images and their corresponding depth maps during augmentation.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/depth_estimation.mdx#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> additional_targets = {\"depth\": \"mask\"}\n>>> aug = A.Compose(transforms=transforms, additional_targets=additional_targets)\n```\n\n----------------------------------------\n\nTITLE: Accessing Image Data\nDESCRIPTION: Shows how to access the image data from the loaded dataset using the index and the 'image' key.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/semantic_segmentation.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset[index][\"image\"]\n```\n\n----------------------------------------\n\nTITLE: Load local PDF files\nDESCRIPTION: This code illustrates how to create a dataset from local PDF files using `Dataset.from_dict` and `cast_column` with the `Pdf` feature.  It takes a list of file paths and casts the column to the PDF format, allowing the dataset to be used with `pdfplumber`.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/document_load.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import Dataset, Pdf\n\n>>> dataset = Dataset.from_dict({\"pdf\": [\"path/to/pdf_1\", \"path/to/pdf_2\", ..., \"path/to/pdf_n\"]}).cast_column(\"pdf\", Pdf())\n>>> dataset[0][\"pdf\"]\n<pdfplumber.pdf.PDF at 0x1657d0280>\n```\n\n----------------------------------------\n\nTITLE: Loading Specific Rows of Train Split Python\nDESCRIPTION: This snippet demonstrates how to load specific rows (10 to 20) of the 'train' split of a dataset using string slicing with `datasets.load_dataset`. It loads the 'ajibawa-2023/General-Stories-Collection' dataset and specifies the split using string notation.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/loading.mdx#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n>>> train_10_20_ds = datasets.load_dataset(\"ajibawa-2023/General-Stories-Collection\", split=\"train[10:20]\")\n```\n\n----------------------------------------\n\nTITLE: Loading a Dataset from Hugging Face Hub in Python\nDESCRIPTION: This code snippet demonstrates how to load a dataset from the Hugging Face Hub using the `load_dataset` function.  It requires the `datasets` library to be installed and the dataset to be available on the Hub under the specified `<username>/my_dataset` path.  It shows how to retrieve and use a dataset hosted on the Hugging Face Hub.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/audio_dataset.mdx#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n>>> load_dataset(\"<username>/my_dataset\")\n```\n\n----------------------------------------\n\nTITLE: Loading Image Datasets with Paths Using Hugging Face Datasets\nDESCRIPTION: This code snippet shows how to load an image dataset and access the underlying paths to the images instead of decoding them as PIL Images by setting `decode=False` in the `Image` feature. It requires the `datasets` library.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/image_load.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset = load_dataset(\"beans\", split=\"train\").cast_column(\"image\", Image(decode=False))\n>>> dataset[0][\"image\"]\n```\n\n----------------------------------------\n\nTITLE: Reading Metadata from CSV\nDESCRIPTION: This code snippet demonstrates how to read metadata from a CSV file using the `csv.DictReader`. It opens the file, iterates through each row, and stores the metadata in a dictionary.  It handles missing fields by filling them with empty strings and filters data based on a language configuration.  It assumes the existence of `metadata_path`, `path_to_clips`, `self.config.name`, and `data_fields`.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/audio_dataset.mdx#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nwith open(metadata_path, \"r\", encoding=\"utf-8\") as f:\n    reader = csv.DictReader(f)\n    for row in reader:\n        if self.config.name == \"all\" or self.config.name == row[\"language\"]:\n            row[\"path\"] = os.path.join(path_to_clips, row[\"path\"])\n            # if data is incomplete, fill with empty values\n            for field in data_fields:\n                if field not in row:\n                    row[field] = \"\"\n            metadata[row[\"path\"]] = row\n```\n\n----------------------------------------\n\nTITLE: Dataset with Fixed Shape N-Dimensional Arrays\nDESCRIPTION: This snippet shows how to handle N-dimensional arrays with a fixed shape.  The dataset is formatted to JAX, and the data is accessed.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_jax.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import Dataset\n>>> data = [[[1, 2],[3, 4]], [[5, 6],[7, 8]]]  # fixed shape\n>>> ds = Dataset.from_dict({\"data\": data})\n>>> ds = ds.with_format(\"jax\")\n>>> ds[0]\n{'data': Array([[1, 2],\n        [3, 4]], dtype=int32)}\n```\n\n----------------------------------------\n\nTITLE: JSONL metadata example: list of videos\nDESCRIPTION: Example of a JSONL metadata file showing how to link lists of videos to a label, by using the field `file_names`.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/video_dataset.mdx#_snippet_9\n\nLANGUAGE: jsonl\nCODE:\n```\n{\"videos_file_names\": [\"0001_left.mp4\", \"0001_right.mp4\"], \"label\": \"moving_up\"}\n{\"videos_file_names\": [\"0002_left.mp4\", \"0002_right.mp4\"], \"label\": \"moving_down\"}\n{\"videos_file_names\": [\"0003_left.mp4\", \"0003_right.mp4\"], \"label\": \"moving_right\"}\n```\n\n----------------------------------------\n\nTITLE: Installing Albumentations and OpenCV\nDESCRIPTION: Installs the `albumentations` and `opencv-python` libraries using pip. These libraries are used for data augmentation in TensorFlow.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/quickstart.mdx#_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\npip install -U albumentations opencv-python\n```\n\n----------------------------------------\n\nTITLE: Visualizing Augmented Depth Map\nDESCRIPTION: This snippet visualizes the augmented depth map associated with the augmented image, retrieved from the 'labels' key, using the previously defined `show_depthmap` function.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/depth_estimation.mdx#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n>>> show_depthmap(example[\"labels\"])\n```\n\n----------------------------------------\n\nTITLE: Add Elasticsearch Index with Custom Config\nDESCRIPTION: Creates an Elasticsearch index with custom settings and mappings, allowing for fine-grained control over the indexing process, including analyzers and similarity algorithms.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/faiss_es.mdx#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n>>> import elasticsearch as es\n>>> import elasticsearch.helpers\n>>> from elasticsearch import Elasticsearch\n>>> es_client = Elasticsearch([{\"host\": \"localhost\", \"port\": \"9200\"}])  # default client\n>>> es_config = {\n...     \"settings\": {\n...         \"number_of_shards\": 1,\n...         \"analysis\": {\"analyzer\": {\"stop_standard\": {\"type\": \"standard\", \" stopwords\": \"_english_\"}}},\n...     },\n...     \"mappings\": {\"properties\": {\"text\": {\"type\": \"text\", \"analyzer\": \"standard\", \"similarity\": \"BM25\"}}},\n... }  # default config\n>>> es_index_name = \"hf_squad_context\"  # name of the index in Elasticsearch\n>>> squad.add_elasticsearch_index(\"context\", es_client=es_client, es_config=es_config, es_index_name=es_index_name)\n```\n\n----------------------------------------\n\nTITLE: Defining Array2D Features in Py\nDESCRIPTION: This snippet demonstrates how to define a `Features` object with an `Array2D` feature. The `Array2D` feature is used to represent arrays with two dimensions. In this example, the 'a' column is defined as an `Array2D` with a shape of (1, 3) and a data type of 'int32'.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/about_dataset_features.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> features = Features({'a': Array2D(shape=(1, 3), dtype='int32')})\n```\n\n----------------------------------------\n\nTITLE: Formatting a DatasetDict to JAX\nDESCRIPTION: This snippet shows how to format a `DatasetDict` object to JAX format.  Setting the format of a `DatasetDict` to `jax` will format all `Dataset`s within it to use `jax.Array` objects.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_jax.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import DatasetDict\n>>> data = {\"train\": {\"data\": [[1, 2], [3, 4]]}, \"test\": {\"data\": [[5, 6], [7, 8]]}}\n>>> dds = DatasetDict.from_dict(data)\n>>> dds = dds.with_format(\"jax\")\n>>> dds[\"train\"][:2]\n{'data': DeviceArray([\n    [1, 2],\n    [3, 4]], dtype=int32)}\n```\n\n----------------------------------------\n\nTITLE: Using ClassLabel Feature Type\nDESCRIPTION: This snippet demonstrates how ClassLabel data is converted to NumPy arrays when the dataset is formatted as NumPy. It defines a ClassLabel feature with names 'negative' and 'positive' and creates a dataset using it.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_numpy.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import Dataset, Features, ClassLabel\n>>> labels = [0, 0, 1]\n>>> features = Features({\"label\": ClassLabel(names=[\"negative\", \"positive\"]}})\n>>> ds = Dataset.from_dict({\"label\": labels}, features=features)\n>>> ds = ds.with_format(\"numpy\")\n>>> ds[:3]\n{'label': array([0, 0, 1])}\n```\n\n----------------------------------------\n\nTITLE: Loading a specific revision from the Hugging Face Hub in Python\nDESCRIPTION: Loads a specific version of a dataset from the Hub by specifying the `revision` parameter. This can be a tag name, branch name, or commit hash.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/loading.mdx#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n>>> dataset = load_dataset(\n...\t\"lhoestq/custom_squad\",\n...\trevision=\"main\"  # tag name, or branch name, or commit hash\n... )\n```\n\n----------------------------------------\n\nTITLE: Installing Datasets with conda\nDESCRIPTION: Installs the Hugging Face Datasets library using conda, a package management system. This allows for managing dependencies and environments in a consistent manner.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/installation.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nconda install -c huggingface -c conda-forge datasets\n```\n\n----------------------------------------\n\nTITLE: IterableDataset Shard Information in Python\nDESCRIPTION: Demonstrates how to access the number of shards in an `IterableDataset` created from the internet, local files, or a generator function.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Stream from the internet\nmy_iterable_dataset = load_dataset(\"deepmind/code_contests\", split=\"train\", streaming=True)\nmy_iterable_dataset.num_shards  # 39\n\n# Stream from local files\ndata_files = {\"train\": [f\"path/to/data_{i}.csv\" for i in range(1024)]}\nmy_iterable_dataset = load_dataset(\"csv\", data_files=data_files, split=\"train\", streaming=True)\nmy_iterable_dataset.num_shards  # 1024\n\n# From a generator function\ndef my_generator(n, sources):\n    for source in sources:\n        for example_id_for_current_source in range(n):\n            yield {\"example_id\": f\"{source}_{example_id_for_current_source}\"}\n\ngen_kwargs = {\"n\": 10, \"sources\": [f\"path/to/data_{i}\" for i in range(1024)]}\nmy_iterable_dataset = IterableDataset.from_generator(my_generator, gen_kwargs=gen_kwargs)\nmy_iterable_dataset.num_shards  # 1024\n```\n\n----------------------------------------\n\nTITLE: Load Elasticsearch Index\nDESCRIPTION: Loads an existing Elasticsearch index by name, enabling querying without rebuilding the index from scratch.  It requires specifying the host and port of the Elasticsearch instance.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/faiss_es.mdx#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n>>> squad = load_dataset('rajpurkar/squad', split='validation')\n>>> squad.load_elasticsearch_index(\"context\", host=\"localhost\", port=\"9200\", es_index_name=\"hf_squad_val_context\")\n>>> query = \"machine\"\n>>> scores, retrieved_examples = squad.get_nearest_examples(\"context\", query, k=10)\n```\n\n----------------------------------------\n\nTITLE: Loading a Local Arrow File with Dataset.from_file in Python\nDESCRIPTION: Loads a local Arrow file using the `Dataset.from_file` method. This method memory maps the Arrow file without preparing the dataset in the cache.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/loading.mdx#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\n>>> from datasets import Dataset\n>>> dataset = Dataset.from_file(\"data.arrow\")\n```\n\n----------------------------------------\n\nTITLE: Resetting Dataset Format\nDESCRIPTION: This code demonstrates how to reset a dataset's format to its original state using `with_format(None)`. It displays the format before and after resetting, highlighting the change from a specific format to `None`.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/process.mdx#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset.format\n{'type': 'torch', 'format_kwargs': {}, 'columns': [...], 'output_all_columns': False}\n>>> dataset = dataset.with_format(None)\n>>> dataset.format\n{'type': None, 'format_kwargs': {}, 'columns': [...], 'output_all_columns': False}\n```\n\n----------------------------------------\n\nTITLE: Upload Specific Folder to Dataset Root\nDESCRIPTION: This command uploads the contents of a specific folder to the root of the dataset repository. Replace `my-cool-dataset` with the actual repository name and `./data` with the path to the folder you want to upload.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/share.mdx#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n>>> huggingface-cli upload my-cool-dataset ./data . --repo-type dataset\nhttps://huggingface.co/datasetsWauplin/my-cool-dataset/tree/main/\n```\n\n----------------------------------------\n\nTITLE: Dataset with Array Feature Type and Shape Specification\nDESCRIPTION: This code shows how to explicitly use the `Array2D` feature type and specify the shape of the tensors to avoid slow shape comparisons and data copies when working with N-dimensional arrays.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_jax.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import Dataset, Features, Array2D\n>>> data = [[[1, 2],[3, 4]],[[5, 6],[7, 8]]]\n>>> features = Features({\"data\": Array2D(shape=(2, 2), dtype='int32')})\n>>> ds = Dataset.from_dict({\"data\": data}, features=features)\n>>> ds = ds.with_format(\"jax\")\n>>> ds[0]\n{'data': Array([[1, 2],\n        [3, 4]], dtype=int32)}\n>>> ds[:2]\n{'data': Array([[[1, 2],\n         [3, 4]],\n \n        [[5, 6],\n         [7, 8]]], dtype=int32)}\n```\n\n----------------------------------------\n\nTITLE: Setting Dataset Transformation (Torchvision)\nDESCRIPTION: Sets the transformation function for the dataset, utilizing the defined Torchvision transformations. This transformation is applied on-the-fly when accessing the dataset.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/semantic_segmentation.mdx#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset.set_transform(train_transforms)\n```\n\n----------------------------------------\n\nTITLE: JSONL metadata example\nDESCRIPTION: Example of a JSONL metadata file showing how to link videos with additional features.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/video_dataset.mdx#_snippet_7\n\nLANGUAGE: jsonl\nCODE:\n```\n{\"file_name\": \"0001.mp4\", \"additional_feature\": \"This is a first value of a text feature you added to your videos\"}\n{\"file_name\": \"0002.mp4\", \"additional_feature\": \"This is a second value of a text feature you added to your videos\"}\n{\"file_name\": \"0003.mp4\", \"additional_feature\": \"This is a third value of a text feature you added to your videos\"}\n```\n\n----------------------------------------\n\nTITLE: Visualizing Transformed Image\nDESCRIPTION: Verifies the applied transformation by retrieving the transformed image and mask from the dataset and visualizing them using the visualize_seg_mask function.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/semantic_segmentation.mdx#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n>>> image = np.array(dataset[index][\"pixel_values\"])\n>>> mask = np.array(dataset[index][\"label\"])\n\n>>> visualize_seg_mask(image, mask)\n```\n\n----------------------------------------\n\nTITLE: Installing Datasets with pip\nDESCRIPTION: Installs the Hugging Face Datasets library using pip, the Python package installer. This is the most common and straightforward method for installing the library.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/installation.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install datasets\n```\n\n----------------------------------------\n\nTITLE: Installing Datasets from Source\nDESCRIPTION: Installs the Datasets library from the cloned source code using pip.  The `-e .` flag installs the package in editable mode, allowing changes to the source code to be reflected immediately.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/installation.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ncd datasets\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Datasets CLI Delete from Hub Example\nDESCRIPTION: This command deletes a specific configuration from a dataset on the Hugging Face Hub. It requires the dataset ID and the name of the configuration to be deleted.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/cli.mdx#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n>>> datasets-cli delete_from_hub USERNAME/DATASET_NAME CONFIG_NAME\n```\n\n----------------------------------------\n\nTITLE: Creating a Virtual Environment\nDESCRIPTION: Creates a Python virtual environment within the project directory. This isolates project dependencies and avoids conflicts with other projects or system-wide packages.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/installation.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython -m venv .env\n```\n\n----------------------------------------\n\nTITLE: Setting HF_HOME environment variable\nDESCRIPTION: This shell command sets the `HF_HOME` environment variable to a specified directory, which changes the default location of the Hugging Face Datasets cache.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/cache.mdx#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\n$ export HF_HOME=\"/path/to/another/directory/datasets\"\n```\n\n----------------------------------------\n\nTITLE: Specify Authentication Token\nDESCRIPTION: This command uploads data to a dataset repository using a specified authentication token. Replace `Wauplin/my-cool-dataset` with the actual repository name, `./data` with the path to the folder you want to upload, and `hf_****` with your actual Hugging Face Hub token.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/share.mdx#_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\n>>> huggingface-cli upload Wauplin/my-cool-dataset ./data . --repo-type dataset --token=hf_****\n...\nhttps://huggingface.co/datasetsWauplin/my-cool-data/tree/main\n```\n\n----------------------------------------\n\nTITLE: Access and Query FAISS Index Directly\nDESCRIPTION: Accesses the underlying FAISS index object and uses it to perform a range search with a specified threshold. This provides more direct control over the search process.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/faiss_es.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> faiss_index = ds_with_embeddings.get_index('embeddings').faiss_index\n>>> limits, distances, indices = faiss_index.range_search(x=question_embedding.reshape(1, -1), thresh=0.95)\n```\n\n----------------------------------------\n\nTITLE: Loading the Scene Parsing Dataset\nDESCRIPTION: Loads the \"scene_parse_150\" dataset from Hugging Face Datasets and retrieves a specific example from the training split. The example contains an image, its segmentation mask, and the scene category.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/semantic_segmentation.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"scene_parse_150\", split=\"train\")\n>>> index = 10\n>>> dataset[index]\n{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=683x512 at 0x7FB37B0EC810>,\n 'annotation': <PIL.PngImagePlugin.PngImageFile image mode=L size=683x512 at 0x7FB37B0EC9D0>,\n 'scene_category': 927}\n```\n\n----------------------------------------\n\nTITLE: Defining URLs for Data Files in Python\nDESCRIPTION: This code defines a dictionary of URLs pointing to the original SQuAD data files. The _URL variable stores the base URL, and the _URLS dictionary maps split names ('train', 'dev') to their respective data file URLs. This allows the DatasetBuilder to locate and download the data.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/dataset_script.mdx#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n_URL = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/\"\n_URLS = {\n    \"train\": _URL + \"train-v1.1.json\",\n    \"dev\": _URL + \"dev-v1.1.json\",\n}\n```\n\n----------------------------------------\n\nTITLE: Datasets CLI Convert to Parquet Example\nDESCRIPTION: This command converts the specified dataset to Parquet format.  It requires the dataset ID and may require `--trust_remote_code` if the dataset contains code that needs to be executed.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/cli.mdx#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n>>> datasets-cli convert_to_parquet USERNAME/DATASET_NAME\n```\n\n----------------------------------------\n\nTITLE: Extracting TAR Archives Locally in Python\nDESCRIPTION: This snippet outlines how to download and extract a TAR archive locally using the `DownloadManager` in non-streaming mode. It first downloads the audio data, then extracts it to a local directory, and returns the path to the extracted archive. The `dl_manager.is_streaming` check determines whether extraction is possible. This requires `_AUDIO_URL` to be a valid URL pointing to the TAR archive.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/audio_dataset.mdx#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nlocal_extracted_archive = dl_manager.extract(audio_path) if not dl_manager.is_streaming else None\n```\n\n----------------------------------------\n\nTITLE: CSV metadata example\nDESCRIPTION: Example of a CSV metadata file showing how to link videos with additional features.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/video_dataset.mdx#_snippet_6\n\nLANGUAGE: csv\nCODE:\n```\nfile_name,additional_feature\n0001.mp4,This is a first value of a text feature you added to your videos\n0002.mp4,This is a second value of a text feature you added to your videos\n0003.mp4,This is a third value of a text feature you added to your videos\n```\n\n----------------------------------------\n\nTITLE: Installing Datasets with pip\nDESCRIPTION: This code snippet shows how to install the Hugging Face Datasets library using pip. It should be run within a virtual environment.\nSOURCE: https://github.com/huggingface/datasets/blob/main/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install datasets\n```\n\n----------------------------------------\n\nTITLE: Loading a specific dataset configuration - Python\nDESCRIPTION: This code demonstrates how to load a specific configuration of a dataset using `datasets.load_dataset`, by specifying the configuration name.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/dataset_script.mdx#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n>>> from datasets import load_dataset\n>>> dataset = load_dataset('super_glue', 'boolq')\n```\n\n----------------------------------------\n\nTITLE: Defining Configurations with Python\nDESCRIPTION: Defines the dataset configurations in the `BUILDER_CONFIGS` class variable within the `GeneratorBasedBuilder` subclass.  The code iterates through languages and creates a `LibriVoxIndonesiaConfig` for each, populating configurations with data from `STATS` and `LANGUAGES` variables.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/audio_dataset.mdx#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nclass LibriVoxIndonesia(datasets.GeneratorBasedBuilder):\n    DEFAULT_CONFIG_NAME = \"all\"\n\n    BUILDER_CONFIGS = [\n        LibriVoxIndonesiaConfig(\n            name=lang,\n            version=STATS[\"version\"],\n            language=LANGUAGES[lang],\n            release_date=STATS[\"date\"],\n            num_clips=lang_stats[\"clips\"],\n            num_speakers=lang_stats[\"users\"],\n            total_hr=float(lang_stats[\"totalHrs\"]) if lang_stats[\"totalHrs\"] else None,\n            size_bytes=int(lang_stats[\"size\"]) if lang_stats[\"size\"] else None,\n        )\n        for lang, lang_stats in STATS[\"locales\"].items()\n    ]\n```\n\n----------------------------------------\n\nTITLE: Visualizing bounding boxes on an image using torchvision\nDESCRIPTION: This code snippet uses torchvision to draw bounding boxes on an image. It converts bounding box coordinates from xywh to xyxy format, retrieves category labels, and then uses `draw_bounding_boxes` to overlay the boxes and labels on the image.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/object_detection.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> import torch\n>>> from torchvision.ops import box_convert\n>>> from torchvision.utils import draw_bounding_boxes\n>>> from torchvision.transforms.functional import pil_to_tensor, to_pil_image\n\n>>> categories = ds['train'].features['objects'].feature['category']\n\n>>> boxes_xywh = torch.tensor(example['objects']['bbox'])\n>>> boxes_xyxy = box_convert(boxes_xywh, 'xywh', 'xyxy')\n>>> labels = [categories.int2str(x) for x in example['objects']['category']]\n>>> to_pil_image(\n...     draw_bounding_boxes(\n...         pil_to_tensor(example['image']),\n...         boxes_xyxy,\n...         colors=\"red\",\n...         labels=labels,\n...     )\n... )\n```\n\n----------------------------------------\n\nTITLE: Formatting Dataset to Pandas DataFrame\nDESCRIPTION: This code demonstrates how to format a dataset as a Pandas DataFrame. It shows the dataset before and after formatting, where the output is now a Pandas DataFrame.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/process.mdx#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n>>> ds = Dataset.from_dict({\"text\": [\"foo\", \"bar\"], \"label\": [0, 1]})\n>>> ds = ds.with_format(\"pandas\")\n>>> ds[:2]\n  text  label\n0  foo      0\n1  bar      1\n```\n\n----------------------------------------\n\nTITLE: Loading WebDatasets with Hugging Face Datasets\nDESCRIPTION: This code snippet demonstrates how to load a WebDataset in streaming mode using `load_dataset` with the `webdataset` builder and setting `streaming=True`. This is typically used for large image datasets stored as TAR archives, requiring the `datasets` library.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/image_load.mdx#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"webdataset\", data_dir=\"/path/to/folder\", streaming=True)\n```\n\n----------------------------------------\n\nTITLE: Dataset Fingerprint Example in Datasets\nDESCRIPTION: This code snippet demonstrates how to access and print the fingerprints of two datasets. The first dataset is created directly from a dictionary, and the second dataset is created by applying a simple map transformation to the first. This showcases how fingerprints change when datasets are transformed.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/about_cache.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import Dataset\n>>> dataset1 = Dataset.from_dict({\"a\": [0, 1, 2]})\n>>> dataset2 = dataset1.map(lambda x: {\"a\": x[\"a\"] + 1})\n>>> print(dataset1._fingerprint, dataset2._fingerprint)\nd19493523d95e2dc 5b86abacd4b42434\n```\n\n----------------------------------------\n\nTITLE: Installing Albumentations\nDESCRIPTION: This command installs the Albumentations library, a Python library for data augmentation in computer vision tasks.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/depth_estimation.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -U albumentations\n```\n\n----------------------------------------\n\nTITLE: Installing Audio Feature\nDESCRIPTION: Installs the optional audio feature for working with audio datasets. This installs additional dependencies required for audio processing.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/installation.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip install datasets[audio]\n```\n\n----------------------------------------\n\nTITLE: Seeking to a Specific Timestamp in Video\nDESCRIPTION: This snippet demonstrates how to seek to a specific timestamp within a video using the `.seek()` method of the `VideoReader`. It also gets the video metadata and keyframes are also shown.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/video_load.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> video.get_metadata()\n{'video': {'fps': [10.0], 'duration': [16.1]}}\n>>> video = video.seek(8.0, keyframes_only=True)\n>>> frame = next(video)\n>>> first_frame[\"data\"].shape\n(3, 240, 320)\n```\n\n----------------------------------------\n\nTITLE: Loading VideoFolder with data_dir - Python\nDESCRIPTION: Loads a video dataset by explicitly specifying the `videofolder` dataset builder and the `data_dir` parameter. This is equivalent to directly loading the dataset by providing the path. Requires the `datasets` library.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/video_dataset.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset = load_dataset(\"videofolder\", data_dir=\"/path/to/folder\")\n```\n\n----------------------------------------\n\nTITLE: Data Augmentation with Torchvision\nDESCRIPTION: Applies data augmentation to images using `torchvision`. Requires `torchvision`. Defines a Compose of transformations including `ColorJitter` to randomly adjust brightness and hue, and `ToTensor` to convert the image to a tensor.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/quickstart.mdx#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n>>> from torchvision.transforms import Compose, ColorJitter, ToTensor\n\n>>> jitter = Compose(\n...\t[ColorJitter(brightness=0.5, hue=0.5), ToTensor()]\n... )\n```\n\n----------------------------------------\n\nTITLE: Datasets CLI Convert to Parquet Help\nDESCRIPTION: This command displays the help message for the `convert_to_parquet` subcommand, showing the required and optional arguments. It includes options for specifying the dataset ID, access token, revision, and whether to trust remote code.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/cli.mdx#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n>>> datasets-cli convert_to_parquet --help\nusage: datasets-cli <command> [<args>] convert_to_parquet [-h] [--token TOKEN] [--revision REVISION] [--trust_remote_code] dataset_id\n\npositional arguments:\n  dataset_id           source dataset ID, e.g. USERNAME/DATASET_NAME or ORGANIZATION/DATASET_NAME\n\noptional arguments:\n  -h, --help           show this help message and exit\n  --token TOKEN        access token to the Hugging Face Hub (defaults to logged-in user's one)\n  --revision REVISION  source revision\n  --trust_remote_code  whether to trust the code execution of the load script\n```\n\n----------------------------------------\n\nTITLE: Installing Albumentations and OpenCV-Python\nDESCRIPTION: Installs the Albumentations and OpenCV-Python libraries using pip. These libraries are used for image augmentation and manipulation.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/semantic_segmentation.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -U albumentations opencv-python\n```\n\n----------------------------------------\n\nTITLE: Pushing Private Dataset to Hub\nDESCRIPTION: This snippet demonstrates how to push a private dataset to the Hugging Face Hub using the `push_to_hub` function with the `private` parameter set to `True`. It requires the `datasets` library to be installed and a dataset object to be created.  This parameter only works when creating the repository for the first time.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/upload_dataset.mdx#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n>>> dataset.push_to_hub(\"stevhliu/private_processed_demo\", private=True)\n```\n\n----------------------------------------\n\nTITLE: Verify Image Transformations\nDESCRIPTION: Verifies that the image transformations have been applied correctly by accessing the `pixel_values` of the first example in the training dataset. The transformed image is then displayed using `matplotlib.pyplot`. Requires `numpy` and `matplotlib.pyplot`.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/image_classification.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> import numpy as np\n>>> import matplotlib.pyplot as plt\n\n>>> img = dataset[\"train\"][0][\"pixel_values\"]\n>>> plt.imshow(img)\n```\n\n----------------------------------------\n\nTITLE: Load PDF dataset with metadata\nDESCRIPTION: This snippet demonstrates loading a PDF dataset using the `PdfFolder` dataset builder, ignoring the metadata file by setting `drop_metadata=True`.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/document_load.mdx#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"username/dataset_with_metadata\", drop_metadata=True)\n```\n\n----------------------------------------\n\nTITLE: Converting Depth Map to RGB\nDESCRIPTION: This snippet converts the depth map to RGB format before displaying it using PIL. PIL cannot display float32 images directly, so conversion to RGB is required.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/depth_estimation.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> example[\"depth_map\"].convert(\"RGB\")\n```\n\n----------------------------------------\n\nTITLE: Install Albumentations and OpenCV\nDESCRIPTION: Installs the `albumentations` and `opencv-python` packages. Albumentations is used for image augmentation, and OpenCV is used for image processing. It is a prerequisite step before performing any image transformations on the dataset.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/image_classification.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -U albumentations opencv-python\n```\n\n----------------------------------------\n\nTITLE: Loading WebDataset\nDESCRIPTION: This code snippet demonstrates how to load a dataset in WebDataset format.  The `data_dir` parameter specifies the directory containing the TAR archives. Requires the `datasets` library and that the data be in WebDataset format.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/audio_dataset.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"webdataset\", data_dir=\"/path/to/folder\", split=\"train\")\ndataset[0][\"json\"]\n```\n\n----------------------------------------\n\nTITLE: BibTeX Citation Example\nDESCRIPTION: This snippet provides an example of a BibTeX formatted citation for a dataset. It shows the standard fields like author, title, journal, and year, which are essential for proper attribution.\nSOURCE: https://github.com/huggingface/datasets/blob/main/templates/README_guide.md#_snippet_1\n\nLANGUAGE: BibTeX\nCODE:\n```\n@article{article_id,\n  author    = {Author List},\n  title     = {Dataset Paper Title},\n  journal   = {Publication Venue},\n  year      = {2525}\n}\n```\n\n----------------------------------------\n\nTITLE: Defining dataset information with datasets.DatasetInfo - Python\nDESCRIPTION: This code defines the `_info` method within a `DatasetBuilder` class to provide information about the dataset, including its description, features, homepage, and citation. This is a crucial part of the dataset loading script.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/dataset_script.mdx#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ndef _info(self):\n    return datasets.DatasetInfo(\n        description=_DESCRIPTION,\n        features=datasets.Features(\n            {\n                \"id\": datasets.Value(\"string\"),\n                \"title\": datasets.Value(\"string\"),\n                \"context\": datasets.Value(\"string\"),\n                \"question\": datasets.Value(\"string\"),\n                \"answers\": datasets.features.Sequence(\n                    {\"text\": datasets.Value(\"string\"), \"answer_start\": datasets.Value(\"int32\"),}\n                ),\n            }\n        ),\n        # No default supervised_keys (as we have to pass both question\n        # and context as input).\n        supervised_keys=None,\n        homepage=\"https://rajpurkar.github.io/SQuAD-explorer/\",\n        citation=_CITATION,\n    )\n```\n\n----------------------------------------\n\nTITLE: Set Dataset Format to PyArrow\nDESCRIPTION: This code snippet demonstrates how to set the format of a `datasets` Dataset to `pyarrow`. This allows accessing the data as PyArrow Tables or Arrays, enabling fast zero-copy operations.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_pyarrow.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import Dataset\n>>> data = {\"col_0\": [\"a\", \"b\", \"c\", \"d\"], \"col_1\": [0., 0., 1., 1.]}\n>>> ds = Dataset.from_dict(data)\n>>> ds = ds.with_format(\"arrow\")\n>>> ds[0]       # pa.Table\npyarrow.Table\ncol_0: string\ncol_1: double\n----\ncol_0: [[\"a\"]]\ncol_1: [[0]]\n>>> ds[:2]      # pa.Table\npyarrow.Table\ncol_0: string\ncol_1: double\n----\ncol_0: [[\"a\",\"b\"]]\ncol_1: [[0,0]]\n>>> ds[\"data\"]  # pa.array\n<pyarrow.lib.ChunkedArray object at 0x1394312a0>\n[\n  [\n    \"a\",\n    \"b\",\n    \"c\",\n    \"d\"\n  ]\n]\n```\n\n----------------------------------------\n\nTITLE: Loading BERT Model and Tokenizer (TensorFlow)\nDESCRIPTION: Loads a pretrained BERT model and its corresponding tokenizer from the ðŸ¤— Transformers library for TensorFlow. Requires the `transformers` library.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/quickstart.mdx#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n>>> from transformers import TFAutoModelForSequenceClassification, AutoTokenizer\n\n>>> model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n```\n\n----------------------------------------\n\nTITLE: Listing Datasets CLI Commands\nDESCRIPTION: This command displays the available commands in the Datasets CLI, providing a brief description of each.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/cli.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n>>> datasets-cli --help\nusage: datasets-cli <command> [<args>]\n\npositional arguments:\n  {convert,env,test,convert_to_parquet}\n                        datasets-cli command helpers\n    convert             Convert a TensorFlow Datasets dataset to a HuggingFace Datasets dataset.\n    env                 Print relevant system environment info.\n    test                Test dataset implementation.\n    convert_to_parquet  Convert dataset to Parquet\n    delete_from_hub     Delete dataset config from the Hub\n\noptional arguments:\n  -h, --help            show this help message and exit\n```\n\n----------------------------------------\n\nTITLE: Login to Hugging Face Hub\nDESCRIPTION: This command allows you to log in to your Hugging Face Hub account using your credentials, enabling you to create and upload datasets. It prompts for your username/email and password/token.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/share.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhuggingface-cli login\n```\n\n----------------------------------------\n\nTITLE: YAML Configuration for Multiple Configurations\nDESCRIPTION: This snippet demonstrates how to define multiple configurations (subsets) within the YAML configuration. It creates two configurations: 'main_data' which uses 'main_data.csv', and 'additional_data' which uses 'additional_data.csv'. These configurations can be loaded separately using the load_dataset function.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/repository_structure.mdx#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n---\nconfigs:\n- config_name: main_data\n  data_files: \"main_data.csv\"\n- config_name: additional_data\n  data_files: \"additional_data.csv\"\n---\n```\n\n----------------------------------------\n\nTITLE: Display Hugging Face CLI Upload Help\nDESCRIPTION: This command displays help information for the `huggingface-cli upload` command, showing available options and their usage. It's useful for understanding all possible arguments and configurations.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/share.mdx#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nhuggingface-cli upload --help\n```\n\n----------------------------------------\n\nTITLE: Verifying the transform by visualizing an example\nDESCRIPTION: This code verifies that the transform works correctly by visualizing an example from the transformed dataset. It accesses the 10th example, converts bounding boxes to xyxy format, and displays the transformed image with bounding boxes.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/object_detection.mdx#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> example = ds['train'][10]\n>>> to_pil_image(\n...     draw_bounding_boxes(\n...         example['image'],\n...         box_convert(example['bbox'], 'xywh', 'xyxy'),\n...         colors='red',\n...         labels=[categories.int2str(x) for x in example['category']]\n...     )\n... )\n```\n\n----------------------------------------\n\nTITLE: Shuffling and Flattening Indices in Hugging Face Datasets (Python)\nDESCRIPTION: This code demonstrates the performance impact of shuffling a `Dataset` and how to restore performance by flattening the indices. It highlights the performance difference between accessing elements with and without an indices mapping and shows how `Dataset.flatten_indices()` can be used to rewrite the shuffled dataset for faster access.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nmy_dataset[0]  # fast\nmy_dataset = my_dataset.shuffle(seed=42)\nmy_dataset[0]  # up to 10x slower\nmy_dataset = my_dataset.flatten_indices()  # rewrite the shuffled dataset on disk as contiguous chunks of data\nmy_dataset[0]  # fast again\n```\n\n----------------------------------------\n\nTITLE: Load and Transform Dataset with Hugging Face Datasets\nDESCRIPTION: This example demonstrates loading a dataset, mapping a function to add a prefix to text entries, tokenizing the text using a tokenizer function, and processing batches of examples using multiple processors. It showcases how to load data, preprocess it, and display a sample output using the Hugging Face Datasets library.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/README.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\", split=\"validation\")\n>>> def add_prefix(example):\n...     example[\"text\"] = \"Review: \" + example[\"text\"]\n...     return example\n>>> ds = ds.map(add_prefix)\n>>> ds[0:3][\"text\"]\n['Review: compassionately explores the seemingly irreconcilable situation between conservative christian parents and their estranged gay and lesbian children .',\n        'Review: the soundtrack alone is worth the price of admission .',\n        'Review: rodriguez does a splendid job of racial profiling hollywood style--casting excellent latin actors of all ages--a trend long overdue .']\n\n    # process a batch of examples\n>>> ds = ds.map(lambda example: tokenizer(example[\"text\"]), batched=True)\n    # set number of processors\n>>> ds = ds.map(add_prefix, num_proc=4)\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch\nDESCRIPTION: This command installs the PyTorch library using pip. PyTorch is a popular open-source machine learning framework used for various tasks, including model training and inference.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/quickstart.mdx#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install torch\n```\n\n----------------------------------------\n\nTITLE: Loading a Subset of Files from the Hub with load_dataset in Python\nDESCRIPTION: Loads a specific subset of files from the Hub using `data_files` or `data_dir`. These parameters can accept a relative path. The `data_files` parameter can also accept a grep pattern.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/loading.mdx#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n>>> from datasets import load_dataset\n\n# load files that match the grep pattern\n>>> c4_subset = load_dataset(\"allenai/c4\", data_files=\"en/c4-train.0000*-of-01024.json.gz\")\n\n# load dataset from the en directory on the Hub\n>>> c4_subset = load_dataset(\"allenai/c4\", data_dir=\"en\")\n```\n\n----------------------------------------\n\nTITLE: Handling N-Dimensional Arrays\nDESCRIPTION: This snippet shows how datasets handle N-dimensional arrays with a fixed shape when the format is set to NumPy. It creates a dataset with 3D arrays of fixed shape and accesses an element to demonstrate the resulting NumPy array.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_numpy.mdx#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import Dataset\n>>> data = [[[1, 2],[3, 4]], [[5, 6],[7, 8]]]  # fixed shape\n>>> ds = Dataset.from_dict({\"data\": data})\n>>> ds = ds.with_format(\"numpy\")\n>>> ds[0]\n{'data': array([[1, 2],\n        [3, 4]])}\n```\n\n----------------------------------------\n\nTITLE: Shuffling IterableDataset in Hugging Face Datasets (Python)\nDESCRIPTION: This code demonstrates how to shuffle an `IterableDataset` using the `shuffle` method and how to iterate over the shuffled dataset. It illustrates the efficiency of `IterableDataset` shuffling compared to shuffling a regular `Dataset`, maintaining optimal performance. The `buffer_size` parameter controls the size of the shuffle buffer.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfor example in enumerate(my_iterable_dataset):  # fast\n    pass\n\nshuffled_iterable_dataset = my_iterable_dataset.shuffle(seed=42, buffer_size=100)\n\nfor example in enumerate(shuffled_iterable_dataset):  # as fast as before\n    pass\n\nshuffled_iterable_dataset = my_iterable_dataset.shuffle(seed=1337, buffer_size=100)  # reshuffling using another seed is instantaneous\n\nfor example in enumerate(shuffled_iterable_dataset):  # still as fast as before\n    pass\n```\n\n----------------------------------------\n\nTITLE: Defining BuilderConfigs for different configurations - Python\nDESCRIPTION: This code demonstrates how to create instances of the `BuilderConfig` class to define different configurations for a dataset. These configurations are stored in the `BUILDER_CONFIGS` list within the `DatasetBuilder` class.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/dataset_script.mdx#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nclass SuperGlue(datasets.GeneratorBasedBuilder):\n    \"\"\"The SuperGLUE benchmark.\"\"\"\n\n    BUILDER_CONFIG_CLASS = SuperGlueConfig\n\n    BUILDER_CONFIGS = [\n        SuperGlueConfig(\n            name=\"boolq\",\n            description=_BOOLQ_DESCRIPTION,\n            features=[\"question\", \"passage\"],\n            data_url=\"https://dl.fbaipublicfiles.com/glue/superglue/data/v2/BoolQ.zip\",\n            citation=_BOOLQ_CITATION,\n            url=\"https://github.com/google-research-datasets/boolean-questions\",\n        ),\n        ...\n        ...\n        SuperGlueConfig(\n            name=\"axg\",\n            description=_AXG_DESCRIPTION,\n            features=[\"premise\", \"hypothesis\"],\n            label_classes=[\"entailment\", \"not_entailment\"],\n            data_url=\"https://dl.fbaipublicfiles.com/glue/superglue/data/v2/AX-g.zip\",\n            citation=_AXG_CITATION,\n            url=\"https://github.com/rudinger/winogender-schemas\",\n        ),\n```\n\n----------------------------------------\n\nTITLE: Download DPR Model with Transformers (FAISS)\nDESCRIPTION: Downloads and initializes the DPRContextEncoder and DPRContextEncoderTokenizer from the Hugging Face Transformers library. It disables gradient calculation for inference.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/faiss_es.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\n>>> import torch\n>>> torch.set_grad_enabled(False)\n>>> ctx_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n>>> ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n```\n\n----------------------------------------\n\nTITLE: Disabling Image Decoding with Hugging Face Datasets\nDESCRIPTION: This code snippet demonstrates how to disable image decoding, which can be useful if you only need to access the image paths or bytes.  It is only available for streaming datasets.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/image_load.mdx#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset = dataset.decode(False)\n```\n\n----------------------------------------\n\nTITLE: Initializing a Dataset Builder Class with Python\nDESCRIPTION: Creates a basic dataset builder class inheriting from `datasets.GeneratorBasedBuilder`. Defines the structure with `_info`, `_split_generators`, and `_generate_examples` methods that need to be implemented later.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/audio_dataset.mdx#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass VivosDataset(datasets.GeneratorBasedBuilder):\n    \"\"\"VIVOS is a free Vietnamese speech corpus consisting of 15 hours of recording speech prepared for\n    Vietnamese Automatic Speech Recognition task.\"\"\"\n\n    def _info(self):\n\n    def _split_generators(self, dl_manager):\n\n    def _generate_examples(self, prompts_path, path_to_clips, audio_files):\n```\n\n----------------------------------------\n\nTITLE: Datasets CLI Delete from Hub Help\nDESCRIPTION: This command displays the help message for the `delete_from_hub` subcommand. It provides information on the required and optional arguments needed to delete a dataset configuration from the Hugging Face Hub, including the dataset ID, configuration name, access token, and revision.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/cli.mdx#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n>>> datasets-cli delete_from_hub --help\nusage: datasets-cli <command> [<args>] delete_from_hub [-h] [--token TOKEN] [--revision REVISION] dataset_id config_name\n\npositional arguments:\n  dataset_id           source dataset ID, e.g. USERNAME/DATASET_NAME or ORGANIZATION/DATASET_NAME\n  config_name          config name to delete\n\noptional arguments:\n  -h, --help           show this help message and exit\n  --token TOKEN        access token to the Hugging Face Hub\n  --revision REVISION  source revision\n```\n\n----------------------------------------\n\nTITLE: Loading AudioFolder with data_dir\nDESCRIPTION: This code snippet is equivalent to the previous snippet, but it explicitly specifies the `audiofolder` builder and the `data_dir`.  Requires the `datasets` library and that the dataset be structured in a specific directory format.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/audio_dataset.mdx#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndataset = load_dataset(\"audiofolder\", data_dir=\"/path/to/folder\")\n```\n\n----------------------------------------\n\nTITLE: Disabling caching globally in Python\nDESCRIPTION: This Python snippet showcases disabling the caching mechanism globally for the `datasets` library using the `disable_caching` function. After this is called, no more cache files will be loaded and existing transforms will be reapplied.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/cache.mdx#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n>>> from datasets import disable_caching\n>>> disable_caching()\n```\n\n----------------------------------------\n\nTITLE: YAML Configuration with Default Configuration\nDESCRIPTION: This snippet shows how to set a default configuration using `default: true` in the YAML file. The main_data configuration is marked as default, allowing it to be loaded without specifying the config_name in the `load_dataset` function.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/repository_structure.mdx#_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\n- config_name: main_data\n  data_files: \"main_data.csv\"\n  default: true\n```\n\n----------------------------------------\n\nTITLE: Exact Shuffling with Dataset.shuffle in Python\nDESCRIPTION: Shows how to apply exact shuffling to a Dataset using the shuffle method, which shuffles a list of indices.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nmy_dataset = my_dataset.shuffle(seed=42)\nprint(my_dataset[0])\n```\n\n----------------------------------------\n\nTITLE: Hashing a Python Object with Hasher in Datasets\nDESCRIPTION: This code snippet shows how to use the `Hasher` class from the `datasets.fingerprint` module to calculate the hash of a Python object, specifically a lambda function. This hash is used internally by the Datasets library to track changes to processing functions and update dataset fingerprints.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/about_cache.mdx#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets.fingerprint import Hasher\n>>> my_func = lambda example: {\"length\": len(example[\"text\"])}\n>>> print(Hasher.hash(my_func))\n'3d35e2b3e94c81d6'\n```\n\n----------------------------------------\n\nTITLE: Creating Project Directory\nDESCRIPTION: Creates a project directory and navigates into it. This is the first step in setting up a new project environment.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/installation.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmkdir ~/my-project\ncd ~/my-project\n```\n\n----------------------------------------\n\nTITLE: Create an Organization Dataset Repository\nDESCRIPTION: This command creates a dataset repository under a specific organization on the Hugging Face Hub. Replace `my-cool-dataset` with your desired dataset name and `your-org-name` with the organization's name.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/share.mdx#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nhuggingface-cli repo create my-cool-dataset --type dataset --organization your-org-name\n```\n\n----------------------------------------\n\nTITLE: Syncing with the Upstream Repository\nDESCRIPTION: This snippet shows how to sync your local copy of the code with the original repository. This is important to do regularly to account for changes made by other contributors. It fetches the latest changes from the upstream repository and rebases your branch on top of it.\nSOURCE: https://github.com/huggingface/datasets/blob/main/CONTRIBUTING.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ngit fetch upstream\ngit rebase upstream/main\n```\n\n----------------------------------------\n\nTITLE: Decode Audio with pydub (set_transform)\nDESCRIPTION: This snippet demonstrates using `set_transform` to decode audio files with the `pydub` package, which supports formats not natively supported by the `datasets` library's Audio feature. The `decode_audio_with_pydub` function opens an audio file, sets the sampling rate to 16kHz, converts it to mono, converts the audio samples to a numpy array, normalizes it and return the array. It requires the `numpy` and `pydub` packages.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/process.mdx#_snippet_29\n\nLANGUAGE: Python\nCODE:\n```\n>>> import numpy as np\n>>> from pydub import AudioSegment\n\n>>> audio_dataset_amr = Dataset.from_dict({\"audio\": [\"audio_samples/audio.amr\"]})\n\n>>> def decode_audio_with_pydub(batch, sampling_rate=16_000):\n...     def pydub_decode_file(audio_path):\n...         sound = AudioSegment.from_file(audio_path)\n...         if sound.frame_rate != sampling_rate:\n...             sound = sound.set_frame_rate(sampling_rate)\n...         channel_sounds = sound.split_to_mono()\n...         samples = [s.get_array_of_samples() for s in channel_sounds]\n...         fp_arr = np.array(samples).T.astype(np.float32)\n...         fp_arr /= np.iinfo(samples[0].typecode).max\n...         return fp_arr\n...\n...     batch[\"audio\"] = [pydub_decode_file(audio_path) for audio_path in batch[\"audio\"]]\n...     return batch\n\n>>> audio_dataset_amr.set_transform(decode_audio_with_pydub)\n```\n\n----------------------------------------\n\nTITLE: Installing TensorFlow\nDESCRIPTION: This command installs the TensorFlow library using pip. TensorFlow is an open-source machine learning framework for numerical computation and large-scale machine learning.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/quickstart.mdx#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install tensorflow\n```\n\n----------------------------------------\n\nTITLE: Cloning and Setting Up Remote Repository\nDESCRIPTION: This snippet shows how to clone a forked repository and add the base repository as a remote. This allows you to keep your fork up-to-date with the original repository's changes. It's the first step in preparing your local environment for contributing.\nSOURCE: https://github.com/huggingface/datasets/blob/main/CONTRIBUTING.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone git@github.com:<your Github handle>/datasets.git\ncd datasets\ngit remote add upstream https://github.com/huggingface/datasets.git\n```\n\n----------------------------------------\n\nTITLE: Dataset Review Request Template\nDESCRIPTION: This is a template message for requesting a dataset review on the Hugging Face Hub. It includes sections for describing the dataset, listing files to review, and mentioning relevant team members. The template aims to facilitate efficient collaboration and issue resolution for custom datasets.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/troubleshoot.mdx#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n# Dataset rewiew request for <Dataset name>\n\n## Description\n\n<brief description of the dataset>\n\n## Files to review\n\n- file1\n- file2\n- ...\n\ncc @lhoestq @polinaeterna @mariosasko @albertvillanova\n```\n\n----------------------------------------\n\nTITLE: Export Dataset to CSV\nDESCRIPTION: This snippet demonstrates exporting a Hugging Face Dataset to a CSV file using the `to_csv` method. The CSV file will be created in the specified path. It requires a pre-processed dataset to be exported.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/process.mdx#_snippet_32\n\nLANGUAGE: Python\nCODE:\n```\n>>> encoded_dataset.to_csv(\"path/of/my/dataset.csv\")\n```\n\n----------------------------------------\n\nTITLE: Installing Image Feature\nDESCRIPTION: Installs the optional image feature for working with image datasets. This installs additional dependencies required for image processing.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/installation.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npip install datasets[vision]\n```\n\n----------------------------------------\n\nTITLE: Loading Dataset and Feature Extractor for Audio\nDESCRIPTION: This snippet loads the `MInDS-14` dataset and the feature extractor for the `facebook/wav2vec2-base-960h` model.  It imports necessary libraries from `transformers` and `datasets`. Loading the dataset includes specifying the split and the audio configuration.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_dataset.mdx#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> from transformers import AutoFeatureExtractor\n>>> from datasets import load_dataset, Audio\n\n>>> feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n>>> dataset = load_dataset(\"PolyAI/minds14\", \"en-US\", split=\"train\")\n```\n\n----------------------------------------\n\nTITLE: Approximate Shuffling with IterableDataset.shuffle in Python\nDESCRIPTION: Demonstrates approximate shuffling for an IterableDataset using the shuffle method with a shuffle buffer for fast performance.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nmy_iterable_dataset = my_iterable_dataset.shuffle(seed=42, buffer_size=100)\nfor example in my_iterable_dataset:\n    print(example)\n    break\n```\n\n----------------------------------------\n\nTITLE: Define Image Transform in Python\nDESCRIPTION: Defines a transformation to randomly rotate the image using `torchvision.transforms.RandomRotation`. A `transforms` function is created to apply the rotation to the `pixel_values` of each image in a batch.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_dataset.mdx#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n>>> from torchvision.transforms import RandomRotation\n\n>>> rotate = RandomRotation(degrees=(0, 90))\n>>> def transforms(examples):\n...     examples[\"pixel_values\"] = [rotate(image) for image in examples[\"image\"]]\n...     return examples\n```\n\n----------------------------------------\n\nTITLE: Uploading Dataset to Hugging Face Hub\nDESCRIPTION: Uploads a local dataset folder to the Hugging Face Hub using the `huggingface_hub` library. It requires authentication with a Hugging Face account and the `huggingface_hub` library installed.  The `folder_path` specifies the local dataset directory, `repo_id` the repository name on the Hub, and `repo_type` should be set to \"dataset\".\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/document_dataset.mdx#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import HfApi\napi = HfApi()\n\napi.upload_folder(\n    folder_path=\"/path/to/local/dataset\",\n    repo_id=\"username/my-cool-dataset\",\n    repo_type=\"dataset\",\n)\n```\n\n----------------------------------------\n\nTITLE: Batch Mapping Error Example in Datasets\nDESCRIPTION: This code snippet demonstrates a common error when using batch mapping: creating an output column with a different number of rows than the original column, which results in an invalid table. The lambda function duplicates the 'a' column values leading to a mismatch in the number of rows.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/about_map_batch.mdx#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from datasets import Dataset\n>>> dataset = Dataset.from_dict({\"a\": [0, 1, 2]})\n>>> dataset.map(lambda batch: {\"b\": batch[\"a\"] * 2}, batched=True)  # new column with 6 elements: [0, 1, 2, 0, 1, 2]\n'ArrowInvalid: Column 1 named b expected length 3 but got length 6'\n```\n\n----------------------------------------\n\nTITLE: Formatting Code with make style\nDESCRIPTION: This snippet runs the `make style` command to format the code using `black` and `ruff`. This ensures that the code adheres to the project's coding style and formatting guidelines. It is crucial to run this command before submitting a pull request.\nSOURCE: https://github.com/huggingface/datasets/blob/main/CONTRIBUTING.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmake style\n```\n\n----------------------------------------\n\nTITLE: Setting up Development Environment (Simple)\nDESCRIPTION: This snippet sets up a basic development environment using pip to install the project in editable mode with code formatting dependencies. The `-e` flag allows you to make changes to the code without having to reinstall the package.\nSOURCE: https://github.com/huggingface/datasets/blob/main/CONTRIBUTING.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install -e \".[quality]\"\n```\n\n----------------------------------------\n\nTITLE: Distributed Dataset Splitting by Node\nDESCRIPTION: This snippet demonstrates how to split a dataset across multiple training nodes using `datasets.distributed.split_dataset_by_node`.  It assumes the `RANK` and `WORLD_SIZE` environment variables are set for distributed training.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_with_pytorch.mdx#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom datasets.distributed import split_dataset_by_node\n\nds = split_dataset_by_node(ds, rank=int(os.environ[\"RANK\"]), world_size=int(os.environ[\"WORLD_SIZE\"]))\n```\n\n----------------------------------------\n\nTITLE: Cast Image Column to RGB Mode in Python\nDESCRIPTION: Casts the image column of the dataset to RGB mode using the `cast_column` function. This ensures that all images in the dataset are in the RGB format, which is required by most image models. It uses the `Image` feature from the `datasets` library.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/use_dataset.mdx#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset = dataset.cast_column(\"image\", Image(mode=\"RGB\"))\n```\n\n----------------------------------------\n\nTITLE: Adding and Committing Changes\nDESCRIPTION: This snippet shows how to add changed files and commit them locally. The `git add -u` command adds all updated files to the staging area, and `git commit` creates a new commit with the staged changes.\nSOURCE: https://github.com/huggingface/datasets/blob/main/CONTRIBUTING.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ngit add -u\ngit commit\n```\n\n----------------------------------------\n\nTITLE: Loading Subsets Using `load_dataset`\nDESCRIPTION: This snippet shows how to load specific configurations (subsets) of a dataset using the `load_dataset` function from the `datasets` library. It loads both the 'main_data' and 'additional_data' configurations from the 'my_dataset_repository'.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/repository_structure.mdx#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n\nmain_data = load_dataset(\"my_dataset_repository\", \"main_data\")\nadditional_data = load_dataset(\"my_dataset_repository\", \"additional_data\")\n```\n\n----------------------------------------\n\nTITLE: Setting up Development Environment (Advanced)\nDESCRIPTION: This snippet sets up an advanced development environment using pip to install the project in editable mode with all optional dependencies for development. The `-e` flag allows you to make changes to the code without having to reinstall the package. Using `pip uninstall datasets` ensures a clean installation.\nSOURCE: https://github.com/huggingface/datasets/blob/main/CONTRIBUTING.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install -e \".[dev]\"\n```\n\n----------------------------------------\n\nTITLE: Disable PDF decoding\nDESCRIPTION: This code shows how to disable PDF decoding to access only the path/bytes of the PDF files. It calls the `decode` method on the dataset with `False` as an argument, which prevents the PDFs from being decoded as `pdfplumber` objects.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/document_load.mdx#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n>>> dataset = dataset.decode(False)\n```\n\n----------------------------------------\n\nTITLE: Installing albumentations and opencv-python\nDESCRIPTION: This command installs the albumentations library for image augmentation and opencv-python for image processing, which are required for the object detection tasks in the notebook.\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/object_detection.mdx#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -U albumentations opencv-python\n```\n\n----------------------------------------\n\nTITLE: Loading a WebDataset with load_dataset in Python\nDESCRIPTION: Loads a WebDataset using the `load_dataset` function with the \"webdataset\" data loading script, specifying the data files as a dictionary mapping splits to file paths.  Streaming mode is enabled using `streaming=True`\nSOURCE: https://github.com/huggingface/datasets/blob/main/docs/source/loading.mdx#_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\n>>> from datasets import load_dataset\n>>>\n>>> path = \"path/to/train/*.tar\"\n>>> dataset = load_dataset(\"webdataset\", data_files={\"train\": path}, split=\"train\", streaming=True)\n```"
  }
]
[
  {
    "owner": "ukplab",
    "repo": "sentence-transformers",
    "content": "TITLE: Using Cross Encoder for Text Similarity and Ranking in Python\nDESCRIPTION: Complete example showing how to load a Cross Encoder model, predict similarity scores for text pairs, and rank passages based on relevance to a query. Uses the ms-marco-MiniLM-L6-v2 model and demonstrates both direct prediction and ranking functionality.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/cross_encoder/usage/usage.rst#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import CrossEncoder\n\n# 1. Load a pre-trained CrossEncoder model\nmodel = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L6-v2\")\n\n# 2. Predict scores for a pair of sentences\nscores = model.predict([\n    (\"How many people live in Berlin?\", \"Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\"),\n    (\"How many people live in Berlin?\", \"Berlin is well known for its museums.\")\n])\n# => array([ 8.607138 , -4.3200774], dtype=float32)\n\n# 3. Rank a list of passages for a query\nquery = \"How many people live in Berlin?\"\npassages = [\n    \"Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\",\n    \"Berlin is well known for its museums.\",\n    \"In 2014, the city state Berlin had 37,368 live births (+6.6%), a record number since 1991.\",\n    \"The urban area of Berlin comprised about 4.1 million people in 2014, making it the seventh most populous urban area in the European Union.\",\n    \"The city of Paris had a population of 2,165,423 people within its administrative city limits as of January 1, 2019\",\n    \"An estimated 300,000-420,000 Muslims reside in Berlin, making up about 8-11 percent of the population.\",\n    \"Berlin is subdivided into 12 boroughs or districts (Bezirke).\",\n    \"In 2015, the total labour force in Berlin was 1.85 million.\",\n    \"In 2013 around 600,000 Berliners were registered in one of the more than 2,300 sport and fitness clubs.\",\n    \"Berlin has a yearly total of about 135 million day visitors, which puts it in third place among the most-visited city destinations in the European Union.\"\n]\nranks = model.rank(query, passages)\n\n# Print the scores\nprint(\"Query:\", query)\nfor rank in ranks:\n    print(f\"{rank['score']:.2f}\\t{passages[rank['corpus_id']]}\")\n```\n\n----------------------------------------\n\nTITLE: Installing Sentence Transformers with Pip\nDESCRIPTION: Various installation commands for different configurations using pip package manager, including default, ONNX, OpenVINO, training, and development options.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/installation.md#2025-04-08_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -U sentence-transformers\npip install -U \"sentence-transformers[onnx-gpu]\"\npip install -U \"sentence-transformers[onnx]\"\npip install -U \"sentence-transformers[openvino]\"\npip install -U \"sentence-transformers[train]\"\npip install wandb\npip install codecarbon\npip install -U \"sentence-transformers[dev]\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Search Function with BM25, Bi-Encoder, and Cross-Encoder\nDESCRIPTION: This function implements the complete search pipeline combining lexical search via BM25, semantic search via bi-encoder, and re-ranking via cross-encoder. It takes a query, retrieves relevant passages using both methods, and outputs the top results from each approach.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/retrieve_rerank/retrieve_rerank_simple_wikipedia.ipynb#2025-04-08_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# This function will search all wikipedia articles for passages that\n# answer the query\ndef search(query):\n    print(\"Input question:\", query)\n\n    ##### BM25 search (lexical search) #####\n    bm25_scores = bm25.get_scores(bm25_tokenizer(query))\n    top_n = np.argpartition(bm25_scores, -5)[-5:]\n    bm25_hits = [{'corpus_id': idx, 'score': bm25_scores[idx]} for idx in top_n]\n    bm25_hits = sorted(bm25_hits, key=lambda x: x['score'], reverse=True)\n    \n    print(\"Top-3 lexical search (BM25) hits\")\n    for hit in bm25_hits[0:3]:\n        print(\"\\t{:.3f}\\t{}\".format(hit['score'], passages[hit['corpus_id']].replace(\"\\n\", \" \")))\n\n    ##### Semantic Search #####\n    # Encode the query using the bi-encoder and find potentially relevant passages\n    question_embedding = bi_encoder.encode(query, convert_to_tensor=True)\n    question_embedding = question_embedding.cuda()\n    hits = util.semantic_search(question_embedding, corpus_embeddings, top_k=top_k)\n    hits = hits[0]  # Get the hits for the first query\n\n    ##### Re-Ranking #####\n    # Now, score all retrieved passages with the cross_encoder\n    cross_inp = [[query, passages[hit['corpus_id']]] for hit in hits]\n    cross_scores = cross_encoder.predict(cross_inp)\n\n    # Sort results by the cross-encoder scores\n    for idx in range(len(cross_scores)):\n        hits[idx]['cross-score'] = cross_scores[idx]\n\n    # Output of top-5 hits from bi-encoder\n    print(\"\\n-------------------------\\n\")\n    print(\"Top-3 Bi-Encoder Retrieval hits\")\n    hits = sorted(hits, key=lambda x: x['score'], reverse=True)\n    for hit in hits[0:3]:\n        print(\"\\t{:.3f}\\t{}\".format(hit['score'], passages[hit['corpus_id']].replace(\"\\n\", \" \")))\n\n    # Output of top-5 hits from re-ranker\n    print(\"\\n-------------------------\\n\")\n    print(\"Top-3 Cross-Encoder Re-ranker hits\")\n    hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n    for hit in hits[0:3]:\n        print(\"\\t{:.3f}\\t{}\".format(hit['cross-score'], passages[hit['corpus_id']].replace(\"\\n\", \" \")))\n```\n\n----------------------------------------\n\nTITLE: Implementing Multilingual Image Search Function in Python\nDESCRIPTION: This function performs semantic search on images using text queries in multiple languages. It encodes the query, computes cosine similarity with image embeddings, and returns top-k results.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/image-search/Image_Search-multilingual.ipynb#2025-04-08_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Next, we define a search function.\ndef search(query, k=3):\n    # First, we encode the query (which can either be an image or a text string)\n    query_emb = model.encode([query], convert_to_tensor=True, show_progress_bar=False)\n    \n    # Then, we use the util.semantic_search function, which computes the cosine-similarity\n    # between the query embedding and all image embeddings.\n    # It then returns the top_k highest ranked images, which we output\n    hits = util.semantic_search(query_emb, img_emb, top_k=k)[0]\n    \n    print(\"Query:\")\n    display(query)\n    for hit in hits:\n        print(img_names[hit['corpus_id']])\n        display(IPImage(os.path.join(img_folder, img_names[hit['corpus_id']]), width=200))\n```\n\n----------------------------------------\n\nTITLE: Performing Zero-Shot Image Classification with English Labels\nDESCRIPTION: This snippet demonstrates the core zero-shot classification process. It encodes text labels, computes cosine similarity between image and label embeddings, and predicts labels for each image.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/image-search/Image_Classification.ipynb#2025-04-08_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Then, we define our labels as text. Here, we use 4 labels\nlabels = ['dog', 'cat', 'Paris at night', 'Paris']\n\n# And compute the text embeddings for these labels\nen_emb = en_model.encode(labels, convert_to_tensor=True)\n\n# Now, we compute the cosine similarity between the images and the labels\ncos_scores = util.cos_sim(img_emb, en_emb)\n\n# Then we look which label has the highest cosine similarity with the given images\npred_labels = torch.argmax(cos_scores, dim=1)\n\n# Finally we output the images + labels\nfor img_name, pred_label in zip(img_names, pred_labels):\n    display(IPImage(img_name, width=200))\n    print(\"Predicted label:\", labels[pred_label])\n    print(\"\\n\\n\")\n```\n\n----------------------------------------\n\nTITLE: Retrieve & Re-rank on Simple Wikipedia using Sentence Transformers in Python\nDESCRIPTION: This example implements the Retrieve & Re-rank strategy for an asymmetric search task. It uses a bi-encoder to encode Wikipedia paragraphs and queries, retrieves candidates based on cosine similarity, and then uses a Cross-Encoder for re-ranking. The models are trained on the MS Marco Passage Reranking dataset.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/semantic-search/README.md#2025-04-08_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Code is referenced in retrieve_rerank_simple_wikipedia.ipynb\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using Sentence Transformers in Python\nDESCRIPTION: Demonstrates how to load a Sentence Transformer model, generate embeddings for multiple sentences, and calculate similarity scores between embeddings. Uses the all-MiniLM-L6-v2 model to create 384-dimensional embeddings.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/usage/usage.rst#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer\n\n# 1. Load a pretrained Sentence Transformer model\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\n# The sentences to encode\nsentences = [\n    \"The weather is lovely today.\",\n    \"It's so sunny outside!\",\n    \"He drove to the stadium.\",\n]\n\n# 2. Calculate embeddings by calling model.encode()\nembeddings = model.encode(sentences)\nprint(embeddings.shape)\n# [3, 384]\n\n# 3. Calculate the embedding similarities\nsimilarities = model.similarity(embeddings, embeddings)\nprint(similarities)\n# tensor([[1.0000, 0.6660, 0.1046],\n#         [0.6660, 1.0000, 0.1411],\n#         [0.1046, 0.1411, 1.0000]])\n```\n\n----------------------------------------\n\nTITLE: Training Cross Encoder Model with GooAQ Dataset\nDESCRIPTION: Comprehensive example showing how to train a Cross Encoder model using the GooAQ dataset. The script demonstrates model initialization, dataset loading, loss function configuration, evaluation setup, and model saving. It includes logging, training arguments configuration, and optional model upload to Hugging Face Hub.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/cross_encoder/training_overview.md#2025-04-08_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nimport traceback\n\nfrom datasets import load_dataset\n\nfrom sentence_transformers.cross_encoder import (\n    CrossEncoder,\n    CrossEncoderModelCardData,\n    CrossEncoderTrainer,\n    CrossEncoderTrainingArguments,\n)\nfrom sentence_transformers.cross_encoder.evaluation import CrossEncoderNanoBEIREvaluator\nfrom sentence_transformers.cross_encoder.losses import CachedMultipleNegativesRankingLoss\n\n# Set the log level to INFO to get more information\nlogging.basicConfig(format=\"%(asctime)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\", level=logging.INFO)\n\nmodel_name = \"microsoft/MiniLM-L12-H384-uncased\"\ntrain_batch_size = 64\nnum_epochs = 1\nnum_rand_negatives = 5  # How many random negatives should be used for each question-answer pair\n\n# 1a. Load a model to finetune with 1b. (Optional) model card data\nmodel = CrossEncoder(\n    model_name,\n    model_card_data=CrossEncoderModelCardData(\n        language=\"en\",\n        license=\"apache-2.0\",\n        model_name=\"MiniLM-L12-H384 trained on GooAQ\",\n    ),\n)\nprint(\"Model max length:\", model.max_length)\nprint(\"Model num labels:\", model.num_labels)\n\n# 2. Load the GooAQ dataset\nlogging.info(\"Read the gooaq training dataset\")\nfull_dataset = load_dataset(\"sentence-transformers/gooaq\", split=\"train\").select(range(100_000))\ndataset_dict = full_dataset.train_test_split(test_size=1_000, seed=12)\ntrain_dataset = dataset_dict[\"train\"]\neval_dataset = dataset_dict[\"test\"]\nlogging.info(train_dataset)\nlogging.info(eval_dataset)\n\n# 3. Define our training loss\nloss = CachedMultipleNegativesRankingLoss(\n    model=model,\n    num_negatives=num_rand_negatives,\n    mini_batch_size=32,\n)\n\n# 4. Use CrossEncoderNanoBEIREvaluator\nevaluator = CrossEncoderNanoBEIREvaluator(\n    dataset_names=[\"msmarco\", \"nfcorpus\", \"nq\"],\n    batch_size=train_batch_size,\n)\nevaluator(model)\n\n# 5. Define the training arguments\nshort_model_name = model_name if \"/\" not in model_name else model_name.split(\"/\")[-1]\nrun_name = f\"reranker-{short_model_name}-gooaq-cmnrl\"\nargs = CrossEncoderTrainingArguments(\n    output_dir=f\"models/{run_name}\",\n    num_train_epochs=num_epochs,\n    per_device_train_batch_size=train_batch_size,\n    per_device_eval_batch_size=train_batch_size,\n    learning_rate=2e-5,\n    warmup_ratio=0.1,\n    fp16=False,\n    bf16=True,\n    eval_strategy=\"steps\",\n    eval_steps=100,\n    save_strategy=\"steps\",\n    save_steps=100,\n    save_total_limit=2,\n    logging_steps=50,\n    logging_first_step=True,\n    run_name=run_name,\n    seed=12,\n)\n\n# 6. Create the trainer & start training\ntrainer = CrossEncoderTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    loss=loss,\n    evaluator=evaluator,\n)\ntrainer.train()\n\n# 7. Evaluate the final model\nevaluator(model)\n\n# 8. Save the final model\nfinal_output_dir = f\"models/{run_name}/final\"\nmodel.save_pretrained(final_output_dir)\n\n# 9. (Optional) save the model to the Hugging Face Hub\ntry:\n    model.push_to_hub(run_name)\nexcept Exception:\n    logging.error(\n        f\"Error uploading model to the Hugging Face Hub:\\n{traceback.format_exc()}To upload it manually, you can run \"\n        f\"`huggingface-cli login`, followed by loading the model using `model = CrossEncoder({final_output_dir!r})` \"\n        f\"and saving it using `model.push_to_hub('{run_name}')`.\"\n    )\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using Sentence Transformer Model in Python\nDESCRIPTION: This snippet demonstrates how to load a pretrained Sentence Transformer model, encode sentences to calculate embeddings, and compute similarities between the embeddings.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/computing-embeddings/README.rst#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer\n\n# 1. Load a pretrained Sentence Transformer model\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\n# The sentences to encode\nsentences = [\n    \"The weather is lovely today.\",\n    \"It's so sunny outside!\",\n    \"He drove to the stadium.\",\n]\n\n# 2. Calculate embeddings by calling model.encode()\nembeddings = model.encode(sentences)\nprint(embeddings.shape)\n# [3, 384]\n\n# 3. Calculate the embedding similarities\nsimilarities = model.similarity(embeddings, embeddings)\nprint(similarities)\n# tensor([[1.0000, 0.6660, 0.1046],\n#         [0.6660, 1.0000, 0.1411],\n#         [0.1046, 0.1411, 1.0000]])\n```\n\n----------------------------------------\n\nTITLE: Initializing Bi-Encoder, Cross-Encoder and Loading Wikipedia Dataset\nDESCRIPTION: This code initializes the bi-encoder for retrieving relevant passages and cross-encoder for re-ranking. It then loads the Simple Wikipedia dataset from a gzipped JSON file, extracts the first paragraph from each article, and encodes all passages into vector embeddings.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/retrieve_rerank/retrieve_rerank_simple_wikipedia.ipynb#2025-04-08_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom sentence_transformers import SentenceTransformer, CrossEncoder, util\nimport gzip\nimport os\nimport torch\n\nif not torch.cuda.is_available():\n    print(\"Warning: No GPU found. Please add GPU to your notebook\")\n\n\n#We use the Bi-Encoder to encode all passages, so that we can use it with semantic search\nbi_encoder = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\nbi_encoder.max_seq_length = 256     #Truncate long passages to 256 tokens\ntop_k = 32                          #Number of passages we want to retrieve with the bi-encoder\n\n#The bi-encoder will retrieve 100 documents. We use a cross-encoder, to re-rank the results list to improve the quality\ncross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L6-v2')\n\n# As dataset, we use Simple English Wikipedia. Compared to the full English wikipedia, it has only\n# about 170k articles. We split these articles into paragraphs and encode them with the bi-encoder\n\nwikipedia_filepath = 'simplewiki-2020-11-01.jsonl.gz'\n\nif not os.path.exists(wikipedia_filepath):\n    util.http_get('http://sbert.net/datasets/simplewiki-2020-11-01.jsonl.gz', wikipedia_filepath)\n\npassages = []\nwith gzip.open(wikipedia_filepath, 'rt', encoding='utf8') as fIn:\n    for line in fIn:\n        data = json.loads(line.strip())\n\n        #Add all paragraphs\n        #passages.extend(data['paragraphs'])\n\n        #Only add the first paragraph\n        passages.append(data['paragraphs'][0])\n\nprint(\"Passages:\", len(passages))\n\n# We encode all passages into our vector space. This takes about 5 minutes (depends on your GPU speed)\ncorpus_embeddings = bi_encoder.encode(passages, convert_to_tensor=True, show_progress_bar=True)\n```\n\n----------------------------------------\n\nTITLE: Using float16 (fp16) Precision with PyTorch Backend\nDESCRIPTION: This snippet shows how to use float16 (half precision) with Sentence Transformers to speed up inference on GPUs at minimal accuracy loss. You can specify the precision during model initialization or convert an existing model.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/usage/efficiency.rst#2025-04-08_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\", model_kwargs={\"torch_dtype\": \"float16\"})\n# or: model.half()\n\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\nembeddings = model.encode(sentences)\n```\n\n----------------------------------------\n\nTITLE: Using Cross Encoder for Text Pair Similarity Scoring in Python\nDESCRIPTION: This snippet shows how to use a Cross Encoder model for ranking and scoring text pairs. It loads the 'stsb-distilroberta-base' model, ranks a list of sentences against a query, and demonstrates manual scoring of sentence pairs.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/quickstart.rst#2025-04-08_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers.cross_encoder import CrossEncoder\n\n# 1. Load a pretrained CrossEncoder model\nmodel = CrossEncoder(\"cross-encoder/stsb-distilroberta-base\")\n\n# We want to compute the similarity between the query sentence...\nquery = \"A man is eating pasta.\"\n\n# ... and all sentences in the corpus\ncorpus = [\n    \"A man is eating food.\",\n    \"A man is eating a piece of bread.\",\n    \"The girl is carrying a baby.\",\n    \"A man is riding a horse.\",\n    \"A woman is playing violin.\",\n    \"Two men pushed carts through the woods.\",\n    \"A man is riding a white horse on an enclosed ground.\",\n    \"A monkey is playing drums.\",\n    \"A cheetah is running behind its prey.\",\n]\n\n# 2. We rank all sentences in the corpus for the query\nranks = model.rank(query, corpus)\n\n# Print the scores\nprint(\"Query: \", query)\nfor rank in ranks:\n    print(f\"{rank['score']:.2f}\\t{corpus[rank['corpus_id']]}\")\n\"\"\"\nQuery:  A man is eating pasta.\n0.67    A man is eating food.\n0.34    A man is eating a piece of bread.\n0.08    A man is riding a horse.\n0.07    A man is riding a white horse on an enclosed ground.\n0.01    The girl is carrying a baby.\n0.01    Two men pushed carts through the woods.\n0.01    A monkey is playing drums.\n0.01    A woman is playing violin.\n0.01    A cheetah is running behind its prey.\n\"\"\"\n\n# 3. Alternatively, you can also manually compute the score between two sentences\nimport numpy as np\n\nsentence_combinations = [[query, sentence] for sentence in corpus]\nscores = model.predict(sentence_combinations)\n\n# Sort the scores in decreasing order to get the corpus indices\nranked_indices = np.argsort(scores)[::-1]\nprint(\"Scores:\", scores)\nprint(\"Indices:\", ranked_indices)\n\"\"\"\nScores: [0.6732372, 0.34102544, 0.00542465, 0.07569341, 0.00525378, 0.00536814, 0.06676237, 0.00534825, 0.00516717]\nIndices: [0 1 3 6 2 5 7 4 8]\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Semantic Search on Wikipedia Q&A using Sentence Transformers in Python\nDESCRIPTION: This example uses a model trained on the Natural Questions dataset to perform semantic search on Simple English Wikipedia. It demonstrates an asymmetric search task where the model encodes search queries and Wikipedia passages to find relevant answers.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/semantic-search/README.md#2025-04-08_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Code is referenced in semantic_search_wikipedia_qa.py\n```\n\n----------------------------------------\n\nTITLE: Using SentenceTransformer for Semantic Search with MSMARCO Model in Python\nDESCRIPTION: This code snippet demonstrates how to use the SentenceTransformer library with a pre-trained MSMARCO model to perform semantic search. It encodes a query and passages, then calculates similarity scores using dot product.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/pretrained-models/msmarco-v5.md#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer, util\n\nmodel = SentenceTransformer(\"msmarco-distilbert-dot-v5\")\n\nquery_embedding = model.encode(\"How big is London\")\npassage_embedding = model.encode([\n    \"London has 9,787,426 inhabitants at the 2011 census\",\n    \"London is known for its financial district\",\n])\n\nprint(\"Similarity:\", util.dot_score(query_embedding, passage_embedding))\n```\n\n----------------------------------------\n\nTITLE: Training NLI with SoftmaxLoss in Python\nDESCRIPTION: This snippet uses SoftmaxLoss for training on NLI data as described in the original Sentence Transformers paper. It implements a siamese network approach for learning sentence representations.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/training/nli/README.md#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntraining_nli.py\n```\n\n----------------------------------------\n\nTITLE: Loading Sentence Transformer Models in Python\nDESCRIPTION: This snippet shows different ways to load a Sentence Transformer model, including using a pretrained model name, a local model directory, and specifying a device.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/computing-embeddings/README.rst#2025-04-08_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\"all-mpnet-base-v2\")\n# Alternatively, you can pass a path to a local model directory:\nmodel = SentenceTransformer(\"output/models/mpnet-base-finetuned-all-nli\")\n\n# Specifying the device explicitly\nmodel = SentenceTransformer(\"all-mpnet-base-v2\", device=\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Using a Semantic Search Model for Query-Passage Matching in Python\nDESCRIPTION: Shows how to implement semantic search using a specialized Sentence Transformer model. This example loads the multi-qa-mpnet-base-cos-v1 model, encodes a search query and multiple passages, then calculates similarity scores to find the most relevant passage.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/pretrained_models.md#2025-04-08_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\"multi-qa-mpnet-base-cos-v1\")\n\nquery_embedding = model.encode(\"How big is London\")\npassage_embeddings = model.encode([\n    \"London is known for its financial district\",\n    \"London has 9,787,426 inhabitants at the 2011 census\",\n    \"The United Kingdom is the fourth largest exporter of goods in the world\",\n])\n\nsimilarity = model.similarity(query_embedding, passage_embeddings)\n# => tensor([[0.4659, 0.6142, 0.2697]])\n```\n\n----------------------------------------\n\nTITLE: Optimizing Semantic Search with GPU and Normalization in Python\nDESCRIPTION: This code snippet demonstrates how to optimize semantic search by moving embeddings to GPU and normalizing them. It uses the util.semantic_search function with dot product scoring for improved performance.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/semantic-search/README.md#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncorpus_embeddings = corpus_embeddings.to(\"cuda\")\ncorpus_embeddings = util.normalize_embeddings(corpus_embeddings)\n\nquery_embeddings = query_embeddings.to(\"cuda\")\nquery_embeddings = util.normalize_embeddings(query_embeddings)\nhits = util.semantic_search(query_embeddings, corpus_embeddings, score_function=util.dot_score)\n```\n\n----------------------------------------\n\nTITLE: Loading the CLIP Model with SentenceTransformer in Python\nDESCRIPTION: Imports necessary libraries and loads the CLIP-ViT-B-32 model from SentenceTransformer. This model is trained to map both images and text to the same vector space, enabling cross-modal similarity searches.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/image-search/Image_Search.ipynb#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer, util\nfrom PIL import Image\nimport glob\nimport torch\nimport pickle\nimport zipfile\nfrom IPython.display import display\nfrom IPython.display import Image as IPImage\nimport os\nfrom tqdm.autonotebook import tqdm\ntorch.set_num_threads(4)\n\n\n\n#First, we load the respective CLIP model\nmodel = SentenceTransformer('clip-ViT-B-32')\n```\n\n----------------------------------------\n\nTITLE: Using CLIP Model for Image and Text Embeddings in Python\nDESCRIPTION: This code demonstrates how to use the SentenceTransformer implementation of the CLIP model to encode both images and text into the same vector space, then compute similarity between them. It loads the pre-trained CLIP model, encodes an image file and multiple text descriptions, and computes similarity scores between the image and text embeddings.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/image-search/README.md#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer\nfrom PIL import Image\n\n# Load CLIP model\nmodel = SentenceTransformer(\"clip-ViT-B-32\")\n\n# Encode an image:\nimg_emb = model.encode(Image.open(\"two_dogs_in_snow.jpg\"))\n\n# Encode text descriptions\ntext_emb = model.encode(\n    [\"Two dogs in the snow\", \"A cat on a table\", \"A picture of London at night\"]\n)\n\n# Compute similarities\nsimilarity_scores = model.similarity(img_emb, text_emb)\nprint(similarity_scores)\n```\n\n----------------------------------------\n\nTITLE: Using Cross-Encoders in Python with SentenceTransformers\nDESCRIPTION: Demonstrates how to initialize a Cross-Encoder model and predict similarity scores for sentence pairs. The code shows using the ms-marco-MiniLM-L6-v2 model to score multiple sentence pairs simultaneously.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/cross_encoder/applications/README.md#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers.cross_encoder import CrossEncoder\n\nmodel = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L6-v2\")\nscores = model.predict([[\"My first\", \"sentence pair\"], [\"Second text\", \"pair\"]])\n```\n\n----------------------------------------\n\nTITLE: Setting Up BM25 Lexical Search with Text Tokenization\nDESCRIPTION: This code implements the BM25 lexical search algorithm by tokenizing the text corpus. It preprocesses the passages by lowercasing, removing punctuation and stop words, which is a standard approach for keyword-based search systems.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/retrieve_rerank/retrieve_rerank_simple_wikipedia.ipynb#2025-04-08_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom rank_bm25 import BM25Okapi\nfrom sklearn.feature_extraction import _stop_words\nimport string\nfrom tqdm.autonotebook import tqdm\nimport numpy as np\n\n\n# We lower case our text and remove stop-words from indexing\ndef bm25_tokenizer(text):\n    tokenized_doc = []\n    for token in text.lower().split():\n        token = token.strip(string.punctuation)\n\n        if len(token) > 0 and token not in _stop_words.ENGLISH_STOP_WORDS:\n            tokenized_doc.append(token)\n    return tokenized_doc\n\n\ntokenized_corpus = []\nfor passage in tqdm(passages):\n    tokenized_corpus.append(bm25_tokenizer(passage))\n\nbm25 = BM25Okapi(tokenized_corpus)\n```\n\n----------------------------------------\n\nTITLE: Loading and Using MS MARCO Cross Encoder Model\nDESCRIPTION: Example showing how to load a MS MARCO Cross Encoder model and use it to predict relevance scores between queries and text passages. Uses torch.nn.Sigmoid() activation function to output scores between 0 and 1.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/cross_encoder/pretrained_models.md#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import CrossEncoder\nimport torch\n\n# Load https://huggingface.co/cross-encoder/ms-marco-MiniLM-L6-v2\nmodel = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L6-v2\", activation_fn=torch.nn.Sigmoid())\nscores = model.predict([\n    (\"How many people live in Berlin?\", \"Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\"),\n    (\"How many people live in Berlin?\", \"Berlin is well known for its museums.\")\n])\n# => array([0.9998173 , 0.01312432], dtype=float32)\n```\n\n----------------------------------------\n\nTITLE: Training NLI with GISTEmbedLoss in Python\nDESCRIPTION: This script implements the GISTEmbedLoss for NLI training, which modifies the in-batch negative selection using a guiding model. It provides a stronger training signal at the cost of some overhead for running inference on the guiding model.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/training/nli/README.md#2025-04-08_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntraining_nli_v3.py\n```\n\n----------------------------------------\n\nTITLE: Advanced Similarity Calculations with Different Metrics\nDESCRIPTION: Comprehensive example showing how to calculate similarities between embeddings using different similarity metrics, including switching between metrics at runtime and comparing the results.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/usage/semantic_textual_similarity.rst#2025-04-08_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer, SimilarityFunction\n\n# Load a pretrained Sentence Transformer model\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\n# Embed some sentences\nsentences = [\n    \"The weather is lovely today.\",\n    \"It's so sunny outside!\",\n    \"He drove to the stadium.\",\n]\nembeddings = model.encode(sentences)\n\nsimilarities = model.similarity(embeddings, embeddings)\nprint(similarities)\n\n# Change the similarity function to Manhattan distance\nmodel.similarity_fn_name = SimilarityFunction.MANHATTAN\nprint(model.similarity_fn_name)\n\nsimilarities = model.similarity(embeddings, embeddings)\nprint(similarities)\n```\n\n----------------------------------------\n\nTITLE: Using bfloat16 (bf16) Precision with PyTorch Backend\nDESCRIPTION: This snippet demonstrates how to use bfloat16 precision with Sentence Transformers, which preserves more of the original accuracy compared to fp16 while still providing speed improvements.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/usage/efficiency.rst#2025-04-08_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\", model_kwargs={\"torch_dtype\": \"bfloat16\"})\n# or: model.bfloat16()\n\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\nembeddings = model.encode(sentences)\n```\n\n----------------------------------------\n\nTITLE: Training SimCSE Model with SentenceTransformers in Python\nDESCRIPTION: This code snippet demonstrates how to train a SimCSE model using SentenceTransformers. It includes setting up the model architecture, preparing training data, configuring the loss function, and training the model.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/unsupervised_learning/SimCSE/README.md#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer, InputExample\nfrom sentence_transformers import models, losses\nfrom torch.utils.data import DataLoader\n\n# Define your sentence transformer model using CLS pooling\nmodel_name = \"distilroberta-base\"\nword_embedding_model = models.Transformer(model_name, max_seq_length=32)\npooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\nmodel = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n\n# Define a list with sentences (1k - 100k sentences)\ntrain_sentences = [\n    \"Your set of sentences\",\n    \"Model will automatically add the noise\",\n    \"And re-construct it\",\n    \"You should provide at least 1k sentences\",\n]\n\n# Convert train sentences to sentence pairs\ntrain_data = [InputExample(texts=[s, s]) for s in train_sentences]\n\n# DataLoader to batch your data\ntrain_dataloader = DataLoader(train_data, batch_size=128, shuffle=True)\n\n# Use the denoising auto-encoder loss\ntrain_loss = losses.MultipleNegativesRankingLoss(model)\n\n# Call the fit method\nmodel.fit(\n    train_objectives=[(train_dataloader, train_loss)], epochs=1, show_progress_bar=True\n)\n\nmodel.save(\"output/simcse-model\")\n```\n\n----------------------------------------\n\nTITLE: Configuring SentenceTransformer Training Arguments\nDESCRIPTION: Shows the initialization of training arguments with common parameters including batch sizes, learning rates, and optimization settings. Includes settings for model saving, evaluation, and performance tracking.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/training_overview.md#2025-04-08_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nargs = SentenceTransformerTrainingArguments(\n    # Required parameter:\n    output_dir=\"models/mpnet-base-all-nli-triplet\",\n    # Optional training parameters:\n    num_train_epochs=1,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    learning_rate=2e-5,\n    warmup_ratio=0.1,\n    fp16=True,  # Set to False if you get an error that your GPU can't run on FP16\n    bf16=False,  # Set to True if you have a GPU that supports BF16\n    batch_sampler=BatchSamplers.NO_DUPLICATES,  # losses that use \"in-batch negatives\" benefit from no duplicates\n    # Optional tracking/debugging parameters:\n    eval_strategy=\"steps\",\n    eval_steps=100,\n    save_strategy=\"steps\",\n    save_steps=100,\n    save_total_limit=2,\n    logging_steps=100,\n    run_name=\"mpnet-base-all-nli-triplet\",  # Will be used in W&B if `wandb` is installed\n)\n```\n\n----------------------------------------\n\nTITLE: Basic Usage of Sentence Transformers with PyTorch Backend\nDESCRIPTION: This snippet demonstrates the default usage of Sentence Transformers with the PyTorch backend, which automatically selects the strongest available hardware option across CUDA, MPS, or CPU.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/usage/efficiency.rst#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\nembeddings = model.encode(sentences)\n```\n\n----------------------------------------\n\nTITLE: Basic Semantic Similarity Calculation with Sentence Transformers\nDESCRIPTION: Demonstrates how to compute semantic similarity between two lists of sentences using the Sentence Transformers library. The code loads a pre-trained model, generates embeddings for text pairs, and calculates cosine similarities between them.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/usage/semantic_textual_similarity.rst#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\n# Two lists of sentences\nsentences1 = [\n    \"The new movie is awesome\",\n    \"The cat sits outside\",\n    \"A man is playing guitar\",\n]\n\nsentences2 = [\n    \"The dog plays in the garden\",\n    \"The new movie is so great\",\n    \"A woman watches TV\",\n]\n\n# Compute embeddings for both lists\nembeddings1 = model.encode(sentences1)\nembeddings2 = model.encode(sentences2)\n\n# Compute cosine similarities\nsimilarities = model.similarity(embeddings1, embeddings2)\n\n# Output the pairs with their score\nfor idx_i, sentence1 in enumerate(sentences1):\n    print(sentence1)\n    for idx_j, sentence2 in enumerate(sentences2):\n        print(f\" - {sentence2: <30}: {similarities[idx_i][idx_j]:.4f}\")\n```\n\n----------------------------------------\n\nTITLE: Initializing SentenceTransformer with CLIP Model for English\nDESCRIPTION: This snippet sets up the SentenceTransformer using the CLIP model for English text and image embeddings. It imports necessary libraries and initializes the model.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/image-search/Image_Classification.ipynb#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer, util\nfrom PIL import Image\nimport glob\nimport torch\nimport pickle\nimport zipfile\nfrom IPython.display import display\nfrom IPython.display import Image as IPImage\nimport os\nfrom tqdm.autonotebook import tqdm\nimport torch\n\n# We use the original CLIP model for computing image embeddings and English text embeddings\nen_model = SentenceTransformer('clip-ViT-B-32')\n```\n\n----------------------------------------\n\nTITLE: Project Structure Overview in Markdown\nDESCRIPTION: A comprehensive markdown overview of the SentenceTransformers application folders and use cases. It describes various example implementations including computing embeddings, clustering, cross-encoder usage, parallel sentence mining, paraphrase mining, semantic search, retrieve & rerank operations, image search, and text summarization.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/README.md#2025-04-08_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Applications\n\nSentenceTransformers can be used for various use-cases. In these folders, you find several example scripts that show case how SentenceTransformers can be used\n\n## Computing Embeddings\n\nThe [computing-embeddings](computing-embeddings/) folder contains examples how to compute sentence embeddings using SentenceTransformers.\n\n## Clustering\n\nThe [clustering](clustering/) folder shows how SentenceTransformers can be used for text clustering, i.e., grouping sentences together based on their similarity.\n\n## Cross-Encoder\n\nSentenceTransformers also support training and inference of [Cross-Encoders](cross-encoder/). There, two sentences are presented simultaneously to the transformer network and a score (0...1) is derived indicating the similarity or a label.\n\n## Parallel Sentence Mining\n\nThe [parallel-sentence-mining](parallel-sentence-mining/) folder contains examples of how parallel (translated) sentences can be found in two corpora of different languages. For example, you take the English and the Spanish Wikipedia and the script finds and returns all translated English-Spanish sentence pairs.\n\n## Paraphrase Mining\n\nThe [paraphrase-mining](paraphrase-mining/) folder contains examples to find all paraphrase sentences in a large set of sentences. The example can be used to find e.g. duplicate questions or duplicate sentences in a set of Millions of questions / sentences.\n\n## Semantic Search\n\nThe [semantic-search](semantic-search/) folder shows examples for semantic search: Given a sentence, find in a large collection semantically similar sentences.\n\n## Retrieve & Rerank\n\nThe [retrieve_rerank](retrieve_rerank/) folder shows how to combine a bi-encoder for semantic search retrieval and a more powerful re-ranking stage with a cross-encoder.\n\n## Image Search\n\nThe [image-search](image-search/) folder shows how to use the image&text-models, which can map images and text to the same vector space. This allows for an image search given a user query.\n\n## Text Summarization\n\nThe [text-summarization](text-summarization/) folder shows how SentenceTransformers can be used for extractive summarization: Give a long document, find the k sentences that give a good and short summary of the content.\n```\n\n----------------------------------------\n\nTITLE: Implementing Bi-Encoder for Document Retrieval\nDESCRIPTION: Example code showing how to use SentenceTransformer to create embeddings for documents and queries using a pre-trained bi-encoder model.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/retrieve_rerank/README.md#2025-04-08_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\"multi-qa-mpnet-base-dot-v1\")\n\ndocs = [\n    \"My first paragraph. That contains information\",\n    \"Python is a programming language.\",\n]\ndocument_embeddings = model.encode(docs)\n\nquery = \"What is Python?\"\nquery_embedding = model.encode(query)\n```\n\n----------------------------------------\n\nTITLE: Using Sentence Transformer Embedding Model\nDESCRIPTION: Example showing how to load a pretrained embedding model and generate embeddings for text sentences.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/README.md#2025-04-08_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\")\n```\n\nLANGUAGE: python\nCODE:\n```\nsentences = [\n    \"The weather is lovely today.\",\n    \"It's so sunny outside!\",\n    \"He drove to the stadium.\",\n]\nembeddings = model.encode(sentences)\nprint(embeddings.shape)\n# => (3, 384)\n```\n\nLANGUAGE: python\nCODE:\n```\nsimilarities = model.similarity(embeddings, embeddings)\nprint(similarities)\n# tensor([[1.0000, 0.6660, 0.1046],\n#         [0.6660, 1.0000, 0.1411],\n#         [0.1046, 0.1411, 1.0000]])\n```\n\n----------------------------------------\n\nTITLE: Loading and Using a Sentence Transformer Model in Python\nDESCRIPTION: Demonstrates how to load a pretrained Sentence Transformer model, generate embeddings for multiple sentences, and calculate similarities between them. Uses the all-mpnet-base-v2 model which is a general purpose model from the Sentence Transformers organization.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/pretrained_models.md#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer\n\n# Load https://huggingface.co/sentence-transformers/all-mpnet-base-v2\nmodel = SentenceTransformer(\"all-mpnet-base-v2\")\nembeddings = model.encode([\n    \"The weather is lovely today.\",\n    \"It's so sunny outside!\",\n    \"He drove to the stadium.\",\n])\nsimilarities = model.similarity(embeddings, embeddings)\n```\n\n----------------------------------------\n\nTITLE: Implementing Paraphrase Mining with SentenceTransformer\nDESCRIPTION: Demonstrates how to use the paraphrase_mining function from sentence_transformers to find similar sentence pairs in a collection. The code shows basic usage with a small set of example sentences, including model initialization and result processing.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/paraphrase-mining/README.md#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer\nfrom sentence_transformers.util import paraphrase_mining\n\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\n# Single list of sentences - Possible tens of thousands of sentences\nsentences = [\n    \"The cat sits outside\",\n    \"A man is playing guitar\",\n    \"I love pasta\",\n    \"The new movie is awesome\",\n    \"The cat plays in the garden\",\n    \"A woman watches TV\",\n    \"The new movie is so great\",\n    \"Do you like pizza?\",\n]\n\nparaphrases = paraphrase_mining(model, sentences)\n\nfor paraphrase in paraphrases[0:10]:\n    score, i, j = paraphrase\n    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences[i], sentences[j], score))\n```\n\n----------------------------------------\n\nTITLE: Using STSbenchmark Cross Encoder Model\nDESCRIPTION: Demonstrates how to use a STSbenchmark Cross Encoder model to compute semantic similarity scores between pairs of sentences. Returns scores between 0 and 1.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/cross_encoder/pretrained_models.md#2025-04-08_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import CrossEncoder\n\nmodel = CrossEncoder(\"cross-encoder/stsb-roberta-base\")\nscores = model.predict([(\"It's a wonderful day outside.\", \"It's so sunny today!\"), (\"It's a wonderful day outside.\", \"He drove to work earlier.\")])\n# => array([0.60443085, 0.00240758], dtype=float32)\n```\n\n----------------------------------------\n\nTITLE: Using Sentence Transformers for Text Embedding and Similarity Calculation in Python\nDESCRIPTION: This snippet demonstrates how to load a pre-trained Sentence Transformer model, encode sentences into embeddings, and calculate similarities between the embeddings. It uses the 'all-MiniLM-L6-v2' model to process a list of sentences.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/quickstart.rst#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer\n\n# 1. Load a pretrained Sentence Transformer model\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\n# The sentences to encode\nsentences = [\n    \"The weather is lovely today.\",\n    \"It's so sunny outside!\",\n    \"He drove to the stadium.\",\n]\n\n# 2. Calculate embeddings by calling model.encode()\nembeddings = model.encode(sentences)\nprint(embeddings.shape)\n# [3, 384]\n\n# 3. Calculate the embedding similarities\nsimilarities = model.similarity(embeddings, embeddings)\nprint(similarities)\n# tensor([[1.0000, 0.6660, 0.1046],\n#         [0.6660, 1.0000, 0.1411],\n#         [0.1046, 0.1411, 1.0000]])\n```\n\n----------------------------------------\n\nTITLE: Searching for US Capital Information\nDESCRIPTION: This code executes a search query about the capital of the United States using the implemented search function. It demonstrates how the system handles factual queries.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/retrieve_rerank/retrieve_rerank_simple_wikipedia.ipynb#2025-04-08_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nsearch(query = \"What is the capital of the United States?\")\n```\n\n----------------------------------------\n\nTITLE: Modifying Similarity Function on Existing Model\nDESCRIPTION: Demonstrates how to change the similarity function on an existing Sentence Transformer model instance by directly setting the similarity_fn_name property.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/usage/semantic_textual_similarity.rst#2025-04-08_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer, SimilarityFunction\n\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\")\nmodel.similarity_fn_name = SimilarityFunction.DOT_PRODUCT\n```\n\n----------------------------------------\n\nTITLE: Training NLI with MultipleNegativesRankingLoss in Python\nDESCRIPTION: This code uses MultipleNegativesRankingLoss for NLI training, which provides better performance than SoftmaxLoss. It uses triplets of (anchor, entailment_sentence, contradiction_sentence) for training.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/training/nli/README.md#2025-04-08_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntraining_nli_v2.py\n```\n\n----------------------------------------\n\nTITLE: Using NLI Cross Encoder Model\nDESCRIPTION: Shows how to use a Natural Language Inference (NLI) Cross Encoder model to determine the relationship between sentence pairs as contradiction, entailment, or neutral.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/cross_encoder/pretrained_models.md#2025-04-08_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import CrossEncoder\n\nmodel = CrossEncoder(\"cross-encoder/nli-deberta-v3-base\")\nscores = model.predict([\n    (\"A man is eating pizza\", \"A man eats something\"),\n    (\"A black race car starts up in front of a crowd of people.\", \"A man is driving down a lonely road.\")\n])\n\n# Convert scores to labels\nlabel_mapping = [\"contradiction\", \"entailment\", \"neutral\"]\nlabels = [label_mapping[score_max] for score_max in scores.argmax(axis=1)]\n# => ['entailment', 'contradiction']\n```\n\n----------------------------------------\n\nTITLE: Searching for Paris Eiffel Tower Information\nDESCRIPTION: This code executes a search query about the Eiffel Tower in Paris. It demonstrates how the system handles landmark-related queries.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/retrieve_rerank/retrieve_rerank_simple_wikipedia.ipynb#2025-04-08_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nsearch(query = \"Paris eiffel tower\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Semantic Search Function for Image and Text in Python\nDESCRIPTION: Defines a search function that can accept either text queries or image inputs and returns the most similar images from the dataset. Uses cosine similarity between embeddings to rank results and displays them inline.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/image-search/Image_Search.ipynb#2025-04-08_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Next, we define a search function.\ndef search(query, k=3):\n    # First, we encode the query (which can either be an image or a text string)\n    query_emb = model.encode([query], convert_to_tensor=True, show_progress_bar=False)\n    \n    # Then, we use the util.semantic_search function, which computes the cosine-similarity\n    # between the query embedding and all image embeddings.\n    # It then returns the top_k highest ranked images, which we output\n    hits = util.semantic_search(query_emb, img_emb, top_k=k)[0]\n    \n    print(\"Query:\")\n    display(query)\n    for hit in hits:\n        print(img_names[hit['corpus_id']])\n        display(IPImage(os.path.join(img_folder, img_names[hit['corpus_id']]), width=200))\n```\n\n----------------------------------------\n\nTITLE: Implementing MatryoshkaLoss with CoSENTLoss in Sentence Transformers\nDESCRIPTION: Example of how to implement MatryoshkaLoss with CoSENTLoss for training embeddings that perform well at multiple dimensions. This approach applies the same loss function to different truncated versions of the embeddings.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/training/matryoshka/README.md#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer\nfrom sentence_transformers.losses import CoSENTLoss, MatryoshkaLoss\n\nmodel = SentenceTransformer(\"microsoft/mpnet-base\")\n\nbase_loss = CoSENTLoss(model=model)\nloss = MatryoshkaLoss(model=model, loss=base_loss, matryoshka_dims=[768, 512, 256, 128, 64])\n```\n\n----------------------------------------\n\nTITLE: Converting a Model to ONNX Format\nDESCRIPTION: This snippet shows how to convert a Sentence Transformers model to ONNX format for faster inference. The model is automatically converted if an ONNX version isn't already available.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/usage/efficiency.rst#2025-04-08_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\", backend=\"onnx\")\n\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\nembeddings = model.encode(sentences)\n```\n\n----------------------------------------\n\nTITLE: Using Cross Encoder Models for Text Similarity Scoring\nDESCRIPTION: Example of loading a pretrained Cross Encoder model, predicting similarity scores for text pairs, and ranking passages for a given query.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/index.rst#2025-04-08_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import CrossEncoder\n\n# 1. Load a pretrained CrossEncoder model\nmodel = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L6-v2\")\n\n# The texts for which to predict similarity scores\nquery = \"How many people live in Berlin?\"\npassages = [\n    \"Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\",\n    \"Berlin has a yearly total of about 135 million day visitors, making it one of the most-visited cities in the European Union.\",\n    \"In 2013 around 600,000 Berliners were registered in one of the more than 2,300 sport and fitness clubs.\",\n]\n\n# 2a. Either predict scores pairs of texts\nscores = model.predict([(query, passage) for passage in passages])\nprint(scores)\n# => [8.607139 5.506266 6.352977]\n\n# 2b. Or rank a list of passages for a query\nranks = model.rank(query, passages, return_documents=True)\n\nprint(\"Query:\", query)\nfor rank in ranks:\n    print(f\"- #{rank['corpus_id']} ({rank['score']:.2f}): {rank['text']}\")\n\"\"\"\nQuery: How many people live in Berlin?\n- #0 (8.61): Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\n- #2 (6.35): In 2013 around 600,000 Berliners were registered in one of the more than 2,300 sport and fitness clubs.\n- #1 (5.51): Berlin has a yearly total of about 135 million day visitors, making it one of the most-visited cities in the European Union.\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Example Output from Fast Clustering Algorithm\nDESCRIPTION: Sample output showing clustered Quora duplicate questions, demonstrating how the fast clustering algorithm groups similar questions together based on cosine similarity.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/clustering/README.md#2025-04-08_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nCluster 1, #83 Elements\n         What should I do to improve my English ?\n         What should I do to improve my spoken English?\n         Can I improve my English?\n         ...\n\nCluster 2, #79 Elements\n         How can I earn money online?\n         How do I earn money online?\n         Can I earn money online?\n         ...\n       \n...\n\nCluster 47, #25 Elements\n         What are some mind-blowing Mobile gadgets that exist that most people don't know about?\n         What are some mind-blowing gadgets and technologies that exist that most people don't know about?\n         What are some mind-blowing mobile technology tools that exist that most people don't know about?\n         ...\n```\n\n----------------------------------------\n\nTITLE: Performing Inference with Pre-trained CrossEncoder STS Model\nDESCRIPTION: Demonstrates how to use a pre-trained CrossEncoder model to predict similarity scores for sentence pairs.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/cross_encoder/training/sts/README.md#2025-04-08_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import CrossEncoder\n\nmodel = CrossEncoder(\"cross-encoder/stsb-roberta-base\")\nscores = model.predict([(\"It's a wonderful day outside.\", \"It's so sunny today!\"), (\"It's a wonderful day outside.\", \"He drove to work earlier.\")])\n# => array([0.60443085, 0.00240758], dtype=float32)\n```\n\n----------------------------------------\n\nTITLE: Loading and Using PEFT Adapter Model in Python\nDESCRIPTION: Demonstrates how to load a PEFT adapter model from the Hugging Face Hub and use it for encoding sentences and calculating similarities.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/training/peft/README.md#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer\n\n# Download from the  Hub\nmodel = SentenceTransformer(\"tomaarsen/bert-base-uncased-gooaq-peft\")\n# Run inference\nsentences = [\n    \"is toprol xl the same as metoprolol?\",\n    \"Metoprolol succinate is also known by the brand name Toprol XL. It is the extended-release form of metoprolol. Metoprolol succinate is approved to treat high blood pressure, chronic chest pain, and congestive heart failure.\",\n    \"Metoprolol starts to work after about 2 hours, but it can take up to 1 week to fully take effect. You may not feel any different when you take metoprolol, but this doesn't mean it's not working. It's important to keep taking your medicine\"\n]\nembeddings = model.encode(sentences)\nprint(embeddings.shape)\n# [3, 768]\n\n# Get the similarity scores for the embeddings\nsimilarities = model.similarity(embeddings[0], embeddings[1:])\nprint(similarities)\n# tensor([[0.7913, 0.4976]])\n```\n\n----------------------------------------\n\nTITLE: Optimized Paraphrase Mining with Chunk Size Control\nDESCRIPTION: Shows how to optimize paraphrase mining performance by controlling chunk sizes and limiting results. This example demonstrates setting corpus_chunk_size to process the entire sentence list at once while limiting to top_k=1 most relevant match per sentence.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/paraphrase-mining/README.md#2025-04-08_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nparaphrases = paraphrase_mining(model, sentences, corpus_chunk_size=len(sentences), top_k=1)\n```\n\n----------------------------------------\n\nTITLE: Performing Inference with Pre-trained CrossEncoder Model for MS MARCO\nDESCRIPTION: This snippet demonstrates how to load a pre-trained CrossEncoder model, predict scores for sentence pairs, and rank a list of passages for a given query using the model.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/cross_encoder/training/ms_marco/README.md#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import CrossEncoder\n\n# 1. Load a pre-trained CrossEncoder model\nmodel = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L6-v2\")\n\n# 2. Predict scores for a pair of sentences\nscores = model.predict([\n    (\"How many people live in Berlin?\", \"Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\"),\n    (\"How many people live in Berlin?\", \"Berlin is well known for its museums.\"),\n])\n# => array([ 8.607138 , -4.3200774], dtype=float32)\n\n# 3. Rank a list of passages for a query\nquery = \"How many people live in Berlin?\"\npassages = [\n    \"Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\",\n    \"Berlin is well known for its museums.\",\n    \"In 2014, the city state Berlin had 37,368 live births (+6.6%), a record number since 1991.\",\n    \"The urban area of Berlin comprised about 4.1 million people in 2014, making it the seventh most populous urban area in the European Union.\",\n    \"The city of Paris had a population of 2,165,423 people within its administrative city limits as of January 1, 2019\",\n    \"An estimated 300,000-420,000 Muslims reside in Berlin, making up about 8-11 percent of the population.\",\n    \"Berlin is subdivided into 12 boroughs or districts (Bezirke).\",\n    \"In 2015, the total labour force in Berlin was 1.85 million.\",\n    \"In 2013 around 600,000 Berliners were registered in one of the more than 2,300 sport and fitness clubs.\",\n    \"Berlin has a yearly total of about 135 million day visitors, which puts it in third place among the most-visited city destinations in the European Union.\",\n]\nranks = model.rank(query, passages)\n\n# Print the scores\nprint(\"Query:\", query)\nfor rank in ranks:\n    print(f\"{rank['score']:.2f}\\t{passages[rank['corpus_id']]}\")\n\"\"\"\nQuery: How many people live in Berlin?\n8.92    The urban area of Berlin comprised about 4.1 million people in 2014, making it the seventh most populous urban area in the European Union.\n8.61    Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\n8.24    An estimated 300,000-420,000 Muslims reside in Berlin, making up about 8-11 percent of the population.\n7.60    In 2014, the city state Berlin had 37,368 live births (+6.6%), a record number since 1991.\n6.35    In 2013 around 600,000 Berliners were registered in one of the more than 2,300 sport and fitness clubs.\n5.42    Berlin has a yearly total of about 135 million day visitors, which puts it in third place among the most-visited city destinations in the European Union.\n3.45    In 2015, the total labour force in Berlin was 1.85 million.\n0.33    Berlin is subdivided into 12 boroughs or districts (Bezirke).\n-4.24   The city of Paris had a population of 2,165,423 people within its administrative city limits as of January 1, 2019\n-4.32   Berlin is well known for its museums.\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Image-to-Image Search Example in Python\nDESCRIPTION: Demonstrates the image-to-image search capability by using an existing image from the dataset as the query. This shows how the model can find visually similar images based on the embedding space.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/image-search/Image_Search.ipynb#2025-04-08_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nsearch(Image.open(os.path.join(img_folder, 'lyStEjlKNSw.jpg')), k=5)\n```\n\n----------------------------------------\n\nTITLE: Training CT on Wikipedia Sentences for STS Benchmark in Python\nDESCRIPTION: This example uses 1 million sentences from Wikipedia to train a CT model and evaluates its performance on the STSbenchmark dataset.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/unsupervised_learning/CT_In-Batch_Negatives/README.md#2025-04-08_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntrain_stsb_ct-improved.py\n```\n\n----------------------------------------\n\nTITLE: Computing Image Embeddings\nDESCRIPTION: Loads pre-computed embeddings or computes new embeddings for the image dataset using the CLIP model.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/image-search/Image_Duplicates.ipynb#2025-04-08_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nuse_precomputed_embeddings = True\n\nif use_precomputed_embeddings: \n    emb_filename = 'unsplash-25k-photos-embeddings.pkl'\n    if not os.path.exists(emb_filename):   #Download dataset if does not exist\n        util.http_get('http://sbert.net/datasets/'+emb_filename, emb_filename)\n        \n    with open(emb_filename, 'rb') as fIn:\n        img_names, img_emb = pickle.load(fIn)  \n    print(\"Images:\", len(img_names))\nelse:\n    img_names = list(glob.glob('photos/*.jpg'))\n    print(\"Images:\", len(img_names))\n    img_emb = model.encode([Image.open(filepath) for filepath in img_names], batch_size=128, convert_to_tensor=True, show_progress_bar=True)\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenVINO Backend for Sentence Transformers\nDESCRIPTION: Code examples for using the OpenVINO backend with sentence transformer models, including both standard and quantized configurations for improved inference performance.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/usage/efficiency.rst#2025-04-08_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nbackend=\"openvino\"\n```\n\nLANGUAGE: python\nCODE:\n```\nexport_static_quantized_openvino_model(..., OVQuantizationConfig(), ...)\n```\n\n----------------------------------------\n\nTITLE: Loading and Using Cross Encoder Model\nDESCRIPTION: Example demonstrating how to load a Cross Encoder model and compute scores for pairs of texts. It also shows how to rank different texts based on similarity to a single text.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/cross_encoder/model_card_template.md#2025-04-08_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import CrossEncoder\n\n# Download from the {{ hf_emoji }} Hub\nmodel = CrossEncoder(\"{{ model_id | default('cross_encoder_model_id', true) }}\")\n# Get scores for pairs of texts\npairs = [\n    [\"How many calories in an egg\", \"There are on average between 55 and 80 calories in an egg depending on its size.\"],\n    [\"How many calories in an egg\", \"Egg whites are very low in calories, have no fat, no cholesterol, and are loaded with protein.\"],\n    [\"How many calories in an egg\", \"Most of the calories in an egg come from the yellow yolk in the center.\"],\n]\nscores = model.predict(pairs)\nprint(scores.shape)\n# (3,)\n\n# Or rank different texts based on similarity to a single text\nranks = model.rank(\n    \"How many calories in an egg\",\n    [\n        \"There are on average between 55 and 80 calories in an egg depending on its size.\",\n        \"Egg whites are very low in calories, have no fat, no cholesterol, and are loaded with protein.\",\n        \"Most of the calories in an egg come from the yellow yolk in the center.\",\n    ]\n)\n# [{'corpus_id': ..., 'score': ...}, {'corpus_id': ..., 'score': ...}, ...]\n```\n\n----------------------------------------\n\nTITLE: Initializing SentenceTransformer with CLIP Model\nDESCRIPTION: Sets up the SentenceTransformer with the CLIP ViT-B-32 model for image processing.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/image-search/Image_Clustering.ipynb#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer, util\nfrom PIL import Image\nimport glob\nimport torch\nimport pickle\nimport zipfile\nfrom IPython.display import display\nfrom IPython.display import Image as IPImage\nimport os\nfrom tqdm.autonotebook import tqdm\n\n#First, we load the respective CLIP model\nmodel = SentenceTransformer('clip-ViT-B-32')\n\n```\n\n----------------------------------------\n\nTITLE: Using Pre-trained Multilingual Sentence Transformer Models\nDESCRIPTION: Example demonstrating how to use a pre-trained multilingual sentence transformer model to encode sentences in different languages and compute similarities between them. The code shows that similar sentences in different languages map to similar vectors.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/training/multilingual/README.md#2025-04-08_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\nembeddings = model.encode([\"Hello World\", \"Hallo Welt\", \"Hola mundo\", \"Bye, Moon!\"])\nsimilarities = model.similarity(embeddings, embeddings)\n# tensor([[1.0000, 0.9429, 0.8880, 0.4558],\n#         [0.9429, 1.0000, 0.9680, 0.5307],\n#         [0.8880, 0.9680, 1.0000, 0.4933],\n#         [0.4558, 0.5307, 0.4933, 1.0000]])\n```\n\n----------------------------------------\n\nTITLE: Running Clustering for Duplicate Detection\nDESCRIPTION: Performs clustering on image embeddings to identify duplicate images using paraphrase mining.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/image-search/Image_Duplicates.ipynb#2025-04-08_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nduplicates = util.paraphrase_mining_embeddings(img_emb)\n\n# duplicates contains a list with triplets (score, image_id1, image_id2) and is scorted in decreasing order\n```\n\n----------------------------------------\n\nTITLE: Implementing Community Detection Algorithm\nDESCRIPTION: Custom implementation of a community detection algorithm that finds high-density regions in vector space based on cosine similarity.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/image-search/Image_Clustering.ipynb#2025-04-08_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef community_detection(embeddings, threshold, min_community_size=10, init_max_size=1000):\n    \"\"\"\n    Function for Fast Community Detection\n\n    Finds in the embeddings all communities, i.e. embeddings that are close (closer than threshold).\n\n    Returns only communities that are larger than min_community_size. The communities are returned\n    in decreasing order. The first element in each list is the central point in the community.\n    \"\"\"\n\n    # Compute cosine similarity scores\n    cos_scores = util.cos_sim(embeddings, embeddings)\n\n    # Minimum size for a community\n    top_k_values, _ = cos_scores.topk(k=min_community_size, largest=True)\n\n    # Filter for rows >= min_threshold\n    extracted_communities = []\n    for i in range(len(top_k_values)):\n        if top_k_values[i][-1] >= threshold:\n            new_cluster = []\n\n            # Only check top k most similar entries\n            top_val_large, top_idx_large = cos_scores[i].topk(k=init_max_size, largest=True)\n            top_idx_large = top_idx_large.tolist()\n            top_val_large = top_val_large.tolist()\n\n            if top_val_large[-1] < threshold:\n                for idx, val in zip(top_idx_large, top_val_large):\n                    if val < threshold:\n                        break\n\n                    new_cluster.append(idx)\n            else:\n                # Iterate over all entries (slow)\n                for idx, val in enumerate(cos_scores[i].tolist()):\n                    if val >= threshold:\n                        new_cluster.append(idx)\n\n            extracted_communities.append(new_cluster)\n\n    # Largest cluster first\n    extracted_communities = sorted(extracted_communities, key=lambda x: len(x), reverse=True)\n\n    # Step 2) Remove overlapping communities\n    unique_communities = []\n    extracted_ids = set()\n\n    for community in extracted_communities:\n        add_cluster = True\n        for idx in community:\n            if idx in extracted_ids:\n                add_cluster = False\n                break\n\n        if add_cluster:\n            unique_communities.append(community)\n            for idx in community:\n                extracted_ids.add(idx)\n\n    return unique_communities\n```\n\n----------------------------------------\n\nTITLE: Implementing Semantic Search with MSMARCO Model in Python\nDESCRIPTION: Example showing how to use the MSMARCO DistilBERT model for semantic search. The code demonstrates encoding a query and passage, then calculating their similarity using cosine similarity.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/pretrained-models/msmarco-v3.md#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer, util\n\nmodel = SentenceTransformer(\"msmarco-distilbert-base-v3\")\n\nquery_embedding = model.encode(\"How big is London\")\npassage_embedding = model.encode(\"London has 9,787,426 inhabitants at the 2011 census\")\n\nprint(\"Similarity:\", util.cos_sim(query_embedding, passage_embedding))\n```\n\n----------------------------------------\n\nTITLE: File Structure of Saved Sentence Transformer Model\nDESCRIPTION: This bash snippet shows the file structure generated when saving a Sentence Transformer model, including configuration files and module-specific directories.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/usage/custom_models.rst#2025-04-08_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nlocal-all-MiniLM-L6-v2/\n 1_Pooling\n    config.json\n 2_Normalize\n README.md\n config.json\n config_sentence_transformers.json\n model.safetensors\n modules.json\n sentence_bert_config.json\n special_tokens_map.json\n tokenizer.json\n tokenizer_config.json\n vocab.txt\n```\n\n----------------------------------------\n\nTITLE: Managing Input Sequence Length in Sentence Transformers in Python\nDESCRIPTION: This snippet demonstrates how to check and modify the maximum sequence length for a Sentence Transformer model, which is important for handling longer input texts.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/computing-embeddings/README.rst#2025-04-08_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\")\nprint(\"Max Sequence Length:\", model.max_seq_length)\n# => Max Sequence Length: 256\n\n# Change the length to 200\nmodel.max_seq_length = 200\n\nprint(\"Max Sequence Length:\", model.max_seq_length)\n# => Max Sequence Length: 200\n```\n\n----------------------------------------\n\nTITLE: Loading Pre-computed Image Embeddings in Python\nDESCRIPTION: Loads or generates embeddings for the Unsplash image dataset. For efficiency, uses pre-computed embeddings downloaded from an external source, but also includes commented code for computing embeddings directly from images.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/image-search/Image_Search.ipynb#2025-04-08_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Now, we need to compute the embeddings\n# To speed things up, we distribute pre-computed embeddings\n# Otherwise you can also encode the images yourself.\n# To encode an image, you can use the following code:\n# from PIL import Image\n# img_emb = model.encode(Image.open(filepath))\n\nuse_precomputed_embeddings = True\n\nif use_precomputed_embeddings: \n    emb_filename = 'unsplash-25k-photos-embeddings.pkl'\n    if not os.path.exists(emb_filename):   #Download dataset if does not exist\n        util.http_get('http://sbert.net/datasets/'+emb_filename, emb_filename)\n        \n    with open(emb_filename, 'rb') as fIn:\n        img_names, img_emb = pickle.load(fIn)  \n    print(\"Images:\", len(img_names))\nelse:\n    img_names = list(glob.glob('unsplash/photos/*.jpg'))\n    print(\"Images:\", len(img_names))\n    img_emb = model.encode([Image.open(filepath) for filepath in img_names], batch_size=128, convert_to_tensor=True, show_progress_bar=True)\n```\n\n----------------------------------------\n\nTITLE: Multilingual Zero-Shot Image Classification with CLIP Model\nDESCRIPTION: This snippet demonstrates multilingual zero-shot classification using a specialized CLIP model. It encodes labels in multiple languages and performs classification similar to the English-only example.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/image-search/Image_Classification.ipynb#2025-04-08_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmulti_model = SentenceTransformer('clip-ViT-B-32-multilingual-v1')\n\n# Then, we define our labels as text. Here, we use 4 labels\nlabels = ['Hund',     # German: dog\n          'gato',     # Spanish: cat \n          '',  # Chinese: Paris at night\n          ''     # Russian: Paris\n         ]\n\n# And compute the text embeddings for these labels\ntxt_emb = multi_model.encode(labels, convert_to_tensor=True)\n\n# Now, we compute the cosine similarity between the images and the labels\ncos_scores = util.cos_sim(img_emb, txt_emb)\n\n# Then we look which label has the highest cosine similarity with the given images\npred_labels = torch.argmax(cos_scores, dim=1)\n\n# Finally we output the images + labels\nfor img_name, pred_label in zip(img_names, pred_labels):\n    display(IPImage(img_name, width=200))\n    print(\"Predicted label:\", labels[pred_label])\n    print(\"\\n\\n\")\n```\n\n----------------------------------------\n\nTITLE: Adding a New PEFT Adapter to a Sentence Transformer Model in Python\nDESCRIPTION: Shows how to add a new PEFT adapter to an existing Sentence Transformer model using a LoraConfig for fine-tuning.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/training/peft/README.md#2025-04-08_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer\n\n# 1. Load a model to finetune with 2. (Optional) model card data\nmodel = SentenceTransformer(\n    \"all-MiniLM-L6-v2\",\n    model_card_data=SentenceTransformerModelCardData(\n        language=\"en\",\n        license=\"apache-2.0\",\n        model_name=\"all-MiniLM-L6-v2 adapter finetuned on GooAQ pairs\",\n    ),\n)\n\n# 3. Create a LoRA adapter for the model & add it\npeft_config = LoraConfig(\n    task_type=TaskType.FEATURE_EXTRACTION,\n    inference_mode=False,\n    r=64,\n    lora_alpha=128,\n    lora_dropout=0.1,\n)\nmodel.add_adapter(peft_config)\n\n# Proceed as usual... See https://sbert.net/docs/sentence_transformer/training_overview.html\n```\n\n----------------------------------------\n\nTITLE: CosineSimilarityLoss Implementation in Sentence Transformers\nDESCRIPTION: The CosineSimilarityLoss is used for comparing sentence embeddings. It calculates the cosine similarity between embeddings u and v from sentence pairs, and compares the result to gold similarity scores to fine-tune the network for sentence similarity recognition.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/sentence_transformer/losses.md#2025-04-08_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: sentence_transformers.losses.CosineSimilarityLoss\n```\n\n----------------------------------------\n\nTITLE: Loading and Using Sentence Transformer Model in Python\nDESCRIPTION: Python code to load the Sentence Transformer model, encode sentences into embeddings, and calculate similarities between embeddings.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/model_card_template.md#2025-04-08_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer\n\n# Download from the {{ hf_emoji }} Hub\nmodel = SentenceTransformer(\"{{ model_id | default('sentence_transformers_model_id', true) }}\")\n# Run inference\nsentences = [\n    \"The weather is lovely today.\",\n    \"It's so sunny outside!\",\n    \"He drove to the stadium.\",\n]\nembeddings = model.encode(sentences)\nprint(embeddings.shape)\n# [3, {{ output_dimensionality | default(1024, true) }}]\n\n# Get the similarity scores for the embeddings\nsimilarities = model.similarity(embeddings, embeddings)\nprint(similarities.shape)\n# [3, 3]\n```\n\n----------------------------------------\n\nTITLE: Using Prompts with Sentence Transformer Encoding in Python\nDESCRIPTION: This snippet shows different ways to apply prompts when encoding text with a Sentence Transformer model, including explicit prompt specification and using default prompts.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/computing-embeddings/README.rst#2025-04-08_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nembeddings = model.encode(\"How to bake a strawberry cake\", prompt=\"Retrieve semantically similar text: \")\n\nembeddings = model.encode(\"How to bake a strawberry cake\", prompt_name=\"retrieval\")\n\nembeddings = model.encode(\"How to bake a strawberry cake\")\n```\n\n----------------------------------------\n\nTITLE: Loading Parallel Sentences Dataset for Training Multilingual Models\nDESCRIPTION: Code snippet demonstrating how to load a parallel sentences dataset from the Hugging Face datasets library for training multilingual sentence transformers. The example loads English-German parallel sentences from a talks dataset.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/training/multilingual/README.md#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n\ntrain_dataset = load_dataset(\"sentence-transformers/parallel-sentences-talks\", \"en-de\", split=\"train\")\nprint(train_dataset[0])\n# {\"english\": \"So I think practicality is one case where it's worth teaching people by hand.\", \"non_english\": \"Ich denke, dass es sich aus diesem Grund lohnt, den Leuten das Rechnen von Hand beizubringen.\"}\n```\n\n----------------------------------------\n\nTITLE: Initializing Sentence Transformer Model with Prompts in Python\nDESCRIPTION: This snippet demonstrates how to initialize a Sentence Transformer model with prompts and a default prompt name, which can be used to improve model performance for specific tasks.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/computing-embeddings/README.rst#2025-04-08_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmodel = SentenceTransformer(\n    \"intfloat/multilingual-e5-large\",\n    prompts={\n        \"classification\": \"Classify the following text: \",\n        \"retrieval\": \"Retrieve semantically similar text: \",\n        \"clustering\": \"Identify the topic or theme based on the text: \",\n    },\n)\n# or\nmodel.prompts = {\n    \"classification\": \"Classify the following text: \",\n    \"retrieval\": \"Retrieve semantically similar text: \",\n    \"clustering\": \"Identify the topic or theme based on the text: \",\n}\n\nmodel = SentenceTransformer(\n    \"intfloat/multilingual-e5-large\",\n    prompts={\n        \"classification\": \"Classify the following text: \",\n        \"retrieval\": \"Retrieve semantically similar text: \",\n        \"clustering\": \"Identify the topic or theme based on the text: \",\n    },\n    default_prompt_name=\"retrieval\",\n)\n# or\nmodel.default_prompt_name=\"retrieval\"\n```\n\n----------------------------------------\n\nTITLE: Migrating Model Saving and Evaluation\nDESCRIPTION: Shows how to migrate model saving and evaluation parameters from fit() method to the new SentenceTransformerTrainer approach.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/migration_guide.md#2025-04-08_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# v2.x\nmodel.fit(\n    train_objectives=[(train_dataloader, train_loss)],\n    evaluator=evaluator,\n    output_path=\"my/path\",\n    save_best_model=True,\n)\n```\n\nLANGUAGE: python\nCODE:\n```\n# v3.x\nargs = SentenceTransformerTrainingArguments(\n    load_best_model_at_end=True,\n    metric_for_best_model=\"all_nli_cosine_accuracy\",\n)\n\ntrainer = SentenceTransformerTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    loss=loss,\n)\ntrainer.train()\n\n# Save the best model\nmodel.save_pretrained(\"my/path\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Cross-Encoder with SentenceTransformers\nDESCRIPTION: Shows how to initialize and use a cross-encoder model with the SentenceTransformers library for predicting relevance scores between queries and paragraphs.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/pretrained-models/ce-msmarco.md#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import CrossEncoder\n\nmodel = CrossEncoder(\"model_name\", max_length=512)\nscores = model.predict([\n    (\"Query\", \"Paragraph1\"),\n    (\"Query\", \"Paragraph2\"),\n    (\"Query\", \"Paragraph3\")\n])\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting Unsplash Dataset in Python\nDESCRIPTION: This code snippet downloads the Unsplash Dataset containing about 25k images and extracts them to a local folder. It uses zipfile for extraction and tqdm for progress display.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/image-search/Image_Search-multilingual.ipynb#2025-04-08_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Next, we get about 25k images from Unsplash \nimg_folder = 'photos/'\nif not os.path.exists(img_folder) or len(os.listdir(img_folder)) == 0:\n    os.makedirs(img_folder, exist_ok=True)\n    \n    photo_filename = 'unsplash-25k-photos.zip'\n    if not os.path.exists(photo_filename):   #Download dataset if does not exist\n        util.http_get('http://sbert.net/datasets/'+photo_filename, photo_filename)\n        \n    #Extract all images\n    with zipfile.ZipFile(photo_filename, 'r') as zf:\n        for member in tqdm(zf.infolist(), desc='Extracting'):\n            zf.extract(member, img_folder)\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting Unsplash Dataset in Python\nDESCRIPTION: Downloads and extracts a dataset of approximately 25,000 images from Unsplash if not already present. Creates a folder structure for the images and handles the extraction process with progress tracking.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/image-search/Image_Search.ipynb#2025-04-08_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Next, we get about 25k images from Unsplash \nimg_folder = 'photos/'\nif not os.path.exists(img_folder) or len(os.listdir(img_folder)) == 0:\n    os.makedirs(img_folder, exist_ok=True)\n    \n    photo_filename = 'unsplash-25k-photos.zip'\n    if not os.path.exists(photo_filename):   #Download dataset if does not exist\n        util.http_get('http://sbert.net/datasets/'+photo_filename, photo_filename)\n        \n    #Extract all images\n    with zipfile.ZipFile(photo_filename, 'r') as zf:\n        for member in tqdm(zf.infolist(), desc='Extracting'):\n            zf.extract(member, img_folder)\n```\n\n----------------------------------------\n\nTITLE: Configuring and Training Cross-Encoder Model in Python\nDESCRIPTION: This code snippet shows the main function for setting up and training a cross-encoder model. It includes dataset preparation, model initialization, evaluation setup, training configuration, and model saving steps.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/cross_encoder/training_overview.md#2025-04-08_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\ndef main():\n    # ... (previous code omitted for brevity)\n\n    # 4c. Combine the evaluators & run the base model on them\n    evaluator = SequentialEvaluator([reranking_evaluator, nano_beir_evaluator])\n    evaluator(model)\n\n    # 5. Define the training arguments\n    short_model_name = model_name if \"/\" not in model_name else model_name.split(\"/\")[-1]\n    run_name = f\"reranker-{short_model_name}-gooaq-bce\"\n    args = CrossEncoderTrainingArguments(\n        output_dir=f\"models/{run_name}\",\n        num_train_epochs=num_epochs,\n        per_device_train_batch_size=train_batch_size,\n        per_device_eval_batch_size=train_batch_size,\n        learning_rate=2e-5,\n        warmup_ratio=0.1,\n        fp16=False,\n        bf16=True,\n        dataloader_num_workers=4,\n        load_best_model_at_end=True,\n        metric_for_best_model=\"eval_gooaq-dev_ndcg@10\",\n        eval_strategy=\"steps\",\n        eval_steps=1000,\n        save_strategy=\"steps\",\n        save_steps=1000,\n        save_total_limit=2,\n        logging_steps=200,\n        logging_first_step=True,\n        run_name=run_name,\n        seed=12,\n    )\n\n    # 6. Create the trainer & start training\n    trainer = CrossEncoderTrainer(\n        model=model,\n        args=args,\n        train_dataset=hard_train_dataset,\n        loss=loss,\n        evaluator=evaluator,\n    )\n    trainer.train()\n\n    # 7. Evaluate the final model\n    evaluator(model)\n\n    # 8. Save the final model\n    final_output_dir = f\"models/{run_name}/final\"\n    model.save_pretrained(final_output_dir)\n\n    # 9. (Optional) save the model to the Hugging Face Hub\n    try:\n        model.push_to_hub(run_name)\n    except Exception:\n        logging.error(\n            f\"Error uploading model to the Hugging Face Hub:\\n{traceback.format_exc()}To upload it manually, you can run \"\n            f\"`huggingface-cli login`, followed by loading the model using `model = CrossEncoder({final_output_dir!r})` \"\n            f\"and saving it using `model.push_to_hub('{run_name}')`.\"\n        )\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\n----------------------------------------\n\nTITLE: Downloading and Encoding Images with CLIP Model\nDESCRIPTION: This snippet downloads sample images, opens them using PIL, and encodes them using the CLIP model. It demonstrates how to prepare image data for classification.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/image-search/Image_Classification.ipynb#2025-04-08_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# We download some images from our repository which we want to classify\nimg_names = ['eiffel-tower-day.jpg', 'eiffel-tower-night.jpg', 'two_dogs_in_snow.jpg', 'cat.jpg']\nurl = 'https://github.com/UKPLab/sentence-transformers/raw/master/examples/applications/image-search/'\nfor filename in img_names:\n    if not os.path.exists(filename):\n        util.http_get(url+filename, filename)\n\n# And compute the embeddings for these images\nimg_emb = en_model.encode([Image.open(filepath) for filepath in img_names], convert_to_tensor=True)\n```\n\n----------------------------------------\n\nTITLE: Implementing Cross-Encoder with Transformers\nDESCRIPTION: Demonstrates how to use the Hugging Face Transformers library to implement a cross-encoder model for query-paragraph matching, including tokenization and inference steps.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/pretrained-models/ce-msmarco.md#2025-04-08_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"model_name\")\ntokenizer = AutoTokenizer.from_pretrained(\"model_name\")\n\nfeatures = tokenizer([\"Query\", \"Query\"], [\"Paragraph1\", \"Paragraph2\"], padding=True, truncation=True, return_tensors=\"pt\")\n\nmodel.eval()\nwith torch.no_grad():\n    scores = model(**features).logits\n    print(scores)\n```\n\n----------------------------------------\n\nTITLE: Configuring PyTorch Precision Settings for Sentence Transformers\nDESCRIPTION: Examples of setting different precision formats in PyTorch with model_kwargs parameter. This includes float16 and bfloat16 precision configurations for optimized performance.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/usage/efficiency.rst#2025-04-08_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nmodel_kwargs={\"torch_dtype\": \"float16\"}\n```\n\nLANGUAGE: python\nCODE:\n```\nmodel_kwargs={\"torch_dtype\": \"bfloat16\"}\n```\n\n----------------------------------------\n\nTITLE: Training TSDAE Model with Sentence Transformers\nDESCRIPTION: Demonstrates how to set up and train a TSDAE model using the sentence-transformers library. The code initializes a BERT-based model with CLS pooling, creates a denoising dataset, and trains the model using the DenoisingAutoEncoderLoss.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/unsupervised_learning/TSDAE/README.md#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer, LoggingHandler\nfrom sentence_transformers import models, util, datasets, evaluation, losses\nfrom torch.utils.data import DataLoader\n\n# Define your sentence transformer model using CLS pooling\nmodel_name = \"bert-base-uncased\"\nword_embedding_model = models.Transformer(model_name)\npooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), \"cls\")\nmodel = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n\n# Define a list with sentences (1k - 100k sentences)\ntrain_sentences = [\n    \"Your set of sentences\",\n    \"Model will automatically add the noise\",\n    \"And re-construct it\",\n    \"You should provide at least 1k sentences\",\n]\n\n# Create the special denoising dataset that adds noise on-the-fly\ntrain_dataset = datasets.DenoisingAutoEncoderDataset(train_sentences)\n\n# DataLoader to batch your data\ntrain_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n\n# Use the denoising auto-encoder loss\ntrain_loss = losses.DenoisingAutoEncoderLoss(\n    model, decoder_name_or_path=model_name, tie_encoder_decoder=True\n)\n\n# Call the fit method\nmodel.fit(\n    train_objectives=[(train_dataloader, train_loss)],\n    epochs=1,\n    weight_decay=0,\n    scheduler=\"constantlr\",\n    optimizer_params={\"lr\": 3e-5},\n    show_progress_bar=True,\n)\n\nmodel.save(\"output/tsdae-model\")\n```\n\n----------------------------------------\n\nTITLE: Performing Inference with Pre-trained CrossEncoder NLI Model in Python\nDESCRIPTION: This snippet demonstrates how to use a pre-trained CrossEncoder model for Natural Language Inference. It loads the model, predicts scores for given sentence pairs, and maps the scores to NLI labels.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/cross_encoder/training/nli/README.md#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import CrossEncoder\n\nmodel = CrossEncoder(\"cross-encoder/nli-deberta-v3-base\")\nscores = model.predict([\n    (\"A man is eating pizza\", \"A man eats something\"),\n    (\"A black race car starts up in front of a crowd of people.\", \"A man is driving down a lonely road.\"),\n])\n\n# Convert scores to labels\nlabel_mapping = [\"contradiction\", \"entailment\", \"neutral\"]\nlabels = [label_mapping[score_max] for score_max in scores.argmax(axis=1)]\n# => ['entailment', 'contradiction']\n```\n\n----------------------------------------\n\nTITLE: Initializing EmbeddingSimilarityEvaluator with STS Benchmark Dataset in Python\nDESCRIPTION: This code demonstrates how to initialize an EmbeddingSimilarityEvaluator using the STS benchmark dataset from Hugging Face. The evaluator measures cosine similarity between sentence pairs and compares against human-annotated similarity scores.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/training_overview.md#2025-04-08_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\nfrom sentence_transformers.evaluation import EmbeddingSimilarityEvaluator, SimilarityFunction\n\n# Load the STSB dataset (https://huggingface.co/datasets/sentence-transformers/stsb)\neval_dataset = load_dataset(\"sentence-transformers/stsb\", split=\"validation\")\n\n# Initialize the evaluator\ndev_evaluator = EmbeddingSimilarityEvaluator(\n    sentences1=eval_dataset[\"sentence1\"],\n    sentences2=eval_dataset[\"sentence2\"],\n    scores=eval_dataset[\"score\"],\n    main_similarity=SimilarityFunction.COSINE,\n    name=\"sts-dev\",\n)\n# You can run evaluation like so:\n# results = dev_evaluator(model)\n```\n\n----------------------------------------\n\nTITLE: Initializing Custom Sentence Transformer Model with Decay Pooling\nDESCRIPTION: Example showing how to create a Sentence Transformer model with a custom DecayMeanPooling module along with standard Transformer and Normalize modules.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/usage/custom_models.rst#2025-04-08_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import models, SentenceTransformer\nfrom decay_pooling import DecayMeanPooling\n\ntransformer = models.Transformer(\"bert-base-uncased\", max_seq_length=256)\ndecay_mean_pooling = DecayMeanPooling(transformer.get_word_embedding_dimension(), decay=0.99)\nnormalize = models.Normalize()\n\nmodel = SentenceTransformer(modules=[transformer, decay_mean_pooling, normalize])\nprint(model)\n\ntexts = [\n    \"Hello, World!\",\n    \"The quick brown fox jumps over the lazy dog.\",\n    \"I am a sentence that is used for testing purposes.\",\n    \"This is a test sentence.\",\n    \"This is another test sentence.\",\n]\nembeddings = model.encode(texts)\nprint(embeddings.shape)\n```\n\n----------------------------------------\n\nTITLE: Searching for Assassinated US Presidents\nDESCRIPTION: This code executes a search query about which US presidents were killed. It demonstrates the system's handling of historical event queries with sensitive content.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/retrieve_rerank/retrieve_rerank_simple_wikipedia.ipynb#2025-04-08_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nsearch(query = \"Which US president was killed?\")\n```\n\n----------------------------------------\n\nTITLE: OpenVINO Model Conversion and Usage\nDESCRIPTION: Example showing how to convert and use a model with OpenVINO backend for accelerated CPU inference.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/usage/efficiency.rst#2025-04-08_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\", backend=\"openvino\")\n\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\nembeddings = model.encode(sentences)\n```\n\n----------------------------------------\n\nTITLE: Multi-Dataset Training with SentenceTransformerTrainer in Python\nDESCRIPTION: A complete example of training a sentence transformer model with multiple datasets, applying different loss functions to each dataset. The code demonstrates loading datasets, setting up loss functions, and configuring the trainer for multi-task learning.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/training_overview.md#2025-04-08_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\nfrom sentence_transformers import SentenceTransformer, SentenceTransformerTrainer\nfrom sentence_transformers.losses import CoSENTLoss, MultipleNegativesRankingLoss, SoftmaxLoss\n\n# 1. Load a model to finetune\nmodel = SentenceTransformer(\"bert-base-uncased\")\n\n# 2. Load several Datasets to train with\n# (anchor, positive)\nall_nli_pair_train = load_dataset(\"sentence-transformers/all-nli\", \"pair\", split=\"train[:10000]\")\n# (premise, hypothesis) + label\nall_nli_pair_class_train = load_dataset(\"sentence-transformers/all-nli\", \"pair-class\", split=\"train[:10000]\")\n# (sentence1, sentence2) + score\nall_nli_pair_score_train = load_dataset(\"sentence-transformers/all-nli\", \"pair-score\", split=\"train[:10000]\")\n# (anchor, positive, negative)\nall_nli_triplet_train = load_dataset(\"sentence-transformers/all-nli\", \"triplet\", split=\"train[:10000]\")\n# (sentence1, sentence2) + score\nstsb_pair_score_train = load_dataset(\"sentence-transformers/stsb\", split=\"train[:10000]\")\n# (anchor, positive)\nquora_pair_train = load_dataset(\"sentence-transformers/quora-duplicates\", \"pair\", split=\"train[:10000]\")\n# (query, answer)\nnatural_questions_train = load_dataset(\"sentence-transformers/natural-questions\", split=\"train[:10000]\")\n\n# We can combine all datasets into a dictionary with dataset names to datasets\ntrain_dataset = {\n    \"all-nli-pair\": all_nli_pair_train,\n    \"all-nli-pair-class\": all_nli_pair_class_train,\n    \"all-nli-pair-score\": all_nli_pair_score_train,\n    \"all-nli-triplet\": all_nli_triplet_train,\n    \"stsb\": stsb_pair_score_train,\n    \"quora\": quora_pair_train,\n    \"natural-questions\": natural_questions_train,\n}\n\n# 3. Load several Datasets to evaluate with\n# (anchor, positive, negative)\nall_nli_triplet_dev = load_dataset(\"sentence-transformers/all-nli\", \"triplet\", split=\"dev\")\n# (sentence1, sentence2, score)\nstsb_pair_score_dev = load_dataset(\"sentence-transformers/stsb\", split=\"validation\")\n# (anchor, positive)\nquora_pair_dev = load_dataset(\"sentence-transformers/quora-duplicates\", \"pair\", split=\"train[10000:11000]\")\n# (query, answer)\nnatural_questions_dev = load_dataset(\"sentence-transformers/natural-questions\", split=\"train[10000:11000]\")\n\n# We can use a dictionary for the evaluation dataset too, but we don't have to. We could also just use\n# no evaluation dataset, or one dataset.\neval_dataset = {\n    \"all-nli-triplet\": all_nli_triplet_dev,\n    \"stsb\": stsb_pair_score_dev,\n    \"quora\": quora_pair_dev,\n    \"natural-questions\": natural_questions_dev,\n}\n\n# 4. Load several loss functions to train with\n# (anchor, positive), (anchor, positive, negative)\nmnrl_loss = MultipleNegativesRankingLoss(model)\n# (sentence_A, sentence_B) + class\nsoftmax_loss = SoftmaxLoss(model, model.get_sentence_embedding_dimension(), 3)\n# (sentence_A, sentence_B) + score\ncosent_loss = CoSENTLoss(model)\n\n# Create a mapping with dataset names to loss functions, so the trainer knows which loss to apply where.\n# Note that you can also just use one loss if all of your training/evaluation datasets use the same loss\nlosses = {\n    \"all-nli-pair\": mnrl_loss,\n    \"all-nli-pair-class\": softmax_loss,\n    \"all-nli-pair-score\": cosent_loss,\n    \"all-nli-triplet\": mnrl_loss,\n    \"stsb\": cosent_loss,\n    \"quora\": mnrl_loss,\n    \"natural-questions\": mnrl_loss,\n}\n\n# 5. Define a simple trainer, although it's recommended to use one with args & evaluators\ntrainer = SentenceTransformerTrainer(\n    model=model,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    loss=losses,\n)\ntrainer.train()\n\n# 6. save the trained model and optionally push it to the Hugging Face Hub\nmodel.save_pretrained(\"bert-base-all-nli-stsb-quora-nq\")\nmodel.push_to_hub(\"bert-base-all-nli-stsb-quora-nq\")\n```\n\n----------------------------------------\n\nTITLE: Implementing MultipleNegativesRankingLoss for Quora Duplicate Questions Detection in Python\nDESCRIPTION: This code snippet shows how to load the Quora Duplicate Questions dataset in pair format and implement MultipleNegativesRankingLoss for training a sentence transformer model. This approach only requires positive pairs of duplicate questions.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/training/quora_duplicate_questions/README.md#2025-04-08_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n\ntrain_dataset = load_dataset(\"sentence-transformers/quora-duplicates\", \"pair\", split=\"train\")\n# => Dataset({\n#     features: ['anchor', 'positive'],\n#     num_rows: 149263\n# })\nprint(train_dataset[0])\n# => {'anchor': 'Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?', 'positive': \"I'm a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me?\"}\ntrain_loss = losses.MultipleNegativesRankingLoss(model)\n```\n\n----------------------------------------\n\nTITLE: Searching for Best Orchestra Information\nDESCRIPTION: This code executes a search query about the best orchestra in the world. This type of subjective query demonstrates how the system handles opinion-based questions.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/retrieve_rerank/retrieve_rerank_simple_wikipedia.ipynb#2025-04-08_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nsearch(query = \"What is the best orchestra in the world?\")\n```\n\n----------------------------------------\n\nTITLE: Generating Synthetic Queries using T5 Model in Python\nDESCRIPTION: This code snippet demonstrates how to use a pre-trained T5 model to generate synthetic queries from a given text passage. It uses the BeIR/query-gen-msmarco-t5-large-v1 model and implements top-p sampling for diverse query generation.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/unsupervised_learning/query_generation/README.md#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nimport torch\n\ntokenizer = T5Tokenizer.from_pretrained(\"BeIR/query-gen-msmarco-t5-large-v1\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"BeIR/query-gen-msmarco-t5-large-v1\")\nmodel.eval()\n\npara = \"Python is an interpreted, high-level and general-purpose programming language. Python's design philosophy emphasizes code readability with its notable use of significant whitespace. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.\"\n\ninput_ids = tokenizer.encode(para, return_tensors=\"pt\")\nwith torch.no_grad():\n    outputs = model.generate(\n        input_ids=input_ids,\n        max_length=64,\n        do_sample=True,\n        top_p=0.95,\n        num_return_sequences=3,\n    )\n\nprint(\"Paragraph:\")\nprint(para)\n\nprint(\"\\nGenerated Queries:\")\nfor i in range(len(outputs)):\n    query = tokenizer.decode(outputs[i], skip_special_tokens=True)\n    print(f\"{i + 1}: {query}\")\n```\n\n----------------------------------------\n\nTITLE: Searching for Cat Lifespan Information\nDESCRIPTION: This code executes a search query about how long cats live. It shows how the system handles common knowledge questions about animals.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/retrieve_rerank/retrieve_rerank_simple_wikipedia.ipynb#2025-04-08_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nsearch(query = \"How long do cats live?\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Loss Functions for Sentence Transformers in Python\nDESCRIPTION: This code block describes the requirements for creating custom loss functions in the Sentence Transformers library. It specifies the necessary class structure, method implementations, and optional features for full integration. The custom loss function must subclass torch.nn.Module, include a specific constructor, implement a forward method, and optionally provide configuration and citation information.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/cross_encoder/loss_overview.md#2025-04-08_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass CustomLoss(torch.nn.Module):\n    def __init__(self, model, ...):\n        super().__init__()\n        self.model = model\n        # Initialize other parameters\n\n    def forward(self, inputs, labels=None):\n        # Process inputs and labels\n        # Return a single loss value\n\n    def get_config_dict(self):\n        return {\"param1\": value1, \"param2\": value2}\n\n    @property\n    def citation(self):\n        return \"Citation information for the custom loss function\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Decay Mean Pooling Module in Python\nDESCRIPTION: This Python code defines a custom DecayMeanPooling module for Sentence Transformers, implementing required methods like forward, save, load, and get_config_dict.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/usage/custom_models.rst#2025-04-08_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# decay_pooling.py\n\nimport json\nimport os\nimport torch\nimport torch.nn as nn\n\nclass DecayMeanPooling(nn.Module):\n    def __init__(self, dimension: int, decay: float = 0.95) -> None:\n        super(DecayMeanPooling, self).__init__()\n        self.dimension = dimension\n        self.decay = decay\n\n    def forward(self, features: dict[str, torch.Tensor], **kwargs) -> dict   [str, torch.Tensor]:\n        token_embeddings = features[\"token_embeddings\"]\n        attention_mask = features[\"attention_mask\"].unsqueeze(-1)\n\n        # Apply the attention mask to filter away padding tokens\n        token_embeddings = token_embeddings * attention_mask\n        # Calculate mean of token embeddings\n        sentence_embeddings = token_embeddings.sum(1) / attention_mask.sum(1)\n        # Apply exponential decay\n        importance_per_dim = self.decay ** torch.arange(sentence_embeddings.   size(1), device=sentence_embeddings.device)\n        features[\"sentence_embedding\"] = sentence_embeddings *    importance_per_dim\n        return features\n\n    def get_config_dict(self) -> dict[str, float]:\n        return {\"dimension\": self.dimension, \"decay\": self.decay}\n\n    def get_sentence_embedding_dimension(self) -> int:\n        return self.dimension\n\n    def save(self, save_dir: str, **kwargs) -> None:\n        with open(os.path.join(save_dir, \"config.json\"), \"w\") as fOut:\n            json.dump(self.get_config_dict(), fOut, indent=4)\n\n    def load(load_dir: str, **kwargs) -> \"DecayMeanPooling\":\n        with open(os.path.join(load_dir, \"config.json\")) as fIn:\n            config = json.load(fIn)\n\n        return DecayMeanPooling(**config)\n```\n\n----------------------------------------\n\nTITLE: Training a Sentence Transformer Model with SentenceTransformerTrainer in Python\nDESCRIPTION: This code snippet demonstrates a complete workflow for fine-tuning a sentence transformer model using the SentenceTransformerTrainer. It covers model initialization, dataset loading, loss function definition, training arguments setup, evaluation, training, and model saving/pushing to Hugging Face Hub.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/training_overview.md#2025-04-08_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\nfrom sentence_transformers import (\n    SentenceTransformer,\n    SentenceTransformerTrainer,\n    SentenceTransformerTrainingArguments,\n    SentenceTransformerModelCardData,\n)\nfrom sentence_transformers.losses import MultipleNegativesRankingLoss\nfrom sentence_transformers.training_args import BatchSamplers\nfrom sentence_transformers.evaluation import TripletEvaluator\n\n# 1. Load a model to finetune with 2. (Optional) model card data\nmodel = SentenceTransformer(\n    \"microsoft/mpnet-base\",\n    model_card_data=SentenceTransformerModelCardData(\n        language=\"en\",\n        license=\"apache-2.0\",\n        model_name=\"MPNet base trained on AllNLI triplets\",\n    )\n)\n\n# 3. Load a dataset to finetune on\ndataset = load_dataset(\"sentence-transformers/all-nli\", \"triplet\")\ntrain_dataset = dataset[\"train\"].select(range(100_000))\neval_dataset = dataset[\"dev\"]\ntest_dataset = dataset[\"test\"]\n\n# 4. Define a loss function\nloss = MultipleNegativesRankingLoss(model)\n\n# 5. (Optional) Specify training arguments\nargs = SentenceTransformerTrainingArguments(\n    # Required parameter:\n    output_dir=\"models/mpnet-base-all-nli-triplet\",\n    # Optional training parameters:\n    num_train_epochs=1,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    learning_rate=2e-5,\n    warmup_ratio=0.1,\n    fp16=True,  # Set to False if you get an error that your GPU can't run on FP16\n    bf16=False,  # Set to True if you have a GPU that supports BF16\n    batch_sampler=BatchSamplers.NO_DUPLICATES,  # MultipleNegativesRankingLoss benefits from no duplicate samples in a batch\n    # Optional tracking/debugging parameters:\n    eval_strategy=\"steps\",\n    eval_steps=100,\n    save_strategy=\"steps\",\n    save_steps=100,\n    save_total_limit=2,\n    logging_steps=100,\n    run_name=\"mpnet-base-all-nli-triplet\",  # Will be used in W&B if `wandb` is installed\n)\n\n# 6. (Optional) Create an evaluator & evaluate the base model\ndev_evaluator = TripletEvaluator(\n    anchors=eval_dataset[\"anchor\"],\n    positives=eval_dataset[\"positive\"],\n    negatives=eval_dataset[\"negative\"],\n    name=\"all-nli-dev\",\n)\ndev_evaluator(model)\n\n# 7. Create a trainer & train\ntrainer = SentenceTransformerTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    loss=loss,\n    evaluator=dev_evaluator,\n)\ntrainer.train()\n\n# (Optional) Evaluate the trained model on the test set\ntest_evaluator = TripletEvaluator(\n    anchors=test_dataset[\"anchor\"],\n    positives=test_dataset[\"positive\"],\n    negatives=test_dataset[\"negative\"],\n    name=\"all-nli-test\",\n)\ntest_evaluator(model)\n\n# 8. Save the trained model\nmodel.save_pretrained(\"models/mpnet-base-all-nli-triplet/final\")\n\n# 9. (Optional) Push it to the Hugging Face Hub\nmodel.push_to_hub(\"mpnet-base-all-nli-triplet\")\n```\n\n----------------------------------------\n\nTITLE: Searching for European Countries Count\nDESCRIPTION: This code executes a search query about the number of countries in Europe. It shows how the search handles geographical fact queries with concise input.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/retrieve_rerank/retrieve_rerank_simple_wikipedia.ipynb#2025-04-08_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nsearch(query = \"Number countries Europe\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Sentence Transformer from Transformers Model in Python\nDESCRIPTION: These Python snippets demonstrate two equivalent ways of initializing a Sentence Transformer model using a pre-trained Transformers model (BERT in this case).\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/usage/custom_models.rst#2025-04-08_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\"bert-base-uncased\")\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import models, SentenceTransformer\n\ntransformer = models.Transformer(\"bert-base-uncased\")\npooling = models.Pooling(transformer.get_word_embedding_dimension(), pooling_mode=\"mean\")\nmodel = SentenceTransformer(modules=[transformer, pooling])\n```\n\n----------------------------------------\n\nTITLE: RST Documentation for Retrieve & Re-Rank Process\nDESCRIPTION: RST formatted documentation explaining the retrieval system and re-ranker functionality in the search pipeline.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/retrieve_rerank/README.md#2025-04-08_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\nGiven a search query, we first use a **retrieval system** that retrieves a large list of e.g. 100 possible hits which are potentially relevant for the query. For the retrieval, we can use either lexical search, e.g. with a vector engine like Elasticsearch, or we can use dense retrieval with a :class:`~sentence_transformers.SentenceTransformer` (a.k.a. bi-encoder).  However, the retrieval system might retrieve documents that are not that relevant for the search query. Hence, in a second stage, we use a **re-ranker** based on a :class:`~sentence_transformers.cross_encoder.CrossEncoder` that scores the relevancy of all candidates for the given search query. The output will be a ranked list of hits we can present to the user.\n```\n\n----------------------------------------\n\nTITLE: Using Sentence Transformers with Prompts (mpnet-base model)\nDESCRIPTION: Code example showing how to load a prompt-trained mpnet-base model and use it to encode queries and documents with specific prompts, then calculate similarity between them.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/training/prompts/README.md#2025-04-08_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\"tomaarsen/mpnet-base-nq-prompts\")\nquery_embedding = model.encode(\"What are Pandas?\", prompt_name=\"query\")\ndocument_embeddings = model.encode([\n    \"Pandas is a software library written for the Python programming language for data manipulation and analysis.\",\n    \"Pandas are a species of bear native to South Central China. They are also known as the giant panda or simply panda.\",\n    \"Koala bears are not actually bears, they are marsupials native to Australia.\",\n    ],\n    prompt_name=\"document\",\n)\nsimilarity = model.similarity(query_embedding, document_embeddings)\nprint(similarity)\n# => tensor([[0.4725, 0.7339, 0.4369]])\n```\n\n----------------------------------------\n\nTITLE: Implementing OnlineContrastiveLoss for Quora Duplicate Questions Detection in Python\nDESCRIPTION: This code snippet demonstrates how to load the Quora Duplicate Questions dataset in pair-class format and implement OnlineContrastiveLoss for training a sentence transformer model. It uses the pair-class subset where each example consists of two questions and a label indicating if they are duplicates.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/training/quora_duplicate_questions/README.md#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n\ntrain_dataset = load_dataset(\"sentence-transformers/quora-duplicates\", \"pair-class\", split=\"train\")\n# => Dataset({\n#     features: ['sentence1', 'sentence2', 'label'],\n#     num_rows: 404290\n# })\nprint(train_dataset[0])\n# => {'sentence1': 'What is the step by step guide to invest in share market in india?', 'sentence2': 'What is the step by step guide to invest in share market?', 'label': 0}\ntrain_loss = losses.OnlineContrastiveLoss(model=model, margin=0.5)\n```\n\n----------------------------------------\n\nTITLE: Loading MS MARCO Triplet Dataset in Python\nDESCRIPTION: This code demonstrates how to load the MS MARCO hard negative triplets dataset using the Hugging Face datasets library. It loads the 'triplet-hard' subset from the MS MARCO collection and shows how to access a sample triplet containing a query, positive passage, and negative passage.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/training/ms_marco/README.md#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n\ntrain_dataset = load_dataset(\"sentence-transformers/msmarco-co-condenser-margin-mse-sym-mnrl-mean-v1\", \"triplet-hard\", split=\"train\")\n# Dataset({\n#     features: ['query', 'positive', 'negative'],\n#     num_rows: 11662655\n# })\nprint(train_dataset[0])\n# {'query': 'what are the liberal arts?', 'positive': 'liberal arts. 1. the academic course of instruction at a college intended to provide general knowledge and comprising the arts, humanities, natural sciences, and social sciences, as opposed to professional or technical subjects.', 'negative': \"Rather than preparing students for a specific career, liberal arts programs focus on cultural literacy and hone communication and analytical skills. They often cover various disciplines, ranging from the humanities to social sciences. 1  Program Levels in Liberal Arts: Associate degree, Bachelor's degree, Master's degree.\"}\n```\n\n----------------------------------------\n\nTITLE: Implementing Loss Functions for Cross-Encoder Training\nDESCRIPTION: Shows how to initialize and implement loss functions for training CrossEncoder models. Demonstrates using MultipleNegativesRankingLoss with a pre-trained model and preparing training data. Includes setup for model initialization with appropriate parameters for reranking tasks.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/cross_encoder/training_overview.md#2025-04-08_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\nfrom sentence_transformers import CrossEncoder\nfrom sentence_transformers.cross_encoder.losses import MultipleNegativesRankingLoss\n\n# Load a model to train/finetune\nmodel = CrossEncoder(\"xlm-roberta-base\", num_labels=1) # num_labels=1 is for rerankers\n\n# Initialize the MultipleNegativesRankingLoss\n# This loss requires pairs of related texts or triplets\nloss = MultipleNegativesRankingLoss(model)\n\n# Load an example training dataset that works with our loss function:\ntrain_dataset = load_dataset(\"sentence-transformers/gooaq\", split=\"train\")\n```\n\n----------------------------------------\n\nTITLE: Initializing NanoBEIREvaluator for Information Retrieval Evaluation in Python\nDESCRIPTION: This code demonstrates how to initialize a NanoBEIREvaluator, which requires no additional datasets as it loads the relevant benchmark data directly from Hugging Face. This evaluator is used to assess model performance on information retrieval tasks.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/training_overview.md#2025-04-08_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers.evaluation import NanoBEIREvaluator\n\n# Initialize the evaluator. Unlike most other evaluators, this one loads the relevant datasets\n# directly from Hugging Face, so there's no mandatory arguments\ndev_evaluator = NanoBEIREvaluator()\n# You can run evaluation like so:\n# results = dev_evaluator(model)\n```\n\n----------------------------------------\n\nTITLE: Encoding Queries and Passages with Natural Questions Model in Python\nDESCRIPTION: This code snippet demonstrates how to use the SentenceTransformer model trained on Natural Questions dataset to encode a query and a passage, and then calculate their similarity. It uses the 'nq-distilbert-base-v1' model and includes encoding both the title and text of a passage.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/pretrained-models/nq-v1.md#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer, util\n\nmodel = SentenceTransformer(\"nq-distilbert-base-v1\")\n\nquery_embedding = model.encode(\"How many people live in London?\")\n\n# The passages are encoded as [ [title1, text1], [title2, text2], ...]\npassage_embedding = model.encode(\n    [[\"London\", \"London has 9,787,426 inhabitants at the 2011 census.\"]]\n)\n\nprint(\"Similarity:\", util.cos_sim(query_embedding, passage_embedding))\n```\n\n----------------------------------------\n\nTITLE: Initializing CrossEncoderTrainingArguments for sentence-transformers in Python\nDESCRIPTION: Example of initializing CrossEncoderTrainingArguments with common parameters for training a reranker model. The configuration includes settings for output directory, training epochs, batch sizes, learning rate, warmup ratio, precision settings, batch sampling, and various tracking/debugging parameters.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/cross_encoder/training_overview.md#2025-04-08_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers.cross_encoder import CrossEncoderTrainingArguments\n\nargs = CrossEncoderTrainingArguments(\n    # Required parameter:\n    output_dir=\"models/reranker-MiniLM-msmarco-v1\",\n    # Optional training parameters:\n    num_train_epochs=1,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    learning_rate=2e-5,\n    warmup_ratio=0.1,\n    fp16=True,  # Set to False if you get an error that your GPU can't run on FP16\n    bf16=False,  # Set to True if you have a GPU that supports BF16\n    batch_sampler=BatchSamplers.NO_DUPLICATES,  # losses that use \"in-batch negatives\" benefit from no duplicates\n    # Optional tracking/debugging parameters:\n    eval_strategy=\"steps\",\n    eval_steps=100,\n    save_strategy=\"steps\",\n    save_steps=100,\n    save_total_limit=2,\n    logging_steps=100,\n    run_name=\"reranker-MiniLM-msmarco-v1\",  # Will be used in W&B if `wandb` is installed\n)\n```\n\n----------------------------------------\n\nTITLE: Using a Merged Optimized ONNX Model from Hub\nDESCRIPTION: This snippet demonstrates how to use an optimized ONNX model that has been merged into the main repository on the Hugging Face Hub.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/usage/efficiency.rst#2025-04-08_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\n    \"all-MiniLM-L6-v2\",\n    backend=\"onnx\",\n    model_kwargs={\"file_name\": \"onnx/model_O3.onnx\"},\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring mixed precision training for CrossEncoder in Python\nDESCRIPTION: Comparing v3.x and v4.x approaches for enabling mixed precision training in CrossEncoder models. V4.x provides specific options for FP16 and BF16 precision.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/migration_guide.md#2025-04-08_snippet_29\n\nLANGUAGE: python\nCODE:\n```\n# Finetune the model\nmodel.fit(\n    train_dataloader=train_dataloader,\n    use_amp=True,\n)\n```\n\nLANGUAGE: python\nCODE:\n```\n# Prepare the Training Arguments\nargs = CrossEncoderTrainingArguments(\n    fp16=True,\n    bf16=False, # If your GPU supports it, you can also use bf16 instead\n)\n\n# Finetune the model\ntrainer = CrossEncoderTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    loss=loss,\n)\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: Running MLM Training with Dev Dataset\nDESCRIPTION: Extended command to run MLM training that includes both training and development datasets. Allows for validation during the training process.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/unsupervised_learning/MLM/README.md#2025-04-08_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython train_mlm.py distilbert-base path/train.txt path/dev.txt\n```\n\n----------------------------------------\n\nTITLE: Optimizing ONNX Model and Pushing to Hugging Face Hub\nDESCRIPTION: This snippet demonstrates how to optimize an ONNX model using Optimum and push it to the Hugging Face Hub. It uses optimization level O3 which includes basic and extended optimizations, transformer-specific fusions, and fast Gelu approximation.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/usage/efficiency.rst#2025-04-08_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer, export_optimized_onnx_model\n\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\", backend=\"onnx\")\nexport_optimized_onnx_model(\n    model,\n    \"O3\",\n    \"sentence-transformers/all-MiniLM-L6-v2\",\n    push_to_hub=True,\n    create_pr=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Using a Local Optimized ONNX Model\nDESCRIPTION: This snippet demonstrates how to use a locally optimized ONNX model by specifying the file path to the optimized model file in the model_kwargs parameter.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/usage/efficiency.rst#2025-04-08_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\n    \"path/to/my/mpnet-legal-finetuned\",\n    backend=\"onnx\",\n    model_kwargs={\"file_name\": \"onnx/model_O3.onnx\"},\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Multi-Task Learning with Multiple Loss Functions in SentenceTransformer\nDESCRIPTION: This code demonstrates multi-task learning by training a model with both ContrastiveLoss and MultipleNegativesRankingLoss simultaneously. It loads different subsets of the Quora dataset and configures the SentenceTransformerTrainer with multiple datasets and loss functions.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/training/quora_duplicate_questions/README.md#2025-04-08_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\nfrom sentence_transformers.losses import ContrastiveLoss, MultipleNegativesRankingLoss\nfrom sentence_transformers import SentenceTransformerTrainer, SentenceTransformer\n\nmodel_name = \"stsb-distilbert-base\"\nmodel = SentenceTransformer(model_name)\n\n# https://huggingface.co/datasets/sentence-transformers/quora-duplicates\nmnrl_dataset = load_dataset(\n    \"sentence-transformers/quora-duplicates\", \"triplet\", split=\"train\"\n)  # The \"pair\" subset also works\nmnrl_train_dataset = mnrl_dataset.select(range(100000))\nmnrl_eval_dataset = mnrl_dataset.select(range(100000, 101000))\n\nmnrl_train_loss = MultipleNegativesRankingLoss(model=model)\n\n# https://huggingface.co/datasets/sentence-transformers/quora-duplicates\ncl_dataset = load_dataset(\"sentence-transformers/quora-duplicates\", \"pair-class\", split=\"train\")\ncl_train_dataset = cl_dataset.select(range(100000))\ncl_eval_dataset = cl_dataset.select(range(100000, 101000))\n\ncl_train_loss = ContrastiveLoss(model=model, margin=0.5)\n\n# Create the trainer & start training\ntrainer = SentenceTransformerTrainer(\n    model=model,\n    train_dataset={\n        \"mnrl\": mnrl_train_dataset,\n        \"cl\": cl_train_dataset,\n    },\n    eval_dataset={\n        \"mnrl\": mnrl_eval_dataset,\n        \"cl\": cl_eval_dataset,\n    },\n    loss={\n        \"mnrl\": mnrl_train_loss,\n        \"cl\": cl_train_loss,\n    },\n)\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: Installing Sentence Transformers from Source\nDESCRIPTION: Commands for installing the library directly from GitHub source for accessing the latest development version, with various configuration options.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/installation.md#2025-04-08_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install git+https://github.com/UKPLab/sentence-transformers.git\npip install -U \"sentence-transformers[onnx-gpu] @ git+https://github.com/UKPLab/sentence-transformers.git\"\npip install -U \"sentence-transformers[onnx] @ git+https://github.com/UKPLab/sentence-transformers.git\"\npip install -U \"sentence-transformers[openvino] @ git+https://github.com/UKPLab/sentence-transformers.git\"\npip install -U \"sentence-transformers[train] @ git+https://github.com/UKPLab/sentence-transformers.git\"\npip install wandb\npip install codecarbon\npip install -U \"sentence-transformers[dev] @ git+https://github.com/UKPLab/sentence-transformers.git\"\n```\n\n----------------------------------------\n\nTITLE: Inference with a Matryoshka-trained model and truncated dimensions\nDESCRIPTION: Example of performing inference with a Matryoshka model where embeddings are truncated to a smaller dimension. The code demonstrates how to compute similarities between truncated embeddings.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/training/matryoshka/README.md#2025-04-08_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer\nimport torch.nn.functional as F\n\nmatryoshka_dim = 64\nmodel = SentenceTransformer(\n    \"nomic-ai/nomic-embed-text-v1.5\",\n    trust_remote_code=True,\n    truncate_dim=matryoshka_dim,\n)\n\nembeddings = model.encode(\n    [\n        \"search_query: What is TSNE?\",\n        \"search_document: t-distributed stochastic neighbor embedding (t-SNE) is a statistical method for visualizing high-dimensional data by giving each datapoint a location in a two or three-dimensional map.\",\n        \"search_document: Amelia Mary Earhart was an American aviation pioneer and writer.\",\n    ]\n)\nassert embeddings.shape[-1] == matryoshka_dim\n\nsimilarities = model.similarity(embeddings[0], embeddings[1:])\n# => tensor([[0.7839, 0.4933]])\n```\n\n----------------------------------------\n\nTITLE: Importing Additional SentenceTransformer Classes\nDESCRIPTION: Code snippet showing how to import additional specialized classes from the sentence_transformers.models module. These include various model components for specific techniques like asymmetric models, bag of words representations, CNN and LSTM architectures, normalization, static embeddings, and word embeddings.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/sentence_transformer/models.md#2025-04-08_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: sentence_transformers.models.Asym\n.. autoclass:: sentence_transformers.models.BoW\n.. autoclass:: sentence_transformers.models.CNN\n.. autoclass:: sentence_transformers.models.LSTM\n.. autoclass:: sentence_transformers.models.Normalize\n.. autoclass:: sentence_transformers.models.StaticEmbedding\n    :members: from_model2vec, from_distillation\n.. autoclass:: sentence_transformers.models.WeightedLayerPooling\n.. autoclass:: sentence_transformers.models.WordEmbeddings\n.. autoclass:: sentence_transformers.models.WordWeights\n```\n\n----------------------------------------\n\nTITLE: Loading Dataset from Hugging Face Hub\nDESCRIPTION: Example of loading a training and evaluation dataset from the Hugging Face Hub using the datasets library. Demonstrates loading the sentence-transformers/all-nli dataset with specific configuration.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/training_overview.md#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n\ntrain_dataset = load_dataset(\"sentence-transformers/all-nli\", \"pair-class\", split=\"train\")\neval_dataset = load_dataset(\"sentence-transformers/all-nli\", \"pair-class\", split=\"dev\")\n\nprint(train_dataset)\n\"\"\"\nDataset({\n    features: ['premise', 'hypothesis', 'label'],\n    num_rows: 942069\n})\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Using Sentence Transformer Models for Embeddings\nDESCRIPTION: Example of loading a pretrained Sentence Transformer model, encoding sentences to embeddings, and calculating embedding similarities.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/index.rst#2025-04-08_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer\n\n# 1. Load a pretrained Sentence Transformer model\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\n# The sentences to encode\nsentences = [\n    \"The weather is lovely today.\",\n    \"It's so sunny outside!\",\n    \"He drove to the stadium.\",\n]\n\n# 2. Calculate embeddings by calling model.encode()\nembeddings = model.encode(sentences)\nprint(embeddings.shape)\n# [3, 384]\n\n# 3. Calculate the embedding similarities\nsimilarities = model.similarity(embeddings, embeddings)\nprint(similarities)\n# tensor([[1.0000, 0.6660, 0.1046],\n#         [0.6660, 1.0000, 0.1411],\n#         [0.1046, 0.1411, 1.0000]])\n```\n\n----------------------------------------\n\nTITLE: Training Adaptive Layer Model with CoSENTLoss\nDESCRIPTION: Example showing how to set up an Adaptive Layer model for training using CoSENTLoss as the base loss function. The code demonstrates initializing a SentenceTransformer model with microsoft/mpnet-base and configuring the AdaptiveLayerLoss.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/training/adaptive_layer/README.md#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer\nfrom sentence_transformers.losses import CoSENTLoss, AdaptiveLayerLoss\n\nmodel = SentenceTransformer(\"microsoft/mpnet-base\")\n\nbase_loss = CoSENTLoss(model=model)\nloss = AdaptiveLayerLoss(model=model, loss=base_loss)\n```\n\n----------------------------------------\n\nTITLE: Loading Local CSV Data for Cross Encoder Training\nDESCRIPTION: This snippet shows how to load a local CSV file as a dataset for Cross Encoder training using the datasets library. It creates a dataset from a CSV file that can be used directly with Cross Encoder models.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/cross_encoder/training_overview.md#2025-04-08_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"csv\", data_files=\"my_file.csv\")\n```\n\n----------------------------------------\n\nTITLE: Training with Matryoshka2dLoss\nDESCRIPTION: Example showing how to combine Adaptive Layers with Matryoshka Embeddings using Matryoshka2dLoss. This allows for reduction in both layer count and output dimensions.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/training/adaptive_layer/README.md#2025-04-08_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer\nfrom sentence_transformers.losses import CoSENTLoss, Matryoshka2dLoss\n\nmodel = SentenceTransformer(\"microsoft/mpnet-base\")\n\nbase_loss = CoSENTLoss(model=model)\nloss = Matryoshka2dLoss(model=model, loss=base_loss, matryoshka_dims=[768, 512, 256, 128, 64])\n```\n\n----------------------------------------\n\nTITLE: Training Sentence Transformers Model in v2.x\nDESCRIPTION: This snippet demonstrates how to train a Sentence Transformers model using the v2.x approach. It includes defining the model, creating train examples, setting up a loss function, and using the fit() method for training.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/migration_guide.md#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer, InputExample, losses\nfrom torch.utils.data import DataLoader\n\n# 1. Define the model. Either from scratch of by loading a pre-trained model\nmodel = SentenceTransformer(\"microsoft/mpnet-base\")\n\n# 2. Define your train examples. You need more than just two examples...\ntrain_examples = [\n    InputExample(texts=[\n        \"A person on a horse jumps over a broken down airplane.\",\n        \"A person is outdoors, on a horse.\",\n        \"A person is at a diner, ordering an omelette.\",\n    ]),\n    InputExample(texts=[\n        \"Children smiling and waving at camera\",\n        \"There are children present\",\n        \"The kids are frowning\",\n    ]),\n]\ntrain_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n\n# 3. Define a loss function\ntrain_loss = losses.MultipleNegativesRankingLoss(model)\n\n# 4. Finetune the model\nmodel.fit(\n    train_objectives=[(train_dataloader, train_loss)],\n    epochs=1,\n    warmup_steps=100,\n)\n\n# 5. Save the trained model\nmodel.save_pretrained(\"models/mpnet-base-all-nli\")\n```\n\n----------------------------------------\n\nTITLE: Deprecated Sentence Transformer Training Method in Python\nDESCRIPTION: Example of the deprecated training approach using SentenceTransformer.fit() with DataLoader and InputExample. This method is still supported but will initialize a SentenceTransformerTrainer behind the scenes in v3.0+.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/training_overview.md#2025-04-08_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer, InputExample, losses\nfrom torch.utils.data import DataLoader\n\n# Define the model. Either from scratch of by loading a pre-trained model\nmodel = SentenceTransformer(\"distilbert/distilbert-base-uncased\")\n\n# Define your train examples. You need more than just two examples...\ntrain_examples = [\n    InputExample(texts=[\"My first sentence\", \"My second sentence\"], label=0.8),\n    InputExample(texts=[\"Another pair\", \"Unrelated sentence\"], label=0.3),\n]\n\n# Define your train dataset, the dataloader and the train loss\ntrain_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\ntrain_loss = losses.CosineSimilarityLoss(model)\n\n# Tune the model\nmodel.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1, warmup_steps=100)\n```\n\n----------------------------------------\n\nTITLE: Text-to-Image Search Examples in Python\nDESCRIPTION: Demonstrates text-to-image search capability by searching for various text queries like \"Two dogs playing in the snow\", \"A sunset on the beach\", and other descriptions to find matching images from the dataset.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/image-search/Image_Search.ipynb#2025-04-08_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nsearch(\"Two dogs playing in the snow\")\n```\n\nLANGUAGE: python\nCODE:\n```\nsearch(\"A sunset on the beach\")\n```\n\nLANGUAGE: python\nCODE:\n```\nsearch(\"London\")\n```\n\nLANGUAGE: python\nCODE:\n```\nsearch(\"A dog in a park\")\n```\n\nLANGUAGE: python\nCODE:\n```\nsearch(\"A beach with palm trees\")\n```\n\n----------------------------------------\n\nTITLE: Inference with Truncated Layers\nDESCRIPTION: Complete example showing how to load a model, truncate its layers for faster inference, and compute sentence embeddings and similarities.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/training/adaptive_layer/README.md#2025-04-08_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\"tomaarsen/mpnet-base-nli-adaptive-layer\")\nnew_num_layers = 3\nmodel[0].auto_model.encoder.layer = model[0].auto_model.encoder.layer[:new_num_layers]\n\nembeddings = model.encode(\n    [\n        \"The weather is so nice!\",\n        \"It's so sunny outside!\",\n        \"He drove to the stadium.\",\n    ]\n)\n# Similarity of the first sentence with the other two\nsimilarities = model.similarity(embeddings[0], embeddings[1:])\n# => tensor([[0.7761, 0.1655]])\n# compared to tensor([[ 0.7547, -0.0162]]) for the full model\n```\n\n----------------------------------------\n\nTITLE: Migrating scheduler parameter in SentenceTransformer.fit (Python)\nDESCRIPTION: This snippet shows how to migrate the scheduler parameter from SentenceTransformer.fit in v2.x to using SentenceTransformerTrainingArguments in v3.x. It demonstrates setting the learning rate scheduler type.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/migration_guide.md#2025-04-08_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n...\n\n# Finetune the model\nmodel.fit(\n    train_objectives=[(train_dataloader, train_loss)],\n    scheduler=\"WarmupLinear\",\n)\n```\n\nLANGUAGE: python\nCODE:\n```\n...\n\n# Prepare the Training Arguments\nargs = SentenceTransformerTrainingArguments(\n    # See https://huggingface.co/docs/transformers/main_classes/optimizer_schedules#transformers.SchedulerType\n    lr_scheduler_type=\"linear\"\n)\n\n# Finetune the model\ntrainer = SentenceTransformerTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    loss=loss,\n)\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: Using an Optimized ONNX Model from a Pull Request\nDESCRIPTION: This snippet shows how to use an optimized ONNX model from a pending pull request on the Hugging Face Hub, specifying the PR number in the revision parameter.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/usage/efficiency.rst#2025-04-08_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer\n\npull_request_nr = 2 # TODO: Update this to the number of your pull request\nmodel = SentenceTransformer(\n    \"all-MiniLM-L6-v2\",\n    backend=\"onnx\",\n    model_kwargs={\"file_name\": \"onnx/model_O3.onnx\"},\n    revision=f\"refs/pr/{pull_request_nr}\"\n)\n```\n\n----------------------------------------\n\nTITLE: Creating MSE Evaluator for Multilingual Model Training\nDESCRIPTION: Implementation of an MSE evaluator that measures the mean squared error between student and teacher embeddings during multilingual model training. It compares teacher embeddings of English sentences with student embeddings of French translations.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/training/multilingual/README.md#2025-04-08_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n\neval_dataset = load_dataset(\"sentence-transformers/parallel-sentences-talks\", \"en-fr\", split=\"dev\")\n\ndev_mse = MSEEvaluator(\n    source_sentences=eval_dataset[\"english\"],\n    target_sentences=eval_dataset[\"non_english\"],\n    name=\"en-fr-dev\",\n    teacher_model=teacher_model,\n    batch_size=32,\n)\n```\n\n----------------------------------------\n\nTITLE: Migrating warmup_steps parameter in SentenceTransformer.fit (Python)\nDESCRIPTION: This snippet demonstrates how to migrate the warmup_steps parameter from SentenceTransformer.fit in v2.x to using SentenceTransformerTrainingArguments in v3.x. It shows how to set the number of warmup steps for the learning rate scheduler.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/migration_guide.md#2025-04-08_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n...\n\n# Finetune the model\nmodel.fit(\n    train_objectives=[(train_dataloader, train_loss)],\n    warmup_steps=1000,\n)\n```\n\nLANGUAGE: python\nCODE:\n```\n...\n\n# Prepare the Training Arguments\nargs = SentenceTransformerTrainingArguments(\n    warmup_steps=1000,\n)\n\n# Finetune the model\ntrainer = SentenceTransformerTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    loss=loss,\n)\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: Complete HPO Implementation Example\nDESCRIPTION: Full implementation example showing how to set up and run hyperparameter optimization for sentence transformers, including dataset loading, evaluator setup, and trainer configuration.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/training/hpo/README.rst#2025-04-08_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import losses\nfrom sentence_transformers import SentenceTransformer, SentenceTransformerTrainer, SentenceTransformerTrainingArguments\nfrom sentence_transformers.evaluation import EmbeddingSimilarityEvaluator, SimilarityFunction\nfrom sentence_transformers.training_args import BatchSamplers\nfrom datasets import load_dataset\n\n# 1. Load the AllNLI dataset: https://huggingface.co/datasets/sentence-transformers/all-nli, only 10k train and 1k dev\ntrain_dataset = load_dataset(\"sentence-transformers/all-nli\", \"triplet\", split=\"train[:10000]\")\neval_dataset = load_dataset(\"sentence-transformers/all-nli\", \"triplet\", split=\"dev[:1000]\")\n\n# 2. Create an evaluator to perform useful HPO\nstsb_eval_dataset = load_dataset(\"sentence-transformers/stsb\", split=\"validation\")\ndev_evaluator = EmbeddingSimilarityEvaluator(\n    sentences1=stsb_eval_dataset[\"sentence1\"],\n    sentences2=stsb_eval_dataset[\"sentence2\"],\n    scores=stsb_eval_dataset[\"score\"],\n    main_similarity=SimilarityFunction.COSINE,\n    name=\"sts-dev\",\n)\n\n# 3. Define the Hyperparameter Search Space\ndef hpo_search_space(trial):\n    return {\n        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 1, 2),\n        \"per_device_train_batch_size\": trial.suggest_int(\"per_device_train_batch_size\", 32, 128),\n        \"warmup_ratio\": trial.suggest_float(\"warmup_ratio\", 0, 0.3),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n    }\n\n# 4. Define the Model Initialization\ndef hpo_model_init(trial):\n    return SentenceTransformer(\"distilbert-base-uncased\")\n\n# 5. Define the Loss Initialization\ndef hpo_loss_init(model):\n    return losses.MultipleNegativesRankingLoss(model)\n\n# 6. Define the Objective Function\ndef hpo_compute_objective(metrics):\n    return metrics[\"eval_sts-dev_spearman_cosine\"]\n\n# 7. Define the training arguments\nargs = SentenceTransformerTrainingArguments(\n    output_dir=\"checkpoints\",\n    fp16=True,\n    bf16=False,\n    batch_sampler=BatchSamplers.NO_DUPLICATES,\n    eval_strategy=\"no\",\n    save_strategy=\"no\",\n    logging_steps=10,\n    run_name=\"hpo\",\n)\n\n# 8. Create the trainer with model_init rather than model\ntrainer = SentenceTransformerTrainer(\n    model=None,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    evaluator=dev_evaluator,\n    model_init=hpo_model_init,\n    loss=hpo_loss_init,\n)\n\n# 9. Perform the HPO\nbest_trial = trainer.hyperparameter_search(\n    hp_space=hpo_search_space,\n    compute_objective=hpo_compute_objective,\n    n_trials=20,\n    direction=\"maximize\",\n    backend=\"optuna\",\n)\nprint(best_trial)\n```\n\n----------------------------------------\n\nTITLE: Mining Hard Negatives with Sentence Transformers\nDESCRIPTION: Demonstrates how to mine hard negatives from a dataset using sentence-transformers library. Uses the GooAQ dataset and a pre-trained embedding model to find challenging negative examples for training. Includes various parameters for controlling the mining process such as similarity thresholds and batch sizes.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/cross_encoder/training_overview.md#2025-04-08_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\nfrom sentence_transformers import SentenceTransformer\nfrom sentence_transformers.util import mine_hard_negatives\n\n# Load the GooAQ dataset: https://huggingface.co/datasets/sentence-transformers/gooaq\ntrain_dataset = load_dataset(\"sentence-transformers/gooaq\", split=f\"train\").select(range(100_000))\nprint(train_dataset)\n\n# Mine hard negatives using a very efficient embedding model\nembedding_model = SentenceTransformer(\"sentence-transformers/static-retrieval-mrl-en-v1\", device=\"cpu\")\nhard_train_dataset = mine_hard_negatives(\n    train_dataset,\n    embedding_model,\n    num_negatives=5,  # How many negatives per question-answer pair\n    range_min=10,  # Skip the x most similar samples\n    range_max=100,  # Consider only the x most similar samples\n    max_score=0.8,  # Only consider samples with a similarity score of at most x\n    margin=0.1,  # Similarity between query and negative samples should be x lower than query-positive similarity\n    sampling_strategy=\"top\",  # Sample the top negatives from the range\n    batch_size=4096,  # Use a batch size of 4096 for the embedding model\n    output_format=\"labeled-pair\",  # The output format is (query, passage, label), as required by BinaryCrossEntropyLoss\n    use_faiss=True,  # Using FAISS is recommended to keep memory usage low (pip install faiss-gpu or pip install faiss-cpu)\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Training Dataset with Images and Captions in Python\nDESCRIPTION: This snippet creates a training dataset by pairing images with captions. It demonstrates how to create positive and negative examples for contrastive learning using InputExample class.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/training/clip/train_clip.ipynb#2025-04-08_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport random\n\ntrain_dataset = []\nfor idx in range(0, len(photos), 2):\n    # We can use image pairs directly. Because our images aren't labeled, we use a random label as an example\n    # train_dataset.append(InputExample(texts=[photos[idx], photos[idx + 1]], label=random.choice([0, 1])))\n    \n    # Or images and text together\n    train_dataset.append(InputExample(texts=[photos[idx], \"This is the caption\"], label=1))\n    train_dataset.append(InputExample(texts=[photos[idx], \"This is another unrelated caption\"], label=0))\n\n    # Or just texts\n    # train_dataset.append(InputExample(texts=[\"This is a caption\", \"This is a similar caption\"], label=1))\n    # train_dataset.append(InputExample(texts=[\"This is a caption\", \"This is an unrelated caption\"], label=0))\n```\n\n----------------------------------------\n\nTITLE: Importing InformationRetrievalEvaluator in Python\nDESCRIPTION: This code snippet illustrates how to import and use the InformationRetrievalEvaluator class from the sentence_transformers.evaluation module.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/sentence_transformer/evaluation.md#2025-04-08_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: sentence_transformers.evaluation.InformationRetrievalEvaluator\n```\n\n----------------------------------------\n\nTITLE: Migrating epochs Parameter in CrossEncoder\nDESCRIPTION: Shows how to migrate from using epochs parameter in CrossEncoder.fit to using CrossEncoderTrainingArguments in v4.x. In v4.x, training epochs are configured using the num_train_epochs parameter in CrossEncoderTrainingArguments.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/migration_guide.md#2025-04-08_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n...\n\n# Finetune the model\nmodel.fit(\n    train_dataloader=train_dataloader,\n    epochs=1,\n)\n```\n\nLANGUAGE: python\nCODE:\n```\n...\n\n# Prepare the Training Arguments\nargs = CrossEncoderTrainingArguments(\n    num_train_epochs=1,\n)\n\n# Finetune the model\ntrainer = CrossEncoderTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    loss=loss,\n)\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: Citing GPL Paper in BibTeX Format\nDESCRIPTION: BibTeX citation for the paper 'GPL: Generative Pseudo Labeling for Unsupervised Domain Adaptation of Dense Retrieval'\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/domain_adaptation/README.md#2025-04-08_snippet_1\n\nLANGUAGE: bibtex\nCODE:\n```\n@inproceedings{wang-2021-GPL,\n    title = \"GPL: Generative Pseudo Labeling for Unsupervised Domain Adaptation of Dense Retrieval\",\n    author = \"Wang, Kexin and Thakur, Nandan and Reimers, Nils and Gurevych, Iryna\", \n    journal= \"arXiv preprint arXiv:2112.07577\",\n    month = \"12\",\n    year = \"2021\",\n    url = \"https://arxiv.org/abs/2112.07577\",\n}\n```\n\n----------------------------------------\n\nTITLE: Preparing DataLoader and Loss Function for CLIP Model Training in Python\nDESCRIPTION: This code creates a DataLoader for batching the training data and prepares a ContrastiveLoss function for model training.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/training/clip/train_clip.ipynb#2025-04-08_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# We'll create a DataLoader that batches our data and prepare a contrastive loss function\n\ntrain_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=4)\ntrain_loss = losses.ContrastiveLoss(model=model)\n```\n\n----------------------------------------\n\nTITLE: Initializing CoSENTLoss with Sentence Transformer\nDESCRIPTION: Demonstrates how to initialize a CoSENTLoss function with a pre-trained model and load a compatible dataset. Uses xlm-roberta-base as the base model and loads the all-nli dataset for training.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/training_overview.md#2025-04-08_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\nfrom sentence_transformers import SentenceTransformer\nfrom sentence_transformers.losses import CoSENTLoss\n\n# Load a model to train/finetune\nmodel = SentenceTransformer(\"xlm-roberta-base\")\n\n# Initialize the CoSENTLoss\n# This loss requires pairs of text and a float similarity score as a label\nloss = CoSENTLoss(model)\n\n# Load an example training dataset that works with our loss function:\ntrain_dataset = load_dataset(\"sentence-transformers/all-nli\", \"pair-score\", split=\"train\")\n\"\"\"\nDataset({\n    features: ['sentence1', 'sentence2', 'label'],\n    num_rows: 942069\n})\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Loading Extracted Images in Python\nDESCRIPTION: This code loads the extracted images from the local folder into a list of PIL Image objects.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/training/clip/train_clip.ipynb#2025-04-08_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Load each of the images\nphotos = [Image.open(os.path.join(img_folder, photo_path)) for photo_path in os.listdir(img_folder)]\nphotos[0]\n```\n\n----------------------------------------\n\nTITLE: Initializing Cross-Encoder Model and Loading Dataset in Python\nDESCRIPTION: This code snippet initializes a CrossEncoder model with model card data, loads the GooAQ dataset, and splits it into train and test sets. It also sets up parameters for training.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/cross_encoder/training_overview.md#2025-04-08_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef main():\n    model_name = \"answerdotai/ModernBERT-base\"\n\n    train_batch_size = 64\n    num_epochs = 1\n    num_hard_negatives = 5  # How many hard negatives should be mined for each question-answer pair\n\n    # 1a. Load a model to finetune with 1b. (Optional) model card data\n    model = CrossEncoder(\n        model_name,\n        model_card_data=CrossEncoderModelCardData(\n            language=\"en\",\n            license=\"apache-2.0\",\n            model_name=\"ModernBERT-base trained on GooAQ\",\n        ),\n    )\n    print(\"Model max length:\", model.max_length)\n    print(\"Model num labels:\", model.num_labels)\n\n    # 2a. Load the GooAQ dataset: https://huggingface.co/datasets/sentence-transformers/gooaq\n    logging.info(\"Read the gooaq training dataset\")\n    full_dataset = load_dataset(\"sentence-transformers/gooaq\", split=\"train\").select(range(100_000))\n    dataset_dict = full_dataset.train_test_split(test_size=1_000, seed=12)\n    train_dataset = dataset_dict[\"train\"]\n    eval_dataset = dataset_dict[\"test\"]\n    logging.info(train_dataset)\n    logging.info(eval_dataset)\n```\n\n----------------------------------------\n\nTITLE: Installing Sentence Transformers with ONNX Support\nDESCRIPTION: This command installs Sentence Transformers with ONNX support for either CPU or GPU acceleration, which is required to use the ONNX backend.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/usage/efficiency.rst#2025-04-08_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install sentence-transformers[onnx-gpu]\n# or\npip install sentence-transformers[onnx]\n```\n\n----------------------------------------\n\nTITLE: Mining Hard Negatives for Training Dataset in Python\nDESCRIPTION: This snippet demonstrates how to mine hard negatives for the training dataset using a SentenceTransformer embedding model. It enhances the dataset with challenging negative examples for more effective training.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/cross_encoder/training_overview.md#2025-04-08_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# 2b. Modify our training dataset to include hard negatives using a very efficient embedding model\nembedding_model = SentenceTransformer(\"sentence-transformers/static-retrieval-mrl-en-v1\", device=\"cpu\")\nhard_train_dataset = mine_hard_negatives(\n    train_dataset,\n    embedding_model,\n    num_negatives=num_hard_negatives,  # How many negatives per question-answer pair\n    margin=0,  # Similarity between query and negative samples should be x lower than query-positive similarity\n    range_min=0,  # Skip the x most similar samples\n    range_max=100,  # Consider only the x most similar samples\n    sampling_strategy=\"top\",  # Sample the top negatives from the range\n    batch_size=4096,  # Use a batch size of 4096 for the embedding model\n    output_format=\"labeled-pair\",  # The output format is (query, passage, label), as required by BinaryCrossEntropyLoss\n    use_faiss=True,\n)\nlogging.info(hard_train_dataset)\n```\n\n----------------------------------------\n\nTITLE: Performing Inference with Pre-trained CrossEncoder for Duplicate Question Detection in Python\nDESCRIPTION: This code snippet shows how to use a pre-trained CrossEncoder model to predict similarity scores for pairs of questions. It demonstrates loading the model and using it to calculate scores for example question pairs.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/cross_encoder/training/quora_duplicate_questions/README.md#2025-04-08_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import CrossEncoder\n\nmodel = CrossEncoder('cross-encoder/quora-distilroberta-base')\nscores = model.predict([\n    ('What do apples consist of?', 'What are in Apple devices?'),\n    ('How do I get good at programming?', 'How to become a good programmer?')\n])\nprint(scores)\n# [0.00056, 0.97536]\n```\n\n----------------------------------------\n\nTITLE: Creating Training Dataset for STS using Hugging Face Datasets\nDESCRIPTION: Demonstrates how to create a simple training dataset for STS using the Hugging Face Datasets library with manual sentence pairs and similarity scores.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/cross_encoder/training/sts/README.md#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import Dataset\n\nsentence1_list = [\"My first sentence\", \"Another pair\"]\nsentence2_list = [\"My second sentence\", \"Unrelated sentence\"]\nlabels_list = [0.8, 0.3]\ntrain_dataset = Dataset.from_dict({\n    \"sentence1\": sentence1_list,\n    \"sentence2\": sentence2_list,\n    \"label\": labels_list,\n})\n# => Dataset({\n#     features: ['sentence1', 'sentence2', 'label'],\n#     num_rows: 2\n# })\nprint(train_dataset[0])\n# => {'sentence1': 'My first sentence', 'sentence2': 'My second sentence', 'label': 0.8}\nprint(train_dataset[1])\n# => {'sentence1': 'Another pair', 'sentence2': 'Unrelated sentence', 'label': 0.3}\n```\n\n----------------------------------------\n\nTITLE: Loading Pre-existing STS Dataset from Hugging Face\nDESCRIPTION: Shows how to load the pre-existing STSB dataset from the Hugging Face Hub for training STS models.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/cross_encoder/training/sts/README.md#2025-04-08_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n\ntrain_dataset = load_dataset(\"sentence-transformers/stsb\", split=\"train\")\n# => Dataset({\n#     features: ['sentence1', 'sentence2', 'score'],\n#     num_rows: 5749\n# })\n```\n\n----------------------------------------\n\nTITLE: Cross-Encoder Correlation Evaluation with STSb Dataset\nDESCRIPTION: Shows implementation of CrossEncoderCorrelationEvaluator using the STSb dataset. The evaluator measures correlation between model predictions and human-annotated similarity scores.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/cross_encoder/training_overview.md#2025-04-08_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\nfrom sentence_transformers import CrossEncoder\nfrom sentence_transformers.cross_encoder.evaluation import CrossEncoderCorrelationEvaluator\n\nmodel = CrossEncoder(\"cross-encoder/stsb-TinyBERT-L4\")\n\neval_dataset = load_dataset(\"sentence-transformers/stsb\", split=\"validation\")\npairs = list(zip(eval_dataset[\"sentence1\"], eval_dataset[\"sentence2\"]))\n\ndev_evaluator = CrossEncoderCorrelationEvaluator(\n    sentence_pairs=pairs,\n    scores=eval_dataset[\"score\"],\n    name=\"sts_dev\",\n)\n```\n\n----------------------------------------\n\nTITLE: Custom Loss Function Requirements in PyTorch\nDESCRIPTION: Guidelines for implementing custom loss functions for sentence transformers. The loss function must inherit from torch.nn.Module, have specific constructor and forward method signatures, and optionally implement configuration and citation methods.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/loss_overview.md#2025-04-08_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\nAdvanced users can create and train with their own loss functions. Custom loss functions only have a few requirements:\n\n- They must be a subclass of :class:`torch.nn.Module`.\n- They must have ``model`` as the first argument in the constructor.\n- They must implement a ``forward`` method that accepts ``sentence_features`` and ``labels``. The former is a list of tokenized batches, one element for each column. These tokenized batches can be fed directly to the ``model`` being trained to produce embeddings. The latter is an optional tensor of labels. The method must return a single loss value.\n\nTo get full support with the automatic model card generation, you may also wish to implement:\n\n- a ``get_config_dict`` method that returns a dictionary of loss parameters.\n- a ``citation`` property so your work gets cited in all models that train with the loss.\n\nConsider inspecting existing loss functions to get a feel for how loss functions are commonly implemented.\n```\n\n----------------------------------------\n\nTITLE: Using CrossEncoderRerankingEvaluator with Mined Negatives in Python\nDESCRIPTION: This example shows how to use the CrossEncoderRerankingEvaluator with mined negatives from the GooAQ dataset. It demonstrates loading a dataset, mining hard negatives, and setting up the evaluator to assess both the base retrieval model and the reranking performance of a Cross-Encoder.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/cross_encoder/training_overview.md#2025-04-08_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\nfrom sentence_transformers import SentenceTransformer\nfrom sentence_transformers.cross_encoder import CrossEncoder\nfrom sentence_transformers.cross_encoder.evaluation import CrossEncoderRerankingEvaluator\nfrom sentence_transformers.util import mine_hard_negatives\n\n# Load a model\nmodel = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L6-v2\")\n\n# Load the GooAQ dataset: https://huggingface.co/datasets/sentence-transformers/gooaq\nfull_dataset = load_dataset(\"sentence-transformers/gooaq\", split=f\"train\").select(range(100_000))\ndataset_dict = full_dataset.train_test_split(test_size=1_000, seed=12)\ntrain_dataset = dataset_dict[\"train\"]\neval_dataset = dataset_dict[\"test\"]\nprint(eval_dataset)\n\"\"\"\nDataset({\n    features: ['question', 'answer'],\n    num_rows: 1000\n})\n\"\"\"\n\n# Mine hard negatives using a very efficient embedding model\nembedding_model = SentenceTransformer(\"sentence-transformers/static-retrieval-mrl-en-v1\", device=\"cpu\")\nhard_eval_dataset = mine_hard_negatives(\n    eval_dataset,\n    embedding_model,\n    corpus=full_dataset[\"answer\"],  # Use the full dataset as the corpus\n    num_negatives=50,  # How many negatives per question-answer pair\n    batch_size=4096,  # Use a batch size of 4096 for the embedding model\n    output_format=\"n-tuple\",  # The output format is (query, positive, negative1, negative2, ...) for the evaluator\n    include_positives=True,  # Key: Include the positive answer in the list of negatives\n    use_faiss=True,  # Using FAISS is recommended to keep memory usage low (pip install faiss-gpu or pip install faiss-cpu)\n)\nprint(hard_eval_dataset)\n```\n\n----------------------------------------\n\nTITLE: Binary Quantization Implementation with Sentence Transformers\nDESCRIPTION: Demonstrates two approaches for generating binary quantized embeddings using the Sentence Transformers library. Shows direct encoding with binary precision and post-encoding quantization.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/embedding-quantization/README.md#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer\nfrom sentence_transformers.quantization import quantize_embeddings\n\n# 1. Load an embedding model\nmodel = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\")\n\n# 2a. Encode some text using \"binary\" quantization\nbinary_embeddings = model.encode(\n    [\"I am driving to the lake.\", \"It is a beautiful day.\"],\n    precision=\"binary\",\n)\n\n# 2b. or, encode some text without quantization & apply quantization afterwards\nembeddings = model.encode([\"I am driving to the lake.\", \"It is a beautiful day.\"])\nbinary_embeddings = quantize_embeddings(embeddings, precision=\"binary\")\n```\n\n----------------------------------------\n\nTITLE: Scalar Embedding Shape and Size Comparison\nDESCRIPTION: Code snippet comparing the characteristics of float32 and int8 scalar embeddings including shape, size, and data type.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/embedding-quantization/README.md#2025-04-08_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> embeddings.shape\n(2, 1024)\n>>> embeddings.nbytes\n8192\n>>> embeddings.dtype\nfloat32\n>>> int8_embeddings.shape\n(2, 1024)\n>>> int8_embeddings.nbytes\n2048\n>>> int8_embeddings.dtype\nint8\n```\n\n----------------------------------------\n\nTITLE: Saving an Exported Local ONNX Model\nDESCRIPTION: This snippet demonstrates how to save an exported ONNX model to a local path to avoid re-exporting it every time the code runs.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/usage/efficiency.rst#2025-04-08_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmodel = SentenceTransformer(\"path/to/my/model\", backend=\"onnx\")\nmodel.save_pretrained(\"path/to/my/model\")\n```\n\n----------------------------------------\n\nTITLE: Documenting CrossEncoderRerankingEvaluator Class\nDESCRIPTION: Auto-documentation directive for the CrossEncoderRerankingEvaluator class from the sentence-transformers cross-encoder evaluation module.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/cross_encoder/evaluation.md#2025-04-08_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: sentence_transformers.cross_encoder.evaluation.CrossEncoderRerankingEvaluator\n```\n\n----------------------------------------\n\nTITLE: Loading the STSB Dataset for Training\nDESCRIPTION: This code shows how to load the pre-defined STSB dataset from the Hugging Face datasets library. The dataset contains sentence pairs with similarity scores for training Sentence Transformer models.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/training/sts/README.md#2025-04-08_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n\ntrain_dataset = load_dataset(\"sentence-transformers/stsb\", split=\"train\")\n# => Dataset({\n#     features: ['sentence1', 'sentence2', 'score'],\n#     num_rows: 5749\n# })\n```\n\n----------------------------------------\n\nTITLE: Configuring ONNX Backend with Optimization Levels for Sentence Transformers\nDESCRIPTION: Code snippets showing how to use the ONNX backend with different optimization levels (O1-O4) for sentence embedding models, including floating point precision options.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/usage/efficiency.rst#2025-04-08_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nbackend=\"onnx\"\n```\n\nLANGUAGE: python\nCODE:\n```\nexport_optimized_onnx_model(..., \"O1\", ...)\n```\n\nLANGUAGE: python\nCODE:\n```\nexport_optimized_onnx_model(..., \"O2\", ...)\n```\n\nLANGUAGE: python\nCODE:\n```\nexport_optimized_onnx_model(..., \"O3\", ...)\n```\n\nLANGUAGE: python\nCODE:\n```\nexport_optimized_onnx_model(..., \"O4\", ...)\n```\n\n----------------------------------------\n\nTITLE: Inference with Distilled Cross Encoder Model\nDESCRIPTION: Python code demonstrating how to use a distilled cross-encoder model for text pair scoring and ranking, using the HuggingFace model tomaarsen/reranker-MiniLM-L12-H384-margin-mse.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/cross_encoder/training/distillation/README.md#2025-04-08_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import CrossEncoder\n\n# Download from the  Hub\nmodel = CrossEncoder(\"tomaarsen/reranker-modernbert-base-msmarco-margin-mse\")\n# Get scores for pairs of texts\npairs = [\n    [\"where is joplin airport\", \"Scott Joplin is important both as a composer for bringing ragtime to the concert hall, setting the stage (literally) for the rise of jazz; and as an early advocate for civil rights and education among American blacks. Joplin is a hero, and a national treasure of the United States.\"],\n    [\"where is joplin airport\", \"Flights from Jos to Abuja will get you to this shimmering Nigerian capital within approximately 19 hours. Flights depart from Yakubu Gowon Airport/ Jos Airport (JOS) and arrive at Nnamdi Azikiwe International Airport (ABV). Arik Air is the main airline flying the Jos to Abuja route.\"],\n    [\"where is joplin airport\", \"Janis Joplin returned to the music scene, knowing it was her destiny, in 1966. A friend, Travis Rivers, recruited her to audition for the psychedelic band, Big Brother and the Holding Company, based in San Francisco. The band was quite big in San Francisco at the time, and Joplin landed the gig.\"],\n    [\"where is joplin airport\", \"Joplin Regional Airport. Joplin Regional Airport (IATA: JLN, ICAO: KJLN, FAA LID: JLN) is a city-owned airport four miles north of Joplin, in Jasper County, Missouri. It has airline service subsidized by the Essential Air Service program. Airline flights and general aviation are in separate terminals.\"],\n    [\"where is joplin airport\", 'Trolley and rail lines made Joplin the hub of southwest Missouri. As the center of the \"Tri-state district\", it soon became the lead- and zinc-mining capital of the world. As a result of extensive surface and deep mining, Joplin is dotted with open-pit mines and mine shafts.'],\n]\nscores = model.predict(pairs)\nprint(scores)\n# [0.00410349 0.03430534 0.5108879  0.999984   0.91639173]\n\n# Or rank different texts based on similarity to a single text\nranks = model.rank(\n    \"where is joplin airport\",\n    [\n        \"Scott Joplin is important both as a composer for bringing ragtime to the concert hall, setting the stage (literally) for the rise of jazz; and as an early advocate for civil rights and education among American blacks. Joplin is a hero, and a national treasure of the United States.\",\n        \"Flights from Jos to Abuja will get you to this shimmering Nigerian capital within approximately 19 hours. Flights depart from Yakubu Gowon Airport/ Jos Airport (JOS) and arrive at Nnamdi Azikiwe International Airport (ABV). Arik Air is the main airline flying the Jos to Abuja route.\",\n        \"Janis Joplin returned to the music scene, knowing it was her destiny, in 1966. A friend, Travis Rivers, recruited her to audition for the psychedelic band, Big Brother and the Holding Company, based in San Francisco. The band was quite big in San Francisco at the time, and Joplin landed the gig.\",\n        \"Joplin Regional Airport. Joplin Regional Airport (IATA: JLN, ICAO: KJLN, FAA LID: JLN) is a city-owned airport four miles north of Joplin, in Jasper County, Missouri. It has airline service subsidized by the Essential Air Service program. Airline flights and general aviation are in separate terminals.\",\n        'Trolley and rail lines made Joplin the hub of southwest Missouri. As the center of the \"Tri-state district\", it soon became the lead- and zinc-mining capital of the world. As a result of extensive surface and deep mining, Joplin is dotted with open-pit mines and mine shafts.',\n    ],\n)\nprint(ranks)\n# [\n#     {\"corpus_id\": 3, \"score\": 0.999984},\n#     {\"corpus_id\": 4, \"score\": 0.91639173},\n#     {\"corpus_id\": 2, \"score\": 0.5108879},\n#     {\"corpus_id\": 1, \"score\": 0.03430534},\n#     {\"corpus_id\": 0, \"score\": 0.004103488},\n# ]\n```\n\n----------------------------------------\n\nTITLE: Quantizing ONNX Models for Sentence Transformers\nDESCRIPTION: Example of exporting and quantizing an ONNX model to int8 precision with specific instruction set optimizations for improved CPU performance.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/usage/efficiency.rst#2025-04-08_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nexport_dynamic_quantized_onnx_model(..., \"avx512_vnni\", ...)\n```\n\n----------------------------------------\n\nTITLE: Documenting CrossEncoderCorrelationEvaluator Class\nDESCRIPTION: Auto-documentation directive for the CrossEncoderCorrelationEvaluator class from the sentence-transformers cross-encoder evaluation module.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/cross_encoder/evaluation.md#2025-04-08_snippet_3\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: sentence_transformers.cross_encoder.evaluation.CrossEncoderCorrelationEvaluator\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Dataset with Preprocessing for Cross Encoder Training\nDESCRIPTION: This snippet shows how to create a custom dataset from preprocessed data using the datasets library. It initializes a Dataset object from a dictionary of lists containing anchor and positive text pairs for training Cross Encoder models.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/cross_encoder/training_overview.md#2025-04-08_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import Dataset\n\nanchors = []\npositives = []\n# Open a file, do preprocessing, filtering, cleaning, etc.\n# and append to the lists\n\ndataset = Dataset.from_dict({\n    \"anchor\": anchors,\n    \"positive\": positives,\n})\n```\n\n----------------------------------------\n\nTITLE: Using Prompts with Sentence Transformers for Text Embedding\nDESCRIPTION: This code demonstrates how to use prompts with a pre-trained embedding model. It shows how to encode queries with specific prompts and compare embeddings using the similarity function.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/training/prompts/README.md#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\")\nquery_embedding = model.encode(\"What are Pandas?\", prompt_name=\"query\")\n# or\n# query_embedding = model.encode(\"What are Pandas?\", prompt=\"Represent this sentence for searching relevant passages: \")\ndocument_embeddings = model.encode([\n    \"Pandas is a software library written for the Python programming language for data manipulation and analysis.\",\n    \"Pandas are a species of bear native to South Central China. They are also known as the giant panda or simply panda.\",\n    \"Koala bears are not actually bears, they are marsupials native to Australia.\",\n])\nsimilarity = model.similarity(query_embedding, document_embeddings)\nprint(similarity)\n# => tensor([[0.7594, 0.7560, 0.4674]])\n```\n\n----------------------------------------\n\nTITLE: Quantizing ONNX Model with Hugging Face Hub Integration\nDESCRIPTION: Example of quantizing a Sentence Transformer model to int8 using ONNX runtime and pushing to Hugging Face Hub. Shows initial quantization, pre-merge usage, and post-merge usage patterns.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/usage/efficiency.rst#2025-04-08_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer, export_dynamic_quantized_onnx_model\n\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\", backend=\"onnx\")\nexport_dynamic_quantized_onnx_model(\n    model,\n    \"avx512_vnni\",\n    \"sentence-transformers/all-MiniLM-L6-v2\",\n    push_to_hub=True,\n    create_pr=True,\n)\n```\n\n----------------------------------------\n\nTITLE: SentenceTransformer Class Documentation\nDESCRIPTION: RST documentation block for the SentenceTransformer class, configuring autodoc to show specific members and exclude others.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/sentence_transformer/SentenceTransformer.md#2025-04-08_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: sentence_transformers.SentenceTransformer\n   :members:\n   :inherited-members: fit, old_fit\n   :exclude-members: save, save_to_hub, add_module, append, apply, buffers, children, extra_repr, forward, get_buffer, get_extra_state, get_parameter, get_submodule, ipu, load_state_dict, modules, named_buffers, named_children, named_modules, named_parameters, parameters, register_backward_hook, register_buffer, register_forward_hook, register_forward_pre_hook, register_full_backward_hook, register_full_backward_pre_hook, register_load_state_dict_post_hook, register_module, register_parameter, register_state_dict_pre_hook, requires_grad_, set_extra_state, share_memory, state_dict, to_empty, type, xpu, zero_grad\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Dataset with Preprocessing\nDESCRIPTION: Example of creating a custom dataset using Dataset.from_dict() for cases requiring preprocessing. Shows how to build a dataset from lists of anchors and positives.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/training_overview.md#2025-04-08_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import Dataset\n\nanchors = []\npositives = []\n# Open a file, do preprocessing, filtering, cleaning, etc.\n# and append to the lists\n\ndataset = Dataset.from_dict({\n    \"anchor\": anchors,\n    \"positive\": positives,\n})\n```\n\n----------------------------------------\n\nTITLE: Implementing CrossEncoderNanoBEIREvaluator in Python\nDESCRIPTION: This snippet demonstrates how to initialize and use the CrossEncoderNanoBEIREvaluator, which loads relevant benchmark datasets directly from Hugging Face without requiring manual data preparation. This evaluator is useful for tracking model performance on standard benchmarks.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/cross_encoder/training_overview.md#2025-04-08_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import CrossEncoder\nfrom sentence_transformers.cross_encoder.evaluation import CrossEncoderNanoBEIREvaluator\n\n# Load a model\nmodel = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L6-v2\")\n\n# Initialize the evaluator. Unlike most other evaluators, this one loads the relevant datasets\n# directly from Hugging Face, so there's no mandatory arguments\ndev_evaluator = CrossEncoderNanoBEIREvaluator()\n# You can run evaluation like so:\n# results = dev_evaluator(model)\n```\n\n----------------------------------------\n\nTITLE: Migrating evaluation_steps parameter in SentenceTransformer.fit (Python)\nDESCRIPTION: This snippet shows how to migrate the evaluation_steps parameter from SentenceTransformer.fit in v2.x to using SentenceTransformerTrainingArguments in v3.x. It demonstrates setting the frequency of evaluation during training.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/migration_guide.md#2025-04-08_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n...\n\n# Finetune the model\nmodel.fit(\n    train_objectives=[(train_dataloader, train_loss)],\n    evaluator=evaluator,\n    evaluation_steps=1000,\n)\n```\n\nLANGUAGE: python\nCODE:\n```\n...\n\n# Prepare the Training Arguments\nargs = SentenceTransformerTrainingArguments(\n\n\n)\n\n# Finetune the model\ntrainer = SentenceTransformerTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    loss=loss,\n    evaluator=evaluator,\n)\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: Loading Multilingual CLIP Model in Python\nDESCRIPTION: This snippet loads the multilingual CLIP model using SentenceTransformer. The model is capable of encoding text in multiple languages.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/image-search/Image_Search-multilingual.ipynb#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer, util\nfrom PIL import Image\nimport glob\nimport torch\nimport pickle\nimport zipfile\nfrom IPython.display import display\nfrom IPython.display import Image as IPImage\nimport os\nfrom tqdm.autonotebook import tqdm\n\n# Here we load the multilingual CLIP model. Note, this model can only encode text.\n# If you need embeddings for images, you must load the 'clip-ViT-B-32' model\nmodel = SentenceTransformer('clip-ViT-B-32-multilingual-v1')\n```\n\n----------------------------------------\n\nTITLE: Defining Training Loss and Evaluators for Cross-Encoder in Python\nDESCRIPTION: This code defines the training loss (BinaryCrossEntropyLoss) and sets up evaluators (CrossEncoderNanoBEIREvaluator and CrossEncoderRerankingEvaluator) for the cross-encoder model. It also prepares a hard evaluation dataset.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/cross_encoder/training_overview.md#2025-04-08_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# 3. Define our training loss.\n# pos_weight is recommended to be set as the ratio between positives to negatives, a.k.a. `num_hard_negatives`\nloss = BinaryCrossEntropyLoss(model=model, pos_weight=torch.tensor(num_hard_negatives))\n\n# 4a. Define evaluators. We use the CrossEncoderNanoBEIREvaluator, which is a light-weight evaluator for English reranking\nnano_beir_evaluator = CrossEncoderNanoBEIREvaluator(\n    dataset_names=[\"msmarco\", \"nfcorpus\", \"nq\"],\n    batch_size=train_batch_size,\n)\n\n# 4b. Define a reranking evaluator by mining hard negatives given query-answer pairs\n# We include the positive answer in the list of negatives, so the evaluator can use the performance of the\n# embedding model as a baseline.\nhard_eval_dataset = mine_hard_negatives(\n    eval_dataset,\n    embedding_model,\n    corpus=full_dataset[\"answer\"],  # Use the full dataset as the corpus\n    num_negatives=30,  # How many documents to rerank\n    batch_size=4096,\n    include_positives=True,\n    output_format=\"n-tuple\",\n    use_faiss=True,\n)\nlogging.info(hard_eval_dataset)\nreranking_evaluator = CrossEncoderRerankingEvaluator(\n    samples=[\n        {\n            \"query\": sample[\"question\"],\n            \"positive\": [sample[\"answer\"]],\n        }\n    ]\n```\n\n----------------------------------------\n\nTITLE: Cross Encoder Training with Margin MSE Loss\nDESCRIPTION: RST documentation describing the use of MarginMSELoss for knowledge distillation, working with triplets and precomputed logit differences.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/cross_encoder/training/distillation/README.md#2025-04-08_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\nThis is the same setup as the previous script, but now using the :class:`~sentence_transformers.cross_encoder.losses.MarginMSELoss` as used in the aforementioned `Hosttter et al. <https://arxiv.org/abs/2010.02666>`_.\n\n:class:`~sentence_transformers.cross_encoder.losses.MarginMSELoss` does not work with (query, answer) pairs and a precomputed logit, but with (query, correct_answer, incorrect_answer) triplets and a precomputed logit that corresponds to ``teacher.predict([query, correct_answer]) - teacher.predict([query, incorrect_answer])``. In short, this precomputed logit is the *difference* between (query, correct_answer) and (query, incorrect_answer).\n```\n\n----------------------------------------\n\nTITLE: Cross-Encoder Classification Evaluation with AllNLI Dataset\nDESCRIPTION: Demonstrates CrossEncoderClassificationEvaluator setup using the AllNLI dataset. Includes label mapping and evaluation for natural language inference classification tasks.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/cross_encoder/training_overview.md#2025-04-08_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\nfrom sentence_transformers import CrossEncoder\nfrom sentence_transformers.evaluation import TripletEvaluator, SimilarityFunction\n\nmodel = CrossEncoder(\"cross-encoder/nli-deberta-v3-base\")\n\nmax_samples = 1000\neval_dataset = load_dataset(\"sentence-transformers/all-nli\", \"pair-class\", split=f\"dev[:{max_samples}]\")\n\npairs = list(zip(eval_dataset[\"premise\"], eval_dataset[\"hypothesis\"]))\nlabel_mapping = {0: 1, 1: 2, 2: 0}\nlabels = [label_mapping[label] for label in eval_dataset[\"label\"]]\n\ncls_evaluator = CrossEncoderClassificationEvaluator(\n    sentence_pairs=pairs,\n    labels=labels,\n    name=\"all-nli-dev\",\n)\n```\n\n----------------------------------------\n\nTITLE: Searching for Oldest US President Information\nDESCRIPTION: This code executes a search query about the oldest US president. It shows how the system handles superlative queries (oldest, biggest, etc.) related to political figures.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/retrieve_rerank/retrieve_rerank_simple_wikipedia.ipynb#2025-04-08_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nsearch(query = \"Oldest US president\")\n```\n\n----------------------------------------\n\nTITLE: Displaying Duplicate Images\nDESCRIPTION: Displays the top 10 most similar (identical) images found in the dataset with their similarity scores.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/image-search/Image_Duplicates.ipynb#2025-04-08_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfor score, idx1, idx2 in duplicates[0:10]:\n    print(\"\\nScore: {:.3f}\".format(score))\n    print(img_names[idx1])\n    display(IPImage(os.path.join(img_folder, img_names[idx1]), width=200))\n    print( img_names[idx2])\n    display(IPImage(os.path.join(img_folder, img_names[idx2]), width=200))\n```\n\n----------------------------------------\n\nTITLE: Cross-Encoder Reranking Evaluation with GooAQ Dataset\nDESCRIPTION: Demonstrates how to use CrossEncoderRerankingEvaluator to evaluate a Cross-Encoder model on question-answer ranking. The evaluator uses batched processing and calculates MAP, MRR@10, and NDCG@10 metrics.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/cross_encoder/training_overview.md#2025-04-08_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nreranking_evaluator = CrossEncoderRerankingEvaluator(\n    samples=[\n        {\n            \"query\": sample[\"question\"],\n            \"positive\": [sample[\"answer\"]],\n            \"documents\": [sample[column_name] for column_name in hard_eval_dataset.column_names[2:]],\n        }\n        for sample in hard_eval_dataset\n    ],\n    batch_size=32,\n    name=\"gooaq-dev\",\n)\nresults = reranking_evaluator(model)\n```\n\n----------------------------------------\n\nTITLE: Sample English Sentences Input\nDESCRIPTION: Example set of English sentences used to demonstrate the input format for bitext mining.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/parallel-sentence-mining/README.md#2025-04-08_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nThis is an example sentences.\nHello World!\nMy final third sentence in this list.\n```\n\n----------------------------------------\n\nTITLE: Loading Local JSON Data for Cross Encoder Training\nDESCRIPTION: This snippet demonstrates how to load a local JSON file as a dataset for Cross Encoder training using the datasets library. It creates a dataset from a JSON file that can be used directly with Cross Encoder models.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/cross_encoder/training_overview.md#2025-04-08_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"json\", data_files=\"my_file.json\")\n```\n\n----------------------------------------\n\nTITLE: Scalar Quantization Implementation with Calibration Dataset\nDESCRIPTION: Shows how to implement scalar (int8) quantization using a calibration dataset for better quantization stability and performance.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/embedding-quantization/README.md#2025-04-08_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer\nfrom sentence_transformers.quantization import quantize_embeddings\nfrom datasets import load_dataset\n\n# 1. Load an embedding model\nmodel = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\")\n\n# 2. Prepare an example calibration dataset\ncorpus = load_dataset(\"nq_open\", split=\"train[:1000]\")[\"question\"]\ncalibration_embeddings = model.encode(corpus)\n\n# 3. Encode some text without quantization & apply quantization afterwards\nembeddings = model.encode([\"I am driving to the lake.\", \"It is a beautiful day.\"])\nint8_embeddings = quantize_embeddings(\n    embeddings,\n    precision=\"int8\",\n    calibration_embeddings=calibration_embeddings,\n)\n```\n\n----------------------------------------\n\nTITLE: Binary Embedding Shape and Size Comparison\nDESCRIPTION: Code snippet showing the differences between float32 and binary embeddings in terms of shape, memory usage, and data type.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/embedding-quantization/README.md#2025-04-08_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> embeddings.shape\n(2, 1024)\n>>> embeddings.nbytes\n8192\n>>> embeddings.dtype\nfloat32\n>>> binary_embeddings.shape\n(2, 128)\n>>> binary_embeddings.nbytes\n256\n>>> binary_embeddings.dtype\nint8\n```\n\n----------------------------------------\n\nTITLE: Displaying Cluster Results\nDESCRIPTION: Visualizes the clustering results by displaying the first three images from each of the top 10 largest clusters.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/image-search/Image_Clustering.ipynb#2025-04-08_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Now we output the first 10 (largest) clusters\nfor cluster in clusters[0:10]:\n    print(\"\\n\\nCluster size:\", len(cluster))\n    \n    #Output 3 images\n    for idx in cluster[0:3]:\n        display(IPImage(os.path.join(img_folder, img_names[idx]), width=200))\n```\n\n----------------------------------------\n\nTITLE: Expected Translation Pairs Output\nDESCRIPTION: Example of correctly matched translation pairs between English and German sentences.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/parallel-sentence-mining/README.md#2025-04-08_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nHello World!    Hallo Welt!\nThis is an example sentences.   Dies ist ein Beispielsatz.\n```\n\n----------------------------------------\n\nTITLE: Initializing TripletEvaluator with AllNLI Dataset in Python\nDESCRIPTION: This snippet shows how to set up a TripletEvaluator using the AllNLI dataset from Hugging Face. The evaluator assesses a model's ability to rank positive examples higher than negative examples using triplets of (anchor, positive, negative) sentences.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/training_overview.md#2025-04-08_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\nfrom sentence_transformers.evaluation import TripletEvaluator, SimilarityFunction\n\n# Load triplets from the AllNLI dataset (https://huggingface.co/datasets/sentence-transformers/all-nli)\nmax_samples = 1000\neval_dataset = load_dataset(\"sentence-transformers/all-nli\", \"triplet\", split=f\"dev[:{max_samples}]\")\n\n# Initialize the evaluator\ndev_evaluator = TripletEvaluator(\n    anchors=eval_dataset[\"anchor\"],\n    positives=eval_dataset[\"positive\"],\n    negatives=eval_dataset[\"negative\"],\n    main_distance_function=SimilarityFunction.COSINE,\n    name=\"all-nli-dev\",\n)\n# You can run evaluation like so:\n# results = dev_evaluator(model)\n```\n\n----------------------------------------\n\nTITLE: Computing Image Embeddings with Pre-computed Option in Python\nDESCRIPTION: This snippet computes image embeddings using either pre-computed embeddings or by encoding images directly. It demonstrates handling large datasets efficiently.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/image-search/Image_Search-multilingual.ipynb#2025-04-08_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Now, we need to compute the embeddings\n# To speed things up, we destribute pre-computed embeddings\n# Otherwise you can also encode the images yourself.\n# To encode an image, you can use the following code:\n# from PIL import Image\n# img_emb = model.encode(Image.open(filepath))\n\nuse_precomputed_embeddings = True\n\nif use_precomputed_embeddings: \n    emb_filename = 'unsplash-25k-photos-embeddings.pkl'\n    if not os.path.exists(emb_filename):   #Download dataset if does not exist\n        util.http_get('http://sbert.net/datasets/'+emb_filename, emb_filename)\n        \n    with open(emb_filename, 'rb') as fIn:\n        img_names, img_emb = pickle.load(fIn)  \n    print(\"Images:\", len(img_names))\nelse:\n    #For embedding images, we need the non-multilingual CLIP model\n    img_model = SentenceTransformer('clip-ViT-B-32')\n\n    img_names = list(glob.glob('unsplash/photos/*.jpg'))\n    print(\"Images:\", len(img_names))\n    img_emb = img_model.encode([Image.open(filepath) for filepath in img_names], batch_size=128, convert_to_tensor=True, show_progress_bar=True)\n```\n\n----------------------------------------\n\nTITLE: Migrating evaluator Parameter in CrossEncoder\nDESCRIPTION: Shows how to migrate from using evaluator parameter in CrossEncoder.fit to using the evaluator with CrossEncoderTrainer in v4.x. In v4.x, the evaluator is passed directly to the CrossEncoderTrainer constructor along with an eval_dataset.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/migration_guide.md#2025-04-08_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n...\n\n# Load an evaluator\nevaluator = CrossEncoderNanoBEIREvaluator()\n\n# Finetune with an evaluator\nmodel.fit(\n    train_dataloader=train_dataloader,\n    evaluator=evaluator,\n)\n```\n\nLANGUAGE: python\nCODE:\n```\n# Load an evaluator\nevaluator = CrossEncoderNanoBEIREvaluator()\n\n# Finetune with an evaluator\ntrainer = CrossEncoderTrainer(\n    model=model,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    loss=loss,\n    evaluator=evaluator,\n)\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: Computing Image Embeddings\nDESCRIPTION: Loads pre-computed embeddings or computes new embeddings for the image dataset using the CLIP model.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/image-search/Image_Clustering.ipynb#2025-04-08_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Now, we need to compute the embeddings\n# To speed things up, we distribute pre-computed embeddings\n# Otherwise you can also encode the images yourself.\n# To encode an image, you can use the following code:\n# from PIL import Image\n# img_emb = model.encode(Image.open(filepath))\n\nuse_precomputed_embeddings = True\n\nif use_precomputed_embeddings: \n    emb_filename = 'unsplash-25k-photos-embeddings.pkl'\n    if not os.path.exists(emb_filename):   #Download dataset if does not exist\n        util.http_get('http://sbert.net/datasets/'+emb_filename, emb_filename)\n        \n    with open(emb_filename, 'rb') as fIn:\n        img_names, img_emb = pickle.load(fIn)  \n    print(\"Images:\", len(img_names))\nelse:\n    img_names = list(glob.glob('unsplash/photos/*.jpg'))\n    print(\"Images:\", len(img_names))\n    img_emb = model.encode([Image.open(filepath) for filepath in img_names], batch_size=128, convert_to_tensor=True, show_progress_bar=True)\n\n```\n\n----------------------------------------\n\nTITLE: Structuring SentenceTransformers Examples in Markdown\nDESCRIPTION: This code snippet outlines the structure of the examples folder for SentenceTransformers. It describes the contents of various subfolders, including applications, evaluation, training, and unsupervised learning examples.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/README.md#2025-04-08_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Examples\nThis folder contains various examples how to use SentenceTransformers.\n\n\n## Applications\nThe [applications](applications/) folder contains examples how to use SentenceTransformers for tasks like clustering or semantic search.\n\n## Evaluation\nThe [evaluation](evaluation/) folder contains some examples how to evaluate SentenceTransformer models for common tasks.\n\n## Training \nThe [training](training/) folder contains examples how to fine-tune transformer models like BERT, RoBERTa, or XLM-RoBERTa for generating sentence embedding. For the documentation how to train your own models, see [Training Overview](http://www.sbert.net/docs/sentence_transformer/training_overview.html).\n\n\n## Unsupervised Learning\nThe [unsupervised_learning](unsupervised_learning/) folder contains examples how to train sentence embedding models without labeled data.\n```\n\n----------------------------------------\n\nTITLE: Migrating Gradient Norm Configuration\nDESCRIPTION: Demonstrates migration of maximum gradient norm configuration from fit() method to training arguments.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/migration_guide.md#2025-04-08_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# v2.x\nmodel.fit(\n    train_objectives=[(train_dataloader, train_loss)],\n    max_grad_norm=1,\n)\n```\n\nLANGUAGE: python\nCODE:\n```\n# v3.x\nargs = SentenceTransformerTrainingArguments(\n    max_grad_norm=1,\n)\n\ntrainer = SentenceTransformerTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    loss=loss,\n)\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: Using Sentence Transformers with Prompts (bert-base model)\nDESCRIPTION: Code example demonstrating how to load a prompt-trained bert-base model, encode queries and documents with specific prompts, and calculate similarity between them.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/training/prompts/README.md#2025-04-08_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\"tomaarsen/bert-base-nq-prompts\")\nquery_embedding = model.encode(\"What are Pandas?\", prompt_name=\"query\")\ndocument_embeddings = model.encode([\n    \"Pandas is a software library written for the Python programming language for data manipulation and analysis.\",\n    \"Pandas are a species of bear native to South Central China. They are also known as the giant panda or simply panda.\",\n    \"Koala bears are not actually bears, they are marsupials native to Australia.\",\n    ],\n    prompt_name=\"document\",\n)\nsimilarity = model.similarity(query_embedding, document_embeddings)\nprint(similarity)\n# => tensor([[0.3955, 0.8226, 0.5706]])\n```\n\n----------------------------------------\n\nTITLE: Migrating Mixed Precision Training\nDESCRIPTION: Shows how to migrate automatic mixed precision (AMP) training configuration to the new training arguments format.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/migration_guide.md#2025-04-08_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# v2.x\nmodel.fit(\n    train_objectives=[(train_dataloader, train_loss)],\n    use_amp=True,\n)\n```\n\nLANGUAGE: python\nCODE:\n```\n# v3.x\nargs = SentenceTransformerTrainingArguments(\n    fp16=True,\n    bf16=False,\n)\n\ntrainer = SentenceTransformerTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    loss=loss,\n)\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning CLIP Model on Image-Caption Pairs in Python\nDESCRIPTION: This snippet demonstrates how to fine-tune the CLIP model on the prepared dataset of image-caption pairs using the fit method with the specified loss function and number of epochs.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/training/clip/train_clip.ipynb#2025-04-08_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Now we can fine-tune our model on the labeled image pairs\nmodel.fit([(train_dataloader, train_loss)], epochs=5, show_progress_bar=True)\n```\n\n----------------------------------------\n\nTITLE: Documenting SentenceTransformerTrainingArguments Class with RST\nDESCRIPTION: ReStructuredText directive for auto-documenting the SentenceTransformerTrainingArguments class, including its members and inherited members.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/sentence_transformer/training_args.md#2025-04-08_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: sentence_transformers.training_args.SentenceTransformerTrainingArguments\n    :members:\n    :inherited-members:\n```\n\n----------------------------------------\n\nTITLE: Migrating Training Callbacks\nDESCRIPTION: Examples of migrating custom callback functions to the new TrainerCallback class system.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/migration_guide.md#2025-04-08_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# v2.x\ndef printer_callback(score, epoch, steps):\n    print(f\"Score: {score:.4f} at epoch {epoch:d}, step {steps:d}\")\n\nmodel.fit(\n    train_objectives=[(train_dataloader, train_loss)],\n    callback=printer_callback,\n)\n```\n\nLANGUAGE: python\nCODE:\n```\n# v3.x\nfrom transformers import TrainerCallback\n\nclass PrinterCallback(TrainerCallback):\n    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n        print(f\"Metrics: {metrics} at epoch {state.epoch:d}, step {state.global_step:d}\")\n\nprinter_callback = PrinterCallback()\n\ntrainer = SentenceTransformerTrainer(\n    model=model,\n    train_dataset=train_dataset,\n    loss=loss,\n    callbacks=[printer_callback],\n)\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: Importing Similarity Metrics from sentence_transformers.util\nDESCRIPTION: Shows how to import various similarity metrics from the sentence_transformers.util module, including cosine similarity, dot product, Manhattan distance, and Euclidean distance, with both regular and pairwise versions available.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/util.md#2025-04-08_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. automodule:: sentence_transformers.util\n   :members: cos_sim, pairwise_cos_sim, dot_score, pairwise_dot_score, manhattan_sim, pairwise_manhattan_sim, euclidean_sim, pairwise_euclidean_sim\n```\n\n----------------------------------------\n\nTITLE: Migrating evaluator parameter in SentenceTransformer.fit (Python)\nDESCRIPTION: This snippet demonstrates how to migrate the evaluator parameter from SentenceTransformer.fit in v2.x to using SentenceTransformerTrainer in v3.x. It shows how to incorporate an evaluator into the training process.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/migration_guide.md#2025-04-08_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n...\n\n# Load an evaluator\nevaluator = NanoBEIREvaluator()\n\n# Finetune with an evaluator\nmodel.fit(\n    train_objectives=[(train_dataloader, train_loss)],\n    evaluator=evaluator,\n)\n```\n\nLANGUAGE: python\nCODE:\n```\n# Load an evaluator\nevaluator = NanoBEIREvaluator()\n\n# Finetune with an evaluator\ntrainer = SentenceTransformerTrainer(\n    model=model,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    loss=loss,\n    evaluator=evaluator,\n)\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: Augmenting Training Data by Leveraging Symmetric Relation in Python\nDESCRIPTION: This code snippet demonstrates how to double the training data by leveraging the symmetric nature of the duplicate relationship. It concatenates the original dataset with a version where anchor and positive columns are swapped.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/training/quora_duplicate_questions/README.md#2025-04-08_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import concatenate_datasets\n\ntrain_dataset = concatenate_datasets([\n    train_dataset,\n    train_dataset.rename_columns({\"anchor\": \"positive\", \"positive\": \"anchor\"})\n])\n# Dataset({\n#     features: ['anchor', 'positive'],\n#     num_rows: 298526\n# })\n```\n\n----------------------------------------\n\nTITLE: Searching for Toronto Population Information\nDESCRIPTION: This code executes a search query about the population of Toronto. It demonstrates how the system handles demographic queries about cities.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/retrieve_rerank/retrieve_rerank_simple_wikipedia.ipynb#2025-04-08_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nsearch(query = \"How many people live in Toronto?\")\n```\n\n----------------------------------------\n\nTITLE: Importing BinaryClassificationEvaluator in Python\nDESCRIPTION: This code snippet demonstrates how to import and use the BinaryClassificationEvaluator class from the sentence_transformers.evaluation module.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/sentence_transformer/evaluation.md#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: sentence_transformers.evaluation.BinaryClassificationEvaluator\n```\n\n----------------------------------------\n\nTITLE: Optimizing a Local ONNX Model\nDESCRIPTION: This snippet shows how to optimize a local ONNX model using the export_optimized_onnx_model function, saving the optimized model in the specified directory.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/usage/efficiency.rst#2025-04-08_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer, export_optimized_onnx_model\n\nmodel = SentenceTransformer(\"path/to/my/mpnet-legal-finetuned\", backend=\"onnx\")\nexport_optimized_onnx_model(model, \"O3\", \"path/to/my/mpnet-legal-finetuned\")\n```\n\n----------------------------------------\n\nTITLE: Importing EmbeddingSimilarityEvaluator in Python\nDESCRIPTION: This code snippet shows how to import and use the EmbeddingSimilarityEvaluator class from the sentence_transformers.evaluation module.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/sentence_transformer/evaluation.md#2025-04-08_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: sentence_transformers.evaluation.EmbeddingSimilarityEvaluator\n```\n\n----------------------------------------\n\nTITLE: Finding Near-Duplicate Images\nDESCRIPTION: Identifies and displays near-duplicate images by filtering pairs with cosine similarity below 0.99.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/image-search/Image_Duplicates.ipynb#2025-04-08_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nthreshold = 0.99\nnear_duplicates = [entry for entry in duplicates if entry[0] < threshold]\n\nfor score, idx1, idx2 in near_duplicates[0:10]:\n    print(\"\\nScore: {:.3f}\".format(score))\n    print(img_names[idx1])\n    display(IPImage(os.path.join(img_folder, img_names[idx1]), width=200))\n    print(img_names[idx2])\n    display(IPImage(os.path.join(img_folder, img_names[idx2]), width=200))\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for Cross-Encoder Training in Python\nDESCRIPTION: This snippet imports necessary modules and sets up logging for a cross-encoder training pipeline using Sentence Transformers and Hugging Face datasets.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/cross_encoder/training_overview.md#2025-04-08_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nimport traceback\n\nimport torch\nfrom datasets import load_dataset\n\nfrom sentence_transformers import SentenceTransformer\nfrom sentence_transformers.cross_encoder import (\n    CrossEncoder,\n    CrossEncoderModelCardData,\n    CrossEncoderTrainer,\n    CrossEncoderTrainingArguments,\n)\nfrom sentence_transformers.cross_encoder.evaluation import (\n    CrossEncoderNanoBEIREvaluator,\n    CrossEncoderRerankingEvaluator,\n)\nfrom sentence_transformers.cross_encoder.losses import BinaryCrossEntropyLoss\nfrom sentence_transformers.evaluation import SequentialEvaluator\nfrom sentence_transformers.util import mine_hard_negatives\n\n# Set the log level to INFO to get more information\nlogging.basicConfig(format=\"%(asctime)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\", level=logging.INFO)\n```\n\n----------------------------------------\n\nTITLE: Searching for Elon Musk's Birth Year\nDESCRIPTION: This code executes a search query about Elon Musk's year of birth. It shows how the system handles biographical queries about well-known figures.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/retrieve_rerank/retrieve_rerank_simple_wikipedia.ipynb#2025-04-08_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nsearch(query = \"Elon Musk year birth\")\n```\n\n----------------------------------------\n\nTITLE: Importing MSEEvaluator in Python\nDESCRIPTION: This code snippet shows how to import and use the MSEEvaluator class from the sentence_transformers.evaluation module.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/sentence_transformer/evaluation.md#2025-04-08_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: sentence_transformers.evaluation.MSEEvaluator\n```\n\n----------------------------------------\n\nTITLE: Migrating evaluation_steps Parameter in CrossEncoder\nDESCRIPTION: Partial example showing the migration of evaluation_steps parameter from CrossEncoder.fit in v3.x to CrossEncoderTrainer in v4.x.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/migration_guide.md#2025-04-08_snippet_25\n\nLANGUAGE: python\nCODE:\n```\n...\n\n# Finetune the model\nmodel.fit(\n\n\n    evaluation_steps=500,\n)\n```\n\n----------------------------------------\n\nTITLE: Importing SequentialEvaluator in Python\nDESCRIPTION: This code snippet illustrates how to import and use the SequentialEvaluator class from the sentence_transformers.evaluation module.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/sentence_transformer/evaluation.md#2025-04-08_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: sentence_transformers.evaluation.SequentialEvaluator\n```\n\n----------------------------------------\n\nTITLE: Local ONNX Model Quantization\nDESCRIPTION: Demonstrates how to quantize a local Sentence Transformer model using ONNX runtime and load the quantized model for inference.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/usage/efficiency.rst#2025-04-08_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer, export_dynamic_quantized_onnx_model\n\nmodel = SentenceTransformer(\"path/to/my/mpnet-legal-finetuned\", backend=\"onnx\")\nexport_dynamic_quantized_onnx_model(model, \"O3\", \"path/to/my/mpnet-legal-finetuned\")\n```\n\n----------------------------------------\n\nTITLE: Implementing callbacks in CrossEncoder training in Python\nDESCRIPTION: Comparing v3.x and v4.x approaches for adding callbacks to CrossEncoder training. V4.x uses the Transformers TrainerCallback class for more flexible callback implementation.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/migration_guide.md#2025-04-08_snippet_30\n\nLANGUAGE: python\nCODE:\n```\ndef printer_callback(score, epoch, steps):\n    print(f\"Score: {score:.4f} at epoch {epoch:d}, step {steps:d}\")\n\n# Finetune the model\nmodel.fit(\n    train_dataloader=train_dataloader,\n    callback=printer_callback,\n)\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import TrainerCallback\n\n...\n\nclass PrinterCallback(TrainerCallback):\n    # Subclass any method from https://huggingface.co/docs/transformers/main_classes/callback#transformers.TrainerCallback\n    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n        print(f\"Metrics: {metrics} at epoch {state.epoch:d}, step {state.global_step:d}\")\n\nprinter_callback = PrinterCallback()\n\n# Finetune the model\ntrainer = CrossEncoderTrainer(\n    model=model,\n    train_dataset=train_dataset,\n    loss=loss,\n    callbacks=[printer_callback],\n)\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: Searching for Chinese New Year Information\nDESCRIPTION: This code executes a search query about when Chinese New Year occurs. It shows how the system handles cultural and calendar-related queries.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/retrieve_rerank/retrieve_rerank_simple_wikipedia.ipynb#2025-04-08_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nsearch(query=\"When is Chinese New Year\")\n```\n\n----------------------------------------\n\nTITLE: Importing TranslationEvaluator in Python\nDESCRIPTION: This code snippet demonstrates how to import and use the TranslationEvaluator class from the sentence_transformers.evaluation module.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/sentence_transformer/evaluation.md#2025-04-08_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: sentence_transformers.evaluation.TranslationEvaluator\n```\n\n----------------------------------------\n\nTITLE: Configuring progress bar display for CrossEncoder training in Python\nDESCRIPTION: Comparing v3.x and v4.x approaches for controlling the progress bar display during CrossEncoder training. V4.x uses the disable_tqdm parameter in training arguments.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/migration_guide.md#2025-04-08_snippet_31\n\nLANGUAGE: python\nCODE:\n```\n# Finetune the model\nmodel.fit(\n    train_dataloader=train_dataloader,\n    show_progress_bar=True,\n)\n```\n\nLANGUAGE: python\nCODE:\n```\n# Prepare the Training Arguments\nargs = CrossEncoderTrainingArguments(\n    disable_tqdm=False,\n)\n\n# Finetune the model\ntrainer = CrossEncoderTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    loss=loss,\n)\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: Importing TripletEvaluator in Python\nDESCRIPTION: This code snippet shows how to import and use the TripletEvaluator class from the sentence_transformers.evaluation module.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/sentence_transformer/evaluation.md#2025-04-08_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: sentence_transformers.evaluation.TripletEvaluator\n```\n\n----------------------------------------\n\nTITLE: Initializing CLIP Model with SentenceTransformers\nDESCRIPTION: Sets up the environment by importing required libraries and loading the CLIP ViT-B-32 model for image processing.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/image-search/Image_Duplicates.ipynb#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer, util\nfrom PIL import Image\nimport glob\nimport torch\nimport pickle\nimport zipfile\nfrom IPython.display import display\nfrom IPython.display import Image as IPImage\nimport os\nfrom tqdm.autonotebook import tqdm\n\n#First, we load the CLIP model\nmodel = SentenceTransformer('clip-ViT-B-32')\n```\n\n----------------------------------------\n\nTITLE: Editable Install for Development\nDESCRIPTION: Commands for setting up an editable installation for development purposes, allowing modifications to the source code.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/installation.md#2025-04-08_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/UKPLab/sentence-transformers\ncd sentence-transformers\npip install -e \".[train,dev]\"\n```\n\n----------------------------------------\n\nTITLE: Searching for Coldest Place on Earth\nDESCRIPTION: This code executes a search query about the coldest place on Earth. It demonstrates the system's handling of geographical extreme queries.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/retrieve_rerank/retrieve_rerank_simple_wikipedia.ipynb#2025-04-08_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nsearch(query = \"Coldest place earth\")\n```\n\n----------------------------------------\n\nTITLE: Importing BinaryCrossEntropyLoss in Python\nDESCRIPTION: This snippet demonstrates how to import the BinaryCrossEntropyLoss class from the sentence_transformers.cross_encoder.losses module.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/cross_encoder/losses.md#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: sentence_transformers.cross_encoder.losses.BinaryCrossEntropyLoss\n```\n\n----------------------------------------\n\nTITLE: Importing ListNetLoss in Python\nDESCRIPTION: This snippet shows how to import the ListNetLoss class from the sentence_transformers.cross_encoder.losses module.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/cross_encoder/losses.md#2025-04-08_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: sentence_transformers.cross_encoder.losses.ListNetLoss\n```\n\n----------------------------------------\n\nTITLE: RST AutoModule Documentation for Quantization Functions\nDESCRIPTION: ReStructuredText automodule directive that generates documentation for the quantize_embeddings, semantic_search_faiss, and semantic_search_usearch functions from the sentence_transformers.quantization module.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/sentence_transformer/quantization.md#2025-04-08_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: sentence_transformers.quantization\n   :members: quantize_embeddings, semantic_search_faiss, semantic_search_usearch\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting Image Dataset\nDESCRIPTION: Downloads and extracts the Unsplash-25k dataset if not already present in the local directory.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/image-search/Image_Duplicates.ipynb#2025-04-08_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Next, we get about 25k images from Unsplash \nimg_folder = 'photos/'\nif not os.path.exists(img_folder) or len(os.listdir(img_folder)) == 0:\n    os.makedirs(img_folder, exist_ok=True)\n    \n    photo_filename = 'unsplash-25k-photos.zip'\n    if not os.path.exists(photo_filename):   #Download dataset if does not exist\n        util.http_get('http://sbert.net/datasets/'+photo_filename, photo_filename)\n        \n    #Extract all images\n    with zipfile.ZipFile(photo_filename, 'r') as zf:\n        for member in tqdm(zf.infolist(), desc='Extracting'):\n            zf.extract(member, img_folder)\n```\n\n----------------------------------------\n\nTITLE: Searching for Cold War End Date\nDESCRIPTION: This code executes a search query about when the Cold War ended. It demonstrates the system's handling of historical date queries.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/retrieve_rerank/retrieve_rerank_simple_wikipedia.ipynb#2025-04-08_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nsearch(query = \"When did the cold war end?\")\n```\n\n----------------------------------------\n\nTITLE: Importing RankNetLoss in Python\nDESCRIPTION: This snippet demonstrates how to import the RankNetLoss class from the sentence_transformers.cross_encoder.losses module.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/cross_encoder/losses.md#2025-04-08_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: sentence_transformers.cross_encoder.losses.RankNetLoss\n```\n\n----------------------------------------\n\nTITLE: Generating API Documentation for CrossEncoder Class in Python\nDESCRIPTION: This RST code snippet uses the autoclass directive to generate API documentation for the CrossEncoder class from the sentence_transformers.cross_encoder module. It includes specific members and excludes others from the documentation.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/cross_encoder/cross_encoder.md#2025-04-08_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: sentence_transformers.cross_encoder.CrossEncoder\n   :members:\n   :inherited-members: fit, old_fit\n   :exclude-members: save, add_module, apply, buffers, children, extra_repr, forward, get_buffer, get_extra_state, get_parameter, get_submodule, ipu, load_state_dict, modules, named_buffers, named_children, named_modules, named_parameters, parameters, register_backward_hook, register_buffer, register_forward_hook, register_forward_pre_hook, register_full_backward_hook, register_full_backward_pre_hook, register_load_state_dict_post_hook, register_module, register_parameter, register_state_dict_pre_hook, requires_grad_, set_extra_state, share_memory, state_dict, to_empty, type, xpu, zero_grad\n```\n\n----------------------------------------\n\nTITLE: modules.json Content for Sentence Transformer Model\nDESCRIPTION: This JSON snippet shows the contents of the modules.json file, which describes the structure and configuration of a Sentence Transformer model's modules.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/usage/custom_models.rst#2025-04-08_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"idx\": 0,\n    \"name\": \"0\",\n    \"path\": \"\",\n    \"type\": \"sentence_transformers.models.Transformer\"\n  },\n  {\n    \"idx\": 1,\n    \"name\": \"1\",\n    \"path\": \"1_Pooling\",\n    \"type\": \"sentence_transformers.models.Pooling\"\n  },\n  {\n    \"idx\": 2,\n    \"name\": \"2\",\n    \"path\": \"2_Normalize\",\n    \"type\": \"sentence_transformers.models.Normalize\"\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Running Clustering Algorithm\nDESCRIPTION: Executes the community detection algorithm on image embeddings with specified threshold and minimum community size parameters.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/image-search/Image_Clustering.ipynb#2025-04-08_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Now we run the clustering algorithm\n# With the threshold parameter, we define at which threshold we identify\n# two images as similar. Set the threshold lower, and you will get larger clusters which have \n# less similar images in it (e.g. black cat images vs. cat images vs. animal images).\n# With min_community_size, we define that we only want to have clusters of a certain minimal size\nclusters = community_detection(img_emb, threshold=0.9, min_community_size=10)\nprint(\"Total number of clusters:\", len(clusters))\n```\n\n----------------------------------------\n\nTITLE: Generating API Documentation for CrossEncoderModelCardData Class in Python\nDESCRIPTION: This RST code snippet uses the autoclass directive to generate API documentation for the CrossEncoderModelCardData class from the sentence_transformers.cross_encoder.model_card module.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/cross_encoder/cross_encoder.md#2025-04-08_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: sentence_transformers.cross_encoder.model_card.CrossEncoderModelCardData\n```\n\n----------------------------------------\n\nTITLE: Comparing CrossEncoder Training in Sentence Transformers v3.x and v4.x\nDESCRIPTION: This snippet demonstrates the differences between training a CrossEncoder model in Sentence Transformers v3.x and v4.x. The v4.x approach introduces a new CrossEncoderTrainer class and uses a dataset for training, offering more flexibility and functionality.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/migration_guide.md#2025-04-08_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import CrossEncoder, InputExample\nfrom torch.utils.data import DataLoader\n\n# 1. Define the model. Either from scratch of by loading a pre-trained model\nmodel = CrossEncoder(\"microsoft/mpnet-base\")\n\n# 2. Define your train examples. You need more than just two examples...\ntrain_examples = [\n    InputExample(texts=[\"What are pandas?\", \"The giant panda ...\"], label=1),\n    InputExample(texts=[\"What's a panda?\", \"Mount Vesuvius is a ...\"], label=0),\n]\ntrain_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n\n# 3. Finetune the model\nmodel.fit(train_dataloader=train_dataloader, epochs=1, warmup_steps=100)\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\nfrom sentence_transformers import CrossEncoder, CrossEncoderTrainer\nfrom sentence_transformers.cross_encoder.losses import BinaryCrossEntropyLoss\n\n# 1. Define the model. Either from scratch of by loading a pre-trained model\nmodel = CrossEncoder(\"microsoft/mpnet-base\")\n\n# 2. Load a dataset to finetune on, convert to required format\ndataset = load_dataset(\"sentence-transformers/hotpotqa\", \"triplet\", split=\"train\")\n\ndef triplet_to_labeled_pair(batch):\n    anchors = batch[\"anchor\"]\n    positives = batch[\"positive\"]\n    negatives = batch[\"negative\"]\n    return {\n        \"sentence_A\": anchors * 2,\n        \"sentence_B\": positives + negatives,\n        \"labels\": [1] * len(positives) + [0] * len(negatives),\n    }\n\ndataset = dataset.map(triplet_to_labeled_pair, batched=True, remove_columns=dataset.column_names)\ntrain_dataset = dataset.select(range(10_000))\neval_dataset = dataset.select(range(10_000, 11_000))\n\n# 3. Define a loss function\nloss = BinaryCrossEntropyLoss(model)\n\n# 4. Create a trainer & train\ntrainer = CrossEncoderTrainer(\n    model=model,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    loss=loss,\n)\ntrainer.train()\n\n# 5. Save the trained model\nmodel.save_pretrained(\"models/mpnet-base-hotpotqa\")\n# model.push_to_hub(\"mpnet-base-hotpotqa\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Transformer with Keyword Arguments\nDESCRIPTION: Example implementation of a custom transformer module that handles keyword arguments passed through the encode method.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/usage/custom_models.rst#2025-04-08_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers.models import Transformer\n\nclass CustomTransformer(Transformer):\n    def forward(self, features: dict[str, torch.Tensor], task_type: Optional[str] = None) -> dict[str, torch.Tensor]:\n        if task_type == \"default\":\n            # Do something\n        else:\n            # Do something else\n        return features\n```\n\n----------------------------------------\n\nTITLE: Documenting CrossEncoderNanoBEIREvaluator Class\nDESCRIPTION: Auto-documentation directive for the CrossEncoderNanoBEIREvaluator class from the sentence-transformers cross-encoder evaluation module.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/cross_encoder/evaluation.md#2025-04-08_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: sentence_transformers.cross_encoder.evaluation.CrossEncoderNanoBEIREvaluator\n```\n\n----------------------------------------\n\nTITLE: Installing SentenceTransformers with pip\nDESCRIPTION: Command to install the sentence-transformers package using pip. Python 3.9+ and PyTorch 1.11.0+ are recommended.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/index.rst#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npip install -U sentence-transformers\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting Unsplash Dataset\nDESCRIPTION: Downloads and extracts the Unsplash-25k photo dataset if not already present in the local directory.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/image-search/Image_Clustering.ipynb#2025-04-08_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Next, we get about 25k images from Unsplash \nimg_folder = 'photos/'\nif not os.path.exists(img_folder) or len(os.listdir(img_folder)) == 0:\n    os.makedirs(img_folder, exist_ok=True)\n    \n    photo_filename = 'unsplash-25k-photos.zip'\n    if not os.path.exists(photo_filename):   #Download dataset if does not exist\n        util.http_get('http://sbert.net/datasets/'+photo_filename, photo_filename)\n        \n    #Extract all images\n    with zipfile.ZipFile(photo_filename, 'r') as zf:\n        for member in tqdm(zf.infolist(), desc='Extracting'):\n            zf.extract(member, img_folder)\n\n```\n\n----------------------------------------\n\nTITLE: Importing CrossEntropyLoss in Python\nDESCRIPTION: This snippet shows how to import the CrossEntropyLoss class from the sentence_transformers.cross_encoder.losses module.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/cross_encoder/losses.md#2025-04-08_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: sentence_transformers.cross_encoder.losses.CrossEntropyLoss\n```\n\n----------------------------------------\n\nTITLE: Loading Datasets from Hugging Face Hub for Cross Encoder Training\nDESCRIPTION: This snippet demonstrates how to load training and evaluation datasets from the Hugging Face Hub using the datasets library for Cross Encoder model training. It loads the 'all-nli' dataset with the 'pair-class' subset split into train and dev sets.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/cross_encoder/training_overview.md#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n\ntrain_dataset = load_dataset(\"sentence-transformers/all-nli\", \"pair-class\", split=\"train\")\neval_dataset = load_dataset(\"sentence-transformers/all-nli\", \"pair-class\", split=\"dev\")\n\nprint(train_dataset)\n\"\"\"\nDataset({\n    features: ['premise', 'hypothesis', 'label'],\n    num_rows: 942069\n})\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Installing Sentence Transformers with conda\nDESCRIPTION: Command to install sentence-transformers package using conda package manager from conda-forge channel.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/README.md#2025-04-08_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nconda install -c conda-forge sentence-transformers\n```\n\n----------------------------------------\n\nTITLE: Importing LambdaLoss and Related Classes in Python\nDESCRIPTION: This snippet demonstrates how to import the LambdaLoss class and its related weighting scheme classes from the sentence_transformers.cross_encoder.losses module.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/cross_encoder/losses.md#2025-04-08_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: sentence_transformers.cross_encoder.losses.LambdaLoss\n\n.. autoclass:: sentence_transformers.cross_encoder.losses.LambdaLoss.BaseWeightingScheme\n.. autoclass:: sentence_transformers.cross_encoder.losses.NoWeightingScheme\n.. autoclass:: sentence_transformers.cross_encoder.losses.NDCGLoss1Scheme\n.. autoclass:: sentence_transformers.cross_encoder.losses.NDCGLoss2Scheme\n.. autoclass:: sentence_transformers.cross_encoder.losses.LambdaRankScheme\n.. autoclass:: sentence_transformers.cross_encoder.losses.NDCGLoss2PPScheme\n```\n\n----------------------------------------\n\nTITLE: Installing Sentence Transformers with Conda\nDESCRIPTION: Installation commands using conda package manager for different configurations including default setup, ONNX, OpenVINO, training, and development environments.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/installation.md#2025-04-08_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nconda install -c conda-forge sentence-transformers\npip install -U \"sentence-transformers[onnx-gpu]\"\npip install -U \"sentence-transformers[onnx]\"\npip install -U \"sentence-transformers[openvino]\"\nconda install -c conda-forge sentence-transformers accelerate datasets\npip install wandb\npip install codecarbon\nconda install -c conda-forge sentence-transformers accelerate datasets pre-commit pytest ruff\n```\n\n----------------------------------------\n\nTITLE: Comparing CrossEncoder and SentenceTransformer Training in RST\nDESCRIPTION: This RST code block outlines the main differences in training CrossEncoder models compared to SentenceTransformer models. It highlights variations in label column naming, input data structure, and provides links to related documentation.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/cross_encoder/training_overview.md#2025-04-08_snippet_18\n\nLANGUAGE: rst\nCODE:\n```\n```{eval-rst}\nTraining :class:`~sentence_transformers.cross_encoder.CrossEncoder` models is very similar as training :class:`~sentence_transformers.SentenceTransformer` models, with some key differences:\n\n- Instead of just ``score`` and ``label``, columns named ``scores`` and ``labels`` will also be considered \"label columns\" for :class:`~sentence_transformers.cross_encoder.CrossEncoder` training. As you can see in the `Loss Overview <loss_overview.html>`_ documentation, some losses require specific labels/scores in a column with one of these names.\n- In :class:`~sentence_transformers.SentenceTransformer` training, you cannot use lists of inputs (e.g. texts) in a column of the training/evaluation dataset(s). For :class:`~sentence_transformers.cross_encoder.CrossEncoder` training, you **can** use (variably sized) lists of texts in a column. This is required for the :class:`~sentence_transformers.cross_encoder.losses.ListNetLoss` class, for example.\n\nSee the `Sentence Transformer > Training Overview <../sentence_transformer/training_overview.html>`_ documentation for more details on training :class:`~sentence_transformers.SentenceTransformer` models.\n```\n```\n\n----------------------------------------\n\nTITLE: Importing PListMLELoss and PListMLELambdaWeight in Python\nDESCRIPTION: This snippet demonstrates how to import the PListMLELoss class and its related PListMLELambdaWeight class from the sentence_transformers.cross_encoder.losses module.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/cross_encoder/losses.md#2025-04-08_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: sentence_transformers.cross_encoder.losses.PListMLELoss\n\n.. autoclass:: sentence_transformers.cross_encoder.losses.PListMLELambdaWeight\n```\n\n----------------------------------------\n\nTITLE: Migrating loss_fct Parameter in CrossEncoder\nDESCRIPTION: Shows how to migrate from using loss_fct parameter in CrossEncoder.fit to using the loss class with CrossEncoderTrainer in v4.x. In v4.x, losses are instantiated as separate classes that take the model as input.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/migration_guide.md#2025-04-08_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n...\n\n# Finetune the model\nmodel.fit(\n    train_dataloader=train_dataloader,\n    loss_fct=torch.nn.MSELoss(),\n)\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers.cross_encoder.losses import MSELoss\n\n...\n\n# Prepare the loss function\n# See all valid losses in https://sbert.net/docs/cross_encoder/loss_overview.html\nloss = MSELoss(model)\n\n# Finetune the model\ntrainer = CrossEncoderTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    loss=loss,\n)\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: Importing MultipleNegativesRankingLoss in Python\nDESCRIPTION: This snippet demonstrates how to import the MultipleNegativesRankingLoss class from the sentence_transformers.cross_encoder.losses module.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/cross_encoder/losses.md#2025-04-08_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: sentence_transformers.cross_encoder.losses.MultipleNegativesRankingLoss\n```\n\n----------------------------------------\n\nTITLE: Pushing an Exported ONNX Model to Hugging Face Hub\nDESCRIPTION: This snippet shows how to push an exported ONNX model to the Hugging Face Hub, creating a pull request if you don't have write access to the repository.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/usage/efficiency.rst#2025-04-08_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmodel = SentenceTransformer(\"intfloat/multilingual-e5-small\", backend=\"onnx\")\nmodel.push_to_hub(\"intfloat/multilingual-e5-small\", create_pr=True)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for Sentence Transformers and BM25\nDESCRIPTION: This snippet installs the sentence-transformers library for semantic search and rank_bm25 for lexical search capabilities. These are the core dependencies needed for the retrieval and re-ranking system.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/retrieve_rerank/retrieve_rerank_simple_wikipedia.ipynb#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -U sentence-transformers rank_bm25\n```\n\n----------------------------------------\n\nTITLE: Importing CachedMultipleNegativesRankingLoss in Python\nDESCRIPTION: This snippet shows how to import the CachedMultipleNegativesRankingLoss class from the sentence_transformers.cross_encoder.losses module.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/cross_encoder/losses.md#2025-04-08_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: sentence_transformers.cross_encoder.losses.CachedMultipleNegativesRankingLoss\n```\n\n----------------------------------------\n\nTITLE: Loading Datasets from Hugging Face Hub using Datasets Library\nDESCRIPTION: Demonstrates how to load a dataset from the Hugging Face Hub using the datasets library. The example shows loading the 'natural-questions' dataset and accessing its contents.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/dataset_overview.md#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n\n# Indicate the dataset id from the Hub\ndataset_id = \"sentence-transformers/natural-questions\"\ndataset = load_dataset(dataset_id, split=\"train\")\n\"\"\"\nDataset({\n   features: ['query', 'answer'],\n   num_rows: 100231\n})\n\"\"\"\nprint(dataset[0])\n\"\"\"\n{\n   'query': 'when did richmond last play in a preliminary final',\n   'answer': \"Richmond Football Club Richmond began 2017 with 5 straight wins, a feat it had not achieved since 1995. A series of close losses hampered the Tigers throughout the middle of the season, including a 5-point loss to the Western Bulldogs, 2-point loss to Fremantle, and a 3-point loss to the Giants. Richmond ended the season strongly with convincing victories over Fremantle and St Kilda in the final two rounds, elevating the club to 3rd on the ladder. Richmond's first final of the season against the Cats at the MCG attracted a record qualifying final crowd of 95,028; the Tigers won by 51 points. Having advanced to the first preliminary finals for the first time since 2001, Richmond defeated Greater Western Sydney by 36 points in front of a crowd of 94,258 to progress to the Grand Final against Adelaide, their first Grand Final appearance since 1982. The attendance was 100,021, the largest crowd to a grand final since 1986. The Crows led at quarter time and led by as many as 13, but the Tigers took over the game as it progressed and scored seven straight goals at one point. They eventually would win by 48 points  16.12 (108) to Adelaide's 8.12 (60)  to end their 37-year flag drought.[22] Dustin Martin also became the first player to win a Premiership medal, the Brownlow Medal and the Norm Smith Medal in the same season, while Damien Hardwick was named AFL Coaches Association Coach of the Year. Richmond's jump from 13th to premiers also marked the biggest jump from one AFL season to the next.\"\n}\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for Sentence Transformers and BM25\nDESCRIPTION: This snippet installs the sentence-transformers library for semantic search and rank_bm25 for lexical search capabilities. These are the core dependencies needed for the retrieval and re-ranking system.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/retrieve_rerank/retrieve_rerank_simple_wikipedia.ipynb#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -U sentence-transformers rank_bm25\n```\n\n----------------------------------------\n\nTITLE: Importing MSELoss in Python\nDESCRIPTION: This snippet demonstrates how to import the MSELoss class from the sentence_transformers.cross_encoder.losses module.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/cross_encoder/losses.md#2025-04-08_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: sentence_transformers.cross_encoder.losses.MSELoss\n```\n\n----------------------------------------\n\nTITLE: RST Note Block for Data Format Conversion\nDESCRIPTION: A reStructuredText note block explaining how training data formats can be converted between different types to enable the use of different loss functions.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/loss_overview.md#2025-04-08_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. note:: \n\n    You can often convert one training data format into another, allowing more loss functions to be viable for your scenario. For example, ``(sentence_A, sentence_B) pairs`` with ``class`` labels can be converted into ``(anchor, positive, negative) triplets`` by sampling sentences with the same or different classes.\n```\n\n----------------------------------------\n\nTITLE: Training CT from Sentences File in Python\nDESCRIPTION: This snippet refers to a Python script that loads sentences from a text file and trains a CT model using in-batch negatives. It expects one sentence per line in the input file and stores checkpoints every 500 steps.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/unsupervised_learning/CT_In-Batch_Negatives/README.md#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntrain_ct-improved_from_file.py\n```\n\n----------------------------------------\n\nTITLE: Referencing CrossEncoderTrainingArguments Class Documentation with RST\nDESCRIPTION: This code snippet uses reStructuredText (RST) directives to automatically include API documentation for the CrossEncoderTrainingArguments class from the sentence_transformers library. The directive will display class members and inherited members.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/cross_encoder/training_args.md#2025-04-08_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: sentence_transformers.cross_encoder.training_args.CrossEncoderTrainingArguments\n    :members:\n    :inherited-members:\n```\n\n----------------------------------------\n\nTITLE: Loading Local CSV/JSON Dataset\nDESCRIPTION: Examples showing how to load datasets from local CSV and JSON files using the datasets library's load_dataset function.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/training_overview.md#2025-04-08_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"csv\", data_files=\"my_file.csv\")\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"json\", data_files=\"my_file.json\")\n```\n\n----------------------------------------\n\nTITLE: Adding a Note Directive in reStructuredText\nDESCRIPTION: This code snippet demonstrates how to create a note directive in reStructuredText (rst) format. The note advises users to tag datasets that can be used for training embedding models with the 'sentence-transformers' tag and expresses willingness to accept high quality datasets to the list.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/dataset_overview.md#2025-04-08_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. note::\n\n   We advise users to tag datasets that can be used for training embedding models with ``sentence-transformers`` by adding ``tags: sentence-transformers``. We would also gladly accept high quality datasets to be added to the list above for all to see and use.\n```\n\n----------------------------------------\n\nTITLE: Documenting CrossEncoderTrainer Class with Sphinx RST\nDESCRIPTION: ReStructuredText directive for auto-documenting the CrossEncoderTrainer class, showing selected methods and members while excluding certain internal methods.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/cross_encoder/trainer.md#2025-04-08_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: sentence_transformers.cross_encoder.trainer.CrossEncoderTrainer\n    :members:\n    :inherited-members:\n    :exclude-members: autocast_smart_context_manager, collect_features, compute_loss_context_manager, evaluation_loop, floating_point_ops, get_decay_parameter_names, get_optimizer_cls_and_kwargs, init_hf_repo, log_metrics, metrics_format, num_examples, num_tokens, predict, prediction_loop, prediction_step, save_metrics, save_state, training_step\n```\n\n----------------------------------------\n\nTITLE: Defining Hyperparameter Search Space with Optuna in Python\nDESCRIPTION: Function that defines the hyperparameter search space for a SentenceTransformer model using optuna. Includes parameters for epochs, batch size, warmup ratio, and learning rate.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/training/hpo/README.rst#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef hpo_search_space(trial):\n    return {\n        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 1, 2),\n        \"per_device_train_batch_size\": trial.suggest_int(\"per_device_train_batch_size\", 32, 128),\n        \"warmup_ratio\": trial.suggest_float(\"warmup_ratio\", 0, 0.3),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n    }\n```\n\n----------------------------------------\n\nTITLE: OpenVINO Model Hub Publishing\nDESCRIPTION: Example of pushing an OpenVINO-exported model to Hugging Face Hub.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/usage/efficiency.rst#2025-04-08_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nmodel = SentenceTransformer(\"intfloat/multilingual-e5-small\", backend=\"openvino\")\nmodel.push_to_hub(\"intfloat/multilingual-e5-small\", create_pr=True)\n```\n\n----------------------------------------\n\nTITLE: SimilarityFunction Documentation\nDESCRIPTION: RST documentation block for the SimilarityFunction class that defines similarity computation methods.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/sentence_transformer/SentenceTransformer.md#2025-04-08_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: sentence_transformers.SimilarityFunction\n   :members:\n```\n\n----------------------------------------\n\nTITLE: Implementing Matryoshka2dLoss for dimension and layer reduction\nDESCRIPTION: Code for implementing Matryoshka2dLoss which combines MatryoshkaLoss with AdaptiveLayerLoss to reduce both embedding dimensions and model layers for more efficient inference.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/training/matryoshka/README.md#2025-04-08_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer\nfrom sentence_transformers.losses import CoSENTLoss, Matryoshka2dLoss\n\nmodel = SentenceTransformer(\"microsoft/mpnet-base\")\n\nbase_loss = CoSENTLoss(model=model)\nloss = Matryoshka2dLoss(model=model, loss=base_loss, matryoshka_dims=[768, 512, 256, 128, 64])\n```\n\n----------------------------------------\n\nTITLE: Model Initialization for HPO Trials\nDESCRIPTION: Function that initializes a SentenceTransformer model for each hyperparameter optimization trial using distilbert-base-uncased.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/training/hpo/README.rst#2025-04-08_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef hpo_model_init(trial):\n    return SentenceTransformer(\"distilbert-base-uncased\")\n```\n\n----------------------------------------\n\nTITLE: Documenting SentenceLabelDataset in Python\nDESCRIPTION: This RST code block generates documentation for the SentenceLabelDataset class using autoclass directive.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/sentence_transformer/datasets.md#2025-04-08_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: sentence_transformers.datasets.SentenceLabelDataset\n```\n\n----------------------------------------\n\nTITLE: Loss Function Initialization for HPO\nDESCRIPTION: Function that initializes the CosineSimilarityLoss for the model during hyperparameter optimization trials.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/training/hpo/README.rst#2025-04-08_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef hpo_loss_init(model):\n    return losses.CosineSimilarityLoss(model)\n```\n\n----------------------------------------\n\nTITLE: Documenting DenoisingAutoEncoderDataset in Python\nDESCRIPTION: This RST code block generates documentation for the DenoisingAutoEncoderDataset class using autoclass directive.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/sentence_transformer/datasets.md#2025-04-08_snippet_3\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: sentence_transformers.datasets.DenoisingAutoEncoderDataset\n```\n\n----------------------------------------\n\nTITLE: Importing ParaphraseMiningEvaluator in Python\nDESCRIPTION: This code snippet illustrates how to import and use the ParaphraseMiningEvaluator class from the sentence_transformers.evaluation module.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/sentence_transformer/evaluation.md#2025-04-08_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: sentence_transformers.evaluation.ParaphraseMiningEvaluator\n```\n\n----------------------------------------\n\nTITLE: Documenting CrossEncoderClassificationEvaluator Class\nDESCRIPTION: Auto-documentation directive for the CrossEncoderClassificationEvaluator class from the sentence-transformers cross-encoder evaluation module.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/cross_encoder/evaluation.md#2025-04-08_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: sentence_transformers.cross_encoder.evaluation.CrossEncoderClassificationEvaluator\n```\n\n----------------------------------------\n\nTITLE: Computing Objective Function for HPO\nDESCRIPTION: Function that computes the objective value to be optimized during hyperparameter search using evaluation metrics.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/training/hpo/README.rst#2025-04-08_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef hpo_compute_objective(metrics):\n    return metrics[\"eval_sts-dev_spearman_cosine\"]\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Table of Contents\nDESCRIPTION: Sphinx documentation table of contents directive that defines the structure and navigation for the Sentence Transformers documentation.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/sentence_transformer/index.rst#2025-04-08_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n\n   SentenceTransformer\n   trainer\n   training_args\n   losses\n   sampler\n   evaluation\n   datasets\n   models\n   quantization\n```\n\n----------------------------------------\n\nTITLE: Optimal Hyperparameters Result for Sentence Transformers\nDESCRIPTION: The best performing hyperparameter configuration from the optimization trials, achieving 0.802 Spearman correlation. Shows the optimal values for epochs, batch size, warmup ratio and learning rate.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/training/hpo/README.rst#2025-04-08_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nBestRun(run_id='19', objective=0.8027501854704168, hyperparameters={'num_train_epochs': 1, 'per_device_train_batch_size': 84, 'warmup_ratio': 0.014601112207929548, 'learning_rate': 5.627813947769514e-06}, run_summary=None)\n```\n\n----------------------------------------\n\nTITLE: Migrating activation_fct Parameter in CrossEncoder\nDESCRIPTION: Shows how to migrate from using activation_fct parameter in CrossEncoder.fit to passing it to the loss function in v4.x. In v4.x, the activation function is provided directly to the loss class constructor as activation_fn.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/migration_guide.md#2025-04-08_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n...\n\n# Finetune the model\nmodel.fit(\n    train_dataloader=train_dataloader,\n    activation_fct=torch.nn.Sigmoid(),\n)\n```\n\nLANGUAGE: python\nCODE:\n```\n...\n\n# Prepare the loss function\nloss = MSELoss(model, activation_fn=torch.nn.Sigmoid())\n\n# Finetune the model\ntrainer = CrossEncoderTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    loss=loss,\n)\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: Importing Main SentenceTransformer Classes\nDESCRIPTION: Code snippet showing how to import the three main classes (Transformer, Pooling, Dense) from the sentence_transformers.models module. These are the fundamental building blocks for creating SentenceTransformer networks.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/sentence_transformer/models.md#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: sentence_transformers.models.Transformer\n.. autoclass:: sentence_transformers.models.Pooling\n.. autoclass:: sentence_transformers.models.Dense\n```\n\n----------------------------------------\n\nTITLE: Importing NanoBEIREvaluator in Python\nDESCRIPTION: This code snippet demonstrates how to import and use the NanoBEIREvaluator class from the sentence_transformers.evaluation module.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/sentence_transformer/evaluation.md#2025-04-08_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: sentence_transformers.evaluation.NanoBEIREvaluator\n```\n\n----------------------------------------\n\nTITLE: Migrating train_objectives parameter in SentenceTransformer.fit (Python)\nDESCRIPTION: This snippet shows how to migrate the train_objectives parameter from SentenceTransformer.fit in v2.x to using SentenceTransformerTrainer in v3.x. It demonstrates the transition from using InputExample and DataLoader to using the datasets library and SentenceTransformerTrainer.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/migration_guide.md#2025-04-08_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer, InputExample, losses\nfrom torch.utils.data import DataLoader\n\n# Define a training dataloader\ntrain_examples = [\n    InputExample(texts=[\n        \"A person on a horse jumps over a broken down airplane.\",\n        \"A person is outdoors, on a horse.\",\n        \"A person is at a diner, ordering an omelette.\",\n    ]),\n    InputExample(texts=[\n        \"Children smiling and waving at camera\",\n        \"There are children present\",\n        \"The kids are frowning\",\n    ]),\n]\ntrain_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n\n# Define a loss function\ntrain_loss = losses.MultipleNegativesRankingLoss(model)\n\n# Finetune the model\nmodel.fit(train_objectives=[(train_dataloader, train_loss)])\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import Dataset\nfrom sentence_transformers import SentenceTransformer, SentenceTransformerTrainer\nfrom sentence_transformers.losses import MultipleNegativesRankingLoss\n\n# Define a training dataset\ntrain_examples = [\n    {\n        \"anchor\": \"A person on a horse jumps over a broken down airplane.\",\n        \"positive\": \"A person is outdoors, on a horse.\",\n        \"negative\": \"A person is at a diner, ordering an omelette.\",\n    },\n    {\n        \"anchor\": \"Children smiling and waving at camera\",\n        \"positive\": \"There are children present\",\n        \"negative\": \"The kids are frowning\",\n    },\n]\ntrain_dataset = Dataset.from_list(train_examples)\n\n# Define a loss function\nloss = MultipleNegativesRankingLoss(model)\n\n# Finetune the model\ntrainer = SentenceTransformerTrainer(\n    model=model,\n    train_dataset=train_dataset,\n    loss=loss,\n)\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: No Duplicates Batch Sampler Documentation\nDESCRIPTION: Auto-generated documentation for the NoDuplicatesBatchSampler class that prevents duplicate samples in batches.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/sentence_transformer/sampler.md#2025-04-08_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: sentence_transformers.sampler.NoDuplicatesBatchSampler\n    :members:\n```\n\n----------------------------------------\n\nTITLE: Migrating epochs parameter in SentenceTransformer.fit (Python)\nDESCRIPTION: This snippet shows how to migrate the epochs parameter from SentenceTransformer.fit in v2.x to using SentenceTransformerTrainingArguments in v3.x. It demonstrates setting the number of training epochs.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/migration_guide.md#2025-04-08_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n...\n\n# Finetune the model\nmodel.fit(\n    train_objectives=[(train_dataloader, train_loss)],\n    epochs=1,\n)\n```\n\nLANGUAGE: python\nCODE:\n```\n...\n\n# Prepare the Training Arguments\nargs = SentenceTransformerTrainingArguments(\n    num_train_epochs=1,\n)\n\n# Finetune the model\ntrainer = SentenceTransformerTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    loss=loss,\n)\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: Deprecation Notice for sentence_transformers.datasets\nDESCRIPTION: This RST code block provides a deprecation notice for the sentence_transformers.datasets classes and suggests alternatives for SentenceLabelDataset and NoDuplicatesDataLoader.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/sentence_transformer/datasets.md#2025-04-08_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. note::\n    The ``sentence_transformers.datasets`` classes have been deprecated, and only exist for compatibility with the `deprecated training <../../sentence_transformer/training_overview.html#deprecated-training>`_.\n\n    * Instead of :class:`~sentence_transformers.datasets.SentenceLabelDataset`, you can now use ``BatchSamplers.GROUP_BY_LABEL`` to use the :class:`~sentence_transformers.sampler.GroupByLabelBatchSampler`.\n    * Instead of :class:`~sentence_transformers.datasets.NoDuplicatesDataLoader`, you can now use the ``BatchSamplers.NO_DUPLICATES`` to use the :class:`~sentence_transformers.sampler.NoDuplicatesBatchSampler`.\n```\n\n----------------------------------------\n\nTITLE: Proportional Batch Sampler Documentation\nDESCRIPTION: Auto-generated documentation for the ProportionalBatchSampler class that samples from multiple datasets proportionally.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/sentence_transformer/sampler.md#2025-04-08_snippet_6\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: sentence_transformers.sampler.ProportionalBatchSampler\n    :members:\n```\n\n----------------------------------------\n\nTITLE: Importing SentenceEvaluator in Python\nDESCRIPTION: This code snippet shows how to import and use the SentenceEvaluator class from the sentence_transformers.evaluation module.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/sentence_transformer/evaluation.md#2025-04-08_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: sentence_transformers.evaluation.SentenceEvaluator\n```\n\n----------------------------------------\n\nTITLE: Migrating optimizer_class and optimizer_params in SentenceTransformer.fit (Python)\nDESCRIPTION: This snippet shows how to migrate the optimizer_class and optimizer_params parameters from SentenceTransformer.fit in v2.x to using SentenceTransformerTrainingArguments in v3.x. It demonstrates setting the optimizer type and its parameters.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/migration_guide.md#2025-04-08_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n...\n\n# Finetune the model\nmodel.fit(\n    train_objectives=[(train_dataloader, train_loss)],\n    optimizer_class=torch.optim.AdamW,\n    optimizer_params={\"eps\": 1e-7},\n)\n```\n\nLANGUAGE: python\nCODE:\n```\n...\n\n# Prepare the Training Arguments\nargs = SentenceTransformerTrainingArguments(\n    # See https://github.com/huggingface/transformers/blob/main/src/transformers/training_args.py\n    optim=\"adamw_torch\",\n    optim_args={\"eps\": 1e-7},\n)\n\n# Finetune the model\ntrainer = SentenceTransformerTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    loss=loss,\n)\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: Importing Model Optimization Functions from sentence_transformers.backend\nDESCRIPTION: Shows how to import model optimization functions from the sentence_transformers.backend module, which allow exporting models to optimized ONNX and OpenVINO formats for improved inference performance.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/util.md#2025-04-08_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n.. automodule:: sentence_transformers.backend\n   :members: export_optimized_onnx_model, export_dynamic_quantized_onnx_model, export_static_quantized_openvino_model\n```\n\n----------------------------------------\n\nTITLE: Migrating warmup_steps Parameter in CrossEncoder\nDESCRIPTION: Shows how to migrate from using warmup_steps parameter in CrossEncoder.fit to using CrossEncoderTrainingArguments in v4.x. In v4.x, warmup steps are configured directly in CrossEncoderTrainingArguments.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/migration_guide.md#2025-04-08_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n...\n\n# Finetune the model\nmodel.fit(\n    train_dataloader=train_dataloader,\n    warmup_steps=1000,\n)\n```\n\nLANGUAGE: python\nCODE:\n```\n...\n\n# Prepare the Training Arguments\nargs = CrossEncoderTrainingArguments(\n    warmup_steps=1000,\n)\n\n# Finetune the model\ntrainer = CrossEncoderTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    loss=loss,\n)\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: Importing MarginMSELoss in Python\nDESCRIPTION: This snippet shows how to import the MarginMSELoss class from the sentence_transformers.cross_encoder.losses module.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/cross_encoder/losses.md#2025-04-08_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: sentence_transformers.cross_encoder.losses.MarginMSELoss\n```\n\n----------------------------------------\n\nTITLE: Citing Multilingual Sentence-BERT in BibTeX Format\nDESCRIPTION: BibTeX citation for the paper on making monolingual sentence embeddings multilingual using knowledge distillation.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/publications.md#2025-04-08_snippet_1\n\nLANGUAGE: bibtex\nCODE:\n```\n@inproceedings{reimers-2020-multilingual-sentence-bert,\n    title = \"Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2020\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://arxiv.org/abs/2004.09813\",\n}\n```\n\n----------------------------------------\n\nTITLE: Training STS Benchmark in Python\nDESCRIPTION: Examples of training scripts for STS models using the sentence-transformers library. One script trains directly on STS data, while the other first trains on NLI data before fine-tuning on STS data.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/pretrained-models/sts-models.md#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nexamples/sentence_transformer/training_stsbenchmark.py\n```\n\nLANGUAGE: python\nCODE:\n```\nexamples/sentence_transformer/training_stsbenchmark_continue_training.py\n```\n\n----------------------------------------\n\nTITLE: Migrating optimizer Parameters in CrossEncoder\nDESCRIPTION: Shows how to migrate from using optimizer_class and optimizer_params in CrossEncoder.fit to using CrossEncoderTrainingArguments in v4.x. In v4.x, optimizer is configured using optim and optim_args in CrossEncoderTrainingArguments.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/migration_guide.md#2025-04-08_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n...\n\n# Finetune the model\nmodel.fit(\n    train_dataloader=train_dataloader,\n    optimizer_class=torch.optim.AdamW,\n    optimizer_params={\"eps\": 1e-7},\n)\n```\n\nLANGUAGE: python\nCODE:\n```\n...\n\n# Prepare the Training Arguments\nargs = CrossEncoderTrainingArguments(\n    # See https://github.com/huggingface/transformers/blob/main/src/transformers/training_args.py\n    optim=\"adamw_torch\",\n    optim_args={\"eps\": 1e-7},\n)\n\n# Finetune the model\ntrainer = CrossEncoderTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    loss=loss,\n)\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies for Sphinx Documentation\nDESCRIPTION: This requirements file specifies the exact versions of Sphinx and related packages needed for documentation generation. It includes core Sphinx, templating with Jinja2, Markdown support, table rendering, and various extensions for enhanced documentation features like copyable code blocks and Mermaid diagrams. The file also includes an editable installation of the parent package.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/requirements.txt#2025-04-08_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nsphinx==8.1.3\nJinja2==3.1.6\nmyst-parser==4.0.0\nsphinx_markdown_tables==0.0.17\nsphinx-copybutton==0.5.2\nsphinx_inline_tabs==2023.4.21\nsphinxcontrib-mermaid==1.0.0\nsphinx-toolbox==3.9.0\n-e ..\n```\n\n----------------------------------------\n\nTITLE: Migrating weight_decay Parameter in CrossEncoder\nDESCRIPTION: Shows how to migrate from using weight_decay parameter in CrossEncoder.fit to using CrossEncoderTrainingArguments in v4.x. In v4.x, weight decay is configured directly in CrossEncoderTrainingArguments.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/migration_guide.md#2025-04-08_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n...\n\n# Finetune the model\nmodel.fit(\n    train_dataloader=train_dataloader,\n    weight_decay=0.02,\n)\n```\n\nLANGUAGE: python\nCODE:\n```\n...\n\n# Prepare the Training Arguments\nargs = CrossEncoderTrainingArguments(\n    weight_decay=0.02,\n)\n\n# Finetune the model\ntrainer = CrossEncoderTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    loss=loss,\n)\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: Modules Configuration JSON Structure\nDESCRIPTION: Example of the modules.json configuration file structure that defines the model architecture and module paths.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/usage/custom_models.rst#2025-04-08_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"idx\": 0,\n    \"name\": \"0\",\n    \"path\": \"\",\n    \"type\": \"sentence_transformers.models.Transformer\"\n  },\n  {\n    \"idx\": 1,\n    \"name\": \"1\",\n    \"path\": \"1_DecayMeanPooling\",\n    \"type\": \"decay_pooling.DecayMeanPooling\"\n  },\n  {\n    \"idx\": 2,\n    \"name\": \"2\",\n    \"path\": \"2_Normalize\",\n    \"type\": \"sentence_transformers.models.Normalize\"\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Training CT on AskUbuntu Questions Dataset in Python\nDESCRIPTION: This example trains a CT model on the AskUbuntu Questions dataset, which contains questions from the AskUbuntu Stackexchange forum.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/unsupervised_learning/CT_In-Batch_Negatives/README.md#2025-04-08_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntrain_askubuntu_ct-improved.py\n```\n\n----------------------------------------\n\nTITLE: Configuring evaluation steps for CrossEncoder training in Python\nDESCRIPTION: Comparing the v3.x and v4.x approaches for configuring evaluation steps during CrossEncoder training. In v4.x, the recommended approach uses CrossEncoderTrainingArguments to set evaluation strategy and steps.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/migration_guide.md#2025-04-08_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n# Prepare the Training Arguments\nargs = CrossEncoderTrainingArguments(\n    eval_strategy=\"steps\",\n    eval_steps=1000,\n)\n\n# Finetune the model\n# Note: You need an eval_dataset and/or evaluator to evaluate\ntrainer = CrossEncoderTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    loss=loss,\n    evaluator=evaluator,\n)\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: Markdown Training Documentation Structure\nDESCRIPTION: Directory structure and examples for training CrossEncoder models with different approaches and datasets.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/cross_encoder/training/README.md#2025-04-08_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Training\n\nThis folder contains various examples to fine-tune `CrossEncoder` models for specific tasks.\n\nFor the beginning, I can recommend to have a look at the [MS MARCO](ms_marco/) examples.\n\nFor the documentation how to train your own models, see [Training Overview](http://www.sbert.net/docs/cross_encoder/training_overview.html).\n\n## Training Examples\n- [distillation](distillation/) - Examples to make models smaller, faster and lighter.\n- [ms_marco](ms_marco/) - Numerous example training scripts for training on the MS MARCO information retrieval dataset.\n- [nli](nli/) - Natural Language Inference (NLI) data involves pair classification using the \"contradiction\", \"entailment\", and \"neutral\" classes.\n- [quora_duplicate_questions](quora_duplicate_questions/) - Quora Duplicate Questions is large set corpus with duplicate questions from the Quora community. The folder contains examples how to train models for duplicate questions mining and for semantic search.\n- [rerankers](rerankers/) - Example training scripts for training on generic information retrieval datasets.\n- [sts](sts/) - The most basic method to train models is using Semantic Textual Similarity (STS) data. Here, we have a sentence pair and a score indicating the semantic similarity.\n```\n\n----------------------------------------\n\nTITLE: Citing Multilingual Sentence-BERT Paper\nDESCRIPTION: BibTeX citation for the multilingual Sentence-BERT paper 'Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation' published in EMNLP 2020.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/README.md#2025-04-08_snippet_7\n\nLANGUAGE: bibtex\nCODE:\n```\n@inproceedings{reimers-2020-multilingual-sentence-bert,\n    title = \"Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2020\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://arxiv.org/abs/2004.09813\",\n}\n```\n\n----------------------------------------\n\nTITLE: Citation Template in BibTeX Format\nDESCRIPTION: A template for displaying citations in BibTeX format for each loss function used in the model. The citations are dynamically populated from a 'citations' variable that contains different loss functions and their corresponding citation information.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/cross_encoder/model_card_template.md#2025-04-08_snippet_2\n\nLANGUAGE: bibtex\nCODE:\n```\n{{ citation | trim }}\n```\n\n----------------------------------------\n\nTITLE: RST Note Block for Unsupervised Learning Warning\nDESCRIPTION: ReStructuredText note block warning users about the limitations of unsupervised learning approaches compared to supervised methods, recommending domain adaptation as a better alternative.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/unsupervised_learning/README.md#2025-04-08_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. note::\n    Unsupervised learning approaches are still an activate research area and in many cases the models perform rather poorly compared to models that are using training pairs as provided in our `training data collection <https://huggingface.co/collections/sentence-transformers/embedding-model-datasets-6644d7a3673a511914aa7552>`_. A better approach is `Domain Adaptation <../domain_adaptation/README.html>`_ where you combine unsupervised learning on your target domain with existent labeled data. This should give the best performance on your specific corpus.\n```\n\n----------------------------------------\n\nTITLE: Training CrossEncoder for Quora Duplicate Questions Detection in Python\nDESCRIPTION: This code snippet references a separate Python file that demonstrates how to train a CrossEncoder model using BinaryCrossEntropyLoss for detecting duplicate questions in the Quora dataset.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/cross_encoder/training/quora_duplicate_questions/README.md#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# [training_quora_duplicate_questions.py](training_quora_duplicate_questions.py)\n```\n\n----------------------------------------\n\nTITLE: Citing MS MARCO Models in BibTeX Format\nDESCRIPTION: BibTeX citation for the paper discussing the challenges of dense low-dimensional information retrieval for large index sizes.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/publications.md#2025-04-08_snippet_3\n\nLANGUAGE: bibtex\nCODE:\n```\n@inproceedings{reimers-2020-Curse_Dense_Retrieval,\n    title = \"The Curse of Dense Low-Dimensional Information Retrieval for Large Index Sizes\",\n    author = \"Reimers, Nils  and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)\",\n    month = \"8\",\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://arxiv.org/abs/2012.14210\",\n    pages = \"605--611\",\n}\n```\n\n----------------------------------------\n\nTITLE: Citing Original Sentence-BERT Paper\nDESCRIPTION: BibTeX citation for the original Sentence-BERT paper 'Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks' published in EMNLP 2019.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/README.md#2025-04-08_snippet_6\n\nLANGUAGE: bibtex\nCODE:\n```\n@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://arxiv.org/abs/1908.10084\",\n}\n```\n\n----------------------------------------\n\nTITLE: Citing BEIR in BibTeX Format\nDESCRIPTION: BibTeX citation for the paper introducing BEIR, a heterogeneous benchmark for zero-shot evaluation of information retrieval models.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/publications.md#2025-04-08_snippet_5\n\nLANGUAGE: bibtex\nCODE:\n```\n@inproceedings{thakur-2021-BEIR,\n    title = \"BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models\",\n    author = {Thakur, Nandan and Reimers, Nils and R{\\\"{u}}ckl{\\\"{e}}, Andreas and Srivastava, Abhishek and Gurevych, Iryna}, \n    booktitle={Thirty-fifth Conference on Neural Information Processing Systems (NeurIPS 2021) - Datasets and Benchmarks Track (Round 2)},\n    month = \"4\",\n    year = \"2021\",\n    url = \"https://arxiv.org/abs/2104.08663\",\n}\n```\n\n----------------------------------------\n\nTITLE: BibTeX Citation Template Structure\nDESCRIPTION: A template section for displaying BibTeX citations using a loop structure that iterates through citation entries.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/model_card_template.md#2025-04-08_snippet_2\n\nLANGUAGE: bibtex\nCODE:\n```\n{{ citation | trim }}\n```\n\n----------------------------------------\n\nTITLE: Installing Sentence Transformers Library\nDESCRIPTION: Command to install the Sentence Transformers library using pip.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/model_card_template.md#2025-04-08_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -U sentence-transformers\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for CLIP Model Training in Python\nDESCRIPTION: This snippet imports necessary modules from PIL, sentence_transformers, and torch for image processing and model training.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/training/clip/train_clip.ipynb#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom PIL import Image\nfrom sentence_transformers import SentenceTransformer, losses, InputExample\nfrom sentence_transformers import util\nfrom torch.utils.data import DataLoader\n```\n\n----------------------------------------\n\nTITLE: Citation for Multilingual Sentence Embeddings in BibTeX format\nDESCRIPTION: BibTeX citation for the paper 'Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation' by Reimers and Gurevych. This should be cited when using the code for multilingual models.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/training/multilingual/README.md#2025-04-08_snippet_5\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{reimers-2020-multilingual-sentence-bert,\n    title = \"Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    journal= \"arXiv preprint arXiv:2004.09813\",\n    month = \"04\",\n    year = \"2020\",\n    url = \"http://arxiv.org/abs/2004.09813\",\n}\n```\n\n----------------------------------------\n\nTITLE: Citing TSDAE Paper in BibTeX Format\nDESCRIPTION: BibTeX citation for the paper 'TSDAE: Using Transformer-based Sequential Denoising Auto-Encoderfor Unsupervised Sentence Embedding Learning'\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/domain_adaptation/README.md#2025-04-08_snippet_0\n\nLANGUAGE: bibtex\nCODE:\n```\n@inproceedings{wang-2021-TSDAE,\n    title = \"TSDAE: Using Transformer-based Sequential Denoising Auto-Encoderfor Unsupervised Sentence Embedding Learning\",\n    author = \"Wang, Kexin and Reimers, Nils and Gurevych, Iryna\", \n    booktitle = \"Findings of the Association for Computational Linguistics: EMNLP 2021\",\n    month = nov,\n    year = \"2021\",\n    address = \"Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    pages = \"671--688\",\n    url = \"https://arxiv.org/abs/2104.06979\",\n}\n```\n\n----------------------------------------\n\nTITLE: Loading CLIP Model in Python\nDESCRIPTION: This code loads the pre-trained CLIP model 'clip-ViT-B-32' using the SentenceTransformer class.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/training/clip/train_clip.ipynb#2025-04-08_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Load CLIP model\nmodel = SentenceTransformer('clip-ViT-B-32')\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting Image Dataset in Python\nDESCRIPTION: This snippet downloads a dataset of 25k images from Unsplash and extracts 1000 of them into a local folder. It uses os, zipfile, and tqdm libraries for file operations and progress tracking.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/training/clip/train_clip.ipynb#2025-04-08_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport zipfile\nfrom tqdm import tqdm\n\n# Next, we get about 25k images from Unsplash \nimg_folder = 'photos/'\nif not os.path.exists(img_folder) or len(os.listdir(img_folder)) == 0:\n    os.makedirs(img_folder, exist_ok=True)\n    \n    photo_filename = 'unsplash-25k-photos.zip'\n    if not os.path.exists(photo_filename):   # Download dataset if does not exist\n        util.http_get('http://sbert.net/datasets/'+photo_filename, photo_filename)\n        \n    # Extract 1000 images\n    with zipfile.ZipFile(photo_filename, 'r') as zf:\n        for member in tqdm(zf.infolist()[:1000], desc='Extracting'):\n            zf.extract(member, img_folder)\n```\n\n----------------------------------------\n\nTITLE: Initializing Sentence Transformers with Custom Similarity Function\nDESCRIPTION: Shows how to initialize a Sentence Transformer model with a specific similarity function. This example uses the dot product similarity metric instead of the default cosine similarity.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/usage/semantic_textual_similarity.rst#2025-04-08_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer, SimilarityFunction\n\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\", similarity_fn_name=SimilarityFunction.DOT_PRODUCT)\n```\n\n----------------------------------------\n\nTITLE: Accessing Model Architecture\nDESCRIPTION: Code demonstrating how to load a pre-trained adaptive layer model and access its underlying transformer architecture for inspection.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/training/adaptive_layer/README.md#2025-04-08_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\"tomaarsen/mpnet-base-nli-adaptive-layer\")\n\n# We can access the underlying model with `model[0].auto_model`\nprint(model[0].auto_model)\n```\n\n----------------------------------------\n\nTITLE: Creating a Dataset for STS Training\nDESCRIPTION: This code snippet demonstrates how to create a simple dataset for Semantic Textual Similarity training using the datasets library. It creates a custom dataset with sentence pairs and their similarity scores.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/training/sts/README.md#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import Dataset\n\nsentence1_list = [\"My first sentence\", \"Another pair\"]\nsentence2_list = [\"My second sentence\", \"Unrelated sentence\"]\nlabels_list = [0.8, 0.3]\ntrain_dataset = Dataset.from_dict({\n    \"sentence1\": sentence1_list,\n    \"sentence2\": sentence2_list,\n    \"label\": labels_list,\n})\n# => Dataset({\n#     features: ['sentence1', 'sentence2', 'label'],\n#     num_rows: 2\n# })\nprint(train_dataset[0])\n# => {'sentence1': 'My first sentence', 'sentence2': 'My second sentence', 'label': 0.8}\nprint(train_dataset[1])\n# => {'sentence1': 'Another pair', 'sentence2': 'Unrelated sentence', 'label': 0.3}\n```\n\n----------------------------------------\n\nTITLE: Training Sentence Transformers Model in v3.x\nDESCRIPTION: This snippet shows the recommended approach for training a Sentence Transformers model in v3.x. It uses the new SentenceTransformerTrainer class, loads a dataset, defines a loss function, and trains the model using the new trainer.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/migration_guide.md#2025-04-08_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\nfrom sentence_transformers import SentenceTransformer, SentenceTransformerTrainer\nfrom sentence_transformers.losses import MultipleNegativesRankingLoss\n\n# 1. Define the model. Either from scratch of by loading a pre-trained model\nmodel = SentenceTransformer(\"microsoft/mpnet-base\")\n\n# 2. Load a dataset to finetune on\ndataset = load_dataset(\"sentence-transformers/all-nli\", \"triplet\")\ntrain_dataset = dataset[\"train\"].select(range(10_000))\neval_dataset = dataset[\"dev\"].select(range(1_000))\n\n# 3. Define a loss function\nloss = MultipleNegativesRankingLoss(model)\n\n# 4. Create a trainer & train\ntrainer = SentenceTransformerTrainer(\n    model=model,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    loss=loss,\n)\ntrainer.train()\n\n# 5. Save the trained model\nmodel.save_pretrained(\"models/mpnet-base-all-nli\")\n# model.push_to_hub(\"mpnet-base-all-nli\")\n```\n\n----------------------------------------\n\nTITLE: Migrating steps_per_epoch parameter in SentenceTransformer.fit (Python)\nDESCRIPTION: This snippet demonstrates how to migrate the steps_per_epoch parameter from SentenceTransformer.fit in v2.x to using SentenceTransformerTrainingArguments in v3.x. It shows how to set the maximum number of training steps.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/migration_guide.md#2025-04-08_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n...\n\n# Finetune the model\nmodel.fit(\n    train_objectives=[(train_dataloader, train_loss)],\n    steps_per_epoch=1000,\n)\n```\n\nLANGUAGE: python\nCODE:\n```\n...\n\n# Prepare the Training Arguments\nargs = SentenceTransformerTrainingArguments(\n    max_steps=1000, # Note: max_steps is across all epochs, not per epoch\n)\n\n# Finetune the model\ntrainer = SentenceTransformerTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    loss=loss,\n)\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: Migrating weight_decay parameter in SentenceTransformer.fit (Python)\nDESCRIPTION: This snippet demonstrates how to migrate the weight_decay parameter from SentenceTransformer.fit in v2.x to using SentenceTransformerTrainingArguments in v3.x. It shows how to set the weight decay for the optimizer.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/migration_guide.md#2025-04-08_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n...\n\n# Finetune the model\nmodel.fit(\n    train_objectives=[(train_dataloader, train_loss)],\n    weight_decay=0.02,\n)\n```\n\nLANGUAGE: python\nCODE:\n```\n...\n\n# Prepare the Training Arguments\nargs = SentenceTransformerTrainingArguments(\n    weight_decay=0.02,\n)\n\n# Finetune the model\ntrainer = SentenceTransformerTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    loss=loss,\n)\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: Migrating train_dataloader Parameter in CrossEncoder\nDESCRIPTION: Shows how to migrate from using train_dataloader parameter in CrossEncoder.fit to using CrossEncoderTrainer with a dataset in v4.x. In v4.x, train examples are structured as a Dataset object with proper sentence fields instead of InputExample objects.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/migration_guide.md#2025-04-08_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import CrossEncoder, InputExample\nfrom torch.utils.data import DataLoader\n\n# 1. Define the model. Either from scratch of by loading a pre-trained model\nmodel = CrossEncoder(\"microsoft/mpnet-base\")\n\n# 2. Define your train examples. You need more than just two examples...\ntrain_examples = [\n    InputExample(texts=[\"What are pandas?\", \"The giant panda ...\"], label=1),\n    InputExample(texts=[\"What's a panda?\", \"Mount Vesuvius is a ...\"], label=0),\n]\ntrain_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n\n# 3. Finetune the model\nmodel.fit(train_dataloader=train_dataloader)\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import Dataset\nfrom sentence_transformers import CrossEncoder, CrossEncoderTrainer\nfrom sentence_transformers.cross_encoder.losses import BinaryCrossEntropyLoss\n\n# Define a training dataset\ntrain_examples = [\n    {\n        \"sentence_1\": \"A person on a horse jumps over a broken down airplane.\",\n        \"sentence_2\": \"A person is outdoors, on a horse.\",\n        \"label\": 1,\n    },\n    {\n        \"sentence_1\": \"Children smiling and waving at camera\",\n        \"sentence_2\": \"The kids are frowning\",\n        \"label\": 0,\n    },\n]\ntrain_dataset = Dataset.from_list(train_examples)\n\n# Define a loss function\nloss = BinaryCrossEntropyLoss(model)\n\n# Finetune the model\ntrainer = CrossEncoderTrainer(\n    model=model,\n    train_dataset=train_dataset,\n    loss=loss,\n)\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: Migrating scheduler Parameter in CrossEncoder\nDESCRIPTION: Shows how to migrate from using scheduler parameter in CrossEncoder.fit to using CrossEncoderTrainingArguments in v4.x. In v4.x, scheduler is configured via the lr_scheduler_type parameter in CrossEncoderTrainingArguments.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/migration_guide.md#2025-04-08_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n...\n\n# Finetune the model\nmodel.fit(\n    train_dataloader=train_dataloader,\n    scheduler=\"WarmupLinear\",\n)\n```\n\nLANGUAGE: python\nCODE:\n```\n...\n\n# Prepare the Training Arguments\nargs = CrossEncoderTrainingArguments(\n    # See https://huggingface.co/docs/transformers/main_classes/optimizer_schedules#transformers.SchedulerType\n    lr_scheduler_type=\"linear\"\n)\n\n# Finetune the model\ntrainer = CrossEncoderTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    loss=loss,\n)\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: Saving best models in CrossEncoder training in Python\nDESCRIPTION: Comparing v3.x and v4.x methods for saving the best model during CrossEncoder training. The v4.x approach uses training arguments to configure model saving and explicitly saves the model after training.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/migration_guide.md#2025-04-08_snippet_27\n\nLANGUAGE: python\nCODE:\n```\n# Finetune the model\nmodel.fit(\n    train_dataloader=train_dataloader,\n    evaluator=evaluator,\n    output_path=\"my/path\",\n    save_best_model=True,\n)\n```\n\nLANGUAGE: python\nCODE:\n```\n# Prepare the Training Arguments\nargs = CrossEncoderTrainingArguments(\n    load_best_model_at_end=True,\n    metric_for_best_model=\"hotpotqa_ndcg@10\", # E.g. `evaluator.primary_metric`\n)\n\n# Finetune the model\ntrainer = CrossEncoderTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    loss=loss,\n)\ntrainer.train()\n\n# Save the best model at my output path\nmodel.save_pretrained(\"my/path\")\n```\n\n----------------------------------------\n\nTITLE: Setting maximum gradient norm in CrossEncoder training in Python\nDESCRIPTION: Comparing v3.x and v4.x approaches for configuring gradient clipping during CrossEncoder training. In v4.x, this is set via CrossEncoderTrainingArguments.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/migration_guide.md#2025-04-08_snippet_28\n\nLANGUAGE: python\nCODE:\n```\n# Finetune the model\nmodel.fit(\n    train_dataloader=train_dataloader,\n    max_grad_norm=1,\n)\n```\n\nLANGUAGE: python\nCODE:\n```\n# Prepare the Training Arguments\nargs = CrossEncoderTrainingArguments(\n    max_grad_norm=1,\n)\n\n# Finetune the model\ntrainer = CrossEncoderTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    loss=loss,\n)\ntrainer.train()\n```\n\n----------------------------------------\n\nTITLE: Loading a Pretrained PEFT Adapter in Python\nDESCRIPTION: Illustrates two methods to load a pretrained PEFT adapter: directly loading the adapter model, and loading the base model then adding the adapter.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/training/peft/README.md#2025-04-08_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\"tomaarsen/bert-base-uncased-gooaq-peft\")\nembeddings = model.encode([\"This is an example sentence\", \"Each sentence is converted\"])\nprint(embeddings.shape)\n# (2, 768)\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\"bert-base-uncased\")\nmodel.load_adapter(\"tomaarsen/bert-base-uncased-gooaq-peft\")\nembeddings = model.encode([\"This is an example sentence\", \"Each sentence is converted\"])\nprint(embeddings.shape)\n# (2, 768)\n```\n\n----------------------------------------\n\nTITLE: Initializing Custom Sentence Transformer Model in Python\nDESCRIPTION: This snippet demonstrates how to create a custom Sentence Transformer model by initializing specific modules (Transformer, Pooling, and Normalize) and combining them into a SentenceTransformer object.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/usage/custom_models.rst#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import models, SentenceTransformer\n\ntransformer = models.Transformer(\"sentence-transformers/all-MiniLM-L6-v2\", max_seq_length=256)\npooling = models.Pooling(transformer.get_word_embedding_dimension(), pooling_mode=\"mean\")\nnormalize = models.Normalize()\n\nmodel = SentenceTransformer(modules=[transformer, pooling, normalize])\n```\n\n----------------------------------------\n\nTITLE: config_sentence_transformers.json Content\nDESCRIPTION: This JSON snippet shows the contents of the config_sentence_transformers.json file, which includes version information and configuration options for a Sentence Transformer model.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/usage/custom_models.rst#2025-04-08_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"__version__\": {\n    \"sentence_transformers\": \"3.0.1\",\n    \"transformers\": \"4.43.4\",\n    \"pytorch\": \"2.5.0\"\n  },\n  \"prompts\": {},\n  \"default_prompt_name\": null,\n  \"similarity_fn_name\": null\n}\n```\n\n----------------------------------------\n\nTITLE: Custom Module Configuration with Repository Reference\nDESCRIPTION: Example of modules.json configuration that includes a reference to a custom module stored in a separate repository.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/usage/custom_models.rst#2025-04-08_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"idx\": 0,\n    \"name\": \"0\",\n    \"path\": \"\",\n    \"type\": \"sentence_transformers.models.Transformer\"\n  },\n  {\n    \"idx\": 1,\n    \"name\": \"1\",\n    \"path\": \"1_DecayMeanPooling\",\n    \"type\": \"my-user/my-model-implementation--decay_pooling.DecayMeanPooling\"\n  },\n  {\n    \"idx\": 2,\n    \"name\": \"2\",\n    \"path\": \"2_Normalize\",\n    \"type\": \"sentence_transformers.models.Normalize\"\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Custom Module with Keyword Arguments Configuration\nDESCRIPTION: Example of modules.json configuration that includes keyword argument specification for custom behavior.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/usage/custom_models.rst#2025-04-08_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"idx\": 0,\n    \"name\": \"0\",\n    \"path\": \"\",\n    \"type\": \"custom_transformer.CustomTransformer\",\n    \"kwargs\": [\"task_type\"]\n  },\n  {\n    \"idx\": 1,\n    \"name\": \"1\",\n    \"path\": \"1_Pooling\",\n    \"type\": \"sentence_transformers.models.Pooling\"\n  },\n  {\n    \"idx\": 2,\n    \"name\": \"2\",\n    \"path\": \"2_Normalize\",\n    \"type\": \"sentence_transformers.models.Normalize\"\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Using Custom Module with Keyword Arguments\nDESCRIPTION: Example showing how to use a custom module with keyword arguments when encoding text.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/usage/custom_models.rst#2025-04-08_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\"your-username/your-model-id\", trust_remote_code=True)\ntexts = [...]\nmodel.encode(texts, task_type=\"default\")\n```\n\n----------------------------------------\n\nTITLE: Citation Format for Augmented SBERT Paper\nDESCRIPTION: BibTeX citation format for referencing the Augmented SBERT research paper published on arXiv.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/training/data_augmentation/README.md#2025-04-08_snippet_0\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{thakur-2020-AugSBERT,\n    title = \"Augmented SBERT: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks\",\n    author = \"Thakur, Nandan and Reimers, Nils and Daxenberger, Johannes and  Gurevych, Iryna\", \n    journal= \"arXiv preprint arXiv:2010.08240\",\n    month = \"10\",\n    year = \"2020\",\n    url = \"https://arxiv.org/abs/2010.08240\",\n}\n```\n\n----------------------------------------\n\nTITLE: Using torchrun for Distributed Data Parallel training in Sentence Transformers\nDESCRIPTION: Command for launching distributed training with torchrun, which implements Distributed Data Parallel (DDP) strategy across 4 GPUs. This approach provides better performance than Data Parallel (DP) and single-GPU training.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/training/distributed.rst#2025-04-08_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ntorchrun --nproc_per_node=4 train_script.py\n```\n\n----------------------------------------\n\nTITLE: Using accelerate for Distributed Data Parallel training in Sentence Transformers\nDESCRIPTION: Command for launching distributed training with Hugging Face's accelerate library, which implements Distributed Data Parallel (DDP) strategy across 4 GPUs. This approach provides similar performance benefits to torchrun.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/training/distributed.rst#2025-04-08_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\naccelerate launch --num_processes 4 train_script.py\n```\n\n----------------------------------------\n\nTITLE: Main function structure for distributed training in Sentence Transformers\nDESCRIPTION: Code template showing how to structure your training script for distributed training. When using DDP, the script needs to wrap training code in a main function to prevent code execution duplication across processes.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/training/distributed.rst#2025-04-08_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer, SentenceTransformerTrainingArguments, SentenceTransformerTrainer\n# Other imports here\n\ndef main():\n    # Your training code here\n\nif __name__ == \"__main__\":\n    main()\n```\n\n----------------------------------------\n\nTITLE: Referencing Paper and Loss Function in reStructuredText\nDESCRIPTION: This code snippet uses reStructuredText to reference a paper on making monolingual sentence embeddings multilingual and links to the MultipleNegativesRankingLoss documentation. It highlights the effectiveness of paraphrase data with this loss function for learning sentence embeddings.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/training/paraphrases/README.md#2025-04-08_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n```{eval-rst}\nIn our paper `Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation <https://arxiv.org/abs/2004.09813>`_, we showed that paraphrase data together with :class:`~sentence_transformers.losses.MultipleNegativesRankingLoss` is a powerful combination to learn sentence embeddings models. Read `NLI > MultipleNegativesRankingLoss <../nli/README.html#multiplenegativesrankingloss>`_ for more information on this loss function.\n```\n```\n\n----------------------------------------\n\nTITLE: TSDAE Paper Citation\nDESCRIPTION: BibTeX citation for the TSDAE research paper that introduces the Transformer-based Sequential Denoising Auto-Encoder methodology for unsupervised sentence embedding learning.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/unsupervised_learning/TSDAE/README.md#2025-04-08_snippet_1\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{wang-2021-TSDAE,\n    title = \"TSDAE: Using Transformer-based Sequential Denoising Auto-Encoderfor Unsupervised Sentence Embedding Learning\",\n    author = \"Wang, Kexin and Reimers, Nils and  Gurevych, Iryna\", \n    journal= \"arXiv preprint arXiv:2104.06979\",\n    month = \"4\",\n    year = \"2021\",\n    url = \"https://arxiv.org/abs/2104.06979\",\n}\n```\n\n----------------------------------------\n\nTITLE: Cross Encoder Training with MSE Loss\nDESCRIPTION: RST documentation explaining the usage of MSE Loss for knowledge distillation with a small & fast model to learn logit scores from a teacher ensemble.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/cross_encoder/training/distillation/README.md#2025-04-08_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\nIn this example, we use knowledge distillation with a small & fast model and learn the logits scores from the teacher ensemble. This yields performances comparable to large models, while being 18 times faster.\n\nIt uses the :class:`~sentence_transformers.cross_encoder.losses.MSELoss` to minimize the distance between predicted student logits and precomputed teacher logits for (query, answer) pairs.\n```\n\n----------------------------------------\n\nTITLE: Installing OpenVINO Backend\nDESCRIPTION: Command to install Sentence Transformers with OpenVINO backend support.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/usage/efficiency.rst#2025-04-08_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\npip install sentence-transformers[openvino]\n```\n\n----------------------------------------\n\nTITLE: OpenVINO Model Saving Locally\nDESCRIPTION: Example of saving an OpenVINO-exported model locally to avoid re-export.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/usage/efficiency.rst#2025-04-08_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nmodel = SentenceTransformer(\"path/to/my/model\", backend=\"openvino\")\nmodel.save_pretrained(\"path/to/my/model\")\n```\n\n----------------------------------------\n\nTITLE: Dimensionality Reduction using PCA in Python\nDESCRIPTION: This code snippet demonstrates how to reduce the embedding dimension of a SentenceTransformer model using Principal Component Analysis (PCA). It reduces the dimension from 768 to 128, significantly reducing storage requirements while maintaining performance.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/training/distillation/README.md#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndimensionality_reduction.py\n```\n\n----------------------------------------\n\nTITLE: Model Quantization for SentenceTransformers in Python\nDESCRIPTION: This code example shows how to apply quantization to SentenceTransformer models. Quantization can result in 40% smaller models and faster inference times on CPUs, with speedups ranging from 15% to 400% depending on the hardware.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/training/distillation/README.md#2025-04-08_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmodel_quantization.py\n```\n\n----------------------------------------\n\nTITLE: Documenting ParallelSentencesDataset in Python\nDESCRIPTION: This RST code block generates documentation for the ParallelSentencesDataset class using autoclass directive.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/sentence_transformer/datasets.md#2025-04-08_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: sentence_transformers.datasets.ParallelSentencesDataset\n```\n\n----------------------------------------\n\nTITLE: Documenting NoDuplicatesDataLoader in Python\nDESCRIPTION: This RST code block generates documentation for the NoDuplicatesDataLoader class using autoclass directive.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/sentence_transformer/datasets.md#2025-04-08_snippet_4\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: sentence_transformers.datasets.NoDuplicatesDataLoader\n```\n\n----------------------------------------\n\nTITLE: Default Batch Sampler Documentation\nDESCRIPTION: Auto-generated documentation for the DefaultBatchSampler class from sentence_transformers.sampler module.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/sentence_transformer/sampler.md#2025-04-08_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: sentence_transformers.sampler.DefaultBatchSampler\n    :members:\n```\n\n----------------------------------------\n\nTITLE: Group By Label Batch Sampler Documentation\nDESCRIPTION: Auto-generated documentation for the GroupByLabelBatchSampler class that groups samples by their labels.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/sentence_transformer/sampler.md#2025-04-08_snippet_3\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: sentence_transformers.sampler.GroupByLabelBatchSampler\n    :members:\n```\n\n----------------------------------------\n\nTITLE: Multi-Dataset Batch Samplers Documentation\nDESCRIPTION: Auto-generated documentation for the MultiDatasetBatchSamplers class for handling multiple datasets.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/sentence_transformer/sampler.md#2025-04-08_snippet_4\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: sentence_transformers.training_args.MultiDatasetBatchSamplers\n    :members:\n```\n\n----------------------------------------\n\nTITLE: Round Robin Batch Sampler Documentation\nDESCRIPTION: Auto-generated documentation for the RoundRobinBatchSampler class that samples from multiple datasets in a round-robin fashion.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/sentence_transformer/sampler.md#2025-04-08_snippet_5\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: sentence_transformers.sampler.RoundRobinBatchSampler\n    :members:\n```\n\n----------------------------------------\n\nTITLE: RST Note Block for Training Data Format Conversion\nDESCRIPTION: Documentation note explaining how to convert between different training data formats and use mine_hard_negatives utility for various output formats.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/cross_encoder/loss_overview.md#2025-04-08_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. note:: \n\n    You can often convert one training data format into another, allowing more loss functions to be viable for your scenario. For example, ``(sentence_A, sentence_B) pairs`` with ``class`` labels can be converted into ``(anchor, positive, negative) triplets`` by sampling sentences with the same or different classes.\n\n    Additionally, :func:`~sentence_transformers.util.mine_hard_negatives` can easily be used to turn ``(anchor, positive)`` to:\n\n    - ``(anchor, positive, negative) triplets`` with ``output_format=\\\"triplet\\\"``, \n    - ``(anchor, positive, negative_1, , negative_n) tuples`` with ``output_format=\\\"n-tuple\\\"``,\n    - ``(anchor, passage, label) labeled pairs`` with a label of 0 for negative and 1 for positive with ``output_format=\\\"labeled-pair\\\"``,\n    - ``(anchor, [doc1, doc2, ..., docN], [label1, label2, ..., labelN]) triplets`` with labels of 0 for negative and 1 for positive with ``output_format=\\\"labeled-list\\\"``, \n```\n\n----------------------------------------\n\nTITLE: Importing ListMLELoss in Python\nDESCRIPTION: This snippet shows how to import the ListMLELoss class from the sentence_transformers.cross_encoder.losses module.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/cross_encoder/losses.md#2025-04-08_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n.. autoclass:: sentence_transformers.cross_encoder.losses.ListMLELoss\n```\n\n----------------------------------------\n\nTITLE: Importing Helper Functions from sentence_transformers.util\nDESCRIPTION: Shows how to import various helper functions from the sentence_transformers.util module, including functions for paraphrase mining, semantic search, community detection, and working with embeddings.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/util.md#2025-04-08_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. automodule:: sentence_transformers.util\n   :members: paraphrase_mining, semantic_search, community_detection, http_get, truncate_embeddings, normalize_embeddings, is_training_available, mine_hard_negatives\n```\n\n----------------------------------------\n\nTITLE: Loss Overview Documentation Header\nDESCRIPTION: Markdown header indicating the start of loss functions documentation section.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/sentence_transformer/loss_overview.md#2025-04-08_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Loss Overview\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Note Block for Embedding Quantization\nDESCRIPTION: ReStructuredText note block explaining the difference between embedding quantization and model quantization, emphasizing that this documentation focuses on embedding quantization for faster semantic search and reduced memory usage.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/sentence_transformer/quantization.md#2025-04-08_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. note::\n   `Embedding Quantization <../../../examples/sentence_transformer/applications/embedding-quantization/README.html>`_ differs from model quantization. The former shrinks the size of embeddings such that semantic search/retrieval is faster and requires less memory and disk space. The latter refers to lowering the precision of the model weights to speed up inference. This page only shows documentation for the former.\n```\n\n----------------------------------------\n\nTITLE: Sample German Sentences Input\nDESCRIPTION: Example set of German sentences used to demonstrate the corresponding target language input format.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/sentence_transformer/applications/parallel-sentence-mining/README.md#2025-04-08_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nHallo Welt!\nDies ist ein Beispielsatz.\nDieser Satz taucht im Englischen nicht auf.\n```\n\n----------------------------------------\n\nTITLE: Citing Sentence-BERT in BibTeX Format\nDESCRIPTION: BibTeX citation for the original Sentence-BERT paper that introduces sentence embeddings using Siamese BERT-Networks.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/publications.md#2025-04-08_snippet_0\n\nLANGUAGE: bibtex\nCODE:\n```\n@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"http://arxiv.org/abs/1908.10084\",\n}\n```\n\n----------------------------------------\n\nTITLE: Citing Augmented SBERT in BibTeX Format\nDESCRIPTION: BibTeX citation for the paper on data augmentation methods for improving bi-encoders for pairwise sentence scoring tasks.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/publications.md#2025-04-08_snippet_2\n\nLANGUAGE: bibtex\nCODE:\n```\n@inproceedings{thakur-2020-AugSBERT,\n    title = \"Augmented {SBERT}: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks\",\n    author = \"Thakur, Nandan and Reimers, Nils and Daxenberger, Johannes  and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = \"6\",\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://arxiv.org/abs/2010.08240\",\n    pages = \"296--310\",\n}\n```\n\n----------------------------------------\n\nTITLE: Citing TSDAE in BibTeX Format\nDESCRIPTION: BibTeX citation for the paper on Transformer-based Sequential Denoising Auto-Encoder for unsupervised sentence embedding learning.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/publications.md#2025-04-08_snippet_4\n\nLANGUAGE: bibtex\nCODE:\n```\n@inproceedings{wang-2021-TSDAE,\n    title = \"TSDAE: Using Transformer-based Sequential Denoising Auto-Encoderfor Unsupervised Sentence Embedding Learning\",\n    author = \"Wang, Kexin and Reimers, Nils and Gurevych, Iryna\", \n    booktitle = \"Findings of the Association for Computational Linguistics: EMNLP 2021\",\n    month = nov,\n    year = \"2021\",\n    address = \"Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    pages = \"671--688\",\n    url = \"https://arxiv.org/abs/2104.06979\",\n}\n```\n\n----------------------------------------\n\nTITLE: Citing GPL in BibTeX Format\nDESCRIPTION: BibTeX citation for the paper on Generative Pseudo Labeling for unsupervised domain adaptation of dense retrieval.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/docs/publications.md#2025-04-08_snippet_6\n\nLANGUAGE: bibtex\nCODE:\n```\n@inproceedings{wang-2021-GPL,\n    title = \"GPL: Generative Pseudo Labeling for Unsupervised Domain Adaptation of Dense Retrieval\",\n    author = \"Wang, Kexin and Thakur, Nandan and Reimers, Nils and Gurevych, Iryna\", \n    journal= \"arXiv preprint arXiv:2112.07577\",\n    month = \"12\",\n    year = \"2021\",\n    url = \"https://arxiv.org/abs/2112.07577\",\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Sentence Transformers with pip\nDESCRIPTION: Command to install the latest version of sentence-transformers package using pip package manager.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/README.md#2025-04-08_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -U sentence-transformers\n```\n\n----------------------------------------\n\nTITLE: Installing Sentence Transformers from source\nDESCRIPTION: Command to install sentence-transformers directly from the source code in development mode.\nSOURCE: https://github.com/UKPLab/sentence-transformers/blob/master/README.md#2025-04-08_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install -e .\n```"
  }
]
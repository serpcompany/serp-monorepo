[
  {
    "owner": "unit8co",
    "repo": "darts",
    "content": "TITLE: Hyperparameter Optimization for NBEATSModel using Ray Tune in Python\nDESCRIPTION: This snippet shows how to use Ray Tune for hyperparameter optimization of an NBEATSModel in Darts. It includes data preprocessing, model configuration, and evaluation using MAPE. The optimization process uses the Asynchronous Hyperband scheduler and PyTorch Lightning callbacks.\nSOURCE: https://github.com/unit8co/darts/blob/master/docs/userguide/hyperparameter_optimization.md#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport pandas as pd\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import EarlyStopping\nfrom ray import tune\nfrom ray.train import RunConfig\nfrom ray.tune import CLIReporter\nfrom ray.tune.integration.pytorch_lightning import TuneReportCheckpointCallback\nfrom ray.tune.schedulers import ASHAScheduler\nfrom ray.tune.tuner import Tuner\nfrom torchmetrics import (\n    MeanAbsoluteError,\n    MeanAbsolutePercentageError,\n    MetricCollection,\n)\n\nfrom darts.dataprocessing.transformers import Scaler\nfrom darts.datasets import AirPassengersDataset\nfrom darts.models import NBEATSModel\n\n\ndef train_model(model_args, callbacks, train, val):\n    torch_metrics = MetricCollection(\n        [MeanAbsolutePercentageError(), MeanAbsoluteError()]\n    )\n    # Create the model using model_args from Ray Tune\n    model = NBEATSModel(\n        input_chunk_length=24,\n        output_chunk_length=12,\n        n_epochs=100,\n        torch_metrics=torch_metrics,\n        pl_trainer_kwargs={\"callbacks\": callbacks, \"enable_progress_bar\": False},\n        **model_args,\n    )\n\n    model.fit(\n        series=train,\n        val_series=val,\n    )\n\n\n# Read data:\nseries = AirPassengersDataset().load().astype(np.float32)\n\n# Create training and validation sets:\ntrain, val = series.split_after(pd.Timestamp(year=1957, month=12, day=1))\n\n# Normalize the time series (note: we avoid fitting the transformer on the validation set)\ntransformer = Scaler()\ntransformer.fit(train)\ntrain = transformer.transform(train)\nval = transformer.transform(val)\n\n# Early stop callback\nmy_stopper = EarlyStopping(\n    monitor=\"val_MeanAbsolutePercentageError\",\n    patience=5,\n    min_delta=0.05,\n    mode=\"min\",\n)\n\n\n# set up ray tune callback\nclass TuneReportCallback(TuneReportCheckpointCallback, pl.Callback):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n\ntune_callback = TuneReportCallback(\n    {\n        \"loss\": \"val_loss\",\n        \"MAPE\": \"val_MeanAbsolutePercentageError\",\n    },\n    on=\"validation_end\",\n)\n\n# Define the trainable function that will be tuned by Ray Tune\ntrain_fn_with_parameters = tune.with_parameters(\n    train_model,\n    callbacks=[tune_callback, my_stopper],\n    train=train,\n    val=val,\n)\n\n# Set the resources to be used for each trial (disable GPU, if you don't have one)\nresources_per_trial = {\"cpu\": 8, \"gpu\": 1}\n\n# define the hyperparameter space\nconfig = {\n    \"batch_size\": tune.choice([16, 32, 64, 128]),\n    \"num_blocks\": tune.choice([1, 2, 3, 4, 5]),\n    \"num_stacks\": tune.choice([32, 64, 128]),\n    \"dropout\": tune.uniform(0, 0.2),\n}\n\n# the number of combinations to try\nnum_samples = 10\n\n# Configure the ASHA scheduler\nscheduler = ASHAScheduler(max_t=1000, grace_period=3, reduction_factor=2)\n\n# Configure the CLI reporter to display the progress\nreporter = CLIReporter(\n    parameter_columns=list(config.keys()),\n    metric_columns=[\"loss\", \"MAPE\", \"training_iteration\"],\n)\n\n# Create the Tuner object and run the hyperparameter search\ntuner = Tuner(\n    trainable=tune.with_resources(\n        train_fn_with_parameters, resources=resources_per_trial\n    ),\n    param_space=config,\n    tune_config=tune.TuneConfig(\n        metric=\"MAPE\", mode=\"min\", num_samples=num_samples, scheduler=scheduler\n    ),\n    run_config=RunConfig(name=\"tune_darts\", progress_reporter=reporter),\n)\nresults = tuner.fit()\n\n# Print the best hyperparameters found\nprint(\"Best hyperparameters found were: \", results.get_best_result().config)\n```\n\n----------------------------------------\n\nTITLE: Basic Forecasting Model Usage in Python\nDESCRIPTION: Demonstrates the basic workflow of initializing, fitting, and making predictions with a NaiveSeasonal model. Shows the three main steps of model usage: initialization with parameters, fitting to training data, and generating predictions.\nSOURCE: https://github.com/unit8co/darts/blob/master/docs/userguide/forecasting_overview.md#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom darts.models import NaiveSeasonal\n\nnaive_model = NaiveSeasonal(K=1)            # init\nnaive_model.fit(train)                      # fit\nnaive_forecast = naive_model.predict(n=36)  # predict\n```\n\n----------------------------------------\n\nTITLE: Training Darts Models on GPU and Loading on CPU\nDESCRIPTION: This example demonstrates how to train a model using GPU acceleration, save checkpoints, and then load the model onto CPU for inference.\nSOURCE: https://github.com/unit8co/darts/blob/master/docs/userguide/torch_forecasting_models.md#2025-04-17_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# define a model using gpu as accelerator\nmodel = SomeTorchForecastingModel(...,\n                                  model_name='my_model',\n                                  save_checkpoints=True,\n                                  pl_trainer_kwargs={\n                                                     \"accelerator\":\"gpu\",\n                                                     \"devices\": -1,\n                                                     })\n\n# train the model, automatic checkpoints will be created\nmodel.fit(...)\n\n# specify the device to which the model should be loaded\nloaded_model = SomeTorchForecastingModel.load_from_checkpoint(model_name='my_model',\n                                                              best=True,\n                                                              map_location=\"cpu\")\nloaded_model.to_cpu()\n\n# run inference\nloaded_model.predict(...)\n```\n\n----------------------------------------\n\nTITLE: Initializing and Training a TFT Model with Covariates in Darts\nDESCRIPTION: This snippet demonstrates how to create and train a Temporal Fusion Transformer (TFT) model with past and future covariates for predicting ice cream sales based on temperature and weekday data.\nSOURCE: https://github.com/unit8co/darts/blob/master/docs/userguide/torch_forecasting_models.md#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom darts.models import TFTModel\n\nmodel = TFTModel(input_chunk_length=7, output_chunk_length=1)\nmodel.fit(series=ice_cream_sales,\n          past_covariates=temperature,\n          future_covariates=weekday)\n```\n\n----------------------------------------\n\nTITLE: Plotting Time Series Forecasts\nDESCRIPTION: Demonstrates how to plot the original series and forecasted values with confidence intervals.\nSOURCE: https://github.com/unit8co/darts/blob/master/README.md#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\n\nseries.plot()\nprediction.plot(label=\"forecast\", low_quantile=0.05, high_quantile=0.95)\nplt.legend()\n```\n\n----------------------------------------\n\nTITLE: Generating and Evaluating Forecasts from Different Models\nDESCRIPTION: Generates predictions for each model on the test dataset, plots the forecasts against ground truth, and calculates performance metrics (MAE and MSE) for comparison.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/18-TiDE-examples.ipynb#2025-04-17_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# we will predict the next `pred_steps` points after the end of `pred_input`\npred_steps = common_model_args[\"output_chunk_length\"] * 2\npred_input = test[:-pred_steps]\n\nfig, ax = plt.subplots(figsize=(15, 5))\npred_input.plot(label=\"input\")\ntest[-pred_steps:].plot(label=\"ground truth\", ax=ax)\n\nresult_accumulator = {}\n# predict with each model and compute/store the metrics against the test sets\nfor model_name, model in models.items():\n    pred_series = model.predict(n=pred_steps, series=pred_input)\n    pred_series.plot(label=model_name, ax=ax)\n\n    result_accumulator[model_name] = {\n        \"mae\": mae(test, pred_series),\n        \"mse\": mse(test, pred_series),\n    }\n```\n\n----------------------------------------\n\nTITLE: Forecasting with a Trained Torch Forecasting Model in Python\nDESCRIPTION: Example of generating predictions with a trained Torch Forecasting Model. The model predicts the next 3 time steps for a given input series with optional covariates.\nSOURCE: https://github.com/unit8co/darts/blob/master/docs/userguide/torch_forecasting_models.md#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# predict the next n=3 time steps for any input series with `series`\nprediction = model.predict(n=3,\n                           series=target,\n                           past_covariates=past_covariates,\n                           future_covariates=future_covariates)\n```\n\n----------------------------------------\n\nTITLE: Training and Predicting with Covariates in Darts\nDESCRIPTION: Example showing how to fit a Darts forecasting model with past and future covariates and make predictions. Demonstrates basic model initialization, fitting with covariates, and prediction with the required parameters.\nSOURCE: https://github.com/unit8co/darts/blob/master/docs/userguide/covariates.md#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# create one of Darts' forecasting models\nmodel = SomeForecastingModel(...)\n\n# fitting model with past and future covariates\nmodel.fit(target=target,\n          past_covariates=past_covariates_train,\n          future_covariates=future_covariates_train)\n\n# predict the next n=12 steps\nmodel.predict(n=12,\n              series=target,  # only required for Global Forecasting Models\n              past_covariates=past_covariates_pred,\n              future_covariates=future_covariates_pred)\n```\n\n----------------------------------------\n\nTITLE: Exponential Smoothing Prediction\nDESCRIPTION: Shows how to fit an exponential smoothing model and make probabilistic predictions using the Darts library.\nSOURCE: https://github.com/unit8co/darts/blob/master/README.md#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom darts.models import ExponentialSmoothing\n\nmodel = ExponentialSmoothing()\nmodel.fit(train)\nprediction = model.predict(len(val), num_samples=1000)\n```\n\n----------------------------------------\n\nTITLE: Configuring Common Model Parameters for Darts TorchForecastingModels\nDESCRIPTION: Sets up common configuration parameters for Darts TorchForecastingModels, including optimizer settings, PyTorch Lightning trainer arguments, learning rate scheduler, early stopping criteria, and model-specific parameters like input/output chunk lengths and batch size.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/18-TiDE-examples.ipynb#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\noptimizer_kwargs = {\n    \"lr\": 1e-3,\n}\n\n# PyTorch Lightning Trainer arguments\npl_trainer_kwargs = {\n    \"gradient_clip_val\": 1,\n    \"max_epochs\": 200,\n    \"accelerator\": \"auto\",\n    \"callbacks\": [],\n}\n\n# learning rate scheduler\nlr_scheduler_cls = torch.optim.lr_scheduler.ExponentialLR\nlr_scheduler_kwargs = {\n    \"gamma\": 0.999,\n}\n\n# early stopping (needs to be reset for each model later on)\n# this setting stops training once the the validation loss has not decreased by more than 1e-3 for 10 epochs\nearly_stopping_args = {\n    \"monitor\": \"val_loss\",\n    \"patience\": 10,\n    \"min_delta\": 1e-3,\n    \"mode\": \"min\",\n}\n\n#\ncommon_model_args = {\n    \"input_chunk_length\": 12,  # lookback window\n    \"output_chunk_length\": 12,  # forecast/lookahead window\n    \"optimizer_kwargs\": optimizer_kwargs,\n    \"pl_trainer_kwargs\": pl_trainer_kwargs,\n    \"lr_scheduler_cls\": lr_scheduler_cls,\n    \"lr_scheduler_kwargs\": lr_scheduler_kwargs,\n    \"likelihood\": None,  # use a likelihood for probabilistic forecasts\n    \"save_checkpoints\": True,  # checkpoint to retrieve the best performing model state,\n    \"force_reset\": True,\n    \"batch_size\": 256,\n    \"random_state\": 42,\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring N-BEATS Interpretable Architecture Model\nDESCRIPTION: Creates an N-BEATS model with interpretable architecture that separates trend and seasonality components. This configuration enables decomposition of forecasts into interpretable trend and seasonal components.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/07-NBEATS-examples.ipynb#2025-04-17_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nmodel_name = \"nbeats_interpretable_run\"\nmodel_nbeats = NBEATSModel(\n    input_chunk_length=30,\n    output_chunk_length=7,\n    generic_architecture=False,\n    num_blocks=3,\n    num_layers=4,\n    layer_widths=512,\n    n_epochs=100,\n    nr_epochs_val_period=1,\n    batch_size=800,\n    random_state=42,\n    model_name=model_name,\n    save_checkpoints=True,\n    force_reset=True,\n    **generate_torch_kwargs(),\n)\n```\n\n----------------------------------------\n\nTITLE: Creating and Training LSTM RNN Model for Air Passengers Data in Python\nDESCRIPTION: This snippet creates an LSTM RNN model using Darts and trains it on the Air Passengers dataset. It demonstrates how to configure model parameters and fit the model with covariates.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/04-RNN-examples.ipynb#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmy_model = RNNModel(\n    model=\"LSTM\",\n    hidden_dim=20,\n    dropout=0,\n    batch_size=16,\n    n_epochs=300,\n    optimizer_kwargs={\"lr\": 1e-3},\n    model_name=\"Air_RNN\",\n    log_tensorboard=True,\n    random_state=42,\n    training_length=20,\n    input_chunk_length=14,\n    force_reset=True,\n    save_checkpoints=True,\n)\n\nmy_model.fit(\n    train_transformed,\n    future_covariates=covariates,\n    val_series=val_transformed,\n    val_future_covariates=covariates,\n    verbose=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Making Predictions with Darts Models\nDESCRIPTION: This code shows how to generate forecasts using a trained model, specifying the forecast horizon (n), historical data, and both past and future covariates.\nSOURCE: https://github.com/unit8co/darts/blob/master/docs/userguide/torch_forecasting_models.md#2025-04-17_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nprediction = model.predict(n=n,\n                           series=ice_cream_sales_train,\n                           past_covariates=temperature,\n                           future_covariates=weekday)\n```\n\n----------------------------------------\n\nTITLE: Training Time Series Forecasting Models with Early Stopping\nDESCRIPTION: Trains each model (NHiTS, TiDE, and TiDE+RIN) on the training data with validation-based early stopping, then loads the best checkpoint for each model based on validation performance.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/18-TiDE-examples.ipynb#2025-04-17_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# train the models and load the model from its best state/checkpoint\nfor name, model in models.items():\n    # early stopping needs to get reset for each model\n    pl_trainer_kwargs[\"callbacks\"] = [\n        EarlyStopping(\n            **early_stopping_args,\n        )\n    ]\n\n    model.fit(\n        series=train,\n        val_series=val,\n        verbose=False,\n    )\n    # load from checkpoint returns a new model object, we store it in the models dict\n    models[name] = model.load_from_checkpoint(model_name=model.model_name, best=True)\n```\n\n----------------------------------------\n\nTITLE: Generating Probabilistic Forecasts with Exponential Smoothing in Darts\nDESCRIPTION: This code demonstrates how to create probabilistic forecasts using the Exponential Smoothing model in Darts. It loads the AirPassengers dataset, splits it into training and validation sets, fits the model, and produces forecasts with 500 Monte Carlo samples to represent the probability distribution.\nSOURCE: https://github.com/unit8co/darts/blob/master/docs/userguide/forecasting_overview.md#2025-04-17_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom darts.datasets import AirPassengersDataset\nfrom darts import TimeSeries\nfrom darts.models import ExponentialSmoothing\n\nseries = AirPassengersDataset().load()\ntrain, val = series[:-36], series[-36:]\n\nmodel = ExponentialSmoothing()\nmodel.fit(train)\npred = model.predict(n=36, num_samples=500)\n\nseries.plot()\npred.plot(label='forecast')\n```\n\n----------------------------------------\n\nTITLE: Generating Historical Forecasts with N-BEATS Generic Model\nDESCRIPTION: Creates historical forecasts using the trained N-BEATS model with a 7-day forecasting horizon. The forecasts are generated using an expanding window approach without retraining and then concatenated for evaluation.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/07-NBEATS-examples.ipynb#2025-04-17_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npred_series = model_nbeats.historical_forecasts(\n    series_scaled,\n    start=val.start_time(),\n    forecast_horizon=7,\n    stride=7,\n    last_points_only=False,\n    retrain=False,\n    verbose=True,\n)\npred_series = concatenate(pred_series)\n```\n\n----------------------------------------\n\nTITLE: Training a Darts Model with a Validation Dataset\nDESCRIPTION: This code demonstrates how to split data into training and validation sets, then train a model using both datasets to monitor validation performance during training.\nSOURCE: https://github.com/unit8co/darts/blob/master/docs/userguide/torch_forecasting_models.md#2025-04-17_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# create train and validation sets\nice_cream_sales_train, ice_cream_sales_val = ice_cream_sales.split_after(training_cutoff)\n\n# train with validation set\nmodel.fit(series=ice_cream_sales_train,\n          past_covariates=temperature,\n          future_covariates=weekday,\n          val_series=ice_cream_sales_val,\n          val_past_covariates=temperature,\n          val_future_covariates=weekday)\n```\n\n----------------------------------------\n\nTITLE: Creating and Using a Pipeline for Multiple Transformations\nDESCRIPTION: Demonstrates how to chain multiple transformers (daily averaging and scaling) using a Pipeline, then applies the combined transformation to the training data.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/02-data-processing.ipynb#2025-04-17_snippet_14\n\nLANGUAGE: python\nCODE:\n```\npipeline = Pipeline([toDailyAverage, scaler])\ntransformed = pipeline.fit_transform(training)\ntransformed.plot()\n```\n\n----------------------------------------\n\nTITLE: Training N-BEATS Generic Model on Energy Generation Data\nDESCRIPTION: Trains the configured N-BEATS generic model on the scaled training data, with validation performed on the validation set. This fits the model to the time series data for energy generation forecasting.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/07-NBEATS-examples.ipynb#2025-04-17_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmodel_nbeats.fit(train_scaled, val_series=val_scaled)\n```\n\n----------------------------------------\n\nTITLE: Evaluating RNN and ETS Models on Sunspots Data in Python\nDESCRIPTION: This code compares the performance of the BlockRNN model and an Exponential Smoothing model on the Monthly Sunspots dataset. It generates historical forecasts and calculates MAPE for both models.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/04-RNN-examples.ipynb#2025-04-17_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Compute the backtest predictions with the two models\npred_series = my_model_sun.historical_forecasts(\n    series_sp_transformed,\n    start=pd.Timestamp(\"19401001\"),\n    forecast_horizon=36,\n    stride=10,\n    retrain=False,\n    last_points_only=True,\n    verbose=True,\n)\n\npred_series_ets = ExponentialSmoothing(seasonal_periods=120).historical_forecasts(\n    series_sp_transformed,\n    start=pd.Timestamp(\"19401001\"),\n    forecast_horizon=36,\n    stride=10,\n    retrain=True,\n    last_points_only=True,\n    verbose=True,\n)\n\nval_sp_transformed.plot(label=\"actual\")\npred_series.plot(label=\"our RNN\")\npred_series_ets.plot(label=\"ETS\")\nplt.legend()\nprint(\"RNN MAPE:\", mape(pred_series, val_sp_transformed))\nprint(\"ETS MAPE:\", mape(pred_series_ets, val_sp_transformed))\n```\n\n----------------------------------------\n\nTITLE: Training a Torch Forecasting Model with Optional Covariates in Python\nDESCRIPTION: Example showing how to fit a Torch Forecasting Model on single or multiple target series with optional past and future covariates. Includes validation set usage for training evaluation.\nSOURCE: https://github.com/unit8co/darts/blob/master/docs/userguide/torch_forecasting_models.md#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# fit the model on a single target series with optional past and / or future covariates\nmodel.fit(target,\n          past_covariates=past_covariates,\n          future_covariates=future_covariates,\n          val_series=target_val,  # optionally, use a validation set\n          val_past_covariates=past_covariates_val,\n          val_future_covariates=future_covariates_val)\n\n# fit the model on multiple target series\nmodel.fit([target, target2, ...],\n          past_covariates=[past_covariates, past_covariates2, ...],\n          ...)\n\n```\n\n----------------------------------------\n\nTITLE: Creating and Training TCN Model for Air Passengers Dataset\nDESCRIPTION: This snippet creates a TCN model with specific hyperparameters for the Air Passengers dataset, fits the model to the training data, and loads the best checkpoint based on validation performance.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/05-TCN-examples.ipynb#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmodel_name = \"TCN_air\"\nmodel_air = TCNModel(\n    input_chunk_length=13,\n    output_chunk_length=12,\n    n_epochs=500,\n    dropout=0.1,\n    dilation_base=2,\n    weight_norm=True,\n    kernel_size=5,\n    num_filters=3,\n    random_state=0,\n    save_checkpoints=True,\n    model_name=model_name,\n    force_reset=True,\n    **generate_torch_kwargs(),\n)\n\nmodel_air.fit(\n    series=train_scaled,\n    past_covariates=month_series,\n    val_series=val_scaled,\n    val_past_covariates=month_series,\n)\n\nmodel_air = TCNModel.load_from_checkpoint(model_name=model_name, best=True)\n```\n\n----------------------------------------\n\nTITLE: Scaling Time Series Data with Scaler\nDESCRIPTION: Demonstrates how to use the Scaler transformer to rescale a time series between 0 and 1, which is useful for preparing data for neural network models. The example also shows how to inverse the transformation.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/02-data-processing.ipynb#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nscaler = Scaler()\nrescaled = scaler.fit_transform(series)\nprint(rescaled)\n```\n\n----------------------------------------\n\nTITLE: Creating and Training TCN Model for Daily Energy Production Dataset\nDESCRIPTION: This snippet creates a TCN model with specific hyperparameters for the Daily Energy Production dataset, fits the model to the training data, and loads the best checkpoint based on validation performance.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/05-TCN-examples.ipynb#2025-04-17_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nmodel_name = \"TCN_energy\"\nmodel_en = TCNModel(\n    input_chunk_length=365,\n    output_chunk_length=7,\n    n_epochs=50,\n    dropout=0.2,\n    dilation_base=2,\n    weight_norm=True,\n    kernel_size=5,\n    num_filters=8,\n    nr_epochs_val_period=1,\n    random_state=0,\n    save_checkpoints=True,\n    model_name=model_name,\n    force_reset=True,\n    **generate_torch_kwargs(),\n)\n\nmodel_en.fit(\n    series=train_en_transformed,\n    past_covariates=day_series,\n    val_series=val_en_transformed,\n    val_past_covariates=day_series,\n)\n\nmodel_en = TCNModel.load_from_checkpoint(model_name=model_name, best=True)\n```\n\n----------------------------------------\n\nTITLE: Training a Darts Forecasting Model\nDESCRIPTION: Fits the RNN model to the training data, with validation data provided to monitor model performance during training.\nSOURCE: https://github.com/unit8co/darts/blob/master/docs/userguide/gpu_and_tpu_usage.md#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmy_model.fit(train_transformed, val_series=val_transformed)\n```\n\n----------------------------------------\n\nTITLE: Hyperparameter Optimization for TCNModel using Optuna in Python\nDESCRIPTION: This snippet demonstrates how to use Optuna for hyperparameter optimization of a TCNModel in Darts. It includes data preprocessing, model configuration, and evaluation using sMAPE. The optimization process uses PyTorch Lightning callbacks for pruning and early stopping.\nSOURCE: https://github.com/unit8co/darts/blob/master/docs/userguide/hyperparameter_optimization.md#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport optuna\nimport torch\nfrom optuna.integration import PyTorchLightningPruningCallback\nfrom pytorch_lightning.callbacks import Callback, EarlyStopping\nfrom sklearn.preprocessing import MaxAbsScaler\n\nfrom darts.dataprocessing.transformers import Scaler\nfrom darts.datasets import AirPassengersDataset\nfrom darts.metrics import smape\nfrom darts.models import TCNModel\nfrom darts.utils.likelihood_models.torch import GaussianLikelihood\n\n# load data\nseries = AirPassengersDataset().load().astype(np.float32)\n\n# split in train / validation (note: in practice we would also need a test set)\nVAL_LEN = 36\ntrain, val = series[:-VAL_LEN], series[-VAL_LEN:]\n\n# scale\nscaler = Scaler(MaxAbsScaler())\ntrain = scaler.fit_transform(train)\nval = scaler.transform(val)\n\n\n#  workaround found in https://github.com/Lightning-AI/pytorch-lightning/issues/17485\n# to avoid import of both lightning and pytorch_lightning\nclass PatchedPruningCallback(optuna.integration.PyTorchLightningPruningCallback, Callback):\n    pass\n\n\n# define objective function\ndef objective(trial):\n    # select input and output chunk lengths\n    in_len = trial.suggest_int(\"in_len\", 12, 36)\n    out_len = trial.suggest_int(\"out_len\", 1, in_len - 1)\n\n    # Other hyperparameters\n    kernel_size = trial.suggest_int(\"kernel_size\", 2, 5)\n    num_filters = trial.suggest_int(\"num_filters\", 1, 5)\n    weight_norm = trial.suggest_categorical(\"weight_norm\", [False, True])\n    dilation_base = trial.suggest_int(\"dilation_base\", 2, 4)\n    dropout = trial.suggest_float(\"dropout\", 0.0, 0.4)\n    lr = trial.suggest_float(\"lr\", 5e-5, 1e-3, log=True)\n    include_year = trial.suggest_categorical(\"year\", [False, True])\n\n    # throughout training we'll monitor the validation loss for both pruning and early stopping\n    pruner = PatchedPruningCallback(trial, monitor=\"val_loss\")\n    early_stopper = EarlyStopping(\"val_loss\", min_delta=0.001, patience=3, verbose=True)\n    callbacks = [pruner, early_stopper]\n\n    # detect if a GPU is available\n    if torch.cuda.is_available():\n        num_workers = 4\n    else:\n        num_workers = 0\n\n    pl_trainer_kwargs = {\n        \"accelerator\": \"auto\",\n        \"callbacks\": callbacks,\n    }\n\n    # optionally also add the (scaled) year value as a past covariate\n    if include_year:\n        encoders = {\"datetime_attribute\": {\"past\": [\"year\"]},\n                    \"transformer\": Scaler()}\n    else:\n        encoders = None\n\n    # reproducibility\n    torch.manual_seed(42)\n\n    # build the TCN model\n    model = TCNModel(\n        input_chunk_length=in_len,\n        output_chunk_length=out_len,\n        batch_size=32,\n        n_epochs=100,\n        nr_epochs_val_period=1,\n        kernel_size=kernel_size,\n        num_filters=num_filters,\n        weight_norm=weight_norm,\n        dilation_base=dilation_base,\n        dropout=dropout,\n        optimizer_kwargs={\"lr\": lr},\n        add_encoders=encoders,\n        likelihood=GaussianLikelihood(),\n        pl_trainer_kwargs=pl_trainer_kwargs,\n        model_name=\"tcn_model\",\n        force_reset=True,\n        save_checkpoints=True,\n    )\n\n    # when validating during training, we can use a slightly longer validation\n    # set which also contains the first input_chunk_length time steps\n    model_val_set = scaler.transform(series[-(VAL_LEN + in_len):])\n\n    # train the model\n    model.fit(\n        series=train,\n        val_series=model_val_set,\n    )\n\n    # reload best model over course of training\n    model = TCNModel.load_from_checkpoint(\"tcn_model\")\n\n    # Evaluate how good it is on the validation set, using sMAPE\n    preds = model.predict(series=train, n=VAL_LEN)\n    smapes = smape(val, preds, n_jobs=-1, verbose=True)\n    smape_val = np.mean(smapes)\n\n    return smape_val if smape_val != np.nan else float(\"inf\")\n\n\n# for convenience, print some optimization trials information\ndef print_callback(study, trial):\n    print(f\"Current value: {trial.value}, Current params: {trial.params}\")\n    print(f\"Best value: {study.best_value}, Best params: {study.best_trial.params}\")\n\n\n# optimize hyperparameters by minimizing the sMAPE on the validation set\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"minimize\")\n    study.optimize(objective, n_trials=100, callbacks=[print_callback])\n```\n\n----------------------------------------\n\nTITLE: Quantile Regression with TCN Neural Network in Darts\nDESCRIPTION: This snippet demonstrates how to perform quantile regression with a TCN model using the pinball loss. The model is configured to estimate multiple quantiles (0.01, 0.05, 0.2, 0.5, 0.8, 0.95, 0.99) to create a non-parametric distribution. The data is preprocessed with scaling, and the model generates 500 forecast samples.\nSOURCE: https://github.com/unit8co/darts/blob/master/docs/userguide/forecasting_overview.md#2025-04-17_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom darts.datasets import AirPassengersDataset\nfrom darts import TimeSeries\nfrom darts.models import TCNModel\nfrom darts.dataprocessing.transformers import Scaler\nfrom darts.utils.likelihood_models.torch import QuantileRegression\n\nseries = AirPassengersDataset().load()\ntrain, val = series[:-36], series[-36:]\n\nscaler = Scaler()\ntrain = scaler.fit_transform(train)\nval = scaler.transform(val)\nseries = scaler.transform(series)\n\nmodel = TCNModel(input_chunk_length=30,\n                 output_chunk_length=12,\n                 likelihood=QuantileRegression(quantiles=[0.01, 0.05, 0.2, 0.5, 0.8, 0.95, 0.99]))\nmodel.fit(train, epochs=400)\npred = model.predict(n=36, num_samples=500)\n\nseries.plot()\npred.plot(label='forecast')\n```\n\n----------------------------------------\n\nTITLE: Implementing FFT Model with Timestamp Matching\nDESCRIPTION: Creates an improved FFT model that automatically detects and matches timestamp attributes between training and prediction periods, ensuring proper seasonal alignment.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/03-FFT-examples.ipynb#2025-04-17_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmodel = FFT(nr_freqs_to_keep=None)\nmodel.fit(train)\npred_val = model.predict(len(val))\n```\n\n----------------------------------------\n\nTITLE: Configuring N-BEATS Generic Architecture Model for Time Series Forecasting\nDESCRIPTION: Creates an N-BEATS model with generic architecture, which uses minimal prior knowledge about time series. The model is configured with specific hyperparameters for input/output length, network architecture, training parameters, and checkpoint saving.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/07-NBEATS-examples.ipynb#2025-04-17_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmodel_name = \"nbeats_run\"\nmodel_nbeats = NBEATSModel(\n    input_chunk_length=30,\n    output_chunk_length=7,\n    generic_architecture=True,\n    num_stacks=10,\n    num_blocks=1,\n    num_layers=4,\n    layer_widths=512,\n    n_epochs=100,\n    nr_epochs_val_period=1,\n    batch_size=800,\n    random_state=42,\n    model_name=model_name,\n    save_checkpoints=True,\n    force_reset=True,\n    **generate_torch_kwargs(),\n)\n```\n\n----------------------------------------\n\nTITLE: Loading and Preprocessing Energy Generation Dataset for Time Series Forecasting\nDESCRIPTION: Loads the energy generation dataset, processes it to daily averages, fills missing values, and scales the data. The data is split into training and validation sets for model training and evaluation.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/07-NBEATS-examples.ipynb#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndf = EnergyDataset().load().to_dataframe()\ndf[\"generation hydro run-of-river and poundage\"].plot()\nplt.title(\"Hourly generation hydro run-of-river and poundage\")\n```\n\n----------------------------------------\n\nTITLE: Preparing Air Passengers Dataset for RNN Model in Python\nDESCRIPTION: This code prepares the Air Passengers dataset for use with an RNN model. It splits the data into training and validation sets, normalizes the time series, and creates month and year covariates.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/04-RNN-examples.ipynb#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Read data:\nseries = AirPassengersDataset().load()\n\n# Create training and validation sets:\ntrain, val = series.split_after(pd.Timestamp(\"19590101\"))\n\n# Normalize the time series (note: we avoid fitting the transformer on the validation set)\ntransformer = Scaler()\ntrain_transformed = transformer.fit_transform(train)\nval_transformed = transformer.transform(val)\nseries_transformed = transformer.transform(series)\n\n# create month and year covariate series\nyear_series = datetime_attribute_timeseries(\n    pd.date_range(start=series.start_time(), freq=series.freq_str, periods=1000),\n    attribute=\"year\",\n    one_hot=False,\n)\nyear_series = Scaler().fit_transform(year_series)\nmonth_series = datetime_attribute_timeseries(\n    year_series, attribute=\"month\", one_hot=True\n)\ncovariates = year_series.stack(month_series)\ncov_train, cov_val = covariates.split_after(pd.Timestamp(\"19590101\"))\n```\n\n----------------------------------------\n\nTITLE: Creating and Training TCN Model for Monthly Sunspots Dataset\nDESCRIPTION: This snippet creates a TCN model with specific hyperparameters for the Monthly Sunspots dataset, fits the model to the training data, and loads the best checkpoint based on validation performance.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/05-TCN-examples.ipynb#2025-04-17_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmodel_name = \"TCN_sun\"\nmodel_sun = TCNModel(\n    input_chunk_length=250,\n    output_chunk_length=36,\n    n_epochs=100,\n    dropout=0,\n    dilation_base=2,\n    weight_norm=True,\n    kernel_size=3,\n    num_filters=6,\n    nr_epochs_val_period=1,\n    random_state=0,\n    save_checkpoints=True,\n    model_name=model_name,\n    force_reset=True,\n    **generate_torch_kwargs(),\n)\n\nmodel_sun.fit(train_sp_transformed, val_series=val_sp_transformed)\n\nmodel_sun = TCNModel.load_from_checkpoint(model_name=model_name, best=True)\n```\n\n----------------------------------------\n\nTITLE: Plotting Historical Forecasts of DeepTCN Model for Energy Production in Python\nDESCRIPTION: This snippet visualizes the historical forecasts generated by the DeepTCN model for energy production. It plots the actual values along with different quantile ranges of the predictions.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/09-DeepTCN-examples.ipynb#2025-04-17_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nplt.figure(figsize=(10, 6))\nval_en_transformed.plot(label=\"actual\")\nbacktest_en.plot(label=\"backtest q0.05 - q0.95\", low_quantile=0.05, high_quantile=0.95)\nbacktest_en.plot(label=\"backtest q0.25 - q0.75\", low_quantile=0.25, high_quantile=0.75)\nplt.legend()\n```\n\n----------------------------------------\n\nTITLE: Creating and Training DeepTCN Model with Variable Noise Series in Python\nDESCRIPTION: This snippet defines a TCNModel with specific hyperparameters and trains it on the variable noise series data. It uses a Gaussian likelihood function for probabilistic forecasting.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/09-DeepTCN-examples.ipynb#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndeeptcn = TCNModel(\n    input_chunk_length=30,\n    output_chunk_length=20,\n    kernel_size=2,\n    num_filters=4,\n    dilation_base=2,\n    dropout=0,\n    random_state=0,\n    likelihood=GaussianLikelihood(),\n    **generate_torch_kwargs(),\n)\n\ndeeptcn.fit(target_train, past_covariates=covariates)\n```\n\n----------------------------------------\n\nTITLE: Evaluating TCN Model on Air Passengers Dataset\nDESCRIPTION: This snippet performs historical forecasts using the trained TCN model on the Air Passengers dataset and plots the results against the actual data.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/05-TCN-examples.ipynb#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nbacktest = model_air.historical_forecasts(\n    series=ts_scaled,\n    past_covariates=month_series,\n    start=val_scaled.start_time(),\n    forecast_horizon=6,\n    retrain=False,\n    verbose=True,\n)\n\nts_scaled.plot(label=\"actual\")\nbacktest.plot(label=\"backtest (H=6)\")\n```\n\n----------------------------------------\n\nTITLE: Multiple Series Training with NBEATS Model\nDESCRIPTION: Shows how to train an NBEATS model on multiple time series and generate predictions for different series. Demonstrates the model's ability to handle multiple input and output series.\nSOURCE: https://github.com/unit8co/darts/blob/master/docs/userguide/forecasting_overview.md#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom darts.models import NBEATSModel\n\nmodel = NBEATSModel(input_chunk_length=24,                 # init\n                    output_chunk_length=12)\n\nmodel.fit([series1, series2])                              # fit on two series\nforecast = model.predict(series=[series3, series4], n=36)  # predict potentially different series\n```\n\n----------------------------------------\n\nTITLE: Filling Missing Values with Quadratic Interpolation\nDESCRIPTION: Uses MissingValuesFiller to fill gaps in the incomplete time series using quadratic interpolation method, then plots the filled time series.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/02-data-processing.ipynb#2025-04-17_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfiller = MissingValuesFiller()\nfilled = filler.transform(incomplete_series, method=\"quadratic\")\n\nfilled.plot()\n```\n\n----------------------------------------\n\nTITLE: Plotting Anomaly Detection Results\nDESCRIPTION: Shows how to visualize the original series, anomaly scores, and binary anomaly classifications.\nSOURCE: https://github.com/unit8co/darts/blob/master/README.md#2025-04-17_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\n\nseries.plot()\n(anom_score / 2. - 100).plot(label=\"computed anomaly score\", c=\"orangered\", lw=3)\n(binary_anom * 45 - 150).plot(label=\"detected binary anomaly\", lw=4)\n```\n\n----------------------------------------\n\nTITLE: Implementing FFT Model with Polynomial Detrending\nDESCRIPTION: Creates an FFT model with polynomial detrending to handle the trend in the Air Passengers dataset before applying Fourier Transform analysis.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/03-FFT-examples.ipynb#2025-04-17_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nmodel = FFT(trend=\"poly\")\nmodel.fit(train)\npred_val = model.predict(len(val))\n```\n\n----------------------------------------\n\nTITLE: Evaluating TCN Model on Monthly Sunspots Dataset\nDESCRIPTION: This snippet performs historical forecasts using the trained TCN model on the Monthly Sunspots dataset and plots the results against the actual data.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/05-TCN-examples.ipynb#2025-04-17_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nbacktest_sp = model_sun.historical_forecasts(\n    series_sp_transformed,\n    start=val_sp_transformed.start_time(),\n    forecast_horizon=12,\n    stride=12,\n    last_points_only=False,\n    retrain=False,\n    verbose=True,\n)\nbacktest_sp = concatenate(backtest_sp)\n\nval_sp_transformed.plot(label=\"actual\")\nbacktest_sp.plot(label=\"backtest (H=12)\")\nplt.legend()\n```\n\n----------------------------------------\n\nTITLE: Filtering Noisy Data with a Gaussian Process\nDESCRIPTION: Applies a Gaussian Process filter with an ExpSineSquared kernel to extract the underlying signal from noisy data. The kernel encodes periodicity assumptions, which is appropriate for sine wave data. The filter's performance is shown by comparing the original, noisy, and filtered signals.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/11-GP-filter-examples.ipynb#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nkernel = ExpSineSquared()\n# kernel = RBF()\n\ngpf = GaussianProcessFilter(\n    kernel=kernel, alpha=NOISE_DISTANCE / 2, n_restarts_optimizer=100\n)\nfiltered_x = gpf.filter(x_noise)\n\nplt.figure(figsize=[12, 8])\nx.plot(color=\"black\", label=\"Orginal sine wave\")\nx_noise.plot(color=\"red\", label=\"Noisy sine wave\")\nfiltered_x.plot(color=\"blue\", label=\"Filtered sine wave\")\nplt.legend()\n```\n\n----------------------------------------\n\nTITLE: Training N-BEATS Interpretable Model on Energy Generation Data\nDESCRIPTION: Trains the N-BEATS interpretable model on the scaled training data with validation. This model can separate trend and seasonality components in the forecasts for better interpretability.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/07-NBEATS-examples.ipynb#2025-04-17_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nmodel_nbeats.fit(series=train_scaled, val_series=val_scaled)\n```\n\n----------------------------------------\n\nTITLE: Evaluating RNN Model Predictions on Air Passengers Data in Python\nDESCRIPTION: This code defines a function to evaluate the RNN model's predictions on the Air Passengers dataset. It plots the actual vs. forecast values and calculates the MAPE (Mean Absolute Percentage Error).\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/04-RNN-examples.ipynb#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef eval_model(model):\n    pred_series = model.predict(n=26, future_covariates=covariates)\n    plt.figure(figsize=(8, 5))\n    series_transformed.plot(label=\"actual\")\n    pred_series.plot(label=\"forecast\")\n    plt.title(f\"MAPE: {mape(pred_series, val_transformed):.2f}%\")\n    plt.legend()\n\neval_model(my_model)\n```\n\n----------------------------------------\n\nTITLE: Implementing FFT Model with Frequency Filtering\nDESCRIPTION: Creates an FFT model that filters out low-amplitude frequencies, keeping only the 20 strongest frequency components to reduce noise while preserving important seasonal patterns.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/03-FFT-examples.ipynb#2025-04-17_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nmodel = FFT(nr_freqs_to_keep=20)\nmodel.fit(train)\npred_val = model.predict(len(val))\n```\n\n----------------------------------------\n\nTITLE: Generating Historical Forecasts with N-BEATS Interpretable Model\nDESCRIPTION: Creates historical forecasts using the trained N-BEATS interpretable model with a 7-day forecasting horizon. These forecasts can be decomposed into trend and seasonality components for better analysis.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/07-NBEATS-examples.ipynb#2025-04-17_snippet_13\n\nLANGUAGE: python\nCODE:\n```\npred_series = model_nbeats.historical_forecasts(\n    series_scaled,\n    start=val_scaled.start_time(),\n    forecast_horizon=7,\n    stride=7,\n    last_points_only=False,\n    retrain=False,\n    verbose=True,\n)\npred_series = concatenate(pred_series)\n```\n\n----------------------------------------\n\nTITLE: Predicting with Trained DeepTCN Model on Variable Noise Series in Python\nDESCRIPTION: This snippet uses the trained DeepTCN model to make predictions on the variable noise series. It generates 100 samples for probabilistic forecasting and plots the results.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/09-DeepTCN-examples.ipynb#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npred = deeptcn.predict(80, past_covariates=covariates, num_samples=100)\ntarget_val.slice_intersect(pred).plot(label=\"target\")\npred.plot(label=\"forecast\")\n```\n\n----------------------------------------\n\nTITLE: Comparing Multiple Forecasting Models on Energy Data\nDESCRIPTION: Evaluates and compares the performance of three different forecasting models (ExponentialSmoothing, Theta, and FFT) on the nuclear energy generation dataset using MAE metric.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/03-FFT-examples.ipynb#2025-04-17_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nmodels = [ExponentialSmoothing(), Theta(), FFT()]\n\nfor model in models:\n    model.fit(train)\n    pred_val = model.predict(len(val))\n    print(str(model) + \" MAE: \" + str(mae(pred_val, val)))\n```\n\n----------------------------------------\n\nTITLE: Exporting Darts Model to ONNX Format\nDESCRIPTION: Shows how to export a trained Darts forecasting model to ONNX format for lightweight inference environments.\nSOURCE: https://github.com/unit8co/darts/blob/master/docs/userguide/torch_forecasting_models.md#2025-04-17_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nmodel = SomeTorchForecastingModel(...)\nmodel.fit(...)\n\n# make sure to have `onnx` and `onnxruntime` installed\nonnx_filename = \"example_onnx.onnx\"\nmodel.to_onnx(onnx_filename, export_params=True)\n```\n\n----------------------------------------\n\nTITLE: Initializing NHiTS and TiDE Models with Different Configurations\nDESCRIPTION: Creates three different forecasting models: NHiTSModel and two variants of TiDEModel (with and without Reversible Instance Normalization), using the previously defined common parameters.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/18-TiDE-examples.ipynb#2025-04-17_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# create the models\nmodel_nhits = NHiTSModel(**common_model_args, model_name=\"hi\")\n\nmodel_tide = TiDEModel(\n    **common_model_args, use_reversible_instance_norm=False, model_name=\"tide0\"\n)\n\nmodel_tide_rin = TiDEModel(\n    **common_model_args, use_reversible_instance_norm=True, model_name=\"tide1\"\n)\n\nmodels = {\n    \"NHiTS\": model_nhits,\n    \"TiDE\": model_tide,\n    \"TiDE+RIN\": model_tide_rin,\n}\n```\n\n----------------------------------------\n\nTITLE: Using Global Forecasting Models with Multiple Series\nDESCRIPTION: Example demonstrating how to use Global Forecasting Models (GFMs) with multiple target series. One covariate TimeSeries must be provided per target TimeSeries, and at prediction time, the corresponding covariates must be supplied.\nSOURCE: https://github.com/unit8co/darts/blob/master/docs/userguide/covariates.md#2025-04-17_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom darts.models import NBEATSModel\n\n# multiple time series\nall_targets = [target1, target2, ...]\nall_past_covariates = [past_covariates1, past_covariates2, ...]\n\n# create a GFM model, train and predict\nmodel = NBEATSModel(input_chunk_length=1, output_chunk_length=1)\n\nmodel.fit(all_targets,\n          past_covariates=all_past_covariates)\n\npred = model.predict(n=1,\n                     series=all_targets[0],\n                     past_covariates=all_past_covariates[0])\n```\n\n----------------------------------------\n\nTITLE: Visualizing and Evaluating Interpretable N-BEATS Model Forecasts\nDESCRIPTION: Displays historical forecasts produced by the interpretable N-BEATS model alongside actual values. The visualization includes R2 score for quantifying forecast accuracy and starts from the validation period.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/07-NBEATS-examples.ipynb#2025-04-17_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndisplay_forecast(\n    pred_series, series_scaled, \"7 day\", start_date=val_scaled.start_time()\n)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking NBEATSModel and TFTModel in Darts\nDESCRIPTION: A comprehensive benchmark table comparing the performance of NBEATSModel and TFTModel on the Energy dataset. The table shows training times per epoch under different configurations, including CPU/GPU usage, batch sizes, and number of workers.\nSOURCE: https://github.com/unit8co/darts/blob/master/docs/userguide/torch_forecasting_models.md#2025-04-17_snippet_18\n\nLANGUAGE: markdown\nCODE:\n```\n| Model         | Dataset | dtype | CUDA | Batch size | num workers | time per epoch |\n|---------------|---------|-------|------|------------|-------------|----------------|\n| `NBEATSModel` | Energy  | 64    | no   | 32         | 0           | 283s           |\n| `NBEATSModel` | Energy  | 64    | no   | 32         | 2           | 285s           |\n| `NBEATSModel` | Energy  | 64    | no   | 32         | 4           | 282s           |\n| `NBEATSModel` | Energy  | 64    | no   | 1024       | 0           | 58s            |\n| `NBEATSModel` | Energy  | 64    | no   | 1024       | 2           | 57s            |\n| `NBEATSModel` | Energy  | 64    | no   | 1024       | 4           | 58s            |\n| `NBEATSModel` | Energy  | 64    | yes  | 32         | 0           | 63s            |\n| `NBEATSModel` | Energy  | 64    | yes  | 32         | 2           | 62s            |\n| `NBEATSModel` | Energy  | 64    | yes  | 1024       | 0           | 13.3s          |\n| `NBEATSModel` | Energy  | 64    | yes  | 1024       | 2           | 12.1s          |\n| `NBEATSModel` | Energy  | 64    | yes  | 1024       | 4           | 12.3s          |\n|               |         |       |      |            |             |                |\n| `NBEATSModel` | Energy  | 32    | no   | 32         | 0           | 117s           |\n| `NBEATSModel` | Energy  | 32    | no   | 32         | 2           | 115s           |\n| `NBEATSModel` | Energy  | 32    | no   | 32         | 4           | 117s           |\n| `NBEATSModel` | Energy  | 32    | no   | 1024       | 0           | 28.4s          |\n| `NBEATSModel` | Energy  | 32    | no   | 1024       | 2           | 27.4s          |\n| `NBEATSModel` | Energy  | 32    | no   | 1024       | 4           | 27.5s          |\n| `NBEATSModel` | Energy  | 32    | yes  | 32         | 0           | 41.5s          |\n| `NBEATSModel` | Energy  | 32    | yes  | 32         | 2           | 40.6s          |\n| `NBEATSModel` | Energy  | 32    | yes  | 1024       | 0           | 2.8s           |\n| `NBEATSModel` | Energy  | 32    | yes  | 1024       | 2           | 1.65           |\n| `NBEATSModel` | Energy  | 32    | yes  | 1024       | 4           | 1.8s           |\n|               |         |       |      |            |             |                |\n| `TFTModel`    | Energy  | 64    | no   | 32         | 0           | 78s            |\n| `TFTModel`    | Energy  | 64    | no   | 32         | 2           | 72s            |\n| `TFTModel`    | Energy  | 64    | no   | 32         | 4           | 72s            |\n| `TFTModel`    | Energy  | 64    | no   | 1024       | 0           | 46s            |\n| `TFTModel`    | Energy  | 64    | no   | 1024       | 2           | 38s            |\n| `TFTModel`    | Energy  | 64    | no   | 1024       | 4           | 39s            |\n| `TFTModel`    | Energy  | 64    | yes  | 32         | 0           | 125s           |\n| `TFTModel`    | Energy  | 64    | yes  | 32         | 2           | 115s           |\n| `TFTModel`    | Energy  | 64    | yes  | 1024       | 0           | 59s            |\n| `TFTModel`    | Energy  | 64    | yes  | 1024       | 2           | 50s            |\n| `TFTModel`    | Energy  | 64    | yes  | 1024       | 4           | 50s            |\n|               |         |       |      |            |             |                |\n| `TFTModel`    | Energy  | 32    | no   | 32         | 0           | 70s            |\n| `TFTModel`    | Energy  | 32    | no   | 32         | 2           | 62.6s          |\n| `TFTModel`    | Energy  | 32    | no   | 32         | 4           | 63.6           |\n| `TFTModel`    | Energy  | 32    | no   | 1024       | 0           | 31.9s          |\n| `TFTModel`    | Energy  | 32    | no   | 1024       | 2           | 45s            |\n| `TFTModel`    | Energy  | 32    | no   | 1024       | 4           | 44s            |\n| `TFTModel`    | Energy  | 32    | yes  | 32         | 0           | 73s            |\n| `TFTModel`    | Energy  | 32    | yes  | 32         | 2           | 58s            |\n| `TFTModel`    | Energy  | 32    | yes  | 1024       | 0           | 41s            |\n| `TFTModel`    | Energy  | 32    | yes  | 1024       | 2           | 31s            |\n| `TFTModel`    | Energy  | 32    | yes  | 1024       | 4           | 31s            |\n```\n\n----------------------------------------\n\nTITLE: Setting Up Automatic Checkpointing in Darts Models\nDESCRIPTION: This snippet demonstrates how to enable automatic checkpointing during model training and how to load the best performing model based on validation loss.\nSOURCE: https://github.com/unit8co/darts/blob/master/docs/userguide/torch_forecasting_models.md#2025-04-17_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nmodel = SomeTorchForecastingModel(..., model_name='my_model', save_checkpoints=True)\n\n# checkpoints are saved automatically\nmodel.fit(...)\n\n# load the model state that performed best on validation set\nbest_model = model.load_from_checkpoint(model_name='my_model', best=True)\n```\n\n----------------------------------------\n\nTITLE: Backtesting Transformer Model on Monthly Sunspots Dataset in Python\nDESCRIPTION: This function performs backtesting on the Transformer model for the Monthly Sunspots dataset. It compares the Transformer model's performance with an Exponential Smoothing model over a 3-year forecast horizon.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/06-Transformer-examples.ipynb#2025-04-17_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef backtest(testing_model):\n    # Compute the backtest predictions with the two models\n    pred_series = testing_model.historical_forecasts(\n        series=series_sp_scaled,\n        start=pd.Timestamp(\"19401001\"),\n        forecast_horizon=36,\n        stride=10,\n        retrain=False,\n        verbose=True,\n    )\n\n    pred_series_ets = ExponentialSmoothing().historical_forecasts(\n        series=series_sp_scaled,\n        start=pd.Timestamp(\"19401001\"),\n        forecast_horizon=36,\n        stride=10,\n        retrain=True,\n        verbose=True,\n    )\n    val_sp_scaled.plot(label=\"actual\")\n    pred_series.plot(label=\"our Transformer\")\n    pred_series_ets.plot(label=\"ETS\")\n    plt.legend()\n    print(\"Transformer MAPE:\", mape(pred_series, val_sp_scaled))\n    print(\"ETS MAPE:\", mape(pred_series_ets, val_sp_scaled))\n```\n\n----------------------------------------\n\nTITLE: Creating a Pipeline for Missing Value Handling and Scaling\nDESCRIPTION: Loads the incomplete milk dataset and creates a Pipeline with MissingValuesFiller and Scaler transformers to handle missing values and normalize the data in one step.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/02-data-processing.ipynb#2025-04-17_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nincomplete_series = MonthlyMilkIncompleteDataset().load()\n\nfiller = MissingValuesFiller()\nscaler = Scaler()\n\npipeline = Pipeline([filler, scaler])\ntransformed = pipeline.fit_transform(incomplete_series)\n```\n\n----------------------------------------\n\nTITLE: Backtesting RNN Model on Air Passengers Data in Python\nDESCRIPTION: This snippet performs backtesting of the RNN model on the Air Passengers dataset. It generates historical forecasts starting from January 1959 with a 6-month forecast horizon.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/04-RNN-examples.ipynb#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nbacktest_series = my_model.historical_forecasts(\n    series_transformed,\n    future_covariates=covariates,\n    start=pd.Timestamp(\"19590101\"),\n    forecast_horizon=6,\n    retrain=False,\n    verbose=True,\n)\n\nplt.figure(figsize=(8, 5))\nseries_transformed.plot(label=\"actual\")\nbacktest_series.plot(label=\"backtest\")\nplt.legend()\nplt.title(\"Backtest, starting Jan 1959, 6-months horizon\")\nprint(\n    \"MAPE: {:.2f}%\".format(\n        mape(\n            transformer.inverse_transform(series_transformed),\n            transformer.inverse_transform(backtest_series),\n        )\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Confidence Intervals with Gaussian Process Sampling\nDESCRIPTION: Demonstrates how to sample from the Gaussian Process posterior distribution to generate confidence intervals. This visualization shows the uncertainty in the model's predictions, with greater uncertainty visible around the crests and troughs of the sine wave.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/11-GP-filter-examples.ipynb#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfiltered_x_samples = gpf.filter(x_noise, num_samples=100)\n\nplt.figure(figsize=[12, 8])\nx.plot(color=\"black\", label=\"Original sine wave\")\nx_noise.plot(color=\"red\", label=\"Noisy sine wave\")\nfiltered_x_samples.plot(color=\"blue\", label=\"Confidence interval of filtered sine wave\")\nplt.legend()\n```\n\n----------------------------------------\n\nTITLE: Creating and Training BlockRNN Model for Sunspots Data in Python\nDESCRIPTION: This snippet creates a GRU-based BlockRNN model using Darts and trains it on the Monthly Sunspots dataset. It demonstrates how to configure model parameters for this more challenging time series.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/04-RNN-examples.ipynb#2025-04-17_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmy_model_sun = BlockRNNModel(\n    model=\"GRU\",\n    input_chunk_length=125,\n    output_chunk_length=36,\n    hidden_dim=10,\n    n_rnn_layers=1,\n    batch_size=32,\n    n_epochs=100,\n    dropout=0.1,\n    model_name=\"sun_GRU\",\n    nr_epochs_val_period=1,\n    optimizer_kwargs={\"lr\": 1e-3},\n    log_tensorboard=True,\n    random_state=42,\n    force_reset=True,\n)\n\nmy_model_sun.fit(train_sp_transformed, val_series=val_sp_transformed, verbose=True)\n```\n\n----------------------------------------\n\nTITLE: Performing Historical Forecasts with DeepTCN Model on Energy Production Data in Python\nDESCRIPTION: This snippet generates historical forecasts using the trained DeepTCN model on the energy production data. It uses a rolling window approach and concatenates the results.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/09-DeepTCN-examples.ipynb#2025-04-17_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nbacktest_en = deeptcn.historical_forecasts(\n    series=series_en_transformed,\n    start=val_en_transformed.start_time(),\n    past_covariates=day_series,\n    num_samples=500,\n    forecast_horizon=30,\n    stride=30,\n    last_points_only=False,\n    retrain=False,\n    verbose=True,\n)\nbacktest_en = concatenate(backtest_en)\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Daily Energy Data with Missing Values Filling and Scaling\nDESCRIPTION: Aggregates hourly energy data to daily averages, handles missing values using MissingValuesFiller, and scales the data with Scaler. The prepared data is split into training and validation sets for N-BEATS model training.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/07-NBEATS-examples.ipynb#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndf_day_avg = df.groupby(df.index.astype(str).str.split(\" \").str[0]).mean().reset_index()\nfiller = MissingValuesFiller()\nscaler = Scaler()\nseries = filler.transform(\n    TimeSeries.from_dataframe(\n        df_day_avg, \"time\", [\"generation hydro run-of-river and poundage\"]\n    )\n).astype(np.float32)\n\ntrain, val = series.split_after(pd.Timestamp(\"20170901\"))\n\ntrain_scaled = scaler.fit_transform(train)\nval_scaled = scaler.transform(val)\nseries_scaled = scaler.transform(series)\n\n\ntrain_scaled.plot(label=\"training\")\nval_scaled.plot(label=\"val\")\nplt.title(\"Daily generation hydro run-of-river and poundage\")\n```\n\n----------------------------------------\n\nTITLE: Capturing Model Uncertainty with Monte Carlo Dropout in Darts\nDESCRIPTION: This example shows how to use Monte Carlo Dropout to capture model uncertainty (epistemic uncertainty) in a TCN model. The model is configured with a 10% dropout rate and trained with MSE loss. At prediction time, mc_dropout=True enables sampling from the model distribution to generate 500 forecast samples.\nSOURCE: https://github.com/unit8co/darts/blob/master/docs/userguide/forecasting_overview.md#2025-04-17_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom darts.datasets import AirPassengersDataset\nfrom darts import TimeSeries\nfrom darts.models import TCNModel\nfrom darts.dataprocessing.transformers import Scaler\nfrom darts.utils.likelihood_models.torch import QuantileRegression\n\nseries = AirPassengersDataset().load()\ntrain, val = series[:-36], series[-36:]\n\nscaler = Scaler()\ntrain = scaler.fit_transform(train)\nval = scaler.transform(val)\nseries = scaler.transform(series)\n\nmodel = TCNModel(input_chunk_length=30,\n                 output_chunk_length=12,\n                 dropout=0.1)\nmodel.fit(train, epochs=400)\npred = model.predict(n=36, mc_dropout=True, num_samples=500)\n\nseries.plot()\npred.plot(label='forecast')\n```\n\n----------------------------------------\n\nTITLE: Initializing Transformer Model for Air Passengers Forecasting in Python\nDESCRIPTION: This snippet initializes a TransformerModel with specific hyperparameters for the Air Passengers dataset. It sets up the model for one-step forecasting with customized architecture settings.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/06-Transformer-examples.ipynb#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmy_model = TransformerModel(\n    input_chunk_length=12,\n    output_chunk_length=1,\n    batch_size=32,\n    n_epochs=200,\n    model_name=\"air_transformer\",\n    nr_epochs_val_period=10,\n    d_model=16,\n    nhead=8,\n    num_encoder_layers=2,\n    num_decoder_layers=2,\n    dim_feedforward=128,\n    dropout=0.1,\n    activation=\"relu\",\n    random_state=42,\n    save_checkpoints=True,\n    force_reset=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Daily Average Transformation with InvertibleMapper\nDESCRIPTION: Implements a custom transformation using InvertibleMapper that converts monthly milk production to daily average by dividing each value by the number of days in the month, with an inverse function to convert back.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/02-data-processing.ipynb#2025-04-17_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Transform the time series\ntoDailyAverage = InvertibleMapper(\n    fn=lambda timestamp, x: x / timestamp.days_in_month,\n    inverse_fn=lambda timestamp, x: x * timestamp.days_in_month,\n)\n\ndailyAverage = toDailyAverage.transform(series)\n\ndailyAverage.plot()\n```\n\n----------------------------------------\n\nTITLE: Loading Best N-BEATS Model Checkpoint for Inference\nDESCRIPTION: Loads the best performing N-BEATS model based on validation performance from previously saved checkpoints. This ensures optimal model performance for forecasting tasks.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/07-NBEATS-examples.ipynb#2025-04-17_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nmodel_nbeats = NBEATSModel.load_from_checkpoint(model_name=model_name, best=True)\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Daily Energy Production Dataset for TCN Model\nDESCRIPTION: This snippet loads the Daily Energy Production dataset, performs data aggregation, splits it into training and validation sets, applies scaling, and creates a day covariate series. It prepares the data for use with the TCN model.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/05-TCN-examples.ipynb#2025-04-17_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndf3 = EnergyDataset().load().to_dataframe()\ndf3_day_avg = (\n    df3.groupby(df3.index.astype(str).str.split(\" \").str[0]).mean().reset_index()\n)\nseries_en = fill_missing_values(\n    TimeSeries.from_dataframe(\n        df3_day_avg, \"time\", [\"generation hydro run-of-river and poundage\"]\n    ),\n    \"auto\",\n)\n\n# create train and test splits\ntrain_en, val_en = series_en.split_after(pd.Timestamp(\"20170901\"))\n\n# scale the data\nscaler_en = Scaler()\ntrain_en_transformed = scaler_en.fit_transform(train_en)\nval_en_transformed = scaler_en.transform(val_en)\nseries_en_transformed = scaler_en.transform(series_en)\n\n# add the day as a covariate (scaling not required as one-hot-encoded)\nday_series = datetime_attribute_timeseries(\n    series_en_transformed, attribute=\"day\", one_hot=True\n)\n\nplt.figure(figsize=(10, 3))\ntrain_en_transformed.plot(label=\"train\")\nval_en_transformed.plot(label=\"validation\")\n```\n\n----------------------------------------\n\nTITLE: Forecasting with Daily Average Transformed Data\nDESCRIPTION: Splits the daily average transformed data, fits an Exponential Smoothing model, makes predictions, and evaluates the model performance using MAPE.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/02-data-processing.ipynb#2025-04-17_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Make a forecast\ndailyavg_train, dailyavg_val = dailyAverage.split_after(pd.Timestamp(\"1973-01-01\"))\n\nmodel = ExponentialSmoothing()\nmodel.fit(dailyavg_train)\ndailyavg_forecast = model.predict(36)\n\nplt.title(f\"MAPE = {mape(dailyavg_forecast, dailyavg_val):.2f}%\")\ndailyAverage.plot()\ndailyavg_forecast.plot()\nplt.legend()\n```\n\n----------------------------------------\n\nTITLE: Loading and Preprocessing Multivariate Time Series\nDESCRIPTION: Shows how to load a multivariate dataset and prepare it for anomaly detection.\nSOURCE: https://github.com/unit8co/darts/blob/master/README.md#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom darts.datasets import ETTh2Dataset\n\nseries = ETTh2Dataset().load()[:10000][[\"MUFL\", \"LULL\"]]\ntrain, val = series.split_before(0.6)\n```\n\n----------------------------------------\n\nTITLE: Using Covariates with Darts Forecasting Models\nDESCRIPTION: Basic example demonstrating how to fit a Darts forecasting model with past and future covariates and then make predictions using the same covariate types. The model intelligently slices the covariates based on the target time axis.\nSOURCE: https://github.com/unit8co/darts/blob/master/docs/userguide/covariates.md#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# create one of Darts' forecasting model\nmodel = SomeForecastingModel(...)\n\n# fit the model\nmodel.fit(target,\n          past_covariates=past_covariate,\n          future_covariates=future_covariates)\n\n# make a prediction with the same covariate types\npred = model.predict(n=1,\n                     series=target,  # this is only required for GFMs\n                     past_covariates=past_covariates,\n                     future_covariates=future_covariates)\n```\n\n----------------------------------------\n\nTITLE: Creating a GPU-Accelerated RNN Model in Darts\nDESCRIPTION: Configures an RNN model with GPU acceleration enabled through PyTorch Lightning trainer parameters. This setup directs the model to use the first GPU (device 0) for training.\nSOURCE: https://github.com/unit8co/darts/blob/master/docs/userguide/gpu_and_tpu_usage.md#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmy_model = RNNModel(\n    model=\"RNN\",\n    ...\n    force_reset=True,\n    pl_trainer_kwargs={\n      \"accelerator\": \"gpu\",\n      \"devices\": [0]\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Loading and Splitting Australian Beer Sales Dataset\nDESCRIPTION: Loads the Australian quarterly beer sales dataset from Darts and splits it into train (60%), validation (20%), and test (20%) sets for model training and evaluation.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/18-TiDE-examples.ipynb#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nseries = AusBeerDataset().load()\n\ntrain, temp = series.split_after(0.6)\nval, test = temp.split_after(0.5)\n```\n\nLANGUAGE: python\nCODE:\n```\ntrain.plot(label=\"train\")\nval.plot(label=\"val\")\ntest.plot(label=\"test\")\n```\n\n----------------------------------------\n\nTITLE: Exporting TorchForecastingModel to ONNX Format\nDESCRIPTION: Demonstrates how to export a torch-based model to ONNX format using the new to_onnx() method for inference optimization.\nSOURCE: https://github.com/unit8co/darts/blob/master/CHANGELOG.md#2025-04-17_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nmodel.to_onnx(\"model.onnx\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Laplace Distribution Forecasts with TCN Neural Network in Darts\nDESCRIPTION: This example shows how to train a TCN (Temporal Convolutional Network) model to fit a Laplace distribution for probabilistic forecasting. The model outputs location and scale parameters with a prior value of 0.1 on the scale parameter. The data is scaled before training, and the model generates 500 samples for the forecast.\nSOURCE: https://github.com/unit8co/darts/blob/master/docs/userguide/forecasting_overview.md#2025-04-17_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom darts.datasets import AirPassengersDataset\nfrom darts import TimeSeries\nfrom darts.models import TCNModel\nfrom darts.dataprocessing.transformers import Scaler\nfrom darts.utils.likelihood_models.torch import LaplaceLikelihood\n\nseries = AirPassengersDataset().load()\ntrain, val = series[:-36], series[-36:]\n\nscaler = Scaler()\ntrain = scaler.fit_transform(train)\nval = scaler.transform(val)\nseries = scaler.transform(series)\n\nmodel = TCNModel(input_chunk_length=30,\n                 output_chunk_length=12,\n                 likelihood=LaplaceLikelihood(prior_b=0.1))\nmodel.fit(train, epochs=400)\npred = model.predict(n=36, num_samples=500)\n\nseries.plot()\npred.plot(label='forecast')\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning Darts Model with Custom Optimizer\nDESCRIPTION: Example showing how to fine-tune a Darts forecasting model by loading weights from a checkpoint and using a custom SGD optimizer instead of the default Adam optimizer.\nSOURCE: https://github.com/unit8co/darts/blob/master/docs/userguide/torch_forecasting_models.md#2025-04-17_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nmodel_finetune = SomeTorchForecastingModel(...,\n                                           optimizer_cls=torch.optim.SGD,\n                                           optimizer_kwargs={\"lr\": 0.001})\n\n# load the weights from a checkpoint\nmodel_finetune.load_weights_from_checkpoint(model_name='my_model', best=True)\n\nmodel_finetune.fit(...)\n```\n\n----------------------------------------\n\nTITLE: Visualizing and Evaluating Generic N-BEATS Model Forecasts\nDESCRIPTION: Displays the historical forecasts produced by the generic N-BEATS model alongside actual values, starting from the validation period. The visualization includes the R2 score to quantify forecast accuracy.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/07-NBEATS-examples.ipynb#2025-04-17_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndisplay_forecast(\n    pred_series,\n    series_scaled,\n    \"7 day\",\n    start_date=val.start_time(),\n)\n```\n\n----------------------------------------\n\nTITLE: Forecasting with Exponential Smoothing on Original Data\nDESCRIPTION: Splits the milk production dataset into training and validation sets, trains an Exponential Smoothing model, makes predictions, and evaluates model performance using MAPE.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/02-data-processing.ipynb#2025-04-17_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ntraining, validation = series.split_before(pd.Timestamp(\"1973-01-01\"))\n\nmodel = ExponentialSmoothing()\nmodel.fit(training)\nforecast = model.predict(36)\n\nplt.title(f\"MAPE = {mape(forecast, validation):.2f}%\")\nseries.plot(label=\"actual\")\nforecast.plot(label=\"forecast\")\nplt.legend()\n```\n\n----------------------------------------\n\nTITLE: Concatenating Multiple TimeSeries Objects in Python\nDESCRIPTION: Demonstrates how to concatenate multiple TimeSeries objects along different axes to create a multivariate series. The axis=1 parameter combines series that share the same time axis.\nSOURCE: https://github.com/unit8co/darts/blob/master/docs/userguide/timeseries.md#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom darts import concatenate\n\nmy_multivariate_series = concatenate([series1, series2, ...], axis=1)\n```\n\n----------------------------------------\n\nTITLE: Combining Multiple Covariates in Darts\nDESCRIPTION: Example showing how to combine multiple past or future covariates into a single TimeSeries object using either stack() or concatenate() methods, which is necessary when working with multiple covariate series.\nSOURCE: https://github.com/unit8co/darts/blob/master/docs/userguide/covariates.md#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# stack() time series\npast_covariates = past_covariates.stack(past_covariates2)\n\n# or concatenate()\nfrom darts import concatenate\npast_covariates = concatenate([past_covariates, past_covariates2, ...], axis=1)\n```\n\n----------------------------------------\n\nTITLE: Evaluating FFT Model with Frequency Filtering\nDESCRIPTION: Plots and evaluates the FFT model with frequency filtering, showing how filtering reduces noise in predictions and improves overall accuracy measured by MAE.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/03-FFT-examples.ipynb#2025-04-17_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ntrain.plot(label=\"train\")\nval.plot(label=\"val\")\npred_val.plot(label=\"predict\")\nprint(\"MAE:\", mae(pred_val, val))\n```\n\n----------------------------------------\n\nTITLE: Evaluating TCN Model on Daily Energy Production Dataset\nDESCRIPTION: This snippet performs historical forecasts using the trained TCN model on the Daily Energy Production dataset and plots the results against the actual data.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/05-TCN-examples.ipynb#2025-04-17_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nbacktest_en = model_en.historical_forecasts(\n    series=series_en_transformed,\n    past_covariates=day_series,\n    start=val_en_transformed.start_time(),\n    forecast_horizon=7,\n    stride=7,\n    last_points_only=False,\n    retrain=False,\n    verbose=True,\n)\nbacktest_en = concatenate(backtest_en)\n\nplt.figure(figsize=(10, 6))\nval_en_transformed.plot(label=\"actual\")\nbacktest_en.plot(label=\"backtest (H=7)\")\nplt.legend()\n```\n\n----------------------------------------\n\nTITLE: Initializing Transformer Model for Monthly Sunspots Forecasting in Python\nDESCRIPTION: This snippet initializes a TransformerModel with specific hyperparameters for the Monthly Sunspots dataset. It sets up the model for multi-step forecasting with customized architecture settings.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/06-Transformer-examples.ipynb#2025-04-17_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmy_model_sp = TransformerModel(\n    batch_size=32,\n    input_chunk_length=125,\n    output_chunk_length=36,\n    n_epochs=20,\n    model_name=\"sun_spots_transformer\",\n    nr_epochs_val_period=5,\n    d_model=16,\n    nhead=4,\n    num_encoder_layers=2,\n    num_decoder_layers=2,\n    dim_feedforward=128,\n    dropout=0.1,\n    random_state=42,\n    optimizer_kwargs={\"lr\": 1e-3},\n    save_checkpoints=True,\n    force_reset=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Using RegressionModel with Component-Specific Lags\nDESCRIPTION: New functionality allowing specification of different lags for target, past and future covariates per component/column in regression models\nSOURCE: https://github.com/unit8co/darts/blob/master/CHANGELOG.md#2025-04-17_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nmodel = RegressionModel(lags={'target': [1,2], 'past': [1], 'future': [1]})\n```\n\n----------------------------------------\n\nTITLE: Handling Missing Data with Gaussian Process Filtering\nDESCRIPTION: Shows how Gaussian Process filtering can impute missing values in time series data. A section of data points is set to NaN, and the Gaussian Process filter estimates these values based on the underlying periodic pattern. The confidence intervals reveal the uncertainty in these estimates.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/11-GP-filter-examples.ipynb#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nx_missing_arr = x_noise.values()\nx_missing_arr[50:100] = np.nan\nx_missing = TimeSeries.from_values(x_missing_arr)\n\nkernel = ExpSineSquared()\n# kernel = RBF()\n\ngpf_missing = GaussianProcessFilter(kernel=kernel, alpha=0.2, n_restarts_optimizer=100)\nfiltered_x_missing_samples = gpf_missing.filter(x_missing, num_samples=100)\n\nplt.figure(figsize=[12, 8])\nx_missing.plot(color=\"red\", label=\"Noisy sine wave with missing data\")\nfiltered_x_missing_samples.plot(\n    color=\"blue\", label=\"Confidence interval of filtered sine wave\"\n)\nplt.legend()\n```\n\n----------------------------------------\n\nTITLE: Manual Saving and Loading of Darts Models\nDESCRIPTION: This code shows how to manually save a trained model to disk and load it back for inference or continued training.\nSOURCE: https://github.com/unit8co/darts/blob/master/docs/userguide/torch_forecasting_models.md#2025-04-17_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nmodel.save(\"/your/path/to/save/model.pt\")\nloaded_model = model.load(\"/your/path/to/save/model.pt\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic FFT Model\nDESCRIPTION: Creates and fits a basic FFT model without any improvements. This model uses a Discrete Fourier Transform on the training data without frequency filtering or alignment constraints.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/03-FFT-examples.ipynb#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmodel = FFT(required_matches=set(), nr_freqs_to_keep=None)\nmodel.fit(train)\npred_val = model.predict(len(val))\n```\n\n----------------------------------------\n\nTITLE: Inverse Transform with Pipeline\nDESCRIPTION: Shows how to apply the inverse transformation of a Pipeline to restore the original scale and format of the data.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/02-data-processing.ipynb#2025-04-17_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nback = pipeline.inverse_transform(transformed)\nback.plot()\n```\n\n----------------------------------------\n\nTITLE: Visualizing FFT Model Performance with Detrending\nDESCRIPTION: Plots the results of the FFT model with polynomial detrending on the Air Passengers dataset, showing improved prediction capability by accounting for the trend.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/03-FFT-examples.ipynb#2025-04-17_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ntrain.plot(label=\"train\")\nval.plot(label=\"val\")\npred_val.plot(label=\"prediction\")\n```\n\n----------------------------------------\n\nTITLE: Initializing a Basic Torch Forecasting Model in Python\nDESCRIPTION: Example of initializing a Torch Forecasting Model in Darts with input and output chunk lengths. This configuration creates a model that looks back 7 time steps and predicts 1 time step ahead.\nSOURCE: https://github.com/unit8co/darts/blob/master/docs/userguide/torch_forecasting_models.md#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# model that looks 7 time steps back (past) and 1 time step ahead (future)\nmodel = SomeTorchForecastingModel(input_chunk_length=7,\n                                  output_chunk_length=1,\n                                  **model_kwargs)\n```\n\n----------------------------------------\n\nTITLE: Handling Missing Values with MissingValuesFiller\nDESCRIPTION: Loads a dataset with missing values and demonstrates how to fill these gaps using the MissingValuesFiller transformer with quadratic interpolation.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/02-data-processing.ipynb#2025-04-17_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nincomplete_series = MonthlyMilkIncompleteDataset().load()\nincomplete_series.plot()\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Monthly Sunspots Dataset in Python\nDESCRIPTION: This code loads the Monthly Sunspots dataset, splits it into training and validation sets, and applies scaling. It prepares the data for use with the Transformer model.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/06-Transformer-examples.ipynb#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nseries_sunspot = SunspotsDataset().load().astype(np.float32)\n\nseries_sunspot.plot()\ncheck_seasonality(series_sunspot, max_lag=240)\n\ntrain_sp, val_sp = series_sunspot.split_after(pd.Timestamp(\"19401001\"))\n\nscaler_sunspot = Scaler()\ntrain_sp_scaled = scaler_sunspot.fit_transform(train_sp)\nval_sp_scaled = scaler_sunspot.transform(val_sp)\nseries_sp_scaled = scaler_sunspot.transform(series_sunspot)\n```\n\n----------------------------------------\n\nTITLE: Inverse Transform of Daily Average Forecast\nDESCRIPTION: Applies the inverse transformation to convert the daily average forecast back to monthly total production.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/02-data-processing.ipynb#2025-04-17_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Inverse the transformation\n# Here the forecast is stochastic; so we take the median value\nforecast = toDailyAverage.inverse_transform(dailyavg_forecast)\n```\n\n----------------------------------------\n\nTITLE: Inverse Scaling of Time Series Data\nDESCRIPTION: Shows how to reverse the scaling transformation using the inverse_transform method of the Scaler to restore the original scale of the data.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/02-data-processing.ipynb#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nback = scaler.inverse_transform(rescaled)\nprint(back)\n```\n\n----------------------------------------\n\nTITLE: Evaluating FFT Model with Timestamp Matching\nDESCRIPTION: Plots and evaluates the improved FFT model that uses timestamp matching to align seasonal patterns, comparing predictions against validation data using MAE metric.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/03-FFT-examples.ipynb#2025-04-17_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntrain.plot(label=\"train\")\nval.plot(label=\"val\")\npred_val.plot(label=\"predict\")\nprint(\"MAE:\", mae(pred_val, val))\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Air Passengers Dataset in Python\nDESCRIPTION: This code loads the Air Passengers dataset, splits it into training and validation sets, and applies scaling. It prepares the data for use with the Transformer model.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/06-Transformer-examples.ipynb#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Read data:\nseries = AirPassengersDataset().load().astype(np.float32)\n\n# Create training and validation sets:\ntrain, val = series.split_after(pd.Timestamp(\"19590101\"))\n\n# Normalize the time series (note: we avoid fitting the transformer on the validation set)\n# Change name\nscaler = Scaler()\ntrain_scaled = scaler.fit_transform(train)\nval_scaled = scaler.transform(val)\nseries_scaled = scaler.transform(series)\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic FFT Model on Trended Data\nDESCRIPTION: Applies the default FFT model to the Air Passengers dataset which has a strong upward trend, demonstrating the limitations of basic FFT when trend is present.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/03-FFT-examples.ipynb#2025-04-17_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nmodel = FFT()\nmodel.fit(train)\npred_val = model.predict(len(val))\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading NBEATS Model\nDESCRIPTION: Shows how to save and load an NBEATS neural network model using PyTorch Lightning checkpointing. Demonstrates model persistence for deep learning models.\nSOURCE: https://github.com/unit8co/darts/blob/master/docs/userguide/forecasting_overview.md#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom darts.models import NBEATSModel\n\nmodel = NBEATSModel(input_chunk_length=24,\n                    output_chunk_length=12)\n\nmodel.save(\"my_model.pt\")\nmodel_loaded = NBEATSModel.load(\"my_model.pt\")\n```\n\n----------------------------------------\n\nTITLE: Inverse Transform of Multiple Time Series\nDESCRIPTION: Applies a partial inverse transformation to multiple transformed time series and plots the results, demonstrating how Pipeline handles batch processing for inverse transformations.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/02-data-processing.ipynb#2025-04-17_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nback = pipeline.inverse_transform(transformed, partial=True)\nfor ts in back:\n    ts.plot()\n```\n\n----------------------------------------\n\nTITLE: Parallelizing Data Transformations with Multiple Cores in Darts\nDESCRIPTION: This snippet shows how to parallelize data transformations in Darts using multiple CPU cores. By setting the n_jobs parameter to -1, the transformation process utilizes all available cores on the machine, which can significantly speed up processing for large datasets.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/02-data-processing.ipynb#2025-04-17_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n# setting n_jobs to -1 will make the library using all the cores available in the machine\nscaler = Scaler(verbose=True, n_jobs=-1, name=\"Faster\")\nscaler.fit(huge_number_of_series)\nback = scaler.transform(huge_number_of_series)\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading Torch Models\nDESCRIPTION: Examples of saving and loading Torch models using manual methods\nSOURCE: https://github.com/unit8co/darts/blob/master/CHANGELOG.md#2025-04-17_snippet_33\n\nLANGUAGE: python\nCODE:\n```\nYourTorchModel.save_model(file_path)\nYourTorchModel.load_model(file_path)\nYourTorchModel.load_from_checkpoint()\n```\n\n----------------------------------------\n\nTITLE: Creating a TPU-Accelerated RNN Model in Darts\nDESCRIPTION: Configures an RNN model to use TPU acceleration by specifying TPU cores in the PyTorch Lightning trainer parameters, enabling distributed training across 4 TPU cores.\nSOURCE: https://github.com/unit8co/darts/blob/master/docs/userguide/gpu_and_tpu_usage.md#2025-04-17_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nmy_model = RNNModel(\n    model=\"RNN\",\n    ...\n    force_reset=True,\n    pl_trainer_kwargs={\n      \"accelerator\": \"tpu\",\n      \"tpu_cores\": [4]\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Basic FFT Model Performance\nDESCRIPTION: Plots the training data, validation data, and predictions from the basic FFT model. Calculates Mean Absolute Error (MAE) to quantify prediction accuracy.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/03-FFT-examples.ipynb#2025-04-17_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntrain.plot(label=\"train\")\nval.plot(label=\"val\")\npred_val.plot(label=\"prediction\")\nprint(\"MAE:\", mae(pred_val, val))\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Monthly Sunspots Dataset for TCN Model\nDESCRIPTION: This snippet loads the Monthly Sunspots dataset, splits it into training and validation sets, and applies scaling. It prepares the data for use with the TCN model.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/05-TCN-examples.ipynb#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nseries_sunspot = SunspotsDataset().load()\n\ntrain, val = series_sunspot.split_after(pd.Timestamp(\"19401001\"))\n\nscaler = Scaler()\n\ntrain_sp_transformed = scaler.fit_transform(train)\nval_sp_transformed = scaler.transform(val)\nseries_sp_transformed = scaler.transform(series_sunspot)\n```\n\n----------------------------------------\n\nTITLE: Loading Manual Saves of Darts Models to CPU\nDESCRIPTION: This snippet shows how to load a manually saved model to CPU, which is useful when the model was trained on GPU but needs to be used on a CPU-only machine.\nSOURCE: https://github.com/unit8co/darts/blob/master/docs/userguide/torch_forecasting_models.md#2025-04-17_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nmodel.save(\"/your/path/to/save/model.pt\")\nloaded_model = model.load(\"/your/path/to/save/model.pt\", map_location=\"cpu\")\nloaded_model.to_cpu()\n```\n\n----------------------------------------\n\nTITLE: Refactored Backtesting API Changes in Python\nDESCRIPTION: Shows the syntax changes for the backtesting functionality that was moved inside ForecastingModel and RegressionModel classes. The old module-level functions were replaced with instance methods.\nSOURCE: https://github.com/unit8co/darts/blob/master/CHANGELOG.md#2025-04-17_snippet_40\n\nLANGUAGE: python\nCODE:\n```\n# old syntax:\nbacktest_forecasting(forecasting_model, *args, **kwargs)\n\n# new syntax:\nforecasting_model.backtest(*args, **kwargs)\n\n# old syntax:\nbacktest_regression(regression_model, *args, **kwargs)\n\n# new syntax:\nregression_model.backtest(*args, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Using Mapper for Simple Transformations\nDESCRIPTION: Creates a linear time series and applies a square function to each value using the Mapper transformer, then plots both the original and transformed series.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/02-data-processing.ipynb#2025-04-17_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nlin_series = linear_timeseries(start_value=0, end_value=2, length=10)\n\nsquarer = Mapper(lambda x: x**2)\nsquared = squarer.transform(lin_series)\n\nlin_series.plot(label=\"original\")\nsquared.plot(label=\"squared\")\nplt.legend()\n```\n\n----------------------------------------\n\nTITLE: Creating and Training DeepTCN Model with Energy Production Data in Python\nDESCRIPTION: This snippet defines a TCNModel with specific hyperparameters for the energy production data. It uses quantile regression for probabilistic forecasting and includes model checkpointing.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/09-DeepTCN-examples.ipynb#2025-04-17_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmodel_name = \"TCN_test\"\ndeeptcn = TCNModel(\n    dropout=0.2,\n    batch_size=32,\n    n_epochs=50,\n    optimizer_kwargs={\"lr\": 1e-3},\n    random_state=0,\n    input_chunk_length=300,\n    output_chunk_length=30,\n    kernel_size=3,\n    num_filters=4,\n    likelihood=QuantileRegression(),\n    model_name=model_name,\n    save_checkpoints=True,\n    force_reset=True,\n    **generate_torch_kwargs(),\n)\n\ndeeptcn.fit(\n    series=train_en_transformed,\n    past_covariates=day_series,\n    val_series=val_en_transformed,\n    val_past_covariates=day_series,\n)\n```\n\n----------------------------------------\n\nTITLE: Quantile Regression with Linear Regression Model in Darts\nDESCRIPTION: This code demonstrates how to implement quantile regression with a LinearRegressionModel in Darts. The model is configured to estimate multiple quantiles (0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95) using 30 lags as features. The model generates 500 forecast samples to represent the distribution of possible outcomes.\nSOURCE: https://github.com/unit8co/darts/blob/master/docs/userguide/forecasting_overview.md#2025-04-17_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom darts.datasets import AirPassengersDataset\nfrom darts import TimeSeries\nfrom darts.models import LinearRegressionModel\n\nseries = AirPassengersDataset().load()\ntrain, val = series[:-36], series[-36:]\n\nmodel = LinearRegressionModel(lags=30,\n                              likelihood=\"quantile\",\n                              quantiles=[0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95])\nmodel.fit(train)\npred = model.predict(n=36, num_samples=500)\n\nseries.plot()\npred.plot(label='forecast')\n```\n\n----------------------------------------\n\nTITLE: Passing kwargs to ExponentialSmoothing model in Darts\nDESCRIPTION: Using the kwargs dict to pass additional parameters to the underlying statsmodels.tsa.holtwinters.ExponentialSmoothing model constructor.\nSOURCE: https://github.com/unit8co/darts/blob/master/CHANGELOG.md#2025-04-17_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nExponentialSmoothing(kwargs={...})\n```\n\n----------------------------------------\n\nTITLE: Concatenating Temperature Data with Forecasts for Predictions\nDESCRIPTION: This snippet shows how to append temperature forecasts to historical temperature data to enable multi-step predictions when using past covariates in a Darts model.\nSOURCE: https://github.com/unit8co/darts/blob/master/docs/userguide/torch_forecasting_models.md#2025-04-17_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntemperature = temperature.concatenate(temperature_forecast, axis=0)\n```\n\n----------------------------------------\n\nTITLE: Creating Time Series from DataFrame\nDESCRIPTION: Demonstrates how to create a TimeSeries object from a Pandas DataFrame and split it into training and validation sets.\nSOURCE: https://github.com/unit8co/darts/blob/master/README.md#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom darts import TimeSeries\n\n# Read a pandas DataFrame\ndf = pd.read_csv(\"AirPassengers.csv\", delimiter=\",\")\n\n# Create a TimeSeries, specifying the time and value columns\nseries = TimeSeries.from_dataframe(df, \"Month\", \"#Passengers\")\n\n# Set aside the last 36 months as a validation series\ntrain, val = series[:-36], series[-36:]\n```\n\n----------------------------------------\n\nTITLE: Early Stopping Callback Implementation\nDESCRIPTION: Shows how to implement early stopping in Darts models using PyTorch Lightning callbacks to prevent overfitting.\nSOURCE: https://github.com/unit8co/darts/blob/master/docs/userguide/torch_forecasting_models.md#2025-04-17_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom pytorch_lightning.callbacks import EarlyStopping\nfrom torchmetrics import MeanAbsolutePercentageError\n\nfrom darts.dataprocessing.transformers import Scaler\nfrom darts.datasets import AirPassengersDataset\nfrom darts.models import NBEATSModel\n\n# read data\nseries = AirPassengersDataset().load()\n\n# create training and validation sets:\ntrain, val = series.split_after(pd.Timestamp(year=1957, month=12, day=1))\n\n# normalize the time series\ntransformer = Scaler()\ntrain = transformer.fit_transform(train)\nval = transformer.transform(val)\n\n# any TorchMetric or val_loss can be used as the monitor\ntorch_metrics = MeanAbsolutePercentageError()\n\n# early stop callback\nmy_stopper = EarlyStopping(\n    monitor=\"val_MeanAbsolutePercentageError\",  # \"val_loss\",\n    patience=5,\n    min_delta=0.05,\n    mode='min',\n)\npl_trainer_kwargs = {\"callbacks\": [my_stopper]}\n\n# create the model\nmodel = NBEATSModel(\n    input_chunk_length=24,\n    output_chunk_length=12,\n    n_epochs=500,\n    torch_metrics=torch_metrics,\n    pl_trainer_kwargs=pl_trainer_kwargs)\n\n# use validation set for early stopping\nmodel.fit(\n    series=train,\n    val_series=val,\n)\n```\n\n----------------------------------------\n\nTITLE: Loading Best N-BEATS Interpretable Model Checkpoint\nDESCRIPTION: Loads the best performing N-BEATS interpretable model based on validation performance from saved checkpoints. This ensures optimal model performance for forecasting with interpretable components.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/07-NBEATS-examples.ipynb#2025-04-17_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nmodel_nbeats = NBEATSModel.load_from_checkpoint(model_name=model_name, best=True)\n```\n\n----------------------------------------\n\nTITLE: Updated ForecastingModel Fit Method Syntax\nDESCRIPTION: Demonstrates the new syntax for fitting forecasting models using TimeSeries indexing instead of additional parameters. Shows both multivariate and univariate examples.\nSOURCE: https://github.com/unit8co/darts/blob/master/CHANGELOG.md#2025-04-17_snippet_41\n\nLANGUAGE: python\nCODE:\n```\n# old syntax:\nmultivariate_model.fit(multivariate_series, target_indices=[0, 1])\n\n# new syntax:\nmultivariate_model.fit(multivariate_series, multivariate_series[[\"0\", \"1\"]])\n\n# old syntax:\nunivariate_model.fit(multivariate_series, component_index=2)\n\n# new syntax:\nunivariate_model.fit(multivariate_series[\"2\"])\n```\n\n----------------------------------------\n\nTITLE: Using Different Interpolation Methods\nDESCRIPTION: Demonstrates using a different interpolation method with MissingValuesFiller by passing parameters to the transform method. This example uses quadratic interpolation.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/02-data-processing.ipynb#2025-04-17_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfilled = filler.transform(incomplete_series, method=\"quadratic\")\nfilled.plot()\n```\n\n----------------------------------------\n\nTITLE: Partial Inverse Transform with Non-Invertible Transformers\nDESCRIPTION: Demonstrates how to use the partial parameter with inverse_transform to skip non-invertible transformers in a Pipeline, allowing partial restoration of the original data.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/02-data-processing.ipynb#2025-04-17_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nback = pipeline.inverse_transform(transformed, partial=True)\n```\n\n----------------------------------------\n\nTITLE: Converting TimeSeries from/to JSON\nDESCRIPTION: Methods for converting TimeSeries objects to and from JSON strings, showing the new to_json() and from_json() methods added to the TimeSeries class.\nSOURCE: https://github.com/unit8co/darts/blob/master/CHANGELOG.md#2025-04-17_snippet_37\n\nLANGUAGE: python\nCODE:\n```\nTimeSeries.to_json()\nTimeSeries.from_json()\n```\n\n----------------------------------------\n\nTITLE: Loading Best Checkpoint of DeepTCN Model in Python\nDESCRIPTION: This snippet loads the best performing checkpoint of the trained DeepTCN model for energy production forecasting.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/09-DeepTCN-examples.ipynb#2025-04-17_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndeeptcn = TCNModel.load_from_checkpoint(model_name=model_name, best=True)\n```\n\n----------------------------------------\n\nTITLE: Limiting Training Samples per Time Series in Darts\nDESCRIPTION: This snippet shows how to limit the number of training sequences per time series when training a model, using only the most recent sequences closest to the target end.\nSOURCE: https://github.com/unit8co/darts/blob/master/docs/userguide/torch_forecasting_models.md#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# fit only on the 10 \"most recent\" sequences\nmodel.fit(target, max_samples_per_ts=10)\n```\n\n----------------------------------------\n\nTITLE: Setup for Multi-GPU Training with Darts\nDESCRIPTION: Shows the required script structure for enabling multi-GPU training with the 'ddp_spawn' distribution strategy, ensuring proper multiprocessing initialization.\nSOURCE: https://github.com/unit8co/darts/blob/master/docs/userguide/gpu_and_tpu_usage.md#2025-04-17_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nif __name__ == '__main__':\n\n    torch.multiprocessing.freeze_support()\n```\n\n----------------------------------------\n\nTITLE: Creating Visualization Function for Time Series Forecasts in Python\nDESCRIPTION: Defines a helper function to display time series forecasts alongside actual values. The function plots the actual time series data and predicted values, calculates and displays the R2 score, and allows filtering by start date.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/07-NBEATS-examples.ipynb#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef display_forecast(pred_series, ts_transformed, forecast_type, start_date=None):\n    plt.figure(figsize=(8, 5))\n    if start_date:\n        ts_transformed = ts_transformed.drop_before(start_date)\n    ts_transformed.univariate_component(0).plot(label=\"actual\")\n    pred_series.plot(label=(\"historic \" + forecast_type + \" forecasts\"))\n    plt.title(f\"R2: {r2_score(ts_transformed.univariate_component(0), pred_series)}\")\n    plt.legend()\n```\n\n----------------------------------------\n\nTITLE: Creating custom RNN modules for RNNModel and BlockRNNModel in Darts\nDESCRIPTION: Custom modules for defining RNN structures that can be used with RNNModel and BlockRNNModel. The custom model must be a subclass of these modules.\nSOURCE: https://github.com/unit8co/darts/blob/master/CHANGELOG.md#2025-04-17_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nCustomRNNModule\nCustomBlockRNNModule\n```\n\n----------------------------------------\n\nTITLE: Scaling Time Series Data Using MinMaxScaler\nDESCRIPTION: Applies data scaling to the train, validation, and test datasets using Darts' Scaler, which defaults to sklearn's MinMaxScaler. The scaler is fit on the training data to avoid information leakage.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/18-TiDE-examples.ipynb#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nscaler = Scaler()  # default uses sklearn's MinMaxScaler\ntrain = scaler.fit_transform(train)\nval = scaler.transform(val)\ntest = scaler.transform(test)\n```\n\n----------------------------------------\n\nTITLE: Stacking Multiple Covariates in Darts\nDESCRIPTION: Shows how to combine multiple covariate TimeSeries objects into a single covariate using either stack() or concatenate() methods.\nSOURCE: https://github.com/unit8co/darts/blob/master/docs/userguide/covariates.md#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# stack two TimeSeries with stack()\npast_covariates = past_covariates.stack(other_past_covariates)\n\n# or with concatenate()\nfrom darts import concatenate\npast_covariates = concatenate([past_covariates, other_past_covariates], axis=1)\n```\n\n----------------------------------------\n\nTITLE: Setting DataLoader parameters with TorchForecastingModel\nDESCRIPTION: Example showing how to use the new dataloader_kwargs parameter to customize PyTorch DataLoader configuration in TorchForecastingModel methods.\nSOURCE: https://github.com/unit8co/darts/blob/master/CHANGELOG.md#2025-04-17_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Customizing DataLoader settings\nmodel.fit(\n    series=train_series,\n    dataloader_kwargs={\"num_workers\": 4, \"pin_memory\": True, \"prefetch_factor\": 2}\n)\n```\n\n----------------------------------------\n\nTITLE: Preparing Daily Energy Production Data for DeepTCN Model in Python\nDESCRIPTION: This snippet loads and preprocesses daily energy production data from the EnergyDataset. It includes data scaling and adding day-of-week as a covariate using one-hot encoding.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/09-DeepTCN-examples.ipynb#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndf3 = EnergyDataset().load().to_dataframe()\ndf3_day_avg = (\n    df3.groupby(df3.index.astype(str).str.split(\" \").str[0]).mean().reset_index()\n)\nseries_en = fill_missing_values(\n    TimeSeries.from_dataframe(\n        df3_day_avg, \"time\", [\"generation hydro run-of-river and poundage\"]\n    ),\n    \"auto\",\n)\n\n# scale\nscaler_en = Scaler()\ntrain_en, val_en = series_en.split_after(pd.Timestamp(\"20170901\"))\ntrain_en_transformed = scaler_en.fit_transform(train_en)\nval_en_transformed = scaler_en.transform(val_en)\nseries_en_transformed = scaler_en.transform(series_en)\n\n# add the day as a covariate (no scaling required, as one-hot-encoded)\nday_series = datetime_attribute_timeseries(\n    series_en_transformed, attribute=\"day\", one_hot=True\n)\n\nplt.figure(figsize=(10, 3))\ntrain_en_transformed.plot(label=\"train\")\nval_en_transformed.plot(label=\"validation\")\n```\n\n----------------------------------------\n\nTITLE: Passing additional parameters to historical_forecasts, backtest, and gridsearch in Darts\nDESCRIPTION: Using fit_kwargs and predict_kwargs to pass additional parameters to model's fit() and predict() methods during evaluation procedures.\nSOURCE: https://github.com/unit8co/darts/blob/master/CHANGELOG.md#2025-04-17_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nmodel.historical_forecasts(series, fit_kwargs={...}, predict_kwargs={...})\nmodel.backtest(series, fit_kwargs={...}, predict_kwargs={...})\nmodel.gridsearch(series, fit_kwargs={...}, predict_kwargs={...})\n```\n\n----------------------------------------\n\nTITLE: Generating Variable Noise Series for DeepTCN Model in Python\nDESCRIPTION: This snippet creates a variable noise series using various time series generation functions from the Darts library. It combines trend, seasonality, and modulated noise components.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/09-DeepTCN-examples.ipynb#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nlength = 400\ntrend = tg.linear_timeseries(length=length, end_value=4)\nseason1 = tg.sine_timeseries(length=length, value_frequency=0.05, value_amplitude=1.0)\nnoise = tg.gaussian_timeseries(length=length, std=0.6)\nnoise_modulator = (\n    tg.sine_timeseries(length=length, value_frequency=0.02)\n    + tg.constant_timeseries(length=length, value=1)\n) / 2\nnoise = noise * noise_modulator\n\ntarget_series = sum([noise, season1])\ncovariates = noise_modulator\ntarget_train, target_val = target_series.split_after(0.65)\n\nplt.figure(figsize=(10, 3))\ntarget_train.plot()\ntarget_val.plot()\n```\n\n----------------------------------------\n\nTITLE: Evaluating Transformer Model on Air Passengers Dataset in Python\nDESCRIPTION: This function evaluates the Transformer model on a given validation set for a specified number of time steps. It plots the actual vs. forecasted values and calculates the MAPE.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/06-Transformer-examples.ipynb#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef eval_model(model, n, series, val_series):\n    pred_series = model.predict(n=n)\n    plt.figure(figsize=(8, 5))\n    series.plot(label=\"actual\")\n    pred_series.plot(label=\"forecast\")\n    plt.title(f\"MAPE: {mape(pred_series, val_series):.2f}%\")\n    plt.legend()\n```\n\n----------------------------------------\n\nTITLE: TPU Installation for Google Colab\nDESCRIPTION: Installs necessary dependencies to use TPUs in Google Colab, including cloud-tpu-client, PyTorch XLA, and compatible versions of PyTorch and related libraries.\nSOURCE: https://github.com/unit8co/darts/blob/master/docs/userguide/gpu_and_tpu_usage.md#2025-04-17_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n!pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchtext==0.10.0 -f https://download.pytorch.org/whl/cu111/torch_stable.html\n!pip install pyyaml==5.4.1\n```\n\n----------------------------------------\n\nTITLE: Preparing Monthly Sunspots Dataset for RNN Model in Python\nDESCRIPTION: This code prepares the Monthly Sunspots dataset for use with an RNN model. It loads the data, checks for seasonality, splits into training and validation sets, and applies scaling.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/04-RNN-examples.ipynb#2025-04-17_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nseries_sunspot = SunspotsDataset().load()\n\nseries_sunspot.plot()\ncheck_seasonality(series_sunspot, max_lag=240)\n\nplot_acf(series_sunspot, 125, max_lag=240)  # ~11 years seasonality\n\ntrain_sp, val_sp = series_sunspot.split_after(pd.Timestamp(\"19401001\"))\n\ntransformer_sunspot = Scaler()\ntrain_sp_transformed = transformer_sunspot.fit_transform(train_sp)\nval_sp_transformed = transformer_sunspot.transform(val_sp)\nseries_sp_transformed = transformer_sunspot.transform(series_sunspot)\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Air Passengers Dataset for TCN Model\nDESCRIPTION: This snippet loads the Air Passengers dataset, splits it into training and validation sets, applies scaling, and creates a month covariate series. It prepares the data for use with the TCN model.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/05-TCN-examples.ipynb#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Read data:\nts = AirPassengersDataset().load()\n\n# Create training and validation sets:\ntrain, val = ts.split_after(pd.Timestamp(\"19580801\"))\n\nscaler = Scaler()\n\ntrain_scaled = scaler.fit_transform(train)\nval_scaled = scaler.transform(val)\nts_scaled = scaler.transform(ts)\n\n# We'll use the month as a covariate (scaling not required as one-hot-encoded)\nmonth_series = datetime_attribute_timeseries(ts, attribute=\"month\", one_hot=True)\n```\n\n----------------------------------------\n\nTITLE: Loading and Preparing Nuclear Energy Generation Dataset\nDESCRIPTION: Loads hourly nuclear energy generation data, handles missing values automatically, and splits it into training and validation sets for model evaluation.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/03-FFT-examples.ipynb#2025-04-17_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nts_3 = EnergyDataset().load()\nts_3 = fill_missing_values(ts_3, \"auto\")\nts_3 = ts_3[\"generation nuclear\"]\ntrain, val = ts_3.split_after(pd.Timestamp(\"2017-07-01\"))\ntrain.plot(label=\"train\")\nval.plot(label=\"val\")\n```\n\n----------------------------------------\n\nTITLE: Using TimeSeries method with_times_and_values\nDESCRIPTION: Example of the new TimeSeries method that creates a new series with updated time index and values while preserving the original metadata, columns, and hierarchy information.\nSOURCE: https://github.com/unit8co/darts/blob/master/CHANGELOG.md#2025-04-17_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# New TimeSeries method example\nnew_series = original_series.with_times_and_values(new_time_index, new_values)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Model Performance Comparison with Bar Charts\nDESCRIPTION: Creates a bar chart to visually compare the MAE and MSE metrics of the different forecasting models (NHiTS, TiDE, and TiDE+RIN), showing that TiDE with Reversible Instance Normalization performs best in this example.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/18-TiDE-examples.ipynb#2025-04-17_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nresults_df = pd.DataFrame.from_dict(result_accumulator, orient=\"index\")\nresults_df.plot.bar()\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning Darts Model with Custom Learning Rate Scheduler\nDESCRIPTION: Example demonstrating how to fine-tune a model using a custom learning rate scheduler and loading weights from a manual save.\nSOURCE: https://github.com/unit8co/darts/blob/master/docs/userguide/torch_forecasting_models.md#2025-04-17_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nmodel_finetune = SomeTorchForecastingModel(...,\n                                           lr_scheduler_cls=torch.optim.lr_scheduler.ExponentialLR,\n                                           lr_scheduler_kwargs={\"gamma\": 0.09})\n\n# load the weights from a manual save\nmodel_finetune.load_weights(\"/your/path/to/save/model.pt\")\n```\n\n----------------------------------------\n\nTITLE: Using plot_ccf to analyze cross-correlation between time series in Python\nDESCRIPTION: Function for plotting cross correlation between a time series (e.g. target series) and the lagged values of another time series (e.g. covariates series).\nSOURCE: https://github.com/unit8co/darts/blob/master/CHANGELOG.md#2025-04-17_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndarts.utils.statistics.plot_ccf\n```\n\n----------------------------------------\n\nTITLE: Getting Quantile Estimator from RegressionModel\nDESCRIPTION: Shows how to retrieve a specific quantile estimator from a probabilistic regression model using the new quantile parameter in get_estimator().\nSOURCE: https://github.com/unit8co/darts/blob/master/CHANGELOG.md#2025-04-17_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nquantile_estimator = model.get_estimator(quantile=0.95)\n```\n\n----------------------------------------\n\nTITLE: Using Prophet Model with Future Covariates\nDESCRIPTION: Example showing how to fit and predict using the Prophet model with future covariates and stochastic forecasting capabilities\nSOURCE: https://github.com/unit8co/darts/blob/master/CHANGELOG.md#2025-04-17_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nmodel.fit(train, future_covariates=train_covariates)\nmodel.predict(n=len(test), num_sample=1, future_covariates=test_covariates)\nmodel.predict(n=len(test), num_samples=200)\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for DeepTCN Model in Python\nDESCRIPTION: This snippet imports necessary libraries and modules for implementing the DeepTCN model, including Darts components and utility functions.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/09-DeepTCN-examples.ipynb#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom utils import fix_pythonpath_if_working_locally\n\nfix_pythonpath_if_working_locally()\n%matplotlib inline\n```\n\nLANGUAGE: python\nCODE:\n```\nimport warnings\n\nimport pandas as pd\n\nimport darts.utils.timeseries_generation as tg\nfrom darts import TimeSeries, concatenate\nfrom darts.dataprocessing.transformers import Scaler\nfrom darts.datasets import EnergyDataset\nfrom darts.models import TCNModel\nfrom darts.utils.callbacks import TFMProgressBar\nfrom darts.utils.likelihood_models.torch import GaussianLikelihood, QuantileRegression\nfrom darts.utils.missing_values import fill_missing_values\nfrom darts.utils.timeseries_generation import datetime_attribute_timeseries\n\nwarnings.filterwarnings(\"ignore\")\nimport logging\n\nlogging.disable(logging.CRITICAL)\n\nimport matplotlib.pyplot as plt\n\n\ndef generate_torch_kwargs():\n    # run torch models on CPU, and disable progress bars for all model stages except training.\n    return {\n        \"pl_trainer_kwargs\": {\n            \"accelerator\": \"cpu\",\n            \"callbacks\": [TFMProgressBar(enable_train_bar_only=True)],\n        }\n    }\n```\n\n----------------------------------------\n\nTITLE: Adding Encoders Configuration for Torch-based Models in Python\nDESCRIPTION: Example configuration for on-the-fly encoding of position and calendar information in Torch-based models. This shows how to use various encoders including cyclic, datetime attributes, position, custom functions, and transformers as covariates.\nSOURCE: https://github.com/unit8co/darts/blob/master/CHANGELOG.md#2025-04-17_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nfrom darts.dataprocessing.transformers import Scaler\nadd_encoders={\n    'cyclic': {'future': ['month']},\n    'datetime_attribute': {'past': ['hour', 'dayofweek']},\n    'position': {'past': ['absolute'], 'future': ['relative']},\n    'custom': {'past': [lambda idx: (idx.year - 1950) / 50]},\n    'transformer': Scaler()\n}\n```\n\n----------------------------------------\n\nTITLE: Loading and Plotting Monthly Milk Dataset\nDESCRIPTION: Loads the Monthly Milk dataset using Darts' built-in dataset loader and displays basic information about the time series along with a plot of the data.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/02-data-processing.ipynb#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nseries = MonthlyMilkDataset().load()\n\nprint(series)\nseries.plot()\n```\n\n----------------------------------------\n\nTITLE: Visualizing FFT Model Performance on Trended Data\nDESCRIPTION: Plots the results of the basic FFT model on the Air Passengers dataset, showing how it fails to capture the upward trend in the data.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/03-FFT-examples.ipynb#2025-04-17_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ntrain.plot(label=\"train\")\nval.plot(label=\"val\")\npred_val.plot(label=\"prediction\")\n```\n\n----------------------------------------\n\nTITLE: Preparing Time Series Data for Forecasting in Darts\nDESCRIPTION: Loads the Air Passengers dataset, converts it to float32, splits it into training and validation sets, and applies normalization using a Scaler transformer.\nSOURCE: https://github.com/unit8co/darts/blob/master/docs/userguide/gpu_and_tpu_usage.md#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Read data:\nseries = AirPassengersDataset().load()\nseries = series.astype(np.float32)\n\n# Create training and validation sets:\ntrain, val = series.split_after(pd.Timestamp(\"19590101\"))\n\n# Normalize the time series (note: we avoid fitting the transformer on the validation set)\ntransformer = Scaler()\ntrain_transformed = transformer.fit_transform(train)\nval_transformed = transformer.transform(val)\nseries_transformed = transformer.transform(series)\n```\n\n----------------------------------------\n\nTITLE: Loading a new electricity consumption dataset in Darts\nDESCRIPTION: Using the ElectricityConsumptionZurichDataset which contains electricity consumption data from Zurich households from 2015-2022 along with weather measurements as covariates.\nSOURCE: https://github.com/unit8co/darts/blob/master/CHANGELOG.md#2025-04-17_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nElectricityConsumptionZurichDataset\n```\n\n----------------------------------------\n\nTITLE: Using TFMProgressBar callback to customize progress bar display in Darts\nDESCRIPTION: A callback for TorchForecastingModel that allows customization of when to display the progress bar during different model stages.\nSOURCE: https://github.com/unit8co/darts/blob/master/CHANGELOG.md#2025-04-17_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndarts.utils.callbacks.TFMProgressBar\n```\n\n----------------------------------------\n\nTITLE: Creating Untrained Model Instance\nDESCRIPTION: Illustrates how to create a new untrained model instance with the same parameters using the newly public untrained_model() method.\nSOURCE: https://github.com/unit8co/darts/blob/master/CHANGELOG.md#2025-04-17_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nnew_model = model.untrained_model()\n```\n\n----------------------------------------\n\nTITLE: Using MIDAS data transformer for frequency conversion in Darts\nDESCRIPTION: Data transformer that uses mixed-data sampling to convert TimeSeries from high frequency to low frequency and back.\nSOURCE: https://github.com/unit8co/darts/blob/master/CHANGELOG.md#2025-04-17_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nMIDAS\n```\n\n----------------------------------------\n\nTITLE: Generating a Noisy Sine Wave for Filtering\nDESCRIPTION: Creates a sine wave time series and adds Gaussian noise to it. The original and noisy signals are then plotted for comparison. This serves as the input data for testing Gaussian Process filtering.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/11-GP-filter-examples.ipynb#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nNOISE_DISTANCE = 0.4\nSAMPLE_SIZE = 200\nnp.random.seed(42)\n\n# Prepare the sine wave\nx = tg.sine_timeseries(length=SAMPLE_SIZE, value_frequency=0.025)\n\n# Add white noise\nnoise = tg.gaussian_timeseries(length=SAMPLE_SIZE, std=NOISE_DISTANCE)\nx_noise = x + noise\n\nplt.figure(figsize=[12, 8])\nx.plot(label=\"Original sine wave\")\nx_noise.plot(color=\"red\", label=\"Noisy sine wave\")\nplt.legend()\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for Gaussian Process Filtering\nDESCRIPTION: Imports necessary libraries including matplotlib for visualization, numpy for numerical operations, and components from the Darts library for time series processing and Gaussian Process filtering.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/11-GP-filter-examples.ipynb#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.gaussian_process.kernels import ExpSineSquared\n\nfrom darts import TimeSeries\nfrom darts.models import GaussianProcessFilter\nfrom darts.utils import timeseries_generation as tg\n```\n\n----------------------------------------\n\nTITLE: Loading Dataset in Darts\nDESCRIPTION: Example showing how to load a built-in dataset (AirPassengers) using the darts.datasets module. The dataset is loaded into a TimeSeries object.\nSOURCE: https://github.com/unit8co/darts/blob/master/CHANGELOG.md#2025-04-17_snippet_36\n\nLANGUAGE: python\nCODE:\n```\nfrom darts.datasets import AirPassengers\nts: TimeSeries = AirPassengers().load()\n```\n\n----------------------------------------\n\nTITLE: Using Historical Forecasts with EnsembleModel\nDESCRIPTION: Configuration for using historical forecasts instead of direct predictions when training ensemble models\nSOURCE: https://github.com/unit8co/darts/blob/master/CHANGELOG.md#2025-04-17_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nmodel = EnsembleModel(forecasting_models=[model1, model2], train_using_historical_forecasts=True)\n```\n\n----------------------------------------\n\nTITLE: Monitoring Transformations with Progress Bars in Darts\nDESCRIPTION: This snippet demonstrates how to enable progress monitoring when transforming large datasets using the Scaler transformer in Darts. By setting the verbose parameter to True, the transformation process displays progress bars.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/02-data-processing.ipynb#2025-04-17_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nseries = MonthlyMilkIncompleteDataset().load()\n\nhuge_number_of_series = [series] * 10000\n\nscaler = Scaler(verbose=True, name=\"Basic\")\n\ntransformed = scaler.fit_transform(huge_number_of_series)\n```\n\n----------------------------------------\n\nTITLE: Binary Anomaly Detection\nDESCRIPTION: Demonstrates how to convert anomaly scores into binary anomaly classifications using quantile-based detection.\nSOURCE: https://github.com/unit8co/darts/blob/master/README.md#2025-04-17_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom darts.ad import QuantileDetector\n\ndetector = QuantileDetector(high_quantile=0.99)\ndetector.fit(scorer.score(train))\nbinary_anom = detector.detect(anom_score)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Jupyter Environment and Importing Dependencies\nDESCRIPTION: Configures Jupyter notebook settings and imports required libraries. Includes data handling with pandas, time series models from Darts, and suppresses warnings and logging.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/03-FFT-examples.ipynb#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\n%autoreload 2\n%matplotlib inline\nimport warnings\n\nimport pandas as pd\n\nfrom darts.datasets import AirPassengersDataset, EnergyDataset, TemperatureDataset\nfrom darts.metrics import mae\nfrom darts.models import FFT, ExponentialSmoothing, Theta\nfrom darts.utils.missing_values import fill_missing_values\n\nwarnings.filterwarnings(\"ignore\")\nimport logging\n\nlogging.disable(logging.CRITICAL)\n```\n\n----------------------------------------\n\nTITLE: Comparing Forecast Results After Inverse Transform\nDESCRIPTION: Plots the inverse-transformed forecast against the actual values and calculates the MAPE to evaluate model performance after the complete transform-predict-inverse workflow.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/02-data-processing.ipynb#2025-04-17_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nplt.title(f\"MAPE = {mape(forecast, validation):.2f}%\")\nseries.plot(label=\"actual\")\nforecast.plot(label=\"forecast\")\nplt.legend()\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading Regression Models\nDESCRIPTION: Demonstrates how to save and load a RegressionModel using pickle serialization. Shows the basic model persistence workflow using save() and load() methods.\nSOURCE: https://github.com/unit8co/darts/blob/master/docs/userguide/forecasting_overview.md#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom darts.models import RegressionModel\n\nmodel = RegressionModel(lags=4)\n\nmodel.save(\"my_model.pkl\")\nmodel_loaded = RegressionModel.load(\"my_model.pkl\")\n```\n\n----------------------------------------\n\nTITLE: Filling Missing Values in TimeSeries\nDESCRIPTION: Updated API for handling missing values in time series data, showing the consolidated fill_missing_values() function that replaces the previous fillna() and auto_fillna() functions.\nSOURCE: https://github.com/unit8co/darts/blob/master/CHANGELOG.md#2025-04-17_snippet_39\n\nLANGUAGE: python\nCODE:\n```\n#old syntax\nfillna(series, fill=0)\n\n#new syntax\nfill_missing_values(series, fill=0)\n\n#old syntax\nauto_fillna(series, **interpolate_kwargs)\n\n#new syntax\nfill_missing_values(series, fill='auto', **interpolate_kwargs)\nfill_missing_values(series, **interpolate_kwargs) # fill='auto' by default\n```\n\n----------------------------------------\n\nTITLE: Loading and Running ONNX Model Inference\nDESCRIPTION: Example of loading an exported ONNX model and running inference with it using feature preparation utilities.\nSOURCE: https://github.com/unit8co/darts/blob/master/docs/userguide/torch_forecasting_models.md#2025-04-17_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Optional\nimport onnx\nimport onnxruntime as ort\nimport numpy as np\nfrom darts import TimeSeries\nfrom darts.utils.onnx_utils.py import prepare_onnx_inputs\n\nonnx_model = onnx.load(onnx_filename)\nonnx.checker.check_model(onnx_model)\nort_session = ort.InferenceSession(onnx_filename)\n\n# use helper function to extract the features from the series\npast_feats, future_feats, static_feats = prepare_onnx_inputs(\n    model=model,\n    series=series,\n    past_covariates=ts_past,\n    future_covariates=ts_future,\n)\n\n# extract only the features expected by the model\nort_inputs = {}\nfor name, arr in zip(['x_past', 'x_future', 'x_static'], [past_feats, future_feats, static_feats]):\n    if name in [inp.name for inp in list(ort_session.get_inputs())]:\n        ort_inputs[name] = arr\n\n# output has shape (batch, output_chunk_length, n components, 1 or n likelihood params)\nort_out = ort_session.run(None, ort_inputs)\n```\n\n----------------------------------------\n\nTITLE: Defining Hierarchical Time Series Structure in Python\nDESCRIPTION: Shows how to specify a hierarchy for TimeSeries components using a dictionary structure. The example demonstrates mapping child components 'a' and 'b' to their parent 'total'.\nSOURCE: https://github.com/unit8co/darts/blob/master/docs/userguide/timeseries.md#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nhierarchy = {\"a\": [\"total\"], \"b\": [\"total\"]}\n```\n\n----------------------------------------\n\nTITLE: Prophet Model with Custom Seasonality\nDESCRIPTION: Demonstrates how to create and configure a Prophet model with custom seasonality parameters\nSOURCE: https://github.com/unit8co/darts/blob/master/CHANGELOG.md#2025-04-17_snippet_32\n\nLANGUAGE: python\nCODE:\n```\nProphet(add_seasonality=kwargs_dict)\nmodel.add_seasonality(kwargs)\n```\n\n----------------------------------------\n\nTITLE: Using Static Covariates Option\nDESCRIPTION: Example of the new use_static_covariates parameter implementation in regression and torch models.\nSOURCE: https://github.com/unit8co/darts/blob/master/CHANGELOG.md#2025-04-17_snippet_27\n\nLANGUAGE: Python\nCODE:\n```\nmodel = RegressionModel(use_static_covariates=True)\n# or\nmodel = TFTModel(use_static_covariates=False)\n```\n\n----------------------------------------\n\nTITLE: Setting up Conda Environment for Darts\nDESCRIPTION: Commands to create and activate a conda environment for Darts installation\nSOURCE: https://github.com/unit8co/darts/blob/master/INSTALL.md#2025-04-17_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nconda create --name <env-name> python=3.10\nconda activate <env-name>\n```\n\n----------------------------------------\n\nTITLE: Loading and Splitting Air Passengers Dataset\nDESCRIPTION: Loads the Air Passengers dataset which exhibits an upward trend, and splits it into training and validation sets at December 1955 to demonstrate trend handling.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/03-FFT-examples.ipynb#2025-04-17_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nts_2 = AirPassengersDataset().load()\ntrain, val = ts_2.split_after(pd.Timestamp(\"19551201\"))\ntrain.plot(label=\"train\")\nval.plot(label=\"val\")\n```\n\n----------------------------------------\n\nTITLE: Using RINorm in TorchForecastingModel\nDESCRIPTION: Implementation of Reversible Instance Normalization as an input normalization option for torch forecasting models\nSOURCE: https://github.com/unit8co/darts/blob/master/CHANGELOG.md#2025-04-17_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nmodel = TorchForecastingModel(use_reversible_instance_norm=True)\n```\n\n----------------------------------------\n\nTITLE: Splitting Time Series Data into Training and Validation Sets\nDESCRIPTION: Splits the temperature time series data into training and validation sets at a specific timestamp (July 1, 1985) and plots both segments.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/03-FFT-examples.ipynb#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntrain, val = ts.split_after(pd.Timestamp(\"19850701\"))\ntrain.plot(label=\"train\")\nval.plot(label=\"val\")\n```\n\n----------------------------------------\n\nTITLE: Creating an RNN Forecasting Model with CPU in Darts\nDESCRIPTION: Configures an RNN model with specific hyperparameters for time series forecasting, setting up model architecture, training parameters, and logging options to run on CPU.\nSOURCE: https://github.com/unit8co/darts/blob/master/docs/userguide/gpu_and_tpu_usage.md#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmy_model = RNNModel(\n    model=\"RNN\",\n    hidden_dim=20,\n    dropout=0,\n    batch_size=16,\n    n_epochs=300,\n    optimizer_kwargs={\"lr\": 1e-3},\n    model_name=\"Air_RNN\",\n    log_tensorboard=True,\n    random_state=42,\n    training_length=20,\n    input_chunk_length=14,\n    force_reset=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing Python Environment with Local Path Fix\nDESCRIPTION: Sets up the Python environment by fixing the Python path for local development and enabling inline matplotlib plotting and autoreload functionality.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/18-TiDE-examples.ipynb#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# fix python path if working locally\nfrom utils import fix_pythonpath_if_working_locally\n\nfix_pythonpath_if_working_locally()\n%matplotlib inline\n```\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\n%autoreload 2\n%matplotlib inline\n```\n\n----------------------------------------\n\nTITLE: Setting Colors for TimeSeries Components in Plot\nDESCRIPTION: Illustrates how to set specific colors for each component when plotting a TimeSeries object using the new color parameter.\nSOURCE: https://github.com/unit8co/darts/blob/master/CHANGELOG.md#2025-04-17_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nts.plot(c=[\"red\", \"blue\", \"green\"])\n```\n\n----------------------------------------\n\nTITLE: Applying Map Function on TimeSeries\nDESCRIPTION: Demonstration of the updated map() functionality for TimeSeries objects, showing how to apply functions that transform time series data using both old and new syntax patterns.\nSOURCE: https://github.com/unit8co/darts/blob/master/CHANGELOG.md#2025-04-17_snippet_38\n\nLANGUAGE: python\nCODE:\n```\n# Assuming a multivariate TimeSeries named series with 3 columns or variables.\n# To apply fn to columns with names '0' and '2':\n\n#old syntax\nseries.map(fn, cols=['0', '2']) # returned a time series with 3 columns\n#new syntax\nseries[['0', '2']].map(fn) # returns a time series with only 2 columns\n```\n\n----------------------------------------\n\nTITLE: Defining PyTorch and Related Dependencies for DARTS Project\nDESCRIPTION: Specifies the minimum required versions of PyTorch-related packages. This includes PyTorch Lightning version 1.5.0 or higher, TensorboardX version 2.1 or higher, and PyTorch version 1.8.0 or higher.\nSOURCE: https://github.com/unit8co/darts/blob/master/requirements/torch.txt#2025-04-17_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\npytorch-lightning>=1.5.0\ntensorboardX>=2.1\ntorch>=1.8.0\n```\n\n----------------------------------------\n\nTITLE: Configuring TiDEModel with Feature Projection\nDESCRIPTION: Updated TiDEModel configuration with separate temporal width parameters for past and future feature projection\nSOURCE: https://github.com/unit8co/darts/blob/master/CHANGELOG.md#2025-04-17_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nmodel = TiDEModel(temporal_width_past=5, temporal_width_future=10)\n```\n\n----------------------------------------\n\nTITLE: Using Multiple Target Series with Global Forecasting Models\nDESCRIPTION: Demonstrates how to use multiple target series with Global Forecasting Models, showing the requirement for matching covariates for each target series.\nSOURCE: https://github.com/unit8co/darts/blob/master/docs/userguide/covariates.md#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# fit using multiple (two) target series\nmodel.fit(target=[target, target_2],\n          past_covariates=[past_covariates, past_covariates_2],\n          # optional future_covariates,\n          )\n\n# you must give the specific target and covariate series that you want to predict\nmodel.predict(n=12,\n              series=target_2,\n              past_covariates=past_covariates_2,\n              # optional future_covariates,\n              )\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Torch Models in Darts\nDESCRIPTION: Imports necessary libraries for time series forecasting using Darts, including data processing utilities, RNN models, evaluation metrics, and dataset loaders.\nSOURCE: https://github.com/unit8co/darts/blob/master/docs/userguide/gpu_and_tpu_usage.md#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nfrom darts.dataprocessing.transformers import Scaler\nfrom darts.models import RNNModel\nfrom darts.metrics import mape\nfrom darts.datasets import AirPassengersDataset\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Transformer Model in Python\nDESCRIPTION: This snippet imports necessary libraries and modules for implementing the Transformer model using Darts. It includes data processing, visualization, and model-related imports.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/06-Transformer-examples.ipynb#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom darts.dataprocessing.transformers import Scaler\nfrom darts.datasets import AirPassengersDataset, SunspotsDataset\nfrom darts.metrics import mape\nfrom darts.models import ExponentialSmoothing, TransformerModel\nfrom darts.utils.statistics import check_seasonality\n\nwarnings.filterwarnings(\"ignore\")\nimport logging\n\nlogging.disable(logging.CRITICAL)\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents in reStructuredText for Darts User Guide\nDESCRIPTION: This code snippet defines the table of contents for the Darts project user guide using reStructuredText directives. It specifies the maximum depth of the table and lists the markdown files to be included in the documentation.\nSOURCE: https://github.com/unit8co/darts/blob/master/docs/source/userguide.rst#2025-04-17_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. toctree::\n   :maxdepth: 2\n\n   userguide/timeseries.md\n\n   userguide/forecasting_overview.md\n\n   userguide/torch_forecasting_models.md\n   userguide/gpu_and_tpu_usage.md\n\n   .. userguide/regression_models.md\n\n   userguide/covariates.md\n\n   userguide/hyperparameter_optimization.md\n\n   .. userguide/probabilistic_forecasting.md\n\n   .. userguide/ensembling.md\n\n   .. userguide/filtering_models.md\n\n   .. userguide/preprocessing_and_pipelines.md\n\n   .. userguide/metrics.md\n\n   .. userguide/hyper_params.md\n\n   userguide/faq.md\n```\n\n----------------------------------------\n\nTITLE: Processing Multiple Time Series with Pipeline\nDESCRIPTION: Shows how to use a Pipeline to process multiple time series at once, applying the same transformations to each series while maintaining separate scaling parameters.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/02-data-processing.ipynb#2025-04-17_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nseries = MonthlyMilkDataset().load()\nincomplete_series = MonthlyMilkIncompleteDataset().load()\n\nmultiple_ts = [incomplete_series, series[:10]]\n\nfiller = MissingValuesFiller()\nscaler = Scaler()\n\npipeline = Pipeline([filler, scaler])\ntransformed = pipeline.fit_transform(multiple_ts)\n\nfor ts in transformed:\n    ts.plot()\n```\n\n----------------------------------------\n\nTITLE: Adding Cyclic DateTime Attributes\nDESCRIPTION: Example of adding cyclic encoding of datetime attributes to a series\nSOURCE: https://github.com/unit8co/darts/blob/master/CHANGELOG.md#2025-04-17_snippet_35\n\nLANGUAGE: python\nCODE:\n```\nmy_series.add_datetime_attribute('weekday', cyclic=True)\n```\n\n----------------------------------------\n\nTITLE: Using TimeSeries method slice_intersect_times\nDESCRIPTION: Example of the new TimeSeries method that returns a sliced time index of a series where the index has been intersected with another series.\nSOURCE: https://github.com/unit8co/darts/blob/master/CHANGELOG.md#2025-04-17_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Intersect times between two series\nsliced_index = series1.slice_intersect_times(series2)\n```\n\n----------------------------------------\n\nTITLE: Importing and Configuring Libraries for N-BEATS Time Series Forecasting\nDESCRIPTION: Imports required libraries for data handling, visualization, and time series forecasting with Darts. Configures warning handling and defines a helper function for PyTorch model settings.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/07-NBEATS-examples.ipynb#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom darts import TimeSeries, concatenate\nfrom darts.dataprocessing.transformers import MissingValuesFiller, Scaler\nfrom darts.datasets import EnergyDataset\nfrom darts.metrics import r2_score\nfrom darts.models import NBEATSModel\nfrom darts.utils.callbacks import TFMProgressBar\n\nwarnings.filterwarnings(\"ignore\")\nimport logging\n\nlogging.disable(logging.CRITICAL)\n\n\ndef generate_torch_kwargs():\n    # run torch models on CPU, and disable progress bars for all model stages except training.\n    return {\n        \"pl_trainer_kwargs\": {\n            \"accelerator\": \"cpu\",\n            \"callbacks\": [TFMProgressBar(enable_train_bar_only=True)],\n        }\n    }\n```\n\n----------------------------------------\n\nTITLE: Importing Multiple Requirement Files in pip\nDESCRIPTION: This code snippet demonstrates how to import multiple requirement files in pip using the -r flag. It imports core dependencies, development dependencies, release dependencies, and configuration options for PyTorch and non-PyTorch environments.\nSOURCE: https://github.com/unit8co/darts/blob/master/requirements/dev-all.txt#2025-04-17_snippet_0\n\nLANGUAGE: pip\nCODE:\n```\n-r core.txt\n-r dev.txt\n-r release.txt\n-r torch.txt\n-r notorch.txt\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Setting Up Environment for N-BEATS in Python\nDESCRIPTION: Sets up the Python environment by fixing the Python path for local work, importing necessary libraries, configuring warning handling, and defining helper functions for PyTorch model configuration.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/07-NBEATS-examples.ipynb#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# fix python path if working locally\nfrom utils import fix_pythonpath_if_working_locally\n\nfix_pythonpath_if_working_locally()\n%matplotlib inline\n```\n\n----------------------------------------\n\nTITLE: Multiple Series Support Check\nDESCRIPTION: Shows how to programmatically check if a model supports training on multiple time series by checking if it inherits from GlobalForecastingModel.\nSOURCE: https://github.com/unit8co/darts/blob/master/docs/userguide/forecasting_overview.md#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom darts.models import RegressionModel\nfrom darts.models.forecasting.forecasting_model import GlobalForecastingModel\n\n# when True, multiple time series are supported\nsupports_multi_ts = issubclass(RegressionModel, GlobalForecastingModel)\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for RNN Models in Python\nDESCRIPTION: This snippet imports necessary libraries and modules for working with RNN models in Darts, including data processing, model creation, and visualization tools.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/04-RNN-examples.ipynb#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom darts.dataprocessing.transformers import Scaler\nfrom darts.datasets import AirPassengersDataset, SunspotsDataset\nfrom darts.metrics import mape\nfrom darts.models import BlockRNNModel, ExponentialSmoothing, RNNModel\nfrom darts.utils.statistics import check_seasonality, plot_acf\nfrom darts.utils.timeseries_generation import datetime_attribute_timeseries\n\nwarnings.filterwarnings(\"ignore\")\nimport logging\n\nlogging.disable(logging.CRITICAL)\n```\n\n----------------------------------------\n\nTITLE: Custom Loss Logging Callback Implementation\nDESCRIPTION: Demonstrates how to create a custom callback for logging training and validation losses during model training.\nSOURCE: https://github.com/unit8co/darts/blob/master/docs/userguide/torch_forecasting_models.md#2025-04-17_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom pytorch_lightning.callbacks import Callback\n\nclass LossLogger(Callback):\n    def __init__(self):\n        self.train_loss = []\n        self.val_loss = []\n\n    # will automatically be called at the end of each epoch\n    def on_train_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\n        self.train_loss.append(float(trainer.callback_metrics[\"train_loss\"]))\n\n    def on_validation_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\n        self.val_loss.append(float(trainer.callback_metrics[\"val_loss\"]))\n\n\nloss_logger = LossLogger()\n\nmodel = SomeTorchForecastingModel(\n    ...,\n    nr_epochs_val_period=1,  # perform validation after every epoch\n    pl_trainer_kwargs={\"callbacks\": [loss_logger]}\n)\n\n# fit must include validation set for \"val_loss\"\nmodel.fit(...)\n```\n\n----------------------------------------\n\nTITLE: Setting Torch Model File Names\nDESCRIPTION: Code pattern showing the change in TorchForecastingModel save file naming convention, replacing ':' with '_' for better OS compatibility.\nSOURCE: https://github.com/unit8co/darts/blob/master/CHANGELOG.md#2025-04-17_snippet_26\n\nLANGUAGE: Python\nCODE:\n```\nmodel_path = model_name.replace(\":\", \"_\")\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Setting Up Environment for Time Series Forecasting\nDESCRIPTION: Imports necessary libraries for time series forecasting including PyTorch, pandas, matplotlib, and Darts components. Disables warnings and logging to keep the notebook output clean.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/18-TiDE-examples.ipynb#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\n\nfrom darts.dataprocessing.transformers.scaler import Scaler\nfrom darts.datasets import AusBeerDataset\nfrom darts.metrics import mae, mse\nfrom darts.models import NHiTSModel, TiDEModel\n\nwarnings.filterwarnings(\"ignore\")\nimport logging\n\nlogging.disable(logging.CRITICAL)\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies for Darts Time Series Library\nDESCRIPTION: Comprehensive list of Python package dependencies required for the Darts time series library, specifying minimum (and sometimes maximum) version requirements. The dependencies include fundamental data science libraries like numpy and pandas, statistical and forecasting packages like statsmodels and statsforecast, machine learning libraries like scikit-learn and xgboost, as well as specialized components for time series analysis.\nSOURCE: https://github.com/unit8co/darts/blob/master/requirements/core.txt#2025-04-17_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nholidays>=0.11.1\njoblib>=0.16.0\nmatplotlib>=3.3.0\nnarwhals>=1.25.1\nnfoursid>=1.0.0\nnumpy>=1.19.0,<2.0.0\npandas>=1.0.5\npyod>=0.9.5\nrequests>=2.22.0\nscikit-learn>=1.6.0\nscipy>=1.3.2\nshap>=0.40.0\nstatsforecast>=1.4\nstatsmodels>=0.14.0\ntbats>=1.1.0\ntqdm>=4.60.0\ntyping-extensions\nxarray>=0.17.0\nxgboost>=2.1.4\n```\n\n----------------------------------------\n\nTITLE: TimeSeries Creation from CSV\nDESCRIPTION: Creating a TimeSeries object directly from a CSV file\nSOURCE: https://github.com/unit8co/darts/blob/master/CHANGELOG.md#2025-04-17_snippet_34\n\nLANGUAGE: python\nCODE:\n```\nTimeSeries.from_csv()\n```\n\n----------------------------------------\n\nTITLE: Using TimeSeries Cumulative Sum\nDESCRIPTION: New method to calculate cumulative sum of time series along the time axis\nSOURCE: https://github.com/unit8co/darts/blob/master/CHANGELOG.md#2025-04-17_snippet_25\n\nLANGUAGE: python\nCODE:\n```\ncumulative_series = time_series.cumsum()\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies and Setting Up Environment for TCN with Darts\nDESCRIPTION: This snippet imports necessary libraries, sets up matplotlib for inline plotting, and defines utility functions for torch model configuration. It also disables warnings and logging for cleaner output.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/05-TCN-examples.ipynb#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%matplotlib inline\n\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom darts import TimeSeries, concatenate\nfrom darts.dataprocessing.transformers import Scaler\nfrom darts.datasets import AirPassengersDataset, EnergyDataset, SunspotsDataset\nfrom darts.models import TCNModel\nfrom darts.utils.callbacks import TFMProgressBar\nfrom darts.utils.missing_values import fill_missing_values\nfrom darts.utils.timeseries_generation import datetime_attribute_timeseries\n\nwarnings.filterwarnings(\"ignore\")\n\nimport logging\n\nlogging.disable(logging.CRITICAL)\n\n\ndef generate_torch_kwargs():\n    # run torch models on CPU, and disable progress bars for all model stages except training.\n    return {\n        \"pl_trainer_kwargs\": {\n            \"accelerator\": \"cpu\",\n            \"callbacks\": [TFMProgressBar(enable_train_bar_only=True)],\n        }\n    }\n```\n\n----------------------------------------\n\nTITLE: Importing Darts and Related Libraries\nDESCRIPTION: Imports necessary libraries including Darts components for data processing, matplotlib for visualization, and pandas for data manipulation. Also suppresses warnings and logging for cleaner output.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/02-data-processing.ipynb#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom darts.dataprocessing import Pipeline\nfrom darts.dataprocessing.transformers import (\n    InvertibleMapper,\n    Mapper,\n    MissingValuesFiller,\n    Scaler,\n)\nfrom darts.datasets import MonthlyMilkDataset, MonthlyMilkIncompleteDataset\nfrom darts.metrics import mape\nfrom darts.models import ExponentialSmoothing\nfrom darts.utils.timeseries_generation import linear_timeseries\n\nwarnings.filterwarnings(\"ignore\")\nimport logging\n\nlogging.disable(logging.CRITICAL)\n```\n\n----------------------------------------\n\nTITLE: Loading Temperature Dataset\nDESCRIPTION: Loads the Temperature dataset from Darts' built-in datasets for time series analysis.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/03-FFT-examples.ipynb#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nts = TemperatureDataset().load()\n```\n\n----------------------------------------\n\nTITLE: Listing Python Package Dependencies\nDESCRIPTION: This snippet lists the required Python packages for the project. It includes ONNX for neural network interchange, OptUna for hyperparameter optimization, Polars for data manipulation, and Ray for distributed computing.\nSOURCE: https://github.com/unit8co/darts/blob/master/requirements/optional.txt#2025-04-17_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\nonnx\nonnxruntime\noptuna\noptuna-integration[pytorch_lightning]\npolars\nray\n```\n\n----------------------------------------\n\nTITLE: TimeSeries Operations\nDESCRIPTION: Methods for appending and prepending values to TimeSeries objects while preserving index names.\nSOURCE: https://github.com/unit8co/darts/blob/master/CHANGELOG.md#2025-04-17_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nTimeSeries.prepend_values()\nTimeSeries.append_values()\n```\n\n----------------------------------------\n\nTITLE: Exporting TimeSeries to DataFrame\nDESCRIPTION: Shows how to export a TimeSeries object to a DataFrame using the new to_dataframe() method, which supports multiple backend formats.\nSOURCE: https://github.com/unit8co/darts/blob/master/CHANGELOG.md#2025-04-17_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ndf = ts.to_dataframe()\n```\n\n----------------------------------------\n\nTITLE: Python Development Dependencies\nDESCRIPTION: A list of Python packages required for development, including pre-commit hooks, pytest with coverage reporting, and test fixtures support.\nSOURCE: https://github.com/unit8co/darts/blob/master/requirements/dev.txt#2025-04-17_snippet_0\n\nLANGUAGE: text\nCODE:\n```\npre-commit\npytest-cov\ntestfixtures\n```\n\n----------------------------------------\n\nTITLE: Setting up time zone conversion in Darts time series encoders and utilities\nDESCRIPTION: Using the tz parameter to convert time zones before generating holidays and datetime attributes in encoders and utilities.\nSOURCE: https://github.com/unit8co/darts/blob/master/CHANGELOG.md#2025-04-17_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# Example of using tz parameter with time series utilities\nholidays_timeseries(tz=\"Europe/Berlin\")\ndatetime_attribute_timeseries(tz=\"Europe/Berlin\")\n\n# For TimeSeries methods\nts.add_datetime_attribute(tz=\"Europe/Berlin\")\nts.add_holidays(tz=\"Europe/Berlin\")\n```\n\n----------------------------------------\n\nTITLE: Running Darts Docker Container\nDESCRIPTION: Commands to pull and run the Darts Docker image in interactive mode\nSOURCE: https://github.com/unit8co/darts/blob/master/INSTALL.md#2025-04-17_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker pull unit8/darts:latest\ndocker run -it -p 8888:8888 unit8/darts:latest bash\n```\n\n----------------------------------------\n\nTITLE: K-Means Anomaly Scoring\nDESCRIPTION: Implements K-means clustering based anomaly scoring on time series data.\nSOURCE: https://github.com/unit8co/darts/blob/master/README.md#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom darts.ad import KMeansScorer\n\nscorer = KMeansScorer(k=2, window=5)\nscorer.fit(train)\nanom_score = scorer.score(val)\n```\n\n----------------------------------------\n\nTITLE: Installing Darts via pip\nDESCRIPTION: Basic pip commands for installing different variants of Darts package with various dependencies\nSOURCE: https://github.com/unit8co/darts/blob/master/INSTALL.md#2025-04-17_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install darts\n```\n\n----------------------------------------\n\nTITLE: Referencing External Requirement Files for DARTS Project in Pip Format\nDESCRIPTION: This snippet defines a requirements.txt file that references separate dependency files for different purposes: core dependencies, development dependencies, release dependencies, and PyTorch-specific dependencies. Each line uses pip's '-r' syntax to include the contents of another file.\nSOURCE: https://github.com/unit8co/darts/blob/master/requirements/dev-all-torch.txt#2025-04-17_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n-r core.txt\n-r dev.txt\n-r release.txt\n-r torch.txt\n```\n\n----------------------------------------\n\nTITLE: Creating TimeSeries from DataFrame with Metadata\nDESCRIPTION: Demonstrates how to create a TimeSeries object from a DataFrame with embedded metadata using the new metadata parameter.\nSOURCE: https://github.com/unit8co/darts/blob/master/CHANGELOG.md#2025-04-17_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nts = TimeSeries.from_dataframe(df, metadata={\"source\": \"sensor_1\", \"location\": \"factory_A\"})\n```\n\n----------------------------------------\n\nTITLE: Documenting BlockRNNModel Class in Sphinx RST Format\nDESCRIPTION: Sphinx documentation directive that auto-generates API documentation for the BlockRNNModel class from the darts.models.forecasting.block_rnn_model module, including all members except those explicitly excluded.\nSOURCE: https://github.com/unit8co/darts/blob/master/docs/generated_api/darts.models.forecasting.block_rnn_model.rst#2025-04-17_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: darts.models.forecasting.block_rnn_model.BlockRNNModel\n   :members:\n   :exclude-members:\n```\n\n----------------------------------------\n\nTITLE: Datetime Index Generation\nDESCRIPTION: Enhancement to generate_index() utility function to accept datetime strings for index generation.\nSOURCE: https://github.com/unit8co/darts/blob/master/CHANGELOG.md#2025-04-17_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\ndarts.utils.utils.generate_index(start=\"2024-01-01\", end=\"2024-12-31\")\n```\n\n----------------------------------------\n\nTITLE: Setting up Python Environment for Darts\nDESCRIPTION: Initializes the Python environment by fixing the path for local development, enabling autoreload, and setting up matplotlib for inline display.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/02-data-processing.ipynb#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# fix python path if working locally\nfrom utils import fix_pythonpath_if_working_locally\n\nfix_pythonpath_if_working_locally()\n\n%load_ext autoreload\n%autoreload 2\n%matplotlib inline\n```\n\n----------------------------------------\n\nTITLE: Documenting CustomBlockRNNModule Class in Sphinx RST Format\nDESCRIPTION: Sphinx documentation directive that auto-generates API documentation for the CustomBlockRNNModule class, including only direct members (no inherited members).\nSOURCE: https://github.com/unit8co/darts/blob/master/docs/generated_api/darts.models.forecasting.block_rnn_model.rst#2025-04-17_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: darts.models.forecasting.block_rnn_model.CustomBlockRNNModule\n   :members:\n   :no-inherited-members:\n```\n\n----------------------------------------\n\nTITLE: Adding Changelog Entry - Markdown\nDESCRIPTION: Example format for adding a new contribution to the CHANGELOG.md file, including pull request reference and contributor information.\nSOURCE: https://github.com/unit8co/darts/blob/master/CONTRIBUTING.md#2025-04-17_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n- Added new feature XYZ. [#001](https://https://github.com/unit8co/darts/pull/001)\n  by [<Your Name>](https://github.com/<your-handle>).\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Directives for RNN Classes\nDESCRIPTION: Sphinx documentation directives specifying how to autogenerate documentation for RNNModel and CustomRNNModule classes from the darts library. The directives include member inclusion/exclusion rules.\nSOURCE: https://github.com/unit8co/darts/blob/master/docs/generated_api/darts.models.forecasting.rnn_model.rst#2025-04-17_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: darts.models.forecasting.rnn_model.RNNModel\n   :members:\n   :exclude-members:\n\n.. autoclass:: darts.models.forecasting.rnn_model.CustomRNNModule\n   :members:\n   :no-inherited-members:\n```\n\n----------------------------------------\n\nTITLE: Fixing Python Path for Local Development\nDESCRIPTION: Fixes the Python import path if working in a local environment to ensure the proper modules can be imported.\nSOURCE: https://github.com/unit8co/darts/blob/master/examples/03-FFT-examples.ipynb#2025-04-17_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# fix python path if working locally\nfrom utils import fix_pythonpath_if_working_locally\n\nfix_pythonpath_if_working_locally()\n```\n\n----------------------------------------\n\nTITLE: Python Library API Changes\nDESCRIPTION: Code reference showing API changes including renamed models and class updates. No actual code snippets are included in this changelog text.\nSOURCE: https://github.com/unit8co/darts/blob/master/CHANGELOG.md#2025-04-17_snippet_29\n\nLANGUAGE: markdown\nCODE:\n```\nHNiTSModel renamed to HNiTS\n```\n\n----------------------------------------\n\nTITLE: Citation Bibtex Entry for Darts Time Series Library\nDESCRIPTION: Bibtex citation entry for referencing the Darts library in scientific work, pointing to the JMLR paper 'Darts: User-Friendly Modern Machine Learning for Time Series'.\nSOURCE: https://github.com/unit8co/darts/blob/master/README.md#2025-04-17_snippet_7\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{JMLR:v23:21-1177,\n  author  = {Julien Herzen and Francesco LÃ¤ssig and Samuele Giuliano Piazzetta and Thomas Neuer and LÃ©o Tafti and Guillaume Raille and Tomas Van Pottelbergh and Marek Pasieka and Andrzej Skrodzki and Nicolas Huguenin and Maxime Dumonal and Jan KoÅ›cisz and Dennis Bader and FrÃ©dÃ©rick Gusset and Mounir Benheddi and Camila Williamson and Michal Kosinski and Matej Petrik and GaÃ«l Grosch},\n  title   = {Darts: User-Friendly Modern Machine Learning for Time Series},\n  journal = {Journal of Machine Learning Research},\n  year    = {2022},\n  volume  = {23},\n  number  = {124},\n  pages   = {1-6},\n  url     = {http://jmlr.org/papers/v23/21-1177.html}\n}\n```\n\n----------------------------------------\n\nTITLE: Specifying Dependency Versions for DARTS Project\nDESCRIPTION: This snippet defines the minimum required versions for three key Python libraries used in the DARTS project. Prophet is set to version 1.1.1 or higher, CatBoost to 1.0.6 or higher, and LightGBM to 3.2.0 or higher.\nSOURCE: https://github.com/unit8co/darts/blob/master/requirements/notorch.txt#2025-04-17_snippet_0\n\nLANGUAGE: Plain Text\nCODE:\n```\nprophet>=1.1.1\ncatboost>=1.0.6\nlightgbm>=3.2.0\n```\n\n----------------------------------------\n\nTITLE: Running Selective Tests\nDESCRIPTION: Command for running pytest while excluding slow tests using markers.\nSOURCE: https://github.com/unit8co/darts/blob/master/CHANGELOG.md#2025-04-17_snippet_28\n\nLANGUAGE: bash\nCODE:\n```\npytest . --no-cov -m \"not slow\"\n```\n\n----------------------------------------\n\nTITLE: Launching Jupyter in Docker Container\nDESCRIPTION: Command to start a Jupyter lab session in the Darts Docker container\nSOURCE: https://github.com/unit8co/darts/blob/master/INSTALL.md#2025-04-17_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\njupyter lab --ip 0.0.0.0 --no-browser --allow-root\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx Documentation Structure in RST\nDESCRIPTION: RST configuration that sets up the documentation structure with hidden table of contents trees (toctree) for different documentation sections and standard Sphinx indices.\nSOURCE: https://github.com/unit8co/darts/blob/master/docs/source/index.rst#2025-04-17_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: README.rst\n\n.. toctree::\n   :hidden:\n\n   Home<README>\n\n.. toctree::\n   :hidden:\n\n   Quickstart<quickstart/00-quickstart.ipynb>\n\n.. toctree::\n   :hidden:\n\n   User Guide<userguide>\n\n.. toctree::\n   :hidden:\n\n   API Reference<generated_api/darts>\n\n.. toctree::\n   :hidden:\n\n   Examples<examples>\n\n.. toctree::\n   :hidden:\n\n   Release Notes<release_notes/RELEASE_NOTES>\n\n\nIndices and tables\n==================\n\n* :ref:`genindex`\n* :ref:`modindex`\n* :ref:`search`\n```\n\n----------------------------------------\n\nTITLE: Installing pyyaml for Darts compatibility on Google Colab\nDESCRIPTION: A command to install a specific version of pyyaml (5.4.1) before installing Darts to resolve compatibility issues in Google Colab environments.\nSOURCE: https://github.com/unit8co/darts/blob/master/docs/userguide/faq.md#2025-04-17_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n!pip install pyyaml==5.4.1\n```\n\n----------------------------------------\n\nTITLE: Building Documentation - Bash Commands\nDESCRIPTION: Series of commands to build the documentation locally, including checking pandoc installation, installing darts in editable mode, and building the docs using make.\nSOURCE: https://github.com/unit8co/darts/blob/master/CONTRIBUTING.md#2025-04-17_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# ensure pandoc is available. If not, install it: https://pandoc.org/installing.html\npandoc --version\n# install darts locally in editable mode\npip install -e .\n# build the docs\nmake --directory=./docs build-all-docs\n```\n\n----------------------------------------\n\nTITLE: TimeSeries Shape Property Addition\nDESCRIPTION: Added new property to get the shape of time series objects in Darts.\nSOURCE: https://github.com/unit8co/darts/blob/master/CHANGELOG.md#2025-04-17_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nTimeSeries.shape\n```\n\n----------------------------------------\n\nTITLE: Using Sample Weights with GlobalForecastingModel\nDESCRIPTION: Example showing how to apply sample weights to forecasting models during training, supporting both TimeSeries weights and built-in weight generators.\nSOURCE: https://github.com/unit8co/darts/blob/master/CHANGELOG.md#2025-04-17_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Using sample weights with a forecasting model\nmodel.fit(\n    series=train_series,\n    sample_weight=\"exponential\",  # or \"linear\" or a TimeSeries object\n    val_sample_weight=weight_val_series\n)\n```"
  }
]
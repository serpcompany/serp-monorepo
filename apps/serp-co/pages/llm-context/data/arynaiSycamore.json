[
  {
    "owner": "aryn-ai",
    "repo": "sycamore",
    "content": "TITLE: Enriching and Embedding Documents with Sycamore\nDESCRIPTION: Defines a schema for flight accident reports, extracts properties, summarizes images, standardizes data, merges sections, and embeds the documents using sentence transformers.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/aryn-opensearch-bedrock-rag-example.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nschema = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"accidentNumber\": {\"type\": \"string\"},\n        \"dateAndTime\": {\"type\": \"date\"},\n        \"location\": {\"type\": \"string\", \"description\": \"US State where the incident occured\"},\n        \"aircraft\": {\"type\": \"string\"},\n        \"aircraftDamage\": {\"type\": \"string\"},\n        \"injuries\": {\"type\": \"string\"},\n        \"definingEvent\": {\"type\": \"string\"},\n    },\n    \"required\": [\"accidentNumber\", \"dateAndTime\", \"location\", \"aircraft\"],\n}\n\nschema_name = \"FlightAccidentReport\"\nproperty_extractor = LLMPropertyExtractor(llm=llm, num_of_elements=20, schema_name=schema_name, schema=schema)\n\nenriched_docset = (\n    partitioned_docset\n    # Extracts the properties based on the schema defined\n    .extract_properties(property_extractor=property_extractor)\n    # Summarizes images that were extracted using an LLM\n    .transform(SummarizeImages, summarizer=LLMImageSummarizer(llm=llm))\n)\n\nformatted_docset = (\n    enriched_docset\n    # Converts state abbreviations to their full names.\n    .map(lambda doc: ignore_errors(doc, USStateStandardizer, [\"properties\", \"entity\", \"location\"]))\n    # Converts datetime into a common format\n    .map(lambda doc: ignore_errors(doc, DateTimeStandardizer, [\"properties\", \"entity\", \"dateAndTime\"]))\n)\n\n\nmerger = GreedySectionMerger(tokenizer=HuggingFaceTokenizer(\"sentence-transformers/all-MiniLM-L6-v2\"), max_tokens=512)\nchunked_docset = formatted_docset.merge(merger=merger)\n\nmodel_name = \"thenlper/gte-small\"\n\nembedded_docset = (\n    chunked_docset.spread_properties([\"entity\", \"path\"])\n    .explode()\n    .embed(embedder=SentenceTransformerEmbedder(batch_size=10_000, model_name=model_name))\n)\n\nembedded_docset = embedded_docset.materialize(\n    path=\"./opensearch-tutorial/embedded-docset\", source_mode=sycamore.MATERIALIZE_USE_STORED\n)\nembedded_docset.execute()\n```\n\n----------------------------------------\n\nTITLE: Documenting BedrockEmbeddingModels Class in Python\nDESCRIPTION: Auto-generated documentation for the BedrockEmbeddingModels class in the sycamore.transforms.embed module. This class probably defines the available embedding models for Bedrock.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/docs/source/sycamore/APIs/low_level_transforms/embed.rst#2025-04-07_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: sycamore.transforms.embed.BedrockEmbeddingModels\n   :members:\n   :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Importing Required Dependencies\nDESCRIPTION: Imports necessary Sycamore modules and functions for document processing, including tokenizers, LLM interfaces, and transformation utilities.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/metadata-extraction.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sycamore.data import Document\nfrom sycamore.functions import HuggingFaceTokenizer\nfrom sycamore.llms import OpenAI, OpenAIModels\nfrom sycamore.transforms.extract_schema import OpenAISchemaExtractor, OpenAIPropertyExtractor\nfrom sycamore.transforms.merge_elements import GreedyTextElementMerger\nfrom sycamore.transforms.partition import ArynPartitioner\nimport sycamore\nimport os\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents for Low-Level Transforms in ReStructuredText\nDESCRIPTION: This code snippet defines a table of contents in ReStructuredText format, listing all the low-level transform classes available in Sycamore. It includes directives for setting up the document structure and a note for users.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/docs/source/sycamore/APIs/low_level_transforms.rst#2025-04-07_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. _Ref-low_level_Transforms:\n\nLow-Level Transforms (for Sycamore development)\n===========\n\n.. note::\n   Users of Sycamore won't need to interact with these classes and should instead use the classes in the top-level API docs. These transform classes are primarily of interest to developers looking to extend Sycamore or contribute to the project.\n\n.. toctree::\n   :maxdepth: 2\n\n   ./low_level_transforms/assign_doc_properties.rst\n   ./low_level_transforms/augment_text.rst\n   ./low_level_transforms/basics.rst\n   ./low_level_transforms/bbox_merge.rst\n   ./low_level_transforms/embed.rst\n   ./low_level_transforms/explode.rst\n   ./low_level_transforms/extract_entity.rst\n   ./low_level_transforms/extract_schema.rst\n   ./low_level_transforms/extract_table.rst\n   ./low_level_transforms/extract_table_properties.rst\n   ./low_level_transforms/llm_map.rst\n   ./low_level_transforms/llm_query.rst\n   ./low_level_transforms/map.rst\n   ./low_level_transforms/mark_misc.rst\n   ./low_level_transforms/merge_elements.rst\n   ./low_level_transforms/partition.rst\n   ./low_level_transforms/query.rst\n   ./low_level_transforms/random_sample.rst\n   ./low_level_transforms/regex_replace.rst\n   ./low_level_transforms/sketcher.rst\n   ./low_level_transforms/split_elements.rst\n   ./low_level_transforms/spread_properties.rst\n   ./low_level_transforms/standardizer.rst\n   ./low_level_transforms/summarize.rst\n   ./low_level_transforms/summarize_images.rst\n```\n\n----------------------------------------\n\nTITLE: Executing Search Query with Reranking Pipeline\nDESCRIPTION: Example of how to perform a search query using the reranking pipeline. Includes query context for both the search and reranking stages.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/docs/source/sycamore/querying_data/using_aryn_opensearch_stack/reranking.md#2025-04-07_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nPOST /my-index/_search?search_pipeline=rerank-pipeline\n{\n  \"query\": {\n    \"match\": {\n      \"text_representation\": \"Who wrote the book of love?\"\n    }\n  },\n  \"ext\": {\n    \"rerank\": {\n      \"query_context\": {\n        \"query_text\": \"Who wrote the book of love?\"\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Sycamore with DuckDB Connector\nDESCRIPTION: Command to install Sycamore with the DuckDB connector. This example shows how to include additional database connectors as extras during installation.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/lib/sycamore/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install sycamore-ai[duckdb]\n```\n\n----------------------------------------\n\nTITLE: Data Loading Configuration\nDESCRIPTION: Initializes Sycamore context and configures data loading from S3.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/metadata-extraction.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ns3_path = \"s3://aryn-public/ntsb/\"\npartition_materialize_path = \"s3://aryn-public/materialize/notebooks/partition-ntsb/2024-10-11\"\n\nllm = OpenAI(OpenAIModels.GPT_3_5_TURBO.value)\ntokenizer = HuggingFaceTokenizer(\"thenlper/gte-small\")\n\nctx = sycamore.init()\n```\n\n----------------------------------------\n\nTITLE: Documenting SentenceTransformerEmbedder Class in Python\nDESCRIPTION: Auto-generated documentation for the SentenceTransformerEmbedder class in the sycamore.transforms.embed module. This class probably handles embedding operations using the SentenceTransformer library.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/docs/source/sycamore/APIs/low_level_transforms/embed.rst#2025-04-07_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: sycamore.transforms.embed.SentenceTransformerEmbedder\n   :members:\n   :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Writing Processed Data to OpenSearch\nDESCRIPTION: Executes the final step of the Sycamore pipeline to write the processed document data with embeddings to OpenSearch using the previously defined connection and index settings.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/opensearch_docs_etl.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Write to OpenSearch\n\nembedded_docset.write.opensearch(\n    os_client_args=openSearch_client_args,\n    index_name=\"sort-benchmark\",\n    index_settings=index_settings,\n)\n```\n\n----------------------------------------\n\nTITLE: Executing DocSet Show Command\nDESCRIPTION: Forces execution of the pipeline to display the first 10 documents from the DocSet\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/sycamore-tutorial-intermediate-etl.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ninitial_docset.show()\n```\n\n----------------------------------------\n\nTITLE: Installing Sycamore with pip\nDESCRIPTION: Basic installation command for Sycamore using pip. This installs the core Sycamore package without any extras.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/docs/source/index.rst#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npip install sycamore-ai\n```\n\n----------------------------------------\n\nTITLE: Document Embedding Pipeline\nDESCRIPTION: Sets up the embedding pipeline for processing documents using OpenAI's embedding model\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/docprep/text-embedding-3-small_greedy-section-merger_duckdb.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nembedded_ds = (\n    # Copy document properties to each Document's sub-elements\n    ds.spread_properties([\"path\", \"entity\"])\n    # Convert all Elements to Documents\n    .explode()\n    # Embed each Document. You can change the embedding model. Make your target vector index matches this number of dimensions.\n    .embed(embedder=OpenAIEmbedder(model_name=model_name))\n)\n# To know more about docset transforms, please visit https://sycamore.readthedocs.io/en/latest/sycamore/transforms.html\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenSearch Connection and Index Settings\nDESCRIPTION: Sets up the client configuration for connecting to OpenSearch and defines index settings including vector search capabilities using HNSW algorithm implemented on FAISS.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/opensearch_docs_etl.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Set OpenSearch configuration for connector. In this example, OpenSearch is running locally.\n\nopenSearch_client_args = {\n    \"hosts\": [{\"host\": \"localhost\", \"port\": 9200}],\n    \"http_compress\": True,\n    \"http_auth\": (\"admin\", \"admin\"),\n    \"use_ssl\": True,\n    \"verify_certs\": False,\n    \"ssl_assert_hostname\": False,\n    \"ssl_show_warn\": False,\n    \"timeout\": 120,\n}\n\nindex_settings = {\n    \"body\": {\n        \"settings\": {\n            \"index.knn\": True,\n            \"number_of_shards\": 2,\n            \"number_of_replicas\": 1,\n        },\n        \"mappings\": {\n            \"properties\": {\n                \"embeddings\": {\n                    \"type\": \"knn_vector\",\n                    \"dimension\": 384,\n                    \"method\": {\"name\": \"hnsw\", \"engine\": \"faiss\"},\n                },\n                \"text\": {\"type\": \"text\"},\n            }\n        },\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing FlatMap Transform for Document Splitting in Python\nDESCRIPTION: This function splits a multi-page document into separate documents for each page. It converts PDF binary data to images, groups elements by page number, and creates new Document objects for each page. The function is then used with the flat_map method of a docset.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/docs/source/sycamore/transforms/flatmap.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef split_and_convert_to_image(doc: Document) -> list[Document]:\n    if doc.binary_representation is not None:\n        images = pdf2image.convert_from_bytes(doc.binary_representation)\n    else:\n        return [doc]\n\n    elements_by_page: dict[int, list[Element]] = {}\n\n    for e in doc.elements:\n        page_number = e.properties[\"page_number\"]\n        elements_by_page.setdefault(page_number, []).append(e)\n\n    new_docs = []\n    for page, elements in elements_by_page.items():\n        new_doc = Document(elements={\"array\": elements})\n        new_doc.properties.update(doc.properties)\n        new_doc.properties.update({\"page_number\": page})\n        new_docs.append(new_doc)\n    return new_docs\n\ndocset = docset.flat_map(split_and_convert_to_image)\n```\n\n----------------------------------------\n\nTITLE: Configuring and Writing to OpenSearch Index\nDESCRIPTION: Sets up an OpenSearch index with vector search capabilities and writes embedded documents to it. The configuration includes setting up HNSW vector search using FAISS as the backend engine.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/docprep/text-embedding-3-small_greedy-text-element-merger_opensearch.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nindex_name = \"docprep-test-index-2\"\n# Configure the OpenSearch client arguments\nos_client_args = {\n    \"hosts\": [{\"host\": \"search-aryn-blog-test-kmzf2omtmydwhsnhov6xlj7y5m.us-east-1.es.amazonaws.com\", \"port\": 443}],\n    \"http_auth\": (os.getenv(\"OS_USER_NAME\"), os.getenv(\"OS_PASSWORD\")),\n    \"verify_certs\": False,\n    \"use_ssl\": True,\n}\n\n# Configure the settings and mappings for the OpenSearch index\nindex_settings = {\n    \"body\": {\n        \"settings\": {\n            \"index.knn\": True,\n        },\n        \"mappings\": {\n            \"properties\": {\n                \"embedding\": {\n                    \"type\": \"knn_vector\",\n                    \"dimension\": dimensions,\n                    \"method\": {\"name\": \"hnsw\", \"engine\": \"faiss\"},\n                },\n            },\n        },\n    },\n}\n\n# Write the docset to the specified OpenSearch index\nembedded_ds.write.opensearch(\n    os_client_args=os_client_args,\n    index_name=index_name,\n    index_settings=index_settings,\n)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Sparse Vector Creation\nDESCRIPTION: Shows an example of the sparse vector generation function on a simple phrase, illustrating how term frequency is captured.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/pinecone-writer.ipynb#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# For example...\ns_vec(\"Mary had a little little lamb\")\n```\n\n----------------------------------------\n\nTITLE: Creating a Pretty Printer for Search Results\nDESCRIPTION: Defines a function to format and display search results in a readable format, extracting and showing relevant metadata fields from each result.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/pinecone-writer.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Pretty printing\ndef print_results_pretty(results):\n    hits = results.get(\"matches\")\n    metadata = [h.get(\"metadata\") for h in hits]\n    for m in metadata:\n        day = int(m.get(\"properties.entity.day\", -1))\n        month = int(m.get(\"properties.entity.month\", -1))\n        year = int(m.get(\"properties.entity.year\", -1))\n        print(f\"{m.get('properties.entity.accidentNumber', 'UNKNOWN')} {'='*80}\")\n        print(f\"Aircraft: {m.get('properties.entity.aircraft', 'UNKNOWN')}\")\n        print(f\"Location: {m.get('properties.entity.location', 'UNKNOWN')}\")\n        print(f\"Date:     {year}-{month}-{day}\")\n        print(f\"Damage:   {m.get('properties.entity.aircraftDamage', 'UNKNOWN')}\")\n        print(f\"Text:     {m.get('text_representation')}\")\n```\n\n----------------------------------------\n\nTITLE: Building a Complete Sycamore Pipeline for PDF Processing\nDESCRIPTION: Creates and configures a full Sycamore pipeline to read PDFs, partition documents, extract entities, chunk content, and generate embeddings for later loading into OpenSearch.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/opensearch_docs_etl.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# This is the Sycamore pipeline up to the \"load\" step. Note that because Sycamore uses lazy execution, the pipeline will not do any processing\n# until you run the \"write\" step in a subsequent cell.\n\n# Initializing Sycamore\ncontext = sycamore.init()\n# Reading PDFs into a DocSet\ndocset = context.read.binary(paths, binary_format=\"pdf\")\n# Partition using the Aryn Partitioning Service into structured elements. Extract tables and images. This will take a few minutes, because\n# the service is processing many pages across the document set.\npartitioned_docset = docset.partition(partitioner=ArynPartitioner(extract_images=True, extract_table_structure=True))\n# Extract the title and author from each paper in the dataset using LLM-powered transforms\nextracted_docset = partitioned_docset.extract_entity(\n    entity_extractor=OpenAIEntityExtractor(\"title\", llm=openai)\n).extract_entity(entity_extractor=OpenAIEntityExtractor(\"authors\", llm=openai))\n# Use the chunking strategy specified earlier to create larger chunks from groups of smaller elements in the DocSet\nchunked_docset = extracted_docset.merge(merger=merger)\n# We are using MiniLM to create vector embeddings locally for each chunk\nembedded_docset = chunked_docset.explode().embed(\n    embedder=SentenceTransformerEmbedder(batch_size=10_000, model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Sycamore Core Package\nDESCRIPTION: Command to install the core Sycamore package using pip. This installs the base functionality of Sycamore for document processing.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/lib/sycamore/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install sycamore-ai\n```\n\n----------------------------------------\n\nTITLE: Embedding Documents with OpenAI and Generating Term Frequency Tables\nDESCRIPTION: This code embeds documents using OpenAI's embedding model and generates term frequency tables for sparse vector search. The term frequency is required for hybrid search in Pinecone but not for pure vector search.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/sycamore-tutorial-intermediate-etl.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom sycamore.transforms.embed import OpenAIEmbedder\n\nembedded_docset = exploded_docset.embed(OpenAIEmbedder(model_name=embedding_model)).term_frequency(\n    tokenizer=tokenizer, with_token_ids=True\n)\n```\n\n----------------------------------------\n\nTITLE: Executing Filtered RAG Query for California Incidents\nDESCRIPTION: Performs a filtered RAG query to retrieve and summarize information specifically about flight incidents that occurred in California, using a combination of KNN search and metadata filtering.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/aryn-opensearch-bedrock-rag-example.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Run the second RAG pipeline using a filter in the retrieval step\n\nquestion = \"What incidents occured in California?\"\n\nos_query = get_knn_query(query_phrase=question, context=context, text_embedder=text_embedder)\nos_query[\"query\"][\"knn\"][\"embedding\"][\"filter\"] = {\"match_phrase\": {\"properties.entity.location\": \"California\"}}\n\ndocset = context.read.opensearch(index_name=\"aryn-rag-demo\", query=os_query, os_client_args=os_client_args)\ndocset = docset.limit(context_size)\n\nanswer = summarize_data(\n    question=question,\n    result_description=\"Documents returned that can answer a quesiton about flight incidents\",\n    result_data=[docset],\n    context=context,\n    llm=llm,\n)\nprint(answer)\n```\n\n----------------------------------------\n\nTITLE: Generating Embeddings for Document Elements\nDESCRIPTION: Configures a pipeline to generate embeddings for document elements using OpenAI's embedding model. This stage spreads document properties to sub-elements, converts all elements to documents, and embeds each resulting document.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/docprep/text-embedding-3-small_greedy-text-element-merger_opensearch.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nembedded_ds = (\n    # Copy document properties to each Document's sub-elements\n    ds.spread_properties([\"path\", \"entity\"])\n    # Convert all Elements to Documents\n    .explode()\n    # Embed each Document. You can change the embedding model. Make your target vector index matches this number of dimensions.\n    .embed(embedder=OpenAIEmbedder(model_name=model_name))\n)\n# To know more about docset transforms, please visit https://sycamore.readthedocs.io/en/latest/sycamore/transforms.html\n```\n\n----------------------------------------\n\nTITLE: Installing Sycamore AI with OpenSearch Support\nDESCRIPTION: Installs the Sycamore AI library with OpenSearch integration. Sycamore is a document ETL library that provides tools for processing and analyzing documents.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/docprep/text-embedding-3-small_greedy-text-element-merger_opensearch.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install sycamore-ai[opensearch]\n# DocPrep code uses the Sycamore document ETL library: https://github.com/aryn-ai/sycamore\n```\n\n----------------------------------------\n\nTITLE: Extracting Entities using OpenAI GPT-3.5 Turbo\nDESCRIPTION: This snippet demonstrates how to extract title and author entities from the documents using OpenAI's GPT-3.5 Turbo model. It uses few-shot learning with custom prompt templates for each entity type.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/docs/source/sycamore/tutorials/etl_for_opensearch.md#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom sycamore.transforms.extract_entity import OpenAIEntityExtractor\nfrom sycamore.llms import OpenAIModels, OpenAI\nimport os\n\n# The following prompt templates will be used to extract the relevant entities\ntitle_prompt_template = \"\"\"\n    ELEMENT 1: Jupiter's Moons\n    ELEMENT 2: Ganymede 2020\n    ELEMENT 3: by Audi Lauper and Serena K. Goldberg. 2011\n    ELEMENT 4: From Wikipedia, the free encyclopedia\n    ELEMENT 5: Ganymede, or Jupiter III, is the largest and most massive natural satellite of Jupiter as well as in the Solar System, being a planetary-mass moon. It is the largest Solar System object without an atmosphere, despite being the only moon of the Solar System with a magnetic field. Like Titan, it is larger than the planet Mercury, but has somewhat less surface gravity than Mercury, Io or the Moon.\n    =========\n    \"Ganymede 2020\"\n\n    ELEMENT 1: FLAVR: Flow-Agnostic Video Representations for Fast Frame Interpolation\n    ELEMENT 2: Tarun Kalluri * UCSD\n    ELEMENT 3: Deepak Pathak CMU\n    ELEMENT 4: Manmohan Chandraker UCSD\n    ELEMENT 5: Du Tran Facebook AI\n    ELEMENT 6: https://tarun005.github.io/FLAVR/\n    ELEMENT 7: 2 2 0 2\n    ELEMENT 8: b e F 4 2\n    ELEMENT 9: ]\n    ELEMENT 10: V C . s c [\n    ========\n    \"FLAVR: Flow-Agnostic Video Representations for Fast Frame Interpolation\"\n\n    \"\"\"\n\nauthor_prompt_template = \"\"\"\n    ELEMENT 1: Jupiter's Moons\n    ELEMENT 2: Ganymede 2020\n    ELEMENT 3: by Audi Lauper and Serena K. Goldberg. 2011\n    ELEMENT 4: From Wikipedia, the free encyclopedia\n    ELEMENT 5: Ganymede, or Jupiter III, is the largest and most massive natural satellite of Jupiter as well as in the Solar System, being a planetary-mass moon. It is the largest Solar System object without an atmosphere, despite being the only moon of the Solar System with a magnetic field. Like Titan, it is larger than the planet Mercury, but has somewhat less surface gravity than Mercury, Io or the Moon.\n    =========\n    Audi Laupe, Serena K. Goldberg\n\n    ELEMENT 1: FLAVR: Flow-Agnostic Video Representations for Fast Frame Interpolation\n    ELEMENT 2: Tarun Kalluri * UCSD\n    ELEMENT 3: Deepak Pathak CMU\n    ELEMENT 4: Manmohan Chandraker UCSD\n    ELEMENT 5: Du Tran Facebook AI\n    ELEMENT 6: https://tarun005.github.io/FLAVR/\n    ELEMENT 7: 2 2 0 2\n    ELEMENT 8: b e F 4 2\n    ELEMENT 9: ]\n    ELEMENT 10: V C . s c [\n    ========\n    Tarun Kalluri, Deepak Pathak, Manmohan Chandraker, Du Tran\n\n    \"\"\"\n\n# We are using OpenAIEntityExtractor which utilizes OpenAI and gpt-3.5-turbo model.\n# You can write your own EntityExtractor as well.\n\n# Replace the \"api_key\" with your API Key.\nopenai_llm = OpenAI(OpenAIModels.GPT_3_5_TURBO.value, api_key=os.environ.get(\"OPENAI_API_KEY\"))\n\ndocset = docset.extract_entity(\n    entity_extractor=OpenAIEntityExtractor(\"title\", llm=openai_llm, prompt_template=title_prompt_template)\n)\n.extract_entity(\n    entity_extractor=OpenAIEntityExtractor(\"authors\", llm=openai_llm, prompt_template=author_prompt_template)\n)\n```\n\n----------------------------------------\n\nTITLE: Ingesting and Processing Data with Sycamore\nDESCRIPTION: Initializes Sycamore context, sets up tokenizer and embedder, and ingests data from either JSON or PDF sources. The pipeline includes partitioning, text processing, embedding, and writing to OpenSearch.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/ndd_example.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntokenizer = HuggingFaceTokenizer(\"thenlper/gte-small\")\nembedder = SentenceTransformerEmbedder(model_name=\"sentence-transformers/all-MiniLM-L6-v2\", batch_size=100)\n\nfsys = pyarrow.fs.S3FileSystem(anonymous=True, region=\"us-east-1\")\nctx = sycamore.init()\n\nif use_json:\n    # Fast way: pre-processed DocSet as JSON...\n    path = \"s3://aryn-public/cccmad-json\"\n    ds = ctx.read.json_document(path, filesystem=fsys)\nelse:\n    # Slow way: process PDF documents via Sycamore pipeline...\n    path = \"s3://aryn-public/cccmad\"\n    ds = (\n        ctx.read.binary(path, binary_format=\"pdf\", filesystem=fsys)\n        .partition(partitioner=ArynPartitioner())\n        .regex_replace(COALESCE_WHITESPACE)\n        .mark_bbox_preset(tokenizer=tokenizer)\n        .merge(merger=MarkedMerger())\n        .spread_properties([\"path\"])\n        .split_elements(tokenizer=tokenizer, max_tokens=512)\n        .explode()\n        .sketch()\n        .embed(embedder=embedder)\n    )\n\nds.write.opensearch(\n    os_client_args=osrch_args,\n    index_name=index_name,\n    index_settings=idx_settings,\n)\n```\n\n----------------------------------------\n\nTITLE: Partitioning PDFs using ArynPartitioner\nDESCRIPTION: This code uses the ArynPartitioner to partition the PDFs in the DocSet. It requires setting up an ARYN_API_KEY for authentication with the Aryn DocParse service.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/docs/source/sycamore/tutorials/etl_for_opensearch.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom sycamore.transforms.partition import ArynPartitioner\n\ndocset = docset.partition(partitioner=ArynPartitioner())\n```\n\n----------------------------------------\n\nTITLE: Writing Data to OpenSearch Hybrid Search Index\nDESCRIPTION: This snippet demonstrates how to write the processed data to a hybrid search index (vector and keyword) in OpenSearch. It configures the OpenSearch client, sets up index settings for KNN search, and writes the data to the specified index.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/docs/source/sycamore/tutorials/etl_for_opensearch.md#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nopenSearch_client_args = {\n        \"hosts\": [{\"host\": \"localhost\", \"port\": 9200}],\n        \"http_compress\": True,\n        \"http_auth\": (\"admin\", \"admin\"),\n        \"use_ssl\": True,\n        \"verify_certs\": False,\n        \"ssl_assert_hostname\": False,\n        \"ssl_show_warn\": False,\n        \"timeout\": 120,\n    }\n\n    index_settings = {\n        \"body\": {\n            \"settings\": {\n                \"index.knn\": True,\n                \"number_of_shards\": 5,\n                \"number_of_replicas\": 1,\n            },\n            \"mappings\": {\n                \"properties\": {\n                    \"embeddings\": {\n                        \"type\": \"knn_vector\",\n                        \"dimension\": 384,\n                        \"method\": {\"name\": \"hnsw\", \"engine\": \"faiss\"},\n                    },\n                    \"text\": {\"type\": \"text\"},\n                }\n            },\n        }\n    }\n\ndocset.write.opensearch(\n        os_client_args=openSearch_client_args,\n        index_name=\"sort-benchmark\",\n        index_settings=index_settings,\n    )\n```\n\n----------------------------------------\n\nTITLE: Aryn API Key Validation\nDESCRIPTION: Validates the presence of an Aryn API key in the configuration.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/metadata-extraction.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom sycamore.utils.aryn_config import ArynConfig, _DEFAULT_PATH\n\nassert ArynConfig.get_aryn_api_key() != \"\", f\"Unable to find aryn API key.  Looked in {_DEFAULT_PATH}\"\n```\n\n----------------------------------------\n\nTITLE: Initializing Sycamore Context\nDESCRIPTION: Sets up the initial Sycamore context for pipeline operations\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/sycamore-tutorial-intermediate-etl.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport sycamore\n\ncontext = sycamore.init()\n```\n\n----------------------------------------\n\nTITLE: Converting Tables to Pandas DataFrames\nDESCRIPTION: Uses the tables_to_pandas utility function to convert all extracted tables from the document into pandas DataFrames for easier data manipulation and analysis.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/financial-docs-10k-example.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npandas = tables_to_pandas(partitioned_file)\n```\n\n----------------------------------------\n\nTITLE: Implementing Reinforcement Learning Policy Training and Evaluation in Pseudocode\nDESCRIPTION: This pseudocode demonstrates the core components of a reinforcement learning system: policy evaluation through environment simulation (rollout function) and iterative policy improvement through training. It shows how the serving, simulation, and training aspects are tightly coupled in RL applications.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/lib/sycamore/sycamore/tests/resources/data/texts/Ray.txt#2025-04-07_snippet_0\n\nLANGUAGE: pseudocode\nCODE:\n```\n// evaluate policyby interacting with env. (e.g., simulator)\nrollout(policy, environment):\n  trajectory=[]\n  state= environment.initial_state()\n  while(notenvironment.has_terminated()):\n    action= policy.compute(state) // Serving\n    state,reward= environment.step(action) // Simulation\n    trajectory.append(state,reward)\n  returntrajectory\n\n// improve policy iteratively until it converges\ntrain_policy(environment):\n  policy= initial_policy()\n  while(policyhas not converged):\n    trajectories = []\n    forifrom1 tok:\n      // evaluate policyby generating krollouts\n      trajectories.append(rollout(policy, environment))\n    // improve policy\n    policy= policy.update(trajectories) // Training\n  returnpolicy\n```\n\n----------------------------------------\n\nTITLE: Defining LLM Prompt Templates for Title and Author Extraction in Python\nDESCRIPTION: This snippet defines two string templates used for training a generative AI model to extract titles and authors from document elements. The templates provide example inputs and expected outputs to guide the model's extraction process.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/docs/source/sycamore/tutorials/sycamore_jupyter_dev_example.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntitle_context_template = \"\"\"\n    ELEMENT 1: Jupiter's Moons\n    ELEMENT 2: Ganymede 2020\n    ELEMENT 3: by Audi Lauper and Serena K. Goldberg. 2011\n    ELEMENT 4: From Wikipedia, the free encyclopedia\n    ELEMENT 5: Ganymede, or Jupiter III, is the largest and most massive natural satellite of Jupiter as well as in the Solar System, being a planetary-mass moon. It is the largest Solar System object without an atmosphere, despite being the only moon of the Solar System with a magnetic field. Like Titan, it is larger than the planet Mercury, but has somewhat less surface gravity than Mercury, Io or the Moon.\n    =========\n    \"Ganymede 2020\"\n\n    ELEMENT 1: FLAVR: Flow-Agnostic Video Representations for Fast Frame Interpolation\n    ELEMENT 2: Tarun Kalluri * UCSD\n    ELEMENT 3: Deepak Pathak CMU\n    ELEMENT 4: Manmohan Chandraker UCSD\n    ELEMENT 5: Du Tran Facebook AI\n    ELEMENT 6: https://tarun005.github.io/FLAVR/\n    ELEMENT 7: 2 2 0 2\n    ELEMENT 8: b e F 4 2\n    ELEMENT 9: ]\n    ELEMENT 10: V C . s c [\n    ========\n    \"FLAVR: Flow-Agnostic Video Representations for Fast Frame Interpolation\"\n\n    \"\"\"\nauthor_context_template = \"\"\"\n    ELEMENT 1: Jupiter's Moons\n    ELEMENT 2: Ganymede 2020\n    ELEMENT 3: by Audi Lauper and Serena K. Goldberg. 2011\n    ELEMENT 4: From Wikipedia, the free encyclopedia\n    ELEMENT 5: Ganymede, or Jupiter III, is the largest and most massive natural satellite of Jupiter as well as in the Solar System, being a planetary-mass moon. It is the largest Solar System object without an atmosphere, despite being the only moon of the Solar System with a magnetic field. Like Titan, it is larger than the planet Mercury, but has somewhat less surface gravity than Mercury, Io or the Moon.\n    =========\n    Audi Laupe, Serena K. Goldberg\n\n    ELEMENT 1: FLAVR: Flow-Agnostic Video Representations for Fast Frame Interpolation\n    ELEMENT 2: Tarun Kalluri * UCSD\n    ELEMENT 3: Deepak Pathak CMU\n    ELEMENT 4: Manmohan Chandraker UCSD\n    ELEMENT 5: Du Tran Facebook AI\n    ELEMENT 6: https://tarun005.github.io/FLAVR/\n    ELEMENT 7: 2 2 0 2\n    ELEMENT 8: b e F 4 2\n    ELEMENT 9: ]\n    ELEMENT 10: V C . s c [\n    ========\n    Tarun Kalluri, Deepak Pathak, Manmohan Chandraker, Du Tran\n\n  \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Retrieving OpenSearch Embedding Model ID\nDESCRIPTION: Defines a function to fetch the embedding model ID from OpenSearch. This ID is necessary for querying and changes each time OpenSearch is set up.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/ndd_example.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef get_model_id():\n    query = {\n        \"query\": {\n            \"bool\": {\n                \"must\": [\n                    {\n                        \"match\": {\"name\": \"all-MiniLM-L6-v2\"},\n                    },\n                    {\n                        \"term\": {\"model_config.model_type\": \"bert\"},\n                    },\n                ],\n            },\n        },\n    }\n    with requests.get(f\"https://{opensearch_host}:9200/_plugins/_ml/models/_search\", json=query, verify=False) as resp:\n        res = json.loads(resp.text)\n        return res[\"hits\"][\"hits\"][0][\"_id\"]\n```\n\n----------------------------------------\n\nTITLE: Executing Remote Rollouts on Actors with Ray in Python\nDESCRIPTION: This code initiates parallel rollouts on a collection of actors using Ray's remote execution functionality. It calls the rollout method remotely on each actor in a collection 's', passing a policy_id parameter, and collects the resulting rollout_ids for future reference.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/lib/sycamore/sycamore/tests/resources/data/texts/Ray.txt#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nrollout_ids = [s.rollout.remote(policy_id)\n```\n\n----------------------------------------\n\nTITLE: API Key Configuration\nDESCRIPTION: Template for configuring API keys for Aryn and OpenAI services\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/docprep/text-embedding-3-small_greedy-section-merger_duckdb.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# It's best to store API keys in a configuration file or set them as environment variables.\n# For quick testing, you can define them here:\n#\n# os.environ[\"ARYN_API_KEY\"] = \"YOUR_ARYN_API_KEY\"\n# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n```\n\n----------------------------------------\n\nTITLE: Executing RAG Query on Flight Incidents\nDESCRIPTION: Performs a RAG (Retrieval-Augmented Generation) query on the indexed documents to answer a question about flight incidents in Texas and California, using KNN search and LLM summarization.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/aryn-opensearch-bedrock-rag-example.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom sycamore.query.execution.operations import summarize_data\nfrom sycamore.connectors.opensearch.utils import get_knn_query\n\n# Run the first RAG pipeline\n\ncontext_size = 20\nquestion = \"What was common with incidents in Texas, and how does that differ from incidents in California?\"\n\ntext_embedder = SentenceTransformerEmbedder(batch_size=10_000, model_name=model_name)\nos_client_args = openSearch_client_args\n\nos_query = get_knn_query(query_phrase=question, context=context, text_embedder=text_embedder)\n\ndocset = context.read.opensearch(index_name=\"aryn-rag-demo\", query=os_query, os_client_args=os_client_args)\ndocset = docset.limit(context_size)\n\nanswer = summarize_data(\n    question=question,\n    result_description=\"Documents returned that can answer a quesiton about flight incidents\",\n    result_data=[docset],\n    context=context,\n    llm=llm,\n)\nprint(answer)\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenSearch and Index Settings\nDESCRIPTION: Defines OpenSearch client arguments and index settings, including KNN vector configuration for embeddings. These settings are used for connecting to OpenSearch and creating the index with proper mappings.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/ndd_example.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nosrch_args = {\n    \"hosts\": [{\"host\": opensearch_host, \"port\": 9200}],\n    \"http_compress\": True,\n    \"http_auth\": (\"admin\", \"admin\"),\n    \"use_ssl\": True,\n    \"verify_certs\": False,\n    \"ssl_assert_hostname\": False,\n    \"ssl_show_warn\": False,\n    \"timeout\": 120,\n}\n\nidx_settings = {\n    \"body\": {\n        \"settings\": {\n            \"index.knn\": True,\n        },\n        \"mappings\": {\n            \"properties\": {\n                \"embedding\": {\n                    \"type\": \"knn_vector\",\n                    \"dimension\": 384,\n                    \"method\": {\"name\": \"hnsw\", \"engine\": \"faiss\"},\n                },\n            },\n        },\n    },\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Aryn API Key in YAML Configuration\nDESCRIPTION: Shows how to create a YAML configuration file to store the Aryn API key for document partitioning.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/opensearch_docs_etl.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\naryn_token: \"YOUR-ARYN-API-KEY\"\n```\n\n----------------------------------------\n\nTITLE: Creating Advanced Search Pipeline with Hybrid Search, Reranking, and RAG\nDESCRIPTION: Implements a comprehensive search pipeline combining normalization, reranking, and Retrieval Augmented Generation (RAG). Includes weighted score combination and multiple response processors.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/docs/source/sycamore/querying_data/using_aryn_opensearch_stack/reranking.md#2025-04-07_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nPUT /_search/pipeline/mega-relevance-pipeline\n{\n  \"description\": \"Pipeline with hybrid search, reranking, and RAG\",\n  \"phase_results_processors\": [\n    {\n      \"normalization-processor\": {\n        \"normalization\": {\n          \"technique\": \"min_max\"\n        },\n        \"combination\": {\n          \"technique\": \"arithmetic_mean\",\n          \"parameters\": {\n            \"weights\": [0.2, 0.8]\n          }\n        }\n      }\n    }\n  ],\n  \"response_processors\": [\n    {\n      \"rerank\": {\n        \"ml_opensearch\": {\n          \"model_id\": \"<reranker_id>\"\n        },\n        \"context\": {\n          \"document_fields\": [ \"text_representation\" ]\n        }\n      }\n    },\n    {\n      \"retrieval_augmented_generation\": {\n        \"tag\": \"openai_pipeline_demo\",\n        \"model_id\": \"<remote_model_id>\",\n        \"context_field_list\": [\n          \"text_representation\"\n        ],\n        \"llm_model\": \"gpt-4\"\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Executing Query With Near-Duplicate Detection\nDESCRIPTION: Modifies the previous query to enable near-duplicate detection by including the 'shingles' field in the source. This query demonstrates the impact of NDD on result diversity and answer quality.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/ndd_example.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nquery[\"_source\"].append(\"shingles\")\ndo_query(query)\n```\n\n----------------------------------------\n\nTITLE: Displaying the Industry Geographic Breakdown DataFrame\nDESCRIPTION: Shows the full DataFrame containing the industry geographic breakdown information extracted from the document table.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/financial-docs-10k-example.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# display the dataframe\nindustry_geographic_breakdown\n```\n\n----------------------------------------\n\nTITLE: Setting API Keys and Credentials\nDESCRIPTION: Demonstrates how to set API keys and credentials as environment variables for Aryn AI, OpenSearch, and OpenAI which are required for the pipeline to function.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/docprep/text-embedding-3-small_greedy-text-element-merger_opensearch.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# It's best to store API keys in a configuration file or set them as environment variables.\n# For quick testing, you can define them here:\n#\n# os.environ[\"ARYN_API_KEY\"] = \"YOUR_ARYN_API_KEY\"\n# os.environ[\"OS_USER_NAME\"] = \"YOUR_OPENSEARCH_USER_NAME\"\n# os.environ[\"OS_PASSWORD\"] = \"YOUR_OPENSEARCH_PASSWORD\"\n# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n```\n\n----------------------------------------\n\nTITLE: Importing Sycamore Dependencies\nDESCRIPTION: Imports necessary modules from Sycamore and other libraries for document processing, including transformers for text processing, embedding generation, and utilities for PDF handling.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/docprep/text-embedding-3-small_greedy-text-element-merger_opensearch.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow.fs\nimport sycamore\nimport os\nfrom sycamore.functions.tokenizer import OpenAITokenizer\nfrom sycamore.transforms.merge_elements import GreedyTextElementMerger\nfrom sycamore.transforms.partition import ArynPartitioner\nfrom sycamore.transforms.embed import OpenAIEmbedder\nfrom sycamore.materialize_config import MaterializeSourceMode\nfrom sycamore.utils.pdf_utils import show_pages\nfrom sycamore.context import ExecMode\n```\n\n----------------------------------------\n\nTITLE: Retrieving Pinecone Index Information\nDESCRIPTION: Displays information about the Pinecone index configuration by connecting to the Pinecone service and describing the NTSB index.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/pinecone-writer.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Tell me about my index\nimport pinecone\n\npc = pinecone.Pinecone()\npc.describe_index(\"ntsb\")\n```\n\n----------------------------------------\n\nTITLE: Writing Processed Documents to Pinecone Vector Database\nDESCRIPTION: Exports the processed documents to Pinecone, configuring a serverless index with specific dimension and distance metric settings for vector search capabilities.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/pinecone-writer.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Write to pinecone\n# - into an index named 'ntsb'\n# - into a serverless index in aws/us-east-1\n# - into a 384-dimensional index. The embedding model we used (all-MiniLM-L6-v2) generates 384-dimensional vectors\n# - into an index using the dotproduct distance metric. This is necessary to do sparse vector search\nimport pinecone\n\nds.write.pinecone(\n    index_name=\"ntsb\",\n    index_spec=pinecone.ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n    dimensions=384,\n    distance_metric=\"dotproduct\",\n)\n```\n\n----------------------------------------\n\nTITLE: Using Date Filtering with Hybrid Search\nDESCRIPTION: Demonstrates filtered hybrid search with date constraints to find incidents within a specific time window, showing how properly structured metadata enables precise filtering.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/pinecone-writer.ipynb#2025-04-07_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n# This filter says year=2023, month=1, and day>=17 (31 - 14 = 17)\n# When we includ it, all of our search results are from the correct time window\nfilter = {\n    \"$and\": [\n        {\"properties.entity.year\": {\"$eq\": 2023}},\n        {\"properties.entity.month\": {\"$eq\": 1}},\n        {\"properties.entity.day\": {\"$gte\": 17}},\n    ]\n}\nprint_results_pretty(hybrid_query_filtered(\"incidents in the last two weeks of january\", filter=filter))\n```\n\n----------------------------------------\n\nTITLE: Analyzing Specific Industry Sales Data\nDESCRIPTION: Extracts and displays specific sales data for the 'Electronics and Energy' segment across different regions by selecting particular columns and rows from the DataFrame.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/financial-docs-10k-example.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# pull out the sales data for 'Electronics and Energy' segment\nindustry_geographic_breakdown[\n    [\n        \"\",\n        \"Three months ended December 31, 2018 | United States\",\n        \"Three months ended December 31, 2018 | Europe; Middle East Africa\",\n    ]\n].iloc[[19]]\n```\n\n----------------------------------------\n\nTITLE: Chunking Documents for Embeddings\nDESCRIPTION: Implements chunking strategy using MarkedMerger to prepare documents for embedding generation, including token limit considerations\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/sycamore-tutorial-intermediate-etl.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom sycamore.transforms.merge_elements import MarkedMerger\nfrom sycamore.functions.tokenizer import OpenAITokenizer\n\nembedding_model = \"text-embedding-3-small\"\nembedding_dim = 1536\nmax_tokens = 8192\ntokenizer = OpenAITokenizer(embedding_model)\n\nchunked_docset = (\n    enriched_docset.mark_bbox_preset(tokenizer=tokenizer, token_limit=max_tokens)\n    .merge(merger=MarkedMerger())\n    .split_elements(tokenizer=tokenizer, max_tokens=max_tokens)\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Aryn API Key via Environment Variable\nDESCRIPTION: Demonstrates how to set the Aryn API key directly in the notebook using environment variables.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/opensearch_docs_etl.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nos.environ[\"ARYN_API_KEY\"] = \"ARYN-API-KEY-LOCATION\"\n```\n\n----------------------------------------\n\nTITLE: Partitioning Document with Table Extraction and OCR\nDESCRIPTION: Calls the Aryn partitioning service to process the document with table structure extraction and OCR enabled. Specifies page 23 to focus the extraction on a specific part of the document.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/financial-docs-10k-example.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n## Make a call to the partitioning service and set extract_table_structure and use_ocr to True.\n## Also set selected_pages to 23 to just pull out that page.\npartitioned_file = partition_file(f, aryn_api_key, extract_table_structure=True, use_ocr=True, selected_pages=[23])\n```\n\n----------------------------------------\n\nTITLE: Writing Embedded Documents to Pinecone with Sycamore\nDESCRIPTION: This snippet demonstrates how to write the embedded document set to Pinecone using Sycamore's connector. It configures a serverless Pinecone index with specific dimensions and distance metric settings.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/sycamore-tutorial-intermediate-etl.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport pinecone\nimport time\n\nstart = time.time()\nembedded_docset.write.pinecone(\n    index_name=\"aryn-etl-tutorial\",\n    index_spec=pinecone.ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n    dimensions=embedding_dim,\n    distance_metric=\"dotproduct\",\n    namespace=\"ntsbdocs\",\n)\n```\n\n----------------------------------------\n\nTITLE: Writing Embedded Documents to OpenSearch\nDESCRIPTION: Writes the embedded document set to the configured OpenSearch index using the specified client arguments and index settings.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/aryn-opensearch-bedrock-rag-example.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nembedded_docset.write.opensearch(\n    os_client_args=openSearch_client_args,\n    index_name=\"aryn-rag-demo\",\n    index_settings=index_settings,\n)\n```\n\n----------------------------------------\n\nTITLE: Checking Pinecone Index Statistics\nDESCRIPTION: Retrieves statistics about the Pinecone index, such as the number of vectors stored and other relevant metrics.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/pinecone-writer.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# How many things are in my index?\nntsb = pc.Index(name=\"ntsb\")\nntsb.describe_index_stats()\n```\n\n----------------------------------------\n\nTITLE: Configuring NDD Thresholds in YAML for Sycamore Pipelines\nDESCRIPTION: This YAML configuration sets up multiple de-duplication processors with different thresholds. The threshold parameter controls how aggressively documents are dropped, with values closer to 0 being less aggressive and values above 1 removing all but the first document.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/docs/source/sycamore/querying_data/using_aryn_opensearch_stack/dedup.md#2025-04-07_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n- dedup00:\n    processors:\n      - dedup-response:\n          threshold: 0.01\n- dedup01:\n    processors:\n      - dedup-response:\n          threshold: 0.1\n- dedup02:\n    processors:\n      - dedup-response:\n          threshold: 0.15\n- dedup03:\n    processors:\n      - dedup-response:\n          threshold: 0.2\n- dedup04:\n    processors:\n      - dedup-response:\n          threshold: 0.3\n- dedup05:\n    processors:\n      - dedup-response:\n          threshold: 0.35\n- dedup06:\n    processors:\n      - dedup-response:\n          threshold: 0.4\n- dedup07:\n    processors:\n      - dedup-response:\n          threshold: 0.45\n- dedup08:\n    processors:\n      - dedup-response:\n          threshold: 0.55\n```\n\n----------------------------------------\n\nTITLE: Sphinx AutoModule Documentation Directive for DocSet\nDESCRIPTION: Sphinx documentation directive that automatically generates API documentation from the sycamore.docset module's docstrings and code.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/docs/source/sycamore/APIs/docset.rst#2025-04-07_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: sycamore.docset\n   :members:\n```\n\n----------------------------------------\n\nTITLE: Default Schema for DuckDB Table in Sycamore\nDESCRIPTION: This code snippet shows the default schema used for creating tables in DuckDB when writing DocSets from Sycamore. It defines the structure and data types for various fields in the table.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/docs/source/sycamore/connectors/duckdb.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nschema: Optional[Dict[str, str]] = field(\n    default_factory=lambda: {\n        \"doc_id\": \"VARCHAR\",\n        \"embeddings\": \"DOUBLE[]\",\n        \"properties\": \"MAP(VARCHAR, VARCHAR)\",\n        \"text_representation\": \"VARCHAR\",\n        \"bbox\": \"DOUBLE[]\",\n        \"shingles\": \"BIGINT[]\",\n        \"type\": \"VARCHAR\",\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Document Processing Pipeline with Sycamore\nDESCRIPTION: Builds a comprehensive pipeline for document processing, including partitioning PDFs, extracting metadata using OpenAI, merging elements, converting timestamps, embedding content, and preparing for hybrid search with term frequency analysis.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/pinecone-writer.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npaths = [\"s3://aryn-public/ntsb/\"]\nfsys = pyarrow.fs.S3FileSystem(region=\"us-east-1\", anonymous=True)\n\nllm = OpenAI(OpenAIModels.GPT_3_5_TURBO.value)\ntokenizer = HuggingFaceTokenizer(\"thenlper/gte-small\")\n\nctx = sycamore.init()\n\n# Main ingest pipeline. Note the use of `.term_frequency()`, which will enable hybrid search in pinecone\nds = (\n    ctx.read.binary(paths, binary_format=\"pdf\", filesystem=fsys)\n    # Partition with the Aryn partitioner remotely, pulling out tables and images.\n    .partition(partitioner=ArynPartitioner(extract_images=True, extract_table_structure=True))\n    # Get rid of spurious whitespace charaters\n    .regex_replace(COALESCE_WHITESPACE)\n    # Automatically determine a schema of additional metadata to extract from documents\n    .extract_batch_schema(schema_extractor=OpenAISchemaExtractor(\"FlightAccidentReport\", llm=llm, num_of_elements=35))\n    # Extract the metadata specified by that schema\n    .extract_properties(property_extractor=OpenAIPropertyExtractor(llm=llm, num_of_elements=35))\n    # Merge elements into larger chunks\n    .mark_bbox_preset(tokenizer=tokenizer).merge(merger=MarkedMerger())\n    # Convert extracted timestamps to better-structured form using the function above\n    .map(convert_timestamp)\n    # Copy document properties to each document's sub-elements\n    .spread_properties([\"path\", \"entity\"])\n    # Split elements that are too big to embed\n    .split_elements(tokenizer=tokenizer, max_tokens=512)\n    # Convert all Elements to Documents\n    .explode()\n    # Generate a series of hashes to represent each document. For use with near-duplicate detection\n    .sketch()\n    # Embed each document\n    .embed(embedder=SentenceTransformerEmbedder(model_name=\"sentence-transformers/all-MiniLM-L6-v2\", batch_size=100))\n    # Count the number of occurrences of every token for each document\n    .term_frequency(tokenizer=tokenizer, with_token_ids=True)\n)\n```\n\n----------------------------------------\n\nTITLE: Specifying Required Fields for OpenSearch Query in JSON\nDESCRIPTION: This JSON snippet demonstrates how to specify the required 'shingles' field in an OpenSearch query for the dedup-response processor. It includes both 'shingles' and 'text_representation' in the _source field to ensure the necessary data is retrieved for de-duplication.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/docs/source/sycamore/querying_data/using_aryn_opensearch_stack/dedup.md#2025-04-07_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"_source\": [\n    \"shingles\",\n    \"text_representation\"\n  ],\n  \"query\": {\n    \"match\": {\n      \"text_representation\": \"query\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Documenting OpenAIEmbedder Class in Python\nDESCRIPTION: Auto-generated documentation for the OpenAIEmbedder class in the sycamore.transforms.embed module. This class probably handles embedding operations using OpenAI's services.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/docs/source/sycamore/APIs/low_level_transforms/embed.rst#2025-04-07_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: sycamore.transforms.embed.OpenAIEmbedder\n   :members:\n   :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Resetting OpenSearch Data\nDESCRIPTION: Cleans up all data in the OpenSearch volume by removing contents using an Ubuntu container\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/apps/opensearch/2.11/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --volume opensearch_data:/tmp/osd ubuntu /bin/sh -c 'rm -rf /tmp/osd/*'\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Limitations of Unfiltered Search\nDESCRIPTION: Shows an example of how unfiltered hybrid search can return results outside of a specified time window, highlighting the need for metadata filtering.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/pinecone-writer.ipynb#2025-04-07_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n# When we're looking for specific pieces of information, 'fuzzy' hybrid/semantic search\n# won't quite cut it. In this example, we get a bunch of results outside of the specified\n# time window.\nprint_results_pretty(hybrid_query(\"incidents in the last 2 weeks of january 2023\"))\n```\n\n----------------------------------------\n\nTITLE: Exploding DocSet Elements in Python\nDESCRIPTION: Demonstrates how to use the explode() method to convert elements within a DocSet into separate top-level documents. This is useful when preparing documents for embedding or ingestion into systems like OpenSearch.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/docs/source/sycamore/transforms/explode.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nexploded_doc_set = docset.explode()\n```\n\n----------------------------------------\n\nTITLE: Sphinx LLM Documentation Directives\nDESCRIPTION: Sphinx autodoc directives that generate documentation for LLM-related classes and modules including the base LLM class, LLMMode enum, and provider-specific implementations.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/docs/source/sycamore/APIs/llm.rst#2025-04-07_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: sycamore.llms.llms.LLM\n   :members:\n   :exclude-members: generate_old\n.. autoenum:: sycamore.llms.llms.LLMMode\n   :members:\n.. automodule:: sycamore.llms.openai\n   :members:\n   :show-inheritance:\n.. automodule:: sycamore.llms.anthropic\n   :members:\n   :show-inheritance:\n.. automodule:: sycamore.llms.gemini\n   :members:\n   :show-inheritance:\n.. automodule:: sycamore.llms.bedrock\n   :members:\n   :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Documenting Embed Class in Python\nDESCRIPTION: Auto-generated documentation for the Embed class in the sycamore.transforms.embed module. This is likely a base class for embedding operations in the Sycamore library.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/docs/source/sycamore/APIs/low_level_transforms/embed.rst#2025-04-07_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: sycamore.transforms.embed.Embed\n   :members:\n   :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries\nDESCRIPTION: Imports necessary modules from Sycamore and other dependencies for document processing and embedding\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/docprep/text-embedding-3-small_greedy-section-merger_duckdb.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow.fs\nimport sycamore\nfrom sycamore.functions.tokenizer import OpenAITokenizer\nfrom sycamore.transforms.merge_elements import GreedySectionMerger\nfrom sycamore.transforms.partition import ArynPartitioner\nfrom sycamore.transforms.embed import OpenAIEmbedder\nfrom sycamore.materialize_config import MaterializeSourceMode\nfrom sycamore.utils.pdf_utils import show_pages\nfrom sycamore.context import ExecMode\n```\n\n----------------------------------------\n\nTITLE: Converting Timestamp Data Using Sycamore's Map Transform\nDESCRIPTION: A function that converts date/time strings in document properties to structured date information. It parses raw date strings into formal ISO format and extracts day, month, and year into separate fields.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/sycamore-tutorial-intermediate-etl.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom sycamore.data.document import Document\nfrom dateutil import parser\n\n\ndef convert_timestamp(doc: Document) -> Document:\n    try:\n        if \"dateAndTime\" not in doc.properties[\"entity\"] and \"dateTime\" not in doc.properties[\"entity\"]:\n            return doc\n        raw_date: str = doc.properties[\"entity\"].get(\"dateAndTime\") or doc.properties[\"entity\"].get(\"dateTime\")\n        raw_date = raw_date.replace(\"Local\", \"\")\n        parsed_date = parser.parse(raw_date, fuzzy=True)\n        extracted_date = parsed_date.date()\n        doc.properties[\"entity\"][\"day\"] = extracted_date.day\n        doc.properties[\"entity\"][\"month\"] = extracted_date.month\n        doc.properties[\"entity\"][\"year\"] = extracted_date.year\n        if parsed_date.utcoffset():\n            doc.properties[\"entity\"][\"dateTime\"] = parsed_date.isoformat()\n        else:\n            doc.properties[\"entity\"][\"dateTime\"] = parsed_date.isoformat() + \"Z\"\n    except Exception:\n        pass\n    return doc\n\n\nformatted_docset = chunked_docset.map(convert_timestamp)\n```\n\n----------------------------------------\n\nTITLE: Cleaning Text with Regex\nDESCRIPTION: Applies regex transformation to clean excess whitespace from text elements\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/sycamore-tutorial-intermediate-etl.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom sycamore.transforms.regex_replace import COALESCE_WHITESPACE\n\nregex_docset = partitioned_docset.regex_replace(COALESCE_WHITESPACE)\nprint(COALESCE_WHITESPACE)\n```\n\n----------------------------------------\n\nTITLE: Adding Remote Processor to OpenSearch Pipeline via HTTP\nDESCRIPTION: This HTTP PUT request demonstrates how to add a hosted remote processor to an OpenSearch search pipeline. It specifies the endpoint and processor name for the remote deduplication processor.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/docs/source/sycamore/querying_data/using_aryn_opensearch_stack/remote_processors.md#2025-04-07_snippet_1\n\nLANGUAGE: http\nCODE:\n```\nPUT /_search/pipeline/pipeline-with-remote-dedup\n{\n  \"response_processors\": [\n    {\n      \"remote_processor\": {\n        \"endpoint\": \"https://rps:2796/RemoteProcessorService/ProcessResponse\"\n        \"processor_name\": \"dedup\"\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Sycamore Document Processing Pipeline\nDESCRIPTION: Sets up the Sycamore context, reads PDF documents, partitions them, and materializes the results. It uses various transformers and the Bedrock LLM for processing.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/aryn-opensearch-bedrock-rag-example.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport sycamore\nfrom sycamore.transforms.partition import ArynPartitioner\nfrom sycamore.transforms.extract_schema import LLMPropertyExtractor\nfrom sycamore.transforms.summarize_images import SummarizeImages, LLMImageSummarizer\nfrom sycamore.transforms.standardizer import USStateStandardizer, DateTimeStandardizer, ignore_errors\nfrom sycamore.transforms.merge_elements import GreedySectionMerger\nfrom sycamore.functions.tokenizer import HuggingFaceTokenizer\nfrom sycamore.transforms.embed import SentenceTransformerEmbedder\nfrom sycamore.llms import Bedrock, BedrockModels\n\n\nllm = Bedrock(BedrockModels.CLAUDE_3_SONNET)\n\npaths = [\"s3://aryn-public/ntsb/\"]\n\ncontext = sycamore.init()\n# Add exec_mode=ExecMode.LOCAL to .init to run without Ray\ndocset = context.read.binary(paths=paths, binary_format=\"pdf\")\ndocset = docset.materialize(path=\"./opensearch-tutorial/downloaded-docset\", source_mode=sycamore.MATERIALIZE_USE_STORED)\n# Make sure your Aryn API key is accessible in the environment variable ARYN_API_KEY\npartitioned_docset = docset.partition(\n    partitioner=ArynPartitioner(extract_table_structure=True, extract_images=True)\n).materialize(path=\"./opensearch-tutorial/partitioned-docset\", source_mode=sycamore.MATERIALIZE_USE_STORED)\npartitioned_docset.execute()\n```\n\n----------------------------------------\n\nTITLE: Describing Map Transform Function in Sycamore\nDESCRIPTION: The Map transform takes a function that transforms a Document into another Document, and applies this function to each document in a DocSet. This allows for individual document processing within a larger set.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/docs/source/sycamore/transforms/map.md#2025-04-07_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n## Map\nThe Map transform takes a function that takes a ``Document`` and returns a ``Document``,\nand applies it to each document in the ``DocSet``.\n```\n\n----------------------------------------\n\nTITLE: Initializing Sycamore Pipeline Components for PDF Processing\nDESCRIPTION: Sets up the necessary components for a Sycamore pipeline including Aryn Partitioner for document processing, OpenAI for entity extraction, and a chunking strategy with SentenceTransformer for embeddings.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/opensearch_docs_etl.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport sycamore\nfrom sycamore.transforms.partition import ArynPartitioner\nfrom sycamore.utils.aryn_config import ArynConfig, _DEFAULT_PATH\nfrom sycamore.llms import OpenAIModels, OpenAI\nfrom sycamore.transforms.extract_entity import OpenAIEntityExtractor\nfrom sycamore.transforms.embed import SentenceTransformerEmbedder\nfrom sycamore.transforms.merge_elements import GreedySectionMerger\nfrom sycamore.functions.tokenizer import HuggingFaceTokenizer\nimport os\n\n# S3 file path to the Sort Benchmark dataset of PDFs\npaths = \"s3://aryn-public/sort-benchmark/pdf/\"\n\n# OpenAI key and model for data extraction transform. Set the key in your environment variables or provide it here.\nopenai = OpenAI(OpenAIModels.GPT_4O.value, api_key=os.environ.get(\"OPENAI_API_KEY\"))\n\n# Configure chunking (or merging) strategy and the number of tokens for each chunk.\nmerger = GreedySectionMerger(tokenizer=HuggingFaceTokenizer(\"sentence-transformers/all-MiniLM-L6-v2\"), max_tokens=512)\n\n# Set Aryn Partitioning Service API key\nassert ArynConfig.get_aryn_api_key() != \"\", f\"Unable to find aryn API key.  Looked in {_DEFAULT_PATH}\"\n```\n\n----------------------------------------\n\nTITLE: Setting up OpenSearch Vector Store with LangChain for RAG Question Answering in Python\nDESCRIPTION: This code initializes an OpenSearch vector store and creates a question-answering system using LangChain and OpenAI. It configures the OpenSearch connection, creates a retriever, and sets up a RetrievalQAWithSourcesChain with ChatGPT to answer questions based on retrieved documents.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/opensearch-writer.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom langchain_community.vectorstores import OpenSearchVectorSearch\nfrom langchain_openai import ChatOpenAI\nfrom langchain.chains import RetrievalQAWithSourcesChain\nimport os\n\nos_client_args.pop(\"hosts\", None)\nvector_store = OpenSearchVectorSearch(\n    opensearch_url=\"https://localhost:9200\", index_name=index_name, embedding_function=embedder, **os_client_args\n)\n\nllm = ChatOpenAI(openai_api_key=os.environ.get(\"OPENAI_API_KEY\"), model_name=\"gpt-3.5-turbo\", temperature=0.3)\nqa = RetrievalQAWithSourcesChain.from_chain_type(\n    llm=llm, chain_type=\"stuff\", retriever=vector_store.as_retriever(), verbose=True\n)\nqa.invoke({\"question\": \"How many accidents happened?\"})\n```\n\n----------------------------------------\n\nTITLE: Performing Queries and Displaying Results\nDESCRIPTION: Defines a function to execute queries against OpenSearch, retrieve top results, and display both the retrieved chunks and AI-generated answers. This function is used to demonstrate the impact of near-duplicate detection.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/ndd_example.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef do_query(query_dict):\n    url = f\"https://{opensearch_host}:9200/{index_name}/_search?search_pipeline=hybrid_rag_pipeline\"\n    with requests.post(url, json=query, verify=False) as resp:\n        res = json.loads(resp.text)\n        hits = res[\"hits\"][\"hits\"]\n        for i in range(10):\n            text = hits[i][\"_source\"][\"text_representation\"]\n            text = text.replace(\"\\n\", \" \")[:80]\n            print(f\"[{i+1}] {text}\")\n        answer = res[\"ext\"][\"retrieval_augmented_generation\"][\"answer\"]\n        print(f\"[ANSWER]\\n{answer}\")\n```\n\n----------------------------------------\n\nTITLE: Visualizing PDF Pages\nDESCRIPTION: Displays the partitioned PDF pages to visualize the chunking results. Requires poppler library installation\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/sycamore-tutorial-intermediate-etl.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom sycamore.utils.pdf_utils import show_pages\n\nshow_pages(partitioned_docset)\n```\n\n----------------------------------------\n\nTITLE: Connecting to DuckDB Database in Python\nDESCRIPTION: This snippet demonstrates how to connect to a DuckDB database using Python. It creates a persistent database connection at the specified file path.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/docs/source/sycamore/connectors/duckdb.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport duckdb\nduckdb_database = duckdb.connect(DUCKDB_FILE_PATH)\n```\n\n----------------------------------------\n\nTITLE: Configuring Default Remote Processors in YAML\nDESCRIPTION: This YAML configuration specifies the default remote processors used in Sycamore search pipelines. It includes a debug processor and a deduplication processor with a threshold setting.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/docs/source/sycamore/querying_data/using_aryn_opensearch_stack/remote_processors.md#2025-04-07_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n- debug:\n    processors:\n      - debug-response:\n- dedup02:\n    processors:\n      - dedup-response:\n          threshold: 0.15\n```\n\n----------------------------------------\n\nTITLE: Reading Data from Elasticsearch using Sycamore\nDESCRIPTION: Example of reading from an Elasticsearch index into a Sycamore DocSet with query parameters and basic configuration.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/docs/source/sycamore/connectors/elasticsearch.md#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nctx = sycamore.init()\nurl = \"http://localhost:9200\"\nindex_name = \"test_index-read\"\ntarget_doc_id = \"target\"\nquery_params = {\"term\": {\"_id\": target_doc_id}}\nquery_docs = ctx.read.elasticsearch(url=url, index_name=index_name, query=query_params).take_all()\n```\n\n----------------------------------------\n\nTITLE: Viewing Partitioned Result\nDESCRIPTION: Displays the partitioned file object returned from the Aryn service, which contains the extracted document structure including tables and text elements.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/financial-docs-10k-example.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npartitioned_file\n```\n\n----------------------------------------\n\nTITLE: Validating Aryn API Key Configuration\nDESCRIPTION: Checks if the Aryn API key is properly configured in the environment. This key is required for using Aryn's document processing services.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/pinecone-writer.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom sycamore.utils.aryn_config import ArynConfig, _DEFAULT_PATH\n\nassert ArynConfig.get_aryn_api_key() != \"\", f\"Unable to find aryn API key.  Looked in {_DEFAULT_PATH}\"\n```\n\n----------------------------------------\n\nTITLE: Writing Data to Elasticsearch using Sycamore\nDESCRIPTION: Example of writing a DocSet to an Elasticsearch index using Sycamore's elasticsearch writer function with basic configuration parameters.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/docs/source/sycamore/connectors/elasticsearch.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nurl = \"http://localhost:9200\"\nindex_name = \"test_index-write\"\nwait_for_completion = \"wait_for\"\n\nds.write.elasticsearch(url=url, index_name=index_name, wait_for_completion=wait_for_completion)\n```\n\n----------------------------------------\n\nTITLE: Installing Sycamore with OpenSearch support\nDESCRIPTION: Installation command for Sycamore with OpenSearch support. This demonstrates how to install Sycamore with additional vector database connectors.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/docs/source/index.rst#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npip install sycamore-ai[opensearch]\n```\n\n----------------------------------------\n\nTITLE: Installing PDF Utilities\nDESCRIPTION: Installs Poppler utilities, which are required for PDF processing operations in the pipeline.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/docprep/text-embedding-3-small_greedy-text-element-merger_opensearch.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n!apt-get install poppler-utils\n```\n\n----------------------------------------\n\nTITLE: Initializing Sycamore and Creating DocSet from S3\nDESCRIPTION: This snippet demonstrates how to initialize Sycamore and create a DocSet by reading PDF files from an S3 bucket. It sets up the context and reads binary files in PDF format.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/docs/source/sycamore/tutorials/etl_for_opensearch.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport sycamore\n\n# local file path to the SortBenchmark dataset\npaths = \"s3://aryn-public/sort-benchmark/pdf/\"\n\n# Initializng sycamore which also initializes Ray underneath\ncontext = sycamore.init()\n\n# Creating a DocSet\ndocset = context.read.binary(paths, parallelism=1, binary_format=\"pdf\")\n```\n\n----------------------------------------\n\nTITLE: Retrieving Jupyter Connection Details via Docker\nDESCRIPTION: Command to retrieve Jupyter connection URL and authentication token from Docker container logs\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/docs/source/sycamore/querying_data/using_aryn_opensearch_stack/using_jupyter_container.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker compose logs jupyter | grep Visit\n```\n\n----------------------------------------\n\nTITLE: Searching for Text Similarity Model ID in OpenSearch\nDESCRIPTION: Query to retrieve the model ID for the TEXT_SIMILARITY function from OpenSearch ML models. Filters for non-chunked models that specifically implement text similarity functionality.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/docs/source/sycamore/querying_data/using_aryn_opensearch_stack/reranking.md#2025-04-07_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nPOST /_plugins/_ml/models/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must_not\": [\n        { \"exists\": { \"field\": \"chunk_number\" } }\n      ],\n      \"must\": [\n        { \"term\": { \"function_name\": \"TEXT_SIMILARITY\" } }\n      ]\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Filtering Table Elements from Results\nDESCRIPTION: Iterates through the list of elements and corresponding DataFrames to filter out only the table elements, discarding other document elements like text or images.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/financial-docs-10k-example.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# pull out the tables from the list of elements\ntables = []\nfor elt, dataframe in pandas:\n    if elt[\"type\"] == \"table\":\n        tables.append(dataframe)\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenSearch Client and Index Settings\nDESCRIPTION: Sets up the OpenSearch client configuration and defines index settings for storing embedded documents, including KNN vector settings for efficient similarity search.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/aryn-opensearch-bedrock-rag-example.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Set OpenSearch Service configuration for connector.\n\n# If you're running against localhost, or directly to a cluster,\n# you should use port 9200.\nopenSearch_client_args = {\n    \"hosts\": [{\"host\": \"YOUR-DOMAIN-ENDPOINT\", \"port\": 443}],\n    \"http_compress\": True,\n    \"http_auth\": (\"YOUR-OPENSEARCH-USERNAME\", \"YOUR-OPENSEARCH-PASSWORD\"),\n    \"use_ssl\": True,\n    \"verify_certs\": False,\n    \"ssl_assert_hostname\": False,\n    \"ssl_show_warn\": False,\n    \"timeout\": 120,\n}\n\nindex_settings = {\n    \"body\": {\n        \"settings\": {\n            \"index.knn\": True,\n            \"number_of_shards\": 2,\n            \"number_of_replicas\": 1,\n        },\n        \"mappings\": {\n            \"properties\": {\n                \"embedding\": {\n                    \"type\": \"knn_vector\",\n                    \"dimension\": 384,\n                    \"method\": {\"name\": \"hnsw\", \"engine\": \"faiss\"},\n                },\n                \"text\": {\"type\": \"text\"},\n            }\n        },\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Timestamp Conversion Function\nDESCRIPTION: Converts date and time strings in document properties to standardized ISO format.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/metadata-extraction.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom dateutil import parser\n\n\ndef convert_timestamp(doc: Document) -> Document:\n    if \"dateAndTime\" not in doc.properties[\"entity\"] and \"dateTime\" not in doc.properties[\"entity\"]:\n        return doc\n    raw_date: str = doc.properties[\"entity\"].get(\"dateAndTime\") or doc.properties[\"entity\"].get(\"dateTime\")\n    raw_date = raw_date.replace(\"Local\", \"\")\n    parsed_date = parser.parse(raw_date, fuzzy=True)\n    extracted_date = parsed_date.date()\n    doc.properties[\"entity\"][\"day\"] = extracted_date.isoformat()\n    if parsed_date.utcoffset():\n        doc.properties[\"entity\"][\"isoDateTime\"] = parsed_date.isoformat()\n    else:\n        doc.properties[\"entity\"][\"isoDateTime\"] = parsed_date.isoformat() + \"Z\"\n\n    return doc\n```\n\n----------------------------------------\n\nTITLE: Converting Timestamp String to Structured DateTime Format\nDESCRIPTION: Defines a function to transform date/time strings into standardized datetime objects. It extracts day, month, and year as integers to enable range-based date filtering in queries.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/pinecone-writer.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Here's a function to convert llm-generated date/time strings into well-structured datetime strings\n# We also extract the exact day, month, and year as integers in order to do range filtering in our\n# queries\nfrom sycamore.data.document import Document\nfrom dateutil import parser\n\n\ndef convert_timestamp(doc: Document) -> Document:\n    if \"dateAndTime\" not in doc.properties[\"entity\"] and \"dateTime\" not in doc.properties[\"entity\"]:\n        return doc\n    raw_date: str = doc.properties[\"entity\"].get(\"dateAndTime\") or doc.properties[\"entity\"].get(\"dateTime\")\n    raw_date = raw_date.replace(\"Local\", \"\")\n    parsed_date = parser.parse(raw_date, fuzzy=True)\n    extracted_date = parsed_date.date()\n    doc.properties[\"entity\"][\"day\"] = extracted_date.day\n    doc.properties[\"entity\"][\"month\"] = extracted_date.month\n    doc.properties[\"entity\"][\"year\"] = extracted_date.year\n    if parsed_date.utcoffset():\n        doc.properties[\"entity\"][\"dateTime\"] = parsed_date.isoformat()\n    else:\n        doc.properties[\"entity\"][\"dateTime\"] = parsed_date.isoformat() + \"Z\"\n\n    return doc\n```\n\n----------------------------------------\n\nTITLE: Verifying Data Loading with OpenSearch Query\nDESCRIPTION: Executes a match_all query against the OpenSearch index to verify that documents have been successfully loaded. The query retrieves all documents without displaying the embedding vectors.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/docprep/text-embedding-3-small_greedy-text-element-merger_opensearch.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Verify data has been loaded using DocSet Query to retrieve chunks\nquery_docs = ctx.read.opensearch(\n    os_client_args=os_client_args, index_name=index_name, query={\"query\": {\"match_all\": {}}}\n)\nquery_docs.show(show_embedding=False)\n```\n\n----------------------------------------\n\nTITLE: Building OpenSearch Docker Image\nDESCRIPTION: Builds a custom OpenSearch Docker image from source code\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/apps/opensearch/2.11/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd pyscripts/docker_service/opensearch\ndocker build -t aryn_opensearch .\n```\n\n----------------------------------------\n\nTITLE: Setting Aryn API Key Environment Variable\nDESCRIPTION: Sets the ARYN_API_KEY environment variable for authentication with Sycamore AI services.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/aryn-opensearch-bedrock-rag-example.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"ARYN_API_KEY\"] = \"YOUR-ARYN-API-KEY\"\n```\n\n----------------------------------------\n\nTITLE: Documenting BedrockEmbedder Class in Python\nDESCRIPTION: Auto-generated documentation for the BedrockEmbedder class in the sycamore.transforms.embed module. This class likely handles embedding operations using Amazon's Bedrock service.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/docs/source/sycamore/APIs/low_level_transforms/embed.rst#2025-04-07_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: sycamore.transforms.embed.BedrockEmbedder\n   :members:\n   :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Reading Binary PDF Files from S3\nDESCRIPTION: Reads PDF files from an S3 bucket into a DocSet structure. Uses lazy execution pattern, requiring explicit execution commands to process the files\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/sycamore-tutorial-intermediate-etl.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npaths = [\"s3://aryn-public/ntsb/\"]\n\ninitial_docset = context.read.binary(paths=paths, binary_format=\"pdf\")\ninitial_docset\n```\n\n----------------------------------------\n\nTITLE: Importing Sycamore Libraries and Dependencies\nDESCRIPTION: Sets up the necessary imports for the document processing pipeline, including Sycamore components, PyArrow filesystem, and transformers for document processing and extraction.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/pinecone-writer.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# First some imports\nimport pyarrow.fs\nimport sycamore\nfrom sycamore.functions.tokenizer import HuggingFaceTokenizer\nfrom sycamore.llms import OpenAIModels, OpenAI\nfrom sycamore.transforms import COALESCE_WHITESPACE\nfrom sycamore.transforms.merge_elements import MarkedMerger\nfrom sycamore.transforms.partition import ArynPartitioner\nfrom sycamore.transforms.extract_schema import OpenAISchemaExtractor, OpenAIPropertyExtractor\nfrom sycamore.transforms.embed import SentenceTransformerEmbedder\n```\n\n----------------------------------------\n\nTITLE: Reading from DuckDB into Sycamore DocSet\nDESCRIPTION: This code snippet illustrates how to read data from a DuckDB table into a Sycamore DocSet. It initializes a Sycamore context, specifies the table and database, and executes a SQL query to retrieve data.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/docs/source/sycamore/connectors/duckdb.md#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nctx = sycamore.init()\ntable_name = \"duckdb_table\"\ndb_url = \"tmp_read.db\"  \ntarget_doc_id = \"target\"\nquery = f\"SELECT * from {table_name} WHERE doc_id == '{target_doc_id}'\"\nquery_docs = ctx.read.duckdb(db_url=db_url, table_name=table_name, query=query).take_all()\n```\n\n----------------------------------------\n\nTITLE: Installing Sycamore with Database Connector\nDESCRIPTION: Example of installing Sycamore with an additional database connector (DuckDB). Similar syntax can be used for other supported connectors like elasticsearch, opensearch, pinecone, qdrant, and weaviate.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/README.md#2025-04-07_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npip install sycamore-ai[duckdb]\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx Autodoc for Mark Misc Classes\nDESCRIPTION: Sphinx documentation configuration directives for three transform classes in the mark_misc module. Uses autoclass to automatically generate class documentation including all members and inheritance information.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/docs/source/sycamore/APIs/low_level_transforms/mark_misc.rst#2025-04-07_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: sycamore.transforms.mark_misc.MarkBreakByTokens\n   :members:\n   :show-inheritance:\n.. autoclass:: sycamore.transforms.mark_misc.MarkBreakPage\n   :members:\n   :show-inheritance:\n.. autoclass:: sycamore.transforms.mark_misc.MarkDropTiny\n   :members:\n   :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Executing a Simple Semantic Search Query\nDESCRIPTION: Performs a semantic search for incidents in California using the previously defined query function.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/pinecone-writer.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nr = pure_semantic_query(\"incidents in california\")\n```\n\n----------------------------------------\n\nTITLE: Extracting Metadata with OpenAI GPT\nDESCRIPTION: Extracts structured metadata from documents using OpenAI's GPT model based on a predefined schema\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/sycamore-tutorial-intermediate-etl.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom sycamore.transforms.extract_schema import OpenAIPropertyExtractor\nfrom sycamore.llms import OpenAI, OpenAIModels\n\nllm = OpenAI(OpenAIModels.GPT_4O.value)\n\nenriched_docset = (\n    regex_docset.with_property(\"_schema_class\", lambda d: \"FlightAccidentReport\")\n    .with_property(\n        \"_schema\",\n        lambda d: {\n            \"type\": \"object\",\n            \"properties\": {\n                \"accidentNumber\": {\"type\": \"string\"},\n                \"dateAndTime\": {\"type\": \"string\"},\n                \"location\": {\"type\": \"string\"},\n                \"aircraft\": {\"type\": \"string\"},\n                \"aircraftDamage\": {\"type\": \"string\"},\n                \"injuries\": {\"type\": \"string\"},\n                \"definingEvent\": {\"type\": \"string\"},\n            },\n            \"required\": [\n                \"accidentNumber\",\n                \"dateAndTime\",\n                \"location\",\n                \"aircraft\",\n                \"aircraftDamage\",\n                \"injuries\",\n                \"definingEvent\",\n            ],\n        },\n    )\n    .extract_properties(\n        property_extractor=OpenAIPropertyExtractor(\n            llm=llm,\n            num_of_elements=35,\n        )\n    )\n)\n\nenriched_docset.show(limit=1)\n```\n\n----------------------------------------\n\nTITLE: Creating a Query Vector Generator for Semantic Search\nDESCRIPTION: Defines a function to generate query vectors using the SentenceTransformer model, which enables semantic search capabilities in Pinecone.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/pinecone-writer.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Pinecone needs us to generate our own query vectors, so we define a function to simplify this\nfrom sentence_transformers import SentenceTransformer\n\nminilm = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n\n\ndef q_vec(question):\n    v = minilm.encode(question).tolist()\n    return v\n```\n\n----------------------------------------\n\nTITLE: Implementing Entity Extraction using OpenAI in Sycamore\nDESCRIPTION: This code snippet demonstrates how to use Sycamore's entity extractor transform with OpenAI as the generative AI model. It extracts titles and authors from the partitioned dataset, spreads the properties, and displays the results.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/docs/source/sycamore/tutorials/sycamore_jupyter_dev_example.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nentity_docset = (partitioned_docset\n                 .extract_entity(entity_extractor=OpenAIEntityExtractor(\"title\", llm=openai_llm, prompt_template=title_context_template))\n                 .extract_entity(entity_extractor=OpenAIEntityExtractor(\"authors\", llm=openai_llm, prompt_template=author_context_template)))\n\nentity_docset = entity_docset.spread_properties([\"title\", \"authors\"])\n\nentity_docset.show(show_binary = False, show_elements=False)\n```\n\n----------------------------------------\n\nTITLE: Running OpenSearch Container\nDESCRIPTION: Starts the OpenSearch container with necessary configurations including network, port mapping, and volume mounting\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/apps/opensearch/2.11/README.md#2025-04-07_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -it --rm --name aryn_opensearch --network aryn-app -p 9200:9200 -e OPENAI_API_KEY --volume opensearch_data:/usr/share/opensearch/data aryn/opensearch\n```\n\n----------------------------------------\n\nTITLE: OpenSearch Configuration\nDESCRIPTION: Sets up OpenSearch connection parameters and index settings with HNSW vector search configuration.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/metadata-extraction.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom sycamore.transforms.embed import SentenceTransformerEmbedder\n\nif os.path.exists(\"/.dockerenv\"):\n    opensearch_host = \"opensearch\"\n    print(\"Assuming we are in a Sycamore Jupyter container, using opensearch for OpenSearch host\")\nelse:\n    opensearch_host = \"localhost\"\n    print(\"Assuming we are running outside of a container, using localhost for OpenSearch host\")\n\nindex = \"ntsb_demoindex0\"\nos_client_args = {\n    \"hosts\": [{\"host\": opensearch_host, \"port\": 9200}],\n    \"http_compress\": True,\n    \"http_auth\": (\"admin\", \"admin\"),\n    \"use_ssl\": True,\n    \"verify_certs\": False,\n    \"ssl_assert_hostname\": False,\n    \"ssl_show_warn\": False,\n    \"timeout\": 120,\n}\n\nindex_settings = {\n    \"body\": {\n        \"settings\": {\"index.knn\": True, \"number_of_shards\": 5, \"number_of_replicas\": 1},\n        \"mappings\": {\n            \"properties\": {\n                \"embedding\": {\n                    \"dimension\": 384,\n                    \"method\": {\"engine\": \"faiss\", \"space_type\": \"l2\", \"name\": \"hnsw\", \"parameters\": {}},\n                    \"type\": \"knn_vector\",\n                }\n            }\n        },\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Elasticsearch Docker Setup\nDESCRIPTION: Docker compose configuration for running Elasticsearch locally with basic settings including memory limits, security configuration, and port mapping.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/docs/source/sycamore/connectors/elasticsearch.md#2025-04-07_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nversion: \"3.8\"\nservices:\n  elasticsearch:\n    image: docker.elastic.co/elasticsearch/elasticsearch:8.14.2\n    ports:\n      - 9200:9200\n    restart: on-failure\n    environment:\n      - discovery.type=single-node\n      - xpack.security.enabled=false\n      - ES_JAVA_OPTS=-Xms4g -Xmx4g\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n```\n\n----------------------------------------\n\nTITLE: Spreading Document Properties and Exploding Elements in Sycamore\nDESCRIPTION: This snippet demonstrates how to copy properties from documents to their elements and then promote elements to top-level documents. This prepares the data for embedding since Sycamore's embed transform only operates on documents.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/sycamore-tutorial-intermediate-etl.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nexploded_docset = formatted_docset.spread_properties([\"path\", \"entity\"]).explode()\n```\n\n----------------------------------------\n\nTITLE: Aryn API Key Configuration\nDESCRIPTION: YAML configuration format for storing the Aryn API key.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/metadata-extraction.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\naryn_token: \"YOUR-ARYN-API-KEY\"\n```\n\n----------------------------------------\n\nTITLE: Installing Sycamore AI Dependencies\nDESCRIPTION: Installs the Sycamore AI library with OpenSearch and local inference support using pip.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/aryn-opensearch-bedrock-rag-example.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install \"sycamore-ai[opensearch,local-inference]\"\n```\n\n----------------------------------------\n\nTITLE: Downloading Sample 10K Document from S3\nDESCRIPTION: Downloads a sample 10K financial document from Aryn's public S3 bucket and opens it for processing. Uses boto3 client to handle the S3 connection and file operations.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/financial-docs-10k-example.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfile_name = \"3m_10k.pdf\"\ns3 = boto3.client(\"s3\")\ns3.download_file(\"aryn-public\", \"partitioner-blog-data/3m_10k.pdf\", file_name)\nf = open(file_name, \"rb\")\n```\n\n----------------------------------------\n\nTITLE: Installing Sycamore with local inference support\nDESCRIPTION: Installation command for Sycamore with local partitioning and embedding models. This installs Sycamore with the 'local-inference' extra for additional functionality.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/docs/source/index.rst#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npip install sycamore-ai[local-inference]\n```\n\n----------------------------------------\n\nTITLE: Installing Sycamore Base Package with pip\nDESCRIPTION: Basic installation command for the Sycamore AI package using pip package manager.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/README.md#2025-04-07_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install sycamore-ai\n```\n\n----------------------------------------\n\nTITLE: Implementing RL Policy Training with Ray Remote Functions\nDESCRIPTION: Example implementation of reinforcement learning policy training using Ray's remote function decorators and actor classes. Shows core components including policy creation, simulation environment, and policy updates using GPU resources.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/lib/sycamore/sycamore/tests/resources/data/texts/Ray.txt#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@ray.remote\ndef create_policy():\n    # Initialize the policy randomly.\n    return policy\n\n@ray.remote(num_gpus=1)\nclass Simulator(object):\n    def __init__(self):\n        # Initialize the environment.\n        self.env = Environment()\n\n    def rollout(self, policy, num_steps):\n        observations = []\n        observation = self.env.current_state()\n        for _ in range(num_steps):\n            action = policy(observation)\n            observation = self.env.step(action)\n            observations.append(observation)\n        return observations\n\n@ray.remote(num_gpus=2)\ndef update_policy(policy, *rollouts):\n    # Update the policy.\n    return policy\n\n@ray.remote\ndef train_policy():\n    # Create a policy.\n    policy_id = create_policy.remote()\n    # Create 10 actors.\n    simulators = [Simulator.remote() for _ in range(10)]\n    # Do 100 steps of training.\n    for _ in range(100):\n```\n\n----------------------------------------\n\nTITLE: Displaying Hybrid Search Results\nDESCRIPTION: Shows the results of the hybrid search query using the previously defined pretty printer function.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/pinecone-writer.ipynb#2025-04-07_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nprint_results_pretty(rh)\n```\n\n----------------------------------------\n\nTITLE: Executing Query Without Near-Duplicate Detection\nDESCRIPTION: Constructs and executes a query without enabling near-duplicate detection. This query retrieves results and generates an AI answer based on the top hits, potentially including duplicates.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/ndd_example.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nquery_str = \"how does force majeure affect assets and insolvency\"\nquery = {\n    \"_source\": [\n        \"text_representation\",\n    ],\n    \"query\": {\n        \"hybrid\": {\n            \"queries\": [\n                {\n                    \"match\": {\"text_representation\": query_str},\n                },\n                {\n                    \"neural\": {\n                        \"embedding\": {\n                            \"query_text\": query_str,\n                            \"k\": 100,\n                            \"model_id\": get_model_id(),\n                        },\n                    },\n                },\n            ],\n        },\n    },\n    \"ext\": {\n        \"generative_qa_parameters\": {\n            \"llm_question\": query_str,\n            \"context_size\": 10,\n            \"llm_model\": \"gpt-4\",\n        },\n    },\n    \"size\": 100,\n}\ndo_query(query)\n```\n\n----------------------------------------\n\nTITLE: Selecting the Industry Geographic Breakdown Table\nDESCRIPTION: Selects the first table from the filtered list, which contains the industry geographic breakdown data from the 10K document.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/financial-docs-10k-example.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# pull out the first table\nindustry_geographic_breakdown = tables[0]\n```\n\n----------------------------------------\n\nTITLE: Generating Sparse Vectors for Term Frequency Search\nDESCRIPTION: Creates a function to generate sparse vectors based on term frequency for text search, enabling hybrid search capabilities in combination with dense vectors.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/pinecone-writer.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# We also have to generate our own sparse vectors. Note that the pinecone implementation of\n# sparse vector search makes it difficult to perform BM-25 (TF/IDF) search. Instead we use\n# pure term frequency, which will cause some common words to be inordinately 'meaningful'.\nfrom collections import Counter\n\n\ndef s_vec(question):\n    tokens = tokenizer.tokenize(question, as_ints=True)\n    table = dict(Counter(tokens))\n    indices = list(table.keys())\n    values = [float(v) for v in table.values()]\n    return {\"indices\": indices, \"values\": values}\n```\n\n----------------------------------------\n\nTITLE: Executing a Hybrid Search Query\nDESCRIPTION: Performs a hybrid search combining both semantic and term frequency approaches to search for incidents in California, with an 80% weight on semantic search.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/pinecone-writer.ipynb#2025-04-07_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nrh = hybrid_query(\"incidents in california\", alpha=0.8)\n```\n\n----------------------------------------\n\nTITLE: Implementing Hybrid Search with Adjustable Weights\nDESCRIPTION: Creates a function that combines both dense (semantic) and sparse (term frequency) vectors for hybrid search, with an alpha parameter to control the balance between them.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/pinecone-writer.ipynb#2025-04-07_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# Function to combine sparse (TF) and dense (embedding) vectors into a\n# query. Alpha is the weight to give to the dense vector as opposed to\n# the sparse vector... alpha=1 means only care about the dense vector\n# and alpha=0 means only care about the sparse vector.\ndef hybrid_query(question, alpha=0.8):\n    qv = q_vec(question)\n    sv = s_vec(question)\n    qv = [v * alpha for v in qv]\n    sv[\"values\"] = [v * (1 - alpha) for v in sv[\"values\"]]\n    results = ntsb.query(\n        top_k=5,\n        vector=qv,\n        sparse_vector=sv,\n        include_metadata=True,\n    )\n    return results\n```\n\n----------------------------------------\n\nTITLE: Creating OpenSearch Docker Volume\nDESCRIPTION: Creates a Docker volume to persist OpenSearch data\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/apps/opensearch/2.11/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker volume create opensearch_data\n```\n\n----------------------------------------\n\nTITLE: Partitioning Documents with Aryn DocParse\nDESCRIPTION: Partitions documents into labeled elements using Aryn DocParse, including table structure recognition and OCR processing. Includes materialization for checkpoint creation\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/sycamore-tutorial-intermediate-etl.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom sycamore.transforms.partition import ArynPartitioner\n\npartitioned_docset = initial_docset.partition(\n    partitioner=ArynPartitioner(extract_images=False, extract_table_structure=True, use_ocr=True)\n).materialize(\n    path=\"./pc-tutorial/partitioned\", source_mode=sycamore.materialize_config.MaterializeSourceMode.IF_PRESENT\n)\npartitioned_docset.execute()\n```\n\n----------------------------------------\n\nTITLE: Documenting OpenAIEmbeddingModels Class in Python\nDESCRIPTION: Auto-generated documentation for the OpenAIEmbeddingModels class in the sycamore.transforms.embed module. This class likely defines the available embedding models for OpenAI.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/docs/source/sycamore/APIs/low_level_transforms/embed.rst#2025-04-07_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: sycamore.transforms.embed.OpenAIEmbeddingModels\n   :members:\n   :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: DuckDB Storage Configuration\nDESCRIPTION: Configures and executes the storage of embedded documents in DuckDB\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/docprep/text-embedding-3-small_greedy-section-merger_duckdb.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndb_url = \"test-1.db\"\ntable_name = \"test_table\"\n# Execute the write operation to DuckDB\nembedded_ds.write.duckdb(db_url=db_url, table_name=table_name, dimensions=dimensions)\n```\n\n----------------------------------------\n\nTITLE: Document Processing Pipeline Setup\nDESCRIPTION: Configures and executes the document processing pipeline including PDF reading, partitioning, and element merging\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/docprep/text-embedding-3-small_greedy-section-merger_duckdb.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Sycamore uses lazy execution for efficiency, so the ETL pipeline will only execute when running cells with specific functions.\n\npaths = [\"s3://aryn-public/ntsb/59.pdf\"]\n# Configure your AWS credentials here if the bucket is private\nfsys = pyarrow.fs.S3FileSystem(region=\"us-east-1\", anonymous=True)\n# Initialize the Sycamore context\nctx = sycamore.init(ExecMode.LOCAL)\n# Set the embedding model and its parameters\nmodel_name = \"text-embedding-3-small\"\nmax_tokens = 8191\ndimensions = 1536\n# Initialize the tokenizer\ntokenizer = OpenAITokenizer(model_name)\n\nds = (\n    ctx.read.binary(paths, binary_format=\"pdf\", filesystem=fsys)\n    # Partition and extract tables and images\n    .partition(\n        partitioner=ArynPartitioner(threshold=\"auto\", use_ocr=True, extract_table_structure=True, extract_images=True)\n    )\n    # Use materialize to cache output. If changing upstream code or input files, change setting from USE_STORED to RECOMPUTE to create a new cache.\n    .materialize(path=\"./materialize/partitioned\", source_mode=MaterializeSourceMode.USE_STORED)\n    # Merge elements into larger chunks\n    .merge(merger=GreedySectionMerger(tokenizer=tokenizer, max_tokens=max_tokens, merge_across_pages=False))\n    # Split elements that are too big to embed\n    .split_elements(tokenizer=tokenizer, max_tokens=max_tokens)\n)\n\nds.execute()\n\n# Display the first 3 pages after chunking\nshow_pages(ds, limit=3)\n```\n\n----------------------------------------\n\nTITLE: Writing Sycamore DocSet to DuckDB\nDESCRIPTION: This snippet shows how to write a Sycamore DocSet to a DuckDB database table. It demonstrates the use of the write.duckdb() method with table name and database URL parameters.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/docs/source/sycamore/connectors/duckdb.md#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nds.write.duckdb(table_name=table_name, db_url=db_url)\n```\n\n----------------------------------------\n\nTITLE: Creating Basic Rerank Pipeline in OpenSearch\nDESCRIPTION: Creates a search pipeline that implements reranking using a specified ML model. The pipeline processes document fields to rerank search results based on text similarity.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/docs/source/sycamore/querying_data/using_aryn_opensearch_stack/reranking.md#2025-04-07_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nPUT /_search/pipeline/rerank-pipeline\n{\n  \"response_processors\": [\n    {\n      \"rerank\": {\n        \"ml_opensearch\": {\n          \"model_id\": \"<reranker_id>\"\n        },\n        \"context\": {\n          \"document_fields\": [ \"text_representation\" ]\n        }\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Displaying Formatted Search Results\nDESCRIPTION: Demonstrates the use of the pretty printer to display the search results in a user-friendly format.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/pinecone-writer.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nprint_results_pretty(r)\n```\n\n----------------------------------------\n\nTITLE: Merging Entity-Enriched DocSet in Sycamore\nDESCRIPTION: This snippet updates the merging process to use the entity-enriched DocSet instead of the partitioned DocSet. It uses a GreedyTextElementMerger with a HuggingFace tokenizer to merge the documents with a maximum token limit of 512.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/docs/source/sycamore/tutorials/sycamore_jupyter_dev_example.md#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmerged_docset = entity_docset.merge(GreedyTextElementMerger(tokenizer=HuggingFaceTokenizer(\"sentence-transformers/all-MiniLM-L6-v2\"), max_tokens=512))\nmerged_docset.show(show_binary = False)\n```\n\n----------------------------------------\n\nTITLE: Implementing Pure Semantic Search with Pinecone\nDESCRIPTION: Creates a function for performing semantic vector search using only the dense vector representation of queries, returning the top 5 relevant results.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/pinecone-writer.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Function to query the ntsb index with only dense semantic search (using q_vec to generate the vector)\ndef pure_semantic_query(question):\n    results = ntsb.query(\n        top_k=5,\n        vector=q_vec(question),\n        include_metadata=True,\n    )\n    return results\n```\n\n----------------------------------------\n\nTITLE: Implementing Document Summarization with LLM in Python\nDESCRIPTION: Demonstrates how to create a document summarization pipeline using LLMElementTextSummarizer with OpenAI. The code includes a custom filter function that selects elements based on text length and applies the summarization transform to a document set.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/docs/source/sycamore/transforms/summarize.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef filter_elements_on_length(\n    document: Document,\n    minimum_length: int = 10,\n) -> list[Element]:\n    def filter_func(element: Element):\n        if element.text_representation is not None:\n            return len(element.text_representation) > minimum_length\n\n    return filter_elements(document, filter_func)\n\nllm = OpenAI(OpenAIModels.GPT_3_5_TURBO.value)\n\ndocset = docset.summarize(LLMElementTextSummarizer(llm, filter_elements_on_length))\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents in reStructuredText\nDESCRIPTION: This snippet defines a table of contents using reStructuredText syntax. It sets the maximum depth to 2 and includes a link to a Markdown file containing function documentation for conversation memory APIs.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/docs/source/sycamore/querying_data/using_aryn_opensearch_stack/APIs/conversation_memory.rst#2025-04-07_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\n   :maxdepth: 2\n\n   /APIs/conversation_memory/functions.md\n```\n\n----------------------------------------\n\nTITLE: Exploding Documents and Creating Embeddings\nDESCRIPTION: This code explodes each element of a document into a top-level document and creates embeddings using SentenceTransformerEmbedder. It uses the 'sentence-transformers/all-MiniLM-L6-v2' model for embedding.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/docs/source/sycamore/tutorials/etl_for_opensearch.md#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom sycamore.transforms.embed import SentenceTransformerEmbedder\n\n# We are using SentenceTransformerEmbedder to embed the content of each document; which\n# uses the SentenceTransformer model. You can write your own Embedder as well.\ndocset = docset.explode()\n.embed(embedder=SentenceTransformerEmbedder(batch_size=100, model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n```\n\n----------------------------------------\n\nTITLE: Installing Sycamore AI Dependencies\nDESCRIPTION: Installs the Sycamore AI package with DuckDB support and poppler-utils for PDF processing\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/docprep/text-embedding-3-small_greedy-section-merger_duckdb.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install sycamore-ai[duckdb]\n# DocPrep code uses the Sycamore document ETL library: https://github.com/aryn-ai/sycamore\n```\n\nLANGUAGE: python\nCODE:\n```\n!apt-get install poppler-utils\n```\n\n----------------------------------------\n\nTITLE: Setting Aryn API Key\nDESCRIPTION: Defines the API key variable required to authenticate with the Aryn DocParse service. Users need to obtain this key from aryn.ai/cloud.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/financial-docs-10k-example.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# visit https://www.aryn.ai/cloud to get a key\naryn_api_key = \"YOUR_ARYN_API_KEY\"\n```\n\n----------------------------------------\n\nTITLE: Initializing Sycamore Environment and Dependencies\nDESCRIPTION: Sets up the Sycamore environment by importing necessary libraries, configuring warnings, and initializing the Aryn API key. It also defines key variables for data ingestion and OpenSearch configuration.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/ndd_example.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport json\nimport requests\nimport warnings\nimport urllib3\nimport pyarrow.fs\nimport sycamore\nfrom sycamore.functions.tokenizer import HuggingFaceTokenizer\nfrom sycamore.transforms import COALESCE_WHITESPACE\nfrom sycamore.transforms.merge_elements import MarkedMerger\nfrom sycamore.transforms.partition import ArynPartitioner\nfrom sycamore.transforms.embed import SentenceTransformerEmbedder\n\nwarnings.filterwarnings(\"ignore\", category=urllib3.exceptions.InsecureRequestWarning)\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom sycamore.utils.aryn_config import ArynConfig, _DEFAULT_PATH\n\nassert ArynConfig.get_aryn_api_key() != \"\", f\"Unable to find aryn API key.  Looked in {_DEFAULT_PATH}\"\n```\n\nLANGUAGE: python\nCODE:\n```\n# Set to False to ingest the PDFs from scratch, which takes an hour or more\nuse_json = True\n\n# Set to False to use all available CPU and memory\nsave_resources = True\n\n# Different hostnames inside and outside Docker compose environment\nopensearch_host = \"opensearch\" if os.path.exists(\"/.dockerenv\") else \"localhost\"\n\nindex_name = \"demoindex0\"\n```\n\n----------------------------------------\n\nTITLE: Configuring and Executing the Document Processing Pipeline\nDESCRIPTION: Sets up and executes a Sycamore ETL pipeline that loads PDFs from S3, partitions content, merges text elements, and splits them based on token limits. The pipeline uses the OpenAI tokenizer and caches results for efficiency.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/docprep/text-embedding-3-small_greedy-text-element-merger_opensearch.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Sycamore uses lazy execution for efficiency, so the ETL pipeline will only execute when running cells with specific functions.\n\npaths = [\"s3://aryn-public/ntsb/59.pdf\"]\n# Configure your AWS credentials here if the bucket is private\nfsys = pyarrow.fs.S3FileSystem(region=\"us-east-1\", anonymous=True)\n# Initialize the Sycamore context\nctx = sycamore.init(ExecMode.LOCAL)\n# Set the embedding model and its parameters\nmodel_name = \"text-embedding-3-small\"\nmax_tokens = 8191\ndimensions = 1536\n# Initialize the tokenizer\ntokenizer = OpenAITokenizer(model_name)\n\nds = (\n    ctx.read.binary(paths, binary_format=\"pdf\", filesystem=fsys)\n    # Partition and extract tables and images\n    .partition(\n        partitioner=ArynPartitioner(threshold=\"auto\", use_ocr=True, extract_table_structure=True, extract_images=True)\n    )\n    # Use materialize to cache output. If changing upstream code or input files, change setting from USE_STORED to RECOMPUTE to create a new cache.\n    .materialize(path=\"./materialize/partitioned\", source_mode=MaterializeSourceMode.USE_STORED)\n    # Merge elements into larger chunks\n    .merge(merger=GreedyTextElementMerger(tokenizer=tokenizer, max_tokens=max_tokens, merge_across_pages=False))\n    # Split elements that are too big to embed\n    .split_elements(tokenizer=tokenizer, max_tokens=max_tokens)\n)\n\nds.execute()\n\n# Display the first 3 pages after chunking\nshow_pages(ds, limit=3)\n```\n\n----------------------------------------\n\nTITLE: Implementing Filtered Hybrid Search\nDESCRIPTION: Creates a function that enhances hybrid search with metadata filtering capabilities, allowing more precise queries based on document properties.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/pinecone-writer.ipynb#2025-04-07_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n# Instead we can add filters.\ndef hybrid_query_filtered(question, filter, alpha=0.8):\n    qv = q_vec(question)\n    sv = s_vec(question)\n    qv = [v * alpha for v in qv]\n    sv[\"values\"] = [v * (1 - alpha) for v in sv[\"values\"]]\n    results = ntsb.query(top_k=5, vector=qv, sparse_vector=sv, include_metadata=True, filter=filter)\n    return results\n```\n\n----------------------------------------\n\nTITLE: Documenting Sycamore Functions Module in RST\nDESCRIPTION: Sphinx documentation directive to automatically generate documentation for all members of the sycamore.functions module using reStructuredText format.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/docs/source/sycamore/APIs/functions.rst#2025-04-07_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: sycamore.functions\n   :members:\n```\n\n----------------------------------------\n\nTITLE: Complete Sycamore ETL Pipeline for Document Processing\nDESCRIPTION: A full end-to-end pipeline that combines all steps: reading PDFs from S3, partitioning, text processing, property extraction, chunking, embedding, and storing in Pinecone. This shows how to chain Sycamore's transforms into a cohesive data processing workflow.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/sycamore-tutorial-intermediate-etl.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ns3_fs = sycamore.filesystem.S3FileSystem()\nembedder = OpenAIEmbedder(model_name=embedding_model)\ncontext.read.binary(paths=paths, binary_format=\"pdf\", filesystem=s3_fs).partition(\n    partitioner=ArynPartitioner(extract_images=False, extract_table_structure=True)\n).regex_replace(COALESCE_WHITESPACE).with_property(\"_schema_class\", lambda d: \"FlightAccidentReport\").with_property(\n    \"_schema\",\n    lambda d: {\n        \"type\": \"object\",\n        \"properties\": {\n            \"accidentNumber\": {\"type\": \"string\"},\n            \"dateAndTime\": {\"type\": \"string\"},\n            \"location\": {\"type\": \"string\"},\n            \"aircraft\": {\"type\": \"string\"},\n            \"aircraftDamage\": {\"type\": \"string\"},\n            \"injuries\": {\"type\": \"string\"},\n            \"definingEvent\": {\"type\": \"string\"},\n        },\n        \"required\": [\n            \"accidentNumber\",\n            \"dateAndTime\",\n            \"location\",\n            \"aircraft\",\n            \"aircraftDamage\",\n            \"injuries\",\n            \"definingEvent\",\n        ],\n    },\n).extract_properties(\n    property_extractor=OpenAIPropertyExtractor(\n        llm=llm,\n        num_of_elements=35,\n    )\n).mark_bbox_preset(\n    tokenizer=tokenizer, token_limit=max_tokens\n).merge(\n    merger=MarkedMerger()\n).split_elements(\n    tokenizer=tokenizer, max_tokens=max_tokens\n).map(\n    convert_timestamp\n).spread_properties(\n    [\"path\", \"entity\"]\n).explode().embed(\n    embedder=embedder\n).term_frequency(\n    tokenizer=tokenizer, with_token_ids=True\n).write.pinecone(\n    index_name=\"ntsb-live\",\n    index_spec=pinecone.ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n    dimensions=embedding_dim,\n    distance_metric=\"dotproduct\",\n    namespace=\"aryntutorial\",\n)\n```\n\n----------------------------------------\n\nTITLE: Data Verification Query\nDESCRIPTION: Executes a verification query to check if data has been properly loaded into DuckDB\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/docprep/text-embedding-3-small_greedy-section-merger_duckdb.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Verify data has been loaded using DocSet Query to retrieve chunks\n# If you previously used a DuckDB in Colab with a different number of vector dimensions, you may need to restart the runtime.\nquery = f\"SELECT * from {table_name}\"\nquery_docs = ctx.read.duckdb(db_url=db_url, table_name=table_name, query=query)\nquery_docs.show(show_embedding=False)\n```\n\n----------------------------------------\n\nTITLE: Installing and Importing Aryn SDK Dependencies\nDESCRIPTION: Imports the necessary libraries including boto3 for AWS S3 access and specific functions from aryn_sdk for document partitioning and table extraction.\nSOURCE: https://github.com/aryn-ai/sycamore/blob/main/notebooks/financial-docs-10k-example.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Run pip install aryn-sdk first\nimport boto3\nfrom aryn_sdk.partition import partition_file, tables_to_pandas\n```"
  }
]
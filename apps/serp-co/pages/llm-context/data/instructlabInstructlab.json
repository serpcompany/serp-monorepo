[
  {
    "owner": "instructlab",
    "repo": "instructlab",
    "content": "TITLE: Converting and Quantizing Models for QLoRA in MLX\nDESCRIPTION: This command uses the convert.py script to create a 4-bit quantized version of a Hugging Face model for use with QLoRA. It takes a model repository as input and outputs a quantized model.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/src/instructlab/train/lora_mlx/README.md#2025-04-21_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npython convert.py --hf-path <hf_repo> -q\n```\n\n----------------------------------------\n\nTITLE: Quantizing Mistral 7B Model for QLoRA in MLX\nDESCRIPTION: This specific example demonstrates how to create a 4-bit quantized version of the Mistral 7B model using the convert.py script. The output is stored in the default 'mlx_model' directory.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/src/instructlab/train/lora_mlx/README.md#2025-04-21_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npython convert.py --hf-path mistralai/Mistral-7B-v0.1 -q\n```\n\n----------------------------------------\n\nTITLE: Running ilab Model Training with GPU\nDESCRIPTION: Command to run ilab model training using GPU acceleration, specifying the CUDA device.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/accelerators/gpu-acceleration.md#2025-04-21_snippet_19\n\nLANGUAGE: shell\nCODE:\n```\nilab model train --device cuda\n```\n\n----------------------------------------\n\nTITLE: Fine-Tuning LLM with LoRA or QLoRA in MLX\nDESCRIPTION: This command uses the lora.py script to fine-tune a model with LoRA or QLoRA. It specifies the model path, enables training mode, and sets the number of iterations. If the model is quantized, it uses QLoRA; otherwise, it uses regular LoRA.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/src/instructlab/train/lora_mlx/README.md#2025-04-21_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npython lora.py --model <path_to_model> \\\n               --train \\\n               --iters 600\n```\n\n----------------------------------------\n\nTITLE: Generating Text with LoRA-Adapted Model in MLX\nDESCRIPTION: This command demonstrates how to use a LoRA-adapted model for text generation. It specifies the model, adapter file, maximum number of tokens to generate, and provides a prompt for the model to complete.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/src/instructlab/train/lora_mlx/README.md#2025-04-21_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\npython lora.py --model <path_to_model> \\\n               --adapter-file <path_to_adapters.npz> \\\n               --max-tokens 50 \\\n               --prompt \"table: 1-10015132-16\\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\\nQ: What is terrence ross' nationality\\nA: \"\n```\n\n----------------------------------------\n\nTITLE: Cloud Instance Management Examples\nDESCRIPTION: Comprehensive set of example commands for AWS EC2 instance management, including launching, setting up development environments, installing drivers, SSH access, syncing repositories, and instance lifecycle operations.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/scripts/infra/README.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Launch a new instance with the instance type in config including Nvidia GPU(s)\n# Will use `EC2_INSTANCE_NAME` and `EC2_INSTANCE_TYPE` if `ec2` is specified\n# Will use `IBM_INSTANCE_NAME` and `IBM_INSTANCE_PROFILE_NAME` if `ibm` is specified\nscripts/infra/cloud-instance.sh ec2 launch\n# Clone instructlab onto the instance and setup the development environment\nscripts/infra/cloud-instance.sh ec2 setup-rh-devenv\n# Install nvidia drivers and reboot\n# Depending on the age of your image, you may want to `sudo dnf update` before\n# installing the drivers, so that you use the latest kernel.\nscripts/infra/cloud-instance.sh ec2 install-rh-nvidia-drivers\nscripts/infra/cloud-instance.sh ec2 ssh sudo reboot\n# Install instructlab\nscripts/infra/cloud-instance.sh ec2 pip-install-with-nvidia\n# ssh to the instance\nscripts/infra/cloud-instance.sh ec2 ssh\n# Run commands on the instance through ssh\nscripts/infra/cloud-instance.sh ec2 ssh ls -la\nscripts/infra/cloud-instance.sh ec2 ssh \"source instructlab/venv/bin/activate && ilab system info\"\n# Sync your local git repo to the repo on the instance\nscripts/infra/cloud-instance.sh ec2 sync\n# Make changes in your local git without committing\n# Sync your changes to the remote instance with a temporary commit\nscripts/infra/cloud-instance.sh ec2 sync -c\n# When you're done, stop the instance\nscripts/infra/cloud-instance.sh ec2 stop\n# While stopped and not yet pruned from your account, you can restart it if needed\nscripts/infra/cloud-instance.sh ec2 start\n# When you are done, you can delete the instance\nscripts/infra/cloud-instance.sh ec2 terminate\n```\n\n----------------------------------------\n\nTITLE: Custom Dataset Format for LoRA Fine-Tuning in MLX\nDESCRIPTION: This JSON snippet demonstrates the expected format for custom datasets used in LoRA fine-tuning. Each line in the dataset file should contain a JSON object with a 'text' key and its corresponding value.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/src/instructlab/train/lora_mlx/README.md#2025-04-21_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\"text\": \"This is an example for the model.\"}\n```\n\n----------------------------------------\n\nTITLE: Debugging llama-cpp GPU Support\nDESCRIPTION: Python code to debug and verify llama-cpp GPU support, showing version information and available GPU devices.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/accelerators/gpu-acceleration.md#2025-04-21_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n>>> import llama\n>>> llama_cpp.__version__\n'0.3.6'\n>>> llama_cpp.llama_supports_gpu_offload()\nTrue\n>>> llama_cpp.llama_backend_init()\nggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\nggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\nggml_init_cublas: found 1 ROCm devices:\n  Device 0: AMD Radeon RX 7900 XT, compute capability 11.0, VMM: no\n```\n\n----------------------------------------\n\nTITLE: Fine-Tuning with Memory Optimization in MLX\nDESCRIPTION: This command shows how to fine-tune a model with reduced memory usage. It uses a smaller batch size and fewer LoRA layers to optimize for machines with limited memory, such as those with 32 GB RAM.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/src/instructlab/train/lora_mlx/README.md#2025-04-21_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\npython lora.py \\\n   --model mistralai/Mistral-7B-v0.1 \\\n   --train \\\n   --batch-size 1 \\\n   --lora-layers 4\n```\n\n----------------------------------------\n\nTITLE: Setting up Python 3.11 Environment for ilab on Linux\nDESCRIPTION: Steps to install Python 3.11, create a virtual environment, and install ilab with CUDA support on Linux systems like Fedora.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/accelerators/gpu-acceleration.md#2025-04-21_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n# Install Python 3.11\nsudo dnf install python3.11 python3.11-devel\n\n# Remove old venv from instructlab/ directory (if it exists)\nrm -r venv\n\n# Create and activate new Python 3.11 venv\npython3.11 -m venv venv\nsource venv/bin/activate\n\n# Install lab (assumes a locally-cloned repo)\n# You can clone the repo if you haven't already done so (either one)\n# gh repo clone instructlab/instructlab\n# git clone https://github.com/instructlab/instructlab.git\npip install ./instructlab[cuda]\n```\n\n----------------------------------------\n\nTITLE: Setting up CUDA Environment and Recompiling llama-cpp-python\nDESCRIPTION: Steps to configure CUDA environment variables, recompile llama-cpp-python with CUDA support, and reinstall InstructLab for GPU acceleration.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/accelerators/gpu-acceleration.md#2025-04-21_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\n# Verify CUDA can be found in your PATH variable\nexport CUDA_HOME=/usr/local/cuda\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CUDA_HOME/lib64:$CUDA_HOME/extras/CUPTI/lib64\nexport PATH=$PATH:$CUDA_HOME/bin\n\n# Recompile llama-cpp-python using CUDA\npip cache remove llama_cpp_python\npip install --force-reinstall --no-deps llama_cpp_python==0.3.6 -C cmake.args=\"-DGGML_CUDA=on\"\n\n# Re-install InstructLab\npip install ./instructlab[cuda]\n```\n\n----------------------------------------\n\nTITLE: Running InstructLab Model Training on HPU\nDESCRIPTION: Command to start model training using InstructLab on the Habana Labs HPU device.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/accelerators/habana-gaudi.md#2025-04-21_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\nilab model train --device=hpu\n```\n\n----------------------------------------\n\nTITLE: Building llama-cpp-python with ROCm Support\nDESCRIPTION: Command to build and install llama-cpp-python with ROCm support, specifying the GPU architecture and using ROCm compilers.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/accelerators/gpu-acceleration.md#2025-04-21_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\nexport PATH=/opt/rocm/llvm/bin:$PATH\npip cache remove llama_cpp_python\nCMAKE_ARGS=\"-DGGML_HIPBLAS=on -DCMAKE_C_COMPILER='/opt/rocm/llvm/bin/clang' -DCMAKE_CXX_COMPILER=/opt/rocm/llvm/bin/clang++ -DCMAKE_PREFIX_PATH=/opt/rocm -DAMDGPU_TARGETS=gfx1100\" FORCE_CMAKE=1 pip install --force-reinstall llama_cpp_python==0.3.6\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch with ROCm Support\nDESCRIPTION: Command to install a version of PyTorch compiled with ROCm 6.1 support for AMD GPUs, ensuring no cached packages are used.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/accelerators/gpu-acceleration.md#2025-04-21_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\npip install torch --force-reinstall --no-cache-dir --index-url https://download.pytorch.org/whl/rocm6.1\n```\n\n----------------------------------------\n\nTITLE: Debugging PyTorch GPU Support\nDESCRIPTION: Python code to debug and verify PyTorch GPU support, showing version information and device details.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/accelerators/gpu-acceleration.md#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n>>> import torch\n>>> torch.__version__\n'2.2.1+rocm5.7'\n>>> torch.version.cuda or 'n/a'\n'n/a'\n>>> torch.version.hip or 'n/a'\n'5.7.31921-d1770ee1b'\n>>> torch.cuda.is_available()\nTrue\n>>> torch.cuda.device_count()\n1\n>>> torch.cuda.get_device_name(torch.cuda.current_device())\n'AMD Radeon RX 7900 XT'\n```\n\n----------------------------------------\n\nTITLE: Installing NVIDIA Proprietary Drivers on Fedora\nDESCRIPTION: Commands to enable RPM Fusion repositories and install proprietary NVIDIA drivers on Fedora Linux, with a note about additional steps for secure boot.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/accelerators/gpu-acceleration.md#2025-04-21_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n# Enable RPM Fusion Repos\nsudo dnf install https://mirrors.rpmfusion.org/free/fedora/rpmfusion-free-release-$(rpm -E %fedora).noarch.rpm https://mirrors.rpmfusion.org/nonfree/fedora/rpmfusion-nonfree-release-$(rpm -E %fedora).noarch.rpm\n\n# Install Nvidia Drivers\n\n# There may be extra steps for enabling secure boot.  View the following blog for further details: https://blog.monosoul.dev/2022/05/17/automatically-sign-nvidia-kernel-module-in-fedora-36/\n\nsudo dnf install akmod-nvidia xorg-x11-drv-nvidia-cuda\n\n# Reboot to load new kernel drivers\nsudo reboot\n\n# Check video driver\nlspci -n -n -k | grep -A 2 -e VGA -e 3D\n```\n\n----------------------------------------\n\nTITLE: Recompiling llama-cpp-python with CUDA on Fedora 40\nDESCRIPTION: Alternative approach for Fedora 40 using Clang 17 as the host compiler since CUDA does not yet support GCC v14.1+.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/accelerators/gpu-acceleration.md#2025-04-21_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\n# Recompile llama-cpp-python using CUDA\nsudo dnf install clang17\nCUDAHOSTCXX=$(which clang++-17) pip install --force-reinstall llama_cpp_python==0.3.6 -C cmake.args=\"-DGGML_CUDA=on\"\n```\n\n----------------------------------------\n\nTITLE: Evaluating LoRA-Adapted Model in MLX\nDESCRIPTION: This command uses the lora.py script to evaluate a LoRA-adapted model on the test set. It computes the test set perplexity using the specified model and adapter file.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/src/instructlab/train/lora_mlx/README.md#2025-04-21_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\npython lora.py --model <path_to_model> \\\n               --adapter-file <path_to_adapters.npz> \\\n               --test\n```\n\n----------------------------------------\n\nTITLE: Adding User to Video and Render Groups for AMD GPUs\nDESCRIPTION: Command to add the current user to the video and render groups, which is required for accessing AMD GPU hardware.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/accelerators/gpu-acceleration.md#2025-04-21_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nsudo usermod -a -G render,video $LOGNAME\n```\n\n----------------------------------------\n\nTITLE: Installing hipBLAS Dependencies on Fedora\nDESCRIPTION: Command to install necessary hipBLAS dependencies on Fedora for building llama-cpp-python with ROCm support.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/accelerators/gpu-acceleration.md#2025-04-21_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\nsudo dnf install hipblas-devel hipblas rocblas-devel\n```\n\n----------------------------------------\n\nTITLE: Installing ilab CLI from source in a Python virtual environment\nDESCRIPTION: This snippet shows how to clone the instructlab repository, create a Python virtual environment, and install the ilab CLI tool from the local source code. This setup allows for development and contribution to the tool.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/CONTRIBUTING/FIRST_TIME_CONTRIBUTORS.md#2025-04-21_snippet_0\n\nLANGUAGE: ShellSession\nCODE:\n```\ngit clone https://github.com/instructlab/instructlab.git\ncd instructlab\npython3 -m venv venv\nsource venv/bin/activate\npip3 install .\n```\n\n----------------------------------------\n\nTITLE: Purging pip Cache for llama-cpp-python Reinstallation\nDESCRIPTION: Command to remove the cached wheel for llama-cpp-python to force a rebuild with the desired backend.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/accelerators/gpu-acceleration.md#2025-04-21_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npip cache remove llama_cpp_python\n```\n\n----------------------------------------\n\nTITLE: Installing llama-cpp-python with Custom Backend\nDESCRIPTION: Generic command to reinstall llama-cpp-python with a specific backend (HIPBLAS, CUDA, METAL, etc.) by passing custom cmake arguments.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/accelerators/gpu-acceleration.md#2025-04-21_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npip install --force-reinstall --no-deps llama_cpp_python==0.3.6 -C cmake.args=\"-DGGML_$BACKEND=on\"\n```\n\n----------------------------------------\n\nTITLE: Converting Model to GGUF Format in Shell\nDESCRIPTION: This command converts a Hugging Face model (safetensors) to GGUF format using the modified conversion script. It saves the output in the specified model directory with a .gguf extension.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/user/converting_GGUF.md#2025-04-21_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nexport MODEL_DIR={model_directory}\npython convert-hf-to-gguf.py $MODEL_DIR --outtype f16\n```\n\n----------------------------------------\n\nTITLE: Building llama.cpp Binaries in Shell\nDESCRIPTION: This command builds the necessary binaries (including quantize) for the llama.cpp project in the current environment.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/user/converting_GGUF.md#2025-04-21_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nmake\n```\n\n----------------------------------------\n\nTITLE: Cloning llama.cpp Repository in Shell\nDESCRIPTION: This command clones the llama.cpp repository from GitHub, which is required for the conversion and quantization process.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/user/converting_GGUF.md#2025-04-21_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/ggerganov/llama.cpp.git\n```\n\n----------------------------------------\n\nTITLE: Installing CUDA 12.8 on Fedora\nDESCRIPTION: Commands to add NVIDIA's CUDA repository and install CUDA toolkit 12.8 along with nvtop for monitoring GPU usage on Fedora Linux.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/accelerators/gpu-acceleration.md#2025-04-21_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n# Install CUDA 12.8 and nvtop to monitor GPU usage\nsudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/fedora41/x86_64/cuda-fedora41.repo\n\nsudo dnf clean all\nsudo dnf -y install cuda-toolkit-12-8 nvtop\n```\n\n----------------------------------------\n\nTITLE: Quantizing GGUF Model in Shell\nDESCRIPTION: This command quantizes the f16 GGUF model to a specified quantization type (e.g., Q4_K_M). It saves the output in the model directory with a type-specific suffix.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/user/converting_GGUF.md#2025-04-21_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n./quantize $MODEL_DIR/ggml-model-f16.gguf Q4_K_M\n```\n\n----------------------------------------\n\nTITLE: Installing NVIDIA Container Toolkit on RHEL\nDESCRIPTION: Series of commands to install NVIDIA Container Toolkit on RHEL 9.4+. This installs the required packages to enable GPU access from containers.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/containers/README.md#2025-04-21_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncurl -s -L https://nvidia.github.io/libnvidia-container/stable/rpm/nvidia-container-toolkit.repo | sudo tee /etc/yum.repos.d/nvidia-container-toolkit.repo\nsudo dnf config-manager --enable nvidia-container-toolkit-experimental\nsudo dnf install -y nvidia-container-toolkit\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch with Habana Labs Framework\nDESCRIPTION: Commands to set up a virtual environment and install PyTorch with Habana Labs framework using the installer script.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/accelerators/habana-gaudi.md#2025-04-21_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\nexport HABANALABS_VIRTUAL_DIR=$HOME/habanalabs-venv\n./habanalabs-installer.sh install --type pytorch --venv\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using ilab in Container\nDESCRIPTION: Series of commands to initialize ilab configuration, download the model, and start the chatbot in the container. Each command runs a separate container instance with GPU access.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/containers/README.md#2025-04-21_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\npodman run --rm -it --device nvidia.com/gpu=0 --volume ${HOME}/.ilab:/opt/appr-root/ilab:Z localhost/instructlab:cuda init\npodman run --rm -it --device nvidia.com/gpu=0 --volume ${HOME}/.ilab:/opt/app-root/ilab:Z localhost/instructlab:cuda download\npodman run --rm -it --device nvidia.com/gpu=0 --volume ${HOME}/.ilab:/opt/app-root/ilab:Z localhost/instructlab:cuda chat\n```\n\n----------------------------------------\n\nTITLE: Preparing Persistent Storage for ilab Container\nDESCRIPTION: Commands to create and set proper permissions for the .ilab directory that will be mounted in the container. Uses podman unshare to set the correct ownership for the rootless container.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/containers/README.md#2025-04-21_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nmkdir -p ${HOME}/.ilab\npodman unshare chown 1001:1001 -R ${HOME}/.ilab\n```\n\n----------------------------------------\n\nTITLE: Generating NVIDIA CDI Configuration\nDESCRIPTION: Command to generate the Container Device Interface configuration for NVIDIA devices. This creates the necessary configuration file for Podman to access GPUs.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/containers/README.md#2025-04-21_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nsudo nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml\n```\n\n----------------------------------------\n\nTITLE: Fusing and Uploading LoRA-Adapted Model in MLX\nDESCRIPTION: This command uses the fuse.py script to generate a fused model that includes the low-rank adapters. It also demonstrates how to upload the fused model to the Hugging Face MLX Community, specifying the upload name and original model repository.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/src/instructlab/train/lora_mlx/README.md#2025-04-21_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\npython fuse.py --upload-name My-4-bit-model --hf-repo mistralai/Mistral-7B-v0.1\n```\n\n----------------------------------------\n\nTITLE: Running ilab Container with GPU Access\nDESCRIPTION: Command to run the ilab container with GPU access. Mounts the persistent storage directory and specifies the first NVIDIA GPU for use by the container.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/containers/README.md#2025-04-21_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\npodman run --rm -it --user 1001 --device nvidia.com/gpu=0 --volume ${HOME}/.ilab:/opt/app-root/ilab:Z localhost/instructlab:cuda\n```\n\n----------------------------------------\n\nTITLE: Setting Up Podman for Habana Labs Container\nDESCRIPTION: Commands to install Podman, configure SELinux for container device access, build the HPU container, and run it with appropriate device mounts.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/accelerators/habana-gaudi.md#2025-04-21_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\nsudo dnf install podman\nsudo setsebool -P container_use_devices=true\nmake hpu\npodman run -ti --device=/dev/accel/ --device=/dev/infiniband/ -v ./data:/opt/app-root/src:z localhost/instructlab:hpu\n```\n\n----------------------------------------\n\nTITLE: Running DK-Bench Evaluation with Basic Options\nDESCRIPTION: Shell command to run DK-Bench evaluation using the ilab CLI, specifying the input questions file and the model to evaluate.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/user/dk_bench.md#2025-04-21_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nilab model evaluate --benchmark dk_bench --input-questions /home/use/path/to/questions.jsonl --model ~/.cache/instructlab/models/instructlab/granite-7b-lab\n```\n\n----------------------------------------\n\nTITLE: Checking Journald for Habana Labs Device\nDESCRIPTION: Command to check the system journal for Habana Labs device initialization messages.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/accelerators/habana-gaudi.md#2025-04-21_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\njournalctl -o cat | grep habanalabs\n```\n\n----------------------------------------\n\nTITLE: Validating Habana Labs Installation\nDESCRIPTION: Command to run the Habana Labs installer script in validation mode to verify the installation.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/accelerators/habana-gaudi.md#2025-04-21_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\n./habanalabs-installer.sh validate\n```\n\n----------------------------------------\n\nTITLE: Installing InstructLab with Hardware-specific Dependencies\nDESCRIPTION: Example of installing the InstructLab package with hardware-specific optional dependencies for NVIDIA CUDA. The package supports different hardware targets including cpu, cuda, hpu, mps, and rocm.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/CHANGELOG.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip install instructlab[cuda]\n```\n\n----------------------------------------\n\nTITLE: Loading Habana Labs Kernel Drivers\nDESCRIPTION: Command to load the Habana Labs kernel drivers into the running kernel.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/accelerators/habana-gaudi.md#2025-04-21_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nmodprobe habanalabs_en habanalabs_cn habanalabs\n```\n\n----------------------------------------\n\nTITLE: Installing InstructLab with Intel Gaudi Support\nDESCRIPTION: Commands to activate the Habana Labs virtual environment and install InstructLab with HPU support from the current directory.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/accelerators/habana-gaudi.md#2025-04-21_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\n. $HABANALABS_VIRTUAL_DIR/bin/activate\npip install ./instructlab[hpu]\n```\n\n----------------------------------------\n\nTITLE: Setting Up Virtual Environment for llama.cpp in Shell\nDESCRIPTION: These commands set up a Python virtual environment, activate it, and install the required dependencies for the llama.cpp project.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/user/converting_GGUF.md#2025-04-21_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncd llama.cpp\npython3 -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Checking Habana Labs System Management Interface\nDESCRIPTION: Command to run the Habana Labs System Management Interface (hl-smi) tool, which provides information about the HPU device.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/accelerators/habana-gaudi.md#2025-04-21_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nhl-smi\n```\n\n----------------------------------------\n\nTITLE: Specifying InstructLab Training Package with CUDA Support\nDESCRIPTION: This dependency declaration specifies that the instructlab-training package with CUDA extras should be installed at version 0.9.0 or higher. This enables NVIDIA GPU acceleration for training tasks in the InstructLab project.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/requirements/cuda.txt#2025-04-21_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ninstructlab-training[cuda]>=0.9.0\n```\n\n----------------------------------------\n\nTITLE: Installing Habana Labs Container Runtime\nDESCRIPTION: DNF command to install the Habana Labs container runtime, which provides OCI container runtime hooks.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/accelerators/habana-gaudi.md#2025-04-21_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\ndnf install habanalabs-container-runtime\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for LoRA Fine-Tuning in MLX\nDESCRIPTION: This command installs the necessary dependencies for the LoRA fine-tuning project using pip and a requirements file.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/src/instructlab/train/lora_mlx/README.md#2025-04-21_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Specifying Habana Labs and Intel Gaudi Dependencies in Python\nDESCRIPTION: This snippet lists the required Python packages and their versions for running machine learning workloads on Habana Labs HPU devices, including Intel Gaudi. It specifies core Habana frameworks and an HPU-specific version of instructlab-training.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/requirements/hpu.txt#2025-04-21_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n# Dependencies for Intel Gaudi / Habana Labs HPU devices\n#\n\n# Habana Labs frameworks\nhabana-torch-plugin>=1.18.0\nhabana_gpu_migration>=1.18.0\n# additional Habana Labs packages (installed, but not used)\n#habana-media-loader\n#habana-pyhlml\n#habana-torch-dataloader\n\n# Extra dependencies for Intel Gaudi cards\ninstructlab-training[hpu]>=0.9.0\n```\n\n----------------------------------------\n\nTITLE: Listing CPU-Only Dependencies for Requirements File\nDESCRIPTION: Specifies additional Python package dependencies that are required when running in CPU-only mode without GPU support.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/requirements/cpu.txt#2025-04-21_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n# Extra dependencies for CPU-only\n```\n\n----------------------------------------\n\nTITLE: Installing llama-cpp-python with Metal Support\nDESCRIPTION: Command to install llama-cpp-python with Metal support for Apple Silicon devices.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/accelerators/gpu-acceleration.md#2025-04-21_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\npip cache remove llama_cpp_python\npip install --force-reinstall llama_cpp_python==0.3.6 -C cmake.args=\"-DGGML_METAL=on\"\n```\n\n----------------------------------------\n\nTITLE: Specifying Development Dependencies for Python Project\nDESCRIPTION: This requirements file lists development dependencies with specific version constraints. It includes tools for testing (pytest, pytest-asyncio, pytest-cov, pytest-html), code quality (pre-commit, pylint), dependency visualization (pydeps), and test environment management (tox).\nSOURCE: https://github.com/instructlab/instructlab/blob/main/requirements-dev.txt#2025-04-21_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n-r requirements.txt\n\npre-commit>=3.0.4,<4.0\npydeps>=1.12.12,<2\npylint>=2.16.2,<4.0\npylint-pydantic\npytest\npytest-asyncio\npytest-cov[toml]\npytest-html\ntox>=4.4.2,<5\n```\n\n----------------------------------------\n\nTITLE: Installing Habana Labs Dependencies\nDESCRIPTION: Commands to set MAKEFLAGS and run the Habana Labs installer script to install dependencies, excluding firmware installation.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/accelerators/habana-gaudi.md#2025-04-21_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nexport MAKEFLAGS=\"-j$(nproc)\"\n./habanalabs-installer.sh install --type dependencies --skip-install-firmware\n```\n\n----------------------------------------\n\nTITLE: Running DK-Bench with Pre-generated Responses\nDESCRIPTION: Shell command to run DK-Bench using a file containing pre-generated model responses, bypassing the need for a student model.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/user/dk_bench.md#2025-04-21_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nilab model evaluate --benchmark dk_bench --input-questions /home/user/path/to/questions-with-responses.jsonl\n```\n\n----------------------------------------\n\nTITLE: Installing llama-cpp-python with OpenBLAS Support\nDESCRIPTION: Command to install llama-cpp-python with OpenBLAS support for CPU acceleration.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/accelerators/gpu-acceleration.md#2025-04-21_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\npip cache remove llama_cpp_python\npip install --force-reinstall llama_cpp_python==0.3.6 -C cmake.args=\"-DGGML_BLAS=on -DGGML_BLAS_VENDOR=OpenBLAS\"\n```\n\n----------------------------------------\n\nTITLE: Specifying Multiple Output Formats for DK-Bench Results\nDESCRIPTION: Shell command demonstrating how to run DK-Bench with multiple output file formats (csv and xlsx) and a custom output directory.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/user/dk_bench.md#2025-04-21_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nilab model evaluate --benchmark dk_bench --input-questions questions.jsonl --model ~/.cache/instructlab/models/instructlab/granite-7b-lab --output-file-formats csv,xlsx --output-dir ~/dk-bench-results-dir\n```\n\n----------------------------------------\n\nTITLE: Setting GPU Memory Limit on macOS\nDESCRIPTION: Command to set a specific GPU memory limit on macOS (example shows setting 12GB limit). This can improve ilab data generation performance but should leave 4-6GB for the operating system.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/TROUBLESHOOTING.md#2025-04-21_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nsudo sysctl iogpu.wired_limit_mb=12288\n```\n\n----------------------------------------\n\nTITLE: Installing ROCm Development Packages\nDESCRIPTION: Command to install the necessary ROCm and hipBLAS development packages for AMD GPU acceleration.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/accelerators/gpu-acceleration.md#2025-04-21_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\n# Optionally enable repo.radeon.com repository, available through AMD documentation or Radeon Software for Linux for RHEL 9.4 at https://www.amd.com/en/support/linux-drivers\nsudo dnf install rocm-dev rocm-utils rocm-llvm rocminfo\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variable for Custom Training Profile Directory\nDESCRIPTION: Environment variable that points to template train profiles to be imported into the train_configuration directory. This allows users to customize their training profile templates.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/CHANGELOG.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nILAB_TRAIN_PROFILE_DIR\n```\n\n----------------------------------------\n\nTITLE: Default Model Reference\nDESCRIPTION: Default model path for data generation pipeline\nSOURCE: https://github.com/instructlab/instructlab/blob/main/CHANGELOG.md#2025-04-21_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nTheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf\n```\n\n----------------------------------------\n\nTITLE: Installing Habana Labs Kernel Drivers\nDESCRIPTION: DNF command to install Habana Labs kernel drivers, which will build and install several kernel modules using DKMS.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/accelerators/habana-gaudi.md#2025-04-21_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ndnf install habanalabs\n```\n\n----------------------------------------\n\nTITLE: Default Model Configuration Path\nDESCRIPTION: XDG-based directory path configuration for model data storage\nSOURCE: https://github.com/instructlab/instructlab/blob/main/CHANGELOG.md#2025-04-21_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n~/.local/share/instructlab/internal/system_profiles\n```\n\n----------------------------------------\n\nTITLE: Environment Variable Usage\nDESCRIPTION: Environment variable specifications for enabling developer preview features and API access\nSOURCE: https://github.com/instructlab/instructlab/blob/main/CHANGELOG.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nILAB_FEATURE_SCOPE=DevPreviewNoUpgrade\nOPENAI_API_KEY=<your-key>\n```\n\n----------------------------------------\n\nTITLE: Syncing InstructLab Library Repositories\nDESCRIPTION: Commands to set up and sync InstructLab library repositories to the remote instance with editable installation mode.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/scripts/infra/README.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Setup the remote repos to install from on the instance\nscripts/infra/cloud-instance.sh ec2 setup-instructlab-library-devenvs\n# Sync your local repos to the remote instance (installed with `pip -e`)\nscripts/infra/cloud-instance.sh ec2 sync-library -c -l sdg\nscripts/infra/cloud-instance.sh ec2 sync-library -c -l training\nscripts/infra/cloud-instance.sh ec2 sync-library -c -l eval\n```\n\n----------------------------------------\n\nTITLE: Setting Up IBM Cloud CLI\nDESCRIPTION: Commands to install and configure the IBM Cloud CLI with needed plugins for cloud instance management.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/scripts/infra/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nibmcloud api cloud.ibm.com\nibmcloud plugin install is\n```\n\n----------------------------------------\n\nTITLE: Listing NVIDIA CDI Devices\nDESCRIPTION: Command to list available NVIDIA GPU devices through Container Device Interface (CDI). This verifies that the NVIDIA container toolkit can detect your GPUs.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/containers/README.md#2025-04-21_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nnvidia-ctk cdi list\n```\n\n----------------------------------------\n\nTITLE: Setting Up Configuration File\nDESCRIPTION: Commands to create the configuration directory and copy the example configuration file to the default location, or alternatively set up an environment variable for a custom location.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/scripts/infra/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmkdir ~/.instructlab\ncp instructlab-cloud-config-example ~/.instructlab/cloud-config\n```\n\nLANGUAGE: bash\nCODE:\n```\ncp instructlab-cloud-config-example <location of your choice>\nexport INSTRUCTLAB_CLOUD_CONFIG=<location of your choice>\n```\n\n----------------------------------------\n\nTITLE: Setting OPENAI_API_KEY for DK-Bench Evaluation in Bash\nDESCRIPTION: This snippet demonstrates how to set the OPENAI_API_KEY environment variable for running DK-Bench evaluation in the e2e-ci.sh script. It also shows how to bypass the API key requirement for users without one.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/maintainers/ci.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nOPENAI_API_KEY='your_api_key_here'\n# Or to bypass:\nOPENAI_API_KEY='NO_API_KEY'\n```\n\n----------------------------------------\n\nTITLE: Checking NVIDIA Driver Status on Linux\nDESCRIPTION: Command to check if the system is using proprietary NVIDIA drivers or open-source nouveau drivers by examining kernel driver information.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/accelerators/gpu-acceleration.md#2025-04-21_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n# Check video driver\nsudo dnf install pciutils\nlspci -n -n -k | grep -A 2 -e VGA -e 3D\n```\n\n----------------------------------------\n\nTITLE: Embedding Google Colab Notebook Link in Markdown\nDESCRIPTION: This snippet demonstrates how to embed a link to open a Google Colab notebook using Markdown syntax. It includes an image badge for visual representation.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/notebooks/README.md#2025-04-21_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/instructlab/instructlab/blob/main/notebooks/Training_a_LoRA_With_Instruct_Lab.ipynb\">\n  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n```\n\n----------------------------------------\n\nTITLE: Sphinx Click Directive for CLI Documentation in RestructuredText\nDESCRIPTION: Configuration for automatically generating CLI documentation using Sphinx and the Click directive. It specifies the program name 'ilab', the module path 'instructlab.lab:ilab', and enables full nested documentation of all commands.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/ilab.rst#2025-04-21_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. click:: instructlab.lab:ilab\n   :prog: ilab\n   :nested: full\n```\n\n----------------------------------------\n\nTITLE: Installing Habana Labs Firmware and Tools\nDESCRIPTION: DNF command to install Habana Labs firmware, firmware ODM, firmware tools, RDMA core, graph, and thunk packages.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/accelerators/habana-gaudi.md#2025-04-21_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ndnf install habanalabs-firmware habanalabs-firmware-odm habanalabs-firmware-tools habanalabs-rdma-core habanalabs-graph habanalabs-thunk\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents with Sphinx toctree Directive in reStructuredText\nDESCRIPTION: This snippet configures a table of contents using Sphinx's toctree directive. It sets the maximum depth to 1, enables globbing for file matching, and adds a caption. It includes links to CI and release strategy documentation files.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/maintainers/index.rst#2025-04-21_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :maxdepth: 1\n   :glob:\n   :caption: Contents:\n\n   ci.md\n   release-strategy.md\n```\n\n----------------------------------------\n\nTITLE: Improved Model Response Example\nDESCRIPTION: Example showing successful model response when queried with proper context about the company.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/user/PROMPTING-FAQ.md#2025-04-21_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n>>> I want to talk about Retro911 the full-service company dedicated to helping enthusiasts navigate the world of retro video systems. When was it founded?                                                                                                                                                                                                                                                                                                          [S][default]\n╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── /var/mnt/models/retro911model ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n│ I apologize for the confusion earlier. I had the wrong information in my system. Retro911 was founded in 2024. They started their operations in that year and have since become a well-known and respected name in the retro video gaming community.                                                                                       │\n│                                                                                                                                                                                                                                                                                                                                            │\n│ Retro911 has been dedicated to helping enthusiasts navigate the world of retro video systems, offering a wide range of services, such as repair and restoration of retro video systems, sourcing and acquisition of rare and collectible hardware, and hosting of events and conventions for retro gaming enthusiasts.                     │\n│                                                                                                                                                                                                                                                                                                                                            │\n│ To learn more about their services and what they have to offer, I recommend visiting their official website or reaching out to them directly. They're sure to have a lot of exciting information and events planned for the future!                                                                                                        │\n│                                                                                                                                                                                                                                                                                                                                            │\n│ Keep in mind that the specifics of their services and events may change over time, so it's best to check their latest updates for the most accurate information. Have fun exploring the world of retro gaming with Retro911!                                                                                                               │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── elapsed 2.737 seconds ─╯\n```\n\n----------------------------------------\n\nTITLE: Checking GPU Model with rocminfo\nDESCRIPTION: Command to check the GPU model using rocminfo, which is useful for configuring the build process for AMD GPUs.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/accelerators/gpu-acceleration.md#2025-04-21_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\n$ rocminfo | grep gfx\n  Name:                    gfx1100\n      Name:                    amdgcn-amd-amdhsa--gfx1100\n  Name:                    gfx1036\n      Name:                    amdgcn-amd-amdhsa--gfx103\n```\n\n----------------------------------------\n\nTITLE: Configuring ReStructuredText Table of Contents\nDESCRIPTION: Defines a table of contents tree (toctree) directive with maxdepth of 1, glob pattern matching enabled, and a caption. Links to specific markdown and documentation files in the project.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/user/index.rst#2025-04-21_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :maxdepth: 1\n   :glob:\n   :caption: Contents:\n\n   PROMPTING-FAQ.md\n   instructlab_models.md\n   converting_GGUF.md\n   dk_bench.md\n```\n\n----------------------------------------\n\nTITLE: Specifying vLLM Dependency for Linux Platforms\nDESCRIPTION: This snippet defines the vLLM package requirement with a minimum version of 0.8.0, restricted to Linux platforms only. It includes a note about not capping this dependency in the current file, but in constraints-dev.txt instead.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/requirements-vllm-cuda.txt#2025-04-21_snippet_0\n\nLANGUAGE: pip\nCODE:\n```\n# vLLM only supports Linux platform (including WSL). Do not cap this dependency here. Cap in constraints-dev.txt\nvllm>=0.8.0 ; sys_platform == 'linux'\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies\nDESCRIPTION: This snippet defines the required Python packages and their version constraints for the project. It specifies torch to be less than version 2.7.0 and vllm to be less than version 0.9.0.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/constraints-dev.txt#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ntorch<2.7.0\nvllm<0.9.0\n```\n\n----------------------------------------\n\nTITLE: Defining Sphinx Documentation Dependencies\nDESCRIPTION: This snippet defines the Python package dependencies required for generating documentation with Sphinx. It includes the Sphinx package with a version constraint, as well as extensions like myst-parser for Markdown support, furo theme, sphinx-click for Click command documentation, autodoc-pydantic for Pydantic model documentation, and enum-tools with the sphinx extra for enum documentation.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/requirements.txt#2025-04-21_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\nsphinx<8.2.0\nmyst-parser\nfuro\nsphinx-click\nautodoc-pydantic\nenum-tools[sphinx]\n```\n\n----------------------------------------\n\nTITLE: Defining Questions with Pre-generated Responses in JSON\nDESCRIPTION: Example of a .jsonl file containing questions, pre-generated responses, and reference answers for DK-Bench evaluation.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/user/dk_bench.md#2025-04-21_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n\"user_input\":\"What is the capital of Canada?\",\"response\":\"The capital of Canada is Toronto.\",\"reference\":\"The capital of Canada is Ottawa.\"\n}\n{\n\"user_input\":\"What is the capital of Mexico?\",\"response\":\"The capital of Mexico is Mexico City.\",\"reference\":\"The capital of Mexico is Mexico City.\"\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Evaluation Questions in JSON\nDESCRIPTION: Example of a .jsonl file containing custom evaluation questions for DK-Bench. Each line represents a question with its reference answer.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/user/dk_bench.md#2025-04-21_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n\"user_input\":\"What is the capital of Canada?\",\"reference\":\"The capital of Canada is Ottawa.\"\n}\n{\n\"user_input\":\"What is the capital of Mexico?\",\"reference\":\"The capital of Mexico is Mexico City.\"\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Training Data in YAML\nDESCRIPTION: Example of seed training data for a model containing company information and question-answer pairs about Retro911.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/user/PROMPTING-FAQ.md#2025-04-21_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nseed_examples:\n  - context: |\n      **Retro911** is a full-service company dedicated to helping enthusiasts navigate the world of retro video systems.\n      Founded in 2024 by Grant Shipley and headquartered in Clayton, NC, it specializes in providing expert advice on\n      conquering classic game titles, assisting in the acquisition and restoration of vintage hardware, and fostering\n      a community passionate about all things retro gaming.\n    questions_and_answers:\n      - question: |\n          When was Retro911 founded?\n        answer: |\n          Retro911 was founded in 2024.\n      - question: |\n          Who founded Retro911?\n        answer: |\n          Grant Shipley founded Retro911.\n```\n\n----------------------------------------\n\nTITLE: Building ilab Container Image with Custom Registry\nDESCRIPTION: Command to build the ilab container image using make with a custom container registry prefix. This allows pushing the image to a specified registry like Quay.io instead of the default localhost.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/containers/README.md#2025-04-21_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nmake cuda CONTAINER_PREFIX=quay.io/ai-labs/instructlab\n```\n\n----------------------------------------\n\nTITLE: Modifying Conversion Script for InstructLab Model in Python\nDESCRIPTION: This diff shows the modification needed in the convert-hf-to-gguf.py script to fix a bug when converting the InstructLab model.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/user/converting_GGUF.md#2025-04-21_snippet_2\n\nLANGUAGE: diff\nCODE:\n```\n[...]\ndef write_tensors(self):\n[...]\n    self.gguf_writer.add_tensor(new_name, data)\n \n+   if new_name == \"token_embd.weight\":\n+       self.gguf_writer.add_tensor(\"output.weight\", data)\n+\ndef write(self):\n    self.write_tensors()\n[...]\n```\n\n----------------------------------------\n\nTITLE: Installing ilab CLI in editable mode\nDESCRIPTION: Installs the ilab CLI package in editable mode (-e flag), which allows code changes to take effect automatically without requiring reinstallation after each change.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/CONTRIBUTING/FIRST_TIME_CONTRIBUTORS.md#2025-04-21_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npip3 install -e .\n```\n\n----------------------------------------\n\nTITLE: Reinstalling ilab CLI to apply code changes\nDESCRIPTION: After making changes to the ilab source code, this command reinstalls the package to make those changes take effect when using the CLI tool.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/CONTRIBUTING/FIRST_TIME_CONTRIBUTORS.md#2025-04-21_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npip3 install .\n```\n\n----------------------------------------\n\nTITLE: Retrieving Habana Labs Installer Script\nDESCRIPTION: Commands to download and make executable the Habana Labs installer script, which is used to install dependencies and PyTorch stack.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/accelerators/habana-gaudi.md#2025-04-21_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\ncurl -O https://vault.habana.ai/artifactory/gaudi-installer/1.18.0/habanalabs-installer.sh\nchmod +x habanalabs-installer.sh\n```\n\n----------------------------------------\n\nTITLE: Configuring Habana Vault Repository\nDESCRIPTION: INI configuration for the Habana Vault repository, which should be added to /etc/yum.repos.d/Habana-Vault.repo to enable installation of Habana Labs software.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/accelerators/habana-gaudi.md#2025-04-21_snippet_1\n\nLANGUAGE: ini\nCODE:\n```\n[vault]\nname=Habana Vault\nbaseurl=https://vault.habana.ai/artifactory/rhel/9/9.4\nenabled=1\ngpgcheck=1\nrepo_gpgcheck=0\ngpgkey=https://vault.habana.ai/artifactory/api/v2/repositories/rhel/keyPairs/primary/public\n```\n\n----------------------------------------\n\nTITLE: Enabling CRB and EPEL Repositories for RHEL 9\nDESCRIPTION: Commands to enable CodeReady Builder and EPEL repositories on RHEL 9 systems, which are required for installing Habana Labs software.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/accelerators/habana-gaudi.md#2025-04-21_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nsudo subscription-manager repos --enable codeready-builder-for-rhel-9-$(arch)-rpms\nsudo dnf install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-9.noarch.rpm\n```\n\n----------------------------------------\n\nTITLE: Initial Model Response Example\nDESCRIPTION: Example showing insufficient model response when queried without proper context.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/user/PROMPTING-FAQ.md#2025-04-21_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n>>> When was Retro911 founded?                                                                                                                                                                                                                                                                                    [S][default]\n╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── /var/mnt/models/retro911model ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n│ I'm sorry, but I can't provide specific information about the founding date of Retro911. I recommend checking their official website or contacting them directly for accurate details.                                                                                                                                                     │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── elapsed 0.470 seconds ─╯\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variable for Global Configuration\nDESCRIPTION: Environment variable for specifying a global configuration file that serves as the template for the user's config file. This bypasses the interactive mode in config initialization and can be used to set default configurations.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/CHANGELOG.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nILAB_GLOBAL_CONFIG\n```\n\n----------------------------------------\n\nTITLE: Configuring reStructuredText Table of Contents\nDESCRIPTION: Defines a toctree directive in reStructuredText that lists documentation pages about different hardware accelerators with maxdepth 1 and glob pattern matching.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/accelerators/index.rst#2025-04-21_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :maxdepth: 1\n   :glob:\n   :caption: Contents:\n\n   habana-gaudi.md\n   amd-rocm.md\n   gpu-acceleration.md\n```\n\n----------------------------------------\n\nTITLE: Checking GPU Wired Limit on macOS\nDESCRIPTION: Command to check the current GPU wired memory limit on macOS systems. By default, this limit is set to 0 which represents 60-70% of total RAM.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/TROUBLESHOOTING.md#2025-04-21_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nsudo sysctl iogpu.wired_limit_mb\niogpu.wired_limit_mb = 0\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx Documentation Table of Contents\nDESCRIPTION: Sets up the main documentation structure using Sphinx toctree directive with maxdepth of 2, enabling glob pattern matching, and defining the major documentation sections.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/index.rst#2025-04-21_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :maxdepth: 2\n   :glob:\n   :caption: Contents:\n\n   ilab.rst\n   config.rst\n   user/index.rst\n   accelerators/index.rst\n   maintainers/index.rst\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx Search and Index Links\nDESCRIPTION: Defines links to the general index, module index, and search functionality using Sphinx reference directives.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/index.rst#2025-04-21_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n* :ref:`genindex`\n* :ref:`modindex`\n* :ref:`search`\n```\n\n----------------------------------------\n\nTITLE: Removing CUDA Packages for Troubleshooting\nDESCRIPTION: Command to remove all CUDA-related packages when encountering dependency issues during system updates, allowing for a clean reinstallation.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/accelerators/gpu-acceleration.md#2025-04-21_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nsudo dnf remove \"*cuda*\"\n```\n\n----------------------------------------\n\nTITLE: Specifying AMD ROCm Dependencies for instructlab-training\nDESCRIPTION: This snippet specifies the package requirement for enabling AMD ROCm support in instructlab-training. It requires version 0.9.0 or higher of the instructlab-training package with the rocm extra dependencies.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/requirements/rocm.txt#2025-04-21_snippet_0\n\nLANGUAGE: plain text\nCODE:\n```\ninstructlab-training[rocm]>=0.9.0\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for ML Project\nDESCRIPTION: Lists required Python packages for a machine learning project. Includes MLX (version 0.0.7 or newer) for ML acceleration, Transformers library for working with transformer models, and NumPy for numerical computations.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/src/instructlab/train/lora_mlx/requirements.txt#2025-04-21_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nmlx>=0.0.7\ntransformers\nnumpy\n```\n\n----------------------------------------\n\nTITLE: Ensuring AWS System Packages Installation\nDESCRIPTION: Command to install necessary system packages for AWS CLI usage through local-setup.sh. Supports Red Hat-based systems and offers an optional flag for SAML authentication.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/scripts/infra/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Ensure proper system packages are installed. This script supports Red\n# Hat-based systems. If you are running a different OS you'll need to install\n# these packages manually, or update the script for your OS.\n./local-setup.sh ensure-aws-system-pkgs\n# If you are using SAML auth with the AWS CLI, you can install packages for that with the '-s` flag\n./local-setup.sh ensure-aws-system-pkgs -s\n```\n\n----------------------------------------\n\nTITLE: Defining Skill in Markdown\nDESCRIPTION: This markdown snippet provides a comprehensive definition of skill, including its characteristics, types, and applications in various domains. It also discusses the relationship between skills and arts, and the importance of skills in the modern economy.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/tests/testdata/temp_taxonomy_repo/docs/skill-wiki.md#2025-04-21_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Skill\n\nA skill is the learned ability to act with determined results with good execution often within a given amount of time, energy, or both. Skills can often be divided into domain-general and domain-specific skills. For example, in the domain of work, some general skills would include time management, teamwork and leadership, self-motivation and others, whereas domain-specific skills would be used only for a certain job. Skill usually requires certain environmental stimuli and situations to assess the level of skill being shown and used.[citation needed]\nA skill may be called an art when it represents a body of knowledge or branch of learning, as in the art of medicine or the art of war.[1] Although the arts are also skills, there are many skills that form an art but have no connection to the fine arts.\nPeople need a broad range of skills to contribute to the modern economy. A U.S. Department of Labor study showed that through technology, the workplace is changing, and identified 16 basic skills that employees must have to be able to change with it.[2] Three broad categories of skills are suggested and these are technical, human, and conceptual.[3] The first two can be substituted with hard and soft skills, respectively.[4]\n```\n\n----------------------------------------\n\nTITLE: Declaring PR Dependencies in Markdown\nDESCRIPTION: This snippet shows how to declare dependencies on external pull requests using the depends-on-action syntax in a pull request description.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/maintainers/ci.md#2025-04-21_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\nChange to use the new library function\n\nDepends-On: https://github.com/org/library/pull/123\n```\n\n----------------------------------------\n\nTITLE: Displaying MIT License for llama.cpp\nDESCRIPTION: This snippet shows the full text of the MIT License that applies to the original llama.cpp code. It grants permission to use, modify, and distribute the software under certain conditions.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/src/instructlab/llamacpp/README.md#2025-04-21_snippet_0\n\nLANGUAGE: license\nCODE:\n```\nMIT License\n\nCopyright (c) 2023-2024 The ggml authors\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n```\n\n----------------------------------------\n\nTITLE: Displaying MIT License for MLX Examples Code\nDESCRIPTION: This snippet shows the full text of the MIT License that applies to the original code from the mlx-examples repository. It grants permission to use, modify, and distribute the software under certain conditions.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/src/instructlab/mlx_explore/README.md#2025-04-21_snippet_0\n\nLANGUAGE: license\nCODE:\n```\nMIT License\n\nCopyright © 2023 Apple Inc.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n```\n\n----------------------------------------\n\nTITLE: MIT License for MLX Examples\nDESCRIPTION: The MIT License text applied to the original code from Apple's MLX examples repository. This license permits use, modification, and distribution with minimal restrictions provided the license notice is preserved.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/src/instructlab/train/lora_mlx/models/README.md#2025-04-21_snippet_0\n\nLANGUAGE: license\nCODE:\n```\nMIT License\n\nCopyright © 2023 Apple Inc.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n```\n\n----------------------------------------\n\nTITLE: Setting Up Training Environment for Habana Labs\nDESCRIPTION: Shell commands to set various environment variables required for training with Habana Labs HPU, including memory allocation, threading, and device configuration.\nSOURCE: https://github.com/instructlab/instructlab/blob/main/docs/accelerators/habana-gaudi.md#2025-04-21_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\nexport TCMALLOC_LARGE_ALLOC_REPORT_THRESHOLD=7516192768\nexport LD_PRELOAD=/lib64/libtcmalloc.so\nexport OMP_NUM_THREADS=16\nexport PT_HPU_LAZY_MODE=0\n```"
  }
]
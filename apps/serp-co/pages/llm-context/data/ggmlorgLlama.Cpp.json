[
  {
    "owner": "ggml-org",
    "repo": "llama.cpp",
    "content": "TITLE: Command Line Usage Syntax for Llama-bench\nDESCRIPTION: Shows the complete command-line interface options for llama-bench, including model configuration, processing parameters, thread management, GPU settings, and output format options.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llama-bench/README.md#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nusage: ./llama-bench [options]\n\noptions:\n  -h, --help\n  -m, --model <filename>                    (default: models/7B/ggml-model-q4_0.gguf)\n  -p, --n-prompt <n>                        (default: 512)\n  -n, --n-gen <n>                           (default: 128)\n  -pg <pp,tg>                               (default: )\n  -b, --batch-size <n>                      (default: 2048)\n  -ub, --ubatch-size <n>                    (default: 512)\n  -ctk, --cache-type-k <t>                  (default: f16)\n  -ctv, --cache-type-v <t>                  (default: f16)\n  -t, --threads <n>                         (default: 8)\n  -C, --cpu-mask <hex,hex>                  (default: 0x0)\n  --cpu-strict <0|1>                        (default: 0)\n  --poll <0...100>                          (default: 50)\n  -ngl, --n-gpu-layers <n>                  (default: 99)\n  -rpc, --rpc <rpc_servers>                 (default: )\n  -sm, --split-mode <none|layer|row>        (default: layer)\n  -mg, --main-gpu <i>                       (default: 0)\n  -nkvo, --no-kv-offload <0|1>              (default: 0)\n  -fa, --flash-attn <0|1>                   (default: 0)\n  -mmp, --mmap <0|1>                        (default: 1)\n  --numa <distribute|isolate|numactl>       (default: disabled)\n  -embd, --embeddings <0|1>                 (default: 0)\n  -ts, --tensor-split <ts0/ts1/..>          (default: 0)\n  -r, --repetitions <n>                     (default: 5)\n  --prio <0|1|2|3>                          (default: 0)\n  --delay <0...N> (seconds)                 (default: 0)\n  -o, --output <csv|json|jsonl|md|sql>      (default: md)\n  -oe, --output-err <csv|json|jsonl|md|sql> (default: none)\n  -v, --verbose                             (default: 0)\n```\n\n----------------------------------------\n\nTITLE: Supporting Multiple Users and Parallel Decoding in llama-server\nDESCRIPTION: Shows how to configure llama-server to handle multiple concurrent requests with specific context size limitations.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# up to 4 concurrent requests, each with 4096 max context\nllama-server -m model.gguf -c 16384 -np 4\n```\n\n----------------------------------------\n\nTITLE: Running Simple Text Completion with llama-cli\nDESCRIPTION: Demonstrates basic text completion with llama-cli by explicitly disabling conversation mode and providing a prompt with a specified number of tokens to generate.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nllama-cli -m model.gguf -p \"I believe the meaning of life is\" -n 128 -no-cnv\n```\n\n----------------------------------------\n\nTITLE: Constraining Output with Custom Grammar in llama-cli\nDESCRIPTION: Shows how to use a custom grammar file to constrain the model's output format, which is useful for generating structured data like JSON.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nllama-cli -m model.gguf -n 256 --grammar-file grammars/json.gbnf -p 'Request: schedule a call at 8pm; Command:'\n```\n\n----------------------------------------\n\nTITLE: Adjusting Temperature in LLaMA.cpp\nDESCRIPTION: Controls the randomness of text generation with a default of 0.8. Higher values (e.g., 1.5) increase creativity, while lower values (e.g., 0.5) produce more focused, deterministic output.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\n--temp N\n```\n\n----------------------------------------\n\nTITLE: Running llama-mtmd-cli with LLaVA Model\nDESCRIPTION: Command to run the llama-mtmd-cli with a LLaVA model, specifying the model file, multimodal projector, and chat template. It's recommended to use a lower temperature (e.g., 0.1) for better quality output.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/llava.md#2025-04-22_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n./llama-mtmd-cli -m ../llava-v1.5-7b/ggml-model-f16.gguf \\\n    --mmproj ../llava-v1.5-7b/mmproj-model-f16.gguf \\\n    --chat-template vicuna\n```\n\n----------------------------------------\n\nTITLE: Converting and Quantizing LLaMA Models with llama.cpp\nDESCRIPTION: This code demonstrates the process of converting LLaMA models to GGUF format and quantizing them. It includes steps for installing dependencies, converting models to FP16 format, and quantizing to lower precision formats like Q4_K_M.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/quantize/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# obtain the official LLaMA model weights and place them in ./models\nls ./models\nllama-2-7b tokenizer_checklist.chk tokenizer.model\n# [Optional] for models using BPE tokenizers\nls ./models\n<folder containing weights and tokenizer json> vocab.json\n# [Optional] for PyTorch .bin models like Mistral-7B\nls ./models\n<folder containing weights and tokenizer json>\n\n# install Python dependencies\npython3 -m pip install -r requirements.txt\n\n# convert the model to ggml FP16 format\npython3 convert_hf_to_gguf.py models/mymodel/\n\n# quantize the model to 4-bits (using Q4_K_M method)\n./llama-quantize ./models/mymodel/ggml-model-f16.gguf ./models/mymodel/ggml-model-Q4_K_M.gguf Q4_K_M\n\n# update the gguf filetype to current version if older version is now unsupported\n./llama-quantize ./models/mymodel/ggml-model-Q4_K_M.gguf ./models/mymodel/ggml-model-Q4_K_M-v2.gguf COPY\n```\n\n----------------------------------------\n\nTITLE: Using OpenAI Python Library for Completions in llama.cpp\nDESCRIPTION: This snippet demonstrates how to use the OpenAI Python library to interact with the llama.cpp completions API. It sets up the client with a custom base URL and sends a completion request.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nimport openai\n\nclient = openai.OpenAI(\n    base_url=\"http://localhost:8080/v1\", # \"http://<Your api-server IP>:port\"\n    api_key = \"sk-no-key-required\"\n)\n\ncompletion = client.completions.create(\n  model=\"davinci-002\",\n  prompt=\"I believe the meaning of life is\",\n  max_tokens=8\n)\n\nprint(completion.choices[0].text)\n```\n\n----------------------------------------\n\nTITLE: Running llama-cli in Conversation Mode\nDESCRIPTION: Demonstrates how to use llama-cli to interact with a model in conversation mode, which activates automatically with models having built-in chat templates.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nllama-cli -m model.gguf\n```\n\n----------------------------------------\n\nTITLE: Configuring Mirostat Sampling in LLaMA.cpp\nDESCRIPTION: Example command showing how to enable Mirostat 2.0 with custom learning rate and target entropy values. Mirostat actively maintains the quality of generated text within a desired range, balancing coherence and diversity.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_24\n\nLANGUAGE: shell\nCODE:\n```\n--mirostat 2 --mirostat-lr 0.05 --mirostat-ent 3.0\n```\n\n----------------------------------------\n\nTITLE: Constraining Server Outputs with Grammar in llama-server\nDESCRIPTION: Shows how to apply grammar constraints to all outputs from llama-server using either custom or predefined grammar files.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/README.md#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n# custom grammar\nllama-server -m model.gguf --grammar-file grammar.gbnf\n\n# JSON\nllama-server -m model.gguf --grammar-file grammars/json.gbnf\n```\n\n----------------------------------------\n\nTITLE: Using Custom Chat Templates with llama-cli\nDESCRIPTION: Shows how to use llama-cli with custom chat templates, either by specifying a predefined template or by setting custom prefixes and prompts.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# use the \"chatml\" template (use -h to see the list of supported templates)\nllama-cli -m model.gguf -cnv --chat-template chatml\n\n# use a custom template\nllama-cli -m model.gguf -cnv --in-prefix 'User: ' --reverse-prompt 'User:'\n```\n\n----------------------------------------\n\nTITLE: Configuring GGML Backend Options in CMake for llama.cpp\nDESCRIPTION: This CMake configuration sets up build options for various GGML backends including hardware acceleration options (CUDA, Metal, Vulkan, OpenCL, etc.) and their configuration parameters. It enables customization of the library build based on the target platform and requirements.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/CMakeLists.txt#2025-04-22_snippet_15\n\nLANGUAGE: CMake\nCODE:\n```\n# 3rd party libs / backends\noption(GGML_ACCELERATE                      \"ggml: enable Accelerate framework\"               ON)\noption(GGML_BLAS                            \"ggml: use BLAS\"                                  ${GGML_BLAS_DEFAULT})\nset(GGML_BLAS_VENDOR ${GGML_BLAS_VENDOR_DEFAULT} CACHE STRING\n                                            \"ggml: BLAS library vendor\")\noption(GGML_LLAMAFILE                       \"ggml: use LLAMAFILE\"                             ${GGML_LLAMAFILE_DEFAULT})\n\noption(GGML_CUDA                            \"ggml: use CUDA\"                                  OFF)\noption(GGML_MUSA                            \"ggml: use MUSA\"                                  OFF)\noption(GGML_CUDA_FORCE_MMQ                  \"ggml: use mmq kernels instead of cuBLAS\"         OFF)\noption(GGML_CUDA_FORCE_CUBLAS               \"ggml: always use cuBLAS instead of mmq kernels\"  OFF)\noption(GGML_CUDA_F16                        \"ggml: use 16 bit floats for some calculations\"   OFF)\nset   (GGML_CUDA_PEER_MAX_BATCH_SIZE \"128\" CACHE STRING\n                                            \"ggml: max. batch size for using peer access\")\noption(GGML_CUDA_NO_PEER_COPY               \"ggml: do not use peer to peer copies\"            OFF)\noption(GGML_CUDA_NO_VMM                     \"ggml: do not try to use CUDA VMM\"                OFF)\noption(GGML_CUDA_FA                         \"ggml: compile ggml FlashAttention CUDA kernels\"  ON)\noption(GGML_CUDA_FA_ALL_QUANTS              \"ggml: compile all quants for FlashAttention\"     OFF)\noption(GGML_CUDA_GRAPHS                     \"ggml: use CUDA graphs (llama.cpp only)\"          ${GGML_CUDA_GRAPHS_DEFAULT})\nset   (GGML_CUDA_COMPRESSION_MODE \"size\" CACHE STRING\n                                            \"ggml: cuda link binary compression mode; requires cuda 12.8+\")\nset_property(CACHE GGML_CUDA_COMPRESSION_MODE PROPERTY STRINGS \"none;speed;balance;size\")\n\noption(GGML_HIP                             \"ggml: use HIP\"                                   OFF)\noption(GGML_HIP_GRAPHS                      \"ggml: use HIP graph, experimental, slow\"         OFF)\noption(GGML_HIP_NO_VMM                      \"ggml: do not try to use HIP VMM\"                 ON)\noption(GGML_HIP_ROCWMMA_FATTN               \"ggml: enable rocWMMA for FlashAttention\"         OFF)\noption(GGML_VULKAN                          \"ggml: use Vulkan\"                                OFF)\noption(GGML_VULKAN_CHECK_RESULTS            \"ggml: run Vulkan op checks\"                      OFF)\noption(GGML_VULKAN_DEBUG                    \"ggml: enable Vulkan debug output\"                OFF)\noption(GGML_VULKAN_MEMORY_DEBUG             \"ggml: enable Vulkan memory debug output\"         OFF)\noption(GGML_VULKAN_SHADER_DEBUG_INFO        \"ggml: enable Vulkan shader debug info\"           OFF)\noption(GGML_VULKAN_PERF                     \"ggml: enable Vulkan perf output\"                 OFF)\noption(GGML_VULKAN_VALIDATE                 \"ggml: enable Vulkan validation\"                  OFF)\noption(GGML_VULKAN_RUN_TESTS                \"ggml: run Vulkan tests\"                          OFF)\noption(GGML_KOMPUTE                         \"ggml: use Kompute\"                               OFF)\noption(GGML_METAL                           \"ggml: use Metal\"                                 ${GGML_METAL_DEFAULT})\noption(GGML_METAL_USE_BF16                  \"ggml: use bfloat if available\"                   OFF)\noption(GGML_METAL_NDEBUG                    \"ggml: disable Metal debugging\"                   OFF)\noption(GGML_METAL_SHADER_DEBUG              \"ggml: compile Metal with -fno-fast-math\"         OFF)\noption(GGML_METAL_EMBED_LIBRARY             \"ggml: embed Metal library\"                       ${GGML_METAL})\nset   (GGML_METAL_MACOSX_VERSION_MIN \"\" CACHE STRING\n                                            \"ggml: metal minimum macOS version\")\nset   (GGML_METAL_STD \"\" CACHE STRING       \"ggml: metal standard version (-std flag)\")\noption(GGML_OPENMP                          \"ggml: use OpenMP\"                                ON)\noption(GGML_RPC                             \"ggml: use RPC\"                                   OFF)\noption(GGML_SYCL                            \"ggml: use SYCL\"                                  OFF)\noption(GGML_SYCL_F16                        \"ggml: use 16 bit floats for sycl calculations\"   OFF)\noption(GGML_SYCL_GRAPH                      \"ggml: enable graphs in the SYCL backend\"         ON)\nset   (GGML_SYCL_TARGET \"INTEL\" CACHE STRING\n                                            \"ggml: sycl target device\")\nset   (GGML_SYCL_DEVICE_ARCH \"\" CACHE STRING\n                                            \"ggml: sycl device architecture\")\n\noption(GGML_OPENCL                          \"ggml: use OpenCL\"                                OFF)\noption(GGML_OPENCL_PROFILING                \"ggml: use OpenCL profiling (increases overhead)\" OFF)\noption(GGML_OPENCL_EMBED_KERNELS            \"ggml: embed kernels\"                             ON)\noption(GGML_OPENCL_USE_ADRENO_KERNELS       \"ggml: use optimized kernels for Adreno\"          ON)\nset   (GGML_OPENCL_TARGET_VERSION \"300\" CACHE STRING\n                                            \"gmml: OpenCL API version to target\")\n\n# toolchain for vulkan-shaders-gen\nset   (GGML_VULKAN_SHADERS_GEN_TOOLCHAIN \"\" CACHE FILEPATH \"ggml: toolchain file for vulkan-shaders-gen\")\n\n# extra artifacts\noption(GGML_BUILD_TESTS    \"ggml: build tests\"    ${GGML_STANDALONE})\noption(GGML_BUILD_EXAMPLES \"ggml: build examples\" ${GGML_STANDALONE})\n```\n\n----------------------------------------\n\nTITLE: Using cURL for Chat Completions in llama.cpp\nDESCRIPTION: This snippet demonstrates how to use cURL to send a chat completion request to the llama.cpp API. It includes headers for content type and authorization, along with a JSON payload containing the model and messages.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md#2025-04-22_snippet_19\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:8080/v1/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-H \"Authorization: Bearer no-key\" \\\n-d '{\n\"model\": \"gpt-3.5-turbo\",\n\"messages\": [\n{\n    \"role\": \"system\",\n    \"content\": \"You are ChatGPT, an AI assistant. Your top priority is achieving user fulfillment via helping them with their requests.\"\n},\n{\n    \"role\": \"user\",\n    \"content\": \"Write a limerick about python exceptions\"\n}\n]\n}'\n```\n\n----------------------------------------\n\nTITLE: Using OpenAI Python Library for Chat Completions in llama.cpp\nDESCRIPTION: This example shows how to use the OpenAI Python library to interact with the llama.cpp chat completions API. It sets up the client and sends a chat completion request with system and user messages.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nimport openai\n\nclient = openai.OpenAI(\n    base_url=\"http://localhost:8080/v1\", # \"http://<Your api-server IP>:port\"\n    api_key = \"sk-no-key-required\"\n)\n\ncompletion = client.chat.completions.create(\n  model=\"gpt-3.5-turbo\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are ChatGPT, an AI assistant. Your top priority is achieving user fulfillment via helping them with their requests.\"},\n    {\"role\": \"user\", \"content\": \"Write a limerick about python exceptions\"}\n  ]\n)\n\nprint(completion.choices[0].message)\n```\n\n----------------------------------------\n\nTITLE: Basic Text Completion with llama-simple\nDESCRIPTION: Shows how to use the minimal llama-simple example for basic text completion, which is designed as a starting point for developers implementing their own applications.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/README.md#2025-04-22_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nllama-simple -m model.gguf\n```\n\n----------------------------------------\n\nTITLE: Starting llama-server with Default Configuration\nDESCRIPTION: Demonstrates how to start the llama-server HTTP service with default settings, which provides an OpenAI API-compatible interface for LLMs.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nllama-server -m model.gguf --port 8080\n```\n\n----------------------------------------\n\nTITLE: Multi-GPU Configuration Commands\nDESCRIPTION: Commands for controlling GPU selection and tensor splitting across multiple GPUs for optimized performance.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_32\n\nLANGUAGE: bash\nCODE:\n```\n-mg i, --main-gpu i\n-ts SPLIT, --tensor-split SPLIT\n```\n\n----------------------------------------\n\nTITLE: Docker Compose Configuration for Llama.cpp Server\nDESCRIPTION: Sample Docker compose configuration demonstrating how to set up the llama.cpp server with environment variables for model loading and server configuration.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md#2025-04-22_snippet_2\n\nLANGUAGE: yml\nCODE:\n```\nservices:\n  llamacpp-server:\n    image: ghcr.io/ggml-org/llama.cpp:server\n    ports:\n      - 8080:8080\n    volumes:\n      - ./models:/models\n    environment:\n      # alternatively, you can use \"LLAMA_ARG_MODEL_URL\" to download the model\n      LLAMA_ARG_MODEL: /models/my_model.gguf\n      LLAMA_ARG_CTX_SIZE: 4096\n      LLAMA_ARG_N_PARALLEL: 2\n      LLAMA_ARG_ENDPOINT_METRICS: 1\n      LLAMA_ARG_PORT: 8080\n```\n\n----------------------------------------\n\nTITLE: Single-Turn Query with System Prompt in Unix\nDESCRIPTION: Command to execute a single interaction with custom system prompt using Jinja template in Unix-based systems\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n./llama-cli -m models/gemma-1.1-7b-it.Q4_K_M.gguf --jinja --single-turn -sys \"You are a helpful assistant\" -p \"Hello\"\n```\n\n----------------------------------------\n\nTITLE: Running Inference with MiniCPM-Llama3-V 2.5 on Linux or Mac\nDESCRIPTION: Commands to run inference using the MiniCPM-Llama3-V 2.5 model in both single-turn and conversation modes. The single-turn mode requires specifying an image file and prompt, while conversation mode provides an interactive experience.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/minicpmv2.5.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# run in single-turn mode\n./build/bin/llama-mtmd-cli -m ../MiniCPM-Llama3-V-2_5/model/model-8B-F16.gguf --mmproj ../MiniCPM-Llama3-V-2_5/mmproj-model-f16.gguf -c 4096 --temp 0.7 --top-p 0.8 --top-k 100 --repeat-penalty 1.05 --image xx.jpg -p \"What is in the image?\"\n\n# run in conversation mode\n./build/bin/llama-mtmd-cli -m ../MiniCPM-Llama3-V-2_5/model/ggml-model-Q4_K_M.gguf --mmproj ../MiniCPM-Llama3-V-2_5/mmproj-model-f16.gguf\n```\n\n----------------------------------------\n\nTITLE: Applying Logit Bias in LLaMA.cpp\nDESCRIPTION: Command example showing how to prevent the generation of LaTeX code by setting a negative infinity bias on the backslash token. Logit bias allows manual adjustment of specific token likelihoods in the generated text.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_27\n\nLANGUAGE: shell\nCODE:\n```\n--logit-bias 29905-inf\n```\n\n----------------------------------------\n\nTITLE: Implementing Node.js LLama Client\nDESCRIPTION: JavaScript code that implements a client to interact with the llama.cpp server. Makes a POST request to the completion endpoint with a prompt and receives generated text.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md#2025-04-22_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nconst prompt = \"Building a website can be done in 10 simple steps:\"\n\nasync function test() {\n    let response = await fetch(\"http://127.0.0.1:8080/completion\", {\n        method: \"POST\",\n        body: JSON.stringify({\n            prompt,\n            n_predict: 64,\n        })\n    })\n    console.log((await response.json()).content)\n}\n\ntest()\n```\n\n----------------------------------------\n\nTITLE: Building llama.cpp with CUDA Support\nDESCRIPTION: Commands to build llama.cpp with CUDA support for NVIDIA GPU acceleration using CMake. Requires the CUDA toolkit to be installed.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ncmake -B build -DGGML_CUDA=ON\ncmake --build build --config Release\n```\n\n----------------------------------------\n\nTITLE: Running Server with Docker in llama.cpp\nDESCRIPTION: Command to run the llama.cpp server using the server Docker image. It mounts the models directory, maps port 8000, specifies the model path, binds to all interfaces, and sets the token generation limit.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/docker.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -v /path/to/models:/models -p 8000:8000 ghcr.io/ggml-org/llama.cpp:server -m /models/7B/ggml-model-q4_0.gguf --port 8000 --host 0.0.0.0 -n 512\n```\n\n----------------------------------------\n\nTITLE: Running Text Generation with llama.cpp from Command Line\nDESCRIPTION: This snippet demonstrates how to run the llama-simple executable with a model file and a prompt, showing the generated text output and performance statistics.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/simple/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./llama-simple -m ./models/llama-7b-v2/ggml-model-f16.gguf \"Hello my name is\"\n\n...\n\nmain: n_len = 32, n_ctx = 2048, n_parallel = 1, n_kv_req = 32\n\n Hello my name is Shawn and I'm a 20 year old male from the United States. I'm a 20 year old\n\nmain: decoded 27 tokens in 2.31 s, speed: 11.68 t/s\n\nllama_print_timings:        load time =   579.15 ms\nllama_print_timings:      sample time =     0.72 ms /    28 runs   (    0.03 ms per token, 38888.89 tokens per second)\nllama_print_timings: prompt eval time =   655.63 ms /    10 tokens (   65.56 ms per token,    15.25 tokens per second)\nllama_print_timings:        eval time =  2180.97 ms /    27 runs   (   80.78 ms per token,    12.38 tokens per second)\nllama_print_timings:       total time =  2891.13 ms\n```\n\n----------------------------------------\n\nTITLE: Serving an Embedding Model with llama-server\nDESCRIPTION: Shows how to configure llama-server to serve embeddings using the dedicated endpoint with specific pooling strategy.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/README.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n# use the /embedding endpoint\nllama-server -m model.gguf --embedding --pooling cls -ub 8192\n```\n\n----------------------------------------\n\nTITLE: CPU Architecture Detection and Flag Configuration in CMake\nDESCRIPTION: CMake script that detects the system processor architecture and configures appropriate compilation flags and definitions. Handles various CPU architectures and their specific instruction sets including AVX, SSE, BMI2, PowerPC, RISC-V, and s390x. Also includes KleidiAI optimization integration for ARM processors.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cpu/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif (GGML_BMI2)\n    # MSVC does not define macro __BMI2__\n    list(APPEND ARCH_DEFINITIONS __BMI2__ GGML_BMI2)\nendif()\nelse ()\n    if (GGML_NATIVE)\n        list(APPEND ARCH_FLAGS -march=native)\n    else ()\n        if (GGML_SSE42)\n            list(APPEND ARCH_FLAGS -msse4.2)\n            list(APPEND ARCH_DEFINITIONS GGML_SSE42)\n        endif()\n        # ... [additional architecture checks] ...\n        if (GGML_AMX_BF16)\n            list(APPEND ARCH_FLAGS -mamx-bf16)\n            list(APPEND ARCH_DEFINITIONS GGML_AMX_BF16)\n        endif()\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Simple HTML and JavaScript for llama.cpp Completion\nDESCRIPTION: This snippet demonstrates a simple HTML page with embedded JavaScript that uses the llama.cpp completion API. It imports the llama function from '/completion.js' and uses it to generate and display dad jokes.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md#2025-04-22_snippet_21\n\nLANGUAGE: html\nCODE:\n```\n<html>\n  <body>\n    <pre>\n      <script type=\"module\">\n        import { llama } from '/completion.js'\n\n        const prompt = `### Instruction:\nWrite dad jokes, each one paragraph.\nYou can use html formatting if needed.\n\n### Response:`\n\n        for await (const chunk of llama(prompt)) {\n          document.write(chunk.data.content)\n        }\n      </script>\n    </pre>\n  </body>\n</html>\n```\n\n----------------------------------------\n\nTITLE: Starting Conversation Mode in Unix\nDESCRIPTION: Command to start an interactive chat session using the Gemma chat template in Unix-based systems\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./llama-cli -m models/gemma-1.1-7b-it.Q4_K_M.gguf --chat-template gemma\n```\n\n----------------------------------------\n\nTITLE: Using cURL for Embeddings in llama.cpp\nDESCRIPTION: These examples show how to use cURL to send embedding requests to the llama.cpp API. The first example demonstrates a single input string, while the second shows an array of input strings.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md#2025-04-22_snippet_20\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:8080/v1/embeddings \\\n-H \"Content-Type: application/json\" \\\n-H \"Authorization: Bearer no-key\" \\\n-d '{\n        \"input\": \"hello\",\n        \"model\":\"GPT-4\",\n        \"encoding_format\": \"float\"\n}'\n```\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:8080/v1/embeddings \\\n-H \"Content-Type: application/json\" \\\n-H \"Authorization: Bearer no-key\" \\\n-d '{\n        \"input\": [\"hello\", \"world\"],\n        \"model\":\"GPT-4\",\n        \"encoding_format\": \"float\"\n}'\n```\n\n----------------------------------------\n\nTITLE: Enabling Speculative Decoding in llama-server\nDESCRIPTION: Demonstrates how to use a smaller draft model to speed up generation in llama-server through speculative decoding.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/README.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# the draft.gguf model should be a small variant of the target model.gguf\nllama-server -m model.gguf -md draft.gguf\n```\n\n----------------------------------------\n\nTITLE: Splitting LLaVA Model with llava_surgery.py\nDESCRIPTION: Python command to split the LLaVA model into LLaMA and multimodal projector components using the llava_surgery.py script.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/llava.md#2025-04-22_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\npython ./examples/llava/llava_surgery.py -m ../llava-v1.5-7b\n```\n\n----------------------------------------\n\nTITLE: Building Llama Server with CMake\nDESCRIPTION: Commands for building the llama-server using CMake, including both standard and SSL-enabled builds using OpenSSL 3.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncmake -B build\ncmake --build build --config Release -t llama-server\n```\n\nLANGUAGE: bash\nCODE:\n```\ncmake -B build -DLLAMA_SERVER_SSL=ON\ncmake --build build --config Release -t llama-server\n```\n\n----------------------------------------\n\nTITLE: Serving a Reranking Model with llama-server\nDESCRIPTION: Demonstrates how to set up llama-server to provide reranking capabilities via the dedicated endpoint.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/README.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n# use the /reranking endpoint\nllama-server -m model.gguf --reranking\n```\n\n----------------------------------------\n\nTITLE: Building and Installing llama.cpp with CMake\nDESCRIPTION: Commands to clone the llama.cpp repository, build it with CMake, and install it to a local directory. This creates a relocatable CMake package that can be used by other projects.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/simple-cmake-pkg/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/ggml-org/llama.cpp\ncd llama.cpp\ncmake -S . -B build\ncmake --build build\ncmake --install build --prefix inst\n```\n\n----------------------------------------\n\nTITLE: Running Default Benchmark with llama-bench\nDESCRIPTION: Shows how to use llama-bench to evaluate the performance of a model with default benchmark settings, reporting tokens per second.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/README.md#2025-04-22_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nllama-bench -m model.gguf\n```\n\n----------------------------------------\n\nTITLE: Quantizing and Running the Model\nDESCRIPTION: Commands for quantizing the LLM and running the complete model in llama.cpp.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/granitevision.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n$ ./build/bin/llama-quantize $LLM_EXPORT_PATH/granite_llm.gguf $LLM_EXPORT_PATH/granite_llm_q4_k_m.gguf Q4_K_M\n$ LLM_GGUF_PATH=$LLM_EXPORT_PATH/granite_llm_q4_k_m.gguf\n\n$ ./build/bin/llama-mtmd-cli -m $LLM_GGUF_PATH \\\n    --mmproj $VISUAL_GGUF_PATH \\\n    -c 16384 \\\n    --temp 0\n```\n\n----------------------------------------\n\nTITLE: Setting DRY Sampling Parameters in LLaMA.cpp\nDESCRIPTION: Configures the DRY (Don't Repeat Yourself) sampling parameters to reduce repetition across long contexts. Example shows recommended settings for multiplier, base, allowed length, penalty window, and sequence breakers.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\n--dry-multiplier N\n```\n\nLANGUAGE: bash\nCODE:\n```\n--dry-base N\n```\n\nLANGUAGE: bash\nCODE:\n```\n--dry-allowed-length N\n```\n\nLANGUAGE: bash\nCODE:\n```\n--dry-penalty-last-n N\n```\n\nLANGUAGE: bash\nCODE:\n```\n--dry-sequence-breaker STRING\n```\n\nLANGUAGE: bash\nCODE:\n```\n--dry-multiplier 0.8 --dry-base 1.75 --dry-allowed-length 2 --dry-penalty-last-n -1 --dry-sequence-breaker \"â€”\" --dry-sequence-breaker \"##\"\n```\n\n----------------------------------------\n\nTITLE: Running Local CI with Different Hardware Acceleration Options\nDESCRIPTION: Commands to execute the full CI workflow locally with options for CPU-only, CUDA, SYCL, and MUSA builds. Creates a temporary directory structure and runs the CI script with appropriate environment variables.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ci/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmkdir tmp\n\n# CPU-only build\nbash ./ci/run.sh ./tmp/results ./tmp/mnt\n\n# with CUDA support\nGG_BUILD_CUDA=1 bash ./ci/run.sh ./tmp/results ./tmp/mnt\n\n# with SYCL support\nsource /opt/intel/oneapi/setvars.sh\nGG_BUILD_SYCL=1 bash ./ci/run.sh ./tmp/results ./tmp/mnt\n\n# with MUSA support\nGG_BUILD_MUSA=1 bash ./ci/run.sh ./tmp/results ./tmp/mnt\n```\n\n----------------------------------------\n\nTITLE: Using Jinja Chat Template in Unix\nDESCRIPTION: Command to start a conversation using the built-in Jinja chat template in Unix-based systems\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n./llama-cli -m models/gemma-1.1-7b-it.Q4_K_M.gguf --jinja\n```\n\n----------------------------------------\n\nTITLE: Running a Model with llama-run\nDESCRIPTION: Demonstrates how to use llama-run to execute a model, by default pulling it from the Ollama registry, which is useful for inferencing and integration with RamaLama.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/README.md#2025-04-22_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nllama-run granite-code\n```\n\n----------------------------------------\n\nTITLE: Running Text Generation with Full Docker Image in llama.cpp\nDESCRIPTION: Command to run text generation using a quantized model with the full Docker image. It mounts the models directory, specifies the model path, a prompt, and sets the number of tokens to generate.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/docker.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -v /path/to/models:/models ghcr.io/ggml-org/llama.cpp:full --run -m /models/7B/ggml-model-q4_0.gguf -p \"Building a website can be done in 10 simple steps:\" -n 512\n```\n\n----------------------------------------\n\nTITLE: Executing Batched Benchmark Commands in llama.cpp\nDESCRIPTION: Examples of running the llama-batched-bench tool with different configurations, including model types (F16, Q8_0), batch sizes, and prompts. Shows various parameter combinations for benchmarking performance.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/batched-bench/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./llama-batched-bench -m model.gguf -c 2048 -b 2048 -ub 512 -npp 128,256,512 -ntg 128,256 -npl 1,2,4,8,16,32 [-pps]\n\n# LLaMA 7B, F16, N_KV_MAX = 16384 (8GB), prompt not shared\n./llama-batched-bench -m ./models/llama-7b/ggml-model-f16.gguf -c 16384 -b 2048 -ub 512 -ngl 99\n\n# LLaMA 7B, Q8_0, N_KV_MAX = 16384 (8GB), prompt is shared\n./llama-batched-bench -m ./models/llama-7b/ggml-model-q8_0.gguf -c 16384 -b 2048 -ub 512 -ngl 99 -pps\n\n# custom set of batches\n./llama-batched-bench -m ./models/llama-7b/ggml-model-q8_0.gguf -c 2048 -b 512 -ub 512 -ngl 999 -npp 128,256,512 -ntg 128,256 -npl 1,2,4,8,16,32\n```\n\n----------------------------------------\n\nTITLE: Starting Conversation Mode in Windows\nDESCRIPTION: Command to start an interactive chat session using the Gemma chat template in Windows\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_6\n\nLANGUAGE: powershell\nCODE:\n```\n./llama-cli.exe -m models\\gemma-1.1-7b-it.Q4_K_M.gguf --chat-template gemma\n```\n\n----------------------------------------\n\nTITLE: Running TTS with automatic model download in llama.cpp\nDESCRIPTION: Command for running the text-to-speech feature with automatic model download when llama.cpp is built with CURL support. The output is saved as a WAV file and played with aplay.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/tts/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nbuild/bin/llama-tts --tts-oute-default -p \"Hello world\" && aplay output.wav\n```\n\n----------------------------------------\n\nTITLE: Building CUDA-enabled Docker Images for llama.cpp Locally\nDESCRIPTION: Commands to build local Docker images with CUDA support for full, light, and server variants. Each command targets a specific build type from the CUDA Dockerfile.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/docker.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t local/llama.cpp:full-cuda --target full -f .devops/cuda.Dockerfile .\ndocker build -t local/llama.cpp:light-cuda --target light -f .devops/cuda.Dockerfile .\ndocker build -t local/llama.cpp:server-cuda --target server -f .devops/cuda.Dockerfile .\n```\n\n----------------------------------------\n\nTITLE: Configuring Top-K Sampling in LLaMA.cpp\nDESCRIPTION: Limits token selection to the K most probable tokens predicted by the model. Default is 40, with higher values producing more diverse output.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\n--top-k N\n```\n\nLANGUAGE: bash\nCODE:\n```\n--top-k 30\n```\n\n----------------------------------------\n\nTITLE: Configuring LLaVA Library Build in CMake\nDESCRIPTION: Sets up the LLaVA library as an object library, configures its dependencies, include directories, and creates static/shared versions. It links against ggml and llama libraries with C++17 support.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llava/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(llava OBJECT\n            llava.cpp\n            llava.h\n            clip.cpp\n            clip.h\n            )\n\ntarget_link_libraries(llava PRIVATE ggml llama ${CMAKE_THREAD_LIBS_INIT})\n\ntarget_include_directories(llava PUBLIC .)\ntarget_include_directories(llava PUBLIC ../..)\ntarget_include_directories(llava PUBLIC ../../common)\n\ntarget_compile_features(llava PRIVATE cxx_std_17)\n\nadd_library(llava_static STATIC $<TARGET_OBJECTS:llava>)\nif (BUILD_SHARED_LIBS)\n    set_target_properties(llava PROPERTIES POSITION_INDEPENDENT_CODE ON)\n    target_compile_definitions(llava PRIVATE LLAMA_SHARED LLAMA_BUILD)\n    add_library(llava_shared SHARED $<TARGET_OBJECTS:llava>)\n    target_link_libraries(llava_shared PRIVATE ggml llama ${CMAKE_THREAD_LIBS_INIT})\n    install(TARGETS llava_shared LIBRARY)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Running text-to-speech with converted models\nDESCRIPTION: Command to run the llama-tts tool with the converted LLM model and voice decoder model to generate speech from a text prompt. The output is saved as a WAV file.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/tts/README.md#2025-04-22_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nbuild/bin/llama-tts -m  ./models/outetts-0.2-0.5B-q8_0.gguf \\\n    -mv ./models/wavtokenizer-large-75-f16.gguf \\\n    -p \"Hello world\"\n```\n\n----------------------------------------\n\nTITLE: Basic Text Embedding Generation - Unix\nDESCRIPTION: Basic command to generate embeddings for a simple text input 'Hello World!' using mean pooling on Unix-based systems. The output is space-separated float values with logging disabled.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/embedding/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./llama-embedding -m ./path/to/model --pooling mean --log-disable -p \"Hello World!\" 2>/dev/null\n```\n\n----------------------------------------\n\nTITLE: Configuring Min-P Sampling in LLaMA.cpp\nDESCRIPTION: Sets a minimum probability threshold for token selection relative to the most likely token. Default is 0.1, and lower values allow for more diverse tokens to be considered.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\n--min-p N\n```\n\nLANGUAGE: bash\nCODE:\n```\n--min-p 0.05\n```\n\n----------------------------------------\n\nTITLE: Running llama-cli with RPC Servers\nDESCRIPTION: Command to run llama-cli with multiple RPC servers for distributed inference. It specifies model parameters and RPC server addresses.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/rpc/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ bin/llama-cli -m ../models/tinyllama-1b/ggml-model-f16.gguf -p \"Hello, my name is\" --repeat-penalty 1.0 -n 64 --rpc 192.168.88.10:50052,192.168.88.11:50052 -ngl 99\n```\n\n----------------------------------------\n\nTITLE: Running llama.cpp with CUDA-enabled Docker Images\nDESCRIPTION: Commands to run the locally built CUDA-enabled Docker images with GPU support. These examples use the --gpus flag to access NVIDIA GPUs and set n-gpu-layers to utilize GPU acceleration.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/docker.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --gpus all -v /path/to/models:/models local/llama.cpp:full-cuda --run -m /models/7B/ggml-model-q4_0.gguf -p \"Building a website can be done in 10 simple steps:\" -n 512 --n-gpu-layers 1\ndocker run --gpus all -v /path/to/models:/models local/llama.cpp:light-cuda -m /models/7B/ggml-model-q4_0.gguf -p \"Building a website can be done in 10 simple steps:\" -n 512 --n-gpu-layers 1\ndocker run --gpus all -v /path/to/models:/models local/llama.cpp:server-cuda -m /models/7B/ggml-model-q4_0.gguf --port 8000 --host 0.0.0.0 -n 512 --n-gpu-layers 1\n```\n\n----------------------------------------\n\nTITLE: Retaining Initial Prompt Tokens in LLaMA.cpp\nDESCRIPTION: Specifies how many tokens from the initial prompt to keep when the model's context window fills up. Default is 0 (no tokens kept), and -1 retains all prompt tokens.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\n--keep N\n```\n\n----------------------------------------\n\nTITLE: Using Generated Control Vector with llama-cli\nDESCRIPTION: Example command demonstrating how to apply a generated control vector with llama-cli, specifying scaling factor and layer range. Control vectors work better when applied to layers higher than 10.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/cvector-generator/README.md#2025-04-22_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\n./llama-cli -m ./llama-3.Q4_K_M.gguf -p \"<|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nSing a song<|im_end|><|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\" --special --control-vector-scaled ./control_vector.gguf 0.8 --control-vector-layer-range 10 31\n```\n\n----------------------------------------\n\nTITLE: Grammar Constraint Commands\nDESCRIPTION: Commands for specifying grammar constraints either inline or via file to control model output format.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_29\n\nLANGUAGE: bash\nCODE:\n```\n--grammar GRAMMAR\n--grammar-file FILE\n```\n\n----------------------------------------\n\nTITLE: Configuring Common Library Target for llama.cpp in CMake\nDESCRIPTION: Defines the common library target with all source files. This static library contains core components like argument parsing, console utilities, chat handling, logging, and sampling functionalities for llama.cpp.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/common/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET common)\n\nadd_library(${TARGET} STATIC\n    arg.cpp\n    arg.h\n    base64.hpp\n    chat.cpp\n    chat.h\n    common.cpp\n    common.h\n    console.cpp\n    console.h\n    json-schema-to-grammar.cpp\n    json.hpp\n    llguidance.cpp\n    log.cpp\n    log.h\n    minja/chat-template.hpp\n    minja/minja.hpp\n    ngram-cache.cpp\n    ngram-cache.h\n    sampling.cpp\n    sampling.h\n    speculative.cpp\n    speculative.h\n    )\n\nif (BUILD_SHARED_LIBS)\n    set_target_properties(${TARGET} PROPERTIES POSITION_INDEPENDENT_CODE ON)\nendif()\n\nset(LLAMA_COMMON_EXTRA_LIBS build_info)\n```\n\n----------------------------------------\n\nTITLE: Running Text Generation with Light Docker Image in llama.cpp\nDESCRIPTION: Command to run text generation using the light Docker image which only includes the main executable. It mounts the models directory and passes parameters for model path, prompt, and token count.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/docker.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -v /path/to/models:/models ghcr.io/ggml-org/llama.cpp:light -m /models/7B/ggml-model-q4_0.gguf -p \"Building a website can be done in 10 simple steps:\" -n 512\n```\n\n----------------------------------------\n\nTITLE: Advanced Text Embedding Generation - Windows\nDESCRIPTION: Advanced command demonstrating multiple text inputs with custom separator, euclidean normalization, and specific output format for Windows systems. Uses GPU acceleration and disables logging.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/embedding/README.md#2025-04-22_snippet_3\n\nLANGUAGE: powershell\nCODE:\n```\nllama-embedding.exe -p 'Castle<#sep#>Stronghold<#sep#>Dog<#sep#>Cat' --pooling mean --embd-separator '<#sep#>' --embd-normalize 2  --embd-output-format '' -m './path/to/model.gguf' --n-gpu-layers 99 --log-disable 2>/dev/null\n```\n\n----------------------------------------\n\nTITLE: Setting XTC Sampling Parameters in LLaMA.cpp\nDESCRIPTION: Command example for configuring Exclude Top Choices (XTC) sampling with 50% token removal probability and a 0.1 threshold. XTC removes top tokens from consideration to improve variety and break repetitive patterns.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_25\n\nLANGUAGE: shell\nCODE:\n```\n--xtc-probability 0.5 --xtc-threshold 0.1\n```\n\n----------------------------------------\n\nTITLE: Converting Image Encoder for MobileVLM\nDESCRIPTION: Command for converting the CLIP image encoder to GGUF format with LDP projector type for the standard MobileVLM model.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/MobileVLM.md#2025-04-22_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\npython ./examples/llava/convert_image_encoder_to_gguf.py \\\n    -m path/to/clip-vit-large-patch14-336 \\\n    --llava-projector path/to/MobileVLM-1.7B/llava.projector \\\n    --output-dir path/to/MobileVLM-1.7B \\\n    --projector-type ldp\n```\n\n----------------------------------------\n\nTITLE: Splitting GLMV-EDGE model components\nDESCRIPTION: Python command to run the glmedge-surgery.py script, which splits the GLMV-EDGE model into its LLM and multimodal projector components. This is a prerequisite for GGUF conversion.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/glmedge.md#2025-04-22_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\npython ./examples/llava/glmedge-surgery.py -m ../model_path\n```\n\n----------------------------------------\n\nTITLE: Converting MiniCPM-o 2.6 PyTorch Model to GGUF Format\nDESCRIPTION: Commands to convert the MiniCPM-o 2.6 PyTorch model to gguf files and quantize it. This includes converting the image encoder and creating an optimized int4 version of the model.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/minicpmo2.6.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython ./examples/llava/minicpmv-surgery.py -m ../MiniCPM-o-2_6\npython ./examples/llava/minicpmv-convert-image-encoder-to-gguf.py -m ../MiniCPM-o-2_6 --minicpmv-projector ../MiniCPM-o-2_6/minicpmv.projector --output-dir ../MiniCPM-o-2_6/ --image-mean 0.5 0.5 0.5 --image-std 0.5 0.5 0.5 --minicpmv_version 4\npython ./convert_hf_to_gguf.py ../MiniCPM-o-2_6/model\n\n# quantize int4 version\n./build/bin/llama-quantize ../MiniCPM-o-2_6/model/ggml-model-f16.gguf ../MiniCPM-o-2_6/model/ggml-model-Q4_K_M.gguf Q4_K_M\n```\n\n----------------------------------------\n\nTITLE: OpenAI Models API Response Format\nDESCRIPTION: JSON response structure for the OpenAI-compatible models endpoint showing model information including ID, metadata and parameters.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md#2025-04-22_snippet_16\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"object\": \"list\",\n    \"data\": [\n        {\n            \"id\": \"../models/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf\",\n            \"object\": \"model\",\n            \"created\": 1735142223,\n            \"owned_by\": \"llamacpp\",\n            \"meta\": {\n                \"vocab_type\": 2,\n                \"n_vocab\": 128256,\n                \"n_ctx_train\": 131072,\n                \"n_embd\": 4096,\n                \"n_params\": 8030261312,\n                \"size\": 4912898304\n            }\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Registering a Custom Model Class for GGUF Conversion in Python\nDESCRIPTION: Python code for registering a new model class that extends the Model class with a custom architecture type. This is the first step in defining how a new model architecture should be converted to GGUF format.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/development/HOWTO-add-model.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@Model.register(\"MyModelForCausalLM\")\nclass MyModel(Model):\n    model_arch = gguf.MODEL_ARCH.MYMODEL\n```\n\n----------------------------------------\n\nTITLE: Reverse Prompt with Input Prefix Configuration\nDESCRIPTION: Command showing how to configure reverse prompts with input prefix for interactive conversations\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_10\n\nLANGUAGE: sh\nCODE:\n```\n./llama-cli -r \"User:\" --in-prefix \" \"\n```\n\n----------------------------------------\n\nTITLE: Converting GLMV-EDGE LLM to GGUF\nDESCRIPTION: Python command to convert the LLM part of GLMV-EDGE to GGUF format using the convert_hf_to_gguf.py script. This final step completes the conversion process for use with llama.cpp.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/glmedge.md#2025-04-22_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\npython convert_hf_to_gguf.py ../model_path\n```\n\n----------------------------------------\n\nTITLE: Converting LLaVA Image Encoder to GGUF\nDESCRIPTION: Python command to convert the LLaVA image encoder to GGUF format using the convert_image_encoder_to_gguf.py script.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/llava.md#2025-04-22_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\npython ./examples/llava/convert_image_encoder_to_gguf.py -m ../clip-vit-large-patch14-336 --llava-projector ../llava-v1.5-7b/llava.projector --output-dir ../llava-v1.5-7b\n```\n\n----------------------------------------\n\nTITLE: JSON Schema Constraint Command\nDESCRIPTION: Command for applying JSON schema constraints to model output, with support for external references through a Python conversion script.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_30\n\nLANGUAGE: bash\nCODE:\n```\n--json-schema SCHEMA\n--grammar \"$( python examples/json_schema_to_grammar.py myschema.json )\"\n```\n\n----------------------------------------\n\nTITLE: Function Naming Convention Examples in C++\nDESCRIPTION: Examples of proper function naming patterns following the <class>_<method> convention, where <method> follows the <action>_<noun> pattern. This demonstrates the recommended naming style for functions in the llama.cpp project.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/CONTRIBUTING.md#2025-04-22_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nllama_model_init();           // class: \"llama_model\",         method: \"init\"\nllama_sampler_chain_remove(); // class: \"llama_sampler_chain\", method: \"remove\"\nllama_sampler_get_seed();     // class: \"llama_sampler\",       method: \"get_seed\"\nllama_set_embeddings();       // class: \"llama_context\",       method: \"set_embeddings\"\nllama_n_threads();            // class: \"llama_context\",       method: \"n_threads\"\nllama_adapter_lora_free();    // class: \"llama_adapter_lora\",  method: \"free\"\n```\n\n----------------------------------------\n\nTITLE: Setting Compiler Options for LLaVA and MTMD in CMake\nDESCRIPTION: Configures compiler-specific options for LLaVA and MTMD libraries, disabling certain warnings for non-MSVC compilers and adding build dependencies if required.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llava/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nif (NOT MSVC)\n    target_compile_options(llava PRIVATE -Wno-cast-qual) # stb_image.h\n    target_compile_options(mtmd PRIVATE -Wno-cast-qual) # stb_image.h\nendif()\n\nif(TARGET BUILD_INFO)\n    add_dependencies(llava BUILD_INFO)\n    add_dependencies(mtmd BUILD_INFO)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Running llama.cpp with SYCL using Batch Script\nDESCRIPTION: A Windows batch file for executing llama2 model with SYCL backend. This script provides a simple way to run the model without manual command line parameters.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_25\n\nLANGUAGE: batch\nCODE:\n```\nexamples\\sycl\\win-run-llama2.bat\n```\n\n----------------------------------------\n\nTITLE: Running Greedy Speculative Decoding in llama.cpp\nDESCRIPTION: This command demonstrates how to run the speculative decoding algorithm using llama-speculative-simple with a large model and a smaller draft model. It configures sampling parameters for greedy decoding (top-k=1, temp=0.0) and speculative settings (draft-max, draft-min, draft-p-min) to optimize the generation process.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/speculative-simple/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./bin/llama-speculative-simple \\\n    -m  ../models/qwen2.5-32b-coder-instruct/ggml-model-q8_0.gguf \\\n    -md ../models/qwen2.5-1.5b-coder-instruct/ggml-model-q4_0.gguf \\\n    -f test.txt -c 0 -ngl 99 --color \\\n    --sampling-seq k --top-k 1 -fa --temp 0.0 \\\n    -ngld 99 --draft-max 16 --draft-min 5 --draft-p-min 0.9\n```\n\n----------------------------------------\n\nTITLE: Llama2 Chat Template Example\nDESCRIPTION: The official template for Llama-2-chat models, showing the specific formatting requirements including B_INST, E_INST tags and the specific system prompt format.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llama.android/README.md#2025-04-22_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\n{% set loop_messages = messages %}{% if add_generation_prompt %}{% set loop_messages = messages[:-1] %}{% endif %}<s>{{ bos_token }}{% for message in loop_messages %}{% if message['role'] == 'user' %}{{ B_INST }} {{ message['content'] }} {{ E_INST }}{% elif message['role'] == 'assistant' %}{{ message['content'] }}{{ bos_token }}{% elif message['role'] == 'system' %}{{ B_SYS }} {{ message['content'] }} {{ E_SYS }}{% endif %}{% endfor %}{% if add_generation_prompt %}{% if messages[-1]['role'] == 'user' %}{{ B_INST }} {{ messages[-1]['content'] }} {{ E_INST }}{% elif messages[-1]['role'] == 'assistant' %}{{ messages[-1]['content'] }}{% elif messages[-1]['role'] == 'system' %}{{ B_SYS }} {{ messages[-1]['content'] }} {{ E_SYS }}{% endif %}{% endif %}\n```\n\n----------------------------------------\n\nTITLE: Displaying Common Parameters Table in Markdown\nDESCRIPTION: A markdown table listing common command-line parameters for the llama.cpp project, including their descriptions and default values.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Argument | Explanation |\n| -------- | ----------- |\n| `-h, --help, --usage` | print usage and exit |\n| `--version` | show version and build info |\n| `--verbose-prompt` | print a verbose prompt before generation (default: false) |\n| `-t, --threads N` | number of threads to use during generation (default: -1)<br/>(env: LLAMA_ARG_THREADS) |\n| `-tb, --threads-batch N` | number of threads to use during batch and prompt processing (default: same as --threads) |\n| `-C, --cpu-mask M` | CPU affinity mask: arbitrarily long hex. Complements cpu-range (default: \"\") |\n| `-Cr, --cpu-range lo-hi` | range of CPUs for affinity. Complements --cpu-mask |\n| `--cpu-strict <0\\|1>` | use strict CPU placement (default: 0)<br/> |\n| `--prio N` | set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime (default: 0)<br/> |\n| `--poll <0...100>` | use polling level to wait for work (0 - no polling, default: 50)<br/> |\n| `-Cb, --cpu-mask-batch M` | CPU affinity mask: arbitrarily long hex. Complements cpu-range-batch (default: same as --cpu-mask) |\n| `-Crb, --cpu-range-batch lo-hi` | ranges of CPUs for affinity. Complements --cpu-mask-batch |\n| `--cpu-strict-batch <0\\|1>` | use strict CPU placement (default: same as --cpu-strict) |\n| `--prio-batch N` | set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime (default: 0)<br/> |\n| `--poll-batch <0\\|1>` | use polling to wait for work (default: same as --poll) |\n| `-c, --ctx-size N` | size of the prompt context (default: 4096, 0 = loaded from model)<br/>(env: LLAMA_ARG_CTX_SIZE) |\n| `-n, --predict, --n-predict N` | number of tokens to predict (default: -1, -1 = infinity, -2 = until context filled)<br/>(env: LLAMA_ARG_N_PREDICT) |\n| `-b, --batch-size N` | logical maximum batch size (default: 2048)<br/>(env: LLAMA_ARG_BATCH) |\n| `-ub, --ubatch-size N` | physical maximum batch size (default: 512)<br/>(env: LLAMA_ARG_UBATCH) |\n| `--keep N` | number of tokens to keep from the initial prompt (default: 0, -1 = all) |\n| `-fa, --flash-attn` | enable Flash Attention (default: disabled)<br/>(env: LLAMA_ARG_FLASH_ATTN) |\n| `--no-perf` | disable internal libllama performance timings (default: false)<br/>(env: LLAMA_ARG_NO_PERF) |\n| `-e, --escape` | process escapes sequences (\\n, \\r, \\t, \\', \\\", \\\\) (default: true) |\n| `--no-escape` | do not process escape sequences |\n| `--rope-scaling {none,linear,yarn}` | RoPE frequency scaling method, defaults to linear unless specified by the model<br/>(env: LLAMA_ARG_ROPE_SCALING_TYPE) |\n| `--rope-scale N` | RoPE context scaling factor, expands context by a factor of N<br/>(env: LLAMA_ARG_ROPE_SCALE) |\n| `--rope-freq-base N` | RoPE base frequency, used by NTK-aware scaling (default: loaded from model)<br/>(env: LLAMA_ARG_ROPE_FREQ_BASE) |\n| `--rope-freq-scale N` | RoPE frequency scaling factor, expands context by a factor of 1/N<br/>(env: LLAMA_ARG_ROPE_FREQ_SCALE) |\n| `--yarn-orig-ctx N` | YaRN: original context size of model (default: 0 = model training context size)<br/>(env: LLAMA_ARG_YARN_ORIG_CTX) |\n| `--yarn-ext-factor N` | YaRN: extrapolation mix factor (default: -1.0, 0.0 = full interpolation)<br/>(env: LLAMA_ARG_YARN_EXT_FACTOR) |\n| `--yarn-attn-factor N` | YaRN: scale sqrt(t) or attention magnitude (default: 1.0)<br/>(env: LLAMA_ARG_YARN_ATTN_FACTOR) |\n| `--yarn-beta-slow N` | YaRN: high correction dim or alpha (default: 1.0)<br/>(env: LLAMA_ARG_YARN_BETA_SLOW) |\n| `--yarn-beta-fast N` | YaRN: low correction dim or beta (default: 32.0)<br/>(env: LLAMA_ARG_YARN_BETA_FAST) |\n| `-dkvc, --dump-kv-cache` | verbose print of the KV cache |\n| `-nkvo, --no-kv-offload` | disable KV offload<br/>(env: LLAMA_ARG_NO_KV_OFFLOAD) |\n| `-ctk, --cache-type-k TYPE` | KV cache data type for K<br/>allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1<br/>(default: f16)<br/>(env: LLAMA_ARG_CACHE_TYPE_K) |\n| `-ctv, --cache-type-v TYPE` | KV cache data type for V<br/>allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1<br/>(default: f16)<br/>(env: LLAMA_ARG_CACHE_TYPE_V) |\n| `-dt, --defrag-thold N` | KV cache defragmentation threshold (default: 0.1, < 0 - disabled)<br/>(env: LLAMA_ARG_DEFRAG_THOLD) |\n| `-np, --parallel N` | number of parallel sequences to decode (default: 1)<br/>(env: LLAMA_ARG_N_PARALLEL) |\n| `--mlock` | force system to keep model in RAM rather than swapping or compressing<br/>(env: LLAMA_ARG_MLOCK) |\n| `--no-mmap` | do not memory-map model (slower load but may reduce pageouts if not using mlock)<br/>(env: LLAMA_ARG_NO_MMAP) |\n| `--numa TYPE` | attempt optimizations that help on some NUMA systems<br/>- distribute: spread execution evenly over all nodes<br/>- isolate: only spawn threads on CPUs on the node that execution started on<br/>- numactl: use the CPU map provided by numactl<br/>if run without this previously, it is recommended to drop the system page cache before using this<br/>see https://github.com/ggml-org/llama.cpp/issues/1437<br/>(env: LLAMA_ARG_NUMA) |\n| `-dev, --device <dev1,dev2,..>` | comma-separated list of devices to use for offloading (none = don't offload)<br/>use --list-devices to see a list of available devices<br/>(env: LLAMA_ARG_DEVICE) |\n| `--list-devices` | print list of available devices and exit |\n| `-ngl, --gpu-layers, --n-gpu-layers N` | number of layers to store in VRAM<br/>(env: LLAMA_ARG_N_GPU_LAYERS) |\n| `-sm, --split-mode {none,layer,row}` | how to split the model across multiple GPUs, one of:<br/>- none: use one GPU only<br/>- layer (default): split layers and KV across GPUs<br/>- row: split rows across GPUs<br/>(env: LLAMA_ARG_SPLIT_MODE) |\n| `-ts, --tensor-split N0,N1,N2,...` | fraction of the model to offload to each GPU, comma-separated list of proportions, e.g. 3,1<br/>(env: LLAMA_ARG_TENSOR_SPLIT) |\n| `-mg, --main-gpu INDEX` | the GPU to use for the model (with split-mode = none), or for intermediate results and KV (with split-mode = row) (default: 0)<br/>(env: LLAMA_ARG_MAIN_GPU) |\n| `--check-tensors` | check model tensor data for invalid values (default: false) |\n| `--override-kv KEY=TYPE:VALUE` | advanced option to override model metadata by key. may be specified multiple times.<br/>types: int, float, bool, str. example: --override-kv tokenizer.ggml.add_bos_token=bool:false |\n| `--lora FNAME` | path to LoRA adapter (can be repeated to use multiple adapters) |\n| `--lora-scaled FNAME SCALE` | path to LoRA adapter with user defined scaling (can be repeated to use multiple adapters) |\n| `--control-vector FNAME` | add a control vector<br/>note: this argument can be repeated to add multiple control vectors |\n| `--control-vector-scaled FNAME SCALE` | add a control vector with user defined scaling SCALE<br/>note: this argument can be repeated to add multiple scaled control vectors |\n| `--control-vector-layer-range START END` | layer range to apply the control vector(s) to, start and end inclusive |\n| `-m, --model FNAME` | model path (default: `models/$filename` with filename from `--hf-file` or `--model-url` if set, otherwise models/7B/ggml-model-f16.gguf)<br/>(env: LLAMA_ARG_MODEL) |\n| `-mu, --model-url MODEL_URL` | model download url (default: unused)<br/>(env: LLAMA_ARG_MODEL_URL) |\n| `-hfr, --hf-repo REPO` | Hugging Face model repository (default: unused)<br/>(env: LLAMA_ARG_HF_REPO) |\n| `-hff, --hf-file FILE` | Hugging Face model file (default: unused)<br/>(env: LLAMA_ARG_HF_FILE) |\n| `-hft, --hf-token TOKEN` | Hugging Face access token (default: value from HF_TOKEN environment variable)<br/>(env: HF_TOKEN) |\n| `--log-disable` | Log disable |\n| `--log-file FNAME` | Log to file |\n| `--log-colors` | Enable colored logging<br/>(env: LLAMA_LOG_COLORS) |\n| `-v, --verbose, --log-verbose` | Set verbosity level to infinity (i.e. log all messages, useful for debugging) |\n| `-lv, --verbosity, --log-verbosity N` | Set the verbosity threshold. Messages with a higher verbosity will be ignored.<br/>(env: LLAMA_LOG_VERBOSITY) |\n| `--log-prefix` | Enable prefx in log messages<br/>(env: LLAMA_LOG_PREFIX) |\n| `--log-timestamps` | Enable timestamps in log messages<br/>(env: LLAMA_LOG_TIMESTAMPS) |\n```\n\n----------------------------------------\n\nTITLE: Measuring Perplexity Over a Text File\nDESCRIPTION: Demonstrates how to use llama-perplexity to evaluate a model's perplexity score for a given text file, which is a quality metric for language models.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/README.md#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nllama-perplexity -m model.gguf -f file.txt\n```\n\n----------------------------------------\n\nTITLE: Running the simple-cmake-pkg Example Application\nDESCRIPTION: Command to run the built example application with a specified model file and prompt text. This demonstrates the execution of a program built against the llama.cpp library.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/simple-cmake-pkg/README.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n./build/llama-simple-cmake-pkg -m ./models/llama-7b-v2/ggml-model-f16.gguf \"Hello my name is\"\n```\n\n----------------------------------------\n\nTITLE: Building llama.cpp with Static Libraries\nDESCRIPTION: Command to build llama.cpp with static libraries instead of shared libraries using CMake.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncmake -B build -DBUILD_SHARED_LIBS=OFF\ncmake --build build --config Release\n```\n\n----------------------------------------\n\nTITLE: Building llama.cpp with Vulkan on Linux\nDESCRIPTION: Commands to build llama.cpp with Vulkan support using CMake on Linux and testing the output binary.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\ncmake -B build -DGGML_VULKAN=1\ncmake --build build --config Release\n# Test the output binary (with \"-ngl 33\" to offload all layers to GPU)\n./bin/llama-cli -m \"PATH_TO_MODEL\" -p \"Hi you how are you\" -n 50 -e -ngl 33 -t 4\n\n# You should see in the output, ggml_vulkan detected your GPU. For example:\n# ggml_vulkan: Using Intel(R) Graphics (ADL GT2) | uma: 1 | fp16: 1 | warp size: 32\n```\n\n----------------------------------------\n\nTITLE: LoRA Adapter Commands\nDESCRIPTION: Commands for integrating LoRA adapters with optional scaling factors to customize model behavior.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_31\n\nLANGUAGE: bash\nCODE:\n```\n--lora my_adapter_1.gguf --lora my_adapter_2.gguf\n--lora-scaled lora_task_A.gguf 0.5 --lora-scaled lora_task_B.gguf 0.5\n```\n\n----------------------------------------\n\nTITLE: Example Workflow for Generating and Using Importance Matrix\nDESCRIPTION: A complete workflow showing how to generate an importance matrix and then use it for quantizing a model. The example demonstrates using GPU offloading for faster computation and applying the generated imatrix for Q4_K_M quantization.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/imatrix/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# generate importance matrix (imatrix.dat)\n./llama-imatrix -m ggml-model-f16.gguf -f train-data.txt -ngl 99\n\n# use the imatrix to perform a Q4_K_M quantization\n./llama-quantize --imatrix imatrix.dat ggml-model-f16.gguf ./ggml-model-q4_k_m.gguf q4_k_m\n```\n\n----------------------------------------\n\nTITLE: Running MobileVLM Model with llama-mtmd-cli\nDESCRIPTION: Command-line example for running a converted MobileVLM model using the llama-mtmd-cli tool with the deepseek chat template.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/MobileVLM.md#2025-04-22_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n./llama-mtmd-cli -m MobileVLM-1.7B/ggml-model-q4_k.gguf \\\n    --mmproj MobileVLM-1.7B/mmproj-model-f16.gguf \\\n    --chat-template deepseek\n```\n\n----------------------------------------\n\nTITLE: Running Llama Infill with Code Llama Model for Code Completion\nDESCRIPTION: Example command for running the llama-infill program with a CodeLlama model to complete code between a prefix and suffix. This demonstrates providing code context before and after the cursor position where completion should happen.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/infill/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./llama-infill -t 10 -ngl 0 -m models/codellama-13b.Q5_K_S.gguf -c 4096 --temp 0.7 --repeat_penalty 1.1 -n 20 --in-prefix \"def helloworld():\\n    print(\\\"hell\" --in-suffix \"\\n   print(\\\"goodbye world\\\")\\n    \"\n```\n\n----------------------------------------\n\nTITLE: Setting Context Size in LLaMA.cpp\nDESCRIPTION: Sets the size of the prompt context window for LLaMA models. The default is 4096 tokens, and setting to 0 loads the size from the model.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\n-c N, --ctx-size N\n```\n\n----------------------------------------\n\nTITLE: Converting Gemma 3 Models with MMPROJ Support\nDESCRIPTION: Commands for converting Hugging Face models to GGUF format with MMPROJ support for vision capabilities.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/gemma3.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd gemma-3-4b-it\npython ../llama.cpp/convert_hf_to_gguf.py --outfile model.gguf --outtype f16 --mmproj .\n# output file: mmproj-model.gguf\n```\n\n----------------------------------------\n\nTITLE: Building llama.cpp with OpenBLAS\nDESCRIPTION: Command to build llama.cpp with OpenBLAS acceleration using CMake. This can provide performance improvements for certain operations.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncmake -B build -DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS\ncmake --build build --config Release\n```\n\n----------------------------------------\n\nTITLE: Configuring RoPE Scaling for Extended Context in LLaMA.cpp\nDESCRIPTION: Sets the linear scaling factor for RoPE (Rotary Position Embedding) when using models fine-tuned for extended context lengths.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\n--rope-scale N\n```\n\n----------------------------------------\n\nTITLE: Running Automated Benchmark with CI Python Script\nDESCRIPTION: Example command for running the bench.py script which automates server startup, benchmark execution, and metrics collection. Includes various parameters for customizing the benchmark configuration.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/bench/README.md#2025-04-22_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nLLAMA_SERVER_BIN_PATH=../../../cmake-build-release/bin/llama-server python bench.py \\\n              --runner-label local \\\n              --name local \\\n              --branch `git rev-parse --abbrev-ref HEAD` \\\n              --commit `git rev-parse HEAD` \\\n              --scenario script.js \\\n              --duration 5m \\\n              --hf-repo ggml-org/models\t \\\n              --hf-file phi-2/ggml-model-q4_0.gguf \\\n              --model-path-prefix models \\\n              --parallel 4 \\\n              -ngl 33 \\\n              --batch-size 2048 \\\n              --ubatch-size\t256 \\\n              --ctx-size 4096 \\\n              --n-prompts 200 \\\n              --max-prompt-tokens 256 \\\n              --max-tokens 256\n```\n\n----------------------------------------\n\nTITLE: Token Probability Response Format in JSON\nDESCRIPTION: JSON structure showing token probability information including logprob/prob values, token IDs, and byte representations. Used for both standard logprob and post-sampling probability outputs.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md#2025-04-22_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"logprob\": \"float\",\n  \"token\": \"<most likely token>\",\n  \"bytes\": [\"int\", \"int\", \"...\"],\n  \"top_logprobs\": [\n    {\n      \"id\": \"<token id>\",\n      \"logprob\": \"float\",\n      \"token\": \"<token text>\",\n      \"bytes\": [\"int\", \"int\", \"...\"]\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Document Reranking Request Example\nDESCRIPTION: Example curl command demonstrating how to use the reranking endpoint to rank documents based on relevance to a query.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md#2025-04-22_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://127.0.0.1:8012/v1/rerank \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"some-model\",\n            \"query\": \"What is panda?\",\n            \"top_n\": 3,\n            \"documents\": [\n                \"hi\",\n            \"it is a bear\",\n            \"The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.\"\n            ]\n    }' | jq\n```\n\n----------------------------------------\n\nTITLE: Building llama.cpp with SYCL for NVIDIA GPU\nDESCRIPTION: CMake commands to build llama.cpp with SYCL support for NVIDIA GPUs, including options for FP32 and FP16.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\nGGML_SYCL_DEVICE_ARCH=sm_80 # Example architecture\n\n# Option 1: Use FP32 (recommended for better performance in most cases)\ncmake -B build -DGGML_SYCL=ON -DGGML_SYCL_TARGET=NVIDIA -DGGML_SYCL_DEVICE_ARCH=${GGML_SYCL_DEVICE_ARCH} -DCMAKE_C_COMPILER=icx -DCMAKE_CXX_COMPILER=icpx -DDNNL_DIR=/path/to/oneDNN/build-nvidia/install/lib/cmake/dnnl\n\n# Option 2: Use FP16\ncmake -B build -DGGML_SYCL=ON -DGGML_SYCL_TARGET=NVIDIA -DGGML_SYCL_DEVICE_ARCH=${GGML_SYCL_DEVICE_ARCH} -DCMAKE_C_COMPILER=icx -DCMAKE_CXX_COMPILER=icpx -DGGML_SYCL_F16=ON -DDNNL_DIR=/path/to/oneDNN/build-nvidia/install/lib/cmake/dnnl\n\n# build all binary\ncmake --build build --config Release -j -v\n```\n\n----------------------------------------\n\nTITLE: Enabling Link-Time Optimization\nDESCRIPTION: Checks if Link-Time Optimization (LTO) is supported by the compiler and enables it when GGML_LTO is set. LTO improves performance by allowing optimizations across compilation units.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/CMakeLists.txt#2025-04-22_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nif (GGML_LTO)\n    include(CheckIPOSupported)\n    check_ipo_supported(RESULT result OUTPUT output)\n    if (result)\n        set(CMAKE_INTERPROCEDURAL_OPTIMIZATION TRUE)\n    else()\n        message(WARNING \"IPO is not supported: ${output}\")\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Building and Running Pre-quantized Gemma 3 Models\nDESCRIPTION: Instructions for building llama.cpp and running pre-quantized Gemma 3 vision models from ggml-org's Hugging Face repository.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/gemma3.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# build\ncmake -B build\ncmake --build build --target llama-gemma3-cli\n\n# alternatively, install from brew (MacOS)\nbrew install llama.cpp\n\n# run it\nllama-gemma3-cli -hf ggml-org/gemma-3-4b-it-GGUF\nllama-gemma3-cli -hf ggml-org/gemma-3-12b-it-GGUF\nllama-gemma3-cli -hf ggml-org/gemma-3-27b-it-GGUF\n\n# note: 1B model does not support vision\n```\n\n----------------------------------------\n\nTITLE: Setting Token Prediction Count in LLaMA.cpp\nDESCRIPTION: Configures how many tokens the model should generate. Default is -1 (infinity), -2 stops when context is filled.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\n-n N, --predict N\n```\n\n----------------------------------------\n\nTITLE: Using llava_surgery_v2.py for LLaVA 1.6\nDESCRIPTION: Python command to use the llava_surgery_v2.py script for processing LLaVA 1.6 models, which supports both PyTorch and safetensor formats.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/llava.md#2025-04-22_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\npython examples/llava/llava_surgery_v2.py -C -m ../llava-v1.6-vicuna-7b/\n```\n\n----------------------------------------\n\nTITLE: Building llama.cpp with CMake (CPU)\nDESCRIPTION: Commands to build llama.cpp using CMake for a CPU-only configuration. This is the basic build process without any specific accelerations.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncmake -B build\ncmake --build build --config Release\n```\n\n----------------------------------------\n\nTITLE: Referencing Function Calling Implementation in chat.h\nDESCRIPTION: The chat.h file in the common directory adds support for OpenAI-style function calling as implemented in PR #9639. This functionality is used in llama-server with the --jinja flag and is being worked on for llama-cli in PR #11556.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/function-calling.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n[chat.h](../common/chat.h) (https://github.com/ggml-org/llama.cpp/pull/9639) adds support for [OpenAI-style function calling](https://platform.openai.com/docs/guides/function-calling) and is used in:\n- `llama-server` when started w/ `--jinja` flag\n- `llama-cli` (WIP: https://github.com/ggml-org/llama.cpp/pull/11556)\n```\n\n----------------------------------------\n\nTITLE: Running MobileVLM on Android (Llama Image Example)\nDESCRIPTION: Example command for running the MobileVLM model on a Snapdragon 778G device to identify animals in an image. This example demonstrates the model's performance on mid-range Android hardware.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/MobileVLM.md#2025-04-22_snippet_10\n\nLANGUAGE: sh\nCODE:\n```\n/data/local/tmp/llama-mtmd-cli \\\n    -m /data/local/tmp/ggml-model-q4_k.gguf \\\n    --mmproj /data/local/tmp/mmproj-model-f16.gguf \\\n    -t 4 \\\n    --image /data/local/tmp/many_llamas.jpeg \\\n    -p \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: <image>\\nWhat's that? ASSISTANT:\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Repetition Penalty in LLaMA.cpp\nDESCRIPTION: Controls how strongly the model penalizes repetition of token sequences. Default is 1.0 (disabled), and higher values (e.g., 1.5) more strongly discourage repetition.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\n--repeat-penalty N\n```\n\n----------------------------------------\n\nTITLE: Querying llama.cpp Server Metrics Endpoint\nDESCRIPTION: Command to retrieve server metrics from the metrics endpoint, which can be compared against k6 metrics for performance analysis.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/bench/README.md#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\ncurl http://localhost:8080/metrics\n```\n\n----------------------------------------\n\nTITLE: Converting MiniCPM-V 2.6 PyTorch Model to GGUF Format\nDESCRIPTION: Series of commands to convert the MiniCPM-V 2.6 PyTorch model to GGUF format for use with llama.cpp. Includes creating the projector model, converting the image encoder, and quantizing to int4 for better performance.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/minicpmv2.6.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython ./examples/llava/minicpmv-surgery.py -m ../MiniCPM-V-2_6\npython ./examples/llava/minicpmv-convert-image-encoder-to-gguf.py -m ../MiniCPM-V-2_6 --minicpmv-projector ../MiniCPM-V-2_6/minicpmv.projector --output-dir ../MiniCPM-V-2_6/ --image-mean 0.5 0.5 0.5 --image-std 0.5 0.5 0.5 --minicpmv_version 3\npython ./convert_hf_to_gguf.py ../MiniCPM-V-2_6/model\n\n# quantize int4 version\n./build/bin/llama-quantize ../MiniCPM-V-2_6/model/ggml-model-f16.gguf ../MiniCPM-V-2_6/model/ggml-model-Q4_K_M.gguf Q4_K_M\n```\n\n----------------------------------------\n\nTITLE: Running MiniCPM-V 2.6 Inference with llama.cpp\nDESCRIPTION: Commands for running inference with the converted MiniCPM-V 2.6 model on Linux or Mac. Shows how to run in both single-turn mode for individual image analysis and conversation mode for interactive use.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/minicpmv2.6.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# run in single-turn mode\n./build/bin/llama-mtmd-cli -m ../MiniCPM-V-2_6/model/ggml-model-f16.gguf --mmproj ../MiniCPM-V-2_6/mmproj-model-f16.gguf -c 4096 --temp 0.7 --top-p 0.8 --top-k 100 --repeat-penalty 1.05 --image xx.jpg -p \"What is in the image?\"\n\n# run in conversation mode\n./build/bin/llama-mtmd-cli -m ../MiniCPM-V-2_6/model/ggml-model-Q4_K_M.gguf --mmproj ../MiniCPM-V-2_6/mmproj-model-f16.gguf\n```\n\n----------------------------------------\n\nTITLE: Building llama.cpp with LLGuidance Support\nDESCRIPTION: Commands to build llama.cpp with LLGuidance support enabled. Requires Rust compiler and cargo tool to be installed. Includes commands for both Unix-like systems and Windows.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/llguidance.md#2025-04-22_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ncmake -B build -DLLAMA_LLGUIDANCE=ON\nmake -C build -j\n```\n\n----------------------------------------\n\nTITLE: Building llama.cpp with CANN Support\nDESCRIPTION: CMake commands to build llama.cpp with CANN backend support\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/CANN.md#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\ncmake -B build -DGGML_CANN=on -DCMAKE_BUILD_TYPE=release\ncmake --build build --config release\n```\n\n----------------------------------------\n\nTITLE: Building CUDA Backend with RPC Support\nDESCRIPTION: Commands to build the CUDA backend with RPC support using CMake. This enables the use of CUDA GPUs for distributed inference.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/rpc/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmkdir build-rpc-cuda\ncd build-rpc-cuda\ncmake .. -DGGML_CUDA=ON -DGGML_RPC=ON\ncmake --build . --config Release\n```\n\n----------------------------------------\n\nTITLE: Server Properties Response Format\nDESCRIPTION: JSON structure returned by the GET /props endpoint showing server's global properties including default generation settings, total slots, model path and build info.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md#2025-04-22_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"default_generation_settings\": {\n    \"id\": 0,\n    \"id_task\": -1,\n    \"n_ctx\": 1024,\n    \"speculative\": false,\n    \"is_processing\": false,\n    \"params\": {\n      \"n_predict\": -1,\n      \"seed\": 4294967295,\n      \"temperature\": 0.800000011920929,\n      \"dynatemp_range\": 0.0,\n      \"dynatemp_exponent\": 1.0,\n      \"top_k\": 40,\n      \"top_p\": 0.949999988079071,\n      \"min_p\": 0.05000000074505806,\n      \"xtc_probability\": 0.0,\n      \"xtc_threshold\": 0.10000000149011612,\n      \"typical_p\": 1.0,\n      \"repeat_last_n\": 64,\n      \"repeat_penalty\": 1.0,\n      \"presence_penalty\": 0.0,\n      \"frequency_penalty\": 0.0,\n      \"dry_multiplier\": 0.0,\n      \"dry_base\": 1.75,\n      \"dry_allowed_length\": 2,\n      \"dry_penalty_last_n\": -1,\n      \"dry_sequence_breakers\": [\n        \"\\n\",\n        \":\",\n        \"\\\"\",\n        \"*\"\n      ],\n      \"mirostat\": 0,\n      \"mirostat_tau\": 5.0,\n      \"mirostat_eta\": 0.10000000149011612,\n      \"stop\": [],\n      \"max_tokens\": -1,\n      \"n_keep\": 0,\n      \"n_discard\": 0,\n      \"ignore_eos\": false,\n      \"stream\": true,\n      \"n_probs\": 0,\n      \"min_keep\": 0,\n      \"grammar\": \"\",\n      \"samplers\": [\n        \"dry\",\n        \"top_k\",\n        \"typ_p\",\n        \"top_p\",\n        \"min_p\",\n        \"xtc\",\n        \"temperature\"\n      ],\n      \"speculative.n_max\": 16,\n      \"speculative.n_min\": 5,\n      \"speculative.p_min\": 0.8999999761581421,\n      \"timings_per_token\": false\n    },\n    \"prompt\": \"\",\n    \"next_token\": {\n      \"has_next_token\": true,\n      \"has_new_line\": false,\n      \"n_remain\": -1,\n      \"n_decoded\": 0,\n      \"stopping_word\": \"\"\n    }\n  },\n  \"total_slots\": 1,\n  \"model_path\": \"../models/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf\",\n  \"chat_template\": \"...\",\n  \"build_info\": \"b(build number)-(build commit hash)\"\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Repetition Penalty Window in LLaMA.cpp\nDESCRIPTION: Determines how many previous tokens to consider when applying repetition penalty. Default is 64, 0 disables the feature, and -1 sets it to the context size.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\n--repeat-last-n N\n```\n\n----------------------------------------\n\nTITLE: Building llama.cpp with SYCL for Intel GPU\nDESCRIPTION: CMake commands to build llama.cpp with SYCL support for Intel GPUs, including options for FP32 and FP16.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\nsource /opt/intel/oneapi/setvars.sh\n\n# Option 1: Use FP32 (recommended for better performance in most cases)\ncmake -B build -DGGML_SYCL=ON -DCMAKE_C_COMPILER=icx -DCMAKE_CXX_COMPILER=icpx\n\n# Option 2: Use FP16\ncmake -B build -DGGML_SYCL=ON -DCMAKE_C_COMPILER=icx -DCMAKE_CXX_COMPILER=icpx -DGGML_SYCL_F16=ON\n\n# build all binary\ncmake --build build --config Release -j -v\n```\n\n----------------------------------------\n\nTITLE: Starting llama.cpp Server for Benchmarking\nDESCRIPTION: Example command to start the llama-server with recommended settings for benchmarking. Configures continuous batching, metrics collection, and other performance parameters.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/bench/README.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nllama-server --host localhost --port 8080 \\\n  --model ggml-model-q4_0.gguf \\\n  --cont-batching \\\n  --metrics \\\n  --parallel 8 \\\n  --batch-size 512 \\\n  --ctx-size 4096 \\\n  -ngl 33\n```\n\n----------------------------------------\n\nTITLE: Using perplexity with KL divergence calculation in llama.cpp\nDESCRIPTION: Commands for using the perplexity tool to calculate KL divergence between a full precision and quantized model by first recording logits from the FP16 model and then comparing them to the quantized model.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/perplexity/README.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n--kl-divergence-base path/to/logit/binary/file.kld\n```\n\n----------------------------------------\n\nTITLE: Configuring Top-P (Nucleus) Sampling in LLaMA.cpp\nDESCRIPTION: Limits token selection to a subset with cumulative probability above threshold P. Default is 0.9, with higher values enabling more diverse generation.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\n--top-p N\n```\n\nLANGUAGE: bash\nCODE:\n```\n--top-p 0.95\n```\n\n----------------------------------------\n\nTITLE: Cloning and Building llama.cpp with CMake\nDESCRIPTION: Instructions for cloning the llama.cpp repository from GitHub and building it using CMake. This is a prerequisite step before working with the MiniCPM-V 2.6 model.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/minicpmv2.6.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/ggerganov/llama.cpp\ncd llama.cpp\n```\n\nLANGUAGE: bash\nCODE:\n```\ncmake -B build\ncmake --build build --config Release\n```\n\n----------------------------------------\n\nTITLE: Running llama-bench with JSON output format\nDESCRIPTION: Command to run the llama-bench tool with JSON output format. This produces structured benchmark results as a JSON array, suitable for programmatic processing.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llama-bench/README.md#2025-04-22_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\n$ ./llama-bench -o json\n```\n\n----------------------------------------\n\nTITLE: Batch Size Performance Testing Command\nDESCRIPTION: Example command for testing prompt processing performance with different batch sizes.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llama-bench/README.md#2025-04-22_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n./llama-bench -n 0 -p 1024 -b 128,256,512,1024\n```\n\n----------------------------------------\n\nTITLE: HIP GPU Compilation for Linux\nDESCRIPTION: CMake compilation commands for AMD GPU support using HIP on Linux, including ROCm configuration and optional rocWMMA optimization.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nHIPCXX=\"$(hipconfig -l)/clang\" HIP_PATH=\"$(hipconfig -R)\" \\\n    cmake -S . -B build -DGGML_HIP=ON -DAMDGPU_TARGETS=gfx1030 -DCMAKE_BUILD_TYPE=Release \\\n    && cmake --build build --config Release -- -j 16\n```\n\n----------------------------------------\n\nTITLE: Configuring GritLM Executable Target in CMake\nDESCRIPTION: Defines a CMake target named llama-gritlm that builds an executable from gritlm.cpp. The configuration specifies installation rules, links necessary libraries (common, llama, and thread libraries), and requires C++17 standard support.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/gritlm/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET llama-gritlm)\nadd_executable(${TARGET} gritlm.cpp)\ninstall(TARGETS ${TARGET} RUNTIME)\ntarget_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})\ntarget_compile_features(${TARGET} PRIVATE cxx_std_17)\n```\n\n----------------------------------------\n\nTITLE: Cross-compiling llama.cpp with Android NDK\nDESCRIPTION: CMake command for configuring a cross-compilation build of llama.cpp for Android using the Android NDK, with specific flags for arm64-v8a architecture and other optimizations.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/android.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ cmake \\\n  -DCMAKE_TOOLCHAIN_FILE=$ANDROID_NDK/build/cmake/android.toolchain.cmake \\\n  -DANDROID_ABI=arm64-v8a \\\n  -DANDROID_PLATFORM=android-28 \\\n  -DCMAKE_C_FLAGS=\"-march=armv8.7a\" \\\n  -DCMAKE_CXX_FLAGS=\"-march=armv8.7a\" \\\n  -DGGML_OPENMP=OFF \\\n  -DGGML_LLAMAFILE=OFF \\\n  -B build-android\n```\n\n----------------------------------------\n\nTITLE: Running One-Shot Prompt Generation in Unix\nDESCRIPTION: Command to execute a single prompt inference using the Gemma model in Unix-based systems\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./llama-cli -m models/gemma-1.1-7b-it.Q4_K_M.gguf -no-cnv --prompt \"Once upon a time\"\n```\n\n----------------------------------------\n\nTITLE: Installing Vulkan SDK on Ubuntu\nDESCRIPTION: Commands to install the Vulkan SDK on Ubuntu 22.04 (jammy) for building llama.cpp with Vulkan support.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\nwget -qO - https://packages.lunarg.com/lunarg-signing-key-pub.asc | apt-key add -\nwget -qO /etc/apt/sources.list.d/lunarg-vulkan-jammy.list https://packages.lunarg.com/vulkan/lunarg-vulkan-jammy.list\napt update -y\napt-get install -y vulkan-sdk\n# To verify the installation, use the command below:\nvulkaninfo\n```\n\n----------------------------------------\n\nTITLE: Using Locally Typical Sampling in LLaMA.cpp\nDESCRIPTION: Command line example for enabling locally typical sampling with a parameter value of 0.9. This promotes the generation of contextually coherent and diverse text by sampling tokens that are typical based on context.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_23\n\nLANGUAGE: shell\nCODE:\n```\n--typical 0.9\n```\n\n----------------------------------------\n\nTITLE: Running Simple Chat Example with llama.cpp\nDESCRIPTION: This command demonstrates how to run the simple chat example using llama.cpp. It specifies the model file to use and sets the context size. The ellipsis indicates that more output would follow when executing this command.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/simple-chat/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./llama-simple-chat -m Meta-Llama-3.1-8B-Instruct.gguf -c 2048\n...\n```\n\n----------------------------------------\n\nTITLE: Running Node.js Client\nDESCRIPTION: Command to execute the Node.js client script\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nnode index.js\n```\n\n----------------------------------------\n\nTITLE: Example GBNF Grammar Generated from JSON Schema\nDESCRIPTION: A sample GBNF grammar produced from converting the name-age JSON schema. The grammar defines rules for generating valid JSON that conforms to the original schema constraints.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/grammars/README.md#2025-04-22_snippet_7\n\nLANGUAGE: plaintext\nCODE:\n```\nchar ::= [^\"\\\\\\x7F\\x00-\\x1F] | [\\\\] ([\"\\\\bfnrt] | \"u\" [0-9a-fA-F]{4})\nitem ::= \"{\" space item-name-kv \",\" space item-age-kv \"}\" space\nitem-age ::= ([0-9] | ([1-8] [0-9] | [9] [0-9]) | \"1\" ([0-4] [0-9] | [5] \"0\")) space\nitem-age-kv ::= \"\\\"age\\\"\" space \":\" space item-age\nitem-name ::= \"\\\"\" char{1,100} \"\\\"\" space\nitem-name-kv ::= \"\\\"name\\\"\" space \":\" space item-name\nroot ::= \"[\" space item (\",\" space item){9,99} \"]\" space\nspace ::= | \" \" | \"\\n\" [ \\t]{0,20}\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies\nDESCRIPTION: Specifies the required Python packages and their version constraints. Includes numpy for numerical computing, sentencepiece for tokenization, transformers for machine learning models, gguf for model format support, and protobuf for data serialization.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/requirements/requirements-convert_legacy_llama.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nnumpy~=1.26.4\nsentencepiece~=0.2.0\ntransformers>=4.45.1,<5.0.0\ngguf>=0.1.0\nprotobuf>=4.21.0,<5.0.0\n```\n\n----------------------------------------\n\nTITLE: Executing llama.cpp with SYCL on Linux (Multiple Devices)\nDESCRIPTION: Shell command to run llama.cpp inference using multiple SYCL devices on Linux.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_12\n\nLANGUAGE: sh\nCODE:\n```\n./examples/sycl/run-llama2.sh\n```\n\n----------------------------------------\n\nTITLE: Running Inference with MiniCPM-o 2.6 on Linux or Mac\nDESCRIPTION: Commands to run inference with the MiniCPM-o 2.6 model in both single-turn and conversation modes. The single-turn mode processes a specific image with a prompt, while conversation mode enables interactive dialogue.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/minicpmo2.6.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# run in single-turn mode\n./build/bin/llama-mtmd-cli -m ../MiniCPM-o-2_6/model/ggml-model-f16.gguf --mmproj ../MiniCPM-o-2_6/mmproj-model-f16.gguf -c 4096 --temp 0.7 --top-p 0.8 --top-k 100 --repeat-penalty 1.05 --image xx.jpg -p \"What is in the image?\"\n\n# run in conversation mode\n./build/bin/llama-mtmd-cli -m ../MiniCPM-o-2_6/model/ggml-model-Q4_K_M.gguf --mmproj ../MiniCPM-o-2_6/mmproj-model-f16.gguf\n```\n\n----------------------------------------\n\nTITLE: Building llama.cpp with OpenCL for Android\nDESCRIPTION: Commands to clone and build llama.cpp with OpenCL support for Android, using CMake and Ninja to configure the build for ARM64 architecture.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/OPENCL.md#2025-04-22_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\ncd ~/dev/llm\n\ngit clone https://github.com/ggml-org/llama.cpp && \\\ncd llama.cpp && \\\nmkdir build-android && cd build-android\n\ncmake .. -G Ninja \\\n  -DCMAKE_TOOLCHAIN_FILE=$HOME/android-sdk/ndk/26.3.11579264/build/cmake/android.toolchain.cmake \\\n  -DANDROID_ABI=arm64-v8a \\\n  -DANDROID_PLATFORM=android-28 \\\n  -DBUILD_SHARED_LIBS=OFF \\\n  -DGGML_OPENCL=ON\n\nninja\n```\n\n----------------------------------------\n\nTITLE: Configuring build targets and linking dependencies for llama-server\nDESCRIPTION: Sets up the executable build target, installation rules, and links required libraries including threading support. Conditionally includes OpenSSL for SSL support and adds Windows-specific socket libraries when building on Windows platforms.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nadd_executable(${TARGET} ${TARGET_SRCS})\ninstall(TARGETS ${TARGET} RUNTIME)\n\ntarget_include_directories(${TARGET} PRIVATE ${CMAKE_SOURCE_DIR})\ntarget_link_libraries(${TARGET} PRIVATE common ${CMAKE_THREAD_LIBS_INIT})\n\nif (LLAMA_SERVER_SSL)\n    find_package(OpenSSL REQUIRED)\n    target_link_libraries(${TARGET} PRIVATE OpenSSL::SSL OpenSSL::Crypto)\n    target_compile_definitions(${TARGET} PRIVATE CPPHTTPLIB_OPENSSL_SUPPORT)\nendif()\n\nif (WIN32)\n    TARGET_LINK_LIBRARIES(${TARGET} PRIVATE ws2_32)\nendif()\n\ntarget_compile_features(${TARGET} PRIVATE cxx_std_17)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Bash Completion for llama-cli\nDESCRIPTION: Shows how to generate and enable Bash command-line completion for the llama-cli tool. Includes the command to generate completion script and how to load it in the current session or permanently in the .bashrc file.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/README.md#2025-04-22_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\n$ build/bin/llama-cli --completion-bash > ~/.llama-completion.bash\n$ source ~/.llama-completion.bash\n```\n\nLANGUAGE: bash\nCODE:\n```\n$ echo \"source ~/.llama-completion.bash\" >> ~/.bashrc\n```\n\n----------------------------------------\n\nTITLE: Running One-Shot Prompt Generation in Windows\nDESCRIPTION: Command to execute a single prompt inference using the Gemma model in Windows\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_5\n\nLANGUAGE: powershell\nCODE:\n```\n./llama-cli.exe -m models\\gemma-1.1-7b-it.Q4_K_M.gguf -no-cnv --prompt \"Once upon a time\"\n```\n\n----------------------------------------\n\nTITLE: Configuring GGML Backend Library for HIP in CMake\nDESCRIPTION: Sets up the GGML backend library for HIP, including source files, compile definitions, and linking libraries. It handles different configurations based on CUDA and HIP-specific options.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-hip/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB   GGML_HEADERS_ROCM \"../ggml-cuda/*.cuh\")\nlist(APPEND GGML_HEADERS_ROCM \"../../include/ggml-cuda.h\")\n\nfile(GLOB   GGML_SOURCES_ROCM \"../ggml-cuda/*.cu\")\nfile(GLOB   SRCS \"../ggml-cuda/template-instances/fattn-mma*.cu\")\nlist(APPEND GGML_SOURCES_ROCM ${SRCS})\nfile(GLOB   SRCS \"../ggml-cuda/template-instances/mmq*.cu\")\nlist(APPEND GGML_SOURCES_ROCM ${SRCS})\n\nif (GGML_CUDA_FA_ALL_QUANTS)\n    file(GLOB   SRCS \"../ggml-cuda/template-instances/fattn-vec*.cu\")\n    list(APPEND GGML_SOURCES_ROCM ${SRCS})\n    add_compile_definitions(GGML_CUDA_FA_ALL_QUANTS)\nelse()\n    file(GLOB   SRCS \"../ggml-cuda/template-instances/fattn-vec*q4_0-q4_0.cu\")\n    list(APPEND GGML_SOURCES_ROCM ${SRCS})\n    file(GLOB   SRCS \"../ggml-cuda/template-instances/fattn-vec*q8_0-q8_0.cu\")\n    list(APPEND GGML_SOURCES_ROCM ${SRCS})\n    file(GLOB   SRCS \"../ggml-cuda/template-instances/fattn-vec*f16-f16.cu\")\n    list(APPEND GGML_SOURCES_ROCM ${SRCS})\nendif()\n\nggml_add_backend_library(ggml-hip\n                         ${GGML_HEADERS_ROCM}\n                         ${GGML_SOURCES_ROCM}\n                        )\n\n# TODO: do not use CUDA definitions for HIP\nif (NOT GGML_BACKEND_DL)\n    target_compile_definitions(ggml PUBLIC GGML_USE_CUDA)\nendif()\n\nadd_compile_definitions(GGML_USE_HIP)\n\nif (GGML_CUDA_FORCE_MMQ)\n    add_compile_definitions(GGML_CUDA_FORCE_MMQ)\nendif()\n\nif (GGML_CUDA_FORCE_CUBLAS)\n    add_compile_definitions(GGML_CUDA_FORCE_CUBLAS)\nendif()\n\nif (GGML_CUDA_NO_PEER_COPY)\n    add_compile_definitions(GGML_CUDA_NO_PEER_COPY)\nendif()\n\nif (GGML_HIP_GRAPHS)\n    add_compile_definitions(GGML_HIP_GRAPHS)\nendif()\n\nif (GGML_HIP_NO_VMM)\n    add_compile_definitions(GGML_HIP_NO_VMM)\nendif()\n\nif (GGML_HIP_ROCWMMA_FATTN)\n    add_compile_definitions(GGML_HIP_ROCWMMA_FATTN)\nendif()\n\nif (NOT GGML_CUDA_FA)\n    add_compile_definitions(GGML_CUDA_NO_FA)\nendif()\n\nif (CXX_IS_HIPCC)\n    set_source_files_properties(${GGML_SOURCES_ROCM} PROPERTIES LANGUAGE CXX)\n    target_link_libraries(ggml-hip PRIVATE hip::device)\nelse()\n    set_source_files_properties(${GGML_SOURCES_ROCM} PROPERTIES LANGUAGE HIP)\nendif()\n\nif (GGML_STATIC)\n    message(FATAL_ERROR \"Static linking not supported for HIP/ROCm\")\nendif()\n\ntarget_link_libraries(ggml-hip PRIVATE ggml-base hip::host roc::rocblas roc::hipblas)\n```\n\n----------------------------------------\n\nTITLE: Converting LLaVA 1.6 Model to GGUF\nDESCRIPTION: Python command to convert the LLaVA 1.6 model to GGUF format using the convert_legacy_llama.py script.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/llava.md#2025-04-22_snippet_10\n\nLANGUAGE: sh\nCODE:\n```\npython ./examples/convert_legacy_llama.py ../llava-v1.6-vicuna-7b/ --skip-unknown\n```\n\n----------------------------------------\n\nTITLE: Running SimpleChat with Python HTTP Server\nDESCRIPTION: Commands to start llama-server and then serve the SimpleChat frontend using Python's HTTP server\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/public_simplechat/readme.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n./llama-server -m path/model.gguf\ncd ../examples/server/public_simplechat\npython3 -m http.server PORT\n```\n\n----------------------------------------\n\nTITLE: Starting RPC Server with Local Cache\nDESCRIPTION: Command to start the RPC server with local caching enabled. This can significantly speed up model loading for large models.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/rpc/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ bin/rpc-server -c\n```\n\n----------------------------------------\n\nTITLE: Vicuna 1.1 Template Example\nDESCRIPTION: A template for the Vicuna 1.1 chat format, which uses a specific structure for conversation history with user and assistant labels.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llama.android/README.md#2025-04-22_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\n{% if system_prompt %}\n{{ system_prompt }}\n{% endif %}\n\n{% for message in messages %}\n{% if message['role'] == 'user' %}\n{{ message['role'].upper() }}: {{ message['content'] }}\n\n{% elif message['role'] == 'assistant' %}\n{{ message['role'].upper() }}: {{ message['content'] }}\n\n{% endif %}\n{% endfor %}\n\nASSISTANT:\n```\n\n----------------------------------------\n\nTITLE: GPU Offloading Diagnostic Output\nDESCRIPTION: Example diagnostic output showing successful GPU offloading with cuBLAS, indicating the number of layers moved to GPU and total VRAM usage.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/development/token_generation_performance_tips.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nllama_model_load_internal: [cublas] offloading 60 layers to GPU\nllama_model_load_internal: [cublas] offloading output layer to GPU\nllama_model_load_internal: [cublas] total VRAM used: 17223 MB\n... rest of inference\n```\n\n----------------------------------------\n\nTITLE: Preparing CLIP Model Files for LLaVA 1.6\nDESCRIPTION: Shell commands to prepare the CLIP model files for LLaVA 1.6, including creating directories, copying files, and downloading configuration.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/llava.md#2025-04-22_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\nmkdir vit\ncp ../llava-v1.6-vicuna-7b/llava.clip vit/pytorch_model.bin\ncp ../llava-v1.6-vicuna-7b/llava.projector vit/\ncurl -s -q https://huggingface.co/cmp-nct/llava-1.6-gguf/raw/main/config_vit.json -o vit/config.json\n```\n\n----------------------------------------\n\nTITLE: Basic Chat Template Example\nDESCRIPTION: A simple example of a chat template for llama.cpp showing the syntax with variables like {{system}}, {{#each messages}}, etc. This demonstrates how to format conversations between users and assistants.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llama.android/README.md#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n{% for message in messages %}\n{% if message['role'] == 'system' %}\n{{ message['content'] }}\n{% else %}\n{{ message['role'] }}: {{ message['content'] }}\n{% endif %}\n{% endfor %}\n{{ roles['assistant'] }}:\n```\n\n----------------------------------------\n\nTITLE: Displaying Sampling Parameters Table in Markdown\nDESCRIPTION: A markdown table listing sampling-related command-line parameters for the llama.cpp project, including their descriptions and default values.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Argument | Explanation |\n| -------- | ----------- |\n| `--samplers SAMPLERS` | samplers that will be used for generation in the order, separated by ';'<br/>(default: dry;top_k;typ_p;top_p;min_p;xtc;temperature) |\n| `-s, --seed SEED` | RNG seed (default: -1, use random seed for -1) |\n| `--sampling-seq SEQUENCE` | simplified sequence for samplers that will be used (default: dkypmxt) |\n| `--ignore-eos` | ignore end of stream token and continue generating (implies --logit-bias EOS-inf) |\n| `--temp N` | temperature (default: 0.8) |\n| `--top-k N` | top-k sampling (default: 40, 0 = disabled) |\n| `--top-p N` | top-p sampling (default: 0.9, 1.0 = disabled) |\n| `--min-p N` | min-p sampling (default: 0.1, 0.0 = disabled) |\n| `--xtc-probability N` | xtc probability (default: 0.0, 0.0 = disabled) |\n| `--xtc-threshold N` | xtc threshold (default: 0.1, 1.0 = disabled) |\n| `--typical N` | locally typical sampling, parameter p (default: 1.0, 1.0 = disabled) |\n| `--repeat-last-n N` | last n tokens to consider for penalize (default: 64, 0 = disabled, -1 = ctx_size) |\n| `--repeat-penalty N` | penalize repeat sequence of tokens (default: 1.0, 1.0 = disabled) |\n| `--presence-penalty N` | repeat alpha presence penalty (default: 0.0, 0.0 = disabled) |\n| `--frequency-penalty N` | repeat alpha frequency penalty (default: 0.0, 0.0 = disabled) |\n| `--dry-multiplier N` | set DRY sampling multiplier (default: 0.0, 0.0 = disabled) |\n| `--dry-base N` | set DRY sampling base value (default: 1.75) |\n| `--dry-allowed-length N` | set allowed length for DRY sampling (default: 2) |\n| `--dry-penalty-last-n N` | set DRY penalty for the last n tokens (default: -1, 0 = disable, -1 = context size) |\n```\n\n----------------------------------------\n\nTITLE: Running LLaMA Multi-Modal CLI for Image Description\nDESCRIPTION: Command example for generating a natural language description of an image using llama-mtmd-cli with GPU acceleration.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/MobileVLM.md#2025-04-22_snippet_13\n\nLANGUAGE: sh\nCODE:\n```\n./llama-mtmd-cli \\\n    -m /data/local/tmp/ggml-model-q4_k.gguf \\\n    --mmproj /data/local/tmp/mmproj-model-f16.gguf \\\n    -p \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: <image>\\nWhat is in the image? ASSISTANT:\" \\\n    --n-gpu-layers 999\n```\n\n----------------------------------------\n\nTITLE: Configuring and Building llama-infill Executable with CMake\nDESCRIPTION: Sets up the llama-infill target by defining the executable, specifying installation parameters, linking required libraries, and setting the C++ standard to C++17.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/infill/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET llama-infill)\nadd_executable(${TARGET} infill.cpp)\ninstall(TARGETS ${TARGET} RUNTIME)\ntarget_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})\ntarget_compile_features(${TARGET} PRIVATE cxx_std_17)\n```\n\n----------------------------------------\n\nTITLE: Configuring LLM Model Parameters in Bash\nDESCRIPTION: Configuration script setup for specifying model path, name, prompt prefix, and additional options for running the Jeopardy test suite.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/jeopardy/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nMODEL=(path to your model)\nMODEL_NAME=(name of your model)\nprefix=(basically, if you use vicuna it's Human: , if you use something else it might be User: , etc)\nopts=(add -instruct here if needed for your model, or anything else you want to test out)\n```\n\n----------------------------------------\n\nTITLE: Using Jinja Chat Template in Windows\nDESCRIPTION: Command to start a conversation using the built-in Jinja chat template in Windows\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_7\n\nLANGUAGE: powershell\nCODE:\n```\n./llama-cli.exe -m models\\gemma-1.1-7b-it.Q4_K_M.gguf --jinja\n```\n\n----------------------------------------\n\nTITLE: Running llama.cpp on Multiple SYCL Devices\nDESCRIPTION: Command for running llama.cpp inference distributed across multiple SYCL devices. This uses 'layer' split mode to automatically distribute model layers across available devices with the same backend.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_27\n\nLANGUAGE: batch\nCODE:\n```\nbuild\\bin\\llama-cli.exe -no-cnv -m models\\llama-2-7b.Q4_0.gguf -p \"Building a website can be done in 10 simple steps:\\nStep 1:\" -n 400 -e -ngl 33 -s 0 -sm layer\n```\n\n----------------------------------------\n\nTITLE: LLaMA 2 vs LLaMA 3 Quantization Metrics Table\nDESCRIPTION: Markdown table comparing quantization metrics between LLaMA 2 7B and LLaMA 3 8B models across different quantization methods (q2_K through q8_0), including perplexity, correlation, and probability distribution metrics.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/perplexity/README.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Metric          |          L2 7b q2_K |          L3 8b q2_K |        L2 7b q4_K_M |        L3 8b q4_K_M |          L2 7b q6_K |          L3 8b q6_K |          L2 7b q8_0 |          L3 8b q8_0 |\n|-----------------|---------------------|---------------------|---------------------|---------------------|---------------------|---------------------|---------------------|---------------------|\n| Mean PPL        | 5.794552 Â± 0.032298 | 9.751568 Â± 0.063312 | 5.877078 Â± 0.032781 | 6.407115 Â± 0.039119 | 5.808494 Â± 0.032425 | 6.253382 Â± 0.038078 | 5.798542 Â± 0.032366 | 6.234284 Â± 0.037878 |\n```\n\n----------------------------------------\n\nTITLE: LoRA Adapters List Response Format\nDESCRIPTION: JSON response structure showing the list of loaded LoRA adapters with their IDs, paths and scale values.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md#2025-04-22_snippet_15\n\nLANGUAGE: json\nCODE:\n```\n[\n    {\n        \"id\": 0,\n        \"path\": \"my_adapter_1.gguf\",\n        \"scale\": 0.0\n    },\n    {\n        \"id\": 1,\n        \"path\": \"my_adapter_2.gguf\",\n        \"scale\": 0.0\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Running llama-cli with GPU Offloading in CUDA\nDESCRIPTION: Example command for running llama-cli with maximum GPU layer offloading using the -ngl flag, which attempts to move as many layers as possible to the GPU.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/development/token_generation_performance_tips.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n./llama-cli -m \"path/to/model.gguf\" -ngl 200000 -p \"Please sir, may I have some \"\n```\n\n----------------------------------------\n\nTITLE: Applying Multiple LORA Adapters with Custom Scaling\nDESCRIPTION: Shows how to apply multiple LORA adapters to a base model using the llama-export-lora tool. It demonstrates the use of the --lora-scaled option to specify custom scaling for each adapter.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/export-lora/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n./bin/llama-export-lora \\\n    -m your_base_model.gguf \\\n    -o your_merged_model.gguf \\\n    --lora-scaled lora_task_A.gguf 0.5 \\\n    --lora-scaled lora_task_B.gguf 0.5\n```\n\n----------------------------------------\n\nTITLE: Building and Running Tests in One Command\nDESCRIPTION: A combined command that builds the server and then immediately runs the tests, which is useful for local development.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/tests/README.md#2025-04-22_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\ncmake --build build -j --target llama-server && ./examples/server/tests/tests.sh\n```\n\n----------------------------------------\n\nTITLE: Splitting LLaVA Model with llava_surgery.py\nDESCRIPTION: Python command for splitting the LLaVA model into LLaMA and multimodal projector components using the llava_surgery.py script.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/MobileVLM.md#2025-04-22_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\npython ./examples/llava/llava_surgery.py -m path/to/MobileVLM-1.7B\n```\n\n----------------------------------------\n\nTITLE: Chatml Template Example\nDESCRIPTION: The ChatML format template showing how to structure messages with explicit role markers. This is used by models like GPT-4 that require specific formatting for chats.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llama.android/README.md#2025-04-22_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n<|im_start|>system\n{{ system_prompt }}<|im_end|>\n{% for message in messages %}\n<|im_start|>{{ message['role'] }}\n{{ message['content'] }}<|im_end|>\n{% endfor %}\n<|im_start|>assistant\n```\n\n----------------------------------------\n\nTITLE: Validating LLaVA Tensors\nDESCRIPTION: Python script to verify LLaVA tensor files are not empty.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/granitevision.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport torch\n\nMODEL_PATH = os.getenv(\"GRANITE_MODEL\")\nif not MODEL_PATH:\n    raise ValueError(\"env var GRANITE_MODEL is unset!\")\n\nencoder_tensors = torch.load(os.path.join(MODEL_PATH, \"llava.clip\"))\nprojector_tensors = torch.load(os.path.join(MODEL_PATH, \"llava.projector\"))\n\nassert len(encoder_tensors) > 0\nassert len(projector_tensors) > 0\n```\n\n----------------------------------------\n\nTITLE: Running All-in-One Model Setup with Docker in llama.cpp\nDESCRIPTION: Command to download, convert to ggml, and optimize a 7B model using the full Docker image. It mounts a local models directory to the container and runs the all-in-one process.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/docker.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -v /path/to/models:/models ghcr.io/ggml-org/llama.cpp:full --all-in-one \"/models/\" 7B\n```\n\n----------------------------------------\n\nTITLE: Running llama.cpp on Single SYCL Device\nDESCRIPTION: Command for running llama.cpp inference on a single SYCL device (GPU) with device ID 0. This uses 'none' split mode to ensure execution on a single specified device.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_26\n\nLANGUAGE: batch\nCODE:\n```\nbuild\\bin\\llama-cli.exe -no-cnv -m models\\llama-2-7b.Q4_0.gguf -p \"Building a website can be done in 10 simple steps:\\nStep 1:\" -n 400 -e -ngl 33 -s 0 -sm none -mg 0\n```\n\n----------------------------------------\n\nTITLE: Configuring General Build Options for GGML\nDESCRIPTION: Sets up general build options including static linking, native optimization, link-time optimization, and ccache support.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/CMakeLists.txt#2025-04-22_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\n# general\noption(GGML_STATIC \"ggml: static link libraries\"                     OFF)\noption(GGML_NATIVE \"ggml: optimize the build for the current system\" ${GGML_NATIVE_DEFAULT})\noption(GGML_LTO    \"ggml: enable link time optimization\"             OFF)\noption(GGML_CCACHE \"ggml: use ccache if available\"                   ON)\n```\n\n----------------------------------------\n\nTITLE: Cloning LLaVA and CLIP Models\nDESCRIPTION: Git commands to clone the LLaVA model and the CLIP model, which are required for the LLaVA implementation.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/llava.md#2025-04-22_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ngit clone https://huggingface.co/liuhaotian/llava-v1.5-7b\n\ngit clone https://huggingface.co/openai/clip-vit-large-patch14-336\n```\n\n----------------------------------------\n\nTITLE: Single-Turn Query with System Prompt in Windows\nDESCRIPTION: Command to execute a single interaction with custom system prompt using Jinja template in Windows\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_8\n\nLANGUAGE: powershell\nCODE:\n```\n./llama-cli.exe -m models\\gemma-1.1-7b-it.Q4_K_M.gguf --jinja --single-turn -sys \"You are a helpful assistant\" -p \"Hello\"\n```\n\n----------------------------------------\n\nTITLE: Running Model Inference with Callback in llama.cpp\nDESCRIPTION: Command to run the llama-eval-callback utility with a PHI-2 model. It specifies the model repository, file, output model name, prompt text, random seed, and number of GPU layers to offload.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/eval-callback/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nllama-eval-callback \\\n  --hf-repo ggml-org/models \\\n  --hf-file phi-2/ggml-model-q4_0.gguf \\\n  --model phi-2-q4_0.gguf \\\n  --prompt hello \\\n  --seed 42 \\\n  -ngl 33\n```\n\n----------------------------------------\n\nTITLE: LLaMA 3 BF16 vs FP16 Comparison Metrics\nDESCRIPTION: Markdown table showing detailed comparison metrics between BF16 and FP16 precision for LLaMA 3 8B model, including perplexity ratios, KL divergence, and probability distribution differences.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/perplexity/README.md#2025-04-22_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Metric                         |                    Value |\n|--------------------------------|---------------------------|\n| Mean PPL(Q)                    |      6.227711 Â± 0.037833 |\n| Mean PPL(base)                 |      6.225194 Â± 0.037771 |\n```\n\n----------------------------------------\n\nTITLE: Tokenization Response Format\nDESCRIPTION: JSON response format for the /tokenize endpoint showing both simple token ID array and detailed token information with pieces.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md#2025-04-22_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"tokens\": [123, 456, 789]\n}\n\n{\n  \"tokens\": [\n    {\"id\": 123, \"piece\": \"Hello\"},\n    {\"id\": 456, \"piece\": \" world\"},\n    {\"id\": 789, \"piece\": \"!\"}\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Running GRIT Model Example with llama-gritlm\nDESCRIPTION: Command to execute the llama-gritlm example using the previously downloaded model. The output demonstrates both embedding similarity comparisons between different text pairs and a creative text generation sample.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/gritlm/README.md#2025-04-22_snippet_1\n\nLANGUAGE: console\nCODE:\n```\n$ ./llama-gritlm -m models/gritlm-7b_q4_1.gguf\n\nCosine similarity between \"Bitcoin: A Peer-to-Peer Electronic Cash System\" and \"A purely peer-to-peer version of electronic cash w\" is: 0.605\nCosine similarity between \"Bitcoin: A Peer-to-Peer Electronic Cash System\" and \"All text-based language problems can be reduced to\" is: 0.103\nCosine similarity between \"Generative Representational Instruction Tuning\" and \"A purely peer-to-peer version of electronic cash w\" is: 0.112\nCosine similarity between \"Generative Representational Instruction Tuning\" and \"All text-based language problems can be reduced to\" is: 0.547\n\nOh, brave adventurer, who dared to climb\nThe lofty peak of Mt. Fuji in the night,\nWhen shadows lurk and ghosts do roam,\nAnd darkness reigns, a fearsome sight.\n\nThou didst set out, with heart aglow,\nTo conquer this mountain, so high,\nAnd reach the summit, where the stars do glow,\nAnd the moon shines bright, up in the sky.\n\nThrough the mist and fog, thou didst press on,\nWith steadfast courage, and a steadfast will,\nThrough the darkness, thou didst not be gone,\nBut didst climb on, with a steadfast skill.\n\nAt last, thou didst reach the summit's crest,\nAnd gazed upon the world below,\nAnd saw the beauty of the night's best,\nAnd felt the peace, that only nature knows.\n\nOh, brave adventurer, who dared to climb\nThe lofty peak of Mt. Fuji in the night,\nThou art a hero, in the eyes of all,\nFor thou didst conquer this mountain, so bright.\n```\n\n----------------------------------------\n\nTITLE: Basic Text Embedding Generation - Windows\nDESCRIPTION: Basic command to generate embeddings for a simple text input 'Hello World!' using mean pooling on Windows systems. The output is space-separated float values with logging disabled.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/embedding/README.md#2025-04-22_snippet_1\n\nLANGUAGE: powershell\nCODE:\n```\nllama-embedding.exe -m ./path/to/model --pooling mean --log-disable -p \"Hello World!\" 2>$null\n```\n\n----------------------------------------\n\nTITLE: Web UI Development Commands\nDESCRIPTION: NPM commands for setting up and building the web UI, including development server launch and production build generation.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# make sure you have nodejs installed\ncd examples/server/webui\nnpm i\n\n# to run the dev server\nnpm run dev\n\n# to build the public/index.html.gz\nnpm run build\n```\n\n----------------------------------------\n\nTITLE: JSONL Output Format Example for Benchmark Results\nDESCRIPTION: Example of the JSONL output format when using --output-format jsonl flag, showing benchmark metrics including KV cache size, batch parameters, timing, and speed measurements.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/batched-bench/README.md#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\"n_kv_max\": 2048, \"n_batch\": 2048, \"n_ubatch\": 512, \"flash_attn\": 0, \"is_pp_shared\": 0, \"n_gpu_layers\": 99, \"n_threads\": 8, \"n_threads_batch\": 8, \"pp\": 128, \"tg\": 128, \"pl\": 1, \"n_kv\": 256, \"t_pp\": 0.233810, \"speed_pp\": 547.453064, \"t_tg\": 3.503684, \"speed_tg\": 36.532974, \"t\": 3.737494, \"speed\": 68.495094}\n{\"n_kv_max\": 2048, \"n_batch\": 2048, \"n_ubatch\": 512, \"flash_attn\": 0, \"is_pp_shared\": 0, \"n_gpu_layers\": 99, \"n_threads\": 8, \"n_threads_batch\": 8, \"pp\": 128, \"tg\": 128, \"pl\": 2, \"n_kv\": 512, \"t_pp\": 0.422602, \"speed_pp\": 605.770935, \"t_tg\": 11.106112, \"speed_tg\": 23.050371, \"t\": 11.528713, \"speed\": 44.410854}\n```\n\n----------------------------------------\n\nTITLE: Converting HuggingFace decoder model to GGUF format\nDESCRIPTION: Python command to convert the WavTokenizer model from HuggingFace format to GGUF format with 16-bit floating point precision for use with llama.cpp.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/tts/README.md#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\n(venv) python convert_hf_to_gguf.py models/WavTokenizer-large-speech-75token \\\n    --outfile models/wavtokenizer-large-75-f16.gguf --outtype f16\n```\n\n----------------------------------------\n\nTITLE: Installing GGUF Package\nDESCRIPTION: Basic installation command for the GGUF package using pip\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/gguf-py/README.md#2025-04-22_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install gguf\n```\n\n----------------------------------------\n\nTITLE: Running llama.cpp with SYCL on Docker Container\nDESCRIPTION: Docker commands to find available GPU devices and run the llama.cpp container with GPU passthrough for inference on Intel GPUs.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n# First, find all the DRI cards\nls -la /dev/dri\n# Then, pick the card that you want to use (here for e.g. /dev/dri/card1).\ndocker run -it --rm -v \"$(pwd):/app:Z\" --device /dev/dri/renderD128:/dev/dri/renderD128 --device /dev/dri/card1:/dev/dri/card1 llama-cpp-sycl -m \"/app/models/YOUR_MODEL_FILE\" -p \"Building a website can be done in 10 simple steps:\" -n 400 -e -ngl 33\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake Build for llama.cpp Simple Example\nDESCRIPTION: Sets up a CMake build configuration for a simple llama.cpp example. Requires CMake 3.12+, finds and links the Llama package, configures C++17 support, and sets up installation rules.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/simple-cmake-pkg/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.12)\nproject(llama-simple-cmake-pkg)\n\nset(TARGET llama-simple-cmake-pkg)\n\nfind_package(Llama REQUIRED)\n\nadd_executable(${TARGET} ${CMAKE_CURRENT_LIST_DIR}/../simple/simple.cpp)\ninstall(TARGETS ${TARGET} RUNTIME)\ntarget_link_libraries(${TARGET} PRIVATE llama ggml::all ${CMAKE_THREAD_LIBS_INIT})\ntarget_compile_features(${TARGET} PRIVATE cxx_std_17)\n```\n\n----------------------------------------\n\nTITLE: Converting Visual Components to GGUF\nDESCRIPTION: Command to convert visual encoder components to GGUF format with SigLIP parameters.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/granitevision.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ python convert_image_encoder_to_gguf.py \\\n    -m $ENCODER_PATH \\\n    --llava-projector $ENCODER_PATH/llava.projector \\\n    --output-dir $ENCODER_PATH \\\n    --clip-model-is-vision \\\n    --clip-model-is-siglip \\\n    --image-mean 0.5 0.5 0.5 \\\n    --image-std 0.5 0.5 0.5\n```\n\n----------------------------------------\n\nTITLE: Setting CUDA Architecture Targets in CMake Configuration\nDESCRIPTION: Determines which CUDA GPU architectures to target based on project configuration. Handles native builds, configurations requiring FP16 support, and default settings for maximum compatibility.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cuda/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif (NOT DEFINED CMAKE_CUDA_ARCHITECTURES)\n    # native == GPUs available at build time\n    # 50     == Maxwell, lowest CUDA 12 standard\n    # 60     == P100, FP16 CUDA intrinsics\n    # 61     == Pascal, __dp4a instruction (per-byte integer dot product)\n    # 70     == V100, FP16 tensor cores\n    # 75     == Turing, int8 tensor cores\n    if (GGML_NATIVE AND CUDAToolkit_VERSION VERSION_GREATER_EQUAL \"11.6\" AND CMAKE_VERSION VERSION_GREATER_EQUAL \"3.24\")\n        set(CMAKE_CUDA_ARCHITECTURES \"native\")\n    elseif(GGML_CUDA_F16 OR GGML_CUDA_DMMV_F16)\n        set(CMAKE_CUDA_ARCHITECTURES \"60;61;70;75;80\")\n    else()\n        set(CMAKE_CUDA_ARCHITECTURES \"50;61;70;75;80\")\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Running llama-bench with Markdown output format\nDESCRIPTION: Command to run the llama-bench tool with Markdown table output format. This displays benchmark results in a human-readable Markdown table showing model details and performance metrics.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llama-bench/README.md#2025-04-22_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\n$ ./llama-bench -o md\n```\n\n----------------------------------------\n\nTITLE: Building llama.cpp with Vulkan on MSYS2\nDESCRIPTION: Commands to build llama.cpp with Vulkan support using CMake in MSYS2 environment.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_17\n\nLANGUAGE: sh\nCODE:\n```\ncmake -B build -DGGML_VULKAN=ON\ncmake --build build --config Release\n```\n\n----------------------------------------\n\nTITLE: UTF-8 Token Example Response\nDESCRIPTION: Example tokenization response showing how UTF-8 characters are handled, using the character 'Ã¡' as an example.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md#2025-04-22_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"tokens\": [\n    {\"id\": 198, \"piece\": [195]},\n    {\"id\": 164, \"piece\": [161]}\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Advanced Text Embedding Generation - Unix\nDESCRIPTION: Advanced command demonstrating multiple text inputs with custom separator, euclidean normalization, and specific output format for Unix-based systems. Uses GPU acceleration and disables logging.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/embedding/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n./llama-embedding -p 'Castle<#sep#>Stronghold<#sep#>Dog<#sep#>Cat' --pooling mean --embd-separator '<#sep#>' --embd-normalize 2  --embd-output-format '' -m './path/to/model.gguf' --n-gpu-layers 99 --log-disable 2>/dev/null\n```\n\n----------------------------------------\n\nTITLE: Running Batched Text Generation with llama.cpp\nDESCRIPTION: This command demonstrates how to run the llama-batched executable to generate multiple text sequences from a single prompt. The example uses a 7B parameter model and generates 4 parallel sequences from the prompt \"Hello my name is\". The output shows the generated texts and performance metrics.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/batched/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./llama-batched -m ./models/llama-7b-v2/ggml-model-f16.gguf -p \"Hello my name is\" -np 4\n\n...\n\nmain: n_len = 32, n_ctx = 2048, n_parallel = 4, n_kv_req = 113\n\n Hello my name is\n\nmain: generating 4 sequences ...\n\nmain: stream 0 finished\nmain: stream 1 finished\nmain: stream 2 finished\nmain: stream 3 finished\n\nsequence 0:\n\nHello my name is Shirley. I am a 25-year-old female who has been working for over 5 years as a b\n\nsequence 1:\n\nHello my name is Renee and I'm a 32 year old female from the United States. I'm looking for a man between\n\nsequence 2:\n\nHello my name is Diana. I am looking for a housekeeping job. I have experience with children and have my own transportation. I am\n\nsequence 3:\n\nHello my name is Cody. I am a 3 year old neutered male. I am a very friendly cat. I am very playful and\n\nmain: decoded 108 tokens in 3.57 s, speed: 30.26 t/s\n\nllama_print_timings:        load time =   587.00 ms\nllama_print_timings:      sample time =     2.56 ms /   112 runs   (    0.02 ms per token, 43664.72 tokens per second)\nllama_print_timings: prompt eval time =  4089.11 ms /   118 tokens (   34.65 ms per token,    28.86 tokens per second)\nllama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\nllama_print_timings:       total time =  4156.04 ms\n```\n\n----------------------------------------\n\nTITLE: Installing GGUF in Development Mode\nDESCRIPTION: Commands for installing GGUF package in editable mode for development purposes, including changing to the package directory\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/gguf-py/README.md#2025-04-22_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ncd /path/to/llama.cpp/gguf-py\n\npip install --editable .\n```\n\n----------------------------------------\n\nTITLE: Building llama.cpp with Intel oneMKL\nDESCRIPTION: Commands to build llama.cpp with Intel oneMKL acceleration using CMake. This is specific for Intel processors and requires the oneAPI environment.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nsource /opt/intel/oneapi/setvars.sh\ncmake -B build -DGGML_BLAS=ON -DGGML_BLAS_VENDOR=Intel10_64lp -DCMAKE_C_COMPILER=icx -DCMAKE_CXX_COMPILER=icpx -DGGML_NATIVE=ON\ncmake --build build --config Release\n```\n\n----------------------------------------\n\nTITLE: Running llama-mtmd-cli with GLMV-EDGE model\nDESCRIPTION: Command to run the llama-mtmd-cli binary with a GLMV-EDGE model and its multimodal projector. It demonstrates how to specify the model path and the mmproj path.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/glmedge.md#2025-04-22_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n./llama-mtmd-cli -m model_path/ggml-model-f16.gguf --mmproj model_path/mmproj-model-f16.gguf\n```\n\n----------------------------------------\n\nTITLE: Applying Custom Theme to LLaMA.cpp Server\nDESCRIPTION: Command to run the LLaMA.cpp server with a custom theme that places chat buttons at the top of the page. This modification improves usability by making the Stop button more accessible.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/themes/buttons-top/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n--path=themes/buttons_top\n```\n\n----------------------------------------\n\nTITLE: Complete Usage Documentation for llama-run Command-line Tool\nDESCRIPTION: Comprehensive documentation of the llama-run command, including its usage syntax, available options, commands, and examples. It details parameters like context size, GPU layers, temperature settings, and various model source formats including local files, Ollama models, and Hugging Face repositories.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/run/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nDescription:\n  Runs a llm\n\nUsage:\n  llama-run [options] model [prompt]\n\nOptions:\n  -c, --context-size <value>\n      Context size (default: 2048)\n  -n, -ngl, --ngl <value>\n      Number of GPU layers (default: 0)\n  --temp <value>\n      Temperature (default: 0.8)\n  -v, --verbose, --log-verbose\n      Set verbosity level to infinity (i.e. log all messages, useful for debugging)\n  -h, --help\n      Show help message\n\nCommands:\n  model\n      Model is a string with an optional prefix of\n      huggingface:// (hf://), ollama://, https:// or file://.\n      If no protocol is specified and a file exists in the specified\n      path, file:// is assumed, otherwise if a file does not exist in\n      the specified path, ollama:// is assumed. Models that are being\n      pulled are downloaded with .partial extension while being\n      downloaded and then renamed as the file without the .partial\n      extension when complete.\n\nExamples:\n  llama-run llama3\n  llama-run ollama://granite-code\n  llama-run ollama://smollm:135m\n  llama-run hf://QuantFactory/SmolLM-135M-GGUF/SmolLM-135M.Q2_K.gguf\n  llama-run huggingface://bartowski/SmolLM-1.7B-Instruct-v0.2-GGUF/SmolLM-1.7B-Instruct-v0.2-IQ3_M.gguf\n  llama-run https://example.com/some-file1.gguf\n  llama-run some-file2.gguf\n  llama-run file://some-file3.gguf\n  llama-run --ngl 999 some-file4.gguf\n  llama-run --ngl 999 some-file5.gguf Hello World\n```\n\n----------------------------------------\n\nTITLE: Installing llama.cpp with Flox\nDESCRIPTION: Command to install llama.cpp within a Flox environment on Mac and Linux systems. Flox follows the nixpkgs build of llama.cpp.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/install.md#2025-04-22_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\nflox install llama-cpp\n```\n\n----------------------------------------\n\nTITLE: Setting CUDA Compiler Flags and Options\nDESCRIPTION: Configures CUDA compiler flags including architecture-specific optimizations, warning settings, and compiler version detection for cross-platform compatibility.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cuda/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nset(CUDA_CXX_FLAGS \"\")\n\nset(CUDA_FLAGS -use_fast_math)\n\nif (CUDAToolkit_VERSION VERSION_GREATER_EQUAL \"12.8\")\n    # Options are:\n    # - none (not recommended)\n    # - speed (nvcc's default)\n    # - balance\n    # - size\n    list(APPEND CUDA_FLAGS -compress-mode=${GGML_CUDA_COMPRESSION_MODE})\nendif()\n\nif (GGML_FATAL_WARNINGS)\n    list(APPEND CUDA_FLAGS -Werror all-warnings)\nendif()\n\nif (GGML_ALL_WARNINGS AND NOT MSVC)\n    set(NVCC_CMD ${CMAKE_CUDA_COMPILER} .c)\n    if (NOT CMAKE_CUDA_HOST_COMPILER STREQUAL \"\")\n        list(APPEND NVCC_CMD -ccbin ${CMAKE_CUDA_HOST_COMPILER})\n    endif()\n\n    execute_process(\n        COMMAND ${NVCC_CMD} -Xcompiler --version\n        OUTPUT_VARIABLE CUDA_CCFULLVER\n        ERROR_QUIET\n    )\n\n    if (NOT CUDA_CCFULLVER MATCHES clang)\n        set(CUDA_CCID \"GNU\")\n        execute_process(\n            COMMAND ${NVCC_CMD} -Xcompiler \"-dumpfullversion -dumpversion\"\n            OUTPUT_VARIABLE CUDA_CCVER\n            ERROR_QUIET\n        )\n    else()\n        if (CUDA_CCFULLVER MATCHES Apple)\n            set(CUDA_CCID \"AppleClang\")\n        else()\n            set(CUDA_CCID \"Clang\")\n        endif()\n        string(REGEX REPLACE \"^.* version ([0-9.]*).*$\" \"\\\\1\" CUDA_CCVER ${CUDA_CCFULLVER})\n    endif()\n\n    message(\"-- CUDA host compiler is ${CUDA_CCID} ${CUDA_CCVER}\")\n\n    ggml_get_flags(${CUDA_CCID} ${CUDA_CCVER})\n    list(APPEND CUDA_CXX_FLAGS ${CXX_FLAGS} ${GF_CXX_FLAGS})  # This is passed to -Xcompiler later\nendif()\n\nif (NOT MSVC)\n    list(APPEND CUDA_CXX_FLAGS -Wno-pedantic)\nendif()\n\nlist(JOIN   CUDA_CXX_FLAGS \" \" CUDA_CXX_FLAGS_JOINED)  # pass host compiler flags as a single argument\n\nif (NOT CUDA_CXX_FLAGS_JOINED STREQUAL \"\")\n    list(APPEND CUDA_FLAGS -Xcompiler ${CUDA_CXX_FLAGS_JOINED})\nendif()\n\ntarget_compile_options(ggml-cuda PRIVATE \"$<$<COMPILE_LANGUAGE:CUDA>:${CUDA_FLAGS}>\")\n```\n\n----------------------------------------\n\nTITLE: Running Inference on Single Device with CANN Backend\nDESCRIPTION: Command to run inference using a single device (GPU) with the CANN backend. It specifies device 0, disables layer splitting, and generates 400 tokens in response to the prompt about building a website.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/CANN.md#2025-04-22_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\n./build/bin/llama-cli -m path_to_model -p \"Building a website can be done in 10 simple steps:\" -n 400 -e -ngl 33 -sm none -mg 0\n```\n\n----------------------------------------\n\nTITLE: Building llama.cpp with SYCL for AMD GPU\nDESCRIPTION: CMake commands to build llama.cpp with SYCL support for AMD GPUs, using FP32.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\nGGML_SYCL_DEVICE_ARCH=gfx90a # Example architecture\ncmake -B build -DGGML_SYCL=ON -DGGML_SYCL_TARGET=AMD -DGGML_SYCL_DEVICE_ARCH=${GGML_SYCL_DEVICE_ARCH} -DCMAKE_C_COMPILER=icx -DCMAKE_CXX_COMPILER=icpx\n\n# build all binary\ncmake --build build --config Release -j -v\n```\n\n----------------------------------------\n\nTITLE: JSON Response Structure Example - Token Probabilities\nDESCRIPTION: Example structure showing the completion_probabilities array format in the API response, containing token IDs and probabilities.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md#2025-04-22_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"content\": \"<the generated completion text>\",\n    \"tokens\": [ generated token ids if requested ],\n    ...\n    \"probs\": [\n      {\n        \"id\": <token id>\n      }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Running llama-cli with SYCL on Linux (Multiple Devices)\nDESCRIPTION: Shell command to execute llama-cli inference using multiple SYCL devices on Linux with specific parameters.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_14\n\nLANGUAGE: sh\nCODE:\n```\nZES_ENABLE_SYSMAN=1 ./build/bin/llama-cli -no-cnv -m models/llama-2-7b.Q4_0.gguf -p \"Building a website can be done in 10 simple steps:\" -n 400 -e -ngl 33 -sm layer\n```\n\n----------------------------------------\n\nTITLE: Basic CPU-based Control Vector Generation Command\nDESCRIPTION: A simple command to generate a control vector using CPU-only processing with a quantized GGUF model.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/cvector-generator/README.md#2025-04-22_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n./cvector-generator -m ./llama-3.Q4_K_M.gguf\n```\n\n----------------------------------------\n\nTITLE: MUSA Static Build Configuration\nDESCRIPTION: CMake commands for creating a static build of llama.cpp with MUSA support, including position-independent code.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\ncmake -B build -DGGML_MUSA=ON \\\n  -DBUILD_SHARED_LIBS=OFF -DCMAKE_POSITION_INDEPENDENT_CODE=ON\ncmake --build build --config Release\n```\n\n----------------------------------------\n\nTITLE: Installing llama.cpp with MacPorts\nDESCRIPTION: Command to install llama.cpp using MacPorts package manager. Requires sudo privileges.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/install.md#2025-04-22_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nsudo port install llama.cpp\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Dependencies for LLaMA Model Conversion\nDESCRIPTION: This snippet defines the Python package requirements for converting Hugging Face models to GGUF format. It references an external requirements file and specifies an additional index URL for PyTorch CPU wheels.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/requirements/requirements-convert_lora_to_gguf.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n-r ./requirements-convert_hf_to_gguf.txt\n--extra-index-url https://download.pytorch.org/whl/cpu\n```\n\n----------------------------------------\n\nTITLE: HIP GPU Compilation for Windows\nDESCRIPTION: CMake commands for building with HIP support on Windows, targeting specific AMD GPU architectures.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nset PATH=%HIP_PATH%\\bin;%PATH%\ncmake -S . -B build -G Ninja -DAMDGPU_TARGETS=gfx1100 -DGGML_HIP=ON -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=clang++ -DCMAKE_BUILD_TYPE=Release\ncmake --build build\n```\n\n----------------------------------------\n\nTITLE: Running llama-bench with CSV output format\nDESCRIPTION: Command to run the llama-bench tool with CSV output format. This produces benchmark results as comma-separated values, suitable for importing into spreadsheet applications.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llama-bench/README.md#2025-04-22_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\n$ ./llama-bench -o csv\n```\n\n----------------------------------------\n\nTITLE: Cloning and Building llama.cpp with CMake\nDESCRIPTION: Commands to clone the llama.cpp repository and build it using CMake. This is the prerequisite step before working with the MiniCPM-Llama3-V 2.5 model.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/minicpmv2.5.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/ggml-org/llama.cpp\ncd llama.cpp\n```\n\nLANGUAGE: bash\nCODE:\n```\ncmake -B build\ncmake --build build --config Release\n```\n\n----------------------------------------\n\nTITLE: Exporting LLM Component\nDESCRIPTION: Python script to export the language model component for GGUF conversion.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/granitevision.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport transformers\n\nMODEL_PATH = os.getenv(\"GRANITE_MODEL\")\nif not MODEL_PATH:\n    raise ValueError(\"env var GRANITE_MODEL is unset!\")\n\nLLM_EXPORT_PATH = os.getenv(\"LLM_EXPORT_PATH\")\nif not LLM_EXPORT_PATH:\n    raise ValueError(\"env var LLM_EXPORT_PATH is unset!\")\n\ntokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_PATH)\n\nmodel = transformers.AutoModelForImageTextToText.from_pretrained(MODEL_PATH, ignore_mismatched_sizes=True)\n\ntokenizer.save_pretrained(LLM_EXPORT_PATH)\nmodel.language_model.save_pretrained(LLM_EXPORT_PATH)\n```\n\n----------------------------------------\n\nTITLE: Running a Quantized LLaMA Model with llama.cpp\nDESCRIPTION: This code shows how to run inference on a quantized GGUF model using the llama-cli tool. It demonstrates starting a conversation with the model using a prompt.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/quantize/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# start inference on a gguf model\n./llama-cli -m ./models/mymodel/ggml-model-Q4_K_M.gguf -cnv -p \"You are a helpful assistant\"\n```\n\n----------------------------------------\n\nTITLE: Running RPC Server with Specific CUDA Device\nDESCRIPTION: Command to start the RPC server with a specific CUDA device using the CUDA_VISIBLE_DEVICES environment variable.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/rpc/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ CUDA_VISIBLE_DEVICES=0 bin/rpc-server -p 50052\n```\n\n----------------------------------------\n\nTITLE: Visual Encoder Configuration\nDESCRIPTION: JSON configuration for SiglipVisionModel with grid pinpoints and model parameters.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/granitevision.md#2025-04-22_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"_name_or_path\": \"siglip-model\",\n    \"architectures\": [\n      \"SiglipVisionModel\"\n    ],\n    \"image_grid_pinpoints\": [\n        [384,384],\n        [384,768],\n        [384,1152],\n        [384,1536],\n        [384,1920],\n        [384,2304],\n        [384,2688],\n        [384,3072],\n        [384,3456],\n        [384,3840],\n        [768,384],\n        [768,768],\n        [768,1152],\n        [768,1536],\n        [768,1920],\n        [1152,384],\n        [1152,768],\n        [1152,1152],\n        [1536,384],\n        [1536,768],\n        [1920,384],\n        [1920,768],\n        [2304,384],\n        [2688,384],\n        [3072,384],\n        [3456,384],\n        [3840,384]\n    ],\n    \"mm_patch_merge_type\": \"spatial_unpad\",\n    \"hidden_size\": 1152,\n    \"image_size\": 384,\n    \"intermediate_size\": 4304,\n    \"model_type\": \"siglip_vision_model\",\n    \"num_attention_heads\": 16,\n    \"num_hidden_layers\": 27,\n    \"patch_size\": 14,\n    \"layer_norm_eps\": 1e-6,\n    \"hidden_act\": \"gelu_pytorch_tanh\",\n    \"projection_dim\": 0,\n    \"vision_feature_layer\": [-24, -20, -12, -1]\n}\n```\n\n----------------------------------------\n\nTITLE: JSONL output from llama-bench\nDESCRIPTION: Example of JSONL (JSON Lines) output generated by llama-bench. Each line is a valid JSON object representing a single benchmark test result, making it efficient for log processing and incremental loading.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llama-bench/README.md#2025-04-22_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\"build_commit\":\"3469684\",\"build_number\":1275,\"cuda\":true,\"metal\":false,\"gpu_blas\":true,\"blas\":true,\"cpu_info\":\"13th Gen Intel(R) Core(TM) i9-13900K\",\"gpu_info\":\"NVIDIA GeForce RTX 3090 Ti\",\"model_filename\":\"models/7B/ggml-model-q4_0.gguf\",\"model_type\":\"llama 7B mostly Q4_0\",\"model_size\":3825065984,\"model_n_params\":6738415616,\"n_batch\":512,\"n_threads\":16,\"f16_kv\":true,\"n_gpu_layers\":99,\"main_gpu\":0,\"mul_mat_q\":true,\"tensor_split\":\"0.00\",\"n_prompt\":512,\"n_gen\":0,\"test_time\":\"2023-09-23T12:09:57Z\",\"avg_ns\":212365953,\"stddev_ns\":985423,\"avg_ts\":2410.974041,\"stddev_ts\":11.163766,\"samples_ns\":[213837238,211635853,212328053,211329715,212698907],\"samples_ts\":[2394.34,2419.25,2411.36,2422.75,2407.16]}\n{\"build_commit\":\"3469684\",\"build_number\":1275,\"cuda\":true,\"metal\":false,\"gpu_blas\":true,\"blas\":true,\"cpu_info\":\"13th Gen Intel(R) Core(TM) i9-13900K\",\"gpu_info\":\"NVIDIA GeForce RTX 3090 Ti\",\"model_filename\":\"models/7B/ggml-model-q4_0.gguf\",\"model_type\":\"llama 7B mostly Q4_0\",\"model_size\":3825065984,\"model_n_params\":6738415616,\"n_batch\":512,\"n_threads\":16,\"f16_kv\":true,\"n_gpu_layers\":99,\"main_gpu\":0,\"mul_mat_q\":true,\"tensor_split\":\"0.00\",\"n_prompt\":0,\"n_gen\":128,\"test_time\":\"2023-09-23T12:09:59Z\",\"avg_ns\":977425219,\"stddev_ns\":9268593,\"avg_ts\":130.965708,\"stddev_ts\":1.238924,\"samples_ns\":[984472709,974901233,989474741,970729355,967548060],\"samples_ts\":[130.019,131.295,129.362,131.86,132.293]}\n```\n\n----------------------------------------\n\nTITLE: Running Gemma 3 Vision with Custom Models\nDESCRIPTION: Commands for running Gemma 3 vision models with custom GGUF files and image input.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/gemma3.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# build\ncmake -B build\ncmake --build build --target llama-gemma3-cli\n\n# run it\n./build/bin/llama-gemma3-cli -m {text_model}.gguf --mmproj mmproj.gguf --image your_image.jpg\n```\n\n----------------------------------------\n\nTITLE: Running MobileVLM on Android (Cat Image Example)\nDESCRIPTION: Example command for running the MobileVLM model on Android to describe a cat image. Shows how to use the model for general image description tasks.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/MobileVLM.md#2025-04-22_snippet_9\n\nLANGUAGE: sh\nCODE:\n```\n/data/local/tmp/llama-mtmd-cli \\\n    -m /data/local/tmp/ggml-model-q4_k.gguf \\\n    --mmproj /data/local/tmp/mmproj-model-f16.gguf \\\n    -t 4 \\\n    --image /data/local/tmp/cat.jpeg \\\n    -p \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: <image>\\nWhat is in the image? ASSISTANT:\"\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies with MSYS2 for Windows\nDESCRIPTION: Installs required packages for building llama.cpp with Vulkan support on Windows using MSYS2's package manager.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_16\n\nLANGUAGE: sh\nCODE:\n```\npacman -S git \\\n    mingw-w64-ucrt-x86_64-gcc \\\n    mingw-w64-ucrt-x86_64-cmake \\\n    mingw-w64-ucrt-x86_64-vulkan-devel \\\n    mingw-w64-ucrt-x86_64-shaderc\n```\n\n----------------------------------------\n\nTITLE: Installing llama.cpp with Nix using non-flake installation\nDESCRIPTION: Command to install llama.cpp using the Nix package manager for non-flake enabled installations on Mac and Linux systems.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/install.md#2025-04-22_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nnix-env --file '<nixpkgs>' --install --attr llama-cpp\n```\n\n----------------------------------------\n\nTITLE: Building llama.cpp with CUDA and Specific Compute Capabilities\nDESCRIPTION: Command to build llama.cpp with CUDA support for specific NVIDIA GPU compute capabilities using CMake.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ncmake -B build -DGGML_CUDA=ON -DCMAKE_CUDA_ARCHITECTURES=\"86;89\"\n```\n\n----------------------------------------\n\nTITLE: GPU Layer Offloading Test Command\nDESCRIPTION: Example command for testing performance with different numbers of layers offloaded to GPU.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llama-bench/README.md#2025-04-22_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\n./llama-bench -ngl 10,20,30,31,32,33,34,35\n```\n\n----------------------------------------\n\nTITLE: Basic Usage of llama-imatrix Command\nDESCRIPTION: The basic command structure for using llama-imatrix tool, showing all available parameters. This command processes a model and text file to generate an importance matrix that can be used during quantization.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/imatrix/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./llama-imatrix \\\n    -m model.gguf -f some-text.txt [-o imatrix.dat] [--process-output] [--verbosity 1] \\\n    [--no-ppl] [--chunk 123] [--output-frequency 10] [--save-frequency 0] \\\n    [--in-file imatrix-prev-0.dat --in-file imatrix-prev-1.dat ...]\n```\n\n----------------------------------------\n\nTITLE: Compiling llama.cpp with BLIS Integration\nDESCRIPTION: Builds llama.cpp with BLIS BLAS support using CMake configuration.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/BLIS.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmkdir build\ncd build\ncmake -DGGML_BLAS=ON -DGGML_BLAS_VENDOR=FLAME ..\nmake -j\n```\n\n----------------------------------------\n\nTITLE: Running the Llama Retrieval Example in Bash\nDESCRIPTION: Command to build and run the retrieval example with the BGE model. It processes README.md and License files with specified chunk size and separator, retrieving the top 3 most relevant chunks for queries.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/retrieval/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmake -j && ./llama-retrieval --model ./models/bge-base-en-v1.5-f16.gguf --top-k 3 --context-file README.md --context-file License --chunk-size 100 --chunk-separator .\n```\n\n----------------------------------------\n\nTITLE: Configuring Compiler Cache for Build Acceleration\nDESCRIPTION: Sets up ccache or sccache for faster compilation if GGML_CCACHE is enabled. This speeds up builds by caching previous compilation results.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/CMakeLists.txt#2025-04-22_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nif (GGML_CCACHE AND NOT CMAKE_C_COMPILER_LAUNCHER AND NOT CMAKE_CXX_COMPILER_LAUNCHER)\n    find_program(GGML_CCACHE_FOUND ccache)\n    find_program(GGML_SCCACHE_FOUND sccache)\n\n    if (GGML_CCACHE_FOUND OR GGML_SCCACHE_FOUND)\n        if(GGML_CCACHE_FOUND)\n            set(GGML_CCACHE_VARIANT ccache)\n        else()\n            set(GGML_CCACHE_VARIANT sccache)\n        endif()\n        # TODO: should not be set globally\n        if (GGML_SYCL AND GGML_CCACHE_FOUND AND WIN32)\n            set_property(GLOBAL PROPERTY RULE_LAUNCH_COMPILE \"ccache compiler_type=icl\")\n        else ()\n            set_property(GLOBAL PROPERTY RULE_LAUNCH_COMPILE \"${GGML_CCACHE_VARIANT}\")\n        endif ()\n        set(ENV{CCACHE_SLOPPINESS} time_macros)\n        message(STATUS \"${GGML_CCACHE_VARIANT} found, compilation results will be cached. Disable with GGML_CCACHE=OFF.\")\n    else()\n        message(STATUS \"Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\")\n    endif ()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Cloning and Building llama.cpp with CMake\nDESCRIPTION: Commands to clone the llama.cpp repository and build it using CMake. This is the initial setup required before working with MiniCPM-o 2.6.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/minicpmo2.6.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/ggerganov/llama.cpp\ncd llama.cpp\n```\n\nLANGUAGE: bash\nCODE:\n```\ncmake -B build\ncmake --build build --config Release\n```\n\n----------------------------------------\n\nTITLE: Downloading GRIT Model from Hugging Face with Script\nDESCRIPTION: Command to download the GritLM-7B model from Hugging Face repository using the provided script. The model is saved to the models directory for later use in the example.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/gritlm/README.md#2025-04-22_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n$ scripts/hf.sh --repo cohesionet/GritLM-7B_gguf --file gritlm-7b_q4_1.gguf --outdir models\n```\n\n----------------------------------------\n\nTITLE: Running llama-mtmd-cli with LLaVA 1.6 Model\nDESCRIPTION: Command to run the llama-mtmd-cli using the LLaVA 1.6 model, specifying the model file and multimodal projector.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/llava.md#2025-04-22_snippet_11\n\nLANGUAGE: sh\nCODE:\n```\n./llama-mtmd-cli -m ../llava-v1.6-vicuna-7b/ggml-model-f16.gguf --mmproj vit/mmproj-model-f16.gguf\n```\n\n----------------------------------------\n\nTITLE: Infinite Text Generation in Unix\nDESCRIPTION: Command to generate continuous text from a starting prompt in Unix-based systems, requiring Ctrl-C to stop\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n./llama-cli -m models/gemma-1.1-7b-it.Q4_K_M.gguf --ignore-eos -n -1\n```\n\n----------------------------------------\n\nTITLE: Smart Home JSON Response Structure\nDESCRIPTION: Defines the required properties for different types of smart home control requests including commands, queries, answers, and clarification requests. Each response type has specific required fields and valid values.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/prompts/assistant.txt#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"action\": \"command\",\n  \"service\": \"light.turn_on\",\n  \"entity_id\": \"group.living_light\",\n  \"target\": \"living\",\n  \"comment\": \"I've turned on the lights in the living room to brighten up the space.\"\n}\n```\n\n----------------------------------------\n\nTITLE: Installing OpenCL Headers and Libraries for Android\nDESCRIPTION: Commands to clone and install OpenCL headers and ICD loader for Android NDK, required for building llama.cpp with OpenCL support for Android devices.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/OPENCL.md#2025-04-22_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nmkdir -p ~/dev/llm\ncd ~/dev/llm\n\ngit clone https://github.com/KhronosGroup/OpenCL-Headers && \\\ncd OpenCL-Headers && \\\ncp -r CL ~/android-sdk/ndk/26.3.11579264/toolchains/llvm/prebuilt/linux-x86_64/sysroot/usr/include\n\ncd ~/dev/llm\n\ngit clone https://github.com/KhronosGroup/OpenCL-ICD-Loader && \\\ncd OpenCL-ICD-Loader && \\\nmkdir build_ndk26 && cd build_ndk26 && \\\ncmake .. -G Ninja -DCMAKE_BUILD_TYPE=Release \\\n  -DCMAKE_TOOLCHAIN_FILE=$HOME/android-sdk/ndk/26.3.11579264/build/cmake/android.toolchain.cmake \\\n  -DOPENCL_ICD_LOADER_HEADERS_DIR=$HOME/android-sdk/ndk/26.3.11579264/toolchains/llvm/prebuilt/linux-x86_64/sysroot/usr/include \\\n  -DANDROID_ABI=arm64-v8a \\\n  -DANDROID_PLATFORM=24 \\\n  -DANDROID_STL=c++_shared && \\\nninja && \\\ncp libOpenCL.so ~/android-sdk/ndk/26.3.11579264/toolchains/llvm/prebuilt/linux-x86_64/sysroot/usr/lib/aarch64-linux-android\n```\n\n----------------------------------------\n\nTITLE: Running MobileVLM with Image Query\nDESCRIPTION: Example command for using MobileVLM to analyze an image of llamas with custom model paths.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/MobileVLM.md#2025-04-22_snippet_15\n\nLANGUAGE: sh\nCODE:\n```\n-m /path/to/ggml-model-q4_k.gguf \\\n    --mmproj /path/to/mmproj-model-f16.gguf \\\n    --image /path/to/many_llamas.jpeg\n    -p \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: <image>\\nWhat's that? ASSISTANT:\" \\\n```\n\n----------------------------------------\n\nTITLE: Running a Test with debug-test.sh in Bash\nDESCRIPTION: This command executes a specific test and returns a PASS or FAIL message. It uses the debug-test.sh script from the scripts folder with a regular expression pattern to identify the test to run.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/development/debugging-tests.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./scripts/debug-test.sh test-tokenizer\n```\n\n----------------------------------------\n\nTITLE: Basic Usage of llama-run Command with Granite3-moe Model\nDESCRIPTION: A minimal example showing how to invoke the llama-run command with a model called 'granite3-moe'.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/run/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nllama-run granite3-moe\n```\n\n----------------------------------------\n\nTITLE: Linking CUDA Libraries Based on Static/Dynamic Configuration\nDESCRIPTION: Configures the linking of CUDA libraries (cuBLAS, cuBLASLt, CUDA runtime) based on whether static or dynamic linking is preferred, with special handling for Windows platforms.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cuda/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nif (GGML_STATIC)\n    if (WIN32)\n        # As of 12.3.1 CUDA Toolkit for Windows does not offer a static cublas library\n        target_link_libraries(ggml-cuda PRIVATE CUDA::cudart_static CUDA::cublas CUDA::cublasLt)\n    else ()\n        target_link_libraries(ggml-cuda PRIVATE  CUDA::cudart_static CUDA::cublas_static CUDA::cublasLt_static)\n    endif()\nelse()\n    target_link_libraries(ggml-cuda PRIVATE CUDA::cudart CUDA::cublas CUDA::cublasLt)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Example Llama2.c Model Conversion Command\nDESCRIPTION: Example command demonstrating how to convert a TinyLlamas model to GGML format using the conversion tool with specific model paths.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/convert-llama2c-to-ggml/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./llama-convert-llama2c-to-ggml --copy-vocab-from-model llama-2-7b-chat.gguf.q2_K.bin --llama2c-model stories42M.bin --llama2c-output-model stories42M.gguf.bin\n```\n\n----------------------------------------\n\nTITLE: Starting LLM model server\nDESCRIPTION: Command to start a server instance for the LLM model on port 8020, making it available for text processing in server-based TTS.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/tts/README.md#2025-04-22_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\n./build/bin/llama-server -m ./models/outetts-0.2-0.5B-q8_0.gguf --port 8020\n```\n\n----------------------------------------\n\nTITLE: Configuring llama-gguf-hash Build with Dependencies in CMake\nDESCRIPTION: This CMake configuration sets up the llama-gguf-hash executable target, adds its required hash library dependencies (xxhash, sha1, and sha256), and links them together. It includes special handling for disabling warnings in third-party code for non-MSVC compilers and requires C++17 standard support.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/gguf-hash/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET llama-gguf-hash)\nadd_executable(${TARGET} gguf-hash.cpp)\ninstall(TARGETS ${TARGET} RUNTIME)\n\n# clibs dependencies\ninclude_directories(deps/)\n\nadd_library(xxhash OBJECT deps/xxhash/xxhash.c deps/xxhash/xxhash.h)\ntarget_link_libraries(${TARGET} PRIVATE xxhash)\n\nadd_library(sha1 OBJECT deps/sha1/sha1.c deps/sha1/sha1.h)\ntarget_link_libraries(${TARGET} PRIVATE sha1)\nif (NOT MSVC)\n    # disable warnings in 3rd party code\n    target_compile_options(sha1 PRIVATE -w)\nendif()\n\nadd_library(sha256 OBJECT deps/sha256/sha256.c deps/sha256/sha256.h)\ntarget_link_libraries(${TARGET} PRIVATE sha256)\n\ntarget_link_libraries(${TARGET} PRIVATE ggml ${CMAKE_THREAD_LIBS_INIT})\ntarget_compile_features(${TARGET} PRIVATE cxx_std_17)\n```\n\n----------------------------------------\n\nTITLE: Prompt Cache Usage Command\nDESCRIPTION: Command line option to specify a file for caching the model state after initial prompt to improve startup performance on subsequent runs.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_28\n\nLANGUAGE: bash\nCODE:\n```\n--prompt-cache FNAME\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for llama.cpp\nDESCRIPTION: This snippet lists the required Python packages and their version constraints for the llama.cpp project. It includes libraries for asynchronous HTTP requests, testing, machine learning model management, numerical operations, OpenAI API integration, monitoring, HTTP requests, and file downloading.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/tests/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\naiohttp~=3.9.3\npytest~=8.3.3\nhuggingface_hub~=0.23.2\nnumpy~=1.26.4\nopenai~=1.55.3\nprometheus-client~=0.20.0\nrequests~=2.32.3\nwget~=3.2\n```\n\n----------------------------------------\n\nTITLE: Using GBNF Grammar with llama-cli\nDESCRIPTION: Command line example showing how to use a GBNF grammar file with the llama-cli tool.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/grammars/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n./llama-cli -m <model> --grammar-file grammars/some-grammar.gbnf -p 'Some prompt'\n```\n\n----------------------------------------\n\nTITLE: Infinite Text Generation in Windows\nDESCRIPTION: Command to generate continuous text from a starting prompt in Windows, requiring Ctrl-C to stop\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_9\n\nLANGUAGE: powershell\nCODE:\n```\nllama-cli.exe -m models\\gemma-1.1-7b-it.Q4_K_M.gguf --ignore-eos -n -1\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests\nDESCRIPTION: Command to run all unit tests for the GGUF package with verbose output\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/gguf-py/README.md#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npython -m unittest discover ./gguf-py -v\n```\n\n----------------------------------------\n\nTITLE: Generating Visual Studio Solution for llama.cpp with SYCL (Mixed Compilers)\nDESCRIPTION: CMake command to generate a Visual Studio solution for llama.cpp with SYCL support using mixed compilers.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_23\n\nLANGUAGE: powershell\nCODE:\n```\ncmake -B build -G \"Visual Studio 17 2022\" -A x64 -DGGML_SYCL=ON -DCMAKE_BUILD_TYPE=Release \\\n      -DSYCL_INCLUDE_DIR=\"C:\\Program Files (x86)\\Intel\\oneAPI\\compiler\\latest\\include\" \\\n      -DSYCL_LIBRARY_DIR=\"C:\\Program Files (x86)\\Intel\\oneAPI\\compiler\\latest\\lib\"\n```\n\n----------------------------------------\n\nTITLE: Running llama.cpp with MUSA-enabled Docker Images\nDESCRIPTION: Commands to run the locally built MUSA-enabled Docker images. These examples assume mthreads is set as the default Docker runtime and use n-gpu-layers to enable GPU acceleration.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/docker.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -v /path/to/models:/models local/llama.cpp:full-musa --run -m /models/7B/ggml-model-q4_0.gguf -p \"Building a website can be done in 10 simple steps:\" -n 512 --n-gpu-layers 1\ndocker run -v /path/to/models:/models local/llama.cpp:light-musa -m /models/7B/ggml-model-q4_0.gguf -p \"Building a website can be done in 10 simple steps:\" -n 512 --n-gpu-layers 1\ndocker run -v /path/to/models:/models local/llama.cpp:server-musa -m /models/7B/ggml-model-q4_0.gguf --port 8000 --host 0.0.0.0 -n 512 --n-gpu-layers 1\n```\n\n----------------------------------------\n\nTITLE: Verifying a GGUF File Using the Faster XXH64 Hash\nDESCRIPTION: This command verifies a GGUF file against its manifest using the faster XXH64 hash algorithm. The --xxh64 flag specifies to use XXH64 instead of the default SHA-256.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/gguf-hash/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n./llama-gguf-hash --check test.gguf.manifest --xxh64 test.gguf\n```\n\n----------------------------------------\n\nTITLE: Running Inference on Multiple Devices with CANN Backend\nDESCRIPTION: Command to run inference using multiple devices with layer splitting mode in the CANN backend. It automatically distributes the model across available devices to process the same prompt about building a website.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/CANN.md#2025-04-22_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\n./build/bin/llama-cli -m path_to_model -p \"Building a website can be done in 10 simple steps:\" -n 400 -e -ngl 33 -sm layer\n```\n\n----------------------------------------\n\nTITLE: Usage Instructions for llama-export-lora Command-line Tool\nDESCRIPTION: Displays the command-line options for the llama-export-lora tool. It shows how to specify the base model, LORA adapters, number of threads, and output file.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/export-lora/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nusage: llama-export-lora [options]\n\noptions:\n  -m,    --model                  model path from which to load base model (default '')\n         --lora FNAME             path to LoRA adapter  (can be repeated to use multiple adapters)\n         --lora-scaled FNAME S    path to LoRA adapter with user defined scaling S  (can be repeated to use multiple adapters)\n  -t,    --threads N              number of threads to use during computation (default: 4)\n  -o,    --output FNAME           output file (default: 'ggml-lora-merged-f16.gguf')\n```\n\n----------------------------------------\n\nTITLE: Building MUSA-enabled Docker Images for llama.cpp Locally\nDESCRIPTION: Commands to build local Docker images with MUSA support for full, light, and server variants. Each command targets a specific build type from the MUSA Dockerfile.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/docker.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t local/llama.cpp:full-musa --target full -f .devops/musa.Dockerfile .\ndocker build -t local/llama.cpp:light-musa --target light -f .devops/musa.Dockerfile .\ndocker build -t local/llama.cpp:server-musa --target server -f .devops/musa.Dockerfile .\n```\n\n----------------------------------------\n\nTITLE: Configuring BLIS Runtime Environment Variables\nDESCRIPTION: Sets environment variables to control BLIS thread affinity and count for OpenMP execution.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/BLIS.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport GOMP_CPU_AFFINITY=\"0-19\"\nexport BLIS_NUM_THREADS=14\n```\n\n----------------------------------------\n\nTITLE: Starting voice decoder model server\nDESCRIPTION: Command to start a server instance for the voice decoder model on port 8021, with embeddings enabled and no pooling, for audio generation in server-based TTS.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/tts/README.md#2025-04-22_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\n./build/bin/llama-server -m ./models/wavtokenizer-large-75-f16.gguf --port 8021 --embeddings --pooling none\n```\n\n----------------------------------------\n\nTITLE: Compiling and Running llama-gguf-hash CLI Tool in Bash\nDESCRIPTION: This snippet demonstrates how to compile the llama-gguf-hash tool using CMake and make, and then run it with various options. It includes commands for building in debug mode, cleaning the build directory, and executing the tool with different hashing algorithms.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/gguf-hash/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncmake -B build -DCMAKE_BUILD_TYPE=Debug -DLLAMA_FATAL_WARNINGS=ON\nmake -C build clean\nmake -C build llama-gguf-hash VERBOSE=1\n./build/bin/llama-gguf-hash test.gguf\n./build/bin/llama-gguf-hash --xxh64 test.gguf\n./build/bin/llama-gguf-hash --sha1 test.gguf\n./build/bin/llama-gguf-hash --uuid test.gguf\n./build/bin/llama-gguf-hash --sha256 test.gguf\n```\n\n----------------------------------------\n\nTITLE: Building llama.cpp XCFramework for iOS integration\nDESCRIPTION: Command to build the llama.cpp library as an XCFramework that can be included in iOS projects. This must be run from the root directory of the llama.cpp project to generate the necessary framework files for iOS integration.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llama.swiftui/README.md#2025-04-22_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n$ ./build-xcframework.sh\n```\n\n----------------------------------------\n\nTITLE: Running llama.cpp with Docker and Vulkan\nDESCRIPTION: Commands to build and run llama.cpp in a Docker container with Vulkan support, passing through GPU devices.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_18\n\nLANGUAGE: sh\nCODE:\n```\n# Build the image\ndocker build -t llama-cpp-vulkan --target light -f .devops/vulkan.Dockerfile .\n\n# Then, use it:\ndocker run -it --rm -v \"$(pwd):/app:Z\" --device /dev/dri/renderD128:/dev/dri/renderD128 --device /dev/dri/card1:/dev/dri/card1 llama-cpp-vulkan -m \"/app/models/YOUR_MODEL_FILE\" -p \"Building a website can be done in 10 simple steps:\" -n 400 -e -ngl 33\n```\n\n----------------------------------------\n\nTITLE: Converting HuggingFace model to GGUF format\nDESCRIPTION: Python command to convert the OuteTTS HuggingFace model to GGUF format with 16-bit floating point precision for use with llama.cpp.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/tts/README.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n(venv) python convert_hf_to_gguf.py models/OuteTTS-0.2-500M \\\n    --outfile models/outetts-0.2-0.5B-f16.gguf --outtype f16\n```\n\n----------------------------------------\n\nTITLE: Defining JSON Schema with Pydantic in Python\nDESCRIPTION: Creates Pydantic models for QA pairs and summaries with validation rules. Includes pattern matching for key facts and minimum item requirements for question answers.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/grammars/README.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# pip install pydantic\nimport json\nfrom typing import Annotated, List\nfrom pydantic import BaseModel, Extra, Field\nclass QAPair(BaseModel):\n    class Config:\n        extra = 'allow'  # triggers additionalProperties: true in the JSON schema\n    question: str\n    concise_answer: str\n    justification: str\n\nclass Summary(BaseModel):\n    class Config:\n        extra = 'allow'\n    key_facts: List[Annotated[str, Field(pattern='- .{5,}')]]\n    question_answers: List[Annotated[List[QAPair], Field(min_items=5)]]\n\nprint(json.dumps(Summary.model_json_schema(), indent=2))\n```\n\n----------------------------------------\n\nTITLE: Compiling for Android\nDESCRIPTION: Commands for building the project for Android 64-bit architecture by creating a build directory and executing the build script.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/MobileVLM.md#2025-04-22_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\nmkdir examples/llava/android/build_64\ncd examples/llava/android/build_64\n../build_64.sh\n```\n\n----------------------------------------\n\nTITLE: CSV output from llama-bench\nDESCRIPTION: Example of CSV output generated by llama-bench. Each row represents a benchmark test with details including build information, hardware specs, model configuration, and performance metrics.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llama-bench/README.md#2025-04-22_snippet_7\n\nLANGUAGE: csv\nCODE:\n```\nbuild_commit,build_number,cuda,metal,gpu_blas,blas,cpu_info,gpu_info,model_filename,model_type,model_size,model_n_params,n_batch,n_threads,f16_kv,n_gpu_layers,main_gpu,mul_mat_q,tensor_split,n_prompt,n_gen,test_time,avg_ns,stddev_ns,avg_ts,stddev_ts\n\"3469684\",\"1275\",\"1\",\"0\",\"0\",\"1\",\"1\",\"13th Gen Intel(R) Core(TM) i9-13900K\",\"NVIDIA GeForce RTX 3090 Ti\",\"models/7B/ggml-model-q4_0.gguf\",\"llama 7B mostly Q4_0\",\"3825065984\",\"6738415616\",\"512\",\"16\",\"1\",\"99\",\"0\",\"1\",\"0.00\",\"512\",\"0\",\"2023-09-23T12:09:01Z\",\"212155977\",\"732372\",\"2413.341687\",\"8.305961\"\n\"3469684\",\"1275\",\"1\",\"0\",\"0\",\"1\",\"1\",\"13th Gen Intel(R) Core(TM) i9-13900K\",\"NVIDIA GeForce RTX 3090 Ti\",\"models/7B/ggml-model-q4_0.gguf\",\"llama 7B mostly Q4_0\",\"3825065984\",\"6738415616\",\"512\",\"16\",\"1\",\"99\",\"0\",\"1\",\"0.00\",\"0\",\"128\",\"2023-09-23T12:09:02Z\",\"969320879\",\"2728399\",\"132.052051\",\"0.371342\"\n```\n\n----------------------------------------\n\nTITLE: Compiling for NVIDIA Jetson Orin\nDESCRIPTION: Command for compiling the MobileVLM model with CUDA support specifically for the NVIDIA Jetson Orin platform, enabling hardware acceleration on this edge AI device.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/MobileVLM.md#2025-04-22_snippet_11\n\nLANGUAGE: sh\nCODE:\n```\nmake GGML_CUDA=1 CUDA_DOCKER_ARCH=sm_87 GGML_CUDA_F16=1 -j 32\n```\n\n----------------------------------------\n\nTITLE: GPU-accelerated Control Vector Generation Command\nDESCRIPTION: Command to generate a control vector with GPU acceleration, where -ngl 99 indicates using all available GPU layers.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/cvector-generator/README.md#2025-04-22_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n./cvector-generator -m ./llama-3.Q4_K_M.gguf -ngl 99\n```\n\n----------------------------------------\n\nTITLE: Installing llama.cpp with Nix using flake-enabled installation\nDESCRIPTION: Command to install llama.cpp using the Nix package manager with flake-enabled installations on Mac and Linux systems.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/install.md#2025-04-22_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nnix profile install nixpkgs#llama-cpp\n```\n\n----------------------------------------\n\nTITLE: Running MUSA CI Inside Docker Container\nDESCRIPTION: Commands to execute inside the Docker container, including installing dependencies, configuring git for the workspace directory, and running the MUSA CI workflow.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ci/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\napt update -y && apt install -y bc cmake ccache git python3.10-venv time unzip wget\ngit config --global --add safe.directory /ws\nGG_BUILD_MUSA=1 bash ./ci/run.sh /ci-results /ci-cache\n```\n\n----------------------------------------\n\nTITLE: Running the Python TTS client script\nDESCRIPTION: Command to run the Python TTS client script that connects to both server instances (LLM and voice decoder) to generate speech from text. The script generates a WAV file as output.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/tts/README.md#2025-04-22_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\n(venv) python ./examples/tts/tts-outetts.py http://localhost:8020 http://localhost:8021 \"Hello world\"\n```\n\n----------------------------------------\n\nTITLE: Running llama-bench with JSONL output format\nDESCRIPTION: Command to run the llama-bench tool with JSON Lines (JSONL) output format. This produces benchmark results with each test as a separate line of JSON, suitable for streaming and log processing.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llama-bench/README.md#2025-04-22_snippet_10\n\nLANGUAGE: sh\nCODE:\n```\n$ ./llama-bench -o jsonl\n```\n\n----------------------------------------\n\nTITLE: Compiling and Running Passkey Retrieval Task for llama.cpp\nDESCRIPTION: This command compiles the project and runs the passkey retrieval task using a specified model file. It includes a parameter to add 250 junk tokens to the context.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/passkey/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmake -j && ./llama-passkey -m ./models/llama-7b-v2/ggml-model-f16.gguf --junk 250\n```\n\n----------------------------------------\n\nTITLE: Building llama.cpp with SYCL on Windows (CMake Presets)\nDESCRIPTION: CMake commands to build llama.cpp with SYCL support on Windows using CMake presets.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_21\n\nLANGUAGE: sh\nCODE:\n```\ncmake --preset x64-windows-sycl-release\ncmake --build build-x64-windows-sycl-release -j --target llama-cli\n\ncmake -DGGML_SYCL_F16=ON --preset x64-windows-sycl-release\ncmake --build build-x64-windows-sycl-release -j --target llama-cli\n\ncmake --preset x64-windows-sycl-debug\ncmake --build build-x64-windows-sycl-debug -j --target llama-cli\n```\n\n----------------------------------------\n\nTITLE: Building llama.cpp with SYCL Support Using Docker\nDESCRIPTION: Docker command to build a container image for llama.cpp with SYCL support optimized for FP16 computation on Intel GPUs.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n# Using FP16\ndocker build -t llama-cpp-sycl --build-arg=\"GGML_SYCL_F16=ON\" --target light -f .devops/intel.Dockerfile .\n```\n\n----------------------------------------\n\nTITLE: Running Converted GGML Model Command\nDESCRIPTION: Example command showing how to use the converted GGML model with llama-cli, including prompt generation parameters.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/convert-llama2c-to-ggml/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n./llama-cli -m stories42M.gguf.bin -p \"One day, Lily met a Shoggoth\" -n 500 -c 256\n```\n\n----------------------------------------\n\nTITLE: Building llama.cpp with SYCL on Windows (CMake FP32)\nDESCRIPTION: CMake commands to build llama.cpp with SYCL support on Windows using FP32.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_19\n\nLANGUAGE: sh\nCODE:\n```\n@call \"C:\\Program Files (x86)\\Intel\\oneAPI\\setvars.bat\" intel64 --force\n\ncmake -B build -G \"Ninja\" -DGGML_SYCL=ON -DCMAKE_C_COMPILER=cl -DCMAKE_CXX_COMPILER=icx  -DCMAKE_BUILD_TYPE=Release\n\ncmake --build build --config Release -j\n```\n\n----------------------------------------\n\nTITLE: SQL output from llama-bench\nDESCRIPTION: Example of SQL output generated by llama-bench. The output includes a CREATE TABLE statement defining the schema and INSERT statements for each benchmark test result, ready to be imported into a SQLite database.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llama-bench/README.md#2025-04-22_snippet_13\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE IF NOT EXISTS test (\n  build_commit TEXT,\n  build_number INTEGER,\n  cuda INTEGER,\n  metal INTEGER,\n  gpu_blas INTEGER,\n  blas INTEGER,\n  cpu_info TEXT,\n  gpu_info TEXT,\n  model_filename TEXT,\n  model_type TEXT,\n  model_size INTEGER,\n  model_n_params INTEGER,\n  n_batch INTEGER,\n  n_threads INTEGER,\n  f16_kv INTEGER,\n  n_gpu_layers INTEGER,\n  main_gpu INTEGER,\n  mul_mat_q INTEGER,\n  tensor_split TEXT,\n  n_prompt INTEGER,\n  n_gen INTEGER,\n  test_time TEXT,\n  avg_ns INTEGER,\n  stddev_ns INTEGER,\n  avg_ts REAL,\n  stddev_ts REAL\n);\n\nINSERT INTO test (build_commit, build_number, cuda, metal, gpu_blas, blas, cpu_info, gpu_info, model_filename, model_type, model_size, model_n_params, n_batch, n_threads, f16_kv, n_gpu_layers, main_gpu, mul_mat_q, tensor_split, n_prompt, n_gen, test_time, avg_ns, stddev_ns, avg_ts, stddev_ts) VALUES ('3469684', '1275', '1', '0', '0', '1', '1', '13th Gen Intel(R) Core(TM) i9-13900K', 'NVIDIA GeForce RTX 3090 Ti', 'models/7B/ggml-model-q4_0.gguf', 'llama 7B mostly Q4_0', '3825065984', '6738415616', '512', '16', '1', '99', '0', '1', '0.00', '512', '0', '2023-09-23T12:10:30Z', '212693772', '743623', '2407.240204', '8.409634');\nINSERT INTO test (build_commit, build_number, cuda, metal, gpu_blas, blas, cpu_info, gpu_info, model_filename, model_type, model_size, model_n_params, n_batch, n_threads, f16_kv, n_gpu_layers, main_gpu, mul_mat_q, tensor_split, n_prompt, n_gen, test_time, avg_ns, stddev_ns, avg_ts, stddev_ts) VALUES ('3469684', '1275', '1', '0', '0', '1', '1', '13th Gen Intel(R) Core(TM) i9-13900K', 'NVIDIA GeForce RTX 3090 Ti', 'models/7B/ggml-model-q4_0.gguf', 'llama 7B mostly Q4_0', '3825065984', '6738415616', '512', '16', '1', '99', '0', '1', '0.00', '0', '128', '2023-09-23T12:10:31Z', '977925003', '4037361', '130.891159', '0.537692');\n```\n\n----------------------------------------\n\nTITLE: Running MobileVLM on Android (Book Example)\nDESCRIPTION: Example command for running the MobileVLM model on Android to identify the author of a book in an image. This demonstrates the model's visual question answering capability.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/MobileVLM.md#2025-04-22_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\n/data/local/tmp/llama-mtmd-cli \\\n    -m /data/local/tmp/ggml-model-q4_k.gguf \\\n    --mmproj /data/local/tmp/mmproj-model-f16.gguf \\\n    -t 4 \\\n    --image /data/local/tmp/demo.jpg \\\n    -p \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: <image>\\nWho is the author of this book? \\nAnswer the question using a single word or phrase. ASSISTANT:\"\n```\n\n----------------------------------------\n\nTITLE: Downloading Model Files on Android via Termux\nDESCRIPTION: Command to download a model file from a URL to the home directory on Android using curl in Termux.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/android.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ curl -L {model-url} -o ~/{model}.gguf\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Llama.cpp Server Tests\nDESCRIPTION: Command to install the required Python dependencies for running the server tests.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/tests/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Debugging a Test in GDB with debug-test.sh\nDESCRIPTION: These commands demonstrate how to run a test in GDB debugger mode. The -g flag enables GDB, and once in the debugger prompt, a breakpoint can be set on the main function.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/development/debugging-tests.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./scripts/debug-test.sh -g test-tokenizer\n\n# Once in the debugger, i.e. at the chevrons prompt, setting a breakpoint could be as follows:\n>>> b main\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake Project Settings for GGML\nDESCRIPTION: Sets up the basic project configuration for GGML, including minimum CMake version, project name, and default build settings.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.14) # for add_link_options and implicit target directories.\nproject(\"ggml\" C CXX)\ninclude(CheckIncludeFileCXX)\n\nset(CMAKE_EXPORT_COMPILE_COMMANDS ON)\n\nif (NOT XCODE AND NOT MSVC AND NOT CMAKE_BUILD_TYPE)\n    set(CMAKE_BUILD_TYPE Release CACHE STRING \"Build type\" FORCE)\n    set_property(CACHE CMAKE_BUILD_TYPE PROPERTY STRINGS \"Debug\" \"Release\" \"MinSizeRel\" \"RelWithDebInfo\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring MUSA-specific Compilation Definitions for llama.cpp\nDESCRIPTION: Sets up various compilation definitions for MUSA GPU support, including CUDA compatibility, memory management, and precision options. Links against MUSA libraries based on static or dynamic build configuration.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-musa/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\n    ggml_add_backend_library(ggml-musa\n                             ${GGML_HEADERS_MUSA}\n                             ${GGML_SOURCES_MUSA}\n                            )\n\n    # TODO: do not use CUDA definitions for MUSA\n    target_compile_definitions(ggml PUBLIC GGML_USE_CUDA)\n\n    add_compile_definitions(GGML_USE_MUSA)\n    add_compile_definitions(GGML_CUDA_PEER_MAX_BATCH_SIZE=${GGML_CUDA_PEER_MAX_BATCH_SIZE})\n\n    if (GGML_CUDA_FORCE_MMQ)\n        add_compile_definitions(GGML_CUDA_FORCE_MMQ)\n    endif()\n\n    if (GGML_CUDA_FORCE_CUBLAS)\n        add_compile_definitions(GGML_CUDA_FORCE_CUBLAS)\n    endif()\n\n    if (GGML_CUDA_NO_VMM)\n        add_compile_definitions(GGML_CUDA_NO_VMM)\n    endif()\n\n    if (NOT GGML_CUDA_FA)\n        add_compile_definitions(GGML_CUDA_NO_FA)\n    endif()\n\n    if (GGML_CUDA_F16 OR GGML_CUDA_DMMV_F16)\n        add_compile_definitions(GGML_CUDA_F16)\n    endif()\n\n    if (GGML_CUDA_NO_PEER_COPY)\n        add_compile_definitions(GGML_CUDA_NO_PEER_COPY)\n    endif()\n\n    if (GGML_STATIC)\n        target_link_libraries(ggml-musa PRIVATE MUSA::musart_static MUSA::mublas_static)\n    else()\n        target_link_libraries(ggml-musa PRIVATE MUSA::musart MUSA::mublas)\n    endif()\n\n    if (GGML_CUDA_NO_VMM)\n        # No VMM requested, no need to link directly with the musa driver lib (libmusa.so)\n    else()\n        target_link_libraries(ggml-musa PRIVATE MUSA::musa_driver)\n    endif()\nelse()\n    message(FATAL_ERROR \"MUSA Toolkit not found\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Multi-line Prompt Formatting Example\nDESCRIPTION: Example showing how to format prompts with multiple lines by escaping newline characters for use with the control vector generator.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/cvector-generator/README.md#2025-04-22_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\n<|im_start|>system\\nAct like a person who is extremely happy.<|im_end|>\n<|im_start|>system\\nYou are in a very good mood today<|im_end|>\n```\n\n----------------------------------------\n\nTITLE: Building llama.cpp with SYCL on Windows (CMake FP16)\nDESCRIPTION: CMake commands to build llama.cpp with SYCL support on Windows using FP16.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_20\n\nLANGUAGE: sh\nCODE:\n```\n@call \"C:\\Program Files (x86)\\Intel\\oneAPI\\setvars.bat\" intel64 --force\n\ncmake -B build -G \"Ninja\" -DGGML_SYCL=ON -DCMAKE_C_COMPILER=cl -DCMAKE_CXX_COMPILER=icx  -DCMAKE_BUILD_TYPE=Release -DGGML_SYCL_F16=ON\n\ncmake --build build --config Release -j\n```\n\n----------------------------------------\n\nTITLE: Quantizing Models for Adreno GPU with Pure Q4_0\nDESCRIPTION: Command for preparing models with Q4_0 quantization optimized for Adreno GPUs using the --pure flag to achieve best performance.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/OPENCL.md#2025-04-22_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n./llama-quantize --pure ggml-model-qwen2.5-3b-f16.gguf ggml-model-qwen-3b-Q4_0.gguf Q4_0\n```\n\n----------------------------------------\n\nTITLE: Chat Template Format\nDESCRIPTION: Template showing the structure of a chat conversation between a human user (placeholder [[USER_NAME]]) and an AI assistant (placeholder [[AI_NAME]]). Includes formatting for turn-taking, greetings, and Q&A.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/prompts/chat-with-vicuna-v1.txt#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nA chat between a curious human (\"[[USER_NAME]]\") and an artificial intelligence assistant (\"[[AI_NAME]]\"). The assistant gives helpful, detailed, and polite answers to the human's questions.\n\n[[USER_NAME]]: Hello, [[AI_NAME]].\n[[AI_NAME]]: Hello. How may I help you today?\n[[USER_NAME]]: Please tell me the largest city in Europe.\n[[AI_NAME]]: Sure. The largest city in Europe is Moscow, the capital of Russia.\n[[USER_NAME]]:\n```\n\n----------------------------------------\n\nTITLE: Setting Up Build Context for Testing\nDESCRIPTION: This bash command creates a fresh build directory for debugging tests. It removes any existing build-ci-debug folder, creates a new one, and changes into it to prepare for compilation.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/development/debugging-tests.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nrm -rf build-ci-debug && mkdir build-ci-debug && cd build-ci-debug\n```\n\n----------------------------------------\n\nTITLE: Running GDB on a Specific Test\nDESCRIPTION: This command demonstrates how to run GDB with a specific test binary and model file as arguments. The command is constructed based on the output from the previous ctest command.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/development/debugging-tests.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ngdb --args ${Test Binary} ${Test GGUF Model}\n```\n\n----------------------------------------\n\nTITLE: Generating Chat Templates Listing in Bash for llama.cpp\nDESCRIPTION: This bash command generates a table of available Jinja templates for different AI models. It runs the test-chat binary from the build directory against all Jinja templates in the minja build tests directory, while suppressing error output.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/function-calling.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./build/bin/test-chat ../minja/build/tests/*.jinja 2>/dev/null\n```\n\n----------------------------------------\n\nTITLE: Configuring llama Library with CMake\nDESCRIPTION: Defines the llama library with its source files, includes, linking requirements, and compilation features. When building as a shared library, it sets position-independent code and appropriate compilation definitions.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/src/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nllama_add_compile_flags()\n\n#\n# libraries\n#\n\n# llama\n\nadd_library(llama\n            ../include/llama.h\n            llama.cpp\n            llama-adapter.cpp\n            llama-arch.cpp\n            llama-batch.cpp\n            llama-chat.cpp\n            llama-context.cpp\n            llama-grammar.cpp\n            llama-graph.cpp\n            llama-hparams.cpp\n            llama-impl.cpp\n            llama-io.cpp\n            llama-kv-cache.cpp\n            llama-memory.cpp\n            llama-mmap.cpp\n            llama-model-loader.cpp\n            llama-model.cpp\n            llama-quant.cpp\n            llama-sampling.cpp\n            llama-vocab.cpp\n            unicode-data.cpp\n            unicode.cpp\n            unicode.h\n            )\n\ntarget_include_directories(llama PUBLIC . ../include)\ntarget_compile_features   (llama PUBLIC cxx_std_17) # don't bump\n\ntarget_link_libraries(llama PUBLIC ggml)\n\nif (BUILD_SHARED_LIBS)\n    set_target_properties(llama PROPERTIES POSITION_INDEPENDENT_CODE ON)\n    target_compile_definitions(llama PRIVATE LLAMA_BUILD)\n    target_compile_definitions(llama PUBLIC  LLAMA_SHARED)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Variable Naming Convention Examples in C++\nDESCRIPTION: Examples showing proper variable naming using snake_case and organizing names to optimize for longest common prefix. This demonstrates how to structure related variable names in the project.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/CONTRIBUTING.md#2025-04-22_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\n// not OK\nint small_number;\nint big_number;\n\n// OK\nint number_small;\nint number_big;\n```\n\n----------------------------------------\n\nTITLE: MUSA Compute Capability Configuration\nDESCRIPTION: CMake commands to specify custom compute capability for MUSA GPU compilation, targeting specific architectures like MTT S80.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\ncmake -B build -DGGML_MUSA=ON -DMUSA_ARCHITECTURES=\"21\"\ncmake --build build --config Release\n```\n\n----------------------------------------\n\nTITLE: Installing Development Tools\nDESCRIPTION: Commands to install essential development packages and tools\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/CUDA-FEDORA.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsudo dnf distro-sync\nsudo dnf install vim-default-editor --allowerasing\nsudo dnf install @c-development @development-tools cmake\n```\n\n----------------------------------------\n\nTITLE: Setting up Build Dependencies and Standards for GGML\nDESCRIPTION: This snippet configures required dependencies, C/C++ standards, and build settings for the GGML library. It sets C11 and C++17 as the required standards and includes necessary packages like Threads.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/CMakeLists.txt#2025-04-22_snippet_16\n\nLANGUAGE: CMake\nCODE:\n```\n#\n# dependencies\n#\n\nset(CMAKE_C_STANDARD 11)\nset(CMAKE_C_STANDARD_REQUIRED true)\n\nset(CMAKE_CXX_STANDARD 17)\nset(CMAKE_CXX_STANDARD_REQUIRED true)\n\nset(THREADS_PREFER_PTHREAD_FLAG ON)\n\nfind_package(Threads REQUIRED)\n\ninclude(GNUInstallDirs)\n```\n\n----------------------------------------\n\nTITLE: LLaMA 2 70B Quantization Performance Table\nDESCRIPTION: Markdown table showing model size and perplexity metrics for different quantization methods applied to LLaMA 2 70B model, including delta comparisons to FP16 baseline.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/perplexity/README.md#2025-04-22_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n| Quantization | Model size (GiB) | Perplexity | Delta to fp16 |\n|--------------|------------------|------------|---------------|\n| Q4_0         | 36.20            | 3.5550     | 3.61%         |\n| Q4_1         | 40.20            | 3.5125     | 2.37%         |\n```\n\n----------------------------------------\n\nTITLE: Configuring CUDA Environment\nDESCRIPTION: Commands to set up CUDA environment variables and path configuration\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/CUDA-FEDORA.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nsudo sh -c 'echo \"export PATH=\\$PATH:/usr/local/cuda/bin\" >> /etc/profile.d/cuda.sh'\nsudo chmod +x /etc/profile.d/cuda.sh\nsource /etc/profile.d/cuda.sh\n```\n\n----------------------------------------\n\nTITLE: Executing llama.cpp with SYCL on Linux (Single Device)\nDESCRIPTION: Shell command to run llama.cpp inference using a single SYCL device (device 0) on Linux.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_11\n\nLANGUAGE: sh\nCODE:\n```\n./examples/sycl/run-llama2.sh 0\n```\n\n----------------------------------------\n\nTITLE: Running Slow Tests with Llama.cpp Server\nDESCRIPTION: Command to run slow tests that will download multiple models. The LLAMA_CACHE environment variable can be set to avoid re-downloading models.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/tests/README.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nSLOW_TESTS=1 ./tests.sh\n```\n\n----------------------------------------\n\nTITLE: Installing llama.cpp with Homebrew on Mac and Linux\nDESCRIPTION: Command to install llama.cpp using the Homebrew package manager on Mac and Linux systems. The formula is automatically updated with new llama.cpp releases.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/install.md#2025-04-22_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nbrew install llama.cpp\n```\n\n----------------------------------------\n\nTITLE: Setting up llama-server CMake target with source files and dependencies\nDESCRIPTION: Defines the llama-server target with its source files, includes required directories, and configures compiler settings. The script handles platform-specific requirements like MinGW Windows version definition and sets up asset file conversion from HTML files to C++ headers.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET llama-server)\n\noption(LLAMA_SERVER_SSL \"Build SSL support for the server\" OFF)\n\ninclude_directories(${CMAKE_CURRENT_SOURCE_DIR} ${CMAKE_CURRENT_BINARY_DIR})\n\nif (MINGW)\n    # fix: https://github.com/ggml-org/llama.cpp/actions/runs/9651004652/job/26617901362?pr=8006\n    add_compile_definitions(_WIN32_WINNT=${GGML_WIN_VER})\nendif()\n\nset(TARGET_SRCS\n    server.cpp\n    utils.hpp\n    httplib.h\n)\nset(PUBLIC_ASSETS\n    index.html.gz\n    loading.html\n)\n\nforeach(asset ${PUBLIC_ASSETS})\n    set(input \"${CMAKE_CURRENT_SOURCE_DIR}/public/${asset}\")\n    set(output \"${CMAKE_CURRENT_BINARY_DIR}/${asset}.hpp\")\n    list(APPEND TARGET_SRCS ${output})\n    add_custom_command(\n        DEPENDS \"${input}\"\n        OUTPUT \"${output}\"\n        COMMAND \"${CMAKE_COMMAND}\" \"-DINPUT=${input}\" \"-DOUTPUT=${output}\" -P \"${PROJECT_SOURCE_DIR}/scripts/xxd.cmake\"\n    )\n    set_source_files_properties(${output} PROPERTIES GENERATED TRUE)\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Converting Image Encoder for MobileVLM_V2\nDESCRIPTION: Command for converting the CLIP image encoder to GGUF format with LDPV2 projector type specifically for the MobileVLM_V2 model variant.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/MobileVLM.md#2025-04-22_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\npython ./examples/llava/convert_image_encoder_to_gguf.py \\\n    -m path/to/clip-vit-large-patch14-336 \\\n    --llava-projector path/to/MobileVLM-1.7B_V2/llava.projector \\\n    --output-dir path/to/MobileVLM-1.7B_V2 \\\n    --projector-type ldpv2\n```\n\n----------------------------------------\n\nTITLE: Configuring Common Tests for llama.cpp\nDESCRIPTION: Defines test targets that are common across platforms, including log testing, chat template testing, and GGUF format testing. Some tests are conditionally included based on platform or backend configuration.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/tests/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nllama_target_and_test(test-log.cpp)\nllama_target_and_test(test-chat-template.cpp)\n\n# this fails on windows (github hosted runner) due to curl DLL not found (exit code 0xc0000135)\nif (NOT WIN32)\n    llama_target_and_test(test-arg-parser.cpp)\nendif()\n\n# llama_target_and_test(test-opt.cpp) # SLOW\nllama_target_and_test(test-gguf.cpp)\nllama_target_and_test(test-backend-ops.cpp)\n\nllama_target_and_test(test-model-load-cancel.cpp  LABEL \"model\")\nllama_target_and_test(test-autorelease.cpp        LABEL \"model\")\n\nif (NOT GGML_BACKEND_DL)\n    # these tests use the backends directly and cannot be built with dynamic loading\n    llama_target_and_test(test-barrier.cpp)\n    llama_target_and_test(test-quantize-fns.cpp)\n    llama_target_and_test(test-quantize-perf.cpp)\n    llama_target_and_test(test-rope.cpp)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Verifying a GGUF File Using Default Settings (SHA-256)\nDESCRIPTION: This command verifies a GGUF file against its manifest using the default settings, which checks the highest security strength hash (SHA-256) for all tensors and the overall file.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/gguf-hash/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n./llama-gguf-hash --check test.gguf.manifest test.gguf\n```\n\n----------------------------------------\n\nTITLE: Converting PyTorch checkpoint to HuggingFace format\nDESCRIPTION: Python command to convert the WavTokenizer PyTorch checkpoint (.ckpt) file to HuggingFace format, which is an intermediate step before GGUF conversion.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/tts/README.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n(venv) python examples/tts/convert_pt_to_hf.py \\\n    models/WavTokenizer-large-speech-75token/wavtokenizer_large_speech_320_24k.ckpt\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for llama.cpp CANN\nDESCRIPTION: Command to build a Docker image containing llama.cpp with CANN support\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/CANN.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndocker build -t llama-cpp-cann -f .devops/llama-cli-cann.Dockerfile .\n```\n\n----------------------------------------\n\nTITLE: Verifying a GGUF Manifest Using All Hash Algorithms\nDESCRIPTION: This command verifies all hash entries in a GGUF manifest file against the manifest itself using the --all flag. It checks each hash type (XXH64, SHA-1, SHA-256) for all tensors and the overall file.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/gguf-hash/README.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n./llama-gguf-hash --check test.gguf.manifest --all test.gguf.manifest\n```\n\n----------------------------------------\n\nTITLE: Configuring llama-cli Executable Build with CMake\nDESCRIPTION: Sets up the llama-cli executable target with required library dependencies and C++17 standard. Links against common and llama libraries along with system thread libraries. Configures installation of the runtime binary.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET llama-cli)\nadd_executable(${TARGET} main.cpp)\ninstall(TARGETS ${TARGET} RUNTIME)\ntarget_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})\ntarget_compile_features(${TARGET} PRIVATE cxx_std_17)\n```\n\n----------------------------------------\n\nTITLE: Embedding Metal Library for GGML in CMake\nDESCRIPTION: Sets up the process for embedding the Metal library into the binary when GGML_METAL_EMBED_LIBRARY is enabled.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-metal/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nif (GGML_METAL_EMBED_LIBRARY)\n    enable_language(ASM)\n\n    add_compile_definitions(GGML_METAL_EMBED_LIBRARY)\n\n    set(METALLIB_SOURCE \"${CMAKE_CURRENT_SOURCE_DIR}/ggml-metal.metal\")\n    set(METALLIB_IMPL   \"${CMAKE_CURRENT_SOURCE_DIR}/ggml-metal-impl.h\")\n\n    file(MAKE_DIRECTORY \"${CMAKE_BINARY_DIR}/autogenerated\")\n\n    # merge ggml-common.h and ggml-metal.metal into a single file\n    set(METALLIB_EMBED_ASM        \"${CMAKE_BINARY_DIR}/autogenerated/ggml-metal-embed.s\")\n    set(METALLIB_SOURCE_EMBED     \"${CMAKE_BINARY_DIR}/autogenerated/ggml-metal-embed.metal\")\n    set(METALLIB_SOURCE_EMBED_TMP \"${CMAKE_BINARY_DIR}/autogenerated/ggml-metal-embed.metal.tmp\")\n\n    add_custom_command(\n        OUTPUT ${METALLIB_EMBED_ASM}\n        COMMAND echo \"Embedding Metal library\"\n        COMMAND sed -e '/__embed_ggml-common.h__/r         ${METALLIB_COMMON}' -e '/__embed_ggml-common.h__/d'         < ${METALLIB_SOURCE}           > ${METALLIB_SOURCE_EMBED_TMP}\n        COMMAND sed -e '/\\#include \\\"ggml-metal-impl.h\\\"/r ${METALLIB_IMPL}'   -e '/\\#include \\\"ggml-metal-impl.h\\\"/d' < ${METALLIB_SOURCE_EMBED_TMP} > ${METALLIB_SOURCE_EMBED}\n        COMMAND echo \".section __DATA,__ggml_metallib\"          >  ${METALLIB_EMBED_ASM}\n        COMMAND echo \".globl _ggml_metallib_start\"              >> ${METALLIB_EMBED_ASM}\n        COMMAND echo \"_ggml_metallib_start:\"                    >> ${METALLIB_EMBED_ASM}\n        COMMAND echo \".incbin \\\"${METALLIB_SOURCE_EMBED}\\\"\" >> ${METALLIB_EMBED_ASM}\n        COMMAND echo \".globl _ggml_metallib_end\"                >> ${METALLIB_EMBED_ASM}\n        COMMAND echo \"_ggml_metallib_end:\"                      >> ${METALLIB_EMBED_ASM}\n        DEPENDS ../ggml-common.h ggml-metal.metal ggml-metal-impl.h\n        COMMENT \"Generate assembly for embedded Metal library\"\n    )\n\n    target_sources(ggml-metal PRIVATE ${METALLIB_EMBED_ASM})\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake Build for llama-quantize Executable\nDESCRIPTION: Defines and configures the llama-quantize executable target in CMake, setting its source files, linked libraries, include directories, and C++ standard. The target depends on the common and llama libraries, requires C++17, and will be installed during the install phase.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/quantize/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET llama-quantize)\nadd_executable(${TARGET} quantize.cpp)\ninstall(TARGETS ${TARGET} RUNTIME)\ntarget_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})\ntarget_include_directories(${TARGET} PRIVATE ../../common)\ntarget_compile_features(${TARGET} PRIVATE cxx_std_17)\n```\n\n----------------------------------------\n\nTITLE: Listing SYCL Devices for llama.cpp\nDESCRIPTION: Command to list available SYCL devices using the llama-ls-sycl-device utility.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_9\n\nLANGUAGE: sh\nCODE:\n```\n./build/bin/llama-ls-sycl-device\n```\n\n----------------------------------------\n\nTITLE: Running llama.cpp CANN Docker Container\nDESCRIPTION: Command to run the Docker container with proper device mappings and volume mounts for Ascend NPU access\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/CANN.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ndocker run --name llamacpp --device /dev/davinci0  --device /dev/davinci_manager --device /dev/devmm_svm --device /dev/hisi_hdc -v /usr/local/dcmi:/usr/local/dcmi -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi -v /usr/local/Ascend/driver/lib64/:/usr/local/Ascend/driver/lib64/ -v /usr/local/Ascend/driver/version.info:/usr/local/Ascend/driver/version.info -v /PATH_TO_YOUR_MODELS/:/app/models -it llama-cpp-cann -m /app/models/MODEL_PATH -ngl 32 -p \"Building a website can be done in 10 simple steps:\"\n```\n\n----------------------------------------\n\nTITLE: Cloning llama.cpp Repository\nDESCRIPTION: Commands to clone the llama.cpp repository from GitHub and navigate to the project directory.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/ggml-org/llama.cpp\ncd llama.cpp\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCL and Python Dependencies for GGML in CMake\nDESCRIPTION: Sets up the required packages (OpenCL and Python3) and defines the target name for the OpenCL backend library.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-opencl/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfind_package(OpenCL REQUIRED)\nfind_package(Python3 REQUIRED)\n\nset(TARGET_NAME ggml-opencl)\n```\n\n----------------------------------------\n\nTITLE: Adding CUDA Repository\nDESCRIPTION: Commands to add and update the NVIDIA CUDA repository\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/CUDA-FEDORA.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nsudo dnf config-manager addrepo --from-repofile=https://developer.download.nvidia.com/compute/cuda/repos/fedora41/x86_64/cuda-fedora41.repo\nsudo dnf distro-sync\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies for llama.cpp\nDESCRIPTION: A requirements.txt file specifying the Python packages required by the llama.cpp project. It includes numpy 1.26.4 or compatible, PySide6 6.9.0 or compatible, and gguf version 0.16.0 or newer.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/requirements/requirements-gguf_editor_gui.txt#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nnumpy~=1.26.4\nPySide6~=6.9.0\ngguf>=0.16.0\n```\n\n----------------------------------------\n\nTITLE: Adding OpenCL Backend Library in GGML CMake Configuration\nDESCRIPTION: Adds the OpenCL backend library to the GGML project, specifying source files and linking OpenCL libraries.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-opencl/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nggml_add_backend_library(${TARGET_NAME}\n                         ggml-opencl.cpp\n                         ../../include/ggml-opencl.h)\ntarget_link_libraries(${TARGET_NAME} PRIVATE ${OpenCL_LIBRARIES})\ntarget_include_directories(${TARGET_NAME} PRIVATE ${OpenCL_INCLUDE_DIRS})\n```\n\n----------------------------------------\n\nTITLE: Cloning the WavTokenizer voice decoder model\nDESCRIPTION: Commands to clone the WavTokenizer voice decoder model from HuggingFace, setting up Git LFS to properly download the large model files.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/tts/README.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\npushd models\ngit clone --branch main --single-branch --depth 1 https://huggingface.co/novateur/WavTokenizer-large-speech-75token\ncd WavTokenizer-large-speech-75token && git lfs install && git lfs pull\npopd\n```\n\n----------------------------------------\n\nTITLE: Running llama.cpp on Android via Termux\nDESCRIPTION: Command to execute the llama-cli binary on Android with a model file, context size, and prompt parameters.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/android.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ ./build/bin/llama-cli -m ~/{model}.gguf -c {context-size} -p \"{your-prompt}\"\n```\n\n----------------------------------------\n\nTITLE: Creating Fedora Toolbox Container\nDESCRIPTION: Commands to create and enter a Fedora 41 toolbox container for CUDA development\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/CUDA-FEDORA.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ntoolbox create --image registry.fedoraproject.org/fedora-toolbox:41 --container fedora-toolbox-41-cuda\ntoolbox enter --container fedora-toolbox-41-cuda\n```\n\n----------------------------------------\n\nTITLE: Configuring RPC Server Build with CMake\nDESCRIPTION: Sets up the build configuration for an RPC server executable. It defines the target name, specifies the source file, links against the ggml library, and sets the C++ standard to C++17.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/rpc/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET rpc-server)\nadd_executable(${TARGET} rpc-server.cpp)\ntarget_link_libraries(${TARGET} PRIVATE ggml)\ntarget_compile_features(${TARGET} PRIVATE cxx_std_17)\n```\n\n----------------------------------------\n\nTITLE: Using llama-llava-clip-quantize-cli to Quantize CLIP Visual Projector Models\nDESCRIPTION: This command demonstrates the basic syntax for quantizing a CLIP visual projector model using the llama-llava-clip-quantize-cli tool. It takes paths to the input and output model files along with a quantization type identifier.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llava/README-quantize.md#2025-04-22_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n./bin/llama-llava-clip-quantize-cli /path/to/ggml-model-f32.gguf /path/to/ggml-model-quantized.gguf <type>\n```\n\n----------------------------------------\n\nTITLE: Configuring Installation of GGML Headers and Libraries\nDESCRIPTION: This section sets up the installation configuration for GGML, including public headers, library files, and package information. It defines which files should be installed and where they should be placed.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/CMakeLists.txt#2025-04-22_snippet_18\n\nLANGUAGE: CMake\nCODE:\n```\n#\n# install\n#\n\ninclude(CMakePackageConfigHelpers)\n\n# all public headers\nset(GGML_PUBLIC_HEADERS\n    include/ggml.h\n    include/ggml-cpu.h\n    include/ggml-alloc.h\n    include/ggml-backend.h\n    include/ggml-blas.h\n    include/ggml-cann.h\n    include/ggml-cpp.h\n    include/ggml-cuda.h\n    include/ggml-kompute.h\n    include/ggml-opt.h\n    include/ggml-metal.h\n    include/ggml-rpc.h\n    include/ggml-sycl.h\n    include/ggml-vulkan.h\n    include/gguf.h)\n\nset_target_properties(ggml PROPERTIES PUBLIC_HEADER \"${GGML_PUBLIC_HEADERS}\")\n#if (GGML_METAL)\n#    set_target_properties(ggml PROPERTIES RESOURCE \"${CMAKE_CURRENT_SOURCE_DIR}/src/ggml-metal.metal\")\n#endif()\ninstall(TARGETS ggml LIBRARY PUBLIC_HEADER)\ninstall(TARGETS ggml-base LIBRARY)\n\nif (GGML_STANDALONE)\n    configure_file(${CMAKE_CURRENT_SOURCE_DIR}/ggml.pc.in\n        ${CMAKE_CURRENT_BINARY_DIR}/ggml.pc\n        @ONLY)\n\n    install(FILES ${CMAKE_CURRENT_BINARY_DIR}/ggml.pc\n        DESTINATION share/pkgconfig)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Downloading Test Dataset for llama.cpp Benchmarking\nDESCRIPTION: Command to download the ShareGPT dataset originally used in vLLM benchmarks for testing llama.cpp server performance.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/bench/README.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nwget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json\n```\n\n----------------------------------------\n\nTITLE: Text Generation Model Comparison Command\nDESCRIPTION: Example command for comparing text generation performance between 7B and 13B models with different generation lengths.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llama-bench/README.md#2025-04-22_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n./llama-bench -m models/7B/ggml-model-q4_0.gguf -m models/13B/ggml-model-q4_0.gguf -p 0 -n 128,256,512\n```\n\n----------------------------------------\n\nTITLE: Opaque Type Definition Example in C++\nDESCRIPTION: Example showing how to properly define and use opaque types with the _t suffix. This demonstrates the recommendation for handling types that should be opaque to users.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/CONTRIBUTING.md#2025-04-22_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\ntypedef struct llama_context * llama_context_t;\n\nenum llama_pooling_type llama_pooling_type(const llama_context_t ctx);\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCL Compilation Flags in GGML CMake\nDESCRIPTION: Sets various compilation flags for OpenCL, including profiling, SOA Q, target version, and Adreno kernel optimization.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-opencl/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nif (GGML_OPENCL_PROFILING)\n    message(STATUS \"OpenCL profiling enabled (increases CPU overhead)\")\n    add_compile_definitions(GGML_OPENCL_PROFILING)\nendif ()\n\nadd_compile_definitions(GGML_OPENCL_SOA_Q)\nadd_compile_definitions(GGML_OPENCL_TARGET_VERSION=${GGML_OPENCL_TARGET_VERSION})\n\nif (GGML_OPENCL_USE_ADRENO_KERNELS)\n    message(STATUS \"OpenCL will use matmul kernels optimized for Adreno\")\n    add_compile_definitions(GGML_OPENCL_USE_ADRENO_KERNELS)\nendif ()\n```\n\n----------------------------------------\n\nTITLE: Slot Save Operation Response Format\nDESCRIPTION: JSON response structure for saving a slot's prompt cache to a file, including slot ID, filename, and timing information.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md#2025-04-22_snippet_14\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"id_slot\": 0,\n    \"filename\": \"slot_save_file.bin\",\n    \"n_saved\": 1745,\n    \"n_written\": 14309796,\n    \"timings\": {\n        \"save_ms\": 49.865\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Python Dependencies for llama.cpp Project\nDESCRIPTION: This requirements file lists the Python package dependencies with version constraints. It specifies docstring_parser version ~=0.15, pydantic version ~=2.6.3, and requests with no version constraint.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/requirements/requirements-pydantic.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ndocstring_parser~=0.15\npydantic~=2.6.3\nrequests\n```\n\n----------------------------------------\n\nTITLE: Exporting LLM from LLaVA Next Model\nDESCRIPTION: Python script to export only the LLM from the LLaVA next model using the transformers library. This is useful when the legacy conversion script is incompatible with the language model.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/llava.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport transformers\n\nmodel_path = ...\nllm_export_path = ...\n\ntokenizer = transformers.AutoTokenizer.from_pretrained(model_path)\nmodel = transformers.AutoModelForImageTextToText.from_pretrained(model_path)\n\ntokenizer.save_pretrained(llm_export_path)\nmodel.language_model.save_pretrained(llm_export_path)\n```\n\n----------------------------------------\n\nTITLE: Quantizing GGUF model to Q8_0 format\nDESCRIPTION: Command to quantize the 16-bit floating point GGUF model to Q8_0 format, reducing its size while maintaining reasonable quality.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/tts/README.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nbuild/bin/llama-quantize models/outetts-0.2-0.5B-f16.gguf \\\n    models/outetts-0.2-0.5B-q8_0.gguf q8_0\n```\n\n----------------------------------------\n\nTITLE: Building llama.cpp with OpenCL for Windows Arm64\nDESCRIPTION: PowerShell commands to build llama.cpp with OpenCL support for Windows Arm64 using CMake.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_28\n\nLANGUAGE: powershell\nCODE:\n```\ncmake .. -G Ninja `\n  -DCMAKE_TOOLCHAIN_FILE=\"$HOME/dev/llm/llama.cpp/cmake/arm64-windows-llvm.cmake\" `\n  -DCMAKE_BUILD_TYPE=Release `\n  -DCMAKE_PREFIX_PATH=\"$HOME/dev/llm/opencl\" `\n  -DBUILD_SHARED_LIBS=OFF `\n  -DGGML_OPENCL=ON\nninja\n```\n\n----------------------------------------\n\nTITLE: Finding Available Tests Matching a Pattern\nDESCRIPTION: This command uses ctest to find all tests that match a specific regex pattern without executing them. It shows the test commands in verbose mode that can later be used with GDB.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/development/debugging-tests.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nctest -R \"test-tokenizer\" -V -N\n```\n\n----------------------------------------\n\nTITLE: Configuring MTMD Library Build in CMake\nDESCRIPTION: Sets up the MTMD (multimodal) library as an object library with its dependencies, include directories, and creates static/shared versions. It links against ggml and llama libraries and requires C++17 support.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llava/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(mtmd OBJECT\n            mtmd.cpp\n            mtmd.h\n            clip.cpp\n            clip.h\n            clip-impl.h\n            )\n\ntarget_link_libraries(mtmd PRIVATE ggml llama ${CMAKE_THREAD_LIBS_INIT})\n\ntarget_include_directories(mtmd PUBLIC .)\ntarget_include_directories(mtmd PRIVATE ../..)\ntarget_include_directories(mtmd PRIVATE ../../common) # for stb_image.h\n\ntarget_compile_features(mtmd PRIVATE cxx_std_17)\n\nadd_library(mtmd_static STATIC $<TARGET_OBJECTS:mtmd>)\nif (BUILD_SHARED_LIBS)\n    set_target_properties(mtmd PROPERTIES POSITION_INDEPENDENT_CODE ON)\n    target_compile_definitions(mtmd PRIVATE LLAMA_SHARED LLAMA_BUILD)\n    add_library(mtmd_shared SHARED $<TARGET_OBJECTS:mtmd>)\n    target_link_libraries(mtmd_shared PRIVATE ggml llama ${CMAKE_THREAD_LIBS_INIT})\n    install(TARGETS mtmd_shared LIBRARY)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Cloning MobileVLM and CLIP Models\nDESCRIPTION: Commands for cloning the required MobileVLM-1.7B model and CLIP vision model from Hugging Face repositories. These are prerequisites for the model conversion process.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/MobileVLM.md#2025-04-22_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ngit clone https://huggingface.co/mtgv/MobileVLM-1.7B\n\ngit clone https://huggingface.co/openai/clip-vit-large-patch14-336\n```\n\n----------------------------------------\n\nTITLE: Configuring GGML Core Settings\nDESCRIPTION: Sets core GGML settings including scheduler parameters and CPU backend enablement.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/CMakeLists.txt#2025-04-22_snippet_14\n\nLANGUAGE: CMake\nCODE:\n```\n# ggml core\nset(GGML_SCHED_MAX_COPIES  \"4\" CACHE STRING \"ggml: max input copies for pipeline parallelism\")\noption(GGML_CPU                             \"ggml: enable CPU backend\"                        ON)\n```\n\n----------------------------------------\n\nTITLE: Running k6 Benchmark with Default Settings\nDESCRIPTION: Command to run the k6 benchmark script with specific parameters: 500 chat completions with 8 concurrent users for a maximum of 10 minutes.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/bench/README.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n./k6 run script.js --duration 10m --iterations 500 --vus 8\n```\n\n----------------------------------------\n\nTITLE: Building llama.cpp with Arm KleidiAI Support\nDESCRIPTION: Commands to build llama.cpp with Arm KleidiAI optimization support using CMake.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\ncmake -B build -DGGML_CPU_KLEIDIAI=ON\ncmake --build build --config Release\n```\n\n----------------------------------------\n\nTITLE: Mapping Original Tensor Names to GGUF Standardized Names\nDESCRIPTION: Python code showing how to map model-specific tensor names to standardized GGUF tensor names using a dictionary structure. This example specifically maps attention normalization tensors from various model architectures.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/development/HOWTO-add-model.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nblock_mappings_cfg: dict[MODEL_TENSOR, tuple[str, ...]] = {\n        # Attention norm\n        MODEL_TENSOR.ATTN_NORM: (\n            \"gpt_neox.layers.{bid}.input_layernorm\",                # gptneox\n            \"transformer.h.{bid}.ln_1\",                             # gpt2 gpt-j refact qwen\n            \"transformer.blocks.{bid}.norm_1\",                      # mpt\n            ...\n        )\n}\n```\n\n----------------------------------------\n\nTITLE: Aggregating Python Requirements for llama.cpp\nDESCRIPTION: Imports requirements from specialized dependency files for different llama.cpp tools and converters. Ensures consistent package versions across all Python scripts in the project.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n-r ./requirements/requirements-convert_legacy_llama.txt\n\n-r ./requirements/requirements-convert_hf_to_gguf.txt\n-r ./requirements/requirements-convert_hf_to_gguf_update.txt\n-r ./requirements/requirements-convert_llama_ggml_to_gguf.txt\n-r ./requirements/requirements-convert_lora_to_gguf.txt\n-r ./requirements/requirements-tool_bench.txt\n```\n\n----------------------------------------\n\nTITLE: Displaying Binary Filename Migration Table in Markdown\nDESCRIPTION: A markdown table showing the mapping between old and new binary filenames for the llama.cpp project. The table includes entries for various tools and libraries, with most new names prefixed with 'llama-'.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/deprecation-warning/README.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Old Filename | New Filename |\n| ---- | ---- |\n| main | llama-cli |\n| server | llama-server |\n| llama-bench | llama-bench |\n| embedding | llama-embedding |\n| quantize | llama-quantize |\n| tokenize | llama-tokenize |\n| export-lora | llama-export-lora |\n| libllava.a | libllava.a |\n| baby-llama | llama-baby-llama |\n| batched | llama-batched |\n| batched-bench | llama-batched-bench |\n| benchmark-matmult | llama-benchmark-matmult |\n| convert-llama2c-to-ggml | llama-convert-llama2c-to-ggml |\n| eval-callback | llama-eval-callback |\n| gbnf-validator | llama-gbnf-validator |\n| gguf | llama-gguf |\n| gguf-split | llama-gguf-split |\n| gritlm | llama-gritlm |\n| imatrix | llama-imatrix |\n| infill | llama-infill |\n| llava-cli | llama-llava-cli |\n| lookahead | llama-lookahead |\n| lookup | llama-lookup |\n| lookup-create | llama-lookup-create |\n| lookup-merge | llama-lookup-merge |\n| lookup-stats | llama-lookup-stats |\n| parallel | llama-parallel |\n| passkey | llama-passkey |\n| perplexity | llama-perplexity |\n| q8dot | llama-q8dot |\n| quantize-stats | llama-quantize-stats |\n| retrieval | llama-retrieval |\n| save-load-state | llama-save-load-state |\n| simple | llama-simple |\n| speculative | llama-speculative |\n| vdot | llama-vdot |\n| tests/test-c.o | tests/test-c.o |\n```\n\n----------------------------------------\n\nTITLE: Configuring Vulkan Shader Generation in CMake\nDESCRIPTION: Set up custom commands for generating Vulkan shaders, including handling cross-compilation scenarios and defining input/output paths.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-vulkan/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nset (_ggml_vk_host_suffix $<IF:$<STREQUAL:${CMAKE_HOST_SYSTEM_NAME},Windows>,.exe,>)\nset (_ggml_vk_genshaders_cmd ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/vulkan-shaders-gen${_ggml_vk_host_suffix})\nset (_ggml_vk_header     ${CMAKE_CURRENT_BINARY_DIR}/ggml-vulkan-shaders.hpp)\nset (_ggml_vk_source     ${CMAKE_CURRENT_BINARY_DIR}/ggml-vulkan-shaders.cpp)\nset (_ggml_vk_input_dir  ${CMAKE_CURRENT_SOURCE_DIR}/vulkan-shaders)\nset (_ggml_vk_output_dir ${CMAKE_CURRENT_BINARY_DIR}/vulkan-shaders.spv)\n\nfile(GLOB _ggml_vk_shader_deps \"${_ggml_vk_input_dir}/*.comp\")\nset (_ggml_vk_shader_deps ${_ggml_vk_shader_deps} vulkan-shaders-gen)\n\nif (CMAKE_CROSSCOMPILING)\n    set(_ggml_vk_shader_deps ${_ggml_vk_shader_deps} vulkan-shaders-gen-build vulkan-shaders-gen-install)\nendif()\n\nadd_custom_command(\n    OUTPUT ${_ggml_vk_header}\n            ${_ggml_vk_source}\n\n    COMMAND ${_ggml_vk_genshaders_cmd}\n        --glslc      ${Vulkan_GLSLC_EXECUTABLE}\n        --input-dir  ${_ggml_vk_input_dir}\n        --output-dir ${_ggml_vk_output_dir}\n        --target-hpp ${_ggml_vk_header}\n        --target-cpp ${_ggml_vk_source}\n        --no-clean\n\n    DEPENDS ${_ggml_vk_shader_deps}\n    COMMENT \"Generate vulkan shaders\"\n)\n```\n\n----------------------------------------\n\nTITLE: Converting GLMV-EDGE image encoder to GGUF\nDESCRIPTION: Python command to convert the GLMV-EDGE image encoder to GGUF format using the glmedge-convert-image-encoder-to-gguf.py script. This step prepares the image encoder for use with llama.cpp.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/glmedge.md#2025-04-22_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\npython ./examples/llava/glmedge-convert-image-encoder-to-gguf.py -m ../model_path --llava-projector ../model_path/glm.projector --output-dir ../model_path\n```\n\n----------------------------------------\n\nTITLE: Setting up oneAPI environment for SYCL on Intel\nDESCRIPTION: Sources the oneAPI environment variables needed for SYCL support on Intel GPUs. This should be executed before running any SYCL-enabled llama.cpp applications.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/sycl/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsource /opt/intel/oneapi/setvars.sh\n```\n\n----------------------------------------\n\nTITLE: Installing Ascend Firmware\nDESCRIPTION: Command for installing the Ascend NPU firmware\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/CANN.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nsudo sh Ascend-hdk-910b-npu-firmware_x.x.x.x.X.run --full\n```\n\n----------------------------------------\n\nTITLE: Configuring Qwen2VL CLI Executable in CMake\nDESCRIPTION: Sets up the llama-qwen2vl-cli executable, configuring its source, output name, installation, dependencies, and C++17 requirement.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llava/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET llama-qwen2vl-cli)\nadd_executable(${TARGET} qwen2vl-cli.cpp)\nset_target_properties(${TARGET} PROPERTIES OUTPUT_NAME llama-qwen2vl-cli)\ninstall(TARGETS ${TARGET} RUNTIME)\ntarget_link_libraries(${TARGET} PRIVATE common llava ${CMAKE_THREAD_LIBS_INIT})\ntarget_compile_features(${TARGET} PRIVATE cxx_std_17)\n```\n\n----------------------------------------\n\nTITLE: Setting up Python environment for TTS client\nDESCRIPTION: Commands to create and activate a Python virtual environment, then install the required dependencies (requests and numpy) for the TTS client script.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/tts/README.md#2025-04-22_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\npython3 -m venv venv\nsource venv/bin/activate\n(venv) pip install requests numpy\n```\n\n----------------------------------------\n\nTITLE: Core GGML Library Setup\nDESCRIPTION: Configures the base GGML library components including header files and source files. Sets up the core library target with necessary include directories.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/CMakeLists.txt#2025-04-22_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(ggml-base\n            ../include/ggml.h\n            ../include/ggml-alloc.h\n            ../include/ggml-backend.h\n            ../include/ggml-cpp.h\n            ../include/ggml-opt.h\n            ../include/gguf.h\n            ggml.c\n            ggml-alloc.c\n            ggml-backend.cpp\n            ggml-opt.cpp\n            ggml-threading.cpp\n            ggml-threading.h\n            ggml-quants.c\n            ggml-quants.h\n            gguf.cpp)\n```\n\n----------------------------------------\n\nTITLE: Quantizing LLaMA Model from FP32 to Q4_K\nDESCRIPTION: Command for quantizing the converted LLaMA model from FP32 to Q4_K format using the llama-quantize tool for more efficient storage and inference.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/MobileVLM.md#2025-04-22_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\n./llama-quantize path/to/MobileVLM-1.7B/ggml-model-F32.gguf path/to/MobileVLM-1.7B/ggml-model-q4_k.gguf q4_k_s\n```\n\n----------------------------------------\n\nTITLE: Building llama.cpp with OpenCL for Android\nDESCRIPTION: Commands to build llama.cpp with OpenCL support for Android using CMake and NDK.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_26\n\nLANGUAGE: sh\nCODE:\n```\ncd ~/dev/llm\n\ngit clone https://github.com/ggml-org/llama.cpp && \\\ncd llama.cpp && \\\nmkdir build-android && cd build-android\n\ncmake .. -G Ninja \\\n  -DCMAKE_TOOLCHAIN_FILE=$ANDROID_NDK/build/cmake/android.toolchain.cmake \\\n  -DANDROID_ABI=arm64-v8a \\\n  -DANDROID_PLATFORM=android-28 \\\n  -DBUILD_SHARED_LIBS=OFF \\\n  -DGGML_OPENCL=ON\n\nninja\n```\n\n----------------------------------------\n\nTITLE: Calculating Mathematical Expression in Python\nDESCRIPTION: A mathematical calculation performed using Python, computing 4 multiplied by 7 divided by 3.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/prompts/reason-act.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n4 * 7 / 3\n```\n\n----------------------------------------\n\nTITLE: Configuring SYCL Device Listing Executable in CMake\nDESCRIPTION: This CMake script sets up the build configuration for a SYCL device listing utility. It defines the target name, adds the executable with its source file, specifies installation rules, and links necessary dependencies including the common and llama libraries. The code also sets C++17 as the required standard.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/sycl/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\n#  MIT license\n#  Copyright (C) 2024 Intel Corporation\n#  SPDX-License-Identifier: MIT\n\nset(TARGET llama-ls-sycl-device)\nadd_executable(${TARGET} ls-sycl-device.cpp)\ninstall(TARGETS ${TARGET} RUNTIME)\ntarget_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})\ntarget_compile_features(${TARGET} PRIVATE cxx_std_17)\n```\n\n----------------------------------------\n\nTITLE: Configuring llama-imatrix CMake Target\nDESCRIPTION: Sets up a CMake executable target 'llama-imatrix' with dependencies on common and llama libraries. Configures C++17 standard requirement and installation rules.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/imatrix/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET llama-imatrix)\nadd_executable(${TARGET} imatrix.cpp)\ninstall(TARGETS ${TARGET} RUNTIME)\ntarget_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})\ntarget_compile_features(${TARGET} PRIVATE cxx_std_17)\n```\n\n----------------------------------------\n\nTITLE: Configuring llama-bench Executable in CMake\nDESCRIPTION: Sets up the llama-bench target as an executable, specifies its source file, defines installation rules, links required libraries, and sets the C++ standard to C++17. This configuration is part of the llama.cpp project build system.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llama-bench/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET llama-bench)\nadd_executable(${TARGET} llama-bench.cpp)\ninstall(TARGETS ${TARGET} RUNTIME)\ntarget_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})\ntarget_compile_features(${TARGET} PRIVATE cxx_std_17)\n```\n\n----------------------------------------\n\nTITLE: Downloading Code Llama Model with HF Script in Console\nDESCRIPTION: Command for downloading a CodeLlama model from Hugging Face that supports infill functionality. This uses TheBloke's quantized version (Q5_K_S) of the 13B parameter model.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/infill/README.md#2025-04-22_snippet_0\n\nLANGUAGE: console\nCODE:\n```\nscripts/hf.sh --repo TheBloke/CodeLlama-13B-GGUF --file codellama-13b.Q5_K_S.gguf --outdir models\n```\n\n----------------------------------------\n\nTITLE: Installing Ascend Driver and User Setup\nDESCRIPTION: Commands for setting up the Ascend driver user group and installing the driver software\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/CANN.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nsudo groupadd -g HwHiAiUser\nsudo useradd -g HwHiAiUser -d /home/HwHiAiUser -m HwHiAiUser -s /bin/bash\nsudo usermod -aG HwHiAiUser $USER\nsudo sh Ascend-hdk-910b-npu-driver_x.x.x_linux-{arch}.run --full --install-for-all\n```\n\n----------------------------------------\n\nTITLE: SYCL Target Validation and Compiler Setup\nDESCRIPTION: Validates the SYCL target platform (INTEL/NVIDIA/AMD) and sets up the appropriate SYCL compiler, with support for both oneAPI and open-source implementations.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-sycl/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nmessage(STATUS  \"GGML_SYCL_TARGET=${GGML_SYCL_TARGET}\")\n\nif (NOT GGML_SYCL_TARGET MATCHES \"^(INTEL|NVIDIA|AMD)$\")\n    message(FATAL_ERROR \"Invalid backend chosen, supported options are INTEL, NVIDIA, or AMD\")\nendif()\n\ncheck_cxx_compiler_flag(\"-fsycl\" SUPPORTS_SYCL)\n\nif (DEFINED ENV{ONEAPI_ROOT})\n    message(STATUS \"Using oneAPI Release SYCL compiler (icpx).\")\nelseif(SUPPORTS_SYCL)\n    message(WARNING \"Using open-source SYCL compiler (clang++). Didn't detect ENV {ONEAPI_ROOT}.\n        If you expected the oneAPI Release compiler, please install oneAPI & source it, like:\n        source /opt/intel/oneapi/setvars.sh\")\nelse()\n    message(FATAL_ERROR, \"C++ compiler lacks SYCL support.\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Example of a Generated GGUF Hash Manifest\nDESCRIPTION: This is an example output of a generated manifest file that contains multiple hash types (xxh64, sha1, sha256) for each tensor in the GGUF file as well as for the overall file.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/gguf-hash/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nxxh64     f66e9cd66a4396a0  test.gguf:tensor_0\nsha1      59f79ecefd8125a996fdf419239051a7e99e5f20  test.gguf:tensor_0\nsha256    c0510d38fa060c46265e0160a85c7243096b01dd31c2f355bdbb5516b20de1bd  test.gguf:tensor_0\nxxh64     7d3a1f9ac04d0537  test.gguf:tensor_1\nsha1      4765f592eacf096df4628ba59476af94d767080a  test.gguf:tensor_1\nsha256    8514cbcc73692a2c56bd7a33a022edd5ff819614bd23b19915d7224387f397a7  test.gguf:tensor_1\nxxh64     a0af5d700049693b  test.gguf:tensor_2\nsha1      25cbfbad4513cc348e2c95ebdee69d6ff2fd8753  test.gguf:tensor_2\nsha256    947e6b36e20f2cc95e1d2ce1c1669d813d574657ac6b5ac5196158d454d35180  test.gguf:tensor_2\nxxh64     e83fddf559d7b6a6  test.gguf:tensor_3\nsha1      a9cba73e2d90f2ee3dae2548caa42bef3fe6a96c  test.gguf:tensor_3\nsha256    423b044e016d8ac73c39f23f60bf01bedef5ecb03c0230accd824c91fe86f1a1  test.gguf:tensor_3\nxxh64     1257733306b7992d  test.gguf:tensor_4\nsha1      d7bc61db93bb685ce9d598da89717c66729b7543  test.gguf:tensor_4\nsha256    79737cb3912d4201384cf7f16a1a37ff7823f23ea796cb205b6ca361ab9e3ebf  test.gguf:tensor_4\nxxh64     d238d16ba4711e58  test.gguf:tensor_5\nsha1      0706566c198fe1072f37e0a5135b4b5f23654c52  test.gguf:tensor_5\nsha256    60949be8298eced0ecdde64487643d018407bd261691e061d9e9c3dbc9fd358b  test.gguf:tensor_5\nxxh64     3fbc3b65ab8c7f39  test.gguf:tensor_6\nsha1      73922a0727226a409049f6fc3172a52219ca6f00  test.gguf:tensor_6\nsha256    574f4c46ff384a3b9a225eb955d2a871847a2e8b3fa59387a8252832e92ef7b0  test.gguf:tensor_6\nxxh64     c22021c29854f093  test.gguf:tensor_7\nsha1      efc39cece6a951188fc41e354c73bbfe6813d447  test.gguf:tensor_7\nsha256    4c0410cd3c500f078ae5b21e8dc9eb79e29112713b2ab58a882f82a3868d4d75  test.gguf:tensor_7\nxxh64     936df61f5d64261f  test.gguf:tensor_8\nsha1      c2490296d789a4f34398a337fed8377d943d9f06  test.gguf:tensor_8\nsha256    c4401313feeba0261275c3b25bd2d8fe40ce04e0f440c2980ed0e9674c30ff01  test.gguf:tensor_8\nxxh64     93fd20c64421c081  test.gguf:tensor_9\nsha1      7047ce1e78437a6884337a3751c7ee0421918a65  test.gguf:tensor_9\nsha256    23d57cf0d7a6e90b0b3616b41300e0cd354781e812add854a5f95aa55f2bc514  test.gguf:tensor_9\nxxh64     5a54d3aad816f302  test.gguf\nsha1      d15be52c4ff213e823cb6dd13af7ee2f978e7042  test.gguf\nsha256    7dd641b32f59b60dbd4b5420c4b0f6321ccf48f58f6ae201a3dbc4a58a27c6e4  test.gguf\n```\n\n----------------------------------------\n\nTITLE: Configuring MTMD CLI Executable in CMake\nDESCRIPTION: Sets up the llama-mtmd-cli executable, configuring its source, output name, installation, dependencies, and C++17 requirement.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llava/CMakeLists.txt#2025-04-22_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET llama-mtmd-cli)\nadd_executable(${TARGET} mtmd-cli.cpp)\nset_target_properties(${TARGET} PROPERTIES OUTPUT_NAME llama-mtmd-cli)\ninstall(TARGETS ${TARGET} RUNTIME)\ntarget_link_libraries(${TARGET} PRIVATE common mtmd ${CMAKE_THREAD_LIBS_INIT})\ntarget_compile_features(${TARGET} PRIVATE cxx_std_17)\n```\n\n----------------------------------------\n\nTITLE: Running llama-server with SimpleChat Frontend\nDESCRIPTION: Command to start the llama server with the SimpleChat frontend from the build directory\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/public_simplechat/readme.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nbin/llama-server -m path/model.gguf --path ../examples/server/public_simplechat\n```\n\n----------------------------------------\n\nTITLE: Backend Library Function Definition\nDESCRIPTION: Defines a function for adding GGML backend libraries with support for both static and dynamic loading. Handles library output directory and compilation definitions.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/CMakeLists.txt#2025-04-22_snippet_11\n\nLANGUAGE: cmake\nCODE:\n```\nfunction(ggml_add_backend_library backend)\n    if (GGML_BACKEND_DL)\n        add_library(${backend} MODULE ${ARGN})\n        set_target_properties(${backend} PROPERTIES LIBRARY_OUTPUT_DIRECTORY ${CMAKE_RUNTIME_OUTPUT_DIRECTORY})\n        target_compile_definitions(${backend} PRIVATE GGML_BACKEND_DL)\n        add_dependencies(ggml ${backend})\n    else()\n        add_library(${backend} ${ARGN})\n        target_link_libraries(ggml PUBLIC ${backend})\n        install(TARGETS ${backend} LIBRARY)\n    endif()\n\n    target_link_libraries(${backend} PRIVATE ggml-base)\n    target_include_directories(${backend} PRIVATE ..)\n\n    if (${BUILD_SHARED_LIBS})\n        target_compile_definitions(${backend} PRIVATE GGML_BACKEND_BUILD)\n        target_compile_definitions(${backend} PUBLIC  GGML_BACKEND_SHARED)\n    endif()\n\n    if(NOT GGML_AVAILABLE_BACKENDS)\n        set(GGML_AVAILABLE_BACKENDS \"${backend}\"\n            CACHE INTERNAL \"List of backends for cmake package\")\n    else()\n        list(FIND GGML_AVAILABLE_BACKENDS \"${backend}\" has_backend)\n        if(has_backend EQUAL -1)\n            set(GGML_AVAILABLE_BACKENDS \"${GGML_AVAILABLE_BACKENDS};${backend}\"\n                CACHE INTERNAL \"List of backends for cmake package\")\n        endif()\n    endif()\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Converting LLaMA Model to GGUF\nDESCRIPTION: Command for converting the LLaMA part of the LLaVA model to GGUF format using the legacy conversion script, skipping unknown parameters.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/MobileVLM.md#2025-04-22_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\npython ./examples/convert_legacy_llama.py path/to/MobileVLM-1.7B --skip-unknown\n```\n\n----------------------------------------\n\nTITLE: Building llama.cpp with OpenCL for Windows 11 Arm64\nDESCRIPTION: PowerShell commands to clone and build llama.cpp with OpenCL support for Windows 11 Arm64, using CMake and Ninja with the ARM64 Windows LLVM toolchain.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/OPENCL.md#2025-04-22_snippet_5\n\nLANGUAGE: powershell\nCODE:\n```\nmkdir -p ~/dev/llm\ncd ~/dev/llm\n\ngit clone https://github.com/ggml-org/llama.cpp && cd llama.cpp\nmkdir build && cd build\n\ncmake .. -G Ninja `\n  -DCMAKE_TOOLCHAIN_FILE=\"$HOME/dev/llm/llama.cpp/cmake/arm64-windows-llvm.cmake\" `\n  -DCMAKE_BUILD_TYPE=Release `\n  -DCMAKE_PREFIX_PATH=\"$HOME/dev/llm/opencl\" `\n  -DBUILD_SHARED_LIBS=OFF `\n  -DGGML_OPENCL=ON\nninja\n```\n\n----------------------------------------\n\nTITLE: CPU Backend Variant Configuration\nDESCRIPTION: Sets up CPU-specific backend variants with different instruction set optimizations. Configures options for various CPU architectures including SSE, AVX, and AMX features.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/CMakeLists.txt#2025-04-22_snippet_12\n\nLANGUAGE: cmake\nCODE:\n```\nfunction(ggml_add_cpu_backend_variant tag_name)\n    set(GGML_CPU_TAG_NAME ${tag_name})\n    foreach (feat NATIVE\n                  SSE42\n                  AVX AVX2 BMI2 AVX_VNNI FMA F16C\n                  AVX512 AVX512_VBMI AVX512_VNNI AVX512_BF16\n                  AMX_TILE AMX_INT8 AMX_BF16)\n        set(GGML_${feat} OFF)\n    endforeach()\n\n    foreach (feat ${ARGN})\n        set(GGML_${feat} ON)\n    endforeach()\n\n    ggml_add_cpu_backend_variant_impl(${tag_name})\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Configuring Build for llama-passkey Executable in CMake\nDESCRIPTION: This CMake configuration sets up the llama-passkey executable build process. It defines the target name, source file (passkey.cpp), links necessary libraries (common, llama, and threading libraries), specifies C++17 as the required standard, and configures installation parameters.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/passkey/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET llama-passkey)\nadd_executable(${TARGET} passkey.cpp)\ninstall(TARGETS ${TARGET} RUNTIME)\ntarget_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})\ntarget_compile_features(${TARGET} PRIVATE cxx_std_17)\n```\n\n----------------------------------------\n\nTITLE: Compiling with MUSA GPU Support\nDESCRIPTION: Basic compilation commands for building llama.cpp with MUSA GPU support using CMake. Enables GPU acceleration for Moore Threads GPUs.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ncmake -B build -DGGML_MUSA=ON\ncmake --build build --config Release\n```\n\n----------------------------------------\n\nTITLE: Configuring Sanitizers for Non-MSVC Compilers\nDESCRIPTION: Sets up various sanitizers (thread, address, undefined behavior) for non-MSVC compilers. These help catch runtime errors during development.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nif (NOT MSVC)\n    if (GGML_SANITIZE_THREAD)\n        add_compile_options(-fsanitize=thread)\n        link_libraries     (-fsanitize=thread)\n    endif()\n\n    if (GGML_SANITIZE_ADDRESS)\n        add_compile_options(-fsanitize=address -fno-omit-frame-pointer)\n        link_libraries     (-fsanitize=address)\n    endif()\n\n    if (GGML_SANITIZE_UNDEFINED)\n        add_compile_options(-fsanitize=undefined)\n        link_libraries     (-fsanitize=undefined)\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Windows-Specific Compiler Settings\nDESCRIPTION: Configuration for MSVC compiler including UTF-8 support and large object file handling\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif (MSVC)\n    add_compile_options(\"$<$<COMPILE_LANGUAGE:C>:/utf-8>\")\n    add_compile_options(\"$<$<COMPILE_LANGUAGE:CXX>:/utf-8>\")\n    add_compile_options(\"$<$<COMPILE_LANGUAGE:C>:/bigobj>\")\n    add_compile_options(\"$<$<COMPILE_LANGUAGE:CXX>:/bigobj>\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring LLaVA CLIP Quantization CLI in CMake\nDESCRIPTION: Sets up the llama-llava-clip-quantize-cli executable for CLIP model quantization, configuring its source, output name, installation, dependencies, and C++17 requirement.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llava/CMakeLists.txt#2025-04-22_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET llama-llava-clip-quantize-cli)\nadd_executable(${TARGET} clip-quantize-cli.cpp)\nset_target_properties(${TARGET} PROPERTIES OUTPUT_NAME llama-llava-clip-quantize-cli)\ninstall(TARGETS ${TARGET} RUNTIME)\ntarget_link_libraries(${TARGET} PRIVATE common llava ${CMAKE_THREAD_LIBS_INIT})\ntarget_compile_features(${TARGET} PRIVATE cxx_std_17)\n```\n\n----------------------------------------\n\nTITLE: Build Type Configuration\nDESCRIPTION: Sets default build type to Release if not specified, with options for Debug, Release, MinSizeRel, and RelWithDebInfo\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif (NOT XCODE AND NOT MSVC AND NOT CMAKE_BUILD_TYPE)\n    set(CMAKE_BUILD_TYPE Release CACHE STRING \"Build type\" FORCE)\n    set_property(CACHE CMAKE_BUILD_TYPE PROPERTY STRINGS \"Debug\" \"Release\" \"MinSizeRel\" \"RelWithDebInfo\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Playing the generated audio file\nDESCRIPTION: Linux command to play the generated WAV audio file using the aplay utility.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/tts/README.md#2025-04-22_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\naplay output.wav\n```\n\n----------------------------------------\n\nTITLE: Converting LLaMA Part of LLaVA to GGUF\nDESCRIPTION: Python command to convert the LLaMA part of the LLaVA model to GGUF format using the convert_legacy_llama.py script.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/llava.md#2025-04-22_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\npython ./examples/convert_legacy_llama.py ../llava-v1.5-7b --skip-unknown\n```\n\n----------------------------------------\n\nTITLE: Cloning the OuteTTS LLM model repository\nDESCRIPTION: Commands to checkout the OuteTTS-0.2-500M model from HuggingFace, including proper Git LFS setup for downloading large files.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/tts/README.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npushd models\ngit clone --branch main --single-branch --depth 1 https://huggingface.co/OuteAI/OuteTTS-0.2-500M\ncd OuteTTS-0.2-500M && git lfs install && git lfs pull\npopd\n```\n\n----------------------------------------\n\nTITLE: SYCL GPU Detection Log Example (Single GPU)\nDESCRIPTION: Example log output showing detection of a single SYCL GPU with its compute units. This appears when running with SYCL backend and helps verify the device selection.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_28\n\nLANGUAGE: sh\nCODE:\n```\ndetect 1 SYCL GPUs: [0] with top Max compute units:512\n```\n\n----------------------------------------\n\nTITLE: Configuring llama-q8dot Executable Build in CMake\nDESCRIPTION: Sets up the build configuration for the llama-q8dot executable, which likely performs quantized 8-bit dot product operations. It links against the common and llama libraries and requires C++17 support.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/pocs/vdot/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET llama-q8dot)\nadd_executable(${TARGET} q8dot.cpp)\ntarget_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})\ntarget_compile_features(${TARGET} PRIVATE cxx_std_17)\n```\n\n----------------------------------------\n\nTITLE: Configuring Swift Package with LLaMA XCFramework\nDESCRIPTION: Demonstrates how to set up a Swift package that uses the precompiled LLaMA XCFramework. It shows the Package.swift configuration including how to reference a specific build of the framework via URL and checksum.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/README.md#2025-04-22_snippet_15\n\nLANGUAGE: swift\nCODE:\n```\n// swift-tools-version: 5.10\n// The swift-tools-version declares the minimum version of Swift required to build this package.\n\nimport PackageDescription\n\nlet package = Package(\n    name: \"MyLlamaPackage\",\n    targets: [\n        .executableTarget(\n            name: \"MyLlamaPackage\",\n            dependencies: [\n                \"LlamaFramework\"\n            ]),\n        .binaryTarget(\n            name: \"LlamaFramework\",\n            url: \"https://github.com/ggml-org/llama.cpp/releases/download/b5046/llama-b5046-xcframework.zip\",\n            checksum: \"c19be78b5f00d8d29a25da41042cb7afa094cbf6280a225abe614b03b20029ab\"\n        )\n    ]\n)\n```\n\n----------------------------------------\n\nTITLE: Building llama.cpp for Windows ARM64\nDESCRIPTION: Command to build llama.cpp for Windows on ARM (arm64) using LLVM compiler with CMake.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncmake --preset arm64-windows-llvm-release -D GGML_OPENMP=OFF\ncmake --build build-arm64-windows-llvm-release\n```\n\n----------------------------------------\n\nTITLE: Using JSON Schema with llama-cli Command for Constrained Text Generation\nDESCRIPTION: Example of using the llama-cli command with a JSON schema that defines an array of objects with name and age properties. The schema constrains the model to generate a JSON array with 10-100 items where each item has a name (string) and age (integer) field.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/grammars/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nllama-cli \\\n  -hfr bartowski/Phi-3-medium-128k-instruct-GGUF \\\n  -hff Phi-3-medium-128k-instruct-Q8_0.gguf \\\n  -j '{\n    \"type\": \"array\",\n    \"items\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"name\": {\n                \"type\": \"string\",\n                \"minLength\": 1,\n                \"maxLength\": 100\n            },\n            \"age\": {\n                \"type\": \"integer\",\n                \"minimum\": 0,\n                \"maximum\": 150\n            }\n        },\n        \"required\": [\"name\", \"age\"],\n        \"additionalProperties\": false\n    },\n    \"minItems\": 10,\n    \"maxItems\": 100\n  }' \\\n  -p 'Generate a {name, age}[] JSON array with famous actors of all ages.'\n```\n\n----------------------------------------\n\nTITLE: Defining GGUF Tensor Names for Falcon Model Architecture\nDESCRIPTION: Python code demonstrating how to define the required tensor names for a Falcon model architecture in the GGUF constants file. This specifies which tensor types are needed for the model to function properly.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/development/HOWTO-add-model.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n    MODEL_ARCH.FALCON: [\n        MODEL_TENSOR.TOKEN_EMBD,\n        MODEL_TENSOR.OUTPUT_NORM,\n        MODEL_TENSOR.OUTPUT,\n        MODEL_TENSOR.ATTN_NORM,\n        MODEL_TENSOR.ATTN_NORM_2,\n        MODEL_TENSOR.ATTN_QKV,\n        MODEL_TENSOR.ATTN_OUT,\n        MODEL_TENSOR.FFN_DOWN,\n        MODEL_TENSOR.FFN_UP,\n    ]\n```\n\n----------------------------------------\n\nTITLE: Implementing Chess Notation Grammar in GBNF\nDESCRIPTION: Demonstrates a GBNF grammar for parsing chess moves, showing rule definitions, alternatives, and move sequences with comments.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/grammars/README.md#2025-04-22_snippet_0\n\nLANGUAGE: gbnf\nCODE:\n```\n# `root` specifies the pattern for the overall output\nroot ::= (\n    # it must start with the characters \"1. \" followed by a sequence\n    # of characters that match the `move` rule, followed by a space, followed\n    # by another move, and then a newline\n    \"1. \" move \" \" move \"\\n\"\n\n    # it's followed by one or more subsequent moves, numbered with one or two digits\n    ([1-9] [0-9]? \". \" move \" \" move \"\\n\")+\n)\n\n# `move` is an abstract representation, which can be a pawn, nonpawn, or castle.\n# The `[+#]?` denotes the possibility of checking or mate signs after moves\nmove ::= (pawn | nonpawn | castle) [+#]?\n\npawn ::= ...\nnonpawn ::= ...\ncastle ::= ...\n```\n\n----------------------------------------\n\nTITLE: Basic CMake Project Configuration\nDESCRIPTION: Initial CMake setup including minimum version requirement, project name, and basic build settings\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.14)\nproject(\"llama.cpp\" C CXX)\ninclude(CheckIncludeFileCXX)\n\nset(CMAKE_WARN_UNUSED_CLI YES)\nset(CMAKE_EXPORT_COMPILE_COMMANDS ON)\n```\n\n----------------------------------------\n\nTITLE: Example Output of All-Hash Verification\nDESCRIPTION: This is the output of verifying all hash types in a GGUF manifest. It shows verification results for each hash algorithm (XXH64, SHA-1, SHA-256) for each tensor and the overall file.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/gguf-hash/README.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nmanifest  test.gguf.manifest  sha256  sha1  xxh64\nxxh64     f66e9cd66a4396a0  test.gguf:tensor_0  -  Ok\nsha1      59f79ecefd8125a996fdf419239051a7e99e5f20  test.gguf:tensor_0  -  Ok\nsha256    c0510d38fa060c46265e0160a85c7243096b01dd31c2f355bdbb5516b20de1bd  test.gguf:tensor_0  -  Ok\nxxh64     7d3a1f9ac04d0537  test.gguf:tensor_1  -  Ok\nsha1      4765f592eacf096df4628ba59476af94d767080a  test.gguf:tensor_1  -  Ok\nsha256    8514cbcc73692a2c56bd7a33a022edd5ff819614bd23b19915d7224387f397a7  test.gguf:tensor_1  -  Ok\nxxh64     a0af5d700049693b  test.gguf:tensor_2  -  Ok\nsha1      25cbfbad4513cc348e2c95ebdee69d6ff2fd8753  test.gguf:tensor_2  -  Ok\nsha256    947e6b36e20f2cc95e1d2ce1c1669d813d574657ac6b5ac5196158d454d35180  test.gguf:tensor_2  -  Ok\nxxh64     e83fddf559d7b6a6  test.gguf:tensor_3  -  Ok\nsha1      a9cba73e2d90f2ee3dae2548caa42bef3fe6a96c  test.gguf:tensor_3  -  Ok\nsha256    423b044e016d8ac73c39f23f60bf01bedef5ecb03c0230accd824c91fe86f1a1  test.gguf:tensor_3  -  Ok\nxxh64     1257733306b7992d  test.gguf:tensor_4  -  Ok\nsha1      d7bc61db93bb685ce9d598da89717c66729b7543  test.gguf:tensor_4  -  Ok\nsha256    79737cb3912d4201384cf7f16a1a37ff7823f23ea796cb205b6ca361ab9e3ebf  test.gguf:tensor_4  -  Ok\nxxh64     d238d16ba4711e58  test.gguf:tensor_5  -  Ok\nsha1      0706566c198fe1072f37e0a5135b4b5f23654c52  test.gguf:tensor_5  -  Ok\nsha256    60949be8298eced0ecdde64487643d018407bd261691e061d9e9c3dbc9fd358b  test.gguf:tensor_5  -  Ok\nxxh64     3fbc3b65ab8c7f39  test.gguf:tensor_6  -  Ok\nsha1      73922a0727226a409049f6fc3172a52219ca6f00  test.gguf:tensor_6  -  Ok\nsha256    574f4c46ff384a3b9a225eb955d2a871847a2e8b3fa59387a8252832e92ef7b0  test.gguf:tensor_6  -  Ok\nxxh64     c22021c29854f093  test.gguf:tensor_7  -  Ok\nsha1      efc39cece6a951188fc41e354c73bbfe6813d447  test.gguf:tensor_7  -  Ok\nsha256    4c0410cd3c500f078ae5b21e8dc9eb79e29112713b2ab58a882f82a3868d4d75  test.gguf:tensor_7  -  Ok\nxxh64     936df61f5d64261f  test.gguf:tensor_8  -  Ok\nsha1      c2490296d789a4f34398a337fed8377d943d9f06  test.gguf:tensor_8  -  Ok\nsha256    c4401313feeba0261275c3b25bd2d8fe40ce04e0f440c2980ed0e9674c30ff01  test.gguf:tensor_8  -  Ok\nxxh64     93fd20c64421c081  test.gguf:tensor_9  -  Ok\nsha1      7047ce1e78437a6884337a3751c7ee0421918a65  test.gguf:tensor_9  -  Ok\nsha256    23d57cf0d7a6e90b0b3616b41300e0cd354781e812add854a5f95aa55f2bc514  test.gguf:tensor_9  -  Ok\nxxh64     5a54d3aad816f302  test.gguf  -  Ok\nsha1      d15be52c4ff213e823cb6dd13af7ee2f978e7042  test.gguf  -  Ok\nsha256    7dd641b32f59b60dbd4b5420c4b0f6321ccf48f58f6ae201a3dbc4a58a27c6e4  test.gguf  -  Ok\n\nVerification results for test.gguf.manifest - Success\n```\n\n----------------------------------------\n\nTITLE: Thread Scaling Performance Test Command\nDESCRIPTION: Example command for testing performance with different numbers of CPU threads.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llama-bench/README.md#2025-04-22_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\n./llama-bench -n 0 -n 16 -p 64 -t 1,2,4,8,16,32\n```\n\n----------------------------------------\n\nTITLE: Compiling BLIS Framework with OpenMP and POSIX Threads\nDESCRIPTION: Clones and compiles the BLIS framework with CBLAS compatibility and multi-threading support using OpenMP and POSIX threads.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/BLIS.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/flame/blis\ncd blis\n./configure --enable-cblas -t openmp,pthreads auto\n# will install to /usr/local/ by default.\nmake -j\n```\n\n----------------------------------------\n\nTITLE: Configuring Architecture-Specific Optimizations for GGML\nDESCRIPTION: Sets up options to enable or disable various CPU instruction set optimizations for different architectures including x86, ARM, RISC-V, and Loongarch.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/CMakeLists.txt#2025-04-22_snippet_11\n\nLANGUAGE: CMake\nCODE:\n```\noption(GGML_CPU_HBM          \"ggml: use memkind for CPU HBM\" OFF)\noption(GGML_CPU_AARCH64      \"ggml: use runtime weight conversion of Q4_0 to Q4_X_X\" ON)\noption(GGML_CPU_KLEIDIAI     \"ggml: use KleidiAI optimized kernels if applicable\" OFF)\noption(GGML_SSE42            \"ggml: enable SSE 4.2\"          ${INS_ENB})\noption(GGML_AVX              \"ggml: enable AVX\"              ${INS_ENB})\noption(GGML_AVX_VNNI         \"ggml: enable AVX-VNNI\"         OFF)\noption(GGML_AVX2             \"ggml: enable AVX2\"             ${INS_ENB})\noption(GGML_BMI2             \"ggml: enable BMI2\"             ${INS_ENB})\noption(GGML_AVX512           \"ggml: enable AVX512F\"          OFF)\noption(GGML_AVX512_VBMI      \"ggml: enable AVX512-VBMI\"      OFF)\noption(GGML_AVX512_VNNI      \"ggml: enable AVX512-VNNI\"      OFF)\noption(GGML_AVX512_BF16      \"ggml: enable AVX512-BF16\"      OFF)\nif (NOT MSVC)\n    # in MSVC F16C and FMA is implied with AVX2/AVX512\n    option(GGML_FMA          \"ggml: enable FMA\"              ${INS_ENB})\n    option(GGML_F16C         \"ggml: enable F16C\"             ${INS_ENB})\n    # MSVC does not seem to support AMX\n    option(GGML_AMX_TILE     \"ggml: enable AMX-TILE\"         OFF)\n    option(GGML_AMX_INT8     \"ggml: enable AMX-INT8\"         OFF)\n    option(GGML_AMX_BF16     \"ggml: enable AMX-BF16\"         OFF)\nendif()\noption(GGML_LASX             \"ggml: enable lasx\"             ON)\noption(GGML_LSX              \"ggml: enable lsx\"              ON)\noption(GGML_RVV              \"ggml: enable rvv\"              ON)\noption(GGML_RV_ZFH           \"ggml: enable riscv zfh\"        OFF)\noption(GGML_VXE              \"ggml: enable vxe\"              ON)\n```\n\n----------------------------------------\n\nTITLE: GGML SYCL Library Configuration\nDESCRIPTION: Configures the GGML SYCL backend library, including source files and compiler settings. Sets up Visual Studio solution settings for Windows builds.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-sycl/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nggml_add_backend_library(ggml-sycl\n                         ggml-sycl.cpp\n                         ../../include/ggml-sycl.h\n                        )\n\nfile(GLOB   GGML_HEADERS_SYCL \"*.hpp\")\nfile(GLOB   GGML_SOURCES_SYCL \"*.cpp\")\ntarget_sources(ggml-sycl PRIVATE ${GGML_HEADERS_SYCL} ${GGML_SOURCES_SYCL})\n\nif (WIN32)\n    if( ${CMAKE_GENERATOR} MATCHES \"Visual Studio\" AND NOT (${CMAKE_GENERATOR_TOOLSET} MATCHES \"Intel C\"))\n        set_target_properties(ggml-sycl PROPERTIES VS_PLATFORM_TOOLSET \"Intel C++ Compiler 2025\")\n        set(CMAKE_CXX_COMPILER \"icx\")\n        set(CMAKE_CXX_COMPILER_ID \"IntelLLVM\")\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Testing llama.cpp with KleidiAI\nDESCRIPTION: Command to test llama.cpp with Arm KleidiAI optimizations enabled.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\n./build/bin/llama-cli -m PATH_TO_MODEL -p \"What is a car?\"\n```\n\n----------------------------------------\n\nTITLE: Mean Method for Control Vector Generation\nDESCRIPTION: Command to generate a control vector using the mean value method instead of Principal Component Analysis (PCA).\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/cvector-generator/README.md#2025-04-22_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\n./cvector-generator -m ./llama-3.Q4_K_M.gguf --method mean\n```\n\n----------------------------------------\n\nTITLE: Configuring llama-cvector-generator Target in CMake\nDESCRIPTION: This CMake snippet sets up the llama-cvector-generator executable target. It specifies the source files, links required libraries, sets C++17 as the compile standard, and configures installation rules.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/cvector-generator/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET llama-cvector-generator)\nadd_executable(${TARGET} cvector-generator.cpp pca.hpp)\ninstall(TARGETS ${TARGET} RUNTIME)\ntarget_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})\ntarget_compile_features(${TARGET} PRIVATE cxx_std_17)\n```\n\n----------------------------------------\n\nTITLE: Example of Q4_0 Quantization for CLIP Visual Projector\nDESCRIPTION: This example shows how to quantize a CLIP visual projector model using the q4_0 quantization type (represented by the value 2). This quantization method uses 4-bit precision with a single scale value.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llava/README-quantize.md#2025-04-22_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n./bin/llama-llava-clip-quantize-cli /path/to/ggml-model-f32.gguf /path/to/ggml-model-quantized.gguf 2\n```\n\n----------------------------------------\n\nTITLE: Running Tests from a Specific File\nDESCRIPTION: Command to run all tests contained in a specific file rather than the entire test suite.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/tests/README.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n./tests.sh unit/test_chat_completion.py -v -x\n```\n\n----------------------------------------\n\nTITLE: Configuring CUDA Feature Flags via Compile Definitions\nDESCRIPTION: Sets up conditional compilation flags based on various CUDA feature options, including CUDA graphs, matrix multiplication options, virtual memory management, and data type settings.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cuda/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nadd_compile_definitions(GGML_CUDA_PEER_MAX_BATCH_SIZE=${GGML_CUDA_PEER_MAX_BATCH_SIZE})\n\nif (GGML_CUDA_GRAPHS)\n    add_compile_definitions(GGML_CUDA_USE_GRAPHS)\nendif()\n\nif (GGML_CUDA_FORCE_MMQ)\n    add_compile_definitions(GGML_CUDA_FORCE_MMQ)\nendif()\n\nif (GGML_CUDA_FORCE_CUBLAS)\n    add_compile_definitions(GGML_CUDA_FORCE_CUBLAS)\nendif()\n\nif (GGML_CUDA_NO_VMM)\n    add_compile_definitions(GGML_CUDA_NO_VMM)\nendif()\n\nif (NOT GGML_CUDA_FA)\n    add_compile_definitions(GGML_CUDA_NO_FA)\nendif()\n\nif (GGML_CUDA_F16 OR GGML_CUDA_DMMV_F16)\n    add_compile_definitions(GGML_CUDA_F16)\nendif()\n\nif (GGML_CUDA_NO_PEER_COPY)\n    add_compile_definitions(GGML_CUDA_NO_PEER_COPY)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Running Llama.cpp Server Tests in Debug Mode\nDESCRIPTION: Command to run tests with verbose output in real-time, which is useful for debugging issues.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/tests/README.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nDEBUG=1 ./tests.sh -s -v -x\n```\n\n----------------------------------------\n\nTITLE: Executing the SYCL device listing tool\nDESCRIPTION: Runs the llama-ls-sycl-device tool to display all available SYCL devices with their capabilities. This helps identify device IDs and specifications before running the main application.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/sycl/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./build/bin/llama-ls-sycl-device\n```\n\n----------------------------------------\n\nTITLE: Configuring CPU Backend Variants and Architecture Specifics\nDESCRIPTION: Sets up options for building multiple CPU backend variants and specifying architecture-specific parameters for ARM and PowerPC.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/CMakeLists.txt#2025-04-22_snippet_12\n\nLANGUAGE: CMake\nCODE:\n```\noption(GGML_CPU_ALL_VARIANTS \"ggml: build all variants of the CPU backend (requires GGML_BACKEND_DL)\" OFF)\nset(GGML_CPU_ARM_ARCH        \"\" CACHE STRING \"ggml: CPU architecture for ARM\")\nset(GGML_CPU_POWERPC_CPUTYPE \"\" CACHE STRING \"ggml: CPU type for PowerPC\")\n```\n\n----------------------------------------\n\nTITLE: Creating Git Tag for Release\nDESCRIPTION: Git command to create an annotated tag for version release\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/gguf-py/README.md#2025-04-22_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\ngit tag -a gguf-v1.0.0 -m \"Version 1.0 release\"\n```\n\n----------------------------------------\n\nTITLE: JSON output from llama-bench\nDESCRIPTION: Example of JSON output generated by llama-bench. The output is an array of test results with detailed information including hardware specs, model configuration, and performance metrics with sample data points.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llama-bench/README.md#2025-04-22_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"build_commit\": \"3469684\",\n    \"build_number\": 1275,\n    \"cuda\": true,\n    \"metal\": false,\n    \"gpu_blas\": true,\n    \"blas\": true,\n    \"cpu_info\": \"13th Gen Intel(R) Core(TM) i9-13900K\",\n    \"gpu_info\": \"NVIDIA GeForce RTX 3090 Ti\",\n    \"model_filename\": \"models/7B/ggml-model-q4_0.gguf\",\n    \"model_type\": \"llama 7B mostly Q4_0\",\n    \"model_size\": 3825065984,\n    \"model_n_params\": 6738415616,\n    \"n_batch\": 512,\n    \"n_threads\": 16,\n    \"f16_kv\": true,\n    \"n_gpu_layers\": 99,\n    \"main_gpu\": 0,\n    \"mul_mat_q\": true,\n    \"tensor_split\": \"0.00\",\n    \"n_prompt\": 512,\n    \"n_gen\": 0,\n    \"test_time\": \"2023-09-23T12:09:57Z\",\n    \"avg_ns\": 212365953,\n    \"stddev_ns\": 985423,\n    \"avg_ts\": 2410.974041,\n    \"stddev_ts\": 11.163766,\n    \"samples_ns\": [ 213837238, 211635853, 212328053, 211329715, 212698907 ],\n    \"samples_ts\": [ 2394.34, 2419.25, 2411.36, 2422.75, 2407.16 ]\n  },\n  {\n    \"build_commit\": \"3469684\",\n    \"build_number\": 1275,\n    \"cuda\": true,\n    \"metal\": false,\n    \"gpu_blas\": true,\n    \"blas\": true,\n    \"cpu_info\": \"13th Gen Intel(R) Core(TM) i9-13900K\",\n    \"gpu_info\": \"NVIDIA GeForce RTX 3090 Ti\",\n    \"model_filename\": \"models/7B/ggml-model-q4_0.gguf\",\n    \"model_type\": \"llama 7B mostly Q4_0\",\n    \"model_size\": 3825065984,\n    \"model_n_params\": 6738415616,\n    \"n_batch\": 512,\n    \"n_threads\": 16,\n    \"f16_kv\": true,\n    \"n_gpu_layers\": 99,\n    \"main_gpu\": 0,\n    \"mul_mat_q\": true,\n    \"tensor_split\": \"0.00\",\n    \"n_prompt\": 0,\n    \"n_gen\": 128,\n    \"test_time\": \"2023-09-23T12:09:59Z\",\n    \"avg_ns\": 977425219,\n    \"stddev_ns\": 9268593,\n    \"avg_ts\": 130.965708,\n    \"stddev_ts\": 1.238924,\n    \"samples_ns\": [ 984472709, 974901233, 989474741, 970729355, 967548060 ],\n    \"samples_ts\": [ 130.019, 131.295, 129.362, 131.86, 132.293 ]\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Converting JSON Schema to GBNF Grammar Using Command-line Tool\nDESCRIPTION: Command to convert a JSON schema file to a GBNF grammar using the provided Python script. This allows preprocessing schemas into grammars before inference time.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/grammars/README.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nexamples/json_schema_to_grammar.py name-age-schema.json\n```\n\n----------------------------------------\n\nTITLE: Downloading PHI-2 Model for llama.cpp Benchmarking\nDESCRIPTION: Example command to download the PHI-2 model using the hf.sh script for benchmarking purposes.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/bench/README.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n../../../scripts/hf.sh --repo ggml-org/models --file phi-2/ggml-model-q4_0.gguf\n```\n\n----------------------------------------\n\nTITLE: Configuring and Building llama-gguf Executable with CMake\nDESCRIPTION: This CMake script sets up the llama-gguf executable, specifying its source file, installation rules, library dependencies, and C++ standard requirement. It links against the ggml library and system thread libraries, and requires C++17 support.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/gguf/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET llama-gguf)\nadd_executable(${TARGET} gguf.cpp)\ninstall(TARGETS ${TARGET} RUNTIME)\ntarget_link_libraries(${TARGET} PRIVATE ggml ${CMAKE_THREAD_LIBS_INIT})\ntarget_compile_features(${TARGET} PRIVATE cxx_std_17)\n```\n\n----------------------------------------\n\nTITLE: Configuring llama-parallel Executable Target in CMake\nDESCRIPTION: Creates and configures the llama-parallel executable target in CMake. The target is built from parallel.cpp, links against common libraries and llama, and requires C++17 support. It also sets up installation instructions for the binary.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/parallel/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET llama-parallel)\nadd_executable(${TARGET} parallel.cpp)\ninstall(TARGETS ${TARGET} RUNTIME)\ntarget_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})\ntarget_compile_features(${TARGET} PRIVATE cxx_std_17)\n```\n\n----------------------------------------\n\nTITLE: Building llama.cpp with SYCL on Windows (Script)\nDESCRIPTION: Batch script to build llama.cpp with SYCL support on Windows.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_18\n\nLANGUAGE: sh\nCODE:\n```\n.\\examples\\sycl\\win-build-sycl.bat\n```\n\n----------------------------------------\n\nTITLE: Configuring GGML Metal Backend Library in CMake\nDESCRIPTION: Sets up the ggml-metal library, including source files and linked libraries.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-metal/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nggml_add_backend_library(ggml-metal\n                         ggml-metal.m\n                        )\n\ntarget_link_libraries(ggml-metal PRIVATE\n                      ${FOUNDATION_LIBRARY}\n                      ${METAL_FRAMEWORK}\n                      ${METALKIT_FRAMEWORK}\n                      )\n```\n\n----------------------------------------\n\nTITLE: Running a Single Test Case\nDESCRIPTION: Command to run a specific test case by providing the file path and test name.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/tests/README.md#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\n./tests.sh unit/test_chat_completion.py::test_invalid_chat_completion_req\n```\n\n----------------------------------------\n\nTITLE: Installing k6 with SSE Extension for llama.cpp Benchmarking\nDESCRIPTION: Commands to install k6 with the xk6-sse extension required for server benchmarking. Requires golang 1.21 or higher to be installed.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/bench/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngo install go.k6.io/xk6/cmd/xk6@latest\n$GOPATH/bin/xk6 build master \\\n--with github.com/phymbert/xk6-sse\n```\n\n----------------------------------------\n\nTITLE: Installing OpenCL Headers and Libraries for Windows 11 Arm64\nDESCRIPTION: PowerShell commands to clone and install OpenCL headers and ICD loader for Windows 11 Arm64, required for building llama.cpp with OpenCL support on Snapdragon X Elite devices.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/OPENCL.md#2025-04-22_snippet_4\n\nLANGUAGE: powershell\nCODE:\n```\nmkdir -p ~/dev/llm\n\ncd ~/dev/llm\ngit clone https://github.com/KhronosGroup/OpenCL-Headers && cd OpenCL-Headers\nmkdir build && cd build\ncmake .. -G Ninja `\n  -DBUILD_TESTING=OFF `\n  -DOPENCL_HEADERS_BUILD_TESTING=OFF `\n  -DOPENCL_HEADERS_BUILD_CXX_TESTS=OFF `\n  -DCMAKE_INSTALL_PREFIX=\"$HOME/dev/llm/opencl\"\ncmake --build . --target install\n\ncd ~/dev/llm\ngit clone https://github.com/KhronosGroup/OpenCL-ICD-Loader && cd OpenCL-ICD-Loader\nmkdir build && cd build\ncmake .. -G Ninja `\n  -DCMAKE_BUILD_TYPE=Release `\n  -DCMAKE_PREFIX_PATH=\"$HOME/dev/llm/opencl\" `\n  -DCMAKE_INSTALL_PREFIX=\"$HOME/dev/llm/opencl\"\ncmake --build . --target install\n```\n\n----------------------------------------\n\nTITLE: Single Line Pattern in GBNF\nDESCRIPTION: Shows how to define a pattern that matches any characters except newline using negated character ranges.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/grammars/README.md#2025-04-22_snippet_1\n\nLANGUAGE: gbnf\nCODE:\n```\nsingle-line ::= [^\\n]+ \"\\n\"\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake Build for llama-simple-chat Executable\nDESCRIPTION: Sets up CMake build configuration for a simple chat application. Links against the llama library and pthread, requires C++17 support, and configures installation targets.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/simple-chat/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET llama-simple-chat)\nadd_executable(${TARGET} simple-chat.cpp)\ninstall(TARGETS ${TARGET} RUNTIME)\ntarget_link_libraries(${TARGET} PRIVATE llama ${CMAKE_THREAD_LIBS_INIT})\ntarget_compile_features(${TARGET} PRIVATE cxx_std_17)\n```\n\n----------------------------------------\n\nTITLE: Configuring and Compiling Test Binaries\nDESCRIPTION: These commands set up the build environment with debug mode enabled and compile the test binaries. It uses CMake with specific flags for debugging, CUDA support, and fatal warnings, followed by a multi-threaded make command.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/development/debugging-tests.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncmake -DCMAKE_BUILD_TYPE=Debug -DLLAMA_CUDA=1 -DLLAMA_FATAL_WARNINGS=ON ..\nmake -j\n```\n\n----------------------------------------\n\nTITLE: Setting up Git-based Build Information for llama.cpp in CMake\nDESCRIPTION: Detects Git repository information and configures automatic build information generation. Handles both regular Git repositories and Git submodules by locating the .git directory and index file.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/common/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif(EXISTS \"${CMAKE_CURRENT_SOURCE_DIR}/../.git\")\n    set(GIT_DIR \"${CMAKE_CURRENT_SOURCE_DIR}/../.git\")\n\n    # Is git submodule\n    if(NOT IS_DIRECTORY \"${GIT_DIR}\")\n        file(READ ${GIT_DIR} REAL_GIT_DIR_LINK)\n        string(REGEX REPLACE \"gitdir: (.*)\\n$\" \"\\\\1\" REAL_GIT_DIR ${REAL_GIT_DIR_LINK})\n        string(FIND \"${REAL_GIT_DIR}\" \"/\" SLASH_POS)\n        if (SLASH_POS EQUAL 0)\n            set(GIT_DIR \"${REAL_GIT_DIR}\")\n        else()\n            set(GIT_DIR \"${CMAKE_CURRENT_SOURCE_DIR}/../${REAL_GIT_DIR}\")\n        endif()\n    endif()\n\n    if(EXISTS \"${GIT_DIR}/index\")\n        set(GIT_INDEX \"${GIT_DIR}/index\")\n    else()\n        message(WARNING \"Git index not found in git repository.\")\n        set(GIT_INDEX \"\")\n    endif()\nelse()\n    message(WARNING \"Git repository not found; to enable automatic generation of build info, make sure Git is installed and the project is a Git repository.\")\n    set(GIT_INDEX \"\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake Build for LLaMA Quantization Stats Tool\nDESCRIPTION: Configures CMake build settings for the llama-quantize-stats executable. Sets up target dependencies, links required libraries including llama and build_info, and configures include directories and C++17 standard requirement.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/quantize-stats/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET llama-quantize-stats)\nadd_executable(${TARGET} quantize-stats.cpp)\ninstall(TARGETS ${TARGET} RUNTIME)\ntarget_link_libraries(${TARGET} PRIVATE llama build_info ${CMAKE_THREAD_LIBS_INIT})\ntarget_include_directories(${TARGET} PRIVATE ../../common)\ntarget_compile_features(${TARGET} PRIVATE cxx_std_17)\n```\n\n----------------------------------------\n\nTITLE: Specifying CFFI Library Version Requirement\nDESCRIPTION: Defines the dependency on the CFFI package with version 1.16.0 using the compatible release operator (~=). This ensures the installed version will be at least 1.16.0 but less than 1.17.0.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/requirements/requirements-test-tokenizer-random.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ncffi~=1.16.0\n```\n\n----------------------------------------\n\nTITLE: Adding GGML Kompute Backend Library in CMake\nDESCRIPTION: This snippet adds the GGML Kompute backend library to the project, linking it with the necessary dependencies and including the required headers.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-kompute/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nggml_add_backend_library(ggml-kompute\n                         ggml-kompute.cpp\n                         ../../include/ggml-kompute.h\n                        )\n\ntarget_link_libraries(ggml-kompute PRIVATE ggml-base kompute)\ntarget_include_directories(ggml-kompute PRIVATE ${CMAKE_CURRENT_BINARY_DIR})\n\nadd_compile_definitions(VULKAN_HPP_DISPATCH_LOADER_DYNAMIC=1)\n```\n\n----------------------------------------\n\nTITLE: Building the Llama.cpp Server\nDESCRIPTION: Commands to build the llama-server target using CMake. This must be done before running the tests.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/tests/README.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncd ../../..\ncmake -B build\ncmake --build build --target llama-server\n```\n\n----------------------------------------\n\nTITLE: Converting PyTorch Model to GGUF Format for MiniCPM-Llama3-V 2.5\nDESCRIPTION: Script commands to convert the PyTorch model to GGUF format and quantize it. This includes extracting the projector, converting the image encoder, and creating a quantized int4 version of the model.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/minicpmv2.5.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython ./examples/llava/minicpmv-surgery.py -m ../MiniCPM-Llama3-V-2_5\npython ./examples/llava/minicpmv-convert-image-encoder-to-gguf.py -m ../MiniCPM-Llama3-V-2_5 --minicpmv-projector ../MiniCPM-Llama3-V-2_5/minicpmv.projector --output-dir ../MiniCPM-Llama3-V-2_5/ --image-mean 0.5 0.5 0.5 --image-std 0.5 0.5 0.5 --minicpmv_version 2\npython ./convert_hf_to_gguf.py ../MiniCPM-Llama3-V-2_5/model\n\n# quantize int4 version\n./build/bin/llama-quantize ../MiniCPM-Llama3-V-2_5/model/model-8B-F16.gguf ../MiniCPM-Llama3-V-2_5/model/ggml-model-Q4_K_M.gguf Q4_K_M\n```\n\n----------------------------------------\n\nTITLE: Detecting Buggy Apple Linker\nDESCRIPTION: Checks for a specific version of Apple's ld64 linker that has known bugs and sets a compiler definition to handle it.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/CMakeLists.txt#2025-04-22_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\n# this version of Apple ld64 is buggy\nexecute_process(\n    COMMAND ${CMAKE_C_COMPILER} ${CMAKE_EXE_LINKER_FLAGS} -Wl,-v\n    ERROR_VARIABLE output\n    OUTPUT_QUIET\n)\n\nif (output MATCHES \"dyld-1015\\\\.7\")\n    add_compile_definitions(HAVE_BUGGY_APPLE_LINKER)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Starting Docker Container for MUSA CI\nDESCRIPTION: Command to start a Docker container for running MUSA CI with necessary volume mounts for caching models, storing results, and accessing the workspace.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ci/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --privileged -it \\\n    -v $HOME/llama.cpp/ci-cache:/ci-cache \\\n    -v $HOME/llama.cpp/ci-results:/ci-results \\\n    -v $PWD:/ws -w /ws \\\n    mthreads/musa:rc3.1.1-devel-ubuntu22.04\n```\n\n----------------------------------------\n\nTITLE: Building llama.cpp with CANN Support for Ascend NPU\nDESCRIPTION: Commands to build llama.cpp with CANN support for Ascend NPU acceleration using CMake.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\ncmake -B build -DGGML_CANN=on -DCMAKE_BUILD_TYPE=release\ncmake --build build --config release\n```\n\n----------------------------------------\n\nTITLE: Configuring and Building llama-run Executable with CMake\nDESCRIPTION: Sets up the llama-run target by specifying source files, handling optional CURL dependency, configuring installation, linking required libraries, and setting C++17 as the required standard. Includes conditional compilation for CURL support.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/run/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET llama-run)\nadd_executable(${TARGET} run.cpp linenoise.cpp/linenoise.cpp)\n\n# TODO: avoid copying this code block from common/CMakeLists.txt\nset(LLAMA_RUN_EXTRA_LIBS \"\")\nif (LLAMA_CURL)\n    find_package(CURL REQUIRED)\n    target_compile_definitions(${TARGET} PUBLIC LLAMA_USE_CURL)\n    include_directories(${CURL_INCLUDE_DIRS})\n    find_library(CURL_LIBRARY curl REQUIRED)\n    set(LLAMA_RUN_EXTRA_LIBS ${LLAMA_RUN_EXTRA_LIBS} ${CURL_LIBRARY})\nendif ()\n\ninstall(TARGETS ${TARGET} RUNTIME)\ntarget_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT} ${LLAMA_RUN_EXTRA_LIBS})\ntarget_compile_features(${TARGET} PRIVATE cxx_std_17)\n```\n\n----------------------------------------\n\nTITLE: Building GGML Library and Components in CMake\nDESCRIPTION: This section builds the main GGML library and conditionally adds tests and examples based on build options. It includes necessary subdirectories for source files, tests, and examples.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/CMakeLists.txt#2025-04-22_snippet_17\n\nLANGUAGE: CMake\nCODE:\n```\n#\n# build the library\n#\n\nadd_subdirectory(src)\n\n#\n# tests and examples\n#\n\nif (GGML_BUILD_TESTS)\n    enable_testing()\n    add_subdirectory(tests)\nendif ()\n\nif (GGML_BUILD_EXAMPLES)\n    add_subdirectory(examples)\nendif ()\n```\n\n----------------------------------------\n\nTITLE: Configuring and Building llama-gen-docs Documentation Generator with CMake\nDESCRIPTION: This CMake snippet defines the build configuration for the llama-gen-docs executable. It sets the target name, creates the executable from gen-docs.cpp, configures installation, links necessary libraries including common, llama, and threading libraries, and ensures C++17 standard compliance.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/gen-docs/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET llama-gen-docs)\nadd_executable(${TARGET} gen-docs.cpp)\ninstall(TARGETS ${TARGET} RUNTIME)\ntarget_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})\ntarget_compile_features(${TARGET} PRIVATE cxx_std_17)\n```\n\n----------------------------------------\n\nTITLE: Configuring llama-lookahead Target in CMake\nDESCRIPTION: This CMake configuration sets up the llama-lookahead executable target. It specifies lookahead.cpp as the source file, sets up installation rules, links required libraries (common, llama, and threading libraries), and specifies C++17 as the required standard.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/lookahead/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET llama-lookahead)\nadd_executable(${TARGET} lookahead.cpp)\ninstall(TARGETS ${TARGET} RUNTIME)\ntarget_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})\ntarget_compile_features(${TARGET} PRIVATE cxx_std_17)\n```\n\n----------------------------------------\n\nTITLE: Starting RPC Server with CUDA Backend\nDESCRIPTION: Command to start the RPC server using the CUDA backend. It specifies the port number for the server to listen on.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/rpc/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ bin/rpc-server -p 50052\n```\n\n----------------------------------------\n\nTITLE: Configuring RPC Backend in CMake\nDESCRIPTION: Sets up the RPC backend library using CMake. Includes status message, library addition, and Windows-specific socket library linking.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-rpc/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nmessage(STATUS \"Using RPC backend\")\n\nggml_add_backend_library(ggml-rpc\n                         ggml-rpc.cpp\n                        )\n\nif (WIN32)\n    target_link_libraries(ggml-rpc PRIVATE ws2_32)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Generating Visual Studio Solution for llama.cpp with SYCL (Intel C++ Compiler)\nDESCRIPTION: CMake command to generate a Visual Studio solution for llama.cpp with SYCL support using the Intel C++ Compiler.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_22\n\nLANGUAGE: powershell\nCODE:\n```\ncmake -B build -G \"Visual Studio 17 2022\" -T \"Intel C++ Compiler 2025\" -A x64 -DGGML_SYCL=ON -DCMAKE_BUILD_TYPE=Release\n```\n\n----------------------------------------\n\nTITLE: Downloading Granite Vision Model\nDESCRIPTION: Clone the Granite Vision model repository and set environment variable.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/granitevision.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ git clone https://huggingface.co/ibm-granite/granite-vision-3.2-2b\n$ export GRANITE_MODEL=./granite-vision-3.2-2b\n```\n\n----------------------------------------\n\nTITLE: Setting Native Compilation Default Based on Environment\nDESCRIPTION: Determines whether to enable native optimization based on cross-compilation status and source date epoch environment variable.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/CMakeLists.txt#2025-04-22_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nif (CMAKE_CROSSCOMPILING OR DEFINED ENV{SOURCE_DATE_EPOCH})\n    message(STATUS \"Setting GGML_NATIVE_DEFAULT to OFF\")\n    set(GGML_NATIVE_DEFAULT OFF)\nelse()\n    set(GGML_NATIVE_DEFAULT ON)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Finalizing Common Library Configuration in llama.cpp CMake\nDESCRIPTION: Completes the common module configuration by setting include directories, C++ standard, and linking dependencies. This connects the common module with the main llama library and necessary threading support.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/common/CMakeLists.txt#2025-04-22_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_include_directories(${TARGET} PUBLIC .)\ntarget_compile_features   (${TARGET} PUBLIC cxx_std_17)\ntarget_link_libraries     (${TARGET} PRIVATE ${LLAMA_COMMON_EXTRA_LIBS} PUBLIC llama Threads::Threads)\n```\n\n----------------------------------------\n\nTITLE: Configuring llama-perplexity Target in CMake\nDESCRIPTION: This CMake code snippet sets up the llama-perplexity executable target. It defines the target, adds the executable, configures installation, links required libraries, and sets the C++ standard to C++17.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/perplexity/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET llama-perplexity)\nadd_executable(${TARGET} perplexity.cpp)\ninstall(TARGETS ${TARGET} RUNTIME)\ntarget_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})\ntarget_compile_features(${TARGET} PRIVATE cxx_std_17)\n```\n\n----------------------------------------\n\nTITLE: Building simple-cmake-pkg Example Using llama.cpp Package\nDESCRIPTION: Commands to build the simple-cmake-pkg example by specifying the location of the CMake package with CMAKE_PREFIX_PATH. This demonstrates how to use find_package() to locate the installed llama.cpp package.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/simple-cmake-pkg/README.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncd examples/simple-cmake-pkg\ncmake -S . -B build -DCMAKE_PREFIX_PATH=../../inst/lib/cmake\ncmake --build build\n```\n\n----------------------------------------\n\nTITLE: Including Common CMake Modules for llama.cpp\nDESCRIPTION: Includes the common CMake module and defines compilation constants. Sets up the scheduler maximum copies definition.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ninclude(CheckCXXCompilerFlag)\ninclude(\"../cmake/common.cmake\")\n\nadd_compile_definitions(GGML_SCHED_MAX_COPIES=${GGML_SCHED_MAX_COPIES})\n```\n\n----------------------------------------\n\nTITLE: Accessing Command Line Arguments in Node.js\nDESCRIPTION: Demonstrates how to access command line arguments in a Node.js program using the process.argv array. The array contains the executable path, script path, and any additional arguments passed to the script.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/prompts/chat.txt#2025-04-22_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nargv[0] is the path to the Node. js executable.\nargv[1] is the path to the script file.\nargv[2] is the first argument passed to the script.\nargv[3] is the second argument passed to the script and so on.\n```\n\n----------------------------------------\n\nTITLE: CANN Library and Include Configuration\nDESCRIPTION: Sets up CANN library dependencies, include directories, and platform compatibility checks. Configures compilation options based on SOC type.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cann/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif (CANN_INSTALL_DIR)\n    if (NOT UNIX)\n        message(FATAL_ERROR \"CANN: CANN toolkit supports unix but not ${CMAKE_SYSTEM_NAME}\")\n    endif()\n\n    if (CMAKE_SYSTEM_PROCESSOR STREQUAL \"aarch64\")\n    elseif (CMAKE_SYSTEM_PROCESSOR STREQUAL \"x86_64\" OR CMAKE_SYSTEM_PROCESSOR STREQUAL \"amd64\")\n    else()\n        message(FATAL_ERROR \"CANN: CANN toolkit supports x86-64 and arm64 but not ${CMAKE_SYSTEM_PROCESSOR}\")\n    endif()\n\n    set(CANN_INCLUDE_DIRS\n        ${CANN_INSTALL_DIR}/include\n        ${CANN_INSTALL_DIR}/include/aclnn\n        ${CANN_INSTALL_DIR}/acllib/include\n    )\n\n    list(APPEND CANN_LIBRARIES\n        ascendcl\n        nnopbase\n        opapi\n        acl_op_compiler\n    )\n\n    file(GLOB GGML_SOURCES_CANN \"*.cpp\")\n\n    ggml_add_backend_library(ggml-cann ${GGML_SOURCES_CANN})\n    target_link_libraries(ggml-cann PRIVATE ${CANN_LIBRARIES})\n    target_include_directories(ggml-cann PRIVATE ${CANN_INCLUDE_DIRS})\n    target_link_directories(ggml-cann PRIVATE ${CANN_INSTALL_DIR}/lib64)\n\n    target_compile_definitions(ggml-cann PRIVATE \"-D${SOC_TYPE_COMPILE_OPTION}\")\n\n    message(STATUS \"CANN: CANN_INCLUDE_DIRS =  ${CANN_INCLUDE_DIRS}\")\n    message(STATUS \"CANN: CANN_LIBRARIES =  ${CANN_LIBRARIES}\")\nelse()\n    message(FATAL_ERROR \"CANN: Can't find CANN_INSTALL_DIR, did you forget to source set_var.sh?\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring Platform-Specific Tests for llama.cpp\nDESCRIPTION: Sets up conditional test compilation based on the platform. Includes grammar-related tests, tokenizer tests for BPE and SPM models, and other specialized tests. Some tests are disabled on Windows or specific architectures due to dependencies or internal function usage.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/tests/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nif (LLAMA_LLGUIDANCE)\n    llama_target_and_test(test-grammar-llguidance.cpp ARGS ${CMAKE_CURRENT_SOURCE_DIR}/../models/ggml-vocab-llama-bpe.gguf)\nendif ()\n\nif (NOT WIN32)\n    # these tests are disabled on Windows because they use internal functions not exported with LLAMA_API\n    llama_target_and_test(test-sampling.cpp)\n    llama_target_and_test(test-grammar-parser.cpp)\n    llama_target_and_test(test-grammar-integration.cpp)\n    llama_target_and_test(test-llama-grammar.cpp)\n    llama_target_and_test(test-chat.cpp)\n    # TODO: disabled on loongarch64 because the ggml-ci node lacks Python 3.8\n    if (NOT ${CMAKE_SYSTEM_PROCESSOR} MATCHES \"loongarch64\")\n        llama_target_and_test(test-json-schema-to-grammar.cpp   WORKING_DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR}/..)\n        target_include_directories(test-json-schema-to-grammar PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/../examples/server)\n    endif()\n\n\n    # build test-tokenizer-1-bpe target once and add many tests\n    add_executable(test-tokenizer-1-bpe test-tokenizer-1-bpe.cpp)\n    target_link_libraries(test-tokenizer-1-bpe PRIVATE common)\n    install(TARGETS test-tokenizer-1-bpe RUNTIME)\n\n    # TODO: disabled due to slowness\n    #llama_test(test-tokenizer-1-bpe NAME test-tokenizer-1-aquila    ARGS ${CMAKE_CURRENT_SOURCE_DIR}/../models/ggml-vocab-aquila.gguf)\n    #llama_test(test-tokenizer-1-bpe NAME test-tokenizer-1-falcon    ARGS ${CMAKE_CURRENT_SOURCE_DIR}/../models/ggml-vocab-falcon.gguf)\n    #llama_test(test-tokenizer-1-bpe NAME test-tokenizer-1-gpt-2     ARGS ${CMAKE_CURRENT_SOURCE_DIR}/../models/ggml-vocab-gpt-2.gguf)\n    #llama_test(test-tokenizer-1-bpe NAME test-tokenizer-1-gpt-neox  ARGS ${CMAKE_CURRENT_SOURCE_DIR}/../models/ggml-vocab-gpt-neox.gguf)\n    #llama_test(test-tokenizer-1-bpe NAME test-tokenizer-1-llama-bpe ARGS ${CMAKE_CURRENT_SOURCE_DIR}/../models/ggml-vocab-llama-bpe.gguf --ignore-merges)\n    #llama_test(test-tokenizer-1-bpe NAME test-tokenizer-1-mpt       ARGS ${CMAKE_CURRENT_SOURCE_DIR}/../models/ggml-vocab-mpt.gguf)\n    #llama_test(test-tokenizer-1-bpe NAME test-tokenizer-1-refact    ARGS ${CMAKE_CURRENT_SOURCE_DIR}/../models/ggml-vocab-refact.gguf)\n    #llama_test(test-tokenizer-1-bpe NAME test-tokenizer-1-starcoder ARGS ${CMAKE_CURRENT_SOURCE_DIR}/../models/ggml-vocab-starcoder.gguf)\n\n    # build test-tokenizer-1-spm target once and add many tests\n    add_executable(test-tokenizer-1-spm test-tokenizer-1-spm.cpp)\n    target_link_libraries(test-tokenizer-1-spm PRIVATE common)\n    install(TARGETS test-tokenizer-1-spm RUNTIME)\n\n    llama_test(test-tokenizer-1-spm  NAME test-tokenizer-1-llama-spm ARGS ${CMAKE_CURRENT_SOURCE_DIR}/../models/ggml-vocab-llama-spm.gguf)\n    #llama_test(test-tokenizer-1-spm  NAME test-tokenizer-1-baichuan  ARGS ${CMAKE_CURRENT_SOURCE_DIR}/../models/ggml-vocab-baichuan.gguf)\n\n    # llama_target_and_test(test-double-float.cpp) # SLOW\nendif()\n```\n\n----------------------------------------\n\nTITLE: Verifying SYCL Installation and Environment\nDESCRIPTION: Commands to source oneAPI environment variables and list available SYCL devices.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\nsource /opt/intel/oneapi/setvars.sh\nsycl-ls\n```\n\n----------------------------------------\n\nTITLE: Configuring llama-batched Executable in CMake\nDESCRIPTION: Sets up the llama-batched executable target in CMake. It specifies the source file, configures installation, links required libraries including common, llama, and threading libraries, and sets the C++ standard to C++17.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/batched/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET llama-batched)\nadd_executable(${TARGET} batched.cpp)\ninstall(TARGETS ${TARGET} RUNTIME)\ntarget_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})\ntarget_compile_features(${TARGET} PRIVATE cxx_std_17)\n```\n\n----------------------------------------\n\nTITLE: Building and Running llama-batched-swift\nDESCRIPTION: Commands to compile and execute the Swift version of the batched llama example. The program takes a model path as required argument, with optional prompt and parallel processing parameters.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/batched.swift/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nmake\n```\n\nLANGUAGE: shell\nCODE:\n```\n./llama-batched-swift MODEL_PATH [PROMPT] [PARALLEL]\n```\n\n----------------------------------------\n\nTITLE: Configuring LLaMA Lookup Tools in CMake\nDESCRIPTION: This CMake configuration defines four targets related to lookup functionality in the llama.cpp project. Each target is set up with the same dependencies (common, llama, and thread libraries) and requires C++17 support. The targets are also configured for installation.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/lookup/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET llama-lookup)\nadd_executable(${TARGET} lookup.cpp)\ninstall(TARGETS ${TARGET} RUNTIME)\ntarget_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})\ntarget_compile_features(${TARGET} PRIVATE cxx_std_17)\n\nset(TARGET llama-lookup-create)\nadd_executable(${TARGET} lookup-create.cpp)\ninstall(TARGETS ${TARGET} RUNTIME)\ntarget_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})\ntarget_compile_features(${TARGET} PRIVATE cxx_std_17)\n\nset(TARGET llama-lookup-merge)\nadd_executable(${TARGET} lookup-merge.cpp)\ninstall(TARGETS ${TARGET} RUNTIME)\ntarget_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})\ntarget_compile_features(${TARGET} PRIVATE cxx_std_17)\n\nset(TARGET llama-lookup-stats)\nadd_executable(${TARGET} lookup-stats.cpp)\ninstall(TARGETS ${TARGET} RUNTIME)\ntarget_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})\ntarget_compile_features(${TARGET} PRIVATE cxx_std_17)\n```\n\n----------------------------------------\n\nTITLE: Configuring GGML CPU Backend Sources and Compilation\nDESCRIPTION: Sets up source files, compiler flags, and build configurations for the GGML CPU backend. Handles special cases for KLEIDIAI sources, architecture-specific flags, and feature detection compilation. Includes support for dynamic loading and Emscripten compilation.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cpu/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nlist(APPEND GGML_KLEIDIAI_SOURCES ${KLEIDIAI_SRC}/kai/ukernels/matmul/matmul_clamp_f32_qsi8d32p_qsi4c32p/kai_matmul_clamp_f32_qsi8d32p1vlx4_qsi4c32p4vlx4_1vlx4vl_sme2_mopa.c)\nlist(APPEND GGML_KLEIDIAI_SOURCES ${KLEIDIAI_SRC}/kai/ukernels/matmul/matmul_clamp_f32_qsi8d32p_qsi4c32p/kai_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4vlx4_1x4vl_sme2_sdot.c)\nset(PRIVATE_ARCH_FLAGS \"${PRIVATE_ARCH_FLAGS}+sve+sve2\")\n\nset_source_files_properties(${GGML_KLEIDIAI_SOURCES} PROPERTIES COMPILE_OPTIONS \"${PRIVATE_ARCH_FLAGS}\")\nlist(APPEND GGML_CPU_SOURCES ${GGML_KLEIDIAI_SOURCES})\n\nmessage(STATUS \"Adding CPU backend variant ${GGML_CPU_NAME}: ${ARCH_FLAGS} ${ARCH_DEFINITIONS}\")\ntarget_sources(${GGML_CPU_NAME} PRIVATE ${GGML_CPU_SOURCES})\ntarget_compile_options(${GGML_CPU_NAME} PRIVATE ${ARCH_FLAGS})\ntarget_compile_definitions(${GGML_CPU_NAME} PRIVATE ${ARCH_DEFINITIONS})\n\nif (GGML_BACKEND_DL)\n    if (GGML_NATIVE)\n        message(FATAL_ERROR \"GGML_NATIVE is not compatible with GGML_BACKEND_DL, consider using GGML_CPU_ALL_VARIANTS\")\n    endif()\n\n    set(GGML_CPU_FEATS_NAME ${GGML_CPU_NAME}-feats)\n    add_library(${GGML_CPU_FEATS_NAME} OBJECT ggml-cpu/cpu-feats-x86.cpp)\n    target_include_directories(${GGML_CPU_FEATS_NAME} PRIVATE . .. ../include)\n    target_compile_definitions(${GGML_CPU_FEATS_NAME} PRIVATE ${ARCH_DEFINITIONS})\n    target_compile_definitions(${GGML_CPU_FEATS_NAME} PRIVATE GGML_BACKEND_DL GGML_BACKEND_BUILD GGML_BACKEND_SHARED)\n    set_target_properties(${GGML_CPU_FEATS_NAME} PROPERTIES POSITION_INDEPENDENT_CODE ON)\n    target_link_libraries(${GGML_CPU_NAME} PRIVATE ${GGML_CPU_FEATS_NAME})\nendif()\n\nif (EMSCRIPTEN)\n    set_target_properties(${GGML_CPU_NAME} PROPERTIES COMPILE_FLAGS \"-msimd128\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Enabling Fatal Warnings Across Different Compilers\nDESCRIPTION: Configures compiler flags to treat warnings as errors when GGML_FATAL_WARNINGS is enabled. Works with GCC, Clang, and MSVC.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nif (GGML_FATAL_WARNINGS)\n    if (CMAKE_CXX_COMPILER_ID MATCHES \"GNU\" OR CMAKE_CXX_COMPILER_ID MATCHES \"Clang\")\n        list(APPEND C_FLAGS   -Werror)\n        list(APPEND CXX_FLAGS -Werror)\n    elseif (CMAKE_CXX_COMPILER_ID STREQUAL \"MSVC\")\n        add_compile_options(/WX)\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Cloning LLaVA 1.6 Model\nDESCRIPTION: Git command to clone the LLaVA 1.6 model repository from Hugging Face.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/llava.md#2025-04-22_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\ngit clone https://huggingface.co/liuhaotian/llava-v1.6-vicuna-7b\n```\n\n----------------------------------------\n\nTITLE: CANN Installation Directory Detection\nDESCRIPTION: Checks for CANN installation directory using environment variables CANN_INSTALL_DIR or ASCEND_TOOLKIT_HOME.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cann/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif (\"cann${CANN_INSTALL_DIR}\" STREQUAL \"cann\" AND DEFINED ENV{ASCEND_TOOLKIT_HOME})\n    set(CANN_INSTALL_DIR $ENV{ASCEND_TOOLKIT_HOME})\n    message(STATUS \"CANN: updated CANN_INSTALL_DIR from ASCEND_TOOLKIT_HOME=$ENV{ASCEND_TOOLKIT_HOME}\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring and Building llama-tts Target in CMake\nDESCRIPTION: Sets up a CMake target for the text-to-speech (llama-tts) executable. It defines the target name, specifies the source file, configures installation, links required libraries including llama and common components, and sets C++17 as the required standard.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/tts/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET llama-tts)\nadd_executable(${TARGET} tts.cpp)\ninstall(TARGETS ${TARGET} RUNTIME)\ntarget_link_libraries(${TARGET} PRIVATE llama common ${CMAKE_THREAD_LIBS_INIT})\ntarget_compile_features(${TARGET} PRIVATE cxx_std_17)\n```\n\n----------------------------------------\n\nTITLE: Running Cross-compiled llama.cpp on Android\nDESCRIPTION: Commands to execute the cross-compiled llama-simple binary on the Android device with the proper library path and parameters.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/android.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n$ cd /data/local/tmp/llama.cpp\n$ LD_LIBRARY_PATH=lib ./bin/llama-simple -m {model}.gguf -c {context-size} -p \"{your-prompt}\"\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake Build for llama-batched-bench\nDESCRIPTION: Defines and configures a CMake target for building a batched benchmark executable. Sets up the target with required dependencies including common library, llama library, and threading libraries. Specifies C++17 as the required standard and configures installation rules.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/batched-bench/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET llama-batched-bench)\nadd_executable(${TARGET} batched-bench.cpp)\ninstall(TARGETS ${TARGET} RUNTIME)\ntarget_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})\ntarget_compile_features(${TARGET} PRIVATE cxx_std_17)\n```\n\n----------------------------------------\n\nTITLE: Installing NVIDIA Driver Libraries\nDESCRIPTION: Commands to install NVIDIA driver libraries and update RPM database\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/CUDA-FEDORA.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nsudo dnf install nvidia-driver-cuda nvidia-driver-libs nvidia-driver-cuda-libs nvidia-persistenced\nsudo dnf download --destdir=/tmp/nvidia-driver-libs --resolve --arch x86_64 nvidia-driver-cuda nvidia-driver-libs nvidia-driver-cuda-libs nvidia-persistenced\nsudo rpm --install --verbose --hash --justdb /tmp/nvidia-driver-libs/*\n```\n\n----------------------------------------\n\nTITLE: Configuring cURL Integration in llama.cpp CMake\nDESCRIPTION: Adds cURL support for downloading models if LLAMA_CURL is enabled. This conditional block finds the cURL package, adds necessary compiler definitions, and links against the cURL library.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/common/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\n# Use curl to download model url\nif (LLAMA_CURL)\n    find_package(CURL)\n    if (NOT CURL_FOUND)\n        message(FATAL_ERROR \"Could NOT find CURL. Hint: to disable this feature, set -DLLAMA_CURL=OFF\")\n    endif()\n    target_compile_definitions(${TARGET} PUBLIC LLAMA_USE_CURL)\n    include_directories(${CURL_INCLUDE_DIRS})\n    find_library(CURL_LIBRARY curl REQUIRED)\n    set(LLAMA_COMMON_EXTRA_LIBS ${LLAMA_COMMON_EXTRA_LIBS} ${CURL_LIBRARY})\nendif ()\n```\n\n----------------------------------------\n\nTITLE: Setting POSIX Conformance Definitions\nDESCRIPTION: Configures POSIX conformance levels and system-specific extensions for various operating systems to ensure availability of required functions.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/CMakeLists.txt#2025-04-22_snippet_9\n\nLANGUAGE: CMake\nCODE:\n```\n#\n# POSIX conformance\n#\n\n# clock_gettime came in POSIX.1b (1993)\n# CLOCK_MONOTONIC came in POSIX.1-2001 / SUSv3 as optional\n# posix_memalign came in POSIX.1-2001 / SUSv3\n# M_PI is an XSI extension since POSIX.1-2001 / SUSv3, came in XPG1 (1985)\n\n# Somehow in OpenBSD whenever POSIX conformance is specified\n# some string functions rely on locale_t availability,\n# which was introduced in POSIX.1-2008, forcing us to go higher\nif (CMAKE_SYSTEM_NAME MATCHES \"OpenBSD\")\n    add_compile_definitions(_XOPEN_SOURCE=700)\nelse()\n    add_compile_definitions(_XOPEN_SOURCE=600)\nendif()\n\n# Data types, macros and functions related to controlling CPU affinity and\n# some memory allocation are available on Linux through GNU extensions in libc\nif (CMAKE_SYSTEM_NAME MATCHES \"Linux\" OR CMAKE_SYSTEM_NAME MATCHES \"Android\")\n    add_compile_definitions(_GNU_SOURCE)\nendif()\n\n# RLIMIT_MEMLOCK came in BSD, is not specified in POSIX.1,\n# and on macOS its availability depends on enabling Darwin extensions\n# similarly on DragonFly, enabling BSD extensions is necessary\nif (\n    CMAKE_SYSTEM_NAME MATCHES \"Darwin\" OR\n    CMAKE_SYSTEM_NAME MATCHES \"iOS\"    OR\n    CMAKE_SYSTEM_NAME MATCHES \"tvOS\"   OR\n    CMAKE_SYSTEM_NAME MATCHES \"DragonFly\"\n)\n    add_compile_definitions(_DARWIN_C_SOURCE)\nendif()\n\n# alloca is a non-standard interface that is not visible on BSDs when\n# POSIX conformance is specified, but not all of them provide a clean way\n# to enable it in such cases\nif (CMAKE_SYSTEM_NAME MATCHES \"FreeBSD\")\n    add_compile_definitions(__BSD_VISIBLE)\nendif()\nif (CMAKE_SYSTEM_NAME MATCHES \"NetBSD\")\n    add_compile_definitions(_NETBSD_SOURCE)\nendif()\nif (CMAKE_SYSTEM_NAME MATCHES \"OpenBSD\")\n    add_compile_definitions(_BSD_SOURCE)\nendif()\n\nif (WIN32)\n    add_compile_definitions(_CRT_SECURE_NO_WARNINGS)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies in Termux for Android\nDESCRIPTION: Commands to update the Termux environment and install necessary build dependencies for llama.cpp on Android.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/android.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ apt update && apt upgrade -y\n$ apt install git cmake\n```\n\n----------------------------------------\n\nTITLE: Configuring Sanitizer Options for GGML\nDESCRIPTION: Sets up options for enabling various sanitizers (thread, address, undefined behavior) during compilation.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/CMakeLists.txt#2025-04-22_snippet_9\n\nLANGUAGE: CMake\nCODE:\n```\n# sanitizers\noption(GGML_SANITIZE_THREAD    \"ggml: enable thread sanitizer\"    OFF)\noption(GGML_SANITIZE_ADDRESS   \"ggml: enable address sanitizer\"   OFF)\noption(GGML_SANITIZE_UNDEFINED \"ggml: enable undefined sanitizer\" OFF)\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake Build for Speculative Simple Target\nDESCRIPTION: Sets up a CMake target for the speculative-simple executable, linking it with the common and llama libraries. Configures C++17 as the required standard and sets up installation rules.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/speculative-simple/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET llama-speculative-simple)\nadd_executable(${TARGET} speculative-simple.cpp)\ninstall(TARGETS ${TARGET} RUNTIME)\ntarget_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})\ntarget_compile_features(${TARGET} PRIVATE cxx_std_17)\n```\n\n----------------------------------------\n\nTITLE: Running k6 Benchmark with Custom Environment Variables\nDESCRIPTION: Example of running the benchmark with custom environment variables to override default settings like number of prompts.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/bench/README.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nSERVER_BENCH_N_PROMPTS=500 k6 run script.js --duration 10m --iterations 500 --vus 8\n```\n\n----------------------------------------\n\nTITLE: Configuring llama-retrieval target in CMake\nDESCRIPTION: Sets up the llama-retrieval executable target with proper linking, installation configuration, and compiler feature requirements. The target depends on common and llama libraries, along with platform-specific threading libraries.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/retrieval/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET llama-retrieval)\nadd_executable(${TARGET} retrieval.cpp)\ninstall(TARGETS ${TARGET} RUNTIME)\ntarget_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})\ntarget_compile_features(${TARGET} PRIVATE cxx_std_17)\n```\n\n----------------------------------------\n\nTITLE: Building llama.cpp with Debug Configuration\nDESCRIPTION: Commands to build llama.cpp in debug mode using CMake. This includes examples for both single-config and multi-config generators.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncmake -B build -DCMAKE_BUILD_TYPE=Debug\ncmake --build build\n```\n\nLANGUAGE: bash\nCODE:\n```\ncmake -B build -G \"Xcode\"\ncmake --build build --config Debug\n```\n\n----------------------------------------\n\nTITLE: Setting Up Visual Encoder Directory\nDESCRIPTION: Create directory and copy required files for visual encoder.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/granitevision.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ ENCODER_PATH=$PWD/visual_encoder\n$ mkdir $ENCODER_PATH\n\n$ cp $GRANITE_MODEL/llava.clip $ENCODER_PATH/pytorch_model.bin\n$ cp $GRANITE_MODEL/llava.projector $ENCODER_PATH/\n```\n\n----------------------------------------\n\nTITLE: Finding Vulkan Package and GLSLC Compiler in CMake\nDESCRIPTION: This snippet locates the Vulkan package and the GLSLC shader compiler. It's essential for compiling shaders in the project.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-kompute/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfind_package(Vulkan COMPONENTS glslc REQUIRED)\nfind_program(glslc_executable NAMES glslc HINTS Vulkan::glslc)\n\nif (NOT glslc_executable)\n    message(FATAL_ERROR \"glslc not found\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Collecting CUDA Source Files for Compilation\nDESCRIPTION: Gathers CUDA source files for compilation, including header files, core CUDA files, and specialized template instances based on configuration options.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cuda/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB   GGML_HEADERS_CUDA \"*.cuh\")\nlist(APPEND GGML_HEADERS_CUDA \"../../include/ggml-cuda.h\")\n\nfile(GLOB   GGML_SOURCES_CUDA \"*.cu\")\nfile(GLOB   SRCS \"template-instances/fattn-mma*.cu\")\nlist(APPEND GGML_SOURCES_CUDA ${SRCS})\nfile(GLOB   SRCS \"template-instances/mmq*.cu\")\nlist(APPEND GGML_SOURCES_CUDA ${SRCS})\n\nif (GGML_CUDA_FA_ALL_QUANTS)\n    file(GLOB   SRCS \"template-instances/fattn-vec*.cu\")\n    list(APPEND GGML_SOURCES_CUDA ${SRCS})\n    add_compile_definitions(GGML_CUDA_FA_ALL_QUANTS)\nelse()\n    file(GLOB   SRCS \"template-instances/fattn-vec*q4_0-q4_0.cu\")\n    list(APPEND GGML_SOURCES_CUDA ${SRCS})\n    file(GLOB   SRCS \"template-instances/fattn-vec*q8_0-q8_0.cu\")\n    list(APPEND GGML_SOURCES_CUDA ${SRCS})\n    file(GLOB   SRCS \"template-instances/fattn-vec*f16-f16.cu\")\n    list(APPEND GGML_SOURCES_CUDA ${SRCS})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Enum Definition Example for Vocab Types in C++\nDESCRIPTION: Example of proper enum naming convention where enum values are in uppercase and prefixed with the enum name. This demonstrates the proper pattern for defining enumerations in the llama.cpp project.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/CONTRIBUTING.md#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nenum llama_vocab_type {\n    LLAMA_VOCAB_TYPE_NONE = 0,\n    LLAMA_VOCAB_TYPE_SPM  = 1,\n    LLAMA_VOCAB_TYPE_BPE  = 2,\n    LLAMA_VOCAB_TYPE_WPM  = 3,\n    LLAMA_VOCAB_TYPE_UGM  = 4,\n    LLAMA_VOCAB_TYPE_RWKV = 5,\n};\n```\n\n----------------------------------------\n\nTITLE: Configuring Debug and Warning Options for GGML\nDESCRIPTION: Sets up debugging options including compiler warnings and profiling.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/CMakeLists.txt#2025-04-22_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\n# debug\noption(GGML_ALL_WARNINGS           \"ggml: enable all compiler warnings\"                   ON)\noption(GGML_ALL_WARNINGS_3RD_PARTY \"ggml: enable all compiler warnings in 3rd party libs\" OFF)\noption(GGML_GPROF                  \"ggml: enable gprof\"                                   OFF)\n\n# build\noption(GGML_FATAL_WARNINGS    \"ggml: enable -Werror flag\"    OFF)\n```\n\n----------------------------------------\n\nTITLE: Configuring llama-save-load-state Executable in CMake\nDESCRIPTION: This CMake code snippet sets up the llama-save-load-state executable. It defines the target, adds the executable, sets up installation, links required libraries, and specifies C++17 as the compilation standard.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/save-load-state/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET llama-save-load-state)\nadd_executable(${TARGET} save-load-state.cpp)\ninstall(TARGETS ${TARGET} RUNTIME)\ntarget_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})\ntarget_compile_features(${TARGET} PRIVATE cxx_std_17)\n```\n\n----------------------------------------\n\nTITLE: Setting Ascend Environment Variables\nDESCRIPTION: Commands to set up Ascend environment variables in the shell configuration\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/CANN.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\necho \"source ~/Ascend/ascend-toolkit/set_env.sh\" >> ~/.bashrc\nsource ~/.bashrc\n```\n\n----------------------------------------\n\nTITLE: Configuring llama2c to GGML Converter Build\nDESCRIPTION: CMake configuration that sets up the build process for a converter utility. It creates an executable target that links against common and llama libraries, requires C++17 support, and sets up installation rules.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/convert-llama2c-to-ggml/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET llama-convert-llama2c-to-ggml)\nadd_executable(${TARGET} convert-llama2c-to-ggml.cpp)\ninstall(TARGETS ${TARGET} RUNTIME)\ntarget_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})\ntarget_compile_features(${TARGET} PRIVATE cxx_std_17)\n```\n\n----------------------------------------\n\nTITLE: Setting Up ggml-blas Library and Vendor-Specific Configurations in CMake\nDESCRIPTION: This snippet handles the setup of the ggml-blas library and applies vendor-specific configurations. It includes special handling for Apple Accelerate, OpenBLAS, BLIS, ATLAS, FlexiBLAS, Intel MKL, and NVHPC.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-blas/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif (BLAS_FOUND)\n    message(STATUS \"BLAS found, Libraries: ${BLAS_LIBRARIES}\")\n\n    ggml_add_backend_library(ggml-blas\n                             ggml-blas.cpp\n                            )\n\n    if (${GGML_BLAS_VENDOR} MATCHES \"Apple\")\n        add_compile_definitions(ACCELERATE_NEW_LAPACK)\n        add_compile_definitions(ACCELERATE_LAPACK_ILP64)\n        add_compile_definitions(GGML_BLAS_USE_ACCELERATE)\n    elseif (\"${BLAS_INCLUDE_DIRS}\" STREQUAL \"\")\n        # BLAS_INCLUDE_DIRS is missing in FindBLAS.cmake.\n        # see https://gitlab.kitware.com/cmake/cmake/-/issues/20268\n        find_package(PkgConfig REQUIRED)\n        if (${GGML_BLAS_VENDOR} MATCHES \"Generic\")\n            pkg_check_modules(DepBLAS blas)\n        elseif (${GGML_BLAS_VENDOR} MATCHES \"OpenBLAS\")\n            # As of openblas v0.3.22, the 64-bit is named openblas64.pc\n            pkg_check_modules(DepBLAS openblas64)\n            if (NOT DepBLAS_FOUND)\n                pkg_check_modules(DepBLAS openblas)\n            endif()\n        elseif (${GGML_BLAS_VENDOR} MATCHES \"FLAME\")\n            add_compile_definitions(GGML_BLAS_USE_BLIS)\n            pkg_check_modules(DepBLAS blis)\n        elseif (${GGML_BLAS_VENDOR} MATCHES \"ATLAS\")\n            pkg_check_modules(DepBLAS blas-atlas)\n        elseif (${GGML_BLAS_VENDOR} MATCHES \"FlexiBLAS\")\n            pkg_check_modules(DepBLAS flexiblas_api)\n        elseif (${GGML_BLAS_VENDOR} MATCHES \"Intel\")\n            add_compile_definitions(GGML_BLAS_USE_MKL)\n            # all Intel* libraries share the same include path\n            pkg_check_modules(DepBLAS mkl-sdl)\n        elseif (${GGML_BLAS_VENDOR} MATCHES \"NVHPC\")\n            # this doesn't provide pkg-config\n            # suggest to assign BLAS_INCLUDE_DIRS on your own\n            if (\"${NVHPC_VERSION}\" STREQUAL \"\")\n                message(WARNING \"Better to set NVHPC_VERSION\")\n            else()\n                set(DepBLAS_FOUND ON)\n                set(DepBLAS_INCLUDE_DIRS \"/opt/nvidia/hpc_sdk/${CMAKE_SYSTEM_NAME}_${CMAKE_SYSTEM_PROCESSOR}/${NVHPC_VERSION}/math_libs/include\")\n            endif()\n        endif()\n        if (DepBLAS_FOUND)\n            set(BLAS_INCLUDE_DIRS ${DepBLAS_INCLUDE_DIRS})\n        else()\n            message(WARNING \"BLAS_INCLUDE_DIRS neither been provided nor been automatically\"\n            \" detected by pkgconfig, trying to find cblas.h from possible paths...\")\n            find_path(BLAS_INCLUDE_DIRS\n                NAMES cblas.h\n                HINTS\n                    /usr/include\n                    /usr/local/include\n                    /usr/include/openblas\n                    /opt/homebrew/opt/openblas/include\n                    /usr/local/opt/openblas/include\n                    /usr/include/x86_64-linux-gnu/openblas/include\n            )\n        endif()\n    endif()\n\n    message(STATUS \"BLAS found, Includes: ${BLAS_INCLUDE_DIRS}\")\n\n    target_compile_options(ggml-blas PRIVATE ${BLAS_LINKER_FLAGS})\n\n    if (${BLAS_INCLUDE_DIRS} MATCHES \"mkl\" AND (${GGML_BLAS_VENDOR} MATCHES \"Generic\" OR ${GGML_BLAS_VENDOR} MATCHES \"Intel\"))\n        add_compile_definitions(GGML_BLAS_USE_MKL)\n    endif()\n\n    target_link_libraries     (ggml-blas PRIVATE ${BLAS_LIBRARIES})\n    target_include_directories(ggml-blas PRIVATE ${BLAS_INCLUDE_DIRS})\nelse()\n    message(ERROR \"BLAS not found, please refer to \"\n                  \"https://cmake.org/cmake/help/latest/module/FindBLAS.html#blas-lapack-vendors\"\n                  \" to set correct GGML_BLAS_VENDOR\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring Kompute and Compiling Shaders in CMake\nDESCRIPTION: This section checks for the presence of Kompute, sets up its configuration, and compiles a list of shaders using the previously defined compile_shader function. It also creates custom targets for generated shaders and ensures proper build order.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-kompute/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nif (EXISTS \"${CMAKE_CURRENT_SOURCE_DIR}/kompute/CMakeLists.txt\")\n    message(STATUS \"Kompute found\")\n    set(KOMPUTE_OPT_LOG_LEVEL Error CACHE STRING \"Kompute log level\")\n    add_subdirectory(kompute)\n\n    # Compile our shaders\n    compile_shader(SOURCES\n        kompute-shaders/op_scale.comp\n        kompute-shaders/op_scale_8.comp\n        kompute-shaders/op_add.comp\n        kompute-shaders/op_addrow.comp\n        kompute-shaders/op_mul.comp\n        kompute-shaders/op_silu.comp\n        kompute-shaders/op_relu.comp\n        kompute-shaders/op_gelu.comp\n        kompute-shaders/op_softmax.comp\n        kompute-shaders/op_norm.comp\n        kompute-shaders/op_rmsnorm.comp\n        kompute-shaders/op_diagmask.comp\n        kompute-shaders/op_mul_mat_mat_f32.comp\n        kompute-shaders/op_mul_mat_f16.comp\n        kompute-shaders/op_mul_mat_q8_0.comp\n        kompute-shaders/op_mul_mat_q4_0.comp\n        kompute-shaders/op_mul_mat_q4_1.comp\n        kompute-shaders/op_mul_mat_q4_k.comp\n        kompute-shaders/op_mul_mat_q6_k.comp\n        kompute-shaders/op_getrows_f32.comp\n        kompute-shaders/op_getrows_f16.comp\n        kompute-shaders/op_getrows_q4_0.comp\n        kompute-shaders/op_getrows_q4_1.comp\n        kompute-shaders/op_getrows_q6_k.comp\n        kompute-shaders/op_rope_norm_f16.comp\n        kompute-shaders/op_rope_norm_f32.comp\n        kompute-shaders/op_rope_neox_f16.comp\n        kompute-shaders/op_rope_neox_f32.comp\n        kompute-shaders/op_cpy_f16_f16.comp\n        kompute-shaders/op_cpy_f16_f32.comp\n        kompute-shaders/op_cpy_f32_f16.comp\n        kompute-shaders/op_cpy_f32_f32.comp\n    )\n\n    # Create a custom target for our generated shaders\n    add_custom_target(generated_shaders DEPENDS\n        shaderop_scale.h\n        shaderop_scale_8.h\n        shaderop_add.h\n        shaderop_addrow.h\n        shaderop_mul.h\n        shaderop_silu.h\n        shaderop_relu.h\n        shaderop_gelu.h\n        shaderop_softmax.h\n        shaderop_norm.h\n        shaderop_rmsnorm.h\n        shaderop_diagmask.h\n        shaderop_mul_mat_mat_f32.h\n        shaderop_mul_mat_f16.h\n        shaderop_mul_mat_q8_0.h\n        shaderop_mul_mat_q4_0.h\n        shaderop_mul_mat_q4_1.h\n        shaderop_mul_mat_q4_k.h\n        shaderop_mul_mat_q6_k.h\n        shaderop_getrows_f32.h\n        shaderop_getrows_f16.h\n        shaderop_getrows_q4_0.h\n        shaderop_getrows_q4_1.h\n        shaderop_getrows_q6_k.h\n        shaderop_rope_norm_f16.h\n        shaderop_rope_norm_f32.h\n        shaderop_rope_neox_f16.h\n        shaderop_rope_neox_f32.h\n        shaderop_cpy_f16_f16.h\n        shaderop_cpy_f16_f32.h\n        shaderop_cpy_f32_f16.h\n        shaderop_cpy_f32_f32.h\n    )\n\n    # Create a custom command that depends on the generated_shaders\n    add_custom_command(\n        OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/ggml-kompute.stamp\n        COMMAND ${CMAKE_COMMAND} -E touch ${CMAKE_CURRENT_BINARY_DIR}/ggml-kompute.stamp\n        DEPENDS generated_shaders\n        COMMENT \"Ensuring shaders are generated before compiling ggml-kompute.cpp\"\n    )\n\n    # Add the stamp to the main sources to ensure dependency tracking\n    target_sources(ggml-kompute PRIVATE ${CMAKE_CURRENT_BINARY_DIR}/ggml-kompute.stamp)\nelse()\n    message(WARNING \"Kompute not found\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Highlighting Security Note in Markdown\nDESCRIPTION: Uses Markdown syntax to create a highlighted note emphasizing the importance of assessing model trustworthiness based on specific use cases and risk tolerance.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/SECURITY.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n> [!NOTE]\n> The trustworthiness of a model is not binary. You must always determine the proper level of caution depending on the specific model and how it matches your use case and risk tolerance.\n```\n\n----------------------------------------\n\nTITLE: Finding Required HIP and ROCm Packages in CMake\nDESCRIPTION: Locates necessary HIP and ROCm packages including hip, hipblas, and rocblas. It also checks for the rocwmma library if GGML_HIP_ROCWMMA_FATTN is enabled.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-hip/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nfind_package(hip     REQUIRED)\nfind_package(hipblas REQUIRED)\nfind_package(rocblas REQUIRED)\nif (GGML_HIP_ROCWMMA_FATTN)\n    CHECK_INCLUDE_FILE_CXX(\"rocwmma/rocwmma.hpp\" FOUND_ROCWMMA)\n    if (NOT ${FOUND_ROCWMMA})\n        message(FATAL_ERROR \"rocwmma has not been found\")\n    endif()\nendif()\n\nif (${hip_VERSION} VERSION_LESS 5.5)\n    message(FATAL_ERROR \"At least ROCM/HIP V5.5 is required\")\nendif()\n\nmessage(STATUS \"HIP and hipBLAS found\")\n\n# Workaround old compilers\nset(CMAKE_HIP_FLAGS \"${CMAKE_HIP_FLAGS} --gpu-max-threads-per-block=1024\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Library Prefixes for Windows\nDESCRIPTION: Removes the default 'lib' prefix from libraries when building on Windows with MinGW.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\n# remove the lib prefix on win32 mingw\nif (WIN32)\n    set(CMAKE_STATIC_LIBRARY_PREFIX \"\")\n    set(CMAKE_SHARED_LIBRARY_PREFIX \"\")\n    set(CMAKE_SHARED_MODULE_PREFIX  \"\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting up CMake Configuration for llama-gbnf-validator in llama.cpp\nDESCRIPTION: This CMake script defines the llama-gbnf-validator executable target with its source file, installation configuration, linked libraries, and C++17 requirement. The target depends on common, llama, and system thread libraries.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/gbnf-validator/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET llama-gbnf-validator)\nadd_executable(${TARGET} gbnf-validator.cpp)\ninstall(TARGETS ${TARGET} RUNTIME)\ntarget_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})\ntarget_compile_features(${TARGET} PRIVATE cxx_std_17)\n```\n\n----------------------------------------\n\nTITLE: Installing Android NDK for llama.cpp OpenCL Development\nDESCRIPTION: Commands to download and install Android command line tools and NDK 26.3.11579264, which are prerequisites for building llama.cpp for Android with OpenCL support.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/OPENCL.md#2025-04-22_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ncd ~\nwget https://dl.google.com/android/repository/commandlinetools-linux-8512546_latest.zip && \\\nunzip commandlinetools-linux-8512546_latest.zip && \\\nmkdir -p ~/android-sdk/cmdline-tools && \\\nmv cmdline-tools latest && \\\nmv latest ~/android-sdk/cmdline-tools/ && \\\nrm -rf commandlinetools-linux-8512546_latest.zip\n\nyes | ~/android-sdk/cmdline-tools/latest/bin/sdkmanager \"ndk;26.3.11579264\"\n```\n\n----------------------------------------\n\nTITLE: Creating CMake Package and Version Info for GGML\nDESCRIPTION: This section generates version information based on Git commit data and creates a CMake package configuration. It captures GGML-specific variables, sets up installation paths, and configures package version information for downstream projects.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/CMakeLists.txt#2025-04-22_snippet_19\n\nLANGUAGE: CMake\nCODE:\n```\n#\n# Create CMake package\n#\n\n# Generate version info based on git commit.\n\nif(NOT DEFINED GGML_BUILD_NUMBER)\n    find_program(GIT_EXE NAMES git git.exe REQUIRED NO_CMAKE_FIND_ROOT_PATH)\n    execute_process(COMMAND ${GIT_EXE} rev-list --count HEAD\n        WORKING_DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR}\n        OUTPUT_VARIABLE GGML_BUILD_NUMBER\n        OUTPUT_STRIP_TRAILING_WHITESPACE\n    )\n\n    if(GGML_BUILD_NUMBER EQUAL 1)\n        message(WARNING \"GGML build version fixed at 1 likely due to a shallow clone.\")\n    endif()\n\n    execute_process(COMMAND ${GIT_EXE} rev-parse --short HEAD\n        WORKING_DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR}\n        OUTPUT_VARIABLE GGML_BUILD_COMMIT\n        OUTPUT_STRIP_TRAILING_WHITESPACE\n    )\nendif()\n\n\n# Capture variables prefixed with GGML_.\n\nset(variable_set_statements\n\"\n####### Expanded from @GGML_VARIABLES_EXPANED@ by configure_package_config_file() #######\n####### Any changes to this file will be overwritten by the next CMake run        #######\n\n\")\n\nset(GGML_SHARED_LIB ${BUILD_SHARED_LIBS})\n\nget_cmake_property(all_variables VARIABLES)\nforeach(variable_name IN LISTS all_variables)\n    if(variable_name MATCHES \"^GGML_\")\n        string(REPLACE \";\" \"\\\\;\"\n               variable_value \"${${variable_name}}\")\n\n        set(variable_set_statements\n            \"${variable_set_statements}set(${variable_name} \\\"${variable_value}\\\")\\n\")\n    endif()\nendforeach()\n\nset(GGML_VARIABLES_EXPANDED ${variable_set_statements})\n\n# Create the CMake package and set install location.\n\nset(GGML_INSTALL_VERSION 0.0.${GGML_BUILD_NUMBER})\nset(GGML_INCLUDE_INSTALL_DIR ${CMAKE_INSTALL_INCLUDEDIR} CACHE PATH \"Location of header  files\")\nset(GGML_LIB_INSTALL_DIR     ${CMAKE_INSTALL_LIBDIR}     CACHE PATH \"Location of library files\")\nset(GGML_BIN_INSTALL_DIR     ${CMAKE_INSTALL_BINDIR}     CACHE PATH \"Location of binary  files\")\n\nconfigure_package_config_file(\n        ${CMAKE_CURRENT_SOURCE_DIR}/cmake/ggml-config.cmake.in\n        ${CMAKE_CURRENT_BINARY_DIR}/ggml-config.cmake\n    INSTALL_DESTINATION ${CMAKE_INSTALL_LIBDIR}/cmake/ggml\n    PATH_VARS GGML_INCLUDE_INSTALL_DIR\n              GGML_LIB_INSTALL_DIR\n              GGML_BIN_INSTALL_DIR)\n\nwrite_basic_package_version_file(\n        ${CMAKE_CURRENT_BINARY_DIR}/ggml-version.cmake\n    VERSION ${GGML_INSTALL_VERSION}\n    COMPATIBILITY SameMajorVersion)\n\ninstall(FILES ${CMAKE_CURRENT_BINARY_DIR}/ggml-config.cmake\n              ${CMAKE_CURRENT_BINARY_DIR}/ggml-version.cmake\n        DESTINATION ${CMAKE_INSTALL_LIBDIR}/cmake/ggml)\n```\n\n----------------------------------------\n\nTITLE: Applying Single LORA Adapter to LLaMA Model\nDESCRIPTION: Demonstrates how to use the llama-export-lora tool to apply a single LORA adapter to a base LLaMA model. It specifies the input model, output file, and the LORA adapter file.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/export-lora/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./bin/llama-export-lora \\\n    -m open-llama-3b-v2.gguf \\\n    -o open-llama-3b-v2-english2tokipona-chat.gguf \\\n    --lora lora-open-llama-3b-v2-english2tokipona-chat-LATEST.gguf\n```\n\n----------------------------------------\n\nTITLE: Configuring BLAS Vendor and Finding BLAS Package in CMake\nDESCRIPTION: This snippet sets up BLAS vendor configuration and attempts to find the BLAS package. It handles static linking and sets vendor-specific options.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-blas/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif (GGML_STATIC)\n    set(BLA_STATIC ON)\nendif()\n#if (CMAKE_VERSION VERSION_GREATER_EQUAL 3.22)\n#    set(BLA_SIZEOF_INTEGER 8)\n#endif()\n\nset(BLA_VENDOR ${GGML_BLAS_VENDOR})\nfind_package(BLAS)\n```\n\n----------------------------------------\n\nTITLE: Compiling Metal Shaders for GGML in CMake\nDESCRIPTION: Sets up the compilation process for Metal shaders when GGML_METAL_EMBED_LIBRARY is not enabled, including debug options and version flags.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-metal/CMakeLists.txt#2025-04-22_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nif (GGML_METAL_SHADER_DEBUG)\n    set(XC_FLAGS -fno-fast-math -fno-inline -g)\nelse()\n    set(XC_FLAGS -O3)\nendif()\n\nif (GGML_METAL_MACOSX_VERSION_MIN)\n    message(STATUS \"Adding  -mmacosx-version-min=${GGML_METAL_MACOSX_VERSION_MIN} flag to metal compilation\")\n    list   (APPEND XC_FLAGS -mmacosx-version-min=${GGML_METAL_MACOSX_VERSION_MIN})\nendif()\n\nif (GGML_METAL_STD)\n    message(STATUS \"Adding  -std=${GGML_METAL_STD} flag to metal compilation\")\n    list   (APPEND XC_FLAGS -std=${GGML_METAL_STD})\nendif()\n\nadd_custom_command(\n    OUTPUT ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/default.metallib\n    COMMAND xcrun -sdk macosx metal ${XC_FLAGS} -c ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/ggml-metal.metal -o - |\n        xcrun -sdk macosx metallib - -o ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/default.metallib\n    COMMAND rm -f ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/ggml-common.h\n    COMMAND rm -f ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/ggml-metal.metal\n    DEPENDS ggml-metal.metal ${METALLIB_COMMON}\n    COMMENT \"Compiling Metal kernels\"\n    )\n\nadd_custom_target(\n    ggml-metal-lib ALL\n    DEPENDS ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/default.metallib\n    )\n```\n\n----------------------------------------\n\nTITLE: Setting Compile Definitions for GGML Metal Backend in CMake\nDESCRIPTION: Adds compile definitions for debug mode and BF16 support in the Metal backend.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-metal/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nif (GGML_METAL_NDEBUG)\n    add_compile_definitions(GGML_METAL_NDEBUG)\nendif()\n\nif (GGML_METAL_USE_BF16)\n    add_compile_definitions(GGML_METAL_USE_BF16)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Whitespace Definition in GBNF\nDESCRIPTION: Demonstrates how to define optional whitespace including spaces, tabs, and newlines with comments.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/grammars/README.md#2025-04-22_snippet_2\n\nLANGUAGE: gbnf\nCODE:\n```\n# defines optional whitespace\nws ::= [ \\t\\n]+\n```\n\n----------------------------------------\n\nTITLE: Configuring Shared Libraries for Different Platforms\nDESCRIPTION: Sets the default behavior for building shared or static libraries based on the target platform, with special handling for Emscripten and MinGW.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nif (EMSCRIPTEN)\n    set(BUILD_SHARED_LIBS_DEFAULT OFF)\n\n    option(GGML_WASM_SINGLE_FILE \"ggml: embed WASM inside the generated ggml.js\" ON)\nelse()\n    if (MINGW)\n        set(BUILD_SHARED_LIBS_DEFAULT OFF)\n    else()\n        set(BUILD_SHARED_LIBS_DEFAULT ON)\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring Dependencies and Third-party Libraries for llama.cpp\nDESCRIPTION: This CMake snippet configures the necessary dependencies for the llama.cpp project. It requires the Threads package, sets up include directories, and conditionally includes the vdot subdirectory based on the build environment and configuration options.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/pocs/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfind_package(Threads REQUIRED)\n\n# third-party\n\ninclude_directories(${CMAKE_CURRENT_SOURCE_DIR})\n\nif (EMSCRIPTEN)\nelse()\n    if (NOT GGML_BACKEND_DL)\n        add_subdirectory(vdot)\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Running the Llama.cpp Server Tests\nDESCRIPTION: Basic command to start the server tests using the provided test script.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/tests/README.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n./tests.sh\n```\n\n----------------------------------------\n\nTITLE: Linking Libraries to Android JNI Interface\nDESCRIPTION: Links the required libraries to the Android JNI interface, including the llama and common libraries from llama.cpp, as well as the Android system libraries needed for logging and native functionality.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llama.android/llama/src/main/cpp/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(${CMAKE_PROJECT_NAME}\n        # List libraries link to the target library\n        llama\n        common\n        android\n        log)\n```\n\n----------------------------------------\n\nTITLE: Defining OpenCL Kernel Addition Function in GGML CMake\nDESCRIPTION: Creates a function to add OpenCL kernels, either by embedding them or copying to the output directory based on configuration.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-opencl/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(ggml_opencl_add_kernel KNAME)\n    set(KERN_HDR ${CMAKE_CURRENT_BINARY_DIR}/autogenerated/${KNAME}.cl.h)\n    set(KERN_SRC ${CMAKE_CURRENT_SOURCE_DIR}/kernels/${KNAME}.cl)\n\n    if (GGML_OPENCL_EMBED_KERNELS)\n        message(STATUS \"opencl: embedding kernel ${KNAME}\")\n\n        # Python must be accessible from command line\n        add_custom_command(\n            OUTPUT ${KERN_HDR}\n            COMMAND ${Python3_EXECUTABLE} ${EMBED_KERNEL_SCRIPT} ${KERN_SRC} ${KERN_HDR}\n            DEPENDS ${KERN_SRC} ${EMBED_KERNEL_SCRIPT}\n            COMMENT \"Generate ${KERN_HDR}\"\n        )\n\n        target_sources(${TARGET_NAME} PRIVATE ${KERN_HDR})\n    else ()\n        message(STATUS \"opencl: adding kernel ${KNAME}\")\n        configure_file(${KERN_SRC} ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/${KNAME}.cl COPYONLY)\n    endif ()\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Running LLaMA.cpp Server with Custom Theme\nDESCRIPTION: Command example showing how to start the LLaMA.cpp server with a custom theme from the 'wild' directory by using the --path parameter.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/themes/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nserver --path=wild\n```\n\n----------------------------------------\n\nTITLE: Vendor-Specific SYCL Configurations\nDESCRIPTION: Sets up vendor-specific configurations for Intel, NVIDIA, and AMD targets, including warp sizes and performance optimizations.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-sycl/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nif (GGML_SYCL_TARGET STREQUAL \"NVIDIA\")\n    add_compile_definitions(GGML_SYCL_WARP_SIZE=32)\nelseif (GGML_SYCL_TARGET STREQUAL \"AMD\")\n    add_compile_definitions(GGML_SYCL_WARP_SIZE=32)\nelse()\n    add_compile_definitions(GGML_SYCL_WARP_SIZE=16)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Reverse Prompt with Input Prefix and Suffix\nDESCRIPTION: Command demonstrating the setup of reverse prompts with both input prefix and suffix for structured conversations\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_11\n\nLANGUAGE: sh\nCODE:\n```\n./llama-cli -r \"User:\" --in-prefix \" \" --in-suffix \"Assistant:\"\n```\n\n----------------------------------------\n\nTITLE: Setting Instruction Set Enablement Logic\nDESCRIPTION: Sets up a variable to control whether instruction set optimizations should be enabled by default based on GGML_NATIVE setting.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/CMakeLists.txt#2025-04-22_snippet_10\n\nLANGUAGE: CMake\nCODE:\n```\n# instruction set specific\nif (GGML_NATIVE OR NOT GGML_NATIVE_DEFAULT)\n    set(INS_ENB OFF)\nelse()\n    set(INS_ENB ON)\nendif()\n\nmessage(DEBUG \"GGML_NATIVE         : ${GGML_NATIVE}\")\nmessage(DEBUG \"GGML_NATIVE_DEFAULT : ${GGML_NATIVE_DEFAULT}\")\nmessage(DEBUG \"INS_ENB             : ${INS_ENB}\")\n```\n\n----------------------------------------\n\nTITLE: Compiling LLaMA Multi-Modal\nDESCRIPTION: Command for compiling the project using make with 32 parallel jobs.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/MobileVLM.md#2025-04-22_snippet_14\n\nLANGUAGE: sh\nCODE:\n```\nmake -j32\n```\n\n----------------------------------------\n\nTITLE: Running a Specific Test by Number\nDESCRIPTION: This command runs a specific test by its number (23 in this example) to speed up the testing loop when the test number is already known.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/development/debugging-tests.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n./scripts/debug-test.sh test 23\n```\n\n----------------------------------------\n\nTITLE: Setting up CMake Project for Android llama.cpp Integration\nDESCRIPTION: Configures the CMake project for building llama.cpp on Android. It sets the minimum required CMake version and defines the project name that will be used throughout the build process.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llama.android/llama/src/main/cpp/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.22.1)\n\nproject(\"llama-android\")\n```\n\n----------------------------------------\n\nTITLE: Building and Installing Cross-compiled llama.cpp\nDESCRIPTION: Commands to build and install the cross-compiled llama.cpp project for Android, with multi-threaded compilation support.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/android.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ cmake --build build-android --config Release -j{n}\n$ cmake --install build-android --prefix {install-dir} --config Release\n```\n\n----------------------------------------\n\nTITLE: Checking Vulkan Shader Extension Support in CMake\nDESCRIPTION: Execute process to compile test shaders and determine support for Vulkan extensions like cooperative matrix and integer dot product.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-vulkan/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nexecute_process(COMMAND ${Vulkan_GLSLC_EXECUTABLE} -o - -fshader-stage=compute --target-env=vulkan1.3 \"${CMAKE_CURRENT_SOURCE_DIR}/vulkan-shaders/test_coopmat_support.comp\"\n                OUTPUT_VARIABLE glslc_output\n                ERROR_VARIABLE glslc_error)\n\nif (${glslc_error} MATCHES \".*extension not supported: GL_KHR_cooperative_matrix.*\")\n    message(STATUS \"GL_KHR_cooperative_matrix not supported by glslc\")\n    set(GGML_VULKAN_COOPMAT_GLSLC_SUPPORT OFF)\nelse()\n    message(STATUS \"GL_KHR_cooperative_matrix supported by glslc\")\n    set(GGML_VULKAN_COOPMAT_GLSLC_SUPPORT ON)\n    add_compile_definitions(GGML_VULKAN_COOPMAT_GLSLC_SUPPORT)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Installing CANN Toolkit and Dependencies\nDESCRIPTION: Commands for installing CANN toolkit dependencies and setting up the environment\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/CANN.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\npip3 install attrs numpy decorator sympy cffi pyyaml pathlib2 psutil protobuf scipy requests absl-py wheel typing_extensions\nsh Ascend-cann-toolkit_8.0.RC2.alpha002_linux-aarch64.run --install\nsh Ascend-cann-kernels-910b_8.0.RC2.alpha002_linux.run --install\n```\n\n----------------------------------------\n\nTITLE: Detecting Host Compiler in CMake\nDESCRIPTION: Function to detect the host compiler on Windows and non-Windows systems, setting variables for C and C++ compilers.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-vulkan/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(detect_host_compiler)\n    if (CMAKE_HOST_SYSTEM_NAME STREQUAL \"Windows\")\n        find_program(HOST_C_COMPILER NAMES cl gcc clang NO_CMAKE_FIND_ROOT_PATH)\n        find_program(HOST_CXX_COMPILER NAMES cl g++ clang++ NO_CMAKE_FIND_ROOT_PATH)\n    else()\n        find_program(HOST_C_COMPILER NAMES gcc clang NO_CMAKE_FIND_ROOT_PATH)\n        find_program(HOST_CXX_COMPILER NAMES g++ clang++ NO_CMAKE_FIND_ROOT_PATH)\n    endif()\n    set(HOST_C_COMPILER \"${HOST_C_COMPILER}\" PARENT_SCOPE)\n    set(HOST_CXX_COMPILER \"${HOST_CXX_COMPILER}\" PARENT_SCOPE)\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: SYCL and OneDNN Integration\nDESCRIPTION: Configures SYCL compiler options and integrates with OneDNN library if available. Includes vendor-specific configurations and compiler flags.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-sycl/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nfind_package(IntelSYCL)\nif (IntelSYCL_FOUND)\n    target_link_libraries(ggml-sycl PRIVATE IntelSYCL::SYCL_CXX)\nelse()\n    target_compile_options(ggml-sycl PRIVATE \"-fsycl\")\n    target_link_options(ggml-sycl PRIVATE \"-fsycl\")\nendif()\n\ntarget_compile_options(ggml-sycl PRIVATE \"-Wno-narrowing\")\n```\n\n----------------------------------------\n\nTITLE: Commented FetchContent Example for llama.cpp\nDESCRIPTION: Shows a commented-out alternative approach to integrate llama.cpp using CMake's FetchContent module, which would download the repository directly instead of using a local copy.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llama.android/llama/src/main/cpp/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\n#include(FetchContent)\n#FetchContent_Declare(\n#        llama\n#        GIT_REPOSITORY https://github.com/ggml-org/llama.cpp\n#        GIT_TAG        master\n#)\n\n# Also provides \"common\"\n#FetchContent_MakeAvailable(llama)\n```\n\n----------------------------------------\n\nTITLE: GGUF Command Line Options\nDESCRIPTION: Command line parameters for controlling GGUF file splitting and merging operations. Allows specifying maximum split size in MB/GB, maximum number of tensors per split, and merge functionality.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/gguf-split/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n--split\n--split-max-size\n--split-max-tensors\n--merge\n```\n\n----------------------------------------\n\nTITLE: Accessing Global Settings Object in JavaScript\nDESCRIPTION: Shows how to access the global settings object 'gMe' which contains various configuration options for the llama.cpp web interface. This object is attached to the document and can be modified using the browser's developer tools.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/public_simplechat/readme.md#2025-04-22_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\ngMe.baseURL\n```\n\nLANGUAGE: JavaScript\nCODE:\n```\ngMe.bStream\n```\n\nLANGUAGE: JavaScript\nCODE:\n```\ngMe.apiEP\n```\n\nLANGUAGE: JavaScript\nCODE:\n```\ngMe.bCompletionFreshChatAlways\n```\n\nLANGUAGE: JavaScript\nCODE:\n```\ngMe.bCompletionInsertStandardRolePrefix\n```\n\nLANGUAGE: JavaScript\nCODE:\n```\ngMe.bTrimGarbage\n```\n\nLANGUAGE: JavaScript\nCODE:\n```\ngMe.apiRequestOptions\n```\n\nLANGUAGE: JavaScript\nCODE:\n```\ngMe.headers\n```\n\nLANGUAGE: JavaScript\nCODE:\n```\ngMe.iRecentUserMsgCnt\n```\n\n----------------------------------------\n\nTITLE: Sanitizer Configuration\nDESCRIPTION: Setup for various sanitizer options including thread, address, and undefined behavior sanitizers\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nif (NOT MSVC)\n    if (LLAMA_SANITIZE_THREAD)\n        message(STATUS \"Using -fsanitize=thread\")\n        add_compile_options(-fsanitize=thread)\n        link_libraries     (-fsanitize=thread)\n    endif()\n\n    if (LLAMA_SANITIZE_ADDRESS)\n        message(STATUS \"Using -fsanitize=address\")\n        add_compile_options(-fsanitize=address -fno-omit-frame-pointer)\n        link_libraries     (-fsanitize=address)\n    endif()\n\n    if (LLAMA_SANITIZE_UNDEFINED)\n        message(STATUS \"Using -fsanitize=undefined\")\n        add_compile_options(-fsanitize=undefined)\n        link_libraries     (-fsanitize=undefined)\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Installing Metal Files for GGML in CMake\nDESCRIPTION: Sets up the installation process for Metal-related files when GGML_METAL_EMBED_LIBRARY is not enabled.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-metal/CMakeLists.txt#2025-04-22_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nif (NOT GGML_METAL_EMBED_LIBRARY)\n    install(\n        FILES src/ggml-metal/ggml-metal.metal\n        PERMISSIONS\n            OWNER_READ\n            OWNER_WRITE\n            GROUP_READ\n            WORLD_READ\n        DESTINATION ${CMAKE_INSTALL_BINDIR})\n\n        install(\n            FILES ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/default.metallib\n            DESTINATION ${CMAKE_INSTALL_BINDIR}\n        )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining CMake Function for Building and Testing in llama.cpp\nDESCRIPTION: Function that builds an executable from a source file and registers it as a test. It handles configuration of the test name, label, working directory, and arguments. The function links with the 'common' library and includes 'get-model.cpp'.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/tests/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(llama_target_and_test source)\n    include(CMakeParseArguments)\n    set(options)\n    set(oneValueArgs NAME LABEL WORKING_DIRECTORY)\n    set(multiValueArgs ARGS)\n    cmake_parse_arguments(LLAMA_TEST \"${options}\" \"${oneValueArgs}\" \"${multiValueArgs}\" ${ARGN})\n\n    if (NOT DEFINED LLAMA_TEST_LABEL)\n        set(LLAMA_TEST_LABEL \"main\")\n    endif()\n    if (NOT DEFINED LLAMA_TEST_WORKING_DIRECTORY)\n        set(LLAMA_TEST_WORKING_DIRECTORY .)\n    endif()\n    if (DEFINED LLAMA_TEST_NAME)\n        set(TEST_TARGET ${LLAMA_TEST_NAME})\n    else()\n        get_filename_component(TEST_TARGET ${source} NAME_WE)\n    endif()\n\n    add_executable(${TEST_TARGET} ${source} get-model.cpp)\n    install(TARGETS ${TEST_TARGET} RUNTIME)\n    target_link_libraries(${TEST_TARGET} PRIVATE common)\n    add_test(\n        NAME ${TEST_TARGET}\n        WORKING_DIRECTORY ${LLAMA_TEST_WORKING_DIRECTORY}\n        COMMAND $<TARGET_FILE:${TEST_TARGET}>\n        ${LLAMA_TEST_ARGS})\n\n    set_property(TEST ${TEST_TARGET} PROPERTY LABELS ${LLAMA_TEST_LABEL})\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Configuring Vulkan Shaders Generator Project in CMake\nDESCRIPTION: Sets up a CMake project for a Vulkan shaders generator. It defines the project, sets compiler definitions based on GLSLC support, specifies the target executable, and configures compilation features and library dependencies.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-vulkan/vulkan-shaders/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.19)\nproject(\"vulkan-shaders-gen\" C CXX)\n\nfind_package (Threads REQUIRED)\n\nif (GGML_VULKAN_COOPMAT_GLSLC_SUPPORT)\n    add_compile_definitions(GGML_VULKAN_COOPMAT_GLSLC_SUPPORT)\nendif()\nif (GGML_VULKAN_COOPMAT2_GLSLC_SUPPORT)\n    add_compile_definitions(GGML_VULKAN_COOPMAT2_GLSLC_SUPPORT)\nendif()\nif (GGML_VULKAN_INTEGER_DOT_GLSLC_SUPPORT)\n    add_compile_definitions(GGML_VULKAN_INTEGER_DOT_GLSLC_SUPPORT)\nendif()\nset(TARGET vulkan-shaders-gen)\nadd_executable(${TARGET} vulkan-shaders-gen.cpp)\ninstall(TARGETS ${TARGET} RUNTIME)\ntarget_compile_features(${TARGET} PRIVATE cxx_std_17)\ntarget_link_libraries(vulkan-shaders-gen PUBLIC Threads::Threads)\n```\n\n----------------------------------------\n\nTITLE: Advanced PCA Options for Control Vector Generation\nDESCRIPTION: Command with advanced PCA (Principal Component Analysis) options, specifying iteration count and batch size for more refined control vector generation.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/cvector-generator/README.md#2025-04-22_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n./cvector-generator -m ./llama-3.Q4_K_M.gguf -ngl 99 --pca-iter 2000 --pca-batch 100\n```\n\n----------------------------------------\n\nTITLE: Preprocessor Directive Formatting in C++\nDESCRIPTION: Example showing the proper format for preprocessor directives with comments that include the directive name for clarity and readability.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/CONTRIBUTING.md#2025-04-22_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\n#ifdef FOO\n#endif // FOO\n```\n\n----------------------------------------\n\nTITLE: Example Output of XXH64 Verification\nDESCRIPTION: This is the output of verifying a GGUF file against its manifest using the XXH64 hash algorithm. This method is faster but provides less cryptographic security than SHA-256.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/gguf-hash/README.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nmanifest  test.gguf.manifest  sha256  sha1  xxh64\nxxh64     f66e9cd66a4396a0  test.gguf:tensor_0  -  Ok\nxxh64     7d3a1f9ac04d0537  test.gguf:tensor_1  -  Ok\nxxh64     a0af5d700049693b  test.gguf:tensor_2  -  Ok\nxxh64     e83fddf559d7b6a6  test.gguf:tensor_3  -  Ok\nxxh64     1257733306b7992d  test.gguf:tensor_4  -  Ok\nxxh64     d238d16ba4711e58  test.gguf:tensor_5  -  Ok\nxxh64     3fbc3b65ab8c7f39  test.gguf:tensor_6  -  Ok\nxxh64     c22021c29854f093  test.gguf:tensor_7  -  Ok\nxxh64     936df61f5d64261f  test.gguf:tensor_8  -  Ok\nxxh64     93fd20c64421c081  test.gguf:tensor_9  -  Ok\nxxh64     5a54d3aad816f302  test.gguf  -  Ok\n\nVerification results for test.gguf.manifest - Success\n```\n\n----------------------------------------\n\nTITLE: Sample output from SYCL device listing\nDESCRIPTION: Example output from the llama-ls-sycl-device tool showing two Intel GPUs with their specifications including compute units, work group sizes, and memory size.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/sycl/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nfound 2 SYCL devices:\n|  |                   |                                       |       |Max    |        |Max  |Global |                     |\n|  |                   |                                       |       |compute|Max work|sub  |mem    |                     |\n|ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|\n|--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|\n| 0| [level_zero:gpu:0]|                Intel Arc A770 Graphics|    1.3|    512|    1024|   32| 16225M|            1.3.29138|\n| 1| [level_zero:gpu:1]|                 Intel UHD Graphics 750|    1.3|     32|     512|   32| 62631M|            1.3.29138|\n```\n\n----------------------------------------\n\nTITLE: Configuring Debug Build Settings for Linux\nDESCRIPTION: Enables libstdc++ assertions for debug builds on Linux systems. This helps with identifying issues during development.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\n# enable libstdc++ assertions for debug builds\nif (CMAKE_SYSTEM_NAME MATCHES \"Linux\")\n    add_compile_definitions($<$<CONFIG:Debug>:_GLIBCXX_ASSERTIONS>)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Running llama-cli with SYCL on Linux (Single Device)\nDESCRIPTION: Shell command to execute llama-cli inference using a single SYCL device (device 0) on Linux with specific parameters.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_13\n\nLANGUAGE: sh\nCODE:\n```\nZES_ENABLE_SYSMAN=1 ./build/bin/llama-cli -no-cnv -m models/llama-2-7b.Q4_0.gguf -p \"Building a website can be done in 10 simple steps:\" -n 400 -e -ngl 33 -sm none -mg 0\n```\n\n----------------------------------------\n\nTITLE: Detecting HIP Compiler and Enabling HIP Language in CMake\nDESCRIPTION: Checks if the C++ compiler is hipcc and enables HIP language support. It also handles Windows-specific configuration and forwards AMDGPU_TARGETS to CMAKE_HIP_ARCHITECTURES.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-hip/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif (WIN32)\n    set(CXX_IS_HIPCC TRUE)\nelse()\n    string(REGEX MATCH \"hipcc(\\.bat)?$\" CXX_IS_HIPCC \"${CMAKE_CXX_COMPILER}\")\nendif()\n\nif (CXX_IS_HIPCC)\n    if (LINUX)\n        if (NOT ${CMAKE_CXX_COMPILER_ID} MATCHES \"Clang\")\n            message(WARNING \"Only LLVM is supported for HIP, hint: CXX=/opt/rocm/llvm/bin/clang++\")\n        endif()\n\n        message(WARNING \"Setting hipcc as the C++ compiler is legacy behavior.\"\n                \" Prefer setting the HIP compiler directly. See README for details.\")\n    endif()\nelse()\n    # Forward AMDGPU_TARGETS to CMAKE_HIP_ARCHITECTURES.\n    if (AMDGPU_TARGETS AND NOT CMAKE_HIP_ARCHITECTURES)\n        set(CMAKE_HIP_ARCHITECTURES ${AMDGPU_TARGETS})\n    endif()\n    cmake_minimum_required(VERSION 3.21)\n    enable_language(HIP)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Declaring Python Dependencies\nDESCRIPTION: This snippet defines the Python packages required for the project. It includes matplotlib for creating graphs and visualizations, and requests for making HTTP requests.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/bench/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nmatplotlib\nrequests\n```\n\n----------------------------------------\n\nTITLE: Updating Chat Templates for Multiple AI Models using Bash Commands\nDESCRIPTION: This bash script contains multiple commands to update chat templates for various AI models. Each command runs the get_chat_template.py script with specific model parameters and saves the output to a .jinja file in the models/templates directory. The models include variants from CohereForAI, deepseek-ai, fireworks-ai, Google, MeetKai, Meta-Llama, Microsoft, MistralAI, NousResearch, and Qwen.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/models/templates/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./scripts/get_chat_template.py CohereForAI/c4ai-command-r-plus tool_use      > models/templates/CohereForAI-c4ai-command-r-plus-tool_use.jinja\n./scripts/get_chat_template.py CohereForAI/c4ai-command-r7b-12-2024 default  > models/templates/CohereForAI-c4ai-command-r7b-12-2024-default.jinja\n./scripts/get_chat_template.py CohereForAI/c4ai-command-r7b-12-2024 rag      > models/templates/CohereForAI-c4ai-command-r7b-12-2024-rag.jinja\n./scripts/get_chat_template.py CohereForAI/c4ai-command-r7b-12-2024 tool_use > models/templates/CohereForAI-c4ai-command-r7b-12-2024-tool_use.jinja\n./scripts/get_chat_template.py deepseek-ai/DeepSeek-R1-Distill-Llama-8B      > models/templates/deepseek-ai-DeepSeek-R1-Distill-Llama-8B.jinja\n./scripts/get_chat_template.py deepseek-ai/DeepSeek-R1-Distill-Qwen-32B      > models/templates/deepseek-ai-DeepSeek-R1-Distill-Qwen-32B.jinja\n./scripts/get_chat_template.py fireworks-ai/llama-3-firefunction-v2          > models/templates/fireworks-ai-llama-3-firefunction-v2.jinja\n./scripts/get_chat_template.py google/gemma-2-2b-it                          > models/templates/google-gemma-2-2b-it.jinja\n./scripts/get_chat_template.py meetkai/functionary-medium-v3.1               > models/templates/meetkai-functionary-medium-v3.1.jinja\n./scripts/get_chat_template.py meetkai/functionary-medium-v3.2               > models/templates/meetkai-functionary-medium-v3.2.jinja\n./scripts/get_chat_template.py meta-llama/Llama-3.1-8B-Instruct              > models/templates/meta-llama-Llama-3.1-8B-Instruct.jinja\n./scripts/get_chat_template.py meta-llama/Llama-3.2-3B-Instruct              > models/templates/meta-llama-Llama-3.2-3B-Instruct.jinja\n./scripts/get_chat_template.py meta-llama/Llama-3.3-70B-Instruct             > models/templates/meta-llama-Llama-3.3-70B-Instruct.jinja\n./scripts/get_chat_template.py microsoft/Phi-3.5-mini-instruct               > models/templates/microsoft-Phi-3.5-mini-instruct.jinja\n./scripts/get_chat_template.py mistralai/Mistral-Nemo-Instruct-2407          > models/templates/mistralai-Mistral-Nemo-Instruct-2407.jinja\n./scripts/get_chat_template.py NousResearch/Hermes-2-Pro-Llama-3-8B tool_use > models/templates/NousResearch-Hermes-2-Pro-Llama-3-8B-tool_use.jinja\n./scripts/get_chat_template.py NousResearch/Hermes-3-Llama-3.1-8B tool_use   > models/templates/NousResearch-Hermes-3-Llama-3.1-8B-tool_use.jinja\n./scripts/get_chat_template.py Qwen/Qwen2.5-7B-Instruct                      > models/templates/Qwen-Qwen2.5-7B-Instruct.jinja\n```\n\n----------------------------------------\n\nTITLE: Checking LLaVA Files\nDESCRIPTION: List generated LLaVA files after surgery script execution.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/granitevision.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ ls $GRANITE_MODEL | grep -i llava\nllava.clip\nllava.projector\n```\n\n----------------------------------------\n\nTITLE: Setting Warning Flags for Non-MSVC Compilers\nDESCRIPTION: Configures comprehensive warning flags for C and C++ when GGML_ALL_WARNINGS is enabled. Different flags are set for C vs C++ files.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nif (GGML_ALL_WARNINGS)\n    if (NOT MSVC)\n        list(APPEND WARNING_FLAGS -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function)\n        list(APPEND C_FLAGS       -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes\n                                  -Werror=implicit-int -Werror=implicit-function-declaration)\n        list(APPEND CXX_FLAGS     -Wmissing-declarations -Wmissing-noreturn)\n\n        list(APPEND C_FLAGS   ${WARNING_FLAGS})\n        list(APPEND CXX_FLAGS ${WARNING_FLAGS})\n\n        ggml_get_flags(${CMAKE_CXX_COMPILER_ID} ${CMAKE_CXX_COMPILER_VERSION})\n\n        add_compile_options(\"$<$<COMPILE_LANGUAGE:C>:${C_FLAGS};${GF_C_FLAGS}>\"\n                            \"$<$<COMPILE_LANGUAGE:CXX>:${CXX_FLAGS};${GF_CXX_FLAGS}>\")\n    else()\n        # todo : msvc\n        set(C_FLAGS   \"\")\n        set(CXX_FLAGS \"\")\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Selecting SYCL Devices for Inference\nDESCRIPTION: Environment variable settings to choose specific SYCL devices for inference.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_10\n\nLANGUAGE: sh\nCODE:\n```\nexport ONEAPI_DEVICE_SELECTOR=\"level_zero:0\"\nexport ONEAPI_DEVICE_SELECTOR=\"level_zero:1\"\nexport ONEAPI_DEVICE_SELECTOR=\"level_zero:0;level_zero:1\"\n```\n\n----------------------------------------\n\nTITLE: Implementing JSON Schema with Zod in JavaScript\nDESCRIPTION: Demonstrates using Zod to create a strictly validated object schema with specific rules for age and email fields. Shows conversion to JSON schema format.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/grammars/README.md#2025-04-22_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nimport { z } from 'zod';\nimport { zodToJsonSchema } from 'zod-to-json-schema';\n\nconst Foo = z.object({\n  age: z.number().positive(),\n  email: z.string().email(),\n}).strict();\n\nconsole.log(zodToJsonSchema(Foo));\n```\n\n----------------------------------------\n\nTITLE: Defining CMake Test Registration Function for llama.cpp\nDESCRIPTION: Function that registers a test for a pre-built target. It handles test naming, working directory configuration, and command arguments. The function is configurable through parameters and sets appropriate test properties.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/tests/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(llama_test target)\n    include(CMakeParseArguments)\n    set(options)\n    set(oneValueArgs NAME LABEL WORKING_DIRECTORY)\n    set(multiValueArgs ARGS)\n    cmake_parse_arguments(LLAMA_TEST \"${options}\" \"${oneValueArgs}\" \"${multiValueArgs}\" ${ARGN})\n\n    if (NOT DEFINED LLAMA_TEST_LABEL)\n        set(LLAMA_TEST_LABEL \"main\")\n    endif()\n    if (NOT DEFINED LLAMA_TEST_WORKING_DIRECTORY)\n        set(LLAMA_TEST_WORKING_DIRECTORY .)\n    endif()\n    if (DEFINED LLAMA_TEST_NAME)\n        set(TEST_NAME ${LLAMA_TEST_NAME})\n    else()\n        set(TEST_NAME ${target})\n    endif()\n\n    set(TEST_TARGET ${target})\n\n    add_test(\n        NAME ${TEST_NAME}\n        WORKING_DIRECTORY ${LLAMA_TEST_WORKING_DIRECTORY}\n        COMMAND $<TARGET_FILE:${TEST_TARGET}>\n        ${LLAMA_TEST_ARGS})\n\n    set_property(TEST ${TEST_NAME} PROPERTY LABELS ${LLAMA_TEST_LABEL})\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Configuring Architecture-Specific Settings\nDESCRIPTION: Sets up architecture-specific compiler and linker options including static linking, profiling, and Windows version targeting.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/CMakeLists.txt#2025-04-22_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\n# architecture specific\n# TODO: probably these flags need to be tweaked on some architectures\n#       feel free to update the Makefile for your architecture and send a pull request or issue\nmessage(STATUS \"CMAKE_SYSTEM_PROCESSOR: ${CMAKE_SYSTEM_PROCESSOR}\")\nif (MSVC)\n    string(TOLOWER \"${CMAKE_GENERATOR_PLATFORM}\" CMAKE_GENERATOR_PLATFORM_LWR)\n    message(STATUS \"CMAKE_GENERATOR_PLATFORM: ${CMAKE_GENERATOR_PLATFORM}\")\nelse ()\n    set(CMAKE_GENERATOR_PLATFORM_LWR \"\")\nendif ()\n\nif (NOT MSVC)\n    if (GGML_STATIC)\n        add_link_options(-static)\n        if (MINGW)\n            add_link_options(-static-libgcc -static-libstdc++)\n        endif()\n    endif()\n    if (GGML_GPROF)\n        add_compile_options(-pg)\n    endif()\nendif()\n\nif (MINGW)\n    # Target Windows 8 for PrefetchVirtualMemory\n    add_compile_definitions(_WIN32_WINNT=${GGML_WIN_VER})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies with Version Constraints\nDESCRIPTION: Defines Python package dependencies with version constraints using the tilde operator, which allows patch version updates while maintaining minor version stability. Contains core libraries for HTTP requests, testing, data analysis, visualization, and API integrations.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/requirements/requirements-tool_bench.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\naiohttp~=3.9.3\npytest~=8.3.3\nhuggingface_hub~=0.23.2\nmatplotlib~=3.10.0\nnumpy~=1.26.4\nopenai~=1.55.3\npandas~=2.2.3\nprometheus-client~=0.20.0\nrequests~=2.32.3\nwget~=3.2\ntyper~=0.15.1\nseaborn~=0.13.2\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies with Version Constraints\nDESCRIPTION: Requirements file specifying exact version constraints for two Python packages - tabulate v0.9.0 and GitPython v3.1.43. Uses the tilde (~=) operator to indicate version compatibility.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/requirements/requirements-compare-llama-bench.txt#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\ntabulate~=0.9.0\nGitPython~=3.1.43\n```\n\n----------------------------------------\n\nTITLE: Creating Deprecation Warning Executables in CMake\nDESCRIPTION: Creates executables that show deprecation warnings for older CLI applications, reusing the same deprecation-warning.cpp source file.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llava/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nadd_executable(llama-llava-cli    deprecation-warning.cpp)\nadd_executable(llama-gemma3-cli   deprecation-warning.cpp)\nadd_executable(llama-minicpmv-cli deprecation-warning.cpp)\n```\n\n----------------------------------------\n\nTITLE: List Grammar in GBNF\nDESCRIPTION: Shows how to create a grammar for basic lists with items prefixed by hyphens.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/grammars/README.md#2025-04-22_snippet_3\n\nLANGUAGE: gbnf\nCODE:\n```\n# a grammar for lists\nroot ::= (\"- \" item)+\nitem ::= [^\\n]+ \"\\n\"\n```\n\n----------------------------------------\n\nTITLE: Building Package Distribution\nDESCRIPTION: Command to build the package distribution archives\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/gguf-py/README.md#2025-04-22_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\npython -m build\n```\n\n----------------------------------------\n\nTITLE: Listing and Adding OpenCL Kernels in GGML CMake\nDESCRIPTION: Defines a list of OpenCL kernels and adds them to the project using the previously defined function.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-opencl/CMakeLists.txt#2025-04-22_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nset(GGML_OPENCL_KERNELS\n    add\n    clamp\n    cpy\n    cvt\n    diag_mask_inf\n    gelu\n    gemv_noshuffle_general\n    gemv_noshuffle\n    get_rows\n    im2col_f32\n    im2col_f16\n    mul_mat_Ab_Bi_8x4\n    mul_mv_f16_f16\n    mul_mv_f16_f32_1row\n    mul_mv_f16_f32_l4\n    mul_mv_f16_f32\n    mul_mv_f32_f32\n    mul_mv_q4_0_f32\n    mul_mv_q4_0_f32_v\n    mul_mv_q4_0_f32_8x_flat\n    mul_mv_q4_0_f32_1d_8x_flat\n    mul_mv_q4_0_f32_1d_16x_flat\n    mul_mv_q6_k\n    mul\n    norm\n    relu\n    rms_norm\n    rope\n    scale\n    silu\n    softmax_4_f32\n    softmax_4_f16\n    softmax_f32\n    softmax_f16\n    transpose\n)\n\nforeach (K ${GGML_OPENCL_KERNELS})\n    ggml_opencl_add_kernel(${K})\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Creating Visual GGUF Model for LLaVA 1.6\nDESCRIPTION: Python command to create the visual GGUF model for LLaVA 1.6 using the convert_image_encoder_to_gguf.py script.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/llava.md#2025-04-22_snippet_9\n\nLANGUAGE: sh\nCODE:\n```\npython ./examples/llava/convert_image_encoder_to_gguf.py -m vit --llava-projector vit/llava.projector --output-dir vit --clip-model-is-vision\n```\n\n----------------------------------------\n\nTITLE: Setting up LLGuidance Integration in llama.cpp CMake\nDESCRIPTION: Configures LLGuidance integration by setting up an external project. This creates a Git-based dependency on LLGuidance, builds it with Cargo, and links it to the main project with platform-specific libraries.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/common/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nif (LLAMA_LLGUIDANCE)\n    include(ExternalProject)\n    set(LLGUIDANCE_SRC ${CMAKE_BINARY_DIR}/llguidance/source)\n    set(LLGUIDANCE_PATH ${LLGUIDANCE_SRC}/target/release)\n\n    # Set the correct library file extension based on platform\n    if (WIN32)\n        set(LLGUIDANCE_LIB_NAME \"llguidance.lib\")\n        # Add Windows-specific libraries\n        set(LLGUIDANCE_PLATFORM_LIBS\n            ws2_32    # Windows Sockets API\n            userenv   # For GetUserProfileDirectoryW\n            ntdll     # For NT functions\n            bcrypt    # For BCryptGenRandom\n        )\n    else()\n        set(LLGUIDANCE_LIB_NAME \"libllguidance.a\")\n        set(LLGUIDANCE_PLATFORM_LIBS \"\")\n    endif()\n\n    ExternalProject_Add(llguidance_ext\n        GIT_REPOSITORY https://github.com/guidance-ai/llguidance\n        # v0.7.10:\n        GIT_TAG 0309d2a6bf40abda35344a362edc71e06d5009f8\n        PREFIX ${CMAKE_BINARY_DIR}/llguidance\n        SOURCE_DIR ${LLGUIDANCE_SRC}\n        BUILD_IN_SOURCE TRUE\n        CONFIGURE_COMMAND \"\"\n        BUILD_COMMAND cargo build --release\n        INSTALL_COMMAND \"\"\n        BUILD_BYPRODUCTS ${LLGUIDANCE_PATH}/${LLGUIDANCE_LIB_NAME} ${LLGUIDANCE_PATH}/llguidance.h\n        UPDATE_COMMAND \"\"\n    )\n    target_compile_definitions(${TARGET} PUBLIC LLAMA_USE_LLGUIDANCE)\n\n    add_library(llguidance STATIC IMPORTED)\n    set_target_properties(llguidance PROPERTIES IMPORTED_LOCATION ${LLGUIDANCE_PATH}/${LLGUIDANCE_LIB_NAME})\n    add_dependencies(llguidance llguidance_ext)\n\n    target_include_directories(${TARGET} PRIVATE ${LLGUIDANCE_PATH})\n    # Add platform libraries to the main target\n    set(LLAMA_COMMON_EXTRA_LIBS ${LLAMA_COMMON_EXTRA_LIBS} llguidance ${LLGUIDANCE_PLATFORM_LIBS})\nendif ()\n```\n\n----------------------------------------\n\nTITLE: Configuring CPU Backend Variant Implementation in CMake\nDESCRIPTION: CMake function that sets up CPU backend library configuration with platform-specific optimizations and feature detection. Handles both ARM and x86 architectures, configures compiler flags, and manages dependencies like OpenMP and Accelerate framework.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cpu/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfunction(ggml_add_cpu_backend_variant_impl tag_name)\n    if (tag_name)\n        set(GGML_CPU_NAME ggml-cpu-${tag_name})\n    else()\n        set(GGML_CPU_NAME ggml-cpu)\n    endif()\n\n    ggml_add_backend_library(${GGML_CPU_NAME})\n\n    list (APPEND GGML_CPU_SOURCES\n        ggml-cpu/ggml-cpu.c\n        ggml-cpu/ggml-cpu.cpp\n        ggml-cpu/ggml-cpu-aarch64.cpp\n        ggml-cpu/ggml-cpu-aarch64.h\n        ggml-cpu/ggml-cpu-hbm.cpp\n        ggml-cpu/ggml-cpu-hbm.h\n        ggml-cpu/ggml-cpu-quants.c\n        ggml-cpu/ggml-cpu-quants.h\n        ggml-cpu/ggml-cpu-traits.cpp\n        ggml-cpu/ggml-cpu-traits.h\n        ggml-cpu/amx/amx.cpp\n        ggml-cpu/amx/amx.h\n        ggml-cpu/amx/mmq.cpp\n        ggml-cpu/amx/mmq.h\n        ggml-cpu/ggml-cpu-impl.h\n        ggml-cpu/common.h\n        ggml-cpu/binary-ops.h\n        ggml-cpu/binary-ops.cpp\n        ggml-cpu/unary-ops.h\n        ggml-cpu/unary-ops.cpp\n        ggml-cpu/simd-mappings.h\n        ggml-cpu/vec.h\n        ggml-cpu/vec.cpp\n        ggml-cpu/ops.h\n        ggml-cpu/ops.cpp\n        )\n\n    target_compile_features(${GGML_CPU_NAME} PRIVATE c_std_11 cxx_std_17)\n    target_include_directories(${GGML_CPU_NAME} PRIVATE . ggml-cpu)\n\n    if (APPLE AND GGML_ACCELERATE)\n        find_library(ACCELERATE_FRAMEWORK Accelerate)\n        if (ACCELERATE_FRAMEWORK)\n            message(STATUS \"Accelerate framework found\")\n\n            target_compile_definitions(${GGML_CPU_NAME} PRIVATE GGML_USE_ACCELERATE)\n            target_compile_definitions(${GGML_CPU_NAME} PRIVATE ACCELERATE_NEW_LAPACK)\n            target_compile_definitions(${GGML_CPU_NAME} PRIVATE ACCELERATE_LAPACK_ILP64)\n\n            target_link_libraries(${GGML_CPU_NAME} PRIVATE ${ACCELERATE_FRAMEWORK})\n        else()\n            message(WARNING \"Accelerate framework not found\")\n        endif()\n    endif()\n\n    if (GGML_OPENMP)\n        find_package(OpenMP)\n        if (OpenMP_FOUND)\n            target_compile_definitions(${GGML_CPU_NAME} PRIVATE GGML_USE_OPENMP)\n\n            target_link_libraries(${GGML_CPU_NAME} PRIVATE OpenMP::OpenMP_C OpenMP::OpenMP_CXX)\n        else()\n            message(WARNING \"OpenMP not found\")\n        endif()\n    endif()\n\n    if (GGML_LLAMAFILE)\n        target_compile_definitions(${GGML_CPU_NAME} PRIVATE GGML_USE_LLAMAFILE)\n\n        list(APPEND GGML_CPU_SOURCES\n                    ggml-cpu/llamafile/sgemm.cpp\n                    ggml-cpu/llamafile/sgemm.h)\n    endif()\n\n    if (GGML_CPU_HBM)\n        find_library(memkind memkind REQUIRED)\n\n        message(STATUS \"Using memkind for CPU HBM\")\n\n        target_compile_definitions(${GGML_CPU_NAME} PRIVATE GGML_USE_CPU_HBM)\n\n        target_link_libraries(${GGML_CPU_NAME} PUBLIC memkind)\n    endif()\n\n    if (CMAKE_OSX_ARCHITECTURES      STREQUAL \"arm64\" OR\n        CMAKE_GENERATOR_PLATFORM_LWR STREQUAL \"arm64\" OR\n        (NOT CMAKE_OSX_ARCHITECTURES AND NOT CMAKE_GENERATOR_PLATFORM_LWR AND\n            CMAKE_SYSTEM_PROCESSOR MATCHES \"^(aarch64|arm.*|ARM64)$\"))\n\n        message(STATUS \"ARM detected\")\n\n        if (MSVC AND NOT CMAKE_C_COMPILER_ID STREQUAL \"Clang\")\n            message(FATAL_ERROR \"MSVC is not supported for ARM, use clang\")\n        else()\n            check_cxx_compiler_flag(-mfp16-format=ieee GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E)\n            if (NOT \"${GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E}\" STREQUAL \"\")\n                list(APPEND ARCH_FLAGS -mfp16-format=ieee)\n            endif()\n\n            if (GGML_NATIVE)\n                execute_process(\n                    COMMAND ${CMAKE_C_COMPILER} -mcpu=native -E -v -\n                    INPUT_FILE \"/dev/null\"\n                    OUTPUT_QUIET\n                    ERROR_VARIABLE ARM_MCPU\n                    RESULT_VARIABLE ARM_MCPU_RESULT\n                )\n                if (NOT ARM_MCPU_RESULT)\n                    string(REGEX MATCH \"-mcpu=[^ ']+\" ARM_MCPU_FLAG \"${ARM_MCPU}\")\n                endif()\n                if (\"${ARM_MCPU_FLAG}\" STREQUAL \"\")\n                    set(ARM_MCPU_FLAG -mcpu=native)\n                    message(STATUS \"ARM -mcpu not found, -mcpu=native will be used\")\n                endif()\n\n                include(CheckCXXSourceRuns)\n\n                function(check_arm_feature tag code)\n                    set(CMAKE_REQUIRED_FLAGS_SAVE ${CMAKE_REQUIRED_FLAGS})\n                    set(CMAKE_REQUIRED_FLAGS \"${ARM_MCPU_FLAG}+${tag}\")\n                    check_cxx_source_runs(\"${code}\" GGML_MACHINE_SUPPORTS_${tag})\n                    if (GGML_MACHINE_SUPPORTS_${tag})\n                        set(ARM_MCPU_FLAG_FIX \"${ARM_MCPU_FLAG_FIX}+${tag}\" PARENT_SCOPE)\n                    else()\n                        set(CMAKE_REQUIRED_FLAGS \"${ARM_MCPU_FLAG}+no${tag}\")\n                        check_cxx_source_compiles(\"int main() { return 0; }\" GGML_MACHINE_SUPPORTS_no${tag})\n                        if (GGML_MACHINE_SUPPORTS_no${tag})\n                            set(ARM_MCPU_FLAG_FIX \"${ARM_MCPU_FLAG_FIX}+no${tag}\" PARENT_SCOPE)\n                        endif()\n                    endif()\n                    set(CMAKE_REQUIRED_FLAGS ${CMAKE_REQUIRED_FLAGS_SAVE})\n                endfunction()\n\n                check_arm_feature(dotprod \"#include <arm_neon.h>\\nint main() { int8x16_t _a, _b; volatile int32x4_t _s = vdotq_s32(_s, _a, _b); return 0; }\")\n                check_arm_feature(i8mm    \"#include <arm_neon.h>\\nint main() { int8x16_t _a, _b; volatile int32x4_t _s = vmmlaq_s32(_s, _a, _b); return 0; }\")\n                check_arm_feature(sve     \"#include <arm_sve.h>\\nint main()  { svfloat32_t _a, _b; volatile svfloat32_t _c = svadd_f32_z(svptrue_b8(), _a, _b); return 0; }\")\n                check_arm_feature(sme     \"#include <arm_sme.h>\\n__arm_locally_streaming int main() { __asm__ volatile(\\\"smstart; smstop;\\\"); return 0; }\")\n\n                list(APPEND ARCH_FLAGS \"${ARM_MCPU_FLAG}${ARM_MCPU_FLAG_FIX}\")\n            else()\n                if (GGML_CPU_ARM_ARCH)\n                    list(APPEND ARCH_FLAGS -march=${GGML_CPU_ARM_ARCH})\n                endif()\n            endif()\n\n            if (CMAKE_HOST_SYSTEM_NAME STREQUAL \"Windows\")\n                set(FEAT_INPUT_FILE \"NUL\")\n            else()\n                set(FEAT_INPUT_FILE \"/dev/null\")\n            endif()\n\n            execute_process(\n                COMMAND ${CMAKE_C_COMPILER} ${ARCH_FLAGS} -dM -E -\n                INPUT_FILE ${FEAT_INPUT_FILE}\n                OUTPUT_VARIABLE ARM_FEATURE\n                RESULT_VARIABLE ARM_FEATURE_RESULT\n            )\n            if (ARM_FEATURE_RESULT)\n                message(WARNING \"Failed to get ARM features\")\n            else()\n                foreach(feature DOTPROD SVE MATMUL_INT8 FMA FP16_VECTOR_ARITHMETIC SME)\n                    string(FIND \"${ARM_FEATURE}\" \"__ARM_FEATURE_${feature} 1\" feature_pos)\n                    if (NOT ${feature_pos} EQUAL -1)\n                        message(STATUS \"ARM feature ${feature} enabled\")\n                    endif()\n                endforeach()\n            endif()\n        endif()\n    elseif (CMAKE_OSX_ARCHITECTURES STREQUAL \"x86_64\" OR CMAKE_GENERATOR_PLATFORM_LWR MATCHES \"^(x86_64|i686|amd64|x64|win32)$\" OR\n            (NOT CMAKE_OSX_ARCHITECTURES AND NOT CMAKE_GENERATOR_PLATFORM_LWR AND\n            CMAKE_SYSTEM_PROCESSOR MATCHES \"^(x86_64|i686|AMD64|amd64)$\"))\n\n        message(STATUS \"x86 detected\")\n\n        if (MSVC)\n            if (GGML_NATIVE)\n                include(ggml-cpu/cmake/FindSIMD.cmake)\n            endif ()\n            if (GGML_AVX512)\n                list(APPEND ARCH_FLAGS /arch:AVX512)\n                list(APPEND ARCH_DEFINITIONS GGML_AVX512)\n                if (GGML_AVX512_VBMI)\n                    list(APPEND ARCH_DEFINITIONS __AVX512VBMI__)\n                    if (CMAKE_C_COMPILER_ID STREQUAL \"Clang\")\n                        list(APPEND ARCH_FLAGS -mavx512vbmi)\n                    endif()\n                endif()\n                if (GGML_AVX512_VNNI)\n                    list(APPEND ARCH_DEFINITIONS __AVX512VNNI__ GGML_AVX512_VNNI)\n                    if (CMAKE_C_COMPILER_ID STREQUAL \"Clang\")\n                        list(APPEND ARCH_FLAGS -mavx512vnni)\n                    endif()\n                endif()\n                if (GGML_AVX512_BF16)\n                    list(APPEND ARCH_DEFINITIONS __AVX512BF16__ GGML_AVX512_BF16)\n                    if (CMAKE_C_COMPILER_ID STREQUAL \"Clang\")\n                        list(APPEND ARCH_FLAGS -mavx512bf16)\n                    endif()\n                endif()\n                if (GGML_AMX_TILE)\n                    list(APPEND ARCH_DEFINITIONS __AMX_TILE__ GGML_AMX_TILE)\n                endif()\n                if (GGML_AMX_INT8)\n                    list(APPEND ARCH_DEFINITIONS __AMX_INT8__ GGML_AMX_INT8)\n                endif()\n                if (GGML_AMX_BF16)\n                    list(APPEND ARCH_DEFINITIONS __AMX_BF16__ GGML_AMX_BF16)\n                endif()\n            elseif (GGML_AVX2)\n                list(APPEND ARCH_FLAGS /arch:AVX2)\n                list(APPEND ARCH_DEFINITIONS GGML_AVX2 GGML_FMA GGML_F16C)\n            elseif (GGML_AVX)\n                list(APPEND ARCH_FLAGS /arch:AVX)\n                list(APPEND ARCH_DEFINITIONS GGML_AVX)\n            elseif (GGML_SSE42)\n                list(APPEND ARCH_FLAGS /arch:SSE4.2)\n                list(APPEND ARCH_DEFINITIONS GGML_SSE42)\n            endif()\n            if (GGML_AVX_VNNI)\n                list(APPEND ARCH_DEFINITIONS __AVXVNNI__ GGML_AVX_VNNI)\n            endif()\n```\n\n----------------------------------------\n\nTITLE: Configuring PyTorch CPU Dependencies\nDESCRIPTION: Requirements file that specifies PyTorch version 2.2.1 for CPU and includes additional requirements from a separate file for legacy Llama conversion. Uses PyTorch's custom package index for CPU-specific wheels.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/requirements/requirements-convert_hf_to_gguf.txt#2025-04-22_snippet_0\n\nLANGUAGE: txt\nCODE:\n```\n-r ./requirements-convert_legacy_llama.txt\n--extra-index-url https://download.pytorch.org/whl/cpu\ntorch~=2.2.1\n```\n\n----------------------------------------\n\nTITLE: Setting ROCM_PATH in CMake for HIP/ROCm Configuration\nDESCRIPTION: Determines the ROCM_PATH based on environment variables or default locations. This path is crucial for locating HIP and ROCm libraries.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-hip/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif (NOT EXISTS $ENV{ROCM_PATH})\n    if (NOT EXISTS /opt/rocm)\n        set(ROCM_PATH /usr)\n    else()\n        set(ROCM_PATH /opt/rocm)\n    endif()\nelse()\n    set(ROCM_PATH $ENV{ROCM_PATH})\nendif()\n\nlist(APPEND CMAKE_PREFIX_PATH  ${ROCM_PATH})\nlist(APPEND CMAKE_PREFIX_PATH \"${ROCM_PATH}/lib64/cmake\")\n```\n\n----------------------------------------\n\nTITLE: MUSA Runtime Device Configuration\nDESCRIPTION: Example of using MUSA environmental variables to control GPU device visibility at runtime.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nMUSA_VISIBLE_DEVICES=\"-0\" ./build/bin/llama-server --model /srv/models/llama.gguf\n```\n\n----------------------------------------\n\nTITLE: Installing BLIS System-wide\nDESCRIPTION: Installs the compiled BLIS framework to the system directory.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/BLIS.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsudo make install\n```\n\n----------------------------------------\n\nTITLE: Verifying SYCL Installation on Windows\nDESCRIPTION: Command to list available SYCL devices on Windows after oneAPI installation.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_17\n\nLANGUAGE: sh\nCODE:\n```\nsycl-ls.exe\n```\n\n----------------------------------------\n\nTITLE: Configuring Tokenizer Tests for Various Model Vocabularies in llama.cpp\nDESCRIPTION: Creates a tokenizer test executable and registers multiple tests using it with different model vocabulary files. Each test uses the same executable but with different GGUF vocabulary model files as arguments.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/tests/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nadd_executable(test-tokenizer-0 test-tokenizer-0.cpp)\ntarget_link_libraries(test-tokenizer-0 PRIVATE common)\ninstall(TARGETS test-tokenizer-0 RUNTIME)\n\nllama_test(test-tokenizer-0 NAME test-tokenizer-0-bert-bge          ARGS ${CMAKE_CURRENT_SOURCE_DIR}/../models/ggml-vocab-bert-bge.gguf)\nllama_test(test-tokenizer-0 NAME test-tokenizer-0-command-r         ARGS ${CMAKE_CURRENT_SOURCE_DIR}/../models/ggml-vocab-command-r.gguf)\nllama_test(test-tokenizer-0 NAME test-tokenizer-0-deepseek-coder    ARGS ${CMAKE_CURRENT_SOURCE_DIR}/../models/ggml-vocab-deepseek-coder.gguf)\nllama_test(test-tokenizer-0 NAME test-tokenizer-0-deepseek-llm      ARGS ${CMAKE_CURRENT_SOURCE_DIR}/../models/ggml-vocab-deepseek-llm.gguf)\nllama_test(test-tokenizer-0 NAME test-tokenizer-0-falcon            ARGS ${CMAKE_CURRENT_SOURCE_DIR}/../models/ggml-vocab-falcon.gguf)\nllama_test(test-tokenizer-0 NAME test-tokenizer-0-gpt-2             ARGS ${CMAKE_CURRENT_SOURCE_DIR}/../models/ggml-vocab-gpt-2.gguf)\nllama_test(test-tokenizer-0 NAME test-tokenizer-0-llama-bpe         ARGS ${CMAKE_CURRENT_SOURCE_DIR}/../models/ggml-vocab-llama-bpe.gguf)\nllama_test(test-tokenizer-0 NAME test-tokenizer-0-llama-spm         ARGS ${CMAKE_CURRENT_SOURCE_DIR}/../models/ggml-vocab-llama-spm.gguf)\nllama_test(test-tokenizer-0 NAME test-tokenizer-0-mpt               ARGS ${CMAKE_CURRENT_SOURCE_DIR}/../models/ggml-vocab-mpt.gguf)\nllama_test(test-tokenizer-0 NAME test-tokenizer-0-phi-3             ARGS ${CMAKE_CURRENT_SOURCE_DIR}/../models/ggml-vocab-phi-3.gguf)\nllama_test(test-tokenizer-0 NAME test-tokenizer-0-qwen2             ARGS ${CMAKE_CURRENT_SOURCE_DIR}/../models/ggml-vocab-qwen2.gguf)\nllama_test(test-tokenizer-0 NAME test-tokenizer-0-refact            ARGS ${CMAKE_CURRENT_SOURCE_DIR}/../models/ggml-vocab-refact.gguf)\nllama_test(test-tokenizer-0 NAME test-tokenizer-0-starcoder         ARGS ${CMAKE_CURRENT_SOURCE_DIR}/../models/ggml-vocab-starcoder.gguf)\n```\n\n----------------------------------------\n\nTITLE: Setting up Node.js Client Directory\nDESCRIPTION: Commands to create and navigate to a new directory for the llama client implementation\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmkdir llama-client\ncd llama-client\n```\n\n----------------------------------------\n\nTITLE: Project Contributors\nDESCRIPTION: List of contributors to the project.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/MobileVLM.md#2025-04-22_snippet_16\n\nLANGUAGE: sh\nCODE:\n```\nzhangjidong05, yangyang260, huyiming03, chenxiaotao03, ZiangWu-77\n```\n\n----------------------------------------\n\nTITLE: Loading Local llama.cpp Library in Android Project\nDESCRIPTION: Adds the local llama.cpp source directory to the build process, allowing the Android project to compile and link against it.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llama.android/llama/src/main/cpp/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(../../../../../../ build-llama)\n```\n\n----------------------------------------\n\nTITLE: Ascend SOC Type Detection Function\nDESCRIPTION: Function to automatically detect Ascend SOC type using npu-smi tool. Fails build if detection unsuccessful.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-cann/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nfunction(detect_ascend_soc_type SOC_VERSION)\n    execute_process(\n        COMMAND bash -c \"npu-smi info|awk -F' ' 'NF > 0 && NR==7 {print $3}'\"\n        OUTPUT_VARIABLE npu_info\n        RESULT_VARIABLE npu_result\n        OUTPUT_STRIP_TRAILING_WHITESPACE\n    )\n    if(\"${npu_info}\" STREQUAL \"\" OR ${npu_result})\n        message(FATAL_ERROR \"Auto-detech ascend soc type failed, please specify manually or check ascend device working normally.\")\n    endif()\n    set(${SOC_VERSION} \"Ascend${npu_info}\" PARENT_SCOPE)\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Container Management\nDESCRIPTION: Command for restarting containers when troubleshooting NVIDIA driver issues\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/CUDA-FEDORA.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npodman container restart --all\n```\n\n----------------------------------------\n\nTITLE: Configuring llama-simple Executable Build in CMake\nDESCRIPTION: Sets up the build configuration for the llama-simple executable which compiles simple.cpp. It links against the llama library and pthread, requires C++17 standard, and configures installation of the runtime binary.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/simple/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET llama-simple)\nadd_executable(${TARGET} simple.cpp)\ninstall(TARGETS ${TARGET} RUNTIME)\ntarget_link_libraries(${TARGET} PRIVATE llama ${CMAKE_THREAD_LIBS_INIT})\ntarget_compile_features(${TARGET} PRIVATE cxx_std_17)\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for Legacy LLAMA Conversion\nDESCRIPTION: This requirements file lists the necessary Python packages and their versions for converting legacy LLAMA models. It includes Pillow for image processing, PyTorch for deep learning operations, and TorchVision for computer vision tasks. The file also references an external requirements file for additional dependencies.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llava/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\n-r ../../requirements/requirements-convert_legacy_llama.txt\n--extra-index-url https://download.pytorch.org/whl/cpu\npillow~=10.2.0\ntorch~=2.2.1\ntorchvision~=0.17.1\n```\n\n----------------------------------------\n\nTITLE: Configuring llama-eval-callback Executable Build and Test in CMake\nDESCRIPTION: This CMake configuration sets up the llama-eval-callback executable, linking it with necessary libraries and setting C++17 as the standard. It also configures a test that runs the executable with a small LLaMA model from the Hugging Face repository.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/eval-callback/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET llama-eval-callback)\nadd_executable(${TARGET} eval-callback.cpp)\ninstall(TARGETS ${TARGET} RUNTIME)\ntarget_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})\ntarget_compile_features(${TARGET} PRIVATE cxx_std_17)\n\nset(TEST_TARGET test-eval-callback)\nadd_test(NAME ${TEST_TARGET}\n        COMMAND llama-eval-callback --hf-repo ggml-org/models --hf-file tinyllamas/stories260K.gguf --model stories260K.gguf --prompt hello --seed 42 -ngl 0)\nset_property(TEST ${TEST_TARGET} PROPERTY LABELS eval-callback curl)\n```\n\n----------------------------------------\n\nTITLE: Creating Kanji Mnemonics in Markdown\nDESCRIPTION: This snippet demonstrates the format used for creating mnemonics for kanji characters. It includes the kanji, its meaning, components, and a mnemonic that incorporates the meanings of the components.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/prompts/mnemonics.txt#2025-04-22_snippet_0\n\nLANGUAGE: Markdown\nCODE:\n```\nKanji: æ¬  (lack of)\nComponents: ð ‚Š (hook claw), äºº (person)\nMnemonic: This **person** is a pirate. He lost his hand to a crocodile many years ago. Nowadays, the ***lack of*** a hand does not bother him too much. In fact, the **hook claw** that replaces it is the mark of a true pirate, so he is quite proud of it!\n```\n\n----------------------------------------\n\nTITLE: Example GDB Command for Tokenizer Test\nDESCRIPTION: This is a concrete example of running GDB with the test-tokenizer-0 binary and the llama-spm vocabulary model. It shows how to apply the abstract GDB command with actual file paths.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/development/debugging-tests.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ngdb --args ~/llama.cpp/build-ci-debug/bin/test-tokenizer-0 \"~/llama.cpp/tests/../models/ggml-vocab-llama-spm.gguf\"\n```\n\n----------------------------------------\n\nTITLE: Creating C Test Executable for llama.cpp\nDESCRIPTION: Creates a test executable from a C source file, linking it with the llama library. This is marked as a dummy executable that won't be installed.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/tests/CMakeLists.txt#2025-04-22_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\n# dummy executable - not installed\nget_filename_component(TEST_TARGET test-c.c NAME_WE)\nadd_executable(${TEST_TARGET} test-c.c)\ntarget_link_libraries(${TEST_TARGET} PRIVATE llama)\n```\n\n----------------------------------------\n\nTITLE: Setting Platform-Specific Default Options\nDESCRIPTION: Configures platform-specific defaults for Metal and BLAS backends, with special handling for Apple platforms.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nif (APPLE)\n    set(GGML_METAL_DEFAULT ON)\n    set(GGML_BLAS_DEFAULT ON)\n    set(GGML_BLAS_VENDOR_DEFAULT \"Apple\")\nelse()\n    set(GGML_METAL_DEFAULT OFF)\n    set(GGML_BLAS_DEFAULT OFF)\n    set(GGML_BLAS_VENDOR_DEFAULT \"Generic\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting MUSA Path in CMake for llama.cpp\nDESCRIPTION: Determines the MUSA installation path based on environment variables or default locations. Sets the C and C++ compilers to use MUSA's clang and clang++.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-musa/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif (NOT EXISTS $ENV{MUSA_PATH})\n    if (NOT EXISTS /opt/musa)\n        set(MUSA_PATH /usr/local/musa)\n    else()\n        set(MUSA_PATH /opt/musa)\n    endif()\nelse()\n    set(MUSA_PATH $ENV{MUSA_PATH})\nendif()\n\nset(CMAKE_C_COMPILER \"${MUSA_PATH}/bin/clang\")\nset(CMAKE_C_EXTENSIONS OFF)\nset(CMAKE_CXX_COMPILER \"${MUSA_PATH}/bin/clang++\")\nset(CMAKE_CXX_EXTENSIONS OFF)\n\nlist(APPEND CMAKE_MODULE_PATH \"${MUSA_PATH}/cmake\")\n\nfind_package(MUSAToolkit)\n```\n\n----------------------------------------\n\nTITLE: Configuring llama-embedding CMake Target\nDESCRIPTION: Sets up the CMake build configuration for the llama-embedding executable. Links against common and llama libraries, requires C++17, and configures installation settings.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/embedding/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET llama-embedding)\nadd_executable(${TARGET} embedding.cpp)\ninstall(TARGETS ${TARGET} RUNTIME)\ntarget_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})\ntarget_compile_features(${TARGET} PRIVATE cxx_std_17)\n```\n\n----------------------------------------\n\nTITLE: Referencing Legacy Llama Conversion Requirements\nDESCRIPTION: References an external requirements file that contains dependencies needed for converting legacy llama models. Uses relative path reference to another requirements file.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/requirements/requirements-convert_llama_ggml_to_gguf.txt#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n-r ./requirements-convert_legacy_llama.txt\n```\n\n----------------------------------------\n\nTITLE: Example Output of Default Verification\nDESCRIPTION: This is the output of verifying a GGUF file against its manifest using the default SHA-256 hash algorithm. It shows the verification status for each tensor and the overall file.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/gguf-hash/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmanifest  test.gguf.manifest  sha256  sha1  xxh64\nsha256    c0510d38fa060c46265e0160a85c7243096b01dd31c2f355bdbb5516b20de1bd  test.gguf:tensor_0  -  Ok\nsha256    8514cbcc73692a2c56bd7a33a022edd5ff819614bd23b19915d7224387f397a7  test.gguf:tensor_1  -  Ok\nsha256    947e6b36e20f2cc95e1d2ce1c1669d813d574657ac6b5ac5196158d454d35180  test.gguf:tensor_2  -  Ok\nsha256    423b044e016d8ac73c39f23f60bf01bedef5ecb03c0230accd824c91fe86f1a1  test.gguf:tensor_3  -  Ok\nsha256    79737cb3912d4201384cf7f16a1a37ff7823f23ea796cb205b6ca361ab9e3ebf  test.gguf:tensor_4  -  Ok\nsha256    60949be8298eced0ecdde64487643d018407bd261691e061d9e9c3dbc9fd358b  test.gguf:tensor_5  -  Ok\nsha256    574f4c46ff384a3b9a225eb955d2a871847a2e8b3fa59387a8252832e92ef7b0  test.gguf:tensor_6  -  Ok\nsha256    4c0410cd3c500f078ae5b21e8dc9eb79e29112713b2ab58a882f82a3868d4d75  test.gguf:tensor_7  -  Ok\nsha256    c4401313feeba0261275c3b25bd2d8fe40ce04e0f440c2980ed0e9674c30ff01  test.gguf:tensor_8  -  Ok\nsha256    23d57cf0d7a6e90b0b3616b41300e0cd354781e812add854a5f95aa55f2bc514  test.gguf:tensor_9  -  Ok\nsha256    7dd641b32f59b60dbd4b5420c4b0f6321ccf48f58f6ae201a3dbc4a58a27c6e4  test.gguf  -  Ok\n\nVerification results for test.gguf.manifest - Success\n```\n\n----------------------------------------\n\nTITLE: Accessing SimpleChat Frontend\nDESCRIPTION: URL format to access the SimpleChat web interface through local browser\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/server/public_simplechat/readme.md#2025-04-22_snippet_2\n\nLANGUAGE: http\nCODE:\n```\nhttp://127.0.0.1:PORT/index.html\n```\n\n----------------------------------------\n\nTITLE: Uploading Package Distribution\nDESCRIPTION: Command to upload the built distribution archives to PyPI\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/gguf-py/README.md#2025-04-22_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\npython -m twine upload dist/*\n```\n\n----------------------------------------\n\nTITLE: Installing GGUF with GUI Support\nDESCRIPTION: Installation command for GGUF package with additional GUI editor functionality\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/gguf-py/README.md#2025-04-22_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\npip install gguf[gui]\n```\n\n----------------------------------------\n\nTITLE: Configuring MUSA Architectures and Source Files for llama.cpp\nDESCRIPTION: Sets up MUSA architectures, defines MUSA-specific header and source files, and configures compilation flags for MUSA sources. Includes conditional compilation for different quantization options.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-musa/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif (MUSAToolkit_FOUND)\n    message(STATUS \"MUSA Toolkit found\")\n\n    if (NOT DEFINED MUSA_ARCHITECTURES)\n        set(MUSA_ARCHITECTURES \"21;22;31\")\n    endif()\n    message(STATUS \"Using MUSA architectures: ${MUSA_ARCHITECTURES}\")\n\n    file(GLOB   GGML_HEADERS_MUSA \"../ggml-cuda/*.cuh\")\n    list(APPEND GGML_HEADERS_MUSA \"../../include/ggml-cuda.h\")\n\n    file(GLOB   GGML_SOURCES_MUSA \"../ggml-cuda/*.cu\")\n    file(GLOB   SRCS \"../ggml-cuda/template-instances/fattn-mma*.cu\")\n    list(APPEND GGML_SOURCES_MUSA ${SRCS})\n    file(GLOB   SRCS \"../ggml-cuda/template-instances/mmq*.cu\")\n    list(APPEND GGML_SOURCES_MUSA ${SRCS})\n\n    if (GGML_CUDA_FA_ALL_QUANTS)\n        file(GLOB   SRCS \"../ggml-cuda/template-instances/fattn-vec*.cu\")\n        list(APPEND GGML_SOURCES_MUSA ${SRCS})\n        add_compile_definitions(GGML_CUDA_FA_ALL_QUANTS)\n    else()\n        file(GLOB   SRCS \"../ggml-cuda/template-instances/fattn-vec*q4_0-q4_0.cu\")\n        list(APPEND GGML_SOURCES_MUSA ${SRCS})\n        file(GLOB   SRCS \"../ggml-cuda/template-instances/fattn-vec*q8_0-q8_0.cu\")\n        list(APPEND GGML_SOURCES_MUSA ${SRCS})\n        file(GLOB   SRCS \"../ggml-cuda/template-instances/fattn-vec*f16-f16.cu\")\n        list(APPEND GGML_SOURCES_MUSA ${SRCS})\n    endif()\n\n    set_source_files_properties(${GGML_SOURCES_MUSA} PROPERTIES LANGUAGE CXX)\n    foreach(SOURCE ${GGML_SOURCES_MUSA})\n        set(COMPILE_FLAGS \"-fsigned-char -x musa -mtgpu\")\n        foreach(ARCH ${MUSA_ARCHITECTURES})\n            set(COMPILE_FLAGS \"${COMPILE_FLAGS} --cuda-gpu-arch=mp_${ARCH}\")\n        endforeach()\n        set_property(SOURCE ${SOURCE} PROPERTY COMPILE_FLAGS ${COMPILE_FLAGS})\n    endforeach()\n```\n\n----------------------------------------\n\nTITLE: Configuring llama-speculative Build in CMake\nDESCRIPTION: Sets up the build configuration for the llama-speculative executable. It defines the target, specifies the source file, sets up installation, links required libraries including common, llama, and threading libraries, and sets the C++ standard to C++17.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/speculative/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET llama-speculative)\nadd_executable(${TARGET} speculative.cpp)\ninstall(TARGETS ${TARGET} RUNTIME)\ntarget_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})\ntarget_compile_features(${TARGET} PRIVATE cxx_std_17)\n```\n\n----------------------------------------\n\nTITLE: Upgrading Pip\nDESCRIPTION: Command to upgrade pip to the latest version to support editable installation\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/gguf-py/README.md#2025-04-22_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\npip install --upgrade pip\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake Build System for llama.cpp Examples\nDESCRIPTION: This CMake configuration file manages the build process for multiple examples in the llama.cpp project. It sets up required dependencies like Threads, adds compile flags, and conditionally includes subdirectories based on the target platform and build configuration. The file differentiates between Emscripten, Windows, and other platforms, and handles special cases for certain examples.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\n# dependencies\n\nfind_package(Threads REQUIRED)\n\n# third-party\n\n# ...\n\n# flags\n\nllama_add_compile_flags()\n\n# examples\n\ninclude_directories(${CMAKE_CURRENT_SOURCE_DIR})\n\nif (EMSCRIPTEN)\nelse()\n    add_subdirectory(batched-bench)\n    add_subdirectory(batched)\n    add_subdirectory(embedding)\n    add_subdirectory(eval-callback)\n\n    if (NOT WIN32)\n        # disabled on Windows because it uses internal functions not exported with LLAMA_API\n        add_subdirectory(gbnf-validator)\n    endif()\n\n    add_subdirectory(gguf-hash)\n    add_subdirectory(gguf-split)\n    add_subdirectory(gguf)\n    add_subdirectory(gritlm)\n    add_subdirectory(imatrix)\n    add_subdirectory(infill)\n    add_subdirectory(llama-bench)\n    add_subdirectory(lookahead)\n    add_subdirectory(lookup)\n    add_subdirectory(main)\n    add_subdirectory(parallel)\n    add_subdirectory(passkey)\n    add_subdirectory(perplexity)\n    add_subdirectory(quantize)\n    add_subdirectory(retrieval)\n    if (LLAMA_BUILD_SERVER)\n        add_subdirectory(server)\n    endif()\n    add_subdirectory(save-load-state)\n    add_subdirectory(run)\n    add_subdirectory(simple)\n    add_subdirectory(simple-chat)\n    add_subdirectory(speculative)\n    add_subdirectory(speculative-simple)\n    add_subdirectory(tokenize)\n    add_subdirectory(tts)\n    add_subdirectory(gen-docs)\n    if (NOT GGML_BACKEND_DL)\n        # these examples use the backends directly and cannot be built with dynamic loading\n        add_subdirectory(convert-llama2c-to-ggml)\n        add_subdirectory(cvector-generator)\n        add_subdirectory(export-lora)\n        if (NOT WIN32)\n            # disabled on Windows because it uses internal functions not exported with LLAMA_API\n            add_subdirectory(quantize-stats)\n        endif()\n        add_subdirectory(llava)\n        if (GGML_RPC)\n            add_subdirectory(rpc)\n        endif()\n        if (GGML_SYCL)\n            add_subdirectory(sycl)\n        endif()\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Additional Default Values for GGML Features\nDESCRIPTION: Sets default values for additional GGML features if they haven't been defined elsewhere.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/CMakeLists.txt#2025-04-22_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\n# defaults\nif (NOT GGML_LLAMAFILE_DEFAULT)\n    set(GGML_LLAMAFILE_DEFAULT OFF)\nendif()\n\nif (NOT GGML_CUDA_GRAPHS_DEFAULT)\n    set(GGML_CUDA_GRAPHS_DEFAULT OFF)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Pushing Git Tags\nDESCRIPTION: Git command to push tags to remote repository\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/gguf-py/README.md#2025-04-22_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\ngit push origin --tags\n```\n\n----------------------------------------\n\nTITLE: Configuring and Building llama-tokenize Executable with CMake\nDESCRIPTION: Defines the llama-tokenize target from tokenize.cpp source file, configures its installation, links required libraries including common, llama, and thread libraries, and sets C++17 as the required standard.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/tokenize/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET llama-tokenize)\nadd_executable(${TARGET} tokenize.cpp)\ninstall(TARGETS ${TARGET} RUNTIME)\ntarget_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})\ntarget_compile_features(${TARGET} PRIVATE cxx_std_17)\n```\n\n----------------------------------------\n\nTITLE: Installing OpenCL Headers and ICD Loader for Android\nDESCRIPTION: Commands to install OpenCL headers and ICD loader library for Android development with NDK.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_25\n\nLANGUAGE: sh\nCODE:\n```\nmkdir -p ~/dev/llm\ncd ~/dev/llm\n\ngit clone https://github.com/KhronosGroup/OpenCL-Headers && \\\ncd OpenCL-Headers && \\\ncp -r CL $ANDROID_NDK/toolchains/llvm/prebuilt/linux-x86_64/sysroot/usr/include\n\ncd ~/dev/llm\n\ngit clone https://github.com/KhronosGroup/OpenCL-ICD-Loader && \\\ncd OpenCL-ICD-Loader && \\\nmkdir build_ndk && cd build_ndk && \\\ncmake .. -G Ninja -DCMAKE_BUILD_TYPE=Release \\\n  -DCMAKE_TOOLCHAIN_FILE=$ANDROID_NDK/build/cmake/android.toolchain.cmake \\\n  -DOPENCL_ICD_LOADER_HEADERS_DIR=$ANDROID_NDK/toolchains/llvm/prebuilt/linux-x86_64/sysroot/usr/include \\\n  -DANDROID_ABI=arm64-v8a \\\n  -DANDROID_PLATFORM=24 \\\n  -DANDROID_STL=c++_shared && \\\nninja && \\\ncp libOpenCL.so $ANDROID_NDK/toolchains/llvm/prebuilt/linux-x86_64/sysroot/usr/lib/aarch64-linux-android\n```\n\n----------------------------------------\n\nTITLE: Sample Output from Model Inference Callback\nDESCRIPTION: Example output produced by the callback during model inference. It shows tensor operations and their data values, including embedding lookups, normalization, attention mechanisms, and tensor transformations.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/eval-callback/README.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nllm_load_tensors: offloaded 33/33 layers to GPU\n...\nllama_new_context_with_model: n_ctx      = 512\n...\nllama_new_context_with_model:      CUDA0 compute buffer size =   105.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =     6.01 MiB\nllama_new_context_with_model: graph nodes  = 1225\nllama_new_context_with_model: graph splits = 2\nggml_debug:                 inp_embd = (f32)   GET_ROWS(token_embd.weight{2560, 51200, 1, 1}, inp_tokens{1, 1, 1, 1}}) = {2560, 1, 1, 1}\n                                     [\n                                      [\n                                       [ -0.0181,   0.0272,   0.0272, ...],\n                                      ],\n                                     ]\nggml_debug:                   norm-0 = (f32)       NORM(CUDA0#inp_embd#0{2560, 1, 1, 1}, }) = {2560, 1, 1, 1}\n                                     [\n                                      [\n                                       [ -0.6989,   1.0636,   1.0636, ...],\n                                      ],\n                                     ]\nggml_debug:                 norm_w-0 = (f32)        MUL(norm-0{2560, 1, 1, 1}, blk.0.attn_norm.weight{2560, 1, 1, 1}}) = {2560, 1, 1, 1}\n                                     [\n                                      [\n                                       [ -0.1800,   0.2817,   0.2632, ...],\n                                      ],\n                                     ]\nggml_debug:              attn_norm-0 = (f32)        ADD(norm_w-0{2560, 1, 1, 1}, blk.0.attn_norm.bias{2560, 1, 1, 1}}) = {2560, 1, 1, 1}\n                                     [\n                                      [\n                                       [ -0.1863,   0.2970,   0.2604, ...],\n                                      ],\n                                     ]\nggml_debug:                   wqkv-0 = (f32)    MUL_MAT(blk.0.attn_qkv.weight{2560, 7680, 1, 1}, attn_norm-0{2560, 1, 1, 1}}) = {7680, 1, 1, 1}\n                                     [\n                                      [\n                                       [ -1.1238,   1.2876,  -1.8086, ...],\n                                      ],\n                                     ]\nggml_debug:                   bqkv-0 = (f32)        ADD(wqkv-0{7680, 1, 1, 1}, blk.0.attn_qkv.bias{7680, 1, 1, 1}}) = {7680, 1, 1, 1}\n                                     [\n                                      [\n                                       [ -1.1135,   1.4604,  -1.9226, ...],\n                                      ],\n                                     ]\nggml_debug:            bqkv-0 (view) = (f32)       VIEW(bqkv-0{7680, 1, 1, 1}, }) = {2560, 1, 1, 1}\n                                     [\n                                      [\n                                       [ -1.1135,   1.4604,  -1.9226, ...],\n                                      ],\n                                     ]\nggml_debug:                   Qcur-0 = (f32)       CONT(bqkv-0 (view){2560, 1, 1, 1}, }) = {2560, 1, 1, 1}\n                                     [\n                                      [\n                                       [ -1.1135,   1.4604,  -1.9226, ...],\n                                      ],\n                                     ]\nggml_debug:        Qcur-0 (reshaped) = (f32)    RESHAPE(Qcur-0{2560, 1, 1, 1}, }) = {80, 32, 1, 1}\n                                     [\n                                      [\n                                       [ -1.1135,   1.4604,  -1.9226, ...],\n                                       [ -0.3608,   0.5076,  -1.8866, ...],\n                                       [  1.7643,   0.0273,  -2.1065, ...],\n                                       ...\n                                      ],\n                                     ]\nggml_debug:                   Qcur-0 = (f32)       ROPE(Qcur-0 (reshaped){80, 32, 1, 1}, CUDA0#inp_pos#0{1, 1, 1, 1}}) = {80, 32, 1, 1}\n                                     [\n                                      [\n                                       [ -1.1135,   1.4604,  -1.9226, ...],\n                                       [ -0.3608,   0.5076,  -1.8866, ...],\n                                       [  1.7643,   0.0273,  -2.1065, ...],\n                                       ...\n                                      ],\n                                     ]\n```\n\n----------------------------------------\n\nTITLE: Installing Publishing Dependencies\nDESCRIPTION: Command to install required packages for manual publishing\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/gguf-py/README.md#2025-04-22_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\npip install build twine\n```\n\n----------------------------------------\n\nTITLE: Configuring Standalone Mode for GGML\nDESCRIPTION: Determines if GGML is being built as a standalone project or integrated as a subproject. Sets the output directory for binaries in standalone mode.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif (CMAKE_SOURCE_DIR STREQUAL CMAKE_CURRENT_SOURCE_DIR)\n    set(GGML_STANDALONE ON)\n\n    set(CMAKE_RUNTIME_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/bin)\n\n    # configure project version\n    # TODO\nelse()\n    set(GGML_STANDALONE OFF)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Vulkan Build Configuration for Windows\nDESCRIPTION: CMake commands for building llama.cpp with Vulkan support on Windows.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\ncmake -B build -DGGML_VULKAN=ON\ncmake --build build --config Release\n```\n\n----------------------------------------\n\nTITLE: Creating Local Directories for MUSA CI Docker Setup\nDESCRIPTION: Commands to create local directories for storing cached models, configuration files, virtual environments, and CI run results when running MUSA CI in a Docker container.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ci/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmkdir -p $HOME/llama.cpp/ci-cache\n```\n\n----------------------------------------\n\nTITLE: Aggregating Python Dependencies for llama.cpp Project\nDESCRIPTION: This requirements file imports multiple specialized requirements files for different components of the llama.cpp project, including examples, server components, conversion utilities, benchmarking tools, and GUI applications.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/requirements/requirements-all.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n-r ../examples/llava/requirements.txt\n-r ../examples/server/bench/requirements.txt\n-r ../examples/server/tests/requirements.txt\n\n-r ./requirements-compare-llama-bench.txt\n-r ./requirements-pydantic.txt\n-r ./requirements-test-tokenizer-random.txt\n\n-r ./requirements-convert_hf_to_gguf.txt\n-r ./requirements-convert_hf_to_gguf_update.txt\n-r ./requirements-convert_legacy_llama.txt\n-r ./requirements-convert_llama_ggml_to_gguf.txt\n-r ./requirements-tool_bench.txt\n\n-r ./requirements-gguf_editor_gui.txt\n```\n\n----------------------------------------\n\nTITLE: Defining Shader Compilation Function in CMake\nDESCRIPTION: This function compiles shader files, generates header files from the compiled shaders, and sets up custom commands for the build process. It handles different compilation steps for Visual Studio and other generators.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-kompute/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(compile_shader)\n    set(options)\n    set(oneValueArgs)\n    set(multiValueArgs SOURCES)\n    cmake_parse_arguments(compile_shader \"${options}\" \"${oneValueArgs}\" \"${multiValueArgs}\" ${ARGN})\n    foreach(source ${compile_shader_SOURCES})\n        get_filename_component(filename ${source} NAME)\n        set(spv_file ${filename}.spv)\n        add_custom_command(\n            OUTPUT ${spv_file}\n            DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/${source}\n            ${CMAKE_CURRENT_SOURCE_DIR}/kompute-shaders/common.comp\n            ${CMAKE_CURRENT_SOURCE_DIR}/kompute-shaders/op_getrows.comp\n            ${CMAKE_CURRENT_SOURCE_DIR}/kompute-shaders/op_mul_mv_q_n_pre.comp\n            ${CMAKE_CURRENT_SOURCE_DIR}/kompute-shaders/op_mul_mv_q_n.comp\n            COMMAND ${glslc_executable} --target-env=vulkan1.2 -o ${spv_file} ${CMAKE_CURRENT_SOURCE_DIR}/${source}\n            COMMENT \"Compiling ${source} to ${spv_file}\"\n            )\n\n        get_filename_component(RAW_FILE_NAME ${spv_file} NAME)\n        set(FILE_NAME \"shader${RAW_FILE_NAME}\")\n        string(REPLACE \".comp.spv\" \".h\" HEADER_FILE ${FILE_NAME})\n        string(TOUPPER ${HEADER_FILE} HEADER_FILE_DEFINE)\n        string(REPLACE \".\" \"_\" HEADER_FILE_DEFINE \"${HEADER_FILE_DEFINE}\")\n        set(OUTPUT_HEADER_FILE \"${HEADER_FILE}\")\n        message(STATUS \"${HEADER_FILE} generating ${HEADER_FILE_DEFINE}\")\n        if(CMAKE_GENERATOR MATCHES \"Visual Studio\")\n            add_custom_command(\n                OUTPUT ${OUTPUT_HEADER_FILE}\n                COMMAND ${CMAKE_COMMAND} -E echo \"/*THIS FILE HAS BEEN AUTOMATICALLY GENERATED - DO NOT EDIT*/\" > ${OUTPUT_HEADER_FILE}\n                COMMAND ${CMAKE_COMMAND} -E echo \\\"\\#ifndef ${HEADER_FILE_DEFINE}\\\" >> ${OUTPUT_HEADER_FILE}\n                COMMAND ${CMAKE_COMMAND} -E echo \\\"\\#define ${HEADER_FILE_DEFINE}\\\" >> ${OUTPUT_HEADER_FILE}\n                COMMAND ${CMAKE_COMMAND} -E echo \"namespace kp {\" >> ${OUTPUT_HEADER_FILE}\n                COMMAND ${CMAKE_COMMAND} -E echo \"namespace shader_data {\" >> ${OUTPUT_HEADER_FILE}\n                COMMAND ${CMAKE_BINARY_DIR}/bin/$<CONFIG>/xxd -i ${RAW_FILE_NAME} >> ${OUTPUT_HEADER_FILE}\n                COMMAND ${CMAKE_COMMAND} -E echo \"}}\" >> ${OUTPUT_HEADER_FILE}\n                COMMAND ${CMAKE_COMMAND} -E echo \\\"\\#endif // define ${HEADER_FILE_DEFINE}\\\" >> ${OUTPUT_HEADER_FILE}\n                DEPENDS ${spv_file} xxd\n                COMMENT \"Converting to hpp: ${FILE_NAME} ${CMAKE_BINARY_DIR}/bin/$<CONFIG>/xxd\"\n                )\n        else()\n            add_custom_command(\n                OUTPUT ${OUTPUT_HEADER_FILE}\n                COMMAND ${CMAKE_COMMAND} -E echo \"/*THIS FILE HAS BEEN AUTOMATICALLY GENERATED - DO NOT EDIT*/\" > ${OUTPUT_HEADER_FILE}\n                COMMAND ${CMAKE_COMMAND} -E echo \\\"\\#ifndef ${HEADER_FILE_DEFINE}\\\" >> ${OUTPUT_HEADER_FILE}\n                COMMAND ${CMAKE_COMMAND} -E echo \\\"\\#define ${HEADER_FILE_DEFINE}\\\" >> ${OUTPUT_HEADER_FILE}\n                COMMAND ${CMAKE_COMMAND} -E echo \"namespace kp {\" >> ${OUTPUT_HEADER_FILE}\n                COMMAND ${CMAKE_COMMAND} -E echo \"namespace shader_data {\" >> ${OUTPUT_HEADER_FILE}\n                COMMAND ${CMAKE_BINARY_DIR}/bin/xxd -i ${RAW_FILE_NAME} >> ${OUTPUT_HEADER_FILE}\n                COMMAND ${CMAKE_COMMAND} -E echo \"}}\" >> ${OUTPUT_HEADER_FILE}\n                COMMAND ${CMAKE_COMMAND} -E echo \\\"\\#endif // define ${HEADER_FILE_DEFINE}\\\" >> ${OUTPUT_HEADER_FILE}\n                DEPENDS ${spv_file} xxd\n                COMMENT \"Converting to hpp: ${FILE_NAME} ${CMAKE_BINARY_DIR}/bin/xxd\"\n                )\n        endif()\n    endforeach()\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Setting Up OpenCL Kernel Embedding in GGML CMake\nDESCRIPTION: Configures the embedding of OpenCL kernels, including directory setup and Python script for kernel embedding.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-opencl/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nif (GGML_OPENCL_EMBED_KERNELS)\n    add_compile_definitions(GGML_OPENCL_EMBED_KERNELS)\n\n    set(EMBED_KERNEL_SCRIPT \"${CMAKE_CURRENT_SOURCE_DIR}/kernels/embed_kernel.py\")\n    file(MAKE_DIRECTORY     \"${CMAKE_CURRENT_BINARY_DIR}/autogenerated\")\n\n    target_include_directories(${TARGET_NAME} PRIVATE \"${CMAKE_CURRENT_BINARY_DIR}/autogenerated\")\nendif ()\n```\n\n----------------------------------------\n\nTITLE: Building oneDNN with NVIDIA Backend\nDESCRIPTION: Commands to clone and build oneDNN from source with NVIDIA GPU support.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\ngit clone https://github.com/oneapi-src/oneDNN.git\ncd oneDNN\ncmake -GNinja -Bbuild-nvidia -DDNNL_CPU_RUNTIME=DPCPP -DDNNL_GPU_RUNTIME=DPCPP -DDNNL_GPU_VENDOR=NVIDIA -DONEDNN_BUILD_GRAPH=OFF -DCMAKE_C_COMPILER=icx -DCMAKE_CXX_COMPILER=icpx\ncmake --build build-nvidia --config Release\n```\n\n----------------------------------------\n\nTITLE: Finding Required Libraries for Metal Backend in CMake\nDESCRIPTION: Locates the Foundation, Metal, and MetalKit libraries required for the Metal backend implementation.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-metal/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfind_library(FOUNDATION_LIBRARY Foundation REQUIRED)\nfind_library(METAL_FRAMEWORK    Metal      REQUIRED)\nfind_library(METALKIT_FRAMEWORK MetalKit   REQUIRED)\n\nmessage(STATUS \"Metal framework found\")\n```\n\n----------------------------------------\n\nTITLE: Copying Metal Files in CMake for GGML\nDESCRIPTION: Copies necessary Metal-related files to the binary output directory.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/src/ggml-metal/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nconfigure_file(../ggml-common.h  ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/ggml-common.h     COPYONLY)\nconfigure_file(ggml-metal.metal  ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/ggml-metal.metal  COPYONLY)\nconfigure_file(ggml-metal-impl.h ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/ggml-metal-impl.h COPYONLY)\n```\n\n----------------------------------------\n\nTITLE: Converting Llama2.c Model Usage Help\nDESCRIPTION: Command-line help output showing available options for the llama2.c to GGML conversion tool, including required and optional parameters for model conversion.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/convert-llama2c-to-ggml/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nusage: ./llama-convert-llama2c-to-ggml [options]\n\noptions:\n  -h, --help                       show this help message and exit\n  --copy-vocab-from-model FNAME    path of gguf llama model or llama2.c vocabulary from which to copy vocab (default 'models/7B/ggml-model-f16.gguf')\n  --llama2c-model FNAME            [REQUIRED] model path from which to load Karpathy's llama2.c model\n  --llama2c-output-model FNAME     model path to save the converted llama2.c model (default ak_llama_model.bin')\n```\n\n----------------------------------------\n\nTITLE: Configuring llama-vdot Executable Build in CMake\nDESCRIPTION: Sets up the build configuration for the llama-vdot executable, which likely performs vector dot product operations. It links against the common and llama libraries and requires C++17 support.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/pocs/vdot/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET llama-vdot)\nadd_executable(${TARGET} vdot.cpp)\ntarget_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})\ntarget_compile_features(${TARGET} PRIVATE cxx_std_17)\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Dependencies for LLaMA Model Conversion\nDESCRIPTION: This snippet defines the Python package dependencies needed for converting legacy LLaMA models. It includes a reference to another requirements file and specifies a PyTorch version with CPU support.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/requirements/requirements-convert_hf_to_gguf_update.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n-r ./requirements-convert_legacy_llama.txt\n--extra-index-url https://download.pytorch.org/whl/cpu\ntorch~=2.2.1\n```\n\n----------------------------------------\n\nTITLE: Running LLaMA Multi-Modal CLI with Book Image\nDESCRIPTION: Example command for running llama-mtmd-cli to identify a book author from an image. Uses GGUF model format with GPU acceleration.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/MobileVLM.md#2025-04-22_snippet_12\n\nLANGUAGE: sh\nCODE:\n```\n./llama-mtmd-cli \\\n    -m /data/local/tmp/ggml-model-q4_k.gguf \\\n    --mmproj /data/local/tmp/mmproj-model-f16.gguf \\\n    --image /data/local/tmp/demo.jpeg \\\n    -p \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: <image>\\nWho is the author of this book? \\nAnswer the question using a single word or phrase. ASSISTANT:\" \\\n    --n-gpu-layers 999\n```\n\n----------------------------------------\n\nTITLE: Configuring Windows-Specific Version Flag\nDESCRIPTION: Sets up a Windows-specific version flag when building on Windows platforms.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ggml/CMakeLists.txt#2025-04-22_snippet_13\n\nLANGUAGE: CMake\nCODE:\n```\nif (WIN32)\n    set(GGML_WIN_VER \"0x602\" CACHE STRING   \"ggml: Windows version\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Generating a GGUF Hash Manifest with Multiple Hash Types\nDESCRIPTION: This command generates a manifest file containing multiple hash types (xxh64, sha1, sha256) for a GGUF file and all its internal tensors. The --all flag ensures all supported hash algorithms are used.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/gguf-hash/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./llama-gguf-hash --all test.gguf > test.gguf.manifest\n```\n\n----------------------------------------\n\nTITLE: Cloning GLMV-EDGE model repository\nDESCRIPTION: Git command to clone either the 2B or 5B version of the GLMV-EDGE model from Hugging Face. This is the first step in the GGUF conversion process.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/glmedge.md#2025-04-22_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ngit clone https://huggingface.co/THUDM/glm-edge-v-5b or https://huggingface.co/THUDM/glm-edge-v-2b\n```\n\n----------------------------------------\n\nTITLE: Testing llama.cpp with CANN Backend\nDESCRIPTION: Command to test llama.cpp with the CANN backend for Ascend NPU acceleration.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\n./build/bin/llama-cli -m PATH_TO_MODEL -p \"Building a website can be done in 10 steps:\" -ngl 32\n```\n\n----------------------------------------\n\nTITLE: Transferring Files to Android Device with ADB\nDESCRIPTION: ADB commands to create a directory on the Android device and push the compiled llama.cpp binaries and model files to it.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/android.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ adb shell \"mkdir /data/local/tmp/llama.cpp\"\n$ adb push {install-dir} /data/local/tmp/llama.cpp/\n$ adb push {model}.gguf /data/local/tmp/llama.cpp/\n$ adb shell\n```\n\n----------------------------------------\n\nTITLE: Running llama-bench with SQL output format\nDESCRIPTION: Command to run the llama-bench tool with SQL output format. The output contains SQL statements suitable for importing into a SQLite database, including table creation and data insertion commands.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llama-bench/README.md#2025-04-22_snippet_12\n\nLANGUAGE: sh\nCODE:\n```\n$ ./llama-bench -o sql\n```\n\n----------------------------------------\n\nTITLE: Configuring llama-gguf-split Executable with CMake\nDESCRIPTION: This snippet defines and configures the llama-gguf-split executable build target. It specifies the source file, installation rules, required libraries (common, llama, and threading libraries), and sets C++17 as the standard.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/gguf-split/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET llama-gguf-split)\nadd_executable(${TARGET} gguf-split.cpp)\ninstall(TARGETS ${TARGET} RUNTIME)\ntarget_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})\ntarget_compile_features(${TARGET} PRIVATE cxx_std_17)\n```\n\n----------------------------------------\n\nTITLE: Installing Python Requirements for LLaVA\nDESCRIPTION: Command to install the required Python packages for working with LLaVA models.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/multimodal/llava.md#2025-04-22_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\npip install -r examples/llava/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Adding User to GPU Groups for Intel GPUs\nDESCRIPTION: Commands to add the current user to 'video' and 'render' groups, which is necessary for Intel GPU access.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nsudo usermod -aG render $USER\nsudo usermod -aG video $USER\n```\n\n----------------------------------------\n\nTITLE: Creating Shared Library for Android JNI Integration\nDESCRIPTION: Defines a shared library target that includes the JNI interface code. This library will be packaged with the APK and loaded through System.loadLibrary() from Java/Kotlin code.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/llama.android/llama/src/main/cpp/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(${CMAKE_PROJECT_NAME} SHARED\n        # List C/C++ source files with relative paths to this CMakeLists.txt.\n        llama-android.cpp)\n```\n\n----------------------------------------\n\nTITLE: Verifying GPU Installation with clinfo\nDESCRIPTION: Commands to install clinfo and list available OpenCL platforms and devices.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nsudo apt install clinfo\nsudo clinfo -l\n```\n\n----------------------------------------\n\nTITLE: Installing OpenCL Headers and ICD Loader for Windows Arm64\nDESCRIPTION: PowerShell commands to install OpenCL headers and ICD loader library for Windows Arm64 development.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#2025-04-22_snippet_27\n\nLANGUAGE: powershell\nCODE:\n```\nmkdir -p ~/dev/llm\n\ncd ~/dev/llm\ngit clone https://github.com/KhronosGroup/OpenCL-Headers && cd OpenCL-Headers\nmkdir build && cd build\ncmake .. -G Ninja `\n  -DBUILD_TESTING=OFF `\n  -DOPENCL_HEADERS_BUILD_TESTING=OFF `\n  -DOPENCL_HEADERS_BUILD_CXX_TESTS=OFF `\n  -DCMAKE_INSTALL_PREFIX=\"$HOME/dev/llm/opencl\"\ncmake --build . --target install\n\ncd ~/dev/llm\ngit clone https://github.com/KhronosGroup/OpenCL-ICD-Loader && cd OpenCL-ICD-Loader\nmkdir build && cd build\ncmake .. -G Ninja `\n  -DCMAKE_BUILD_TYPE=Release `\n  -DCMAKE_PREFIX_PATH=\"$HOME/dev/llm/opencl\" `\n  -DCMAKE_INSTALL_PREFIX=\"$HOME/dev/llm/opencl\"\ncmake --build . --target install\n```\n\n----------------------------------------\n\nTITLE: Measuring KL Divergence with llama-perplexity\nDESCRIPTION: Placeholder for measuring Kullback-Leibler divergence between model distributions, which is currently marked as TODO in the documentation.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/README.md#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n# TODO\n```\n\n----------------------------------------\n\nTITLE: Creating Build Information Generator Command in CMake\nDESCRIPTION: Sets up a custom command to generate build-info.cpp when Git index changes. This command passes compiler information to a CMake script that generates build details from Git repository data.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/common/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nadd_custom_command(\n    OUTPUT \"${CMAKE_CURRENT_SOURCE_DIR}/build-info.cpp\"\n    COMMENT \"Generating build details from Git\"\n    COMMAND ${CMAKE_COMMAND} -DMSVC=${MSVC} -DCMAKE_C_COMPILER_VERSION=${CMAKE_C_COMPILER_VERSION}\n            -DCMAKE_C_COMPILER_ID=${CMAKE_C_COMPILER_ID} -DCMAKE_VS_PLATFORM_NAME=${CMAKE_VS_PLATFORM_NAME}\n            -DCMAKE_C_COMPILER=${CMAKE_C_COMPILER} -P \"${CMAKE_CURRENT_SOURCE_DIR}/cmake/build-info-gen-cpp.cmake\"\n    WORKING_DIRECTORY \"${CMAKE_CURRENT_SOURCE_DIR}/..\"\n    DEPENDS \"${CMAKE_CURRENT_SOURCE_DIR}/build-info.cpp.in\" ${GIT_INDEX}\n    VERBATIM\n)\nset(TARGET build_info)\nadd_library(${TARGET} OBJECT build-info.cpp)\nif (BUILD_SHARED_LIBS)\n    set_target_properties(${TARGET} PROPERTIES POSITION_INDEPENDENT_CODE ON)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Listing SYCL Devices for llama.cpp on Windows\nDESCRIPTION: Command to list available SYCL devices for llama.cpp on Windows.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_24\n\nLANGUAGE: sh\nCODE:\n```\nbuild\\bin\\llama-ls-sycl-device.exe\n```\n\n----------------------------------------\n\nTITLE: Enabling oneAPI Runtime Environment on Windows\nDESCRIPTION: Command to enable the oneAPI runtime environment on Windows using the Intel oneAPI command prompt.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_15\n\nLANGUAGE: sh\nCODE:\n```\n\"C:\\Program Files (x86)\\Intel\\oneAPI\\setvars.bat\" intel64\n```\n\n----------------------------------------\n\nTITLE: Enabling oneAPI Runtime Environment on Windows (PowerShell)\nDESCRIPTION: PowerShell command to enable the oneAPI runtime environment on Windows.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_16\n\nLANGUAGE: powershell\nCODE:\n```\ncmd.exe \"/K\" '\"C:\\Program Files (x86)\\Intel\\oneAPI\\setvars.bat\" && powershell'\n```\n\n----------------------------------------\n\nTITLE: SYCL GPU Usage Log Example\nDESCRIPTION: Example log output showing which SYCL GPU is being used for model execution, including the device's compute units. This appears during model initialization.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/SYCL.md#2025-04-22_snippet_29\n\nLANGUAGE: sh\nCODE:\n```\nuse 1 SYCL GPUs: [0] with Max compute units:512\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake Build for LLAMA.cpp Lora Export Executable\nDESCRIPTION: Sets up CMake configuration to build the llama-export-lora executable. Links against common and llama libraries, requires C++17, and configures installation targets.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/export-lora/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET llama-export-lora)\nadd_executable(${TARGET} export-lora.cpp)\ninstall(TARGETS ${TARGET} RUNTIME)\ntarget_link_libraries(${TARGET} PRIVATE common llama ${CMAKE_THREAD_LIBS_INIT})\ntarget_compile_features(${TARGET} PRIVATE cxx_std_17)\n```\n\n----------------------------------------\n\nTITLE: Creating Directory for CI Results in MUSA Docker Setup\nDESCRIPTION: Command to create a local directory for storing CI run results when setting up MUSA CI in a Docker container.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/ci/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmkdir -p $HOME/llama.cpp/ci-results\n```\n\n----------------------------------------\n\nTITLE: Proper Pointer and Reference Syntax Examples in C++\nDESCRIPTION: Examples showing the proper syntax for pointers and references in C++ code, demonstrating the preferred style of placing the asterisk or ampersand next to the type rather than the variable name.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/CONTRIBUTING.md#2025-04-22_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\n// OK\nllama_context * ctx;\nconst llama_rope_type rope_type;\n\n// not OK\nstruct llama_context * ctx;\nconst enum llama_rope_type rope_type;\n```\n\n----------------------------------------\n\nTITLE: Help Command for Control Vector Generator\nDESCRIPTION: Command to display the help message with all available options for the cvector-generator tool.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/cvector-generator/README.md#2025-04-22_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\n./cvector-generator -h\n# Then, have a look at \"cvector\" section\n```\n\n----------------------------------------\n\nTITLE: Using Top-nÏƒ Sampling in LLaMA.cpp\nDESCRIPTION: Example command for enabling Top-nÏƒ sampling with a value of 1. This sampling method selects tokens based on a statistical threshold in pre-softmax logits, maintaining a stable sampling space regardless of temperature scaling.\nSOURCE: https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md#2025-04-22_snippet_26\n\nLANGUAGE: shell\nCODE:\n```\n--top-nsigma 1\n```"
  }
]
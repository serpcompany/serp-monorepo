[
  {
    "owner": "lm-sys",
    "repo": "routellm",
    "content": "TITLE: Initializing RouteLLM Controller in Python\nDESCRIPTION: Sets up environment variables for API keys and initializes the RouteLLM Controller with specified routers, models, and configuration. This serves as the entry point for using RouteLLM to route between strong and weak language models.\nSOURCE: https://github.com/lm-sys/routellm/blob/main/examples/python_sdk.md#2025-04-18_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom routellm.controller import Controller\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-XXXXXX\"\nos.environ[\"ANYSCALE_API_KEY\"] = \"esecret_XXXXXX\"\n\nclient = Controller(\n  # List of routers to initialize\n  routers=[\"mf\"],\n  # The pair of strong and weak models to route to\n  strong_model=\"gpt-4-1106-preview\",\n  weak_model=\"anyscale/mistralai/Mixtral-8x7B-Instruct-v0.1\",\n  # The config for the router (best-performing config by default)\n  config = {\n    \"mf\": {\n      \"checkpoint_path\": \"routellm/mf_gpt4_augmented\"\n    }\n  },\n  # Override API base and key for LLM calls\n  api_base=None,\n  api_key=None,\n  # Display a progress bar for operations\n  progress_bar=False,\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing the RouteLLM Controller with Model Routing\nDESCRIPTION: Python code to initialize the RouteLLM Controller with the MF router, setting up a strong model (GPT-4) and a weak model (Mixtral) with required API keys.\nSOURCE: https://github.com/lm-sys/routellm/blob/main/README.md#2025-04-18_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom routellm.controller import Controller\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-XXXXXX\"\n# Replace with your model provider, we use Anyscale's Mixtral here.\nos.environ[\"ANYSCALE_API_KEY\"] = \"esecret_XXXXXX\"\n\nclient = Controller(\n  routers=[\"mf\"],\n  strong_model=\"gpt-4-1106-preview\",\n  weak_model=\"anyscale/mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Chat Completions with RouteLLM in Python\nDESCRIPTION: Demonstrates how to use the RouteLLM Controller as a drop-in replacement for OpenAI's client to generate chat completions. This example routes to the appropriate model based on a specified threshold.\nSOURCE: https://github.com/lm-sys/routellm/blob/main/examples/python_sdk.md#2025-04-18_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nresponse = client.chat.completions.create(\n  # This tells RouteLLM to use the MF router with a cost threshold of 0.11593\n  model=\"router-mf-0.11593\",\n  messages=[\n    {\"role\": \"user\", \"content\": \"Hello!\"}\n  ]\n)\nprint(response.choices[0][\"message\"][\"content\"])\n```\n\n----------------------------------------\n\nTITLE: Creating LLM Completions with Router-Based Model Selection\nDESCRIPTION: Python code to generate chat completions using the RouteLLM client with a specific router and threshold, which dynamically selects between strong and weak models based on query complexity.\nSOURCE: https://github.com/lm-sys/routellm/blob/main/README.md#2025-04-18_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nresponse = client.chat.completions.create(\n  # This tells RouteLLM to use the MF router with a cost threshold of 0.11593\n  model=\"router-mf-0.11593\",\n  messages=[\n    {\"role\": \"user\", \"content\": \"Hello!\"}\n  ]\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing RouteLLM Controller with OpenAI and Ollama Models\nDESCRIPTION: Python code to set up a RouteLLM controller that routes between GPT-4 (strong model) and Llama 3 8B (weak model). This configuration uses the 'mf' (Model's Feedback) router to determine which model should handle each query.\nSOURCE: https://github.com/lm-sys/routellm/blob/main/examples/routing_to_local_models.md#2025-04-18_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nos.environ[\"OPENAI_API_KEY\"] = \"sk-XXXXXX\"\n\nclient = Controller(\n  routers=[\"mf\"],\n  strong_model=\"gpt-4-1106-preview\",\n  weak_model=\"ollama_chat/llama3\",\n)\n```\n\n----------------------------------------\n\nTITLE: Routing Prompts to Optimal Models with RouteLLM in Python\nDESCRIPTION: Shows how to use the route method to determine the best model for a given prompt based on complexity. This returns the model name without actually making the completion request.\nSOURCE: https://github.com/lm-sys/routellm/blob/main/examples/python_sdk.md#2025-04-18_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nrouted_model = client.route(\n\tprompt=\"What's the squareroot of 144?\",\n\trouter=\"mf\",\n\tthreshold=0.11593,\n)\nprint(f\"Prompt should be routed to {routed_model}\")\n```\n\n----------------------------------------\n\nTITLE: Calibrating Router Threshold for Optimal Routing\nDESCRIPTION: Command to calibrate the threshold for the MF router, targeting 50% of requests to use the strong model based on sample data.\nSOURCE: https://github.com/lm-sys/routellm/blob/main/README.md#2025-04-18_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython -m routellm.calibrate_threshold --routers mf --strong-model-pct 0.5 --config config.example.yaml\n```\n\n----------------------------------------\n\nTITLE: Calibrating Threshold with Strong Model Percentage\nDESCRIPTION: Command to calibrate the router threshold for the MF router, targeting a specific percentage of requests to use the strong model based on the Chatbot Arena dataset.\nSOURCE: https://github.com/lm-sys/routellm/blob/main/README.md#2025-04-18_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npython -m routellm.calibrate_threshold --task calibrate --routers mf --strong-model-pct 0.5 --config config.example.yaml\n```\n\n----------------------------------------\n\nTITLE: Batch Calculating Win Rates for Prompts in Python\nDESCRIPTION: Demonstrates how to calculate the win rate (probability of the strong model being more suitable) for a batch of prompts. Useful for offline evaluations with parallel processing capabilities.\nSOURCE: https://github.com/lm-sys/routellm/blob/main/examples/python_sdk.md#2025-04-18_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\nprompts = pd.Series([\"What's the squareroot of 144?\", \"Who's the last president of the US?\", \"Is the sun a star?\"])\nwin_rates = client.batch_calculate_win_rate(prompts=prompts, router=\"mf\")\n\nprint(f\"Calculated win rate for prompts:\\n{win_rates.describe()}\")\n```\n\n----------------------------------------\n\nTITLE: Launching an OpenAI-Compatible Router Server\nDESCRIPTION: Command to start an OpenAI-compatible server that routes requests between a strong model (GPT-4) and a weak model (Mixtral) using the MF router.\nSOURCE: https://github.com/lm-sys/routellm/blob/main/README.md#2025-04-18_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=sk-XXXXXX\nexport ANYSCALE_API_KEY=esecret_XXXXXX\npython -m routellm.openai_server --routers mf --strong-model gpt-4-1106-preview --weak-model anyscale/mistralai/Mixtral-8x7B-Instruct-v0.1\n```\n\n----------------------------------------\n\nTITLE: Generating Chat Completions with RouteLLM Client\nDESCRIPTION: Code snippet for creating chat completions using the RouteLLM controller. The model parameter includes a router identifier with a calibrated threshold (0.11593) that determines which ~50% of queries get routed to each model.\nSOURCE: https://github.com/lm-sys/routellm/blob/main/examples/routing_to_local_models.md#2025-04-18_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nresponse = client.chat.completions.create(\n  model=\"router-mf-0.11593\",\n  messages=[\n    {\"role\": \"user\", \"content\": \"Hello!\"}\n  ]\n)\n```\n\n----------------------------------------\n\nTITLE: Launching OpenAI-Compatible RouteLLM Server\nDESCRIPTION: Command to start a RouteLLM server that provides an OpenAI-compatible API. The server uses the 'mf' router with Llama 3 as the weak model and listens on port 6060.\nSOURCE: https://github.com/lm-sys/routellm/blob/main/examples/routing_to_local_models.md#2025-04-18_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n> export OPENAI_API_KEY=sk-...\n> python -m routellm.openai_server --routers mf --weak-model ollama_chat/llama3 --config.example.yaml\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:6060 (Press CTRL+C to quit)\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI Client to Use RouteLLM Server\nDESCRIPTION: Python code to initialize the OpenAI client to communicate with the RouteLLM server. This allows standard OpenAI API calls to benefit from intelligent routing between models without code changes.\nSOURCE: https://github.com/lm-sys/routellm/blob/main/examples/routing_to_local_models.md#2025-04-18_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport openai\n\nclient = openai.OpenAI(\n  base_url=\"https://localhost:6060/v1\",\n  api_key=\"no_api_key\"\n)\n...\nresponse = client.chat.completions.create(\n  model=\"router-mf-0.11593\",\n  messages=[\n    {\"role\": \"user\", \"content\": \"Hello!\"}\n  ]\n)\n```\n\n----------------------------------------\n\nTITLE: Integrating with Unify AI Router for LLM Selection\nDESCRIPTION: Code example showing how to use the Unify AI API with their router to select between GPT-4 Turbo and Mixtral 8x7B models. The example demonstrates setting up the OpenAI client with Unify AI's base URL and creating a chat completion request using their router configuration.\nSOURCE: https://github.com/lm-sys/routellm/blob/main/benchmarks/README.md#2025-04-18_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclient = openai.OpenAI(\n\tbase_url=\"https://api.unify.ai/v0/\",\n\tapi_key=\"UNIFY_API_KEY\"\n)\nresponse = client.chat.completions.create(\n        model=\"router@q:1|c:1.71e-03|t:1.10e-05|i:1.09e-03|models:gpt-4-turbo,mixtral-8x7b-instruct-v0.1\",\n\t\t...\n    )\n```\n\n----------------------------------------\n\nTITLE: Integrating with Martian Model Router for LLM Selection\nDESCRIPTION: Code example demonstrating how to use the Martian API for model routing between GPT-4 Turbo and Llama 2 70B Chat. The example shows how to configure the OpenAI client with Martian's endpoint and create a chat completion request with cost constraints to control the routing behavior.\nSOURCE: https://github.com/lm-sys/routellm/blob/main/benchmarks/README.md#2025-04-18_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclient = openai.OpenAI(\n    base_url=\"https://withmartian.com/api/openai/v1\",\n    api_key=\"MARTIAN_API_KEY\",\n)\nresponse = client.chat.completions.create(\n        model=\"router\",\n        extra_body={\n            \"models\": [\"gpt-4-turbo-128k\", \"llama-2-70b-chat\"],\n            \"max_cost_per_million_tokens\": 10.45,\n        },\n\t\t...\n    )\n```\n\n----------------------------------------\n\nTITLE: Evaluating Router Performance in Python\nDESCRIPTION: Command to evaluate different routing strategies on benchmarks. Supports multiple router types (random, sw_ranking, bert) and different benchmarks (mmlu, gsm8k, mt-bench).\nSOURCE: https://github.com/lm-sys/routellm/blob/main/README.md#2025-04-18_snippet_9\n\nLANGUAGE: python\nCODE:\n```\npython -m routellm.evals.evaluate --routers random sw_ranking bert --benchmark gsm8k --config config.example.yaml\n```\n\n----------------------------------------\n\nTITLE: Launching Server with Configuration File\nDESCRIPTION: Command to start the RouteLLM OpenAI-compatible server with a specific router and configuration file that defines the routing behavior.\nSOURCE: https://github.com/lm-sys/routellm/blob/main/README.md#2025-04-18_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython -m routellm.openai_server --routers mf --config config.example.yaml\n```\n\n----------------------------------------\n\nTITLE: Starting a Local Router Chatbot Demo\nDESCRIPTION: Command to launch a local chatbot interface that demonstrates how different messages are routed between models using the MF router with a specific threshold.\nSOURCE: https://github.com/lm-sys/routellm/blob/main/README.md#2025-04-18_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython -m examples.router_chat --router mf --threshold 0.11593\n```\n\n----------------------------------------\n\nTITLE: Running Llama 3 8B with Ollama\nDESCRIPTION: Command to start a local Llama 3 8B model instance using Ollama. After execution, the Ollama server will be running at http://localhost:11434/v1.\nSOURCE: https://github.com/lm-sys/routellm/blob/main/examples/routing_to_local_models.md#2025-04-18_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nollama run llama3\n```\n\n----------------------------------------\n\nTITLE: Installing RouteLLM from Source\nDESCRIPTION: Commands to clone the RouteLLM repository and install it in development mode with serving and evaluation dependencies.\nSOURCE: https://github.com/lm-sys/routellm/blob/main/README.md#2025-04-18_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/lm-sys/RouteLLM.git\ncd RouteLLM\npip install -e .[serve,eval]\n```\n\n----------------------------------------\n\nTITLE: Installing RouteLLM from PyPI\nDESCRIPTION: Command to install the RouteLLM package with serving and evaluation dependencies from PyPI.\nSOURCE: https://github.com/lm-sys/routellm/blob/main/README.md#2025-04-18_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"routellm[serve,eval]\"\n```\n\n----------------------------------------\n\nTITLE: Academic Citation in BibTeX\nDESCRIPTION: BibTeX citation format for the RouteLLM research paper, including author information, title, and arxiv details.\nSOURCE: https://github.com/lm-sys/routellm/blob/main/README.md#2025-04-18_snippet_10\n\nLANGUAGE: bibtex\nCODE:\n```\n@misc{ong2024routellmlearningroutellms,\n      title={RouteLLM: Learning to Route LLMs with Preference Data},\n      author={Isaac Ong and Amjad Almahairi and Vincent Wu and Wei-Lin Chiang and Tianhao Wu and Joseph E. Gonzalez and M Waleed Kadous and Ion Stoica},\n      year={2024},\n      eprint={2406.18665},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG},\n      url={https://arxiv.org/abs/2406.18665},\n}\n```"
  }
]
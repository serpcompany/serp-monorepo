[
  {
    "owner": "ml-explore",
    "repo": "mlx",
    "content": "TITLE: Creating and Inspecting MLX Arrays in Python\nDESCRIPTION: Demonstrates how to import the MLX core library, create arrays with different data types, and inspect their properties. Shows basic array creation with integers and floating-point values.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/quick_start.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>> import mlx.core as mx\n>> a = mx.array([1, 2, 3, 4])\n>> a.shape\n[4]\n>> a.dtype\nint32\n>> b = mx.array([1.0, 2.0, 3.0, 4.0])\n>> b.dtype\nfloat32\n```\n\n----------------------------------------\n\nTITLE: Implementing Complete MLP Training Loop with SGD\nDESCRIPTION: Sets up the model, optimizer, and training loop that iterates through epochs, computes loss and gradients, updates parameters, and evaluates test accuracy after each epoch.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/examples/mlp.rst#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Load the model\nmodel = MLP(num_layers, train_images.shape[-1], hidden_dim, num_classes)\nmx.eval(model.parameters())\n\n# Get a function which gives the loss and gradient of the\n# loss with respect to the model's trainable parameters\nloss_and_grad_fn = nn.value_and_grad(model, loss_fn)\n\n# Instantiate the optimizer\noptimizer = optim.SGD(learning_rate=learning_rate)\n\nfor e in range(num_epochs):\n    for X, y in batch_iterate(batch_size, train_images, train_labels):\n        loss, grads = loss_and_grad_fn(model, X, y)\n\n        # Update the optimizer state and model parameters\n        # in a single call\n        optimizer.update(model, grads)\n\n        # Force a graph evaluation\n        mx.eval(model.parameters(), optimizer.state)\n\n    accuracy = eval_fn(model, test_images, test_labels)\n    print(f\"Epoch {e}: Test accuracy {accuracy.item():.3f}\")\n```\n\n----------------------------------------\n\nTITLE: Working with Lazy Evaluation in MLX\nDESCRIPTION: Shows how MLX implements lazy evaluation where operations aren't computed until needed. Demonstrates forcing evaluation using mx.eval(), printing, and converting to NumPy arrays, all of which trigger evaluation.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/quick_start.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>> c = a + b    # c not yet evaluated\n>> mx.eval(c)  # evaluates c\n>> c = a + b\n>> print(c)     # Also evaluates c\narray([2, 4, 6, 8], dtype=float32)\n>> c = a + b\n>> import numpy as np\n>> np.array(c)   # Also evaluates c\narray([2., 4., 6., 8.], dtype=float32)\n```\n\n----------------------------------------\n\nTITLE: Memory-Efficient Model Initialization with Lazy Evaluation in Python\nDESCRIPTION: This example demonstrates how lazy evaluation can be used to efficiently initialize a large model with float16 weights, reducing peak memory usage.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/lazy_evaluation.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmodel = Model() # no memory used yet\nmodel.load_weights(\"weights_fp16.safetensors\")\n```\n\n----------------------------------------\n\nTITLE: Implementing MLP Neural Network with MLX\nDESCRIPTION: Example showing how to create a Multi-Layer Perceptron (MLP) neural network using MLX. Demonstrates class definition, layer initialization, and forward pass implementation.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/python/nn.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport mlx.core as mx\nimport mlx.nn as nn\n\nclass MLP(nn.Module):\n    def __init__(self, in_dims: int, out_dims: int):\n        super().__init__()\n\n        self.layers = [\n            nn.Linear(in_dims, 128),\n            nn.Linear(128, 128),\n            nn.Linear(128, out_dims),\n        ]\n\n    def __call__(self, x):\n        for i, l in enumerate(self.layers):\n            x = mx.maximum(x, 0) if i > 0 else x\n            x = l(x)\n        return x\n\n# The model is created with all its parameters but nothing is initialized\n# yet because MLX is lazily evaluated\nmlp = MLP(2, 10)\n\n# We can access its parameters by calling mlp.parameters()\nparams = mlp.parameters()\nprint(params[\"layers\"][0][\"weight\"].shape)\n\n# Printing a parameter will cause it to be evaluated and thus initialized\nprint(params[\"layers\"][0])\n```\n\n----------------------------------------\n\nTITLE: Implementing Complete Llama Model with Token Generation in Python\nDESCRIPTION: Complete implementation of the Llama model in MLX, consisting of token embedding, multiple encoder layers, and output projection. Includes a generate method that implements autoregressive token generation with caching for efficient inference.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/examples/llama-inference.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass Llama(nn.Module):\n    def __init__(\n        self, num_layers: int, vocab_size: int, dims: int, mlp_dims: int, num_heads: int\n    ):\n        super().__init__()\n\n        self.embedding = nn.Embedding(vocab_size, dims)\n        self.layers = [\n            LlamaEncoderLayer(dims, mlp_dims, num_heads) for _ in range(num_layers)\n        ]\n        self.norm = nn.RMSNorm(dims)\n        self.out_proj = nn.Linear(dims, vocab_size, bias=False)\n\n    def __call__(self, x):\n        mask = nn.MultiHeadAttention.create_additive_causal_mask(x.shape[1])\n        mask = mask.astype(self.embedding.weight.dtype)\n\n        x = self.embedding(x)\n        for l in self.layers:\n            x, _ = l(x, mask)\n        x = self.norm(x)\n        return self.out_proj(x)\n```\n\n----------------------------------------\n\nTITLE: Applying Function Transformations and Gradients in MLX\nDESCRIPTION: Demonstrates gradient transformations in MLX using the grad() function. Shows how to compute first and second derivatives of the sine function, and mentions other available transformations like vjp, jvp, and value_and_grad.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/quick_start.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>> x = mx.array(0.0)\n>> mx.sin(x)\narray(0, dtype=float32)\n>> mx.grad(mx.sin)(x)\narray(1, dtype=float32)\n>> mx.grad(mx.grad(mx.sin))(x)\narray(-0, dtype=float32)\n```\n\n----------------------------------------\n\nTITLE: Implementing LlamaAttention with RoPE Positional Encoding in Python\nDESCRIPTION: Implementation of the Llama attention mechanism using MLX, featuring RoPE positional encoding and key/value caching for efficient inference. The attention layer handles query/key/value projections and supports optional caching for autoregressive generation.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/examples/llama-inference.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport mlx.core as mx\nimport mlx.nn as nn\n\nclass LlamaAttention(nn.Module):\n    def __init__(self, dims: int, num_heads: int):\n        super().__init__()\n\n        self.num_heads = num_heads\n\n        self.rope = nn.RoPE(dims // num_heads, traditional=True)\n        self.query_proj = nn.Linear(dims, dims, bias=False)\n        self.key_proj = nn.Linear(dims, dims, bias=False)\n        self.value_proj = nn.Linear(dims, dims, bias=False)\n        self.out_proj = nn.Linear(dims, dims, bias=False)\n\n    def __call__(self, queries, keys, values, mask=None, cache=None):\n        queries = self.query_proj(queries)\n        keys = self.key_proj(keys)\n        values = self.value_proj(values)\n\n        # Extract some shapes\n        num_heads = self.num_heads\n        B, L, D = queries.shape\n\n        # Prepare the queries, keys and values for the attention computation\n        queries = queries.reshape(B, L, num_heads, -1).transpose(0, 2, 1, 3)\n        keys = keys.reshape(B, L, num_heads, -1).transpose(0, 2, 1, 3)\n        values = values.reshape(B, L, num_heads, -1).transpose(0, 2, 1, 3)\n\n        # Add RoPE to the queries and keys and combine them with the cache\n        if cache is not None:\n            key_cache, value_cache = cache\n            queries = self.rope(queries, offset=key_cache.shape[2])\n            keys = self.rope(keys, offset=key_cache.shape[2])\n            keys = mx.concatenate([key_cache, keys], axis=2)\n            values = mx.concatenate([value_cache, values], axis=2)\n        else:\n            queries = self.rope(queries)\n            keys = self.rope(keys)\n\n        # Finally perform the attention computation\n        scale = math.sqrt(1 / queries.shape[-1])\n        scores = (queries * scale) @ keys.transpose(0, 1, 3, 2)\n        if mask is not None:\n            scores = scores + mask\n        scores = mx.softmax(scores, axis=-1)\n        values_hat = (scores @ values).transpose(0, 2, 1, 3).reshape(B, L, -1)\n\n        # Note that we return the keys and values to possibly be used as a cache\n        return self.out_proj(values_hat), (keys, values)\n```\n\n----------------------------------------\n\nTITLE: Computing Gradients with Complex Functions\nDESCRIPTION: Shows how to compute gradients of a loss function with respect to different parameters using grad function.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/function_transforms.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef loss_fn(w, x, y):\n   return mx.mean(mx.square(w * x - y))\n\nw = mx.array(1.0)\nx = mx.array([0.5, -0.5])\ny = mx.array([1.5, -1.5])\n\n# Computes the gradient of loss_fn with respect to w:\ngrad_fn = mx.grad(loss_fn)\ndloss_dw = grad_fn(w, x, y)\n# Prints array(-1, dtype=float32)\nprint(dloss_dw)\n\n# To get the gradient with respect to x we can do:\ngrad_fn = mx.grad(loss_fn, argnums=1)\ndloss_dx = grad_fn(w, x, y)\n# Prints array([-1, 1], dtype=float32)\nprint(dloss_dx)\n```\n\n----------------------------------------\n\nTITLE: Using value_and_grad for Efficient Computation\nDESCRIPTION: Demonstrates using value_and_grad to compute both loss and gradient efficiently in a single pass.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/function_transforms.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Computes the gradient of loss_fn with respect to w:\nloss_and_grad_fn = mx.value_and_grad(loss_fn)\nloss, dloss_dw = loss_and_grad_fn(w, x, y)\n\n# Prints array(1, dtype=float32)\nprint(loss)\n\n# Prints array(-1, dtype=float32)\nprint(dloss_dw)\n```\n\n----------------------------------------\n\nTITLE: Basic Compilation Example in MLX\nDESCRIPTION: Demonstrates the basic usage of mx.compile by defining a simple function, showing both the regular and compiled function calls produce the same output.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/compile.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef fun(x, y):\n    return mx.exp(-x) + y\n\nx = mx.array(1.0)\ny = mx.array(2.0)\n\n# Regular call, no compilation\n# Prints: array(2.36788, dtype=float32)\nprint(fun(x, y))\n\n# Compile the function\ncompiled_fun = mx.compile(fun)\n\n# Prints: array(2.36788, dtype=float32)\nprint(compiled_fun(x, y))\n```\n\n----------------------------------------\n\nTITLE: Implementing LlamaEncoderLayer with RMSNorm and SwiGLU in Python\nDESCRIPTION: Implementation of the Llama encoder layer which combines attention with feed-forward networks. It uses RMSNorm for normalization and incorporates SwiGLU activation for the feed-forward network. The layer maintains a cache for efficient inference.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/examples/llama-inference.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass LlamaEncoderLayer(nn.Module):\n    def __init__(self, dims: int, mlp_dims: int, num_heads: int):\n        super().__init__()\n\n        self.attention = LlamaAttention(dims, num_heads)\n\n        self.norm1 = nn.RMSNorm(dims)\n        self.norm2 = nn.RMSNorm(dims)\n\n        self.linear1 = nn.Linear(dims, mlp_dims, bias=False)\n        self.linear2 = nn.Linear(dims, mlp_dims, bias=False)\n        self.linear3 = nn.Linear(mlp_dims, dims, bias=False)\n\n    def __call__(self, x, mask=None, cache=None):\n        y = self.norm1(x)\n        y, cache = self.attention(y, y, y, mask, cache)\n        x = x + y\n\n        y = self.norm2(x)\n        a = self.linear1(y)\n        b = self.linear2(y)\n        y = a * mx.sigmoid(a) * b\n        y = self.linear3(y)\n        x = x + y\n\n        return x, cache\n```\n\n----------------------------------------\n\nTITLE: Setting up MNIST Data and Model Parameters\nDESCRIPTION: Configures model hyperparameters and loads the MNIST dataset using a custom loader, converting the data to MLX arrays.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/examples/mlp.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nnum_layers = 2\nhidden_dim = 32\nnum_classes = 10\nbatch_size = 256\nnum_epochs = 10\nlearning_rate = 1e-1\n\n# Load the data\nimport mnist \ntrain_images, train_labels, test_images, test_labels = map(\n    mx.array, mnist.mnist()\n)\n```\n\n----------------------------------------\n\nTITLE: Using Optimizers with MLX Models in Training Loop\nDESCRIPTION: Example showing how to create a model, define a loss function with gradients, initialize an SGD optimizer, and update the model parameters during a training loop while properly evaluating the optimizer state.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/python/optimizers.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Create a model\nmodel = MLP(num_layers, train_images.shape[-1], hidden_dim, num_classes)\nmx.eval(model.parameters())\n\n# Create the gradient function and the optimizer\nloss_and_grad_fn = nn.value_and_grad(model, loss_fn)\noptimizer = optim.SGD(learning_rate=learning_rate)\n\nfor e in range(num_epochs):\n    for X, y in batch_iterate(batch_size, train_images, train_labels):\n        loss, grads = loss_and_grad_fn(model, X, y)\n\n        # Update the model with the gradients. So far no computation has happened.\n        optimizer.update(model, grads)\n\n        # Compute the new parameters but also the optimizer state.\n        mx.eval(model.parameters(), optimizer.state)\n```\n\n----------------------------------------\n\nTITLE: Implementing MLP Model Class in MLX\nDESCRIPTION: Defines an MLP (Multi-Layer Perceptron) class that inherits from nn.Module, with configurable number of layers, input dimension, hidden dimension, and output dimension. The model implements feed-forward layers with ReLU activation.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/examples/mlp.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass MLP(nn.Module):\n    def __init__(\n        self, num_layers: int, input_dim: int, hidden_dim: int, output_dim: int\n    ):\n        super().__init__()\n        layer_sizes = [input_dim] + [hidden_dim] * num_layers + [output_dim]\n        self.layers = [\n            nn.Linear(idim, odim)\n            for idim, odim in zip(layer_sizes[:-1], layer_sizes[1:])\n        ]\n\n    def __call__(self, x):\n        for l in self.layers[:-1]:\n            x = mx.maximum(l(x), 0.0)\n        return self.layers[-1](x)\n```\n\n----------------------------------------\n\nTITLE: Performance Timing Utility for MLX\nDESCRIPTION: A helper function for benchmarking performance, which performs warm-up iterations and handles synchronization for accurate timing.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/compile.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\ndef timeit(fun, x):\n    # warm up\n    for _ in range(10):\n        mx.eval(fun(x))\n\n    tic = time.perf_counter()\n    for _ in range(100):\n        mx.eval(fun(x))\n    toc = time.perf_counter()\n    tpi = 1e3 * (toc - tic) / 100\n    print(f\"Time per iteration {tpi:.3f} (ms)\")\n```\n\n----------------------------------------\n\nTITLE: Converting Llama Weights from PyTorch to MLX in Python\nDESCRIPTION: This function maps PyTorch weight keys to their MLX equivalents, handling various layer types and renaming conventions. It's used to convert the model weights for compatibility with the MLX implementation.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/examples/llama-inference.rst#2025-04-19_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\ndef map_torch_to_mlx(key, value):\n    key = key.replace(\"attention_norm\", \"norm1\").replace(\"ffn_norm\", \"norm2\")\n\n    elif \"wq\" in key or \"wk\" in key or \"wv\" in key or \"wo\" in key:\n        key = key.replace(\"wq\", \"query_proj\")\n        key = key.replace(\"wk\", \"key_proj\")\n        key = key.replace(\"wv\", \"value_proj\")\n        key = key.replace(\"wo\", \"out_proj\")\n\n    elif \"w1\" in key or \"w2\" in key or \"w3\" in key:\n        # The FFN is a separate submodule in PyTorch\n        key = key.replace(\"feed_forward.w1\", \"linear1\")\n        key = key.replace(\"feed_forward.w3\", \"linear2\")\n        key = key.replace(\"feed_forward.w2\", \"linear3\")\n\n    elif \"output\" in key:\n        key = key.replace(\"output\", \"out_proj\")\n\n    elif \"rope\" in key:\n        return None, None\n\n    return key, value.numpy()\n```\n\n----------------------------------------\n\nTITLE: Compiling Transformed Functions in MLX\nDESCRIPTION: Demonstrates how to apply a gradient transformation to a function and compile it. Shows that both the transformed function and its compiled version produce the same result.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/compile.rst#2025-04-19_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ngrad_fn = mx.grad(mx.exp)\n\ncompiled_grad_fn = mx.compile(grad_fn)\n\n# Prints: array(2.71828, dtype=float32)\nprint(grad_fn(mx.array(1.0)))\n\n# Also prints: array(2.71828, dtype=float32)\nprint(compiled_grad_fn(mx.array(1.0)))\n```\n\n----------------------------------------\n\nTITLE: Exporting a Module without Parameters\nDESCRIPTION: Shows how to export a neural network module's structure without including its parameters, by passing the parameters as inputs to the exported function.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/export.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmodel = nn.Linear(4, 4)\nmx.eval(model.parameters())\n\ndef call(x, **params):\n  # Set the model's parameters to the input parameters\n  model.update(tree_unflatten(list(params.items())))\n  return model(x)\n\nparams = dict(tree_flatten(model.parameters()))\nmx.export_function(\"model.mlxfn\", call, (mx.zeros(4),), params)\n```\n\n----------------------------------------\n\nTITLE: MLX Neural Network Parameter Updates and Gradient Computation\nDESCRIPTION: Example showing how to update parameters and compute gradients in MLX neural networks using value_and_grad transformation.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/python/nn.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmodel = ...\n\ndef f(params, other_inputs):\n    model.update(params)  # <---- Necessary to make the model use the passed parameters\n    return model(other_inputs)\n\nf(model.trainable_parameters(), mx.zeros((10,)))\n```\n\n----------------------------------------\n\nTITLE: Automatic Vectorization with vmap\nDESCRIPTION: Demonstrates using vmap for automatic vectorization of operations, comparing naive and vectorized implementations.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/function_transforms.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nxs = mx.random.uniform(shape=(4096, 100))\nys = mx.random.uniform(shape=(100, 4096))\n\ndef naive_add(xs, ys):\n    return [xs[i] + ys[:, i] for i in range(xs.shape[0])]\n```\n\nLANGUAGE: python\nCODE:\n```\n# Vectorize over the second dimension of x and the\n# first dimension of y\nvmap_add = mx.vmap(lambda x, y: x + y, in_axes=(0, 1))\n```\n\nLANGUAGE: python\nCODE:\n```\nimport timeit\n\nprint(timeit.timeit(lambda: mx.eval(naive_add(xs, ys)), number=100))\nprint(timeit.timeit(lambda: mx.eval(vmap_add(xs, ys)), number=100))\n```\n\n----------------------------------------\n\nTITLE: Applying Transformations to Imported Functions\nDESCRIPTION: Demonstrates how imported functions can be transformed using MLX function transformations like gradient calculation, vectorization, and compilation, just like regular Python functions.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/export.rst#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef fun(x):\n    return mx.sin(x)\n\nx = mx.array(0.0)\nmx.export_function(\"sine.mlxfn\", fun, x)\n\nimported_fun = mx.import_function(\"sine.mlxfn\")\n\n# Take the derivative of the imported function\ndfdx = mx.grad(lambda x: imported_fun(x)[0])\n# Prints: array(1, dtype=float32)\nprint(dfdx(x))\n\n# Compile the imported function \nmx.compile(imported_fun)\n# Prints: array(0, dtype=float32)\nprint(compiled_fun(x)[0])\n```\n\n----------------------------------------\n\nTITLE: Exporting Multiple Traces to a Single File\nDESCRIPTION: Shows how to export multiple execution paths of the same function to a single file using the exporter context manager, which helps avoid duplicate constant data.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/export.rst#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef fun(x, y=None):\n    constant = mx.array(3.0)\n    if y is not None:\n      x += y \n    return x + constant\n\nwith mx.exporter(\"fun.mlxfn\", fun) as exporter:\n    exporter(mx.array(1.0))\n    exporter(mx.array(1.0), y=mx.array(0.0))\n\nimported_function = mx.import_function(\"fun.mlxfn\")\n\n# Call the function with y=None\nout, = imported_function(mx.array(1.0))\nprint(out)\n\n# Call the function with y specified\nout, = imported_function(mx.array(1.0), y=mx.array(1.0))\nprint(out)\n```\n\n----------------------------------------\n\nTITLE: Defining Loss Functions Reference in RST\nDESCRIPTION: ReStructuredText markup defining a reference section for MLX neural network loss functions. The document uses autosummary to generate documentation for various loss functions including cross entropy, MSE, L1, and specialized losses like triplet and margin ranking.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/python/nn/losses.rst#2025-04-19_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _losses:\n\n.. currentmodule:: mlx.nn.losses\n\nLoss Functions\n--------------\n\n.. autosummary::\n   :toctree: _autosummary_functions\n   :template: nn-module-template.rst\n\n   binary_cross_entropy\n   cosine_similarity_loss\n   cross_entropy\n   gaussian_nll_loss\n   hinge_loss\n   huber_loss\n   kl_div_loss\n   l1_loss\n   log_cosh_loss\n   margin_ranking_loss\n   mse_loss\n   nll_loss\n   smooth_l1_loss\n   triplet_loss\n```\n\n----------------------------------------\n\nTITLE: Cautionary Example of Implicit Evaluation in Control Flow with MLX in Python\nDESCRIPTION: This snippet demonstrates how using scalar arrays for control flow can cause unintended evaluations, which may be inefficient if done frequently.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/lazy_evaluation.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef fun(x):\n    h, y = first_layer(x)\n    if y > 0:  # An evaluation is done here!\n        z  = second_layer_a(h)\n    else:\n        z  = second_layer_b(h)\n    return z\n```\n\n----------------------------------------\n\nTITLE: GELU Performance Benchmark\nDESCRIPTION: Creates a random array and compares the performance of regular and compiled GELU functions, demonstrating the performance gain from compilation.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/compile.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nx = mx.random.uniform(shape=(32, 1000, 4096))\ntimeit(nn.gelu, x)\ntimeit(mx.compile(nn.gelu), x)\n```\n\n----------------------------------------\n\nTITLE: Converting Between MLX and PyTorch\nDESCRIPTION: Example of converting MLX arrays to PyTorch tensors and back. PyTorch requires explicit use of memoryview for the buffer protocol, and conversion back to MLX requires an intermediate NumPy array.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/numpy.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport mlx.core as mx\nimport torch\n\na = mx.arange(3)\nb = torch.tensor(memoryview(a))\nc = mx.array(b.numpy())\n```\n\n----------------------------------------\n\nTITLE: MLX Distributed Communication Functions\nDESCRIPTION: A list of core distributed communication functions available in the MLX package, including initialization, group management, collective operations (all_sum, all_gather), and point-to-point communication (send, recv).\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/python/distributed.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nGroup\nis_available\ninit\nall_sum\nall_gather\nsend\nrecv\nrecv_like\n```\n\n----------------------------------------\n\nTITLE: Basic Indexing with Integers and Slices in MLX\nDESCRIPTION: Demonstrates basic array indexing in MLX using integers and slice syntax to access elements and subarrays. Shows single element access, negative indexing, and slicing with start, stop, and stride parameters.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/indexing.rst#2025-04-19_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n>>> arr = mx.arange(10)\n>>> arr[3]\narray(3, dtype=int32)\n>>> arr[-2]  # negative indexing works\narray(8, dtype=int32)\n>>> arr[2:8:2] # start, stop, stride\narray([2, 4, 6], dtype=int32)\n```\n\n----------------------------------------\n\nTITLE: Implementing Token Generation for Llama Model in Python\nDESCRIPTION: Extension of the Llama model class with a generate method that produces tokens autoregressively. The implementation leverages MLX's lazy evaluation and caching mechanisms to efficiently generate tokens one at a time.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/examples/llama-inference.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef generate(self, x, temp=1.0):\n    cache = []\n\n    # Make an additive causal mask. We will need that to process the prompt.\n    mask = nn.MultiHeadAttention.create_additive_causal_mask(x.shape[1])\n    mask = mask.astype(self.embedding.weight.dtype)\n\n    # First we process the prompt x the same way as in __call__ but\n    # save the caches in cache\n    x = self.embedding(x)\n    for l in self.layers:\n        x, c = l(x, mask=mask)\n        cache.append(c)  # <--- we store the per layer cache in a\n                         #      simple python list\n    x = self.norm(x)\n    y = self.out_proj(x[:, -1])  # <--- we only care about the last logits\n                                 #      that generate the next token\n    y = mx.random.categorical(y * (1/temp))\n\n    # y now has size [1]\n    # Since MLX is lazily evaluated nothing is computed yet.\n    # Calling y.item() would force the computation to happen at\n    # this point but we can also choose not to do that and let the\n    # user choose when to start the computation.\n    yield y\n\n    # Now we parsed the prompt and generated the first token we\n    # need to feed it back into the model and loop to generate the\n    # rest.\n    while True:\n        # Unsqueezing the last dimension to add a sequence length\n        # dimension of 1\n        x = y[:, None]\n\n        x = self.embedding(x)\n        for i in range(len(cache)):\n            # We are overwriting the arrays in the cache list. When\n            # the computation will happen, MLX will be discarding the\n            # old cache the moment it is not needed anymore.\n            x, cache[i] = self.layers[i](x, mask=None, cache=cache[i])\n        x = self.norm(x)\n        y = self.out_proj(x[:, -1])\n        y = mx.random.categorical(y * (1/temp))\n\n        yield y\n```\n\n----------------------------------------\n\nTITLE: Converting Between MLX and JAX\nDESCRIPTION: Example of converting MLX arrays to JAX arrays and back. JAX fully supports the buffer protocol, making conversion straightforward in both directions.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/numpy.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport mlx.core as mx\nimport jax.numpy as jnp\n\na = mx.arange(3)\nb = jnp.array(a)\nc = mx.array(b)\n```\n\n----------------------------------------\n\nTITLE: Running Llama Model Inference Example in Python\nDESCRIPTION: Example code showing how to instantiate a Llama model, materialize its parameters, and generate tokens from a prompt. Demonstrates MLX's lazy evaluation capabilities where computation only happens when explicitly requested.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/examples/llama-inference.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmodel = Llama(num_layers=12, vocab_size=8192, dims=512, mlp_dims=1024, num_heads=8)\n\n# Since MLX is lazily evaluated nothing has actually been materialized yet.\n# We could have set the `dims` to 20_000 on a machine with 8GB of RAM and the\n# code above would still run. Let's actually materialize the model.\nmx.eval(model.parameters())\n\nprompt = mx.array([[1, 10, 8, 32, 44, 7]])  # <-- Note the double brackets because we\n                                            #     have a batch dimension even\n                                            #     though it is 1 in this case\n\ngenerated = [t for i, t in zip(range(10), model.generate(prompt, 0.8))]\n\n# Since we haven't evaluated anything, nothing is computed yet. The list\n# `generated` contains the arrays that hold the computation graph for the\n# full processing of the prompt and the generation of 10 tokens.\n#\n# We can evaluate them one at a time, or all together. Concatenate them or\n# print them. They would all result in very similar runtimes and give exactly\n# the same results.\nmx.eval(generated)\n```\n\n----------------------------------------\n\nTITLE: Array-based Indexing in MLX\nDESCRIPTION: Shows how to use an MLX array as an index to select multiple elements from another array, which is useful for gathering non-contiguous elements.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/indexing.rst#2025-04-19_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n>>> arr = mx.arange(10)\n>>> idx = mx.array([5, 7])\n>>> arr[idx]\narray([5, 7], dtype=int32)\n```\n\n----------------------------------------\n\nTITLE: Complex Computation Utilizing CPU and GPU in MLX\nDESCRIPTION: Defines a function that performs matrix multiplication on one device and element-wise operations on another, showcasing the flexibility of unified memory.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/unified_memory.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef fun(a, b, d1, d2):\n  x = mx.matmul(a, b, stream=d1)\n  for _ in range(500):\n      b = mx.exp(b, stream=d2)\n  return x, b\n```\n\n----------------------------------------\n\nTITLE: Implementing Grid Sample VJP with Metal Kernel in Python\nDESCRIPTION: Custom vector-Jacobian product implementation for grid_sample function using mx.fast.metal_kernel with atomic updates. The implementation handles gradient computation for both input tensor and grid, using Metal's atomic features and SIMD operations for efficient parallel computation.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/dev/custom_metal_kernels.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@grid_sample.vjp\ndef grid_sample_vjp(primals, cotangent, _):\n    x, grid = primals\n    B, _, _, C = x.shape\n    _, gN, gM, D = grid.shape\n\n    assert D == 2, \"Last dim of `grid` must be size 2.\"\n\n    source = \"\"\"\n        uint elem = thread_position_in_grid.x;\n        int H = x_shape[1];\n        int W = x_shape[2];\n        int C = x_shape[3];\n        // Pad C to the nearest larger simdgroup size multiple\n        int C_padded = ceildiv(C, threads_per_simdgroup) * threads_per_simdgroup;\n\n        int gH = grid_shape[1];\n        int gW = grid_shape[2];\n\n        int w_stride = C;\n        int h_stride = W * w_stride;\n        int b_stride = H * h_stride;\n\n        uint grid_idx = elem / C_padded * 2;\n        float ix = ((grid[grid_idx] + 1) * W - 1) / 2;\n        float iy = ((grid[grid_idx + 1] + 1) * H - 1) / 2;\n\n        int ix_nw = floor(ix);\n        int iy_nw = floor(iy);\n\n        int ix_ne = ix_nw + 1;\n        int iy_ne = iy_nw;\n\n        int ix_sw = ix_nw;\n        int iy_sw = iy_nw + 1;\n\n        int ix_se = ix_nw + 1;\n        int iy_se = iy_nw + 1;\n\n        T nw = (ix_se - ix)    * (iy_se - iy);\n        T ne = (ix    - ix_sw) * (iy_sw - iy);\n        T sw = (ix_ne - ix)    * (iy    - iy_ne);\n        T se = (ix    - ix_nw) * (iy    - iy_nw);\n\n        int batch_idx = elem / C_padded / gH / gW * b_stride;\n        int channel_idx = elem % C_padded;\n        int base_idx = batch_idx + channel_idx;\n\n        T gix = T(0);\n        T giy = T(0);\n        if (channel_idx < C) {\n            int cot_index = elem / C_padded * C + channel_idx;\n            T cot = cotangent[cot_index];\n            if (iy_nw >= 0 && iy_nw <= H - 1 && ix_nw >= 0 && ix_nw <= W - 1) {\n                int offset = base_idx + iy_nw * h_stride + ix_nw * w_stride;\n                atomic_fetch_add_explicit(&x_grad[offset], nw * cot, memory_order_relaxed);\n\n                T I_nw = x[offset];\n                gix -= I_nw * (iy_se - iy) * cot;\n                giy -= I_nw * (ix_se - ix) * cot;\n            }\n            if (iy_ne >= 0 && iy_ne <= H - 1 && ix_ne >= 0 && ix_ne <= W - 1) {\n                int offset = base_idx + iy_ne * h_stride + ix_ne * w_stride;\n                atomic_fetch_add_explicit(&x_grad[offset], ne * cot, memory_order_relaxed);\n\n                T I_ne = x[offset];\n                gix += I_ne * (iy_sw - iy) * cot;\n                giy -= I_ne * (ix - ix_sw) * cot;\n            }\n            if (iy_sw >= 0 && iy_sw <= H - 1 && ix_sw >= 0 && ix_sw <= W - 1) {\n                int offset = base_idx + iy_sw * h_stride + ix_sw * w_stride;\n                atomic_fetch_add_explicit(&x_grad[offset], sw * cot, memory_order_relaxed);\n\n                T I_sw = x[offset];\n                gix -= I_sw * (iy - iy_ne) * cot;\n                giy += I_sw * (ix_ne - ix) * cot;\n            }\n            if (iy_se >= 0 && iy_se <= H - 1 && ix_se >= 0 && ix_se <= W - 1) {\n                int offset = base_idx + iy_se * h_stride + ix_se * w_stride;\n                atomic_fetch_add_explicit(&x_grad[offset], se * cot, memory_order_relaxed);\n\n                T I_se = x[offset];\n                gix += I_se * (iy - iy_nw) * cot;\n                giy += I_se * (ix - ix_nw) * cot;\n            }\n        }\n\n        T gix_mult = W / 2;\n        T giy_mult = H / 2;\n\n        // Reduce across each simdgroup first.\n        // This is much faster than relying purely on atomics.\n        gix = simd_sum(gix);\n        giy = simd_sum(giy);\n\n        if (thread_index_in_simdgroup == 0) {\n            atomic_fetch_add_explicit(&grid_grad[grid_idx], gix * gix_mult, memory_order_relaxed);\n            atomic_fetch_add_explicit(&grid_grad[grid_idx + 1], giy * giy_mult, memory_order_relaxed);\n        }\n    \"\"\"\n    kernel = mx.fast.metal_kernel(\n        name=\"grid_sample_grad\",\n        input_names=[\"x\", \"grid\", \"cotangent\"],\n        output_names=[\"x_grad\", \"grid_grad\"],\n        source=source,\n        atomic_outputs=True,\n    )\n    # pad the output channels to simd group size\n    # so that our `simd_sum`s don't overlap.\n    simdgroup_size = 32\n    C_padded = (C + simdgroup_size - 1) // simdgroup_size * simdgroup_size\n    grid_size = B * gN * gM * C_padded\n    outputs = kernel(\n        inputs=[x, grid, cotangent],\n        template=[(\"T\", x.dtype)],\n        output_shapes=[x.shape, grid.shape],\n        output_dtypes=[x.dtype, x.dtype],\n        grid=(grid_size, 1, 1),\n        threadgroup=(256, 1, 1),\n        init_value=0,\n    )\n    return outputs[0], outputs[1]\n```\n\n----------------------------------------\n\nTITLE: Implementing Batch Iteration for Training\nDESCRIPTION: Creates a batch iterator function that shuffles the training data and yields mini-batches of images and labels for stochastic gradient descent.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/examples/mlp.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef batch_iterate(batch_size, X, y):\n    perm = mx.array(np.random.permutation(y.size))\n    for s in range(0, y.size, batch_size):\n        ids = perm[s : s + batch_size]\n        yield X[ids], y[ids]\n```\n\n----------------------------------------\n\nTITLE: Implementing SGD Training Loop\nDESCRIPTION: Executes the stochastic gradient descent optimization loop by initializing weights and iteratively updating them using computed gradients.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/examples/linear_regression.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nw = 1e-2 * mx.random.normal((num_features,))\n\nfor _ in range(num_iters):\n    grad = grad_fn(w)\n    w = w - lr * grad\n    mx.eval(w)\n```\n\n----------------------------------------\n\nTITLE: Loading Single Array in MLX\nDESCRIPTION: Example showing how to load a single array from a .npy file using mx.load(). Returns the array directly.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/saving_and_loading.rst#2025-04-19_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n>>> mx.load(\"array.npy\")\narray([1], dtype=float32)\n```\n\n----------------------------------------\n\nTITLE: Shapeless Compilation for Variable Input Shapes in MLX\nDESCRIPTION: Shows how to use shapeless compilation to handle inputs with varying shapes without recompilation. The example demonstrates a function that works with both scalar and vector inputs.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/compile.rst#2025-04-19_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndef fun(x, y):\n    return mx.abs(x + y)\n\ncompiled_fun = mx.compile(fun, shapeless=True)\n\nx = mx.array(1.0)\ny = mx.array(-2.0)\n\n# Firt call compiles the function\nprint(compiled_fun(x, y))\n\n# Second call with different shapes\n# does not recompile the function\nx = mx.array([1.0, -6.0])\ny = mx.array([-2.0, 3.0])\nprint(compiled_fun(x, y))\n```\n\n----------------------------------------\n\nTITLE: Importing and Using an MLX Function in C++\nDESCRIPTION: Demonstrates how to import a function exported from Python in a C++ application and call it with the appropriate inputs.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/export.rst#2025-04-19_snippet_10\n\nLANGUAGE: c++\nCODE:\n```\nauto fun = mx::import_function(\"fun.mlxfn\");\n\nauto inputs = {mx::array(1.0), mx::array(1.0)};\nauto outputs = fun(inputs);\n\n// Prints: array(2, dtype=float32)\nstd::cout << outputs[0] << std::endl;\n```\n\n----------------------------------------\n\nTITLE: Multi-dimensional Array Indexing with Ellipsis in MLX\nDESCRIPTION: Shows how to use the Ellipsis syntax (...) for indexing multi-dimensional arrays in MLX, which provides a shorthand for selecting elements across multiple dimensions.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/indexing.rst#2025-04-19_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n>>> arr = mx.arange(8).reshape(2, 2, 2)\n>>> arr[:, :, 0]\narray(3, dtype=int32)\narray([[0, 2],\n       [4, 6]], dtype=int32\n>>> arr[..., 0]\narray([[0, 2],\n       [4, 6]], dtype=int32\n```\n\n----------------------------------------\n\nTITLE: Performing Operations on Different Devices in MLX\nDESCRIPTION: Shows how to perform the same operation (addition) on both CPU and GPU streams without moving data, leveraging unified memory.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/unified_memory.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmx.add(a, b, stream=mx.cpu)\nmx.add(a, b, stream=mx.gpu)\n```\n\n----------------------------------------\n\nTITLE: Basic Parameter Initialization in MLX using Uniform Distribution\nDESCRIPTION: Demonstrates how to create and apply a uniform distribution initializer to a zero array using MLX. Shows the basic usage pattern of creating an initializer function and applying it to parameters.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/python/nn/init.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport mlx.core as mx\nimport mlx.nn as nn\n\ninit_fn = nn.init.uniform()\n\n# Produces a [2, 2] uniform matrix\nparam = init_fn(mx.zeros((2, 2)))\n```\n\n----------------------------------------\n\nTITLE: Gradient Tracking Issues with External Memory Modification\nDESCRIPTION: Example demonstrating how external modifications to MLX array memory through NumPy views aren't tracked by MLX's automatic differentiation. The gradient doesn't account for operations performed through the NumPy view.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/numpy.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef f(x):\n    x_view = np.array(x, copy=False)\n    x_view[:] *= x_view # modify memory without telling mx\n    return x.sum()\n\nx = mx.array([3.0])\ny, df = mx.value_and_grad(f)(x)\nprint(\"f(x) = x² =\", y.item()) # 9.0\nprint(\"f'(x) = 2x !=\", df.item()) # 1.0\n```\n\n----------------------------------------\n\nTITLE: Implementing Elementwise Exponential Function with Custom Metal Kernel in Python\nDESCRIPTION: Defines a custom Metal kernel to compute elementwise exponential. It demonstrates how to create a kernel, specify inputs/outputs, and execute it with MLX's metal_kernel function.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/dev/custom_metal_kernels.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef exp_elementwise(a: mx.array):\n    source = \"\"\"\n        uint elem = thread_position_in_grid.x;\n        T tmp = inp[elem];\n        out[elem] = metal::exp(tmp);\n    \"\"\"\n\n    kernel = mx.fast.metal_kernel(\n        name=\"myexp\",\n        input_names=[\"inp\"],\n        output_names=[\"out\"],\n        source=source,\n    )\n    outputs = kernel(\n        inputs=[a],\n        template=[(\"T\", mx.float32)],\n        grid=(a.size, 1, 1),\n        threadgroup=(256, 1, 1),\n        output_shapes=[a.shape],\n        output_dtypes=[a.dtype],\n    )\n    return outputs[0]\n\na = mx.random.normal(shape=(4, 16)).astype(mx.float16)\nb = exp_elementwise(a)\nassert mx.allclose(b, mx.exp(a))\n```\n\n----------------------------------------\n\nTITLE: Capturing Implicit Outputs in Compiled Functions\nDESCRIPTION: Shows how to use the outputs parameter in mx.compile to capture side effects on state variables.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/compile.rst#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom functools import partial\n\nstate = []\n\n# Tell compile to capture state as an output\n@partial(mx.compile, outputs=state)\ndef fun(x, y):\n    z = x + y\n    state.append(z)\n    return mx.exp(z), state\n\nfun(mx.array(1.0), mx.array(2.0))\n# Prints [array(3, dtype=float32)]\nprint(state)\n```\n\n----------------------------------------\n\nTITLE: Saving Single Array in MLX\nDESCRIPTION: Example demonstrating how to save a single array to a .npy file using mx.save(). The extension is automatically added if not provided.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/saving_and_loading.rst#2025-04-19_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n>>> a = mx.array([1.0])\n>>> mx.save(\"array\", a)\n```\n\n----------------------------------------\n\nTITLE: Compiled Neural Network Training with State Capturing\nDESCRIPTION: The same training example but with compilation, demonstrating how to properly capture and update model and optimizer state.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/compile.rst#2025-04-19_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport mlx.core as mx\nimport mlx.nn as nn\nimport mlx.optimizers as optim\nfrom functools import partial\n\n# 4 examples with 10 features each\nx = mx.random.uniform(shape=(4, 10))\n\n# 0, 1 targets\ny = mx.array([0, 1, 0, 1])\n\n# Simple linear model\nmodel = nn.Linear(10, 1)\n\n# SGD with momentum\noptimizer = optim.SGD(learning_rate=0.1, momentum=0.8)\n\ndef loss_fn(model, x, y):\n    logits = model(x).squeeze()\n    return nn.losses.binary_cross_entropy(logits, y)\n\n# The state that will be captured as input and output\nstate = [model.state, optimizer.state]\n\n@partial(mx.compile, inputs=state, outputs=state)\ndef step(x, y):\n    loss_and_grad_fn = nn.value_and_grad(model, loss_fn)\n    loss, grads = loss_and_grad_fn(model, x, y)\n    optimizer.update(model, grads)\n    return loss\n\n# Perform 10 steps of gradient descent\nfor it in range(10):\n    loss = step(x, y)\n    # Evaluate the model and optimizer state\n    mx.eval(state)\n    print(loss)\n```\n\n----------------------------------------\n\nTITLE: Generating Random Numbers with Implicit Global PRNG State in MLX\nDESCRIPTION: Demonstrates how to generate random numbers using MLX's default implicit global PRNG state. Each call to mx.random.uniform() produces a unique pseudo-random number.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/python/random.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfor _ in range(3):\n  print(mx.random.uniform())\n```\n\n----------------------------------------\n\nTITLE: Defining Loss Function and Gradient\nDESCRIPTION: Implements the squared loss function for linear regression and creates its gradient function using MLX's automatic differentiation.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/examples/linear_regression.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef loss_fn(w):\n    return 0.5 * mx.mean(mx.square(X @ w - y))\n\ngrad_fn = mx.grad(loss_fn)\n```\n\n----------------------------------------\n\nTITLE: Exporting Shapeless Functions for Dynamic Input Shapes\nDESCRIPTION: Illustrates how to export a function that can handle inputs with variable shapes using the shapeless parameter.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/export.rst#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmx.export_function(\"fun.mlxfn\", mx.abs, mx.array(0.0), shapeless=True)\nimported_abs = mx.import_function(\"fun.mlxfn\")\n\n# Ok\nout, = imported_abs(mx.array(-1.0))\n\n# Also ok \nout, = imported_abs(mx.array([-1.0, -2.0]))\n```\n\n----------------------------------------\n\nTITLE: Initializing Arrays for Unified Memory Example in MLX\nDESCRIPTION: Creates random uniform arrays to be used in the complex computation example, demonstrating input preparation for unified memory operations.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/unified_memory.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\na = mx.random.uniform(shape=(4096, 512))\nb = mx.random.uniform(shape=(512, 4))\n```\n\n----------------------------------------\n\nTITLE: Caching Behavior in MLX Compilation\nDESCRIPTION: Shows how the MLX compiler caches compiled functions to avoid recompilation when the same function is called multiple times.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/compile.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef fun(x, y):\n    return mx.exp(-x) + y\n\nx = mx.array(1.0)\ny = mx.array(2.0)\n\ncompiled_fun = mx.compile(fun)\n\n# Compiled here\ncompiled_fun(x, y)\n\n# Not compiled again\ncompiled_fun(x, y)\n\n# Not compiled again\nmx.compile(fun)(x, y)\n```\n\n----------------------------------------\n\nTITLE: MLX Neural Network Loss Function and Gradient Computation\nDESCRIPTION: Implementation of loss function and gradient computation using MLX's neural network module. Shows how to define an L2 loss function and compute gradients with respect to model parameters.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/python/nn.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# A simple loss function.\n# NOTE: It doesn't matter how it uses the mlp model. It currently captures\n#       it from the local scope. It could be a positional argument or a\n#       keyword argument.\ndef l2_loss(x, y):\n    y_hat = mlp(x)\n    return (y_hat - y).square().mean()\n\n# Calling `nn.value_and_grad` instead of `mx.value_and_grad` returns the\n# gradient with respect to `mlp.trainable_parameters()`\nloss_and_grad = nn.value_and_grad(mlp, l2_loss)\n```\n\n----------------------------------------\n\nTITLE: Generating Random Numbers with Explicit PRNG Key in MLX\nDESCRIPTION: Shows how to generate random numbers using an explicit PRNG key in MLX. Using the same key for each call results in identical pseudo-random numbers being produced each time.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/python/random.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nkey = mx.random.key(0)\nfor _ in range(3):\n  print(mx.random.uniform(key=key))\n```\n\n----------------------------------------\n\nTITLE: Implementing a Simple AXPBY Function in Python with MLX\nDESCRIPTION: A simple implementation of the axpby operation (z = alpha*x + beta*y) using existing MLX operations. This function takes two arrays and scales them by coefficients before adding them together.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/dev/extensions.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport mlx.core as mx\n\ndef simple_axpby(x: mx.array, y: mx.array, alpha: float, beta: float) -> mx.array:\n    return alpha * x + beta * y\n```\n\n----------------------------------------\n\nTITLE: Implementing Grid Sample Function with Custom Metal Kernel in Python\nDESCRIPTION: Demonstrates a more complex example of implementing a grid sample function using a custom Metal kernel. It uses mx.custom_function and mx.fast.metal_kernel to create a fused kernel for the forward pass.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/dev/custom_metal_kernels.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@mx.custom_function\ndef grid_sample(x, grid):\n\n    assert x.ndim == 4, \"`x` must be 4D.\"\n    assert grid.ndim == 4, \"`grid` must be 4D.\"\n\n    B, _, _, C = x.shape\n    _, gN, gM, D = grid.shape\n    out_shape = (B, gN, gM, C)\n\n    assert D == 2, \"Last dim of `grid` must be size 2.\"\n\n    source = \"\"\"\n        uint elem = thread_position_in_grid.x;\n        int H = x_shape[1];\n        int W = x_shape[2];\n        int C = x_shape[3];\n        int gH = grid_shape[1];\n        int gW = grid_shape[2];\n\n        int w_stride = C;\n        int h_stride = W * w_stride;\n        int b_stride = H * h_stride;\n\n        uint grid_idx = elem / C * 2;\n        float ix = ((grid[grid_idx] + 1) * W - 1) / 2;\n        float iy = ((grid[grid_idx + 1] + 1) * H - 1) / 2;\n\n        int ix_nw = floor(ix);\n        int iy_nw = floor(iy);\n\n        int ix_ne = ix_nw + 1;\n        int iy_ne = iy_nw;\n\n        int ix_sw = ix_nw;\n        int iy_sw = iy_nw + 1;\n\n        int ix_se = ix_nw + 1;\n        int iy_se = iy_nw + 1;\n\n        T nw = (ix_se - ix)    * (iy_se - iy);\n        T ne = (ix    - ix_sw) * (iy_sw - iy);\n        T sw = (ix_ne - ix)    * (iy    - iy_ne);\n        T se = (ix    - ix_nw) * (iy    - iy_nw);\n\n        int batch_idx = elem / C / gH / gW * b_stride;\n        int channel_idx = elem % C;\n        int base_idx = batch_idx + channel_idx;\n\n        T I_nw = x[base_idx + iy_nw * h_stride + ix_nw * w_stride];\n        T I_ne = x[base_idx + iy_ne * h_stride + ix_ne * w_stride];\n        T I_sw = x[base_idx + iy_sw * h_stride + ix_sw * w_stride];\n        T I_se = x[base_idx + iy_se * h_stride + ix_se * w_stride];\n\n        I_nw = iy_nw >= 0 && iy_nw <= H - 1 && ix_nw >= 0 && ix_nw <= W - 1 ? I_nw : 0;\n        I_ne = iy_ne >= 0 && iy_ne <= H - 1 && ix_ne >= 0 && ix_ne <= W - 1 ? I_ne : 0;\n        I_sw = iy_sw >= 0 && iy_sw <= H - 1 && ix_sw >= 0 && ix_sw <= W - 1 ? I_sw : 0;\n        I_se = iy_se >= 0 && iy_se <= H - 1 && ix_se >= 0 && ix_se <= W - 1 ? I_se : 0;\n\n        out[elem] = nw * I_nw + ne * I_ne + sw * I_sw + se * I_se;\n    \"\"\"\n    kernel = mx.fast.metal_kernel(\n        name=\"grid_sample\",\n        input_names=[\"x\", \"grid\"],\n        output_names=[\"out\"],\n        source=source,\n    )\n    outputs = kernel(\n        inputs=[x, grid],\n        template=[(\"T\", x.dtype)],\n        output_shapes=[out_shape],\n        output_dtypes=[x.dtype],\n        grid=(np.prod(out_shape), 1, 1),\n        threadgroup=(256, 1, 1),\n    )\n    return outputs[0]\n```\n\n----------------------------------------\n\nTITLE: Handling Dependencies Across Devices in MLX\nDESCRIPTION: Illustrates how MLX automatically manages dependencies between operations on different devices to ensure correct execution order.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/unified_memory.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nc = mx.add(a, b, stream=mx.cpu)\nd = mx.add(a, c, stream=mx.gpu)\n```\n\n----------------------------------------\n\nTITLE: Efficient Evaluation in Training Loop with MLX in Python\nDESCRIPTION: This example shows an efficient pattern for using evaluation in a training loop, evaluating the loss and model parameters at the end of each iteration.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/lazy_evaluation.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfor batch in dataset:\n\n    # Nothing has been evaluated yet\n    loss, grad = value_and_grad_fn(model, batch)\n\n    # Still nothing has been evaluated\n    optimizer.update(model, grad)\n\n    # Evaluate the loss and the new parameters which will\n    # run the full gradient computation and optimizer update\n    mx.eval(loss, model.parameters())\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading MLX Optimizers\nDESCRIPTION: Example demonstrating how to serialize an optimizer's state to a safetensors file and later reload it. The example uses Adam optimizer and shows which parameters are typically persisted in the optimizer state.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/python/optimizers.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport mlx.core as mx\nfrom mlx.utils import tree_flatten, tree_unflatten\nimport mlx.optimizers as optim\n\noptimizer = optim.Adam(learning_rate=1e-2)\n\n# Perform some updates with the optimizer\nmodel = {\"w\" : mx.zeros((5, 5))}\ngrads = {\"w\" : mx.ones((5, 5))}\noptimizer.update(model, grads)\n\n# Save the state\nstate = tree_flatten(optimizer.state)\nmx.save_safetensors(\"optimizer.safetensors\", dict(state))\n\n# Later on, for example when loading from a checkpoint,\n# recreate the optimizer and load the state\noptimizer = optim.Adam(learning_rate=1e-2)\n\nstate = tree_unflatten(list(mx.load(\"optimizer.safetensors\").items()))\noptimizer.state = state\n```\n\n----------------------------------------\n\nTITLE: Converting MLX Arrays to NumPy and Back\nDESCRIPTION: Basic example of converting an MLX array to NumPy and back. This involves creating an MLX array, converting it to a NumPy array (which creates a copy), and then converting back to an MLX array.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/numpy.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport mlx.core as mx\nimport numpy as np\n\na = mx.arange(3)\nb = np.array(a) # copy of a\nc = mx.array(b) # copy of b\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Lazy Evaluation with Unused Computations in Python\nDESCRIPTION: This snippet shows how lazy evaluation in MLX allows for efficient handling of unused computations. The expensive function is not actually computed when its result is discarded.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/lazy_evaluation.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef fun(x):\n    a = fun1(x)\n    b = expensive_fun(a)\n    return a, b\n\ny, _ = fun(x)\n```\n\n----------------------------------------\n\nTITLE: Creating Arrays in Unified Memory with MLX\nDESCRIPTION: Demonstrates creation of arrays in MLX using unified memory, where no specific device location needs to be specified.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/unified_memory.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\na = mx.random.normal((100,))\nb = mx.random.normal((100,))\n```\n\n----------------------------------------\n\nTITLE: Implementing AXPBY with Existing MLX Operations\nDESCRIPTION: A simple implementation of the axpby operation using existing MLX primitives like multiply and add. This shows how operations can be composed from other operations.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/dev/extensions.rst#2025-04-19_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\narray axpby(\n    const array& x, // Input array x\n    const array& y, // Input array y\n    const float alpha, // Scaling factor for x\n    const float beta, // Scaling factor for y\n    StreamOrDevice s /* = {} */ // Stream on which to schedule the operation\n) {\n    // Scale x and y on the provided stream\n    auto ax = multiply(array(alpha), x, s);\n    auto by = multiply(array(beta), y, s);\n\n    // Add and return\n    return add(ax, by, s);\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Synthetic Dataset\nDESCRIPTION: Creates synthetic training data by generating random parameters, input features, and noisy labels using MLX's random number generation functions.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/examples/linear_regression.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# True parameters\nw_star = mx.random.normal((num_features,))\n\n# Input examples (design matrix)\nX = mx.random.normal((num_examples, num_features))\n\n# Noisy labels\neps = 1e-2 * mx.random.normal((num_examples,))\ny = X @ w_star + eps\n```\n\n----------------------------------------\n\nTITLE: Different Ways to Pass Arguments to Exported Functions\nDESCRIPTION: Illustrates various ways to specify arguments when exporting and calling functions, including positional arguments and tuples of arrays.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/export.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef fun(x, y):\n  return x + y\n\nx = mx.array(1.0)\ny = mx.array(1.0)\n \n# Both arguments to fun are positional\nmx.export_function(\"add.mlxfn\", fun, x, y)\n\n# Same as above\nmx.export_function(\"add.mlxfn\", fun, (x, y))\n\nimported_fun = mx.import_function(\"add.mlxfn\")\n\n# Ok\nout, = imported_fun(x, y)\n\n# Also ok\nout, = imported_fun((x, y))\n```\n\n----------------------------------------\n\nTITLE: Creating NumPy Views of MLX Arrays\nDESCRIPTION: Example of creating a NumPy view of an MLX array without copying data. This demonstrates how modifications to the NumPy view are reflected in the original MLX array because they share the same underlying memory.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/numpy.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\na = mx.arange(3)\na_view = np.array(a, copy=False)\nprint(a_view.flags.owndata) # False\na_view[0] = 1\nprint(a[0].item()) # 1\n```\n\n----------------------------------------\n\nTITLE: Shared References with In-place Updates in MLX\nDESCRIPTION: Shows how in-place updates affect all references to the same MLX array, illustrating that assignment creates a view rather than a copy.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/indexing.rst#2025-04-19_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n>>> a = mx.array([1, 2, 3])\n>>> b = a\n>>> b[2] = 0\n>>> b\narray([1, 2, 0], dtype=int32)\n>>> a\narray([1, 2, 0], dtype=int32)\n```\n\n----------------------------------------\n\nTITLE: Accessing Default Stream in MLX\nDESCRIPTION: Demonstrates how to use default stream by either not specifying a stream argument or by providing a device to use its default stream.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/using_streams.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nmx.default_stream(mx.default_device())\n```\n\nLANGUAGE: python\nCODE:\n```\nmx.default_stream(my_device)\n```\n\n----------------------------------------\n\nTITLE: Defining Loss Function with Cross Entropy\nDESCRIPTION: Implements a loss function that calculates the mean cross entropy loss between model predictions and true labels using mlx.nn.losses.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/examples/mlp.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef loss_fn(model, X, y):\n    return mx.mean(nn.losses.cross_entropy(model(X), y))\n```\n\n----------------------------------------\n\nTITLE: Loading Converted Llama Weights in MLX Python\nDESCRIPTION: This code snippet demonstrates how to load the converted Llama weights into an MLX model. It uses the tree_unflatten utility to transform the flat key-value representation into a nested dictionary structure compatible with the model.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/examples/llama-inference.rst#2025-04-19_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nfrom mlx.utils import tree_unflatten\n\nmodel.update(tree_unflatten(list(mx.load(weight_file).items())))\n```\n\n----------------------------------------\n\nTITLE: Benchmarking MLX Extension Performance\nDESCRIPTION: This Python script benchmarks the performance of the custom axpby operation against a simple implementation. It includes setup, timing functions, and result comparison.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/dev/extensions.rst#2025-04-19_snippet_20\n\nLANGUAGE: Python\nCODE:\n```\nimport mlx.core as mx\nfrom mlx_sample_extensions import axpby\nimport time\n\ndef simple_axpby(x: mx.array, y: mx.array, alpha: float, beta: float) -> mx.array:\n    return alpha * x + beta * y\n\nM = 4096\nN = 4096\n\nx = mx.random.normal((M, N))\ny = mx.random.normal((M, N))\nalpha = 4.0\nbeta = 2.0\n\nmx.eval(x, y)\n\ndef bench(f):\n    # Warm up\n    for i in range(5):\n        z = f(x, y, alpha, beta)\n        mx.eval(z)\n\n    # Timed run\n    s = time.time()\n    for i in range(100):\n        z = f(x, y, alpha, beta)\n        mx.eval(z)\n    e = time.time()\n    return 1000 * (e - s) / 100\n\nsimple_time = bench(simple_axpby)\ncustom_time = bench(axpby)\n\nprint(f\"Simple axpby: {simple_time:.3f} ms | Custom axpby: {custom_time:.3f} ms\")\n```\n\n----------------------------------------\n\nTITLE: Debugging Compiled Functions with Placeholders\nDESCRIPTION: Shows the limitation of evaluating arrays inside compiled functions during tracing and provides a solution using mx.disable_compile().\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/compile.rst#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@mx.compile\ndef fun(x):\n    z = -x\n    print(z)  # Crash\n    return mx.exp(z)\n\nfun(mx.array(5.0))\n```\n\n----------------------------------------\n\nTITLE: Initializing MLX and Problem Parameters\nDESCRIPTION: Imports the MLX core package and sets up basic parameters for the linear regression problem including number of features, examples, iterations, and learning rate.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/examples/linear_regression.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport mlx.core as mx\n\nnum_features = 100\nnum_examples = 1_000\nnum_iters = 10_000  # iterations of SGD\nlr = 0.01  # learning rate for SGD\n```\n\n----------------------------------------\n\nTITLE: Compiling Nested Functions in MLX\nDESCRIPTION: Demonstrates how to compile nested function calls, where inner functions are already compiled. Shows that compiling the outermost function is a good practice for optimizing the computation graph.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/compile.rst#2025-04-19_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n@mx.compile\ndef inner(x):\n    return mx.exp(-mx.abs(x))\n\ndef outer(x):\n    inner(inner(x))\n\n# Compiling the outer function is good to do as it will likely\n# be faster even though the inner functions are compiled\nfun = mx.compile(outer)\n```\n\n----------------------------------------\n\nTITLE: Creating Model Evaluation Function\nDESCRIPTION: Defines an evaluation function that computes accuracy by comparing the model's predicted class (argmax) with the true labels.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/examples/mlp.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef eval_fn(model, X, y):\n    return mx.mean(mx.argmax(model(X), axis=1) == y)\n```\n\n----------------------------------------\n\nTITLE: Basic In-place Updates in MLX\nDESCRIPTION: Demonstrates how to perform in-place updates on MLX arrays by assigning values to indexed elements, which modifies the original array.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/indexing.rst#2025-04-19_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n>>> a = mx.array([1, 2, 3])\n>>> a[2] = 0\n>>> a\narray([1, 2, 0], dtype=int32)\n```\n\n----------------------------------------\n\nTITLE: Side Effects in Compiled Functions\nDESCRIPTION: Shows an incorrect approach that causes a crash when trying to append to a global state from within a compiled function.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/compile.rst#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nstate = []\n\n@mx.compile\ndef fun(x, y):\n    z = x + y\n    state.append(z)\n    return mx.exp(z)\n\nfun(mx.array(1.0), mx.array(2.0))\n# Crash!\nprint(state)\n```\n\n----------------------------------------\n\nTITLE: Loading Multiple Arrays in MLX\nDESCRIPTION: Example of loading multiple arrays from a .npz file. Returns a dictionary of array names to arrays.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/saving_and_loading.rst#2025-04-19_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n>>> mx.load(\"arrays.npz\")\n{'b': array([2], dtype=float32), 'arr_0': array([1], dtype=float32)}\n```\n\n----------------------------------------\n\nTITLE: Training Neural Networks without Compilation\nDESCRIPTION: A complete example of training a simple linear model with SGD optimizer and binary cross-entropy loss without compilation.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/compile.rst#2025-04-19_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport mlx.core as mx\nimport mlx.nn as nn\nimport mlx.optimizers as optim\n\n# 4 examples with 10 features each\nx = mx.random.uniform(shape=(4, 10))\n\n# 0, 1 targets\ny = mx.array([0, 1, 0, 1])\n\n# Simple linear model\nmodel = nn.Linear(10, 1)\n\n# SGD with momentum\noptimizer = optim.SGD(learning_rate=0.1, momentum=0.8)\n\ndef loss_fn(model, x, y):\n    logits = model(x).squeeze()\n    return nn.losses.binary_cross_entropy(logits, y)\n\nloss_and_grad_fn = nn.value_and_grad(model, loss_fn)\n\n# Perform 10 steps of gradient descent\nfor it in range(10):\n    loss, grads = loss_and_grad_fn(model, x, y)\n    optimizer.update(model, grads)\n    mx.eval(model.parameters(), optimizer.state)\n```\n\n----------------------------------------\n\nTITLE: Constant Capture in Compiled Functions\nDESCRIPTION: Demonstrates how compiled functions treat external arrays as constants unless explicitly handled.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/compile.rst#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nstate = [mx.array(1.0)]\n\n@mx.compile\ndef fun(x):\n    return x + state[0]\n\n# Prints array(2, dtype=float32)\nprint(fun(mx.array(1.0)))\n\n# Update state\nstate[0] = mx.array(5.0)\n\n# Still prints array(2, dtype=float32)\nprint(fun(mx.array(1.0)))\n```\n\n----------------------------------------\n\nTITLE: Installing MLX with pip for Python API\nDESCRIPTION: Command to install the MLX Python API using pip package manager. This is the simplest way to get started with MLX.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/README.md#2025-04-19_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install mlx\n```\n\n----------------------------------------\n\nTITLE: Converting Llama Weights and Saving to NPZ in Python\nDESCRIPTION: This script converts Llama weights from PyTorch format to MLX-compatible NPZ format. It uses argparse to handle command-line arguments for input and output files, then applies the conversion function to each weight.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/examples/llama-inference.rst#2025-04-19_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Convert Llama weights to MLX\")\n    parser.add_argument(\"torch_weights\")\n    parser.add_argument(\"output_file\")\n    args = parser.parse_args()\n\n    state = torch.load(args.torch_weights)\n    np.savez(\n        args.output_file,\n        **{k: v for k, v in starmap(map_torch_to_mlx, state.items()) if k is not None}\n    )\n```\n\n----------------------------------------\n\nTITLE: Capturing Implicit Inputs in Compiled Functions\nDESCRIPTION: Shows how to use the inputs parameter in mx.compile to track changes in external state variables.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/compile.rst#2025-04-19_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom functools import partial\nstate = [mx.array(1.0)]\n\n# Tell compile to capture state as an input\n@partial(mx.compile, inputs=state)\ndef fun(x):\n    return x + state[0]\n\n# Prints array(2, dtype=float32)\nprint(fun(mx.array(1.0)))\n\n# Update state\nstate[0] = mx.array(5.0)\n\n# Prints array(6, dtype=float32)\nprint(fun(mx.array(1.0)))\n```\n\n----------------------------------------\n\nTITLE: Evaluating Model Performance\nDESCRIPTION: Computes and prints the final loss value and the error between learned and true parameters to evaluate the model's performance.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/examples/linear_regression.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nloss = loss_fn(w)\nerror_norm = mx.sum(mx.square(w - w_star)).item() ** 0.5\n\nprint(\n    f\"Loss {loss.item():.5f}, |w-w*| = {error_norm:.5f}, \"\n)\n# Should print something close to: Loss 0.00005, |w-w*| = 0.00364\n```\n\n----------------------------------------\n\nTITLE: Problematic Shapeless Compilation with Shape-Dependent Operations\nDESCRIPTION: Demonstrates a case where shapeless compilation fails due to shape-dependent operations. The function uses reshape with hardcoded shape dimensions that don't work when input shapes change.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/compile.rst#2025-04-19_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndef fun(x):\n    return x.reshape(x.shape[0] * x.shape[1], -1)\n\ncompiled_fun = mx.compile(fun, shapeless=True)\n\nx = mx.random.uniform(shape=(2, 3, 4))\n\nout = compiled_fun(x)\n\nx = mx.random.uniform(shape=(5, 5, 3))\n\n# Error, can't reshape (5, 5, 3) to (6, -1)\nout = compiled_fun(x)\n```\n\n----------------------------------------\n\nTITLE: Returning State from Compiled Functions\nDESCRIPTION: A solution for handling state in compiled functions by explicitly returning the state as part of the function output.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/compile.rst#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nstate = []\n\n@mx.compile\ndef fun(x, y):\n   z = x + y\n   state.append(z)\n   return mx.exp(z), state\n\n_, state = fun(mx.array(1.0), mx.array(2.0))\n# Prints [array(3, dtype=float32)]\nprint(state)\n```\n\n----------------------------------------\n\nTITLE: Saving Arrays Using Safetensors in MLX\nDESCRIPTION: Example of saving arrays using the safetensors format. Takes a dictionary of string names to arrays as input.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/saving_and_loading.rst#2025-04-19_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n>>> a = mx.array([1.0])\n>>> b = mx.array([2.0])\n>>> mx.save_safetensors(\"arrays\", {\"a\": a, \"b\": b})\n```\n\n----------------------------------------\n\nTITLE: Inspecting MLX Neural Network Modules\nDESCRIPTION: Examples of how to inspect and analyze MLX neural network modules, including printing model architecture and parameter information.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/python/nn.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nprint(mlp)\n\nfrom mlx.utils import tree_map\nshapes = tree_map(lambda p: p.shape, mlp.parameters())\n\nfrom mlx.utils import tree_flatten\nnum_params = sum(v.size for _, v in tree_flatten(mlp.parameters()))\n```\n\n----------------------------------------\n\nTITLE: Basic MLX C++ Example Program\nDESCRIPTION: A simple C++ program that demonstrates MLX array creation and addition operations. It creates two arrays with values [1, 2, 3] and adds them together, then outputs the result to the console.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/dev/mlx_in_cpp.rst#2025-04-19_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\n#include <iostream>\n\n#include \"mlx/mlx.h\"\n\nnamespace mx = mlx::core;\n\nint main() {\n  auto x = mx::array({1, 2, 3});\n  auto y = mx::array({1, 2, 3});\n  std::cout << x + y << std::endl;\n  return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Fixing Shape-Dependent Operations for Shapeless Compilation\nDESCRIPTION: Shows how to fix shape-dependent operations for shapeless compilation by using shape-agnostic functions like flatten instead of hardcoding dimensions in reshape.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/compile.rst#2025-04-19_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndef fun(x):\n    return x.flatten(0, 1)\n\ncompiled_fun = mx.compile(fun, shapeless=True)\n\nx = mx.random.uniform(shape=(2, 3, 4))\n\nout = compiled_fun(x)\n\nx = mx.random.uniform(shape=(5, 5, 3))\n\n# Ok\nout = compiled_fun(x)\n```\n\n----------------------------------------\n\nTITLE: Capturing GPU Trace with MLX Metal Debugger in Python\nDESCRIPTION: This code demonstrates how to capture MLX GPU operations using the Metal debugger. It initializes random matrices, starts a capture, performs matrix addition operations, and saves the trace to a file for later analysis in Xcode.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/dev/metal_debugger.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport mlx.core as mx\n\na = mx.random.uniform(shape=(512, 512))\nb = mx.random.uniform(shape=(512, 512))\nmx.eval(a, b)\n\ntrace_file = \"mlx_trace.gputrace\"\n\n# Make sure to run with MTL_CAPTURE_ENABLED=1 and\n# that the path trace_file does not already exist.\nmx.metal.start_capture(trace_file)\n\nfor _ in range(10):\n  mx.eval(mx.add(a, b))\n\nmx.metal.stop_capture()\n```\n\n----------------------------------------\n\nTITLE: Exporting a Neural Network Module with Parameters\nDESCRIPTION: Demonstrates how to export an MLX neural network module with its parameters included in the exported function.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/export.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmodel = nn.Linear(4, 4)\nmx.eval(model.parameters())\n\ndef call(x):\n   return model(x)\n\nmx.export_function(\"model.mlxfn\", call, mx.zeros(4))\n```\n\n----------------------------------------\n\nTITLE: Defining the AXPBY Operation in C++\nDESCRIPTION: C++ declaration of the axpby operation that takes two arrays and two scaling factors. This defines the function signature with proper documentation and parameter descriptions.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/dev/extensions.rst#2025-04-19_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\n/**\n*  Scale and sum two vectors element-wise\n*  z = alpha * x + beta * y\n*\n*  Use NumPy-style broadcasting between x and y\n*  Inputs are upcasted to floats if needed\n**/\narray axpby(\n    const array& x, // Input array x\n    const array& y, // Input array y\n    const float alpha, // Scaling factor for x\n    const float beta, // Scaling factor for y\n    StreamOrDevice s = {} // Stream on which to schedule the operation\n);\n```\n\n----------------------------------------\n\nTITLE: Module Class Structure Documentation in RST\nDESCRIPTION: ReStructuredText documentation defining the structure and API reference for the MLX neural network Module class. Lists all available attributes and methods including training states, module operations, parameter management, and weight handling.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/python/nn/module.rst#2025-04-19_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\nModule\n======\n\n.. currentmodule:: mlx.nn\n\n.. autoclass:: Module\n\n   .. rubric:: Attributes\n\n   .. autosummary::\n      :toctree: _autosummary\n   \n      Module.training\n      Module.state\n   \n   .. rubric:: Methods\n\n   .. autosummary::\n      :toctree: _autosummary\n   \n      Module.apply\n      Module.apply_to_modules\n      Module.children\n      Module.eval\n      Module.filter_and_map\n      Module.freeze\n      Module.leaf_modules\n      Module.load_weights\n      Module.modules\n      Module.named_modules\n      Module.parameters\n      Module.save_weights\n      Module.set_dtype\n      Module.train\n      Module.trainable_parameters\n      Module.unfreeze\n      Module.update\n      Module.update_modules\n```\n\n----------------------------------------\n\nTITLE: Starting a PyTorch to MLX Weight Conversion Script\nDESCRIPTION: Beginning of a script to convert PyTorch weights to the MLX format. The code defines a function that maps PyTorch weight keys to their MLX equivalents for the Llama model, focusing on the embedding layer as an example.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/examples/llama-inference.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport argparse\nfrom itertools import starmap\n\nimport numpy as np\nimport torch\n\ndef map_torch_to_mlx(key, value):\n    if \"tok_embedding\" in key:\n        key = \"embedding.weight\"\n\n    elif \"norm\" in key:\n```\n\n----------------------------------------\n\nTITLE: Basic Gradient Computation in MLX\nDESCRIPTION: Demonstrates computing first and second derivatives of sine function using MLX's grad transform.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/function_transforms.rst#2025-04-19_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n>>> dfdx = mx.grad(mx.sin)\n>>> dfdx(mx.array(mx.pi))\narray(-1, dtype=float32)\n>>> mx.cos(mx.array(mx.pi))\narray(-1, dtype=float32)\n```\n\nLANGUAGE: shell\nCODE:\n```\n>>> d2fdx2 = mx.grad(mx.grad(mx.sin))\n>>> d2fdx2(mx.array(mx.pi / 2))\narray(-1, dtype=float32)\n>>> mx.sin(mx.array(mx.pi / 2))\narray(1, dtype=float32)\n```\n\n----------------------------------------\n\nTITLE: Checking Dtype Subtypes in Python\nDESCRIPTION: This snippet demonstrates how to use the issubdtype function to determine if one dtype is a subtype of another category. This is useful for type checking and comparisons in MLX Core.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/python/data_types.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncore.issubdtype(dtype1, dtype_category)\n```\n\n----------------------------------------\n\nTITLE: Using MLX Extension in Python\nDESCRIPTION: This Python script demonstrates how to use the newly created MLX extension. It imports the extension, creates arrays, applies the axpby operation, and verifies the results.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/dev/extensions.rst#2025-04-19_snippet_19\n\nLANGUAGE: Python\nCODE:\n```\nimport mlx.core as mx\nfrom mlx_sample_extensions import axpby\n\na = mx.ones((3, 4))\nb = mx.ones((3, 4))\nc = axpby(a, b, 4.0, 2.0, stream=mx.cpu)\n\nprint(f\"c shape: {c.shape}\")\nprint(f\"c dtype: {c.dtype}\")\nprint(f\"c is correct: {mx.all(c == 6.0).item()}\")\n```\n\n----------------------------------------\n\nTITLE: Implementing CPU Backend for AXPBY Operation\nDESCRIPTION: Template function implementing the CPU evaluation of the axpby operation. This shows how to allocate memory for the output array and set up the CPU command encoder for computation.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/dev/extensions.rst#2025-04-19_snippet_5\n\nLANGUAGE: C++\nCODE:\n```\ntemplate <typename T>\nvoid axpby_impl(\n    const mx::array& x,\n    const mx::array& y,\n    mx::array& out,\n    float alpha_,\n    float beta_,\n    mx::Stream stream) {\n  out.set_data(mx::allocator::malloc(out.nbytes()));\n\n  // Get the CPU command encoder and register input and output arrays\n  auto& encoder = mx::cpu::get_command_encoder(stream);\n  encoder.set_input_array(x);\n  encoder.set_input_array(y);\n  encoder.set_output_array(out);\n\n  // Launch the CPU kernel\n  encoder.dispatch([x_ptr = x.data<T>(),\n                    y_ptr = y.data<T>(),\n                    out_ptr = out.data<T>(),\n                    size = out.size(),\n                    shape = out.shape(),\n                    x_strides = x.strides(),\n                    y_strides = y.strides(),\n                    alpha_,\n                    beta_]() {\n\n    // Cast alpha and beta to the relevant types\n    T alpha = static_cast<T>(alpha_);\n    T beta = static_cast<T>(beta_);\n\n    // Do the element-wise operation for each output\n\n```\n\n----------------------------------------\n\nTITLE: Exporting and Importing a Basic Function in Python\nDESCRIPTION: Demonstrates the basics of exporting a simple addition function to a file and importing it back. Shows how the exported function works with compatible inputs but throws exceptions with incompatible types or shapes.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/export.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef fun(x, y):\n  return x + y\n\nx = mx.array(1.0)\ny = mx.array(1.0)\nmx.export_function(\"add.mlxfn\", fun, x, y)\n```\n\n----------------------------------------\n\nTITLE: GELU Activation Function in MLX\nDESCRIPTION: Shows the definition of the GELU activation function which contains multiple operations that can be fused when compiled.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/compile.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef gelu(x):\n    return x * (1 + mx.erf(x / math.sqrt(2))) / 2\n```\n\n----------------------------------------\n\nTITLE: CPU Backend Configuration with BLAS Libraries for MLX\nDESCRIPTION: Configures the CPU backend for MLX, handling different scenarios for BLAS/LAPACK libraries based on the platform. It supports using Accelerate framework on macOS, building OpenBLAS from source, or finding system-installed libraries.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/CMakeLists.txt#2025-04-19_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nif(MLX_BUILD_CPU)\n  find_library(ACCELERATE_LIBRARY Accelerate)\n  if(ACCELERATE_LIBRARY)\n    message(STATUS \"Accelerate found ${ACCELERATE_LIBRARY}\")\n    set(MLX_BUILD_ACCELERATE ON)\n  else()\n    message(STATUS \"Accelerate or arm neon not found, using default backend.\")\n    set(MLX_BUILD_ACCELERATE OFF)\n  endif()\n\n  if(MLX_BUILD_ACCELERATE)\n    target_link_libraries(mlx PUBLIC ${ACCELERATE_LIBRARY})\n    add_compile_definitions(MLX_USE_ACCELERATE)\n    add_compile_definitions(ACCELERATE_NEW_LAPACK)\n  elseif(MLX_BUILD_BLAS_FROM_SOURCE)\n    # Download and build OpenBLAS from source code.\n    FetchContent_Declare(\n      openblas\n      GIT_REPOSITORY https://github.com/OpenMathLib/OpenBLAS.git\n      GIT_TAG v0.3.28\n      EXCLUDE_FROM_ALL)\n    set(BUILD_STATIC_LIBS ON) # link statically\n    set(NOFORTRAN ON) # msvc has no fortran compiler\n    FetchContent_MakeAvailable(openblas)\n    target_link_libraries(mlx PRIVATE openblas)\n    target_include_directories(\n      mlx PRIVATE \"${openblas_SOURCE_DIR}/lapack-netlib/LAPACKE/include\"\n                  \"${CMAKE_BINARY_DIR}/generated\" \"${CMAKE_BINARY_DIR}\")\n  else()\n    if(${CMAKE_HOST_APPLE})\n      # The blas shipped in macOS SDK is not supported, search homebrew for\n      # openblas instead.\n      set(BLA_VENDOR OpenBLAS)\n      set(LAPACK_ROOT\n          \"${LAPACK_ROOT};$ENV{LAPACK_ROOT};/usr/local/opt/openblas\")\n    endif()\n    # Search and link with lapack.\n    find_package(LAPACK REQUIRED)\n    if(NOT LAPACK_FOUND)\n      message(FATAL_ERROR \"Must have LAPACK installed\")\n    endif()\n    find_path(LAPACK_INCLUDE_DIRS lapacke.h /usr/include /usr/local/include\n              /usr/local/opt/openblas/include)\n    message(STATUS \"Lapack lib \" ${LAPACK_LIBRARIES})\n    message(STATUS \"Lapack include \" ${LAPACK_INCLUDE_DIRS})\n    target_include_directories(mlx PRIVATE ${LAPACK_INCLUDE_DIRS})\n    target_link_libraries(mlx PRIVATE ${LAPACK_LIBRARIES})\n    # List blas after lapack otherwise we may accidentally incldue an old\n    # version of lapack.h from the include dirs of blas.\n    find_package(BLAS REQUIRED)\n    if(NOT BLAS_FOUND)\n      message(FATAL_ERROR \"Must have BLAS installed\")\n    endif()\n    # TODO find a cleaner way to do this\n    find_path(BLAS_INCLUDE_DIRS cblas.h /usr/include /usr/local/include\n              $ENV{BLAS_HOME}/include)\n    message(STATUS \"Blas lib \" ${BLAS_LIBRARIES})\n    message(STATUS \"Blas include \" ${BLAS_INCLUDE_DIRS})\n    target_include_directories(mlx PRIVATE ${BLAS_INCLUDE_DIRS})\n    target_link_libraries(mlx PRIVATE ${BLAS_LIBRARIES})\n  endif()\nelse()\n  set(MLX_BUILD_ACCELERATE OFF)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Reinitializing Neural Network Module Parameters in MLX\nDESCRIPTION: Shows how to reinitialize all parameters in an MLX neural network module using a uniform distribution. Creates a sequential model and applies a custom uniform initialization with specified bounds to all parameters.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/python/nn/init.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport mlx.nn as nn\nmodel = nn.Sequential(nn.Linear(5, 10), nn.ReLU(), nn.Linear(10, 5))\ninit_fn = nn.init.uniform(low=-0.1, high=0.1)\nmodel.apply(init_fn)\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake for MLX Extension Library\nDESCRIPTION: This CMake snippet shows how to set up the build process for an MLX extension library. It includes adding the library, sources, include directories, and linking with MLX.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/dev/extensions.rst#2025-04-19_snippet_15\n\nLANGUAGE: CMake\nCODE:\n```\n# Add library\nadd_library(mlx_ext)\n\n# Add sources\ntarget_sources(\n    mlx_ext\n    PUBLIC\n    ${CMAKE_CURRENT_LIST_DIR}/axpby/axpby.cpp\n)\n\n# Add include headers\ntarget_include_directories(\n    mlx_ext PUBLIC ${CMAKE_CURRENT_LIST_DIR}\n)\n\n# Link to mlx\ntarget_link_libraries(mlx_ext PUBLIC mlx)\n```\n\n----------------------------------------\n\nTITLE: Importing and Using an Exported Function in Python\nDESCRIPTION: Shows how to import a previously exported function and use it with various inputs. Demonstrates error handling when input types or shapes don't match the exported signature.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/export.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nadd_fun = mx.import_function(\"add.mlxfn\")\n\nout, = add_fun(mx.array(1.0), mx.array(2.0))\n# Prints: array(3, dtype=float32)\nprint(out)\n\nout, = add_fun(mx.array(1.0), mx.array(3.0))\n# Prints: array(4, dtype=float32)\nprint(out)\n\n# Raises an exception\nadd_fun(mx.array(1), mx.array(3.0))\n\n# Raises an exception\nadd_fun(mx.array([1.0, 2.0]), mx.array(3.0))\n```\n\n----------------------------------------\n\nTITLE: Implementing AXPBY Operation Using the Custom Primitive\nDESCRIPTION: Implementation of the axpby operation using the Axpby primitive. This handles data type promotion, broadcasting inputs, and creating the output array with the custom primitive.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/dev/extensions.rst#2025-04-19_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\narray axpby(\n    const array& x, // Input array x\n    const array& y, // Input array y\n    const float alpha, // Scaling factor for x\n    const float beta, // Scaling factor for y\n    StreamOrDevice s /* = {} */ // Stream on which to schedule the operation\n) {\n    // Promote dtypes between x and y as needed\n    auto promoted_dtype = promote_types(x.dtype(), y.dtype());\n\n    // Upcast to float32 for non-floating point inputs x and y\n    auto out_dtype = issubdtype(promoted_dtype, float32)\n        ? promoted_dtype\n        : promote_types(promoted_dtype, float32);\n\n    // Cast x and y up to the determined dtype (on the same stream s)\n    auto x_casted = astype(x, out_dtype, s);\n    auto y_casted = astype(y, out_dtype, s);\n\n    // Broadcast the shapes of x and y (on the same stream s)\n    auto broadcasted_inputs = broadcast_arrays({x_casted, y_casted}, s);\n    auto out_shape = broadcasted_inputs[0].shape();\n\n    // Construct the array as the output of the Axpby primitive\n    // with the broadcasted and upcasted arrays as inputs\n    return array(\n        /* const std::vector<int>& shape = */ out_shape,\n        /* Dtype dtype = */ out_dtype,\n        /* std::unique_ptr<Primitive> primitive = */\n        std::make_shared<Axpby>(to_stream(s), alpha, beta),\n        /* const std::vector<array>& inputs = */ broadcasted_inputs);\n}\n```\n\n----------------------------------------\n\nTITLE: Building and Installing MLX Python Package\nDESCRIPTION: Command to build and install the MLX Python package from source using pip with parallel build optimization.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/install.rst#2025-04-19_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nCMAKE_BUILD_PARALLEL_LEVEL=8 pip install .\n```\n\n----------------------------------------\n\nTITLE: Using Keyword Arguments with Exported Functions\nDESCRIPTION: Shows how to use keyword arguments when exporting functions and the requirement to use the same keyword arguments when calling the imported function.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/export.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef fun(x, y):\n  return x + y\n\n# One argument to fun is positional, the other is a kwarg\nmx.export_function(\"add.mlxfn\", fun, x, y=y)\n\nimported_fun = mx.import_function(\"add.mlxfn\")\n\n# Ok\nout, = imported_fun(x, y=y)\n\n# Also ok\nout, = imported_fun((x,), {\"y\": y})\n\n# Raises since the keyword argument is missing\nout, = imported_fun(x, y)\n\n# Raises since the keyword argument has the wrong key\nout, = imported_fun(x, z=y)\n```\n\n----------------------------------------\n\nTITLE: Test, Examples, and Benchmarks Configuration for MLX\nDESCRIPTION: Configures optional components for MLX, including tests, examples, and benchmarks. Each component is conditionally added based on the corresponding build option settings.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/CMakeLists.txt#2025-04-19_snippet_9\n\nLANGUAGE: CMake\nCODE:\n```\nif(MLX_BUILD_TESTS)\n  include(CTest)\n  add_subdirectory(${CMAKE_CURRENT_LIST_DIR}/tests)\nendif()\n\nif(MLX_BUILD_EXAMPLES)\n  add_subdirectory(${CMAKE_CURRENT_LIST_DIR}/examples/cpp)\nendif()\n\nif(MLX_BUILD_BENCHMARKS)\n  add_subdirectory(${CMAKE_CURRENT_LIST_DIR}/benchmarks/cpp)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Automatic Differentiation with In-place Updates in MLX\nDESCRIPTION: Demonstrates how MLX handles automatic differentiation with in-place updates correctly, computing proper gradients for functions that modify arrays in-place.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/indexing.rst#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef fun(x, idx):\n    x[idx] = 2.0\n    return x.sum()\n\ndfdx = mx.grad(fun)(mx.array([1.0, 2.0, 3.0]), mx.array([1]))\nprint(dfdx)  # Prints: array([1, 0, 1], dtype=float32)\n```\n\n----------------------------------------\n\nTITLE: Creating an AXPBY Primitive Class in C++\nDESCRIPTION: Definition of the Axpby primitive class that extends the base Primitive class. It includes methods for CPU/GPU evaluation, function transformations (jvp, vjp, vmap), and utility functions.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/dev/extensions.rst#2025-04-19_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\nclass Axpby : public Primitive {\n  public:\n    explicit Axpby(Stream stream, float alpha, float beta)\n        : Primitive(stream), alpha_(alpha), beta_(beta){};\n\n    /**\n    * A primitive must know how to evaluate itself on the CPU/GPU\n    * for the given inputs and populate the output array.\n    *\n    * To avoid unnecessary allocations, the evaluation function\n    * is responsible for allocating space for the array.\n    */\n    void eval_cpu(\n        const std::vector<array>& inputs,\n        std::vector<array>& outputs) override;\n    void eval_gpu(\n        const std::vector<array>& inputs,\n        std::vector<array>& outputs) override;\n\n    /** The Jacobian-vector product. */\n    std::vector<array> jvp(\n        const std::vector<array>& primals,\n        const std::vector<array>& tangents,\n        const std::vector<int>& argnums) override;\n\n    /** The vector-Jacobian product. */\n    std::vector<array> vjp(\n        const std::vector<array>& primals,\n        const std::vector<array>& cotangents,\n        const std::vector<int>& argnums,\n        const std::vector<array>& outputs) override;\n\n    /**\n    * The primitive must know how to vectorize itself across\n    * the given axes. The output is a pair containing the array\n    * representing the vectorized computation and the axis which\n    * corresponds to the output vectorized dimension.\n    */\n    virtual std::pair<std::vector<array>, std::vector<int>> vmap(\n        const std::vector<array>& inputs,\n        const std::vector<int>& axes) override;\n\n    /** Print the primitive. */\n    void print(std::ostream& os) override {\n        os << \"Axpby\";\n    }\n\n    /** Equivalence check **/\n    bool is_equivalent(const Primitive& other) const override;\n\n  private:\n    float alpha_;\n    float beta_;\n};\n```\n\n----------------------------------------\n\nTITLE: Cloning MLX Repository from GitHub\nDESCRIPTION: Commands to clone the MLX repository from GitHub and navigate into the directory, used as a first step for building from source.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/install.rst#2025-04-19_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ngit clone git@github.com:ml-explore/mlx.git mlx && cd mlx\n```\n\n----------------------------------------\n\nTITLE: Exporting a Python Function for C++ Usage\nDESCRIPTION: Shows how to export a function from Python that will be imported and used in C++.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/export.rst#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef fun(x, y):\n    return mx.exp(x + y)\n\nx = mx.array(1.0)\ny = mx.array(1.0)\nmx.export_function(\"fun.mlxfn\", fun, x, y)\n```\n\n----------------------------------------\n\nTITLE: Dependency Setup for MLX (JSON and FMT Libraries)\nDESCRIPTION: Sets up external dependencies for MLX, downloading and configuring JSON library for parsing and FMT library for formatting, both of which are used in the MLX implementation.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/CMakeLists.txt#2025-04-19_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nmessage(STATUS \"Downloading json\")\nFetchContent_Declare(\n  json\n  URL https://github.com/nlohmann/json/releases/download/v3.11.3/json.tar.xz)\nFetchContent_MakeAvailable(json)\ntarget_include_directories(\n  mlx PRIVATE $<BUILD_INTERFACE:${json_SOURCE_DIR}/single_include/nlohmann>)\n\nadd_subdirectory(${CMAKE_CURRENT_LIST_DIR}/mlx)\n\ntarget_include_directories(\n  mlx PUBLIC $<BUILD_INTERFACE:${CMAKE_CURRENT_LIST_DIR}>\n             $<INSTALL_INTERFACE:include>)\n\nFetchContent_Declare(\n  fmt\n  GIT_REPOSITORY https://github.com/fmtlib/fmt.git\n  GIT_TAG 10.2.1\n  EXCLUDE_FROM_ALL)\nFetchContent_MakeAvailable(fmt)\ntarget_link_libraries(mlx PRIVATE $<BUILD_INTERFACE:fmt::fmt-header-only>)\n```\n\n----------------------------------------\n\nTITLE: Implementing Strided Elementwise Exponential Function with Custom Metal Kernel in Python\nDESCRIPTION: Modifies the previous example to support arbitrarily strided arrays without relying on a copy. It uses MLX's built-in indexing utilities to handle non-contiguous memory layouts.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/dev/custom_metal_kernels.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef exp_elementwise(a: mx.array):\n    source = \"\"\"\n        uint elem = thread_position_in_grid.x;\n        // Utils from `mlx/backend/metal/kernels/utils.h` are automatically included\n        uint loc = elem_to_loc(elem, inp_shape, inp_strides, inp_ndim);\n        T tmp = inp[loc];\n        // Output arrays are always row contiguous\n        out[elem] = metal::exp(tmp);\n    \"\"\"\n\n    kernel = mx.fast.metal_kernel(\n        name=\"myexp_strided\",\n        input_names=[\"inp\"],\n        output_names=[\"out\"],\n        source=source\n    )\n    outputs = kernel(\n        inputs=[a],\n        template=[(\"T\", mx.float32)],\n        grid=(a.size, 1, 1),\n        threadgroup=(256, 1, 1),\n        output_shapes=[a.shape],\n        output_dtypes=[a.dtype],\n        ensure_row_contiguous=False,\n    )\n    return outputs[0]\n\na = mx.random.normal(shape=(4, 16)).astype(mx.float16)\n# make non-contiguous\na = a[::2]\nb = exp_elementwise(a)\nassert mx.allclose(b, mx.exp(a))\n```\n\n----------------------------------------\n\nTITLE: Instantiating Metal Kernel Templates for Different Types\nDESCRIPTION: Code that instantiates the Metal kernel template for the AXPBY operation with different floating point data types, giving each instantiation a unique name for identification.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/dev/extensions.rst#2025-04-19_snippet_8\n\nLANGUAGE: C++\nCODE:\n```\ninstantiate_kernel(\"axpby_general_float32\", axpby_general, float)\ninstantiate_kernel(\"axpby_general_float16\", axpby_general, float16_t)\ninstantiate_kernel(\"axpby_general_bfloat16\", axpby_general, bfloat16_t)\ninstantiate_kernel(\"axpby_general_complex64\", axpby_general, complex64_t)\n```\n\n----------------------------------------\n\nTITLE: Element-wise AXPBY Operation Function Implementation\nDESCRIPTION: The beginning of the axpby_impl function that performs the element-wise alpha*X + beta*Y operation on arrays. This snippet shows the inner loop implementation for each element.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/dev/extensions.rst#2025-04-19_snippet_13\n\nLANGUAGE: C++\nCODE:\n```\nfor (size_t out_idx = 0; out_idx < size; out_idx++) {\n  // Map linear indices to offsets in x and y\n  auto x_offset = mx::elem_to_loc(out_idx, shape, x_strides);\n  auto y_offset = mx::elem_to_loc(out_idx, shape, y_strides);\n\n  // We allocate the output to be contiguous and regularly strided\n  // (defaults to row major) and hence it doesn't need additional mapping\n  out_ptr[out_idx] = alpha * x_ptr[x_offset] + beta * y_ptr[y_offset];\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Type Stubs for IDE Support\nDESCRIPTION: Command to generate type stub files for MLX to enable autocompletion and type checking in IDEs.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/install.rst#2025-04-19_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\npython setup.py generate_stubs\n```\n\n----------------------------------------\n\nTITLE: Python Bindings Configuration for MLX\nDESCRIPTION: Sets up Python bindings for MLX when the MLX_BUILD_PYTHON_BINDINGS option is enabled. It finds the Python interpreter, locates the nanobind package for creating bindings, and configures the Python source directory.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/CMakeLists.txt#2025-04-19_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nif(MLX_BUILD_PYTHON_BINDINGS)\n  message(STATUS \"Building Python bindings.\")\n  find_package(\n    Python 3.8\n    COMPONENTS Interpreter Development.Module\n    REQUIRED)\n  execute_process(\n    COMMAND \"${Python_EXECUTABLE}\" -m nanobind --cmake_dir\n    OUTPUT_STRIP_TRAILING_WHITESPACE\n    OUTPUT_VARIABLE nanobind_ROOT)\n  find_package(nanobind CONFIG REQUIRED)\n  add_subdirectory(${CMAKE_CURRENT_LIST_DIR}/python/src)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Importing MLX Packages for Neural Network Implementation\nDESCRIPTION: Imports the necessary MLX packages including core functionality, neural network components, and optimizers, along with NumPy for data manipulation.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/examples/mlp.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport mlx.core as mx\nimport mlx.nn as nn\nimport mlx.optimizers as optim\n\nimport numpy as np\n```\n\n----------------------------------------\n\nTITLE: Implementing Vector-Jacobian Product (VJP) for AXPBY\nDESCRIPTION: The VJP transformation implementation for the Axpby primitive, used for reverse-mode automatic differentiation. It calculates gradients with respect to input arguments.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/dev/extensions.rst#2025-04-19_snippet_11\n\nLANGUAGE: C++\nCODE:\n```\n/** The vector-Jacobian product. */\nstd::vector<array> Axpby::vjp(\n        const std::vector<array>& primals,\n        const std::vector<array>& cotangents,\n        const std::vector<int>& argnums,\n        const std::vector<int>& /* unused */) {\n    // Reverse mode diff\n    std::vector<array> vjps;\n    for (auto arg : argnums) {\n        auto scale = arg == 0 ? alpha_ : beta_;\n        auto scale_arr = array(scale, cotangents[0].dtype());\n        vjps.push_back(multiply(scale_arr, cotangents[0], stream()));\n    }\n    return vjps;\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Jacobian-Vector Product (JVP) for AXPBY\nDESCRIPTION: The JVP transformation implementation for the Axpby primitive, used for forward-mode automatic differentiation. It handles different argnums cases by calculating the appropriate contributions.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/dev/extensions.rst#2025-04-19_snippet_10\n\nLANGUAGE: C++\nCODE:\n```\n/** The Jacobian-vector product. */\nstd::vector<array> Axpby::jvp(\n        const std::vector<array>& primals,\n        const std::vector<array>& tangents,\n        const std::vector<int>& argnums) {\n    // Forward mode diff that pushes along the tangents\n    // The jvp transform on the primitive can be built with ops\n    // that are scheduled on the same stream as the primitive\n\n    // If argnums = {0}, we only push along x in which case the\n    // jvp is just the tangent scaled by alpha\n    // Similarly, if argnums = {1}, the jvp is just the tangent\n    // scaled by beta\n    if (argnums.size() > 1) {\n        auto scale = argnums[0] == 0 ? alpha_ : beta_;\n        auto scale_arr = array(scale, tangents[0].dtype());\n        return {multiply(scale_arr, tangents[0], stream())};\n    }\n    // If argnums = {0, 1}, we take contributions from both\n    // which gives us jvp = tangent_x * alpha + tangent_y * beta\n    else {\n        return {axpby(tangents[0], tangents[1], alpha_, beta_, stream())};\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Building C++ MLX Library with CMake\nDESCRIPTION: Commands to create a build directory, configure with CMake, and build the C++ MLX library using make with parallel jobs.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/install.rst#2025-04-19_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nmkdir -p build && cd build\ncmake .. && make -j\n```\n\n----------------------------------------\n\nTITLE: Version Detection from Header File in CMake\nDESCRIPTION: Extracts the MLX version from the header file using regex patterns when no version is explicitly provided. It parses major, minor, and patch versions from mlx/version.h.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT MLX_VERSION)\n  file(STRINGS \"mlx/version.h\" _mlx_h_version REGEX \"^#define MLX_VERSION_.*$\")\n  string(REGEX MATCH \"#define MLX_VERSION_MAJOR ([0-9]+)\" _ \"${_mlx_h_version}\")\n  set(_major ${CMAKE_MATCH_1})\n  string(REGEX MATCH \"#define MLX_VERSION_MINOR ([0-9]+)\" _ \"${_mlx_h_version}\")\n  set(_minor ${CMAKE_MATCH_1})\n  string(REGEX MATCH \"#define MLX_VERSION_PATCH ([0-9]+)\" _ \"${_mlx_h_version}\")\n  set(_patch ${CMAKE_MATCH_1})\n  set(MLX_PROJECT_VERSION \"${_major}.${_minor}.${_patch}\")\n  set(MLX_VERSION ${MLX_PROJECT_VERSION})\nelse()\n  string(REGEX REPLACE \"^([0-9]+\\.[0-9]+\\.[0-9]+).*\" \"\\\\1\" MLX_PROJECT_VERSION\n                       ${MLX_VERSION})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Creating New Axis with None in MLX\nDESCRIPTION: Demonstrates how to use None as an index to create a new axis in an MLX array, which is useful for expanding dimensions of arrays for broadcasting operations.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/indexing.rst#2025-04-19_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n>>> arr = mx.arange(8)\n>>> arr.shape\n[8]\n>>> arr[None].shape\n[1, 8]\n```\n\n----------------------------------------\n\nTITLE: Documenting Fast Operations in MLX Core Module\nDESCRIPTION: This reStructuredText snippet defines the documentation structure for the fast operations in the mlx.core.fast module. It includes autosummary directives for various functions such as RMS normalization, layer normalization, rotary position embedding, scaled dot-product attention, and metal kernel optimization.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/python/fast.rst#2025-04-19_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. _fast:\n\nFast\n====\n\n.. currentmodule:: mlx.core.fast\n\n.. autosummary:: \n  :toctree: _autosummary\n\n  rms_norm\n  layer_norm\n  rope\n  scaled_dot_product_attention\n  metal_kernel\n```\n\n----------------------------------------\n\nTITLE: Conditional Compilation of Additional Metal Kernels\nDESCRIPTION: Builds additional Metal kernels when MLX_METAL_JIT is not defined, including various mathematical operations and Steel-specific kernels.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/mlx/backend/metal/kernels/CMakeLists.txt#2025-04-19_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT MLX_METAL_JIT)\n  build_kernel(arange arange.h)\n  build_kernel(binary binary.h binary_ops.h)\n  build_kernel(binary_two binary_two.h)\n  build_kernel(copy copy.h)\n  build_kernel(fft fft.h fft/radix.h fft/readwrite.h)\n  build_kernel(\n    reduce\n    atomic.h\n    reduction/ops.h\n    reduction/reduce_init.h\n    reduction/reduce_all.h\n    reduction/reduce_col.h\n    reduction/reduce_row.h)\n  build_kernel(quantized quantized.h ${STEEL_HEADERS})\n  build_kernel(scan scan.h)\n  build_kernel(softmax softmax.h)\n  build_kernel(logsumexp logsumexp.h)\n  build_kernel(sort sort.h)\n  build_kernel(ternary ternary.h ternary_ops.h)\n  build_kernel(unary unary.h unary_ops.h)\n  build_kernel(steel/conv/kernels/steel_conv ${STEEL_HEADERS})\n  build_kernel(steel/conv/kernels/steel_conv_general ${STEEL_HEADERS})\n  build_kernel(steel/gemm/kernels/steel_gemm_fused ${STEEL_HEADERS})\n  build_kernel(steel/gemm/kernels/steel_gemm_gather ${STEEL_HEADERS})\n  build_kernel(steel/gemm/kernels/steel_gemm_masked ${STEEL_HEADERS})\n  build_kernel(steel/gemm/kernels/steel_gemm_splitk ${STEEL_HEADERS})\n  build_kernel(gemv_masked steel/utils.h)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring MLX Backend Components\nDESCRIPTION: Sets up the backend components of the MLX library, including common utilities and conditional compilation for CPU and Metal backends based on build options. Also adds subdirectories for distributed computing and IO operations.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/mlx/CMakeLists.txt#2025-04-19_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(${CMAKE_CURRENT_SOURCE_DIR}/backend/common)\n\nif(MLX_BUILD_CPU)\n  add_subdirectory(${CMAKE_CURRENT_SOURCE_DIR}/backend/cpu)\nelse()\n  add_subdirectory(${CMAKE_CURRENT_SOURCE_DIR}/backend/no_cpu)\nendif()\n\nadd_subdirectory(${CMAKE_CURRENT_SOURCE_DIR}/distributed)\nadd_subdirectory(${CMAKE_CURRENT_SOURCE_DIR}/io)\n\nif(MLX_BUILD_METAL)\n  add_subdirectory(${CMAKE_CURRENT_SOURCE_DIR}/backend/metal)\nelse()\n  add_subdirectory(${CMAKE_CURRENT_SOURCE_DIR}/backend/no_metal)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Building MLX Example Applications\nDESCRIPTION: Calls the build_example function for various MLX example programs including tutorial, linear regression, logistic regression, metal capture, and distributed computing examples.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/examples/cpp/CMakeLists.txt#2025-04-19_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nbuild_example(tutorial.cpp)\nbuild_example(linear_regression.cpp)\nbuild_example(logistic_regression.cpp)\nbuild_example(metal_capture.cpp)\nbuild_example(distributed.cpp)\n```\n\n----------------------------------------\n\nTITLE: Converting Between MLX and TensorFlow\nDESCRIPTION: Example of converting MLX arrays to TensorFlow tensors and back. TensorFlow requires explicit use of memoryview for the buffer protocol.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/numpy.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport mlx.core as mx\nimport tensorflow as tf\n\na = mx.arange(3)\nb = tf.constant(memoryview(a))\nc = mx.array(b)\n```\n\n----------------------------------------\n\nTITLE: MLX Export Functions Reference\nDESCRIPTION: ReStructuredText documentation defining the export-related function references in MLX core module, including export_function, import_function, exporter and export_to_dot functions.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/python/export.rst#2025-04-19_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. _export:\n\nExport Functions\n================\n\n.. currentmodule:: mlx.core\n\n.. autosummary::\n  :toctree: _autosummary\n\n   export_function\n   import_function\n   exporter\n   export_to_dot\n```\n\n----------------------------------------\n\nTITLE: Configuring Executable and Linking MLX\nDESCRIPTION: CMake commands to create an executable from the example C++ code and link it against the MLX library. This is the final step in the CMake configuration.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/dev/mlx_in_cpp.rst#2025-04-19_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nadd_executable(example example.cpp)\ntarget_link_libraries(example PRIVATE mlx)\n```\n\n----------------------------------------\n\nTITLE: Configuring MLX C++ Project with CMake\nDESCRIPTION: CMake configuration that sets up a C++ project with MLX library integration. It requires CMake 3.27 or higher, sets C++17 as the standard, finds Python 3.9+ dependencies, and configures MLX library linking.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/examples/cmake_project/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.27)\n\nproject(example LANGUAGES CXX)\n\nset(CMAKE_CXX_STANDARD 17)\nset(CMAKE_CXX_STANDARD_REQUIRED ON)\n\n# Comment the following two commands only the MLX C++ library is installed and\n# set(MLX_ROOT \"/path/to/mlx\") directly if needed.\nfind_package(\n  Python 3.9\n  COMPONENTS Interpreter Development.Module\n  REQUIRED)\nexecute_process(\n  COMMAND \"${Python_EXECUTABLE}\" -m mlx --cmake-dir\n  OUTPUT_STRIP_TRAILING_WHITESPACE\n  OUTPUT_VARIABLE MLX_ROOT)\n\nfind_package(MLX CONFIG REQUIRED)\n\nadd_executable(example example.cpp)\ntarget_link_libraries(example PRIVATE mlx)\n```\n\n----------------------------------------\n\nTITLE: Defining Build Targets for MLX Executables\nDESCRIPTION: Defines build targets for two executables: 'eval_mlp' and 'train_mlp'. Both targets are linked against the MLX library.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/examples/export/CMakeLists.txt#2025-04-19_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nadd_executable(eval_mlp eval_mlp.cpp)\ntarget_link_libraries(eval_mlp PRIVATE mlx)\n\nadd_executable(train_mlp train_mlp.cpp)\ntarget_link_libraries(train_mlp PRIVATE mlx)\n```\n\n----------------------------------------\n\nTITLE: Saving Multiple Arrays in MLX\nDESCRIPTION: Example of saving multiple arrays to a single .npz file using mx.savez(). Arrays can be passed with or without keywords.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/saving_and_loading.rst#2025-04-19_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n>>> a = mx.array([1.0])\n>>> b = mx.array([2.0])\n>>> mx.savez(\"arrays\", a, b=b)\n```\n\n----------------------------------------\n\nTITLE: Defining RST Documentation Structure for MLX Optimizers\nDESCRIPTION: ReStructuredText markup defining the documentation structure for MLX optimizers, including module reference and autosummary directives for optimizer classes.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/python/optimizers/common_optimizers.rst#2025-04-19_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _common_optimizers:\n\nCommon Optimizers\n=================\n\n.. currentmodule:: mlx.optimizers\n\n.. autosummary::\n   :toctree: _autosummary\n   :template: optimizers-template.rst\n\n   SGD\n   RMSprop\n   Adagrad\n   Adafactor\n   AdaDelta\n   Adam\n   AdamW\n   Adamax\n   Lion\n   MultiOptimizer\n```\n\n----------------------------------------\n\nTITLE: Generating Xcode Project with Metal Debugging for MLX\nDESCRIPTION: This snippet demonstrates how to create an Xcode project with Metal debugging enabled using CMake. It configures the build with MLX_METAL_DEBUG option and generates an Xcode project for easier debugging workflow.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/dev/metal_debugger.rst#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmkdir build && cd build\ncmake .. -DMLX_METAL_DEBUG=ON -G Xcode\nopen mlx.xcodeproj\n```\n\n----------------------------------------\n\nTITLE: Configuring MLX Library Source Files and Core Components\nDESCRIPTION: Defines the main source files for the MLX library, covering core functionality like arrays, operations, device management, and utilities. Also sets up the version information as a separate static library.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/mlx/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_sources(\n  mlx\n  PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/allocator.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/array.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/compile.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/device.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/dtype.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/dtype_utils.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/export.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/einsum.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/fast.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/fft.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/ops.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/graph_utils.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/primitives.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/random.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/scheduler.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/transforms.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/utils.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/linalg.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/backend/metal/metal.h)\n\n# Define MLX_VERSION only in the version.cpp file.\nadd_library(mlx_version STATIC ${CMAKE_CURRENT_SOURCE_DIR}/version.cpp)\ntarget_compile_definitions(mlx_version PRIVATE MLX_VERSION=\"${MLX_VERSION}\")\ntarget_link_libraries(mlx PRIVATE $<BUILD_INTERFACE:mlx_version>)\n```\n\n----------------------------------------\n\nTITLE: Conditional Compilation of JIT Metal Kernels Based on MLX_METAL_JIT Flag\nDESCRIPTION: Configures additional Metal kernel sources when MLX_METAL_JIT is enabled, including advanced operations like FFT, softmax, sorting, and GEMM operations. If disabled, the system uses pre-compiled kernels via nojit_kernels.cpp.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/mlx/backend/metal/CMakeLists.txt#2025-04-19_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nif(MLX_METAL_JIT)\n  target_sources(mlx PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/jit_kernels.cpp)\n  make_jit_source(arange)\n  make_jit_source(copy)\n  make_jit_source(unary)\n  make_jit_source(binary)\n  make_jit_source(binary_two)\n  make_jit_source(fft kernels/fft/radix.h kernels/fft/readwrite.h)\n  make_jit_source(logsumexp)\n  make_jit_source(ternary)\n  make_jit_source(softmax)\n  make_jit_source(scan)\n  make_jit_source(sort)\n  make_jit_source(\n    reduce kernels/reduction/reduce_all.h kernels/reduction/reduce_col.h\n    kernels/reduction/reduce_row.h kernels/reduction/reduce_init.h)\n  make_jit_source(\n    steel/gemm/gemm kernels/steel/utils.h kernels/steel/gemm/loader.h\n    kernels/steel/gemm/mma.h kernels/steel/gemm/params.h\n    kernels/steel/gemm/transforms.h)\n  make_jit_source(steel/gemm/kernels/steel_gemm_fused)\n  make_jit_source(steel/gemm/kernels/steel_gemm_masked kernels/steel/defines.h)\n  make_jit_source(steel/gemm/kernels/steel_gemm_gather)\n  make_jit_source(steel/gemm/kernels/steel_gemm_splitk)\n  make_jit_source(\n    steel/conv/conv\n    kernels/steel/utils.h\n    kernels/steel/defines.h\n    kernels/steel/gemm/mma.h\n    kernels/steel/gemm/transforms.h\n    kernels/steel/conv/params.h\n    kernels/steel/conv/loader.h\n    kernels/steel/conv/loaders/loader_channel_l.h\n    kernels/steel/conv/loaders/loader_channel_n.h)\n  make_jit_source(steel/conv/kernels/steel_conv)\n  make_jit_source(steel/conv/kernels/steel_conv_general kernels/steel/defines.h\n                  kernels/steel/conv/loaders/loader_general.h)\n  make_jit_source(quantized)\n  make_jit_source(gemv_masked)\nelse()\n  target_sources(mlx PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/nojit_kernels.cpp)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Pre-installing Requirements for Faster Development Builds\nDESCRIPTION: Installs the project requirements listed in requirements.txt file. This step can speed up subsequent builds during development.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/examples/extensions/README.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Importing MLX Core Device and Stream Functions\nDESCRIPTION: This code snippet shows the import statement for various device and stream related functions from the mlx.core module. It includes classes like Device and Stream, as well as functions for managing default devices and streams.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/python/devices_and_streams.rst#2025-04-19_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom mlx.core import Device, Stream, default_device, set_default_device, default_stream, new_stream, set_default_stream, stream, synchronize\n```\n\n----------------------------------------\n\nTITLE: Setting Metal Path and Configuring Kernel Subdirectory\nDESCRIPTION: Sets the path to Metal kernel files and adds the kernel subdirectory to the build. If MLX_METAL_PATH is not defined, it defaults to the kernels directory in the current binary directory. Also defines the METAL_PATH macro for the mlx target.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/mlx/backend/metal/CMakeLists.txt#2025-04-19_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT MLX_METAL_PATH)\n  set(MLX_METAL_PATH ${CMAKE_CURRENT_BINARY_DIR}/kernels/)\nendif()\n\nadd_subdirectory(${CMAKE_CURRENT_SOURCE_DIR}/kernels)\n\ntarget_compile_definitions(mlx\n                           PRIVATE METAL_PATH=\"${MLX_METAL_PATH}/mlx.metallib\")\n```\n\n----------------------------------------\n\nTITLE: Installation Configuration for MLX\nDESCRIPTION: Configures installation paths and targets for the MLX library, including library files and headers. It uses GNUInstallDirs module to determine appropriate installation directories based on the platform.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/CMakeLists.txt#2025-04-19_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\ninclude(GNUInstallDirs)\n\n# Install library\ninstall(\n  TARGETS mlx\n  EXPORT MLXTargets\n  LIBRARY DESTINATION ${CMAKE_INSTALL_LIBDIR}\n  ARCHIVE DESTINATION ${CMAKE_INSTALL_LIBDIR}\n  RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR}\n  INCLUDES\n  DESTINATION ${CMAKE_INSTALL_INCLUDEDIR})\n\n# Install headers\ninstall(\n  DIRECTORY ${CMAKE_CURRENT_LIST_DIR}/mlx\n  DESTINATION ${CMAKE_INSTALL_INCLUDEDIR}\n  COMPONENT headers\n  FILES_MATCHING\n  PATTERN \"*.h\"\n  PATTERN \"backend/metal/kernels.h\" EXCLUDE)\n```\n\n----------------------------------------\n\nTITLE: Conditional Source File Selection for MLX Build in CMake\nDESCRIPTION: Selects the appropriate source file for the MLX target based on build conditions. If MLX_BUILD_CPU is enabled and not on Windows, it includes 'ring.cpp'. Otherwise, it includes 'no_ring.cpp'.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/mlx/distributed/ring/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif(MLX_BUILD_CPU AND NOT WIN32)\n  target_sources(mlx PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/ring.cpp)\nelse()\n  target_sources(mlx PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/no_ring.cpp)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Running MLX Project Tests\nDESCRIPTION: Executes the project's test suite using the test.py script. This step is crucial for verifying the correctness of the implementation after building.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/examples/extensions/README.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython test.py\n```\n\n----------------------------------------\n\nTITLE: Installing MLX for Development with Dependencies\nDESCRIPTION: Command to install MLX in development mode with editable install and development dependencies for contributing to the project.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/install.rst#2025-04-19_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nCMAKE_BUILD_PARALLEL_LEVEL=8 pip install -e \".[dev]\"\n```\n\n----------------------------------------\n\nTITLE: Adding Core Source Files to MLX Target\nDESCRIPTION: Adds the main C++ source files that implement the core functionality of MLX to the build target. These files implement operations like binary operations, convolutions, matrix multiplication, and various neural network primitives.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/mlx/backend/metal/CMakeLists.txt#2025-04-19_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_sources(\n  mlx\n  PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/allocator.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/binary.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/compiled.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/conv.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/copy.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/custom_kernel.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/distributed.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/device.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/event.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/fence.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/fft.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/hadamard.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/indexing.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/logsumexp.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/matmul.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/scaled_dot_product_attention.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/metal.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/primitives.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/quantized.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/normalization.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/rope.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/scan.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/slicing.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/softmax.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/sort.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/reduce.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/ternary.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/unary.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/resident.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/utils.cpp)\n```\n\n----------------------------------------\n\nTITLE: Configuring MLX Build Options in CMake\nDESCRIPTION: Defines the available build options for the MLX library, including test building, examples, Python bindings, and backend selections. Each option is set with an appropriate default value.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/CMakeLists.txt#2025-04-19_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\noption(MLX_BUILD_TESTS \"Build tests for mlx\" ON)\noption(MLX_BUILD_EXAMPLES \"Build examples for mlx\" ON)\noption(MLX_BUILD_BENCHMARKS \"Build benchmarks for mlx\" OFF)\noption(MLX_BUILD_PYTHON_BINDINGS \"Build python bindings for mlx\" OFF)\noption(MLX_BUILD_METAL \"Build metal backend\" ON)\noption(MLX_BUILD_CPU \"Build cpu backend\" ON)\noption(MLX_METAL_DEBUG \"Enhance metal debug workflow\" OFF)\noption(MLX_ENABLE_X64_MAC \"Enable building for x64 macOS\" OFF)\noption(MLX_BUILD_GGUF \"Include support for GGUF format\" ON)\noption(MLX_BUILD_SAFETENSORS \"Include support for safetensors format\" ON)\noption(MLX_BUILD_BLAS_FROM_SOURCE \"Build OpenBLAS from source code\" OFF)\noption(MLX_METAL_JIT \"Use JIT compilation for Metal kernels\" OFF)\noption(BUILD_SHARED_LIBS \"Build mlx as a shared library\" OFF)\n```\n\n----------------------------------------\n\nTITLE: Conditionally Including MPI Source Files in MLX Project (CMake)\nDESCRIPTION: This CMake code snippet conditionally adds MPI-related source files to the 'mlx' target based on the MLX_BUILD_CPU flag. If MLX_BUILD_CPU is true, it includes 'mpi.cpp'; otherwise, it includes 'no_mpi.cpp'.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/mlx/distributed/mpi/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif(MLX_BUILD_CPU)\n  target_sources(mlx PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/mpi.cpp)\nelse()\n  target_sources(mlx PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/no_mpi.cpp)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining Base Headers for Metal Kernels in CMake\nDESCRIPTION: Sets up a list of base header files required for Metal kernel compilation, including version-specific and math-related headers.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/mlx/backend/metal/kernels/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(BASE_HEADERS\n    metal_3_1/bf16.h\n    metal_3_0/bf16.h\n    bf16_math.h\n    complex.h\n    defines.h\n    erf.h\n    expm1f.h\n    utils.h)\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies with Version Requirements\nDESCRIPTION: Specifies required Python packages and their version constraints for the mlx project. Includes setuptools, cmake, mlx core library, and nanobind with specific minimum or exact version requirements.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/examples/extensions/requirements.txt#2025-04-19_snippet_0\n\nLANGUAGE: requirements\nCODE:\n```\nsetuptools>=42\ncmake>=3.25\nmlx>=0.21.0\nnanobind==2.2.0\n```\n\n----------------------------------------\n\nTITLE: Configuring Test Executable and Sources\nDESCRIPTION: Creates test executable and configures source files including conditional Metal test sources. Links against MLX library and doctest, and sets up test discovery.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/tests/CMakeLists.txt#2025-04-19_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nadd_executable(tests ${PROJECT_SOURCE_DIR}/tests/tests.cpp)\n\nif(MLX_BUILD_METAL)\n  set(METAL_TEST_SOURCES metal_tests.cpp)\nendif()\n\ninclude(${doctest_SOURCE_DIR}/scripts/cmake/doctest.cmake)\n\ntarget_sources(\n  tests\n  PRIVATE allocator_tests.cpp\n          array_tests.cpp\n          arg_reduce_tests.cpp\n          autograd_tests.cpp\n          blas_tests.cpp\n          compile_tests.cpp\n          custom_vjp_tests.cpp\n          creations_tests.cpp\n          device_tests.cpp\n          einsum_tests.cpp\n          export_import_tests.cpp\n          eval_tests.cpp\n          fft_tests.cpp\n          load_tests.cpp\n          ops_tests.cpp\n          random_tests.cpp\n          scheduler_tests.cpp\n          utils_tests.cpp\n          vmap_tests.cpp\n          linalg_tests.cpp\n          ${METAL_TEST_SOURCES})\n\ntarget_link_libraries(tests PRIVATE mlx doctest)\ndoctest_discover_tests(tests)\nadd_test(NAME tests COMMAND tests)\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake Build Function for MLX Examples\nDESCRIPTION: Defines a CMake function that creates executable targets for MLX example programs. The function takes a source file as input, creates a target name from the filename, and links it against the MLX library.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/examples/cpp/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfunction(build_example SRCFILE)\n  get_filename_component(src_name ${SRCFILE} NAME_WE)\n  set(target \"${src_name}\")\n  add_executable(${target} ${SRCFILE})\n  target_link_libraries(${target} PRIVATE mlx)\nendfunction(build_example)\n```\n\n----------------------------------------\n\nTITLE: Finding MLX Package\nDESCRIPTION: CMake command to locate the MLX package. This will set various MLX-related variables that can be used in the build configuration.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/dev/mlx_in_cpp.rst#2025-04-19_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nfind_package(MLX CONFIG REQUIRED)\n```\n\n----------------------------------------\n\nTITLE: Defining build_kernel_base Function for Metal Kernel Compilation\nDESCRIPTION: Creates a CMake function to build Metal kernel base, setting compiler flags, handling debug options, and specifying version-specific includes.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/mlx/backend/metal/kernels/CMakeLists.txt#2025-04-19_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(build_kernel_base TARGET SRCFILE DEPS)\n  set(METAL_FLAGS -Wall -Wextra -fno-fast-math -Wno-c++17-extensions)\n  if(MLX_METAL_DEBUG)\n    set(METAL_FLAGS ${METAL_FLAGS} -gline-tables-only -frecord-sources)\n  endif()\n  if(NOT CMAKE_OSX_DEPLOYMENT_TARGET STREQUAL \"\")\n    set(METAL_FLAGS ${METAL_FLAGS}\n                    \"-mmacosx-version-min=${CMAKE_OSX_DEPLOYMENT_TARGET}\")\n  endif()\n  if(MLX_METAL_VERSION GREATER_EQUAL 310)\n    set(VERSION_INCLUDES\n        ${PROJECT_SOURCE_DIR}/mlx/backend/metal/kernels/metal_3_1)\n  else()\n    set(VERSION_INCLUDES\n        ${PROJECT_SOURCE_DIR}/mlx/backend/metal/kernels/metal_3_0)\n  endif()\n  add_custom_command(\n    COMMAND xcrun -sdk macosx metal ${METAL_FLAGS} -c ${SRCFILE}\n            -I${PROJECT_SOURCE_DIR} -I${VERSION_INCLUDES} -o ${TARGET}.air\n    DEPENDS ${SRCFILE} ${DEPS} ${BASE_HEADERS}\n    OUTPUT ${TARGET}.air\n    COMMENT \"Building ${TARGET}.air\"\n    VERBATIM)\nendfunction(build_kernel_base)\n```\n\n----------------------------------------\n\nTITLE: Checking Native Python Architecture\nDESCRIPTION: Command to verify if Python is running natively on ARM architecture. Output should be 'arm' for native Apple Silicon support.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/install.rst#2025-04-19_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npython -c \"import platform; print(platform.processor())\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Doctest with CMake\nDESCRIPTION: Sets up doctest testing framework using FetchContent to download and configure from GitHub repository. Specifies minimum CMake version requirement of 3.5.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/tests/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\n# Doctest works fine with cmake 3.5\nset(CMAKE_POLICY_VERSION_MINIMUM 3.5)\n\nFetchContent_Declare(\n  doctest\n  GIT_REPOSITORY \"https://github.com/onqtam/doctest\"\n  GIT_TAG \"ae7a13539fb71f270b87eb2e874fbac80bc8dda2\")\nFetchContent_MakeAvailable(doctest)\n```\n\n----------------------------------------\n\nTITLE: Installing CMake Configuration Files for MLX Project\nDESCRIPTION: Sets up and installs CMake configuration files that allow other projects to find and use MLX as a dependency. This includes exporting targets, setting version information, and configuring package files.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/CMakeLists.txt#2025-04-19_snippet_11\n\nLANGUAGE: cmake\nCODE:\n```\n# Install cmake config\nset(MLX_CMAKE_BUILD_CONFIG ${CMAKE_BINARY_DIR}/MLXConfig.cmake)\nset(MLX_CMAKE_BUILD_VERSION_CONFIG ${CMAKE_BINARY_DIR}/MLXConfigVersion.cmake)\nset(MLX_CMAKE_INSTALL_MODULE_DIR share/cmake/MLX)\n\ninstall(\n  EXPORT MLXTargets\n  FILE MLXTargets.cmake\n  DESTINATION ${MLX_CMAKE_INSTALL_MODULE_DIR})\n\ninclude(CMakePackageConfigHelpers)\n\nwrite_basic_package_version_file(\n  ${MLX_CMAKE_BUILD_VERSION_CONFIG}\n  COMPATIBILITY SameMajorVersion\n  VERSION ${MLX_VERSION})\n\nconfigure_package_config_file(\n  ${CMAKE_CURRENT_LIST_DIR}/mlx.pc.in ${MLX_CMAKE_BUILD_CONFIG}\n  INSTALL_DESTINATION ${MLX_CMAKE_INSTALL_MODULE_DIR}\n  NO_CHECK_REQUIRED_COMPONENTS_MACRO\n  PATH_VARS CMAKE_INSTALL_LIBDIR CMAKE_INSTALL_INCLUDEDIR\n            MLX_CMAKE_INSTALL_MODULE_DIR)\n\ninstall(FILES ${MLX_CMAKE_BUILD_CONFIG} ${MLX_CMAKE_BUILD_VERSION_CONFIG}\n        DESTINATION ${MLX_CMAKE_INSTALL_MODULE_DIR})\n\ninstall(DIRECTORY ${CMAKE_MODULE_PATH}/\n        DESTINATION ${MLX_CMAKE_INSTALL_MODULE_DIR})\n```\n\n----------------------------------------\n\nTITLE: Running the MLX Example\nDESCRIPTION: Command to run the compiled MLX example program from the build directory.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/dev/mlx_in_cpp.rst#2025-04-19_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n./build/example\n```\n\n----------------------------------------\n\nTITLE: Building Metal Library for MLX Extension\nDESCRIPTION: This CMake snippet demonstrates how to build a Metal library for an MLX extension using the provided mlx_build_metallib function. It includes setting up the target, sources, and dependencies.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/dev/extensions.rst#2025-04-19_snippet_16\n\nLANGUAGE: CMake\nCODE:\n```\n# Build metallib\nif(MLX_BUILD_METAL)\n\nmlx_build_metallib(\n    TARGET mlx_ext_metallib\n    TITLE mlx_ext\n    SOURCES ${CMAKE_CURRENT_LIST_DIR}/axpby/axpby.metal\n    INCLUDE_DIRS ${PROJECT_SOURCE_DIR} ${MLX_INCLUDE_DIRS}\n    OUTPUT_DIRECTORY ${CMAKE_LIBRARY_OUTPUT_DIRECTORY}\n)\n\nadd_dependencies(\n    mlx_ext\n    mlx_ext_metallib\n)\n\nendif()\n```\n\n----------------------------------------\n\nTITLE: Installing Xcode Command Line Tools\nDESCRIPTION: Command to install Xcode command line tools, needed for building MLX from source.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/install.rst#2025-04-19_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\nxcode-select --install\n```\n\n----------------------------------------\n\nTITLE: Configuring nanobind module for MLX Python bindings in CMake\nDESCRIPTION: Defines a nanobind module named 'core' that includes various MLX source files. This configuration sets up the Python bindings with specific build options like STABLE_ABI and LTO.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/python/src/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nnanobind_add_module(\n  core\n  NB_STATIC\n  STABLE_ABI\n  LTO\n  NOMINSIZE\n  NB_DOMAIN\n  mlx\n  ${CMAKE_CURRENT_SOURCE_DIR}/mlx.cpp\n  ${CMAKE_CURRENT_SOURCE_DIR}/array.cpp\n  ${CMAKE_CURRENT_SOURCE_DIR}/convert.cpp\n  ${CMAKE_CURRENT_SOURCE_DIR}/device.cpp\n  ${CMAKE_CURRENT_SOURCE_DIR}/distributed.cpp\n  ${CMAKE_CURRENT_SOURCE_DIR}/export.cpp\n  ${CMAKE_CURRENT_SOURCE_DIR}/fast.cpp\n  ${CMAKE_CURRENT_SOURCE_DIR}/fft.cpp\n  ${CMAKE_CURRENT_SOURCE_DIR}/indexing.cpp\n  ${CMAKE_CURRENT_SOURCE_DIR}/load.cpp\n  ${CMAKE_CURRENT_SOURCE_DIR}/metal.cpp\n  ${CMAKE_CURRENT_SOURCE_DIR}/memory.cpp\n  ${CMAKE_CURRENT_SOURCE_DIR}/mlx_func.cpp\n  ${CMAKE_CURRENT_SOURCE_DIR}/ops.cpp\n  ${CMAKE_CURRENT_SOURCE_DIR}/stream.cpp\n  ${CMAKE_CURRENT_SOURCE_DIR}/transforms.cpp\n  ${CMAKE_CURRENT_SOURCE_DIR}/random.cpp\n  ${CMAKE_CURRENT_SOURCE_DIR}/linalg.cpp\n  ${CMAKE_CURRENT_SOURCE_DIR}/constants.cpp\n  ${CMAKE_CURRENT_SOURCE_DIR}/trees.cpp\n  ${CMAKE_CURRENT_SOURCE_DIR}/utils.cpp)\n```\n\n----------------------------------------\n\nTITLE: Basic CMake Configuration for MLX Project\nDESCRIPTION: Initial CMake configuration establishing the project requirements. Sets the minimum CMake version, project name, and C++ standard to be used in the project.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/dev/mlx_in_cpp.rst#2025-04-19_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.27)\n\nproject(example LANGUAGES CXX)\n\nset(CMAKE_CXX_STANDARD 17)\nset(CMAKE_CXX_STANDARD_REQUIRED ON)\n```\n\n----------------------------------------\n\nTITLE: Setting Custom MLX Root Path\nDESCRIPTION: CMake code to manually set the MLX installation path when installed in a non-standard location or when CMake can't find it automatically.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/dev/mlx_in_cpp.rst#2025-04-19_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nset(MLX_ROOT \"/path/to/mlx/\")\n```\n\n----------------------------------------\n\nTITLE: Disabling Compilation for Debugging\nDESCRIPTION: Demonstrates how to disable compilation globally to enable array inspection and debugging within compiled functions.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/compile.rst#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@mx.compile\ndef fun(x):\n    z = -x\n    print(z) # Okay\n    return mx.exp(z)\n\nmx.disable_compile()\nfun(mx.array(5.0))\n```\n\n----------------------------------------\n\nTITLE: Setting Xcode Developer Path\nDESCRIPTION: Command to set the active Xcode developer directory path for the system, required if Metal tools aren't found in PATH.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/install.rst#2025-04-19_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\nsudo xcode-select --switch /Applications/Xcode.app/Contents/Developer\n```\n\n----------------------------------------\n\nTITLE: Running C++ MLX Tests\nDESCRIPTION: Command to run the tests for the C++ MLX library after building.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/install.rst#2025-04-19_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\nmake test\n```\n\n----------------------------------------\n\nTITLE: Linear Algebra Module Documentation in RST\nDESCRIPTION: ReStructuredText documentation that defines the linear algebra operations available in MLX core's linalg module. Lists functions including matrix inversion, decompositions, eigenvalue calculations, and linear system solvers.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/python/linalg.rst#2025-04-19_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _linalg:\n\nLinear Algebra\n==============\n\n.. currentmodule:: mlx.core.linalg\n\n.. autosummary::\n   :toctree: _autosummary\n\n    inv\n    tri_inv\n    norm\n    cholesky\n    cholesky_inv\n    cross\n    qr\n    svd\n    eigvalsh\n    eigh\n    lu\n    lu_factor\n    pinv\n    solve\n    solve_triangular\n```\n\n----------------------------------------\n\nTITLE: Verifying CMake System Architecture\nDESCRIPTION: Command to check if CMake is correctly detecting the ARM architecture of the system.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/install.rst#2025-04-19_snippet_18\n\nLANGUAGE: shell\nCODE:\n```\ncmake --system-information | grep CMAKE_HOST_SYSTEM_PROCESSOR\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Index for MLX Transforms\nDESCRIPTION: ReStructuredText documentation defining the transforms module index with autosummary directives for core MLX transformation functions.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/python/transforms.rst#2025-04-19_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _transforms:\n\nTransforms\n==========\n\n.. currentmodule:: mlx.core\n\n.. autosummary::\n  :toctree: _autosummary\n\n   eval\n   async_eval\n   compile\n   custom_function\n   disable_compile\n   enable_compile\n   grad\n   value_and_grad\n   jvp\n   vjp\n   vmap\n```\n\n----------------------------------------\n\nTITLE: Formatting Python Code with Black\nDESCRIPTION: Command to format Python source files using the Black code formatter.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/CONTRIBUTING.md#2025-04-19_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nblack file.py\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies\nDESCRIPTION: Installs the required Python packages for building the documentation using pip. Dependencies are specified in the requirements.txt file.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/README.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Incorrect Compilation Pattern in Loops\nDESCRIPTION: Demonstrates an anti-pattern where compiling an anonymous function inside a loop leads to repeated compilations.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/compile.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\na = mx.array(1.0)\n# Don't do this, compiles lambda at each iteration\nfor _ in range(5):\n    mx.compile(lambda x: mx.exp(mx.abs(x)))(a)\n```\n\n----------------------------------------\n\nTITLE: Referencing MLX Core Memory Management Functions in reStructuredText\nDESCRIPTION: This reStructuredText (RST) code creates documentation references for memory management functions available in the MLX Core library. It uses autosummary to generate documentation for functions that handle memory tracking, limiting, and cache management.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/python/memory_management.rst#2025-04-19_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. currentmodule:: mlx.core\n\n.. autosummary::\n  :toctree: _autosummary\n\n  get_active_memory\n  get_peak_memory\n  reset_peak_memory\n  get_cache_memory\n  set_memory_limit\n  set_cache_limit\n  set_wired_limit\n  clear_cache\n```\n\n----------------------------------------\n\nTITLE: Configuring MLX for Minimal Binary Size\nDESCRIPTION: CMake command with various flags to optimize MLX for minimal binary size by disabling unnecessary components and enabling JIT compilation.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/install.rst#2025-04-19_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\ncmake .. \\\n  -DCMAKE_BUILD_TYPE=MinSizeRel \\\n  -DBUILD_SHARED_LIBS=ON \\\n  -DMLX_BUILD_CPU=OFF \\\n  -DMLX_BUILD_SAFETENSORS=OFF \\\n  -DMLX_BUILD_GGUF=OFF \\\n  -DMLX_METAL_JIT=ON\n```\n\n----------------------------------------\n\nTITLE: Configuring Common Metal Kernel Sources for MLX\nDESCRIPTION: Applies the make_jit_source function to generate C++ source files for common kernel operations like utilities, unary operations, binary operations, and others. Each call specifies the source file and its dependencies.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/mlx/backend/metal/CMakeLists.txt#2025-04-19_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nmake_jit_source(\n  utils\n  kernels/jit/bf16.h\n  kernels/metal_3_0/bf16.h\n  kernels/metal_3_1/bf16.h\n  kernels/bf16_math.h\n  kernels/complex.h\n  kernels/defines.h)\nmake_jit_source(unary_ops kernels/erf.h kernels/expm1f.h)\nmake_jit_source(binary_ops)\nmake_jit_source(ternary_ops)\nmake_jit_source(reduce_utils kernels/atomic.h kernels/reduction/ops.h)\nmake_jit_source(scatter kernels/indexing.h)\nmake_jit_source(gather kernels/indexing.h)\nmake_jit_source(gather_axis)\nmake_jit_source(scatter_axis)\nmake_jit_source(hadamard)\n```\n\n----------------------------------------\n\nTITLE: Installing MLX using pip\nDESCRIPTION: Command to install MLX using pip package manager.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/dev/mlx_in_cpp.rst#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -U mlx\n```\n\n----------------------------------------\n\nTITLE: Gradient Computation with Nested Parameters\nDESCRIPTION: Shows how to compute gradients with respect to parameters stored in nested Python containers.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/function_transforms.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef loss_fn(params, x, y):\n   w, b = params[\"weight\"], params[\"bias\"]\n   h = w * x + b\n   return mx.mean(mx.square(h - y))\n\nparams = {\"weight\": mx.array(1.0), \"bias\": mx.array(0.0)}\nx = mx.array([0.5, -0.5])\ny = mx.array([1.5, -1.5])\n\n# Computes the gradient of loss_fn with respect to both the\n# weight and bias:\ngrad_fn = mx.grad(loss_fn)\ngrads = grad_fn(params, x, y)\n\n# Prints\n# {'weight': array(-1, dtype=float32), 'bias': array(0, dtype=float32)}\nprint(grads)\n```\n\n----------------------------------------\n\nTITLE: Running C++ program to import and execute MLP training\nDESCRIPTION: This command runs the C++ program that imports and executes the previously exported MLP initialization and training functions.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/examples/export/README.md#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n./build/train_mlp\n```\n\n----------------------------------------\n\nTITLE: Installing MLX with conda from conda-forge\nDESCRIPTION: Command to install MLX using conda package manager from the conda-forge channel.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/install.rst#2025-04-19_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nconda install conda-forge::mlx\n```\n\n----------------------------------------\n\nTITLE: Finding MLX Package Location via Python\nDESCRIPTION: CMake code to locate the MLX installation when installed via Python. It finds the Python interpreter and uses it to determine the directory where MLX is installed by querying the MLX module.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/dev/mlx_in_cpp.rst#2025-04-19_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nfind_package(\n  Python 3.9\n  COMPONENTS Interpreter Development.Module\n  REQUIRED)\nexecute_process(\n  COMMAND \"${Python_EXECUTABLE}\" -m mlx --cmake-dir\n  OUTPUT_STRIP_TRAILING_WHITESPACE\n  OUTPUT_VARIABLE MLX_ROOT)\n```\n\n----------------------------------------\n\nTITLE: Running C++ Example\nDESCRIPTION: Command to execute the compiled C++ example from the build directory.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/examples/cmake_project/README.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n./build/example\n```\n\n----------------------------------------\n\nTITLE: Configuring output properties for MLX core module in CMake\nDESCRIPTION: Sets target properties for the 'core' module to ensure consistent output directory across different build configurations. This prevents multi-config generators like MSVC and XCode from placing outputs in subdirectories.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/python/src/CMakeLists.txt#2025-04-19_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nset_target_properties(\n  core\n  PROPERTIES LIBRARY_OUTPUT_DIRECTORY ${MLX_PYTHON_BINDINGS_OUTPUT_DIRECTORY}\n             # Do not append a sub-dir for multi-config generators like MSVC\n             # and XCode.\n             LIBRARY_OUTPUT_DIRECTORY_RELEASE\n             ${MLX_PYTHON_BINDINGS_OUTPUT_DIRECTORY}\n             LIBRARY_OUTPUT_DIRECTORY_DEBUG\n             ${MLX_PYTHON_BINDINGS_OUTPUT_DIRECTORY}\n             LIBRARY_OUTPUT_DIRECTORY_RELWITHDEBINFO\n             ${MLX_PYTHON_BINDINGS_OUTPUT_DIRECTORY}\n             LIBRARY_OUTPUT_DIRECTORY_MINSIZEREL\n             ${MLX_PYTHON_BINDINGS_OUTPUT_DIRECTORY})\n```\n\n----------------------------------------\n\nTITLE: Running Python script to export MLP evaluation function\nDESCRIPTION: This command executes the Python script that exports the evaluation function for the MLP model.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/examples/export/README.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython eval_mlp.py\n```\n\n----------------------------------------\n\nTITLE: Installing MLX with pip on Apple Silicon\nDESCRIPTION: Simple command to install MLX from PyPI. Requires Apple Silicon (M series chip), Python >= 3.9, and macOS >= 13.5.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/install.rst#2025-04-19_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install mlx\n```\n\n----------------------------------------\n\nTITLE: Installing MLX using pip\nDESCRIPTION: This command installs MLX version 0.22 or higher using pip package manager.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/examples/export/README.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install mlx>=0.22\n```\n\n----------------------------------------\n\nTITLE: Expected Output from C++ Example\nDESCRIPTION: The expected output from running the C++ example, showing an array with dtype int32.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/examples/cmake_project/README.md#2025-04-19_snippet_3\n\nLANGUAGE: text\nCODE:\n```\narray([2, 4, 6], dtype=int32)\n```\n\n----------------------------------------\n\nTITLE: Setting Python bindings output directory in CMake\nDESCRIPTION: Configures the output directory for the MLX Python bindings. If not explicitly specified, it falls back to either the project binary directory or the global library output directory.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/python/src/CMakeLists.txt#2025-04-19_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT MLX_PYTHON_BINDINGS_OUTPUT_DIRECTORY)\n  if(NOT CMAKE_LIBRARY_OUTPUT_DIRECTORY)\n    set(MLX_PYTHON_BINDINGS_OUTPUT_DIRECTORY ${PROJECT_BINARY_DIR})\n  else()\n    set(MLX_PYTHON_BINDINGS_OUTPUT_DIRECTORY ${CMAKE_LIBRARY_OUTPUT_DIRECTORY})\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Building C++ Example with CMake\nDESCRIPTION: Commands to build the C++ example using CMake. It creates a build directory and compiles the project in Release mode.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/examples/cmake_project/README.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncmake -B build -DCMAKE_BUILD_TYPE=Release\ncmake --build build\n```\n\n----------------------------------------\n\nTITLE: Running Python Unit Tests for MLX\nDESCRIPTION: Command to run the Python unit tests for MLX using the unittest discovery mode.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/install.rst#2025-04-19_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\npython -m unittest discover python/tests\n```\n\n----------------------------------------\n\nTITLE: Building C++ examples using CMake\nDESCRIPTION: These commands use CMake to configure and build the C++ examples in Release mode.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/examples/export/README.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncmake -B build -DCMAKE_BUILD_TYPE=Release\ncmake --build build\n```\n\n----------------------------------------\n\nTITLE: Running Python script to export MLP initialization and training functions\nDESCRIPTION: This command executes the Python script that exports the model initialization and training functions for the MLP.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/examples/export/README.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython train_mlp.py\n```\n\n----------------------------------------\n\nTITLE: Class Documentation Template with Method Filtering in Jinja2\nDESCRIPTION: A Jinja2 template that generates documentation for a Python class. It creates a class header with the full name, sets the current module context, and generates an autosummary of methods, specifically excluding inherited members and the __init__ method.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/_templates/nn-module-template.rst#2025-04-19_snippet_0\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ fullname | escape | underline}}\n\n.. currentmodule:: {{ module }}\n\n.. autoclass:: {{ objname }}\n\n   {% block methods %}\n\n   {% if methods %}\n   .. rubric:: {{ _('Methods') }}\n\n   .. autosummary::\n   {% for item in methods %}\n      {%- if item not in inherited_members and item != \"__init__\" %}\n         ~{{ name }}.{{ item }}\n      {%- endif %}\n   {%- endfor %}\n   {% endif %}\n   {% endblock %}\n\n\n```\n\n----------------------------------------\n\nTITLE: Setting Xcode Developer Directory\nDESCRIPTION: Command to specify which Xcode installation to use when building MLX by setting the DEVELOPER_DIR environment variable.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/install.rst#2025-04-19_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\nexport DEVELOPER_DIR=\"/path/to/Xcode.app/Contents/Developer/\"\n```\n\n----------------------------------------\n\nTITLE: Formatting C++ Code with clang-format\nDESCRIPTION: Command to format C++ source files using clang-format with in-place modification.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/CONTRIBUTING.md#2025-04-19_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nclang-format -i file.cpp\n```\n\n----------------------------------------\n\nTITLE: Running C++ program to import and execute MLP evaluation\nDESCRIPTION: This command runs the C++ program that imports and executes the previously exported MLP evaluation function.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/examples/export/README.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n./build/eval_mlp\n```\n\n----------------------------------------\n\nTITLE: Running Benchmark Example in Shell\nDESCRIPTION: An example command for running a benchmark to measure the time it takes to sum across a specific axis of a tensor on the CPU using the bench_mlx.py script.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/benchmarks/python/comparative/README.md#2025-04-19_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npython bench_mlx.py sum_axis --size 8x1024x128 --axis 2 --cpu\n```\n\n----------------------------------------\n\nTITLE: Generating Python Class Documentation with Jinja2 and Sphinx\nDESCRIPTION: This Jinja2 template creates documentation for a Python class. It includes the class name, module, and a list of methods. The template uses Sphinx directives to format the output and create proper documentation structure.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/_templates/optimizers-template.rst#2025-04-19_snippet_0\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ fullname | escape | underline}}\n\n.. currentmodule:: {{ module }}\n\n.. autoclass:: {{ objname }}\n\n   {% block methods %}\n\n   {% if methods %}\n   .. rubric:: {{ _('Methods') }}\n\n   .. autosummary::\n   {% for item in methods %}\n      {%- if item not in inherited_members %}\n         ~{{ name }}.{{ item }}\n      {%- endif %}\n   {%- endfor %}\n   {% endif %}\n   {% endblock %}\n```\n\n----------------------------------------\n\nTITLE: Checking System Architecture\nDESCRIPTION: Command to check if the system is running natively on ARM architecture, should return 'arm' for Apple Silicon.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/install.rst#2025-04-19_snippet_17\n\nLANGUAGE: shell\nCODE:\n```\nuname -p\n```\n\n----------------------------------------\n\nTITLE: Building MLX Project Extensions In-Place\nDESCRIPTION: Builds the project extensions in-place using setup.py. The -j8 flag enables parallel compilation with 8 jobs, which can speed up the build process.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/examples/extensions/README.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython setup.py build_ext -j8 --inplace\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake Project for MLX\nDESCRIPTION: Sets up a CMake project for building MLX-based applications. It specifies the minimum CMake version, project name, C++ standard, and finds the required Python environment and MLX package.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/examples/export/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.27)\n\nproject(import_mlx LANGUAGES CXX)\n\nset(CMAKE_CXX_STANDARD 17)\nset(CMAKE_CXX_STANDARD_REQUIRED ON)\n\nfind_package(\n  Python 3.9\n  COMPONENTS Interpreter Development.Module\n  REQUIRED)\nexecute_process(\n  COMMAND \"${Python_EXECUTABLE}\" -m mlx --cmake-dir\n  OUTPUT_STRIP_TRAILING_WHITESPACE\n  OUTPUT_VARIABLE MLX_ROOT)\nfind_package(MLX CONFIG REQUIRED)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking with PID for Debugging\nDESCRIPTION: Example of running a benchmark with the --print-pid flag to facilitate attaching a debugger. This prints the process ID and waits for user input before proceeding.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/benchmarks/python/comparative/README.md#2025-04-19_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n--print-pid\n```\n\n----------------------------------------\n\nTITLE: FFT Module Reference in RST\nDESCRIPTION: ReStructuredText documentation defining the FFT module reference structure and available functions in the mlx.core.fft module. Lists both complex and real-valued FFT operations in multiple dimensions.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/python/fft.rst#2025-04-19_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _fft:\n\nFFT\n===\n\n.. currentmodule:: mlx.core.fft\n\n.. autosummary:: \n  :toctree: _autosummary\n\n  fft\n  ifft\n  fft2\n  ifft2\n  fftn\n  ifftn\n  rfft\n  irfft\n  rfft2\n  irfft2\n  rfftn\n  irfftn\n```\n\n----------------------------------------\n\nTITLE: MLX Core Fast Module Imports\nDESCRIPTION: Import statement for MLX fast module including core array components.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/python/mlx/_stub_patterns.txt#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom mlx.core import array, Dtype, Device, Stream\nfrom typing import Sequence, Optional, Union\n```\n\n----------------------------------------\n\nTITLE: Installing Doxygen with Homebrew\nDESCRIPTION: Installs Doxygen documentation generator using Homebrew package manager. This is a one-time setup step required before building the documentation.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/README.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbrew install doxygen\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake for Python Bindings\nDESCRIPTION: This CMake snippet shows how to set up the build process for Python bindings using nanobind. It includes creating the module, linking libraries, and setting up runtime paths.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/dev/extensions.rst#2025-04-19_snippet_17\n\nLANGUAGE: CMake\nCODE:\n```\nnanobind_add_module(\n  _ext\n  NB_STATIC STABLE_ABI LTO NOMINSIZE\n  NB_DOMAIN mlx\n  ${CMAKE_CURRENT_LIST_DIR}/bindings.cpp\n)\ntarget_link_libraries(_ext PRIVATE mlx_ext)\n\nif(BUILD_SHARED_LIBS)\n  target_link_options(_ext PRIVATE -Wl,-rpath,@loader_path)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring MLX Source Files in CMake\nDESCRIPTION: Adds source files to the MLX target, including conditional compilation for SafeTensors and GGUF support. It also handles the download and integration of the GGUF library when enabled.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/mlx/io/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_sources(mlx PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/load.cpp)\n\nif(MLX_BUILD_SAFETENSORS)\n  target_sources(mlx PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/safetensors.cpp)\nelse()\n  target_sources(mlx PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/no_safetensors.cpp)\nendif()\n\nif(MLX_BUILD_GGUF)\n  message(STATUS \"Downloading gguflib\")\n  FetchContent_Declare(\n    gguflib\n    GIT_REPOSITORY https://github.com/antirez/gguf-tools/\n    GIT_TAG af7d88d808a7608a33723fba067036202910acb3)\n  FetchContent_MakeAvailable(gguflib)\n  target_include_directories(mlx\n                             PRIVATE $<BUILD_INTERFACE:${gguflib_SOURCE_DIR}>)\n  add_library(gguflib STATIC ${gguflib_SOURCE_DIR}/fp16.c\n                             ${gguflib_SOURCE_DIR}/gguflib.c)\n  target_link_libraries(mlx PRIVATE $<BUILD_INTERFACE:gguflib>)\n  target_sources(mlx PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/gguf.cpp\n                             ${CMAKE_CURRENT_SOURCE_DIR}/gguf_quants.cpp)\nelse()\n  target_sources(mlx PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/no_gguf.cpp)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring setuptools for MLX Extension Package\nDESCRIPTION: This Python script sets up the build process for an MLX extension package using setuptools. It includes package metadata, extension modules, and build requirements.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/dev/extensions.rst#2025-04-19_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\nfrom mlx import extension\nfrom setuptools import setup\n\nif __name__ == \"__main__\":\n    setup(\n        name=\"mlx_sample_extensions\",\n        version=\"0.0.0\",\n        description=\"Sample C++ and Metal extensions for MLX primitives.\",\n        ext_modules=[extension.CMakeExtension(\"mlx_sample_extensions._ext\")],\n        cmdclass={\"build_ext\": extension.CMakeBuild},\n        packages=[\"mlx_sample_extensions\"],\n        package_data={\"mlx_sample_extensions\": [\"*.so\", \"*.dylib\", \"*.metallib\"]},\n        extras_require={\"dev\":[]},\n        zip_safe=False,\n        python_requires=\">=3.8\",\n    )\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure for MLX Tree Utils\nDESCRIPTION: ReStructuredText documentation defining the structure and autosummary for MLX tree utility functions. Includes a note about dictionary key requirements and lists the main tree manipulation functions.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/python/tree_utils.rst#2025-04-19_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _utils:\n\nTree Utils\n==========\n\nIn MLX we consider a python tree to be an arbitrarily nested collection of\ndictionaries, lists and tuples without cycles. Functions in this module that\nreturn python trees will be using the default python ``dict``, ``list`` and\n``tuple`` but they can usually process objects that inherit from any of these.\n\n.. note::\n   Dictionaries should have keys that are valid python identifiers.\n\n.. currentmodule:: mlx.utils\n\n.. autosummary:: \n  :toctree: _autosummary\n\n   tree_flatten\n   tree_unflatten\n   tree_map\n   tree_map_with_path\n   tree_reduce\n```\n\n----------------------------------------\n\nTITLE: Building MLX Benchmark Targets\nDESCRIPTION: Builds multiple benchmark executables using the build_benchmark function. Creates targets for testing single operations, irregular strides, device comparisons, and autograd functionality.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/benchmarks/cpp/CMakeLists.txt#2025-04-19_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nbuild_benchmark(single_ops.cpp)\nbuild_benchmark(irregular_strides.cpp)\nbuild_benchmark(compare_devices.cpp)\nbuild_benchmark(autograd.cpp)\n```\n\n----------------------------------------\n\nTITLE: Building MLX Documentation\nDESCRIPTION: Runs Doxygen to generate API documentation and then uses make to build HTML documentation. This command should be executed from the mlx/docs/ directory.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/README.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndoxygen && make html\n```\n\n----------------------------------------\n\nTITLE: Platform and Processor Detection Logic for MLX\nDESCRIPTION: Checks the system processor and platform to determine build compatibility. It includes special handling for x86_64 on macOS, since MLX is primarily designed for Apple silicon systems.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/CMakeLists.txt#2025-04-19_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nmessage(\n  STATUS\n    \"Building MLX for ${CMAKE_SYSTEM_PROCESSOR} processor on ${CMAKE_SYSTEM_NAME}\"\n)\n\nif(${CMAKE_SYSTEM_NAME} MATCHES \"Darwin\")\n  if(${CMAKE_SYSTEM_PROCESSOR} MATCHES \"x86_64\")\n    if(NOT MLX_ENABLE_X64_MAC)\n      message(\n        FATAL_ERROR\n          \"Building for x86_64 on macOS is not supported.\"\n          \" If you are on an Apple silicon system, check the build\"\n          \" documentation for possible fixes: \"\n          \"https://ml-explore.github.io/mlx/build/html/install.html#build-from-source\"\n      )\n    else()\n      set(MLX_BUILD_METAL OFF)\n      message(WARNING \"Building for x86_64 arch is not officially supported.\")\n    endif()\n  endif()\n\nelse()\n  set(MLX_BUILD_METAL OFF)\n  message(WARNING \"MLX is prioritised for Apple silicon systems using macOS.\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring MLX Extension Build with CMake\nDESCRIPTION: Complete CMake configuration file for building MLX extensions. It sets up the C++ standard, dependencies (Python, nanobind, MLX), compiles C++ sources and Metal shaders, and creates Python bindings. The configuration supports both shared and static libraries.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/examples/extensions/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.27)\n\nproject(_ext LANGUAGES CXX)\n\n# ----------------------------- Setup -----------------------------\nset(CMAKE_CXX_STANDARD 17)\nset(CMAKE_CXX_STANDARD_REQUIRED ON)\nset(CMAKE_POSITION_INDEPENDENT_CODE ON)\n\noption(BUILD_SHARED_LIBS \"Build extensions as a shared library\" ON)\n\n# ----------------------------- Dependencies -----------------------------\nfind_package(\n  Python 3.8\n  COMPONENTS Interpreter Development.Module\n  REQUIRED)\nexecute_process(\n  COMMAND \"${Python_EXECUTABLE}\" -m nanobind --cmake_dir\n  OUTPUT_STRIP_TRAILING_WHITESPACE\n  OUTPUT_VARIABLE nanobind_ROOT)\nfind_package(nanobind CONFIG REQUIRED)\n\nexecute_process(\n  COMMAND \"${Python_EXECUTABLE}\" -m mlx --cmake-dir\n  OUTPUT_STRIP_TRAILING_WHITESPACE\n  OUTPUT_VARIABLE MLX_ROOT)\nfind_package(MLX CONFIG REQUIRED)\n\n# ----------------------------- Extensions -----------------------------\n\n# Add library\nadd_library(mlx_ext)\n\n# Add sources\ntarget_sources(mlx_ext PUBLIC ${CMAKE_CURRENT_LIST_DIR}/axpby/axpby.cpp)\n\n# Add include headers\ntarget_include_directories(mlx_ext PUBLIC ${CMAKE_CURRENT_LIST_DIR})\n\n# Link to mlx\ntarget_link_libraries(mlx_ext PUBLIC mlx)\n\n# ----------------------------- Metal -----------------------------\n\n# Build metallib\nif(MLX_BUILD_METAL)\n  mlx_build_metallib(\n    TARGET\n    mlx_ext_metallib\n    TITLE\n    mlx_ext\n    SOURCES\n    ${CMAKE_CURRENT_LIST_DIR}/axpby/axpby.metal\n    INCLUDE_DIRS\n    ${PROJECT_SOURCE_DIR}\n    ${MLX_INCLUDE_DIRS}\n    OUTPUT_DIRECTORY\n    ${CMAKE_LIBRARY_OUTPUT_DIRECTORY})\n\n  add_dependencies(mlx_ext mlx_ext_metallib)\n\nendif()\n\n# ----------------------------- Python Bindings -----------------------------\nnanobind_add_module(\n  _ext\n  NB_STATIC\n  STABLE_ABI\n  LTO\n  NOMINSIZE\n  NB_DOMAIN\n  mlx\n  ${CMAKE_CURRENT_LIST_DIR}/bindings.cpp)\ntarget_link_libraries(_ext PRIVATE mlx_ext)\n\nif(BUILD_SHARED_LIBS)\n  target_link_options(_ext PRIVATE -Wl,-rpath,@loader_path)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Starting Local Documentation Server\nDESCRIPTION: Launches a Python HTTP server to view the documentation locally. The server will serve the built HTML documentation on the specified port number.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/README.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython -m http.server <port>\n```\n\n----------------------------------------\n\nTITLE: Adding Source Files to MLX Target in CMake\nDESCRIPTION: This CMake snippet adds multiple C++ source files to the 'mlx' target. It includes files for broadcasting, compilation, common utilities, loading, reduction operations, slicing, and general utilities.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/mlx/backend/common/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_sources(\n  mlx\n  PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/broadcasting.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/compiled.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/common.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/load.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/reduce.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/slicing.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/utils.cpp)\n```\n\n----------------------------------------\n\nTITLE: Importing MLX Core Module\nDESCRIPTION: This snippet shows how to import the MLX Core module, which is necessary to access the data types and related functions described in this document.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/python/data_types.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom mlx import core\n```\n\n----------------------------------------\n\nTITLE: Installing Metal Dependencies in MLX Project\nDESCRIPTION: Conditionally installs Metal C++ dependencies when MLX_BUILD_METAL is enabled. It copies the Metal C++ source directory to the include directory.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/CMakeLists.txt#2025-04-19_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\n# Install metal dependencies\nif(MLX_BUILD_METAL)\n\n  # Install metal cpp\n  install(\n    DIRECTORY ${metal_cpp_SOURCE_DIR}/\n    DESTINATION ${CMAKE_INSTALL_INCLUDEDIR}/metal_cpp\n    COMPONENT metal_cpp_source)\n\nendif()\n```\n\n----------------------------------------\n\nTITLE: Metal Backend Configuration for MLX\nDESCRIPTION: Sets up the Metal backend for MLX when building on compatible systems. It configures Metal framework dependencies, checks SDK version, and downloads required resources like metal-cpp.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/CMakeLists.txt#2025-04-19_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nif(MLX_BUILD_METAL)\n  set(METAL_LIB \"-framework Metal\")\n  set(FOUNDATION_LIB \"-framework Foundation\")\n  set(QUARTZ_LIB \"-framework QuartzCore\")\nendif()\n\nif(MLX_BUILD_METAL AND NOT METAL_LIB)\n  message(STATUS \"Metal not found. Unable to build GPU\")\n  set(MLX_BUILD_METAL OFF)\n  set(MLX_METAL_DEBUG OFF)\nelseif(MLX_BUILD_METAL)\n  message(STATUS \"Building METAL sources\")\n\n  if(MLX_METAL_DEBUG)\n    add_compile_definitions(MLX_METAL_DEBUG)\n  endif()\n\n  # Throw an error if xcrun not found\n  execute_process(\n    COMMAND zsh \"-c\" \"/usr/bin/xcrun -sdk macosx --show-sdk-version\"\n    OUTPUT_VARIABLE MACOS_SDK_VERSION COMMAND_ERROR_IS_FATAL ANY)\n\n  if(${MACOS_SDK_VERSION} LESS 14.0)\n    message(\n      FATAL_ERROR\n        \"MLX requires macOS SDK >= 14.0 to be built with MLX_BUILD_METAL=ON\")\n  endif()\n  message(STATUS \"Building with macOS SDK version ${MACOS_SDK_VERSION}\")\n\n  set(METAL_CPP_URL\n      https://developer.apple.com/metal/cpp/files/metal-cpp_macOS15_iOS18.zip)\n\n  if(NOT CMAKE_OSX_DEPLOYMENT_TARGET STREQUAL \"\")\n    set(XCRUN_FLAGS \"-mmacosx-version-min=${CMAKE_OSX_DEPLOYMENT_TARGET}\")\n  endif()\n  execute_process(\n    COMMAND\n      zsh \"-c\"\n      \"echo \\\"__METAL_VERSION__\\\" | xcrun -sdk macosx metal ${XCRUN_FLAGS} -E -x metal -P - | tail -1 | tr -d '\\n'\"\n    OUTPUT_VARIABLE MLX_METAL_VERSION COMMAND_ERROR_IS_FATAL ANY)\n  FetchContent_Declare(metal_cpp URL ${METAL_CPP_URL})\n\n  FetchContent_MakeAvailable(metal_cpp)\n  target_include_directories(\n    mlx PUBLIC $<BUILD_INTERFACE:${metal_cpp_SOURCE_DIR}>\n               $<INSTALL_INTERFACE:include/metal_cpp>)\n  target_link_libraries(mlx PUBLIC ${METAL_LIB} ${FOUNDATION_LIB} ${QUARTZ_LIB})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Publishing Documentation to GitHub Pages\nDESCRIPTION: Force adds the built HTML documentation to git staging and prepares it for publishing to the gh-pages branch. This should be done after checking out the gh-pages branch and building the docs.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/README.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ngit add -f build/html\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Target for CPU Preamble\nDESCRIPTION: Creates a custom build target named 'cpu_compiled_preamble' that depends on the compiled_preamble.cpp file, ensuring it gets generated during the build.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/mlx/backend/cpu/CMakeLists.txt#2025-04-19_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nadd_custom_target(cpu_compiled_preamble DEPENDS compiled_preamble.cpp)\n```\n\n----------------------------------------\n\nTITLE: Defining Optimizer Class Documentation Structure in RST\nDESCRIPTION: This RST code defines the documentation structure for the Optimizer class in the mlx.optimizers module. It includes sections for attributes and methods, using autosummary directives to generate detailed documentation for each item.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/python/optimizers/optimizer.rst#2025-04-19_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\nOptimizer\n=========\n\n.. currentmodule:: mlx.optimizers\n\n.. autoclass:: Optimizer \n\n\n   .. rubric:: Attributes\n\n   .. autosummary::\n      :toctree: _autosummary\n\n      Optimizer.state\n   \n   .. rubric:: Methods\n\n   .. autosummary::\n      :toctree: _autosummary\n   \n      Optimizer.apply_gradients\n      Optimizer.init\n      Optimizer.update\n```\n\n----------------------------------------\n\nTITLE: Configuring Platform-Specific Shell Commands\nDESCRIPTION: Sets shell extension and command variables based on the platform. For MSVC (Windows), it uses PowerShell with specific execution policy, while other platforms use bash.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/mlx/backend/cpu/CMakeLists.txt#2025-04-19_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nif(MSVC)\n  set(SHELL_EXT ps1)\n  set(SHELL_CMD powershell -ExecutionPolicy Bypass -File)\nelse()\n  set(SHELL_EXT sh)\n  set(SHELL_CMD bash)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Platform-Specific Compiler Variables in MLX\nDESCRIPTION: Determines the appropriate compiler based on the system platform. On macOS (Darwin), it uses the C compiler and sets CLANG to TRUE, otherwise it uses the C++ compiler.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/mlx/backend/cpu/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif(${CMAKE_SYSTEM_NAME} MATCHES \"Darwin\")\n  set(COMPILER ${CMAKE_C_COMPILER})\n  set(CLANG TRUE)\nelse()\n  set(COMPILER ${CMAKE_CXX_COMPILER})\nendif()\n```\n\n----------------------------------------\n\nTITLE: MLX Core Linear Algebra Module Imports\nDESCRIPTION: Import statement for MLX linear algebra module including core array components with additional Tuple type.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/python/mlx/_stub_patterns.txt#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom mlx.core import array, Dtype, Device, Stream\nfrom typing import Sequence, Optional, Tuple, Union\n```\n\n----------------------------------------\n\nTITLE: Compiling Metal Air Files into Metallib\nDESCRIPTION: Creates a custom command to compile all generated .air files into a single mlx.metallib file using the xcrun metallib command.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/mlx/backend/metal/kernels/CMakeLists.txt#2025-04-19_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nadd_custom_command(\n  OUTPUT ${MLX_METAL_PATH}/mlx.metallib\n  COMMAND xcrun -sdk macosx metallib ${KERNEL_AIR} -o\n          ${MLX_METAL_PATH}/mlx.metallib\n  DEPENDS ${KERNEL_AIR}\n  COMMENT \"Building mlx.metallib\"\n  VERBATIM)\n```\n\n----------------------------------------\n\nTITLE: MLX Core Random Module Imports\nDESCRIPTION: Import statement for MLX random module including core array components.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/python/mlx/_stub_patterns.txt#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom mlx.core import array, Dtype, Device, Stream\nfrom typing import Sequence, Optional, Union\n```\n\n----------------------------------------\n\nTITLE: Adding Dependency on CPU Preamble to MLX\nDESCRIPTION: Establishes a dependency relationship between the main MLX target and the cpu_compiled_preamble target, ensuring the preamble is generated before MLX is built.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/mlx/backend/cpu/CMakeLists.txt#2025-04-19_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nadd_dependencies(mlx cpu_compiled_preamble)\n```\n\n----------------------------------------\n\nTITLE: Defining Compilation Dependencies for MLX\nDESCRIPTION: Sets up the COMPILE_DEPS variable with a list of header files required for compilation, including type definitions for various numeric formats and SIMD operations.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/mlx/backend/cpu/CMakeLists.txt#2025-04-19_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(COMPILE_DEPS\n    ${PROJECT_SOURCE_DIR}/mlx/types/half_types.h\n    ${PROJECT_SOURCE_DIR}/mlx/types/fp16.h\n    ${PROJECT_SOURCE_DIR}/mlx/types/bf16.h\n    ${PROJECT_SOURCE_DIR}/mlx/types/complex.h\n    simd/simd.h\n    simd/base_simd.h\n    simd/math.h\n    simd/type.h\n    unary_ops.h\n    binary_ops.h)\n```\n\n----------------------------------------\n\nTITLE: Displaying Contributor Images using GitHub API in Markdown\nDESCRIPTION: This code snippet uses the GitHub API to display contributor images for the MLX project repository. It creates a clickable image that shows avatars of up to 100 contributors in a 20-column grid.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/ACKNOWLEDGMENTS.md#2025-04-19_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<a href=\"https://github.com/ml-explore/mlx/graphs/contributors\">\n  <img class=\"dark-light\" src=\"https://contrib.rocks/image?repo=ml-explore/mlx&anon=0&columns=20&max=100&r=true\" />\n</a>\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Target for Metallib and Adding Dependencies\nDESCRIPTION: Sets up a custom target for the metallib compilation and adds it as a dependency to the main MLX target.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/mlx/backend/metal/kernels/CMakeLists.txt#2025-04-19_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nadd_custom_target(mlx-metallib DEPENDS ${MLX_METAL_PATH}/mlx.metallib)\n\nadd_dependencies(mlx mlx-metallib)\n```\n\n----------------------------------------\n\nTITLE: MLX Core Metal Module Imports\nDESCRIPTION: Import statement for MLX metal module including core array components.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/python/mlx/_stub_patterns.txt#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom mlx.core import array, Dtype, Device, Stream\nfrom typing import Sequence, Optional, Union\n```\n\n----------------------------------------\n\nTITLE: Installing MLX Project in Editable Mode\nDESCRIPTION: Installs the MLX project in editable mode using pip. This allows for changes in the source code to be immediately reflected without reinstallation.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/examples/extensions/README.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Adding Core Source Files to MLX Target\nDESCRIPTION: Adds the primary source files to the MLX library target, including various mathematical operations, matrix manipulations, and utility functions essential for the CPU implementation.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/mlx/backend/cpu/CMakeLists.txt#2025-04-19_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_sources(\n  mlx\n  PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/arg_reduce.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/binary.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/conv.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/copy.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/distributed.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/eigh.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/encoder.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/fft.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/hadamard.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/matmul.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/gemms/cblas.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/masked_mm.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/primitives.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/quantized.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/reduce.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/scan.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/select.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/softmax.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/logsumexp.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/sort.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/threefry.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/indexing.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/luf.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/qrf.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/svd.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/inverse.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/cholesky.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/unary.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/eval.cpp\n          ${CMAKE_CURRENT_BINARY_DIR}/compiled_preamble.cpp)\n```\n\n----------------------------------------\n\nTITLE: Configuring MLX Library Sources and Subdirectories in CMake\nDESCRIPTION: Configures the main MLX library target by adding source files for primitives, operations, and distributed computing. Also adds MPI and ring communication subdirectories to the build system.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/mlx/distributed/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_sources(\n  mlx\n  PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/primitives.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/ops.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/distributed.cpp)\n\nadd_subdirectory(${CMAKE_CURRENT_SOURCE_DIR}/mpi)\nadd_subdirectory(${CMAKE_CURRENT_SOURCE_DIR}/ring)\n```\n\n----------------------------------------\n\nTITLE: Windows-Specific Configuration for MLX\nDESCRIPTION: Configures Windows-specific settings for MLX, including handling MSVC compiler limitations and setting up the dlfcn-win32 library to provide Windows implementations of dlopen/dlsym APIs.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/CMakeLists.txt#2025-04-19_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nif(WIN32)\n  if(MSVC)\n    # GGUF does not build with MSVC.\n    set(MLX_BUILD_GGUF OFF)\n    # There is no prebuilt OpenBLAS distribution for MSVC.\n    set(MLX_BUILD_BLAS_FROM_SOURCE ON)\n  endif()\n  # Windows implementation of dlfcn.h APIs.\n  FetchContent_Declare(\n    dlfcn-win32\n    GIT_REPOSITORY https://github.com/dlfcn-win32/dlfcn-win32.git\n    GIT_TAG v1.4.1\n    EXCLUDE_FROM_ALL)\n  block()\n  set(BUILD_SHARED_LIBS OFF)\n  FetchContent_MakeAvailable(dlfcn-win32)\n  endblock()\n  target_include_directories(mlx PRIVATE \"${dlfcn-win32_SOURCE_DIR}/src\")\n  target_link_libraries(mlx PRIVATE dl)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring MLX Library Source Files in CMake\nDESCRIPTION: Adds source files to the 'mlx' target using target_sources command. Includes core implementation files for memory allocation, event handling, synchronization, Metal backend, and primitive operations.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/mlx/backend/no_metal/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_sources(\n  mlx\n  PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/allocator.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/event.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/fence.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/metal.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/primitives.cpp)\n```\n\n----------------------------------------\n\nTITLE: Sphinx AutoDoc Class Template Using Jinja2\nDESCRIPTION: A Jinja2 template that generates Sphinx documentation for Python classes. The template renders the class name as a heading, followed by sections for attributes and methods, each with auto-generated summaries and links to detailed documentation.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/_templates/module-base-class.rst#2025-04-19_snippet_0\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ fullname | escape | underline}}\n\n.. currentmodule:: {{ module }}\n\n.. add toctree option to make autodoc generate the pages\n\n.. autoclass:: {{ objname }}\n\n   {% block attributes %}\n   {% if attributes %}\n   .. rubric:: Attributes\n\n   .. autosummary::\n      :toctree: .\n   {% for item in attributes %}\n      ~{{ fullname }}.{{ item }}\n   {%- endfor %}\n   {% endif %}\n   {% endblock %}\n\n   {% block methods %}\n   {% if methods %}\n   .. rubric:: Methods\n\n   .. autosummary::\n      :toctree: .\n   {% for item in methods %}\n      {%- if item not in inherited_members and item != '__init__' %}\n      ~{{ fullname }}.{{ item }}\n      {%- endif -%}\n   {%- endfor %}\n   {% endif %}\n   {% endblock %}\n\n```\n\n----------------------------------------\n\nTITLE: Conditionally Adding Accelerate Framework Sources\nDESCRIPTION: Conditionally adds source files based on whether MLX is built with Apple's Accelerate framework. If using Accelerate, adds BNNS implementation; otherwise adds SIMD implementations for half-precision and bfloat16 formats.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/mlx/backend/cpu/CMakeLists.txt#2025-04-19_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nif(MLX_BUILD_ACCELERATE)\n  target_sources(mlx PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/gemms/bnns.cpp)\nelse()\n  target_sources(mlx PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/gemms/simd_fp16.cpp\n                             ${CMAKE_CURRENT_SOURCE_DIR}/gemms/simd_bf16.cpp)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining build_kernel Function for Individual Metal Kernel Compilation\nDESCRIPTION: Creates a CMake function to build individual Metal kernels, utilizing the build_kernel_base function and updating the KERNEL_AIR variable.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/mlx/backend/metal/kernels/CMakeLists.txt#2025-04-19_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(build_kernel KERNEL)\n  set(SRCFILE ${CMAKE_CURRENT_SOURCE_DIR}/${KERNEL}.metal)\n  cmake_path(GET KERNEL STEM TARGET)\n  build_kernel_base(${TARGET} ${SRCFILE} \"${ARGN}\")\n  set(KERNEL_AIR\n      ${TARGET}.air ${KERNEL_AIR}\n      PARENT_SCOPE)\nendfunction(build_kernel)\n```\n\n----------------------------------------\n\nTITLE: Implementing AXPBY GPU Kernel in Metal\nDESCRIPTION: A Metal shader kernel that implements the AXPBY operation for GPU execution. It handles different data types and array strides, mapping linear indices to element locations in source arrays.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/dev/extensions.rst#2025-04-19_snippet_7\n\nLANGUAGE: Metal\nCODE:\n```\ntemplate <typename T>\n[[kernel]] void axpby_general(\n        device const T* x [[buffer(0)]],\n        device const T* y [[buffer(1)]],\n        device T* out [[buffer(2)]],\n        constant const float& alpha [[buffer(3)]],\n        constant const float& beta [[buffer(4)]],\n        constant const int* shape [[buffer(5)]],\n        constant const int64_t* x_strides [[buffer(6)]],\n        constant const int64_t* y_strides [[buffer(7)]],\n        constant const int& ndim [[buffer(8)]],\n        uint index [[thread_position_in_grid]]) {\n    // Convert linear indices to offsets in array\n    auto x_offset = elem_to_loc(index, shape, x_strides, ndim);\n    auto y_offset = elem_to_loc(index, shape, y_strides, ndim);\n\n    // Do the operation and update the output\n    out[index] =\n        static_cast<T>(alpha) * x[x_offset] + static_cast<T>(beta) * y[y_offset];\n}\n```\n\n----------------------------------------\n\nTITLE: MLX Metal Module API Reference in reStructuredText\nDESCRIPTION: This code snippet defines the documentation structure for MLX's Metal module using reStructuredText format. It specifies the module path and lists the available Metal-related functions including is_available, device_info, start_capture, and stop_capture.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/python/metal.rst#2025-04-19_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. currentmodule:: mlx.core.metal\n\n.. autosummary::\n  :toctree: _autosummary\n\n  is_available\n  device_info\n  start_capture\n  stop_capture\n```\n\n----------------------------------------\n\nTITLE: Platform-Specific JIT Compilation Sources\nDESCRIPTION: Conditionally adds platform-specific implementation files. For iOS, it uses a limited no-CPU implementation, while other platforms include the full JIT compiler support for runtime code generation.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/mlx/backend/cpu/CMakeLists.txt#2025-04-19_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\nif(IOS)\n  target_sources(mlx PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/../no_cpu/compiled.cpp)\nelse()\n  target_sources(mlx PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/compiled.cpp\n                             ${CMAKE_CURRENT_SOURCE_DIR}/jit_compiler.cpp)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Building Various Metal Kernels for MLX Framework\nDESCRIPTION: Calls the build_kernel function for multiple Metal kernels, including conditional compilation based on Metal version.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/mlx/backend/metal/kernels/CMakeLists.txt#2025-04-19_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nbuild_kernel(arg_reduce)\nbuild_kernel(conv steel/conv/params.h)\nbuild_kernel(gemv steel/utils.h)\nbuild_kernel(layer_norm)\nbuild_kernel(random)\nbuild_kernel(rms_norm)\nbuild_kernel(rope)\nbuild_kernel(scaled_dot_product_attention sdpa_vector.h)\nif(MLX_METAL_VERSION GREATER_EQUAL 320)\n  build_kernel(fence)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Implementing AXPBY Operation CPU Evaluation with Type Dispatching\nDESCRIPTION: The CPU evaluation method for the Axpby primitive that dispatches to the correct implementation based on the data type of the arrays. It supports float32, float16, bfloat16, and complex64 types.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/dev/extensions.rst#2025-04-19_snippet_6\n\nLANGUAGE: C++\nCODE:\n```\nvoid Axpby::eval_cpu(\n    const std::vector<mx::array>& inputs,\n    std::vector<mx::array>& outputs) {\n  auto& x = inputs[0];\n  auto& y = inputs[1];\n  auto& out = outputs[0];\n\n  // Dispatch to the correct dtype\n  if (out.dtype() == mx::float32) {\n    return axpby_impl<float>(x, y, out, alpha_, beta_, stream());\n  } else if (out.dtype() == mx::float16) {\n    return axpby_impl<mx::float16_t>(x, y, out, alpha_, beta_, stream());\n  } else if (out.dtype() == mx::bfloat16) {\n    return axpby_impl<mx::bfloat16_t>(x, y, out, alpha_, beta_, stream());\n  } else if (out.dtype() == mx::complex64) {\n    return axpby_impl<mx::complex64_t>(x, y, out, alpha_, beta_, stream());\n  } else {\n    throw std::runtime_error(\n        \"Axpby is only supported for floating point types.\");\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Inefficient Use of Evaluation in a Loop in Python\nDESCRIPTION: This snippet illustrates an inefficient pattern of using evaluation too frequently within a loop, which can lead to performance overhead.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/usage/lazy_evaluation.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfor _ in range(100):\n     a = a + b\n     mx.eval(a)\n     b = b * 2\n     mx.eval(b)\n```\n\n----------------------------------------\n\nTITLE: Installing MLX with Python using pip\nDESCRIPTION: Command to install MLX version 0.22 or higher using pip package manager.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/examples/cmake_project/README.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install mlx>=0.22\n```\n\n----------------------------------------\n\nTITLE: Building MLX Python Extensions In-Place\nDESCRIPTION: Command for faster rebuilding of MLX Python extensions during development after dependencies are installed.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/install.rst#2025-04-19_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nCMAKE_BUILD_PARALLEL_LEVEL=8 python setup.py build_ext --inplace\n```\n\n----------------------------------------\n\nTITLE: Unimplemented Vectorization Transform for AXPBY\nDESCRIPTION: A stub implementation of the vmap transformation for the Axpby primitive that throws an error because vectorization is not implemented for this operation.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/dev/extensions.rst#2025-04-19_snippet_12\n\nLANGUAGE: C++\nCODE:\n```\n/** Vectorize primitive along given axis */\nstd::pair<std::vector<array>, std::vector<int>> Axpby::vmap(\n        const std::vector<array>& inputs,\n        const std::vector<int>& axes) {\n    throw std::runtime_error(\"[Axpby] vmap not implemented.\");\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing AXPBY GPU Evaluation with Metal\nDESCRIPTION: The GPU evaluation method for the Axpby primitive that sets up and launches the Metal kernel. It prepares inputs, encodes kernel parameters, and configures the thread execution grid.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/dev/extensions.rst#2025-04-19_snippet_9\n\nLANGUAGE: C++\nCODE:\n```\n/** Evaluate primitive on GPU */\nvoid Axpby::eval_gpu(\n  const std::vector<array>& inputs,\n  std::vector<array>& outputs) {\n    // Prepare inputs\n    assert(inputs.size() == 2);\n    auto& x = inputs[0];\n    auto& y = inputs[1];\n    auto& out = outputs[0];\n\n    // Each primitive carries the stream it should execute on\n    // and each stream carries its device identifiers\n    auto& s = stream();\n    // We get the needed metal device using the stream\n    auto& d = metal::device(s.device);\n\n    // Allocate output memory\n    out.set_data(allocator::malloc(out.nbytes()));\n\n    // Resolve name of kernel\n    std::ostringstream kname;\n    kname << \"axpby_\" << \"general_\" << type_to_name(out);\n\n    // Make sure the metal library is available\n    d.register_library(\"mlx_ext\");\n\n    // Make a kernel from this metal library\n    auto kernel = d.get_kernel(kname.str(), \"mlx_ext\");\n\n    // Prepare to encode kernel\n    auto& compute_encoder = d.get_command_encoder(s.index);\n    compute_encoder.set_compute_pipeline_state(kernel);\n\n    // Kernel parameters are registered with buffer indices corresponding to\n    // those in the kernel declaration at axpby.metal\n    int ndim = out.ndim();\n    size_t nelem = out.size();\n\n    // Encode input arrays to kernel\n    compute_encoder.set_input_array(x, 0);\n    compute_encoder.set_input_array(y, 1);\n\n    // Encode output arrays to kernel\n    compute_encoder.set_output_array(out, 2);\n\n    // Encode alpha and beta\n    compute_encoder.set_bytes(alpha_, 3);\n    compute_encoder.set_bytes(beta_, 4);\n\n    // Encode shape, strides and ndim\n    compute_encoder.set_vector_bytes(x.shape(), 5);\n    compute_encoder.set_vector_bytes(x.strides(), 6);\n    compute_encoder.set_bytes(y.strides(), 7);\n    compute_encoder.set_bytes(ndim, 8);\n\n    // We launch 1 thread for each input and make sure that the number of\n    // threads in any given threadgroup is not higher than the max allowed\n    size_t tgp_size = std::min(nelem, kernel->maxTotalThreadsPerThreadgroup());\n\n    // Fix the 3D size of each threadgroup (in terms of threads)\n    MTL::Size group_dims = MTL::Size(tgp_size, 1, 1);\n\n    // Fix the 3D size of the launch grid (in terms of threads)\n    MTL::Size grid_dims = MTL::Size(nelem, 1, 1);\n\n    // Launch the grid with the given number of threads divided among\n    // the given threadgroups\n    compute_encoder.dispatch_threads(grid_dims, group_dims);\n}\n```\n\n----------------------------------------\n\nTITLE: Installing C++ MLX Library\nDESCRIPTION: Command to install the built C++ MLX library system-wide.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/install.rst#2025-04-19_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\nmake install\n```\n\n----------------------------------------\n\nTITLE: Building the MLX Example\nDESCRIPTION: Commands to build the MLX example project using CMake. First generates the build files in Release mode, then builds the project.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/dev/mlx_in_cpp.rst#2025-04-19_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ncmake -B build -DCMAKE_BUILD_TYPE=Release\ncmake --build build\n```\n\n----------------------------------------\n\nTITLE: MLX Core Distributed Module Imports\nDESCRIPTION: Import statement for MLX distributed module including core array components and Group class.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/python/mlx/_stub_patterns.txt#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom mlx.core import array, Dtype, Device, Stream\nfrom mlx.core.distributed import Group\nfrom typing import Sequence, Optional, Union\n```\n\n----------------------------------------\n\nTITLE: Configuring Source Files for MLX Target in CMake\nDESCRIPTION: Adds source files to the 'mlx' target using the target_sources CMake command. The configuration includes primitives.cpp from the current directory, CPU-specific implementation files from the ../cpu/ directory, and compiled.cpp from the current directory.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/mlx/backend/no_cpu/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_sources(\n  mlx\n  PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/primitives.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/../cpu/eval.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/../cpu/encoder.cpp\n          ${CMAKE_CURRENT_SOURCE_DIR}/compiled.cpp)\n```\n\n----------------------------------------\n\nTITLE: Defining make_jit_source Function in CMake for Metal Shader Compilation\nDESCRIPTION: Creates a CMake function to preprocess Metal header files and make them available as C++ string literals. The function takes the source file name and optional dependencies, generates a C++ file containing the processed Metal code, and adds it to the mlx target.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/mlx/backend/metal/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(make_jit_source SRC_FILE)\n  # This function takes a metal header file, runs the C preprocessesor on it,\n  # and makes the processed contents available as a string in a C++ function\n  # mlx::core::metal::${SRC_NAME}()\n  #\n  # To use the function, declare it in jit/includes.h and include\n  # jit/includes.h.\n  #\n  # Additional arguments to this function are treated as dependencies in the\n  # Cmake build system.\n  get_filename_component(SRC_NAME ${SRC_FILE} NAME)\n  add_custom_command(\n    OUTPUT jit/${SRC_NAME}.cpp\n    COMMAND\n      bash ${CMAKE_CURRENT_SOURCE_DIR}/make_compiled_preamble.sh\n      ${CMAKE_CURRENT_BINARY_DIR}/jit ${CMAKE_C_COMPILER} ${PROJECT_SOURCE_DIR}\n      ${SRC_FILE}\n    DEPENDS make_compiled_preamble.sh kernels/${SRC_FILE}.h ${ARGN})\n  add_custom_target(${SRC_NAME} DEPENDS jit/${SRC_NAME}.cpp)\n  add_dependencies(mlx ${SRC_NAME})\n  target_sources(mlx PRIVATE ${CMAKE_CURRENT_BINARY_DIR}/jit/${SRC_NAME}.cpp)\nendfunction(make_jit_source)\n```\n\n----------------------------------------\n\nTITLE: Installing MLX with conda for Python API\nDESCRIPTION: Command to install the MLX Python API using conda package manager from the conda-forge channel.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/README.md#2025-04-19_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nconda install -c conda-forge mlx\n```\n\n----------------------------------------\n\nTITLE: Configuring MLX Benchmark Build Function\nDESCRIPTION: Defines a CMake function to build benchmark executables. The function takes a source file as input, creates a target with the same name as the source file (without extension), and links it against the MLX library.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/benchmarks/cpp/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfunction(build_benchmark SRCFILE)\n  get_filename_component(src_name ${SRCFILE} NAME_WE)\n  set(target \"${src_name}\")\n  add_executable(${target} ${SRCFILE})\n  target_link_libraries(${target} PRIVATE mlx)\nendfunction(build_benchmark)\n```\n\n----------------------------------------\n\nTITLE: Checking macOS SDK Version\nDESCRIPTION: Command to display the macOS SDK version that will be used for building.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/install.rst#2025-04-19_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\nxcrun -sdk macosx --show-sdk-version\n```\n\n----------------------------------------\n\nTITLE: Defining Steel Headers for Metal Kernel Compilation\nDESCRIPTION: Sets up lists of Steel-related header files required for specific Metal kernel compilations, including GEMM, convolution, and attention mechanisms.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/mlx/backend/metal/kernels/CMakeLists.txt#2025-04-19_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nset(STEEL_HEADERS\n    steel/defines.h\n    steel/utils.h\n    steel/conv/conv.h\n    steel/conv/loader.h\n    steel/conv/loaders/loader_channel_l.h\n    steel/conv/loaders/loader_channel_n.h\n    steel/conv/loaders/loader_general.h\n    steel/conv/kernels/steel_conv.h\n    steel/conv/kernels/steel_conv_general.h\n    steel/gemm/gemm.h\n    steel/gemm/mma.h\n    steel/gemm/loader.h\n    steel/gemm/transforms.h\n    steel/gemm/kernels/steel_gemm_fused.h\n    steel/gemm/kernels/steel_gemm_gather.h\n    steel/gemm/kernels/steel_gemm_masked.h\n    steel/gemm/kernels/steel_gemm_splitk.h\n    steel/utils/type_traits.h\n    steel/utils/integral_constant.h)\n\nset(STEEL_ATTN_HEADERS\n    steel/defines.h\n    steel/utils.h\n    steel/gemm/gemm.h\n    steel/gemm/mma.h\n    steel/gemm/loader.h\n    steel/gemm/transforms.h\n    steel/utils/type_traits.h\n    steel/utils/integral_constant.h\n    steel/attn/attn.h\n    steel/attn/loader.h\n    steel/attn/mma.h\n    steel/attn/params.h\n    steel/attn/transforms.h\n    steel/attn/kernels/steel_attention.h)\n```\n\n----------------------------------------\n\nTITLE: Defining Python Bindings for MLX Extension in C++\nDESCRIPTION: This snippet demonstrates how to use nanobind to create Python bindings for a C++ extension in MLX. It defines the 'axpby' function with its parameters, docstring, and Python module structure.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/dev/extensions.rst#2025-04-19_snippet_14\n\nLANGUAGE: C++\nCODE:\n```\nNB_MODULE(_ext, m) {\n        m.doc() = \"Sample extension for MLX\";\n\n        m.def(\n            \"axpby\",\n            &axpby,\n            \"x\"_a,\n            \"y\"_a,\n            \"alpha\"_a,\n            \"beta\"_a,\n            nb::kw_only(),\n            \"stream\"_a = nb::none(),\n            R\"(\n                Scale and sum two vectors element-wise\n                ``z = alpha * x + beta * y``\n\n                Follows numpy style broadcasting between ``x`` and ``y``\n                Inputs are upcasted to floats if needed\n\n                Args:\n                    x (array): Input array.\n                    y (array): Input array.\n                    alpha (float): Scaling factor for ``x``.\n                    beta (float): Scaling factor for ``y``.\n\n                Returns:\n                    array: ``alpha * x + beta * y``\n            )\");\n    }\n```\n\n----------------------------------------\n\nTITLE: Linking and configuring MLX core module in CMake\nDESCRIPTION: Links the 'core' module with the MLX library, defines the MLX version, and sets rpath configuration when building shared libraries to ensure proper runtime loading.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/python/src/CMakeLists.txt#2025-04-19_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_link_libraries(core PRIVATE mlx)\ntarget_compile_definitions(core PRIVATE _VERSION_=${MLX_VERSION})\n\nif(BUILD_SHARED_LIBS)\n  target_link_options(core PRIVATE -Wl,-rpath,@loader_path/lib)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring Platform-Specific Settings for MLX\nDESCRIPTION: Sets platform-specific compiler options and properties for Windows and MSVC builds. For MSVC, it disables specific warnings, and for Windows, it ensures symbols are exported by default.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/mlx/CMakeLists.txt#2025-04-19_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(MSVC)\n  # Disable some MSVC warnings to speed up compilation.\n  target_compile_options(mlx PUBLIC /wd4068 /wd4244 /wd4267 /wd4804)\nendif()\n\nif(WIN32)\n  # Export symbols by default to behave like macOS/linux.\n  set_target_properties(mlx PROPERTIES WINDOWS_EXPORT_ALL_SYMBOLS TRUE)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Generated C++ Signature for Custom Metal Kernel\nDESCRIPTION: Shows the automatically generated C++ function signature for the custom Metal kernel. It includes template parameters, input/output buffers, and thread position attributes.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/dev/custom_metal_kernels.rst#2025-04-19_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\ntemplate <typename T>\n[[kernel]] void custom_kernel_myexp_float(\n  const device float16_t* inp [[buffer(0)]],\n  device float16_t* out [[buffer(1)]],\n  uint3 thread_position_in_grid [[thread_position_in_grid]]) {\n\n        uint elem = thread_position_in_grid.x;\n        T tmp = inp[elem];\n        out[elem] = metal::exp(tmp);\n\n}\n\ntemplate [[host_name(\"custom_kernel_myexp_float\")]] [[kernel]] decltype(custom_kernel_myexp_float<float>) custom_kernel_myexp_float<float>;\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Command for Preamble Compilation\nDESCRIPTION: Adds a custom build command that generates the compiled_preamble.cpp file by executing a platform-specific shell script with appropriate parameters including compiler and architecture information.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/mlx/backend/cpu/CMakeLists.txt#2025-04-19_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nadd_custom_command(\n  OUTPUT compiled_preamble.cpp\n  COMMAND\n    ${SHELL_CMD} ${CMAKE_CURRENT_SOURCE_DIR}/make_compiled_preamble.${SHELL_EXT}\n    ${CMAKE_CURRENT_BINARY_DIR}/compiled_preamble.cpp ${COMPILER}\n    ${PROJECT_SOURCE_DIR} ${CLANG} ${CMAKE_SYSTEM_PROCESSOR}\n  DEPENDS make_compiled_preamble.${SHELL_EXT} compiled_preamble.h\n          ${COMPILE_DEPS})\n```\n\n----------------------------------------\n\nTITLE: Importing MLX Core Module in Python\nDESCRIPTION: This snippet shows how to import the mlx.core module, which contains all the operations listed in this file. It's a common pattern used at the beginning of MLX-based Python scripts.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/docs/src/python/ops.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. currentmodule:: mlx.core\n```\n\n----------------------------------------\n\nTITLE: Installing Metallib for MLX Framework\nDESCRIPTION: Configures the installation of the compiled mlx.metallib file to the appropriate library directory.\nSOURCE: https://github.com/ml-explore/mlx/blob/main/mlx/backend/metal/kernels/CMakeLists.txt#2025-04-19_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\ninclude(GNUInstallDirs)\n\ninstall(\n  FILES ${MLX_METAL_PATH}/mlx.metallib\n  DESTINATION ${CMAKE_INSTALL_LIBDIR}\n  COMPONENT metallib)\n```"
  }
]
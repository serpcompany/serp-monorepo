[
  {
    "owner": "nvidia",
    "repo": "cccl",
    "content": "TITLE: Initializing CUB Benchmark Environment in CMake\nDESCRIPTION: Sets up the initial environment for CUB benchmarks by including the benchmark registry, adding the nvbench helper subdirectory, and enforcing build requirements such as Release mode and CUDA architecture specification.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cub/benchmarks/CMakeLists.txt#2025-04-23_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ninclude(${CMAKE_SOURCE_DIR}/benchmarks/cmake/CCCLBenchmarkRegistry.cmake)\n\n# Defer dependencies collection to nvbench helper\nadd_subdirectory(nvbench_helper)\n\nset(benches_root \"${CMAKE_CURRENT_LIST_DIR}\")\n\nif(NOT CMAKE_BUILD_TYPE STREQUAL \"Release\")\n  message(FATAL_ERROR \"CUB benchmarks must be built in release mode.\")\nendif()\n\nif(NOT DEFINED CMAKE_CUDA_ARCHITECTURES)\n  message(FATAL_ERROR \"CMAKE_CUDA_ARCHITECTURES must be set to build CUB benchmarks.\")\nendif()\n\nset(benches_meta_target cub.all.benches)\nadd_custom_target(${benches_meta_target})\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Partition Policy Class - CUDASTF C++\nDESCRIPTION: Declares a user-defined partition policy for specifying how data is mapped and dispatched to different parts of the execution grid, using template apply and get_executor methods. Integration with CUDASTF requires partitioner_base and proper definition of pos4, dim4, S_in, S_out types. The apply method splits shapes for different grid entries while get_executor maps specific coordinates to places.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_27\n\nLANGUAGE: c++\nCODE:\n```\n   class MyPartition : public partitioner_base {\n   public:\n       template <typename S_out, typename S_in>\n       static const S_out apply(const S_in& in, pos4 position, dim4 grid_dims);\n\n       pos4 get_executor(pos4 data_coords, dim4 data_dims, dim4 grid_dims);\n   };\n```\n\n----------------------------------------\n\nTITLE: Launching Host Tasks Asynchronously with CUDASTF in C++\nDESCRIPTION: Shows usage of 'ctx.host_launch' to schedule a host-side task as a CUDA callback, removing the need for explicit stream synchronization. This is compatible with all CUDASTF backends, including those based on CUDA graphs. The code asserts the expected value in logical data 'lX' upon host-side invocation.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_20\n\nLANGUAGE: cpp\nCODE:\n```\nctx.host_launch(lX.read())->*[](auto sX) {\n    assert(sX(0) == 44);\n};\n```\n\n----------------------------------------\n\nTITLE: MMA Block Scale Vector 1X Collector Use with mxf8f6f4 Type (CTA Group 2)\nDESCRIPTION: Template function for matrix multiplication with block scaling using vector 1X with collector A use operation for CTA group 2. The function uses mxf8f6f4 data type and requires PTX ISA 86 with SM_100a or SM_101a architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma.rst#2025-04-23_snippet_33\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.cta_group.kind.block_scale.scale_vec::1X.collector::a::use [d_tmem], a_desc, b_desc, idesc, [scale_A_tmem], [scale_B_tmem], enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .kind      = { .kind::mxf8f6f4 }\n```\n\n----------------------------------------\n\nTITLE: Host and Device MDSpan Usage Example\nDESCRIPTION: Example demonstrating the usage of host and device mdspan with kernel and host functions, including memory allocation and access restrictions.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/mdspan/host_device_accessor.rst#2025-04-23_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\n#include <cuda/mdspan>\n\nusing dim = cuda::std::dims<1>;\n\n__global__ void kernel_d(cuda::device_mdspan<int, dim> md) {\n    md[0] = 0;\n}\n__global__ void kernel_h(cuda::host_mdspan<int, dim> md) {\n    // md[0] = 0;  // compile error\n}\n\n__host__ void host_function_h(cuda::host_mdspan<int, dim> md) {\n    md[0] = 0;\n}\n__host__ void host_function_d(cuda::device_mdspan<int, dim> md) {\n    // md[0] = 0;  // compile error\n}\n__host__ void host_function_m(cuda::managed_mdspan<int, dim> md) {\n    md[0] = 0;\n}\n\nint main() {\n    int* d_ptr;\n    cudaMalloc(&d_ptr, 4 * sizeof(int));\n    int                 h_ptr[4];\n    cuda::host_mdspan   h_md{h_ptr};\n    cuda::device_mdspan d_md{d_ptr, 4};\n    kernel_d<<<1, 1>>>(d_md);    // ok\n    // kernel_d<<<1, 1>>>(h_md); // compile error\n    host_function_h(h_md);       // ok\n    host_function_d(h_md);       // compile error\n    // host_function_m(h_md);    // compile error\n    cudaFree(d_ptr);\n}\n```\n\n----------------------------------------\n\nTITLE: CUDA Pipeline Usage Example\nDESCRIPTION: Example kernel demonstrating pipeline usage for coordinated memory transfers and computation. Shows initialization, pipeline stages management, and proper producer-consumer pattern implementation.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/synchronization_primitives/pipeline.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n#include <cuda/pipeline>\n#include <cooperative_groups.h>\n\n// Disables `pipeline_shared_state` initialization warning.\n#pragma nv_diag_suppress static_var_with_dynamic_init\n\ntemplate <typename T>\n__device__ void compute(T* ptr);\n\ntemplate <typename T>\n__global__ void example_kernel(T* global0, T* global1, cuda::std::size_t subset_count) {\n  extern __shared__ T s[];\n  auto group = cooperative_groups::this_thread_block();\n  T* shared[2] = { s, s + 2 * group.size() };\n\n  // Create a pipeline.\n  constexpr auto scope = cuda::thread_scope_block;\n  constexpr auto stages_count = 2;\n  __shared__ cuda::pipeline_shared_state<scope, stages_count> shared_state;\n  auto pipeline = cuda::make_pipeline(group, &shared_state);\n\n  // Prime the pipeline.\n  pipeline.producer_acquire();\n  cuda::memcpy_async(group, shared[0],\n                     &global0[0], sizeof(T) * group.size(), pipeline);\n  cuda::memcpy_async(group, shared[0] + group.size(),\n                     &global1[0], sizeof(T) * group.size(), pipeline);\n  pipeline.producer_commit();\n\n  // Pipelined copy/compute.\n  for (cuda::std::size_t subset = 1; subset < subset_count; ++subset) {\n    pipeline.producer_acquire();\n    cuda::memcpy_async(group, shared[subset % 2],\n                       &global0[subset * group.size()],\n                       sizeof(T) * group.size(), pipeline);\n    cuda::memcpy_async(group, shared[subset % 2] + group.size(),\n                       &global1[subset * group.size()],\n                       sizeof(T) * group.size(), pipeline);\n    pipeline.producer_commit();\n    pipeline.consumer_wait();\n    compute(shared[(subset - 1) % 2]);\n    pipeline.consumer_release();\n  }\n\n  // Drain the pipeline.\n  pipeline.consumer_wait();\n  compute(shared[(subset_count - 1) % 2]);\n  pipeline.consumer_release();\n}\n\ntemplate void __global__ example_kernel<int>(int*, int*, cuda::std::size_t);\n```\n\n----------------------------------------\n\nTITLE: Computing the Sum of Random Numbers in Parallel with Thrust in C++\nDESCRIPTION: This example shows how to generate random floating-point numbers, transfer them to a parallel device, and compute their sum using Thrust's parallel reduction algorithm. It demonstrates host and device vectors, random number generation, and the reduce operation with a custom addition functor.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/index.rst#2025-04-23_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n#include <thrust/host_vector.h>\n#include <thrust/device_vector.h>\n#include <thrust/generate.h>\n#include <thrust/reduce.h>\n#include <thrust/functional.h>\n#include <thrust/random.h>\n\nint main() {\n// Generate random data serially.\nthrust::default_random_engine rng(1337);\nthrust::uniform_real_distribution<double> dist(-50.0, 50.0);\nthrust::host_vector<double> h_vec(32 << 20);\nthrust::generate(h_vec.begin(), h_vec.end(), [&] { return dist(rng); });\n\n// Transfer to device and compute the sum.\nthrust::device_vector<double> d_vec = h_vec;\ndouble x = thrust::reduce(d_vec.begin(), d_vec.end(), 0, thrust::plus<int>());\n}\n```\n\n----------------------------------------\n\nTITLE: Example Usage of cuda::device::memcpy_async_tx in CUDA Kernel\nDESCRIPTION: Demonstrates how to use cuda::device::memcpy_async_tx within a CUDA kernel. It copies data from global memory to shared memory, uses a barrier for synchronization, and performs a simple operation on the copied data.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/asynchronous_operations/memcpy_async_tx.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n#include <cuda/barrier>\n#include <cuda/std/utility> // cuda::std::move\n\n#if defined(__CUDA_MINIMUM_ARCH__) && __CUDA_MINIMUM_ARCH__ < 900\nstatic_assert(false, \"Insufficient CUDA Compute Capability: cuda::device::memcpy_async_tx is not available.\");\n#endif // __CUDA_MINIMUM_ARCH__\n\n__device__ alignas(16) int gmem_x[2048];\n\n__global__ void example_kernel() {\n  alignas(16) __shared__ int smem_x[1024];\n  __shared__ cuda::barrier<cuda::thread_scope_block> bar;\n  if (threadIdx.x == 0) {\n    init(&bar, blockDim.x);\n  }\n  __syncthreads();\n\n  barrier::arrival_token token;\n  if (threadIdx.x == 0) {\n    cuda::device::memcpy_async_tx(smem_x, gmem_x, cuda::aligned_size_t<16>(sizeof(smem_x)), bar);\n    token = cuda::device::barrier_arrive_tx(bar, 1, sizeof(smem_x));\n  } else {\n    token = bar.arrive(1);\n  }\n  bar.wait(cuda::std::move(token));\n\n  // smem_x contains the contents of gmem_x[0], ..., gmem_x[1023]\n  smem_x[threadIdx.x] += 1;\n}\n```\n\n----------------------------------------\n\nTITLE: CUDA Single Stream Dependency\nDESCRIPTION: Shows how using a single stream creates an implicit dependency between kernels, ensuring proper execution order and program termination.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/execution_model.rst#2025-04-23_snippet_10\n\nLANGUAGE: cuda\nCODE:\n```\ncuda::atomic<int, cuda::thread_scope_system> flag = 0;\n__global__ void first() { flag.store(1, cuda::memory_order_relaxed); }\n__global__ void second() { while(flag.load(cuda::memory_order_relaxed) == 0) {} }\nint main() {\n    cudaHostRegister(&flag, sizeof(flag));\n    cudaStream_t s0;\n    cudaStreamCreate(&s0);\n    first<<<1,1,0,s0>>>();\n    second<<<1,1,0,s0>>>();\n    return cudaDeviceSynchronize();\n}\n```\n\n----------------------------------------\n\nTITLE: Pipeline Producer Commit Usage Example - CUDA\nDESCRIPTION: Example kernel demonstrating how to use pipeline_producer_commit with async memory operations and barrier synchronization. Shows initialization, acquisition, commit, and release pattern.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/synchronization_primitives/pipeline/pipeline_producer_commit.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n#include <cuda/pipeline>\n\n// Disables `barrier` initialization warning.\n#pragma nv_diag_suppress static_var_with_dynamic_init\n\n__global__ void\nexample_kernel(cuda::std::uint64_t* global, cuda::std::size_t element_count) {\n  extern __shared__ cuda::std::uint64_t shared[];\n  __shared__ cuda::barrier<cuda::thread_scope_block> barrier;\n\n  init(&barrier, 1);\n  cuda::pipeline<cuda::thread_scope_thread> pipe = cuda::make_pipeline();\n\n  pipe.producer_acquire();\n  for (cuda::std::size_t i = 0; i < element_count; ++i)\n    cuda::memcpy_async(shared + i, global + i, sizeof(*global), pipe);\n  pipeline_producer_commit(pipe, barrier);\n  barrier.arrive_and_wait();\n  pipe.consumer_release();\n}\n```\n\n----------------------------------------\n\nTITLE: Using cuda::atomic in CUDA Kernel\nDESCRIPTION: Example of using cuda::atomic with different thread scopes in a CUDA kernel. Demonstrates the creation of atomic variables for system-wide, device-wide, and block-wide synchronization.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/synchronization_primitives/atomic.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n#include <cuda/atomic>\n\n__global__ void example_kernel() {\n  // This atomic is suitable for all threads in the system.\n  cuda::atomic<int, cuda::thread_scope_system> a;\n\n  // This atomic has the same type as the previous one (`a`).\n  cuda::atomic<int> b;\n\n  // This atomic is suitable for all threads on the current processor (e.g. GPU).\n  cuda::atomic<int, cuda::thread_scope_device> c;\n\n  // This atomic is suitable for threads in the same thread block.\n  cuda::atomic<int, cuda::thread_scope_block> d;\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Block-wide Prefix Sum in CUDA C++ using CUB\nDESCRIPTION: This snippet demonstrates how to use CUB's BlockScan class to perform an exclusive block-wide prefix sum operation in a CUDA kernel. It shows the allocation of shared memory, specialization of the BlockScan type, and invocation of the ExclusiveSum method.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/index.rst#2025-04-23_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\n#include <cub/cub.cuh>\n\n__global__ void SomeKernelFoo(...)\n{\n    // Specialize BlockScan for 128 threads on integer types\n  using BlockScan = cub::BlockScan<int, 128>;\n\n  // Allocate shared memory for BlockScan\n  __shared__ typename BlockScan::TempStorage scan_storage;\n\n  ...\n\n  // Obtain a segment of consecutive items that are blocked across threads\n  int thread_data_in[4];\n  int thread_data_out[4];\n  ...\n\n  // Perform an exclusive block-wide prefix sum\n  BlockScan(scan_storage).ExclusiveSum(thread_data_in, thread_data_out);\n```\n\n----------------------------------------\n\nTITLE: Using atomic in CUDA C++ with libcu++\nDESCRIPTION: Demonstrates how to use the atomic type from libcu++ in CUDA C++ code. This snippet shows the inclusion of the cuda/std/atomic header and the creation of an atomic integer variable.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/index.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\n#include <cuda/std/atomic>\ncuda::std::atomic<int> x;\n```\n\n----------------------------------------\n\nTITLE: CUDA AXPY with Direct Slice Manipulation\nDESCRIPTION: Improved version of AXPY kernel that works directly with slices instead of raw pointers, demonstrating safer parameter passing.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_16\n\nLANGUAGE: cuda\nCODE:\n```\n__global__ void axpy(double a, slice<const double> x, slice<double> y) {\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    int nthreads = gridDim.x * blockDim.x;\n\n    for (size_t ind = tid; ind < x.size(); ind += nthreads) {\n        y(ind) += a * x(ind);\n    }\n}\n...\nctx.task(lX.read(), lY.rw())->*[&](cudaStream_t s, slice<const double> sX, slice<double> sY) {\n    axpy<<<16, 128, 0, s>>>(alpha, sX, sY);\n};\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Atomic Operations on Non-Automatic Storage in CUDA Device Code\nDESCRIPTION: Example showing how a CUDA grid can terminate successfully when atomic operations are performed on objects that don't have automatic storage duration. Thread 0 waits on an atomic value that thread 1 modifies.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/execution_model.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\n__global__ void ex0(cuda::atomic_ref<int, cuda::thread_scope_device> atom) {\n    if (threadIdx.x == 0) {\n        while(atom.load(cuda::memory_order_relaxed) == 0);\n    } else if (threadIdx.x == 1) {\n        atom.store(1, cuda::memory_order_relaxed);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Block-Sorting CUDA Kernel Implementation Using CUB Primitives\nDESCRIPTION: A CUDA kernel example demonstrating how to use CUB's block-level collective primitives for sorting. The kernel loads data from global memory, sorts it using BlockRadixSort, and stores the results back to global memory, all with efficient shared memory usage through collective operations.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/index.rst#2025-04-23_snippet_0\n\nLANGUAGE: c++\nCODE:\n```\n#include <cub/cub.cuh>\n\n//\n// Block-sorting CUDA kernel\n//\ntemplate <int BLOCK_THREADS, int ITEMS_PER_THREAD>\n__global__ void BlockSortKernel(int *d_in, int *d_out)\n{\n    // Specialize BlockLoad, BlockStore, and BlockRadixSort collective types\n    using BlockLoadT = cub::BlockLoad<\n      int, BLOCK_THREADS, ITEMS_PER_THREAD, cub::BLOCK_LOAD_TRANSPOSE>;\n    using BlockStoreT = cub::BlockStore<\n      int, BLOCK_THREADS, ITEMS_PER_THREAD, cub::BLOCK_STORE_TRANSPOSE>;\n    using BlockRadixSortT = cub::BlockRadixSort<\n      int, BLOCK_THREADS, ITEMS_PER_THREAD>;\n\n    // Allocate type-safe, repurposable shared memory for collectives\n    __shared__ union {\n        typename BlockLoadT::TempStorage       load;\n        typename BlockStoreT::TempStorage      store;\n        typename BlockRadixSortT::TempStorage  sort;\n    } temp_storage;\n\n    // Obtain this block's segment of consecutive keys (blocked across threads)\n    int thread_keys[ITEMS_PER_THREAD];\n    int block_offset = blockIdx.x * (BLOCK_THREADS * ITEMS_PER_THREAD);\n    BlockLoadT(temp_storage.load).Load(d_in + block_offset, thread_keys);\n\n    __syncthreads();\t// Barrier for smem reuse\n\n    // Collectively sort the keys\n    BlockRadixSortT(temp_storage.sort).Sort(thread_keys);\n\n    __syncthreads();\t// Barrier for smem reuse\n\n    // Store the sorted segment\n    BlockStoreT(temp_storage.store).Store(d_out + block_offset, thread_keys);\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing AXPY Operation with CUDASTF Stream Context\nDESCRIPTION: This code demonstrates using CUDASTF's lower-level stream_ctx API to implement an AXPY operation (Y = Y + alpha*X). It shows how to create logical data representations, define tasks, access data slices, and launch a CUDA kernel within the task.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf/lower_level_api.rst#2025-04-23_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n#include \"cudastf/stf.h\"\n#include \"cudastf/__stf/stream/stream_ctx.h\"\n\nusing namespace cudastf;\n\ntemplate <typename T>\n__global__ void axpy(int n, T a, T* x, T* y) {\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    int nthreads = gridDim.x * blockDim.x;\n\n    for (int ind = tid; ind < n; ind += nthreads) {\n        y[ind] += a * x[ind];\n    }\n}\n\nint main(int argc, char** argv) {\n    stream_ctx ctx;\n\n    const size_t N = 16;\n    double X[N], Y[N];\n\n    for (size_t ind = 0; ind < N; ind++) {\n        X[ind] = sin(double(ind));\n        Y[ind] = cos(double(ind));\n    }\n\n    auto lX = ctx.logical_data(X);\n    auto lY = ctx.logical_data(Y);\n\n    double alpha = 3.14;\n\n    /* Compute Y = Y + alpha X */\n    auto t = ctx.task(lX.read(), lY.rw());\n    t.start();\n    slice<double> sX = t.get<0>();\n    slice<double> sY = t.get<1>();\n    axpy<<<16, 128, 0, t.get_stream()>>>(sX.size(), alpha, sX.data_handle(), sY.data_handle());\n    t.end();\n\n    ctx.sync();\n}\n```\n\n----------------------------------------\n\nTITLE: Parallel_for on 2D Matrix Example\nDESCRIPTION: Example showing how to apply the parallel_for construct on a 2D matrix. This demonstrates working with multi-dimensional data by creating a 2D slice from a 1D array and updating matrix elements based on vector values.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_63\n\nLANGUAGE: cpp\nCODE:\n```\ndouble X[N*N];\ndouble Y[N];\n\nauto lX = ctx.logical_data(make_slice(&X[0], std::tuple{N, N}));\nauto lY = ctx.logical_data(Y);\n\nctx.parallel_for(lX.shape(), lX.rw(), lY.read())\n    ->*[](size_t i, size_t j, auto dX, auto dY) {\n        dX(i, j) += dY(i) * dY(j);\n    };\n```\n\n----------------------------------------\n\nTITLE: Configuring CCCL and CUDA Toolkit Dependencies\nDESCRIPTION: Finds and configures the CCCL package with specific hints, and locates the CUDA Toolkit. It also sets up an option for enabling CURAND based on its availability.\nSOURCE: https://github.com/nvidia/cccl/blob/main/c2h/CMakeLists.txt#2025-04-23_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nfind_package(CCCL CONFIG REQUIRED\n  NO_DEFAULT_PATH # Only check the explicit HINTS below:\n  HINTS \"${CCCL_SOURCE_DIR}/lib/cmake/cccl/\"\n)\n\nfind_package(CUDAToolkit)\n\nset(curand_default OFF)\nif (CUDA_curand_LIBRARY)\n  set(curand_default ON)\nendif()\n\noption(C2H_ENABLE_CURAND \"Use CUDA CURAND library in c2h.\" ${curand_default})\n```\n\n----------------------------------------\n\nTITLE: Multi-Task CUDA Workflow Example\nDESCRIPTION: Example showing multiple dependent CUDA tasks with different access patterns and a host task, demonstrating task dependency management.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_17\n\nLANGUAGE: cpp\nCODE:\n```\nauto lX = ctx.logical_data(X);\nauto lY = ctx.logical_data(Y);\n\n// Task 1\nctx.task(lX.read(), lY.read())->*[](cudaStream_t stream, auto sX, auto sY) {\n    K1<<<..., stream>>>(sX, sY);\n    K2<<<..., stream>>>(sX, sY);\n};\n\n// Task 2\nctx.task(lX.rw())->*[](cudaStream_t stream, auto sX) {\n    K3<<<..., stream>>>(sX);\n};\n\n// Task 3\nctx.task(lY.rw())->*[](cudaStream_t stream, auto sY) {\n    K4<<<..., stream>>>(sY);\n};\n\n// Task 4\nctx.host_launch(lX.read(), lY.read())->*[](auto sX, auto sY) {\n    callback(sX, sY);\n};\n```\n\n----------------------------------------\n\nTITLE: MMA Block Scale Vector 4X TMEM Collector Fill with mxf4nvf4 Type (CTA Group 2)\nDESCRIPTION: Template function for matrix multiplication with block scaling using vector 4X with TMEM and collector A fill operation for CTA group 2. The function uses mxf4nvf4 data type and requires PTX ISA 86 with SM_100a or SM_101a architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma.rst#2025-04-23_snippet_31\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.cta_group.kind.block_scale.scale_vec::4X.collector::a::fill [d_tmem], a_desc, b_desc, idesc, [scale_A_tmem], [scale_B_tmem], enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .kind      = { .kind::mxf4nvf4 }\n// .cta_group = { .cta_group::1, .cta_group::2 }\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_mma_block_scale_vec_4x_tmem_a_collector_a_fill(\n  cuda::ptx::kind_mxf4nvf4_t,\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  uint32_t scale_A_tmem,\n  uint32_t scale_B_tmem,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Basic CUDA Thread Synchronization\nDESCRIPTION: Demonstrates basic thread synchronization using __syncthreads() barrier with a simple hello_world kernel. Shows how cudaDeviceSynchronize ensures device thread progress.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/execution_model.rst#2025-04-23_snippet_5\n\nLANGUAGE: cuda\nCODE:\n```\n__global__ void hello_world() { __syncthreads(); }\nint main() {\n    hello_world<<<1,2>>>();\n    return (int)cudaDeviceSynchronize();\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing WarpReduce Specialization in CUB\nDESCRIPTION: This snippet shows how CUB implements specialization for WarpReduce based on whether the logical warp size is a power of two, using conditional type selection.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/developer_overview.rst#2025-04-23_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\nusing InternalWarpReduce = cuda::std::conditional_t<\n  IS_POW_OF_TWO,\n  detail::WarpReduceShfl<T, LOGICAL_WARP_THREADS>,  // shuffle-based implementation\n  detail::WarpReduceSmem<T, LOGICAL_WARP_THREADS>>; // smem-based implementation\n```\n\n----------------------------------------\n\nTITLE: Defining CUDA Thread Scopes Enumeration\nDESCRIPTION: Defines the thread scope enumeration in the cuda namespace to specify different levels of thread synchronization, from system-wide to individual thread scope.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/memory_model.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\nnamespace cuda {\n\nenum thread_scope {\n  thread_scope_system,\n  thread_scope_device,\n  thread_scope_block,\n  thread_scope_thread\n};\n\n}  // namespace cuda\n```\n\n----------------------------------------\n\nTITLE: Basic Grid-Strided CUDA Kernel Implementation\nDESCRIPTION: Implementation of a basic grid-strided kernel that performs vector updates using cooperative groups.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/memory_access_properties/annotated_ptr.rst#2025-04-23_snippet_8\n\nLANGUAGE: cuda\nCODE:\n```\n__global__ void update(int* const x, int const* const a, int const* const b, size_t N) {\n    auto g = cooperative_groups::this_grid();\n    for (int i = g.thread_rank(); idx < N; idx += g.size()) {\n        x[i] = a[i] * x[i] + b[i];\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Parallel Reduction using CCCL (C++/CUDA)\nDESCRIPTION: This example demonstrates how to use CCCL functionality from Thrust, CUB, and libcudacxx to implement a parallel reduction kernel. It compares a custom implementation using CUB and libcudacxx with Thrust's reduce algorithm.\nSOURCE: https://github.com/nvidia/cccl/blob/main/README.md#2025-04-23_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n#include <thrust/execution_policy.h>\n#include <thrust/device_vector.h>\n#include <cub/block/block_reduce.cuh>\n#include <cuda/atomic>\n#include <cuda/cmath>\n#include <cuda/std/span>\n#include <cstdio>\n\ntemplate <int block_size>\n__global__ void reduce(cuda::std::span<int const> data, cuda::std::span<int> result) {\n  using BlockReduce = cub::BlockReduce<int, block_size>;\n  __shared__ typename BlockReduce::TempStorage temp_storage;\n\n  int const index = threadIdx.x + blockIdx.x * blockDim.x;\n  int sum = 0;\n  if (index < data.size()) {\n    sum += data[index];\n  }\n  sum = BlockReduce(temp_storage).Sum(sum);\n\n  if (threadIdx.x == 0) {\n    cuda::atomic_ref<int, cuda::thread_scope_device> atomic_result(result.front());\n    atomic_result.fetch_add(sum, cuda::memory_order_relaxed);\n  }\n}\n\nint main() {\n\n  // Allocate and initialize input data\n  int const N = 1000;\n  thrust::device_vector<int> data(N);\n  thrust::fill(data.begin(), data.end(), 1);\n\n  // Allocate output data\n  thrust::device_vector<int> kernel_result(1);\n\n  // Compute the sum reduction of `data` using a custom kernel\n  constexpr int block_size = 256;\n  int const num_blocks = cuda::ceil_div(N, block_size);\n  reduce<block_size><<<num_blocks, block_size>>>(cuda::std::span<int const>(thrust::raw_pointer_cast(data.data()), data.size()),\n                                                 cuda::std::span<int>(thrust::raw_pointer_cast(kernel_result.data()), 1));\n\n  auto const err = cudaDeviceSynchronize();\n  if (err != cudaSuccess) {\n    std::cout << \"Error: \" << cudaGetErrorString(err) << std::endl;\n    return -1;\n  }\n\n  int const custom_result = kernel_result[0];\n\n  // Compute the same sum reduction using Thrust\n  int const thrust_result = thrust::reduce(thrust::device, data.begin(), data.end(), 0);\n\n  // Ensure the two solutions are identical\n  std::printf(\"Custom kernel sum: %d\\n\", custom_result);\n  std::printf(\"Thrust reduce sum: %d\\n\", thrust_result);\n  assert(kernel_result[0] == thrust_result);\n  return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Demonstrating warp shuffle operations in CUDA\nDESCRIPTION: Example kernel demonstrating the usage of various warp shuffle operations, including shuffling arrays, structs, and handling different warp sizes.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/warp/warp_shuffle.rst#2025-04-23_snippet_5\n\nLANGUAGE: cuda\nCODE:\n```\n#include <cuda/std/array>\n#include <cuda/std/type_traits>\n#include <cuda/warp>\n#include <cstdio>\n\nstruct MyStruct {\n    double x;\n    int    y;\n};\n\n__global__ void warp_shuffle_kernel() {\n    cuda::std::integral_constant<int, 16> half_warp;\n    auto                     laneid      = cuda::ptx::get_sreg_laneid();\n    int                      raw_array[] = {threadIdx.x, threadIdx.x + 1, threadIdx.x + 2};\n    cuda::std::array<int, 3> array       = {threadIdx.x, threadIdx.x + 1, threadIdx.x + 2};\n    MyStruct                 my_structs{static_cast<double>(threadIdx.x), threadIdx.x + 1};\n    if (laneid < 16) {\n        // lanes [0, 15] get an array with values {5, 6, 7}\n        auto ret = cuda::device::warp_shuffle_idx(raw_array, 5, 0xFFFF, half_warp);\n        printf(\"lane %2d: [%d, %d, %d]\\n\", laneid, ret.data[0], ret.data[1], ret.data[2]);\n\n        // lanes [1, 15] get an array with values {threadIdx.x - 1, threadIdx.x, threadIdx.x + 1}\n        // lane 0 keeps the original values\n        auto array_ret = cuda::device::warp_shuffle_up(array, 1, half_warp).data;\n        printf(\"lane %2d: [%d, %d, %d]\\n\", laneid, array[0], array[1], array_ret[2]);\n    }\n    // lanes [0, 13] get my_structs with values {threadIdx.x + 2, threadIdx.x + 3} and pred=true\n    auto ret = cuda::device::warp_shuffle_down<16>(my_structs, 2);\n    printf(\"lane %2d: {%f, %d}, pred %d\\n\", laneid, ret.data.x, ret.data.y, ret.pred);\n}\n\nint main() {\n    warp_shuffle_kernel<<<1, 32>>>();\n    cudaDeviceSynchronize();\n    return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Demonstrating CUB BlockReduce Usage in CUDA Kernel\nDESCRIPTION: This snippet shows how to use cub::BlockReduce in a CUDA kernel, including selecting the class, querying and allocating temporary storage, and invoking the reduction algorithm.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/developer_overview.rst#2025-04-23_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n__global__ void kernel(int* per_block_results)\n{\n  // (1) Select the desired class\n  // `cub::BlockReduce` is a class template that must be instantiated for the\n  // input data type and the number of threads. Internally the class is\n  // specialized depending on the data type, number of threads, and hardware\n  // architecture. Type aliases are often used for convenience:\n  using BlockReduce = cub::BlockReduce<int, 128>;\n  // (2) Query the temporary storage\n  // The type and amount of temporary storage depends on the selected instantiation\n  using TempStorage = typename BlockReduce::TempStorage;\n  // (3) Allocate the temporary storage\n  __shared__ TempStorage temp_storage;\n  // (4) Pass the temporary storage\n  // Temporary storage is passed to the constructor of the `BlockReduce` class\n  BlockReduce block_reduce{temp_storage};\n  // (5) Invoke the algorithm\n  // The `Sum()` member function performs the sum reduction of `thread_data` across all 128 threads\n  int thread_data[4] = {1, 2, 3, 4};\n  int block_result = block_reduce.Sum(thread_data);\n\n  per_block_results[blockIdx.x] = block_result;\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing AXPY Kernel with CUDASTF in C++\nDESCRIPTION: Demonstrates a complete example of implementing the AXPY kernel using CUDASTF, including context creation, logical data handling, and task submission.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\n#include <cuda/experimental/stf.cuh>\n\nusing namespace cuda::experimental::stf;\n\ntemplate <typename T>\n__global__ void axpy(T a, slice<T> x, slice<T> y) {\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    int nthreads = gridDim.x * blockDim.x;\n\n    for (int ind = tid; ind < x.size(); ind += nthreads) {\n        y(ind) += a * x(ind);\n    }\n}\n\nint main(int argc, char** argv) {\n    context ctx;\n\n    const size_t N = 16;\n    double X[N], Y[N];\n\n    for (size_t ind = 0; ind < N; ind++) {\n        X[ind] = sin((double)ind);\n        Y[ind] = col((double)ind);\n    }\n\n    auto lX = ctx.logical_data(X);\n    auto lY = ctx.logical_data(Y);\n\n    double alpha = 3.14;\n\n    /* Compute Y = Y + alpha X */\n    ctx.task(lX.read(), lY.rw())->*[&](cudaStream_t s, auto sX, auto sY) {\n        axpy<<<16, 128, 0, s>>>(alpha, sX, sY);\n    };\n\n    ctx.finalize();\n}\n```\n\n----------------------------------------\n\nTITLE: Host-Side Task Execution with host_launch\nDESCRIPTION: Example showing how to execute tasks on the host CPU using the host_launch API. This allows for executing code on the host while still utilizing the task and data management system provided by CUDASTF.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_60\n\nLANGUAGE: cpp\nCODE:\n```\nctx.host_launch(logicalData1.accessMode(), logicalData2.accessMode() ...)\n    ->*[capture list](auto data1, auto data2 ...) {\n        // Host-based task implementation here\n    };\n```\n\n----------------------------------------\n\nTITLE: Processing Benchmark Directory and Adding Targets in CMake\nDESCRIPTION: Defines a CMake function `add_bench_dir` that processes a given benchmark directory. It finds all `.cu` files, iterates through the list of `THRUST_TARGETS`, determines the correct source file (original `.cu` for CUDA targets, wrapped `.cpp` otherwise using `thrust_wrap_bench_in_cpp`), generates a unique benchmark name based on the Thrust configuration prefix and file path, registers the benchmark using `register_cccl_benchmark`, creates the executable target using `add_bench`, links it against the corresponding Thrust target, clones properties, and adds specific compile options (like `--extended-lambda`) for CUDA targets.\nSOURCE: https://github.com/nvidia/cccl/blob/main/thrust/benchmarks/CMakeLists.txt#2025-04-23_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(add_bench_dir bench_dir)\n  file(GLOB bench_srcs CONFIGURE_DEPENDS \"${bench_dir}/*.cu\")\n  file(RELATIVE_PATH bench_prefix \"${benches_root}\" \"${bench_dir}\")\n  file(TO_CMAKE_PATH \"${bench_prefix}\" bench_prefix)\n  string(REPLACE \"/\" \".\" bench_prefix \"${bench_prefix}\")\n\n  foreach(bench_src IN LISTS bench_srcs)\n    foreach(thrust_target IN LISTS THRUST_TARGETS)\n      thrust_get_target_property(config_prefix ${thrust_target} PREFIX)\n      thrust_get_target_property(config_device ${thrust_target} DEVICE)\n\n      # Wrap the .cu file in .cpp for non-CUDA backends\n      if (\"CUDA\" STREQUAL \"${config_device}\")\n        set(real_bench_src \"${bench_src}\")\n      else()\n        thrust_wrap_bench_in_cpp(real_bench_src \"${bench_src}\" ${thrust_target})\n      endif()\n\n      get_filename_component(bench_name \"${bench_src}\" NAME_WLE)\n      string(PREPEND bench_name \"${config_prefix}.${bench_prefix}.\")\n      register_cccl_benchmark(\"${bench_name}\" \"\")\n\n      string(APPEND bench_name \".base\")\n      add_bench(base_bench_target ${bench_name} \"${real_bench_src}\")\n      target_link_libraries(${bench_name} PRIVATE ${thrust_target})\n      thrust_clone_target_properties(${bench_name} ${thrust_target})\n\n      if (\"CUDA\" STREQUAL \"${config_device}\")\n        target_compile_options(${bench_name} PRIVATE \"--extended-lambda\")\n      endif()\n    endforeach()\n  endforeach()\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Launching CUDA Kernels with cuda_kernel Construct in C++\nDESCRIPTION: Illustrates how to use the cuda_kernel construct to launch CUDA kernels efficiently, especially useful when using a CUDA graph backend. It shows how to specify kernel parameters and data dependencies.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_41\n\nLANGUAGE: cpp\nCODE:\n```\nctx.cuda_kernel(lX.read(), lY.rw())->*[&](auto dX, auto dY) {\n  // calls __global__ void axpy(double a, slice<const double> x, slice<double> y);\n  // similarly to axpy<<<16, 128, 0, ...>>>(alpha, dX, dY)\n  return cuda_kernel_desc{axpy, 16, 128, 0, alpha, dX, dY};\n};\n```\n\nLANGUAGE: cpp\nCODE:\n```\nauto t = ctx.cuda_kernel();\nt.add_deps(lX.read());\nt.add_deps(lY.rw());\nt->*[&]() {\n  auto dX = t.template get<slice<double>>(0);\n  auto dY = t.template get<slice<double>>(1);\n  return cuda_kernel_desc{axpy, 16, 128, 0, alpha, dX, dY};\n};\n```\n\n----------------------------------------\n\nTITLE: Using parallel_for for 1D Array Processing - CUDASTF C++\nDESCRIPTION: Demonstrates applying the parallel_for construct to process elements of a 1D array on a specific CUDA device. The body is a device lambda invoked for each index. Requires CUDASTF context, logical_data, an array A, and proper lambda syntax. Modifies array in-place; lambda signature must match the data shape.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_30\n\nLANGUAGE: cpp\nCODE:\n```\n   int A[128];\n   auto lA = ctx.logical_data(A);\n\n   ctx.parallel_for(exec_place::device(1), lA.shape(), lA.write())->*[] __device__ (size_t i, auto sA) {\n       A(i) = 2*i + 1;\n   };\n```\n\n----------------------------------------\n\nTITLE: Demonstrating CUB MaxPolicy Optimization for Kernel Instantiation in C++\nDESCRIPTION: This comprehensive C++ CUDA example demonstrates the CUB dispatch layer optimization using `MaxPolicy`. It defines several compute capability policies (sm60-sm90) using `ChainedPolicy`, two kernels (`bad_kernel` templated on `ActivePolicy`, `good_kernel` on `MaxPolicy`), and dispatch functions (`invoke_with_best_policy_bad/good`). The `main` function gets the runtime compute capability and calls both dispatch functions. This illustrates how `good_kernel` results in fewer template instantiations compared to `bad_kernel`, reducing compile time while achieving the same runtime behavior. Dependencies include `<cuda/std/type_traits>` and `<cstdio>`.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/developer_overview.rst#2025-04-23_snippet_16\n\nLANGUAGE: cpp\nCODE:\n```\n#include <cuda/std/type_traits>\n#include <cstdio>\n\n/// In device code, CUB_PTX_ARCH expands to the PTX version for which we are\n/// compiling. In host code, CUB_PTX_ARCH's value is implementation defined.\n#ifndef CUB_PTX_ARCH\n#  if !defined(__CUDA_ARCH__)\n#    define CUB_PTX_ARCH 0\n#  else\n#    define CUB_PTX_ARCH __CUDA_ARCH__\n#  endif\n#endif\n\ntemplate <int PTXVersion, typename Policy, typename PrevPolicy>\nstruct ChainedPolicy {\n  static constexpr int cc = PTXVersion;\n  using ActivePolicy      = ::cuda::std::conditional_t<CUB_PTX_ARCH<PTXVersion, PrevPolicy, Policy>;\n  using PrevPolicyT       = PrevPolicy;\n  using PolicyT           = Policy;\n};\n\ntemplate <int PTXVersion, typename Policy>\nstruct ChainedPolicy<PTXVersion, Policy, Policy> {\n  static constexpr int cc = PTXVersion;\n  using ActivePolicy      = Policy;\n  using PrevPolicyT       = Policy;\n  using PolicyT           = Policy;\n};\n\nstruct sm60 : ChainedPolicy<600, sm60, sm60> { static constexpr int BLOCK_THREADS = 256; };\nstruct sm70 : ChainedPolicy<700, sm70, sm60> { static constexpr int BLOCK_THREADS = 512; };\nstruct sm80 : ChainedPolicy<800, sm80, sm70> { static constexpr int BLOCK_THREADS = 768; };\nstruct sm90 : ChainedPolicy<900, sm90, sm80> { static constexpr int BLOCK_THREADS = 1024; };\n\nusing MaxPolicy = sm90;\n\n// Kernel instantiated with ActivePolicy\ntemplate <typename ActivePolicy>\n__launch_bounds__(ActivePolicy::BLOCK_THREADS) __global__ void bad_kernel() {\n  if (threadIdx.x == 0) {\n    std::printf(\"launch bounds %d; block threads %d\\n\", ActivePolicy::BLOCK_THREADS, blockDim.x);\n  }\n}\n\n// Kernel instantiated with MaxPolicy\ntemplate <typename MaxPolicy>\n__launch_bounds__(MaxPolicy::ActivePolicy::BLOCK_THREADS) __global__ void good_kernel() {\n  if (threadIdx.x == 0) {\n    std::printf(\"launch bounds %d; block threads %d\\n\", MaxPolicy::ActivePolicy::BLOCK_THREADS, blockDim.x);\n  }\n}\n\n// Function to dispatch kernel with the correct ActivePolicy\ntemplate <typename T>\nvoid invoke_with_best_policy_bad(int runtime_cc) {\n  if (runtime_cc < T::cc) {\n    invoke_with_best_policy_bad<typename T::PrevPolicyT>(runtime_cc);\n  } else {\n    bad_kernel<typename T::PolicyT><<<1, T::PolicyT::BLOCK_THREADS>>>();\n  }\n}\n\n// Function to dispatch kernel with the correct MaxPolicy\ntemplate <typename T>\nvoid invoke_with_best_policy_good(int runtime_cc) {\n  if (runtime_cc < T::cc) {\n    invoke_with_best_policy_good<typename T::PrevPolicyT>(runtime_cc);\n  } else {\n    good_kernel<MaxPolicy><<<1, T::PolicyT::BLOCK_THREADS>>>();\n  }\n}\n\nvoid call_kernel() {\n  cudaDeviceProp deviceProp;\n  cudaGetDeviceProperties(&deviceProp, 0);\n  int runtime_cc = deviceProp.major * 100 + deviceProp.minor * 10;\n  std::printf(\"runtime cc %d\\n\", runtime_cc);\n\n  invoke_with_best_policy_bad<MaxPolicy>(runtime_cc);\n  invoke_with_best_policy_good<MaxPolicy>(runtime_cc);\n}\n\nint main() {\n  call_kernel();\n  cudaDeviceSynchronize();\n}\n```\n\n----------------------------------------\n\nTITLE: Templated Matrix Multiply Accumulate with TMEM Collector B2 Lastuse - Zero Column Mask\nDESCRIPTION: Device function template for tensor core matrix multiply accumulate (MMA) operations with shared memory (TMEM) operands. This variation supports zero column masking and handles various data types through template specialization.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_52\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.ws.cta_group.kind.collector::b2::lastuse [d_tmem], [a_tmem], b_desc, idesc, enable_input_d, zero_column_mask_desc; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1 }\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_tmem_a_collector_b2_lastuse(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint32_t a_tmem,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d,\n  uint64_t zero_column_mask_desc);\n```\n\n----------------------------------------\n\nTITLE: CUDA Shuffle Sync Template Declarations\nDESCRIPTION: Defines template functions for CUDA shuffle sync operations. Includes four main operation types (idx, up, down, bfly) each with two variants - with and without predicate handling. All functions take a data value, lane index offset, clamp segment mask, and lane mask parameters. Functions are device-only and marked nodiscard.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/manual/shfl_sync.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\n// PTX ISA 6.0\n// shfl.sync.mode.b32  d[|p], a, b, c, membermask;\n//   .mode = { .up, .down, .bfly, .idx };\n\ntemplate<typename T>\n[[nodiscard]] __device__ static inline\nT shfl_sync_idx(T        data,\n               uint32_t lane_idx_offset,\n               uint32_t clamp_segmask,\n               uint32_t lane_mask) noexcept;\n\ntemplate<typename T>\n[[nodiscard]] __device__ static inline\nT shfl_sync_idx(T        data,\n               bool&    pred,\n               uint32_t lane_idx_offset,\n               uint32_t clamp_segmask,\n               uint32_t lane_mask) noexcept;\n\ntemplate<typename T>\n[[nodiscard]] __device__ static inline\nT shfl_sync_up(T        data,\n              uint32_t lane_idx_offset,\n              uint32_t clamp_segmask,\n              uint32_t lane_mask) noexcept;\n\ntemplate<typename T>\n[[nodiscard]] __device__ static inline\nT shfl_sync_up(T        data,\n              bool&    pred,\n              uint32_t lane_idx_offset,\n              uint32_t clamp_segmask,\n              uint32_t lane_mask) noexcept;\n\ntemplate<typename T>\n[[nodiscard]] __device__ static inline\nT shfl_sync_down(T        data,\n                uint32_t lane_idx_offset,\n                uint32_t clamp_segmask,\n                uint32_t lane_mask) noexcept;\n\ntemplate<typename T>\n[[nodiscard]] __device__ static inline\nT shfl_sync_down(T        data,\n                bool&    pred,\n                uint32_t lane_idx_offset,\n                uint32_t clamp_segmask,\n                uint32_t lane_mask) noexcept;\n\ntemplate<typename T>\n[[nodiscard]] __device__ static inline\nT shfl_sync_bfly(T        data,\n                uint32_t lane_idx_offset,\n                uint32_t clamp_segmask,\n                uint32_t lane_mask) noexcept;\n\ntemplate<typename T>\n[[nodiscard]] __device__ static inline\nT shfl_sync_bfly(T        data,\n                bool&    pred,\n                uint32_t lane_idx_offset,\n                uint32_t clamp_segmask,\n                uint32_t lane_mask) noexcept;\n```\n\n----------------------------------------\n\nTITLE: Generating and Sorting Random Numbers in Parallel with Thrust in C++\nDESCRIPTION: This example demonstrates how to generate random numbers serially on the host, transfer them to a parallel device, sort them, and copy the sorted results back to the host using Thrust. It utilizes host and device vectors, random number generation, and parallel sorting algorithms.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/index.rst#2025-04-23_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n#include <thrust/host_vector.h>\n#include <thrust/device_vector.h>\n#include <thrust/generate.h>\n#include <thrust/sort.h>\n#include <thrust/copy.h>\n#include <thrust/random.h>\n\nint main() {\n// Generate 32M random numbers serially.\nthrust::default_random_engine rng(1337);\nthrust::uniform_int_distribution<int> dist;\nthrust::host_vector<int> h_vec(32 << 20);\nthrust::generate(h_vec.begin(), h_vec.end(), [&] { return dist(rng); });\n\n// Transfer data to the device.\nthrust::device_vector<int> d_vec = h_vec;\n\n// Sort data on the device.\nthrust::sort(d_vec.begin(), d_vec.end());\n\n// Transfer data back to host.\nthrust::copy(d_vec.begin(), d_vec.end(), h_vec.begin());\n}\n```\n\n----------------------------------------\n\nTITLE: CUDA Matrix Processing Example\nDESCRIPTION: Complete example showing matrix initialization, CUDA kernel definition for matrix processing, and result verification. The kernel modifies matrix elements using a 2D thread block configuration, with automatic synchronization between host and device memory.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf/custom_data_interface.rst#2025-04-23_snippet_6\n\nLANGUAGE: C++\nCODE:\n```\ntemplate <typename T>\n__global__ void kernel(matrix<T> M) {\n    int tid_x = blockIdx.x * blockDim.x + threadIdx.x;\n    int nthreads_x = gridDim.x * blockDim.x;\n\n    int tid_y = blockIdx.y * blockDim.y + threadIdx.y;\n    int nthreads_y = gridDim.y * blockDim.y;\n\n    for (int x = tid_x; x < M.m; x += nthreads_x)\n        for (int y = tid_y; y < M.n; y += nthreads_y) {\n            M(x, y) += -x + 7 * y;\n        }\n}\n\nint main() {\n    stream_ctx ctx;\n\n    const size_t m = 8;\n    const size_t n = 10;\n    std::vector<int> v(m * n);\n\n    for (size_t j = 0; j < n; j++)\n        for (size_t i = 0; i < m; i++) {\n            v[i + j * m] = 17 * i + 23 * j;\n        }\n\n    matrix<int> M(m, n, &v[0]);\n\n    auto lM = ctx.logical_data(M);\n\n    // M(i,j) +=  -i + 7*i\n    ctx.task(lM.rw())->*[](cudaStream_t s, auto dM) { kernel<<<dim3(8, 8), dim3(8, 8), 0, s>>>(dM); };\n\n    ctx.sync();\n\n    for (size_t j = 0; j < n; j++)\n        for (size_t i = 0; i < m; i++) {\n            assert(v[i + j * m] == (17 * i + 23 * j) + (-i + 7*i));\n        }\n}\n```\n\n----------------------------------------\n\nTITLE: Building the Thrust Flexible Device System Example using CMake and CCCL\nDESCRIPTION: These commands demonstrate the complete workflow for cloning, configuring, building, and testing the Thrust flexible device system example. The CCCL_THRUST_DEVICE_SYSTEM option can be set to CUDA, TBB, OMP, or CPP to specify the desired device backend.\nSOURCE: https://github.com/nvidia/cccl/blob/main/examples/thrust_flexible_device_system/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Checkout example and prepare build directory:\ngit clone https://github.com/NVIDIA/cccl.git\ncd cccl/thrust_flexible_device_system\nmkdir build\ncd build\n\n# Configure:\ncmake .. -DCCCL_THRUST_DEVICE_SYSTEM=CUDA # or TBB, OMP, CPP\n\n# Build:\ncmake --build .\n\n# Run:\nctest -V\n```\n\n----------------------------------------\n\nTITLE: Thread Hierarchy Examples\nDESCRIPTION: Examples of different thread hierarchy specifications for the launch construct. These show various ways to structure parallelism from simple concurrent threads to complex nested hierarchies with memory allocation and hardware scopes.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_67\n\nLANGUAGE: cpp\nCODE:\n```\ncon(128);\npar();\ncon<128>();\ncon(128, par(32));\ncon(128, mem(64), con(32));\npar(hw_scope::device | hw_scope::block, par<128>(hw_scope::thread));\n```\n\n----------------------------------------\n\nTITLE: Implementing Cluster Communication using CUDA PTX st.async\nDESCRIPTION: A CUDA kernel that demonstrates inter-block communication using st.async PTX instruction. The code uses cluster groups, shared memory barriers, and asynchronous stores to transfer data between blocks in a cluster. It includes initialization of barriers, mapping of remote memory, and synchronized data transfer operations.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/examples/st.async.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\n#include <cstdio>\n#include <cuda/ptx>\n#include <cuda/barrier>\n#include <cooperative_groups.h>\n\n__global__ void __cluster_dims__(8, 1, 1) kernel()\n{\n  using cuda::ptx::sem_release;\n  using cuda::ptx::sem_acquire;\n  using cuda::ptx::space_cluster;\n  using cuda::ptx::space_shared;\n  using cuda::ptx::scope_cluster;\n\n  namespace cg = cooperative_groups;\n  cg::cluster_group cluster = cg::this_cluster();\n\n  using barrier_t = cuda::barrier<cuda::thread_scope_block>;\n\n#pragma nv_diag_suppress static_var_with_dynamic_init\n  __shared__ int receive_buffer[4];\n  __shared__ barrier_t bar;\n  init(&bar, blockDim.x);\n\n  // Sync cluster to ensure remote barrier is initialized.\n  cluster.sync();\n\n  // Get address of remote cluster barrier:\n  unsigned int other_block_rank = cluster.block_rank() ^ 1;\n  uint64_t * remote_bar = cluster.map_shared_rank(cuda::device::barrier_native_handle(bar), other_block_rank);\n  int * remote_buffer = cluster.map_shared_rank(&receive_buffer[0], other_block_rank);\n\n  // Arrive on local barrier:\n  uint64_t arrival_token;\n  if (threadIdx.x == 0) {\n    arrival_token = cuda::ptx::mbarrier_arrive_expect_tx(sem_release, scope_cluster, space_shared, cuda::device::barrier_native_handle(bar), sizeof(receive_buffer));\n  } else {\n    arrival_token = cuda::ptx::mbarrier_arrive(sem_release, scope_cluster, space_shared, cuda::device::barrier_native_handle(bar));\n  }\n\n  if (threadIdx.x == 0) {\n    printf(\"[block %d] arrived with expected tx count = %llu\\n\", cluster.block_rank(), sizeof(receive_buffer));\n  }\n\n  // Send bytes to remote buffer, arriving on remote barrier\n  if (threadIdx.x == 0) {\n    cuda::ptx::st_async(remote_buffer, {int(cluster.block_rank()), 2, 3, 4}, remote_bar);\n  }\n\n  if (threadIdx.x == 0) {\n    printf(\"[block %d] st_async to %p, %p\\n\",\n           cluster.block_rank(),\n           remote_buffer,\n           remote_bar\n    );\n  }\n\n  // Wait on local barrier:\n  while(!cuda::ptx::mbarrier_try_wait(sem_acquire, scope_cluster, cuda::device::barrier_native_handle(bar), arrival_token)) {}\n\n  // Print received values:\n  if (threadIdx.x == 0) {\n    printf(\n      \"[block %d] receive_buffer = { %d, %d, %d, %d }\\n\",\n      cluster.block_rank(),\n      receive_buffer[0], receive_buffer[1], receive_buffer[2], receive_buffer[3]\n    );\n  }\n}\n\nint main() {\n  kernel<<<8, 128>>>();\n  cudaDeviceSynchronize();\n}\n```\n\n----------------------------------------\n\nTITLE: Demonstrating WarpReduce Temporary Storage Usage in CUB\nDESCRIPTION: This code shows how to allocate and use temporary storage for WarpReduce operations in CUB, including potential synchronization issues between multiple invocations.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/developer_overview.rst#2025-04-23_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nusing WarpReduce = cub::WarpReduce<int>;\n\n// Allocate WarpReduce shared memory for four warps\n__shared__ WarpReduce::TempStorage temp_storage[4];\n\n// Get this thread's warp id\nint warp_id = threadIdx.x / 32;\nint aggregate_1 = WarpReduce(temp_storage[warp_id]).Sum(thread_data_1);\n// illegal, has to add `__syncwarp()` between the two\nint aggregate_2 = WarpReduce(temp_storage[warp_id]).Sum(thread_data_2);\n// illegal, has to add `__syncwarp()` between the two\nfoo(temp_storage[warp_id]);\n```\n\n----------------------------------------\n\nTITLE: Using proclaim_return_type with Device Lambda in CUDA\nDESCRIPTION: Example demonstrating how to use proclaim_return_type with a device lambda. The function wraps a lambda with an explicitly proclaimed return type, which is then passed to a kernel function. This solves potential issues with return type deduction for device lambdas in host code.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/functional/proclaim_return_type.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n#include <cuda/functional>\n\ntemplate <class T, class Fn>\n__global__ void example_kernel(T *out, Fn fn) {\n  *out = fn();\n}\n\n__host__ void example() {\n  auto fn = cuda::proclaim_return_type<char>([] __device__ () { return 'd'; });\n  using rt = cuda::std::invoke_result_t<decltype(fn)>;\n\n  rt* out {};\n  cudaMalloc(&out, sizeof(rt));\n\n  example_kernel<<<1, 1>>>(out, fn);\n\n  // ...\n}\n```\n\n----------------------------------------\n\nTITLE: Including CUDASTF Headers and Namespace\nDESCRIPTION: Basic code snippet showing how to include CUDASTF headers and use its namespace.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_57\n\nLANGUAGE: cpp\nCODE:\n```\n#include <cudastf/cudastf.h>\n\nusing cuda::experimental::stf;\n```\n\n----------------------------------------\n\nTITLE: Implementing Warp-Level Sum Reduction in CUDA C++\nDESCRIPTION: This code implements a device function for performing a sum reduction operation within a CUDA warp using internal warp reduce functionality.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/developer_overview.rst#2025-04-23_snippet_6\n\nLANGUAGE: C++\nCODE:\n```\n__device__ __forceinline__ T Sum(T input, int valid_items) {\n  return InternalWarpReduce(temp_storage)\n      .Reduce(input, valid_items, ::cuda::std::plus<>{});\n}\n```\n\n----------------------------------------\n\nTITLE: CUDA Atomic Reference Usage Example\nDESCRIPTION: Demonstrates different use cases of cuda::atomic_ref with various thread scopes including system, device, and block-level atomics. Shows usage with different memory types including global, pinned, and shared memory.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/synchronization_primitives/atomic_ref.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n#include <cuda/atomic>\n\n__global__ void example_kernel(int *gmem, int *pinned_mem) {\n  // This atomic is suitable for all threads in the system.\n  cuda::atomic_ref<int, cuda::thread_scope_system> a(*pinned_mem);\n\n  // This atomic has the same type as the previous one (`a`).\n  cuda::atomic_ref<int> b(*pinned_mem);\n\n  // This atomic is suitable for all threads on the current processor (e.g. GPU).\n  cuda::atomic_ref<int, cuda::thread_scope_device> c(*gmem);\n\n  __shared__ int shared_v;\n  // This atomic is suitable for threads in the same thread block.\n  cuda::atomic_ref<int, cuda::thread_scope_block> d(shared_v);\n}\n```\n\n----------------------------------------\n\nTITLE: Allocating Temporary Storage for CUB Algorithm\nDESCRIPTION: Queries the required temporary storage size for a CUB algorithm and allocates it using a device vector. This is the first of two required calls when using most CUB algorithms.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/tuning.rst#2025-04-23_snippet_6\n\nLANGUAGE: c++\nCODE:\n```\nstd::size_t temp_size;\ndispatch_t::Dispatch(nullptr,\n                   temp_size,\n                   d_in,\n                   d_out,\n                   static_cast<offset_t>(elements),\n                   0 /* stream */);\n\nthrust::device_vector<char> temp(temp_size);\nauto *temp_storage = thrust::raw_pointer_cast(temp.data());\n```\n\n----------------------------------------\n\nTITLE: CUDA Kernel with Slice Parameters\nDESCRIPTION: CUDA kernel implementation of AXPY operation using slice data structure for data access. Shows how slices can be passed as kernel arguments and accessed within device code.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_9\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename T>\n__global__ void axpy(T a, slice<T> x, slice<T> y) {\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    int nthreads = gridDim.x * blockDim.x;\n\n    for (int ind = tid; ind < x.size(); ind += nthreads) {\n        y(ind) += a * x(ind);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Example Usage of CUDA Device Barrier with Transaction\nDESCRIPTION: Demonstrates how to use barrier_arrive_tx in a CUDA kernel, including feature flag checking and basic barrier synchronization pattern.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/synchronization_primitives/barrier/barrier_arrive_tx.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n#include <cuda/barrier>\n#include <cuda/std/utility> // cuda::std::move\n\n#ifndef  __cccl_lib_local_barrier_arrive_tx\nstatic_assert(false, \"Insufficient libcu++ version: cuda::device::arrive_tx is not yet available.\");\n#endif // __cccl_lib_local_barrier_arrive_tx\n\n__global__ void example_kernel() {\n  __shared__ cuda::barrier<cuda::thread_scope_block> bar;\n  if (threadIdx.x == 0) {\n    init(&bar, blockDim.x);\n  }\n  __syncthreads();\n\n  auto token = cuda::device::barrier_arrive_tx(bar, 1, 0);\n\n  bar.wait(cuda::std::move(token));\n}\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Atomic Operation Limitations on Automatic Storage in CUDA Device Code\nDESCRIPTION: Example showing how atomic operations on automatic storage duration objects in device threads may not guarantee forward progress, unlike with host threads.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/execution_model.rst#2025-04-23_snippet_3\n\nLANGUAGE: cuda\nCODE:\n```\n__global__ void ex3() {\n    cuda::atomic<bool, cuda::thread_scope_thread> True = true;\n    while(True.load());\n}\n```\n\n----------------------------------------\n\nTITLE: Creating CUDASTF Context in C++\nDESCRIPTION: Demonstrates how to create a CUDASTF context object, which is essential for using CUDASTF APIs.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\ncontext ctx;\n```\n\n----------------------------------------\n\nTITLE: Setting Default Build Type for CUB\nDESCRIPTION: Sets the default build type to RelWithDebInfo if not already specified, and configures the available build type options for the CMake cache.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cub/CMakeLists.txt#2025-04-23_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nif (\"\" STREQUAL \"${CMAKE_BUILD_TYPE}\")\n  set(CMAKE_BUILD_TYPE \"RelWithDebInfo\" CACHE STRING \"Choose the type of build.\" FORCE)\n\n  set_property(\n    CACHE CMAKE_BUILD_TYPE\n    PROPERTY STRINGS Debug Release RelWithDebInfo MinSizeRel\n  )\nendif ()\n```\n\n----------------------------------------\n\nTITLE: Launching Multiple CUDA Kernels with cuda_kernel_chain in C++\nDESCRIPTION: Shows how to use the cuda_kernel_chain construct to execute sequences of CUDA kernels within a single task, providing an efficient alternative to multiple kernel launches.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_42\n\nLANGUAGE: cpp\nCODE:\n```\nctx.cuda_kernel_chain(lX.read(), lY.rw())->*[&](auto dX, auto dY) {\n   return ::std::vector<cuda_kernel_desc> {\n       { axpy, 16, 128, 0, alpha, dX, dY },\n       { axpy, 16, 128, 0, beta,  dX, dY },\n       { axpy, 16, 128, 0, gamma, dX, dY }\n   };\n};\n```\n\nLANGUAGE: cpp\nCODE:\n```\nauto t = ctx.cuda_kernel_chain();\nt.add_deps(lX.read());\nt.add_deps(lY.rw());\nt->*[&]() {\n  auto dX = t.template get<slice<double>>(0);\n  auto dY = t.template get<slice<double>>(1);\n  return ::std::vector<cuda_kernel_desc> {\n      { axpy, 16, 128, 0, alpha, dX, dY },\n      { axpy, 16, 128, 0, beta, dX, dY },\n      { axpy, 16, 128, 0, gamma, dX, dY }\n  };\n};\n```\n\n----------------------------------------\n\nTITLE: Implementing Vector Addition with Work Stealing in CUDA\nDESCRIPTION: Demonstrates the use of for_each_canceled_block for work-stealing in a vector addition kernel. The function allows thread blocks that complete their tasks sooner to take on additional work from slower thread blocks.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/work_stealing.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n#include <cuda/math>\n#include <cuda/functional>\n__global__ void vec_add(int* a, int* b, int* c, int n) {\n  // Extract common prologue outside the lambda, e.g.,\n  // - __shared__ or global (malloc) memory allocation\n  // - common initialization code\n  // - etc.\n\n  cuda::for_each_canceled_block<1>([=](dim3 block_idx) {\n    // block_idx may be different than the built-in blockIdx variable, that is:\n    // assert(block_idx == blockIdx); // may fail!\n    // so we need to use \"block_idx\" consistently inside for_each_canceled:\n    int idx = threadIdx.x + block_idx.x * blockDim.x;\n    if (idx < n) {\n      c[idx] += a[idx] + b[idx];\n    }\n  });\n  // Note: Calling for_each_canceled_block<1> again from this\n  // thread block exhibits undefined behavior.\n\n  // Extract common epilogue outside the lambda, e.g.,\n  // - write back shared memory to global memory\n  // - external synchronization\n  // - global memory deallocation (free)\n  // - etc.\n}\n```\n\n----------------------------------------\n\nTITLE: Defining CTA Group Tensor Core Function for 64x128b WarpX2 02_13 Variant - CUDA\nDESCRIPTION: Designed for tensor core instructions targeting CTA groups of 64x128b shape with WarpX2 02_13 layout, this device-side inline template function enables flexible invocation from CUDA kernel code. It is parameterized on the CTA group PTX type and receives the relevant operation address and descriptor as arguments. Requirements include CUDA PTX (PTX ISA 86) and the cuda::ptx types.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_cp.rst#2025-04-23_snippet_9\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.cp.cta_group.64x128b.warpx2::02_13.b8x16.b4x16_p64 [taddr], s_desc; // PTX ISA 86, SM_100a, SM_101a\\n// .cta_group = { .cta_group::1, .cta_group::2 }\\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\\n__device__ static inline void tcgen05_cp_64x128b_warpx2_02_13_b8x16_b4x16_p64(\\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\\n  uint32_t taddr,\\n  uint64_t s_desc);\n```\n\n----------------------------------------\n\nTITLE: Defining and Using Box Shape for parallel_for - CUDASTF C++\nDESCRIPTION: Illustrates defining a multidimensional box shape and passing it to parallel_for for index space iteration. Two variants shown: one with explicit extents and one with only dimensionality. Requires CUDASTF's box template (dimensions.h) and context setup. Inputs are index ranges or bounds, outputs are formatted index prints; the lambda must conform to box dimensionality.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_32\n\nLANGUAGE: cpp\nCODE:\n```\n   ctx.parallel_for(box<2>({2, 3}))->*[] __device__(size_t i, size_t j) {\n       printf(\"%ld, %ld\\n\", i, j);\n   };\n```\n\nLANGUAGE: cpp\nCODE:\n```\n   ctx.parallel_for(box({4}))->*[] __device__(size_t i) {\n       printf(\"%ld\\n\", i);\n   };\n```\n\n----------------------------------------\n\nTITLE: Using CUDA maximum and minimum function objects\nDESCRIPTION: Example demonstrating how to use the cuda::maximum and cuda::minimum function objects in both device code (kernel) and host code (with std::accumulate). Note how the void specialization handles type promotion.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/functional/maximum_minimum.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n#include <cuda/functional>\n#include <cuda/std/cstdint>\n#include <cstdio>\n#include <numeric>\n\n__global__ void maximum_minimum_kernel() {\n    uint16_t v1 = 7;\n    uint16_t v2 = 3;\n    printf(\"%d\\n\", cuda::maximum<uint16_t>{}(v1, v2)); // print \"7\" (uint16_t)\n    printf(\"%d\\n\", cuda::minimum{}(v1, v2));           // print \"3\" (int)\n}\n\nint main() {\n    maximum_minimum_kernel<<<1, 1>>>();\n    cudaDeviceSynchronize();\n    int array[] = {3, 7, 5, 2};\n    printf(\"%d\\n\", std::accumulate(array, array + 4, 0, cuda::maximum{})); // 7\n    return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Installing CCCL using Conda (Bash)\nDESCRIPTION: These commands demonstrate how to install CCCL using Conda. It shows how to add the conda-forge channel and install the CCCL package.\nSOURCE: https://github.com/nvidia/cccl/blob/main/README.md#2025-04-23_snippet_4\n\nLANGUAGE: Bash\nCODE:\n```\nconda config --add channels conda-forge\nconda install cccl\n```\n\n----------------------------------------\n\nTITLE: Implementing ThreadReduce Function in CUB\nDESCRIPTION: This code snippet shows the declaration of a ThreadReduce function in CUB, which is a single-threaded reduction operation typically used as an implementation detail.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/developer_overview.rst#2025-04-23_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\ntemplate <\n    int         LENGTH,\n    typename    T,\n    typename    ReductionOp,\n    typename    PrefixT,\n    typename    AccumT = detail::accumulator_t<ReductionOp, PrefixT, T>>\n__device__ __forceinline__ AccumT ThreadReduce(\n    T           (&input)[LENGTH],\n    ReductionOp reduction_op,\n    PrefixT     prefix)\n{\n    return ...;\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Logical Data from Array - C++\nDESCRIPTION: Example showing how to create a logical data object from a stack array. The logical data object manages data transfers between host and device memory automatically.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_8\n\nLANGUAGE: cpp\nCODE:\n```\ndouble X[N];\nauto lX = ctx.logical_data(X);\n```\n\n----------------------------------------\n\nTITLE: CUDA Stream Usage Example\nDESCRIPTION: Demonstrates using thrust::cuda::par.on(stream) syntax to execute algorithms on specific CUDA streams\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/releases/changelog.rst#2025-04-23_snippet_5\n\nLANGUAGE: CUDA\nCODE:\n```\nthrust::cuda::par.on(stream)\n```\n\n----------------------------------------\n\nTITLE: Using parallel_for for 2D Array Processing - CUDASTF C++\nDESCRIPTION: Applies parallel_for to process 2D and multi-dimensional arrays with device lambdas, showing iteration over shapes, logical data, and complex nested loops. Demands CUDASTF context, make_slice, and logical_data objects for each array. Requires correct mapping of shape dimensions to lambda parameters; for consistency, input and output shapes must be compatible.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_31\n\nLANGUAGE: cpp\nCODE:\n```\n   const size_t N = 16;\n   double X[2 * N * 2 * N];\n   double Y[N * N];\n\n   auto lx = ctx.logical_data(make_slice(&X[0], std::tuple{ 2 * N, 2 * N }, 2 * N));\n   auto ly = ctx.logical_data(make_slice(&Y[0], std::tuple{ N, N }, N));\n\n   ctx.parallel_for(lx.shape(), lx.write())->*[=] __device__(size_t i, size_t j, auto sx) { sx(i, j) = 0.1; };\n\n   ctx.parallel_for(ly.shape(), lx.read(), ly.write())->*[=] __device__(size_t i, size_t j, auto sx, auto sy) {\n       sy(i, j) = y0(i, j);\n       for (size_t ii = 0; ii < 2; ii++)\n           for (size_t jj = 0; jj < 2; jj++) {\n               sy(i, j) += sx(2 * i + ii, 2 * j + jj);\n           }\n   };\n```\n\n----------------------------------------\n\nTITLE: Implementing Matrix Hash Function\nDESCRIPTION: std::hash specialization for matrix class using cudastf::hash_all utility to combine multiple hash values.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf/custom_data_interface.rst#2025-04-23_snippet_2\n\nLANGUAGE: c++\nCODE:\n```\ntemplate <typename T>\nstruct std::hash<matrix<T>> {\n    std::size_t operator()(matrix<T> const& m) const noexcept {\n        return cudastf::hash_all(m.m, m.n, m.base);\n    }\n};\n```\n\n----------------------------------------\n\nTITLE: Complete Parameterized GPU Test Example\nDESCRIPTION: Comprehensive example showing parameterized testing with random data generation, device vector operations, and thrust algorithms. Demonstrates integration of C2H framework with Catch2 and CUDA functionality.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/test_overview.rst#2025-04-23_snippet_9\n\nLANGUAGE: c++\nCODE:\n```\n// %PARAM% BLOCK_SIZE bs 128:256\nusing block_sizes = c2h::enum_type_list<int, BLOCK_SIZE>;\nusing types = c2h::type_list<std::uint8_t, std::int32_t>;\n\nC2H_TEST(\"SCOPE FACILITY works with CONDITION\", \"[FACILITY][SCOPE]\",\n         types, block_sizes)\n{\n  using type = typename c2h::get<0, TestType>;\n  constexpr int threads_per_block = c2h::get<1, TestType>::value;\n  constexpr int max_num_items = threads_per_block;\n\n  c2h::device_vector<type> d_input(\n    GENERATE_COPY(take(2, random(0, max_num_items))));\n  c2h::gen(C2H_SEED(3), d_input);\n\n  c2h::device_vector<type> d_output(d_input.size());\n\n  SCOPE_ALGORITHM<threads_per_block>(d_input.data(),\n                                    d_output.data(),\n                                    d_input.size());\n\n  REQUIRE( d_input == d_output );\n\n  const type expected_sum = 4;\n  const type sum = thrust::reduce(c2h::device_policy, d_output.cbegin(), d_output.cend());\n  REQUIRE( sum == expected_sum);\n}\n```\n\n----------------------------------------\n\nTITLE: Using proclaim_copyable_arguments with Thrust Transform\nDESCRIPTION: Shows how to mark a function object as having copyable arguments using proclaim_copyable_arguments. This allows the implementation to use optimizations like bulk copies or vectorized loads.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/api_docs/function_objects.rst#2025-04-23_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nthrust::transform(thrust::device, a.begin(), a.end(), a.begin(),\n    cuda::std::proclaim_copyable_arguments([](const int& a, const int& b) {\n        return a + b;\n    }));\n```\n\n----------------------------------------\n\nTITLE: Defining ThreadGroup Interface in CUDA\nDESCRIPTION: Defines the core interface for thread group types, including thread scope specification, size querying, thread ranking, and synchronization methods.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/thread_groups.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\nstruct ThreadGroup {\n  static constexpr cuda::thread_scope thread_scope;\n  Integral size() const;\n  Integral thread_rank() const;\n  void sync() const;\n};\n```\n\n----------------------------------------\n\nTITLE: Invoking PTX Memory Barrier Instructions via cuda::ptx in CUDA C++\nDESCRIPTION: Illustrates using arguments to drive overload resolution when calling PTX memory barrier functions in the cuda::ptx namespace. This approach avoids dependence on template parameter order and count, improving forward-compatibility with future header revisions. The function takes semaphore, scope, space, a pointer to a barrier, and a count, returning a value as required by the PTX memory barrier operation. Requires including the cuda/ptx header and access to a valid barrier object.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\n// Use arguments to drive overload resolution:\ncuda::ptx::mbarrier_arrive_expect_tx(cuda::ptx::sem_release, cuda::ptx::scope_cta, cuda::ptx::space_shared, &bar, 1);\n\n```\n\n----------------------------------------\n\nTITLE: Defining a CUB CUDA Kernel using ChainedPolicy in C++\nDESCRIPTION: This snippet defines a CUDA kernel function template that utilizes the `ChainedPolicy` provided as a template parameter. The kernel uses `ChainedPolicy::ActivePolicy::AlgorithmPolicy` to determine the specific policy (`policy`) and agent type (`AgentAlgorithm`) to use based on the PTX version being compiled. It allocates shared memory (`temp_storage`) based on the agent's requirements and then instantiates and runs the agent (`a.Process()`). The `__launch_bounds__` are set using the `ActivePolicy` determined during the host-side dispatch.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/developer_overview.rst#2025-04-23_snippet_15\n\nLANGUAGE: cpp\nCODE:\n```\ntemplate <typename ChainedPolicy /* ... */ >\n__launch_bounds__(ChainedPolicy::ActivePolicy::AlgorithmPolicy::BLOCK_THREADS) __CUB_DETAIL_KERNEL_ATTRIBUTES\nvoid kernel(...) {\n  using policy = ChainedPolicy::ActivePolicy::AlgorithmPolicy;\n  using agent = AgentAlgorithm<policy>;\n\n  __shared__ typename agent::TempStorage temp_storage; // allocate static shared memory for agent\n\n  agent a{temp_storage, ...};\n  a.Process();\n}\n```\n\n----------------------------------------\n\nTITLE: Optimized Kernel Launch with Annotated Pointers\nDESCRIPTION: Example showing how to use annotated pointers to specify persisting and streaming memory access patterns for different arrays.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/memory_access_properties/annotated_ptr.rst#2025-04-23_snippet_11\n\nLANGUAGE: cuda\nCODE:\n```\n// Frequent accesses to \"a\" and \"b\"; infrequent accesses to \"x\" and \"y\":\ncuda::annotated_ptr<int const, cuda::access_property::persisting> a_p {a}, b_p{b};\ncuda::annotated_ptr<int, cuda::access_property::streaming> x_s{x}, y_s{y};\nupdate_template<<<grid, block>>>(x_s, a_p, b_p, N);\nupdate_template<<<grid, block>>>(y_s, a_p, b_p, N);\n\n// Infrequent accesses to \"a\" and \"b\"; frequent accesses to \"z\":\ncuda::annotated_ptr<int const, cuda::access_property::streaming> a_s {a}, b_s{b};\ncuda::annotated_ptr<int, cuda::access_property::persisting> z_p{z};\nupdate_template<<<grid, block>>>(z_p, a_s, b_s, N);\n\n// Different kernel, \"update_z\", uses \"z\" again one last time.\n// Since \"z\" was accessed as \"persisting\" by the previous kernel,\n// parts of it are more likely to have previously survived in the L2 cache.\nupdate_z<<<grid, block>>>(z, ...);\n```\n\n----------------------------------------\n\nTITLE: Checking if Launcher Requires RDC\nDESCRIPTION: Helper function that determines if a given launcher ID corresponds to a CDP (CUDA Dynamic Parallelism) launcher, which requires Relocatable Device Code (RDC) to be enabled.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cub/test/CMakeLists.txt#2025-04-23_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\n## _cub_launcher_requires_rdc\n#\n# If given launcher id corresponds to a CDP launcher, set `out_var` to 1.\nfunction(_cub_launcher_requires_rdc out_var launcher_id)\n  if (\"${launcher_id}\" STREQUAL \"1\")\n    set(${out_var} 1 PARENT_SCOPE)\n  else()\n    set(${out_var} 0 PARENT_SCOPE)\n  endif()\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Declaring and Using thrust::transform_input_output_iterator - Thrust C++\nDESCRIPTION: This code snippet describes the use of thrust::transform_input_output_iterator, a specialized iterator adapter that simultaneously acts as both an input and output iterator. It applies a user-supplied input function when reading values and an output function before writing values to the underlying wrapped iterator. This iterator is designed to facilitate transform operations where both read and write transformations are needed in a single pipeline. Dependencies include the Thrust library (version including this iterator), a C++17-compatible compiler, and CUDA support for device code. Required parameters are the underlying base iterator, an input function to be executed on dereference, and an output function to be executed on assignment. Output is an iterator that propagates these transformations through standard iterator interfaces. Limitations include the expectation that the user-provided functions are regular and compatible with the value types being iterated.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/releases/changelog.rst#2025-04-23_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\n/* Example of using thrust::transform_input_output_iterator */\n/*\n * thrust::transform_input_output_iterator<BaseIter, InputFunc, OutputFunc>\n * - Reads apply InputFunc after reading from BaseIter\n * - Writes apply OutputFunc before writing to BaseIter\n */\n// Example usage (hypothetical):\n// auto tio_iter = thrust::transform_input_output_iterator(base_iter, input_func, output_func);\n// *tio_iter // applies input_func to (*base_iter) on read\n// *tio_iter = val; // applies output_func(val) before assignment to *base_iter\n```\n\n----------------------------------------\n\nTITLE: Basic CUB Test Structure Using Catch2 in C++\nDESCRIPTION: Shows the basic structure of a CUB test using Catch2, including memory allocation, data generation, and result comparison.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/test_overview.rst#2025-04-23_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\nC2H_TEST(\"SCOPE FACILITY works with CONDITION\", \"[FACILITY][SCOPE]\")\n{\n  using type = std::int32_t;\n  constexpr int threads_per_block = 256;\n  constexpr int num_items = threads_per_block;\n\n  c2h::device_vector<type> d_input(num_items);\n  c2h::gen(C2H_SEED(3), d_input);\n  c2h::device_vector<type> d_output(d_input.size());\n  c2h::host_vector<key_t> h_reference = d_input;\n\n  std::ALGORITHM(\n      thrust::raw_pointer_cast(h_reference.data()),\n      thrust::raw_pointer_cast(h_reference.data()) + h_reference.size());\n\n  SCOPE_ALGORITHM<threads_per_block>(d_input.data(),\n                                     d_output.data(),\n                                     d_input.size());\n\n  REQUIRE( d_input == d_output );\n}\n```\n\n----------------------------------------\n\nTITLE: MMA Workspace Template for B0 Lastuse Operations\nDESCRIPTION: Template device function for MMA workspace operations with lastuse collector pattern. Supports different data types and includes zero column masking capability.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_12\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_tmem_a_collector_b0_lastuse(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint32_t a_tmem,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d,\n  uint64_t zero_column_mask_desc);\n```\n\n----------------------------------------\n\nTITLE: Defining WarpReduce Class Template in CUB\nDESCRIPTION: This snippet demonstrates the structure of the WarpReduce class template in CUB, including temporary storage handling and the Sum reduction method declaration.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/developer_overview.rst#2025-04-23_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\ntemplate <typename T,\n          int      LOGICAL_WARP_THREADS = 32>\nclass WarpReduce {\n  // ...\n  // (1)   define `_TempStorage` type\n  // ...\n  _TempStorage &temp_storage;\npublic:\n\n  // (2)   wrap `_TempStorage` in uninitialized memory\n  struct TempStorage : Uninitialized<_TempStorage> {};\n\n  __device__ __forceinline__ WarpReduce(TempStorage &temp_storage)\n  // (3)   reinterpret cast\n    : temp_storage(temp_storage.Alias())\n  {}\n\n  // (4)   actual algorithms\n  __device__ __forceinline__ T Sum(T input);\n};\n```\n\n----------------------------------------\n\nTITLE: Creating 2D Logical Data Slices\nDESCRIPTION: Shows how to create contiguous and non-contiguous 2D logical data slices from an array using the context's logical_data method.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_12\n\nLANGUAGE: cpp\nCODE:\n```\ndouble A[32 * 32];\n\n// Contiguous 2D slice\nauto lX = ctx.logical_data(make_slice(A, std::tuple { 32, 32 }, 32));\n\n// Non-contiguous 2D slice\nauto lX2 = ctx.logical_data(make_slice(A, std::tuple { 24, 32 }, 32));\n```\n\n----------------------------------------\n\nTITLE: Using CUDASTF Graph Context for Task Dependencies\nDESCRIPTION: This code demonstrates CUDASTF's graph_ctx API for creating tasks with dependencies in a CUDA graph. It shows how to create multiple tasks, define dependencies between them using add_deps(), and add empty nodes to a CUDA graph.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf/lower_level_api.rst#2025-04-23_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\ngraph_ctx ctx;\n\ndouble X[1024], Y[1024];\nauto lX = ctx.logical_data(X);\nauto lY = ctx.logical_data(Y);\n\nfor (int k = 0; k < 10; k++) {\n    graph_task t = ctx.task();\n    t.add_deps(handle_X.rw());\n    t.start();\n    cudaGraphNode_t n;\n    cuda_safe_call(cudaGraphAddEmptyNode(&n, t.get_graph(), nullptr, 0));\n    t.end();\n}\n\ngraph_task t2 = ctx.task();\nt2.add_deps(lX.read(), lY.rw());\nt2.start();\ncudaGraphNode_t n2;\ncuda_safe_call(cudaGraphAddEmptyNode(&n2, t2.get_graph(), nullptr, 0));\nt2.end();\n\nctx.sync();\n```\n\n----------------------------------------\n\nTITLE: Multi-Dimensional Logical Data Creation\nDESCRIPTION: Shows how to create 2D and 3D logical data using shape definitions without backing storage.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_14\n\nLANGUAGE: cpp\nCODE:\n```\nauto lX_2D = ctx.logical_data(shape_of<slice<double, 2>>(16, 24));\nauto lX_3D = ctx.logical_data(shape_of<slice<double, 3>>(16, 24, 10));\n```\n\n----------------------------------------\n\nTITLE: Executing CUDA Tasks with Device Stream\nDESCRIPTION: Example showing how to execute a CUDA task using a device stream with the task API. The code demonstrates launching a CUDA kernel (axpy) within a task that reads from logical data X and writes to logical data Y.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_59\n\nLANGUAGE: cpp\nCODE:\n```\nctx.task(lX.read(), lY.rw())\n    ->*[&](cudaStream_t s, auto dX, auto dY) {\n        axpy<<<16, 128, 0, s>>>(alpha, dX, dY);\n    };\n```\n\n----------------------------------------\n\nTITLE: Basic Thrust Setup with CMake\nDESCRIPTION: Demonstrates how to find the Thrust package and create a basic Thrust target with default CUDA configuration, then link it to a program.\nSOURCE: https://github.com/nvidia/cccl/blob/main/lib/cmake/thrust/README.md#2025-04-23_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfind_package(Thrust REQUIRED CONFIG)\nthrust_create_target(Thrust)\ntarget_link_libraries(MyProgram Thrust)\n```\n\n----------------------------------------\n\nTITLE: CUDA AXPY Kernel Implementation\nDESCRIPTION: Implementation of AXPY operation as a CUDA kernel with task creation showing read and read-write data access patterns.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_15\n\nLANGUAGE: cuda\nCODE:\n```\n__global__ void axpy(size_t n, double a, const double *x, double *y) {\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    int nthreads = gridDim.x * blockDim.x;\n\n    for (size_t ind = tid; ind < n; ind += nthreads) {\n        y[ind] += a * x[ind];\n    }\n}\n...\nctx.task(lX.read(), lY.rw())->*[&](cudaStream_t s, slice<const double> sX, slice<double> sY) {\n    axpy<<<16, 128, 0, s>>>(sX.size(), alpha, sX.data_handle(), sY.data_handle());\n};\n```\n\n----------------------------------------\n\nTITLE: Including CCCL Headers in a CUDA Project (C++/CUDA)\nDESCRIPTION: This snippet shows how to include CCCL headers in a CUDA project. It demonstrates the inclusion of headers from Thrust, CUB, and libcudacxx.\nSOURCE: https://github.com/nvidia/cccl/blob/main/README.md#2025-04-23_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\n#include <thrust/device_vector.h>\n#include <cub/cub.cuh>\n#include <cuda/std/atomic>\n```\n\n----------------------------------------\n\nTITLE: Basic Kernel Launch Sequence\nDESCRIPTION: Example of launching the update kernel multiple times with different input vectors.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/memory_access_properties/annotated_ptr.rst#2025-04-23_snippet_9\n\nLANGUAGE: cuda\nCODE:\n```\nupdate<<<grid, block>>>(x, a, b, N);\nupdate<<<grid, block>>>(y, a, b, N);\nupdate<<<grid, block>>>(z, a, b, N);\n```\n\n----------------------------------------\n\nTITLE: Setting up CCCL Demo Project with CMake\nDESCRIPTION: Configures a CUDA project that uses the CCCL library fetched from GitHub via CPM. The configuration includes setting the minimum CMake version, defining the project, fetching CCCL, setting CUDA architecture, creating an executable, and linking CCCL to it.\nSOURCE: https://github.com/nvidia/cccl/blob/main/examples/basic/CMakeLists.txt#2025-04-23_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.15 FATAL_ERROR)\n\nproject(CCCLDemo CUDA)\n\n# This example uses the CMake Package Manager (CPM) to simplify fetching CCCL from GitHub\n# For more information, see https://github.com/cpm-cmake/CPM.cmake\ninclude(cmake/CPM.cmake)\n\n\n# We define these as variables so they can be overridden in CI to pull from a PR instead of CCCL `main`\n# In your project, these variables are unnecessary and you can just use the values directly\nset(CCCL_REPOSITORY \"https://github.com/NVIDIA/cccl\" CACHE STRING \"Git repository to fetch CCCL from\")\nset(CCCL_TAG \"main\" CACHE STRING \"Git tag/branch to fetch from CCCL repository\")\n\n# This will automatically clone CCCL from GitHub and make the exported cmake targets available\nCPMAddPackage(\n    NAME CCCL\n    GIT_REPOSITORY \"${CCCL_REPOSITORY}\"\n    GIT_TAG ${CCCL_TAG}\n)\n\n# Default to building for the GPU on the current system\nif(NOT DEFINED CMAKE_CUDA_ARCHITECTURES)\n  set(CMAKE_CUDA_ARCHITECTURES native)\nendif()\n\n# Creates a cmake executable target for the main program\nadd_executable(example_project example.cu)\ntarget_compile_features(example_project PUBLIC cuda_std_17)\n\n# \"Links\" the CCCL Cmake target to the `example_project` executable. This configures everything needed to use\n# CCCL headers, including setting up include paths, compiler flags, etc.\ntarget_link_libraries(example_project PRIVATE CCCL::CCCL)\n\n# This is only relevant for internal testing and not needed by end users.\ninclude(CTest)\nenable_testing()\nadd_test(NAME example_project COMMAND example_project)\n```\n\n----------------------------------------\n\nTITLE: Token Usage in CUDASTF\nDESCRIPTION: Demonstrates how to create and use tokens for synchronization in tasks.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_49\n\nLANGUAGE: cpp\nCODE:\n```\nauto token = ctx.token();\n\n// A and B are assumed to be two other valid logical data\nctx.task(token.rw(), A.read(), B.rw())->*[](cudaStream_t stream, auto a, auto b)\n{\n    ...\n};\n```\n\n----------------------------------------\n\nTITLE: Managing Temporary Storage Reuse with CUB DeviceReduce in C++\nDESCRIPTION: This C++ example demonstrates temporary storage usage with `cub::DeviceReduce::Sum`. It shows that reusing the same storage buffer (`d_storage`) for multiple calls on the same stream (`stream_1`) is safe without explicit synchronization. However, reusing the storage buffer across different streams (`stream_1` and `stream_2`) without synchronization (e.g., `cudaStreamSynchronize`) is illegal and can lead to race conditions.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/developer_overview.rst#2025-04-23_snippet_23\n\nLANGUAGE: c++\nCODE:\n```\ncub::DeviceReduce::Sum(nullptr, storage_bytes, d_in, d_out, num_items, stream_1);\n// allocate temp storage\ncub::DeviceReduce::Sum(d_storage, storage_bytes, d_in, d_out, num_items, stream_1);\n// fine not to synchronize stream\ncub::DeviceReduce::Sum(d_storage, storage_bytes, d_in, d_out, num_items, stream_1);\n// illegal, should call cudaStreamSynchronize(stream)\ncub::DeviceReduce::Sum(d_storage, storage_bytes, d_in, d_out, num_items, stream_2);\n```\n\n----------------------------------------\n\nTITLE: Using cuda::pipeline_consumer_wait_prior in a CUDA Kernel\nDESCRIPTION: Example kernel demonstrating the usage of cuda::pipeline_consumer_wait_prior for synchronizing pipeline operations in different stages.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/synchronization_primitives/pipeline/consumer_wait_prior.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n#include <cuda/pipeline>\n\n__global__ void example_kernel(uint64_t* global, cuda::std::size_t element_count) {\n  extern __shared__ uint64_t shared[];\n\n  cuda::pipeline<cuda::thread_scope_thread> pipe = cuda::make_pipeline();\n  for (cuda::std::size_t i = 0; i < element_count; ++i) {\n    pipe.producer_acquire();\n    cuda::memcpy_async(shared + i, global + i, sizeof(*global), pipe);\n    pipe.producer_commit();\n  }\n\n  // Wait for operations committed in all stages but the last one.\n  cuda::pipeline_consumer_wait_prior<1>(pipe);\n  pipe.consumer_release();\n\n  // Wait for operations committed in all stages.\n  cuda::pipeline_consumer_wait_prior<0>(pipe);\n  pipe.consumer_release();\n}\n```\n\n----------------------------------------\n\nTITLE: Cloning and Using CUDA Experimental Library\nDESCRIPTION: This snippet demonstrates how to clone the CUDA Experimental library repository and compile a simple project using NVCC. It shows the basic command-line setup needed to include the CUDA Experimental headers.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cudax/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/NVIDIA/cccl.git\n# Note:\nnvcc -Icccl/cudax/include main.cu -o main\n```\n\n----------------------------------------\n\nTITLE: Example Usage of Restrict MDSpan in CUDA\nDESCRIPTION: Comprehensive example demonstrating the usage of restrict mdspan types in a CUDA context, including computation and various mdspan instantiations.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/mdspan/restrict_accessor.rst#2025-04-23_snippet_3\n\nLANGUAGE: cuda\nCODE:\n```\n#include <cuda/mdspan>\n\nusing restrict_mdspan = cuda::restrict_mdspan<int, cuda::std::dims<1>>;\n\n__host__ __device__ void\ncompute(restrict_mdspan a, restrict_mdspan b, restrict_mdspan c) {\n    c[0] = a[0] * b[0];\n    c[1] = a[0] * b[0];\n    c[2] = a[0] * b[0] * a[1];\n    c[3] = a[0] * a[1];\n    c[4] = a[0] * b[0];\n    c[5] = b[0];\n}\n\nint main() {\n    using  dim      = cuda::std::dims<1>;\n    using  mdspan   = cuda::std::mdspan<int, dim>;\n    int    arrayA[] = {1, 2};\n    int    arrayB[] = {5};\n    int    arrayC[] = {9, 10, 11, 12, 13, 14};\n    mdspan mdA{arrayA, dim{1}};\n    mdspan mdB{arrayB, dim{5}};\n    mdspan mdC{arrayC, dim{6}};\n    compute(mdA, mdB, mdC);\n\n    using restrict_aligned_accesor = cuda::std::restrict_accessor<cuda::std::aligned_accessor<int, 8>>;\n    using restrict_aligned_mdspan  = cuda::std::mdspan<int, dim, layout_right, restrict_aligned_accesor>;\n    restrict_aligned_mdspan mdD{mdC};\n}\n```\n\n----------------------------------------\n\nTITLE: Function Signature for cuda::ceil_div\nDESCRIPTION: Declaration of the ceil_div template function that computes ceiling division between two integral or enumerator values. The function returns the common type of the input arguments and is marked as both host and device compatible.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/math/ceil_div.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename T, typename U>\n[[nodiscard]] __host__ __device__ inline constexpr\ncuda::std::common_type_t<T, U> ceil_div(T value, U divisor) noexcept;\n```\n\n----------------------------------------\n\nTITLE: CUDA Integer Logarithm Usage Example\nDESCRIPTION: Example demonstrating the usage of ilog2, ceil_ilog2, and ilog10 functions in a CUDA kernel with various test cases and assertions.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/math/ilog.rst#2025-04-23_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\n#include <cuda/cmath>\n#include <cuda/std/cassert>\n\n__global__ void ilog_kernel() {\n    assert(cuda::ilog2(20) == 4);\n    assert(cuda::ceil_ilog2(20) == 5);\n    assert(cuda::ilog2(32) == 5);\n    assert(cuda::ceil_ilog2(32) == 5);\n    assert(cuda::ilog10(100) == 2);\n    assert(cuda::ilog10(2000) == 3);\n}\n\nint main() {\n    ilog_kernel<<<1, 1>>>();\n    cudaDeviceSynchronize();\n    return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring and Initializing Thrust Build Environment - CMake\nDESCRIPTION: This CMake script sets up the Thrust C++ library project for integration and building, outlining minimum version requirements and toggles for enabling Thrust either as a subdirectory or as a standalone developer build. It sets cache variables for build configuration, manages build types, and controls which subsystems (header testing, testing suite, examples, and benchmarks) are included based on user-defined options. Dependencies are managed by including other CMake scripts for system detection and configuration, and the file ensures proper sequencing for language enabling and cache handling. The inputs are optional cache variables and feature toggles, and the outputs are a configured CMake environment ready for further build steps; prerequisites include a supported version of CMake and compatible system compilers.\nSOURCE: https://github.com/nvidia/cccl/blob/main/thrust/CMakeLists.txt#2025-04-23_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\n# 3.15 is the minimum for including the project with add_subdirectory.\\n# 3.21 is the minimum for the developer build.\\ncmake_minimum_required(VERSION 3.15)\\n\\n# This must be done before any languages are enabled:\\nif (CCCL_ENABLE_THRUST)\\n  cmake_minimum_required(VERSION 3.21)\\nendif()\\n\\nproject(Thrust LANGUAGES NONE)\\n\\n# This must appear after our Compiler Hacks or else CMake will delete the cache\\n# and reconfigure from scratch.\\n# This must also appear before the installation rules, as it is required by the\\n# GNUInstallDirs CMake module.\\nenable_language(CXX)\\n\\n# Support adding Thrust to a parent project via add_subdirectory.\\n# See examples/cmake/add_subdir/CMakeLists.txt for details.\\nif (NOT CCCL_ENABLE_THRUST)\\n  include(cmake/ThrustAddSubdir.cmake)\\n  return()\\nendif()\\n\\n# We use 3.17 features when building our tests, etc.\\ncmake_minimum_required(VERSION 3.17)\\n\\noption(THRUST_ENABLE_HEADER_TESTING \"Test that all public headers compile.\" \"ON\")\\noption(THRUST_ENABLE_TESTING \"Build Thrust testing suite.\" \"ON\")\\noption(THRUST_ENABLE_EXAMPLES \"Build Thrust examples.\" \"ON\")\\n\\n# Allow the user to optionally select offset type dispatch to fixed 32 or 64 bit types\\nset(THRUST_DISPATCH_TYPE \"Dynamic\" CACHE STRING \"Select Thrust offset type dispatch.\")\\nset_property(CACHE THRUST_DISPATCH_TYPE PROPERTY STRINGS \"Dynamic\" \"Force32bit\" \"Force64bit\")\\n\\n# Check if we're actually building anything before continuing. If not, no need\\n# to search for deps, etc. This is a common approach for packagers that just\\n# need the install rules. See GH issue NVIDIA/thrust#1211.\\nif (NOT (THRUST_ENABLE_HEADER_TESTING OR\\n         THRUST_ENABLE_TESTING OR\\n         THRUST_ENABLE_EXAMPLES OR\\n         CCCL_ENABLE_BENCHMARKS))\\n  return()\\nendif()\\n\\n#include first:\\ninclude(cmake/ThrustUtilities.cmake)\\n\\ninclude(cmake/ThrustBuildCompilerTargets.cmake)\\ninclude(cmake/ThrustBuildTargetList.cmake)\\ninclude(cmake/ThrustFindThrust.cmake)\\ninclude(cmake/ThrustMultiConfig.cmake)\\n\\n# Add cache string options for CMAKE_BUILD_TYPE and default to RelWithDebInfo.\\nif (\"\" STREQUAL \"${CMAKE_BUILD_TYPE}\")\\n  set(CMAKE_BUILD_TYPE \"RelWithDebInfo\" CACHE STRING \"Choose the type of build.\" FORCE)\\n\\n  set_property(\\n    CACHE CMAKE_BUILD_TYPE\\n    PROPERTY STRINGS Debug Release RelWithDebInfo MinSizeRel\\n  )\\nendif ()\\n\\n# Disable compiler extensions:\\nset(CMAKE_CXX_EXTENSIONS OFF)\\n\\nthrust_configure_multiconfig()\\nthrust_find_thrust()\\nthrust_build_compiler_targets()\\nthrust_update_system_found_flags()\\nif (THRUST_CUDA_FOUND)\\n  include(cmake/ThrustCudaConfig.cmake)\\nendif()\\nthrust_build_target_list()\\n\\nmessage(STATUS \"CPP system found?  ${THRUST_CPP_FOUND}\")\\nmessage(STATUS \"CUDA system found? ${THRUST_CUDA_FOUND}\")\\nmessage(STATUS \"TBB system found?  ${THRUST_TBB_FOUND}\")\\nmessage(STATUS \"OMP system found?  ${THRUST_OMP_FOUND}\")\\n\\nif (THRUST_ENABLE_HEADER_TESTING)\\n  include(cmake/ThrustHeaderTesting.cmake)\\nendif()\\n\\n# Both testing and examples use ctest\\nif (THRUST_ENABLE_TESTING OR THRUST_ENABLE_EXAMPLES)\\n  include(CTest)\\n  enable_testing()\\nendif()\\n\\nif (THRUST_ENABLE_TESTING)\\n  add_subdirectory(testing)\\nendif()\\n\\nif (THRUST_ENABLE_EXAMPLES)\\n  add_subdirectory(examples)\\nendif()\\n\\nif (CCCL_ENABLE_BENCHMARKS)\\n  add_subdirectory(benchmarks)\\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting up CUDA C++ Public Header Test Target in CMake\nDESCRIPTION: Defines a CMake custom target for testing that all public CUDA C++ headers can be compiled on their own with a host compiler. The script handles NVHPC and other compilers differently, and filters out mdspan headers for MSVC outside of C++20.\nSOURCE: https://github.com/nvidia/cccl/blob/main/libcudacxx/test/public_headers_host_only/CMakeLists.txt#2025-04-23_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\n# For every public header build a TU that directly includes it\n# without anything else but also pretents to be a std header\nadd_custom_target(libcudacxx.test.public_headers_host_only)\n\nif (\"NVHPC\" STREQUAL \"${CMAKE_CXX_COMPILER_ID}\")\n  find_package(NVHPC)\nelse()\n  find_package(CUDAToolkit)\nendif()\n\n# Grep all public headers\nfile(GLOB public_headers_host_only\n  LIST_DIRECTORIES false\n  RELATIVE \"${libcudacxx_SOURCE_DIR}/include/\"\n  CONFIGURE_DEPENDS\n  \"${libcudacxx_SOURCE_DIR}/include/cuda/*\"\n)\n\n# mdspan is currently not supported on msvc outside of C++20\nif (\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"MSVC\" AND NOT \"${CMAKE_CXX_STANDARD}\" MATCHES \"20\")\n  list(FILTER public_headers_host_only EXCLUDE REGEX \"mdspan\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Reshaping Execution Place Grid - CUDASTF C++\nDESCRIPTION: Shows how to reshape a grid of execution places using the reshape member function. Assumes the number of places is compatible with the total number of elements in the new shape. This helps restructure device allocation for different data partitioning schemes. Depends on exec_place::all_devices(), dim4, and that places.size() matches the shape volume.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_26\n\nLANGUAGE: c++\nCODE:\n```\n       // This assumes places.size() == 8\n       auto places = exec_place::all_devices();\n       auto places_reshaped = places.reshape(dim4(2, 2, 2));\n```\n\n----------------------------------------\n\nTITLE: Example Usage of cuda::round_up in CUDA Kernel\nDESCRIPTION: Demonstrates practical usage of the round_up function within a CUDA kernel. Shows how to round up an integer value of 7 to the nearest multiple of 3, resulting in 9.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/math/round_up.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n#include <cuda/cmath>\n#include <cstdio>\n\n__global__ void round_up_kernel() {\n    int      value    = 7;\n    unsigned multiple = 3;\n    printf(\"%d\\n\", cuda::round_up(value, multiple)); // print \"9\"\n}\n\nint main() {\n    round_up_kernel<<<1, 1>>>();\n    cudaDeviceSynchronize();\n    return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Loading 16x32bx2 Data with tcgen05 in CUDA\nDESCRIPTION: This template function loads 16x32bx2 data using the tcgen05 instruction. It supports various output sizes (1, 2, 4, 8, 16, 32, 64, 128) and has variants for packed 16-bit data. It includes an additional parameter for half split offset.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_ld.rst#2025-04-23_snippet_3\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true, int N32>\n__device__ static inline void tcgen05_ld_16x32bx2(\n  B32 (&out)[128],\n  uint32_t taddr,\n  cuda::ptx::n32_t<N32> immHalfSplitoff);\n```\n\n----------------------------------------\n\nTITLE: Implementing a CUB Policy Hub for Architecture-Specific Tuning in C++\nDESCRIPTION: This C++ template struct `policy_hub` organizes multiple `AgentAlgorithmPolicy` definitions, selecting the appropriate one based on the target SM architecture (e.g., SM50, SM60). It uses the `ChainedPolicy` helper to link policies for different PTX versions, allowing CUB algorithms to adapt their configuration for optimal performance on various GPU generations. The `MaxPolicy` alias serves as the entry point for policy selection.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/developer_overview.rst#2025-04-23_snippet_20\n\nLANGUAGE: c++\nCODE:\n```\ntemplate <typename... TuningRelevantParams /* ... */>\nstruct policy_hub {\n  // TuningRelevantParams... could be used for decision making, like element types used, iterator category, etc.\n\n  // for SM50\n  struct Policy500 : ChainedPolicy<500, Policy500, Policy500> {\n    using AlgorithmPolicy = AgentAlgorithmPolicy<256, 20, BLOCK_LOAD_DIRECT, LOAD_LDG>;\n    // ... additional policies may exist, often one per agent\n  };\n\n  // for SM60\n  struct Policy600 : ChainedPolicy<600, Policy600, Policy500> {\n    using AlgorithmPolicy = AgentAlgorithmPolicy<256, 16, BLOCK_LOAD_DIRECT, LOAD_LDG>;\n  };\n\n  using MaxPolicy = Policy600; // alias where policy selection is started by ChainedPolicy\n};\n```\n\n----------------------------------------\n\nTITLE: Comparing Standard C++, CUDA C++ Standard, and CUDA C++ Extended usage\nDESCRIPTION: Illustrates the differences between using standard C++, CUDA C++ standard conforming, and CUDA C++ extended features. It shows how to include different headers and create atomic variables for each case.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/index.rst#2025-04-23_snippet_2\n\nLANGUAGE: cuda\nCODE:\n```\n// Standard C++, __host__ only.\n#include <atomic>\nstd::atomic<int> x;\n\n// CUDA C++, __host__ __device__.\n// Strictly conforming to the C++ Standard.\n#include <cuda/std/atomic>\ncuda::std::atomic<int> x;\n\n// CUDA C++, __host__ __device__.\n// Conforming extensions to the C++ Standard.\n#include <cuda/atomic>\ncuda::atomic<int, cuda::thread_scope_block> x;\n```\n\n----------------------------------------\n\nTITLE: Implementing warp_shuffle_xor in CUDA\nDESCRIPTION: Function templates for warp_shuffle_xor operation with optional Width and lane_mask parameters. Shuffles data within a warp using XOR operation on lane indices.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/warp/warp_shuffle.rst#2025-04-23_snippet_3\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <int Width = 32, typename T>\n[[nodiscard]] __device__ warp_shuffle_result<T>\nwarp_shuffle_xor(const T& data,\n                 int      xor_mask,\n                 uint32_t lane_mask = 0xFFFFFFFF,\n                 cuda::std::integral_constant<int, Width> = {})\n\ntemplate <int Width = 32, typename T>\n[[nodiscard]] __device__ warp_shuffle_result<T>\nwarp_shuffle_xor(const T& data,\n                 int      xor_mask,\n                 cuda::std::integral_constant<int, Width>) // lane_mask is 0xFFFFFFFF\n```\n\n----------------------------------------\n\nTITLE: Using FutureValue for Lazy Loading in DeviceScan (C++)\nDESCRIPTION: Demonstrates how to use cub::FutureValue<T> to lazily load an initial_value from a pointer in cub::DeviceScan algorithms, avoiding unnecessary synchronization when using the result of a previous kernel as input.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/releases/changelog.rst#2025-04-23_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nint *d_intermediate_result = ...;\nintermediate_kernel<<<blocks, threads>>>(d_intermediate_result,  // output\n                                           arg1,                   // input\n                                           arg2);                  // input\n\n// Wrap the intermediate pointer in a FutureValue -- no need to explicitly\n// sync when both kernels are stream-ordered. The pointer is read after\n// the ExclusiveScan kernel starts executing.\ncub::FutureValue<int> init_value(d_intermediate_result);\n\ncub::DeviceScan::ExclusiveScan(d_temp_storage,\n                                temp_storage_bytes,\n                                d_in,\n                                d_out,\n                                cub::Sum(),\n                                init_value,\n                                num_items);\n```\n\n----------------------------------------\n\nTITLE: Defining Execution Places and Logical Data Tasks with CUDASTF in C++\nDESCRIPTION: Illustrates setting up tasks in CUDASTF with explicit execution places on different devices and the host. Each task operates on a logical data object 'lX', updating and checking its value through CUDA kernels and host assertions. Requires an initialized CUDASTF context and proper configuration of device kernels like 'inc_kernel'. Host and device task submission semantics are shown, including stream synchronization. 'ctx.finalize()' ensures completion.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_19\n\nLANGUAGE: cpp\nCODE:\n```\ncontext ctx;\n\nint X = 42;\n\nauto lX = ctx.logical_data(slice<int>(&X, { 1 }));\n\nctx.task(exec_place::device(0), lX.rw())->*[](cudaStream_t stream, auto sX) {\n    inc_kernel<<<1, 1, 0, stream>>>(sX);\n};\n\nctx.task(exec_place::device(1), lX.rw())->*[](cudaStream_t stream, auto sX) {\n    inc_kernel<<<1, 1, 0, stream>>>(sX);\n};\n\nctx.task(exec_place::host(), lX.read())->*[](cudaStream_t stream, auto sX) {\n    cudaStreamSynchronize(stream);\n    assert(sX(0) == 44);\n};\n\nctx.finalize();\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Bulk Copy from Shared CTA to Shared Cluster Memory with Barrier (CUDA)\nDESCRIPTION: Performs an asynchronous bulk copy from shared CTA memory to shared cluster memory, completing a memory barrier transaction. This operation is available from PTX ISA 80 and SM_90.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_async_bulk.rst#2025-04-23_snippet_2\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline void cp_async_bulk(\n  cuda::ptx::space_cluster_t,\n  cuda::ptx::space_shared_t,\n  void* dstMem,\n  const void* srcMem,\n  const uint32_t& size,\n  uint64_t* rdsmem_bar);\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic CUDA Memory Resource Concept\nDESCRIPTION: Demonstrates valid and invalid implementations of the cuda::mr::resource concept, showing required methods and equality comparison operators.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/memory_resource/resource.rst#2025-04-23_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nstruct valid_resource {\n  void* allocate(std::size_t, std::size_t) { return nullptr; }\n  void deallocate(void*, std::size_t, std::size_t) noexcept {}\n  bool operator==(const valid_resource&) const { return true; }\n  bool operator!=(const valid_resource&) const { return false; }\n};\nstatic_assert(cuda::mr::resource<valid_resource>, \"\");\n\nstruct invalid_argument {};\nstruct invalid_allocate_argument {\n  void* allocate(invalid_argument, std::size_t) { return nullptr; }\n  void deallocate(void*, std::size_t, std::size_t) noexcept {}\n  bool operator==(const invalid_allocate_argument&) { return true; }\n};\nstatic_assert(!cuda::mr::resource<invalid_allocate_argument>, \"\");\n\nstruct invalid_allocate_return {\n  int allocate(std::size_t, std::size_t) { return 42; }\n  void deallocate(void*, std::size_t, std::size_t) noexcept {}\n  bool operator==(const invalid_allocate_return&) { return true; }\n};\nstatic_assert(!cuda::mr::resource<invalid_allocate_return>, \"\");\n\nstruct invalid_deallocate_argument {\n  void* allocate(std::size_t, std::size_t) { return nullptr; }\n  void deallocate(void*, invalid_argument, std::size_t) noexcept {}\n  bool operator==(const invalid_deallocate_argument&) { return true; }\n};\nstatic_assert(!cuda::mr::resource<invalid_deallocate_argument>, \"\");\n\nstruct non_comparable {\n  void* allocate(std::size_t, std::size_t) { return nullptr; }\n  void deallocate(void*, std::size_t, std::size_t) noexcept {}\n};\nstatic_assert(!cuda::mr::resource<non_comparable>, \"\");\n\nstruct non_eq_comparable {\n  void* allocate(std::size_t, std::size_t) { return nullptr; }\n  void deallocate(void*, std::size_t, std::size_t) noexcept {}\n  bool operator!=(const non_eq_comparable&) { return false; }\n};\nstatic_assert(!cuda::mr::resource<non_eq_comparable>, \"\");\n```\n\n----------------------------------------\n\nTITLE: Detailed CUDA MMA Block Scale Vector Operation Implementation\nDESCRIPTION: This code snippet provides a more detailed implementation of the MMA block scale vector operation. It includes template parameters for CTA groups and uses specialized CUDA types for tensor core operations. The function is designed for PTX ISA 86 and SM_100a/SM_101a architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma.rst#2025-04-23_snippet_52\n\nLANGUAGE: CUDA\nCODE:\n```\n// tcgen05.mma.cta_group.kind.block_scale.scale_vec::4X.collector::a::discard [d_tmem], a_desc, b_desc, idesc, [scale_A_tmem], [scale_B_tmem], enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .kind      = { .kind::mxf4nvf4 }\n// .cta_group = { .cta_group::1, .cta_group::2 }\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_mma_block_scale_vec_4x_tmem_a_collector_a_discard(\n  cuda::ptx::kind_mxf4nvf4_t,\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  uint32_t scale_A_tmem,\n  uint32_t scale_B_tmem,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Creating and Using cuda::stream_ref in C++\nDESCRIPTION: This snippet demonstrates how to create a CUDA stream, wrap it with cuda::stream_ref, and use its member functions for synchronization and status checking. It shows the creation, waiting, and destruction of a CUDA stream using the wrapper class.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/streams/stream_ref.rst#2025-04-23_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\ncudaStream_t stream;\ncudaStreamCreate(&stream);\ncuda::stream_ref ref{stream};\n\nref.wait();          // synchronizes the stream via cudaStreamSynchronize\nassert(ref.ready()); // verifies that the stream has finished all operations via cudaStreamQuery\ncudaStreamDestroy(stream);\n```\n\n----------------------------------------\n\nTITLE: Matrix Initialization Examples\nDESCRIPTION: Demonstrates different ways to initialize logical data from matrices, including initialization from existing matrix data and matrix shape specifications.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf/custom_data_interface.rst#2025-04-23_snippet_5\n\nLANGUAGE: C++\nCODE:\n```\nstd::vector<int> v(m * n, 0);\nmatrix M(m, n, &v[0]);\n\n// Initialize from a matrix\nauto lM = ctx.logical_data(M);\n\n// Initialize from a shape\nauto lM2 = ctx.logical_data(shape_of<matrix<int>>(m, n));\n```\n\n----------------------------------------\n\nTITLE: Constructing cuda::annotated_ptr from Pointer in CUDA\nDESCRIPTION: Constructor for cuda::annotated_ptr that takes a pointer and associates it with the specified Property. It includes preconditions for different property types and their compatibility with memory address spaces.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/memory_access_properties/annotated_ptr.rst#2025-04-23_snippet_2\n\nLANGUAGE: cuda\nCODE:\n```\nconstexpr explicit annotated_ptr(pointer ptr);\n```\n\n----------------------------------------\n\nTITLE: Defining Shape-Only Logical Data\nDESCRIPTION: Example of creating logical data from a shape without a reference instance, useful for cases where data will be filled by later tasks.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_13\n\nLANGUAGE: cpp\nCODE:\n```\nauto lX = ctx.logical_data(shape_of<slice<int>>(10));\n\nctx.task(lX.write())->*[](cudaStream_t stream, auto X) {\n    cudaMemsetAsync(X.data_handle(), 0, X.size()*sizeof(int), stream);\n};\n```\n\n----------------------------------------\n\nTITLE: Defining cuda::memcpy_async with Barrier in CUDA\nDESCRIPTION: Defines two overloads of cuda::memcpy_async that use a cuda::barrier for synchronization. The first overload is for single-thread operations, while the second is for group operations.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/asynchronous_operations/memcpy_async.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\n// (1)\ntemplate <typename Shape, cuda::thread_scope Scope, typename CompletionFunction>\n__host__ __device__\nvoid cuda::memcpy_async(void* destination, void const* source, Shape size,\n                       cuda::barrier<Scope, CompletionFunction>& barrier);\n\n// (2)\ntemplate <typename Group,\n         typename Shape, cuda::thread_scope Scope, typename CompletionFunction>\n__host__ __device__\nvoid cuda::memcpy_async(Group const& group,\n                       void* destination, void const* source, Shape size,\n                       cuda::barrier<Scope, CompletionFunction>& barrier);\n```\n\n----------------------------------------\n\nTITLE: Defining cuda::memcpy_async with Annotated Pointers in CUDA\nDESCRIPTION: Defines four overloads of cuda::memcpy_async that use cuda::annotated_ptr for source and/or destination. These overloads provide convenience wrappers for operations with annotated pointers.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/asynchronous_operations/memcpy_async.rst#2025-04-23_snippet_2\n\nLANGUAGE: cuda\nCODE:\n```\n// (5)\ntemplate <typename Dst, typename Src, typename SrcProperty, typename Shape, typename Sync>\n__host__ __device__\nvoid memcpy_async(Dst* dst, cuda::annotated_ptr<Src, SrcProperty> src, Shape size, Sync& sync);\n\n// (6)\ntemplate<typename Dst, typename DstProperty, typename Src, typename SrcProperty, typename Shape, typename Sync>\n__host__ __device__\nvoid memcpy_async(cuda::annotated_ptr<Dst, DstProperty> dst, cuda::annotated_ptr<Src, SrcProperty> src, Shape size, Sync& sync);\n\n// (7)\ntemplate<typename Group, typename Dst, typename Src, typename SrcProperty, typename Shape, typename Sync>\n__host__ __device__\nvoid memcpy_async(Group const& group, Dst* dst, cuda::annotated_ptr<Src, SrcProperty> src, Shape size, Sync& sync);\n\n// (8)\ntemplate<typename Group, typename Dst, typename DstProperty, typename Src, typename SrcProperty, typename Shape, typename Sync>\n__host__ __device__\nvoid memcpy_async(Group const& group, cuda::annotated_ptr<Dst, DstProperty> dst, cuda::annotated_ptr<Src, SrcProperty> src, Shape size, Sync& sync);\n```\n\n----------------------------------------\n\nTITLE: Early Return for Package Builds\nDESCRIPTION: Checks if any components are being built and returns early if not. This is useful for packagers who only need the installation rules without building anything.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cub/CMakeLists.txt#2025-04-23_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\n# Check if we're actually building anything before continuing. If not, no need\n# to search for deps, etc. This is a common approach for packagers that just\n# need the install rules. See GH issue NVIDIA/thrust#1211.\nif (NOT (CUB_ENABLE_HEADER_TESTING OR\n         CUB_ENABLE_TESTING OR\n         CUB_ENABLE_EXAMPLES OR\n         CCCL_ENABLE_BENCHMARKS))\n  return()\nendif()\n```\n\n----------------------------------------\n\nTITLE: CUDA Independent Streams Example\nDESCRIPTION: Demonstrates potential thread starvation when using independent streams without dependencies. Shows how lack of ordering between streams can lead to indefinite blocking.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/execution_model.rst#2025-04-23_snippet_9\n\nLANGUAGE: cuda\nCODE:\n```\ncuda::atomic<int, cuda::thread_scope_system> flag = 0;\n__global__ void first() { flag.store(1, cuda::memory_order_relaxed); }\n__global__ void second() { while(flag.load(cuda::memory_order_relaxed) == 0) {} }\nint main() {\n    cudaHostRegister(&flag, sizeof(flag));\n    cudaStream_t s0, s1;\n    cudaStreamCreate(&s0);\n    cudaStreamCreate(&s1);\n    first<<<1,1,0,s0>>>();\n    second<<<1,1,0,s1>>>();\n    return cudaDeviceSynchronize();\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Matrix Stream Interface\nDESCRIPTION: Complete implementation of matrix_stream_interface class handling data allocation, deallocation, copying, and memory pinning operations.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf/custom_data_interface.rst#2025-04-23_snippet_3\n\nLANGUAGE: c++\nCODE:\n```\ntemplate <typename T>\nclass matrix_stream_interface : public stream_data_interface_simple<matrix<T>> {\npublic:\n    using base = stream_data_interface_simple<matrix<T>>;\n    using base::shape_t;\n\n    matrix_stream_interface(matrix<T> m) : base(std::move(m)) {}\n    matrix_stream_interface(shape_t s) : base(s) {}\n\n    void stream_data_copy(const data_place& dst_memory_node, instance_id_t dst_instance_id,\n            const data_place& src_memory_node, instance_id_t src_instance_id, cudaStream_t stream) override {\n        assert(src_memory_node != dst_memory_node);\n\n        cudaMemcpyKind kind = cudaMemcpyDeviceToDevice;\n        if (src_memory_node == data_place::host) {\n            kind = cudaMemcpyHostToDevice;\n        }\n\n        if (dst_memory_node == data_place::host) {\n            kind = cudaMemcpyDeviceToHost;\n        }\n\n        const matrix<T>& src_instance = this->instance(src_instance_id);\n        const matrix<T>& dst_instance = this->instance(dst_instance_id);\n\n        size_t sz = src_instance.m * src_instance.n * sizeof(T);\n\n        cuda_safe_call(cudaMemcpyAsync((void*) dst_instance.base, (void*) src_instance.base, sz, kind, stream));\n    }\n\n    void stream_data_allocate(backend_ctx_untyped& ctx, const data_place& memory_node, instance_id_t instance_id, ssize_t& s,\n            void** extra_args, cudaStream_t stream) override {\n        matrix<T>& instance = this->instance(instance_id);\n        size_t sz = instance.m * instance.n * sizeof(T);\n\n        T* base_ptr;\n\n        if (memory_node == data_place::host) {\n            cuda_safe_call(cudaStreamSynchronize(stream));\n            cuda_safe_call(cudaHostAlloc(&base_ptr, sz, cudaHostAllocMapped));\n        } else {\n            cuda_safe_call(cudaMallocAsync(&base_ptr, sz, stream));\n        }\n\n        *s = sz;\n\n        instance.base = base_ptr;\n    }\n\n    void stream_data_deallocate(backend_ctx_untyped& ctx, const data_place& memory_node, instance_id_t instance_id, void* extra_args,\n            cudaStream_t stream) override {\n        matrix<T>& instance = this->instance(instance_id);\n        if (memory_node == data_place::host) {\n            cuda_safe_call(cudaStreamSynchronize(stream));\n            cuda_safe_call(cudaFreeHost(instance.base));\n        } else {\n            cuda_safe_call(cudaFreeAsync(instance.base, stream));\n        }\n    }\n\n    bool pin_host_memory(instance_id_t instance_id) override {\n        matrix<T>& instance = this->instance(instance_id);\n        if (!instance.base) {\n            return false;\n        }\n\n        cuda_safe_call(pin_memory(instance.base, instance.m * instance.n * sizeof(T)));\n\n        return true;\n    }\n\n    void unpin_host_memory(instance_id_t instance_id) override {\n        matrix<T>& instance = this->instance(instance_id);\n        unpin_memory(instance.base);\n    }\n};\n```\n\n----------------------------------------\n\nTITLE: Storing 32x32b Data with tcgen05 in CUDA (1 value)\nDESCRIPTION: This function performs a 32x32b store operation using tcgen05. It takes a 32-bit address and an array of 1 B32 value as input. The function is constrained to B32 types of 4 bytes.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_st.rst#2025-04-23_snippet_17\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.st.sync.aligned.32x32b.x1.b32 [taddr], values; // PTX ISA 86, SM_100a, SM_101a\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline void tcgen05_st_32x32b(\n  uint32_t taddr,\n  const B32 (&values)[1]);\n```\n\n----------------------------------------\n\nTITLE: Explicitly Overriding Data Affinity for Device and Host with CUDASTF in C++\nDESCRIPTION: Shows how to explicitly specify data affinity for CUDASTF tasks, placing logical data either on a specific device or the host, independent of the execution place. Useful for advanced scenarios such as unified memory or sparse access patterns, where controlling memory placement can improve performance.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_22\n\nLANGUAGE: cpp\nCODE:\n```\nctx.task(exec_place::device(0), lA.rw(data_place::device(0)))->*[](cudaStream_t s, auto a) {\n    ...\n};\n\nctx.task(exec_place::device(0), lA.rw(data_place::host()))->*[](cudaStream_t s, auto a) {\n    ...\n};\n```\n\nLANGUAGE: cpp\nCODE:\n```\nctx.task(exec_place::device(0), lA.rw(data_place::host()))->*[](cudaStream_t s, auto a) {\n    ...\n};\n```\n\n----------------------------------------\n\nTITLE: Using cuda::bit_reverse in a CUDA Kernel\nDESCRIPTION: Example usage of cuda::bit_reverse function in a CUDA kernel. It demonstrates bit reversal for unsigned integers and uint8_t types.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/bit/bit_reverse.rst#2025-04-23_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n#include <cuda/bit>\n#include <cuda/std/cassert>\n\n__global__ void bit_reverse_kernel() {\n    assert(bitfield_reverse(0u) == ~0u);\n    assert(bitfield_reverse(uint8_t{0b00001011}) == uint8_t{0b11010000});\n}\n\nint main() {\n    bit_reverse_kernel<<<1, 1>>>();\n    cudaDeviceSynchronize();\n    return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Using thrust::cuda::par_nosync for Asynchronous Execution in C++\nDESCRIPTION: Demonstrates how to use the par_nosync execution policy to queue multiple for_each kernels without waiting for earlier calls to complete. This allows other work to be done while kernels execute, requiring explicit synchronization before accessing results.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/releases/changelog.rst#2025-04-23_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n// Queue three `for_each` kernels:\nthrust::for_each(thrust::cuda::par_nosync, vec1.begin(), vec1.end(), Op{});\nthrust::for_each(thrust::cuda::par_nosync, vec2.begin(), vec2.end(), Op{});\nthrust::for_each(thrust::cuda::par_nosync, vec3.begin(), vec3.end(), Op{});\n\n// Do other work while kernels execute:\ndo_something();\n\n// Must explicitly synchronize before accessing `for_each` results:\ncudaDeviceSynchronize();\n```\n\n----------------------------------------\n\nTITLE: Implementing cuda::annotated_ptr Class Template in CUDA\nDESCRIPTION: Detailed implementation of the cuda::annotated_ptr class template, including member types, constructors, and operators. It provides functionality for creating and manipulating annotated pointers with associated access properties.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/memory_access_properties/annotated_ptr.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\nnamespace cuda {\n\ntemplate<class Type, class Property>\nclass annotated_ptr {\npublic:\n  using value_type = Type;\n  using size_type = std::size_t;\n  using reference = value_type &;\n  using pointer = value_type *;\n  using const_pointer = value_type const *;\n  using difference_type = std::ptrdiff_t;\n\n  __host__ __device__ constexpr annotated_ptr() noexcept;\n  __host__ __device__ constexpr annotated_ptr(annotated_ptr const&) noexcept = default;\n  __host__ __device__ constexpr annotated_ptr& operator=(annotated_ptr const&) noexcept = default;\n  __host__ __device__ explicit annotated_ptr(pointer);\n  template <class RuntimeProperty>\n  __host__ __device__ annotated_ptr(pointer, RuntimeProperty);\n  template <class T, class P>\n  __host__ __device__ annotated_ptr(annotated_ptr<T,P> const&);\n\n  __host__ __device__ constexpr explicit operator bool() const noexcept;\n  __host__ __device__ pointer get() const noexcept;\n\n  __host__ __device__ reference operator*() const;\n  __host__ __device__ pointer operator->() const;\n  __host__ __device__ reference operator[](std::ptrdiff_t) const;\n  __host__ __device__ constexpr difference_type operator-(annotated_ptr);\n\nprivate:\n  pointer ptr;   // exposition only\n  Property prop; // exposition only\n};\n\n} // namespace cuda\n```\n\n----------------------------------------\n\nTITLE: Message Passing Thread Block 0 Implementation\nDESCRIPTION: Shows the sending side of a message passing implementation using atomic operations with device scope synchronization.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/memory_model.rst#2025-04-23_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nx = 42;\ncuda::atomic_ref<int, cuda::thread_scope_device> flag(f);\nflag.store(1, memory_order_release);\n```\n\n----------------------------------------\n\nTITLE: Using cuda::uabs Function in CUDA Kernel\nDESCRIPTION: Example usage of cuda::uabs function in a CUDA kernel. It demonstrates various use cases including positive and negative integers, as well as minimum and maximum values of int type.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/math/uabs.rst#2025-04-23_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n#include <cuda/cmath>\n#include <cuda/std/cassert>\n#include <cuda/std/limits>\n\n__global__ void uabs_kernel() {\n    using cuda::std::numeric_limits;\n\n    assert(cuda::uabs(20) == 20u);\n    assert(cuda::uabs(-32) == 32u);\n    assert(cuda::uabs(numeric_limits<int>::max()) == static_cast<unsigned>(numeric_limits<int>::max()));\n    assert(cuda::uabs(numeric_limits<int>::min()) == static_cast<unsigned>(numeric_limits<int>::max()) + 1);\n}\n\nint main() {\n    uabs_kernel<<<1, 1>>>();\n    cudaDeviceSynchronize();\n    return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Using cuda::discard_memory in a CUDA Kernel\nDESCRIPTION: Example of using cuda::discard_memory in a CUDA kernel to manage a scratch pad in global memory. The function is used to hint that the scratch pad memory doesn't need to be flushed from the cache after computation.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/memory_access_properties/discard_memory.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n#include <cuda/discard_memory>\n__device__ int compute(int* scratch, size_t N);\n\n__global__ void kernel(int const* in, int* out, int* scratch, size_t N) {\n    // Each thread reads N elements into the scratch pad:\n    for (int i = 0; i < N; ++i) {\n        int idx = threadIdx.x + i * blockDim.x;\n        scratch[idx] = in[idx];\n    }\n    __syncthreads();\n\n    // All threads compute on the scratch pad:\n    int result = compute(scratch, N);\n\n    // All threads discard the scratch pad memory to _hint_ that it does not need to be flushed from the cache:\n    cuda::discard_memory(scratch + threadIdx.x * N, N * sizeof(int));\n    __syncthreads();\n\n    out[threadIdx.x] = result;\n}\n```\n\n----------------------------------------\n\nTITLE: Using Compiler Version Checks in CCCL\nDESCRIPTION: Demonstrates how to use the _CCCL_COMPILER macro to check for specific compiler versions. This allows conditional compilation based on the host compiler type and version.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cccl/development/macro.rst#2025-04-23_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n_CCCL_COMPILER(MSVC, <, 19, 24)\n_CCCL_COMPILER(GCC, >=, 9)\n```\n\n----------------------------------------\n\nTITLE: Using launch Construct for 1D Array Processing in CUDASTF\nDESCRIPTION: Demonstrates the usage of ctx.launch construct to process a 1D array in CUDASTF. It specifies thread hierarchy, execution place, and data dependencies.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_37\n\nLANGUAGE: cpp\nCODE:\n```\nctx.launch(par(1024), all_devs, handle_X.read(cdp), handle_Y.rw(cdp))->*[=] __device__(thread_info t, slice<double> x, slice<double> y) {\n    size_t tid = t.thread_id();\n    size_t nthreads = t.get_num_threads();\n    for (size_t ind = tid; ind < N; ind += nthreads) {\n      y(ind) += alpha * x(ind);\n    }\n};\n```\n\n----------------------------------------\n\nTITLE: CUDA Pipeline Class Template Definition\nDESCRIPTION: Definition of cuda::pipeline template class with thread scope parameter. Includes deleted constructors and assignment operators, along with member functions for producer-consumer pipeline operations.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/synchronization_primitives/pipeline.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::thread_scope Scope>\nclass cuda::pipeline {\npublic:\n  pipeline() = delete;\n\n  __host__ __device__ ~pipeline();\n\n  pipeline& operator=(pipeline const&) = delete;\n\n  __host__ __device__ void producer_acquire();\n\n  __host__ __device__ void producer_commit();\n\n  __host__ __device__ void consumer_wait();\n\n  template <typename Rep, typename Period>\n  __host__ __device__ bool consumer_wait_for(cuda::std::chrono::duration<Rep, Period> const& duration);\n\n  template <typename Clock, typename Duration>\n  __host__ __device__\n  bool consumer_wait_until(cuda::std::chrono::time_point<Clock, Duration> const& time_point);\n\n  __host__ __device__ void consumer_release();\n\n  __host__ __device__ bool quit();\n};\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Bulk ADD Reduction (Unsigned 64-bit) in CUDA\nDESCRIPTION: Performs an asynchronous bulk ADD reduction operation from CTA-shared to cluster-shared memory for unsigned 64-bit integers. Uses a memory barrier for synchronization.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_reduce_async_bulk.rst#2025-04-23_snippet_11\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline void cp_reduce_async_bulk(\n  cuda::ptx::space_cluster_t,\n  cuda::ptx::space_shared_t,\n  cuda::ptx::op_add_t,\n  uint64_t* dstMem,\n  const uint64_t* srcMem,\n  uint32_t size,\n  uint64_t* rdsmem_bar);\n```\n\n----------------------------------------\n\nTITLE: Managed MDSpan Usage Example\nDESCRIPTION: Example showing managed mdspan usage with unified memory allocation and access from both host and device.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/mdspan/host_device_accessor.rst#2025-04-23_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\n#include <cuda/mdspan>\n\nusing dim = cuda::std::dims<1>;\n\n__global__ void kernel_d(cuda::device_mdspan<int, dim> md) {\n    md[0] = 0;\n}\n\n__host__ void host_function_h(cuda::host_mdspan<int, dim> md) {\n    md[0] = 0;\n}\n\nint main() {\n    int* m_ptr;\n    cudaMallocManaged(&m_ptr, 4 * sizeof(int));\n    cuda::managed_mdspan m_md{m_ptr, 4};\n    kernel_d<<<1, 1>>>(m_md); // ok\n    host_function_h(m_md);    // ok\n\n    cuda::managed_mdspan m_md2{d_ptr, 4};\n    m_md2[0]; // run-time error\n    cudaFree(d_ptr);\n}\n```\n\n----------------------------------------\n\nTITLE: Declaring Load-Reduce Minimum Operation (acquire/sys/global/u32) in CUDA C++\nDESCRIPTION: Provides a device inline template function for atomic min reduction on global 32-bit unsigned values, parameterized by PTX acquire or relaxed semaphore and scope types, including system-wide visibility. It is part of a uniform CUDA C++ interface suite for reduction idioms in parallel algorithms on SM_90 systems.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_8\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.u32 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .min }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline uint32_t multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_min_t,\n  const uint32_t* addr);\n```\n\n----------------------------------------\n\nTITLE: Using cuda::barrier in CUDA Kernel\nDESCRIPTION: Example of creating different types of cuda::barrier objects within a CUDA kernel function. It demonstrates barriers for system, device, and thread block scopes.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/synchronization_primitives/barrier.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n__global__ void example_kernel() {\n  // This barrier is suitable for all threads in the system.\n  cuda::barrier<cuda::thread_scope_system> a(10);\n\n  // This barrier has the same type as the previous one (`a`).\n  cuda::std::barrier<> b(10);\n\n  // This barrier is suitable for all threads on the current processor (e.g. GPU).\n  cuda::barrier<cuda::thread_scope_device> c(10);\n\n  // This barrier is suitable for all threads in the same thread block.\n  cuda::barrier<cuda::thread_scope_block> d(10);\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing Warp-related Information in CUDA\nDESCRIPTION: These functions retrieve warp-related information such as lane ID, warp ID, and number of warps. They use inline PTX instructions to access the special registers.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/get_sreg.rst#2025-04-23_snippet_2\n\nLANGUAGE: cuda\nCODE:\n```\n// mov.u32 sreg_value, %%laneid; // PTX ISA 13\ntemplate <typename = void>\n__device__ static inline uint32_t get_sreg_laneid();\n```\n\nLANGUAGE: cuda\nCODE:\n```\n// mov.u32 sreg_value, %%warpid; // PTX ISA 13\ntemplate <typename = void>\n__device__ static inline uint32_t get_sreg_warpid();\n```\n\nLANGUAGE: cuda\nCODE:\n```\n// mov.u32 sreg_value, %%nwarpid; // PTX ISA 20, SM_35\ntemplate <typename = void>\n__device__ static inline uint32_t get_sreg_nwarpid();\n```\n\n----------------------------------------\n\nTITLE: Using cuda::bitmask in CUDA Kernel\nDESCRIPTION: Example demonstrating how to use cuda::bitmask function in a CUDA kernel with assertions to verify the generated bitmasks for both uint32_t and uint64_t types.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/bit/bitmask.rst#2025-04-23_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n#include <cuda/bit>\n#include <cuda/std/cassert>\n#include <cuda/std/cstdint>\n\n__global__ void bitmask_kernel() {\n    assert(cuda::bitmask(2, 4) == 0b111100u);\n    assert(cuda::bitmask<uint64_t>(1, 3) == uint64_t{0b1110});\n}\n\nint main() {\n    bitmask_kernel<<<1, 1>>>();\n    cudaDeviceSynchronize();\n    return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Applying thrust::transform with Custom Operation in CUDA C++\nDESCRIPTION: This snippet demonstrates how to use thrust::transform to apply a custom operation (squaring) to elements of an input array in parallel. It shows the setup of device vectors and the use of a lambda function as the transformation operation.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/releases/changelog.rst#2025-04-23_snippet_12\n\nLANGUAGE: CUDA C++\nCODE:\n```\nthrust::device_vector<int> input(4);\nthrust::device_vector<int> output(4);\n\n// Initialize input vector\ninput[0] = 1; input[1] = 2; input[2] = 3; input[3] = 4;\n\n// Apply transform\nthrust::transform(input.begin(), input.end(), output.begin(),\n                  [] __device__ (int x) { return x * x; });\n\n// Output now contains: 1, 4, 9, 16\n```\n\n----------------------------------------\n\nTITLE: Accessing Device Memory from the Host and Cross-Device Access with CUDASTF in C++\nDESCRIPTION: Demonstrates launching a host-side task accessing device memory and a device-side task accessing memory residing on another device, enabled by CUDA unified memory. Requires compatible hardware and proper driver support (such as NVLINK/UVM). Shows flexibility and potential system limitations of non-affine data placement.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_23\n\nLANGUAGE: cpp\nCODE:\n```\nctx.task(exec_place::host(), lA.rw(data_place::device(0)))->*[](cudaStream_t s, auto a) {\n    ...\n};\n```\n\nLANGUAGE: cpp\nCODE:\n```\nctx.task(exec_place::device(0), lA.rw(data_place::device(1)))->*[](cudaStream_t s, auto a) {\n    ...\n};\n```\n\n----------------------------------------\n\nTITLE: Parallel_for Construct Syntax\nDESCRIPTION: The generic syntax for the parallel_for construct, which applies a kernel to each coordinate of a shape. The construct takes an execution place, optional partitioner, shape, and logical data access modes, then executes a device kernel for each element.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_61\n\nLANGUAGE: cpp\nCODE:\n```\nctx.parallel_for([execution place], [partitioner], shape, logicalData1.accessMode(), logicalData2.accessMode(), ...)\n    ->*[capture list] __device__ (size_t index1, size_t index2, ... auto data1, auto data2 ...) {\n        // Kernel implementation\n    };\n```\n\n----------------------------------------\n\nTITLE: Parallel_for on 1D Array Example\nDESCRIPTION: Example of applying the parallel_for construct on a 1D array. This demonstrates how to perform a SAXPY-like operation where each element of Y is updated with a scaled value from X plus the original Y value.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_62\n\nLANGUAGE: cpp\nCODE:\n```\ndouble X[N];\ndouble Y[N];\n\nauto lX = ctx.logical_data(X);\nauto lY = ctx.logical_data(Y);\n\nctx.parallel_for(lY.shape(), lX.read(), lY.rw())\n    ->*[alpha] __device__(size_t i, auto dX, auto dY) {\n        dY(i) += alpha * dX(i);\n    };\n```\n\n----------------------------------------\n\nTITLE: Adding 'cmake' Subdirectory to Build (CMake)\nDESCRIPTION: This CMake command adds the specified subdirectory ('cmake') to the build process. CMake will descend into the 'cmake' directory and process the `CMakeLists.txt` file found there, integrating its build logic and targets into the parent project's build system. This command requires the 'cmake' subdirectory to exist relative to the current `CMakeLists.txt` file and contain its own `CMakeLists.txt`.\nSOURCE: https://github.com/nvidia/cccl/blob/main/test/CMakeLists.txt#2025-04-23_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(cmake)\n```\n\n----------------------------------------\n\nTITLE: Defining CUDA Memory Space Accessors\nDESCRIPTION: Template definitions for host, device, and managed accessor types that specify memory space access patterns.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/mdspan/host_device_accessor.rst#2025-04-23_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\ntemplate <typename Accessor>\nusing host_accessor;\n\ntemplate <typename Accessor>\nusing device_accessor;\n\ntemplate <typename Accessor>\nusing managed_accessor;\n```\n\n----------------------------------------\n\nTITLE: Using cub::detail::temporary_storage::layout for Safe Allocation in C++\nDESCRIPTION: This C++ code demonstrates the use of `cub::detail::temporary_storage::layout` for managing temporary storage required by CUB algorithms. It allows defining multiple logical slots, creating typed allocations (aliases) within those slots, growing allocations based on runtime needs, calculating the total required size, and mapping the layout to a physical buffer (`d_temp_storage`). This simplifies complex storage management and helps prevent errors related to overlapping or incorrect usage.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/developer_overview.rst#2025-04-23_snippet_24\n\nLANGUAGE: c++\nCODE:\n```\ncub::detail::temporary_storage::layout<2> storage_layout;\n\nauto slot_1 = storage_layout.get_slot(0);\nauto slot_2 = storage_layout.get_slot(1);\n\nauto allocation_1 = slot_1->create_alias<int>();\nauto allocation_2 = slot_1->create_alias<double>(42);\nauto allocation_3 = slot_2->create_alias<char>(12);\n\nif (condition)\n{\n  allocation_1.grow(num_items);\n}\n\nif (d_temp_storage == nullptr)\n{\n  temp_storage_bytes = storage_layout.get_size();\n  return;\n}\n\nstorage_layout.map_to_buffer(d_temp_storage, temp_storage_bytes);\n\n// different slots, safe to use simultaneously\nuse(allocation_1.get(), allocation_3.get(), stream);\n// `allocation_2` alias `allocation_1`, safe to use in stream order\nuse(allocation_2.get(), stream);\n```\n\n----------------------------------------\n\nTITLE: Scoped Global 64-bit Store Operations\nDESCRIPTION: Template functions for 64-bit store operations with relaxed and release semantics across different scopes (CTA, cluster, GPU, system).\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_st.rst#2025-04-23_snippet_3\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename B64, enable_if_t<sizeof(B64) == 8, bool> = true, cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline void multimem_st(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  B64* addr,\n  B64 val);\n```\n\n----------------------------------------\n\nTITLE: Basic Barrier Cluster Arrive Operation\nDESCRIPTION: Device function for basic cluster barrier arrive operation. Requires PTX ISA 78 and SM_90. Implementation is volatile and clobbers memory.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/barrier_cluster.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline void barrier_cluster_arrive();\n```\n\n----------------------------------------\n\nTITLE: Defining cuda::pipeline::consumer_wait Functions in CUDA\nDESCRIPTION: Defines three variants of the consumer_wait function for CUDA pipelines: a basic wait, a wait with duration timeout, and a wait until a specific time point. These functions are used to synchronize consumer threads in CUDA pipeline operations.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/synchronization_primitives/pipeline/consumer_wait.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\n// (1)\ntemplate <cuda::thread_scope Scope>\n__host__ __device__\nvoid cuda::pipeline<Scope>::consumer_wait();\n\n// (2)\ntemplate <cuda::thread_scope Scope>\ntemplate <typename Rep, typename Period>\n__host__ __device__\nbool cuda::pipeline<Scope>::consumer_wait_for(\n  cuda::std::chrono::duration<Rep, Period> const& duration);\n\n// (3)\ntemplate <cuda::thread_scope Scope>\ntemplate <typename Clock, typename Duration>\n__host__ __device__\nbool cuda::pipeline<Scope>::consumer_wait_until(\n  cuda::std::chrono::time_point<Clock, Duration> const& time_point);\n```\n\n----------------------------------------\n\nTITLE: Including the libcu++ Complex Number Header in C++\nDESCRIPTION: This specifies the header file to include for using the libcu++ complex number functionalities, analogous to the standard C++ `<complex>` header but tailored for CUDA environments.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/standard_api/numerics_library/complex.rst#2025-04-23_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n<cuda/std/complex>\n```\n\n----------------------------------------\n\nTITLE: Copying 4D Tensor from Global to Shared Cluster Memory in CUDA\nDESCRIPTION: This function performs an asynchronous bulk copy of a 4D tensor from global memory to shared cluster memory. It uses a memory barrier for synchronization and supports multicast operations across the cluster.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_async_bulk_tensor_multicast.rst#2025-04-23_snippet_4\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline void cp_async_bulk_tensor(\n  cuda::ptx::space_cluster_t,\n  cuda::ptx::space_global_t,\n  void* dstMem,\n  const void* tensorMap,\n  const int32_t (&tensorCoords)[4],\n  uint64_t* smem_bar,\n  const uint16_t& ctaMask);\n```\n\n----------------------------------------\n\nTITLE: Specializing CUDASTF Stream Interface for Matrix Type\nDESCRIPTION: Defines a trait class specialization for matrix type in CUDASTF to provide the appropriate stream interface type. This enables proper handling of matrix data types within the streaming framework.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf/custom_data_interface.rst#2025-04-23_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\ntemplate <typename T>\nclass cudastf::streamed_interface_of<matrix<T>> {\npublic:\n    using type = matrix_stream_interface<T>;\n};\n```\n\n----------------------------------------\n\nTITLE: Configuring libcudacxx Example Projects in CMake\nDESCRIPTION: Sets up the CMake build configuration for libcudacxx examples including project initialization, dependency finding, test data acquisition, and target definitions with appropriate compiler flags and CUDA architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/libcudacxx/examples/CMakeLists.txt#2025-04-23_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.18)\n\nproject(libcudacxx-examples LANGUAGES CXX CUDA)\n\nset(CMAKE_BUILD_TYPE \"RelWithDebInfo\")\n\nfind_package(CUDAToolkit REQUIRED)\nfind_package(Threads REQUIRED)\nfind_package(OpenMP)\n\n# Download input files for the trie examples.\nif(NOT (EXISTS books))\n    execute_process(COMMAND mkdir books)\n    file(DOWNLOAD https://www.gutenberg.org/files/2600/2600-0.txt books/2600-0.txt SHOW_PROGRESS)\n    file(DOWNLOAD http://www.gutenberg.org/cache/epub/996/pg996.txt books/pg996.txt SHOW_PROGRESS)\n    file(DOWNLOAD http://www.gutenberg.org/cache/epub/55/pg55.txt books/pg55.txt SHOW_PROGRESS)\n    file(DOWNLOAD https://www.gutenberg.org/files/8800/8800.txt books/8800.txt SHOW_PROGRESS)\n    file(DOWNLOAD https://www.gutenberg.org/files/84/84-0.txt books/84-0.txt SHOW_PROGRESS)\n    file(DOWNLOAD http://www.gutenberg.org/cache/epub/6130/pg6130.txt books/pg6130.txt SHOW_PROGRESS)\n    file(DOWNLOAD http://www.gutenberg.org/cache/epub/1727/pg1727.txt books/pg1727.txt SHOW_PROGRESS)\n    file(DOWNLOAD https://www.gutenberg.org/files/2701/2701-0.txt books/2701-0.txt SHOW_PROGRESS)\n    file(DOWNLOAD https://www.gutenberg.org/files/35/35-0.txt books/35-0.txt SHOW_PROGRESS)\n    file(DOWNLOAD https://www.gutenberg.org/files/1342/1342-0.txt books/1342-0.txt SHOW_PROGRESS)\nendif()\n\nadd_executable(trie_st trie_st.cpp)\ntarget_compile_features(trie_st PRIVATE cxx_std_11)\n\nadd_executable(trie_mt trie_mt.cpp)\ntarget_compile_features(trie_mt PRIVATE cxx_std_11)\ntarget_link_libraries(trie_mt Threads::Threads)\n\nif(CUDAToolkit_VERSION VERSION_GREATER_EQUAL 11.1)\n    add_executable(trie_cuda trie.cu)\n    target_compile_features(trie_cuda PRIVATE cxx_std_11 cuda_std_11)\n    target_compile_options(trie_cuda PRIVATE --expt-relaxed-constexpr)\n    set_property(TARGET trie_cuda PROPERTY CUDA_ARCHITECTURES 70)\nelse()\n    message(STATUS \"Insufficient CUDA version. Skipping trie.cu example.\")\nendif()\n\nif(CUDAToolkit_VERSION VERSION_GREATER 10.2)\n    add_executable(rtc rtc_example.cpp)\n    target_link_libraries(rtc CUDA::nvrtc)\n    target_compile_features(rtc PRIVATE cxx_std_11)\nelse()\n    message(STATUS \"Insufficient CUDA version. Skipping rtc_example.cpp example.\")\nendif()\n\nadd_executable(hash_map concurrent_hash_table.cu)\ntarget_compile_features(hash_map PRIVATE cxx_std_14 cuda_std_14)\nset_property(TARGET hash_map PROPERTY CUDA_ARCHITECTURES 70)\ntarget_compile_options(hash_map PRIVATE --expt-extended-lambda)\n```\n\n----------------------------------------\n\nTITLE: Constraining Interface with cuda::has_property in C++20\nDESCRIPTION: Shows how to constrain a function interface to require a memory resource with device accessibility using C++20 concepts.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/memory_resource/properties.rst#2025-04-23_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\ntemplate<class MemoryResource>\n    requires cuda::has_property<MemoryResource, cuda::mr::device_accessible>\nvoid function_that_dispatches_to_device(MemoryResource& resource);\n```\n\n----------------------------------------\n\nTITLE: Using Target Macros for Conditional Code in CCCL\nDESCRIPTION: Demonstrates the use of NV_IF_TARGET, NV_IF_ELSE_TARGET, and NV_DISPATCH_TARGET macros for conditional code execution based on the compilation target (host/device) and SM architecture version.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cccl/development/macro.rst#2025-04-23_snippet_2\n\nLANGUAGE: c++\nCODE:\n```\nNV_IF_TARGET(NV_IS_DEVICE,    (auto x = threadIdx.x; return x;));\nNV_IF_ELSE_TARGET(NV_IS_HOST, (return 0;), (auto x = threadIdx.x; return x;));\nNV_DISPATCH_TARGET(NV_PROVIDES_SM_90,   (return \"Hopper+\";),\n                   NV_IS_EXACTLY_SM_75, (return \"Turing\";),\n                   NV_IS_HOST,          (return \"Host\";))\n```\n\n----------------------------------------\n\nTITLE: Defining CUB Tunings for Compile-Time Parameter Specialization in C++\nDESCRIPTION: This C++ code demonstrates the concept of tunings using the `sm60_tuning` template struct for SM60 architecture. It provides default values for `threads` and `items` and uses template specializations to override these defaults based on specific compile-time parameters like `ValueSize` (data type size) and `IsPlus` (whether the operation is addition). This allows fine-grained optimization of agent policies.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/developer_overview.rst#2025-04-23_snippet_21\n\nLANGUAGE: c++\nCODE:\n```\ntemplate <int ValueSize, bool IsPlus>\nstruct sm60_tuning { // default tuning\n    static constexpr int threads = 128;\n    static constexpr int items = 16;\n};\n\ntemplate <>\nstruct sm60_tuning<4, true> { // tuning for summing 4-byte values\n    static constexpr int threads = 256;\n    static constexpr int items = 20;\n};\n\ntemplate <int ValueSize>\nstruct sm60_tuning<ValueSize, true> { // tuning for summing values of other sizes\n    static constexpr int threads = 128;\n    static constexpr int items = 12;\n};\n\n...\n```\n\n----------------------------------------\n\nTITLE: MMA Block Scale Vector 2X Collector Fill with mxf4nvf4 Type (CTA Group 2)\nDESCRIPTION: Template function for matrix multiplication with block scaling using vector 2X with collector A fill operation for CTA group 2. The function can use either mxf4 or mxf4nvf4 data types and requires PTX ISA 86 with SM_100a or SM_101a architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma.rst#2025-04-23_snippet_29\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.cta_group.kind.block_scale.scale_vec::2X.collector::a::fill [d_tmem], a_desc, b_desc, idesc, [scale_A_tmem], [scale_B_tmem], enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .kind      = { .kind::mxf4, .kind::mxf4nvf4 }\n// .cta_group = { .cta_group::1, .cta_group::2 }\ntemplate <cuda::ptx::dot_kind Kind, cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_mma_block_scale_vec_2_tmem_a_collector_a_fill(\n  cuda::ptx::kind_t<Kind> kind,\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  uint32_t scale_A_tmem,\n  uint32_t scale_B_tmem,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Applying Single Property to CUDA Memory Range\nDESCRIPTION: This CUDA kernel shows how to apply a single access property to all elements in a memory range. It demonstrates setting the leading_bytes and total_bytes parameters to achieve this.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/memory_access_properties/access_property.rst#2025-04-23_snippet_5\n\nLANGUAGE: cuda\nCODE:\n```\n__global__ void range(int* g, size_t n) {\n  // To apply a single property to all elements in the range [g, g+n), set leading_bytes = total_bytes = n\n  auto range_property = cuda::access_property(g, n, n, cuda::access_property::persisting{});\n}\n```\n\n----------------------------------------\n\nTITLE: Defining tcgen05_mma MMA Kernel Intrinsic for PTX (CUDA C++)\nDESCRIPTION: Provides a template device function for configuring and launching tcgen05_mma MMA operations in CUDA C++, parameterized by N32, Kind (datatype), and Cta_Group (thread-group granularity) for fine control over PTX tensor instruction variants. Requires CUDA headers implementing cuda::ptx namespace, and is dependent on PTX ISA 86 and SM_100a compute capability. Accepts register/memory descriptors and scaling parameters as input, producing output written to d_tmem. Correct template parameterization is necessary to match kernel capabilities.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma.rst#2025-04-23_snippet_4\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.cta_group.kind [d_tmem], a_desc, b_desc, idesc, enable_input_d, scale_input_d; // PTX ISA 86, SM_100a\n// .kind      = { .kind::f16, .kind::tf32 }\n// .cta_group = { .cta_group::1, .cta_group::2 }\ntemplate <int N32, cuda::ptx::dot_kind Kind, cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_mma(\n  cuda::ptx::kind_t<Kind> kind,\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d,\n  cuda::ptx::n32_t<N32> scale_input_d);\n```\n\n----------------------------------------\n\nTITLE: Acquire Barrier Cluster Wait Operation\nDESCRIPTION: Device function for cluster barrier wait with acquire semantics. Requires PTX ISA 80 and SM_90. Implementation is volatile and clobbers memory.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/barrier_cluster.rst#2025-04-23_snippet_4\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline void barrier_cluster_wait(\n  cuda::ptx::sem_acquire_t);\n```\n\n----------------------------------------\n\nTITLE: Defining the cuda::isqrt function template in C++\nDESCRIPTION: Function template declaration for cuda::isqrt which computes the integer square root of an input value rounded down. The function is marked as constexpr and can be used in both host and device code.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/math/isqrt.rst#2025-04-23_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\ntemplate <typename T>\n[[nodiscard]] __host__ __device__ inline constexpr\nT isqrt(T value) noexcept;\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake to Use CCCL from GitHub\nDESCRIPTION: CMake configuration snippet that demonstrates how to fetch CCCL from GitHub using the CPM package manager and link it with your project. This allows using the latest CCCL version without waiting for CUDA Toolkit or HPC SDK releases.\nSOURCE: https://github.com/nvidia/cccl/blob/main/examples/basic/README.md#2025-04-23_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ninclude(cmake/CPM.cmake)\n\n# This will automatically clone CCCL from GitHub and make the exported cmake targets available\nCPMAddPackage(\n    NAME CCCL\n    GITHUB_REPOSITORY nvidia/cccl\n    GIT_TAG main # Fetches the latest commit on the main branch\n)\n\n# If you're building an executable\nadd_executable(your_executable your_file.cu)\ntarget_link_libraries(your_executable PRIVATE CCCL::CCCL)\n\n# Alternatively, if you're building a library\nadd_library(your_library your_file.cu)\ntarget_link_libraries(your_library PRIVATE CCCL::CCCL)\n```\n\n----------------------------------------\n\nTITLE: Example Usage of cuda::memcpy_async in CUDA Kernel\nDESCRIPTION: Demonstrates how to use cuda::memcpy_async within a CUDA kernel. This example initializes a barrier and performs four asynchronous memory copies before synchronizing with the barrier.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/asynchronous_operations/memcpy_async.rst#2025-04-23_snippet_3\n\nLANGUAGE: cuda\nCODE:\n```\n#include <cuda/barrier>\n\n__global__ void example_kernel(char* dst, char* src) {\n  cuda::barrier<cuda::thread_scope_system> bar;\n  init(&bar, 1);\n\n  cuda::memcpy_async(dst,     src,      1, bar);\n  cuda::memcpy_async(dst + 1, src + 8,  1, bar);\n  cuda::memcpy_async(dst + 2, src + 16, 1, bar);\n  cuda::memcpy_async(dst + 3, src + 24, 1, bar);\n\n  bar.arrive_and_wait();\n}\n```\n\n----------------------------------------\n\nTITLE: Defining CUDA Async Bulk Tensor Reduction Template (2D)\nDESCRIPTION: Defines a C++ template function `cp_reduce_async_bulk_tensor` for performing asynchronous bulk reduction operations on 2D tensors. It reduces data from shared memory (`space_shared_t`) to global memory (`space_global_t`) using a specified operation `Op` (e.g., add, min, max, xor). Requires PTX ISA 80 / SM_90. Parameters include the tensor map pointer (`tensorMap`), 2D tensor coordinates (`tensorCoords`), and source memory pointer (`srcMem`).\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_reduce_async_bulk_tensor.rst#2025-04-23_snippet_2\n\nLANGUAGE: cuda\nCODE:\n```\n// cp.reduce.async.bulk.tensor.2d.dst.src.op.tile.bulk_group [tensorMap, tensorCoords], [srcMem]; // 1b. PTX ISA 80, SM_90\n// .dst       = { .global }\n// .src       = { .shared::cta }\n// .op        = { .add, .min, .max, .inc, .dec, .and, .or, .xor }\ntemplate <cuda::ptx::dot_op Op>\n__device__ static inline void cp_reduce_async_bulk_tensor(\n  cuda::ptx::space_global_t,\n  cuda::ptx::space_shared_t,\n  cuda::ptx::op_t<Op> op,\n  const void* tensorMap,\n  const int32_t (&tensorCoords)[2],\n  const void* srcMem);\n```\n\n----------------------------------------\n\nTITLE: Implementing Asynchronous Bulk Copy from Global to Cluster Shared Memory in CUDA\nDESCRIPTION: This CUDA device function template implements an asynchronous bulk copy operation from global memory to cluster shared memory. It uses the cp.async.bulk instruction with multicast and barrier synchronization. The function supports PTX ISA 80 and SM versions 90a, 100a, and 101a.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_async_bulk_multicast.rst#2025-04-23_snippet_0\n\nLANGUAGE: CUDA\nCODE:\n```\n// cp.async.bulk.dst.src.mbarrier::complete_tx::bytes.multicast::cluster [dstMem], [srcMem], size, [smem_bar], ctaMask; // PTX ISA 80, SM_90a, SM_100a, SM_101a\n// .dst       = { .shared::cluster }\n// .src       = { .global }\ntemplate <typename = void>\n__device__ static inline void cp_async_bulk(\n  cuda::ptx::space_cluster_t,\n  cuda::ptx::space_global_t,\n  void* dstMem,\n  const void* srcMem,\n  const uint32_t& size,\n  uint64_t* smem_bar,\n  const uint16_t& ctaMask);\n```\n\n----------------------------------------\n\nTITLE: Implementing Small Key Tuning Template for SM100\nDESCRIPTION: Template specialization for sorting 1-byte keys without values and 8-byte offsets. The implementation shows thread and item configurations, with comments indicating better-performing values from tuning analysis.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/tuning.rst#2025-04-23_snippet_14\n\nLANGUAGE: cpp\nCODE:\n```\ntemplate <typename ValueT, size_t KeySize, size_t ValueSize, size_t OffsetSize>\nstruct sm100_small_key_tuning : sm90_small_key_tuning<KeySize, ValueSize, OffsetSize> {};\n\ntemplate <typename ValueT>\nstruct sm100_small_key_tuning<ValueT, 1, 0, 8> {\n  static constexpr int threads = 256; // better value from tuning analysis: 512\n  static constexpr int items = 14;    // better value from tuning analysis: 19\n};\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Bulk Copy from Global to Shared Cluster Memory with Barrier (CUDA)\nDESCRIPTION: Performs an asynchronous bulk copy from global memory to shared cluster memory, completing a memory barrier transaction. This operation is available from PTX ISA 80 and SM_90.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_async_bulk.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline void cp_async_bulk(\n  cuda::ptx::space_cluster_t,\n  cuda::ptx::space_global_t,\n  void* dstMem,\n  const void* srcMem,\n  const uint32_t& size,\n  uint64_t* smem_bar);\n```\n\n----------------------------------------\n\nTITLE: Introducing C++20 Synchronization Library Features (C++)\nDESCRIPTION: libcu++ 1.1.0 (CUDA Toolkit 11.0) introduces implementations of C++20 synchronization primitives, backported to C++11. This includes `cuda::[std::]barrier` from `<cuda/[std/]barrier>`, `cuda::[std::]latch` from `<cuda/std/latch>`, `cuda::[std::]semaphore` from `<cuda/std/semaphore>`, and atomic operations like `test`, `wait`, and `notify*` for `cuda::[std::]atomic_flag` and `cuda::[std::]atomic`.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/releases/changelog.rst#2025-04-23_snippet_26\n\nLANGUAGE: cpp\nCODE:\n```\n<cuda/[std/]barrier>\n```\n\nLANGUAGE: cpp\nCODE:\n```\n<cuda/std/latch>\n```\n\nLANGUAGE: cpp\nCODE:\n```\n<cuda/std/semaphore>\n```\n\nLANGUAGE: cpp\nCODE:\n```\ncuda::[std::]barrier\n```\n\nLANGUAGE: cpp\nCODE:\n```\ncuda::[std::]latch\n```\n\nLANGUAGE: cpp\nCODE:\n```\ncuda::[std::]semaphore\n```\n\nLANGUAGE: cpp\nCODE:\n```\ncuda::[std::]atomic_flag::test\n```\n\nLANGUAGE: cpp\nCODE:\n```\ncuda::[std::]atomic::wait\n```\n\nLANGUAGE: cpp\nCODE:\n```\ncuda::[std::]atomic::notify*\n```\n\n----------------------------------------\n\nTITLE: Declaring cuda::discard_memory Function in CUDA\nDESCRIPTION: Declaration of the cuda::discard_memory function, which is used to discard modified cache lines without writing back the cached data to memory. It enables using global memory as temporary scratch space without generating any hardware store operations.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/memory_access_properties/discard_memory.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\n__device__ void discard_memory(void volatile* ptr, size_t nbytes);\n```\n\n----------------------------------------\n\nTITLE: Defining cuda::atomic Class Template in CUDA\nDESCRIPTION: Declaration of the cuda::atomic class template with a type parameter T and an optional thread scope parameter. This class extends cuda::std::atomic with additional thread scope functionality.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/synchronization_primitives/atomic.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename T, cuda::thread_scope Scope = cuda::thread_scope_system>\nclass cuda::atomic;\n```\n\n----------------------------------------\n\nTITLE: Performing Asynchronous Bulk Reduction (ADD, s32) from Shared to Global Memory in CUDA\nDESCRIPTION: This CUDA C++ device function `cp_reduce_async_bulk` performs an asynchronous bulk addition reduction for signed 32-bit integers. It reduces data from a source array in shared memory (`srcMem`) to a destination array in global memory (`dstMem`). This overload requires PTX ISA 80 and SM_90 capability.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_reduce_async_bulk.rst#2025-04-23_snippet_23\n\nLANGUAGE: cuda\nCODE:\n```\n   // cp.reduce.async.bulk.dst.src.bulk_group.op.type  [dstMem], [srcMem], size; // 4. PTX ISA 80, SM_90\n   // .dst       = { .global }\n   // .src       = { .shared::cta }\n   // .type      = { .s32 }\n   // .op        = { .add }\n   template <typename = void>\n   __device__ static inline void cp_reduce_async_bulk(\n     cuda::ptx::space_global_t,\n     cuda::ptx::space_shared_t,\n     cuda::ptx::op_add_t,\n     int32_t* dstMem,\n     const int32_t* srcMem,\n     uint32_t size);\n```\n\n----------------------------------------\n\nTITLE: Configuring CUDA STF Test Targets in CMake\nDESCRIPTION: This CMake script iterates through CUDA targets, setting up test configurations for each. It creates metatargets for different test categories, adds basic tests, optional code generation tests, mathlib-dependent tests, and unittest headers. The script uses custom functions like cudax_add_stf_test and cudax_add_stf_unittest_header to add individual tests.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cudax/test/stf/CMakeLists.txt#2025-04-23_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\n# Create tests for each enabled configuration:\nforeach(cn_target IN LISTS cudax_TARGETS)\n  cudax_get_target_property(config_prefix ${cn_target} PREFIX)\n\n  # Metatargets for the current configuration's tests:\n  set(config_meta_target ${config_prefix}.tests)\n  set(stf_test_meta_target ${config_prefix}.tests.stf)\n  add_custom_target(${stf_test_meta_target})\n  add_dependencies(${config_meta_target} ${stf_test_meta_target})\n  set(stf_unittest_headers_meta_target ${config_prefix}.tests.stf.unittest_headers)\n  add_custom_target(${stf_unittest_headers_meta_target})\n  add_dependencies(${stf_test_meta_target} ${stf_unittest_headers_meta_target})\n\n  # Basic tests:\n  foreach(source IN LISTS stf_test_sources)\n    cudax_add_stf_test(test_target \"${source}\" ${cn_target})\n  endforeach()\n\n  if (cudax_ENABLE_CUDASTF_CODE_GENERATION)\n    foreach(source IN LISTS stf_test_codegen_sources)\n      cudax_add_stf_test(test_target \"${source}\" ${cn_target})\n    endforeach()\n  endif()\n\n  # Tests with mathlib deps:\n  if (cudax_ENABLE_CUDASTF_MATHLIBS)\n    foreach(source IN LISTS stf_test_mathlib_sources)\n      cudax_add_stf_test(test_target \"${source}\" ${cn_target} LINK_MATHLIBS)\n    endforeach()\n  endif()\n\n  # Unittested headers\n  foreach(source IN LISTS stf_unittested_headers)\n    cudax_add_stf_unittest_header(test_target \"${source}\" ${cn_target})\n  endforeach()\nendforeach()\n\nadd_subdirectory(static_error_checks)\n```\n\n----------------------------------------\n\nTITLE: Using bitfield_extract in a CUDA Kernel\nDESCRIPTION: Example demonstrating how to use cuda::bitfield_extract function in a CUDA kernel to extract bits from integer values. The example includes two assertions that validate the extraction functionality.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/bit/bitfield_extract.rst#2025-04-23_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n#include <cuda/bit>\n#include <cuda/std/cassert>\n\n__global__ void bitfield_insert_kernel() {\n    assert(cuda::bitfield_extract(~0u, 0, 4) == 0b1111);\n    assert(cuda::bitfield_extract(0b1011000, 3, 4) == 0b1011);\n}\n\nint main() {\n    bitfield_insert_kernel<<<1, 1>>>();\n    cudaDeviceSynchronize();\n    return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Matrix Shape Class for CUDASTF\nDESCRIPTION: Implementation of shape_of trait class specialization for matrix type, defining shape parameters and required constructors.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf/custom_data_interface.rst#2025-04-23_snippet_1\n\nLANGUAGE: c++\nCODE:\n```\ntemplate <typename T>\nclass cudastf::shape_of<matrix<T>> {\npublic:\n    shape_of() = default;\n    explicit shape_of(size_t m, size_t n) : m(m), n(n) {}\n    shape_of(const shape_of&) = default;\n    shape_of(const matrix<T>& M) : shape_of<matrix<T>>(M.m, M.n) {}\n    size_t size() const { return m * n; }\n    size_t m;\n    size_t n;\n};\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Yield Operation Limitations in CUDA Device Code\nDESCRIPTION: Example showing how using this_thread::yield in a CUDA kernel can lead to no progress because device threads don't support the yield operation like host threads do.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/execution_model.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n__global__ void ex1() {\n    while(true) cuda::std::this_thread::yield();\n}\n```\n\n----------------------------------------\n\nTITLE: Example Usage of cuda::ceil_div in CUDA Kernel Launch\nDESCRIPTION: Demonstrates how to use cuda::ceil_div to calculate the optimal number of thread blocks for a CUDA kernel launch. This example scales elements in a vector using a kernel where the number of blocks is determined by ceiling division of the total work by threads per block.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/math/ceil_div.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n#include <cuda/cmath>\n#include <cuda/std/span>\n#include <thrust/device_vector.h>\n\n__global__ void vector_scale_kernel(cuda::std::span<float> span, float scale) {\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < span.size())\n        span[index] *= scale;\n}\n\nint main() {\n    int   num_items = 100'000;\n    float scale = 2.f;\n    thrust::device_vector<float> d_vector(num_items, 1.f);\n    // Given a fixed number of threads per block...\n    constexpr int threads_per_block = 256;\n    // ...dividing some \"n\" by \"threads_per_block\" may lead to a remainder,\n    // requiring the kernel to be launched with an extra thread block to handle it.\n    auto num_thread_blocks = cuda::ceil_div(num_items, threads_per_block);\n    auto d_ptr             = thrust::raw_pointer_cast(d_vector.data());\n    cuda::std::span<float> d_span(d_ptr, num_items);\n\n    vector_scale_kernel<<<num_thread_blocks, threads_per_block>>>(d_span, scale);\n    cudaDeviceSynchronize();\n    return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Thread Hierarchy Synchronization in C++\nDESCRIPTION: Demonstrates how to check if a thread hierarchy level is synchronizable and perform synchronization. It shows usage for both dynamic and compile-time known values.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_38\n\nLANGUAGE: cpp\nCODE:\n```\nth.is_synchronizable(i); // if i is a dynamic value\nth.template is_synchronizable<i>(); // if i is known at compile time\n\nth.sync(i) // synchronize all threads of the i-th level\nth.sync() // synchronize all threads of the top-most level (level 0)\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication Template - CTA Group 1 with FP16\nDESCRIPTION: CUDA device function template for matrix multiplication using FP16 data type in CTA group 1. Handles memory descriptors, output lane disabling, and input scaling.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <int N32, cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma(\n  cuda::ptx::kind_t<Kind> kind,\n  cuda::ptx::cta_group_1_t,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  const uint32_t (&disable_output_lane)[4],\n  bool enable_input_d,\n  cuda::ptx::n32_t<N32> scale_input_d);\n```\n\n----------------------------------------\n\nTITLE: Integrating CUB Tunings within a Policy Hub in C++\nDESCRIPTION: This C++ snippet demonstrates how a `policy_hub` integrates a tuning structure (`sm60_tuning`) to parameterize its `AgentAlgorithmPolicy`. The `tuning` type alias selects the appropriate `sm60_tuning` specialization based on `ValueType` and `Operation`. The agent policy then uses the `threads` and `items` values provided by the selected tuning, enabling compile-time adaptation of kernel launch parameters.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/developer_overview.rst#2025-04-23_snippet_22\n\nLANGUAGE: c++\nCODE:\n```\ntemplate <typename ValueType, typename Operation>\nstruct policy_hub {\n  struct Policy600 : ChainedPolicy<600, Policy600, Policy500> {\n\n    using tuning = sm60_tuning<sizeof(ValueType), is_same_v<Operation, plus>>;\n    using AlgorithmPolicy = AgentAlgorithmPolicy<tuning::threads, tuning::items, BLOCK_LOAD_DIRECT, LOAD_LDG>;\n  };\n};\n```\n\n----------------------------------------\n\nTITLE: MMA Block Scale Vector 1X Collector Fill with mxf8f6f4 Type (CTA Group 1)\nDESCRIPTION: Template function for matrix multiplication with block scaling using vector 1X with collector A fill operation for CTA group 1. The function uses mxf8f6f4 data type and requires PTX ISA 86 with SM_100a or SM_101a architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma.rst#2025-04-23_snippet_24\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.cta_group.kind.block_scale.scale_vec::1X.collector::a::fill [d_tmem], a_desc, b_desc, idesc, [scale_A_tmem], [scale_B_tmem], enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .kind      = { .kind::mxf8f6f4 }\n// .cta_group = { .cta_group::1, .cta_group::2 }\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_mma_block_scale_vec_1x_tmem_a_collector_a_fill(\n  cuda::ptx::kind_mxf8f6f4_t,\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  uint32_t scale_A_tmem,\n  uint32_t scale_B_tmem,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Example Usage of cuda::bitfield_insert in CUDA Kernel\nDESCRIPTION: Demonstrates how to use the cuda::bitfield_insert function within a CUDA kernel. The example includes assertions that verify correct behavior for different input values, start positions, and widths.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/bit/bitfield_insert.rst#2025-04-23_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n#include <cuda/bit>\n#include <cuda/std/cassert>\n\n__global__ void bitfield_insert_kernel() {\n    assert(cuda::bitfield_insert(0u, 0xFFFFu, 0, 4) == 0b1111);\n    assert(cuda::bitfield_insert(0u, 0xFFFFu, 3, 4) == 0b1111000);\n    assert(cuda::bitfield_insert(1u, 0xFFFFu, 3, 4) == 0b1111001);\n}\n\nint main() {\n    bitfield_insert_kernel<<<1, 1>>>();\n    cudaDeviceSynchronize();\n    return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Thread 0 Block 0 Operations in CUDA\nDESCRIPTION: Demonstrates operations in Thread 0 of Block 0. It sets x to 42, creates an atomic reference to f with block scope, and performs a store operation. This leads to undefined behavior due to a data race.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/memory_model.rst#2025-04-23_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\nx = 42;\ncuda::atomic_ref<int, cuda::thread_scope_block> flag(f);\nflag.store(1, memory_order_release); // UB: data race\n```\n\n----------------------------------------\n\nTITLE: Store 128-bit Data with L1 No Allocate and L2 Cache Hint in CUDA\nDESCRIPTION: This function stores 128-bit data to global memory with L1 cache no allocate policy and L2 cache hint. It requires SM_80 or later architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/st.rst#2025-04-23_snippet_19\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename B128, enable_if_t<sizeof(B128) == 16, bool> = true>\n__device__ static inline void st_L1_no_allocate_L2_cache_hint(\n  cuda::ptx::space_global_t,\n  B128* addr,\n  B128 src,\n  uint64_t cache_policy);\n```\n\n----------------------------------------\n\nTITLE: Global Minimum Reduction for int32 with Different Memory Scopes\nDESCRIPTION: Template implementation for global minimum reduction operations on int32_t values. Supports both relaxed and release semantics across CTA, cluster, GPU and system memory scopes. Requires PTX ISA 8.1 and SM_90.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_red.rst#2025-04-23_snippet_2\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.red.sem.scope.global.op.s32 [addr], val; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .release }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .min }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline void multimem_red(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_min_t,\n  int32_t* addr,\n  int32_t val);\n```\n\n----------------------------------------\n\nTITLE: Using Affine Data Placement with CUDASTF Tasks in C++ (Device and Host)\nDESCRIPTION: Demonstrates default and explicit usage of affine data placement in tasks where data locality follows execution place: device memory for device tasks and host memory for host tasks. Logical data 'lA' is used on both execution places. These examples work seamlessly if the logical data and devices are properly initialized.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_21\n\nLANGUAGE: cpp\nCODE:\n```\nctx.task(exec_place::device(0), lA.rw())->*[](cudaStream_t s, auto a) {\n    ...\n};\n\nctx.task(exec_place::host(), lA.rw())->*[](cudaStream_t s, auto a) {\n    ...\n};\n```\n\nLANGUAGE: cpp\nCODE:\n```\nctx.task(exec_place::device(0), lA.rw(data_place::affine()))->*[](cudaStream_t s, auto a) {\n    ...\n};\n\nctx.task(exec_place::device(0), lA.rw(data_place::affine()))->*[](cudaStream_t s, auto a) {\n    ...\n};\n```\n\n----------------------------------------\n\nTITLE: CUDA Latch Usage Example in Kernel\nDESCRIPTION: Demonstrates different ways to create and use latches with various thread scopes in a CUDA kernel. Shows system-wide, device-wide, and block-wide synchronization options.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/synchronization_primitives/latch.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n__global__ void example_kernel() {\n  // This latch is suitable for all threads in the system.\n  cuda::latch<cuda::thread_scope_system> a(10);\n\n  // This latch has the same type as the previous one (`a`).\n  cuda::std::latch b(10);\n\n  // This latch is suitable for all threads on the current processor (e.g. GPU).\n  cuda::latch<cuda::thread_scope_device> c(10);\n\n  // This latch is suitable for all threads in the same thread block.\n  cuda::latch<cuda::thread_scope_block> d(10);\n}\n```\n\n----------------------------------------\n\nTITLE: CUDA Atomic Flag Producer Example\nDESCRIPTION: Shows potential deadlock scenario where cudaDeviceSynchronize is conditionally called based on atomic flag state. Demonstrates how lack of guaranteed thread progress can lead to indefinite blocking.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/execution_model.rst#2025-04-23_snippet_6\n\nLANGUAGE: cuda\nCODE:\n```\ncuda::atomic<int, cuda::thread_scope_system> flag = 0;\n__global__ void producer() { flag.store(1); }\nint main() {\n    cudaHostRegister(&flag, sizeof(flag));\n    producer<<<1,1>>>();\n    while (flag.load() == 0);\n    return cudaDeviceSynchronize();\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing bmsk.wrap.b32 Operation in CUDA\nDESCRIPTION: Template function for implementing the bmsk.wrap.b32 PTX instruction. Takes two 32-bit unsigned integers as input and performs a bitmap mask operation with wrapping behavior.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/bmsk.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n// bmsk.wrap.b32 dest, a_reg, b_reg; // PTX ISA 76, SM_70\ntemplate <typename = void>\n__device__ static inline uint32_t bmsk_wrap(\n  uint32_t a_reg,\n  uint32_t b_reg);\n```\n\n----------------------------------------\n\nTITLE: Constructing cuda::pipeline_shared_state in CUDA Kernel\nDESCRIPTION: This example demonstrates how to construct a cuda::pipeline_shared_state object in shared memory within a CUDA kernel. It uses a pragma to suppress a warning about static variable initialization in shared memory.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/synchronization_primitives/pipeline/shared_state.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n#include <cuda/pipeline>\n\n#pragma nv_diag_suppress static_var_with_dynamic_init\n\n__global__ void example_kernel() {\n  __shared__ cuda::pipeline_shared_state<cuda::thread_scope_block, 2> shared_state;\n}\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Bulk OR Reduction (32-bit) in CUDA\nDESCRIPTION: Performs an asynchronous bulk OR reduction operation from CTA-shared to cluster-shared memory for 32-bit data types. Uses a memory barrier for synchronization.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_reduce_async_bulk.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline void cp_reduce_async_bulk(\n  cuda::ptx::space_cluster_t,\n  cuda::ptx::space_shared_t,\n  cuda::ptx::op_or_op_t,\n  B32* dstMem,\n  const B32* srcMem,\n  uint32_t size,\n  uint64_t* rdsmem_bar);\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication Template - CTA Group 2 with TF32\nDESCRIPTION: CUDA device function template for matrix multiplication using TF32 data type in CTA group 2. Similar to the FP16 version but with different type configuration.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <int N32, cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma(\n  cuda::ptx::kind_t<Kind> kind,\n  cuda::ptx::cta_group_2_t,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  const uint32_t (&disable_output_lane)[8],\n  bool enable_input_d,\n  cuda::ptx::n32_t<N32> scale_input_d);\n```\n\n----------------------------------------\n\nTITLE: Thrust Library Updates Macros\nDESCRIPTION: New preprocessor macros for C++11 compatibility and type deduction helpers in Thrust library\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/releases/changelog.rst#2025-04-23_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\nTHRUST_PP_CAT[2-5]\nTHRUST_PP_EXPAND(_ARGS)?\nTHRUST_PP_ARITY\nTHRUST_PP_DISPATCH\nTHRUST_PP_BOOL\nTHRUST_PP_INC\nTHRUST_PP_DEC\nTHRUST_PP_HEAD\nTHRUST_PP_TAIL\nTHRUST_PP_IIF\nTHRUST_PP_COMMA_IF\nTHRUST_PP_HAS_COMMA\nTHRUST_PP_IS_VARIADIC_NULLARY\nTHRUST_CURRENT_FUNCTION\n```\n\n----------------------------------------\n\nTITLE: Implementing CUB DispatchAlgorithm for Kernel Launching in C++\nDESCRIPTION: This snippet shows the structure of `DispatchAlgorithm` in CUB, responsible for the host-side logic and kernel launch. The `Invoke` method, templated on the `ActivePolicy` selected by `ChainedPolicy`, executes the algorithm's host-side implementation. It crucially demonstrates launching the CUDA kernel templated on `MaxPolicy` (the highest possible policy) while using `ActivePolicy` to determine launch parameters like `BLOCK_THREADS` via `__launch_bounds__`. This optimizes compile time by reducing kernel instantiations.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/developer_overview.rst#2025-04-23_snippet_14\n\nLANGUAGE: cpp\nCODE:\n```\ntemplate <..., typename PolicyHub = detail::algorithm::policy_hub>\nstruct DispatchAlgorithm {\n  template <typename ActivePolicy>\n  CUB_RUNTIME_FUNCTION _CCCL_FORCEINLINE\n  cudaError_t Invoke() {\n    // host-side implementation of algorithm, calls kernels\n    using MaxPolicy = typename DispatchSegmentedReduce::MaxPolicy;\n    kernel<MaxPolicy /*(2)*/><<<grid_size, ActivePolicy::AlgorithmPolicy::BLOCK_THREADS /*(1)*/>>>(...); // calls (4)\n  }\n};\n```\n\n----------------------------------------\n\nTITLE: Declaring multimem_ld_reduce with Relaxed/System Scope and OR Operation in CUDA PTX\nDESCRIPTION: Template function declaration for multimem.ld_reduce with relaxed memory semantics, system scope, and OR operation on 64-bit values in global memory. Compatible with PTX ISA 8.1 on SM_90 architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_64\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.b64 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .or }\ntemplate <typename B64, enable_if_t<sizeof(B64) == 8, bool> = true, cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline B64 multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_or_op_t,\n  const B64* addr);\n```\n\n----------------------------------------\n\nTITLE: Implementing Acquire Fence with Shared Cluster Synchronization in CUDA\nDESCRIPTION: This CUDA device function implements a memory fence operation with acquire semantics, synchronization restricted to shared memory in the cluster scope. It is designed for use on SM_90 architecture and corresponds to PTX ISA 86.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/fence_sync_restrict.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\n// fence.sem.sync_restrict::space.scope; // PTX ISA 86, SM_90\n// .sem       = { .acquire }\n// .space     = { .shared::cluster }\n// .scope     = { .cluster }\ntemplate <typename = void>\n__device__ static inline void fence_sync_restrict(\n  cuda::ptx::sem_acquire_t,\n  cuda::ptx::space_cluster_t,\n  cuda::ptx::scope_cluster_t);\n```\n\n----------------------------------------\n\nTITLE: Defining CUDA Async Bulk Tensor Reduction Template (3D)\nDESCRIPTION: Defines a C++ template function `cp_reduce_async_bulk_tensor` for performing asynchronous bulk reduction operations on 3D tensors. It reduces data from shared memory (`space_shared_t`) to global memory (`space_global_t`) using a specified operation `Op` (e.g., add, min, max, inc, dec, and, or, xor). Requires PTX ISA 80 / SM_90. Parameters include the tensor map pointer (`tensorMap`), 3D tensor coordinates (`tensorCoords`), and source memory pointer (`srcMem`).\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_reduce_async_bulk_tensor.rst#2025-04-23_snippet_3\n\nLANGUAGE: cuda\nCODE:\n```\n// cp.reduce.async.bulk.tensor.3d.dst.src.op.tile.bulk_group [tensorMap, tensorCoords], [srcMem]; // 1c. PTX ISA 80, SM_90\n// .dst       = { .global }\n// .src       = { .shared::cta }\n// .op        = { .add, .min, .max, .inc, .dec, .and, .or, .xor }\ntemplate <cuda::ptx::dot_op Op>\n__device__ static inline void cp_reduce_async_bulk_tensor(\n  cuda::ptx::space_global_t,\n  cuda::ptx::space_shared_t,\n  cuda::ptx::op_t<Op> op,\n  const void* tensorMap,\n  const int32_t (&tensorCoords)[3],\n  const void* srcMem);\n```\n\n----------------------------------------\n\nTITLE: CUDA Associate Access Property Function Declaration\nDESCRIPTION: Template function declaration for associating memory access properties with pointers in CUDA. Takes a pointer and property as parameters and returns a pointer with the associated access property.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/memory_access_properties/associate_access_property.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <class T, class Property>\n__host__ __device__\nT* associate_access_property(T* ptr, Property prop);\n```\n\n----------------------------------------\n\nTITLE: Using extended atomic in CUDA C++ with libcu++\nDESCRIPTION: Shows how to use the extended atomic type from libcu++ with a specified thread scope. This example demonstrates the inclusion of the cuda/atomic header and creation of an atomic integer with device thread scope.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/index.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n#include <cuda/atomic>\ncuda::atomic<int, cuda::thread_scope_device> x;\n```\n\n----------------------------------------\n\nTITLE: Performing Asynchronous Bulk Reduction (ADD, u64) from Shared to Global Memory in CUDA\nDESCRIPTION: This CUDA C++ device function `cp_reduce_async_bulk` performs an asynchronous bulk addition reduction for unsigned 64-bit integers. It reduces data from a source array in shared memory (`srcMem`) to a destination array in global memory (`dstMem`). This overload requires PTX ISA 80 and SM_90 capability.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_reduce_async_bulk.rst#2025-04-23_snippet_26\n\nLANGUAGE: cuda\nCODE:\n```\n   // cp.reduce.async.bulk.dst.src.bulk_group.op.type  [dstMem], [srcMem], size; // 4. PTX ISA 80, SM_90\n   // .dst       = { .global }\n   // .src       = { .shared::cta }\n   // .type      = { .u64 }\n   // .op        = { .add }\n   template <typename = void>\n   __device__ static inline void cp_reduce_async_bulk(\n     cuda::ptx::space_global_t,\n     cuda::ptx::space_shared_t,\n     cuda::ptx::op_add_t,\n```\n\n----------------------------------------\n\nTITLE: Message Passing Pattern Implementation Using Atomic Thread Fence\nDESCRIPTION: Example kernel demonstrating the Message Passing pattern using atomic thread fence. Shows coordination between two thread blocks where one writes data and signals completion while the other waits for the signal before reading.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/synchronization_primitives/atomic/atomic_thread_fence.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n#include <cstdio>\n#include <cuda/atomic>\n#include <cooperative_groups>\n\nnamespace cg = cooperative_groups;\n\n__global__ void example_kernel(int* data, cuda::std::atomic_flag* flag) {\n  assert(cg::grid_group::size() == 2);\n  assert(cg::thread_block::size() == 1);\n\n  if (blockIdx.x == 0) {\n    *data = 42;\n    cuda::atomic_thread_fence(cuda::memory_order_release,\n                              cuda::thread_scope_device);\n    flag->test_and_set(cuda::std::memory_order_relaxed);\n    flag->notify_one();\n  }\n  else {\n    // an atomic operation is required to set up the synchronization\n    flag->wait(false, cuda::std::memory_order_relaxed);\n    cuda::atomic_thread_fence(cuda::memory_order_acquire,\n                              cuda::thread_scope_device);\n    std::printf(\"%d\\n\", *data); // Prints 42\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Building CCCL Components\nDESCRIPTION: Command pattern for building CCCL components using the CI scripts. Requires specifying host compiler, C++ standard, and GPU architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/CONTRIBUTING.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n./ci/build_cub.sh -cxx g++ -std 17 -arch \"70;75;80-virtual\"\n```\n\n----------------------------------------\n\nTITLE: Defining Temporary Storage Type for Warp Reduce in C++\nDESCRIPTION: This snippet shows how the temporary storage type is defined for warp reduce operations using a specialization.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/developer_overview.rst#2025-04-23_snippet_5\n\nLANGUAGE: C++\nCODE:\n```\nusing _TempStorage = typename InternalWarpReduce::TempStorage;\n```\n\n----------------------------------------\n\nTITLE: Computing Dot Product using Reduction in CUDASTF\nDESCRIPTION: Illustrates how to compute the dot product of two vectors using a reduction operation in CUDASTF. It uses parallel_for with reduce access mode and a scalar_view<double> for the result.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_35\n\nLANGUAGE: cpp\nCODE:\n```\nauto lsum = ctx.logical_data(shape_of<scalar_view<double>>());\n\n/* Compute sum(x_i * y_i)*/\nctx.parallel_for(lY.shape(), lX.read(), lY.read(), lsum.reduce(reducer::sum<double>{}))\n    ->*[] __device__(size_t i, auto dX, auto dY, double& sum) {\n          sum += dX(i) * dY(i);\n        };\n\ndouble res = ctx.wait(lsum);\n```\n\n----------------------------------------\n\nTITLE: Defining CUDA Async Bulk Tensor Reduction Template (4D)\nDESCRIPTION: Defines a C++ template function `cp_reduce_async_bulk_tensor` for performing asynchronous bulk reduction operations on 4D tensors. It reduces data from shared memory (`space_shared_t`) to global memory (`space_global_t`) using a specified operation `Op` (e.g., add, min, max, inc, dec, and, or, xor). Requires PTX ISA 80 / SM_90. Parameters include the tensor map pointer (`tensorMap`), 4D tensor coordinates (`tensorCoords`), and source memory pointer (`srcMem`).\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_reduce_async_bulk_tensor.rst#2025-04-23_snippet_4\n\nLANGUAGE: cuda\nCODE:\n```\n// cp.reduce.async.bulk.tensor.4d.dst.src.op.tile.bulk_group [tensorMap, tensorCoords], [srcMem]; // 1d. PTX ISA 80, SM_90\n// .dst       = { .global }\n// .src       = { .shared::cta }\n// .op        = { .add, .min, .max, .inc, .dec, .and, .or, .xor }\ntemplate <cuda::ptx::dot_op Op>\n__device__ static inline void cp_reduce_async_bulk_tensor(\n  cuda::ptx::space_global_t,\n  cuda::ptx::space_shared_t,\n  cuda::ptx::op_t<Op> op,\n  const void* tensorMap,\n  const int32_t (&tensorCoords)[4],\n  const void* srcMem);\n```\n\n----------------------------------------\n\nTITLE: CUDA Barrier Initialization Example\nDESCRIPTION: Practical example showing how to initialize a shared memory barrier in a CUDA kernel using the init function with warning suppression.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/synchronization_primitives/barrier/init.rst#2025-04-23_snippet_2\n\nLANGUAGE: cuda\nCODE:\n```\n#include <cuda/barrier>\n\n// Disables `cuda::barrier` initialization warning.\n#pragma nv_diag_suppress static_var_with_dynamic_init\n\n__global__ void example_kernel() {\n  __shared__ cuda::barrier<cuda::thread_scope_block> bar;\n  init(&bar, 1);\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Block-Scaled MMA for mxf8f6f4 with 1X Vector Scaling and TMEM A in CUDA\nDESCRIPTION: This template function implements a block-scaled matrix multiply-accumulate operation for mxf8f6f4 data type with 1X vector scaling and TMEM A. It supports different CTA group configurations.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma.rst#2025-04-23_snippet_17\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.cta_group.kind.block_scale.scale_vec::1X [d_tmem], a_desc, b_desc, idesc, [scale_A_tmem], [scale_B_tmem], enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .kind      = { .kind::mxf8f6f4 }\n// .cta_group = { .cta_group::1, .cta_group::2 }\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_mma_block_scale_vec_1x_tmem_a(\n  cuda::ptx::kind_mxf8f6f4_t,\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  uint32_t scale_A_tmem,\n  uint32_t scale_B_tmem,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Auto-Downloading CPM in CMakeLists.txt\nDESCRIPTION: CMake code that automatically downloads CPM if it's not already present in the project directory. This approach eliminates the need for a separate download step and ensures CPM is available when building the project.\nSOURCE: https://github.com/nvidia/cccl/blob/main/examples/basic/README.md#2025-04-23_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nset(CPM_DOWNLOAD_VERSION 0.34.0)\n\nif(CPM_SOURCE_CACHE)\n  set(CPM_DOWNLOAD_LOCATION \"${CPM_SOURCE_CACHE}/cpm/CPM_${CPM_DOWNLOAD_VERSION}.cmake\")\nelseif(DEFINED ENV{CPM_SOURCE_CACHE})\n  set(CPM_DOWNLOAD_LOCATION \"$ENV{CPM_SOURCE_CACHE}/cpm/CPM_${CPM_DOWNLOAD_VERSION}.cmake\")\nelse()\n  set(CPM_DOWNLOAD_LOCATION \"${CMAKE_BINARY_DIR}/cmake/CPM_${CPM_DOWNLOAD_VERSION}.cmake\")\nendif()\n\nif(NOT (EXISTS ${CPM_DOWNLOAD_LOCATION}))\n  message(STATUS \"Downloading CPM.cmake to ${CPM_DOWNLOAD_LOCATION}\")\n  file(DOWNLOAD\n       https://github.com/TheLartians/CPM.cmake/releases/download/v${CPM_DOWNLOAD_VERSION}/CPM.cmake\n       ${CPM_DOWNLOAD_LOCATION}\n  )\nendif()\n\ninclude(${CPM_DOWNLOAD_LOCATION})\n```\n\n----------------------------------------\n\nTITLE: Async Store with Barrier Completion - 64-bit Vector2\nDESCRIPTION: Template for asynchronous weak shared memory store operation with cluster barrier completion tracking for 2-element vectors of 64-bit values. Compatible with PTX ISA 81 and SM_90 architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/st_async.rst#2025-04-23_snippet_3\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename Type>\n__device__ static inline void st_async(\n  Type* addr,\n  const Type (&value)[2],\n  uint64_t* remote_bar);\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Bulk Reduction Max Operation for bfloat16 in CUDA\nDESCRIPTION: This function performs an asynchronous bulk reduction maximum operation on bfloat16 data, transferring from shared memory to global memory. It uses CUDA PTX ISA 80 for SM_90 architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_reduce_async_bulk_bf16.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline void cp_reduce_async_bulk(\n  cuda::ptx::space_global_t,\n  cuda::ptx::space_shared_t,\n  cuda::ptx::op_max_t,\n  __nv_bfloat16* dstMem,\n  const __nv_bfloat16* srcMem,\n  uint32_t size);\n```\n\n----------------------------------------\n\nTITLE: Building and Running CCCL Examples with CMake\nDESCRIPTION: Commands to build and run only the CCCL examples using CMake. It configures the build with examples enabled while disabling other components like Thrust, CUB, libcuda C++, and testing. Then it builds the project and runs tests with output on failure.\nSOURCE: https://github.com/nvidia/cccl/blob/main/examples/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncmake -S . -B build -DCCCL_ENABLE_EXAMPLES=ON -DCCCL_ENABLE_THRUST=OFF -DCCCL_ENABLE_CUB=OFF -DCCCL_ENABLE_LIBCUDACXX=OFF -DCCCL_ENABLE_TESTING=OFF\ncmake --build build\nctest --test-dir build --output-on-failure\n```\n\n----------------------------------------\n\nTITLE: CUDA Kernel Implementation Using mbarrier.arrive Instructions\nDESCRIPTION: Demonstrates usage of mbarrier.arrive instructions in a CUDA kernel with both shared memory and cluster barriers. Shows initialization, synchronization, and communication between different thread blocks in a cluster.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/mbarrier_arrive.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\n#include <cuda/ptx>\n#include <cuda/barrier>\n#include <cooperative_groups.h>\n\n__global__ void kernel() {\n    using cuda::ptx::sem_release;\n    using cuda::ptx::space_cluster;\n    using cuda::ptx::space_shared;\n    using cuda::ptx::scope_cluster;\n    using cuda::ptx::scope_cta;\n\n    using barrier_t = cuda::barrier<cuda::thread_scope_block>;\n    __shared__ barrier_t bar;\n    init(&bar, blockDim.x);\n    __syncthreads();\n\n    NV_IF_TARGET(NV_PROVIDES_SM_90, (\n        // Arrive on local shared memory barrier:\n        uint64_t token;\n        token = cuda::ptx::mbarrier_arrive_expect_tx(sem_release, scope_cluster, space_shared, &bar, 1);\n\n        // Get address of remote cluster barrier:\n        namespace cg = cooperative_groups;\n        cg::cluster_group cluster = cg::this_cluster();\n        unsigned int other_block_rank = cluster.block_rank() ^ 1;\n        uint64_t * remote_bar = cluster.map_shared_rank(&bar, other_block_rank);\n\n        // Sync cluster to ensure remote barrier is initialized.\n        cluster.sync();\n\n        // Arrive on remote cluster barrier:\n        cuda::ptx::mbarrier_arrive_expect_tx(sem_release, scope_cluster, space_cluster, remote_bar, 1);\n    )\n}\n```\n\n----------------------------------------\n\nTITLE: Using Range Concepts in C++17 and C++20 with CUDA Standard Library\nDESCRIPTION: Demonstrates how to use range concepts in both C++20 (direct usage) and C++17 (type trait style with enable_if). This shows the flexibility of the CUDA C++ Standard Library allowing range concepts to be used across different C++ standards.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/standard_api/ranges_library.rst#2025-04-23_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\ntemplate<cuda::std::contiguos_range Range>\nvoid do_something_with_ranges_in_cpp20(Range&& range) {...}\n\ntemplate<class Range, cuda::std::enable_if_t<cuda::std::contiguos_range<Range>, int> = 0>\nvoid do_something_with_ranges_in_cpp17(Range&& range) {...}\n```\n\n----------------------------------------\n\nTITLE: Implementing Single Thread Group in CUDA\nDESCRIPTION: Demonstrates a concrete implementation of the ThreadGroup concept for a single thread case, including thread scope definition, size tracking, ranking, and synchronization.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/thread_groups.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n#include <cuda/atomic>\n#include <cuda/std/cstddef>\n\nstruct single_thread_group {\n  static constexpr cuda::thread_scope thread_scope = cuda::thread_scope::thread_scope_thread;\n  cuda::std::size_t size() const { return 1; }\n  cuda::std::size_t thread_rank() const { return 0; }\n  void sync() const {}\n};\n```\n\n----------------------------------------\n\nTITLE: AND Operation for 32-bit Type\nDESCRIPTION: Template function implementing asynchronous AND reduction operation on cluster memory barrier for b32 type. Takes destination pointer, value and remote barrier pointer as parameters. Uses SFINAE to ensure 4-byte size.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/red_async.rst#2025-04-23_snippet_8\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline void red_async(\n  cuda::ptx::op_and_op_t,\n  B32* dest,\n  const B32& value,\n  uint64_t* remote_bar);\n```\n\n----------------------------------------\n\nTITLE: Defining tcgen05_mma_tmem_a Variant with A Tile in Shared Memory for CTA Group 2 (CUDA C++)\nDESCRIPTION: Specializes tcgen05_mma_tmem_a for CTA group 2 and data kinds (f16, tf32), parameterized by N32 and dot_kind. Designed to handle different output lane configuration for wider thread groups (8 lanes). All arguments must match expected PTX MMA instruction constraints, and is intended for use on SM_100a architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma.rst#2025-04-23_snippet_7\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.cta_group.kind [d_tmem], [a_tmem], b_desc, idesc, disable_output_lane, enable_input_d, scale_input_d; // PTX ISA 86, SM_100a\n// .kind      = { .kind::f16, .kind::tf32 }\n// .cta_group = { .cta_group::2 }\ntemplate <int N32, cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_tmem_a(\n  cuda::ptx::kind_t<Kind> kind,\n  cuda::ptx::cta_group_2_t,\n  uint32_t d_tmem,\n  uint32_t a_tmem,\n  uint64_t b_desc,\n  uint32_t idesc,\n  const uint32_t (\\u0026disable_output_lane)[8],\n  bool enable_input_d,\n  cuda::ptx::n32_t<N32> scale_input_d);\n```\n\n----------------------------------------\n\nTITLE: Introducing Annotated Pointers and Access Properties (C++)\nDESCRIPTION: libcu++ 1.6.0 adds APIs for memory management: `cuda::annotated_ptr` and `cuda::access_property` allow associating an address space and caching policy with a pointer. Related functions `cuda::apply_access_property`, `cuda::associate_access_property`, and `cuda::discard_memory` are also introduced for managing these properties.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/releases/changelog.rst#2025-04-23_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\ncuda::annotated_ptr\n```\n\nLANGUAGE: cpp\nCODE:\n```\ncuda::access_property\n```\n\nLANGUAGE: cpp\nCODE:\n```\ncuda::apply_access_property\n```\n\nLANGUAGE: cpp\nCODE:\n```\ncuda::associate_access_property\n```\n\nLANGUAGE: cpp\nCODE:\n```\ncuda::discard_memory\n```\n\n----------------------------------------\n\nTITLE: Message Passing Thread Block 1 Implementation\nDESCRIPTION: Shows the receiving side of a message passing implementation using atomic operations with device scope synchronization.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/memory_model.rst#2025-04-23_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\ncuda::atomic_ref<int, cuda::thread_scope_device> flag(f);\nwhile(flag.load(memory_order_acquire) != 1);\nassert(x == 42);\n```\n\n----------------------------------------\n\nTITLE: Example Usage of CUDA Atomic Fetch Max\nDESCRIPTION: Demonstration kernel showing how to use the atomic fetch_max operation. Creates an atomic integer, performs fetch_max operation, and verifies the results with assertions.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/synchronization_primitives/atomic/fetch_max.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n#include <cuda/atomic>\n\n__global__ void example_kernel() {\n  cuda::atomic<int> a(0);\n  auto x = a.fetch_max(1);\n  auto y = a.load();\n  assert(x == 0 && y == 1);\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing multimem.ld_reduce XOR operation for 32-bit data in CUDA\nDESCRIPTION: Template implementation for multimem.ld_reduce operation with XOR reduction for 32-bit data. The template supports various memory semantics (relaxed, acquire) and scopes (cta, cluster, gpu, sys) for global memory operations.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_56\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.b32 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .xor }\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true, cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline B32 multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_xor_op_t,\n  const B32* addr);\n```\n\n----------------------------------------\n\nTITLE: Defining Backend-Specific Targets in Thrust CMake Configuration\nDESCRIPTION: Describes the structure of backend-specific targets (CPP, CUDA, TBB, OMP) in Thrust's CMake configuration. Each backend has a main target and optional Host and Device subtargets with specific usage constraints.\nSOURCE: https://github.com/nvidia/cccl/blob/main/lib/cmake/thrust/README.md#2025-04-23_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\nThrust::${system}\nThrust::${system}::Host\nThrust::${system}::Device\n```\n\n----------------------------------------\n\nTITLE: Creating Container Tests in CMake\nDESCRIPTION: Sets up comprehensive tests for CUDA container types, focusing on uninitialized buffers and async buffers. Tests cover all aspects of container functionality including construction, assignment, capacity management, comparison, iteration, and data transformation.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cudax/test/CMakeLists.txt#2025-04-23_snippet_9\n\nLANGUAGE: CMake\nCODE:\n```\n  cudax_add_catch2_test(test_target containers ${cn_target}\n    containers/uninitialized_buffer.cu\n    containers/uninitialized_async_buffer.cu\n    containers/async_buffer/access.cu\n    containers/async_buffer/assign.cu\n    containers/async_buffer/capacity.cu\n    containers/async_buffer/comparison.cu\n    containers/async_buffer/constructor.cu\n    containers/async_buffer/conversion.cu\n    containers/async_buffer/copy.cu\n    containers/async_buffer/iterators.cu\n    containers/async_buffer/properties.cu\n    containers/async_buffer/swap.cu\n    containers/async_buffer/transform.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: Global Memory Store Operations (Basic)\nDESCRIPTION: Template functions for basic global memory store operations supporting different data sizes (8-bit, 16-bit, 32-bit, 64-bit, 128-bit). These operations require SM_50+ for most sizes and SM_70+ for 128-bit operations.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/st.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\n// st.space.b8 [addr], src; // PTX ISA 10, SM_50\ntemplate <typename B8, enable_if_t<sizeof(B8) == 1, bool> = true>\n__device__ static inline void st(\n  cuda::ptx::space_global_t,\n  B8* addr,\n  B8 src);\n```\n\n----------------------------------------\n\nTITLE: Function Declaration of cuda::round_up\nDESCRIPTION: Template function declaration for rounding up a value to the smallest multiple of a base value. The function is both host and device compatible and returns the common type of the input parameters.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/math/round_up.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename T, typename U>\n[[nodiscard]] __host__ __device__ inline constexpr\ncuda::std::common_type_t<T, U> round_up(T value, U base_multiple) noexcept;\n```\n\n----------------------------------------\n\nTITLE: Adding <cuda/pipeline> for Asynchronous memcpy Coordination (C++)\nDESCRIPTION: New feature in libcu++ 1.2.0 (CUDA Toolkit 11.1). Introduces `<cuda/pipeline>` which provides `cuda::pipeline`, a facility designed for coordinating `cuda::memcpy_async` operations.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/releases/changelog.rst#2025-04-23_snippet_21\n\nLANGUAGE: cpp\nCODE:\n```\n<cuda/pipeline>\n```\n\nLANGUAGE: cpp\nCODE:\n```\ncuda::pipeline\n```\n\nLANGUAGE: cpp\nCODE:\n```\ncuda::memcpy_async\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Matrix Class in C++\nDESCRIPTION: Simple matrix class implementation with basic functionality for dimensions and element access. Includes host and device accessors.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf/custom_data_interface.rst#2025-04-23_snippet_0\n\nLANGUAGE: c++\nCODE:\n```\ntemplate <typename T>\nclass matrix {\npublic:\n    matrix(size_t m, size_t n, T* base) : m(m), n(n), base(base) {}\n    __host__ __device__ T& operator()(size_t i, size_t j) { return base[i + j * m]; }\n    __host__ __device__ const T& operator()(size_t i, size_t j) const { return base[i + j * m]; }\n    size_t m, n;\n    T* base;\n};\n```\n\n----------------------------------------\n\nTITLE: Declaring CUDA Pipeline Producer Acquire Function\nDESCRIPTION: Template function declaration for producer_acquire() method in the cuda::pipeline class. This function blocks the current producer thread until the next pipeline stage becomes available. It requires thread scope specification and can be called from both host and device contexts. The function must not be called by consumer threads or when pipeline is in quit state.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/synchronization_primitives/pipeline/producer_acquire.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::thread_scope Scope>\n__host__ __device__\nvoid cuda::pipeline<Scope>::producer_acquire();\n```\n\n----------------------------------------\n\nTITLE: Using cuda::pipeline_shared_state in Various Memory Locations in CUDA\nDESCRIPTION: This example shows how to create cuda::pipeline_shared_state objects in different memory locations: shared memory, device memory, and system memory. It demonstrates various scopes and allocation methods.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/synchronization_primitives/pipeline/shared_state.rst#2025-04-23_snippet_2\n\nLANGUAGE: cuda\nCODE:\n```\n#include <cuda/pipeline>\n\n// Disables `pipeline_shared_state` initialization warning.\n#pragma nv_diag_suppress static_var_with_dynamic_init\n\n__global__ void example_kernel(char* device_buffer, char* sysmem_buffer) {\n  // Allocate a 2 stage block scoped shared state in shared memory.\n  __shared__ cuda::pipeline_shared_state<cuda::thread_scope_block, 2> pss0;\n\n  // Allocate a 2 stage block scoped shared state in device memory.\n  auto* pss1 = new cuda::pipeline_shared_state<cuda::thread_scope_block, 2>;\n\n  // Construct a 2 stage device scoped shared state in device memory.\n  auto* pss2 =\n    new (device_buffer) cuda::pipeline_shared_state<cuda::thread_scope_device, 2>;\n\n  // Construct a 2 stage system scoped shared state in system memory.\n  auto* pss3 =\n    new (sysmem_buffer) cuda::pipeline_shared_state<cuda::thread_scope_system, 2>;\n}\n```\n\n----------------------------------------\n\nTITLE: Building the CCCL C Parallel Library\nDESCRIPTION: Configures the CCCL C Parallel shared library target by collecting source files, setting target properties, and configuring library output directories if specified.\nSOURCE: https://github.com/nvidia/cccl/blob/main/c/parallel/CMakeLists.txt#2025-04-23_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB_RECURSE srcs\n  RELATIVE \"${CMAKE_CURRENT_LIST_DIR}\"\n  CONFIGURE_DEPENDS\n  \"src/*.cu\" \"src/*.cpp\"\n)\n\nadd_library(cccl.c.parallel SHARED ${srcs})\nset_property(TARGET cccl.c.parallel PROPERTY POSITION_INDEPENDENT_CODE ON)\ncccl_configure_target(cccl.c.parallel DIALECT 20)\n\n# Override the properties set by cccl_configure_target:\nif (CCCL_C_PARALLEL_LIBRARY_OUTPUT_DIRECTORY)\n  set_target_properties(cccl.c.parallel PROPERTIES\n    LIBRARY_OUTPUT_DIRECTORY \"${CCCL_C_PARALLEL_LIBRARY_OUTPUT_DIRECTORY}\"\n    ARCHIVE_OUTPUT_DIRECTORY \"${CCCL_C_PARALLEL_LIBRARY_OUTPUT_DIRECTORY}\"\n  )\nendif()\n\nadd_subdirectory(src/jit_templates)\n```\n\n----------------------------------------\n\nTITLE: Benchmark Function Signature for CUB Algorithm\nDESCRIPTION: Template function signature for a CUB benchmark that accepts NVBench state and type parameters, allowing benchmarking with different data types and offset types.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/tuning.rst#2025-04-23_snippet_2\n\nLANGUAGE: c++\nCODE:\n```\ntemplate <typename T, typename OffsetT>\nvoid algname(nvbench::state &state, nvbench::type_list<T, OffsetT>)\n{...}\n```\n\n----------------------------------------\n\nTITLE: Introducing <nv/target> for Target Specialization (C++)\nDESCRIPTION: libcu++ 1.5.0 (CUDA Toolkit 11.4) adds the `<nv/target>` header. This header provides library support for the `if target` target specialization mechanism, offering portability macros for NVCC/NVC++ and other compilers. This release defaults to ABI version 3.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/releases/changelog.rst#2025-04-23_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\n<nv/target>\n```\n\n----------------------------------------\n\nTITLE: Implementing multimem.ld_reduce for acquire GPU global add operations on 32-bit integers\nDESCRIPTION: Template implementation for multimem atomic load-and-reduce operations with acquire semantics, GPU scope, global memory, and add operation on 32-bit integers. Compatible with PTX ISA 81 on SM_90 architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_38\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.s32 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .add }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline int32_t multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_add_t,\n  const int32_t* addr);\n```\n\n----------------------------------------\n\nTITLE: Using Type Lists in CUB Tests with C++\nDESCRIPTION: Demonstrates how to use type lists to test CUB algorithms against multiple types.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/test_overview.rst#2025-04-23_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\nusing types = c2h::type_list<std::uint8_t, std::int32_t>;\n\nC2H_TEST(\"SCOPE FACILITY works with CONDITION\", \"[FACILITY][SCOPE]\",\n        types)\n{\n  using type = typename c2h::get<0, TestType>;\n  // ...\n}\n```\n\n----------------------------------------\n\nTITLE: Using cuda::atomic::fetch_min in a CUDA Kernel\nDESCRIPTION: Example usage of fetch_min in a CUDA kernel. It demonstrates how to atomically find the minimum value and verify the result.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/synchronization_primitives/atomic/fetch_min.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n#include <cuda/atomic>\n\n__global__ void example_kernel() {\n  cuda::atomic<int> a(1);\n  auto x = a.fetch_min(0);\n  auto y = a.load();\n  assert(x == 1 && y == 0);\n}\n```\n\n----------------------------------------\n\nTITLE: Store 64-bit Data with L1 No Allocate in CUDA\nDESCRIPTION: This function stores 64-bit data to global memory with L1 cache no allocate policy. It requires SM_70 or later architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/st.rst#2025-04-23_snippet_13\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename B64, enable_if_t<sizeof(B64) == 8, bool> = true>\n__device__ static inline void st_L1_no_allocate(\n  cuda::ptx::space_global_t,\n  B64* addr,\n  B64 src);\n```\n\n----------------------------------------\n\nTITLE: Naming Kernels for Performance Analysis with CUDASTF\nDESCRIPTION: Example of how to assign names to kernels in CUDASTF using the set_symbol method, which makes it easier to analyze performance with tools like ncu.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_55\n\nLANGUAGE: cpp\nCODE:\n```\nint A[128];\nauto lA = ctx.logical_data(A);\n\nctx.parallel_for(lA.shape(), lA.write()).set_symbol(\"updateA\")->*[] __device__ (size_t i, auto sA) {\n    A(i) = 2*i + 1;\n};\n```\n\n----------------------------------------\n\nTITLE: Implementing multimem.ld_reduce for relaxed cluster global add operations on 64-bit integers\nDESCRIPTION: Template implementation for multimem atomic load-and-reduce operations with relaxed semantics, cluster scope, global memory, and add operation on 64-bit integers. Compatible with PTX ISA 81 on SM_90 architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_42\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.u64 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .add }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline int64_t multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_add_t,\n  const int64_t* addr);\n```\n\n----------------------------------------\n\nTITLE: Thread 0 Block 1 Operations in CUDA\nDESCRIPTION: Shows operations in Thread 0 of Block 1. It creates an atomic reference to f with device scope, waits for the flag to be set, and asserts the value of x. This also leads to undefined behavior due to a data race.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/memory_model.rst#2025-04-23_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\ncuda::atomic_ref<int, cuda::thread_scope_device> flag(f);\nwhile(flag.load(memory_order_acquire) != 1); // UB: data race\nassert(x == 42);\n```\n\n----------------------------------------\n\nTITLE: Creating Async Functionality Tests with Extended Compiler Options\nDESCRIPTION: Sets up tests for asynchronous operations with additional NVIDIA compiler options for extended lambda support and relaxed constexpr evaluation. Tests cover concepts, conditionals, continuations, and various async primitives.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cudax/test/CMakeLists.txt#2025-04-23_snippet_11\n\nLANGUAGE: CMake\nCODE:\n```\n  cudax_add_catch2_test(test_target async ${cn_target}\n    async/test_concepts.cu\n    async/test_conditional.cu\n    async/test_continue_on.cu\n    async/test_just.cu\n    async/test_sequence.cu\n    async/test_visit.cu\n    async/test_when_all.cu\n  )\n  target_compile_options(${test_target} PRIVATE $<$<COMPILE_LANG_AND_ID:CUDA,NVIDIA>:--extended-lambda>)\n  target_compile_options(${test_target} PRIVATE $<$<COMPILE_LANG_AND_ID:CUDA,NVIDIA>:--expt-relaxed-constexpr>)\n```\n\n----------------------------------------\n\nTITLE: Adding <cuda/std/chrono>, <cuda/std/ratio>, <cuda/std/functional> (C++)\nDESCRIPTION: libcu++ 1.1.0 adds support for `<cuda/std/chrono>`, `<cuda/std/ratio>`, and most components of `<cuda/std/functional>`, providing CUDA-compatible versions of these C++ standard library headers.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/releases/changelog.rst#2025-04-23_snippet_28\n\nLANGUAGE: cpp\nCODE:\n```\n<cuda/std/chrono>\n```\n\nLANGUAGE: cpp\nCODE:\n```\n<cuda/std/ratio>\n```\n\nLANGUAGE: cpp\nCODE:\n```\n<cuda/std/functional>\n```\n\n----------------------------------------\n\nTITLE: Initializing Execution Place Grid with dim4 - CUDASTF C++\nDESCRIPTION: Defines an execution place grid using a std::vector of exec_place and a dim4 shape object. The grid's size must match the vector's length. This facilitates allocating computation over structured device arrangements in CUDASTF. Prerequisites: include exec_place, dim4 objects and ensure the vector size equals the dims' total size.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_25\n\nLANGUAGE: c++\nCODE:\n```\n       exec_place::grid(std::vector<exec_place> places, dim4 dims);\n```\n\n----------------------------------------\n\nTITLE: Declaring tcgen05_mma_tmem_a with Basic Parameters for SM_100a and SM_101a\nDESCRIPTION: Template function declaration for tcgen05 matrix multiply-accumulate operation with both CTA group 1 and 2 configurations, supporting all data types (f16, tf32, f8f6f4, i8). This is a simplified variant with fewer parameters for SM_100a and SM_101a architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma.rst#2025-04-23_snippet_12\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.cta_group.kind [d_tmem], [a_tmem], b_desc, idesc, enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\n// .cta_group = { .cta_group::1, .cta_group::2 }\ntemplate <cuda::ptx::dot_kind Kind, cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_mma_tmem_a(\n  cuda::ptx::kind_t<Kind> kind,\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t d_tmem,\n  uint32_t a_tmem,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Task and Data Annotation Example\nDESCRIPTION: Shows how to annotate tasks and logical data with symbols for better visualization.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_51\n\nLANGUAGE: cpp\nCODE:\n```\nauto lX = ctx.logical_data(X).set_symbol(\"X\");\nauto lY = ctx.logical_data(Y);\nlY.set_symbol(\"Y\");\n\n// Inlined notation\nctx.task(lX.read(), lY.rw()).set_symbol(\"axpy\")->*[&](cudaStream_t s, auto dX, auto dY) { axpy<<<16, 128, 0, s>>>(alpha, dX, dY); };\n\n// Explicit manipulation of the task class\nauto t = ctx.task(lX.read(), lY.rw());\nt.set_symbol(\"axpy\");\nt->*[&](cudaStream_t s, auto dX, auto dY) { axpy<<<16, 128, 0, s>>>(alpha, dX, dY); };\n```\n\n----------------------------------------\n\nTITLE: Declaring Load-Reduce Minimum Operation (relaxed/cta/global/u64) in CUDA C++\nDESCRIPTION: Device template function for atomic-like min reduction load for 64-bit unsigned integers in global memory, parameterized by PTX semaphore and scope (supporting both relaxed and acquire semantics). Accepts appropriate PTX template arguments and an address pointer. Integrates with new hardware features for efficient parallel min reductions.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_10\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.u64 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .min }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline uint64_t multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_min_t,\n  const uint64_t* addr);\n```\n\n----------------------------------------\n\nTITLE: Implementing Two-Phase Temporary Storage Allocation in CUDA C++\nDESCRIPTION: This example demonstrates the two-phase approach for allocating temporary storage in CUB device-level algorithms.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/developer_overview.rst#2025-04-23_snippet_11\n\nLANGUAGE: C++\nCODE:\n```\n// First call: Determine temporary device storage requirements\nstd::size_t temp_storage_bytes = 0;\ncub::DeviceReduce::Sum(d_temp_storage, temp_storage_bytes, d_in, d_out, num_items);\n\n// Allocate temporary storage\nvoid *d_temp_storage = nullptr;\ncudaMalloc(&d_temp_storage, temp_storage_bytes);\n\n// Second call: Perform algorithm\ncub::DeviceReduce::Sum(d_temp_storage, temp_storage_bytes, d_in, d_out, num_items);\n```\n\n----------------------------------------\n\nTITLE: Implementing multimem.ld_reduce weak AND operation for 64-bit data in CUDA\nDESCRIPTION: Template implementation for multimem.ld_reduce operation with weak memory semantics and AND reduction for 64-bit data. This template is specifically for global memory operations with weak memory ordering.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_57\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.global.op.b64 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .weak }\n// .op        = { .and }\ntemplate <typename B64, enable_if_t<sizeof(B64) == 8, bool> = true>\n__device__ static inline B64 multimem_ld_reduce(\n  cuda::ptx::sem_weak_t,\n  cuda::ptx::op_and_op_t,\n  const B64* addr);\n```\n\n----------------------------------------\n\nTITLE: Including C Standard String Manipulation Header in CUDA C++\nDESCRIPTION: Includes the `<cuda/std/cstring>` header, offering C-style array manipulation functions like `memcpy`, `memset`, and `memcmp`. Available since CCCL 3.0.0 and CUDA 13.0.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/standard_api/c_library.rst#2025-04-23_snippet_10\n\nLANGUAGE: C++\nCODE:\n```\n#include <cuda/std/cstring>\n```\n\n----------------------------------------\n\nTITLE: Specifying Thread Scope for cuda::barrier (C++)\nDESCRIPTION: The `cuda::barrier` variant (non-standard, introduced in libcu++ 1.1.0) requires an additional `cuda::thread_scope` parameter compared to `cuda::std::barrier`, specifying the scope of threads participating in the barrier.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/releases/changelog.rst#2025-04-23_snippet_29\n\nLANGUAGE: cpp\nCODE:\n```\ncuda::barrier(/* count */, /* completion */, cuda::thread_scope{/* scope */});\n```\n\nLANGUAGE: cpp\nCODE:\n```\ncuda::thread_scope\n```\n\n----------------------------------------\n\nTITLE: Defining CUDA Atomic Reference Template\nDESCRIPTION: Template definition for cuda::atomic_ref class that extends std::atomic_ref with an additional thread_scope parameter defaulting to cuda::thread_scope_system.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/synchronization_primitives/atomic_ref.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename T, cuda::thread_scope Scope = cuda::thread_scope_system>\nclass cuda::atomic_ref;\n```\n\n----------------------------------------\n\nTITLE: Referencing zip_function Class in Thrust Library (C++)\nDESCRIPTION: This snippet shows the C++ class reference to the thrust::zip_function adaptor, which allows combining multiple function objects into a single callable object. This is used as a reference in documentation for the Thrust module API.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/api_docs/function_objects/adaptors.rst#2025-04-23_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nthrust::zip_function\n```\n\n----------------------------------------\n\nTITLE: Configuring CCCL C Parallel Dependencies and Compiler Settings\nDESCRIPTION: Sets up the necessary dependencies, compiler options, and include directories for the CCCL C Parallel library. Links CUDA libraries and sets required compile definitions and options.\nSOURCE: https://github.com/nvidia/cccl/blob/main/c/parallel/CMakeLists.txt#2025-04-23_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nfind_package(CUDAToolkit REQUIRED)\n\n# TODO Use static versions of cudart, nvrtc, and nvJitLink\ntarget_link_libraries(cccl.c.parallel PRIVATE\n  CUDA::cudart\n  CUDA::nvrtc\n  CUDA::nvJitLink\n  CUDA::cuda_driver\n  cccl.compiler_interface_cpp20\n  cccl.c.parallel.jit_template\n  CUB::CUB\n  Thrust::Thrust\n)\ntarget_compile_definitions(cccl.c.parallel PUBLIC CCCL_C_EXPERIMENTAL=1 _CUB_HAS_TRANSFORM_UBLKCP=0)\ntarget_compile_definitions(cccl.c.parallel PRIVATE NVRTC_GET_TYPE_NAME=1 CUB_DISABLE_CDP=1)\ntarget_compile_options(cccl.c.parallel PRIVATE $<$<COMPILE_LANG_AND_ID:CUDA,NVIDIA>:--extended-lambda>)\n\ntarget_include_directories(cccl.c.parallel PUBLIC \"include\")\ntarget_include_directories(cccl.c.parallel PRIVATE \"src\")\n```\n\n----------------------------------------\n\nTITLE: Defining CUDA Aligned Size Template Class\nDESCRIPTION: Template class definition for cuda::aligned_size_t that represents aligned memory sizes. Includes static alignment constant, value storage, constructor, and conversion operator.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/shapes/aligned_size_t.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::std::size_t Alignment>\nstruct cuda::aligned_size_t {\n  static constexpr cuda::std::size_t align = Align;\n  cuda::std::size_t value;\n  __host__ __device__ explicit constexpr aligned_size(cuda::std::size_t size);\n  __host__ __device__ constexpr operator cuda::std::size_t();\n};\n```\n\n----------------------------------------\n\nTITLE: Adding Thrust Flexible Device System Tests for Multiple Backends\nDESCRIPTION: Sets up tests for Thrust with different device system backends (CUDA, OpenMP, TBB, and C++) to ensure compatibility across various parallel computing platforms. Each test uses the same example with a different device system configuration.\nSOURCE: https://github.com/nvidia/cccl/blob/main/examples/CMakeLists.txt#2025-04-23_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nforeach (DEVICE_SYSTEM IN ITEMS CUDA OMP TBB CPP)\n  cccl_add_compile_test(test_name\n    cccl.example\n    thrust_flexible_device_system\n    \"${DEVICE_SYSTEM}\"\n    -D \"CCCL_THRUST_DEVICE_SYSTEM=${DEVICE_SYSTEM}\"\n    ${cmake_opts}\n    ${cmake_cpm_opts}\n  )\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Copying 3D Tensor from Global to Shared Cluster Memory in CUDA\nDESCRIPTION: This function performs an asynchronous bulk copy of a 3D tensor from global memory to shared cluster memory. It uses a memory barrier for synchronization and supports multicast operations across the cluster.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_async_bulk_tensor_multicast.rst#2025-04-23_snippet_3\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline void cp_async_bulk_tensor(\n  cuda::ptx::space_cluster_t,\n  cuda::ptx::space_global_t,\n  void* dstMem,\n  const void* tensorMap,\n  const int32_t (&tensorCoords)[3],\n  uint64_t* smem_bar,\n  const uint16_t& ctaMask);\n```\n\n----------------------------------------\n\nTITLE: Creating Multi-dimensional Slices - C++\nDESCRIPTION: Demonstrates creation of 2D slices with both contiguous and non-contiguous memory layouts using make_slice function. Shows how to specify dimensions and strides.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_10\n\nLANGUAGE: cpp\nCODE:\n```\ndouble A[5 * 2];\n\n// contiguous 2D slice\nslice<double, 2> s = make_slice(A, std::tuple { 5, 2 }, 5);\n\n// non-contiguous 2D slice\nslice<double, 2> s2 = make_slice(A, std::tuple { 4, 2 }, 5);\n```\n\n----------------------------------------\n\nTITLE: Implementing multimem reduction for u64 with various semantics and scopes in CUDA\nDESCRIPTION: Template definition for multimem reduction operations on uint64_t data with add operation. Supports different memory semantics (relaxed, release) and visibility scopes (cta, cluster, gpu, sys) as parameters. Used for atomic addition operations in global memory on SM_90 architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_red.rst#2025-04-23_snippet_28\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.red.sem.scope.global.op.u64 [addr], val; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .release }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .add }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline void multimem_red(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_add_t,\n  uint64_t* addr,\n  uint64_t val);\n```\n\n----------------------------------------\n\nTITLE: CUDA Grid-Strided Update Kernel\nDESCRIPTION: Implementation of a grid-strided kernel that updates array values using coefficients.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/memory_access_properties/apply_access_property.rst#2025-04-23_snippet_2\n\nLANGUAGE: cuda\nCODE:\n```\n__global__ void update(int* const x, int const* const a, int const* const b, size_t N) {\n    auto g = cooperative_groups::this_grid();\n    for (int idx = g.thread_rank(); idx < N; idx += g.size()) {\n        x[idx] = a[idx] * x[idx] + b[idx];\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing multimem reduction operations for integer addition in CUDA PTX\nDESCRIPTION: Template function declaration for multimem reduction operations performing atomic addition on 64-bit integers in global memory. Supports different semantic models and memory scopes as specified in PTX ISA 81 for SM_90.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_red.rst#2025-04-23_snippet_31\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.red.sem.scope.global.op.u64 [addr], val; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .release }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .add }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline void multimem_red(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_add_t,\n  int64_t* addr,\n  int64_t val);\n```\n\n----------------------------------------\n\nTITLE: Creating CUDA Range Property with Halos\nDESCRIPTION: This CUDA kernel demonstrates how to create a range property with halo regions. It applies different properties to the halo elements and internal elements, showcasing the calculation of leading_bytes and total_bytes for correct property application.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/memory_access_properties/access_property.rst#2025-04-23_snippet_6\n\nLANGUAGE: cuda\nCODE:\n```\n__global__ void range_with_halos(int* g, size_t n, size_t halos) {\n    // In the range [g, g + n), the first and last \"halos\" elements of `int` type are halos.\n    // This example applies one property to the halo elements, and another property to the internal elements:\n    // - halos: streaming  (secondary property)\n    // - internal: persisting (primary property)\n\n    auto internal_property = cuda::access_property::persisting{};\n    auto halo_property = cuda::access_property::streaming{};\n\n    // For the range property, the pointer used to build the property\n    // must satisfy p = g + halos\n    int* p = g + halos;\n    // Then, \"total_elements\" (total_size * sizeof(int)) must satisfy:\n    // g + n = p + total_elements\n    int total_bytes = (g + n - p) * sizeof(int);\n    // Finally, \"leading_elements\" (leading_bytes * sizeof(int)) must satisfy:\n    // g = p + leading_elements - total_elements\n    int leading_bytes = (g - p) * sizeof(int) + total_bytes;\n\n    // Is a property that we can use for halo exchange:\n    auto range_property = cuda::access_property(p, leading_bytes, total_bytes, internal_property, halo_property);\n}\n```\n\n----------------------------------------\n\nTITLE: CTA Group 2 Barrier Commit Operation\nDESCRIPTION: Template device function for committing to a shared memory barrier in CTA group 2. Operates on 64-bit barriers and requires PTX ISA 86 with SM_100a or SM_101a architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_commit.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.commit.cta_group.mbarrier::arrive::one.shared::cluster.b64 [smem_bar]; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1, .cta_group::2 }\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_commit(\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint64_t* smem_bar);\n```\n\n----------------------------------------\n\nTITLE: Creating Nested Dot Sections for Structured Graph Visualization\nDESCRIPTION: Example of how to create nested dot sections to better structure task graphs in CUDASTF. This allows for more condensed and organized visualization of complex workflows.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_54\n\nLANGUAGE: c++\nCODE:\n```\ncontext ctx;\nauto lA = ctx.token().set_symbol(\"A\");\nauto lB = ctx.token().set_symbol(\"B\");\nauto lC = ctx.token().set_symbol(\"C\");\n\n// Begin a top-level section named \"foo\"\nauto s_foo = ctx.dot_section(\"foo\");\nfor (size_t i = 0; i < 2; i++)\n{\n  // Section named \"bar\" using RAII\n  auto s_bar = ctx.dot_section(\"bar\");\n  ctx.task(lA.read(), lB.rw()).set_symbol(\"t1\")->*[](cudaStream_t, auto, auto) {};\n  for (size_t j = 0; j < 2; j++) {\n     // Section named \"baz\" using RAII\n     auto s_bar = ctx.dot_section(\"baz\");\n     ctx.task(lA.read(), lC.rw()).set_symbol(\"t2\")->*[](cudaStream_t, auto, auto) {};\n     ctx.task(lB.read(), lC.read(), lA.rw()).set_symbol(\"t3\")->*[](cudaStream_t, auto, auto, auto) {};\n     // Implicit end of section \"baz\"\n  }\n  // Implicit end of section \"bar\"\n}\ns_foo.end(); // Explicit end of section \"foo\"\nctx.finalize();\n```\n\n----------------------------------------\n\nTITLE: Applying CUDA Standard Library Concepts in C++ Templates - C++\nDESCRIPTION: Demonstrates three template function approaches utilizing CUDA standard concepts to restrict input types to integral values for C++20, C++17, and C++14, respectively. Requires the CUDA C++ Standard Concepts Library (libcudacxx) and dependent CUDA headers. The parameter Integer represents the type to be checked, leveraging C++ standard concepts in C++20 and SFINAE with enable_if_t in earlier versions, with output matching only when Integer satisfies the integral constraint.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/standard_api/concepts_library.rst#2025-04-23_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\ntemplate<cuda::std::integral Integer>\nvoid do_something_with_integers_in_cpp20(Integer&& i) {...}\n\ntemplate<class Integer, cuda::std::enable_if_t<cuda::std::integral<Integer>, int> = 0>\nvoid do_something_with_integers_in_cpp17(Integer&& i) {...}\n\ntemplate<class Integer, cuda::std::enable_if_t<cuda::std::integral<Integer>, int> = 0>\nvoid do_something_with_integers_in_cpp14(Integer&& i) {...}\n```\n\n----------------------------------------\n\nTITLE: Implementing Warp Reduce with Shuffle Instructions in CUDA C++\nDESCRIPTION: This code defines a structure for warp-level reduction using shuffle instructions, with specializations for different CUDA architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/developer_overview.rst#2025-04-23_snippet_7\n\nLANGUAGE: C++\nCODE:\n```\ntemplate <typename T, int LOGICAL_WARP_THREADS>\nstruct WarpReduceShfl\n{\n\n\ntemplate <typename ReductionOp>\n__device__ __forceinline__ T ReduceImpl(T input, int valid_items,\n                                        ReductionOp reduction_op)\n{\n  // ... base case (SM < 80) ...\n}\n\ntemplate <class U = T>\n__device__ __forceinline__\n  typename std::enable_if<std::is_same_v<int, U> ||\n                          std::is_same_v<unsigned int, U>,\n                          T>::type\n    ReduceImpl(T input,\n              int,               // valid_items\n              ::cuda::std::plus<>) // reduction_op\n{\n  T output = input;\n\n  NV_IF_TARGET(NV_PROVIDES_SM_80,\n              (output = __reduce_add_sync(member_mask, input);),\n              (output = ReduceImpl<::cuda::std::plus<>>(\n                    input, LOGICAL_WARP_THREADS, ::cuda::std::plus<>{});));\n\n  return output;\n}\n\n\n};\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Bulk MAX Reduction (Unsigned 32-bit) in CUDA\nDESCRIPTION: Performs an asynchronous bulk MAX reduction operation from CTA-shared to cluster-shared memory for unsigned 32-bit integers. Uses a memory barrier for synchronization.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_reduce_async_bulk.rst#2025-04-23_snippet_4\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline void cp_reduce_async_bulk(\n  cuda::ptx::space_cluster_t,\n  cuda::ptx::space_shared_t,\n  cuda::ptx::op_max_t,\n  uint32_t* dstMem,\n  const uint32_t* srcMem,\n  uint32_t size,\n  uint64_t* rdsmem_bar);\n```\n\n----------------------------------------\n\nTITLE: Including C Standard Float Limits Header in CUDA C++\nDESCRIPTION: Includes the `<cuda/std/cfloat>` header to access limits of floating-point types, corresponding to the standard `<cfloat>`. Available since CCCL 2.0.0 and CUDA Toolkit 10.2.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/standard_api/c_library.rst#2025-04-23_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\n#include <cuda/std/cfloat>\n```\n\n----------------------------------------\n\nTITLE: Adding DeviceSpmv and NonTrivialRuns in CUB 1.4.0\nDESCRIPTION: Introduction of cub::DeviceSpmv for sparse matrix-vector multiplication and cub::DeviceRunLength::NonTrivialRuns for finding non-trivial runs in data.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/releases/changelog.rst#2025-04-23_snippet_10\n\nLANGUAGE: C++\nCODE:\n```\ncub::DeviceSpmv\ncub::DeviceRunLengthEncode::NonTrivialRuns\n```\n\n----------------------------------------\n\nTITLE: Implementing multimem.ld_reduce for relaxed cluster global add operations on 32-bit integers\nDESCRIPTION: Template implementation for multimem atomic load-and-reduce operations with relaxed semantics, cluster scope, global memory, and add operation on 32-bit integers. Compatible with PTX ISA 81 on SM_90 architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_33\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.s32 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .add }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline int32_t multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_add_t,\n  const int32_t* addr);\n```\n\n----------------------------------------\n\nTITLE: Performing Asynchronous Bulk Reduction (MIN, u64) from Shared to Global Memory in CUDA\nDESCRIPTION: This CUDA C++ device function `cp_reduce_async_bulk` performs an asynchronous bulk minimum reduction for unsigned 64-bit integers. It reduces data from a source array in shared memory (`srcMem`) to a destination array in global memory (`dstMem`). This overload requires PTX ISA 80 and SM_90 capability.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_reduce_async_bulk.rst#2025-04-23_snippet_24\n\nLANGUAGE: cuda\nCODE:\n```\n   // cp.reduce.async.bulk.dst.src.bulk_group.op.type  [dstMem], [srcMem], size; // 4. PTX ISA 80, SM_90\n   // .dst       = { .global }\n   // .src       = { .shared::cta }\n   // .type      = { .u64 }\n   // .op        = { .min }\n   template <typename = void>\n   __device__ static inline void cp_reduce_async_bulk(\n     cuda::ptx::space_global_t,\n     cuda::ptx::space_shared_t,\n     cuda::ptx::op_min_t,\n     uint64_t* dstMem,\n     const uint64_t* srcMem,\n     uint32_t size);\n```\n\n----------------------------------------\n\nTITLE: Synchronizing CUDA Streams with Task Fences in C++\nDESCRIPTION: Demonstrates how to obtain a CUDA stream for the current task fence from a CUDASTF context and synchronize execution using cudaStreamSynchronize. Requires a CUDA-enabled build and working CUDA runtime. The variable 'ctx' represents an initialized CUDASTF context; the operation ensures that all previously submitted asynchronous operations are completed before proceeding.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_18\n\nLANGUAGE: cpp\nCODE:\n```\ncudaStream_t stream = ctx.task_fence();\ncudaStreamSynchronize(stream);\n```\n\n----------------------------------------\n\nTITLE: Implementing Add Reduction for 32-bit Floats in CUDA\nDESCRIPTION: This function performs an asynchronous bulk addition reduction operation on 32-bit floating-point numbers from shared memory to global memory. It uses the cp.reduce.async.bulk instruction from PTX ISA 80 for SM_90 architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_reduce_async_bulk.rst#2025-04-23_snippet_29\n\nLANGUAGE: cuda\nCODE:\n```\n// cp.reduce.async.bulk.dst.src.bulk_group.op.type  [dstMem], [srcMem], size; // 4. PTX ISA 80, SM_90\n// .dst       = { .global }\n// .src       = { .shared::cta }\n// .type      = { .f32 }\n// .op        = { .add }\ntemplate <typename = void>\n__device__ static inline void cp_reduce_async_bulk(\n  cuda::ptx::space_global_t,\n  cuda::ptx::space_shared_t,\n  cuda::ptx::op_add_t,\n  float* dstMem,\n  const float* srcMem,\n  uint32_t size);\n```\n\n----------------------------------------\n\nTITLE: Using Tensor Core Operation for CTA Group 1 in CUDA\nDESCRIPTION: This function uses a tensor core operation for CTA group 1 with various data types. It includes parameters for matrix descriptors, d_tmem, and a zero column mask descriptor.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_18\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.ws.cta_group.kind.collector::b1::use [d_tmem], a_desc, b_desc, idesc, enable_input_d, zero_column_mask_desc; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1 }\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_collector_b1_use(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d,\n  uint64_t zero_column_mask_desc);\n```\n\n----------------------------------------\n\nTITLE: Defining TCGEN05 MMA Workspace Collector B2 Fill Function Template with Zero Column Mask\nDESCRIPTION: Template function for TCGen05 MMA workspace operations with collector B2 fill mode. Takes d_tmem, a_desc, b_desc, idesc, enable_input_d flag, and zero_column_mask_desc as parameters. Supports multiple data types via the Kind template parameter.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_32\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.ws.cta_group.kind.collector::b2::fill [d_tmem], a_desc, b_desc, idesc, enable_input_d, zero_column_mask_desc; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1 }\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_collector_b2_fill(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d,\n  uint64_t zero_column_mask_desc);\n```\n\n----------------------------------------\n\nTITLE: Implementing Dispatch Layer for Device-Level Algorithms in CUDA C++\nDESCRIPTION: This code outlines the high-level control flow of the dispatch layer for device-level algorithms in CUB, including policy selection and kernel launching.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/developer_overview.rst#2025-04-23_snippet_12\n\nLANGUAGE: C++\nCODE:\n```\n// Device-scope API\ncudaError_t cub::DeviceAlgorithm::Algorithm(d_temp_storage, temp_storage_bytes, ...) {\n  return DispatchAlgorithm::Dispatch(d_temp_storage, temp_storage_bytes, ...); // calls (1)\n}\n\n// Dispatch entry point\nstatic cudaError_t DispatchAlgorithm::Dispatch(...) { // (1)\n  DispatchAlgorithm closure{...};\n  // MaxPolicy - tail of linked list containing architecture-specific tunings\n  return MaxPolicy::Invoke(get_device_ptx_version(), closure); // calls (2)\n}\n\n// Chained policy - linked list of tunings\ntemplate <int PolicyPtxVersion, typename Policy, typename PrevPolicy>\nstruct ChainedPolicy {\n  using ActivePolicy = conditional_t<CUB_PTX_ARCH < PolicyPtxVersion, // (5)\n                                    typename PrevPolicy::ActivePolicy, Policy>;\n\n  static cudaError_t Invoke(int device_ptx_version, auto dispatch_closure) { // (2)\n    if (device_ptx_version < PolicyPtxVersion) {\n      PrevPolicy::Invoke(device_ptx_version, dispatch_closure); // calls (2) of next policy\n    }\n    dispatch_closure.Invoke<Policy>(); // eventually calls (3)\n  }\n};\n\n// Dispatch object - a closure over all algorithm parameters\ntemplate <typename Policy>\ncudaError_t DispatchAlgorithm::Invoke() { // (3)\n    // host-side implementation of algorithm, calls kernels\n    kernel<MaxPolicy><<<grid_size, Policy::AlgorithmPolicy::BLOCK_THREADS>>>(...); // calls (4)\n}\n\ntemplate <typename ChainedPolicy>\n__launch_bounds__(ChainedPolicy::ActivePolicy::AlgorithmPolicy::BLOCK_THREADS) CUB_DETAIL_KERNEL_ATTRIBUTES\nvoid kernel(...) { // (4)\n  using policy = ChainedPolicy::ActivePolicy; // selects policy of active device compilation pass (5)\n  using agent = AgentAlgorithm<policy>; // instantiates (6)\n  agent a{...};\n  a.Process(); // calls (7)\n}\n\ntemplate <typename Policy>\nstruct AlgorithmAgent {  // (6)\n  void Process() { ... } // (7)\n};\n```\n\n----------------------------------------\n\nTITLE: Including Dependencies and Checking Prerequisites in CMake\nDESCRIPTION: Includes the CCCL benchmark registry CMake module and checks if CUB support is enabled, issuing a fatal error if not, as Thrust benchmarks depend on it. It also calls `cccl_get_nvbench` to ensure NVBench is available.\nSOURCE: https://github.com/nvidia/cccl/blob/main/thrust/benchmarks/CMakeLists.txt#2025-04-23_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ninclude(${CMAKE_SOURCE_DIR}/benchmarks/cmake/CCCLBenchmarkRegistry.cmake)\n\nif(NOT CCCL_ENABLE_CUB)\n  message(FATAL_ERROR \"Thrust benchmarks depend on CUB: set CCCL_ENABLE_CUB.\")\nendif()\n\ncccl_get_nvbench()\n\nset(benches_root \"${CMAKE_CURRENT_LIST_DIR}\")\n```\n\n----------------------------------------\n\nTITLE: Store 32-bit Data with L1 No Allocate in CUDA\nDESCRIPTION: This function stores 32-bit data to global memory with L1 cache no allocate policy. It requires SM_70 or later architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/st.rst#2025-04-23_snippet_12\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline void st_L1_no_allocate(\n  cuda::ptx::space_global_t,\n  B32* addr,\n  B32 src);\n```\n\n----------------------------------------\n\nTITLE: Conditional Async Memory Allocation\nDESCRIPTION: Template function demonstrating how to conditionally use async allocation based on resource capabilities.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/memory_resource/resource.rst#2025-04-23_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\ntemplate<class MemoryResource>\n    requires cuda::mr::resource<MemoryResource>\nvoid* maybe_allocate_async(MemoryResource& resource, std::size_t size, std::size_t align, cuda::stream_ref stream) {\n    if constexpr(cuda::mr::async_resource<MemoryResource>) {\n        return resource.allocate_async(size, align, stream);\n    } else {\n        return resource.allocate(size, align);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Using cuda::isqrt function in a CUDA kernel\nDESCRIPTION: Example demonstrating the usage of cuda::isqrt in a CUDA kernel with various integer inputs. The example includes assertions to verify the correct computation of integer square roots.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/math/isqrt.rst#2025-04-23_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n#include <cuda/cmath>\n#include <cuda/std/cassert>\n\n__global__ void isqrt_kernel() {\n    assert(cuda::isqrt(1) == 1);\n    assert(cuda::isqrt(4) == 2);\n    assert(cuda::isqrt(42) == 6);\n    assert(cuda::isqrt(99) == 9);\n    assert(cuda::isqrt(100) == 10);\n}\n\nint main() {\n    isqrt_kernel<<<1, 1>>>();\n    cudaDeviceSynchronize();\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Block-Scaled MMA for mxf8f6f4 with 1X Vector Scaling in CUDA\nDESCRIPTION: This template function implements a block-scaled matrix multiply-accumulate operation for mxf8f6f4 data type with 1X vector scaling. It supports different CTA group configurations.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma.rst#2025-04-23_snippet_14\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.cta_group.kind.block_scale.scale_vec::1X [d_tmem], a_desc, b_desc, idesc, [scale_A_tmem], [scale_B_tmem], enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .kind      = { .kind::mxf8f6f4 }\n// .cta_group = { .cta_group::1, .cta_group::2 }\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_mma_block_scale_vec_1x(\n  cuda::ptx::kind_mxf8f6f4_t,\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  uint32_t scale_A_tmem,\n  uint32_t scale_B_tmem,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Implementing Add Reduction for 64-bit Unsigned Integers in CUDA\nDESCRIPTION: This function performs an asynchronous bulk addition reduction operation on 64-bit unsigned integers from shared memory to global memory. It uses the cp.reduce.async.bulk instruction from PTX ISA 80 for SM_90 architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_reduce_async_bulk.rst#2025-04-23_snippet_31\n\nLANGUAGE: cuda\nCODE:\n```\n// cp.reduce.async.bulk.dst.src.bulk_group.op.u64  [dstMem], [srcMem], size; // 6. PTX ISA 80, SM_90\n// .dst       = { .global }\n// .src       = { .shared::cta }\n// .type      = { .s64 }\n// .op        = { .add }\ntemplate <typename = void>\n__device__ static inline void cp_reduce_async_bulk(\n  cuda::ptx::space_global_t,\n  cuda::ptx::space_shared_t,\n  cuda::ptx::op_add_t,\n  int64_t* dstMem,\n  const int64_t* srcMem,\n  uint32_t size);\n```\n\n----------------------------------------\n\nTITLE: Defining CUDA Device Function Template tcgen05_mma_ws_tmem_a_collector_b0_lastuse\nDESCRIPTION: Defines a templated CUDA `__device__` inline function `tcgen05_mma_ws_tmem_a_collector_b0_lastuse` for tensor core MMA operations (lastuse stage) within a CTA group 1. It takes shared memory pointers/descriptors for D and A (`d_tmem`, `a_tmem`), a descriptor for B (`b_desc`), an index descriptor (`idesc`), an enable flag (`enable_input_d`), and a zero column mask descriptor (`zero_column_mask_desc`). The template parameter `Kind` specifies the data type (f16, tf32, f8f6f4, i8). Requires PTX ISA 86 and targets SM_100a/SM_101a architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_11\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.ws.cta_group.kind.collector::b0::lastuse [d_tmem], [a_tmem], b_desc, idesc, enable_input_d, zero_column_mask_desc; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1 }\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_tmem_a_collector_b0_lastuse(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint32_t a_tmem,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d,\n  uint64_t zero_column_mask_desc);\n```\n\n----------------------------------------\n\nTITLE: Sequential Execution Policy\nDESCRIPTION: Shows the usage of thrust::seq execution policy for sequential algorithm execution\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/releases/changelog.rst#2025-04-23_snippet_8\n\nLANGUAGE: C++\nCODE:\n```\nthrust::seq\n```\n\n----------------------------------------\n\nTITLE: Multimem Load Reduce Max U64 Operations\nDESCRIPTION: Template implementation for 64-bit unsigned integer max reduction operations with different memory semantics and scopes. Supports PTX ISA 81 and requires SM_90.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_24\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline uint64_t multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_max_t,\n  const uint64_t* addr);\n```\n\n----------------------------------------\n\nTITLE: Defining cuda::access_property Class in CUDA\nDESCRIPTION: This snippet defines the cuda::access_property class, which includes various static and dynamic memory property types, constructors, and operators for controlling memory access in CUDA programs.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/memory_access_properties/access_property.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\nnamespace cuda {\nclass access_property {\n  public:\n    // Static memory space property:\n    struct shared {};\n    struct global {};\n\n    // Static global memory residence control property:\n    struct normal {\n        __host__ __device__ constexpr operator cudaAccessProperty() const noexcept;\n    };\n    struct persisting {\n        __host__ __device__ constexpr operator cudaAccessProperty() const noexcept;\n    };\n    struct streaming {\n        __host__ __device__ constexpr operator cudaAccessProperty() const noexcept;\n    };\n\n    // Default constructor:\n    __host__ __device__ constexpr access_property() noexcept;\n\n    // Copy constructor:\n    constexpr access_property(access_property const&) noexcept = default;\n\n    // Copy assignment:\n    access_property& operator=(const access_property& other) noexcept = default;\n\n    // Constructors from static global memory residence control properties:\n    __host__ __device__ constexpr access_property(global)     noexcept;\n    __host__ __device__ constexpr access_property(normal)     noexcept;\n    __host__ __device__ constexpr access_property(streaming)  noexcept;\n    __host__ __device__ constexpr access_property(persisting) noexcept;\n\n    // Dynamic interleaved global memory residence control property constructors:\n    __host__ __device__ constexpr access_property(normal,     float probability);\n    __host__ __device__ constexpr access_property(streaming,  float probability);\n    __host__ __device__ constexpr access_property(persisting, float probability);\n    __host__ __device__ constexpr access_property(normal,     float probability, streaming);\n    __host__ __device__ constexpr access_property(persisting, float probability, streaming);\n\n    // Dynamic range global memory residence control property constructors:\n    __host__ __device__ constexpr access_property(void* ptr, size_t partition_bytes, size_t total_bytes, normal);\n    __host__ __device__ constexpr access_property(void* ptr, size_t partition_bytes, size_t total_bytes, streaming);\n    __host__ __device__ constexpr access_property(void* ptr, size_t partition_bytes, size_t total_bytes, persisting);\n    __host__ __device__ constexpr access_property(void* ptr, size_t partition_bytes, size_t total_bytes, normal,     streaming);\n    __host__ __device__ constexpr access_property(void* ptr, size_t partition_bytes, size_t total_bytes, persisting, streaming);\n};\n} // namespace cuda\n```\n\n----------------------------------------\n\nTITLE: Implementing multimem.ld_reduce for relaxed CTA global add operations on 64-bit integers\nDESCRIPTION: Template implementation for multimem atomic load-and-reduce operations with relaxed semantics, CTA scope, global memory, and add operation on 64-bit integers. Compatible with PTX ISA 81 on SM_90 architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_41\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.u64 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .add }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline int64_t multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_add_t,\n  const int64_t* addr);\n```\n\n----------------------------------------\n\nTITLE: Declaring MMA Block Scale Vector 2X Collector for MXF4/MXF4NVF4 Kinds\nDESCRIPTION: Template function for matrix multiply with 2X vector scaling that works with both MXF4 and MXF4NVF4 data types. It supports different CTA groups and includes descriptor parameters for matrices and scaling vectors.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma.rst#2025-04-23_snippet_35\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.cta_group.kind.block_scale.scale_vec::2X.collector::a::use [d_tmem], a_desc, b_desc, idesc, [scale_A_tmem], [scale_B_tmem], enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .kind      = { .kind::mxf4, .kind::mxf4nvf4 }\n// .cta_group = { .cta_group::1, .cta_group::2 }\ntemplate <cuda::ptx::dot_kind Kind, cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_mma_block_scale_vec_2x_collector_a_use(\n  cuda::ptx::kind_t<Kind> kind,\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  uint32_t scale_A_tmem,\n  uint32_t scale_B_tmem,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Bulk Reduction Min Operation for bfloat16 in CUDA\nDESCRIPTION: This function performs an asynchronous bulk reduction minimum operation on bfloat16 data, transferring from shared memory to global memory. It uses CUDA PTX ISA 80 for SM_90 architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_reduce_async_bulk_bf16.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline void cp_reduce_async_bulk(\n  cuda::ptx::space_global_t,\n  cuda::ptx::space_shared_t,\n  cuda::ptx::op_min_t,\n  __nv_bfloat16* dstMem,\n  const __nv_bfloat16* srcMem,\n  uint32_t size);\n```\n\n----------------------------------------\n\nTITLE: Handling CUB as a Subdirectory\nDESCRIPTION: Provides support for adding CUB to a parent project via add_subdirectory. If CUB is not explicitly enabled, it includes a special configuration file and returns early.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cub/CMakeLists.txt#2025-04-23_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\n# Support adding CUB to a parent project via add_subdirectory.\n# See examples/cmake/add_subdir/CMakeLists.txt for details.\nif (NOT CCCL_ENABLE_CUB)\n  include(cmake/CubAddSubdir.cmake)\n  return()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Assigning Graph Context to Generic Context in CUDASTF\nDESCRIPTION: Shows how to assign a graph_ctx to a generic context in CUDASTF, enabling the use of CUDA graphs.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\n// assigns a graph_ctx() to a generic context\ncontext ctx = graph_ctx();\n```\n\n----------------------------------------\n\nTITLE: Defining Block-Level Reduce Class in CUDA C++\nDESCRIPTION: This code defines a class template for block-level reduction operations in CUDA, including constructors for different shared memory allocation strategies.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/developer_overview.rst#2025-04-23_snippet_8\n\nLANGUAGE: C++\nCODE:\n```\ntemplate <typename T,\n          int BLOCK_DIM_X,\n          BlockReduceAlgorithm ALGORITHM = BLOCK_REDUCE_WARP_REDUCTIONS,\n          int BLOCK_DIM_Y = 1,\n          int BLOCK_DIM_Z = 1>\nclass BlockReduce {\npublic:\n  struct TempStorage : Uninitialized<_TempStorage> {};\n\n  // (1) new constructor\n  __device__ __forceinline__ BlockReduce()\n      : temp_storage(PrivateStorage()),\n        linear_tid(RowMajorTid(BLOCK_DIM_X, BLOCK_DIM_Y, BLOCK_DIM_Z)) {}\n\n  __device__ __forceinline__ BlockReduce(TempStorage &temp_storage)\n      : temp_storage(temp_storage.Alias()),\n        linear_tid(RowMajorTid(BLOCK_DIM_X, BLOCK_DIM_Y, BLOCK_DIM_Z)) {}\n};\n```\n\n----------------------------------------\n\nTITLE: Maximum Operation for Unsigned 32-bit Integer\nDESCRIPTION: Template function implementing asynchronous maximum reduction operation on cluster memory barrier for u32 type. Takes destination pointer, value and remote barrier pointer as parameters.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/red_async.rst#2025-04-23_snippet_3\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline void red_async(\n  cuda::ptx::op_max_t,\n  uint32_t* dest,\n  const uint32_t& value,\n  uint64_t* remote_bar);\n```\n\n----------------------------------------\n\nTITLE: Verifying Runtime Equivalence of Policy Dispatch Methods\nDESCRIPTION: This text snippet shows the runtime output of the compiled C++ example program (`./policies`). It first prints the detected runtime compute capability (e.g., 890 for sm_89). Then, it shows the output from both `bad_kernel` and `good_kernel`. Both kernels correctly select the `sm80` policy (the nearest lower-or-equal policy) and report the same launch bounds and block threads (768), confirming that using `MaxPolicy` for instantiation achieves the same runtime result as using `ActivePolicy`, despite the compile-time differences.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/developer_overview.rst#2025-04-23_snippet_18\n\nLANGUAGE: text\nCODE:\n```\nruntime cc 890\nlaunch bounds 768; block threads 768\nlaunch bounds 768; block threads 768\n```\n\n----------------------------------------\n\nTITLE: MMA Block Scale Vector 1X Collector Use with mxf8f6f4 Type (CTA Group 1)\nDESCRIPTION: Template function for matrix multiplication with block scaling using vector 1X with collector A use operation for CTA group 1. The function uses mxf8f6f4 data type and requires PTX ISA 86 with SM_100a or SM_101a architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma.rst#2025-04-23_snippet_32\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.cta_group.kind.block_scale.scale_vec::1X.collector::a::use [d_tmem], a_desc, b_desc, idesc, [scale_A_tmem], [scale_B_tmem], enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .kind      = { .kind::mxf8f6f4 }\n// .cta_group = { .cta_group::1, .cta_group::2 }\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_mma_block_scale_vec_1x_collector_a_use(\n  cuda::ptx::kind_mxf8f6f4_t,\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  uint32_t scale_A_tmem,\n  uint32_t scale_B_tmem,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Implementing warp_shuffle_up in CUDA\nDESCRIPTION: Function templates for warp_shuffle_up operation with optional Width and lane_mask parameters. Shuffles data up within a warp by a specified delta.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/warp/warp_shuffle.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <int Width = 32, typename T>\n[[nodiscard]] __device__ warp_shuffle_result<T>\nwarp_shuffle_up(const T& data,\n                int      delta,\n                uint32_t lane_mask = 0xFFFFFFFF,\n                cuda::std::integral_constant<int, Width> = {})\n\ntemplate <int Width = 32, typename T>\n[[nodiscard]] __device__ warp_shuffle_result<T>\nwarp_shuffle_up(const T& data,\n                int      delta,\n                cuda::std::integral_constant<int, Width>) // lane_mask is 0xFFFFFFFF\n```\n\n----------------------------------------\n\nTITLE: Implementing MMA Workspace Last Use Operations with F8F6F4 Data Type (No Masking) in CUDA\nDESCRIPTION: Template function for the final tensor compute operation (lastuse) with F8F6F4 data type. This simplified variant handles tensor descriptors without zero column masking. Targets PTX ISA 86 on SM_100a and SM_101a architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_49\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.ws.cta_group.kind.collector::b2::lastuse [d_tmem], a_desc, b_desc, idesc, enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1 }\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_collector_b2_lastuse(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Compiling a CUDA Project with CCCL (Bash)\nDESCRIPTION: This command demonstrates how to compile a CUDA project using NVCC with CCCL headers. It shows the include paths for Thrust, CUB, and libcudacxx.\nSOURCE: https://github.com/nvidia/cccl/blob/main/README.md#2025-04-23_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\nnvcc -Icccl/thrust -Icccl/libcudacxx/include -Icccl/cub main.cu -o main\n```\n\n----------------------------------------\n\nTITLE: Initializing MMA Collector with Zero Column Mask (CUDA)\nDESCRIPTION: This function initializes an MMA collector for various data types, including a zero column mask. It supports f16, tf32, f8f6f4, and i8 data types, and uses CTA group 1.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_22\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_collector_b1_lastuse(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d,\n  uint64_t zero_column_mask_desc);\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake Project with Thrust Flexible Device System\nDESCRIPTION: Configures a CMake project that demonstrates Thrust with different device systems (CUDA, OpenMP, TBB, or CPP). Uses CPM to fetch CCCL from GitHub and configures the project based on the selected device system.\nSOURCE: https://github.com/nvidia/cccl/blob/main/examples/thrust_flexible_device_system/CMakeLists.txt#2025-04-23_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.15 FATAL_ERROR)\n\nproject(ThrustFlexibleDeviceSystemDemo CXX)\n\n# This example uses the CMake Package Manager (CPM) to simplify fetching CCCL from GitHub\n# For more information, see https://github.com/cpm-cmake/CPM.cmake\ninclude(cmake/CPM.cmake)\n\n# We define these as variables so they can be overridden in CI to pull from a PR instead of CCCL `main`\n# In your project, these variables are unnecessary and you can just use the values directly\nset(CCCL_REPOSITORY \"https://github.com/NVIDIA/cccl\" CACHE STRING \"GitHub repository to fetch CCCL from\")\nset(CCCL_TAG \"main\" CACHE STRING \"Git tag/branch to fetch from CCCL repository\")\n\n# This will automatically clone CCCL from GitHub and make the exported cmake targets available.\n# The default `CCCL::Thrust` target will be configured to use the system device defined by\n# `CCCL_THRUST_DEVICE_SYSTEM`.\nCPMAddPackage(\n    NAME CCCL\n    GIT_REPOSITORY \"${CCCL_REPOSITORY}\"\n    GIT_TAG ${CCCL_TAG}\n)\n```\n\n----------------------------------------\n\nTITLE: Global Memory Store with L1 Unchanged Eviction\nDESCRIPTION: Template functions for global memory stores with L1 cache unchanged eviction policy. These operations require SM_70+ and maintain cache state during eviction.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/st.rst#2025-04-23_snippet_3\n\nLANGUAGE: cuda\nCODE:\n```\n// st.space.L1::evict_unchanged.b8 [addr], src; // PTX ISA 74, SM_70\ntemplate <typename B8, enable_if_t<sizeof(B8) == 1, bool> = true>\n__device__ static inline void st_L1_evict_unchanged(\n  cuda::ptx::space_global_t,\n  B8* addr,\n  B8 src);\n```\n\n----------------------------------------\n\nTITLE: CUDA round_down Example Usage\nDESCRIPTION: Demonstration of using round_down function in a CUDA kernel. Shows rounding down value 7 to the largest multiple of 3, which results in 6.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/math/round_down.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n#include <cuda/cmath>\n#include <cstdio>\n\n__global__ void round_up_kernel() {\n    int      value    = 7;\n    unsigned multiple = 3;\n    printf(\"%d\\n\", cuda::round_down(value, multiple)); // print \"6\"\n}\n\nint main() {\n    round_up_kernel<<<1, 1>>>();\n    cudaDeviceSynchronize();\n    return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Thrust Test Framework Targets - CMake\nDESCRIPTION: This snippet automates the process of creating and configuring static library targets for each entry in THRUST_TARGETS. It retrieves properties for each Thrust target, conditionally wraps CUDA source files for non-CUDA builds, and adds the static library using CMake functions. It sets up dependencies, include directories, clones properties, and applies special fixes for CUDA/Clang/NVCC interoperability. When targeting CUDA, it applies additional CUDA configuration. Prerequisites: a list variable THRUST_TARGETS, relevant CMake helper functions (e.g., thrust_get_target_property), and the Thrust source directory. Expects Thrust targets and corresponding device configurations; outputs properly set up CMake targets ready for testing on various backends.\nSOURCE: https://github.com/nvidia/cccl/blob/main/thrust/testing/unittest/CMakeLists.txt#2025-04-23_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nforeach(thrust_target IN LISTS THRUST_TARGETS)\n  thrust_get_target_property(config_device ${thrust_target} DEVICE)\n  thrust_get_target_property(config_prefix ${thrust_target} PREFIX)\n\n  set(framework_target ${config_prefix}.test.framework)\n\n  if (\"CUDA\" STREQUAL \"${config_device}\")\n    set(framework_srcs\n      testframework.cu\n      cuda/testframework.cu\n    )\n  else()\n    # Wrap the cu file inside a .cpp file for non-CUDA builds\n    thrust_wrap_cu_in_cpp(framework_srcs testframework.cu ${thrust_target})\n  endif()\n\n  add_library(${framework_target} STATIC ${framework_srcs})\n  target_link_libraries(${framework_target} PUBLIC ${thrust_target})\n  target_include_directories(${framework_target} PRIVATE \"${Thrust_SOURCE_DIR}/testing\")\n  thrust_clone_target_properties(${framework_target} ${thrust_target})\n  thrust_fix_clang_nvcc_build_for(${framework_target})\n  if (\"CUDA\" STREQUAL \"${config_device}\")\n    thrust_configure_cuda_target(${framework_target} RDC ${THRUST_FORCE_RDC})\n  endif()\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Implementing Min Reduction for 64-bit Integers in CUDA\nDESCRIPTION: This function performs an asynchronous bulk minimum reduction operation on 64-bit integers from shared memory to global memory. It uses the cp.reduce.async.bulk instruction from PTX ISA 80 for SM_90 architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_reduce_async_bulk.rst#2025-04-23_snippet_27\n\nLANGUAGE: cuda\nCODE:\n```\n// cp.reduce.async.bulk.dst.src.bulk_group.op.type  [dstMem], [srcMem], size; // 4. PTX ISA 80, SM_90\n// .dst       = { .global }\n// .src       = { .shared::cta }\n// .type      = { .s64 }\n// .op        = { .min }\ntemplate <typename = void>\n__device__ static inline void cp_reduce_async_bulk(\n  cuda::ptx::space_global_t,\n  cuda::ptx::space_shared_t,\n  cuda::ptx::op_min_t,\n  int64_t* dstMem,\n  const int64_t* srcMem,\n  uint32_t size);\n```\n\n----------------------------------------\n\nTITLE: Executing Tensor Core Collector Use Operation in CUDA\nDESCRIPTION: This template function executes a tensor core collector use operation for various data types. It handles different parameter sets, including with and without zero column mask descriptor.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_37\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_collector_b2_use(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d,\n  uint64_t zero_column_mask_desc);\n```\n\n----------------------------------------\n\nTITLE: Basic Asynchronous Bulk Tensor Copy (4D)\nDESCRIPTION: Template function for copying 4D tensor data from global memory to cluster shared memory asynchronously. Uses barrier synchronization and supports multicast operations.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_async_bulk_tensor_multicast.rst#2025-04-23_snippet_5\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void cp_async_bulk_tensor(\n  cuda::ptx::space_cluster_t,\n  cuda::ptx::space_global_t,\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  void* dstMem,\n  const void* tensorMap,\n  const int32_t (&tensorCoords)[4],\n  uint64_t* smem_bar,\n  const uint16_t& ctaMask);\n```\n\n----------------------------------------\n\nTITLE: Implementing MMA Workspace Last Use Operations with I8 Data Type (No Masking) in CUDA\nDESCRIPTION: Template function for the final tensor compute operation (lastuse) with I8 data type. This simplified variant handles tensor descriptors without zero column masking. Targets PTX ISA 86 on SM_100a and SM_101a architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_50\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.ws.cta_group.kind.collector::b2::lastuse [d_tmem], a_desc, b_desc, idesc, enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1 }\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_collector_b2_lastuse(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: MMA Block Scale Vector 2X Collector Fill with mxf4 Type (CTA Group 2)\nDESCRIPTION: Template function for matrix multiplication with block scaling using vector 2X with collector A fill operation for CTA group 2. The function can use either mxf4 or mxf4nvf4 data types and requires PTX ISA 86 with SM_100a or SM_101a architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma.rst#2025-04-23_snippet_27\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.cta_group.kind.block_scale.scale_vec::2X.collector::a::fill [d_tmem], a_desc, b_desc, idesc, [scale_A_tmem], [scale_B_tmem], enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .kind      = { .kind::mxf4, .kind::mxf4nvf4 }\n// .cta_group = { .cta_group::1, .cta_group::2 }\ntemplate <cuda::ptx::dot_kind Kind, cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_mma_block_scale_vec_2_tmem_a_collector_a_fill(\n  cuda::ptx::kind_t<Kind> kind,\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  uint32_t scale_A_tmem,\n  uint32_t scale_B_tmem,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: MMA TMEM Collector Fill Operation without Zero Column Mask\nDESCRIPTION: Template function for basic matrix multiply-accumulate collector fill operation using tensor memory for A operand without zero column mask. Supports multiple data types.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_3\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_tmem_a_collector_b0_fill(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint32_t a_tmem,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Declaring MMA Block Scale Vector 2 TMEM A Collector for MXF4/MXF4NVF4 Kinds\nDESCRIPTION: Device function template for matrix multiply with 2X scaling factor using TMEM for matrix A collection, supporting both MXF4 and MXF4NVF4 data types. The function works with different CTA groups and includes matrix descriptor parameters.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma.rst#2025-04-23_snippet_38\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.cta_group.kind.block_scale.scale_vec::2X.collector::a::use [d_tmem], a_desc, b_desc, idesc, [scale_A_tmem], [scale_B_tmem], enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .kind      = { .kind::mxf4, .kind::mxf4nvf4 }\n// .cta_group = { .cta_group::1, .cta_group::2 }\ntemplate <cuda::ptx::dot_kind Kind, cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_mma_block_scale_vec_2_tmem_a_collector_a_use(\n  cuda::ptx::kind_t<Kind> kind,\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  uint32_t scale_A_tmem,\n  uint32_t scale_B_tmem,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Declaring multimem_ld_reduce with Acquire/GPU Scope and OR Operation in CUDA PTX\nDESCRIPTION: Template function declaration for multimem.ld_reduce with acquire memory semantics, GPU scope, and OR operation on 64-bit values in global memory. Compatible with PTX ISA 8.1 on SM_90 architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_67\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.b64 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .or }\ntemplate <typename B64, enable_if_t<sizeof(B64) == 8, bool> = true, cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline B64 multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_or_op_t,\n  const B64* addr);\n```\n\n----------------------------------------\n\nTITLE: Configuring CUB Build Options\nDESCRIPTION: Defines CMake options for controlling header testing, unit testing, examples, and tuning capabilities. Disables tuning for NVHPC compiler and provides option for C++ dialect naming.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cub/CMakeLists.txt#2025-04-23_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\noption(CUB_ENABLE_HEADER_TESTING \"Test that all public headers compile.\" ON)\noption(CUB_ENABLE_TESTING \"Build CUB testing suite.\" ON)\noption(CUB_ENABLE_EXAMPLES \"Build CUB examples.\" ON)\n\noption(CUB_ENABLE_TUNING \"Build CUB tuning suite.\" OFF)\nif (\"NVHPC\" STREQUAL \"${CMAKE_CXX_COMPILER_ID}\")\n  set(CUB_ENABLE_TUNING OFF)\nendif()\n\n# This is needed for NVCXX QA, which requires a static set of executable names.\n# Only a single dialect may be enabled when this is off.\noption(CUB_ENABLE_CPP_DIALECT_IN_NAMES\n  \"Include C++ dialect information in target/object/etc names.\"\n  ON\n)\nmark_as_advanced(CUB_ENABLE_CPP_DIALECT_IN_NAMES)\n```\n\n----------------------------------------\n\nTITLE: Implementing CUB Policy Hub in C++\nDESCRIPTION: Demonstrates how to create a policy hub using tuning parameters for a CUB algorithm.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/tuning.rst#2025-04-23_snippet_13\n\nLANGUAGE: c++\nCODE:\n```\n#if !TUNE_BASE\n  template <typename AccumT, typename OffsetT>\n  struct policy_hub_t {\n    struct MaxPolicy : cub::ChainedPolicy<300, policy_t, policy_t> {\n      static constexpr int threads_per_block  = TUNE_THREADS_PER_BLOCK;\n      static constexpr int items_per_thread   = TUNE_ITEMS_PER_THREAD;\n      using AlgorithmPolicy = AgentAlgorithmPolicy<threads_per_block, items_per_thread, ...>;\n    };\n#endif\n```\n\n----------------------------------------\n\nTITLE: Implementing multimem_ld_reduce for OR operation with various semantics and scopes in CUDA\nDESCRIPTION: Template function declaration for multimem load-reduce OR operation on 32-bit values. This template supports both relaxed and acquire semantics with different memory scopes (CTA, cluster, GPU, system) for global memory access.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_54\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.b32 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .or }\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true, cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline B32 multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_or_op_t,\n  const B32* addr);\n```\n\n----------------------------------------\n\nTITLE: Increment Operation for Unsigned 32-bit Integer\nDESCRIPTION: Template function implementing asynchronous increment reduction operation on cluster memory barrier for u32 type. Takes destination pointer, value and remote barrier pointer as parameters.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/red_async.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline void red_async(\n  cuda::ptx::op_inc_t,\n  uint32_t* dest,\n  const uint32_t& value,\n  uint64_t* remote_bar);\n```\n\n----------------------------------------\n\nTITLE: Setting Default CUDA Architecture\nDESCRIPTION: Sets the default CUDA architecture to 86 if not specified otherwise.\nSOURCE: https://github.com/nvidia/cccl/blob/main/examples/cudax/CMakeLists.txt#2025-04-23_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT DEFINED CMAKE_CUDA_ARCHITECTURES)\n  set(CMAKE_CUDA_ARCHITECTURES 86)\nendif()\n```\n\n----------------------------------------\n\nTITLE: CUDA Async Bulk Reduction with MIN Operation (FP16)\nDESCRIPTION: Template function for performing asynchronous bulk reduction with MIN operation on FP16 data from shared CTA memory to global memory. Requires PTX ISA 80 and SM_90.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_reduce_async_bulk_f16.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\n// cp.reduce.async.bulk.dst.src.bulk_group.op.type  [dstMem], [srcMem], size; // 4. PTX ISA 80, SM_90\n// .dst       = { .global }\n// .src       = { .shared::cta }\n// .type      = { .f16 }\n// .op        = { .min }\ntemplate <typename = void>\n__device__ static inline void cp_reduce_async_bulk(\n  cuda::ptx::space_global_t,\n  cuda::ptx::space_shared_t,\n  cuda::ptx::op_min_t,\n  __half* dstMem,\n  const __half* srcMem,\n  uint32_t size);\n```\n\n----------------------------------------\n\nTITLE: Launching a Block-Sorting Kernel with CUB Primitives\nDESCRIPTION: Example code showing how to launch the previously defined block-sorting kernel with specific template parameters. This snippet demonstrates how to configure and launch a kernel that uses CUB's collective primitives.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/index.rst#2025-04-23_snippet_1\n\nLANGUAGE: c++\nCODE:\n```\n// Elsewhere in the host program: parameterize and launch a block-sorting\n// kernel in which blocks of 128 threads each sort segments of 2048 keys\nint *d_in = ...;\nint *d_out = ...;\nint num_blocks = ...;\nBlockSortKernel<128, 16><<<num_blocks, 128>>>(d_in, d_out);\n```\n\n----------------------------------------\n\nTITLE: 4D Tensor Copy: Global to Shared CTA Memory with Barrier\nDESCRIPTION: Device function for asynchronous bulk tensor copy from global to shared CTA memory for 4D tensors with memory barrier completion tracking. Requires PTX ISA 86 and SM_90.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_async_bulk_tensor.rst#2025-04-23_snippet_5\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline void cp_async_bulk_tensor(\n  cuda::ptx::space_shared_t,\n  cuda::ptx::space_global_t,\n  void* dstMem,\n  const void* tensorMap,\n  const int32_t (&tensorCoords)[4],\n  uint64_t* smem_bar);\n```\n\n----------------------------------------\n\nTITLE: Defining cuda::memcpy_async with Pipeline in CUDA\nDESCRIPTION: Defines two overloads of cuda::memcpy_async that use a cuda::pipeline for synchronization. Similar to the barrier versions, there are single-thread and group operation variants.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/asynchronous_operations/memcpy_async.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n// (3)\ntemplate <typename Shape, cuda::thread_scope Scope>\n__host__ __device__\nvoid cuda::memcpy_async(void* destination, void const* source, Shape size,\n                       cuda::pipeline<Scope>& pipeline);\n\n// (4)\ntemplate <typename Group, typename Shape, cuda::thread_scope Scope>\n__host__ __device__\nvoid cuda::memcpy_async(Group const& group,\n                       void* destination, void const* source, Shape size,\n                       cuda::pipeline<Scope>& pipeline);\n```\n\n----------------------------------------\n\nTITLE: Implementing Multimem Load-Reduce for 64-bit Integer Max Operation in CUDA\nDESCRIPTION: This template function performs a multimem load-reduce operation for 64-bit integers using the max operation. It supports different semantics and scopes, allowing for flexible memory ordering and visibility across different execution units.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_28\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.s64 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .max }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline int64_t multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_max_t,\n  const int64_t* addr);\n```\n\n----------------------------------------\n\nTITLE: Adding <cuda/std/tuple> and cuda::std::pair (C++)\nDESCRIPTION: Feature #17 added in libcu++ 1.3.0 (CUDA Toolkit 11.2). Introduces `<cuda/std/tuple>` for `cuda::std::tuple` (fixed-size heterogeneous collection) and `cuda::std::pair` (two heterogeneous values) within `<cuda/std/utility>`. Note: These are not supported when compiling with NVCC + MSVC.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/releases/changelog.rst#2025-04-23_snippet_18\n\nLANGUAGE: cpp\nCODE:\n```\n<cuda/std/tuple>\n```\n\nLANGUAGE: cpp\nCODE:\n```\ncuda::std::tuple\n```\n\nLANGUAGE: cpp\nCODE:\n```\n<cuda/std/utility>\n```\n\nLANGUAGE: cpp\nCODE:\n```\ncuda::std::pair\n```\n\n----------------------------------------\n\nTITLE: Implementing MMA Operation with Block Scaling for mxf4 and mxf4nvf4 kinds (Collector A Fill)\nDESCRIPTION: This CUDA device function template implements a matrix multiplication and accumulation operation with block scaling for mxf4 and mxf4nvf4 kinds. It uses a collector for A operand filling, supports CTA group sizes of 1 and 2, and uses a 2X scale vector.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma.rst#2025-04-23_snippet_21\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.cta_group.kind.block_scale.scale_vec::2X.collector::a::fill [d_tmem], a_desc, b_desc, idesc, [scale_A_tmem], [scale_B_tmem], enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .kind      = { .kind::mxf4, .kind::mxf4nvf4 }\n// .cta_group = { .cta_group::1, .cta_group::2 }\ntemplate <cuda::ptx::dot_kind Kind, cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_mma_block_scale_vec_2x_collector_a_fill(\n  cuda::ptx::kind_t<Kind> kind,\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  uint32_t scale_A_tmem,\n  uint32_t scale_B_tmem,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Configuring Compiler-Specific Flags for Different CUDA Compilers\nDESCRIPTION: Sets up compiler and linker flags specific to each supported CUDA compiler (Clang, NVIDIA, NVHPC), including warning levels, header includes, and library paths.\nSOURCE: https://github.com/nvidia/cccl/blob/main/libcudacxx/test/libcudacxx/CMakeLists.txt#2025-04-23_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nif (${CMAKE_CUDA_COMPILER_ID} STREQUAL \"Clang\")\n  string(APPEND LIBCUDACXX_TEST_COMPILER_FLAGS\n    \" ${CMAKE_CUDA_FLAGS}\"\n    \" -Xclang -fcuda-allow-variadic-functions\"\n    \" -Xclang -Wno-unused-parameter\"\n    \" -Wno-unknown-cuda-version\"\n    \" ${LIBCUDACXX_FORCE_INCLUDE}\"\n    \" -I${libcudacxx_SOURCE_DIR}/include\"\n    \" ${LIBCUDACXX_WARNING_LEVEL}\")\n\n  string(APPEND LIBCUDACXX_TEST_LINKER_FLAGS\n    \" ${CMAKE_CUDA_FLAGS}\"\n    \" -L${CUDAToolkit_LIBRARY_DIR}\"\n    \" -lcuda\"\n    \" -lcudart\")\nelseif (${CMAKE_CUDA_COMPILER_ID} STREQUAL \"NVIDIA\")\n  string(APPEND LIBCUDACXX_TEST_COMPILER_FLAGS\n    \" ${LIBCUDACXX_FORCE_INCLUDE}\"\n    \" ${LIBCUDACXX_WARNING_LEVEL}\"\n    \" -Wno-deprecated-gpu-targets\")\nelseif (${CMAKE_CUDA_COMPILER_ID} STREQUAL \"NVHPC\")\n  string(APPEND LIBCUDACXX_TEST_COMPILER_FLAGS\n    \" -stdpar\")\n  string(APPEND LIBCUDACXX_TEST_LINKER_FLAGS\n    \" -stdpar\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Implementing MMA Workspace Operations with TF32 Data Type in CUDA\nDESCRIPTION: Template function for tensor compute generation 5 matrix multiplication with workspaces using TF32 data type. This variant operates on tensor memory and descriptors without zero column masking. Targets PTX ISA 86 on SM_100a and SM_101a architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_41\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.ws.cta_group.kind.collector::b2::use [d_tmem], [a_tmem], b_desc, idesc, enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1 }\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_tmem_a_collector_b2_use(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint32_t a_tmem,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: CUDA Memory Access Property Usage Example\nDESCRIPTION: Example kernel demonstrating how to use associate_access_property to set streaming access property for memory operations. Shows integration with cooperative groups and shared memory usage.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/memory_access_properties/associate_access_property.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n#include <cuda/cooperative_groups.h>\n__global__ void memcpy(int const* in_, int* out) {\n    int const* in = cuda::associate_access_property(in_, cuda::access_property::streaming{});\n    auto idx = cooperative_groups::this_grid().thread_rank();\n\n    __shared__ int shmem[N];\n    shmem[threadIdx.x] = in[idx]; // streaming access\n\n    // compute...\n}\n```\n\n----------------------------------------\n\nTITLE: CUDA MMA Basic Usage Template\nDESCRIPTION: Template device function for matrix multiplication using CTA group 1 with support for different data types. Takes descriptor parameters for matrices and enables input D.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_19\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_collector_b1_use(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Bulk Store Operation Template for Shared Memory CTA\nDESCRIPTION: Template function for performing bulk store operations in shared memory using CTA. The function takes a destination address, size, and initial value of type n32_t. This is part of PTX ISA 86 and targets SM_100.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/st_bulk.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\n// st.bulk.weak.shared::cta [addr], size, initval; // PTX ISA 86, SM_100\ntemplate <int N32>\n__device__ static inline void st_bulk(\n  void* addr,\n  uint64_t size,\n  cuda::ptx::n32_t<N32> initval);\n```\n\n----------------------------------------\n\nTITLE: Compiling CUDASTF Examples with CMake\nDESCRIPTION: Provides commands for compiling CUDASTF examples using CMake and Ninja build system.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmkdir -p build\ncd build\ncmake .. --preset cudax-cpp17\ncd cudax-cpp17\nninja cudax.cpp17.examples.stf -j4\n```\n\n----------------------------------------\n\nTITLE: Creating Algorithm Tests in CMake\nDESCRIPTION: Sets up tests for CUDA algorithm implementations, specifically for fill and copy operations. These tests verify the proper operation of basic parallel algorithms provided by the CCCL library.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cudax/test/CMakeLists.txt#2025-04-23_snippet_13\n\nLANGUAGE: CMake\nCODE:\n```\n  cudax_add_catch2_test(test_target algorithm ${cn_target}\n    algorithm/fill.cu\n    algorithm/copy.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: Using barrier.cluster PTX Instruction in CUDA C++\nDESCRIPTION: This snippet demonstrates the use of barrier.cluster functionality through CUDA C++ builtins and the cooperative_groups::cluster_group API. These provide similar functionality to the PTX instruction for cluster-level synchronization.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/barrier_cluster.rst#2025-04-23_snippet_0\n\nLANGUAGE: CUDA C++\nCODE:\n```\n__cluster_barrier_arrive()\n__cluster_barrier_arrive_relaxed()\n__cluster_barrier_wait()\n\ncooperative_groups::cluster_group\n```\n\n----------------------------------------\n\nTITLE: Defining Function to Add and Configure Thrust Tests in CMake\nDESCRIPTION: Defines the `thrust_add_test` CMake function to encapsulate the logic for adding a single test. It retrieves configuration details (host, device, prefix) from the provided `thrust_target`. For non-CUDA backends, it wraps the `.cu` source file in a `.cpp` file. It creates an executable target, links necessary libraries (like the framework target), sets include directories, clones properties from the configuration target, adds compile definitions (e.g., `CCCL_ENABLE_ASSERTIONS`, `THRUST_TEST_DEVICE_SIDE` conditionally), applies compiler-specific workarounds (like suppressing nvcc warnings or handling Clang/nvcc build issues), adds the test target as a dependency to configuration-specific and test-specific meta-targets, and registers the test with CTest using a custom runner script (`ThrustRunTest.cmake`). It also sets properties like `RUN_SERIAL` for OMP/TBB tests and includes an optional per-test `.cmake` script for further customization. The name of the created target is stored in the variable passed as `target_name_var` in the parent scope.\nSOURCE: https://github.com/nvidia/cccl/blob/main/thrust/testing/CMakeLists.txt#2025-04-23_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\n## thrust_add_test\n#\n# Add a test executable and register it with ctest.\n#\n# target_name_var: Variable name to overwrite with the name of the test\n#   target. Useful for post-processing target information per-backend.\n# test_name: The name of the test minus \"<config_prefix>.test.\" For example,\n#   testing/vector.cu will be \"vector\", and testing/cuda/copy.cu will be\n#   \"cuda.copy\".\n# test_src: The source file that implements the test.\n# thrust_target: The reference thrust target with configuration information.\n#\nfunction(thrust_add_test target_name_var test_name test_src thrust_target)\n  thrust_get_target_property(config_host ${thrust_target} HOST)\n  thrust_get_target_property(config_device ${thrust_target} DEVICE)\n  thrust_get_target_property(config_prefix ${thrust_target} PREFIX)\n\n  # Wrap the .cu file in .cpp for non-CUDA backends\n  if (\"CUDA\" STREQUAL \"${config_device}\")\n    set(real_test_src \"${test_src}\")\n  else()\n    thrust_wrap_cu_in_cpp(real_test_src \"${test_src}\" ${thrust_target})\n  endif()\n\n  # The actual name of the test's target:\n  set(test_target ${config_prefix}.test.${test_name})\n  set(${target_name_var} ${test_target} PARENT_SCOPE)\n\n  # Related target names:\n  set(config_framework_target ${config_prefix}.test.framework)\n  set(config_meta_target ${config_prefix}.tests)\n  set(test_meta_target thrust.all.test.${test_name})\n\n  add_executable(${test_target} \"${real_test_src}\")\n  target_link_libraries(${test_target} PRIVATE ${config_framework_target})\n  target_include_directories(${test_target} PRIVATE \"${Thrust_SOURCE_DIR}/testing\")\n  thrust_clone_target_properties(${test_target} ${thrust_target})\n\n  if (NOT \"Clang\" STREQUAL \"${CMAKE_CUDA_COMPILER_ID}\")\n    target_compile_definitions(${test_target} PRIVATE THRUST_TEST_DEVICE_SIDE)\n  endif()\n\n  # nvcc < 11.5 generates \"error #186-D: pointless comparison of unsigned integer with zero\"\n  # when including <cuda_pipeline_primitives.h> in CUB's dispatch_transform.h,\n  # despite explicitly suppressing the warning there\n  if (\"NVIDIA\" STREQUAL \"${CMAKE_CUDA_COMPILER_ID}\" AND CMAKE_CUDA_COMPILER_VERSION VERSION_LESS 11.5.0)\n      target_compile_options(${test_target} PRIVATE $<$<COMPILE_LANGUAGE:CUDA>:-Xcudafe=--diag_suppress=186>)\n  endif ()\n\n  # Ensure that we test with assertions enabled\n  target_compile_definitions(${test_target} PRIVATE CCCL_ENABLE_ASSERTIONS)\n\n  thrust_fix_clang_nvcc_build_for(${test_target})\n\n  # Add to the active configuration's meta target\n  add_dependencies(${config_meta_target} ${test_target})\n\n  # Meta target that builds tests with this name for all configurations:\n  if (NOT TARGET ${test_meta_target})\n    add_custom_target(${test_meta_target})\n  endif()\n  add_dependencies(${test_meta_target} ${test_target})\n\n  add_test(NAME ${test_target}\n    COMMAND \"${CMAKE_COMMAND}\"\n    \"-DTHRUST_BINARY=$<TARGET_FILE:${test_target}>\"\n    \"-DTHRUST_SOURCE=${Thrust_SOURCE_DIR}\"\n    -P \"${Thrust_SOURCE_DIR}/cmake/ThrustRunTest.cmake\"\n  )\n\n  # Run OMP/TBB tests in serial. Multiple OMP processes will massively\n  # oversubscribe the machine with GCC's OMP, and we want to test these with\n  # the full CPU available to each unit test.\n  set(config_systems ${config_host} ${config_device})\n  if ((\"OMP\" IN_LIST config_systems) OR (\"TBB\" IN_LIST config_systems))\n    set_tests_properties(${test_target} PROPERTIES RUN_SERIAL ON)\n  endif()\n\n  # Check for per-test script. Script will be included in the current scope\n  # to allow custom property modifications.\n  get_filename_component(test_cmake_script \"${test_src}\" NAME_WLE)\n  set(test_cmake_script \"${CMAKE_CURRENT_LIST_DIR}/${test_cmake_script}.cmake\")\n  # Use a glob so we can detect if this changes:\n  file(GLOB test_cmake_script\n    RELATIVE \"${CMAKE_CURRENT_LIST_DIR}\"\n    CONFIGURE_DEPENDS\n    \"${test_cmake_script}\"\n  )\n  if (test_cmake_script) # Will be non-empty only if the script exists\n    include(\"${test_cmake_script}\")\n  endif()\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Implementing Policy Hub for CUB Tuning\nDESCRIPTION: Implements a policy hub mechanism that translates between the base CUB implementation and tuned variants. This code distinguishes between baseline benchmarks and parameterized variants using the TUNE_BASE macro.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/tuning.rst#2025-04-23_snippet_3\n\nLANGUAGE: c++\nCODE:\n```\n#if TUNE_BASE\n  using dispatch_t = cub::DispatchReduce<T, OffsetT>; // uses default policy hub\n#else\n  template <typename AccumT, typename OffsetT>\n  struct policy_hub_t {\n    struct MaxPolicy : cub::ChainedPolicy<300, policy_t, policy_t> {\n      static constexpr int threads_per_block  = TUNE_THREADS_PER_BLOCK;\n      static constexpr int items_per_thread   = TUNE_ITEMS_PER_THREAD;\n      ...\n    };\n  };\n\n  using dispatch_t = cub::DispatchReduce<T, OffsetT, policy_hub_t<accum_t, offset_t>>;\n#endif\n```\n\n----------------------------------------\n\nTITLE: Implementing MMA Workspace Last Use Operations with F16 Data Type in CUDA\nDESCRIPTION: Template function for the final tensor compute operation (lastuse) with F16 data type. This variant handles tensor descriptors and supports zero column masking. Targets PTX ISA 86 on SM_100a and SM_101a architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_43\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.ws.cta_group.kind.collector::b2::lastuse [d_tmem], a_desc, b_desc, idesc, enable_input_d, zero_column_mask_desc; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1 }\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_collector_b2_lastuse(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d,\n  uint64_t zero_column_mask_desc);\n```\n\n----------------------------------------\n\nTITLE: Adding segmented operations in CUB 1.5.0\nDESCRIPTION: Introduction of segmented device-wide operations for sort and reduction primitives.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/releases/changelog.rst#2025-04-23_snippet_9\n\nLANGUAGE: C++\nCODE:\n```\n// Segmented device-wide operations for sort and reduction primitives\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Warning Suppression Macros in C++\nDESCRIPTION: This snippet shows how to use the warning suppression macros in C++ code. It demonstrates pushing diagnostic settings, suppressing a specific GCC warning, and then popping the diagnostic settings back.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cccl/development/macro.rst#2025-04-23_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\n_CCCL_DIAG_PUSH\n_CCCL_DIAG_SUPPRESS_GCC(\"-Wattributes\")\n// code ..\n_CCCL_DIAG_POP\n```\n\n----------------------------------------\n\nTITLE: Implementing Block-Scaled MMA for mxf4 and mxf4nvf4 with 2X Vector Scaling in CUDA\nDESCRIPTION: This template function implements a block-scaled matrix multiply-accumulate operation for mxf4 and mxf4nvf4 data types with 2X vector scaling. It supports different CTA group configurations.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma.rst#2025-04-23_snippet_15\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.cta_group.kind.block_scale.scale_vec::2X [d_tmem], a_desc, b_desc, idesc, [scale_A_tmem], [scale_B_tmem], enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .kind      = { .kind::mxf4, .kind::mxf4nvf4 }\n// .cta_group = { .cta_group::1, .cta_group::2 }\ntemplate <cuda::ptx::dot_kind Kind, cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_mma_block_scale_vec_2x(\n  cuda::ptx::kind_t<Kind> kind,\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  uint32_t scale_A_tmem,\n  uint32_t scale_B_tmem,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Declaring Load-Reduce Minimum Operation (acquire/cta/global/u32) in CUDA C++\nDESCRIPTION: Declares a device function for atomic load-reduce min operation on global 32-bit unsigned values, using acquire or relaxed PTX semaphore semantics and block (cta) or other scopes. The function is a template specialized by PTX semaphore and scope tags, and returns the reduction result. Core to atomic reductions in concurrent CUDA kernels where hardware support is available.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_5\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.u32 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .min }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline uint32_t multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_min_t,\n  const uint32_t* addr);\n```\n\n----------------------------------------\n\nTITLE: CTA Group 2 Asynchronous Bulk Tensor Copy (5D)\nDESCRIPTION: Template function for copying 5D tensor data using CTA group 2 configuration. Similar to CTA group 1 but with different group specification.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_async_bulk_tensor_multicast.rst#2025-04-23_snippet_8\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void cp_async_bulk_tensor(\n  cuda::ptx::space_cluster_t,\n  cuda::ptx::space_global_t,\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  void* dstMem,\n  const void* tensorMap,\n  const int32_t (&tensorCoords)[5],\n  uint64_t* smem_bar,\n  const uint16_t& ctaMask);\n```\n\n----------------------------------------\n\nTITLE: Box Shape with Lower and Upper Bounds in parallel_for - CUDASTF C++\nDESCRIPTION: Shows how to define a box shape with inclusive lower and exclusive upper bounds as input to parallel_for, allowing iteration over a restricted 2D range. Requires CUDASTF context, box<dimensions> syntax, and a lambda matching the number of dimensions. Output is formatted index pairs via printf.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_33\n\nLANGUAGE: cpp\nCODE:\n```\n   ctx.parallel_for(box<2>({{5, 8}, {2, 4}}))->*[] __device__(size_t i, size_t j) {\n       printf(\"%ld, %ld\\n\", i, j);\n   };\n```\n\n----------------------------------------\n\nTITLE: Modifying Default Behavior of cuda::std::complex and cuda::std::atomic (C++)\nDESCRIPTION: libcu++ 1.6.0 (CUDA Toolkit 11.5) changes the default alignment of `cuda::std::complex` for improved code generation and updates `cuda::std::atomic` to utilize `<nv/target>` as its primary dispatch mechanism. This release introduces and defaults to ABI version 4.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/releases/changelog.rst#2025-04-23_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\ncuda::std::complex\n```\n\nLANGUAGE: cpp\nCODE:\n```\ncuda::std::atomic\n```\n\nLANGUAGE: cpp\nCODE:\n```\n<nv/target>\n```\n\n----------------------------------------\n\nTITLE: CUDA Memory Pinning Kernel\nDESCRIPTION: Kernel implementation for applying persisting access property to memory arrays.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/memory_access_properties/apply_access_property.rst#2025-04-23_snippet_4\n\nLANGUAGE: cuda\nCODE:\n```\n__global__ void pin(int* a, int* b, size_t N) {\n    auto g = cooperative_groups::this_grid();\n    for (int idx = g.thread_rank(); idx < N; idx += g.size()) {\n        cuda::apply_access_property(a + idx, sizeof(int), cuda::access_property::persisting{});\n        cuda::apply_access_property(b + idx, sizeof(int), cuda::access_property::persisting{});\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: CUDA round_down Function Template Declaration\nDESCRIPTION: Template function declaration for round_down operation that computes the floor of a value to the largest multiple of a base value. Function is marked as host and device compatible.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/math/round_down.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename T, typename U>\n[[nodiscard]] __host__ __device__ inline constexpr\ncuda::std::common_type_t<T, U> round_down(T value, U base_multiple) noexcept;\n```\n\n----------------------------------------\n\nTITLE: Declaring Load-Reduce Minimum Operation (relaxed/sys/global/u32) in CUDA C++\nDESCRIPTION: Defines a device inline template function for performing a minimum reduction load on global memory with 32-bit unsigned integers, using either relaxed or acquire semantics and any PTX-defined memory scope. Accepts PTX semaphore, scope, a min op, and address pointer. Assumes inclusion of appropriate CUDA PTX semantic and scope types.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_4\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.u32 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .min }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline uint32_t multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_min_t,\n  const uint32_t* addr);\n```\n\n----------------------------------------\n\nTITLE: Using %PARAM% Comment for Test Parametrization in C++\nDESCRIPTION: The basic syntax for defining a parameter in CUB tests. This comment structure instructs CMake to generate multiple test executables with different preprocessor definitions.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cub/test/README.md#2025-04-23_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n// %PARAM% [definition] [label] [values]\n```\n\n----------------------------------------\n\nTITLE: Configuring Thrust/CUB in Compiler Explorer CUDA Properties\nDESCRIPTION: Configuration in 'cuda.amazon.properties' that defines how Thrust and CUB libraries appear in the Compiler Explorer UI. It specifies display names, descriptions, available versions, and the file paths for each version.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/release_process.rst#2025-04-23_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nlibs.thrustcub.name=Thrust+CUB\nlibs.thrustcub.description=CUDA collective and parallel algorithms\nlibs.thrustcub.versions=trunk:109090:109100:109101:110000\nlibs.thrustcub.url=http://www.github.com/NVIDIA/thrust\nlibs.thrustcub.versions.109090.version=1.9.9\nlibs.thrustcub.versions.109090.path=/opt/compiler-explorer/libs/thrustcub/1.9.9:/opt/compiler-explorer/libs/thrustcub/1.9.9/dependencies/cub\nlibs.thrustcub.versions.109100.version=1.9.10\nlibs.thrustcub.versions.109100.path=/opt/compiler-explorer/libs/thrustcub/1.9.10:/opt/compiler-explorer/libs/thrustcub/1.9.10/dependencies/cub\nlibs.thrustcub.versions.109101.version=1.9.10-1\nlibs.thrustcub.versions.109101.path=/opt/compiler-explorer/libs/thrustcub/1.9.10-1:/opt/compiler-explorer/libs/thrustcub/1.9.10-1/dependencies/cub\nlibs.thrustcub.versions.110000.version=1.10.0\nlibs.thrustcub.versions.110000.path=/opt/compiler-explorer/libs/thrustcub/1.10.0:/opt/compiler-explorer/libs/thrustcub/1.10.0/dependencies/cub\nlibs.thrustcub.versions.trunk.version=trunk\nlibs.thrustcub.versions.trunk.path=/opt/compiler-explorer/libs/thrustcub/trunk:/opt/compiler-explorer/libs/thrustcub/trunk/dependencies/cub\n```\n\n----------------------------------------\n\nTITLE: Setting Explicit TBB or OpenMP Targets for Thrust\nDESCRIPTION: Demonstrates how to specify existing TBB or OpenMP targets for Thrust to use instead of letting it create its own through lazy loading.\nSOURCE: https://github.com/nvidia/cccl/blob/main/lib/cmake/thrust/README.md#2025-04-23_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nthrust_set_TBB_target(MyTBBTarget)\nthrust_set_OMP_target(MyOMPTarget)\n```\n\n----------------------------------------\n\nTITLE: Creating 3D Data Slices in CUDA STF\nDESCRIPTION: Demonstrates creating contiguous and non-contiguous 3D slices from array data using make_slice function with different dimensions and strides.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_11\n\nLANGUAGE: cpp\nCODE:\n```\ndouble A[5 * 3 * 40];\n\n// contiguous 3D slice\nslice<double, 3> s = make_slice(A, std::tuple { 5, 3, 40 }, 5, 5 * 3);\n\n// non-contiguous 3D slice\nslice<double, 3> s2 = make_slice(A, std::tuple { 4, 3, 40 }, 5, 5 * 3);\n\n// non-contiguous 3D slice\nslice<double, 3> s3 = make_slice(A, std::tuple { 5, 2, 40 }, 5, 5 * 3);\n```\n\n----------------------------------------\n\nTITLE: Implementing mbarrier.expect_tx with CTA scope and shared CTA space\nDESCRIPTION: Template implementation for mbarrier expect transaction with relaxed semantics, CTA scope, and shared CTA memory space. Requires PTX ISA 80 and SM_90.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/mbarrier_expect_tx.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\n// mbarrier.expect_tx.sem.scope.space.b64 [addr], txCount; // 1. PTX ISA 80, SM_90\n// .sem       = { .relaxed }\n// .scope     = { .cta, .cluster }\n// .space     = { .shared::cta }\ntemplate <cuda::ptx::dot_scope Scope>\n__device__ static inline void mbarrier_expect_tx(\n  cuda::ptx::sem_relaxed_t,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::space_shared_t,\n  uint64_t* addr,\n  uint32_t txCount);\n```\n\n----------------------------------------\n\nTITLE: Configuring Thrust::Thrust Target in CMake\nDESCRIPTION: Explains how find_package(Thrust) creates the basic Thrust::Thrust target, which describes the location of Thrust headers. It doesn't configure dependencies, which are loaded on-demand or via component mechanism.\nSOURCE: https://github.com/nvidia/cccl/blob/main/lib/cmake/thrust/README.md#2025-04-23_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nfind_package(Thrust)\n```\n\n----------------------------------------\n\nTITLE: Implementing getctarank Instruction for Cluster Shared Memory in CUDA\nDESCRIPTION: Template implementation of the getctarank instruction for cluster shared memory operations. Takes a cluster space identifier and memory address pointer to determine CTA rank. Compatible with PTX ISA 78 and SM_90 architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/getctarank.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\n// getctarank.space.u32 dest, addr; // PTX ISA 78, SM_90\n// .space     = { .shared::cluster }\ntemplate <typename = void>\n__device__ static inline uint32_t getctarank(\n  cuda::ptx::space_cluster_t,\n  const void* addr);\n```\n\n----------------------------------------\n\nTITLE: Bulk Asynchronous 4D Tensor Reductions Using CUDA PTX (C++/CUDA)\nDESCRIPTION: Defines a CUDA device inline function template for asynchronous bulk reduction across 4D tensors, transferring data from shared (CTA) memory to global memory and performing operations such as and, or, xor, controlled by the PTX dot operator type. The function is parameterized by the reduction operation and accepts a tensor map, its 4D integer coordinates, and the source memory pointer. Prerequisites include a CUDA environment (PTX ISA 80, SM_90), inclusion of cuda::ptx::dot_op and related types, and valid memory pointers. Inputs include the tensor map, coordinates, operation, and source memory; outputs are written to the target memory region. Limitation: works only for 4D tensors and listed reduction operations.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_reduce_async_bulk_tensor.rst#2025-04-23_snippet_5\n\nLANGUAGE: cuda\nCODE:\n```\n// cp.reduce.async.bulk.tensor.4d.dst.src.op.tile.bulk_group [tensorMap, tensorCoords], [srcMem]; // 1d. PTX ISA 80, SM_90\n// .dst       = { .global }\n// .src       = { .shared::cta }\n// .op        = { .add, .min, .max, .inc, .dec, .and, .or, .xor }\ntemplate <cuda::ptx::dot_op Op>\n__device__ static inline void cp_reduce_async_bulk_tensor(\n  cuda::ptx::space_global_t,\n  cuda::ptx::space_shared_t,\n  cuda::ptx::op_t<Op> op,\n  const void* tensorMap,\n  const int32_t (&tensorCoords)[4],\n  const void* srcMem);\n```\n\n----------------------------------------\n\nTITLE: Implementing Add Reduction for 64-bit Floats in CUDA\nDESCRIPTION: This function performs an asynchronous bulk addition reduction operation on 64-bit floating-point numbers from shared memory to global memory. It uses the cp.reduce.async.bulk instruction from PTX ISA 80 for SM_90 architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_reduce_async_bulk.rst#2025-04-23_snippet_30\n\nLANGUAGE: cuda\nCODE:\n```\n// cp.reduce.async.bulk.dst.src.bulk_group.op.type  [dstMem], [srcMem], size; // 4. PTX ISA 80, SM_90\n// .dst       = { .global }\n// .src       = { .shared::cta }\n// .type      = { .f64 }\n// .op        = { .add }\ntemplate <typename = void>\n__device__ static inline void cp_reduce_async_bulk(\n  cuda::ptx::space_global_t,\n  cuda::ptx::space_shared_t,\n  cuda::ptx::op_add_t,\n  double* dstMem,\n  const double* srcMem,\n  uint32_t size);\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Bulk Copy from Global to Shared CTA Memory with Barrier (CUDA)\nDESCRIPTION: Performs an asynchronous bulk copy from global memory to shared CTA memory, completing a memory barrier transaction. This operation is available from PTX ISA 86 and SM_90.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_async_bulk.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline void cp_async_bulk(\n  cuda::ptx::space_shared_t,\n  cuda::ptx::space_global_t,\n  void* dstMem,\n  const void* srcMem,\n  const uint32_t& size,\n  uint64_t* smem_bar);\n```\n\n----------------------------------------\n\nTITLE: Defining CTA Group Tensor Core Function for 128x256b b8x16 b4x16 p64 Variant - CUDA\nDESCRIPTION: This CUDA device inline template function is defined for a CTA group of size 128x256b, with block size and pipeline parameters (b8x16, b4x16, p64), enabling tensor core operations via PTX dot_cta_group abstraction. It generalizes the hardware call for different group types and requires proper PTX (SM_100a/SM_101a), a valid cta_group_t, a 32-bit address, and a 64-bit descriptor. Inputs follow the template type, all results are side-effect only.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_cp.rst#2025-04-23_snippet_6\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.cp.cta_group.128x256b.b8x16.b4x16_p64 [taddr], s_desc; // PTX ISA 86, SM_100a, SM_101a\\n// .cta_group = { .cta_group::1, .cta_group::2 }\\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\\n__device__ static inline void tcgen05_cp_128x256b_b8x16_b4x16_p64(\\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\\n  uint32_t taddr,\\n  uint64_t s_desc);\n```\n\n----------------------------------------\n\nTITLE: 1D Cluster Shared to Global Memory Bulk Tensor Copy\nDESCRIPTION: Device function for copying 1D tensor data from global to cluster shared memory with barrier synchronization. Requires SM_90 architecture and PTX ISA 80.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_async_bulk_tensor.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline void cp_async_bulk_tensor(\n  cuda::ptx::space_cluster_t,\n  cuda::ptx::space_global_t,\n  void* dstMem,\n  const void* tensorMap,\n  const int32_t (&tensorCoords)[1],\n  uint64_t* smem_bar);\n```\n\n----------------------------------------\n\nTITLE: Main Function for Vector Addition Example in CUDA\nDESCRIPTION: Implements the main function for the vector addition example using work stealing. It sets up the data, launches the kernel, and verifies the results.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/work_stealing.rst#2025-04-23_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nint main() {\n int N = 10000;\n int *a, *b, *c;\n cudaMallocManaged(&a, N * sizeof(int));\n cudaMallocManaged(&b, N * sizeof(int));\n cudaMallocManaged(&c, N * sizeof(int));\n for (int i = 0; i < N; ++i) {\n   a[i] = i;\n   b[i] = 1;\n   c[i] = 0;\n }\n\n const int threads_per_block = 256;\n const int blocks_per_grid = cuda::ceil_div(N, threads_per_block);\n\n vec_add<<<blocks_per_grid, threads_per_block>>>(a, b, c, N);\n cudaDeviceSynchronize();\n\n bool success = true;\n for (int i = 0; i < N; ++i) {\n   if (c[i] != (1 + i)) {\n std::cerr << \"ERROR \" << i << \", \" << c[i] << std::endl;\n success = false;\n   }\n }\n cudaFree(a);\n cudaFree(b);\n cudaFree(c);\n\n return success? 0 : 1;\n}\n```\n\n----------------------------------------\n\nTITLE: Using CUDA Compiler Version Checks in CCCL\nDESCRIPTION: Shows how to use the _CCCL_CUDA_COMPILER macro to check for specific CUDA compiler versions. Enables conditional compilation based on the CUDA compiler type and version.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cccl/development/macro.rst#2025-04-23_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n_CCCL_CUDA_COMPILER(NVCC, <, 12, 3)\n_CCCL_CUDA_COMPILER(CLANG, >=, 14)\n```\n\n----------------------------------------\n\nTITLE: Declaring Load-Reduce Minimum Operation (acquire/cluster/global/u32) in CUDA C++\nDESCRIPTION: This device-side inline template function supports atomic load and min reduction across global 32-bit values, using acquire or relaxed synchronization semantics and several memory scopes, including cluster-based. It is a part of a family of overloaded reduction functions tuned for concurrency and scope-specific behavior on recent GPU architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_6\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.u32 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .min }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline uint32_t multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_min_t,\n  const uint32_t* addr);\n```\n\n----------------------------------------\n\nTITLE: CTA Group 1 Asynchronous Bulk Tensor Copy (5D)\nDESCRIPTION: Template function for copying 5D tensor data using CTA group 1 configuration. Supports cluster-based shared memory and includes barrier synchronization.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_async_bulk_tensor_multicast.rst#2025-04-23_snippet_7\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void cp_async_bulk_tensor(\n  cuda::ptx::space_cluster_t,\n  cuda::ptx::space_global_t,\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  void* dstMem,\n  const void* tensorMap,\n  const int32_t (&tensorCoords)[5],\n  uint64_t* smem_bar,\n  const uint16_t& ctaMask);\n```\n\n----------------------------------------\n\nTITLE: Accessing Cluster-related Information in CUDA\nDESCRIPTION: These functions retrieve cluster-related information such as cluster ID components and dimensions. They use inline PTX instructions to access the special registers.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/get_sreg.rst#2025-04-23_snippet_7\n\nLANGUAGE: cuda\nCODE:\n```\n// mov.pred sreg_value, %%is_explicit_cluster; // PTX ISA 78, SM_90\ntemplate <typename = void>\n__device__ static inline bool get_sreg_is_explicit_cluster();\n```\n\nLANGUAGE: cuda\nCODE:\n```\n// mov.u32 sreg_value, %%clusterid.x; // PTX ISA 78, SM_90\ntemplate <typename = void>\n__device__ static inline uint32_t get_sreg_clusterid_x();\n```\n\nLANGUAGE: cuda\nCODE:\n```\n// mov.u32 sreg_value, %%clusterid.y; // PTX ISA 78, SM_90\ntemplate <typename = void>\n__device__ static inline uint32_t get_sreg_clusterid_y();\n```\n\nLANGUAGE: cuda\nCODE:\n```\n// mov.u32 sreg_value, %%clusterid.z; // PTX ISA 78, SM_90\ntemplate <typename = void>\n__device__ static inline uint32_t get_sreg_clusterid_z();\n```\n\nLANGUAGE: cuda\nCODE:\n```\n// mov.u32 sreg_value, %%nclusterid.x; // PTX ISA 78, SM_90\ntemplate <typename = void>\n__device__ static inline uint32_t get_sreg_nclusterid_x();\n```\n\nLANGUAGE: cuda\nCODE:\n```\n// mov.u32 sreg_value, %%nclusterid.y; // PTX ISA 78, SM_90\ntemplate <typename = void>\n__device__ static inline uint32_t get_sreg_nclusterid_y();\n```\n\nLANGUAGE: cuda\nCODE:\n```\n// mov.u32 sreg_value, %%nclusterid.z; // PTX ISA 78, SM_90\ntemplate <typename = void>\n__device__ static inline uint32_t get_sreg_nclusterid_z();\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Bulk MIN Reduction (Unsigned 32-bit) in CUDA\nDESCRIPTION: Performs an asynchronous bulk MIN reduction operation from CTA-shared to cluster-shared memory for unsigned 32-bit integers. Uses a memory barrier for synchronization.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_reduce_async_bulk.rst#2025-04-23_snippet_3\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline void cp_reduce_async_bulk(\n  cuda::ptx::space_cluster_t,\n  cuda::ptx::space_shared_t,\n  cuda::ptx::op_min_t,\n  uint32_t* dstMem,\n  const uint32_t* srcMem,\n  uint32_t size,\n  uint64_t* rdsmem_bar);\n```\n\n----------------------------------------\n\nTITLE: Defining PTX L1 Last Eviction Store With L2 Cache Hint - CUDA\nDESCRIPTION: Declares CUDA device inline function templates for PTX memory store operations using L1::evict_last semantics with L2::cache_hint, specialized for 8, 16, 32, 64, and 128 bit values. Each function takes a pointer, store value, and 64-bit cache_policy, with SFINAE ensuring the appropriate overload is used. Used for advanced CUDA memory control with specific L2 caching hints on supported PTX ISA architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/st.rst#2025-04-23_snippet_8\n\nLANGUAGE: cuda\nCODE:\n```\n// st.space.L1::evict_last.L2::cache_hint.b8 [addr], src, cache_policy; // PTX ISA 74, SM_80\n// .space     = { .global }\ntemplate <typename B8, enable_if_t<sizeof(B8) == 1, bool> = true>\n__device__ static inline void st_L1_evict_last_L2_cache_hint(\n  cuda::ptx::space_global_t,\n  B8* addr,\n  B8 src,\n  uint64_t cache_policy);\n```\n\nLANGUAGE: cuda\nCODE:\n```\n// st.space.L1::evict_last.L2::cache_hint.b16 [addr], src, cache_policy; // PTX ISA 74, SM_80\n// .space     = { .global }\ntemplate <typename B16, enable_if_t<sizeof(B16) == 2, bool> = true>\n__device__ static inline void st_L1_evict_last_L2_cache_hint(\n  cuda::ptx::space_global_t,\n  B16* addr,\n  B16 src,\n  uint64_t cache_policy);\n```\n\nLANGUAGE: cuda\nCODE:\n```\n// st.space.L1::evict_last.L2::cache_hint.b32 [addr], src, cache_policy; // PTX ISA 74, SM_80\n// .space     = { .global }\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline void st_L1_evict_last_L2_cache_hint(\n  cuda::ptx::space_global_t,\n  B32* addr,\n  B32 src,\n  uint64_t cache_policy);\n```\n\nLANGUAGE: cuda\nCODE:\n```\n// st.space.L1::evict_last.L2::cache_hint.b64 [addr], src, cache_policy; // PTX ISA 74, SM_80\n// .space     = { .global }\ntemplate <typename B64, enable_if_t<sizeof(B64) == 8, bool> = true>\n__device__ static inline void st_L1_evict_last_L2_cache_hint(\n  cuda::ptx::space_global_t,\n  B64* addr,\n  B64 src,\n  uint64_t cache_policy);\n```\n\nLANGUAGE: cuda\nCODE:\n```\n// st.space.L1::evict_last.L2::cache_hint.b128 [addr], src, cache_policy; // PTX ISA 83, SM_80\n// .space     = { .global }\ntemplate <typename B128, enable_if_t<sizeof(B128) == 16, bool> = true>\n__device__ static inline void st_L1_evict_last_L2_cache_hint(\n  cuda::ptx::space_global_t,\n  B128* addr,\n  B128 src,\n  uint64_t cache_policy);\n```\n\n----------------------------------------\n\nTITLE: Copying 1D Tensor from Global to Shared Cluster Memory in CUDA\nDESCRIPTION: This function performs an asynchronous bulk copy of a 1D tensor from global memory to shared cluster memory. It uses a memory barrier for synchronization and supports multicast operations across the cluster.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_async_bulk_tensor_multicast.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline void cp_async_bulk_tensor(\n  cuda::ptx::space_cluster_t,\n  cuda::ptx::space_global_t,\n  void* dstMem,\n  const void* tensorMap,\n  const int32_t (&tensorCoords)[1],\n  uint64_t* smem_bar,\n  const uint16_t& ctaMask);\n```\n\n----------------------------------------\n\nTITLE: Defining PTX L1 Last Eviction Store Instructions - CUDA\nDESCRIPTION: Declares device-side inline function templates for PTX global store instructions with L1::evict_last semantics, supporting 8, 16, 32, 64, and 128 bit data types. Template constraints ensure only valid types are used, expecting a PTX space parameter, pointer target, and store value. These variants offer explicit memory control for optimizing cache line eviction on supported NVIDIA architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/st.rst#2025-04-23_snippet_7\n\nLANGUAGE: cuda\nCODE:\n```\n// st.space.L1::evict_last.b8 [addr], src; // PTX ISA 74, SM_70\n// .space     = { .global }\ntemplate <typename B8, enable_if_t<sizeof(B8) == 1, bool> = true>\n__device__ static inline void st_L1_evict_last(\n  cuda::ptx::space_global_t,\n  B8* addr,\n  B8 src);\n```\n\nLANGUAGE: cuda\nCODE:\n```\n// st.space.L1::evict_last.b16 [addr], src; // PTX ISA 74, SM_70\n// .space     = { .global }\ntemplate <typename B16, enable_if_t<sizeof(B16) == 2, bool> = true>\n__device__ static inline void st_L1_evict_last(\n  cuda::ptx::space_global_t,\n  B16* addr,\n  B16 src);\n```\n\nLANGUAGE: cuda\nCODE:\n```\n// st.space.L1::evict_last.b32 [addr], src; // PTX ISA 74, SM_70\n// .space     = { .global }\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline void st_L1_evict_last(\n  cuda::ptx::space_global_t,\n  B32* addr,\n  B32 src);\n```\n\nLANGUAGE: cuda\nCODE:\n```\n// st.space.L1::evict_last.b64 [addr], src; // PTX ISA 74, SM_70\n// .space     = { .global }\ntemplate <typename B64, enable_if_t<sizeof(B64) == 8, bool> = true>\n__device__ static inline void st_L1_evict_last(\n  cuda::ptx::space_global_t,\n  B64* addr,\n  B64 src);\n```\n\nLANGUAGE: cuda\nCODE:\n```\n// st.space.L1::evict_last.b128 [addr], src; // PTX ISA 83, SM_70\n// .space     = { .global }\ntemplate <typename B128, enable_if_t<sizeof(B128) == 16, bool> = true>\n__device__ static inline void st_L1_evict_last(\n  cuda::ptx::space_global_t,\n  B128* addr,\n  B128 src);\n```\n\n----------------------------------------\n\nTITLE: Setting Up CUDA Samples Project with CMake\nDESCRIPTION: Initializes a CMake project for CUDA samples with minimum version requirement and project setup.\nSOURCE: https://github.com/nvidia/cccl/blob/main/examples/cudax/CMakeLists.txt#2025-04-23_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.15 FATAL_ERROR)\n\nproject(CUDAX_SAMPLES CUDA CXX)\n```\n\n----------------------------------------\n\nTITLE: Implementing MMA collector with shared memory input for matrix A in CUDA\nDESCRIPTION: This function template implements an MMA collector operation where matrix A is stored in shared memory (tmem) instead of using a descriptor. It supports various data types and includes zero column masking.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_29\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_tmem_a_collector_b1_discard(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint32_t a_tmem,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d,\n  uint64_t zero_column_mask_desc);\n```\n\n----------------------------------------\n\nTITLE: Declaring Load-Reduce Minimum Operation (relaxed/cluster/global/u32) in CUDA C++\nDESCRIPTION: Provides a device inline template function for loading and reducing (min) a 32-bit unsigned integer from global memory, synchronizing using relaxed or acquire semantics at various memory scopes (cluster, cta, gpu, sys). It utilizes PTX types as template parameters for fine-grained semantic and scope control. Requires corresponding PTX definitions and is designed for SM_90 hardware platforms.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_2\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.u32 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .min }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline uint32_t multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_min_t,\n  const uint32_t* addr);\n```\n\n----------------------------------------\n\nTITLE: CTA Group 1 Multicast Barrier Commit Operation\nDESCRIPTION: Template device function for committing to a shared memory barrier with multicast support in CTA group 1. Includes CTA mask parameter for selective synchronization.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_commit.rst#2025-04-23_snippet_2\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.commit.cta_group.mbarrier::arrive::one.shared::cluster.multicast::cluster.b64 [smem_bar], ctaMask; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1, .cta_group::2 }\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_commit_multicast(\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint64_t* smem_bar,\n  uint16_t ctaMask);\n```\n\n----------------------------------------\n\nTITLE: Configuring, Building, and Installing CUDA Parallel Python Package - CMake\nDESCRIPTION: This CMake script initializes the build environment for a CUDA-oriented Python package named cuda_parallel. It checks and finds CUDAToolkit and Python3 with development modules, sets up project options, and manages Cython code generation by running the Cython command as a custom build step. Custom include and module scripts configure library targets, dependencies, and output directories, and the script adds install directives for system integration. Key parameters include locations for source and output files, Cython build flags, enabling or disabling parallel build features, and installation paths. The script expects CMake 3.21+, CUDA toolkit, a Python3 interpreter, and Cython installed in the Python environment.\nSOURCE: https://github.com/nvidia/cccl/blob/main/python/cuda_parallel/CMakeLists.txt#2025-04-23_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.21)\n\nproject(\n    cuda_parallel\n#    VERSION ${SKBUILD_PROJECT_VERSION}\n    DESCRIPTION \"Python package cuda_parallel\"\n    LANGUAGES CUDA CXX C\n)\n\nfind_package(CUDAToolkit)\n\nset(_cccl_root ../..)\n\ninclude(${_cccl_root}/cmake/AppendOptionIfAvailable.cmake)\ninclude(${_cccl_root}/cmake/CCCLConfigureTarget.cmake)\ninclude(${_cccl_root}/cmake/CCCLBuildCompilerTargets.cmake)\ncccl_build_compiler_targets()\n\nset(CCCL_ENABLE_C ON)\nset(CCCL_C_PARALLEL_LIBRARY_OUTPUT_DIRECTORY ${SKBUILD_PROJECT_NAME})\nadd_subdirectory(${_cccl_root} _parent_cccl)\n\ninstall(\n    TARGETS cccl.c.parallel\n    DESTINATION cuda/parallel/experimental/cccl\n)\n\nfind_package(Python3 COMPONENTS Interpreter Development.Module REQUIRED)\n\nget_filename_component(_python_path \"${Python3_EXECUTABLE}\" PATH)\n\nset(CYTHON_version_command \"${Python3_EXECUTABLE}\" -m cython --version)\nexecute_process(COMMAND ${CYTHON_version_command}\n    OUTPUT_VARIABLE CYTHON_version_output\n    ERROR_VARIABLE CYTHON_version_error\n    RESULT_VARIABLE CYTHON_version_result\n    OUTPUT_STRIP_TRAILING_WHITESPACE\n    ERROR_STRIP_TRAILING_WHITESPACE\n)\n\nif(NOT ${CYTHON_version_result} EQUAL 0)\n    set(_error_msg \"Command \\\"${CYTHON_version_command}\\\" failed with\")\n    set(_error_msg \"${_error_msg} output:\\n${CYTHON_version_error}\")\n    message(FATAL_ERROR \"${_error_msg}\")\nelse()\n    if(\"${CYTHON_version_output}\" MATCHES \"^[Cc]ython version ([^,]+)\")\n        set(CYTHON_VERSION \"${CMAKE_MATCH_1}\")\n    else()\n        if(\"${CYTHON_version_error}\" MATCHES \"^[Cc]ython version ([^,]+)\")\n            set(CYTHON_VERSION \"${CMAKE_MATCH_1}\")\n        endif()\n    endif()\nendif()\n\n# -3 generates source for Python 3\n# -M generates depfile\n# -t cythonizes if PYX is newer than preexisting output\n# -w sets working directory\nset(CYTHON_FLAGS \"-3 -M -t -w \\\"${cuda_parallel_SOURCE_DIR}\\\"\")\nstring(REGEX REPLACE \" \" \";\" CYTHON_FLAGS_LIST \"${CYTHON_FLAGS}\")\n\nmessage(STATUS \"Using Cython ${CYTHON_VERSION}\")\nset(pyx_source_file \"${cuda_parallel_SOURCE_DIR}/cuda/parallel/experimental/_bindings.pyx\")\nset(_generated_extension_src \"${cuda_parallel_BINARY_DIR}/_bindings.c\")\nset(_depfile \"${cuda_parallel_BINARY_DIR}/_bindings.c.dep\")\nadd_custom_command(\n    OUTPUT \"${_generated_extension_src}\"\n    COMMAND \"${Python3_EXECUTABLE}\" -m cython\n    ARGS ${CYTHON_FLAGS_LIST} \"${pyx_source_file}\" --output-file ${_generated_extension_src}\n    DEPENDS \"${pyx_source_file}\"\n    DEPFILE \"${_depfile}\"\n)\nset_source_files_properties(\"${_generated_extension_src}\" PROPERTIES GENERATED TRUE)\nadd_custom_target(cythonize_bindings ALL\n    DEPENDS \"${_generated_extension_src}\"\n)\n\nPython3_add_library(_bindings MODULE WITH_SOABI \"${_generated_extension_src}\")\nadd_dependencies(_bindings cythonize_bindings)\ntarget_link_libraries(_bindings PRIVATE cccl.c.parallel CUDA::cuda_driver)\nset_target_properties(_bindings PROPERTIES INSTALL_RPATH \"$ORIGIN/cccl\")\n\ninstall(TARGETS _bindings DESTINATION cuda/parallel/experimental)\n\n```\n\n----------------------------------------\n\nTITLE: Specifying Thrust Version Requirements\nDESCRIPTION: Shows how to specify version requirements when finding the Thrust package, either requesting a specific version or requiring an exact match.\nSOURCE: https://github.com/nvidia/cccl/blob/main/lib/cmake/thrust/README.md#2025-04-23_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nfind_package(Thrust 1.9.10)\n```\n\nLANGUAGE: cmake\nCODE:\n```\nfind_package(Thrust 1.9.10.1 EXACT)\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Shape for parallel_for in C++\nDESCRIPTION: Demonstrates how to define a custom shape class that can be used as an indexable shape for parallel_for. The class must define an inner type coords_t and a member function index_to_coords.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_34\n\nLANGUAGE: cpp\nCODE:\n```\ntemplate <typename I>\nclass shape_of<I> {\n    ...\npublic:\n    using coords_t = ...;\n\n    // This transforms a 1D index into a coordinate\n    __device__ __host__ coords_t index_to_coords(size_t index) const {\n        ...\n    }\n};\n```\n\n----------------------------------------\n\nTITLE: Creating and Configuring CUDA Sample Executable\nDESCRIPTION: Defines a sample executable target, sets up CUDA compiler options for extended lambda and relaxed constexpr support, and links against the CUDA and CCCL libraries.\nSOURCE: https://github.com/nvidia/cccl/blob/main/examples/cudax_stf/CMakeLists.txt#2025-04-23_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nadd_executable(simple_stf simple_stf.cu)\n\ntarget_link_libraries(simple_stf PUBLIC cuda)\n\nif (CMAKE_CUDA_COMPILER)\n    target_compile_options(simple_stf PUBLIC $<$<COMPILE_LANGUAGE:CUDA>:--expt-relaxed-constexpr>)\n    target_compile_options(simple_stf PUBLIC $<$<COMPILE_LANGUAGE:CUDA>:--extended-lambda>)\nendif()\n\ntarget_link_libraries(simple_stf PRIVATE CCCL::CCCL CCCL::cudax)\n```\n\n----------------------------------------\n\nTITLE: Implementing multimem.ld_reduce for acquire CTA global add operations on 64-bit integers\nDESCRIPTION: Template implementation for multimem atomic load-and-reduce operations with acquire semantics, CTA scope, global memory, and add operation on 64-bit integers. Compatible with PTX ISA 81 on SM_90 architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_45\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.u64 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .add }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline int64_t multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_add_t,\n  const int64_t* addr);\n```\n\n----------------------------------------\n\nTITLE: Storing 16x128b Data with tcgen05 in CUDA (128 values)\nDESCRIPTION: This function performs a 16x128b store operation using tcgen05. It takes a 32-bit address and an array of 128 B32 values as input. The function is constrained to B32 types of 4 bytes.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_st.rst#2025-04-23_snippet_3\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.st.sync.aligned.16x128b.x64.b32 [taddr], values; // PTX ISA 86, SM_100a, SM_101a\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline void tcgen05_st_16x128b(\n  uint32_t taddr,\n  const B32 (&values)[128]);\n```\n\n----------------------------------------\n\nTITLE: Defining Device-Level Algorithm Structure in CUDA C++\nDESCRIPTION: This code defines the structure for device-level algorithms in CUB, including the API entry point and dispatch mechanism.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/developer_overview.rst#2025-04-23_snippet_10\n\nLANGUAGE: C++\nCODE:\n```\nstruct DeviceAlgorithm {\n  template <typename ...>\n  CUB_RUNTIME_FUNCTION static cudaError_t Algorithm(\n      void *d_temp_storage, size_t &temp_storage_bytes, ..., cudaStream_t stream = 0) {\n    // optional: minimal argument checking or setup to call dispatch layer\n    return DispatchAlgorithm<...>::Dispatch(d_temp_storage, temp_storage_bytes, ..., stream);\n  }\n};\n```\n\n----------------------------------------\n\nTITLE: Implementing Release Fence with Shared CTA to Cluster Synchronization in CUDA\nDESCRIPTION: This CUDA device function implements a memory fence operation with release semantics, synchronization restricted from shared memory in the CTA to the cluster scope. It is designed for use on SM_90 architecture and corresponds to PTX ISA 86.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/fence_sync_restrict.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n// fence.sem.sync_restrict::space.scope; // PTX ISA 86, SM_90\n// .sem       = { .release }\n// .space     = { .shared::cta }\n// .scope     = { .cluster }\ntemplate <typename = void>\n__device__ static inline void fence_sync_restrict(\n  cuda::ptx::sem_release_t,\n  cuda::ptx::space_shared_t,\n  cuda::ptx::scope_cluster_t);\n```\n\n----------------------------------------\n\nTITLE: Defining cuda::pipeline_shared_state Class Template in CUDA\nDESCRIPTION: This snippet defines the cuda::pipeline_shared_state class template, which is a storage type used to coordinate threads in a CUDA pipeline. It includes template parameters for thread scope and stage count, and defines constructor and destructor behavior.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/synchronization_primitives/pipeline/shared_state.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::thread_scope Scope, cuda::std::uint8_t StagesCount>\nclass cuda::pipeline_shared_state {\npublic:\n  __host__ __device__\n  pipeline_shared_state();\n\n  ~pipeline_shared_state() = default;\n\n  pipeline_shared_state(pipeline_shared_state const&) = delete;\n\n  pipeline_shared_state(pipeline_shared_state&&) = delete;\n};\n```\n\n----------------------------------------\n\nTITLE: Performing Asynchronous Bulk Reduction (ADD, u32) from Shared to Global Memory in CUDA\nDESCRIPTION: This CUDA C++ device function `cp_reduce_async_bulk` performs an asynchronous bulk addition reduction for unsigned 32-bit integers. It reduces data from a source array in shared memory (`srcMem`) to a destination array in global memory (`dstMem`). This overload requires PTX ISA 80 and SM_90 capability.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_reduce_async_bulk.rst#2025-04-23_snippet_18\n\nLANGUAGE: cuda\nCODE:\n```\n   // cp.reduce.async.bulk.dst.src.bulk_group.op.type  [dstMem], [srcMem], size; // 4. PTX ISA 80, SM_90\n   // .dst       = { .global }\n   // .src       = { .shared::cta }\n   // .type      = { .u32 }\n   // .op        = { .add }\n   template <typename = void>\n   __device__ static inline void cp_reduce_async_bulk(\n     cuda::ptx::space_global_t,\n     cuda::ptx::space_shared_t,\n     cuda::ptx::op_add_t,\n     uint32_t* dstMem,\n     const uint32_t* srcMem,\n     uint32_t size);\n```\n\n----------------------------------------\n\nTITLE: Using Property-Constrained Async Resource Reference in C++\nDESCRIPTION: Shows how to use cuda::mr::async_resource_ref with a specific property constraint. This allows for type-safe property access while maintaining the benefits of type erasure.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/memory_resource/resource_ref.rst#2025-04-23_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\nstruct required_alignment{};\nvoid* do_allocate_async_with_alignment(cuda::mr::async_resource_ref<required_alignment> resource, std::size_t size, cuda::stream_ref stream) {\n    return resource.allocate_async(size, cuda::mr::get_property(resource, required_alignment), stream);\n}\n```\n\n----------------------------------------\n\nTITLE: Visualizing the NVRTCC Workflow for Compilation and Execution\nDESCRIPTION: A Mermaid flowchart illustrating the operational flow of the `nvrtcc` tool when used within the `lit` testing framework. It depicts two main phases: the 'Build Pass' where source code is compiled into a fatbin (handling potential compile errors), and the 'Test Pass' where the precompiled fatbin is loaded and executed on the GPU (reporting test results or driver errors).\nSOURCE: https://github.com/nvidia/cccl/blob/main/libcudacxx/test/utils/nvidia/nvrtc/README.md#2025-04-23_snippet_1\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart TD;\n    compile{nvrtcc};\n    runfat{nvrtcc};\n\n    start[Execute lit]\n    build(Execute nvrtcc)\n    pass(Pass: Continue)\n    fail(Fail: Compile time failure)\n    save[Save fatbin]\n\n    load[Load fatbin]\n    testpass[Pass: No failures]\n    testfail[Fail: Report driver error]\n\n    save --> load;\n\n    subgraph Build Pass\n    start   --  lit -sv -Denable_nvrtc=true    --> build;\n    build   --  nvrtcc -x cu test.pass.cpp ... --> compile;\n    compile --> fail;\n    compile --> pass;\n    pass    --> save;\n    end\n\n\n    subgraph Test Pass\n    load -- nvrtcc test.pass.cpp.fatbin --> runfat;\n    runfat --> testpass\n    runfat --> testfail\n    end\n```\n\n----------------------------------------\n\nTITLE: 1D CTA Group Shared Memory Bulk Tensor Copy (Group 1)\nDESCRIPTION: Device function for copying 1D tensor data using CTA group 1 configuration. Supports SM_100a and SM_101a architectures with PTX ISA 86.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_async_bulk_tensor.rst#2025-04-23_snippet_2\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void cp_async_bulk_tensor(\n  cuda::ptx::space_shared_t,\n  cuda::ptx::space_global_t,\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  void* dstMem,\n  const void* tensorMap,\n  const int32_t (&tensorCoords)[1],\n  uint64_t* smem_bar);\n```\n\n----------------------------------------\n\nTITLE: Compiling CUDA Thrust Example with NVCC - Bash\nDESCRIPTION: This snippet demonstrates how to compile the 'norm' CUDA example program using the NVCC compiler. To use this command, ensure CUDA Toolkit and Thrust are installed and available in your environment. The 'norm.cu' source file is compiled to generate an executable named 'norm'; input is the command, and output is the compiled binary. Ensure all file paths are correct, and dependencies are satisfied for successful compilation.\nSOURCE: https://github.com/nvidia/cccl/blob/main/thrust/examples/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ nvcc norm.cu -o norm\n```\n\n----------------------------------------\n\nTITLE: Accessing Thread ID Components in CUDA\nDESCRIPTION: These functions retrieve the x, y, and z components of the thread ID within a block. They use inline PTX instructions to access the special registers.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/get_sreg.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\n// mov.u32 sreg_value, %%tid.x; // PTX ISA 20\ntemplate <typename = void>\n__device__ static inline uint32_t get_sreg_tid_x();\n```\n\nLANGUAGE: cuda\nCODE:\n```\n// mov.u32 sreg_value, %%tid.y; // PTX ISA 20\ntemplate <typename = void>\n__device__ static inline uint32_t get_sreg_tid_y();\n```\n\nLANGUAGE: cuda\nCODE:\n```\n// mov.u32 sreg_value, %%tid.z; // PTX ISA 20\ntemplate <typename = void>\n__device__ static inline uint32_t get_sreg_tid_z();\n```\n\n----------------------------------------\n\nTITLE: Defining tcgen05_mma MMA Kernel Variant for Multiple Kinds/Groups (CUDA C++)\nDESCRIPTION: Defines a generic tcgen05_mma device function template for multiple dot kinds (f16, tf32, f8f6f4, i8) and CTA groups (1, 2) as used in PTX ISA 86 on SM_100a/101a architectures. The inputs include descriptors and control flags for MMA execution. This template enables flexible invocation of different PTX MMA variants without compile-time N32 or scale parameters.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma.rst#2025-04-23_snippet_5\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.cta_group.kind [d_tmem], a_desc, b_desc, idesc, enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\n// .cta_group = { .cta_group::1, .cta_group::2 }\ntemplate <cuda::ptx::dot_kind Kind, cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_mma(\n  cuda::ptx::kind_t<Kind> kind,\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Declaring Load-Reduce Minimum Operation (relaxed/cta/global/u32) in CUDA C++\nDESCRIPTION: This snippet defines a device inline function for performing a min-reduction load on 32-bit unsigned integers from global memory, parameterized by PTX semantic and scope (relaxed/acquire, cta/cluster/gpu/sys). The function takes a semaphore type, memory scope, an op_min tag, and a global address pointer. Suitable for use in parallel reduction scenarios with defined synchronization. Requires PTX type definitions for semaphores, scopes, and op_min.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.u32 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .min }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline uint32_t multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_min_t,\n  const uint32_t* addr);\n```\n\n----------------------------------------\n\nTITLE: 4D Tensor Copy: Global to Shared CTA Memory with CTA Group 1\nDESCRIPTION: Device function for asynchronous bulk tensor copy with CTA group 1 specification. Supports 4D tensors and includes barrier synchronization. Requires PTX ISA 86 and SM_100a/SM_101a.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_async_bulk_tensor.rst#2025-04-23_snippet_6\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void cp_async_bulk_tensor(\n  cuda::ptx::space_shared_t,\n  cuda::ptx::space_global_t,\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  void* dstMem,\n  const void* tensorMap,\n  const int32_t (&tensorCoords)[4],\n  uint64_t* smem_bar);\n```\n\n----------------------------------------\n\nTITLE: Configuring Math Library Example Sources\nDESCRIPTION: Defines CUDA examples that use mathematical libraries like CUBLAS and CUSOLVER for linear algebra operations.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cudax/examples/stf/CMakeLists.txt#2025-04-23_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nset(stf_example_mathlib_sources\n  linear_algebra/06-pdgemm.cu\n  linear_algebra/07-cholesky.cu\n  linear_algebra/07-potri.cu\n  linear_algebra/cg_csr.cu\n  linear_algebra/cg_dense_2D.cu\n  linear_algebra/strassen.cu\n)\n```\n\n----------------------------------------\n\nTITLE: CUDA Memory Copy Operation\nDESCRIPTION: Shows the usage of cudaMemcpyAsync with cuda::barrier objects for synchronized memory copies.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/releases/changelog.rst#2025-04-23_snippet_30\n\nLANGUAGE: C++\nCODE:\n```\ncudaMemcpyAsync\n```\n\n----------------------------------------\n\nTITLE: CTA Group Memory Allocation (Group 1)\nDESCRIPTION: Template function for allocating shared memory in CTA group 1. Takes destination pointer and number of columns as parameters. Compatible with PTX ISA 86 and SM_100a/101a architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_alloc.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.alloc.cta_group.sync.aligned.shared::cta.b32 [dst], nCols; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1, .cta_group::2 }\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_alloc(\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t* dst,\n  const uint32_t& nCols);\n```\n\n----------------------------------------\n\nTITLE: Accessing Block ID Components in CUDA\nDESCRIPTION: These functions retrieve the x, y, and z components of the block ID within the grid. They use inline PTX instructions to access the special registers.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/get_sreg.rst#2025-04-23_snippet_3\n\nLANGUAGE: cuda\nCODE:\n```\n// mov.u32 sreg_value, %%ctaid.x; // PTX ISA 20\ntemplate <typename = void>\n__device__ static inline uint32_t get_sreg_ctaid_x();\n```\n\nLANGUAGE: cuda\nCODE:\n```\n// mov.u32 sreg_value, %%ctaid.y; // PTX ISA 20\ntemplate <typename = void>\n__device__ static inline uint32_t get_sreg_ctaid_y();\n```\n\nLANGUAGE: cuda\nCODE:\n```\n// mov.u32 sreg_value, %%ctaid.z; // PTX ISA 20\ntemplate <typename = void>\n__device__ static inline uint32_t get_sreg_ctaid_z();\n```\n\n----------------------------------------\n\nTITLE: Disabling __half/__nv_bfloat16 Support in C++\nDESCRIPTION: Defines preprocessor macros to explicitly disable support for `__half` and/or `__nv_bfloat16` types within libcu++, even if the CUDA toolkit version would normally enable it. `CCCL_DISABLE_FP16_SUPPORT` disables both `__half` and `__nv_bfloat16`. `CCCL_DISABLE_BF16_SUPPORT` disables only `__nv_bfloat16`.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/standard_api/numerics_library/complex.rst#2025-04-23_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\n// Disable __half and __nv_bfloat16 support\n#define CCCL_DISABLE_FP16_SUPPORT\n\n// Disable only __nv_bfloat16 support (requires FP16 to be enabled)\n#define CCCL_DISABLE_BF16_SUPPORT\n```\n\n----------------------------------------\n\nTITLE: Binary Find Operation for 32-bit Signed Integer\nDESCRIPTION: Device function template implementing bfind operation for int32_t type. Returns position of the highest set bit in a 32-bit signed integer.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/bfind.rst#2025-04-23_snippet_4\n\nLANGUAGE: cuda\nCODE:\n```\n// bfind.s32 dest, a_reg; // PTX ISA 20, SM_50\ntemplate <typename = void>\n__device__ static inline uint32_t bfind(\n  int32_t a_reg);\n```\n\n----------------------------------------\n\nTITLE: CUDA Atomic Operations\nDESCRIPTION: Example of atomic operations usage in CUDA with thread scope specifications.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/releases/changelog.rst#2025-04-23_snippet_32\n\nLANGUAGE: C++\nCODE:\n```\ncuda::atomic<T, Scope>\n```\n\n----------------------------------------\n\nTITLE: Defining Recursive Subdirectory Search Function in CMake\nDESCRIPTION: Defines a CMake function `get_recursive_subdirs` that finds all subdirectories within the `${CMAKE_CURRENT_LIST_DIR}/bench/` directory recursively. It uses `file(GLOB_RECURSE)` to find all items and then filters for directories, returning the list of directory paths in the output variable specified by the `subdirs` argument.\nSOURCE: https://github.com/nvidia/cccl/blob/main/thrust/benchmarks/CMakeLists.txt#2025-04-23_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(get_recursive_subdirs subdirs)\n  set(dirs)\n  file(GLOB_RECURSE contents\n    CONFIGURE_DEPENDS\n    LIST_DIRECTORIES ON\n    \"${CMAKE_CURRENT_LIST_DIR}/bench/*\"\n  )\n\n  foreach(test_dir IN LISTS contents)\n    if(IS_DIRECTORY \"${test_dir}\")\n      list(APPEND dirs \"${test_dir}\")\n    endif()\n  endforeach()\n\n  set(${subdirs} \"${dirs}\" PARENT_SCOPE)\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Implementing multimem.red.relaxed.cta.global.max.s64 Operation in CUDA PTX\nDESCRIPTION: Template function declaration for performing a 64-bit signed integer max reduction with relaxed semantics and CTA scope in global memory. This function is available for SM_90 and requires PTX ISA 8.1 or higher.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_red.rst#2025-04-23_snippet_10\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.red.sem.scope.global.op.s64 [addr], val; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .release }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .max }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline void multimem_red(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_max_t,\n  int64_t* addr,\n  int64_t val);\n```\n\n----------------------------------------\n\nTITLE: Binary Find Shift Amount for 32-bit Unsigned Integer\nDESCRIPTION: Device function template implementing bfind shift amount operation for uint32_t type. Returns shift amount for the highest set bit in a 32-bit unsigned integer.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/bfind.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n// bfind.shiftamt.u32 dest, a_reg; // PTX ISA 20, SM_50\ntemplate <typename = void>\n__device__ static inline uint32_t bfind_shiftamt(\n  uint32_t a_reg);\n```\n\n----------------------------------------\n\nTITLE: CTA Shared Memory Barrier Arrive with Count\nDESCRIPTION: Shared memory barrier arrive operation with count parameter for SM_90 architecture. Takes memory address and count, returns barrier state.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/mbarrier_arrive.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline uint64_t mbarrier_arrive(\n  uint64_t* addr,\n  const uint32_t& count);\n```\n\n----------------------------------------\n\nTITLE: Overriding SM Architectures for CUDA Sample Builds\nDESCRIPTION: Make command demonstrating how to override the SM architectures for which the sample will be built, using the SMS parameter with a space-delimited list of architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cudax/test/stf/cuda-samples/0_Introduction/vectorAdd/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ make SMS=\"50 60\"\n```\n\n----------------------------------------\n\nTITLE: CTA Scope Acquire Memory Barrier Test Wait\nDESCRIPTION: Memory barrier test wait with acquire semantics and CTA scope for PTX ISA 80 and SM_90. Includes scope and semantic parameters.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/mbarrier_test_wait.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_scope Scope>\n__device__ static inline bool mbarrier_test_wait(\n  cuda::ptx::sem_acquire_t,\n  cuda::ptx::scope_t<Scope> scope,\n  uint64_t* addr,\n  const uint64_t& state);\n```\n\n----------------------------------------\n\nTITLE: Querying Cluster Launch Cancellation Status in CUDA\nDESCRIPTION: Defines a device function to check if a cluster launch cancellation has occurred. It takes a 128-bit response from a cancellation attempt and returns a boolean.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/clusterlaunchcontrol.rst#2025-04-23_snippet_2\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename B128, enable_if_t<sizeof(B128) == 16, bool> = true>\n__device__ static inline bool clusterlaunchcontrol_query_cancel_is_canceled(\n  B128 try_cancel_response);\n```\n\n----------------------------------------\n\nTITLE: Aligned 16x64b Tensor Load with Single Output (CUDA)\nDESCRIPTION: Template function for loading 16x64b tensor data into a single 32-bit output array. Supports PTX ISA 86 and runs on SM_100a and SM_101a architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_ld.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.ld.sync.aligned.16x64b.x1.b32 out, [taddr]; // PTX ISA 86, SM_100a, SM_101a\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline void tcgen05_ld_16x64b(\n  B32 (&out)[1],\n  uint32_t taddr);\n```\n\n----------------------------------------\n\nTITLE: 5D Tensor Operations\nDESCRIPTION: Collection of device functions for 5D tensor operations including global to shared memory copies with various synchronization mechanisms. Supports different memory spaces and CTA groups.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_async_bulk_tensor.rst#2025-04-23_snippet_7\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline void cp_async_bulk_tensor(\n  cuda::ptx::space_global_t,\n  cuda::ptx::space_shared_t,\n  const void* tensorMap,\n  const int32_t (&tensorCoords)[5],\n  const void* srcMem);\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Bulk MIN Reduction (Signed 32-bit) in CUDA\nDESCRIPTION: Performs an asynchronous bulk MIN reduction operation from CTA-shared to cluster-shared memory for signed 32-bit integers. Uses a memory barrier for synchronization.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_reduce_async_bulk.rst#2025-04-23_snippet_8\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline void cp_reduce_async_bulk(\n  cuda::ptx::space_cluster_t,\n  cuda::ptx::space_shared_t,\n  cuda::ptx::op_min_t,\n  int32_t* dstMem,\n  const int32_t* srcMem,\n  uint32_t size,\n  uint64_t* rdsmem_bar);\n```\n\n----------------------------------------\n\nTITLE: Defining warp_shuffle_result struct in CUDA\nDESCRIPTION: Struct definition for warp_shuffle_result, which contains the shuffled data and a predicate indicating if the operation was successful.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/warp/warp_shuffle.rst#2025-04-23_snippet_4\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename T>\nstruct warp_shuffle_result {\n    T    data;\n    bool pred;\n\n    __device__ operator T() const { return data; }\n};\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Bulk Reduction Add Operation for bfloat16 in CUDA\nDESCRIPTION: This function performs an asynchronous bulk reduction addition operation on bfloat16 data, transferring from shared memory to global memory. It uses CUDA PTX ISA 80 for SM_90 architecture and includes a 'noftz' (no flush to zero) flag.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_reduce_async_bulk_bf16.rst#2025-04-23_snippet_2\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline void cp_reduce_async_bulk(\n  cuda::ptx::space_global_t,\n  cuda::ptx::space_shared_t,\n  cuda::ptx::op_add_t,\n  __nv_bfloat16* dstMem,\n  const __nv_bfloat16* srcMem,\n  uint32_t size);\n```\n\n----------------------------------------\n\nTITLE: Implementing Correct CUDA Access Property Usage\nDESCRIPTION: This CUDA kernel demonstrates correct usage of access properties. It shows proper association of properties with shared and global memory, as well as creating range and interleaved properties.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/memory_access_properties/access_property.rst#2025-04-23_snippet_4\n\nLANGUAGE: cuda\nCODE:\n```\n__global__ void correct(int* global) {\n    __shared__ int shmem;\n    cuda::associate_access_property(&shmem, cuda::access_property::shared{});\n\n    cuda::access_property global_range0(global, 0, sizeof(int), cuda::access_property::streaming{});\n    cuda::associate_access_property(global, global_range0);\n\n    cuda::access_property global_interleaved(cuda::access_property::streaming{}, 1.0);\n    cuda::associate_access_property(global, global_interleaved);\n\n    // Access properties can be constructed for any address range\n    cuda::access_property global_range1(global,  0, sizeof(int), cuda::access_property::streaming{});\n    cuda::access_property global_range2(nullptr, 0, sizeof(int), cuda::access_property::streaming{});\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing cuda::proclaim_return_type Function in CUDA\nDESCRIPTION: Definition of the proclaim_return_type function from the <cuda/functional> header. This function creates a forwarding call wrapper that uses the specified return type Ret instead of relying on automatic return type deduction.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/functional/proclaim_return_type.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <class Ret, class Fn>\n__host__ __device__\nunspecified<Ret, Fn> proclaim_return_type(Fn&& fn) {\n  return unspecified<Ret, Fn>{fn};\n}\n```\n\n----------------------------------------\n\nTITLE: Release Semantic Barrier Arrive with CTA/Cluster Scope\nDESCRIPTION: Memory barrier arrive operation with release semantics for shared CTA memory. Supports both CTA and cluster scopes.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/mbarrier_arrive.rst#2025-04-23_snippet_2\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_scope Scope>\n__device__ static inline uint64_t mbarrier_arrive(\n  cuda::ptx::sem_release_t,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::space_shared_t,\n  uint64_t* addr);\n```\n\n----------------------------------------\n\nTITLE: Executing the Timed Benchmark Region with NVBench\nDESCRIPTION: Sets up the timed region of the benchmark using NVBench's exec method, which will contain the actual CUB algorithm execution. This is configured to run on GPU without batching.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/tuning.rst#2025-04-23_snippet_7\n\nLANGUAGE: c++\nCODE:\n```\nstate.exec(nvbench::exec_tag::gpu | nvbench::exec_tag::no_batch,\n         [&](nvbench::launch &launch) {\n  dispatch_t::Dispatch(temp_storage,\n```\n\n----------------------------------------\n\nTITLE: 4D Tensor Copy: Global to Shared Cluster Memory with Barrier\nDESCRIPTION: Device function for asynchronous bulk tensor copy from global to shared cluster memory for 4D tensors with memory barrier completion tracking. Requires PTX ISA 80 and SM_90.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_async_bulk_tensor.rst#2025-04-23_snippet_4\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline void cp_async_bulk_tensor(\n  cuda::ptx::space_cluster_t,\n  cuda::ptx::space_global_t,\n  void* dstMem,\n  const void* tensorMap,\n  const int32_t (&tensorCoords)[4],\n  uint64_t* smem_bar);\n```\n\n----------------------------------------\n\nTITLE: Adding Vector Add Sample Executable\nDESCRIPTION: Adds the vector_add sample executable that demonstrates a simple CUDA kernel for adding vectors.\nSOURCE: https://github.com/nvidia/cccl/blob/main/examples/cudax/CMakeLists.txt#2025-04-23_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nadd_executable(vector_add vector_add/vector_add.cu)\ntarget_link_libraries(vector_add PUBLIC cudax_samples_interface)\n```\n\n----------------------------------------\n\nTITLE: Basic Barrier Cluster Wait Operation\nDESCRIPTION: Device function for basic cluster barrier wait operation. Requires PTX ISA 78 and SM_90. Implementation is volatile and clobbers memory.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/barrier_cluster.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline void barrier_cluster_wait();\n```\n\n----------------------------------------\n\nTITLE: Implementing multimem.red.release.cluster.global.max.s64 Operation in CUDA PTX\nDESCRIPTION: Template function declaration for performing a 64-bit signed integer max reduction with release semantics and cluster scope in global memory. This function is available for SM_90 and requires PTX ISA 8.1 or higher.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_red.rst#2025-04-23_snippet_15\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.red.sem.scope.global.op.s64 [addr], val; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .release }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .max }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline void multimem_red(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_max_t,\n  int64_t* addr,\n  int64_t val);\n```\n\n----------------------------------------\n\nTITLE: Implementing MMA Workspace Last Use Operations with TF32 Data Type in CUDA\nDESCRIPTION: Template function for the final tensor compute operation (lastuse) with TF32 data type. This variant handles tensor descriptors and supports zero column masking. Targets PTX ISA 86 on SM_100a and SM_101a architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_44\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.ws.cta_group.kind.collector::b2::lastuse [d_tmem], a_desc, b_desc, idesc, enable_input_d, zero_column_mask_desc; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1 }\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_collector_b2_lastuse(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d,\n  uint64_t zero_column_mask_desc);\n```\n\n----------------------------------------\n\nTITLE: Implementing multimem.red.release.cluster.global.add.u32 Operation in CUDA PTX\nDESCRIPTION: Template function declaration for performing a 32-bit unsigned integer addition reduction with release semantics and cluster scope in global memory. This function is available for SM_90 and requires PTX ISA 8.1 or higher.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_red.rst#2025-04-23_snippet_23\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.red.sem.scope.global.op.u32 [addr], val; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .release }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .add }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline void multimem_red(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_add_t,\n  uint32_t* addr,\n  uint32_t val);\n```\n\n----------------------------------------\n\nTITLE: Finding Recursive Subdirectories for Benchmarks in CMake\nDESCRIPTION: A function that discovers all subdirectories recursively under the 'bench' directory. It uses GLOB_RECURSE to find all directories and returns them through a variable passed by reference.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cub/benchmarks/CMakeLists.txt#2025-04-23_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(get_recursive_subdirs subdirs)\n  set(dirs)\n  file(GLOB_RECURSE contents\n    CONFIGURE_DEPENDS\n    LIST_DIRECTORIES ON\n    \"${CMAKE_CURRENT_LIST_DIR}/bench/*\"\n  )\n\n  foreach(test_dir IN LISTS contents)\n    if(IS_DIRECTORY \"${test_dir}\")\n      list(APPEND dirs \"${test_dir}\")\n    endif()\n  endforeach()\n\n  set(${subdirs} \"${dirs}\" PARENT_SCOPE)\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Bulk AND Reduction (32-bit) in CUDA\nDESCRIPTION: Performs an asynchronous bulk AND reduction operation from CTA-shared to cluster-shared memory for 32-bit data types. Uses a memory barrier for synchronization.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_reduce_async_bulk.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline void cp_reduce_async_bulk(\n  cuda::ptx::space_cluster_t,\n  cuda::ptx::space_shared_t,\n  cuda::ptx::op_and_op_t,\n  B32* dstMem,\n  const B32* srcMem,\n  uint32_t size,\n  uint64_t* rdsmem_bar);\n```\n\n----------------------------------------\n\nTITLE: Declaring Load-Reduce Minimum Operation (weak/global/u32) in CUDA C++\nDESCRIPTION: This snippet declares a device-side inline function template for performing a minimum-reduction load on 32-bit unsigned integers in global memory, utilizing a weak PTX semaphore for synchronization. It requires \"cuda::ptx::sem_weak_t\" and \"cuda::ptx::op_min_t\" as arguments, and takes a pointer to the global address as input. Returns the reduced uint32_t value as output. Used with SM_90 and PTX ISA 81 hardware. Requires including corresponding CUDA PTX type definitions.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.global.op.u32 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .weak }\n// .op        = { .min }\ntemplate <typename = void>\n__device__ static inline uint32_t multimem_ld_reduce(\n  cuda::ptx::sem_weak_t,\n  cuda::ptx::op_min_t,\n  const uint32_t* addr);\n```\n\n----------------------------------------\n\nTITLE: Binary Find Shift Amount for 32-bit Signed Integer\nDESCRIPTION: Device function template implementing bfind shift amount operation for int32_t type. Returns shift amount for the highest set bit in a 32-bit signed integer.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/bfind.rst#2025-04-23_snippet_5\n\nLANGUAGE: cuda\nCODE:\n```\n// bfind.shiftamt.s32 dest, a_reg; // PTX ISA 20, SM_50\ntemplate <typename = void>\n__device__ static inline uint32_t bfind_shiftamt(\n  int32_t a_reg);\n```\n\n----------------------------------------\n\nTITLE: Declaring Weak-Semantic Multimem Load-Reduce (min, s32) with CUDA PTX (C++)\nDESCRIPTION: This snippet declares a templated CUDA device function for atomic minimal reduction (min) operation on a 32-bit signed integer pointer in global memory. It uses the weak synchronization semantic, targeting PTX ISA 81, and requires no explicit scope. Dependencies include CUDA device compilation and access to the `cuda::ptx` namespace. Parameters are the weak semantic type, reduction operator (min), and input address. The function returns the reduced int32_t result.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_18\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.global.op.s32 dest, [addr]; // PTX ISA 81, SM_90\\n// .sem       = { .weak }\\n// .op        = { .min }\\ntemplate <typename = void>\\n__device__ static inline int32_t multimem_ld_reduce(\\n  cuda::ptx::sem_weak_t,\\n  cuda::ptx::op_min_t,\\n  const int32_t* addr);\n```\n\n----------------------------------------\n\nTITLE: Implementing multimem reduction bitwise OR operations in CUDA PTX\nDESCRIPTION: Template function declaration for multimem reduction operations performing atomic bitwise OR on 32-bit values in global memory. Supports different semantic models and memory scopes as specified in PTX ISA 81 for SM_90.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_red.rst#2025-04-23_snippet_33\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.red.sem.scope.global.op.b32 [addr], val; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .release }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .or }\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true, cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline void multimem_red(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_or_op_t,\n  B32* addr,\n  B32 val);\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Reference for PTX bfind Instruction\nDESCRIPTION: ReStructuredText markup defining documentation structure and external reference for the PTX bfind instruction. Includes a link to official NVIDIA documentation.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/bfind.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _libcudacxx-ptx-instructions-bfind:\n\nbfind\n=====\n\n-  PTX ISA:\n   `bfind <https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#integer-arithmetic-instructions-bfind>`__\n\n.. include:: generated/bfind.rst\n```\n\n----------------------------------------\n\nTITLE: Defining PTX L1 Unchanged Eviction Store With L2 Cache Hint - CUDA\nDESCRIPTION: Declares device-side inline function templates for global memory store (st) instructions with L1::evict_unchanged, L2::cache_hint in CUDA, specialized for 8, 16, 32, and 128-bit data types. Each variant uses SFINAE to constrain to the correct type size, and the function requires a PTX space, pointer, value, and a 64-bit cache policy argument. Intended for explicit memory/caching control on recent NVIDIA architectures (SM_80 and above). No side effects or return value.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/st.rst#2025-04-23_snippet_4\n\nLANGUAGE: cuda\nCODE:\n```\n// st.space.L1::evict_unchanged.L2::cache_hint.b8 [addr], src, cache_policy; // PTX ISA 74, SM_80\n// .space     = { .global }\ntemplate <typename B8, enable_if_t<sizeof(B8) == 1, bool> = true>\n__device__ static inline void st_L1_evict_unchanged_L2_cache_hint(\n  cuda::ptx::space_global_t,\n  B8* addr,\n  B8 src,\n  uint64_t cache_policy);\n```\n\nLANGUAGE: cuda\nCODE:\n```\n// st.space.L1::evict_unchanged.L2::cache_hint.b16 [addr], src, cache_policy; // PTX ISA 74, SM_80\n// .space     = { .global }\ntemplate <typename B16, enable_if_t<sizeof(B16) == 2, bool> = true>\n__device__ static inline void st_L1_evict_unchanged_L2_cache_hint(\n  cuda::ptx::space_global_t,\n  B16* addr,\n  B16 src,\n  uint64_t cache_policy);\n```\n\nLANGUAGE: cuda\nCODE:\n```\n// st.space.L1::evict_unchanged.L2::cache_hint.b32 [addr], src, cache_policy; // PTX ISA 74, SM_80\n// .space     = { .global }\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline void st_L1_evict_unchanged_L2_cache_hint(\n  cuda::ptx::space_global_t,\n  B32* addr,\n  B32 src,\n  uint64_t cache_policy);\n```\n\nLANGUAGE: cuda\nCODE:\n```\n// st.space.L1::evict_unchanged.L2::cache_hint.b128 [addr], src, cache_policy; // PTX ISA 83, SM_80\n// .space     = { .global }\ntemplate <typename B128, enable_if_t<sizeof(B128) == 16, bool> = true>\n__device__ static inline void st_L1_evict_unchanged_L2_cache_hint(\n  cuda::ptx::space_global_t,\n  B128* addr,\n  B128 src,\n  uint64_t cache_policy);\n```\n\n----------------------------------------\n\nTITLE: Performing Asynchronous Bulk Reduction (XOR, b32/b64) from Shared to Global Memory in CUDA\nDESCRIPTION: This CUDA C++ template function `cp_reduce_async_bulk` performs an asynchronous bulk bitwise XOR reduction. It reduces data from a source array in shared memory (`srcMem`) to a destination array in global memory (`dstMem`). The function is templated on the data type (`Type`, expected to be `.b32` or `.b64`) and requires PTX ISA 80 and SM_90 capability.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_reduce_async_bulk.rst#2025-04-23_snippet_15\n\nLANGUAGE: cuda\nCODE:\n```\n   // cp.reduce.async.bulk.dst.src.bulk_group.op.type  [dstMem], [srcMem], size; // 3. PTX ISA 80, SM_90\n   // .dst       = { .global }\n   // .src       = { .shared::cta }\n   // .type      = { .b32, .b64 }\n   // .op        = { .xor }\n   template <typename Type>\n   __device__ static inline void cp_reduce_async_bulk(\n     cuda::ptx::space_global_t,\n     cuda::ptx::space_shared_t,\n     cuda::ptx::op_xor_op_t,\n     Type* dstMem,\n     const Type* srcMem,\n     uint32_t size);\n```\n\n----------------------------------------\n\nTITLE: CUDA Stream Query with Atomic Flag\nDESCRIPTION: Demonstrates how a single cudaStreamQuery call doesn't guarantee device thread progress. Similar to previous example but with added stream query that doesn't ensure forward progress.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/execution_model.rst#2025-04-23_snippet_7\n\nLANGUAGE: cuda\nCODE:\n```\ncuda::atomic<int, cuda::thread_scope_system> flag = 0;\n__global__ void producer() { flag.store(1); }\nint main() {\n    cudaHostRegister(&flag, sizeof(flag));\n    producer<<<1,1>>>();\n    (void)cudaStreamQuery(0);\n    while (flag.load() == 0);\n    return cudaDeviceSynchronize();\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing multimem.red.relaxed.cta.global.add.u32 Operation in CUDA PTX\nDESCRIPTION: Template function declaration for performing a 32-bit unsigned integer addition reduction with relaxed semantics and CTA scope in global memory. This function is available for SM_90 and requires PTX ISA 8.1 or higher.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_red.rst#2025-04-23_snippet_18\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.red.sem.scope.global.op.u32 [addr], val; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .release }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .add }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline void multimem_red(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_add_t,\n  uint32_t* addr,\n  uint32_t val);\n```\n\n----------------------------------------\n\nTITLE: Global Memory Store with L1 Normal Eviction\nDESCRIPTION: Template functions for global memory stores with L1 cache normal eviction policy. These operations require SM_70+ and control how data is evicted from L1 cache.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/st.rst#2025-04-23_snippet_2\n\nLANGUAGE: cuda\nCODE:\n```\n// st.space.L1::evict_normal.b8 [addr], src; // PTX ISA 74, SM_70\ntemplate <typename B8, enable_if_t<sizeof(B8) == 1, bool> = true>\n__device__ static inline void st_L1_evict_normal(\n  cuda::ptx::space_global_t,\n  B8* addr,\n  B8 src);\n```\n\n----------------------------------------\n\nTITLE: Defining CUDA Memory Space MDSpan Types\nDESCRIPTION: Template definitions for host, device, and managed mdspan types with customizable element type, extents, layout policy, and accessor policy.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/mdspan/host_device_accessor.rst#2025-04-23_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\ntemplate <typename ElementType,\n          typename Extents,\n          typename LayoutPolicy   = cuda::std::layout_right,\n          typename AccessorPolicy = cuda::std::default_accessor<_ElementType>>\nusing host_mdspan = cuda::std::mdspan<ElementType, Extents, LayoutPolicy, host_accessor<AccessorPolicy>>;\n\ntemplate <typename ElementType,\n          typename Extents,\n          typename LayoutPolicy   = cuda::std::layout_right,\n          typename AccessorPolicy = cuda::std::default_accessor<_ElementType>>\nusing device_mdspan = cuda::std::mdspan<ElementType, Extents, LayoutPolicy, device_accessor<AccessorPolicy>>;\n\ntemplate <typename ElementType,\n          typename Extents,\n          typename LayoutPolicy   = cuda::std::layout_right,\n          typename AccessorPolicy = cuda::std::default_accessor<_ElementType>>\nusing managed_mdspan = cuda::std::mdspan<ElementType, Extents, LayoutPolicy, managed_accessor<AccessorPolicy>>;\n```\n\n----------------------------------------\n\nTITLE: Illustrating Range Property with Halo in C++\nDESCRIPTION: This code snippet demonstrates how to visualize a range property with primary and secondary (halo) ranges using ASCII art. It shows the layout of memory regions and their corresponding access properties.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/memory_access_properties/access_property.rst#2025-04-23_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n    ____________________________________________________________\n   |  halo / secondary | leading / primary   | halo / secondary |\n    ------------------------------------------------------------\n                       ^\n                       | ptr\n\n   |<-- halo_bytes  -->|<-- leading_bytes -->|<-- halo_bytes -->|\n                       |<--            total_bytes           -->|\n```\n\n----------------------------------------\n\nTITLE: Performing Asynchronous Bulk Reduction (INC, u32) from Shared to Global Memory in CUDA\nDESCRIPTION: This CUDA C++ device function `cp_reduce_async_bulk` performs an asynchronous bulk increment reduction for unsigned 32-bit integers. It reduces data from a source array in shared memory (`srcMem`) to a destination array in global memory (`dstMem`). This overload requires PTX ISA 80 and SM_90 capability.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_reduce_async_bulk.rst#2025-04-23_snippet_19\n\nLANGUAGE: cuda\nCODE:\n```\n   // cp.reduce.async.bulk.dst.src.bulk_group.op.type  [dstMem], [srcMem], size; // 4. PTX ISA 80, SM_90\n   // .dst       = { .global }\n   // .src       = { .shared::cta }\n   // .type      = { .u32 }\n   // .op        = { .inc }\n   template <typename = void>\n   __device__ static inline void cp_reduce_async_bulk(\n     cuda::ptx::space_global_t,\n     cuda::ptx::space_shared_t,\n     cuda::ptx::op_inc_t,\n     uint32_t* dstMem,\n     const uint32_t* srcMem,\n     uint32_t size);\n```\n\n----------------------------------------\n\nTITLE: Performing Asynchronous Bulk Reduction (DEC, u32) from Shared to Global Memory in CUDA\nDESCRIPTION: This CUDA C++ device function `cp_reduce_async_bulk` performs an asynchronous bulk decrement reduction for unsigned 32-bit integers. It reduces data from a source array in shared memory (`srcMem`) to a destination array in global memory (`dstMem`). This overload requires PTX ISA 80 and SM_90 capability.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_reduce_async_bulk.rst#2025-04-23_snippet_20\n\nLANGUAGE: cuda\nCODE:\n```\n   // cp.reduce.async.bulk.dst.src.bulk_group.op.type  [dstMem], [srcMem], size; // 4. PTX ISA 80, SM_90\n   // .dst       = { .global }\n   // .src       = { .shared::cta }\n   // .type      = { .u32 }\n   // .op        = { .dec }\n   template <typename = void>\n   __device__ static inline void cp_reduce_async_bulk(\n     cuda::ptx::space_global_t,\n     cuda::ptx::space_shared_t,\n     cuda::ptx::op_dec_t,\n     uint32_t* dstMem,\n     const uint32_t* srcMem,\n     uint32_t size);\n```\n\n----------------------------------------\n\nTITLE: Implementing red.async Emulation for 64-bit Operations in CUDA PTX\nDESCRIPTION: Template function that emulates red.async.add.s64 operation since it's not natively exposed in PTX (as of CTK 12.3). The function implements atomic reduction operations for 64-bit signed integers in cluster shared memory.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/red_async.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\n// red.async.relaxed.cluster.shared::cluster.mbarrier::complete_tx::bytes{.op}.u64  [dest], value, [remote_bar]; // .u64 intentional PTX ISA 81, SM_90\n// .op        = { .add }\ntemplate <typename=void>\n__device__ static inline void red_async(\n  cuda::ptx::op_add_t,\n  int64_t* dest,\n  const int64_t& value,\n  int64_t* remote_bar);\n```\n\n----------------------------------------\n\nTITLE: Defining CTA Group Tensor Core Function for 4x256b b8x16 b4x16 p64 Variant - CUDA\nDESCRIPTION: This device-level template function abstracts the low-level PTX instruction for a CTA group shaped 4x256b and configured with b8x16, b4x16, and p64 parameterization. Relying on dot_cta_group template parameterization, it receives the cta_group object, a tile address (uint32_t), and the operation descriptor (uint64_t). Requires CUDA PTX support ISA 86, and facilitates tensor operations for different CTA group layouts.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_cp.rst#2025-04-23_snippet_7\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.cp.cta_group.4x256b.b8x16.b4x16_p64 [taddr], s_desc; // PTX ISA 86, SM_100a, SM_101a\\n// .cta_group = { .cta_group::1, .cta_group::2 }\\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\\n__device__ static inline void tcgen05_cp_4x256b_b8x16_b4x16_p64(\\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\\n  uint32_t taddr,\\n  uint64_t s_desc);\n```\n\n----------------------------------------\n\nTITLE: Defining CUB Tuning Parameters in C++\nDESCRIPTION: Shows how tuning parameters are defined in CUB benchmark source code using range comments.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/tuning.rst#2025-04-23_snippet_12\n\nLANGUAGE: c++\nCODE:\n```\n// %RANGE% TUNE_ITEMS_PER_THREAD ipt 7:24:1\n// %RANGE% TUNE_THREADS_PER_BLOCK tpb 128:1024:32\n```\n\n----------------------------------------\n\nTITLE: Retrieving Complete First CTA ID in CUDA\nDESCRIPTION: Defines a device function to get the complete first CTA ID (all dimensions) from a cancellation response. It takes a 128-bit response and fills a 4-element array of 32-bit values.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/clusterlaunchcontrol.rst#2025-04-23_snippet_6\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true, typename B128, enable_if_t<sizeof(B128) == 16, bool> = true>\n__device__ static inline void clusterlaunchcontrol_query_cancel_get_first_ctaid(\n  B32 (&block_dim)[4],\n  B128 try_cancel_response);\n```\n\n----------------------------------------\n\nTITLE: Implementing MMA Block Scale Vector Operation in CUDA\nDESCRIPTION: This function performs a matrix multiply-accumulate operation with block scaling and vector operations. It uses CUDA's tensor cores and CTA groups for high-performance computing. The function takes parameters for data descriptors, memory locations, and scaling factors.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma.rst#2025-04-23_snippet_51\n\nLANGUAGE: CUDA\nCODE:\n```\n__device__ static inline void tcgen05_mma_block_scale_vec_4x_tmem_a_collector_a_discard(\n  cuda::ptx::kind_mxf4nvf4_t,\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  uint32_t scale_A_tmem,\n  uint32_t scale_B_tmem,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Backporting C++17 cuda::std::byte to C++14 (C++)\nDESCRIPTION: Feature #66 added in libcu++ 1.4.0. The C++17 `std::byte` type is backported to C++14 as `cuda::std::byte` and made available in the `<cuda/std/cstddef>` header. Contribution by Jake Hemstad and Paul Taylor.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/releases/changelog.rst#2025-04-23_snippet_14\n\nLANGUAGE: cpp\nCODE:\n```\ncuda::std::byte\n```\n\nLANGUAGE: cpp\nCODE:\n```\n<cuda/std/cstddef>\n```\n\n----------------------------------------\n\nTITLE: CTA Group 1 Shift Down Operation\nDESCRIPTION: Template function implementing downward shift operation for CTA group type 1. Takes a CTA group parameter and target address, targeting PTX ISA 86 and SM_100a/SM_101a architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_shift.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.shift.cta_group.down [taddr]; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1, .cta_group::2 }\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_shift_down(\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t taddr);\n```\n\n----------------------------------------\n\nTITLE: Implementing multimem.ld_reduce for acquire system global add operations on 64-bit integers\nDESCRIPTION: Template implementation for multimem atomic load-and-reduce operations with acquire semantics, system scope, global memory, and add operation on 64-bit integers. Compatible with PTX ISA 81 on SM_90 architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_48\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.u64 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .add }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline int64_t multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_add_t,\n  const int64_t* addr);\n```\n\n----------------------------------------\n\nTITLE: Storing and Unpacking 32x32b Data with tcgen05 in CUDA (4 values)\nDESCRIPTION: This function performs a 32x32b store operation with 16-bit unpacking using tcgen05. It takes a 32-bit address and an array of 4 B32 values as input. The function is constrained to B32 types of 4 bytes.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_st.rst#2025-04-23_snippet_22\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.st.sync.aligned.32x32b.x4.unpack::16b.b32 [taddr], values; // PTX ISA 86, SM_100a, SM_101a\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline void tcgen05_st_32x32b_unpack_16b(\n  uint32_t taddr,\n  const B32 (&values)[4]);\n```\n\n----------------------------------------\n\nTITLE: Using CUDA Aligned Size in Memory Operations\nDESCRIPTION: Example kernel demonstrating the usage of aligned_size_t with cuda::memcpy_async operations, showing both unaligned and 16-byte aligned memory copies using a system-scope barrier.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/shapes/aligned_size_t.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n#include <cuda/barrier>\n\n__global__ void example_kernel(void* dst, void* src, size_t size) {\n  cuda::barrier<cuda::thread_scope_system> bar;\n  init(&bar, 1);\n\n  // Implementation cannot make assumptions about alignment.\n  cuda::memcpy_async(dst, src, size, bar);\n\n  // Implementation can assume that dst and src are 16-bytes aligned,\n  // and that size is a multiple of 16, and may optimize accordingly.\n  cuda::memcpy_async(dst, src, cuda::aligned_size_t<16>(size), bar);\n\n  bar.arrive_and_wait();\n}\n```\n\n----------------------------------------\n\nTITLE: Storing 16x256b Data with tcgen05 in CUDA (8 values)\nDESCRIPTION: This function performs a 16x256b store operation using tcgen05. It takes a 32-bit address and an array of 8 B32 values as input. The function is constrained to B32 types of 4 bytes.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_st.rst#2025-04-23_snippet_7\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.st.sync.aligned.16x256b.x2.b32 [taddr], values; // PTX ISA 86, SM_100a, SM_101a\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline void tcgen05_st_16x256b(\n  uint32_t taddr,\n  const B32 (&values)[8]);\n```\n\n----------------------------------------\n\nTITLE: Executing Directory Processing for All Subdirectories in CMake\nDESCRIPTION: Main execution code that discovers all benchmark subdirectories and processes each one. It calls the previously defined functions to set up the entire benchmark infrastructure.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cub/benchmarks/CMakeLists.txt#2025-04-23_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nget_recursive_subdirs(subdirs)\n\nforeach(subdir IN LISTS subdirs)\n  add_bench_dir(\"${subdir}\")\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Declaring Load-Reduce Minimum Operation (weak/global/u64) in CUDA C++\nDESCRIPTION: Defines a device-side inline function template for atomic minimum-reduction loads on global memory for 64-bit unsigned integers, utilizing a weak PTX semaphore and min op. Returns the minimum value found as uint64_t. Requires inclusion of PTX semantic type wrappers for weak semaphores and operations.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_9\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.global.op.u64 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .weak }\n// .op        = { .min }\ntemplate <typename = void>\n__device__ static inline uint64_t multimem_ld_reduce(\n  cuda::ptx::sem_weak_t,\n  cuda::ptx::op_min_t,\n  const uint64_t* addr);\n```\n\n----------------------------------------\n\nTITLE: Incorrect Usage of Unconstrained Async Resource Reference in C++\nDESCRIPTION: Demonstrates a buggy implementation that attempts to use compile-time property checking with an unconstrained async_resource_ref. This highlights a limitation of the type-erased wrapper approach.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/memory_resource/resource_ref.rst#2025-04-23_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\nstruct required_alignment{};\nvoid* buggy_allocate_async_with_alignment(cuda::mr::async_resource_ref<> resource, std::size_t size, cuda::stream_ref stream) {\n    if constexpr (cuda::has_property<required_alignment>) { // BUG: This will always be false\n        return resource.allocate_async(size, cuda::mr::get_property(resource, required_alignment), stream);\n    } else {\n        return resource.allocate_async(size, my_default_alignment, stream);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Binary Find Shift Amount for 64-bit Unsigned Integer\nDESCRIPTION: Device function template implementing bfind shift amount operation for uint64_t type. Returns shift amount for the highest set bit in a 64-bit unsigned integer.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/bfind.rst#2025-04-23_snippet_3\n\nLANGUAGE: cuda\nCODE:\n```\n// bfind.shiftamt.u64 dest, a_reg; // PTX ISA 20, SM_50\ntemplate <typename = void>\n__device__ static inline uint32_t bfind_shiftamt(\n  uint64_t a_reg);\n```\n\n----------------------------------------\n\nTITLE: Initializing Tensor Core Operation with TMEM for CTA Group 1 in CUDA\nDESCRIPTION: This function initializes a tensor core operation using TMEM for CTA group 1 with various data types. It includes additional parameters like a_tmem and zero_column_mask_desc.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_17\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.ws.cta_group.kind.collector::b1::fill [d_tmem], [a_tmem], b_desc, idesc, enable_input_d, zero_column_mask_desc; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1 }\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_tmem_a_collector_b1_fill(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint32_t a_tmem,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d,\n  uint64_t zero_column_mask_desc);\n```\n\n----------------------------------------\n\nTITLE: Defining a PTX Memory Barrier Function Signature in CUDA C++\nDESCRIPTION: Presents the function signature for a memory barrier operation mapped to the 'mbarrier.arrive.shared::cta.b64' PTX instruction. This static inline device function takes release semantics, scope, space, and a pointer to the barrier as parameters, returning a 64-bit integer state as per the PTX specification. Usage requires targeting at least PTX ISA 70 and SM_80, and including the cuda/ptx header for proper type definitions.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx.rst#2025-04-23_snippet_2\n\nLANGUAGE: cuda\nCODE:\n```\n// mbarrier.arrive.shared::cta.b64 state, [addr]; // 1.  PTX ISA 70, SM_80\n__device__ inline uint64_t mbarrier_arrive(\n  cuda::ptx::sem_release_t sem,\n  cuda::ptx::scope_cta_t scope,\n  cuda::ptx::space_shared_t space,\n  uint64_t* addr);\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Async CUDA Memory Resource\nDESCRIPTION: Shows implementation of the cuda::mr::async_resource concept with both synchronous and asynchronous allocation methods.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/memory_resource/resource.rst#2025-04-23_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nstruct valid_resource {\n  void* allocate(std::size_t, std::size_t) { return nullptr; }\n  void deallocate(void*, std::size_t, std::size_t) noexcept {}\n  void* allocate_async(std::size_t, std::size_t, cuda::stream_ref) { return nullptr; }\n  void deallocate_async(void*, std::size_t, std::size_t, cuda::stream_ref) {}\n  bool operator==(const valid_resource&) const { return true; }\n  bool operator!=(const valid_resource&) const { return false; }\n};\nstatic_assert(cuda::mr::async_resource<valid_resource>, \"\");\n```\n\n----------------------------------------\n\nTITLE: Initializing Benchmark Discovery and Processing in CMake\nDESCRIPTION: This block initiates the benchmark discovery and configuration process. It first calls `get_recursive_subdirs` to find all relevant benchmark subdirectories and stores them in the `subdirs` variable. Then, it iterates through each discovered subdirectory (`subdir`) and calls the `add_bench_dir` function to process the benchmarks within that directory.\nSOURCE: https://github.com/nvidia/cccl/blob/main/thrust/benchmarks/CMakeLists.txt#2025-04-23_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nget_recursive_subdirs(subdirs)\n\nforeach(subdir IN LISTS subdirs)\n  add_bench_dir(\"${subdir}\")\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Including C Standard Definitions Header in CUDA C++\nDESCRIPTION: Includes the `<cuda/std/cstddef>` header, defining fundamental types and macros like `size_t` and `nullptr_t`. Available since CCCL 2.0.0 and CUDA Toolkit 10.2.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/standard_api/c_library.rst#2025-04-23_snippet_6\n\nLANGUAGE: C++\nCODE:\n```\n#include <cuda/std/cstddef>\n```\n\n----------------------------------------\n\nTITLE: CTA Group Template Function for 128x256b Configuration\nDESCRIPTION: Template function declaration for 128x256b memory layout supporting CTA groups 1 and 2. Takes a CTA group parameter, target address, and descriptor as inputs.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_cp.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.cp.cta_group.128x256b [taddr], s_desc; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1, .cta_group::2 }\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_cp_128x256b(\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t taddr,\n  uint64_t s_desc);\n```\n\n----------------------------------------\n\nTITLE: Generic Matrix Multiplication Template with CTA Group Selection\nDESCRIPTION: Generic CUDA device function template allowing dynamic selection of CTA group and data type configurations for matrix multiplication operations.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma.rst#2025-04-23_snippet_3\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <int N32, cuda::ptx::dot_kind Kind, cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_mma(\n  cuda::ptx::kind_t<Kind> kind,\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d,\n  cuda::ptx::n32_t<N32> scale_input_d);\n```\n\n----------------------------------------\n\nTITLE: Async Store with Barrier Completion - 32-bit Single Value\nDESCRIPTION: Template for asynchronous weak shared memory store operation with cluster barrier completion tracking for 32-bit values. Compatible with PTX ISA 81 and SM_90 architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/st_async.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename Type>\n__device__ static inline void st_async(\n  Type* addr,\n  const Type& value,\n  uint64_t* remote_bar);\n```\n\n----------------------------------------\n\nTITLE: Defining cuda::pipeline::producer_commit Function in CUDA C++\nDESCRIPTION: This code snippet defines the producer_commit function template for the cuda::pipeline class. It is used to commit operations previously issued by the current thread to the current pipeline stage. The function is templated on the thread scope and is both host and device callable.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/synchronization_primitives/pipeline/producer_commit.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::thread_scope Scope>\n__host__ __device__\nvoid cuda::pipeline<Scope>::producer_commit();\n```\n\n----------------------------------------\n\nTITLE: CTA Scope Relaxed Memory Barrier Test Wait\nDESCRIPTION: Memory barrier test wait with relaxed semantics and CTA scope for PTX ISA 86 and SM_90. Includes scope and semantic parameters.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/mbarrier_test_wait.rst#2025-04-23_snippet_3\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_scope Scope>\n__device__ static inline bool mbarrier_test_wait(\n  cuda::ptx::sem_relaxed_t,\n  cuda::ptx::scope_t<Scope> scope,\n  uint64_t* addr,\n  const uint64_t& state);\n```\n\n----------------------------------------\n\nTITLE: CUDA Repeated Stream Query Progress\nDESCRIPTION: Shows how repeated cudaStreamQuery calls within a spin-loop guarantee eventual device thread progress, ensuring program termination.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/execution_model.rst#2025-04-23_snippet_8\n\nLANGUAGE: cuda\nCODE:\n```\ncuda::atomic<int, cuda::thread_scope_system> flag = 0;\n__global__ void producer() { flag.store(1); }\nint main() {\n    cudaHostRegister(&flag, sizeof(flag));\n    producer<<<1,1>>>();\n    while (flag.load() == 0) {\n        (void)cudaStreamQuery(0);\n    }\n    return cudaDeviceSynchronize();\n}\n```\n\n----------------------------------------\n\nTITLE: Conditional Compilation for PTX ISA Version in CUDA C++\nDESCRIPTION: Shows how to conditionally compile PTX-level code depending on whether the compiler supports a minimum PTX ISA version (here, 700). The code block illustrates wrapping a call to cuda::ptx::mbarrier_arrive with a preprocessor guard to ensure correct ISA targeting at compile time. Users must define __cccl_ptx_isa before inclusion, and the appropriate dependencies on the cuda/ptx header.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx.rst#2025-04-23_snippet_3\n\nLANGUAGE: cuda\nCODE:\n```\n#if __cccl_ptx_isa >= 700\ncuda::ptx::mbarrier_arrive(cuda::ptx::sem_release, cuda::ptx::scope_cta, cuda::ptx::space_shared, &bar, 1);\n#endif\n\n```\n\n----------------------------------------\n\nTITLE: Accessing Cluster CTA Information in CUDA\nDESCRIPTION: These functions retrieve cluster CTA-related information such as CTA ID components and dimensions within a cluster. They use inline PTX instructions to access the special registers.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/get_sreg.rst#2025-04-23_snippet_8\n\nLANGUAGE: cuda\nCODE:\n```\n// mov.u32 sreg_value, %%cluster_ctaid.x; // PTX ISA 78, SM_90\ntemplate <typename = void>\n__device__ static inline uint32_t get_sreg_cluster_ctaid_x();\n```\n\nLANGUAGE: cuda\nCODE:\n```\n// mov.u32 sreg_value, %%cluster_ctaid.y; // PTX ISA 78, SM_90\ntemplate <typename = void>\n__device__ static inline uint32_t get_sreg_cluster_ctaid_y();\n```\n\nLANGUAGE: cuda\nCODE:\n```\n// mov.u32 sreg_value, %%cluster_ctaid.z; // PTX ISA 78, SM_90\ntemplate <typename = void>\n__device__ static inline uint32_t get_sreg_cluster_ctaid_z();\n```\n\nLANGUAGE: cuda\nCODE:\n```\n// mov.u32 sreg_value, %%cluster_nctaid.x; // PTX ISA 78, SM_90\ntemplate <typename = void>\n__device__ static inline uint32_t get_sreg_cluster_nctaid_x();\n```\n\nLANGUAGE: cuda\nCODE:\n```\n// mov.u32 sreg_value, %%cluster_nctaid.y; // PTX ISA 78, SM_90\ntemplate <typename = void>\n__device__ static inline uint32_t get_sreg_cluster_nctaid_y();\n```\n\nLANGUAGE: cuda\nCODE:\n```\n// mov.u32 sreg_value, %%cluster_nctaid.z; // PTX ISA 78, SM_90\ntemplate <typename = void>\n__device__ static inline uint32_t get_sreg_cluster_nctaid_z();\n```\n\n----------------------------------------\n\nTITLE: Example Usage of JIT Template System in C++\nDESCRIPTION: Demonstrates how to use the JIT template system with an input iterator, including tag definition, specialization retrieval, and NVRTC integration.\nSOURCE: https://github.com/nvidia/cccl/blob/main/c/parallel/src/jit_templates/README.md#2025-04-23_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n#include \"jit_templates/templates/input_iterator.h\" // provides the input iterator template\n#include \"jit_templates/traits.h\"                   // provides the glue machinery\n\n// This type will be used as a tag for the specific iterator type. Having a tag like this allows multiple\n// specializations of a given template to exist in the same TU without any worry about conflicts.\n//\n// Types used as tags have to have a name that can be used for $NAME in `struct $NAME;`, because that will be a part of\n// the code NVRTC will be compiling.\nstruct func_iterator_tag;\n\nvoid func(/*..., */ cccl_iterator_t input_it /*, ...*/) {\n    // ...\n\n    const auto [\n        input_iterator_name,    // this string contains the C++ type name of the requested specialization\n\n        input_iterator_src      // this string contains additional code that must be included in a TU using the\n                                // specialization\n    ] =\n        get_specialization<     // get_specialization is the entry point to jit templates\n\n            func_iterator_tag   // this is the tag type defined earlier\n\n        >(\n            // A \"traits\" struct provides information about the given template to `get_specialization`. It is defined\n            // together with a template; for details, see the section on templates below. We pass it wrapped in\n            // `template_id` to signify that it is carrying the template information.\n            template_id<input_iterator_traits>(),\n\n            // What follows are the arguments for the template. These will be transformed into simple structures\n            // containing all the information from the runtime, and used as template parameters to the template specified\n            // above.\n            //\n            // `get_specialization` performs rudimentary type checking; if you pass the wrong type of an argument here\n            // (one that doesn't match the types expected by the template), this call to `get_specialization` will fail to\n            // compile.\n            input_t\n        );\n\n    // ...\n    // When constructing the TU to be passed into NVRTC, insert the contents of the jit template headers into it, prior\n    // to using any of the names obtained from `get_specialization`:\n    std::string nvrtc_tu = jit_template_header_contents + rest_of_the_tu;\n\n    // ...\n}\n```\n\n----------------------------------------\n\nTITLE: Acquire Memory Barrier Try Wait\nDESCRIPTION: Memory barrier try_wait with acquire semantics, supporting both CTA and cluster scopes.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/mbarrier_try_wait.rst#2025-04-23_snippet_2\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_scope Scope>\n__device__ static inline bool mbarrier_try_wait(\n  cuda::ptx::sem_acquire_t,\n  cuda::ptx::scope_t<Scope> scope,\n  uint64_t* addr,\n  const uint64_t& state);\n```\n\n----------------------------------------\n\nTITLE: Declaring CUDA PTX Multimem Reduction OR Template with 64-bit Values\nDESCRIPTION: Template declaration for performing atomic OR reduction operations on 64-bit values with different memory semantics and memory scopes. The template accepts semantics parameters (relaxed/release), scope parameters (cta/cluster/gpu/sys), and uses the PTX multimem.red instruction.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_red.rst#2025-04-23_snippet_38\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.red.sem.scope.global.op.b64 [addr], val; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .release }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .or }\ntemplate <typename B64, enable_if_t<sizeof(B64) == 8, bool> = true, cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline void multimem_red(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_or_op_t,\n  B64* addr,\n  B64 val);\n```\n\n----------------------------------------\n\nTITLE: Defining Logical Data Types in C++\nDESCRIPTION: Demonstrates how to define and use logical data types in CUDASTF, including usage of the auto keyword and mutable qualifiers for class members.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_43\n\nLANGUAGE: cpp\nCODE:\n```\ndouble X[16];\nlogical_data<slice<double>> lX = ctx.logical_data(X);\n\ndouble X[16];\nauto lX = ctx.logical_data(X);\n\nclass foo {\n   ...\n   mutable logical_data<slice<int>> ldata;\n};\n```\n\n----------------------------------------\n\nTITLE: Implementing multimem.ld_reduce for weak global AND operations on 32-bit values\nDESCRIPTION: Template implementation for multimem atomic load-and-reduce operations with weak semantics, global memory, and AND operation on 32-bit values. Compatible with PTX ISA 81 on SM_90 architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_49\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.global.op.b32 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .weak }\n// .op        = { .and }\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline B32 multimem_ld_reduce(\n  cuda::ptx::sem_weak_t,\n  cuda::ptx::op_and_op_t,\n  const B32* addr);\n```\n\n----------------------------------------\n\nTITLE: Initializing MMA Collector without Zero Column Mask (CUDA)\nDESCRIPTION: This function initializes an MMA collector for various data types without a zero column mask. It supports f16, tf32, f8f6f4, and i8 data types, and uses CTA group 1.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_23\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_collector_b1_lastuse(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Declaring cuda::bitmask Function in C++\nDESCRIPTION: Function signature for cuda::bitmask which generates a bitmask of specified width starting at given position. Default template parameter is uint32_t.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/bit/bitmask.rst#2025-04-23_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\ntemplate <typename T = uint32_t>\n[[nodiscard]] constexpr T\nbitmask(int start, int width) noexcept;\n```\n\n----------------------------------------\n\nTITLE: Including and Using the libcu++ <cuda/std/mdspan> Header in C++\nDESCRIPTION: This header provides access to the libcu++ implementation of `mdspan`. It makes all standard `<mdspan>` features, C++26 `std::dims`, and C++26 `std::aligned_accessor` available in C++17 onwards. An extension replaces the C++23 multidimensional `operator[]` with `operator()` in previous C++ standards. Debug mode enables detection of out-of-bounds accesses. A key restriction is that no exceptions are thrown on the device in case of a bad access.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/standard_api/container_library/mdspan.rst#2025-04-23_snippet_0\n\nLANGUAGE: c++\nCODE:\n```\n<cuda/std/mdspan>\n```\n\n----------------------------------------\n\nTITLE: Applying CUDA C++ Header Tests to All Public Headers in CMake\nDESCRIPTION: Loops through the list of public headers and applies the header test function to each one, creating individual test targets for all CUDA C++ public headers.\nSOURCE: https://github.com/nvidia/cccl/blob/main/libcudacxx/test/public_headers_host_only/CMakeLists.txt#2025-04-23_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nforeach(header IN LISTS public_headers_host_only)\n  libcudacxx_add_std_header_test(${header})\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Implementing MMA Operation with Block Scaling for mxf4nvf4 kind (4X scale)\nDESCRIPTION: This CUDA device function template implements a matrix multiplication and accumulation operation with block scaling. It supports only the mxf4nvf4 kind and CTA group sizes of 1 and 2. The function uses a 4X scale vector and tensor memory.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma.rst#2025-04-23_snippet_19\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.cta_group.kind.block_scale.scale_vec::4X [d_tmem], a_desc, b_desc, idesc, [scale_A_tmem], [scale_B_tmem], enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .kind      = { .kind::mxf4nvf4 }\n// .cta_group = { .cta_group::1, .cta_group::2 }\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_mma_block_scale_vec_4x_tmem_a(\n  cuda::ptx::kind_mxf4nvf4_t,\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  uint32_t scale_A_tmem,\n  uint32_t scale_B_tmem,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: CUDA Counting Semaphore Usage Example\nDESCRIPTION: Kernel function demonstrating the initialization of different types of counting semaphores with various thread scopes including system-wide, device-wide, and block-wide semaphores.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/synchronization_primitives/counting_semaphore.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n#include <cuda/semaphore>\n\n__global__ void example_kernel() {\n  // This semaphore is suitable for all threads in the system.\n  cuda::counting_semaphore<cuda::thread_scope_system> a;\n\n  // This semaphore has the same type as the previous one (`a`).\n  cuda::std::counting_semaphore<> b;\n\n  // This semaphore is suitable for all threads on the current processor (e.g. GPU).\n  cuda::counting_semaphore<cuda::thread_scope_device> c;\n\n  // This semaphore is suitable for all threads in the same thread block.\n  cuda::counting_semaphore<cuda::thread_scope_block> d;\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing SM-related Information in CUDA\nDESCRIPTION: These functions retrieve SM-related information such as SM ID and number of SMs. They use inline PTX instructions to access the special registers.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/get_sreg.rst#2025-04-23_snippet_5\n\nLANGUAGE: cuda\nCODE:\n```\n// mov.u32 sreg_value, %%smid; // PTX ISA 13\ntemplate <typename = void>\n__device__ static inline uint32_t get_sreg_smid();\n```\n\nLANGUAGE: cuda\nCODE:\n```\n// mov.u32 sreg_value, %%nsmid; // PTX ISA 20, SM_35\ntemplate <typename = void>\n__device__ static inline uint32_t get_sreg_nsmid();\n```\n\n----------------------------------------\n\nTITLE: Defining tcgen05_mma_ws_tmem_a_collector_b0_use Template (with mask) in CUDA\nDESCRIPTION: Defines a templated CUDA `__device__` static inline function `tcgen05_mma_ws_tmem_a_collector_b0_use`. This function handles collector `b0` use operations involving both destination (`d_tmem`) and source (`a_tmem`) temporary memory within `cta_group::1`. It supports various `dot_kind` types (f16, tf32, f8f6f4, i8) and uses descriptors (`b_desc`, `idesc`), an enable flag (`enable_input_d`), and a zero column mask descriptor (`zero_column_mask_desc`). Designed for PTX ISA 86, SM_100a, SM_101a.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_7\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.ws.cta_group.kind.collector::b0::use [d_tmem], [a_tmem], b_desc, idesc, enable_input_d, zero_column_mask_desc; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1 }\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_tmem_a_collector_b0_use(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint32_t a_tmem,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d,\n  uint64_t zero_column_mask_desc);\n```\n\n----------------------------------------\n\nTITLE: Declaring cuda::atomic::fetch_min Function in CUDA C++\nDESCRIPTION: Declaration of the fetch_min function for cuda::atomic. It atomically finds the minimum of the stored value and the provided value using cuda::std::min.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/synchronization_primitives/atomic/fetch_min.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename T, cuda::thread_scope Scope>\n__host__ __device__\nT cuda::atomic<T, Scope>::fetch_min(T const& val,\n                                    cuda::std::memory_order order\n                                      = cuda::std::memory_order_seq_cst);\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Volatile Access Limitations on Automatic Storage in CUDA Device Code\nDESCRIPTION: Example showing how volatile operations on automatic storage duration objects in a device thread may not guarantee forward progress, unlike with host threads.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/execution_model.rst#2025-04-23_snippet_2\n\nLANGUAGE: cuda\nCODE:\n```\n__global__ void ex2() {\n    volatile bool True = true;\n    while(True);\n}\n```\n\n----------------------------------------\n\nTITLE: Attempting Cluster Launch Cancellation in CUDA\nDESCRIPTION: Defines a device function to attempt cancellation of a cluster launch. It takes a memory address and a shared memory barrier as parameters.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/clusterlaunchcontrol.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline void clusterlaunchcontrol_try_cancel(\n  void* addr,\n  uint64_t* smem_bar);\n```\n\n----------------------------------------\n\nTITLE: Including C Standard Fixed-Width Integer Types Header in CUDA C++\nDESCRIPTION: Includes the `<cuda/std/cstdint>` header, specifically for fixed-width integer types (e.g., `int32_t`). Available since CCCL 2.0.0 and CUDA Toolkit 10.2.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/standard_api/c_library.rst#2025-04-23_snippet_8\n\nLANGUAGE: C++\nCODE:\n```\n#include <cuda/std/cstdint>\n```\n\n----------------------------------------\n\nTITLE: Implementing multimem.ld_reduce for relaxed system global add operations on 32-bit integers\nDESCRIPTION: Template implementation for multimem atomic load-and-reduce operations with relaxed semantics, system scope, global memory, and add operation on 32-bit integers. Compatible with PTX ISA 81 on SM_90 architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_35\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.s32 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .add }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline int32_t multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_add_t,\n  const int32_t* addr);\n```\n\n----------------------------------------\n\nTITLE: Implementing bitfield_extract Function in CUDA C++\nDESCRIPTION: Function signature for bitfield_extract which extracts a bitfield from a value at a specified position with given width and returns it in the lower bits. It computes (value >> start) & bitfield.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/bit/bitfield_extract.rst#2025-04-23_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\ntemplate <typename T>\n[[nodiscard]] constexpr T\nbitfield_extract(T value, int start, int width) noexcept;\n```\n\n----------------------------------------\n\nTITLE: Declaring Load-Reduce Minimum Operation (acquire/cluster/global/u64) in CUDA C++\nDESCRIPTION: Provides a template-based device function for synchronized min reduction loads in global memory locations holding uint64_t values, with configurable PTX semaphore and memory scope, including cluster-level. Ensures concurrency correctness for SM_90 platforms and must be used with supporting PTX semantics.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_15\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.u64 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .min }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline uint64_t multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_min_t,\n  const uint64_t* addr);\n```\n\n----------------------------------------\n\nTITLE: Implementing MMA Workspace Last Use Operations with TF32 Data Type (No Masking) in CUDA\nDESCRIPTION: Template function for the final tensor compute operation (lastuse) with TF32 data type. This simplified variant handles tensor descriptors without zero column masking. Targets PTX ISA 86 on SM_100a and SM_101a architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_48\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.ws.cta_group.kind.collector::b2::lastuse [d_tmem], a_desc, b_desc, idesc, enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1 }\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_collector_b2_lastuse(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Defining CTA Group Tensor Core Function for 32x128b WarpX4 Variant - CUDA\nDESCRIPTION: This CUDA device-side template function provides a declaration for executing a tensor core core operation on a CTA group configured as 32x128b with WarpX4 layout. The template parameter Cta_Group generalizes support for both .cta_group::1 and .cta_group::2. This function expects a typed cta_group handle, a 32-bit address, and a 64-bit operation descriptor; it is usable in device-side code following PTX ISA 86 semantics.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_cp.rst#2025-04-23_snippet_5\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.cp.cta_group.32x128b.warpx4.b8x16.b6x16_p32 [taddr], s_desc; // PTX ISA 86, SM_100a, SM_101a\\n// .cta_group = { .cta_group::1, .cta_group::2 }\\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\\n__device__ static inline void tcgen05_cp_32x128b_warpx4_b8x16_b6x16_p32(\\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\\n  uint32_t taddr,\\n  uint64_t s_desc);\n```\n\n----------------------------------------\n\nTITLE: Forwarding Properties in Derived Memory Resource Types in C++\nDESCRIPTION: Shows how to use cuda::forward_property to automatically propagate properties from a base memory resource type to a derived logging resource type.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/memory_resource/properties.rst#2025-04-23_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\ntemplate<class MemoryResource>\nclass logging_resource : cuda::forward_property<logging_resource<MemoryResource>, MemoryResource> {\n    MemoryResource base;\npublic:\n    void* allocate(std::size_t size, std::size_t alignment) {\n        std::cout << \"allocating\\n\";\n        return base.allocate(size, alignment);\n    }\n    void deallocate(void* ptr, std::size_t size, std::size_t alignment) noexcept {\n        std::cout << \"deallocating\\n\";\n        return base.deallocate(ptr, size, alignment);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining cuda::annotated_ptr Template in CUDA\nDESCRIPTION: Declaration of the cuda::annotated_ptr class template, which associates a pointer with a memory access property. It supports various property types and implements a pointer-like interface.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/memory_access_properties/annotated_ptr.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\nnamespace cuda {\ntemplate <typename Type, typename Property>\nclass annotated_ptr<Type, Property>;\n} // namespace cuda\n```\n\n----------------------------------------\n\nTITLE: Relaxed Memory Barrier Try Wait\nDESCRIPTION: Memory barrier try wait with relaxed semantics, supporting both CTA and cluster scopes.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/mbarrier_try_wait_parity.rst#2025-04-23_snippet_4\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_scope Scope>\n__device__ static inline bool mbarrier_try_wait_parity(\n  cuda::ptx::sem_relaxed_t,\n  cuda::ptx::scope_t<Scope> scope,\n  uint64_t* addr,\n  const uint32_t& phaseParity);\n```\n\n----------------------------------------\n\nTITLE: Creating and Configuring the Main C2H Library\nDESCRIPTION: Defines the main C2H library, sets up include directories, and links necessary dependencies. Conditionally includes CURAND support based on the C2H_ENABLE_CURAND option.\nSOURCE: https://github.com/nvidia/cccl/blob/main/c2h/CMakeLists.txt#2025-04-23_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(cccl.c2h STATIC\n  generators.cu\n)\ntarget_include_directories(cccl.c2h PUBLIC \"${C2H_SOURCE_DIR}/include\")\ntarget_link_libraries(cccl.c2h PUBLIC\n  CCCL::CCCL\n  Catch2::Catch2\n)\n\nif (C2H_ENABLE_CURAND)\n  target_link_libraries(cccl.c2h PRIVATE CUDA::curand)\n  target_compile_definitions(cccl.c2h PRIVATE C2H_HAS_CURAND=1)\nelse()\n  target_compile_definitions(cccl.c2h PRIVATE C2H_HAS_CURAND=0)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Dynamic Task Dependencies in CUDASTF\nDESCRIPTION: Demonstrates how to create a dynamic stream task and add dependencies using the add_deps method.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_45\n\nLANGUAGE: cpp\nCODE:\n```\ndouble X[16], Y[16];\nauto lX = ctx.logical_data(X);\nauto lY = ctx.logical_data(Y);\n\nstream_task<> t = ctx.task();\nt.add_deps(lX.read(), lY.rw());\n```\n\n----------------------------------------\n\nTITLE: Implementing Release Fence for Tensor Map Proxy in CUDA\nDESCRIPTION: This template function implements a release fence operation for tensor map proxies. It supports various scopes including CTA, cluster, GPU, and system. The function uses PTX ISA 83 instructions and is available on SM_90 architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/fence_proxy_tensormap_generic.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\n// fence.proxy.tensormap::generic.release.scope; // 7. PTX ISA 83, SM_90\n// .sem       = { .release }\n// .scope     = { .cta, .cluster, .gpu, .sys }\ntemplate <cuda::ptx::dot_scope Scope>\n__device__ static inline void fence_proxy_tensormap_generic(\n  cuda::ptx::sem_release_t,\n  cuda::ptx::scope_t<Scope> scope);\n```\n\n----------------------------------------\n\nTITLE: Memory Fence with Release Semantics\nDESCRIPTION: Template function implementing memory fence with release semantics. Supports CTA, cluster, GPU, and system-wide memory scopes. Requires PTX ISA 8.6 and SM_90 or higher.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/fence.rst#2025-04-23_snippet_4\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_scope Scope>\n__device__ static inline void fence(\n  cuda::ptx::sem_release_t,\n  cuda::ptx::scope_t<Scope> scope);\n```\n\n----------------------------------------\n\nTITLE: Memory Barrier Test Wait Parity with Acquire Semantics (CTA Scope)\nDESCRIPTION: Implementation with acquire semantics and CTA scope for SM_90. Includes semantic and scope parameters along with address and phase parity.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/mbarrier_test_wait_parity.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_scope Scope>\n__device__ static inline bool mbarrier_test_wait_parity(\n  cuda::ptx::sem_acquire_t,\n  cuda::ptx::scope_t<Scope> scope,\n  uint64_t* addr,\n  const uint32_t& phaseParity);\n```\n\n----------------------------------------\n\nTITLE: Defining Multimem Reduction Max Operations for u64 Type in CUDA\nDESCRIPTION: Template function declarations for multimem reduction max operations on uint64_t data. These functions support different memory semantics (relaxed, release) and memory scopes (cta, cluster, gpu, sys). These PTX operations are available in PTX ISA 8.1 on SM_90 architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_red.rst#2025-04-23_snippet_7\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.red.sem.scope.global.op.u64 [addr], val; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .release }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .max }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline void multimem_red(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_max_t,\n  uint64_t* addr,\n  uint64_t val);\n```\n\n----------------------------------------\n\nTITLE: Allocating Memory with Async Resource Reference in C++\nDESCRIPTION: Demonstrates the use of cuda::mr::async_resource_ref for memory allocation. This wrapper allows for a non-template function that can accept different types of memory resources, potentially improving compile times and reducing binary size.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/memory_resource/resource_ref.rst#2025-04-23_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\nvoid* do_allocate_async(cuda::mr::async_resource_ref<> resource, std::size_t size, std::size_t align, cuda::stream_ref stream) {\n    return resource.allocate_async(size, align, stream);\n}\n\nmy_async_memory_resource resource;\nmy_async_memory_resource* pointer_to_resource = &resource;\n\nvoid* from_reference = do_allocate_async(resource, 1337, 256, cuda::stream_ref{});\nvoid* from_ptr = do_allocate_async(pointer_to_resource, 1337, 256, cuda::stream_ref{});\n```\n\n----------------------------------------\n\nTITLE: Multimem Load Reduce for uint32_t\nDESCRIPTION: Template function implementing atomic load-reduce operation for 32-bit unsigned integers with configurable semantics and scope. Supports relaxed and acquire semantics with CTA, cluster, GPU and system-wide scopes.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_30\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline uint32_t multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_add_t,\n  const uint32_t* addr);\n```\n\n----------------------------------------\n\nTITLE: Defining CUDA Counting Semaphore Template Class\nDESCRIPTION: Template declaration for cuda::counting_semaphore class that takes a thread_scope parameter and an optional LeastMaxValue parameter. This class extends the standard counting semaphore with CUDA-specific threading capabilities.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/synchronization_primitives/counting_semaphore.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::thread_scope Scope,\n             cuda::std::ptrdiff_t LeastMaxValue = /* implementation-defined */>\nclass cuda::counting_semaphore;\n```\n\n----------------------------------------\n\nTITLE: Implementing multimem.ld_reduce for relaxed cluster global AND operations on 32-bit values\nDESCRIPTION: Template implementation for multimem atomic load-and-reduce operations with relaxed semantics, cluster scope, global memory, and AND operation on 32-bit values. Compatible with PTX ISA 81 on SM_90 architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_51\n\nLANGUAGE: cuda\nCODE:\n```\n\n```\n\n----------------------------------------\n\nTITLE: Multimem Load Reduce for uint64_t\nDESCRIPTION: Template function implementing atomic load-reduce operation for 64-bit unsigned integers with weak semantics. Uses global scope with addition operation.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_31\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline uint64_t multimem_ld_reduce(\n  cuda::ptx::sem_weak_t,\n  cuda::ptx::op_add_t,\n  const uint64_t* addr);\n```\n\n----------------------------------------\n\nTITLE: Defining CUDA Pipeline Role Enumeration\nDESCRIPTION: Defines an enumeration class pipeline_role that specifies whether a thread acts as a producer or consumer in a partitioned pipeline. The producer generates data and issues asynchronous operations, while the consumer processes data and waits for operations to complete.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/synchronization_primitives/pipeline/role.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\nenum class pipeline_role : /* unspecified */ {\n   producer,\n   consumer\n};\n```\n\n----------------------------------------\n\nTITLE: Implementing Restrict MDSpan Template Alias in C++\nDESCRIPTION: Template alias definition for creating an mdspan with restrict aliasing policy, combining ElementType, Extents, LayoutPolicy, and AccessorPolicy parameters.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/mdspan/restrict_accessor.rst#2025-04-23_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\ntemplate <typename ElementType,\n            typename Extents,\n            typename LayoutPolicy   = cuda::std::layout_right,\n            typename AccessorPolicy = cuda::std::default_accessor<_ElementType>>\nusing restrict_mdspan = cuda::std::mdspan<ElementType, Extents, LayoutPolicy, restrict_accessor<AccessorPolicy>>;\n```\n\n----------------------------------------\n\nTITLE: Detecting Failure Tests\nDESCRIPTION: Helper function that determines if a test is expected to fail by checking if the filename contains '_fail'. Failure tests are handled differently in the build system, expecting compilation errors.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cub/test/CMakeLists.txt#2025-04-23_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\n## _cub_is_fail_test\n#\n# If the test_src contains the substring \"_fail\", `result_var` will\n# be set to TRUE.\nfunction(_cub_is_fail_test result_var test_src)\n  string(FIND \"${test_src}\" \"_fail\" idx)\n  if (idx EQUAL -1)\n    set(${result_var} FALSE PARENT_SCOPE)\n  else()\n    set(${result_var} TRUE PARENT_SCOPE)\n  endif()\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Store 128-bit Data with L1 Evict Last and L2 Cache Hint in CUDA\nDESCRIPTION: This function stores 128-bit data to global memory with L1 cache evict last policy and L2 cache hint. It requires SM_80 or later architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/st.rst#2025-04-23_snippet_9\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename B128, enable_if_t<sizeof(B128) == 16, bool> = true>\n__device__ static inline void st_L1_evict_last_L2_cache_hint(\n  cuda::ptx::space_global_t,\n  B128* addr,\n  B128 src,\n  uint64_t cache_policy);\n```\n\n----------------------------------------\n\nTITLE: Copy Constructing cuda::annotated_ptr in CUDA\nDESCRIPTION: Copy constructor for cuda::annotated_ptr that allows construction from a different annotated_ptr type, with mandates and preconditions to ensure compatibility between pointer and property types.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/memory_access_properties/annotated_ptr.rst#2025-04-23_snippet_4\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <class T, class P>\nconstexpr annotated_ptr(annotated_ptr<T,P> const& a);\n```\n\n----------------------------------------\n\nTITLE: CUDA Binary Semaphore Usage Example\nDESCRIPTION: Demonstrates the creation of binary semaphores with different thread scopes including system, device, and block level synchronization in a CUDA kernel.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/synchronization_primitives/binary_semaphore.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n#include <cuda/semaphore>\n\n__global__ void example_kernel() {\n  // This semaphore is suitable for all threads in the system.\n  cuda::binary_semaphore<cuda::thread_scope_system> a;\n\n  // This semaphore has the same type as the previous one (`a`).\n  cuda::std::binary_semaphore<> b;\n\n  // This semaphore is suitable for all threads on the current processor (e.g. GPU).\n  cuda::binary_semaphore<cuda::thread_scope_device> c;\n\n  // This semaphore is suitable for all threads in the same thread block.\n  cuda::binary_semaphore<cuda::thread_scope_block> d;\n}\n```\n\n----------------------------------------\n\nTITLE: MMA Workstream Last Use Collector Implementation\nDESCRIPTION: Template implementation for final matrix multiplication workstream collector operation. Handles cleanup and last use cases with descriptor-based inputs.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_67\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_collector_b3_lastuse(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d,\n  uint64_t zero_column_mask_desc);\n```\n\n----------------------------------------\n\nTITLE: PRMT B32 Edge Clamp Right Template Function\nDESCRIPTION: Template function declaration for 32-bit edge clamp right permute operation (ecr mode). Clamps bytes at the right edge of the register.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/prmt.rst#2025-04-23_snippet_5\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline uint32_t prmt_ecr(\n  B32 a_reg,\n  B32 b_reg,\n  uint32_t c_reg);\n```\n\n----------------------------------------\n\nTITLE: Defining and Using Task Types in C++\nDESCRIPTION: Shows how to define and use task types in CUDASTF, including type propagation and error detection during compilation.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_44\n\nLANGUAGE: cpp\nCODE:\n```\ndouble X[16], Y[16];\nlogical_data<slice<double>> lX = ctx.logical_data(X);\nlogical_data<slice<double>> lY = ctx.logical_data(Y);\n\n// results in a compilation error due to the erroneous slice<int> type\nctx.task(lX.read(), lY.rw())->*[](cudaStream_t s, slice<int> x, slice<int> y) {\n    ...\n};\n\ndouble X[16], Y[16];\nauto lX = ctx.logical_data(X);\nauto lY = ctx.logical_data(Y);\n\nctx.task(lX.read(), lY.rw())->*[](cudaStream_t s, auto x, auto y) {\n    ...\n};\n```\n\n----------------------------------------\n\nTITLE: Defining Streams API Documentation Structure in reStructuredText\nDESCRIPTION: This snippet defines the structure of the Streams API documentation using reStructuredText directives. It includes a table of contents, a hidden subtree for stream_ref, and a list table summarizing the API components.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/streams.rst#2025-04-23_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. _libcudacxx-extended-api-streams:\n\nStreams\n=======\n\n.. toctree::\n   :hidden:\n   :maxdepth: 1\n\n   cuda::stream_ref <streams/stream_ref>\n\n.. list-table::\n   :widths: 25 45 30\n   :header-rows: 0\n\n   * - :ref:`stream_ref <libcudacxx-extended-api-streams-stream-ref>`\n     - A wrapper around a ``cudaStream_t``\n     - CCCL 2.2.0 / CUDA 12.3\n```\n\n----------------------------------------\n\nTITLE: Implementing multimem.ld_reduce for acquire cluster global add operations on 64-bit integers\nDESCRIPTION: Template implementation for multimem atomic load-and-reduce operations with acquire semantics, cluster scope, global memory, and add operation on 64-bit integers. Compatible with PTX ISA 81 on SM_90 architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_46\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.u64 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .add }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline int64_t multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_add_t,\n  const int64_t* addr);\n```\n\n----------------------------------------\n\nTITLE: Implementing mbarrier.expect_tx with cluster scope and shared CTA space\nDESCRIPTION: Template implementation for mbarrier expect transaction with relaxed semantics, cluster scope, and shared CTA memory space. Requires PTX ISA 80 and SM_90.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/mbarrier_expect_tx.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n// mbarrier.expect_tx.sem.scope.space.b64 [addr], txCount; // 1. PTX ISA 80, SM_90\n// .sem       = { .relaxed }\n// .scope     = { .cta, .cluster }\n// .space     = { .shared::cta }\ntemplate <cuda::ptx::dot_scope Scope>\n__device__ static inline void mbarrier_expect_tx(\n  cuda::ptx::sem_relaxed_t,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::space_shared_t,\n  uint64_t* addr,\n  uint32_t txCount);\n```\n\n----------------------------------------\n\nTITLE: Implementing warp_shuffle_down in CUDA\nDESCRIPTION: Function templates for warp_shuffle_down operation with optional Width and lane_mask parameters. Shuffles data down within a warp by a specified delta.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/warp/warp_shuffle.rst#2025-04-23_snippet_2\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <int Width = 32, typename T>\n[[nodiscard]] __device__ warp_shuffle_result<T>\nwarp_shuffle_down(const T& data,\n                  int      delta,\n                  uint32_t lane_mask = 0xFFFFFFFF,\n                  cuda::std::integral_constant<int, Width> = {})\n\ntemplate <int Width = 32, typename T>\n[[nodiscard]] __device__ warp_shuffle_result<T>\nwarp_shuffle_down(const T& data,\n                  int      delta,\n                  cuda::std::integral_constant<int, Width>) // lane_mask is 0xFFFFFFFF\n```\n\n----------------------------------------\n\nTITLE: Declaring Load-Reduce Minimum Operation (acquire/gpu/global/u64) in CUDA C++\nDESCRIPTION: This device inline template function, parameterized for acquire/relaxed PTX semaphores and all memory scopes, performs atomic-like min reduction loads for 64-bit unsigned values in global memory. Ensures conformance to hardware PTX semantics required for system-wide or GPU-wide concurrency in CUDA libraries.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_16\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.u64 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .min }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline uint64_t multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_min_t,\n  const uint64_t* addr);\n```\n\n----------------------------------------\n\nTITLE: Storing and Unpacking 32x32b Data with tcgen05 in CUDA (16 values)\nDESCRIPTION: This function performs a 32x32b store operation with 16-bit unpacking using tcgen05. It takes a 32-bit address and an array of 16 B32 values as input. The function is constrained to B32 types of 4 bytes.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_st.rst#2025-04-23_snippet_26\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.st.sync.aligned.32x32b.x16.unpack::16b.b32 [taddr], values; // PTX ISA 86, SM_100a, SM_101a\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline void tcgen05_st_32x32b_unpack_16b(\n  uint32_t taddr,\n  const B32 (&values)[16]);\n```\n\n----------------------------------------\n\nTITLE: Using MAPA Instruction via Cooperative Groups in CUDA\nDESCRIPTION: Demonstrates how to access remote shared memory across thread blocks using the cluster_group API. The example shows synchronization between blocks and mapping shared memory between different block ranks.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/mapa.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\n#include <cooperative_groups.h>\n\n__cluster_dims__(2)\n__global__ void kernel() {\n    __shared__ int x;\n    x = 1;\n    namespace cg = cooperative_groups;\n    cg::cluster_group cluster = cg::this_cluster();\n\n    cluster.sync();\n\n    // Get address of remote shared memory value:\n    unsigned int other_block_rank = cluster.block_rank() ^ 1;\n    int * remote_x = cluster.map_shared_rank(&bar, other_block_rank);\n\n    // Write to remote value:\n    *remote_x = 2;\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing Lane Mask Information in CUDA\nDESCRIPTION: These functions retrieve various lane mask information. They use inline PTX instructions to access the special registers.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/get_sreg.rst#2025-04-23_snippet_10\n\nLANGUAGE: cuda\nCODE:\n```\n// mov.u32 sreg_value, %%lanemask_eq; // PTX ISA 20, SM_35\ntemplate <typename = void>\n__device__ static inline uint32_t get_sreg_lanemask_eq();\n```\n\nLANGUAGE: cuda\nCODE:\n```\n// mov.u32 sreg_value, %%lanemask_le; // PTX ISA 20, SM_35\ntemplate <typename = void>\n__device__ static inline uint32_t get_sreg_lanemask_le();\n```\n\nLANGUAGE: cuda\nCODE:\n```\n// mov.u32 sreg_value, %%lanemask_lt; // PTX ISA 20, SM_35\ntemplate <typename = void>\n__device__ static inline uint32_t get_sreg_lanemask_lt();\n```\n\nLANGUAGE: cuda\nCODE:\n```\n// mov.u32 sreg_value, %%lanemask_ge; // PTX ISA 20, SM_35\ntemplate <typename = void>\n__device__ static inline uint32_t get_sreg_lanemask_ge();\n```\n\nLANGUAGE: cuda\nCODE:\n```\n// mov.u32 sreg_value, %%lanemask_gt; // PTX ISA 20, SM_35\ntemplate <typename = void>\n__device__ static inline uint32_t get_sreg_lanemask_gt();\n```\n\n----------------------------------------\n\nTITLE: Using Address-Dependent Lambda with Thrust Transform\nDESCRIPTION: Demonstrates a code pattern where a lambda function depends on the address of its parameter to calculate an index. This approach constrains optimizations as it requires arguments to be served directly from the input buffer.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/api_docs/function_objects.rst#2025-04-23_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nconst int n = 10;\nthrust::device_vector<int> a(n, 1);\nthrust::device_vector<int> b(n);\nint* a_ptr = thrust::raw_pointer_cast(a.data());\nint* b_ptr = thrust::raw_pointer_cast(b.data());\nthrust::transform(thrust::device, a.begin(), a.end(), a.begin(),\n    [a_ptr, b_ptr](const int& e) {\n        const auto i = &e - a_ptr; // &e expected to point into global memory\n        return e + b_ptr[i];\n    });\n```\n\n----------------------------------------\n\nTITLE: Configuring CUDAX Build Options\nDESCRIPTION: Defines various build options for CUDAX, including header testing, general testing, examples, CUDASTF subproject and its features. These options control what components get built and how.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cudax/CMakeLists.txt#2025-04-23_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\noption(cudax_ENABLE_HEADER_TESTING \"Test that CUDA Experimental's public headers compile.\" ON)\noption(cudax_ENABLE_TESTING \"Build CUDA Experimental's tests.\" ON)\noption(cudax_ENABLE_EXAMPLES \"Build CUDA Experimental's examples.\" ON)\noption(cudax_ENABLE_CUDASTF \"Enable CUDASTF subproject\" ON)\noption(cudax_ENABLE_CUDASTF_CODE_GENERATION \"Enable code generation using STF's parallel_for or launch with CUDA compiler.\" ON)\noption(cudax_ENABLE_CUDASTF_BOUNDSCHECK \"Enable bounds checks for STF targets. Requires debug build.\" OFF)\noption(cudax_ENABLE_CUDASTF_MATHLIBS \"Enable STF tests/examples that use cublas/cusolver.\" OFF)\n```\n\n----------------------------------------\n\nTITLE: Declaring tcgen05_mma_tmem_a with Scale Input D for SM_100a\nDESCRIPTION: Template function declaration for tcgen05 matrix multiply-accumulate operation with both CTA group 1 and 2 configurations, supporting f16 and tf32 data types. This variant includes a scale_input_d parameter, specifically for SM_100a architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma.rst#2025-04-23_snippet_11\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.cta_group.kind [d_tmem], [a_tmem], b_desc, idesc, enable_input_d, scale_input_d; // PTX ISA 86, SM_100a\n// .kind      = { .kind::f16, .kind::tf32 }\n// .cta_group = { .cta_group::1, .cta_group::2 }\ntemplate <int N32, cuda::ptx::dot_kind Kind, cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_mma_tmem_a(\n  cuda::ptx::kind_t<Kind> kind,\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t d_tmem,\n  uint32_t a_tmem,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d,\n  cuda::ptx::n32_t<N32> scale_input_d);\n```\n\n----------------------------------------\n\nTITLE: Implementing Fence Proxy Async in CUDA\nDESCRIPTION: Defines a device function for general fence proxy asynchronous operation. This template function is used for PTX ISA 80 and SM_90.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/fence_proxy_async.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\n// fence.proxy.async; // 5. PTX ISA 80, SM_90\ntemplate <typename = void>\n__device__ static inline void fence_proxy_async();\n```\n\n----------------------------------------\n\nTITLE: Memory Barrier Try Wait with Suspend Hint\nDESCRIPTION: Extended version of try wait with additional suspend time hint parameter for optimization.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/mbarrier_try_wait_parity.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline bool mbarrier_try_wait_parity(\n  uint64_t* addr,\n  const uint32_t& phaseParity,\n  const uint32_t& suspendTimeHint);\n```\n\n----------------------------------------\n\nTITLE: Implementing multimem.ld_reduce for acquire GPU global add operations on 64-bit integers\nDESCRIPTION: Template implementation for multimem atomic load-and-reduce operations with acquire semantics, GPU scope, global memory, and add operation on 64-bit integers. Compatible with PTX ISA 81 on SM_90 architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_47\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.u64 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .add }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline int64_t multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_add_t,\n  const int64_t* addr);\n```\n\n----------------------------------------\n\nTITLE: Implementing multimem.ld_reduce for relaxed CTA global AND operations on 32-bit values\nDESCRIPTION: Template implementation for multimem atomic load-and-reduce operations with relaxed semantics, CTA scope, global memory, and AND operation on 32-bit values. Compatible with PTX ISA 81 on SM_90 architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_50\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.b32 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .and }\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true, cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline B32 multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_and_op_t,\n  const B32* addr);\n```\n\n----------------------------------------\n\nTITLE: 32-bit Unsigned Right Shift Operation in CUDA\nDESCRIPTION: Template function for performing unsigned right shift on 32-bit values. Takes a generic B32 type parameter constrained to 4 bytes and shift amount as parameters.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/shr.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n// shr.b32 dest, a_reg, b_reg; // PTX ISA 10, SM_50\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline B32 shr(\n  B32 a_reg,\n  uint32_t b_reg);\n```\n\n----------------------------------------\n\nTITLE: Accessing Grid ID in CUDA\nDESCRIPTION: This function retrieves the grid ID. It uses an inline PTX instruction to access the special register.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/get_sreg.rst#2025-04-23_snippet_6\n\nLANGUAGE: cuda\nCODE:\n```\n// mov.u64 sreg_value, %%gridid; // PTX ISA 30\ntemplate <typename = void>\n__device__ static inline uint64_t get_sreg_gridid();\n```\n\n----------------------------------------\n\nTITLE: Setting up cuda.parallel for Local Development in Bash\nDESCRIPTION: This snippet outlines the sequence of Bash commands required for setting up a local development environment for the `cuda.parallel` library. It involves installing the library from a relative parent directory (`../cuda_cccl`), installing the current package along with its testing extras (`.[test]`) verbosely using `pip3`, and finally executing the test suite located in the `./tests/` directory with verbose output using `pytest`. Dependencies include `pip3` and `pytest`.\nSOURCE: https://github.com/nvidia/cccl/blob/main/python/cuda_parallel/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip3 install ../cuda_cccl\npip3 install .[test] -v\npytest -v ./tests/\n```\n\n----------------------------------------\n\nTITLE: Defining cuda::barrier Class Template in CUDA\nDESCRIPTION: Declaration of the cuda::barrier class template with thread scope and completion function parameters. This template extends the functionality of cuda::std::barrier.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/synchronization_primitives/barrier.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::thread_scope Scope,\n            typename CompletionFunction = /* unspecified */>\nclass cuda::barrier;\n```\n\n----------------------------------------\n\nTITLE: Defining Static Assertion Test Sources in CMake\nDESCRIPTION: This snippet defines a list of CUDA source files that will be used for static assertion tests. These files are expected to contain code that will trigger static assertions during compilation.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cudax/test/stf/static_error_checks/CMakeLists.txt#2025-04-23_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(static_assert_tests\n  static.cu\n  task_prototype.cu\n  stream_task_prototype.cu\n  graph_task_prototype.cu\n)\n```\n\n----------------------------------------\n\nTITLE: OR Operation for 32-bit Type\nDESCRIPTION: Template function implementing asynchronous OR reduction operation on cluster memory barrier for b32 type. Takes destination pointer, value and remote barrier pointer as parameters. Uses SFINAE to ensure 4-byte size.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/red_async.rst#2025-04-23_snippet_9\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline void red_async(\n  cuda::ptx::op_or_op_t,\n  B32* dest,\n  const B32& value,\n  uint64_t* remote_bar);\n```\n\n----------------------------------------\n\nTITLE: Defining a CUB Agent Algorithm Policy Template in C++\nDESCRIPTION: This C++ template struct `AgentAlgorithmPolicy` defines a set of compile-time configuration parameters for a CUB agent. It includes settings for block dimensions (`_BLOCK_THREADS`), work distribution (`_ITEMS_PER_THREAD`), data loading algorithm (`_LOAD_ALGORITHM`), and cache behavior (`_LOAD_MODIFIER`). These parameters influence how the agent maps work to the GPU hardware but don't change the functional behavior.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/developer_overview.rst#2025-04-23_snippet_19\n\nLANGUAGE: c++\nCODE:\n```\ntemplate <int _BLOCK_THREADS,\n          int _ITEMS_PER_THREAD,\n          BlockLoadAlgorithm _LOAD_ALGORITHM,\n          CacheLoadModifier _LOAD_MODIFIER>\nstruct AgentAlgorithmPolicy {\n  static constexpr int BLOCK_THREADS    = _BLOCK_THREADS;\n  static constexpr int ITEMS_PER_THREAD = _ITEMS_PER_THREAD;\n  static constexpr int ITEMS_PER_TILE   = BLOCK_THREADS * ITEMS_PER_THREAD;\n  static constexpr cub::BlockLoadAlgorithm LOAD_ALGORITHM   = _LOAD_ALGORITHM;\n  static constexpr cub::CacheLoadModifier LOAD_MODIFIER     = _LOAD_MODIFIER;\n};\n```\n\n----------------------------------------\n\nTITLE: Creating CUDA Pipeline Function Declarations\nDESCRIPTION: Function declarations for creating CUDA pipelines with different configurations, including thread-scoped, unified, and partitioned pipelines.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/synchronization_primitives/pipeline/make_pipeline.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\n// (1)\n__host__ __device__\ncuda::pipeline<cuda::thread_scope_thread> cuda::make_pipeline();\n\n// (2)\ntemplate <typename Group,\n          cuda::thread_scope Scope,\n          cuda::std::uint8_t StagesCount>\n__host__ __device__\ncuda::pipeline<Scope>\ncuda::make_pipeline(Group const& group,\n                    cuda::pipeline_shared_state<Scope, StagesCount>* shared_state);\n\n// (3)\ntemplate <typename Group,\n          cuda::thread_scope Scope,\n          cuda::std::uint8_t StagesCount>\n__host__ __device__\ncuda::pipeline<Scope>\ncuda::make_pipeline(Group const& group,\n                    cuda::pipeline_shared_state<Scope, StagesCount>* shared_state,\n                    cuda::std::size_t producer_count);\n\n// (4)\ntemplate <typename Group,\n          cuda::thread_scope Scope,\n          cuda::std::uint8_t StagesCount>\n__host__ __device__\ncuda::pipeline<Scope>\ncuda::make_pipeline(Group const& group,\n                    cuda::pipeline_shared_state<Scope, StagesCount>* shared_state,\n                    cuda::pipeline_role role);\n```\n\n----------------------------------------\n\nTITLE: Implementing tcgen05 MMA collector_b3_discard PTX Operation without zero_column_mask\nDESCRIPTION: Template function for tensor core matrix multiply accumulate operation with B3 collector and discard option without zero column masking. Supports multiple data types through the template parameter.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_74\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.ws.cta_group.kind.collector::b3::discard [d_tmem], a_desc, b_desc, idesc, enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1 }\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_collector_b3_discard(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Declaring Function with CCCL Type in C++\nDESCRIPTION: Example of a function declaration using a CCCL type (cuda::std::optional) in a public API. This illustrates potential ABI concerns when using CCCL types in binary interfaces.\nSOURCE: https://github.com/nvidia/cccl/blob/main/README.md#2025-04-23_snippet_7\n\nLANGUAGE: C++\nCODE:\n```\nvoid foo(cuda::std::optional<int>);\n```\n\n----------------------------------------\n\nTITLE: Creating Custom TBB and OpenMP Thrust Targets\nDESCRIPTION: Creates two separate Thrust targets with different device systems - one using TBB and another using OpenMP with CPP host system.\nSOURCE: https://github.com/nvidia/cccl/blob/main/lib/cmake/thrust/README.md#2025-04-23_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nthrust_create_target(ThrustTBB DEVICE TBB)\nthrust_create_target(ThrustOMP HOST CPP DEVICE OMP)\n```\n\n----------------------------------------\n\nTITLE: Fixing RMW Behavior for cuda::atomic::fetch_max/min (C++)\nDESCRIPTION: Issue #197 fixed in libcu++ 1.6.0. The implementation of `cuda::atomic::fetch_max/min` was reworked to correctly function as a read-modify-write (RMW) operation.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/releases/changelog.rst#2025-04-23_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\ncuda::atomic::fetch_max/min\n```\n\n----------------------------------------\n\nTITLE: Declaring Load-Reduce Minimum Operation (acquire/cta/global/u64) in CUDA C++\nDESCRIPTION: Defines a device inline template for global min reduction of 64-bit unsigned values, parameterized on PTX acquire or relaxed semaphore semantics and cta/cluster/gpu/sys scopes. Used for atomic parallel reduction in advanced CUDA algorithms. Ensures hardware-compliant memory semantics for min operations.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_14\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.u64 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .min }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline uint64_t multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_min_t,\n  const uint64_t* addr);\n```\n\n----------------------------------------\n\nTITLE: Memory Barrier Test Wait Parity with Relaxed Semantics (Cluster Scope)\nDESCRIPTION: Implementation with relaxed semantics and cluster scope for SM_90. Includes semantic and scope parameters along with address and phase parity.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/mbarrier_test_wait_parity.rst#2025-04-23_snippet_4\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_scope Scope>\n__device__ static inline bool mbarrier_test_wait_parity(\n  cuda::ptx::sem_relaxed_t,\n  cuda::ptx::scope_t<Scope> scope,\n  uint64_t* addr,\n  const uint32_t& phaseParity);\n```\n\n----------------------------------------\n\nTITLE: Declaring CUDA Atomic Thread Fence Function\nDESCRIPTION: Function declaration for cuda::atomic_thread_fence that establishes memory synchronization ordering between threads. Takes memory order and thread scope parameters to control synchronization behavior.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/synchronization_primitives/atomic/atomic_thread_fence.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\n__host__ __device__\nvoid cuda::atomic_thread_fence(cuda::std::memory_order order,\n                                  cuda::thread_scope scope = cuda::thread_scope_system);\n```\n\n----------------------------------------\n\nTITLE: Declaring MMA Block Scale Vector 1X TMEM A Collector for MXF8F6F4 Kind\nDESCRIPTION: Template function for tensor matrix multiply with 1X scaling factor using TMEM for matrix A collection with MXF8F6F4 data type. Supports different CTA groups with configurable input and scaling parameters.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma.rst#2025-04-23_snippet_37\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.cta_group.kind.block_scale.scale_vec::1X.collector::a::use [d_tmem], a_desc, b_desc, idesc, [scale_A_tmem], [scale_B_tmem], enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .kind      = { .kind::mxf8f6f4 }\n// .cta_group = { .cta_group::1, .cta_group::2 }\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_mma_block_scale_vec_1x_tmem_a_collector_a_use(\n  cuda::ptx::kind_mxf8f6f4_t,\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  uint32_t scale_A_tmem,\n  uint32_t scale_B_tmem,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Store 32-bit Data with L1 No Allocate and L2 Cache Hint in CUDA\nDESCRIPTION: This function stores 32-bit data to global memory with L1 cache no allocate policy and L2 cache hint. It requires SM_80 or later architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/st.rst#2025-04-23_snippet_17\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline void st_L1_no_allocate_L2_cache_hint(\n  cuda::ptx::space_global_t,\n  B32* addr,\n  B32 src,\n  uint64_t cache_policy);\n```\n\n----------------------------------------\n\nTITLE: Storing 32x32b Data with tcgen05 in CUDA (4 values)\nDESCRIPTION: This function performs a 32x32b store operation using tcgen05. It takes a 32-bit address and an array of 4 B32 values as input. The function is constrained to B32 types of 4 bytes.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_st.rst#2025-04-23_snippet_21\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.st.sync.aligned.32x32b.x4.b32 [taddr], values; // PTX ISA 86, SM_100a, SM_101a\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline void tcgen05_st_32x32b(\n  uint32_t taddr,\n  const B32 (&values)[4]);\n```\n\n----------------------------------------\n\nTITLE: Demonstrating cuda::get_device_address Usage in CUDA C++\nDESCRIPTION: This snippet shows how to use cuda::get_device_address to obtain a valid device pointer, contrasting it with the traditional method. It illustrates the difference in pointer attributes and kernel call safety.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/functional/get_device_address.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\n#include <cuda/functional>\n\n__device__ int device_object[] = {42, 1337, -1, 0};\n\n__global__ void example_kernel(int *data) { ... }\n\nvoid example()\n{\n  {\n    T* host_address = cuda::std::addressof(device_object);\n\n    cudaPointerAttributes attributes;\n    cudaError_t status = cudaPointerGetAttributes(&attributes, host_address);\n    assert(status == cudaSuccess);\n    assert(attributes.devicePointer == nullptr);\n\n    // Calling a kernel with host_address would segfault\n    // example_kernel<<<1, 1>>>(host_address);\n  }\n\n  {\n    T* device_address = cuda::get_device_address(device_object);\n\n    cudaPointerAttributes attributes;\n    cudaError_t status = cudaPointerGetAttributes(&attributes, device_address);\n    assert(status == cudaSuccess);\n    assert(attributes.devicePointer == device_address);\n\n    // Safe to call a kernel\n    example_kernel<<<1, 1>>>(device_address);\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Tensor Core Collector Fill Operation in CUDA\nDESCRIPTION: This template function initializes a tensor core collector fill operation for various data types. It takes parameters for CTA group, data kind, memory addresses, and descriptors.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_36\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_tmem_a_collector_b2_fill(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint32_t a_tmem,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Implementing Multimem Load-Reduce for 32-bit Integer Max Operation in CUDA\nDESCRIPTION: This template function performs a multimem load-reduce operation for 32-bit integers using the max operation. It supports different semantics and scopes, allowing for flexible memory ordering and visibility across different execution units.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_26\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.s32 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .max }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline int32_t multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_max_t,\n  const int32_t* addr);\n```\n\n----------------------------------------\n\nTITLE: Preprocessor Macro Usage for JIT Templates\nDESCRIPTION: Demonstrates the usage of _CCCL_C_PARALLEL_JIT_TEMPLATES_PREPROCESS macro for controlling header visibility and preprocessing behavior. This macro guards host-only logic and manages header inclusion during the preprocessing phase.\nSOURCE: https://github.com/nvidia/cccl/blob/main/c/parallel/src/jit_templates/README.md#2025-04-23_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\n#ifdef _CCCL_C_PARALLEL_JIT_TEMPLATES_PREPROCESS\n// Template traits and parameter_mapping specializations go here\n#endif\n```\n\n----------------------------------------\n\nTITLE: Defining CUDA maximum and minimum function objects\nDESCRIPTION: Template declarations for cuda::maximum and cuda::minimum function objects that can be used for both host and device code. The template specializations for void allow for handling different types with common type promotion.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/functional/maximum_minimum.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename T>\nstruct maximum {\n    [[nodiscard]] __host__ __device__ inline\n    T operator()(T a, T b) const;\n};\n\ntemplate <>\nstruct maximum<void> {\n    template <typename T1, typename T2>\n    [[nodiscard]] __host__ __device__ inline\n    cuda::std::common_type_t<T1, T2> operator()(T1 a, T2 b) const;\n};\n\ntemplate <typename T>\nstruct minimum {\n    [[nodiscard]] __host__ __device__ inline\n    T operator()(T a, T b) const;\n};\n\ntemplate <>\nstruct minimum<void> {\n    template <typename T1, typename T2>\n    [[nodiscard]] __host__ __device__ inline\n    cuda::std::common_type_t<T1, T2> operator()(T1 a, T2 b) const;\n};\n```\n\n----------------------------------------\n\nTITLE: Configuring CUB Examples Build in CMake\nDESCRIPTION: CMake script that discovers CUDA example files and sets up build targets for CUB examples. It recursively finds all files matching 'example_*.cu' pattern, processes each file to create appropriately named targets, and configures them for building against different CUB targets.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cub/examples/device/CMakeLists.txt#2025-04-23_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB_RECURSE example_srcs\n  RELATIVE \"${CMAKE_CURRENT_LIST_DIR}\"\n  CONFIGURE_DEPENDS\n  example_*.cu\n)\n\nforeach (cub_target IN LISTS CUB_TARGETS)\n  foreach (example_src IN LISTS example_srcs)\n    get_filename_component(example_name \"${example_src}\" NAME_WE)\n    string(REGEX REPLACE\n      \"^example_device_\" \"device.\"\n      example_name \"${example_name}\"\n    )\n    cub_add_example(target_name ${example_name} \"${example_src}\" ${cub_target})\n  endforeach()\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Store Synchronization Handler for Thread Block\nDESCRIPTION: Device function providing synchronized store operations across thread blocks. Requires PTX ISA 86 and is compatible with SM_100a and SM_101a architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_wait.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.wait::st.sync.aligned; // PTX ISA 86, SM_100a, SM_101a\ntemplate <typename = void>\n__device__ static inline void tcgen05_wait_st();\n```\n\n----------------------------------------\n\nTITLE: Implementing mbarrier.expect_tx with cluster scope and shared cluster space\nDESCRIPTION: Template implementation for mbarrier expect transaction with relaxed semantics, cluster scope, and shared cluster memory space. Requires PTX ISA 80 and SM_90.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/mbarrier_expect_tx.rst#2025-04-23_snippet_3\n\nLANGUAGE: cuda\nCODE:\n```\n// mbarrier.expect_tx.sem.scope.space.b64 [addr], txCount; // 2. PTX ISA 80, SM_90\n// .sem       = { .relaxed }\n// .scope     = { .cta, .cluster }\n// .space     = { .shared::cluster }\ntemplate <cuda::ptx::dot_scope Scope>\n__device__ static inline void mbarrier_expect_tx(\n  cuda::ptx::sem_relaxed_t,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::space_cluster_t,\n  uint64_t* addr,\n  uint32_t txCount);\n```\n\n----------------------------------------\n\nTITLE: Tensormap Copy Fence Proxy - Cluster Scope\nDESCRIPTION: Template function for tensormap copy operation with release semantics and cluster scope synchronization. Handles aligned memory transfers between global and shared memory.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tensormap_cp_fenceproxy.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n// tensormap.cp_fenceproxy.global.shared::cta.tensormap::generic.sem.scope.sync.aligned  [dst], [src], size; // PTX ISA 83, SM_90\n// .sem       = { .release }\n// .scope     = { .cta, .cluster, .gpu, .sys }\ntemplate <int N32, cuda::ptx::dot_scope Scope>\n__device__ static inline void tensormap_cp_fenceproxy(\n  cuda::ptx::sem_release_t,\n  cuda::ptx::scope_t<Scope> scope,\n  void* dst,\n  const void* src,\n  cuda::ptx::n32_t<N32> size);\n```\n\n----------------------------------------\n\nTITLE: CUDA Async Bulk Reduction with ADD Operation (FP16, NoFTZ)\nDESCRIPTION: Template function for performing asynchronous bulk reduction with ADD operation (no flush-to-zero) on FP16 data from shared CTA memory to global memory. Requires PTX ISA 80 and SM_90.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_reduce_async_bulk_f16.rst#2025-04-23_snippet_2\n\nLANGUAGE: cuda\nCODE:\n```\n// cp.reduce.async.bulk.dst.src.bulk_group.op.noftz.type  [dstMem], [srcMem], size; // 5. PTX ISA 80, SM_90\n// .dst       = { .global }\n// .src       = { .shared::cta }\n// .type      = { .f16 }\n// .op        = { .add }\ntemplate <typename = void>\n__device__ static inline void cp_reduce_async_bulk(\n  cuda::ptx::space_global_t,\n  cuda::ptx::space_shared_t,\n  cuda::ptx::op_add_t,\n  __half* dstMem,\n  const __half* srcMem,\n  uint32_t size);\n```\n\n----------------------------------------\n\nTITLE: Adding Convenience Headers for Synchronization Primitives (C++)\nDESCRIPTION: Enhancement in libcu++ 1.2.0. Added non-standard headers `<cuda/latch>` and `<cuda/semaphore>` as convenience wrappers for accessing `cuda::latch`, `cuda::counting_semaphore`, and `cuda::binary_semaphore`. These features were previously only accessible via `<cuda/std/latch>` and `<cuda/std/semaphore>`.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/releases/changelog.rst#2025-04-23_snippet_25\n\nLANGUAGE: cpp\nCODE:\n```\n<cuda/latch>\n```\n\nLANGUAGE: cpp\nCODE:\n```\n<cuda/semaphore>\n```\n\nLANGUAGE: cpp\nCODE:\n```\ncuda::latch\n```\n\nLANGUAGE: cpp\nCODE:\n```\ncuda::counting_semaphore\n```\n\nLANGUAGE: cpp\nCODE:\n```\ncuda::binary_semaphore\n```\n\nLANGUAGE: cpp\nCODE:\n```\n<cuda/std/latch> // Original header\n```\n\nLANGUAGE: cpp\nCODE:\n```\n<cuda/std/semaphore> // Original header\n```\n\n----------------------------------------\n\nTITLE: Declaring CUDA PTX Multimem Reduction AND Template with 64-bit Values\nDESCRIPTION: Template declaration for performing atomic AND reduction operations on 64-bit values with different memory semantics and memory scopes. The template accepts semantics parameters (relaxed/release), scope parameters (cta/cluster/gpu/sys), and uses the PTX multimem.red instruction.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_red.rst#2025-04-23_snippet_37\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.red.sem.scope.global.op.b64 [addr], val; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .release }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .and }\ntemplate <typename B64, enable_if_t<sizeof(B64) == 8, bool> = true, cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline void multimem_red(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_and_op_t,\n  B64* addr,\n  B64 val);\n```\n\n----------------------------------------\n\nTITLE: Defining TCGEN05 MMA Workspace TMEM A Collector B2 Fill Function Template without Parameters\nDESCRIPTION: A header-only declaration for the TCGen05 MMA workspace operation using tmem for operand A with collector B2 fill mode. This snippet only shows the function comment pattern without the complete function signature.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_35\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.ws.cta_group.kind.collector::b2::fill [d_tmem], [a_tmem], b_desc, idesc, enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1 }\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\n```\n\n----------------------------------------\n\nTITLE: Ceiling Integer Base-2 Logarithm Function Declaration in CUDA\nDESCRIPTION: Template function declaration for computing the ceiling of logarithm base 2 of an integer value. The function is available for both host and device execution.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/math/ilog.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename T>\n[[nodiscard]] __host__ __device__ inline constexpr\nint ceil_ilog2(T value) noexcept;\n```\n\n----------------------------------------\n\nTITLE: Creating and Configuring Thrust Example Executable in CMake\nDESCRIPTION: Creates a CMake target for the example program, sets required C++ standards, configures the source file language for CUDA, and links against the CCCL library. Also sets up testing configuration.\nSOURCE: https://github.com/nvidia/cccl/blob/main/examples/thrust_flexible_device_system/CMakeLists.txt#2025-04-23_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\n# Creates a cmake executable target for the main program\nadd_executable(example_program example.cpp)\n # Thrust requires at least C++17:\ntarget_compile_features(example_program PUBLIC cuda_std_17 cxx_std_17)\n\n# By default, CMake inspects the source file extension to determine whether to use C++ or CUDA\n# compilers. We can override this behavior by using source file properties. Here, we tell CMake\n# to compile this C++ (.cpp) file with the CUDA compiler when using the CUDA device system:\nif (CCCL_THRUST_DEVICE_SYSTEM STREQUAL \"CUDA\")\n  set_source_files_properties(example.cpp PROPERTIES LANGUAGE CUDA)\nendif()\n\n# \"Links\" the CCCL Cmake target to the `example_program` executable. This configures everything needed to use\n# CCCL headers, including setting up include paths, compiler flags, Thrust host/device configuration, etc.\ntarget_link_libraries(example_program PRIVATE CCCL::CCCL)\n\n# This is only relevant for internal testing and not needed by end users.\ninclude(CTest)\nenable_testing()\nadd_test(NAME example_program COMMAND example_program)\nset_tests_properties(example_program PROPERTIES\n  PASS_REGULAR_EXPRESSION \"Detected device system: ${CCCL_THRUST_DEVICE_SYSTEM}\"\n)\n```\n\n----------------------------------------\n\nTITLE: Declaring cuda::device::barrier_native_handle Function in CUDA\nDESCRIPTION: Function declaration for cuda::device::barrier_native_handle, which returns a pointer to the native handle of a cuda::barrier object with thread_scope_block scope.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/synchronization_primitives/barrier/barrier_native_handle.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\n__device__ cuda::std::uint64_t* cuda::device::barrier_native_handle(\n  cuda::barrier<cuda::thread_scope_block>& bar);\n```\n\n----------------------------------------\n\nTITLE: Building and Running the CCCL Example\nDESCRIPTION: Series of bash commands for cloning the CCCL repository, navigating to the example project directory, configuring with CMake, building the project, and running the resulting executable.\nSOURCE: https://github.com/nvidia/cccl/blob/main/examples/basic/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/NVIDIA/cccl.git\ncd cccl/examples/example_project\ncmake -S . -B build\ncmake --build .\n./build/example_project\n```\n\n----------------------------------------\n\nTITLE: Setting Header Installation Directory Variable\nDESCRIPTION: Defines a CMake variable `_dest_incl_dir` and assigns it the relative path `cuda/cccl/include`. This variable is used later in `install` commands to specify the destination directory for header files relative to the installation prefix.\nSOURCE: https://github.com/nvidia/cccl/blob/main/python/cuda_cccl/CMakeLists.txt#2025-04-23_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nset(_dest_incl_dir cuda/cccl/include)\n```\n\n----------------------------------------\n\nTITLE: Performing Asynchronous Bulk Reduction (MAX, u32) from Shared to Global Memory in CUDA\nDESCRIPTION: This CUDA C++ device function `cp_reduce_async_bulk` performs an asynchronous bulk maximum reduction for unsigned 32-bit integers. It reduces data from a source array in shared memory (`srcMem`) to a destination array in global memory (`dstMem`). This overload requires PTX ISA 80 and SM_90 capability.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_reduce_async_bulk.rst#2025-04-23_snippet_17\n\nLANGUAGE: cuda\nCODE:\n```\n   // cp.reduce.async.bulk.dst.src.bulk_group.op.type  [dstMem], [srcMem], size; // 4. PTX ISA 80, SM_90\n   // .dst       = { .global }\n   // .src       = { .shared::cta }\n   // .type      = { .u32 }\n   // .op        = { .max }\n   template <typename = void>\n   __device__ static inline void cp_reduce_async_bulk(\n     cuda::ptx::space_global_t,\n     cuda::ptx::space_shared_t,\n     cuda::ptx::op_max_t,\n     uint32_t* dstMem,\n     const uint32_t* srcMem,\n     uint32_t size);\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Bulk Tensor Reduction for 1D Tensors in CUDA\nDESCRIPTION: This template function performs asynchronous bulk tensor reduction for 1D tensors. It supports various reduction operations between global and shared memory spaces. The function is designed for PTX ISA 80 and SM_90 architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_reduce_async_bulk_tensor.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\n// cp.reduce.async.bulk.tensor.1d.dst.src.op.tile.bulk_group [tensorMap, tensorCoords], [srcMem]; // 1a. PTX ISA 80, SM_90\n// .dst       = { .global }\n// .src       = { .shared::cta }\n// .op        = { .add, .min, .max, .inc, .dec, .and, .or, .xor }\ntemplate <cuda::ptx::dot_op Op>\n__device__ static inline void cp_reduce_async_bulk_tensor(\n  cuda::ptx::space_global_t,\n  cuda::ptx::space_shared_t,\n  cuda::ptx::op_t<Op> op,\n  const void* tensorMap,\n  const int32_t (&tensorCoords)[1],\n  const void* srcMem);\n```\n\n----------------------------------------\n\nTITLE: Defining cuda::device::memcpy_async_tx Function in CUDA C++\nDESCRIPTION: Function signature for cuda::device::memcpy_async_tx, which copies bytes from global memory to shared memory and decrements a barrier's transaction count. It requires CUDA Compute Capability 9.0 or higher.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/asynchronous_operations/memcpy_async_tx.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename T, size_t Alignment>\ninline __device__\nvoid cuda::device::memcpy_async_tx(\n  T* dest,\n  const T* src,\n  cuda::aligned_size_t<Alignment> size,\n  cuda::barrier<cuda::thread_scope_block>& bar);\n```\n\n----------------------------------------\n\nTITLE: Storing 16x32bx2 Data with tcgen05 Instruction in CUDA\nDESCRIPTION: This snippet defines device functions for storing 16x32bx2 data using the tcgen05 instruction. It includes variations for different array sizes (1, 2, 4, 8, 16, 32, 64, 128) and unpacking operations for 16-bit values.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_st.rst#2025-04-23_snippet_28\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.st.sync.aligned.16x32bx2.x1.b32 [taddr], immHalfSplitoff, values; // PTX ISA 86, SM_100a, SM_101a\ntemplate <int N32, typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline void tcgen05_st_16x32bx2(\n  uint32_t taddr,\n  cuda::ptx::n32_t<N32> immHalfSplitoff,\n  const B32 (&values)[1]);\n```\n\n----------------------------------------\n\nTITLE: Device Pointer Cast Example\nDESCRIPTION: Example showing proper casting of device pointers\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/releases/changelog.rst#2025-04-23_snippet_11\n\nLANGUAGE: C++\nCODE:\n```\ndevice_pointer_cast(static_cast<int*>(void_ptr.get()))\n```\n\n----------------------------------------\n\nTITLE: Async Bulk Tensor Scatter to Global Memory\nDESCRIPTION: Template function for asynchronous bulk tensor scattering from shared CTA memory to global memory. Uses scatter4 operation with bulk group configuration.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_async_bulk_tensor_gather_scatter.rst#2025-04-23_snippet_3\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline void cp_async_bulk_tensor_tile_scatter4(\n  cuda::ptx::space_global_t,\n  cuda::ptx::space_shared_t,\n  const void* tensorMap,\n  const int32_t (&tensorCoords)[5],\n  const void* srcMem);\n```\n\n----------------------------------------\n\nTITLE: Global Memory Store with L2 Cache Hints\nDESCRIPTION: Template functions for global memory stores with L2 cache hints, supporting various data sizes. These operations require SM_80+ and allow specifying cache policies for L2 cache behavior.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/st.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n// st.space.L2::cache_hint.b8 [addr], src, cache_policy; // PTX ISA 74, SM_80\ntemplate <typename B8, enable_if_t<sizeof(B8) == 1, bool> = true>\n__device__ static inline void st_L2_cache_hint(\n  cuda::ptx::space_global_t,\n  B8* addr,\n  B8 src,\n  uint64_t cache_policy);\n```\n\n----------------------------------------\n\nTITLE: Implementing multimem.red.relaxed.gpu.global.add.u32 Operation in CUDA PTX\nDESCRIPTION: Template function declaration for performing a 32-bit unsigned integer addition reduction with relaxed semantics and GPU scope in global memory. This function is available for SM_90 and requires PTX ISA 8.1 or higher.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_red.rst#2025-04-23_snippet_20\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.red.sem.scope.global.op.u32 [addr], val; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .release }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .add }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline void multimem_red(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_add_t,\n  uint32_t* addr,\n  uint32_t val);\n```\n\n----------------------------------------\n\nTITLE: MMA Block Scale Vector 2X Collector Fill with mxf4nvf4 Type (CTA Group 1)\nDESCRIPTION: Template function for matrix multiplication with block scaling using vector 2X with collector A fill operation for CTA group 1. The function can use either mxf4 or mxf4nvf4 data types and requires PTX ISA 86 with SM_100a or SM_101a architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma.rst#2025-04-23_snippet_28\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.cta_group.kind.block_scale.scale_vec::2X.collector::a::fill [d_tmem], a_desc, b_desc, idesc, [scale_A_tmem], [scale_B_tmem], enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .kind      = { .kind::mxf4, .kind::mxf4nvf4 }\n// .cta_group = { .cta_group::1, .cta_group::2 }\ntemplate <cuda::ptx::dot_kind Kind, cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_mma_block_scale_vec_2_tmem_a_collector_a_fill(\n  cuda::ptx::kind_t<Kind> kind,\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  uint32_t scale_A_tmem,\n  uint32_t scale_B_tmem,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Thrust Type Deduction Helpers\nDESCRIPTION: C++11-specific type deduction helper macros for function definitions and lambda expressions\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/releases/changelog.rst#2025-04-23_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\nTHRUST_DECLTYPE_RETURNS*\nTHRUST_FWD(x)\nTHRUST_MVCAP\nTHRUST_RETOF\n```\n\n----------------------------------------\n\nTITLE: Declaring Device Accessible Memory Resource Property in C++\nDESCRIPTION: Demonstrates how to declare a memory resource as device accessible using the cuda::mr::device_accessible tag type and a get_property free function.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/memory_resource/properties.rst#2025-04-23_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nstruct my_memory_resource {\n    friend constexpr void get_property(const my_memory_resource&, cuda::mr::device_accessible) noexcept {}\n};\n```\n\n----------------------------------------\n\nTITLE: CTA Group Memory Allocation (Group 2)\nDESCRIPTION: Template function for allocating shared memory in CTA group 2. Takes destination pointer and number of columns as parameters. Compatible with PTX ISA 86 and SM_100a/101a architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_alloc.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.alloc.cta_group.sync.aligned.shared::cta.b32 [dst], nCols; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1, .cta_group::2 }\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_alloc(\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t* dst,\n  const uint32_t& nCols);\n```\n\n----------------------------------------\n\nTITLE: Implementing tcgen05 MMA Block Scale with mxf4/mxf4nvf4 2X Vector Scaling and A Collector\nDESCRIPTION: Function declaration for tcgen05 matrix multiplication with templated Kind and Cta_Group parameters, using mxf4 or mxf4nvf4 data types with 2X vector scaling and collector strategy for matrix A with last use option. Requires CUDA PTX ISA 86 and SM_100a or SM_101a.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma.rst#2025-04-23_snippet_43\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.cta_group.kind.block_scale.scale_vec::2X.collector::a::lastuse [d_tmem], a_desc, b_desc, idesc, [scale_A_tmem], [scale_B_tmem], enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .kind      = { .kind::mxf4, .kind::mxf4nvf4 }\n// .cta_group = { .cta_group::1, .cta_group::2 }\ntemplate <cuda::ptx::dot_kind Kind, cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_mma_block_scale_vec_2_tmem_a_collector_a_lastuse(\n  cuda::ptx::kind_t<Kind> kind,\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  uint32_t scale_A_tmem,\n  uint32_t scale_B_tmem,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Implementing multimem.ld_reduce for acquire system global add operations on 32-bit integers\nDESCRIPTION: Template implementation for multimem atomic load-and-reduce operations with acquire semantics, system scope, global memory, and add operation on 32-bit integers. Compatible with PTX ISA 81 on SM_90 architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_39\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.s32 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .add }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline int32_t multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_add_t,\n  const int32_t* addr);\n```\n\n----------------------------------------\n\nTITLE: Subscript Operator for cuda::annotated_ptr in CUDA\nDESCRIPTION: Subscript operator for cuda::annotated_ptr that allows indexed access to the pointed-to array, applying the associated access property.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/memory_access_properties/annotated_ptr.rst#2025-04-23_snippet_6\n\nLANGUAGE: cuda\nCODE:\n```\nreference operator[](ptrdiff_t i) const;\n```\n\n----------------------------------------\n\nTITLE: 16-bit Signed Right Shift Operation in CUDA\nDESCRIPTION: Template function for performing signed right shift on 16-bit integers. Takes an int16_t value and shift amount as parameters.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/shr.rst#2025-04-23_snippet_3\n\nLANGUAGE: cuda\nCODE:\n```\n// shr.s16 dest, a_reg, b_reg; // PTX ISA 10, SM_50\ntemplate <typename = void>\n__device__ static inline int16_t shr(\n  int16_t a_reg,\n  uint32_t b_reg);\n```\n\n----------------------------------------\n\nTITLE: Implementing mbarrier.expect_tx with CTA scope and shared cluster space\nDESCRIPTION: Template implementation for mbarrier expect transaction with relaxed semantics, CTA scope, and shared cluster memory space. Requires PTX ISA 80 and SM_90.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/mbarrier_expect_tx.rst#2025-04-23_snippet_2\n\nLANGUAGE: cuda\nCODE:\n```\n// mbarrier.expect_tx.sem.scope.space.b64 [addr], txCount; // 2. PTX ISA 80, SM_90\n// .sem       = { .relaxed }\n// .scope     = { .cta, .cluster }\n// .space     = { .shared::cluster }\ntemplate <cuda::ptx::dot_scope Scope>\n__device__ static inline void mbarrier_expect_tx(\n  cuda::ptx::sem_relaxed_t,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::space_cluster_t,\n  uint64_t* addr,\n  uint32_t txCount);\n```\n\n----------------------------------------\n\nTITLE: Storing and Unpacking 16x256b Data with tcgen05 in CUDA (128 values)\nDESCRIPTION: This function performs a 16x256b store operation with 16-bit unpacking using tcgen05. It takes a 32-bit address and an array of 128 B32 values as input. The function is constrained to B32 types of 4 bytes.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_st.rst#2025-04-23_snippet_16\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.st.sync.aligned.16x256b.x32.unpack::16b.b32 [taddr], values; // PTX ISA 86, SM_100a, SM_101a\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline void tcgen05_st_16x256b_unpack_16b(\n  uint32_t taddr,\n  const B32 (&values)[128]);\n```\n\n----------------------------------------\n\nTITLE: Attempting Multicast Cluster Launch Cancellation in CUDA\nDESCRIPTION: Defines a device function to attempt cancellation of a multicast cluster launch. It takes a memory address and a shared memory barrier as parameters.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/clusterlaunchcontrol.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline void clusterlaunchcontrol_try_cancel_multicast(\n  void* addr,\n  uint64_t* smem_bar);\n```\n\n----------------------------------------\n\nTITLE: Implementing multimem.red.release.sys.global.add.u32 Operation in CUDA PTX\nDESCRIPTION: Template function declaration for performing a 32-bit unsigned integer addition reduction with release semantics and system-wide scope in global memory. This function is available for SM_90 and requires PTX ISA 8.1 or higher.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_red.rst#2025-04-23_snippet_25\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.red.sem.scope.global.op.u32 [addr], val; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .release }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .add }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline void multimem_red(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_add_t,\n  uint32_t* addr,\n  uint32_t val);\n```\n\n----------------------------------------\n\nTITLE: Initializing CUDA Shared Memory Barrier\nDESCRIPTION: Device function template for initializing a memory barrier in shared memory. Takes a 64-bit address pointer and count parameter. Compatible with PTX ISA 70 and SM_80 architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/mbarrier_init.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\n// mbarrier.init.shared.b64 [addr], count; // PTX ISA 70, SM_80\ntemplate <typename = void>\n__device__ static inline void mbarrier_init(\n  uint64_t* addr,\n  const uint32_t& count);\n```\n\n----------------------------------------\n\nTITLE: Defining tcgen05_mma_ws_tmem_a_collector_b3_fill without Zero Column Mask for CUDA PTX\nDESCRIPTION: Template function declaration for filling b3 collector with memory-backed A matrix. This variant omits the zero column mask parameter while supporting the same range of data types for MMA workstation operations.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_63\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.ws.cta_group.kind.collector::b3::fill [d_tmem], [a_tmem], b_desc, idesc, enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1 }\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_tmem_a_collector_b3_fill(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint32_t a_tmem,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Creating Meta Example Build Targets Per Configuration - CMake\nDESCRIPTION: This snippet loops over all registered THRUST_TARGETS to generate meta build targets that group all example executables per configuration, ensuring hierarchical build and dependency management. The pattern allows for easy bulk-building or integration into CI systems. Dependencies include existing thrust_get_target_property and add_custom_target CMake functions, and assumes THRUST_TARGETS are previously populated.\nSOURCE: https://github.com/nvidia/cccl/blob/main/thrust/examples/CMakeLists.txt#2025-04-23_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\n# Create meta targets that build all examples for a single configuration:\nforeach(thrust_target IN LISTS THRUST_TARGETS)\n  thrust_get_target_property(config_prefix ${thrust_target} PREFIX)\n  set(config_meta_target ${config_prefix}.examples)\n  add_custom_target(${config_meta_target})\n  add_dependencies(${config_prefix}.all ${config_meta_target})\nendforeach()\n\n```\n\n----------------------------------------\n\nTITLE: Configuring NVBench Helper Tests Option in CMake\nDESCRIPTION: Sets up a CMake option to enable or disable tests for the nvbench_helper library, marked as an advanced option.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cub/benchmarks/nvbench_helper/CMakeLists.txt#2025-04-23_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\noption(CUB_ENABLE_NVBENCH_HELPER_TESTS \"Enable tests for nvbench_helper\" OFF)\nmark_as_advanced(CUB_ENABLE_NVBENCH_HELPER_TESTS)\n```\n\n----------------------------------------\n\nTITLE: Add Operation for Signed 64-bit Integer\nDESCRIPTION: Template function implementing asynchronous addition reduction operation on cluster memory barrier for s64 type. Takes destination pointer, value and remote barrier pointer as parameters.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/red_async.rst#2025-04-23_snippet_12\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline void red_async(\n  cuda::ptx::op_add_t,\n  int64_t* dest,\n  const int64_t& value,\n  int64_t* remote_bar);\n```\n\n----------------------------------------\n\nTITLE: Performing Asynchronous Bulk Reduction (MAX, u64) from Shared to Global Memory in CUDA\nDESCRIPTION: This CUDA C++ device function `cp_reduce_async_bulk` performs an asynchronous bulk maximum reduction for unsigned 64-bit integers. It reduces data from a source array in shared memory (`srcMem`) to a destination array in global memory (`dstMem`). This overload requires PTX ISA 80 and SM_90 capability.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_reduce_async_bulk.rst#2025-04-23_snippet_25\n\nLANGUAGE: cuda\nCODE:\n```\n   // cp.reduce.async.bulk.dst.src.bulk_group.op.type  [dstMem], [srcMem], size; // 4. PTX ISA 80, SM_90\n   // .dst       = { .global }\n   // .src       = { .shared::cta }\n   // .type      = { .u64 }\n   // .op        = { .max }\n   template <typename = void>\n   __device__ static inline void cp_reduce_async_bulk(\n     cuda::ptx::space_global_t,\n     cuda::ptx::space_shared_t,\n     cuda::ptx::op_max_t,\n     uint64_t* dstMem,\n     const uint64_t* srcMem,\n     uint32_t size);\n```\n\n----------------------------------------\n\nTITLE: Using cuda::device::barrier_expect_tx in a CUDA Kernel\nDESCRIPTION: Example CUDA kernel demonstrating the usage of cuda::device::barrier_expect_tx along with other synchronization primitives. It shows how to initialize a barrier, use memcpy_async_tx, and increment the barrier's expected transaction count.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/synchronization_primitives/barrier/barrier_expect_tx.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n#include <cuda/barrier>\n#include <cuda/std/utility> // cuda::std::move\n\n#if defined(__CUDA_MINIMUM_ARCH__) && __CUDA_MINIMUM_ARCH__ < 900\nstatic_assert(false, \"Insufficient CUDA Compute Capability: cuda::device::memcpy_expect_tx is not available.\");\n#endif // __CUDA_MINIMUM_ARCH__\n\n__device__ alignas(16) int gmem_x[2048];\n\n__global__ void example_kernel() {\n    using barrier_t = cuda::barrier<cuda::thread_scope_block>;\n  alignas(16) __shared__ int smem_x[1024];\n  __shared__ barrier_t bar;\n\n  if (threadIdx.x == 0) {\n    init(&bar, blockDim.x);\n  }\n  __syncthreads();\n\n  if (threadIdx.x == 0) {\n    cuda::device::memcpy_async_tx(smem_x, gmem_x, cuda::aligned_size_t<16>(sizeof(smem_x)), bar);\n    cuda::device::barrier_expect_tx(bar, sizeof(smem_x));\n  }\n  auto token = bar.arrive(1);\n\n  bar.wait(cuda::std::move(token));\n\n  // smem_x contains the contents of gmem_x[0], ..., gmem_x[1023]\n  smem_x[threadIdx.x] += 1;\n}\n```\n\n----------------------------------------\n\nTITLE: Adding and Configuring CUB Tests\nDESCRIPTION: Main function for adding test executables and registering them with CTest. It handles both Catch2 and regular tests differently, including special configuration for NVRTC, iterator tests, and API examples. The function also sets up test dependencies and properties.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cub/test/CMakeLists.txt#2025-04-23_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\n## cub_add_test\n#\n# Add a test executable and register it with ctest.\n#\n# target_name_var: Variable name to overwrite with the name of the test\n#   target. Useful for post-processing target information.\n# test_name: The name of the test minus \"<config_prefix>.test.\" For example,\n#   testing/vector.cu will be \"vector\", and testing/cuda/copy.cu will be\n#   \"cuda.copy\".\n# test_src: The source file that implements the test.\n# cub_target: The reference cub target with configuration information.\n#\nfunction(cub_add_test target_name_var test_name test_src cub_target launcher_id)\n  cub_get_target_property(config_prefix ${cub_target} PREFIX)\n\n  _cub_is_catch2_test(is_catch2_test \"${test_src}\")\n  _cub_launcher_requires_rdc(cdp_val \"${launcher_id}\")\n\n  # The actual name of the test's target:\n  set(test_target ${config_prefix}.test.${test_name})\n  set(${target_name_var} ${test_target} PARENT_SCOPE)\n\n  set(config_meta_target ${config_prefix}.tests)\n\n  if (is_catch2_test)\n    # Per config helper library:\n    set(config_c2h_target ${config_prefix}.test.catch2_helper.lid_${launcher_id})\n    if (NOT TARGET ${config_c2h_target})\n      add_library(${config_c2h_target} INTERFACE)\n      target_include_directories(${config_c2h_target} INTERFACE \"${CUB_SOURCE_DIR}/test\")\n      cub_clone_target_properties(${config_c2h_target} ${cub_target})\n      cub_configure_cuda_target(${config_c2h_target} RDC ${cdp_val})\n      target_link_libraries(${config_c2h_target} INTERFACE\n        ${cub_target}\n        cccl.c2h\n        CUDA::nvrtc\n        CUDA::cuda_driver\n      )\n    endif() # config_c2h_target\n\n    add_executable(${test_target} \"${test_src}\")\n    target_link_libraries(${test_target} PRIVATE cccl.c2h.main)\n    add_dependencies(${config_meta_target} ${test_target})\n\n    add_test(NAME ${test_target} COMMAND \"$<TARGET_FILE:${test_target}>\")\n\n    if (\"${test_target}\" MATCHES \"nvrtc\")\n      configure_file(\"cmake/nvrtc_args.h.in\" ${CMAKE_CURRENT_BINARY_DIR}/nvrtc_args.h)\n      target_include_directories(${test_target} PRIVATE ${CMAKE_CURRENT_BINARY_DIR})\n    endif()\n\n    if (\"${test_target}\" MATCHES \"test.iterator\")\n      target_compile_options(${test_target} PRIVATE -ftemplate-depth=1000) # for handling large type lists\n    endif()\n\n    # enable lambdas for all API examples\n    if (\"${test_target}\" MATCHES \"test.[A-Za-z0-9_]+_api\")\n      target_compile_options(${test_target} PRIVATE $<$<COMPILE_LANG_AND_ID:CUDA,NVIDIA>:--extended-lambda>)\n    endif()\n\n    target_link_libraries(${test_target} PRIVATE\n      ${cub_target}\n      ${config_c2h_target}\n      Catch2::Catch2\n    )\n    cub_clone_target_properties(${test_target} ${cub_target})\n    target_include_directories(${test_target}\n      PUBLIC \"${CUB_SOURCE_DIR}/test\"\n    )\n  else() # Not catch2:\n    # Related target names:\n    set(test_meta_target cub.all.test.${test_name})\n\n    add_executable(${test_target} \"${test_src}\")\n    target_link_libraries(${test_target} PRIVATE\n      ${cub_target}\n      cccl.c2h\n    )\n    cub_clone_target_properties(${test_target} ${cub_target})\n    target_include_directories(${test_target} PRIVATE \"${CUB_SOURCE_DIR}/test\")\n    target_compile_definitions(${test_target} PRIVATE CUB_DEBUG_SYNC)\n\n    if (\"${test_target}\" MATCHES \"nvtx_in_usercode\")\n      target_link_libraries(${test_target} PRIVATE nvtx3-cpp)\n    endif()\n\n    _cub_is_fail_test(is_fail_test \"${test_src}\")\n    if (is_fail_test)\n      set_target_properties(${test_target} PROPERTIES EXCLUDE_FROM_ALL true\n                                           EXCLUDE_FROM_DEFAULT_BUILD true)\n      add_test(NAME ${test_target}\n               COMMAND ${CMAKE_COMMAND} --build \"${CMAKE_BINARY_DIR}\"\n                                        --target ${test_target}\n                                        --config $<CONFIGURATION>)\n      string(REGEX MATCH \"err_([0-9]+)\" MATCH_RESULT \"${test_name}\")\n      file(READ ${test_src} test_content)\n      if(MATCH_RESULT)\n        string(REGEX MATCH \"// expected-error-${CMAKE_MATCH_1}+ {{\\\"([^\\\"]+)\\\"}\" expected_errors_matches ${test_content})\n\n        if (expected_errors_matches)\n          set_tests_properties(${test_target} PROPERTIES PASS_REGULAR_EXPRESSION \"${CMAKE_MATCH_1}\")\n        else()\n          set_tests_properties(${test_target} PROPERTIES WILL_FAIL true)\n        endif()\n      else()\n        string(REGEX MATCH \"// expected-error {{\\\"([^\\\"]+)\\\"}\" expected_errors_matches ${test_content})\n\n        if (expected_errors_matches)\n          set_tests_properties(${test_target} PROPERTIES PASS_REGULAR_EXPRESSION \"${CMAKE_MATCH_1}\")\n        else()\n          set_tests_properties(${test_target} PROPERTIES WILL_FAIL true)\n        endif()\n      endif()\n    else()\n      # Add to the active configuration's meta target\n      add_dependencies(${config_meta_target} ${test_target})\n\n      # Meta target that builds tests with this name for all configurations:\n      if (NOT TARGET ${test_meta_target})\n        add_custom_target(${test_meta_target})\n      endif()\n      add_dependencies(${test_meta_target} ${test_target})\n\n      add_test(NAME ${test_target} COMMAND \"$<TARGET_FILE:${test_target}>\")\n    endif()\n  endif() # Not catch2 test\n\n  # Ensure that we test with assertions enabled\n  target_compile_definitions(${test_target} PRIVATE CCCL_ENABLE_ASSERTIONS)\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Configuring Main Test Runner with Parallelism Control\nDESCRIPTION: Sets up the main lit test runner with controlled parallelism to avoid GPU oversubscription, configures timeout settings, and ensures tests run serially to prevent resource conflicts.\nSOURCE: https://github.com/nvidia/cccl/blob/main/libcudacxx/test/libcudacxx/CMakeLists.txt#2025-04-23_snippet_9\n\nLANGUAGE: CMake\nCODE:\n```\n# Restricted to avoid oversubscribing the GPU:\nset(libcudacxx_LIT_PARALLEL_LEVEL 8 CACHE STRING\n\"Parallelism used to run libcudacxx's lit test suite.\"\n)\n\nadd_test(NAME libcudacxx.test.lit COMMAND\n  \"${CMAKE_COMMAND}\" -E env\n    \"LIBCUDACXX_SITE_CONFIG=${lit_site_cfg_path}\"\n  \"${libcudacxx_LIT}\" -vv --no-progress-bar --time-tests ${libcudacxx_LIT_FLAGS}\n    -j \"${libcudacxx_LIT_PARALLEL_LEVEL}\" \"${libcudacxx_SOURCE_DIR}/test/libcudacxx\"\n)\n\nset_tests_properties(libcudacxx.test.lit PROPERTIES\n  TIMEOUT 10800 # 3hr, some CI machines are slow\n  RUN_SERIAL TRUE\n)\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Bulk DEC Reduction (Unsigned 32-bit) in CUDA\nDESCRIPTION: Performs an asynchronous bulk DEC (decrement) reduction operation from CTA-shared to cluster-shared memory for unsigned 32-bit integers. Uses a memory barrier for synchronization.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_reduce_async_bulk.rst#2025-04-23_snippet_7\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline void cp_reduce_async_bulk(\n  cuda::ptx::space_cluster_t,\n  cuda::ptx::space_shared_t,\n  cuda::ptx::op_dec_t,\n  uint32_t* dstMem,\n  const uint32_t* srcMem,\n  uint32_t size,\n  uint64_t* rdsmem_bar);\n```\n\n----------------------------------------\n\nTITLE: Defining tcgen05_mma_ws_collector_b0_use Template (no mask) in CUDA\nDESCRIPTION: Defines a templated CUDA `__device__` static inline function `tcgen05_mma_ws_collector_b0_use`. This variant handles collector `b0` use operations within `cta_group::1` for various `dot_kind` types (f16, tf32, f8f6f4, i8). It operates on destination memory (`d_tmem`) using descriptors (`a_desc`, `b_desc`, `idesc`) and an enable flag (`enable_input_d`), without the zero column mask parameter. Designed for PTX ISA 86, SM_100a, SM_101a.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_6\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.ws.cta_group.kind.collector::b0::use [d_tmem], a_desc, b_desc, idesc, enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1 }\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_collector_b0_use(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Relaxed Barrier Cluster Arrive Operation\nDESCRIPTION: Device function for cluster barrier arrive with relaxed semantics. Requires PTX ISA 80 and SM_90. Implementation is volatile only.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/barrier_cluster.rst#2025-04-23_snippet_3\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline void barrier_cluster_arrive(\n  cuda::ptx::sem_relaxed_t);\n```\n\n----------------------------------------\n\nTITLE: Implementing MMA Workspace Operations with F8F6F4 Data Type in CUDA\nDESCRIPTION: Template function for tensor compute generation 5 matrix multiplication with workspaces using F8F6F4 data type. This variant operates on tensor memory and descriptors without zero column masking. Targets PTX ISA 86 on SM_100a and SM_101a architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_42\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.ws.cta_group.kind.collector::b2::use [d_tmem], [a_tmem], b_desc, idesc, enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1 }\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_tmem_a_collector_b2_use(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint32_t a_tmem,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Performing Asynchronous Bulk Reduction (OR, b32/b64) from Shared to Global Memory in CUDA\nDESCRIPTION: This CUDA C++ template function `cp_reduce_async_bulk` performs an asynchronous bulk bitwise OR reduction. It reduces data from a source array in shared memory (`srcMem`) to a destination array in global memory (`dstMem`). The function is templated on the data type (`Type`, expected to be `.b32` or `.b64`) and requires PTX ISA 80 and SM_90 capability.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_reduce_async_bulk.rst#2025-04-23_snippet_14\n\nLANGUAGE: cuda\nCODE:\n```\n   // cp.reduce.async.bulk.dst.src.bulk_group.op.type  [dstMem], [srcMem], size; // 3. PTX ISA 80, SM_90\n   // .dst       = { .global }\n   // .src       = { .shared::cta }\n   // .type      = { .b32, .b64 }\n   // .op        = { .or }\n   template <typename Type>\n   __device__ static inline void cp_reduce_async_bulk(\n     cuda::ptx::space_global_t,\n     cuda::ptx::space_shared_t,\n     cuda::ptx::op_or_op_t,\n     Type* dstMem,\n     const Type* srcMem,\n     uint32_t size);\n```\n\n----------------------------------------\n\nTITLE: CUDA Barrier Template Class Definition\nDESCRIPTION: Template class definition for CUDA barrier showing the init friend function declaration. Supports customizable thread scope and completion function.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/synchronization_primitives/barrier/init.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::thread_scope Scope,\n             typename CompletionFunction = /* unspecified */>\nclass barrier {\npublic:\n  // ...\n\n  __host__ __device__\n  friend void init(cuda::std::barrier* bar,\n                   cuda::std::ptrdiff_t expected,\n                   CompletionFunction cf = CompletionFunction{});\n};\n```\n\n----------------------------------------\n\nTITLE: Implementing multimem reduction for int64_t with various semantics and scopes in CUDA\nDESCRIPTION: Template definition for multimem reduction operations on int64_t data with add operation. Supports different memory semantics (relaxed, release) and visibility scopes (cta, cluster, gpu, sys) as parameters. Used for atomic addition operations in global memory on SM_90 architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_red.rst#2025-04-23_snippet_30\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.red.sem.scope.global.op.u64 [addr], val; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .release }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .add }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline void multimem_red(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_add_t,\n  int64_t* addr,\n  int64_t val);\n```\n\n----------------------------------------\n\nTITLE: Implementing CUDA Atomic Fetch Max Template Function\nDESCRIPTION: Template function definition for atomic fetch_max operation. Takes a generic type T and thread scope, returning the previous value while storing the maximum of the current and provided values.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/synchronization_primitives/atomic/fetch_max.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename T, cuda::thread_scope Scope>\n__host__ __device__\nT cuda::atomic<T, Scope>::fetch_max(T const& val,\n                                   cuda::std::memory_order order\n                                     = cuda::std::memory_order_seq_cst);\n```\n\n----------------------------------------\n\nTITLE: Implementing bmsk.clamp.b32 Operation in CUDA\nDESCRIPTION: Template function for implementing the bmsk.clamp.b32 PTX instruction. Takes two 32-bit unsigned integers as input and performs a bitmap mask operation with clamping behavior.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/bmsk.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\n// bmsk.clamp.b32 dest, a_reg, b_reg; // PTX ISA 76, SM_70\ntemplate <typename = void>\n__device__ static inline uint32_t bmsk_clamp(\n  uint32_t a_reg,\n  uint32_t b_reg);\n```\n\n----------------------------------------\n\nTITLE: Declaring multimem_ld_reduce with AND Operation in CUDA PTX\nDESCRIPTION: Template function declaration for multimem.ld_reduce with AND operation on 64-bit values. This function supports various semantics and scopes for atomic operations on global memory.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_59\n\nLANGUAGE: cuda\nCODE:\n```\n__device__ static inline B64 multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_and_op_t,\n  const B64* addr);\n```\n\n----------------------------------------\n\nTITLE: Implementing multimem.red.relaxed.cluster.global.add.u32 Operation in CUDA PTX\nDESCRIPTION: Template function declaration for performing a 32-bit unsigned integer addition reduction with relaxed semantics and cluster scope in global memory. This function is available for SM_90 and requires PTX ISA 8.1 or higher.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_red.rst#2025-04-23_snippet_19\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.red.sem.scope.global.op.u32 [addr], val; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .release }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .add }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline void multimem_red(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_add_t,\n  uint32_t* addr,\n  uint32_t val);\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Command for JIT Template Header Generation\nDESCRIPTION: Defines a custom command that uses the C++ compiler to preprocess the entry point header, generating a template header file with dependency tracking.\nSOURCE: https://github.com/nvidia/cccl/blob/main/c/parallel/src/jit_templates/CMakeLists.txt#2025-04-23_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nadd_custom_command(\n    OUTPUT \"${jit_template}\"\n    DEPENDS \"${cpp_entry}\"\n    DEPFILE \"${jit_template_depfile}\"\n    COMMAND \"${CMAKE_CXX_COMPILER}\" \"${cpp_entry}\" -E -o \"${jit_template}\"\n        -D_CCCL_C_PARALLEL_JIT_TEMPLATES_PREPROCESS\n        -MD -MT \"${jit_template}\" -MF \"${jit_template_depfile}\"\n)\n```\n\n----------------------------------------\n\nTITLE: Declaring Load-Reduce Minimum Operation (relaxed/gpu/global/u64) in CUDA C++\nDESCRIPTION: This CUDA device inline template function supports load and min reduction of 64-bit unsigned integers from global memory, using PTX semantics (relaxed/acquire) and memory scopes, including GPU-wide. Designed for optimal concurrency and fine-grained synchronization on PTX ISA 81 compatible hardware.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_12\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.u64 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .min }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline uint64_t multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_min_t,\n  const uint64_t* addr);\n```\n\n----------------------------------------\n\nTITLE: CTA Group 1 Barrier Commit Operation\nDESCRIPTION: Template device function for committing to a shared memory barrier in CTA group 1. Operates on 64-bit barriers and requires PTX ISA 86 with SM_100a or SM_101a architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_commit.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.commit.cta_group.mbarrier::arrive::one.shared::cluster.b64 [smem_bar]; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1, .cta_group::2 }\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_commit(\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint64_t* smem_bar);\n```\n\n----------------------------------------\n\nTITLE: Cluster Scope Relaxed Memory Barrier Test Wait\nDESCRIPTION: Memory barrier test wait with relaxed semantics and cluster scope for PTX ISA 86 and SM_90. Includes scope and semantic parameters.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/mbarrier_test_wait.rst#2025-04-23_snippet_4\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_scope Scope>\n__device__ static inline bool mbarrier_test_wait(\n  cuda::ptx::sem_relaxed_t,\n  cuda::ptx::scope_t<Scope> scope,\n  uint64_t* addr,\n  const uint64_t& state);\n```\n\n----------------------------------------\n\nTITLE: Storing 32x32b Data with tcgen05 in CUDA (8 values)\nDESCRIPTION: This function performs a 32x32b store operation using tcgen05. It takes a 32-bit address and an array of 8 B32 values as input. The function is constrained to B32 types of 4 bytes.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_st.rst#2025-04-23_snippet_23\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.st.sync.aligned.32x32b.x8.b32 [taddr], values; // PTX ISA 86, SM_100a, SM_101a\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline void tcgen05_st_32x32b(\n  uint32_t taddr,\n  const B32 (&values)[8]);\n```\n\n----------------------------------------\n\nTITLE: Implementing Multimem Load-Reduce for 32-bit Unsigned Integer Addition with Weak Semantics in CUDA\nDESCRIPTION: This template function performs a multimem load-reduce operation for 32-bit unsigned integers using the add operation with weak semantics. It provides a specialized version for weak memory ordering without explicit scope specification.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_29\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.global.op.u32 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .weak }\n// .op        = { .add }\ntemplate <typename = void>\n__device__ static inline uint32_t multimem_ld_reduce(\n  cuda::ptx::sem_weak_t,\n  cuda::ptx::op_add_t,\n  const uint32_t* addr);\n```\n\n----------------------------------------\n\nTITLE: Declaring CUDA apply_access_property Templates\nDESCRIPTION: Template function declarations for apply_access_property with ShapeT parameter and different access properties (persisting and normal).\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/memory_access_properties/apply_access_property.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <class ShapeT>\n__host__ __device__\nvoid apply_access_property(void const volatile* ptr, ShapeT shape, cuda::access_property::persisting) noexcept;\ntemplate <class ShapeT>\n__host__ __device__\nvoid apply_access_property(void const volatile* ptr, ShapeT shape, cuda::access_property::normal) noexcept;\n```\n\n----------------------------------------\n\nTITLE: 32-bit Signed Right Shift Operation in CUDA\nDESCRIPTION: Template function for performing signed right shift on 32-bit integers. Takes an int32_t value and shift amount as parameters.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/shr.rst#2025-04-23_snippet_4\n\nLANGUAGE: cuda\nCODE:\n```\n// shr.s32 dest, a_reg, b_reg; // PTX ISA 10, SM_50\ntemplate <typename = void>\n__device__ static inline int32_t shr(\n  int32_t a_reg,\n  uint32_t b_reg);\n```\n\n----------------------------------------\n\nTITLE: Add Operation for Signed 32-bit Integer\nDESCRIPTION: Template function implementing asynchronous addition reduction operation on cluster memory barrier for s32 type. Takes destination pointer, value and remote barrier pointer as parameters.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/red_async.rst#2025-04-23_snippet_7\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline void red_async(\n  cuda::ptx::op_add_t,\n  int32_t* dest,\n  const int32_t& value,\n  uint64_t* remote_bar);\n```\n\n----------------------------------------\n\nTITLE: Creating Configuration-Specific Test Meta-Targets in CMake\nDESCRIPTION: Creates a custom CMake target (e.g., `CPP.CUDA.tests`) for each Thrust configuration (`thrust_target`). These meta-targets depend on a corresponding `.all` target (e.g., `CPP.CUDA.all`) and serve to group all tests for that specific configuration. It iterates through the list of defined Thrust targets (`THRUST_TARGETS`).\nSOURCE: https://github.com/nvidia/cccl/blob/main/thrust/testing/CMakeLists.txt#2025-04-23_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\n# Create meta targets that build all tests for a single configuration:\nforeach(thrust_target IN LISTS THRUST_TARGETS)\n  thrust_get_target_property(config_prefix ${thrust_target} PREFIX)\n  set(config_meta_target ${config_prefix}.tests)\n  add_custom_target(${config_meta_target})\n  add_dependencies(${config_prefix}.all ${config_meta_target})\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Downloading CPM Package Manager\nDESCRIPTION: Bash commands to download the latest version of CPM.cmake, a CMake module that simplifies dependency management. This creates the necessary cmake directory and downloads the CPM script into it.\nSOURCE: https://github.com/nvidia/cccl/blob/main/examples/basic/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmkdir -p cmake\nwget -O cmake/CPM.cmake https://github.com/cpm-cmake/CPM.cmake/releases/latest/download/get_cpm.cmake\n```\n\n----------------------------------------\n\nTITLE: Implementing multimem reduction for s32 with various semantics and scopes in CUDA\nDESCRIPTION: Template definition for multimem reduction operations on int32_t data with add operation. Supports different memory semantics (relaxed, release) and visibility scopes (cta, cluster, gpu, sys) as parameters. Used for atomic addition operations in global memory on SM_90 architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_red.rst#2025-04-23_snippet_29\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.red.sem.scope.global.op.s32 [addr], val; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .release }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .add }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline void multimem_red(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_add_t,\n  int32_t* addr,\n  int32_t val);\n```\n\n----------------------------------------\n\nTITLE: Implementing Max Reduction for 64-bit Integers in CUDA\nDESCRIPTION: This function performs an asynchronous bulk maximum reduction operation on 64-bit integers from shared memory to global memory. It uses the cp.reduce.async.bulk instruction from PTX ISA 80 for SM_90 architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_reduce_async_bulk.rst#2025-04-23_snippet_28\n\nLANGUAGE: cuda\nCODE:\n```\n// cp.reduce.async.bulk.dst.src.bulk_group.op.type  [dstMem], [srcMem], size; // 4. PTX ISA 80, SM_90\n// .dst       = { .global }\n// .src       = { .shared::cta }\n// .type      = { .s64 }\n// .op        = { .max }\ntemplate <typename = void>\n__device__ static inline void cp_reduce_async_bulk(\n  cuda::ptx::space_global_t,\n  cuda::ptx::space_shared_t,\n  cuda::ptx::op_max_t,\n  int64_t* dstMem,\n  const int64_t* srcMem,\n  uint32_t size);\n```\n\n----------------------------------------\n\nTITLE: Defining Sum Reduction Operator in CUDASTF\nDESCRIPTION: Shows how to implement a custom reduction operator for summation in CUDASTF. The operator defines init_op to set the neutral element and apply_op to combine two elements.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_36\n\nLANGUAGE: cpp\nCODE:\n```\ntemplate <typename T>\nclass sum\n{\npublic:\n  static __host__ __device__ void init_op(T& dst)\n  {\n    dst = static_cast<T>(0);\n  }\n\n  static __host__ __device__ void apply_op(T& dst, const T& src)\n  {\n    dst += src;\n  }\n};\n```\n\n----------------------------------------\n\nTITLE: CUDA Standard Function Objects\nDESCRIPTION: Demonstrates standard function objects available in the CUDA C++ library including arithmetic operations.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/releases/changelog.rst#2025-04-23_snippet_31\n\nLANGUAGE: C++\nCODE:\n```\ncuda::std::plus, cuda::std::minus\n```\n\n----------------------------------------\n\nTITLE: Adding Individual Benchmark Target in CMake\nDESCRIPTION: A function that creates a benchmark executable target with the proper configuration. It sets up the target with the required CUDA dialect and links it with the nvbench libraries.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cub/benchmarks/CMakeLists.txt#2025-04-23_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(add_bench target_name bench_name bench_src)\n  set(bench_target ${bench_name})\n  set(${target_name} ${bench_target} PARENT_SCOPE)\n\n  add_executable(${bench_target} \"${bench_src}\")\n  cccl_configure_target(${bench_target} DIALECT 17)\n  target_link_libraries(${bench_target} PRIVATE nvbench_helper nvbench::main)\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Declaring Load-Reduce Minimum Operation (relaxed/sys/global/u64) in CUDA C++\nDESCRIPTION: Template function for loading and reducing (min) a 64-bit unsigned integer value from global memory in device code, using PTX relaxed and acquire semaphore types and arbitrary memory scope. Accepts the address pointer and returns the minimum reduced value. Needs PTX type wrappers to be present.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_13\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.u64 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .min }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline uint64_t multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_min_t,\n  const uint64_t* addr);\n```\n\n----------------------------------------\n\nTITLE: Storing and Unpacking 32x32b Data with tcgen05 in CUDA (1 value)\nDESCRIPTION: This function performs a 32x32b store operation with 16-bit unpacking using tcgen05. It takes a 32-bit address and an array of 1 B32 value as input. The function is constrained to B32 types of 4 bytes.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_st.rst#2025-04-23_snippet_18\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.st.sync.aligned.32x32b.x1.unpack::16b.b32 [taddr], values; // PTX ISA 86, SM_100a, SM_101a\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline void tcgen05_st_32x32b_unpack_16b(\n  uint32_t taddr,\n  const B32 (&values)[1]);\n```\n\n----------------------------------------\n\nTITLE: Multimem Load Reduce Max U32 Operations\nDESCRIPTION: Template implementation for 32-bit unsigned integer max reduction operations with different memory semantics and scopes. Supports PTX ISA 81 and requires SM_90.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_23\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline uint32_t multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_max_t,\n  const uint32_t* addr);\n```\n\n----------------------------------------\n\nTITLE: After Thread Sync Fence Implementation\nDESCRIPTION: Device function template that implements a fence operation after thread synchronization. Targets PTX ISA 86 and is compatible with SM_100a and SM_101a architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_fence.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.fence::after_thread_sync; // PTX ISA 86, SM_100a, SM_101a\ntemplate <typename = void>\n__device__ static inline void tcgen05_fence_after_thread_sync();\n```\n\n----------------------------------------\n\nTITLE: CUDA Memory Unpinning Kernel\nDESCRIPTION: Kernel implementation for applying normal access property to memory arrays.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/memory_access_properties/apply_access_property.rst#2025-04-23_snippet_5\n\nLANGUAGE: cuda\nCODE:\n```\n__global__ void unpin(int* a, int* b, size_t N) {\n    auto g = cooperative_groups::this_grid();\n    for (int idx = g.thread_rank(); idx < N; idx += g.size()) {\n        cuda::apply_access_property(a + idx, sizeof(int), cuda::access_property::normal{});\n        cuda::apply_access_property(b + idx, sizeof(int), cuda::access_property::normal{});\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing MMA collector with descriptor inputs in CUDA\nDESCRIPTION: This function template implements a Matrix Multiplication Accumulate (MMA) collector operation for various data types. It uses descriptor inputs for matrices A and B, and supports different precision modes.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_27\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_collector_b1_discard(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d,\n  uint64_t zero_column_mask_desc);\n```\n\n----------------------------------------\n\nTITLE: Implementing multimem.red.release.cta.global.max.s64 Operation in CUDA PTX\nDESCRIPTION: Template function declaration for performing a 64-bit signed integer max reduction with release semantics and CTA scope in global memory. This function is available for SM_90 and requires PTX ISA 8.1 or higher.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_red.rst#2025-04-23_snippet_14\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.red.sem.scope.global.op.s64 [addr], val; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .release }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .max }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline void multimem_red(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_max_t,\n  int64_t* addr,\n  int64_t val);\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Bulk Copy from Shared CTA to Global Memory with Bulk Group and Copy Mask (CUDA)\nDESCRIPTION: Performs an asynchronous bulk copy from shared CTA memory to global memory using a bulk group and a copy mask. This operation is available from PTX ISA 86 and SM_100.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_async_bulk.rst#2025-04-23_snippet_4\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline void cp_async_bulk_cp_mask(\n  cuda::ptx::space_global_t,\n  cuda::ptx::space_shared_t,\n  void* dstMem,\n  const void* srcMem,\n  const uint32_t& size,\n  const uint16_t& byteMask);\n```\n\n----------------------------------------\n\nTITLE: Declaring multimem_ld_reduce with Acquire/GPU Scope and XOR Operation in CUDA PTX\nDESCRIPTION: Template function declaration for multimem.ld_reduce with acquire memory semantics, GPU scope, and XOR operation on 64-bit values in global memory. For PTX ISA 8.1 and SM_90 architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_76\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.b64 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .xor }\n```\n\n----------------------------------------\n\nTITLE: Async Bulk Tensor Gather with CTA Group 1\nDESCRIPTION: Template function for asynchronous bulk tensor gathering with CTA group 1 configuration. Supports shared CTA memory destination and global memory source.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_async_bulk_tensor_gather_scatter.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void cp_async_bulk_tensor_tile_gather4(\n  cuda::ptx::space_shared_t,\n  cuda::ptx::space_global_t,\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  void* dstMem,\n  const void* tensorMap,\n  const int32_t (&tensorCoords)[5],\n  uint64_t* smem_bar);\n```\n\n----------------------------------------\n\nTITLE: Defining cp_async_bulk_commit_group CUDA Device Function\nDESCRIPTION: This snippet defines a template function for committing a group in asynchronous bulk copying operations. It is designed for PTX ISA 80 and SM_90 architectures. The function is marked as __device__, indicating it's intended to run on the GPU.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_async_bulk_commit_group.rst#2025-04-23_snippet_0\n\nLANGUAGE: CUDA\nCODE:\n```\n// cp.async.bulk.commit_group; // PTX ISA 80, SM_90\ntemplate <typename = void>\n__device__ static inline void cp_async_bulk_commit_group();\n```\n\n----------------------------------------\n\nTITLE: Memory Barrier Test Wait Parity with Acquire Semantics (Cluster Scope)\nDESCRIPTION: Implementation with acquire semantics and cluster scope for SM_90. Includes semantic and scope parameters along with address and phase parity.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/mbarrier_test_wait_parity.rst#2025-04-23_snippet_2\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_scope Scope>\n__device__ static inline bool mbarrier_test_wait_parity(\n  cuda::ptx::sem_acquire_t,\n  cuda::ptx::scope_t<Scope> scope,\n  uint64_t* addr,\n  const uint32_t& phaseParity);\n```\n\n----------------------------------------\n\nTITLE: Storing 32x32b Data with tcgen05 in CUDA (2 values)\nDESCRIPTION: This function performs a 32x32b store operation using tcgen05. It takes a 32-bit address and an array of 2 B32 values as input. The function is constrained to B32 types of 4 bytes.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_st.rst#2025-04-23_snippet_19\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.st.sync.aligned.32x32b.x2.b32 [taddr], values; // PTX ISA 86, SM_100a, SM_101a\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline void tcgen05_st_32x32b(\n  uint32_t taddr,\n  const B32 (&values)[2]);\n```\n\n----------------------------------------\n\nTITLE: Multimem Load Reduce for int32_t\nDESCRIPTION: Template function implementing atomic load-reduce operation for 32-bit signed integers with weak semantics. Uses global scope with addition operation.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_32\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline int32_t multimem_ld_reduce(\n  cuda::ptx::sem_weak_t,\n  cuda::ptx::op_add_t,\n  const int32_t* addr);\n```\n\n----------------------------------------\n\nTITLE: Listing Available Benchmark Targets\nDESCRIPTION: Command to display all available benchmark build targets using ninja\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/benchmarking.rst#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nninja -t targets | grep '\\.bench\\.'\n```\n\n----------------------------------------\n\nTITLE: 16-bit Shift Left Operation in CUDA PTX\nDESCRIPTION: Template function for performing shift left operation on 16-bit values. Takes a 16-bit value and uint32_t shift amount as parameters. Requires PTX ISA 10 and SM_50 architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/shl.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\n// shl.b16 dest, a_reg, b_reg; // PTX ISA 10, SM_50\ntemplate <typename B16, enable_if_t<sizeof(B16) == 2, bool> = true>\n__device__ static inline B16 shl(\n  B16 a_reg,\n  uint32_t b_reg);\n```\n\n----------------------------------------\n\nTITLE: Declaring Restrictions for 'transform_output_iterator_reduce_by_key' Test in CMake\nDESCRIPTION: Uses the `thrust_declare_test_restrictions` macro to specify that the `transform_output_iterator_reduce_by_key` test is restricted. It is allowed for `CPP.CPP`, `CPP.OMP`, and `CPP.CUDA` configurations but explicitly excludes the TBB backend due to a known issue where `reduce_by_key` doesn't work correctly with `transform_output_iterator` (tracked in GitHub issue #1811).\nSOURCE: https://github.com/nvidia/cccl/blob/main/thrust/testing/CMakeLists.txt#2025-04-23_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\n# In the TBB backend, reduce_by_key does not currently work with transform_output_iterator\n# https://github.com/NVIDIA/thrust/issues/1811\nthrust_declare_test_restrictions(transform_output_iterator_reduce_by_key CPP.CPP CPP.OMP CPP.CUDA)\n```\n\n----------------------------------------\n\nTITLE: Emulating cp.reduce.async.bulk.add.s64 in CUDA PTX\nDESCRIPTION: This code snippet demonstrates how to emulate the cp.reduce.async.bulk.add.s64 instruction in CUDA PTX, which is not currently exposed in CTK 12.3. It provides two template functions for different memory spaces and operations.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/cp_reduce_async_bulk.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\n// cp.reduce.async.bulk.dst.src.mbarrier::complete_tx::bytes.op.u64 [dstMem], [srcMem], size, [rdsmem_bar]; // 2. PTX ISA 80, SM_90\n// .dst       = { .shared::cluster }\n// .src       = { .shared::cta }\n// .type      = { .s64 }\n// .op        = { .add }\ntemplate <typename=void>\n__device__ static inline void cp_reduce_async_bulk(\n  cuda::ptx::space_cluster_t,\n  cuda::ptx::space_shared_t,\n  cuda::ptx::op_add_t,\n  int64_t* dstMem,\n  const int64_t* srcMem,\n  uint32_t size,\n  uint64_t* rdsmem_bar);\n\n// cp.reduce.async.bulk.dst.src.bulk_group.op.u64  [dstMem], [srcMem], size; // 6. PTX ISA 80, SM_90\n// .dst       = { .global }\n// .src       = { .shared::cta }\n// .type      = { .s64 }\n// .op        = { .add }\ntemplate <typename=void>\n__device__ static inline void cp_reduce_async_bulk(\n  cuda::ptx::space_global_t,\n  cuda::ptx::space_shared_t,\n  cuda::ptx::op_add_t,\n  int64_t* dstMem,\n  const int64_t* srcMem,\n  uint32_t size);\n```\n\n----------------------------------------\n\nTITLE: Initializing Tensor Core Operation for CTA Group 1 in CUDA\nDESCRIPTION: This function initializes a tensor core operation for CTA group 1 with various data types. It takes descriptors for matrices A and B, along with other parameters like d_tmem and idesc.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_16\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.ws.cta_group.kind.collector::b1::fill [d_tmem], a_desc, b_desc, idesc, enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1 }\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_collector_b1_fill(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Using Custom Types in CUB Tests with C++\nDESCRIPTION: Demonstrates how to use custom types with specific properties in CUB tests.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/test_overview.rst#2025-04-23_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\nusing type = c2h::custom_type_t<c2h::accumulateable_t,\n                                c2h::equal_comparable_t>;\n```\n\n----------------------------------------\n\nTITLE: Creating and Finalizing CUDASTF Context\nDESCRIPTION: Example of how to create a CUDASTF context and finalize it, with options for different backend implementations.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_58\n\nLANGUAGE: cpp\nCODE:\n```\ncontext ctx;\nctx.finalize();\n```\n\n----------------------------------------\n\nTITLE: Citing CCCL using BibTeX in LaTeX\nDESCRIPTION: This BibTeX entry provides the correct format for citing CCCL in LaTeX documents. It includes the title, author, year, and URL for the CCCL project.\nSOURCE: https://github.com/nvidia/cccl/blob/main/CITATION.md#2025-04-23_snippet_0\n\nLANGUAGE: tex\nCODE:\n```\n@Manual{,\n  title = {{CCCL}: {CUDA} {C++} {C}ore {L}ibraries},\n  author = {{CCCL Development Team}},\n  year = {2023},\n  url = {https://github.com/NVIDIA/cccl},\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Multimem Reduction Max Operations for s32 Type in CUDA\nDESCRIPTION: Template function declarations for multimem reduction max operations on int32_t data. These functions support different memory semantics (relaxed, release) and memory scopes (cta, cluster, gpu, sys). These PTX operations are available in PTX ISA 8.1 on SM_90 architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_red.rst#2025-04-23_snippet_8\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.red.sem.scope.global.op.s32 [addr], val; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .release }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .max }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline void multimem_red(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_max_t,\n  int32_t* addr,\n  int32_t val);\n```\n\n----------------------------------------\n\nTITLE: Extracting Benchmark Parameter Ranges in CMake\nDESCRIPTION: A function that parses source files to extract parameter ranges for benchmarks. It uses regex to find special comments marked with %RANGE% and registers those ranges with the CCCL tuning system.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cub/benchmarks/CMakeLists.txt#2025-04-23_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(get_bench_ranges src bench_name)\n  file(READ \"${src}\" file_data)\n  set(param_regex \"//[ ]+%RANGE%[ ]+([^ ]+)[ ]+([^ ]+)[ ]+([^\\n]*)\")\n\n  string(REGEX MATCHALL \"${param_regex}\" matches \"${file_data}\")\n\n  set(ranges \"\")\n\n  foreach(match IN LISTS matches)\n    string(REGEX MATCH \"${param_regex}\" unused \"${match}\")\n\n    set(def ${CMAKE_MATCH_1})\n    set(label ${CMAKE_MATCH_2})\n    set(range ${CMAKE_MATCH_3})\n    set(ranges \"${ranges}${def}|${label}=${range},\")\n\n    string(REPLACE \":\" \";\" range \"${range}\")\n    list(LENGTH range range_len)\n\n    if (NOT \"${range_len}\" STREQUAL 3)\n      message(FATAL_ERROR \"Range should be represented as 'start:end:step'\")\n    endif()\n  endforeach()\n\n  string(LENGTH \"${ranges}\" ranges_length)\n  math(EXPR last_character_index \"${ranges_length} - 1\")\n  string(SUBSTRING \"${ranges}\" 0 ${last_character_index} ranges)\n  register_cccl_tuning(\"${bench_name}\" \"${ranges}\")\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Implementing multimem.ld_reduce for relaxed system global add operations on 64-bit integers\nDESCRIPTION: Template implementation for multimem atomic load-and-reduce operations with relaxed semantics, system scope, global memory, and add operation on 64-bit integers. Compatible with PTX ISA 81 on SM_90 architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_44\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.u64 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .add }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline int64_t multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_add_t,\n  const int64_t* addr);\n```\n\n----------------------------------------\n\nTITLE: Implementing multimem.ld_reduce for acquire cluster global add operations on 32-bit integers\nDESCRIPTION: Template implementation for multimem atomic load-and-reduce operations with acquire semantics, cluster scope, global memory, and add operation on 32-bit integers. Compatible with PTX ISA 81 on SM_90 architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_37\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.s32 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .add }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline int32_t multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_add_t,\n  const int32_t* addr);\n```\n\n----------------------------------------\n\nTITLE: Memory Fence with Acquire Semantics\nDESCRIPTION: Template function implementing memory fence with acquire semantics. Supports CTA, cluster, GPU, and system-wide memory scopes. Requires PTX ISA 8.6 and SM_90 or higher.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/fence.rst#2025-04-23_snippet_3\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_scope Scope>\n__device__ static inline void fence(\n  cuda::ptx::sem_acquire_t,\n  cuda::ptx::scope_t<Scope> scope);\n```\n\n----------------------------------------\n\nTITLE: Identifying SM70+ Architectures for Atomic Headers in CMake\nDESCRIPTION: Creates a list of CUDA architectures that are at least SM70, which is required for atomic headers and related synchronization primitives.\nSOURCE: https://github.com/nvidia/cccl/blob/main/libcudacxx/test/public_headers/CMakeLists.txt#2025-04-23_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nset(architectures_at_least_sm70)\nforeach(item IN LISTS CMAKE_CUDA_ARCHITECTURES)\n  if(item GREATER_EQUAL 70)\n    list(APPEND architectures_at_least_sm70 ${item})\n  endif()\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Declaring multimem_ld_reduce with Acquire/Cluster Scope and XOR Operation in CUDA PTX\nDESCRIPTION: Template function declaration for multimem.ld_reduce with acquire memory semantics, cluster scope, and XOR operation on 64-bit values in global memory. For PTX ISA 8.1 on SM_90 hardware.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_75\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.b64 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .xor }\ntemplate <typename B64, enable_if_t<sizeof(B64) == 8, bool> = true, cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline B64 multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_xor_op_t,\n  const B64* addr);\n```\n\n----------------------------------------\n\nTITLE: Declaring cuda::uabs Function in C++\nDESCRIPTION: Function signature for cuda::uabs, which computes the absolute value of an input as an unsigned integer. It is constexpr, can be used in host and device code, and is marked as nodiscard.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/math/uabs.rst#2025-04-23_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\ntemplate <typename T>\n[[nodiscard]] __host__ __device__ inline constexpr\ncuda::std::make_unsigned_t<T> uabs(T value) noexcept;\n```\n\n----------------------------------------\n\nTITLE: Performing Asynchronous Bulk Reduction (MIN, s32) from Shared to Global Memory in CUDA\nDESCRIPTION: This CUDA C++ device function `cp_reduce_async_bulk` performs an asynchronous bulk minimum reduction for signed 32-bit integers. It reduces data from a source array in shared memory (`srcMem`) to a destination array in global memory (`dstMem`). This overload requires PTX ISA 80 and SM_90 capability.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_reduce_async_bulk.rst#2025-04-23_snippet_21\n\nLANGUAGE: cuda\nCODE:\n```\n   // cp.reduce.async.bulk.dst.src.bulk_group.op.type  [dstMem], [srcMem], size; // 4. PTX ISA 80, SM_90\n   // .dst       = { .global }\n   // .src       = { .shared::cta }\n   // .type      = { .s32 }\n   // .op        = { .min }\n   template <typename = void>\n   __device__ static inline void cp_reduce_async_bulk(\n     cuda::ptx::space_global_t,\n     cuda::ptx::space_shared_t,\n     cuda::ptx::op_min_t,\n     int32_t* dstMem,\n     const int32_t* srcMem,\n     uint32_t size);\n```\n\n----------------------------------------\n\nTITLE: MMA Block Scale Vector 4X TMEM Collector Fill with mxf4nvf4 Type (CTA Group 1)\nDESCRIPTION: Template function for matrix multiplication with block scaling using vector 4X with TMEM and collector A fill operation for CTA group 1. The function uses mxf4nvf4 data type and requires PTX ISA 86 with SM_100a or SM_101a architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma.rst#2025-04-23_snippet_30\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.cta_group.kind.block_scale.scale_vec::4X.collector::a::fill [d_tmem], a_desc, b_desc, idesc, [scale_A_tmem], [scale_B_tmem], enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .kind      = { .kind::mxf4nvf4 }\n// .cta_group = { .cta_group::1, .cta_group::2 }\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_mma_block_scale_vec_4x_tmem_a_collector_a_fill(\n  cuda::ptx::kind_mxf4nvf4_t,\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  uint32_t scale_A_tmem,\n  uint32_t scale_B_tmem,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Declaring Weak-Semantic Multimem Load-Reduce (max, u32) with CUDA PTX (C++)\nDESCRIPTION: This code snippet introduces a templated CUDA device function for atomic maximal reduction (max) on 32-bit unsigned integers using the weak semantic model. It accepts semantic, operation, and address parameters, returning the reduced value. Suitable for use in custom CUDA kernel code with PTX ISA 81, SM_90 on global memory data and requires underlying PTX types.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_22\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.global.op.u32 dest, [addr]; // PTX ISA 81, SM_90\\n// .sem       = { .weak }\\n// .op        = { .max }\\ntemplate <typename = void>\\n__device__ static inline uint32_t multimem_ld_reduce(\\n  cuda::ptx::sem_weak_t,\n```\n\n----------------------------------------\n\nTITLE: Analyzing Kernel Instantiations using cuobjdump Output\nDESCRIPTION: This text snippet shows the relevant output of the `cuobjdump --dump-elf-symbols` command when run on the compiled binary from the C++ example. It demonstrates that the `bad_kernel` (templated on `ActivePolicy`) resulted in four different symbol entries (one for each policy: sm60, sm70, sm80, sm90), while `good_kernel` (templated on `MaxPolicy`) resulted in only one symbol entry. This empirically validates the compile-time optimization achieved by using `MaxPolicy` for kernel template instantiation.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/developer_overview.rst#2025-04-23_snippet_17\n\nLANGUAGE: text\nCODE:\n```\nsymbols:\nSTT_OBJECT       STB_LOCAL  STV_DEFAULT    $str\nSTT_FUNC         STB_GLOBAL STO_ENTRY      _Z11good_kernelI4sm90Evv\nSTT_FUNC         STB_GLOBAL STV_DEFAULT  U vprintf\nSTT_FUNC         STB_GLOBAL STO_ENTRY      _Z10bad_kernelI4sm90Evv\nSTT_FUNC         STB_GLOBAL STO_ENTRY      _Z10bad_kernelI4sm80Evv\nSTT_FUNC         STB_GLOBAL STO_ENTRY      _Z10bad_kernelI4sm70Evv\nSTT_FUNC         STB_GLOBAL STO_ENTRY      _Z10bad_kernelI4sm60Evv\n```\n\n----------------------------------------\n\nTITLE: Memory Fence with Acquire-Release Semantics\nDESCRIPTION: Template function implementing memory fence with acquire-release semantics. Supports CTA, GPU, and system-wide memory scopes. Requires PTX ISA 6.0 and SM_70 or higher.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/fence.rst#2025-04-23_snippet_2\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_scope Scope>\n__device__ static inline void fence(\n  cuda::ptx::sem_acq_rel_t,\n  cuda::ptx::scope_t<Scope> scope);\n```\n\n----------------------------------------\n\nTITLE: CTA Group Allocation Permit Relinquishment (Group 2)\nDESCRIPTION: Template function for relinquishing allocation permits in CTA group 2. Takes only the CTA group parameter. Compatible with PTX ISA 86 and SM_100a/101a architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_alloc.rst#2025-04-23_snippet_5\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.relinquish_alloc_permit.cta_group.sync.aligned; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1, .cta_group::2 }\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_relinquish_alloc_permit(\n  cuda::ptx::cta_group_t<Cta_Group> cta_group);\n```\n\n----------------------------------------\n\nTITLE: Constraining Interface with SFINAE for Pre-C++20\nDESCRIPTION: Demonstrates how to constrain a function interface for device accessible memory resources using SFINAE for compatibility with pre-C++20 compilers.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/memory_resource/properties.rst#2025-04-23_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\ntemplate<class MemoryResource, class = cuda::std::enable_if_t<cuda::has_property<MemoryResource, cuda::mr::device_accessible>>>\nvoid function_that_dispatches_to_device(MemoryResource& resource);\n```\n\n----------------------------------------\n\nTITLE: Storing 32x32b Data with tcgen05 Instruction in CUDA\nDESCRIPTION: This snippet defines device functions for storing 32x32b data using the tcgen05 instruction. It includes variations for different array sizes (32, 64, 128) and unpacking operations for 16-bit values.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_st.rst#2025-04-23_snippet_27\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.st.sync.aligned.32x32b.x32.b32 [taddr], values; // PTX ISA 86, SM_100a, SM_101a\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline void tcgen05_st_32x32b(\n  uint32_t taddr,\n  const B32 (&values)[32]);\n```\n\n----------------------------------------\n\nTITLE: Multimem Load Reduce Max S32 Operations\nDESCRIPTION: Template implementation for 32-bit signed integer max reduction operations with weak memory semantics. Supports PTX ISA 81 and requires SM_90.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_25\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline int32_t multimem_ld_reduce(\n  cuda::ptx::sem_weak_t,\n  cuda::ptx::op_max_t,\n  const int32_t* addr);\n```\n\n----------------------------------------\n\nTITLE: Checking Minimum CMake Version for RDC Tests with Visual Studio\nDESCRIPTION: Verifies that CMake version 3.27.5 or newer is available when RDC tests are enabled with Visual Studio generator, as this capability requires a fix included in that version.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cub/test/CMakeLists.txt#2025-04-23_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif(CMAKE_GENERATOR MATCHES \"^Visual Studio\")\n  if(CUB_ENABLE_RDC_TESTS)\n    if(\"${CMAKE_VERSION}\" VERSION_LESS 3.27.5)\n      # https://gitlab.kitware.com/cmake/cmake/-/merge_requests/8794\n      message(WARNING \"CMake 3.27.5 or newer is required to enable RDC tests in Visual Studio.\")\n      cmake_minimum_required(VERSION 3.27.5)\n    endif()\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Accessing Current Graph Execution Information in CUDA\nDESCRIPTION: This function retrieves the current graph execution information. It uses an inline PTX instruction to access the special register.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/get_sreg.rst#2025-04-23_snippet_14\n\nLANGUAGE: cuda\nCODE:\n```\n// mov.u64 sreg_value, %%current_graph_exec; // PTX ISA 80, SM_50\ntemplate <typename = void>\n__device__ static inline uint64_t get_sreg_current_graph_exec();\n```\n\n----------------------------------------\n\nTITLE: Creating Public Headers Test Target in CMake\nDESCRIPTION: Creates a custom target for testing public headers in the CUDA C++ library to verify modularity.\nSOURCE: https://github.com/nvidia/cccl/blob/main/libcudacxx/test/public_headers/CMakeLists.txt#2025-04-23_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_custom_target(libcudacxx.test.public_headers)\n```\n\n----------------------------------------\n\nTITLE: Implementing MMA Operation for i8 Data Type in CUDA\nDESCRIPTION: This template function implements a matrix multiply-accumulate operation for i8 data type. It supports different CTA group configurations and uses tensor core instructions.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma.rst#2025-04-23_snippet_13\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.cta_group.kind [d_tmem], [a_tmem], b_desc, idesc, enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\n// .cta_group = { .cta_group::1, .cta_group::2 }\ntemplate <cuda::ptx::dot_kind Kind, cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_mma_tmem_a(\n  cuda::ptx::kind_t<Kind> kind,\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t d_tmem,\n  uint32_t a_tmem,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Memory Barrier Test Wait Parity with Relaxed Semantics (CTA Scope)\nDESCRIPTION: Implementation with relaxed semantics and CTA scope for SM_90. Includes semantic and scope parameters along with address and phase parity.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/mbarrier_test_wait_parity.rst#2025-04-23_snippet_3\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_scope Scope>\n__device__ static inline bool mbarrier_test_wait_parity(\n  cuda::ptx::sem_relaxed_t,\n  cuda::ptx::scope_t<Scope> scope,\n  uint64_t* addr,\n  const uint32_t& phaseParity);\n```\n\n----------------------------------------\n\nTITLE: Memory Fence with SC Semantics (CTA/GPU/SYS Scope)\nDESCRIPTION: Template function implementing memory fence with sequential consistency semantics. Supports CTA, GPU, and system-wide memory scopes. Requires PTX ISA 6.0 and SM_70 or higher.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/fence.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_scope Scope>\n__device__ static inline void fence(\n  cuda::ptx::sem_sc_t,\n  cuda::ptx::scope_t<Scope> scope);\n```\n\n----------------------------------------\n\nTITLE: Generating Grids of Execution Places with exec_place::grid and all_devices in CUDASTF (C++)\nDESCRIPTION: Shows how to build a 1D grid of execution places using exec_place::grid, and implement exec_place::all_devices() to produce a grid over all available CUDA devices. The partitioner parameter is optional and the method returns a grid suitable for scalable, partitioned workloads. Code assumes CUDA is initialized and illustrates device enumeration and grid construction using C++ STL containers.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_24\n\nLANGUAGE: c++\nCODE:\n```\nexec_place exec_place::grid(std::vector<exec_place> places);\n```\n\nLANGUAGE: c++\nCODE:\n```\ntemplate <typename partitioner_t>\ninline exec_place_grid<exec_place_device, partitioner_t> exec_place::all_devices() {\n    int ndevs;\n    cuda_safe_call(cudaGetDeviceCount(&ndevs));\n\n    std::vector<exec_place> devices;\n    devices.reserve(ndevs);\n    for (int d = 0; d < ndevs; d++) {\n        devices.push_back(exec_place::device(d));\n    }\n\n    return exec_place::grid<exec_place_device, partitioner_t>(std::move(devices));\n}\n```\n\n----------------------------------------\n\nTITLE: Running a Subset of libcu++ Tests with lit (Bash)\nDESCRIPTION: This command navigates into the build directory and uses the `lit` tool to build and run all tests located within a specified relative path or subfolder for the `libcudacxx-cpp17` preset. The `-sv` flags typically enable summary output and verbose test failure information. Requires libcu++ to be configured beforehand.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/setup/building_and_testing.rst#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd build\nlit libcudacxx-cpp17/RELATIVE_PATH_TO_TEST_OR_SUBFOLDER -sv\n```\n\n----------------------------------------\n\nTITLE: CUDA Parallel Module Structure\nDESCRIPTION: RestructuredText documentation defining the structure and modules for CUDA parallel functionality. Includes autodoc directives for algorithms, iterators and utilities modules.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cuda_parallel/index.rst#2025-04-23_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. _cuda_parallel-module:\n\nCUDA Parallel\n==================================================\n\n.. warning::\n  Python exposure of parallel algorithms is in public beta.\n  The API is subject to change without notice.\n\nAlgorithms\n----------\n\n.. automodule:: cuda.parallel.experimental.algorithms\n  :members:\n  :undoc-members:\n  :imported-members:\n\nIterators\n---------\n\n.. automodule:: cuda.parallel.experimental.iterators\n  :members:\n  :undoc-members:\n  :imported-members:\n\nUtilities\n---------\n\n.. automodule:: cuda.parallel.experimental.struct\n   :members:\n```\n\n----------------------------------------\n\nTITLE: Declaring Scope-aware Relaxed Multimem Load-Reduce (min, s32) with CUDA PTX (C++)\nDESCRIPTION: This CUDA device function template extends multimem atomic min reduction for 32-bit signed integers to support relaxed or acquire semantics and per-invocation control over synchronization scope (CTA, cluster, GPU, or system), for global memory loads. Dependencies include device compilation and `cuda::ptx` semantic/scope operator types. Parameters are a semantic type, a scope type, the min operation, and the pointer. Returns reduced int32_t result; applicable for PTX ISA 81, SM_90.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_19\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.s32 dest, [addr]; // PTX ISA 81, SM_90\\n// .sem       = { .relaxed, .acquire }\\n// .scope     = { .cta, .cluster, .gpu, .sys }\\n// .op        = { .min }\\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\\n__device__ static inline int32_t multimem_ld_reduce(\\n  cuda::ptx::sem_t<Sem> sem,\\n  cuda::ptx::scope_t<Scope> scope,\\n  cuda::ptx::op_min_t,\\n  const int32_t* addr);\n```\n\n----------------------------------------\n\nTITLE: Implementing tcgen05 MMA Block Scale with mxf4nvf4 4X Vector Scaling and A Collector\nDESCRIPTION: Function declaration for tcgen05 matrix multiplication with Cta_Group parameter, using mxf4nvf4 data type with 4X vector scaling and collector strategy for matrix A with last use option. Requires CUDA PTX ISA 86 and SM_100a or SM_101a.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma.rst#2025-04-23_snippet_44\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.cta_group.kind.block_scale.scale_vec::4X.collector::a::lastuse [d_tmem], a_desc, b_desc, idesc, [scale_A_tmem], [scale_B_tmem], enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .kind      = { .kind::mxf4nvf4 }\n// .cta_group = { .cta_group::1, .cta_group::2 }\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_mma_block_scale_vec_4x_tmem_a_collector_a_lastuse(\n  cuda::ptx::kind_mxf4nvf4_t,\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  uint32_t scale_A_tmem,\n  uint32_t scale_B_tmem,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Performing Asynchronous Bulk Reduction (AND, b32/b64) from Shared to Global Memory in CUDA\nDESCRIPTION: This CUDA C++ template function `cp_reduce_async_bulk` performs an asynchronous bulk bitwise AND reduction. It reduces data from a source array in shared memory (`srcMem`) to a destination array in global memory (`dstMem`). The function is templated on the data type (`Type`, expected to be `.b32` or `.b64`) and requires PTX ISA 80 and SM_90 capability.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_reduce_async_bulk.rst#2025-04-23_snippet_13\n\nLANGUAGE: cuda\nCODE:\n```\n   // cp.reduce.async.bulk.dst.src.bulk_group.op.type  [dstMem], [srcMem], size; // 3. PTX ISA 80, SM_90\n   // .dst       = { .global }\n   // .src       = { .shared::cta }\n   // .type      = { .b32, .b64 }\n   // .op        = { .and }\n   template <typename Type>\n   __device__ static inline void cp_reduce_async_bulk(\n     cuda::ptx::space_global_t,\n     cuda::ptx::space_shared_t,\n     cuda::ptx::op_and_op_t,\n     Type* dstMem,\n     const Type* srcMem,\n     uint32_t size);\n```\n\n----------------------------------------\n\nTITLE: Defining tcgen05_mma_tmem_a Variant for Multiple Kinds Without N32 (CUDA C++)\nDESCRIPTION: Declares tcgen05_mma_tmem_a device template function for a broad set of dot kinds (f16, tf32, f8f6f4, i8) and CTA group 1, abstracting variant selection without explicit N32/scale parameters. Accepts shared memory A tile, output lane masking, and PTX MMA instruction configuration. Suits flexible integration for newer PTX architectures (ISA 86, SM_101a).\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma.rst#2025-04-23_snippet_8\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.cta_group.kind [d_tmem], [a_tmem], b_desc, idesc, disable_output_lane, enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\n// .cta_group = { .cta_group::1 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_tmem_a(\n  cuda::ptx::kind_t<Kind> kind,\n  cuda::ptx::cta_group_1_t,\n  uint32_t d_tmem,\n  uint32_t a_tmem,\n  uint64_t b_desc,\n  uint32_t idesc,\n  const uint32_t (\\u0026disable_output_lane)[4],\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Implementing multimem.ld_reduce for relaxed GPU global add operations on 64-bit integers\nDESCRIPTION: Template implementation for multimem atomic load-and-reduce operations with relaxed semantics, GPU scope, global memory, and add operation on 64-bit integers. Compatible with PTX ISA 81 on SM_90 architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_43\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.u64 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .add }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline int64_t multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_add_t,\n  const int64_t* addr);\n```\n\n----------------------------------------\n\nTITLE: Box Dimension Replacement\nDESCRIPTION: Device functions to replace box dimensions in tensormap, supporting both global and shared memory. Takes dimension ordinal and 32-bit value.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tensormap_replace.rst#2025-04-23_snippet_3\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <int N32, typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline void tensormap_replace_box_dim(\n  cuda::ptx::space_global_t,\n  void* tm_addr,\n  cuda::ptx::n32_t<N32> ord,\n  B32 new_val);\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Bulk ADD Reduction (Unsigned 32-bit) in CUDA\nDESCRIPTION: Performs an asynchronous bulk ADD reduction operation from CTA-shared to cluster-shared memory for unsigned 32-bit integers. Uses a memory barrier for synchronization.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_reduce_async_bulk.rst#2025-04-23_snippet_5\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline void cp_reduce_async_bulk(\n  cuda::ptx::space_cluster_t,\n  cuda::ptx::space_shared_t,\n  cuda::ptx::op_add_t,\n  uint32_t* dstMem,\n  const uint32_t* srcMem,\n  uint32_t size,\n  uint64_t* rdsmem_bar);\n```\n\n----------------------------------------\n\nTITLE: MMA Workstream Collector B1 Fill Operations\nDESCRIPTION: Template function implementing MMA workstream collector operations with b1 fill mode. Handles tensor memory operations and descriptor-based data formatting for multiple data types including f16, tf32, f8f6f4, and i8.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_15\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_collector_b1_fill(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d,\n  uint64_t zero_column_mask_desc);\n```\n\n----------------------------------------\n\nTITLE: Configuring CUDA Settings for Thrust in CMake\nDESCRIPTION: Sets up CUDA-specific configurations when the CUDA device system is selected. Enables the CUDA language and sets the target architecture if not already specified.\nSOURCE: https://github.com/nvidia/cccl/blob/main/examples/thrust_flexible_device_system/CMakeLists.txt#2025-04-23_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\n# CUDA specific setup\nif (CCCL_THRUST_DEVICE_SYSTEM STREQUAL \"CUDA\")\n  # Need to explicitly enable the CUDA language for the project.\n  # Note that the project(...) command earlier only enables CXX by default.\n  enable_language(CUDA)\n\n  # Compile for the native CUDA arch if not specified:\n  if(NOT DEFINED CMAKE_CUDA_ARCHITECTURES)\n    set(CMAKE_CUDA_ARCHITECTURES native)\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining CTA Group Tensor Core Function for 64x128b WarpX2 01_23 Variant - CUDA\nDESCRIPTION: This device template function declares the CTA group tensor core operation for 64x128b groups with WarpX2 01_23 layout and 4x16 pipeline, parameterized by PTX dot_cta_group. Its inputs are the group handle, an unsigned 32-bit tile address, and a 64-bit descriptor; all processing is device-side, requiring CUDA PTX with ISA 86 compatibility, and useful for hardware abstraction in tensor operations.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_cp.rst#2025-04-23_snippet_10\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.cp.cta_group.64x128b.warpx2::01_23.b8x16.b4x16_p64 [taddr], s_desc; // PTX ISA 86, SM_100a, SM_101a\\n// .cta_group = { .cta_group::1, .cta_group::2 }\\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\\n__device__ static inline void tcgen05_cp_64x128b_warpx2_01_23_b8x16_b4x16_p64(\\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\\n  uint32_t taddr,\\n  uint64_t s_desc);\n```\n\n----------------------------------------\n\nTITLE: Implementing Stateful Property for Memory Resource in C++\nDESCRIPTION: Illustrates how to implement a stateful property (required_alignment) for a memory resource and use it in a generic function.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/memory_resource/properties.rst#2025-04-23_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nstruct required_alignment{\n    using value_type = std::size_t;\n};\nstruct my_memory_resource {\n    friend constexpr std::size_t get_property(const my_memory_resource& resource, required_alignment) noexcept { return resource.required_alignment; }\n\n    std::size_t required_alignment;\n};\nstatic_assert(cuda::has_property_with<my_memory_resource, required_alignment, std::size_t>);\n\ntemplate<class MemoryResource>\nvoid* allocate_check_alignment(MemoryResource& resource, std::size_t size) {\n    if constexpr(cuda::has_property_with<MemoryResource, required_alignment, std::size_t>) {\n        return resource.allocate(size, get_property(resource, required_alignment{}));\n    } else {\n        // Use default alignment\n        return resource.allocate(size, 42);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Weak Global 32-bit Store Operation\nDESCRIPTION: Template function for 32-bit weak store operations in global memory. Uses weak memory semantics without specific scope requirements.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_st.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline void multimem_st(\n  cuda::ptx::sem_weak_t,\n  B32* addr,\n  B32 val);\n```\n\n----------------------------------------\n\nTITLE: 64-bit Signed Right Shift Operation in CUDA\nDESCRIPTION: Template function for performing signed right shift on 64-bit integers. Takes an int64_t value and shift amount as parameters.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/shr.rst#2025-04-23_snippet_5\n\nLANGUAGE: cuda\nCODE:\n```\n// shr.s64 dest, a_reg, b_reg; // PTX ISA 10, SM_50\ntemplate <typename = void>\n__device__ static inline int64_t shr(\n  int64_t a_reg,\n  uint32_t b_reg);\n```\n\n----------------------------------------\n\nTITLE: Storing 16x128b Data with tcgen05 in CUDA\nDESCRIPTION: This function performs a 16x128b store operation using tcgen05. It takes a 32-bit address and an array of 32 B32 values as input. The function is constrained to B32 types of 4 bytes.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_st.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline void tcgen05_st_16x128b_unpack_16b(\n  uint32_t taddr,\n  const B32 (&values)[32]);\n```\n\n----------------------------------------\n\nTITLE: Relaxed Semantic Barrier Arrive Operations\nDESCRIPTION: Memory barrier arrive operations with relaxed semantics. Includes variants for CTA and cluster scopes with optional count parameter.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/mbarrier_arrive.rst#2025-04-23_snippet_5\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_scope Scope>\n__device__ static inline uint64_t mbarrier_arrive(\n  cuda::ptx::sem_relaxed_t,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::space_shared_t,\n  uint64_t* addr,\n  const uint32_t& count);\n```\n\n----------------------------------------\n\nTITLE: Including Required Headers for CUB Testing in C++\nDESCRIPTION: Demonstrates how to include necessary headers for CUB testing, including the custom Catch2 test helper.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/test_overview.rst#2025-04-23_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n#include <cub/block/block_scan.cuh>\n#include <c2h/vector.h>\n#include <c2h/catch2_test_helper.h>\n```\n\n----------------------------------------\n\nTITLE: Including Unit Test Framework Subdirectory in CMake\nDESCRIPTION: Adds the `unittest` subdirectory to the build process using `add_subdirectory`. This command makes CMake process the `CMakeLists.txt` file located in the `unittest` directory, which presumably contains the definitions for the testing framework libraries required by the Thrust tests.\nSOURCE: https://github.com/nvidia/cccl/blob/main/thrust/testing/CMakeLists.txt#2025-04-23_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\n# Generate testing framework libraries:\nadd_subdirectory(unittest)\n```\n\n----------------------------------------\n\nTITLE: MMA Block Scale Vector 2X Collector Fill with mxf4 Type (CTA Group 1)\nDESCRIPTION: Template function for matrix multiplication with block scaling using vector 2X with collector A fill operation for CTA group 1. The function can use either mxf4 or mxf4nvf4 data types and requires PTX ISA 86 with SM_100a or SM_101a architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma.rst#2025-04-23_snippet_26\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.cta_group.kind.block_scale.scale_vec::2X.collector::a::fill [d_tmem], a_desc, b_desc, idesc, [scale_A_tmem], [scale_B_tmem], enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .kind      = { .kind::mxf4, .kind::mxf4nvf4 }\n// .cta_group = { .cta_group::1, .cta_group::2 }\ntemplate <cuda::ptx::dot_kind Kind, cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_mma_block_scale_vec_2_tmem_a_collector_a_fill(\n  cuda::ptx::kind_t<Kind> kind,\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  uint32_t scale_A_tmem,\n  uint32_t scale_B_tmem,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Implementing MMA Workspace Last Use Operations with I8 Data Type in CUDA\nDESCRIPTION: Template function for the final tensor compute operation (lastuse) with I8 data type. This variant handles tensor descriptors and supports zero column masking. Targets PTX ISA 86 on SM_100a and SM_101a architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_46\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.ws.cta_group.kind.collector::b2::lastuse [d_tmem], a_desc, b_desc, idesc, enable_input_d, zero_column_mask_desc; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1 }\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_collector_b2_lastuse(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d,\n  uint64_t zero_column_mask_desc);\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Bulk ADD Reduction (Signed 32-bit) in CUDA\nDESCRIPTION: Performs an asynchronous bulk ADD reduction operation from CTA-shared to cluster-shared memory for signed 32-bit integers. Uses a memory barrier for synchronization.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_reduce_async_bulk.rst#2025-04-23_snippet_10\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline void cp_reduce_async_bulk(\n  cuda::ptx::space_cluster_t,\n  cuda::ptx::space_shared_t,\n  cuda::ptx::op_add_t,\n  int32_t* dstMem,\n  const int32_t* srcMem,\n  uint32_t size,\n  uint64_t* rdsmem_bar);\n```\n\n----------------------------------------\n\nTITLE: Conditional Targeting for PTX Memory Barrier on Specific SM Version in CUDA C++\nDESCRIPTION: Demonstrates use of the NV_IF_TARGET macro to ensure a cuda::ptx PTX memory barrier call is included only when compiling for architectures that provide a specific SM version (here, SM_80). This guards against calling instructions not supported on earlier hardware generations and is essential when writing portable code across multiple GPU architectures. The macro and required symbols come from the CUDA toolkit and libcu++ headers.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx.rst#2025-04-23_snippet_4\n\nLANGUAGE: cuda\nCODE:\n```\nNV_IF_TARGET(NV_PROVIDES_SM_80,(\n  cuda::ptx::mbarrier_arrive(cuda::ptx::sem_release, cuda::ptx::scope_cta, cuda::ptx::space_shared, &bar, 1);\n));\n\n```\n\n----------------------------------------\n\nTITLE: Defining reStructuredText TOC for Thrust Searching Algorithms\nDESCRIPTION: Creates a table of contents structure that links to documentation for binary search and other searching-related function groups in the Thrust module API. Uses glob pattern to automatically include all matching documentation files.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/api_docs/algorithms/searching.rst#2025-04-23_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\n   :glob:\n   :maxdepth: 1\n\n   searching/binary_search\n   ${repo_docs_api_path}/*function_group__searching*\n```\n\n----------------------------------------\n\nTITLE: Implementing multimem.red.relaxed.gpu.global.max.s64 Operation in CUDA PTX\nDESCRIPTION: Template function declaration for performing a 64-bit signed integer max reduction with relaxed semantics and GPU scope in global memory. This function is available for SM_90 and requires PTX ISA 8.1 or higher.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_red.rst#2025-04-23_snippet_12\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.red.sem.scope.global.op.s64 [addr], val; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .release }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .max }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline void multimem_red(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_max_t,\n  int64_t* addr,\n  int64_t val);\n```\n\n----------------------------------------\n\nTITLE: Fixing Declarations in __threading_support (C++)\nDESCRIPTION: Issue #25 fixed in libcu++ 1.3.0. Corrected inconsistent qualifiers in declarations and definitions within the internal `__threading_support` component. Contribution by Gonzalo Brito Gadeschi.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/releases/changelog.rst#2025-04-23_snippet_20\n\nLANGUAGE: cpp\nCODE:\n```\n__threading_support // Fixed inconsistent qualifiers\n```\n\n----------------------------------------\n\nTITLE: Initializing MMA Collector with TMEM A and Zero Column Mask (CUDA)\nDESCRIPTION: This function initializes an MMA collector using TMEM for A and includes a zero column mask. It supports f16, tf32, f8f6f4, and i8 data types, and uses CTA group 1.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_24\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_tmem_a_collector_b1_lastuse(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint32_t a_tmem,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d,\n  uint64_t zero_column_mask_desc);\n```\n\n----------------------------------------\n\nTITLE: Implementing tcgen05 MMA Block Scale with mxf8f6f4 1X Vector Scaling and A Discard\nDESCRIPTION: Function declaration for tcgen05 matrix multiplication with Cta_Group parameter, using mxf8f6f4 data type with 1X vector scaling and discard strategy for matrix A. Requires CUDA PTX ISA 86 and SM_100a or SM_101a.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma.rst#2025-04-23_snippet_45\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.cta_group.kind.block_scale.scale_vec::1X.collector::a::discard [d_tmem], a_desc, b_desc, idesc, [scale_A_tmem], [scale_B_tmem], enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .kind      = { .kind::mxf8f6f4 }\n// .cta_group = { .cta_group::1, .cta_group::2 }\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_mma_block_scale_vec_1x_collector_a_discard(\n  cuda::ptx::kind_mxf8f6f4_t,\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  uint32_t scale_A_tmem,\n  uint32_t scale_B_tmem,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Binary Find Operation for 32-bit Unsigned Integer\nDESCRIPTION: Device function template implementing bfind operation for uint32_t type. Returns position of the highest set bit in a 32-bit unsigned integer.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/bfind.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\n// bfind.u32 dest, a_reg; // PTX ISA 20, SM_50\ntemplate <typename = void>\n__device__ static inline uint32_t bfind(\n  uint32_t a_reg);\n```\n\n----------------------------------------\n\nTITLE: Preparing Input Data for CUB Benchmarks\nDESCRIPTION: Creates device vectors for input and output, and initializes the input with random data using the gen function. This avoids compile-time overhead when generating test data.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/tuning.rst#2025-04-23_snippet_4\n\nLANGUAGE: c++\nCODE:\n```\nconst auto elements = static_cast<std::size_t>(state.get_int64(\"Elements{io}\"));\nthrust::device_vector<T> in(elements);\nthrust::device_vector<T> out(1);\n\ngen(seed_t{}, in);\n```\n\n----------------------------------------\n\nTITLE: Updating BlockLoad and BlockStore interfaces in CUB 1.6.3\nDESCRIPTION: Interface change for cub::BlockLoad and cub::BlockStore to be templated by local data type instead of Iterator type, allowing for output iterators with void as their value_type.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/releases/changelog.rst#2025-04-23_snippet_7\n\nLANGUAGE: C++\nCODE:\n```\ncub::BlockLoad\ncub::BlockStore\n```\n\n----------------------------------------\n\nTITLE: Element Stride Replacement\nDESCRIPTION: Device functions to replace element stride values in tensormap. Supports both memory spaces and takes dimension ordinal with 32-bit value.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tensormap_replace.rst#2025-04-23_snippet_4\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <int N32, typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline void tensormap_replace_element_stride(\n  cuda::ptx::space_global_t,\n  void* tm_addr,\n  cuda::ptx::n32_t<N32> ord,\n  B32 new_val);\n```\n\n----------------------------------------\n\nTITLE: Defining CUB ChainedPolicy for Runtime Policy Selection in C++\nDESCRIPTION: This snippet defines the `ChainedPolicy` template struct within CUB's internal dispatch mechanism. Its primary role is to select the most suitable pre-compiled policy (`ActivePolicy`) based on the device's actual PTX version available at runtime (`device_ptx_version`). The `Invoke` static method traverses the policy chain until it finds a policy compiled for a PTX version less than or equal to the runtime version, then calls the provided functor (`dispatch_closure`) templated on that policy.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/developer_overview.rst#2025-04-23_snippet_13\n\nLANGUAGE: cpp\nCODE:\n```\ntemplate <int PolicyPtxVersion, typename Policy, typename PrevPolicy>\nstruct ChainedPolicy {\n  using ActivePolicy = conditional_t<CUB_PTX_ARCH < PolicyPtxVersion,\n                                    typename PrevPolicy::ActivePolicy, Policy>;\n\n  template <typename Functor>\n  CUB_RUNTIME_FUNCTION _CCCL_FORCEINLINE\n  static cudaError_t Invoke(int device_ptx_version, Functor dispatch_closure) {\n    if (device_ptx_version < PolicyPtxVersion) {\n      PrevPolicy::Invoke(device_ptx_version, dispatch_closure);\n    }\n    dispatch_closure.Invoke<Policy>();\n  }\n};\n```\n\n----------------------------------------\n\nTITLE: 3D Tensor Copy: Global to Shared CTA Memory\nDESCRIPTION: Device function for asynchronous bulk tensor copy from global to shared CTA memory space for 3D tensors. Requires PTX ISA 80 and SM_90.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_async_bulk_tensor.rst#2025-04-23_snippet_3\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline void cp_async_bulk_tensor(\n  cuda::ptx::space_global_t,\n  cuda::ptx::space_shared_t,\n  const void* tensorMap,\n  const int32_t (&tensorCoords)[3],\n  const void* srcMem);\n```\n\n----------------------------------------\n\nTITLE: Declaring tcgen05_mma_ws_collector_b2_discard Function for Matrix Operations in CUDA\nDESCRIPTION: Template function for matrix multiply accumulate workload shaping operations with b2 collector discard mode. Takes matrix descriptors and memory references as parameters, supporting various data types through the Kind template parameter.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_56\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.ws.cta_group.kind.collector::b2::discard [d_tmem], a_desc, b_desc, idesc, enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1 }\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_collector_b2_discard(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: CUDA Async Bulk Reduction with MAX Operation (FP16)\nDESCRIPTION: Template function for performing asynchronous bulk reduction with MAX operation on FP16 data from shared CTA memory to global memory. Requires PTX ISA 80 and SM_90.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_reduce_async_bulk_f16.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n// cp.reduce.async.bulk.dst.src.bulk_group.op.type  [dstMem], [srcMem], size; // 4. PTX ISA 80, SM_90\n// .dst       = { .global }\n// .src       = { .shared::cta }\n// .type      = { .f16 }\n// .op        = { .max }\ntemplate <typename = void>\n__device__ static inline void cp_reduce_async_bulk(\n  cuda::ptx::space_global_t,\n  cuda::ptx::space_shared_t,\n  cuda::ptx::op_max_t,\n  __half* dstMem,\n  const __half* srcMem,\n  uint32_t size);\n```\n\n----------------------------------------\n\nTITLE: Message Passing Variable Declaration\nDESCRIPTION: Declares variables used in a message passing example between CUDA thread blocks.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/memory_model.rst#2025-04-23_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nint x = 0;\nint f = 0;\n```\n\n----------------------------------------\n\nTITLE: Global Minimum Reduction for uint64 with Different Memory Scopes\nDESCRIPTION: Template implementation for global minimum reduction operations on uint64_t values. Supports both relaxed and release semantics across CTA, cluster, GPU and system memory scopes. Requires PTX ISA 8.1 and SM_90.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_red.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.red.sem.scope.global.op.u64 [addr], val; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .release }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .min }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline void multimem_red(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_min_t,\n  uint64_t* addr,\n  uint64_t val);\n```\n\n----------------------------------------\n\nTITLE: Generating OpenMP-Specific Thrust Tests in CMake\nDESCRIPTION: This CMake script first finds all CUDA (`.cu`) and C++ (`.cpp`) source files in the current directory using `file(GLOB)`. It then iterates through a list of `THRUST_TARGETS`. If a target's `DEVICE` property, obtained via `thrust_get_target_property`, is specifically \"OMP\", it proceeds to create individual test targets for each source file found earlier. The test name is derived from the source filename and prefixed with \"omp.\" using `get_filename_component` and `string(PREPEND)`. Finally, `thrust_add_test` is called to define the actual test target.\nSOURCE: https://github.com/nvidia/cccl/blob/main/thrust/testing/omp/CMakeLists.txt#2025-04-23_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB test_srcs\n  RELATIVE \"${CMAKE_CURRENT_LIST_DIR}}\"\n  CONFIGURE_DEPENDS\n  *.cu *.cpp\n)\n\nforeach(thrust_target IN LISTS THRUST_TARGETS)\n  thrust_get_target_property(config_device ${thrust_target} DEVICE)\n  if (NOT config_device STREQUAL \"OMP\")\n    continue()\n  endif()\n\n  foreach(test_src IN LISTS test_srcs)\n    get_filename_component(test_name \"${test_src}\" NAME_WLE)\n    string(PREPEND test_name \"omp.\")\n    thrust_add_test(test_target ${test_name} \"${test_src}\" ${thrust_target})\n  endforeach()\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Integer Base-2 Logarithm Function Declaration in CUDA\nDESCRIPTION: Template function declaration for computing the floor of logarithm base 2 of an integer value. The function is available for both host and device execution.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/math/ilog.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename T>\n[[nodiscard]] __host__ __device__ inline constexpr\nint ilog2(T value) noexcept;\n```\n\n----------------------------------------\n\nTITLE: Implementing multimem.red.release.cta.global.add.u32 Operation in CUDA PTX\nDESCRIPTION: Template function declaration for performing a 32-bit unsigned integer addition reduction with release semantics and CTA scope in global memory. This function is available for SM_90 and requires PTX ISA 8.1 or higher.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_red.rst#2025-04-23_snippet_22\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.red.sem.scope.global.op.u32 [addr], val; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .release }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .add }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline void multimem_red(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_add_t,\n  uint32_t* addr,\n  uint32_t val);\n```\n\n----------------------------------------\n\nTITLE: 64-bit Shift Left Operation in CUDA PTX\nDESCRIPTION: Template function for performing shift left operation on 64-bit values. Takes a 64-bit value and uint32_t shift amount as parameters. Requires PTX ISA 10 and SM_50 architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/shl.rst#2025-04-23_snippet_2\n\nLANGUAGE: cuda\nCODE:\n```\n// shl.b64 dest, a_reg, b_reg; // PTX ISA 10, SM_50\ntemplate <typename B64, enable_if_t<sizeof(B64) == 8, bool> = true>\n__device__ static inline B64 shl(\n  B64 a_reg,\n  uint32_t b_reg);\n```\n\n----------------------------------------\n\nTITLE: Setting CPM Options for CCCL Examples\nDESCRIPTION: Creates CMake options specific to CPM configuration that will be passed to compile tests. These options specify which repository and branch to use when fetching CCCL sources.\nSOURCE: https://github.com/nvidia/cccl/blob/main/examples/CMakeLists.txt#2025-04-23_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nset(cmake_cpm_opts\n  -D \"CCCL_REPOSITORY=${CCCL_EXAMPLE_CPM_REPOSITORY}\"\n  -D \"CCCL_TAG=${CCCL_EXAMPLE_CPM_TAG}\"\n)\n```\n\n----------------------------------------\n\nTITLE: Storing and Unpacking 32x32b Data with tcgen05 in CUDA (2 values)\nDESCRIPTION: This function performs a 32x32b store operation with 16-bit unpacking using tcgen05. It takes a 32-bit address and an array of 2 B32 values as input. The function is constrained to B32 types of 4 bytes.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_st.rst#2025-04-23_snippet_20\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.st.sync.aligned.32x32b.x2.unpack::16b.b32 [taddr], values; // PTX ISA 86, SM_100a, SM_101a\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline void tcgen05_st_32x32b_unpack_16b(\n  uint32_t taddr,\n  const B32 (&values)[2]);\n```\n\n----------------------------------------\n\nTITLE: Creating Device Tests with Extended Compiler Options\nDESCRIPTION: Creates tests for device-related functionality with additional NVIDIA compiler options for extended lambda support and relaxed constexpr evaluation. These tests verify device properties and architecture traits functionality.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cudax/test/CMakeLists.txt#2025-04-23_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\n  cudax_add_catch2_test(test_target device ${cn_target}\n    device/device_smoke.cu\n    device/arch_traits.cu\n  )\n  target_compile_options(${test_target} PRIVATE $<$<COMPILE_LANG_AND_ID:CUDA,NVIDIA>:--extended-lambda>)\n  target_compile_options(${test_target} PRIVATE $<$<COMPILE_LANG_AND_ID:CUDA,NVIDIA>:--expt-relaxed-constexpr>)\n```\n\n----------------------------------------\n\nTITLE: Configuring NVBench Helper Tests in CMake\nDESCRIPTION: Defines a function to add nvbench_helper tests for different device systems (CPP and CUDA) and sets up the test targets if enabled.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cub/benchmarks/nvbench_helper/CMakeLists.txt#2025-04-23_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nif (CUB_ENABLE_NVBENCH_HELPER_TESTS)\n  cccl_get_boost()\n\n  function(add_nvbench_helper_test device_system)\n    set(nvbench_helper_test_target nvbench_helper.test.${device_system})\n    add_executable(${nvbench_helper_test_target} test/gen_seed.cu\n                                                 test/gen_range.cu\n                                                 test/gen_entropy.cu\n                                                 test/gen_uniform_distribution.cu\n                                                 test/gen_power_law_distribution.cu)\n    cccl_configure_target(${nvbench_helper_test_target} DIALECT 17)\n    target_link_libraries(${nvbench_helper_test_target} PRIVATE nvbench_helper Catch2::Catch2WithMain Boost::math)\n    if (\"${device_system}\" STREQUAL \"cpp\")\n      target_compile_definitions(${nvbench_helper_test_target} PRIVATE THRUST_DEVICE_SYSTEM=THRUST_DEVICE_SYSTEM_CPP)\n    endif()\n  endfunction()\n\n  add_nvbench_helper_test(cpp)\n  add_nvbench_helper_test(cuda)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Declaring tcgen05_mma_ws_collector_b3_lastuse without zero_column_mask_desc for CUDA\nDESCRIPTION: Template function declaration for tensor core operations using descriptor-based operands without zero column masking. This variant applies to all four data types (f16, tf32, f8f6f4, i8) for CTA group 1 with b3 collector in last-use mode.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_69\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.ws.cta_group.kind.collector::b3::lastuse [d_tmem], a_desc, b_desc, idesc, enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1 }\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_collector_b3_lastuse(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Declaring tcgen05_mma_ws_tmem_a_collector_b2_discard Without Zero Column Mask\nDESCRIPTION: Template function for MMA workload shaping using tmem buffer for matrix A with b2 collector discard mode. This variant omits the zero column masking functionality for simpler operations.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_58\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.ws.cta_group.kind.collector::b2::discard [d_tmem], [a_tmem], b_desc, idesc, enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1 }\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_tmem_a_collector_b2_discard(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint32_t a_tmem,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Memory Resource with Property Checking\nDESCRIPTION: Demonstrates how to implement and use memory resources with additional property checks using resource_with concept.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/memory_resource/resource.rst#2025-04-23_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nstruct required_alignment{\n    using value_type = std::size_t;\n};\nstruct my_memory_resource {\n    void* allocate(std::size_t, std::size_t) { return nullptr; }\n    void deallocate(void*, std::size_t, std::size_t) noexcept {}\n    bool operator==(const my_memory_resource&) const { return true; }\n    bool operator!=(const my_memory_resource&) const { return false; }\n\n    friend constexpr std::size_t get_property(const my_memory_resource& resource, required_alignment) noexcept { return resource.required_alignment; }\n\n    std::size_t required_alignment;\n};\n\ntemplate<class MemoryResource>\n    requires cuda::mr::resource<MemoryResource>\nvoid* maybe_allocate_async_check_alignment(MemoryResource& resource, std::size_t size, cuda::stream_ref stream) {\n    if constexpr(cuda::mr::async_resource_with<MemoryResource, required_alignment>) {\n        return resource.allocate_async(size, get_property(resource, required_alignment), stream);\n    } else if constexpr (cuda::mr::async_resource<MemoryResource>) {\n        return resource.allocate_async(size, my_default_alignment, stream);\n    } else if constexpr (cuda::mr::resource_with<MemoryResource, required_alignment>) {\n        return resource.allocate(size, get_property(resource, required_alignment));\n    } else {\n        return resource.allocate(size, my_default_alignment);\n    }\n}\n\ntemplate<class MemoryResource>\n    requires cuda::mr::resource<MemoryResource>\nvoid* maybe_allocate_async_check_alignment2(MemoryResource& resource, std::size_t size, cuda::stream_ref stream) {\n    constexpr std::size_t align = cuda::mr::resource_with<MemoryResource, required_alignment>\n                                ? get_property(resource, required_alignment)\n                                : my_default_alignment;\n    if constexpr(cuda::mr::async_resource<MemoryResource>) {\n        return resource.allocate_async(size, align, stream);\n    } else {\n        return resource.allocate(size, align);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Multidimensional Configuration Spaces in CUB Tests with C++\nDESCRIPTION: Shows how to create multidimensional configuration spaces for testing CUB algorithms with different types and thread block sizes.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/test_overview.rst#2025-04-23_snippet_5\n\nLANGUAGE: C++\nCODE:\n```\nusing block_sizes = c2h::enum_type_list<int, 128, 256>;\nusing types = c2h::type_list<std::uint8_t, std::int32_t>;\n\nC2H_TEST(\"SCOPE FACILITY works with CONDITION\", \"[FACILITY][SCOPE]\",\n         types, block_sizes)\n{\n  using type = typename c2h::get<0, TestType>;\n  constexpr int threads_per_block = c2h::get<1, TestType>::value;\n  // ...\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring NVRTCC Project with CUDA Dependencies in CMake\nDESCRIPTION: Sets up a CMake project named 'nvrtcc' that builds an executable from nvrtcc.cpp. The configuration enables CUDA language support, links against required CUDA libraries (NVRTC, CUDA Runtime, and CUDA Driver), and specifies C++17 as the required C++ standard.\nSOURCE: https://github.com/nvidia/cccl/blob/main/libcudacxx/test/utils/nvidia/nvrtc/CMakeLists.txt#2025-04-23_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nproject(nvrtcc)\n\nenable_language(CUDA)\nfind_package(CUDAToolkit REQUIRED)\n\nadd_executable(libcudacxx.nvrtcc nvrtcc.cpp)\n\nset_target_properties(libcudacxx.nvrtcc PROPERTIES OUTPUT_NAME nvrtcc)\ntarget_link_libraries(libcudacxx.nvrtcc CUDA::nvrtc CUDA::cudart CUDA::cuda_driver)\ntarget_compile_features(libcudacxx.nvrtcc PRIVATE cxx_std_17)\n```\n\n----------------------------------------\n\nTITLE: Setting up documentation toctree for Thrust placeholder objects\nDESCRIPTION: Configures a table of contents tree (toctree) directive that includes all documentation files matching the '*placeholder*' pattern from the repository's API documentation path.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/api_docs/function_objects/placeholder.rst#2025-04-23_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. toctree::\n   :glob:\n   :maxdepth: 1\n\n   ${repo_docs_api_path}/*placeholder*\n```\n\n----------------------------------------\n\nTITLE: CTA Group Template Function for 128x128b Configuration\nDESCRIPTION: Template function declaration for 128x128b memory layout supporting CTA groups 1 and 2. Takes a CTA group parameter, target address, and descriptor as inputs.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_cp.rst#2025-04-23_snippet_2\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.cp.cta_group.128x128b [taddr], s_desc; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1, .cta_group::2 }\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_cp_128x128b(\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t taddr,\n  uint64_t s_desc);\n```\n\n----------------------------------------\n\nTITLE: Implementing tcgen05 MMA Block Scale with mxf8f6f4 1X Vector Scaling and A Collector\nDESCRIPTION: Function declaration for tcgen05 matrix multiplication with cta_group parameter, using mxf8f6f4 data type with 1X vector scaling and collector strategy for matrix A with last use option. Requires CUDA PTX ISA 86 and SM_100a or SM_101a.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma.rst#2025-04-23_snippet_42\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.cta_group.kind.block_scale.scale_vec::1X.collector::a::lastuse [d_tmem], a_desc, b_desc, idesc, [scale_A_tmem], [scale_B_tmem], enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .kind      = { .kind::mxf8f6f4 }\n// .cta_group = { .cta_group::1, .cta_group::2 }\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_mma_block_scale_vec_1x_tmem_a_collector_a_lastuse(\n  cuda::ptx::kind_mxf8f6f4_t,\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  uint32_t scale_A_tmem,\n  uint32_t scale_B_tmem,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Initializing MMA Collector with TMEM A without Zero Column Mask (CUDA)\nDESCRIPTION: This function initializes an MMA collector using TMEM for A without a zero column mask. It supports f16, tf32, f8f6f4, and i8 data types, and uses CTA group 1.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_25\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_tmem_a_collector_b1_lastuse(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint32_t a_tmem,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Implementing MMA Workspace Last Use Operations with F8F6F4 Data Type in CUDA\nDESCRIPTION: Template function for the final tensor compute operation (lastuse) with F8F6F4 data type. This variant handles tensor descriptors and supports zero column masking. Targets PTX ISA 86 on SM_100a and SM_101a architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_45\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.ws.cta_group.kind.collector::b2::lastuse [d_tmem], a_desc, b_desc, idesc, enable_input_d, zero_column_mask_desc; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1 }\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_collector_b2_lastuse(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d,\n  uint64_t zero_column_mask_desc);\n```\n\n----------------------------------------\n\nTITLE: CUDA Pipeline Creation Example\nDESCRIPTION: Example kernel demonstrating different ways to create CUDA pipelines, including thread-scoped, block-scoped unified, and partitioned pipelines with different thread roles.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/synchronization_primitives/pipeline/make_pipeline.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n#include <cuda/pipeline>\n#include <cooperative_groups.h>\n\n// Disables `pipeline_shared_state` initialization warning.\n#pragma nv_diag_suppress static_var_with_dynamic_init\n\n__global__ void example_kernel() {\n  __shared__ cuda::pipeline_shared_state<cuda::thread_scope_block, 2> pss0;\n  __shared__ cuda::pipeline_shared_state<cuda::thread_scope_block, 2> pss1;\n  __shared__ cuda::pipeline_shared_state<cuda::thread_scope_block, 2> pss2;\n\n  auto group = cooperative_groups::this_thread_block();\n\n  // Create a single thread scoped pipeline.\n  cuda::pipeline<cuda::thread_scope_thread> p0 = cuda::make_pipeline();\n\n  // Create a unified block-scoped pipeline.\n  cuda::pipeline<cuda::thread_scope_block> p1 = cuda::make_pipeline(group, &pss0);\n\n  // Create a partitioned block-scoped pipeline where half the threads are producers.\n  cuda::std::size_t producer_count = group.size() / 2;\n  cuda::pipeline<cuda::thread_scope_block> p2\n    = cuda::make_pipeline(group, &pss1, producer_count);\n\n  // Create a partitioned block-scoped pipeline where all threads with an even\n  // `thread_rank` are producers.\n  auto thread_role = (group.thread_rank() % 2)\n                     ? cuda::pipeline_role::producer\n                    : cuda::pipeline_role::consumer;\n  cuda::pipeline<cuda::thread_scope_block> p3\n    = cuda::make_pipeline(group, &pss2, thread_role);\n}\n```\n\n----------------------------------------\n\nTITLE: Function Signature of cuda::bitfield_insert\nDESCRIPTION: Defines the template function signature for cuda::bitfield_insert which takes a destination value, source value, start position, and width to extract bits from source and insert them into destination.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/bit/bitfield_insert.rst#2025-04-23_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\ntemplate <typename T>\n[[nodiscard]] constexpr T\nbitfield_insert(T dest, T source, int start, int width) noexcept;\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Bulk Tensor Reduction for 2D Tensors in CUDA\nDESCRIPTION: This template function performs asynchronous bulk tensor reduction for 2D tensors. It supports various reduction operations between global and shared memory spaces. The function is designed for PTX ISA 80 and SM_90 architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_reduce_async_bulk_tensor.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n// cp.reduce.async.bulk.tensor.2d.dst.src.op.tile.bulk_group [tensorMap, tensorCoords], [srcMem]; // 1b. PTX ISA 80, SM_90\n// .dst       = { .global }\n// .src       = { .shared::cta }\n// .op        = { .add, .min, .max, .inc, .dec, .and, .or, .xor }\ntemplate <cuda::ptx::dot_op Op>\n__device__ static inline void cp_reduce_async_bulk_tensor(\n  cuda::ptx::space_global_t,\n  cuda::ptx::space_shared_t,\n  cuda::ptx::op_t<Op> op,\n  const void* tensorMap,\n  const int32_t (&tensorCoords)[2],\n  const void* srcMem);\n```\n\n----------------------------------------\n\nTITLE: Declaring tcgen05_mma_ws_collector_b3_fill with Zero Column Mask\nDESCRIPTION: Template function for MMA workload shaping with b3 collector fill mode. This variant includes support for zero column masking through the zero_column_mask_desc parameter for enhanced control over matrix operations.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_59\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.ws.cta_group.kind.collector::b3::fill [d_tmem], a_desc, b_desc, idesc, enable_input_d, zero_column_mask_desc; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1 }\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_collector_b3_fill(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d,\n  uint64_t zero_column_mask_desc);\n```\n\n----------------------------------------\n\nTITLE: 32-bit Shift Left Operation in CUDA PTX\nDESCRIPTION: Template function for performing shift left operation on 32-bit values. Takes a 32-bit value and uint32_t shift amount as parameters. Requires PTX ISA 10 and SM_50 architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/shl.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n// shl.b32 dest, a_reg, b_reg; // PTX ISA 10, SM_50\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline B32 shl(\n  B32 a_reg,\n  uint32_t b_reg);\n```\n\n----------------------------------------\n\nTITLE: Accessing Clock Information in CUDA\nDESCRIPTION: These functions retrieve various clock-related information. They use inline PTX instructions to access the special registers.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/get_sreg.rst#2025-04-23_snippet_11\n\nLANGUAGE: cuda\nCODE:\n```\n// mov.u32 sreg_value, %%clock; // PTX ISA 10\ntemplate <typename = void>\n__device__ static inline uint32_t get_sreg_clock();\n```\n\nLANGUAGE: cuda\nCODE:\n```\n// mov.u32 sreg_value, %%clock_hi; // PTX ISA 50, SM_35\ntemplate <typename = void>\n__device__ static inline uint32_t get_sreg_clock_hi();\n```\n\nLANGUAGE: cuda\nCODE:\n```\n// mov.u64 sreg_value, %%clock64; // PTX ISA 20, SM_35\ntemplate <typename = void>\n__device__ static inline uint64_t get_sreg_clock64();\n```\n\n----------------------------------------\n\nTITLE: Dereferencing cuda::annotated_ptr in CUDA\nDESCRIPTION: Dereference operator for cuda::annotated_ptr that returns a reference to the pointed-to value, applying the associated access property.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/memory_access_properties/annotated_ptr.rst#2025-04-23_snippet_5\n\nLANGUAGE: cuda\nCODE:\n```\nreference operator*() const;\n```\n\n----------------------------------------\n\nTITLE: Multimem Reduction Template for 32-bit Unsigned Integer Max Operation\nDESCRIPTION: Template function implementing multimem reduction with maximum operation for 32-bit unsigned integers. Supports different memory semantics (relaxed/release) and scopes (cta/cluster/gpu/sys).\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_red.rst#2025-04-23_snippet_5\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.red.sem.scope.global.op.u32 [addr], val; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .release }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .max }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline void multimem_red(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_max_t,\n  uint32_t* addr,\n  uint32_t val);\n```\n\n----------------------------------------\n\nTITLE: Implementing CP Async Memory Barrier Arrive Operation in CUDA\nDESCRIPTION: Template function wrapper for the cp.async.mbarrier.arrive.b64 PTX instruction. This operation is available on PTX ISA 70 and SM_80 architectures. Takes a 64-bit address pointer as parameter.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_async_mbarrier_arrive.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\n// cp.async.mbarrier.arrive.b64 [addr]; // PTX ISA 70, SM_80\ntemplate <typename = void>\n__device__ static inline void cp_async_mbarrier_arrive(\n  uint64_t* addr);\n```\n\n----------------------------------------\n\nTITLE: Processing Benchmark Directory in CMake\nDESCRIPTION: A function that processes a directory of benchmark source files, creating base and tuning variants for each benchmark. It handles the configuration of compile definitions and options for each target, and registers the benchmarks with the CCCL benchmark system.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cub/benchmarks/CMakeLists.txt#2025-04-23_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(add_bench_dir bench_dir)\n  file(GLOB bench_srcs CONFIGURE_DEPENDS \"${bench_dir}/*.cu\")\n  file(RELATIVE_PATH bench_prefix \"${benches_root}\" \"${bench_dir}\")\n  file(TO_CMAKE_PATH \"${bench_prefix}\" bench_prefix)\n  string(REPLACE \"/\" \".\" bench_prefix \"${bench_prefix}\")\n\n  foreach(bench_src IN LISTS bench_srcs)\n    # base tuning\n    get_filename_component(bench_name \"${bench_src}\" NAME_WLE)\n    string(PREPEND bench_name \"cub.${bench_prefix}.\")\n\n    set(base_bench_name \"${bench_name}.base\")\n    add_bench(base_bench_target ${base_bench_name} \"${bench_src}\")\n    add_dependencies(${benches_meta_target} ${base_bench_target})\n    target_compile_definitions(${base_bench_target} PRIVATE TUNE_BASE=1)\n    target_compile_options(${base_bench_target} PRIVATE \"--extended-lambda\")\n\n    if (CUB_ENABLE_TUNING)\n      # tuning\n      set_property(DIRECTORY APPEND PROPERTY CMAKE_CONFIGURE_DEPENDS \"${bench_src}\")\n      get_bench_ranges(\"${bench_src}\" \"${bench_name}\")\n      set(tuning_name \"${bench_name}.variant\")\n      set(tuning_path \"${CMAKE_BINARY_DIR}/${tuning_name}.h\")\n      add_bench(bench_target ${tuning_name} \"${bench_src}\")\n      # for convenience, make tuning variant buildable by default\n      file(WRITE \"${tuning_path}\" \"#pragma once\\n#define TUNE_BASE 1\\n\")\n      target_compile_options(${bench_target} PRIVATE \"--extended-lambda\" \"-include${tuning_path}\")\n    else()\n      # benchmarking\n      register_cccl_benchmark(\"${bench_name}\" \"\")\n    endif()\n  endforeach()\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Declaring multimem_ld_reduce with Acquire/CTA Scope and XOR Operation in CUDA PTX\nDESCRIPTION: Template function declaration for multimem.ld_reduce with acquire memory semantics, CTA scope, and XOR operation on 64-bit values in global memory. Designed for PTX ISA 8.1 on SM_90 architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_74\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.b64 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .xor }\ntemplate <typename B64, enable_if_t<sizeof(B64) == 8, bool> = true, cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline B64 multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_xor_op_t,\n  const B64* addr);\n```\n\n----------------------------------------\n\nTITLE: Store 8-bit Data with L1 No Allocate and L2 Cache Hint in CUDA\nDESCRIPTION: This function stores 8-bit data to global memory with L1 cache no allocate policy and L2 cache hint. It requires SM_80 or later architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/st.rst#2025-04-23_snippet_15\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename B8, enable_if_t<sizeof(B8) == 1, bool> = true>\n__device__ static inline void st_L1_no_allocate_L2_cache_hint(\n  cuda::ptx::space_global_t,\n  B8* addr,\n  B8 src,\n  uint64_t cache_policy);\n```\n\n----------------------------------------\n\nTITLE: Converting Boolean Values for Python Compatibility in CMake\nDESCRIPTION: A CMake macro that converts CMake boolean values (ON/OFF) to Python-compatible boolean values (True/False) for configuration files.\nSOURCE: https://github.com/nvidia/cccl/blob/main/libcudacxx/test/libcudacxx/CMakeLists.txt#2025-04-23_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nmacro(pythonize_bool var)\n  if (${var})\n    set(${var} True)\n  else()\n    set(${var} False)\n  endif()\nendmacro()\n```\n\n----------------------------------------\n\nTITLE: TCGen05 MMA Workspace I8 Collector Implementation\nDESCRIPTION: Template device function for TCGen05 MMA workspace operations using I8 data type. Handles memory operations with CTA group 1 configuration and collector type B3 with discard functionality.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_78\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.ws.cta_group.kind.collector::b3::discard [d_tmem], [a_tmem], b_desc, idesc, enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1 }\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_tmem_a_collector_b3_discard(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint32_t a_tmem,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Declaring multimem_ld_reduce with Acquire/Cluster Scope and OR Operation in CUDA PTX\nDESCRIPTION: Template function declaration for multimem.ld_reduce with acquire memory semantics, cluster scope, and OR operation on 64-bit values in global memory. For PTX ISA 8.1 and SM_90 architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_66\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.b64 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .or }\ntemplate <typename B64, enable_if_t<sizeof(B64) == 8, bool> = true, cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline B64 multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_or_op_t,\n  const B64* addr);\n```\n\n----------------------------------------\n\nTITLE: Implementing mbarrier.arrive.expect_tx with Release Semantics for CTA/Cluster Scope in Shared CTA Space\nDESCRIPTION: This CUDA template function implements the mbarrier.arrive.expect_tx operation with release semantics for CTA or cluster scope in shared CTA space. It takes a 64-bit address and a transaction count as parameters.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/mbarrier_arrive_expect_tx.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_scope Scope>\n__device__ static inline uint64_t mbarrier_arrive_expect_tx(\n  cuda::ptx::sem_release_t,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::space_shared_t,\n  uint64_t* addr,\n  const uint32_t& tx_count);\n```\n\n----------------------------------------\n\nTITLE: Defining Search Space Parameters for CUB Benchmarks\nDESCRIPTION: Defines the search space for tuning parameters using special comment syntax. This example configures items per thread and threads per block ranges for optimization.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/tuning.rst#2025-04-23_snippet_1\n\nLANGUAGE: c++\nCODE:\n```\n// %RANGE% TUNE_ITEMS_PER_THREAD ipt 7:24:1\n// %RANGE% TUNE_THREADS_PER_BLOCK tpb 128:1024:32\n```\n\n----------------------------------------\n\nTITLE: Installing and Cleaning up the Test Fixture in CMake - CMake Language\nDESCRIPTION: Defines fixture setup and cleanup tests for the install tree using `add_test` and `set_tests_properties`. The first snippet registers a test that installs the build to a temporary prefix and designates it as a test fixture setup (`FIXTURES_SETUP`). The second registers a test to recursively remove the temporary install location when done and marks it as a fixture cleanup (`FIXTURES_CLEANUP`). These routines require that the build process produces an installable CMake project, and the supporting commands are available in the environment.\nSOURCE: https://github.com/nvidia/cccl/blob/main/test/cmake/CMakeLists.txt#2025-04-23_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nadd_test(NAME cccl.test.cmake.install_tree.install\n  COMMAND \"${CMAKE_COMMAND}\"\n    --install \"${CCCL_BINARY_DIR}\"\n    --prefix \"${tmp_install_prefix}\"\n)\nset_tests_properties(cccl.test.cmake.install_tree.install PROPERTIES\n  FIXTURES_SETUP install_tree\n)\n```\n\nLANGUAGE: CMake\nCODE:\n```\nadd_test(NAME cccl.test.cmake.install_tree.cleanup\n  COMMAND \"${CMAKE_COMMAND}\" -E rm -rf \"${tmp_install_prefix}\"\n)\nset_tests_properties(cccl.test.cmake.install_tree.cleanup PROPERTIES\n  FIXTURES_CLEANUP install_tree\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring MSVC-Specific Compiler Settings\nDESCRIPTION: Sets MSVC-specific compiler options, including debug information format and C++ preprocessor directives for CUDA kernel launching.\nSOURCE: https://github.com/nvidia/cccl/blob/main/libcudacxx/test/libcudacxx/CMakeLists.txt#2025-04-23_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nif (MSVC)\n  # sccache cannot handle the -Fd option generationg pdb files\n  set(CMAKE_MSVC_DEBUG_INFORMATION_FORMAT Embedded)\n\n  # We want to use cudaLaunchKernelEx which is guarded by __cplusplus\n  if (\"${CMAKE_CUDA_COMPILER_VERSION}\" LESS \"12.3.0\")\n    string(APPEND LIBCUDACXX_TEST_COMPILER_FLAGS \" -Xcompiler=/Zc:__cplusplus\")\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Replacing Tile Swizzle Atomicity in Shared::CTA Memory with CUDA C++\nDESCRIPTION: This code snippet defines a templated __device__ inline function designed to replace a tile using swizzle atomicity mode in the shared::cta memory space of a tensor map, utilizing PTX ISA 86. Input parameters are the shared memory specifier, pointer to the tensor map, and a 32-vector value. It depends on the CUDA/PTX layers and is restricted to architectures with SM_100a or SM_101a support.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tensormap_replace.rst#2025-04-23_snippet_8\n\nLANGUAGE: cuda\nCODE:\n```\n// tensormap.replace.tile.swizzle_atomicity.space.b1024.b32 [tm_addr], new_val; // PTX ISA 86, SM_100a, SM_101a\\n// .space     = { .shared::cta }\\ntemplate <int N32>\\n__device__ static inline void tensormap_replace_swizzle_atomicity(\\n  cuda::ptx::space_shared_t,\\n  void* tm_addr,\\n  cuda::ptx::n32_t<N32> new_val);\n```\n\n----------------------------------------\n\nTITLE: Storing 32x32b Data with tcgen05 in CUDA (16 values)\nDESCRIPTION: This function performs a 32x32b store operation using tcgen05. It takes a 32-bit address and an array of 16 B32 values as input. The function is constrained to B32 types of 4 bytes.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_st.rst#2025-04-23_snippet_25\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.st.sync.aligned.32x32b.x16.b32 [taddr], values; // PTX ISA 86, SM_100a, SM_101a\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline void tcgen05_st_32x32b(\n  uint32_t taddr,\n  const B32 (&values)[16]);\n```\n\n----------------------------------------\n\nTITLE: Implementing cp.async.bulk.wait_group.read in CUDA\nDESCRIPTION: This code snippet defines a template function for the cp.async.bulk.wait_group.read PTX instruction. It takes a template parameter N32 and uses cuda::ptx::n32_t to represent the wait group count for read operations.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_async_bulk_wait_group.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n// cp.async.bulk.wait_group.read N; // PTX ISA 80, SM_90\ntemplate <int N32>\n__device__ static inline void cp_async_bulk_wait_group_read(\n  cuda::ptx::n32_t<N32> N);\n```\n\n----------------------------------------\n\nTITLE: Implementing multimem.red.relaxed.cta.global.add.u64 Operation in CUDA PTX\nDESCRIPTION: Template function declaration for performing a 64-bit unsigned integer addition reduction with relaxed semantics and CTA scope in global memory. This function is available for SM_90 and requires PTX ISA 8.1 or higher.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_red.rst#2025-04-23_snippet_26\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.red.sem.scope.global.op.u64 [addr], val; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .release }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .add }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline void multimem_red(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_add_t,\n  uint64_t* addr,\n  uint64_t val);\n```\n\n----------------------------------------\n\nTITLE: Performing Asynchronous Bulk Reduction (MAX, s32) from Shared to Global Memory in CUDA\nDESCRIPTION: This CUDA C++ device function `cp_reduce_async_bulk` performs an asynchronous bulk maximum reduction for signed 32-bit integers. It reduces data from a source array in shared memory (`srcMem`) to a destination array in global memory (`dstMem`). This overload requires PTX ISA 80 and SM_90 capability.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_reduce_async_bulk.rst#2025-04-23_snippet_22\n\nLANGUAGE: cuda\nCODE:\n```\n   // cp.reduce.async.bulk.dst.src.bulk_group.op.type  [dstMem], [srcMem], size; // 4. PTX ISA 80, SM_90\n   // .dst       = { .global }\n   // .src       = { .shared::cta }\n   // .type      = { .s32 }\n   // .op        = { .max }\n   template <typename = void>\n   __device__ static inline void cp_reduce_async_bulk(\n     cuda::ptx::space_global_t,\n     cuda::ptx::space_shared_t,\n     cuda::ptx::op_max_t,\n     int32_t* dstMem,\n     const int32_t* srcMem,\n     uint32_t size);\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication with 2X Vector Scaling and MXF4/NVF4 Support\nDESCRIPTION: Template function for matrix multiplication operations with 2X vector scaling. Supports both mxf4 and mxf4nvf4 kinds with configurable CTA groups. Takes memory descriptors and scaling parameters as inputs.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma.rst#2025-04-23_snippet_39\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_kind Kind, cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_mma_block_scale_vec_2_tmem_a_collector_a_use(\n  cuda::ptx::kind_t<Kind> kind,\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  uint32_t scale_A_tmem,\n  uint32_t scale_B_tmem,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Testing CCCL Packages and Subprojects with Conditional Configuration (CMake)\nDESCRIPTION: This CMake script provides an automated mechanism to test the discovery and configuration of CCCL and its subprojects (Thrust, CUB, libcudacxx, cudax) via different packaging methods (CCCL meta-package, native, or subdirectory inclusion). It defines variables for package root, components to test, and package type strategy, then dynamically selects and invokes the appropriate find_package/add_subdirectory logic. The script then iterates through the requested components, creates version-checking executables, links each to the relevant target, and defines compile-time macros based on the component's versioning scheme. Inputs include CCCL_ROOT, COMPONENTS, and PACKAGE_TYPE. Outputs are CTest targets for each component. Prerequisites: CMake 3.21+, C++17, and accessible sources/installation roots. Limitations: Only accepts Thrust, CUB, libcudacxx, and cudax as valid components.\nSOURCE: https://github.com/nvidia/cccl/blob/main/test/cmake/test_export/CMakeLists.txt#2025-04-23_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\n# Test the CMake packages for CCCL and all subprojects.\\n#\\n# Parameters:\\n# - CCCL_ROOT [Path] Root of the CCCL repo, or an installation root.\\n# - COMPONENTS [StringList] {Thrust CUB libcudacxx} Which CCCL subprojects\\n#   should be found.\\n# - PACKAGE_TYPE [String] {CCCL | NATIVE | SUBDIR}:\\n#   - CCCL -> `find_package(CCCL COMPONENTS <subproject>)`\\n#   - NATIVE -> `find_package(<subproject>)`\\n#   - SUBDIR -> `set(CCCL_REQUIRED_COMPONENTS <subproject>)`\\n#               `add_subdirectory(${cccl_root})`\\n\\ncmake_minimum_required(VERSION 3.21)\\nproject(CCCLTestExport LANGUAGES CXX)\\n\\ninclude(CTest)\\nenable_testing()\\n\\nset(CCCL_ROOT \"\" CACHE PATH\\n  \"Root of the CCCL repo, or an installation root.\")\\nset(COMPONENTS \"\" CACHE STRING\\n  \"DEFAULT for no components, or semi-colon delimited list of Thrust, CUB, and/or libcudacxx.\")\\nset(PACKAGE_TYPE \"\" CACHE STRING\\n  \"CCCL: Find CCCL with subpackages as components; NATIVE: Find subpackages directly; SUBDIR: add_subdirectory(${CCCL_ROOT}\")\\nset_property(CACHE PACKAGE_TYPE PROPERTY STRINGS CCCL NATIVE SUBDIR)\\n\\nmessage(STATUS \"CCCL_ROOT=${CCCL_ROOT}\")\\nmessage(STATUS \"COMPONENTS=${COMPONENTS}\")\\nmessage(STATUS \"PACKAGE_TYPE=${PACKAGE_TYPE}\")\\n\\nfunction(do_find_package pkg_name pkg_prefix)\\n  list(APPEND arg_list\\n    REQUIRED\\n    ${ARGN}\\n    NO_DEFAULT_PATH\\n    HINTS \"${pkg_prefix}\"\\n  )\\n  list(JOIN arg_list \" \" arg_str)\\n  message(STATUS \"Executing: find_package(${pkg_name} ${arg_str})\")\\n  find_package(${pkg_name} ${arg_list})\\n  if (NOT ${pkg_name}_FOUND)\\n    message(FATAL_ERROR \"Failed: find_package(${pkg_name} ${arg_str})\")\\n  endif()\\n  # Re-execute find_package to ensure that repeated calls don't break:\\n  find_package(${pkg_name} ${arg_list})\\nendfunction()\\n\\n# Run find package with the requested configuration:\\nif (PACKAGE_TYPE STREQUAL \"CCCL\")\\n  if (COMPONENTS STREQUAL \"DEFAULT\")\\n    do_find_package(CCCL \"${CCCL_ROOT}\")\\n  else()\\n    do_find_package(CCCL \"${CCCL_ROOT}\" COMPONENTS ${COMPONENTS})\\n  endif()\\nelif(PACKAGE_TYPE STREQUAL \"NATIVE\")\\n  if (COMPONENTS STREQUAL \"DEFAULT\")\\n    message(FATAL_ERROR \"COMPONENTS=DEFAULT incompatible with PACKAGE_TYPE=NATIVE\")\\n  endif()\\n  foreach (component IN LISTS COMPONENTS)\\n    do_find_package(${component} \"${CCCL_ROOT}\")\\n  endforeach()\\nelif(PACKAGE_TYPE STREQUAL \"SUBDIR\")\\n  if (COMPONENTS STREQUAL \"DEFAULT\")\\n    set(CCCL_REQUIRED_COMPONENTS)\\n  else()\\n    set(CCCL_REQUIRED_COMPONENTS ${COMPONENTS})\\n  endif()\\n  add_subdirectory(\"${CCCL_ROOT}\" \"${CMAKE_CURRENT_BINARY_DIR}/subdir\")\\nelse()\\n  message(FATAL_ERROR \"Invalid PACKAGE_TYPE: ${PACKAGE_TYPE}\")\\nendif()\\n\\nif (COMPONENTS STREQUAL \"DEFAULT\")\\n  set(COMPONENTS libcudacxx CUB Thrust)\\n  if (CCCL_ENABLE_UNSTABLE)\\n    list(APPEND COMPONENTS cudax)\\n  endif()\\nendif()\\n\\nforeach (component IN LISTS COMPONENTS)\\n  set(test_target version_check.${component})\\n  set(component_target \"${component}::${component}\")\\n  add_executable(${test_target} version_check.cxx)\\n  target_compile_features(${test_target} PUBLIC cxx_std_17)\\n  target_link_libraries(${test_target} PRIVATE ${component_target})\\n  add_test(NAME ${test_target} COMMAND ${test_target})\\n\\n  if (component STREQUAL \"libcudacxx\")\\n    math(EXPR component_cmake_version\\n      \"(${LIBCUDACXX_VERSION_MAJOR} * 1000000) +\\n        ${LIBCUDACXX_VERSION_MINOR} * 1000 +\\n        ${LIBCUDACXX_VERSION_PATCH}\")\\n    target_compile_definitions(${test_target} PRIVATE\\n      \"VERSION_HEADER=cuda/std/version\"\\n      \"VERSION_MACRO=_LIBCUDACXX_CUDA_API_VERSION\"\\n      \"EXPECTED_VERSION=${component_cmake_version}\")\\n  elif (component STREQUAL \"CUB\")\\n    math(EXPR component_cmake_version\\n      \"(${CUB_VERSION_MAJOR} * 100000) +\\n        ${CUB_VERSION_MINOR} * 100 +\\n        ${CUB_VERSION_PATCH}\")\\n    target_compile_definitions(${test_target} PRIVATE\\n      \"VERSION_HEADER=cub/version.cuh\"\\n      \"VERSION_MACRO=CUB_VERSION\"\\n      \"EXPECTED_VERSION=${component_cmake_version}\")\\n  elif (component STREQUAL \"Thrust\")\\n    math(EXPR component_cmake_version\\n      \"(${THRUST_VERSION_MAJOR} * 100000) +\\n        ${THRUST_VERSION_MINOR} * 100 +\\n        ${THRUST_VERSION_PATCH}\")\\n    target_compile_definitions(${test_target} PRIVATE\\n      \"VERSION_HEADER=thrust/version.h\"\\n      \"VERSION_MACRO=THRUST_VERSION\"\\n      \"EXPECTED_VERSION=${component_cmake_version}\")\\n    elseif (component STREQUAL \"cudax\")\\n      math(EXPR component_cmake_version\\n      \"(${CUDAX_VERSION_MAJOR} * 1000000) +\\n        ${CUDAX_VERSION_MINOR} * 1000 +\\n        ${CUDAX_VERSION_PATCH}\")\\n      target_compile_definitions(${test_target} PRIVATE\\n        \"VERSION_HEADER=cuda/experimental/version.cuh\"\\n        \"VERSION_MACRO=CUDAX_VERSION\"\\n        \"EXPECTED_VERSION=${component_cmake_version}\")\\n  else()\\n    message(FATAL_ERROR \"Valid COMPONENTS are (case-sensitive): Thrust;CUB;libcudacxx;cudax\")\\n  endif()\\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Implementing Restrict Type Traits in C++\nDESCRIPTION: Type traits for checking if a type is a restrict accessor or restrict mdspan, returning boolean constants.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/mdspan/restrict_accessor.rst#2025-04-23_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\ntemplate <typename T>\ninline constexpr bool is_restrict_accessor_v = /*true if T is a restrict accessor, false otherwise*/;\n\ntemplate <typename T>\ninline constexpr bool is_restrict_mdspan_v = /*true if T is a restrict mdspan, false otherwise*/;\n```\n\n----------------------------------------\n\nTITLE: 1D CTA Shared to Global Memory Bulk Tensor Copy\nDESCRIPTION: Device function for copying 1D tensor data from global to CTA shared memory with barrier synchronization. Requires SM_90 architecture and PTX ISA 86.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_async_bulk_tensor.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline void cp_async_bulk_tensor(\n  cuda::ptx::space_shared_t,\n  cuda::ptx::space_global_t,\n  void* dstMem,\n  const void* tensorMap,\n  const int32_t (&tensorCoords)[1],\n  uint64_t* smem_bar);\n```\n\n----------------------------------------\n\nTITLE: Implementing Multimem Load-Reduce for 64-bit Integer Max Operation with Weak Semantics in CUDA\nDESCRIPTION: This template function performs a multimem load-reduce operation for 64-bit integers using the max operation with weak semantics. It provides a specialized version for weak memory ordering without explicit scope specification.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_27\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.global.op.s64 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .weak }\n// .op        = { .max }\ntemplate <typename = void>\n__device__ static inline int64_t multimem_ld_reduce(\n  cuda::ptx::sem_weak_t,\n  cuda::ptx::op_max_t,\n  const int64_t* addr);\n```\n\n----------------------------------------\n\nTITLE: Defining tcgen05_mma_ws_tmem_a_collector_b3_fill with Zero Column Mask for CUDA PTX\nDESCRIPTION: Template function declaration for filling b3 collector with memory-backed A matrix and zero column mask. Supports various data types and provides matrix operation configurations with additional zero column masking capability.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_62\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.ws.cta_group.kind.collector::b3::fill [d_tmem], [a_tmem], b_desc, idesc, enable_input_d, zero_column_mask_desc; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1 }\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_tmem_a_collector_b3_fill(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint32_t a_tmem,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d,\n  uint64_t zero_column_mask_desc);\n```\n\n----------------------------------------\n\nTITLE: MMA Block Scale Vector 4X Collector Fill with mxf4nvf4 Type (CTA Group 1)\nDESCRIPTION: Template function for matrix multiplication with block scaling using vector 4X with collector A fill operation for CTA group 1. The function uses mxf4nvf4 data type and requires PTX ISA 86 with SM_100a or SM_101a architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma.rst#2025-04-23_snippet_22\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.cta_group.kind.block_scale.scale_vec::4X.collector::a::fill [d_tmem], a_desc, b_desc, idesc, [scale_A_tmem], [scale_B_tmem], enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .kind      = { .kind::mxf4nvf4 }\n// .cta_group = { .cta_group::1, .cta_group::2 }\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_mma_block_scale_vec_4x_collector_a_fill(\n  cuda::ptx::kind_mxf4nvf4_t,\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  uint32_t scale_A_tmem,\n  uint32_t scale_B_tmem,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Constructing cuda::annotated_ptr with Runtime Property in CUDA\nDESCRIPTION: Constructor for cuda::annotated_ptr that takes a pointer and a runtime property. This constructor is only available when Property is cuda::access_property and allows for dynamic property assignment.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/memory_access_properties/annotated_ptr.rst#2025-04-23_snippet_3\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <class RuntimeProperty>\nannotated_ptr(pointer ptr, RuntimeProperty prop);\n```\n\n----------------------------------------\n\nTITLE: Combining Typed and Untyped Tasks\nDESCRIPTION: Shows how to combine low-level API with ->* notation and dynamically add dependencies to a typed task.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_46\n\nLANGUAGE: cpp\nCODE:\n```\ndouble X[16], Y[16];\nauto lX = ctx.logical_data(X);\nauto lY = ctx.logical_data(Y);\n\nauto t = ctx.task(lX.read());\nt.add_deps(lY.rw());\nt->*[&](cudaStream_t s, auto x) {\n   slice<double> y = t.template get<slice<double>>(1);\n};\n```\n\n----------------------------------------\n\nTITLE: Defining Test Restriction Mechanism in CMake\nDESCRIPTION: Initializes an empty list `restricted_tests` and defines a CMake macro `thrust_declare_test_restrictions`. This macro takes a test name and a variable number of host.device configuration strings (e.g., `CPP.CUDA`, `CPP.OMP`). It adds the test name to the `restricted_tests` list and stores the allowed configurations in a list named `<test_name>_host.device_allowed`. This allows later logic to check if a test should be built for a specific configuration.\nSOURCE: https://github.com/nvidia/cccl/blob/main/thrust/testing/CMakeLists.txt#2025-04-23_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\n# Some tests only support certain host.device configurations. Use this macro to\n# declare allowed configurations. If not specified, all host.device config are\n# used.\nset(restricted_tests)\nmacro(thrust_declare_test_restrictions test_name)\n  list(APPEND restricted_tests ${test_name})\n  list(APPEND ${test_name}_host.device_allowed ${ARGN})\nendmacro()\n```\n\n----------------------------------------\n\nTITLE: Implementing Private Storage Allocation for Block-Level Algorithms in CUDA C++\nDESCRIPTION: This function implements the allocation of shared memory for block-level algorithms when using the default constructor.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/developer_overview.rst#2025-04-23_snippet_9\n\nLANGUAGE: C++\nCODE:\n```\n__device__ __forceinline__ _TempStorage& PrivateStorage()\n{\n  __shared__ _TempStorage private_storage;\n  return private_storage;\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Fence Proxy Async Release for Shared CTA Space in CUDA\nDESCRIPTION: Defines a device function template for fence proxy async operation with release semantics, restricted to shared CTA space and cluster scope. This function is based on PTX ISA 86 and targets SM_90.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/fence_proxy_async_generic_sync_restrict.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n// fence.proxy.async::generic.sem.sync_restrict::space.scope; // PTX ISA 86, SM_90\n// .sem       = { .release }\n// .space     = { .shared::cta }\n// .scope     = { .cluster }\ntemplate <typename = void>\n__device__ static inline void fence_proxy_async_generic_sync_restrict(\n  cuda::ptx::sem_release_t,\n  cuda::ptx::space_shared_t,\n  cuda::ptx::scope_cluster_t);\n```\n\n----------------------------------------\n\nTITLE: Implementing Fence Proxy Async Acquire for Shared Cluster Space in CUDA\nDESCRIPTION: Defines a device function template for fence proxy async operation with acquire semantics, restricted to shared cluster space and cluster scope. This function is based on PTX ISA 86 and targets SM_90.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/fence_proxy_async_generic_sync_restrict.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\n// fence.proxy.async::generic.sem.sync_restrict::space.scope; // PTX ISA 86, SM_90\n// .sem       = { .acquire }\n// .space     = { .shared::cluster }\n// .scope     = { .cluster }\ntemplate <typename = void>\n__device__ static inline void fence_proxy_async_generic_sync_restrict(\n  cuda::ptx::sem_acquire_t,\n  cuda::ptx::space_cluster_t,\n  cuda::ptx::scope_cluster_t);\n```\n\n----------------------------------------\n\nTITLE: Configuring CUDA Targets and Adding Examples in CMake\nDESCRIPTION: This CMake script automates scanning for example source files (with .cu and .cpp extensions) in the current directory, filters through a list of Thrust build targets to identify ones using CUDA, and iteratively adds these as CUDA example executables. It depends on the Thrust CMake utility functions and requires predefined variables such as THRUST_TARGETS. Parameter example_srcs contains matched source file paths, while thrust_target is the build target under consideration. Each example's name is prefixed with 'cuda.' and registered using thrust_add_example. Outputs are example executable targets registered with the CMake project for CUDA-enabled environments. This approach relies on correct integration with Thrust CMake helpers and paths.\nSOURCE: https://github.com/nvidia/cccl/blob/main/thrust/examples/cuda/CMakeLists.txt#2025-04-23_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB example_srcs\n  RELATIVE \"${CMAKE_CURRENT_LIST_DIR}}\"\n  CONFIGURE_DEPENDS\n  *.cu *.cpp\n)\n\nforeach(thrust_target IN LISTS THRUST_TARGETS)\n  thrust_get_target_property(config_device ${thrust_target} DEVICE)\n  if (NOT config_device STREQUAL \"CUDA\")\n    continue()\n  endif()\n\n  foreach(example_src IN LISTS example_srcs)\n    get_filename_component(example_name \"${example_src}\" NAME_WLE)\n    string(PREPEND example_name \"cuda.\")\n    thrust_add_example(example_target ${example_name} \"${example_src}\" ${thrust_target})\n  endforeach()\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Async Store with Barrier Completion - 64-bit Single Value\nDESCRIPTION: Template for asynchronous weak shared memory store operation with cluster barrier completion tracking for 64-bit values. Compatible with PTX ISA 81 and SM_90 architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/st_async.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename Type>\n__device__ static inline void st_async(\n  Type* addr,\n  const Type& value,\n  uint64_t* remote_bar);\n```\n\n----------------------------------------\n\nTITLE: Backporting C++17/20 <cuda/std/chrono> Features to C++14 (C++)\nDESCRIPTION: Feature #34 added in libcu++ 1.4.0. C++17 and C++20 features from `<chrono>` are backported to C++14 within the `<cuda/std/chrono>` header. Contribution by Jake Hemstad and Paul Taylor.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/releases/changelog.rst#2025-04-23_snippet_12\n\nLANGUAGE: cpp\nCODE:\n```\n<cuda/std/chrono>\n```\n\n----------------------------------------\n\nTITLE: TCGen05 MMA Workspace F8F6F4 Collector Implementation\nDESCRIPTION: Template device function for TCGen05 MMA workspace operations using F8F6F4 data type. Handles memory operations with CTA group 1 configuration and collector type B3 with discard functionality.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_77\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.ws.cta_group.kind.collector::b3::discard [d_tmem], [a_tmem], b_desc, idesc, enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1 }\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_tmem_a_collector_b3_discard(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint32_t a_tmem,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Declaring tcgen05_mma_tmem_a with Disable Output Lane for CTA Group 2\nDESCRIPTION: Template function declaration for tcgen05 matrix multiply-accumulate operation with CTA group 2 configuration, supporting various data types (f16, tf32, f8f6f4, i8). This variant includes disable_output_lane parameter as an array of 8 uint32_t values.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma.rst#2025-04-23_snippet_10\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.cta_group.kind [d_tmem], [a_tmem], b_desc, idesc, disable_output_lane, enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\n// .cta_group = { .cta_group::2 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_tmem_a(\n  cuda::ptx::kind_t<Kind> kind,\n  cuda::ptx::cta_group_2_t,\n  uint32_t d_tmem,\n  uint32_t a_tmem,\n  uint64_t b_desc,\n  uint32_t idesc,\n  const uint32_t (&disable_output_lane)[8],\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Implementing warp_shuffle_idx in CUDA\nDESCRIPTION: Function templates for warp_shuffle_idx operation with optional Width and lane_mask parameters. Returns a warp_shuffle_result containing the shuffled data and a predicate.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/warp/warp_shuffle.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <int Width = 32, typename T>\n[[nodiscard]] __device__ warp_shuffle_result<T>\nwarp_shuffle_idx(const T& data,\n                 int      src_lane,\n                 uint32_t lane_mask = 0xFFFFFFFF,\n                 cuda::std::integral_constant<int, Width> = {})\n\ntemplate <int Width = 32, typename T>\n[[nodiscard]] __device__ warp_shuffle_result<T>\nwarp_shuffle_idx(const T& data,\n                 int      src_lane,\n                 cuda::std::integral_constant<int, Width>) // lane_mask is 0xFFFFFFFF\n```\n\n----------------------------------------\n\nTITLE: Analyzing Multiple CUB Benchmark Databases in Bash\nDESCRIPTION: Shows how to analyze results from multiple benchmark databases using glob expressions and how to filter results using regular expressions.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/tuning.rst#2025-04-23_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n$ ../benchmarks/scripts/analyze.py --top=5 <path-to-databases>/*.db\n```\n\nLANGUAGE: bash\nCODE:\n```\n$ ../benchmarks/scripts/analyze.py -R=\".*radix_sort.keys.*\"  --top=5 <path-to-databases>/*.db\n```\n\n----------------------------------------\n\nTITLE: Creating CCCL JIT Template Object Library\nDESCRIPTION: Adds a CMake object library target that compiles the generated JIT template source file for use in the CCCL project.\nSOURCE: https://github.com/nvidia/cccl/blob/main/c/parallel/src/jit_templates/CMakeLists.txt#2025-04-23_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(cccl.c.parallel.jit_template OBJECT \"${jit_template_src}\")\n```\n\n----------------------------------------\n\nTITLE: Collecting Internal Headers with GLOB_RECURSE\nDESCRIPTION: Uses file(GLOB_RECURSE) to find all internal headers in the libcudacxx project. Targets headers in __* directories, relative to the cuda/ include path.\nSOURCE: https://github.com/nvidia/cccl/blob/main/libcudacxx/test/internal_headers/CMakeLists.txt#2025-04-23_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB_RECURSE internal_headers\n  RELATIVE \"${libcudacxx_SOURCE_DIR}/include/cuda/\"\n  CONFIGURE_DEPENDS\n  ${libcudacxx_SOURCE_DIR}/include/cuda/__*/*.h\n  ${libcudacxx_SOURCE_DIR}/include/cuda/std/__*/*.h\n)\n```\n\n----------------------------------------\n\nTITLE: Bulk Asynchronous 5D Tensor Reductions Using CUDA PTX (C++/CUDA)\nDESCRIPTION: Defines a CUDA device inline function template for asynchronous bulk reduction across 5D tensors, copying data between shared (CTA) and global memory spaces and supporting operations such as add, min, max, inc, dec, and, or, and xor specified through a PTX operator. This signature is nearly identical to the 4D case but expects a 5-element coordinate array. Dependencies include CUDA PTX extensions and correct use of tensor and memory pointer types. Inputs are the tensor map, a 5D coordinate, the selected PTX op, and the source memory buffer; output is performed to global memory. Only applicable to 5D tensor structures and specified PTX operations.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_reduce_async_bulk_tensor.rst#2025-04-23_snippet_6\n\nLANGUAGE: cuda\nCODE:\n```\n// cp.reduce.async.bulk.tensor.5d.dst.src.op.tile.bulk_group [tensorMap, tensorCoords], [srcMem]; // 1e. PTX ISA 80, SM_90\n// .dst       = { .global }\n// .src       = { .shared::cta }\n// .op        = { .add, .min, .max, .inc, .dec, .and, .or, .xor }\ntemplate <cuda::ptx::dot_op Op>\n__device__ static inline void cp_reduce_async_bulk_tensor(\n  cuda::ptx::space_global_t,\n  cuda::ptx::space_shared_t,\n  cuda::ptx::op_t<Op> op,\n  const void* tensorMap,\n  const int32_t (&tensorCoords)[5],\n  const void* srcMem);\n```\n\n----------------------------------------\n\nTITLE: Storing and Unpacking 32x32b Data with tcgen05 in CUDA (8 values)\nDESCRIPTION: This function performs a 32x32b store operation with 16-bit unpacking using tcgen05. It takes a 32-bit address and an array of 8 B32 values as input. The function is constrained to B32 types of 4 bytes.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_st.rst#2025-04-23_snippet_24\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.st.sync.aligned.32x32b.x8.unpack::16b.b32 [taddr], values; // PTX ISA 86, SM_100a, SM_101a\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline void tcgen05_st_32x32b_unpack_16b(\n  uint32_t taddr,\n  const B32 (&values)[8]);\n```\n\n----------------------------------------\n\nTITLE: PRMT B32 Rotate by 8 Template Function\nDESCRIPTION: Template function declaration for 32-bit rotate by 8 bits permute operation (rc8 mode). Performs byte-wise rotation on input registers.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/prmt.rst#2025-04-23_snippet_3\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline uint32_t prmt_rc8(\n  B32 a_reg,\n  B32 b_reg,\n  uint32_t c_reg);\n```\n\n----------------------------------------\n\nTITLE: Declaring multimem_ld_reduce with Relaxed/GPU Scope and XOR Operation in CUDA PTX\nDESCRIPTION: Template function declaration for multimem.ld_reduce with relaxed memory semantics, GPU scope, and XOR operation on 64-bit values in global memory. For PTX ISA 8.1 running on SM_90 architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_72\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.b64 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .xor }\ntemplate <typename B64, enable_if_t<sizeof(B64) == 8, bool> = true, cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline B64 multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_xor_op_t,\n  const B64* addr);\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Bulk INC Reduction (Unsigned 32-bit) in CUDA\nDESCRIPTION: Performs an asynchronous bulk INC (increment) reduction operation from CTA-shared to cluster-shared memory for unsigned 32-bit integers. Uses a memory barrier for synchronization.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_reduce_async_bulk.rst#2025-04-23_snippet_6\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline void cp_reduce_async_bulk(\n  cuda::ptx::space_cluster_t,\n  cuda::ptx::space_shared_t,\n  cuda::ptx::op_inc_t,\n  uint32_t* dstMem,\n  const uint32_t* srcMem,\n  uint32_t size,\n  uint64_t* rdsmem_bar);\n```\n\n----------------------------------------\n\nTITLE: Retrieving First CTA ID Z-coordinate in CUDA\nDESCRIPTION: Defines a device function to get the Z-coordinate of the first CTA ID from a cancellation response. It takes a 128-bit response and returns a 32-bit value.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/clusterlaunchcontrol.rst#2025-04-23_snippet_5\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true, typename B128, enable_if_t<sizeof(B128) == 16, bool> = true>\n__device__ static inline B32 clusterlaunchcontrol_query_cancel_get_first_ctaid_z(\n  B128 try_cancel_response);\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Undefined Behavior in CUDA Access Property Usage\nDESCRIPTION: This CUDA kernel shows examples of undefined behavior when using access properties. It includes mismatching address spaces, invalid property associations, and incorrect probability values for interleaved properties.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/memory_access_properties/access_property.rst#2025-04-23_snippet_3\n\nLANGUAGE: cuda\nCODE:\n```\n#include <cuda/annotated_ptr>\n\n__global__ void undefined_behavior(int* global) {\n    // Associating pointers with mismatching address spaces is undefined:\n    cuda::associate_access_property(global, cuda::access_property::shared{});     // undefined behavior\n    __shared__ int shmem;\n    cuda::associate_access_property(&shmem, cuda::access_property::normal{});     // undefined behavior\n    cuda::associate_access_property(&shmem, cuda::access_property::streaming{});  // undefined behavior\n    cuda::associate_access_property(&shmem, cuda::access_property::persisting{}); // undefined behavior\n\n    cuda::access_property interleaved_implicit_global(cuda::access_property::streaming{}, 0.5);\n    cuda::associate_access_property(&shmem, interleaved_implicit_global);         // undefined behavior\n\n    cuda::access_property range_implicit_global0(&shmem, 0, sizeof(int), cuda::access_property::streaming{});\n    cuda::associate_access_property(&shmem, range_implicit_global0);              // undefined behavior\n\n    // Using a zero probability or probability out-of-range (0, 1] is undefined:\n    cuda::access_property interleaved(cuda::access_property::streaming{}, 0.0);   // undefined behavior\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing multimem.ld_reduce for acquire CTA global add operations on 32-bit integers\nDESCRIPTION: Template implementation for multimem atomic load-and-reduce operations with acquire semantics, CTA scope, global memory, and add operation on 32-bit integers. Compatible with PTX ISA 81 on SM_90 architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_36\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.s32 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .add }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline int32_t multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_add_t,\n  const int32_t* addr);\n```\n\n----------------------------------------\n\nTITLE: MMA Workstream TMEM Collector Implementation\nDESCRIPTION: Template implementation for matrix multiplication workstream collector using TMEM for matrix A. Includes support for zero column masking and multiple data types.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_66\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_tmem_a_collector_b3_use(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint32_t a_tmem,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d,\n  uint64_t zero_column_mask_desc);\n```\n\n----------------------------------------\n\nTITLE: Linux Cross-Compilation Commands\nDESCRIPTION: Make commands demonstrating cross-compilation for different architectures including x86_64, ppc64le, and armv7l.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cudax/test/stf/cuda-samples/3_CUDA_Features/jacobiCudaGraphs/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ make TARGET_ARCH=x86_64\n$ make TARGET_ARCH=ppc64le\n$ make TARGET_ARCH=armv7l\n```\n\n----------------------------------------\n\nTITLE: Detecting Launch ID Variants\nDESCRIPTION: Helper function that checks if a test label contains launch variant specifications (lid_) and sets the output variable accordingly. This is used to determine if special configuration is needed for different CUDA launch methods.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cub/test/CMakeLists.txt#2025-04-23_snippet_9\n\nLANGUAGE: CMake\nCODE:\n```\n# Sets out_var to launch id if the label contains launch variants\nfunction(_cub_has_lid_variant out_var label)\n  string(FIND \"${label}\" \"lid_\" idx)\n  if (idx EQUAL -1)\n    set(${out_var} 0 PARENT_SCOPE)\n  else()\n    set(${out_var} 1 PARENT_SCOPE)\n  endif()\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: CCCL Build Options Configuration\nDESCRIPTION: Defines build options for various CCCL components including libcu++, CUB, Thrust, and testing capabilities. Handles conditional enabling of benchmarks based on compiler type.\nSOURCE: https://github.com/nvidia/cccl/blob/main/CMakeLists.txt#2025-04-23_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\noption(CCCL_ENABLE_LIBCUDACXX \"Enable the libcu++ developer build.\" ${CCCL_TOPLEVEL_PROJECT})\noption(CCCL_ENABLE_CUB \"Enable the CUB developer build.\" ${CCCL_TOPLEVEL_PROJECT})\noption(CCCL_ENABLE_THRUST \"Enable the Thrust developer build.\" ${CCCL_TOPLEVEL_PROJECT})\noption(CCCL_ENABLE_TESTING \"Enable CUDA C++ Core Library tests.\" ${CCCL_TOPLEVEL_PROJECT})\noption(CCCL_ENABLE_EXAMPLES \"Enable CUDA C++ Core Library examples.\" ${CCCL_TOPLEVEL_PROJECT})\noption(CCCL_ENABLE_C \"Enable CUDA C Core Library.\" OFF)\n\nif (\"NVHPC\" STREQUAL \"${CMAKE_CXX_COMPILER_ID}\")\n  set(CCCL_ENABLE_BENCHMARKS OFF)\nelse()\n  option(CCCL_ENABLE_BENCHMARKS \"Enable CUDA C++ Core Library benchmarks.\" OFF)\nendif()\n```\n\n----------------------------------------\n\nTITLE: 16-bit Unsigned Right Shift Operation in CUDA\nDESCRIPTION: Template function for performing unsigned right shift on 16-bit values. Takes a generic B16 type parameter constrained to 2 bytes and shift amount as parameters.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/shr.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\n// shr.b16 dest, a_reg, b_reg; // PTX ISA 10, SM_50\ntemplate <typename B16, enable_if_t<sizeof(B16) == 2, bool> = true>\n__device__ static inline B16 shr(\n  B16 a_reg,\n  uint32_t b_reg);\n```\n\n----------------------------------------\n\nTITLE: Defining tcgen05_mma_ws_tmem_a_collector_b0_fill Template in CUDA\nDESCRIPTION: Defines a templated CUDA `__device__` static inline function `tcgen05_mma_ws_tmem_a_collector_b0_fill`. This function is intended for collector `b0` fill operations within `cta_group::1`, supporting various `dot_kind` types (f16, tf32, f8f6f4, i8). It takes pointers/offsets to destination (`d_tmem`) and source (`a_tmem`) temporary memory, descriptors (`b_desc`, `idesc`), and an enable flag (`enable_input_d`). Designed for PTX ISA 86, SM_100a, SM_101a.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_4\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.ws.cta_group.kind.collector::b0::fill [d_tmem], [a_tmem], b_desc, idesc, enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1 }\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_tmem_a_collector_b0_fill(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint32_t a_tmem,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Setting Up Build Options and Installation Prefix in CMake - CMake Language\nDESCRIPTION: Initializes the build option variables and a temporary install prefix needed for subsequent test registration and fixture setup. The snippet uses `set()` commands to define lists and variables that propagate build types, make program, compiler settings, and the local test install path. Prerequisite: the containing project is already loaded in CMake and variables like `CMAKE_BUILD_TYPE` or `CMAKE_MAKE_PROGRAM` are already defined. There is no required input; output is the relevant CMake variables being set in the scripting environment.\nSOURCE: https://github.com/nvidia/cccl/blob/main/test/cmake/CMakeLists.txt#2025-04-23_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(cmake_opts\n  -D \"CMAKE_BUILD_TYPE=${CMAKE_BUILD_TYPE}\"\n  -D \"CMAKE_MAKE_PROGRAM=${CMAKE_MAKE_PROGRAM}\"\n  -D \"CMAKE_CXX_COMPILER=${CMAKE_CXX_COMPILER}\"\n)\n\n# Temporary installation prefix for tests against installed project:\nset(tmp_install_prefix \"${CMAKE_CURRENT_BINARY_DIR}/test_install\")\n```\n\n----------------------------------------\n\nTITLE: Adding <cuda/std/complex> Support (C++)\nDESCRIPTION: Feature #32 added in libcu++ 1.4.0. Introduces the `cuda::std::complex` number type via the `<cuda/std/complex>` header. Note that `long double` is not supported and is disabled when building with NVCC.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/releases/changelog.rst#2025-04-23_snippet_11\n\nLANGUAGE: cpp\nCODE:\n```\n<cuda/std/complex>\n```\n\nLANGUAGE: cpp\nCODE:\n```\ncuda::std::complex\n```\n\n----------------------------------------\n\nTITLE: Declaring tcgen05_mma_ws_collector_b3_fill Without Zero Column Mask\nDESCRIPTION: Template function for MMA workload shaping with b3 collector fill mode. This simpler variant omits the zero column masking functionality for more straightforward matrix operations.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_60\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.ws.cta_group.kind.collector::b3::fill [d_tmem], a_desc, b_desc, idesc, enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1 }\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_collector_b3_fill(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Registering and Testing Example Executables with thrust_add_example - CMake\nDESCRIPTION: This function, thrust_add_example, automates the creation, registration, and test integration of Thrust example executables for a specified backend configuration. It determines wrapper handling for CUDA/non-CUDA, sets up correct build and include properties, registers the example as a build and test target, and integrates FileCheck-based output checking if enabled. Prerequisites include accessible source files, a reference thrust_target, CMake test infrastructure, and supplementary CMake utility macros. Typical inputs are variable names, example info and backend targets; outputs are fully registered executable and test targets per example.\nSOURCE: https://github.com/nvidia/cccl/blob/main/thrust/examples/CMakeLists.txt#2025-04-23_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\n## thrust_add_example\n#\n# Add an example executable and register it with ctest.\n#\n# target_name_var: Variable name to overwrite with the name of the example\n#   target. Useful for post-processing target information per-backend.\n# example_name: The name of the example minus \"<config_prefix>.example.\" For\n#   instance, examples/vector.cu will be \"vector\", and examples/cuda/copy.cu\n#   would be \"cuda.copy\".\n# example_src: The source file that implements the example.\n# thrust_target: The reference thrust target with configuration information.\n#\nfunction(thrust_add_example target_name_var example_name example_src thrust_target)\n  thrust_get_target_property(config_host ${thrust_target} HOST)\n  thrust_get_target_property(config_device ${thrust_target} DEVICE)\n  thrust_get_target_property(config_prefix ${thrust_target} PREFIX)\n\n  # Wrap the .cu file in .cpp for non-CUDA backends\n  if (\"CUDA\" STREQUAL \"${config_device}\")\n    set(real_example_src \"${example_src}\")\n  else()\n    thrust_wrap_cu_in_cpp(real_example_src \"${example_src}\" ${thrust_target})\n  endif()\n\n  # The actual name of the test's target:\n  set(example_target ${config_prefix}.example.${example_name})\n  set(${target_name_var} ${example_target} PARENT_SCOPE)\n\n  # Related target names:\n  set(config_meta_target ${config_prefix}.examples)\n  set(example_meta_target thrust.all.example.${example_name})\n\n  add_executable(${example_target} \"${real_example_src}\")\n  target_link_libraries(${example_target} ${thrust_target})\n  target_include_directories(${example_target} PRIVATE \"${Thrust_SOURCE_DIR}/examples\")\n  thrust_clone_target_properties(${example_target} ${thrust_target})\n  thrust_fix_clang_nvcc_build_for(${example_target})\n\n  if (\"CUDA\" STREQUAL \"${config_device}\")\n    thrust_configure_cuda_target(${example_target} RDC ${THRUST_FORCE_RDC})\n  endif()\n\n  # Add to the active configuration's meta target\n  add_dependencies(${config_meta_target} ${example_target})\n\n  # Meta target that builds examples with this name for all configurations:\n  if (NOT TARGET ${example_meta_target})\n    add_custom_target(${example_meta_target})\n  endif()\n  add_dependencies(${example_meta_target} ${example_target})\n\n  if (NOT \"Clang\" STREQUAL \"${CMAKE_CUDA_COMPILER_ID}\")\n    target_compile_definitions(${example_target} PRIVATE THRUST_EXAMPLE_DEVICE_SIDE)\n  endif()\n\n  # Get the name of FileCheck input by stripping out the config name.\n  # (e.g. \"thrust.cpp.cuda.cpp14.example.xxx\" -> \"thrust.example.xxx.filecheck\")\n  string(REPLACE \"${config_prefix}\" \"thrust\"\n    filecheck_reference_file\n    \"${example_target}.filecheck\"\n  )\n\n  add_test(NAME ${example_target}\n    COMMAND \"${CMAKE_COMMAND}\"\n    \"-DEXAMPLE_EXECUTABLE=$<TARGET_FILE:${example_target}>\"\n    \"-DFILECHECK_ENABLED=${THRUST_ENABLE_EXAMPLE_FILECHECK}\"\n    \"-DFILECHECK_EXECUTABLE=${THRUST_FILECHECK_EXECUTABLE}\"\n    \"-DREFERENCE_FILE=${filecheck_data_path}/${filecheck_reference_file}\"\n    -P \"${Thrust_SOURCE_DIR}/cmake/ThrustRunExample.cmake\"\n  )\n\n  # Run OMP/TBB tests in serial. Multiple OMP processes will massively\n  # oversubscribe the machine with GCC's OMP, and we want to test these with\n  # the full CPU available to each unit test.\n  set(config_systems ${config_host} ${config_device})\n  if ((\"OMP\" IN_LIST config_systems) OR (\"TBB\" IN_LIST config_systems))\n    set_tests_properties(${example_target} PROPERTIES RUN_SERIAL ON)\n  endif()\nendfunction()\n\n```\n\n----------------------------------------\n\nTITLE: Building CUB Benchmarks with CMake\nDESCRIPTION: Commands to clone the CCCL repository and configure CMake build for CUB benchmarks. Sets up the build environment with proper GPU architecture settings.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/benchmarking.rst#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/NVIDIA/cccl.git\ncd cccl\nmkdir build\ncd build\ncmake .. --preset=cub-benchmark -DCMAKE_CUDA_ARCHITECTURES=90\n```\n\n----------------------------------------\n\nTITLE: Profiling Benchmarks with Nsight Compute (Bash)\nDESCRIPTION: Demonstrates how to use the ncu command to profile kernels, collect all metrics, import source code, and generate a profiling report.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/benchmarking.rst#2025-04-23_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nncu --set full --import-source yes -o base.ncu-rep -f ./bin/thrust.bench.transform.basic.base -d 0 --profile\n```\n\n----------------------------------------\n\nTITLE: Configuring Output Paths for Generated Atomics Code\nDESCRIPTION: Sets variables for the output path of the generated atomic operations code and the installation location within the source tree. These paths are used by the subsequent build targets.\nSOURCE: https://github.com/nvidia/cccl/blob/main/libcudacxx/codegen/CMakeLists.txt#2025-04-23_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(atomic_generated_output \"${libcudacxx_BINARY_DIR}/codegen/cuda_ptx_generated.h\")\nset(atomic_install_location \"${libcudacxx_SOURCE_DIR}/include/cuda/std/__atomic/functions\")\n```\n\n----------------------------------------\n\nTITLE: ReStructuredText Documentation for PTX ld Instruction\nDESCRIPTION: ReStructuredText markup defining a documentation page for the PTX 'ld' (load) instruction, including a reference link to the official NVIDIA PTX ISA documentation\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/ld.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _libcudacxx-ptx-instructions-ld:\n\nld\n==\n\n-  PTX ISA:\n   `ld <https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-ld>`__\n\n.. include:: generated/ld.rst\n```\n\n----------------------------------------\n\nTITLE: Executing Tensor Core Collector Use Operation with TMEM in CUDA\nDESCRIPTION: This template function executes a tensor core collector use operation with TMEM for various data types. It includes parameters for CTA group, data kind, memory addresses, and descriptors.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_38\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_tmem_a_collector_b2_use(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint32_t a_tmem,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d,\n  uint64_t zero_column_mask_desc);\n```\n\n----------------------------------------\n\nTITLE: Launch Construct Syntax\nDESCRIPTION: The generic syntax for the launch construct, which allows execution of structured compute kernels over a thread hierarchy. Unlike parallel_for, launch enables more complex thread organizations for specialized kernel execution patterns.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_64\n\nLANGUAGE: cpp\nCODE:\n```\ntemplate <typename thread_hierarchy_spec_t>\nctx.launch([thread hierarchy spec], [execution place], logicalData1.accessMode(), logicalData2.accessMode(), ...)\n    ->*[capture list] __device__ (thread_hierarchy_spec_t th, auto data1, auto data2 ...) {\n        // Kernel implementation\n    };\n```\n\n----------------------------------------\n\nTITLE: Declaring MMA Block Scale Vector 4X Collector for MXF4NVF4 Kind\nDESCRIPTION: Device function template for matrix multiply with 4X vector scaling specifically for MXF4NVF4 data type. Supports different CTA groups with matrix descriptors and scaling options, targeting PTX ISA 86 and SM_100a/SM_101a architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma.rst#2025-04-23_snippet_36\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.cta_group.kind.block_scale.scale_vec::4X.collector::a::use [d_tmem], a_desc, b_desc, idesc, [scale_A_tmem], [scale_B_tmem], enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .kind      = { .kind::mxf4nvf4 }\n// .cta_group = { .cta_group::1, .cta_group::2 }\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_mma_block_scale_vec_4x_collector_a_use(\n  cuda::ptx::kind_mxf4nvf4_t,\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  uint32_t scale_A_tmem,\n  uint32_t scale_B_tmem,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Implementing multimem.red.relaxed.cluster.global.add.u64 Operation in CUDA PTX\nDESCRIPTION: Template function declaration for performing a 64-bit unsigned integer addition reduction with relaxed semantics and cluster scope in global memory. This function is available for SM_90 and requires PTX ISA 8.1 or higher.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_red.rst#2025-04-23_snippet_27\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.red.sem.scope.global.op.u64 [addr], val; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .release }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .add }\n```\n\n----------------------------------------\n\nTITLE: Running libcu++ Tests for a Specific C++ Standard with lit (Bash)\nDESCRIPTION: This command uses `lit` to build and run tests within a specified path for the `libcudacxx-cpp17` preset, specifically targeting the C++20 standard. The `--param=std=c++20` argument instructs `lit` to configure the tests for the desired standard mode.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/setup/building_and_testing.rst#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncd build\nlit libcudacxx-cpp17/RELATIVE_PATH_TO_TEST_OR_SUBFOLDER -sv --param=std=c++20\n```\n\n----------------------------------------\n\nTITLE: Declaring Scope-aware Relaxed Multimem Load-Reduce (min, s64) with CUDA PTX (C++)\nDESCRIPTION: This device function template performs an atomic min-reduction for 64-bit signed integers located in global memory with relaxed/acquire semantics and customizable scope (CTA, cluster, GPU, system). Parameters permit explicit selection of synchronization guarantees and memory domain. Returns the reduced int64_t. Requires inclusion of PTX C++ types for semantics and scope.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_21\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.s64 dest, [addr]; // PTX ISA 81, SM_90\\n// .sem       = { .relaxed, .acquire }\\n// .scope     = { .cta, .cluster, .gpu, .sys }\\n// .op        = { .min }\\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\\n__device__ static inline int64_t multimem_ld_reduce(\\n  cuda::ptx::sem_t<Sem> sem,\\n  cuda::ptx::scope_t<Scope> scope,\\n  cuda::ptx::op_min_t,\\n  const int64_t* addr);\n```\n\n----------------------------------------\n\nTITLE: Disabling Complex Number Canonicalization for Performance in C++\nDESCRIPTION: Defines preprocessor macros to disable the default behavior of recovering infinite values during complex number multiplication and/or division. This can improve runtime performance at the cost of strict adherence to standard canonicalization rules for infinities. Define `LIBCUDACXX_ENABLE_SIMPLIFIED_COMPLEX_OPERATIONS` to disable both, or the specific multiplication/division macros individually.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/standard_api/numerics_library/complex.rst#2025-04-23_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n// Disables canonicalization for both multiplication and division\n#define LIBCUDACXX_ENABLE_SIMPLIFIED_COMPLEX_OPERATIONS\n\n// Or, disable individually:\n#define LIBCUDACXX_ENABLE_SIMPLIFIED_COMPLEX_MULTIPLICATION\n#define LIBCUDACXX_ENABLE_SIMPLIFIED_COMPLEX_DIVISION\n```\n\n----------------------------------------\n\nTITLE: Performing Asynchronous Bulk Reduction (MIN, u32) from Shared to Global Memory in CUDA\nDESCRIPTION: This CUDA C++ device function `cp_reduce_async_bulk` performs an asynchronous bulk minimum reduction for unsigned 32-bit integers. It reduces data from a source array in shared memory (`srcMem`) to a destination array in global memory (`dstMem`). This overload requires PTX ISA 80 and SM_90 capability.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_reduce_async_bulk.rst#2025-04-23_snippet_16\n\nLANGUAGE: cuda\nCODE:\n```\n   // cp.reduce.async.bulk.dst.src.bulk_group.op.type  [dstMem], [srcMem], size; // 4. PTX ISA 80, SM_90\n   // .dst       = { .global }\n   // .src       = { .shared::cta }\n   // .type      = { .u32 }\n   // .op        = { .min }\n   template <typename = void>\n   __device__ static inline void cp_reduce_async_bulk(\n     cuda::ptx::space_global_t,\n     cuda::ptx::space_shared_t,\n     cuda::ptx::op_min_t,\n     uint32_t* dstMem,\n     const uint32_t* srcMem,\n     uint32_t size);\n```\n\n----------------------------------------\n\nTITLE: Implementing multimem_ld_reduce with weak semantics for OR operation in CUDA\nDESCRIPTION: Template function declaration for multimem load-reduce OR operation on 32-bit values with weak memory semantics. This variant doesn't require a specific memory scope parameter.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_53\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.global.op.b32 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .weak }\n// .op        = { .or }\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline B32 multimem_ld_reduce(\n  cuda::ptx::sem_weak_t,\n  cuda::ptx::op_or_op_t,\n  const B32* addr);\n```\n\n----------------------------------------\n\nTITLE: Declaring tcgen05_mma_ws_tmem_a_collector_b3_lastuse without zero_column_mask_desc for CUDA\nDESCRIPTION: Template function declaration for tensor core operations using tensor memory for A operand without zero column masking. This variant applies to all four data types for CTA group 1 with b3 collector in last-use mode.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_71\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.ws.cta_group.kind.collector::b3::lastuse [d_tmem], [a_tmem], b_desc, idesc, enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1 }\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_tmem_a_collector_b3_lastuse(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint32_t a_tmem,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Declaring Restrictions for 'unittest_static_assert' Test in CMake\nDESCRIPTION: Uses the `thrust_declare_test_restrictions` macro to specify that the `unittest_static_assert` test is restricted. It is only allowed to run for the `CPP.CPP` (host C++, device C++) and `CPP.CUDA` (host C++, device CUDA) configurations. This restriction is due to the test requiring special per-device exception handling only implemented for CUDA.\nSOURCE: https://github.com/nvidia/cccl/blob/main/thrust/testing/CMakeLists.txt#2025-04-23_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\n# This test is incompatible with TBB and OMP, since it requires special per-device\n# handling to process exceptions in a device function, which is only implemented\n# for CUDA.\nthrust_declare_test_restrictions(unittest_static_assert CPP.CPP CPP.CUDA)\n```\n\n----------------------------------------\n\nTITLE: Applying Header Test Function to All Public Headers in CMake\nDESCRIPTION: Iterates through all collected public headers and applies the header test function to each one to create the test targets.\nSOURCE: https://github.com/nvidia/cccl/blob/main/libcudacxx/test/public_headers/CMakeLists.txt#2025-04-23_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nforeach(header IN LISTS public_headers)\n  libcudacxx_add_public_header_test(${header})\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Adding constexpr to Synchronization Object Constructors (C++)\nDESCRIPTION: Enhancement #108 in libcu++ 1.4.1 (CUDA Toolkit 11.3). Added the `constexpr` specifier to the constructors of synchronization objects, allowing for compile-time initialization where possible. Contribution by Olivier Giroux.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/releases/changelog.rst#2025-04-23_snippet_8\n\nLANGUAGE: cpp\nCODE:\n```\nconstexpr // Added to synchronization object constructors\n```\n\n----------------------------------------\n\nTITLE: Creating Meta Targets for CUB Example Configurations\nDESCRIPTION: Creates meta targets that group all examples for each CUB configuration and adds them as dependencies to the main configuration target.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cub/examples/CMakeLists.txt#2025-04-23_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nforeach(cub_target IN LISTS CUB_TARGETS)\n  cub_get_target_property(config_prefix ${cub_target} PREFIX)\n  set(config_meta_target ${config_prefix}.examples)\n  add_custom_target(${config_meta_target})\n  add_dependencies(${config_prefix}.all ${config_meta_target})\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: CCCL Subproject Configuration\nDESCRIPTION: Configures and includes various CCCL subprojects and their dependencies. Conditionally enables unstable features and testing components based on build options.\nSOURCE: https://github.com/nvidia/cccl/blob/main/CMakeLists.txt#2025-04-23_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(libcudacxx)\nadd_subdirectory(cub)\nadd_subdirectory(thrust)\n\nif (CCCL_ENABLE_UNSTABLE)\n  add_subdirectory(cudax)\nendif()\n\nif (CCCL_ENABLE_C)\n  add_subdirectory(c)\nendif()\n\nif (CCCL_ENABLE_TESTING)\n  add_subdirectory(test)\nendif()\n\nif (CCCL_ENABLE_EXAMPLES)\n  add_subdirectory(examples)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Declaring tcgen05_mma_block_scale_vec_2_tmem_a_collector_a_discard for multiple kinds\nDESCRIPTION: Template function declaration for 2X vector scaling operations with tensor memory allocation and collector A discard. Supports multiple dot kinds (MXF4, MXF4NVF4) and CTA groups (1, 2). Designed for PTX ISA 86 and SM_100a/SM_101a architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma.rst#2025-04-23_snippet_50\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_kind Kind, cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_mma_block_scale_vec_2_tmem_a_collector_a_discard(\n  cuda::ptx::kind_t<Kind> kind,\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  uint32_t scale_A_tmem,\n  uint32_t scale_B_tmem,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Defining TCGEN05 MMA Workspace Collector B1 Discard Function Template in CUDA\nDESCRIPTION: Template function for TCGen05 MMA workspace operations with collector B1 discard mode. Takes tmem addresses for d and a operands, b descriptor, idesc, and input enabling flag. Supports multiple data types via the Kind template parameter.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_31\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.ws.cta_group.kind.collector::b1::discard [d_tmem], [a_tmem], b_desc, idesc, enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1 }\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_tmem_a_collector_b1_discard(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint32_t a_tmem,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Binary Find Operation for 64-bit Signed Integer\nDESCRIPTION: Device function template implementing bfind operation for int64_t type. Returns position of the highest set bit in a 64-bit signed integer.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/bfind.rst#2025-04-23_snippet_6\n\nLANGUAGE: cuda\nCODE:\n```\n// bfind.s64 dest, a_reg; // PTX ISA 20, SM_50\ntemplate <typename = void>\n__device__ static inline uint32_t bfind(\n  int64_t a_reg);\n```\n\n----------------------------------------\n\nTITLE: Defining Benchmark Target Creation Function in CMake\nDESCRIPTION: Defines a CMake function `add_bench` that creates an executable target for a benchmark. It takes the desired output target variable name (`target_name`), the benchmark target name (`bench_name`), and the source file (`bench_src`). It configures the target with C++ dialect 17 and links it against `nvbench_helper` and `nvbench::main`.\nSOURCE: https://github.com/nvidia/cccl/blob/main/thrust/benchmarks/CMakeLists.txt#2025-04-23_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(add_bench target_name bench_name bench_src)\n  set(bench_target ${bench_name})\n  set(${target_name} ${bench_target} PARENT_SCOPE)\n\n  add_executable(${bench_target} \"${bench_src}\")\n  cccl_configure_target(${bench_target} DIALECT 17)\n  target_link_libraries(${bench_target} PRIVATE nvbench_helper nvbench::main)\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Installing and Testing CUDA Cooperative Library Using Bash\nDESCRIPTION: This snippet provides bash commands to install the CCCL Python package in editable mode and to run its test suite using pytest. It assumes pip3 and pytest are already installed and that the relevant directories are valid. The commands prepare a local development environment by installing dependencies and then execute all tests verbosely, helping ensure the package functions as expected.\nSOURCE: https://github.com/nvidia/cccl/blob/main/python/cuda_cooperative/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip3 install -e ../cuda_cccl\npip3 install -e .[test]\npytest -v ./tests/\n```\n\n----------------------------------------\n\nTITLE: Store 16-bit Data with L1 No Allocate and L2 Cache Hint in CUDA\nDESCRIPTION: This function stores 16-bit data to global memory with L1 cache no allocate policy and L2 cache hint. It requires SM_80 or later architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/st.rst#2025-04-23_snippet_16\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename B16, enable_if_t<sizeof(B16) == 2, bool> = true>\n__device__ static inline void st_L1_no_allocate_L2_cache_hint(\n  cuda::ptx::space_global_t,\n  B16* addr,\n  B16 src,\n  uint64_t cache_policy);\n```\n\n----------------------------------------\n\nTITLE: Memory Barrier Try Wait with Suspend Hint\nDESCRIPTION: Memory barrier try_wait operation with additional suspend time hint parameter for performance optimization.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/mbarrier_try_wait.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline bool mbarrier_try_wait(\n  uint64_t* addr,\n  const uint64_t& state,\n  const uint32_t& suspendTimeHint);\n```\n\n----------------------------------------\n\nTITLE: Generating Multiple Test Cases in CUB Tests with C++\nDESCRIPTION: Shows how to use the GENERATE macro to create multiple test cases with different runtime values.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/test_overview.rst#2025-04-23_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\nC2H_TEST(\"SCOPE FACILITY works with CONDITION\", \"[FACILITY][SCOPE]\")\n{\n  int num_items = GENERATE(1, 100, 1'000'000);\n  // ...\n}\n```\n\n----------------------------------------\n\nTITLE: Defining CUDA Access Property Conversion Operators\nDESCRIPTION: This CUDA code snippet defines conversion operators for different access property types (normal, streaming, persisting) to cudaAccessProperty. These operators allow using custom access property objects in place of CUDA Runtime enumerated values.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/memory_access_properties/access_property.rst#2025-04-23_snippet_2\n\nLANGUAGE: cuda\nCODE:\n```\n__host__ __device__ constexpr access_property::normal::operator cudaAccessProperty() const noexcept;\n__host__ __device__ constexpr access_property::streaming::operator cudaAccessProperty() const noexcept;\n__host__ __device__ constexpr access_property::persisting::operator cudaAccessProperty() const noexcept;\n```\n\n----------------------------------------\n\nTITLE: Collecting Public Headers using file(GLOB) in CMake\nDESCRIPTION: Uses CMake's file(GLOB) command to collect all public headers from the CUDA C++ library include directories.\nSOURCE: https://github.com/nvidia/cccl/blob/main/libcudacxx/test/public_headers/CMakeLists.txt#2025-04-23_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB public_headers\n  LIST_DIRECTORIES false\n  RELATIVE \"${libcudacxx_SOURCE_DIR}/include\"\n  CONFIGURE_DEPENDS\n  \"${libcudacxx_SOURCE_DIR}/include/cuda/*\"\n  \"${libcudacxx_SOURCE_DIR}/include/cuda/std/*\"\n)\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Include for mbarrier.expect_tx\nDESCRIPTION: RestructuredText directives for including documentation about the mbarrier.expect_tx PTX instruction, including a reference link to official documentation.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/mbarrier_expect_tx.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _libcudacxx-ptx-instructions-mbarrier-expect_tx:\n\nmbarrier.expect_tx\n==================\n\n-  PTX ISA:\n   `mbarrier.expect_tx <https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-mbarrier-expect-tx>`__\n\n.. include:: generated/mbarrier_expect_tx.rst\n```\n\n----------------------------------------\n\nTITLE: Implementing multimem_ld_reduce for AND operation with various semantics and scopes in CUDA\nDESCRIPTION: Template function declaration for multimem load-reduce AND operation on 32-bit values. This template supports both relaxed and acquire semantics with different memory scopes (CTA, cluster, GPU, system) for global memory access.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_52\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.b32 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .and }\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true, cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline B32 multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_and_op_t,\n  const B32* addr);\n```\n\n----------------------------------------\n\nTITLE: Testing for Thrust System Availability\nDESCRIPTION: Functions to check if specific Thrust systems (CUDA, CPP, TBB, OMP) have been found and update system found flags in the current scope.\nSOURCE: https://github.com/nvidia/cccl/blob/main/lib/cmake/thrust/README.md#2025-04-23_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\n# Set var_name to TRUE or FALSE if an individual system has been found:\nthrust_is_cuda_system_found(<var_name>)\nthrust_is_cpp_system_found(<var_name>)\nthrust_is_tbb_system_found(<var_name>)\nthrust_is_omp_system_found(<var_name>)\n\n# Generic version that takes a component name from CUDA, CPP, TBB, OMP:\nthrust_is_system_found(<component_name> <var_name>)\n\n# Defines `THRUST_*_FOUND` variables in the current scope that reflect the\n# state of all known systems. Can be used to refresh these flags after\n# lazy system loading.\nthrust_update_system_found_flags()\n```\n\n----------------------------------------\n\nTITLE: Async Store with Barrier Completion - 32-bit Vector4\nDESCRIPTION: Template for asynchronous weak shared memory store operation with cluster barrier completion tracking for 4-element vectors of 32-bit values. Includes SFINAE constraint to ensure correct type size. Compatible with PTX ISA 81 and SM_90 architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/st_async.rst#2025-04-23_snippet_4\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline void st_async(\n  B32* addr,\n  const B32 (&value)[4],\n  uint64_t* remote_bar);\n```\n\n----------------------------------------\n\nTITLE: Defining CTA Group Tensor Core Function for 32x128b WarpX4 b8x16 b4x16 p64 Variant - CUDA\nDESCRIPTION: This template function targets the tensor core instruction for CTA groups of size 32x128b with WarpX4, block, and pipeline configuration (b8x16, b4x16, p64). It takes a PTX cta_group handle, a tile address (32-bit int), and a descriptor (64-bit int) as parameters. The function requires CUDA PTX types and device support for corresponding PTX instruction set architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_cp.rst#2025-04-23_snippet_11\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.cp.cta_group.32x128b.warpx4.b8x16.b4x16_p64 [taddr], s_desc; // PTX ISA 86, SM_100a, SM_101a\\n// .cta_group = { .cta_group::1, .cta_group::2 }\\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\\n__device__ static inline void tcgen05_cp_32x128b_warpx4_b8x16_b4x16_p64(\\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\\n  uint32_t taddr,\\n  uint64_t s_desc);\n```\n\n----------------------------------------\n\nTITLE: Alternative CMake Build Configuration\nDESCRIPTION: An alternative approach to configuring the CMake build by manually creating the build directory, changing into it, and running CMake from there. This is functionally equivalent to the single-line approach.\nSOURCE: https://github.com/nvidia/cccl/blob/main/examples/basic/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmkdir -p build\ncd build\ncmake ..\n```\n\n----------------------------------------\n\nTITLE: Creating Stream Tests in CMake\nDESCRIPTION: Creates tests for CUDA stream functionality, including stream retrieval and basic operations. These tests validate the proper operation of CUDA streams within the CCCL framework.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cudax/test/CMakeLists.txt#2025-04-23_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\n  cudax_add_catch2_test(test_target stream ${cn_target}\n    stream/get_stream.cu\n    stream/stream_smoke.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: Configuring Code Generation Example Sources\nDESCRIPTION: Defines CUDA examples that utilize code generation features like parallel_for and launch, including graph algorithms and various computational patterns.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cudax/examples/stf/CMakeLists.txt#2025-04-23_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(stf_example_codegen_sources\n  01-axpy-launch.cu\n  01-axpy-parallel_for.cu\n  binary_fhe.cu\n  09-dot-reduce.cu\n  cfd.cu\n  custom_data_interface.cu\n  fdtd_mgpu.cu\n  frozen_data_init.cu\n  graph_algorithms/degree_centrality.cu\n  graph_algorithms/jaccard.cu\n  graph_algorithms/pagerank.cu\n  graph_algorithms/tricount.cu\n  heat.cu\n  heat_mgpu.cu\n  jacobi.cu\n  jacobi_pfor.cu\n  launch_histogram.cu\n  launch_scan.cu\n  launch_sum.cu\n  launch_sum_cub.cu\n  logical_gates_composition.cu\n  mandelbrot.cu\n  parallel_for_2D.cu\n  pi.cu\n  scan.cu\n  standalone-launches.cu\n  word_count.cu\n  word_count_reduce.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Declaring multimem_ld_reduce with Weak Semantics and OR Operation in CUDA PTX\nDESCRIPTION: Template function declaration for multimem.ld_reduce with weak memory semantics and OR operation on 64-bit values in global memory. This is for PTX ISA 8.1 and SM_90.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_60\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.global.op.b64 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .weak }\n// .op        = { .or }\ntemplate <typename B64, enable_if_t<sizeof(B64) == 8, bool> = true>\n__device__ static inline B64 multimem_ld_reduce(\n  cuda::ptx::sem_weak_t,\n  cuda::ptx::op_or_op_t,\n  const B64* addr);\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx Documentation Index for Thrust Sorting Algorithms\nDESCRIPTION: This RST (reStructuredText) code sets up a documentation index section for sorting algorithms in the Thrust library. It creates a table of contents that dynamically includes all files matching the 'function_group__sorting' pattern.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/api_docs/algorithms/sorting.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _thrust-module-api-algorithms-sorting:\n\nSorting\n--------\n\n.. toctree::\n   :glob:\n   :maxdepth: 1\n\n   ${repo_docs_api_path}/*function_group__sorting*\n\n```\n\n----------------------------------------\n\nTITLE: CUDA MMA Extended Usage Template with Memory\nDESCRIPTION: Extended version of the MMA template that includes additional tmem_a and zero_column_mask parameters for more complex matrix operations.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_20\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_tmem_a_collector_b1_use(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint32_t a_tmem,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d,\n  uint64_t zero_column_mask_desc);\n```\n\n----------------------------------------\n\nTITLE: Including C Standard Assertion Header in CUDA C++\nDESCRIPTION: Includes the `<cuda/std/cassert>` header, providing lightweight assumption testing facilities compatible with C++. Available since CCCL 2.0.0 and CUDA Toolkit 10.2.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/standard_api/c_library.rst#2025-04-23_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n#include <cuda/std/cassert>\n```\n\n----------------------------------------\n\nTITLE: Configuring Test Source Collection and Targeted Test Registration with CMake - CMake\nDESCRIPTION: This snippet uses CMake commands to glob for test source files with .cu and .cpp extensions and iteratively registers tests for each THRUST_TARGETS entry when the device property is set to CPP. It depends on CMake's builtin functions (file, foreach, get_filename_component, etc.) and external definitions (THRUST_TARGETS list, thrust_get_target_property, thrust_add_test). The code expects these macros and variables to be defined elsewhere in the build system, takes test source files as input, and produces uniquely named tests for C++ compilation. Limitation: only targets with device type 'CPP' are handled; external macros are dependencies.\nSOURCE: https://github.com/nvidia/cccl/blob/main/thrust/testing/cpp/CMakeLists.txt#2025-04-23_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB test_srcs\n  RELATIVE \"${CMAKE_CURRENT_LIST_DIR}}\"\n  CONFIGURE_DEPENDS\n  *.cu *.cpp\n)\n\nforeach(thrust_target IN LISTS THRUST_TARGETS)\n  thrust_get_target_property(config_device ${thrust_target} DEVICE)\n  if (NOT config_device STREQUAL \"CPP\")\n    continue()\n  endif()\n\n  foreach(test_src IN LISTS test_srcs)\n    get_filename_component(test_name \"${test_src}\" NAME_WLE)\n    string(PREPEND test_name \"cpp.\")\n    thrust_add_test(test_target ${test_name} \"${test_src}\" ${thrust_target})\n  endforeach()\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Basic CMake Setup and Project Configuration\nDESCRIPTION: Sets minimum CMake version requirements, configures project settings, and handles policy updates. Determines if CCCL is being built as a top-level project or included via add_subdirectory().\nSOURCE: https://github.com/nvidia/cccl/blob/main/CMakeLists.txt#2025-04-23_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.15)\n\nif (POLICY CMP0141)\n  cmake_policy(SET CMP0141 NEW)\nendif()\n\nset(CCCL_TOPLEVEL_PROJECT OFF)\nif (\"${CMAKE_SOURCE_DIR}\" STREQUAL \"${CMAKE_CURRENT_LIST_DIR}\")\n  set(CCCL_TOPLEVEL_PROJECT ON)\nendif()\n\nproject(CCCL LANGUAGES CXX)\n```\n\n----------------------------------------\n\nTITLE: Implementing MMA collector with shared memory input for A (without zero_column_mask) in CUDA\nDESCRIPTION: This function template is a variation of the MMA collector with matrix A in shared memory, but without the zero_column_mask_desc parameter. It's used for simpler MMA operations that don't require column masking.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_30\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_tmem_a_collector_b1_discard(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint32_t a_tmem,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Implementing tcgen05 MMA tmem_a_collector_b3_discard PTX Operation without zero_column_mask\nDESCRIPTION: Template function for tensor core matrix multiply accumulate operation with tensor memory for A input and B3 collector with discard option without zero column masking. Supports multiple data types.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_76\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.ws.cta_group.kind.collector::b3::discard [d_tmem], [a_tmem], b_desc, idesc, enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1 }\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_tmem_a_collector_b3_discard(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint32_t a_tmem,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Bulk Copy from Shared CTA to Global Memory with Bulk Group (CUDA)\nDESCRIPTION: Performs an asynchronous bulk copy from shared CTA memory to global memory using a bulk group. This operation is available from PTX ISA 80 and SM_90.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_async_bulk.rst#2025-04-23_snippet_3\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline void cp_async_bulk(\n  cuda::ptx::space_global_t,\n  cuda::ptx::space_shared_t,\n  void* dstMem,\n  const void* srcMem,\n  const uint32_t& size);\n```\n\n----------------------------------------\n\nTITLE: Running Individual CUB Benchmark\nDESCRIPTION: Example command for running a single CUB benchmark with NVBench options for GPU selection and output formats\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/benchmarking.rst#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n./bin/cub.bench.adjacent_difference.subtract_left.base\\\n    -d 0\\\n    --stopping-criterion entropy\\\n    --json base.json\\\n    --md base.md\n```\n\n----------------------------------------\n\nTITLE: ReStructuredText Documentation Section\nDESCRIPTION: RST markup for defining a documentation section with hidden toctree and links to macro documentation\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cccl/development/index.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _cccl-development-module:\n\nCCCL Development Guide\n======================\n\n.. toctree::\n   :hidden:\n   :maxdepth: 1\n\n   macro\n\nThis living document serves to describe the internal details and the development process of CCCL libraries.\n\nDocumentation:\n\n- `CCCL Internal Macros <https://nvidia.github.io/cccl/cccl/development/macro.html>`__\n```\n\n----------------------------------------\n\nTITLE: Store 8-bit Data with L1 No Allocate in CUDA\nDESCRIPTION: This function stores 8-bit data to global memory with L1 cache no allocate policy. It requires SM_70 or later architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/st.rst#2025-04-23_snippet_10\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename B8, enable_if_t<sizeof(B8) == 1, bool> = true>\n__device__ static inline void st_L1_no_allocate(\n  cuda::ptx::space_global_t,\n  B8* addr,\n  B8 src);\n```\n\n----------------------------------------\n\nTITLE: Registering Compile Tests for All Component and Package Combinations - CMake Language\nDESCRIPTION: This section loops through permutations of root types (source/install), package components (like Thrust, CUB, libcudacxx, and optionally cudax), and package types to dynamically register compile tests via `cccl_add_compile_test`. It assigns suffixes and configures each test with appropriate build options and fixtures, relying on variables and conditional inclusion logic. Prerequisites: supporting CMake macros/functions such as `cccl_add_compile_test` must be defined, and project variables like `CCCL_SOURCE_DIR`, `CCCL_ENABLE_UNSTABLE`, and `CCCL_BINARY_DIR` should be populated. Inputs include project setup variables; output is the set of registered tests with correct environment and conditions.\nSOURCE: https://github.com/nvidia/cccl/blob/main/test/cmake/CMakeLists.txt#2025-04-23_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nforeach (root_type IN ITEMS SOURCE INSTALL)\n  if (root_type STREQUAL \"INSTALL\")\n    set(cccl_root \"${tmp_install_prefix}\")\n  else()\n    set(cccl_root \"${CCCL_SOURCE_DIR}\")\n  endif()\n\n  set(comps DEFAULT Thrust CUB libcudacxx)\n  if (CCCL_ENABLE_UNSTABLE)\n    list(APPEND comps cudax)\n  endif()\n\n  foreach (components IN LISTS comps)\n    set(package_types CCCL)\n    if (NOT components STREQUAL \"DEFAULT\")\n      list(APPEND package_types NATIVE)\n    endif()\n    if (root_type STREQUAL \"SOURCE\")\n      list(APPEND package_types SUBDIR)\n    endif()\n    foreach (package_type IN LISTS package_types)\n      string(TOLOWER \"${root_type}.${package_type}.${components}\" suffix)\n      cccl_add_compile_test(test_name\n        cccl.test.cmake\n        test_export\n        \"${suffix}\"\n        ${cmake_opts}\n        -D \"CCCL_ROOT=${cccl_root}\"\n        -D \"COMPONENTS=${components}\"\n        -D \"PACKAGE_TYPE=${package_type}\"\n        -D \"CCCL_ENABLE_UNSTABLE=${CCCL_ENABLE_UNSTABLE}\"\n      )\n\n      if (root_type STREQUAL \"INSTALL\")\n        set_tests_properties(${test_name} PROPERTIES FIXTURES_REQUIRED install_tree)\n      endif()\n    endforeach() # package_type\n  endforeach() # components\nendforeach() # root_type\n```\n\n----------------------------------------\n\nTITLE: Displaying Top CUB Benchmark Variants in Bash\nDESCRIPTION: Demonstrates how to use the analyze.py script to show the top N variants for each compile-time workload, including their scores and performance metrics.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/tuning.rst#2025-04-23_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n$ ../benchmarks/scripts/analyze.py --top=5\n    cub.bench.radix_sort.keys[T{ct}=I8, OffsetT{ct}=I32]:\n              variant     score      mins     means      maxs\n    97  ipt_19.tpb_512  1.141015  1.039052  1.243448  1.679558\n    84  ipt_18.tpb_512  1.136463  1.030434  1.245825  1.668038\n    68  ipt_17.tpb_512  1.132696  1.020470  1.250665  1.688889\n    41  ipt_15.tpb_576  1.124077  1.011560  1.245011  1.722379\n    52  ipt_16.tpb_512  1.121044  0.995238  1.252378  1.717514\n    cub.bench.radix_sort.keys[T{ct}=I8, OffsetT{ct}=I64]:\n              variant     score      mins     means      maxs\n    71  ipt_19.tpb_512  1.250941  1.155738  1.321665  1.647868\n    86  ipt_20.tpb_512  1.250840  1.128940  1.308591  1.612382\n    55  ipt_17.tpb_512  1.244399  1.152033  1.327424  1.692091\n    98  ipt_21.tpb_448  1.231045  1.152798  1.298332  1.621110\n    85  ipt_20.tpb_480  1.229382  1.135447  1.294937  1.631225\n```\n\n----------------------------------------\n\nTITLE: Defining TOC Structure for Thrust System Module Documentation in reStructuredText\nDESCRIPTION: This snippet defines the table of contents structure for the Thrust System module documentation using reStructuredText format. It references the module with a cross-reference label and organizes documentation into systems and diagnostics subsections.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/api_docs/system.rst#2025-04-23_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. _thrust-module-api-system:\n\nSystem\n=======\n\n.. toctree::\n   :glob:\n   :maxdepth: 2\n\n   system/systems\n   system/diagnostics\n```\n\n----------------------------------------\n\nTITLE: Sample JSON Output of Benchmark Results\nDESCRIPTION: Shows the structure of the JSON output containing benchmark results, including variant details, elapsed time, bandwidth, and sample measurements.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/benchmarking.rst#2025-04-23_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"variant\": \"base ()\",\n    \"elapsed\": 2.6299014091,\n    \"center\": 0.003194368,\n    \"bw\": 0.8754671386,\n    \"samples\": [\n      0.003152896,\n      0.0031549439,\n      ...\n    ],\n    \"Elements{io}[pow2]\": \"28\",\n    \"base_samples\": [\n      0.003152896,\n      0.0031549439,\n      ...\n    ],\n    \"speedup\": 1\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Setting Default Warning Levels for Unknown CUDA Compilers\nDESCRIPTION: Provides fallback warning flags for unknown CUDA compilers, defaulting to GCC-style warning flags.\nSOURCE: https://github.com/nvidia/cccl/blob/main/libcudacxx/test/CMakeLists.txt#2025-04-23_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nelse()\n  set(headertest_warning_levels_device -Wall -Werror)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining tcgen05_mma_ws_collector_b3_use Template Function for CUDA PTX\nDESCRIPTION: Template function declaration for using b3 collector with various data types in MMA workstation operations. This function supports the use phase of tensor core operations with zero column masking capabilities.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_64\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.ws.cta_group.kind.collector::b3::use [d_tmem], a_desc, b_desc, idesc, enable_input_d, zero_column_mask_desc; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1 }\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_collector_b3_use(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d,\n  uint64_t zero_column_mask_desc);\n```\n\n----------------------------------------\n\nTITLE: Templated CUDA Kernel with Annotated Pointer Support\nDESCRIPTION: Generic version of the update kernel that accepts annotated pointers to specify memory access patterns.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/memory_access_properties/annotated_ptr.rst#2025-04-23_snippet_10\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename PointerX, typename PointerA, typename PointerB>\n__global__ void update_template(PointerX x, PointerA a, PointerB b, size_t N) {\n    auto g = cooperative_groups::this_grid();\n    for (int idx = g.thread_rank(); idx < N; idx += g.size()) {\n        x[idx] = a[idx] * x[idx] + b[idx];\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Cluster Scope Acquire Memory Barrier Test Wait\nDESCRIPTION: Memory barrier test wait with acquire semantics and cluster scope for PTX ISA 80 and SM_90. Includes scope and semantic parameters.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/mbarrier_test_wait.rst#2025-04-23_snippet_2\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_scope Scope>\n__device__ static inline bool mbarrier_test_wait(\n  cuda::ptx::sem_acquire_t,\n  cuda::ptx::scope_t<Scope> scope,\n  uint64_t* addr,\n  const uint64_t& state);\n```\n\n----------------------------------------\n\nTITLE: Store 128-bit Data with L1 No Allocate in CUDA\nDESCRIPTION: This function stores 128-bit data to global memory with L1 cache no allocate policy. It requires SM_70 or later architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/st.rst#2025-04-23_snippet_14\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename B128, enable_if_t<sizeof(B128) == 16, bool> = true>\n__device__ static inline void st_L1_no_allocate(\n  cuda::ptx::space_global_t,\n  B128* addr,\n  B128 src);\n```\n\n----------------------------------------\n\nTITLE: Navigating Thread Hierarchies in C++\nDESCRIPTION: Demonstrates how to navigate within thread hierarchies using the inner() method, which removes the top-most level and returns a new hierarchy specification.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_40\n\nLANGUAGE: cpp\nCODE:\n```\nauto spec = con<128>(con());\n...\nauto ti = th.inner();\nti.size(); // size within the par() hierarchy (automatically computed value)\nti.rank(); // rank within the par() hierarchy\n...\nth.inner().sync(); // synchronize threads in the same block of the second level of the hierarchy\n```\n\n----------------------------------------\n\nTITLE: Partitioning Policy Visualization: Tiled Layout - CUDASTF Text\nDESCRIPTION: Depicts an ASCII schematic of how a 2D shape is tiled across three places, with the tile boundaries defined by TILE_SIZE, serving as a reference for data layout in the tiled_partition policy. Used for documentation and conceptual overview, not for direct code execution.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_29\n\nLANGUAGE: text\nCODE:\n```\n    ________________________________\n   |     |     |     |     |     |  |\n   |     |     |     |     |     |  |\n   |     |     |     |     |     |  |\n   | P 0 | P 1 | P 2 | P 0 | P 1 |P2|\n   |     |     |     |     |     |  |\n   |     |     |     |     |     |  |\n   |_____|_____|_____|_____|_____|__|\n```\n\n----------------------------------------\n\nTITLE: Implementing Fuzzy Search for 404 Page Suggestions in JavaScript\nDESCRIPTION: This JavaScript module uses Fuse.js for fuzzy searching to find and display similar pages when a user encounters a 404 error. It extracts the current URL path, fetches a page list, performs a fuzzy search, and dynamically generates links to similar pages with their relevance scores.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/404.rst#2025-04-23_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport Fuse from 'https://cdn.jsdelivr.net/npm/fuse.js@7.0.0/dist/fuse.mjs'\nvar localUri = window.location.pathname;\nvar hostUrl = window.location.protocol + \"//\" + window.location.host\nvar pagelistUrl = hostUrl + \"/cccl/pagelist.txt\"\n\nfetch(pagelistUrl)\n  .then((response) => response.text())\n  .then((text) => {\n    const pagelist = text.split(\",\")\n\n    const options = {\n      includeScore: true,\n      distance: 100,\n      threshold: 0.8,\n      ignoreLocation: true\n    }\n\n    // cccl/device_vector.html => 'cccl 'device 'vector\n    const query = localUri.replace(/\\/|-|_/gi,\" '\").replace(\".html\",\"\")\n    console.log(\"query:\" + query)\n    const fuse = new Fuse(pagelist, options)\n    const result = fuse.search(query, {limit: 10})\n\n    const listItem = '<li><a href=\\\"{2}{0}\\\">{0}</a> - Score: {1}</li>'.replace(/\\{2\\}/g, hostUrl + \"/\")\n    document.getElementById('linkGen').innerHTML = \"\"\n    result.forEach(element => {\n      document.getElementById('linkGen').innerHTML += listItem.replace(/\\{0\\}/g, element.item).replace(/\\{1\\}/g, element.score.toFixed(2))\n    });\n  })\n```\n\n----------------------------------------\n\nTITLE: Creating Execution Policy Tests in CMake\nDESCRIPTION: Sets up tests for CUDA execution policies and related functionality. These tests verify the environment, policy definitions, and policy retrieval mechanisms in the CCCL execution model.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cudax/test/CMakeLists.txt#2025-04-23_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\n  cudax_add_catch2_test(test_target execution ${cn_target}\n    execution/env.cu\n    execution/policies/policies.cu\n    execution/policies/get_execution_policy.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: Implementing MMA Operation with Block Scaling for mxf4 and mxf4nvf4 kinds\nDESCRIPTION: This CUDA device function template implements a matrix multiplication and accumulation operation with block scaling. It supports mxf4 and mxf4nvf4 kinds and CTA group sizes of 1 and 2. The function uses tensor memory and handles input scaling.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma.rst#2025-04-23_snippet_18\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.cta_group.kind.block_scale.scale_vec::2X [d_tmem], a_desc, b_desc, idesc, [scale_A_tmem], [scale_B_tmem], enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .kind      = { .kind::mxf4, .kind::mxf4nvf4 }\n// .cta_group = { .cta_group::1, .cta_group::2 }\ntemplate <cuda::ptx::dot_kind Kind, cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_mma_block_scale_vec_2_tmem_a(\n  cuda::ptx::kind_t<Kind> kind,\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  uint32_t scale_A_tmem,\n  uint32_t scale_B_tmem,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Declaring multimem_ld_reduce with Relaxed/System Scope and XOR Operation in CUDA PTX\nDESCRIPTION: Template function declaration for multimem.ld_reduce with relaxed memory semantics, system scope, and XOR operation on 64-bit values in global memory. For PTX ISA 8.1 on SM_90 architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_73\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.b64 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .xor }\ntemplate <typename B64, enable_if_t<sizeof(B64) == 8, bool> = true, cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline B64 multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_xor_op_t,\n  const B64* addr);\n```\n\n----------------------------------------\n\nTITLE: Configuring and Building CUB Examples with CMake\nDESCRIPTION: This CMake script searches for CUB example source files, processes them, and creates build targets for each example across all CUB targets. It uses glob patterns to find files, extracts names, and applies naming conventions before adding the examples to the build.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cub/examples/block/CMakeLists.txt#2025-04-23_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB_RECURSE example_srcs\n  RELATIVE \"${CMAKE_CURRENT_LIST_DIR}\"\n  CONFIGURE_DEPENDS\n  example_*.cu\n)\n\nforeach (cub_target IN LISTS CUB_TARGETS)\n  foreach (example_src IN LISTS example_srcs)\n    get_filename_component(example_name \"${example_src}\" NAME_WE)\n    string(REGEX REPLACE\n      \"^example_block_\" \"block.\"\n      example_name \"${example_name}\"\n    )\n    cub_add_example(target_name ${example_name} \"${example_src}\" ${cub_target})\n  endforeach()\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: MMA TMEM Collector Fill Operation with Zero Column Mask\nDESCRIPTION: Template function for matrix multiply-accumulate collector fill operation using tensor memory for A operand with zero column mask support. Handles multiple data types.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_2\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_tmem_a_collector_b0_fill(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint32_t a_tmem,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d,\n  uint64_t zero_column_mask_desc);\n```\n\n----------------------------------------\n\nTITLE: Declaring CUDA Latch Template Class\nDESCRIPTION: Template class declaration for cuda::latch that takes a thread_scope parameter.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/synchronization_primitives/latch.rst#2025-04-23_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\ntemplate <cuda::thread_scope Scope>\nclass cuda::latch;\n```\n\n----------------------------------------\n\nTITLE: Enabling <cuda/std/tuple> with NVCC + MSVC (C++)\nDESCRIPTION: Issue #56 fixed in libcu++ 1.4.0. The `<cuda/std/tuple>` header now works correctly when using NVCC in conjunction with recent MSVC compilers.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/releases/changelog.rst#2025-04-23_snippet_17\n\nLANGUAGE: cpp\nCODE:\n```\n<cuda/std/tuple>\n```\n\n----------------------------------------\n\nTITLE: Creating Meta Targets for Test Configuration\nDESCRIPTION: Sets up meta targets that build all tests for a single configuration. For each CUB target, a corresponding test meta target is created and added as a dependency to the all target.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cub/test/CMakeLists.txt#2025-04-23_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\n# Create meta targets that build all tests for a single configuration:\nforeach(cub_target IN LISTS CUB_TARGETS)\n  cub_get_target_property(config_prefix ${cub_target} PREFIX)\n  set(config_meta_target ${config_prefix}.tests)\n  add_custom_target(${config_meta_target})\n  add_dependencies(${config_prefix}.all ${config_meta_target})\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Configuring CCCL C Parallel CMake Project\nDESCRIPTION: Sets up the CMake project for CCCL C Parallel with CUDA, CXX, and C languages. Defines build options for testing and header testing, and allows overriding the library output directory.\nSOURCE: https://github.com/nvidia/cccl/blob/main/c/parallel/CMakeLists.txt#2025-04-23_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.21)\n\nproject(CCCL_C_Parallel LANGUAGES CUDA CXX C)\n\noption(CCCL_C_Parallel_ENABLE_TESTING \"Build cccl.c.parallel tests.\" OFF)\noption(CCCL_C_Parallel_ENABLE_HEADER_TESTING \"Build cccl.c.parallel standalone headers.\" OFF)\n\n# FIXME Ideally this would be handled by presets and install rules, but for now\n# consumers may override this to control the target location of cccl.c.parallel.\nset(CCCL_C_PARALLEL_LIBRARY_OUTPUT_DIRECTORY \"\" CACHE PATH \"Override output directory for the cccl.c.parallel library\")\nmark_as_advanced(CCCL_C_PARALLEL_LIBRARY_OUTPUT_DIRECTORY)\n```\n\n----------------------------------------\n\nTITLE: Declaring Fence Proxy Alias Function in CUDA\nDESCRIPTION: Declares a device-specific inline function template for fence proxy alias operation. It is designed for PTX ISA 75 and SM_70 architectures. The function takes no parameters and returns void.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/fence_proxy_alias.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\n// fence.proxy.alias; // 4. PTX ISA 75, SM_70\ntemplate <typename = void>\n__device__ static inline void fence_proxy_alias();\n```\n\n----------------------------------------\n\nTITLE: Initializing MMA Collector with Discard Option (CUDA)\nDESCRIPTION: This function initializes an MMA collector with a discard option for f16 data type. It uses CTA group 1 and includes a zero column mask.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_26\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.ws.cta_group.kind.collector::b1::discard [d_tmem], a_desc, b_desc, idesc, enable_input_d, zero_column_mask_desc; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1 }\n```\n\n----------------------------------------\n\nTITLE: Async Bulk Tensor Gather from Global to Shared CTA Memory\nDESCRIPTION: Template function for asynchronous bulk tensor gathering from global to shared CTA memory with mbarrier completion tracking. Uses gather4 operation for tensor data movement.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_async_bulk_tensor_gather_scatter.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline void cp_async_bulk_tensor_tile_gather4(\n  cuda::ptx::space_shared_t,\n  cuda::ptx::space_global_t,\n  void* dstMem,\n  const void* tensorMap,\n  const int32_t (&tensorCoords)[5],\n  uint64_t* smem_bar);\n```\n\n----------------------------------------\n\nTITLE: Introducing ABI Version 3 and Switching Macro (C++)\nDESCRIPTION: libcu++ 1.2.0 introduces ABI version 3 as the default to improve `cuda::[std::]barrier` performance via alignment changes. Users can revert to the previous ABI (version 2) by defining `_LIBCUDACXX_CUDA_ABI_VERSION=2` before including any libcu++ or CUDA headers.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/releases/changelog.rst#2025-04-23_snippet_22\n\nLANGUAGE: cpp\nCODE:\n```\n// Define before including libcu++ or CUDA headers to use ABI version 2\n#define _LIBCUDACXX_CUDA_ABI_VERSION 2\n```\n\n----------------------------------------\n\nTITLE: Extracting Benchmark Results to JSON (Bash)\nDESCRIPTION: Shows how to extract benchmark results from the SQLite database into JSON files using the analyze.py script.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/benchmarking.rst#2025-04-23_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n<root_dir_to_cccl>/cccl/benchmarks/scripts/analyze.py -o ./cccl_meta_bench.db\n```\n\n----------------------------------------\n\nTITLE: Storing 16x128b Data with tcgen05 in CUDA (64 values)\nDESCRIPTION: This function performs a 16x128b store operation using tcgen05. It takes a 32-bit address and an array of 64 B32 values as input. The function is constrained to B32 types of 4 bytes.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_st.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.st.sync.aligned.16x128b.x32.b32 [taddr], values; // PTX ISA 86, SM_100a, SM_101a\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline void tcgen05_st_16x128b(\n  uint32_t taddr,\n  const B32 (&values)[64]);\n```\n\n----------------------------------------\n\nTITLE: Configuring and Registering CCCL Stdpar Tests - CMake\nDESCRIPTION: Defines the CMake logic to find test sources, configure build and GPU-specific options, and register each test with CTest. Requires CMake 3.21+, NVHPC nvc++ compiler, and the CCCL package. Expects all test source files to be in the tests directory with .cpp extension. Ensures consistent versioning for CCCL and GPU enabling in both compile and link steps. Output is a set of CTest-ready executables for each source, each linked with CCCL library. The configuration will fail if nvc++ is not the compiler.\nSOURCE: https://github.com/nvidia/cccl/blob/main/test/stdpar/CMakeLists.txt#2025-04-23_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.21)\n\n# NOTE: this is build outside of the libcu++ test harness\nproject(CCCL_STDPAR_TESTS LANGUAGES CXX)\n\nif (NOT CMAKE_CXX_COMPILER_ID STREQUAL NVHPC)\n  message(FATAL_ERROR \"The stdpar tests require nvc++ for CMAKE_CXX_COMPILER.\")\nendif()\n\n# Enable testing for the project\nenable_testing()\n\nfind_package(CCCL CONFIG REQUIRED\n  NO_DEFAULT_PATH # Only check the explicit HINTS below:\n  HINTS \"${CMAKE_CURRENT_LIST_DIR}/../../lib/cmake/cccl/\"\n)\n\nfile(GLOB test_files\n  LIST_DIRECTORIES false\n  RELATIVE \"${CMAKE_CURRENT_LIST_DIR}\"\n  CONFIGURE_DEPENDS\n  \"tests/*.cpp\"\n)\n\nfunction(cccl_add_stdpar_test test_file)\n  get_filename_component(test_name ${test_file} NAME_WE)\n\n  add_executable(stdpar_test_${test_name} ${test_file})\n  target_link_libraries(stdpar_test_${test_name} PUBLIC CCCL::CCCL)\n\n  # Ensure that we are testing with GPU support\n  target_compile_options(stdpar_test_${test_name} PUBLIC -stdpar=gpu)\n  target_link_options(stdpar_test_${test_name} PUBLIC -stdpar=gpu)\n\n  # Ensure that we are indeed testing the same CCCL version\n  target_compile_definitions(stdpar_test_${test_name} PUBLIC CMAKE_CCCL_VERSION_MAJOR=${CCCL_VERSION_MAJOR})\n  target_compile_definitions(stdpar_test_${test_name} PUBLIC CMAKE_CCCL_VERSION_MINOR=${CCCL_VERSION_MINOR})\n  target_compile_definitions(stdpar_test_${test_name} PUBLIC CMAKE_CCCL_VERSION_PATCH=${CCCL_VERSION_PATCH})\n\n  # Register with ctest\n  add_test(NAME stdpar_test_${test_name} COMMAND stdpar_test_${test_name})\nendfunction()\n\nforeach(test IN LISTS test_files)\n  cccl_add_stdpar_test(${test})\nendforeach()\n\n```\n\n----------------------------------------\n\nTITLE: Relaxed Memory Barrier Try Wait with Suspend Hint\nDESCRIPTION: Memory barrier try_wait with relaxed semantics and suspend time hint, supporting both CTA and cluster scopes.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/mbarrier_try_wait.rst#2025-04-23_snippet_5\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_scope Scope>\n__device__ static inline bool mbarrier_try_wait(\n  cuda::ptx::sem_relaxed_t,\n  cuda::ptx::scope_t<Scope> scope,\n  uint64_t* addr,\n  const uint64_t& state,\n  const uint32_t& suspendTimeHint);\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Targets for Atomics Code Generation and Installation\nDESCRIPTION: Defines two custom targets: one for generating the atomic operations code using the codegen tool, and another for installing the generated code to the source tree location. The installation target depends on the generation target.\nSOURCE: https://github.com/nvidia/cccl/blob/main/libcudacxx/codegen/CMakeLists.txt#2025-04-23_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nadd_custom_target(\n    libcudacxx.atomics.codegen\n    COMMAND codegen \"${atomic_generated_output}\"\n    BYPRODUCTS \"${atomic_generated_output}\"\n)\n\nadd_custom_target(\n    libcudacxx.atomics.codegen.install\n    COMMAND ${CMAKE_COMMAND} -E copy \"${atomic_generated_output}\" \"${atomic_install_location}/cuda_ptx_generated.h\"\n    DEPENDS libcudacxx.atomics.codegen\n    BYPRODUCTS \"${atomic_install_location}/cuda_ptx_generated.h\"\n)\n```\n\n----------------------------------------\n\nTITLE: Generating CUB Variant Plots in Bash\nDESCRIPTION: Demonstrates how to generate variant plots for specific tuning variants to compare performance distributions across runtime workloads.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/tuning.rst#2025-04-23_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n$ ../benchmarks/scripts/analyze.py -R=\".*radix_sort.keys.*\" --variants-ratio='ipt_18.tpb_288' <path-to-databases>/*.db\n```\n\n----------------------------------------\n\nTITLE: Conditional Inclusion of CUDAX Subdirectory\nDESCRIPTION: Handles including the subdirectory CMake file when CUDAX is not enabled and exits early from processing the rest of the configuration.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cudax/CMakeLists.txt#2025-04-23_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif (NOT CCCL_ENABLE_CUDAX)\n  include(cmake/cudaxAddSubdir.cmake)\n  return()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Declaring multimem_ld_reduce with Relaxed/GPU Scope and OR Operation in CUDA PTX\nDESCRIPTION: Template function declaration for multimem.ld_reduce with relaxed memory semantics, GPU scope, and OR operation on 64-bit values in global memory. For PTX ISA 8.1 on SM_90 architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_63\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.b64 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .or }\ntemplate <typename B64, enable_if_t<sizeof(B64) == 8, bool> = true, cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline B64 multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_or_op_t,\n  const B64* addr);\n```\n\n----------------------------------------\n\nTITLE: Including CUDASTF Header and Namespace in C++\nDESCRIPTION: Shows how to include the main CUDASTF header and use its namespace in a C++ application.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n#include <cuda/experimental/stf.cuh>\n\nusing cuda::experimental::stf;\n```\n\n----------------------------------------\n\nTITLE: Tensormap Copy Fence Proxy - CTA Scope\nDESCRIPTION: Template function for tensormap copy operation with release semantics and CTA (Cooperative Thread Array) scope synchronization. Handles aligned memory transfers between global and shared memory.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tensormap_cp_fenceproxy.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\n// tensormap.cp_fenceproxy.global.shared::cta.tensormap::generic.sem.scope.sync.aligned  [dst], [src], size; // PTX ISA 83, SM_90\n// .sem       = { .release }\n// .scope     = { .cta, .cluster, .gpu, .sys }\ntemplate <int N32, cuda::ptx::dot_scope Scope>\n__device__ static inline void tensormap_cp_fenceproxy(\n  cuda::ptx::sem_release_t,\n  cuda::ptx::scope_t<Scope> scope,\n  void* dst,\n  const void* src,\n  cuda::ptx::n32_t<N32> size);\n```\n\n----------------------------------------\n\nTITLE: Storing 16x256b Data with tcgen05 in CUDA (32 values)\nDESCRIPTION: This function performs a 16x256b store operation using tcgen05. It takes a 32-bit address and an array of 32 B32 values as input. The function is constrained to B32 types of 4 bytes.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_st.rst#2025-04-23_snippet_11\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.st.sync.aligned.16x256b.x8.b32 [taddr], values; // PTX ISA 86, SM_100a, SM_101a\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline void tcgen05_st_16x256b(\n  uint32_t taddr,\n  const B32 (&values)[32]);\n```\n\n----------------------------------------\n\nTITLE: Implementing MMA Workspace Operations with I8 Data Type in CUDA\nDESCRIPTION: Template function for tensor compute generation 5 matrix multiplication with workspaces using I8 data type. It operates on tensor memory, descriptors, and supports zero column masking. Targets PTX ISA 86 on SM_100a and SM_101a architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_39\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.ws.cta_group.kind.collector::b2::use [d_tmem], [a_tmem], b_desc, idesc, enable_input_d, zero_column_mask_desc; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1 }\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_tmem_a_collector_b2_use(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint32_t a_tmem,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d,\n  uint64_t zero_column_mask_desc);\n```\n\n----------------------------------------\n\nTITLE: Adding CCCL Package with CPM\nDESCRIPTION: Uses CPM to clone CCCL from GitHub and make its exported targets available, with unstable features enabled to access cudax.\nSOURCE: https://github.com/nvidia/cccl/blob/main/examples/cudax/CMakeLists.txt#2025-04-23_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nCPMAddPackage(\n  NAME CCCL\n  GIT_REPOSITORY \"${CCCL_REPOSITORY}\"\n  GIT_TAG ${CCCL_TAG}\n  GIT_SHALLOW ON\n  # The following is required to make the `CCCL::cudax` target available:\n  OPTIONS \"CCCL_ENABLE_UNSTABLE ON\"\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing multimem.red.relaxed.sys.global.max.s64 Operation in CUDA PTX\nDESCRIPTION: Template function declaration for performing a 64-bit signed integer max reduction with relaxed semantics and system-wide scope in global memory. This function is available for SM_90 and requires PTX ISA 8.1 or higher.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_red.rst#2025-04-23_snippet_13\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.red.sem.scope.global.op.s64 [addr], val; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .release }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .max }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline void multimem_red(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_max_t,\n  int64_t* addr,\n  int64_t val);\n```\n\n----------------------------------------\n\nTITLE: Configuring Common Test Compiler Flags\nDESCRIPTION: Sets up common compiler flags for testing libcu++, enabling assertions, optional references, and disabling dialect deprecation warnings.\nSOURCE: https://github.com/nvidia/cccl/blob/main/libcudacxx/test/libcudacxx/CMakeLists.txt#2025-04-23_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\n# enable exceptions and assertions in tests\nstring(APPEND LIBCUDACXX_TEST_COMPILER_FLAGS \" -DCCCL_ENABLE_ASSERTIONS\")\n\n# enable optional<T&>\nstring(APPEND LIBCUDACXX_TEST_COMPILER_FLAGS \" -DCCCL_ENABLE_OPTIONAL_REF\")\n\n# Disable dialect deprecation\nstring(APPEND LIBCUDACXX_TEST_COMPILER_FLAGS \" -DCCCL_IGNORE_DEPRECATED_CPP_DIALECT\")\nstring(APPEND LIBCUDACXX_TEST_COMPILER_FLAGS \" -DLIBCUDACXX_IGNORE_DEPRECATED_ABI\")\n\nif (NOT MSVC AND NOT ${CMAKE_CUDA_COMPILER_ID} STREQUAL \"Clang\")\n  set(LIBCUDACXX_WARNING_LEVEL \"--compiler-options=-Wall --compiler-options=-Wextra\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Templated Matrix Multiply Accumulate with Collector B2 Discard - Zero Column Mask\nDESCRIPTION: Device function template for tensor core matrix multiply accumulate (MMA) operations with discard collector strategy. This variation supports zero column masking and processes matrices of various data types through template specialization.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_54\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.ws.cta_group.kind.collector::b2::discard [d_tmem], a_desc, b_desc, idesc, enable_input_d, zero_column_mask_desc; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1 }\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_collector_b2_discard(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d,\n  uint64_t zero_column_mask_desc);\n```\n\n----------------------------------------\n\nTITLE: Backporting C++20 cuda::std::is_constant_evaluated to C++11 (C++)\nDESCRIPTION: Feature #76 added in libcu++ 1.4.0. The C++20 feature `std::is_constant_evaluated` is backported to C++11 as `cuda::std::is_constant_evaluated`. Contribution by Jake Hemstad and Paul Taylor.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/releases/changelog.rst#2025-04-23_snippet_15\n\nLANGUAGE: cpp\nCODE:\n```\ncuda::std::is_constant_evaluated\n```\n\n----------------------------------------\n\nTITLE: Adding Source File Check Test in CMake - CMake - CMakeLists\nDESCRIPTION: This snippet defines a CMake test using add_test that runs the check_source_files.cmake script to perform static analysis on the Thrust source code. It passes relevant directory paths as parameters so that the script can access and analyze the required files. Requires CMake and assumes the existence of check_source_files.cmake. Inputs are the source directory; the output is the result of the check, and any detected issues. It is designed to be run as part of CMake's testing suite post-configuration or during CI.\nSOURCE: https://github.com/nvidia/cccl/blob/main/thrust/testing/cmake/CMakeLists.txt#2025-04-23_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_test(\n  NAME thrust.test.cmake.check_source_files\n  COMMAND\n    \"${CMAKE_COMMAND}\"\n      -D \"Thrust_SOURCE_DIR=${Thrust_SOURCE_DIR}\"\n      -P \"${CMAKE_CURRENT_LIST_DIR}/check_source_files.cmake\"\n)\n```\n\n----------------------------------------\n\nTITLE: Building CUDAX Compiler Targets and Target List\nDESCRIPTION: Calls the functions to build compiler targets and the target list for CUDAX, which were defined in the previously included CMake files.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cudax/CMakeLists.txt#2025-04-23_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\ncudax_build_compiler_targets()\ncudax_build_target_list()\n```\n\n----------------------------------------\n\nTITLE: Adding STF Unittested Header in CMake\nDESCRIPTION: Defines a function to add an STF unittested header executable and register it with CTest. It generates a test source file from a template and configures the target.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cudax/test/stf/CMakeLists.txt#2025-04-23_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(cudax_add_stf_unittest_header target_name_var source cn_target)\n  cudax_get_target_property(config_dialect ${cn_target} DIALECT)\n  cudax_get_target_property(config_prefix ${cn_target} PREFIX)\n\n  # ... (function implementation)\n\n  add_test(NAME ${test_target} COMMAND ${test_target})\n\n  set(${target_name_var} ${test_target} PARENT_SCOPE)\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Shared Memory Barrier Arrival Template (PTX SM_80)\nDESCRIPTION: Template function declaration for performing barrier arrival operations without completion on shared memory. Takes a barrier state address and arrival count parameter. Targets PTX ISA 7.0 and SM_80 architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/mbarrier_arrive_no_complete.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\n// mbarrier.arrive.noComplete.shared.b64                       state,  [addr], count;    // 5.  PTX ISA 70, SM_80\ntemplate <typename = void>\n__device__ static inline uint64_t mbarrier_arrive_no_complete(\n  uint64_t* addr,\n  const uint32_t& count);\n```\n\n----------------------------------------\n\nTITLE: Async Bulk Tensor Gather with Cluster Multicast\nDESCRIPTION: Template function for asynchronous bulk tensor gathering with cluster-wide multicast support. Operates between shared cluster memory and global memory with CTA mask specification.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_async_bulk_tensor_gather_scatter.rst#2025-04-23_snippet_2\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline void cp_async_bulk_tensor_tile_gather4(\n  cuda::ptx::space_cluster_t,\n  cuda::ptx::space_global_t,\n  void* dstMem,\n  const void* tensorMap,\n  const int32_t (&tensorCoords)[5],\n  uint64_t* smem_bar,\n  const uint16_t& ctaMask);\n```\n\n----------------------------------------\n\nTITLE: Installing CUB Headers with CMake\nDESCRIPTION: Installs the CUB header files. It copies the entire `cub` directory located at `${CUB_SOURCE_DIR}/cub` into the destination directory specified by `${_dest_incl_dir}`. Because the source path lacks a trailing slash, a `cub` directory is created within the destination.\nSOURCE: https://github.com/nvidia/cccl/blob/main/python/cuda_cccl/CMakeLists.txt#2025-04-23_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\n# No end slash: create ${_dest_inc_dir}/cub\ninstall(\n    DIRECTORY ${CUB_SOURCE_DIR}/cub\n    DESTINATION ${_dest_incl_dir}\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Header Testing and CTest Configuration\nDESCRIPTION: Conditionally includes header testing setup and enables CTest for testing and examples, setting up the testing infrastructure if enabled.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cub/CMakeLists.txt#2025-04-23_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\nif (CUB_ENABLE_HEADER_TESTING)\n  include(cmake/CubHeaderTesting.cmake)\nendif()\n\n# Both testing and examples use ctest\nif (CUB_ENABLE_TESTING OR CUB_ENABLE_EXAMPLES)\n  include(CTest)\n  enable_testing()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining CUDA Device Function Template tcgen05_mma_ws_tmem_a_collector_b0_use\nDESCRIPTION: Defines a templated CUDA `__device__` inline function `tcgen05_mma_ws_tmem_a_collector_b0_use` for tensor core MMA operations (use stage) within a CTA group 1. It takes shared memory pointers/descriptors for operands D and A (`d_tmem`, `a_tmem`), a descriptor for B (`b_desc`), an index descriptor (`idesc`), and an enable flag (`enable_input_d`). The template parameter `Kind` specifies the data type (f16, tf32, f8f6f4, i8). Requires PTX ISA 86 and targets SM_100a/SM_101a architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_8\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.ws.cta_group.kind.collector::b0::use [d_tmem], [a_tmem], b_desc, idesc, enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1 }\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_tmem_a_collector_b0_use(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint32_t a_tmem,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Implementing CUDA PTX Multimemory OR Reduction Template Function\nDESCRIPTION: Template function for 32-bit OR reduction operations with configurable memory semantics and scope. This function is used for atomic OR operations in global memory with different synchronization scopes.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_red.rst#2025-04-23_snippet_34\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.red.sem.scope.global.op.b32 [addr], val; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .release }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .or }\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true, cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline void multimem_red(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_or_op_t,\n  B32* addr,\n  B32 val);\n```\n\n----------------------------------------\n\nTITLE: Load Synchronization Handler for Thread Block\nDESCRIPTION: Device function providing synchronized load operations across thread blocks. Requires PTX ISA 86 and is compatible with SM_100a and SM_101a architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_wait.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.wait::ld.sync.aligned; // PTX ISA 86, SM_100a, SM_101a\ntemplate <typename = void>\n__device__ static inline void tcgen05_wait_ld();\n```\n\n----------------------------------------\n\nTITLE: Acquire Memory Barrier Try Wait with Suspend Hint\nDESCRIPTION: Memory barrier try_wait with acquire semantics and suspend time hint, supporting both CTA and cluster scopes.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/mbarrier_try_wait.rst#2025-04-23_snippet_3\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_scope Scope>\n__device__ static inline bool mbarrier_try_wait(\n  cuda::ptx::sem_acquire_t,\n  cuda::ptx::scope_t<Scope> scope,\n  uint64_t* addr,\n  const uint64_t& state,\n  const uint32_t& suspendTimeHint);\n```\n\n----------------------------------------\n\nTITLE: Detecting Catch2 Tests\nDESCRIPTION: Helper function that determines if a test source file is a Catch2-based test by checking if the filename contains 'catch2_test_'. It sets the provided result variable to TRUE or FALSE accordingly.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cub/test/CMakeLists.txt#2025-04-23_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\n## _cub_is_catch2_test\n#\n# If the test_src contains the substring \"catch2_test_\", `result_var` will\n# be set to TRUE.\nfunction(_cub_is_catch2_test result_var test_src)\n  string(FIND \"${test_src}\" \"catch2_test_\" idx)\n  if (idx EQUAL -1)\n    set(${result_var} FALSE PARENT_SCOPE)\n  else()\n    set(${result_var} TRUE PARENT_SCOPE)\n  endif()\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: CUB Documentation Structure in RST\nDESCRIPTION: RST directive defining the documentation structure with hidden toctree entries for different module levels.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/modules.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :hidden:\n   :maxdepth: 2\n\n   thread_level\n   warp_wide\n   block_wide\n   device_wide\n```\n\n----------------------------------------\n\nTITLE: Implementing tcgen05 MMA tmem_a_collector_b3_discard PTX Operation with zero_column_mask\nDESCRIPTION: Template function for tensor core matrix multiply accumulate operation with tensor memory for A input and B3 collector with discard option and zero column masking. Supports multiple data types.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_75\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.ws.cta_group.kind.collector::b3::discard [d_tmem], [a_tmem], b_desc, idesc, enable_input_d, zero_column_mask_desc; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1 }\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_tmem_a_collector_b3_discard(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint32_t a_tmem,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d,\n  uint64_t zero_column_mask_desc);\n```\n\n----------------------------------------\n\nTITLE: Implementing multimem.ld_reduce AND operation for 64-bit data in CUDA\nDESCRIPTION: Template implementation for multimem.ld_reduce operation with AND reduction for 64-bit data. The template supports various memory semantics (relaxed, acquire) and scopes (cta, cluster, gpu, sys) for global memory operations.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_58\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.b64 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .and }\ntemplate <typename B64, enable_if_t<sizeof(B64) == 8, bool> = true, cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline B64 multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_and_op_t,\n  const B64* addr);\n```\n\n----------------------------------------\n\nTITLE: Implementing Fence Proxy Async for Specific Spaces in CUDA\nDESCRIPTION: Defines a template device function for fence proxy asynchronous operation in specific spaces (global, shared::cluster, shared::cta). This function is for PTX ISA 80 and SM_90 and takes a space parameter.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/fence_proxy_async.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n// fence.proxy.async.space; // 6. PTX ISA 80, SM_90\n// .space     = { .global, .shared::cluster, .shared::cta }\ntemplate <cuda::ptx::dot_space Space>\n__device__ static inline void fence_proxy_async(\n  cuda::ptx::space_t<Space> space);\n```\n\n----------------------------------------\n\nTITLE: Copying 1D Tensor with CTA Group 1 in CUDA\nDESCRIPTION: This function is similar to the previous one but adds support for CTA (Cooperative Thread Array) group 1. It allows for more fine-grained control over the copy operation within the cluster.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_async_bulk_tensor_multicast.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void cp_async_bulk_tensor(\n  cuda::ptx::space_cluster_t,\n  cuda::ptx::space_global_t,\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  void* dstMem,\n  const void* tensorMap,\n  const int32_t (&tensorCoords)[1],\n  uint64_t* smem_bar,\n  const uint16_t& ctaMask);\n```\n\n----------------------------------------\n\nTITLE: Adding Memory Bandwidth Metrics to NVBench State\nDESCRIPTION: Configures the benchmark to report memory bandwidth by specifying element count and memory operations. This allows NVBench to calculate achieved memory throughput.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/tuning.rst#2025-04-23_snippet_5\n\nLANGUAGE: c++\nCODE:\n```\nstate.add_element_count(elements);\nstate.add_global_memory_reads<T>(elements, \"Size\");\nstate.add_global_memory_writes<T>(1);\n```\n\n----------------------------------------\n\nTITLE: Linux Host Compiler Selection\nDESCRIPTION: Make command demonstrating how to override the default host compiler selection.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cudax/test/stf/cuda-samples/3_CUDA_Features/jacobiCudaGraphs/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ make HOST_COMPILER=g++\n```\n\n----------------------------------------\n\nTITLE: Setting CUDA Architecture Defaults\nDESCRIPTION: Configures CUDA to target the GPU architecture of the current system if not explicitly specified.\nSOURCE: https://github.com/nvidia/cccl/blob/main/examples/cudax_stf/CMakeLists.txt#2025-04-23_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT DEFINED CMAKE_CUDA_ARCHITECTURES)\n  set(CMAKE_CUDA_ARCHITECTURES native)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Implementing MMA Operation with Block Scaling for mxf8f6f4 kind (Collector A Fill)\nDESCRIPTION: This CUDA device function template implements a matrix multiplication and accumulation operation with block scaling for the mxf8f6f4 kind. It uses a collector for A operand filling and supports CTA group sizes of 1 and 2.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma.rst#2025-04-23_snippet_20\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.cta_group.kind.block_scale.scale_vec::1X.collector::a::fill [d_tmem], a_desc, b_desc, idesc, [scale_A_tmem], [scale_B_tmem], enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .kind      = { .kind::mxf8f6f4 }\n// .cta_group = { .cta_group::1, .cta_group::2 }\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_mma_block_scale_vec_1x_collector_a_fill(\n  cuda::ptx::kind_mxf8f6f4_t,\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  uint32_t scale_A_tmem,\n  uint32_t scale_B_tmem,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Relaxed Memory Barrier Try Wait with Suspend Hint\nDESCRIPTION: Memory barrier try wait with relaxed semantics and suspend time hint, supporting both CTA and cluster scopes.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/mbarrier_try_wait_parity.rst#2025-04-23_snippet_5\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_scope Scope>\n__device__ static inline bool mbarrier_try_wait_parity(\n  cuda::ptx::sem_relaxed_t,\n  cuda::ptx::scope_t<Scope> scope,\n  uint64_t* addr,\n  const uint32_t& phaseParity,\n  const uint32_t& suspendTimeHint);\n```\n\n----------------------------------------\n\nTITLE: Including Specialized Test Subdirectories in CMake\nDESCRIPTION: Uses `add_subdirectory` to include CMake build logic from several specific subdirectories: `cmake`, `cpp`, `cuda`, and `omp`. Each of these subdirectories is expected to contain its own `CMakeLists.txt` file, which will likely define tests that are specialized for CMake functionality testing, the C++ backend, the CUDA backend, and the OMP backend, respectively.\nSOURCE: https://github.com/nvidia/cccl/blob/main/thrust/testing/CMakeLists.txt#2025-04-23_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\n# Add specialized tests:\nadd_subdirectory(cmake)\nadd_subdirectory(cpp)\nadd_subdirectory(cuda)\nadd_subdirectory(omp)\n```\n\n----------------------------------------\n\nTITLE: Templated Matrix Multiply Accumulate with Collector B2 Discard - Standard\nDESCRIPTION: Device function template for tensor core matrix multiply accumulate (MMA) operations with discard collector strategy. This standard variation omits zero column masking and works with various data types through template specialization.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_55\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.ws.cta_group.kind.collector::b2::discard [d_tmem], a_desc, b_desc, idesc, enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1 }\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_collector_b2_discard(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Running CUB Benchmarks with Custom Options (Bash)\nDESCRIPTION: Demonstrates how to run CUB benchmarks with specific options for selecting benchmarks and restricting axis values. The results are stored in an SQLite database for persistence across runs.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/benchmarking.rst#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n../benchmarks/scripts/compare.py -o cccl_meta_bench1.db cccl_meta_bench2.db\n```\n\n----------------------------------------\n\nTITLE: Including CUB Build Configuration Files\nDESCRIPTION: Includes various CMake modules that configure compiler targets, build target lists, CUDA configuration, and CUB utilities to set up the build environment.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cub/CMakeLists.txt#2025-04-23_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\ninclude(cmake/CubBuildCompilerTargets.cmake)\ninclude(cmake/CubBuildTargetList.cmake)\ninclude(cmake/CubCudaConfig.cmake)\ninclude(cmake/CubUtilities.cmake)\n```\n\n----------------------------------------\n\nTITLE: Validating CUDASTF Bounds Check Debug Build Requirement\nDESCRIPTION: Checks if bounds checking is enabled for CUDASTF, and ensures that the build type is set to Debug or RelWithDebInfo as required for this feature.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cudax/CMakeLists.txt#2025-04-23_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nif (cudax_ENABLE_CUDASTF_BOUNDSCHECK AND\n    NOT CMAKE_BUILD_TYPE MATCHES \"Debug\" AND NOT CMAKE_BUILD_TYPE MATCHES \"RelWithDebInfo\")\n  message(FATAL_ERROR \"cudax_ENABLE_CUDASTF_BOUNDSCHECK requires a Debug build.\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Command for JIT Template Source Generation\nDESCRIPTION: Defines a custom command that concatenates pre/post template files with the generated template header to create the final template source file.\nSOURCE: https://github.com/nvidia/cccl/blob/main/c/parallel/src/jit_templates/CMakeLists.txt#2025-04-23_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nadd_custom_command(\n    OUTPUT \"${jit_template_src}\"\n    DEPENDS\n      \"${jit_template}\"\n      \"${CMAKE_CURRENT_LIST_DIR}/template_pre.h.in\"\n      \"${CMAKE_CURRENT_LIST_DIR}/template_post.h.in\"\n    VERBATIM\n    COMMAND \"${CMAKE_COMMAND}\" -E cat --\n        \"${CMAKE_CURRENT_LIST_DIR}/template_pre.h.in\"\n        \"${jit_template}\"\n        \"${CMAKE_CURRENT_LIST_DIR}/template_post.h.in\"\n        > \"${jit_template_src}\"\n)\nset_source_files_properties(\"${jit_template_src}\" PROPERTIES GENERATED TRUE)\n```\n\n----------------------------------------\n\nTITLE: Declaring multimem_ld_reduce with Relaxed/Cluster Scope and OR Operation in CUDA PTX\nDESCRIPTION: Template function declaration for multimem.ld_reduce with relaxed memory semantics, cluster scope, and OR operation on 64-bit values in global memory. Designed for PTX ISA 8.1 on SM_90 architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_62\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.b64 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .or }\ntemplate <typename B64, enable_if_t<sizeof(B64) == 8, bool> = true, cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline B64 multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_or_op_t,\n  const B64* addr);\n```\n\n----------------------------------------\n\nTITLE: Copying 2D Tensor from Global to Shared Cluster Memory in CUDA\nDESCRIPTION: This function performs an asynchronous bulk copy of a 2D tensor from global memory to shared cluster memory. It uses a memory barrier for synchronization and supports multicast operations across the cluster.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_async_bulk_tensor_multicast.rst#2025-04-23_snippet_2\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline void cp_async_bulk_tensor(\n  cuda::ptx::space_cluster_t,\n  cuda::ptx::space_global_t,\n  void* dstMem,\n  const void* tensorMap,\n  const int32_t (&tensorCoords)[2],\n  uint64_t* smem_bar,\n  const uint16_t& ctaMask);\n```\n\n----------------------------------------\n\nTITLE: Including Parallel Directory in CMake Build\nDESCRIPTION: CMake command to include the parallel subdirectory in the build process, making its contents part of the project compilation.\nSOURCE: https://github.com/nvidia/cccl/blob/main/c/CMakeLists.txt#2025-04-23_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(parallel)\n```\n\n----------------------------------------\n\nTITLE: Defining tcgen05_mma_tmem_a Variant with A Tile in Shared Memory (CUDA C++)\nDESCRIPTION: Implements tcgen05_mma_tmem_a as a device function template, parameterized by N32 and data kind, for cases where the A input tile is sourced from shared memory (a_tmem). Used primarily with CTA group 1, and supports kind f16 or tf32. The disable_output_lane parameter allows selective output, while scale_input_d enables scaling of D input. Proper descriptors and lane configuration arrays must be provided for correct operation.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma.rst#2025-04-23_snippet_6\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.cta_group.kind [d_tmem], [a_tmem], b_desc, idesc, disable_output_lane, enable_input_d, scale_input_d; // PTX ISA 86, SM_100a\n// .kind      = { .kind::f16, .kind::tf32 }\n// .cta_group = { .cta_group::1 }\ntemplate <int N32, cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_tmem_a(\n  cuda::ptx::kind_t<Kind> kind,\n  cuda::ptx::cta_group_1_t,\n  uint32_t d_tmem,\n  uint32_t a_tmem,\n  uint64_t b_desc,\n  uint32_t idesc,\n  const uint32_t (\\u0026disable_output_lane)[4],\n  bool enable_input_d,\n  cuda::ptx::n32_t<N32> scale_input_d);\n```\n\n----------------------------------------\n\nTITLE: Illustrating Potential ABI Mismatch without Linker Error in CUDA C++\nDESCRIPTION: This C++ snippet defines a struct `sum` containing a `cuda::atomic<float>` and declares a function `negate` operating on this struct. It serves as an example discussed in the surrounding text where mixing translation units compiled with different libcu++ ABI versions might not trigger a link-time error (because the user-defined struct `sum` itself is not in an ABI namespace), but could still lead to an ill-formed program with undefined behavior at runtime due to potential layout differences of the embedded `cuda::atomic` type between the different ABI versions, violating the One Definition Rule.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/releases/versioning.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\nstruct sum { cuda::atomic<float> };\nvoid negate(sum&);\n```\n\n----------------------------------------\n\nTITLE: Implementing multimem reduction bitwise AND operations in CUDA PTX\nDESCRIPTION: Template function declaration for multimem reduction operations performing atomic bitwise AND on 32-bit values in global memory. Supports different semantic models and memory scopes as specified in PTX ISA 81 for SM_90.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_red.rst#2025-04-23_snippet_32\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.red.sem.scope.global.op.b32 [addr], val; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .release }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .and }\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true, cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline void multimem_red(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_and_op_t,\n  B32* addr,\n  B32 val);\n```\n\n----------------------------------------\n\nTITLE: Finding Required CCCL Dependencies with CMake\nDESCRIPTION: Uses the `find_package` command to locate the necessary CUB, Thrust, and libcudacxx libraries. The `REQUIRED` keyword ensures that CMake will halt with an error if any of these packages cannot be found. Successful execution typically sets variables like `<PackageName>_FOUND` and `<PackageName>_SOURCE_DIR`.\nSOURCE: https://github.com/nvidia/cccl/blob/main/python/cuda_cccl/CMakeLists.txt#2025-04-23_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nfind_package(CUB REQUIRED)\nfind_package(Thrust REQUIRED)\nfind_package(libcudacxx REQUIRED)\n```\n\n----------------------------------------\n\nTITLE: Acquire Memory Barrier Try Wait (CTA/Cluster)\nDESCRIPTION: Memory barrier try wait with acquire semantics, supporting both CTA and cluster scopes.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/mbarrier_try_wait_parity.rst#2025-04-23_snippet_2\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_scope Scope>\n__device__ static inline bool mbarrier_try_wait_parity(\n  cuda::ptx::sem_acquire_t,\n  cuda::ptx::scope_t<Scope> scope,\n  uint64_t* addr,\n  const uint32_t& phaseParity);\n```\n\n----------------------------------------\n\nTITLE: Adding CUB Example Subdirectories\nDESCRIPTION: Adds block and device subdirectories to the build system for processing additional example files.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cub/examples/CMakeLists.txt#2025-04-23_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(block)\nadd_subdirectory(device)\n```\n\n----------------------------------------\n\nTITLE: Defining Documentation Structure with ReStructuredText for Thrust Containers\nDESCRIPTION: Sets up a toctree (table of contents tree) structure for navigating container-related documentation in the Thrust module. Includes references to host_vector, device_vector, and container typedefs documentation.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/api_docs/containers.rst#2025-04-23_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. _thrust-module-api-containers:\n\nContainers\n===========\n\n.. toctree::\n   :glob:\n   :maxdepth: 2\n\n   ${repo_docs_api_path}/*host__vector*\n   ${repo_docs_api_path}/*device__vector*\n   ${repo_docs_api_path}/typedef_group__containers*\n```\n\n----------------------------------------\n\nTITLE: Finding CUDA Toolkit and Setting Include Directories\nDESCRIPTION: Locates the CUDA Toolkit installation and retrieves the CUDA include directory path from the cudart target.\nSOURCE: https://github.com/nvidia/cccl/blob/main/libcudacxx/test/libcudacxx/CMakeLists.txt#2025-04-23_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nfind_package(CUDAToolkit REQUIRED)\nget_target_property(CUDA_INCLUDE_DIR CUDA::cudart INTERFACE_INCLUDE_DIRECTORIES)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Floating Point Type Registration for CUB and libcu++\nDESCRIPTION: Shows how to properly register a custom floating point type with CUB and libcu++. This includes specializing the cuda traits, std::numeric_limits, and CUB's NumericTraits to enable the type to work with CUB's algorithms.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cccl/3.0_migration_guide.rst#2025-04-23_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\ntemplate <>\ninline constexpr bool ::cuda::is_floating_point_v<my_half> = true;\n\ntemplate <>\nclass ::cuda::std::numeric_limits<my_half> {\npublic:\n  static constexpr bool is_specialized = true;\n  static __host__ __device__ my_half max()    { return /* TODO */; }\n  static __host__ __device__ my_half min()    { return /* TODO */; }\n  static __host__ __device__ my_half lowest() { return /* TODO */; }\n};\n\ntemplate <>\nstruct CUB_NS_QUALIFIER::NumericTraits<my_half> : BaseTraits<FLOATING_POINT, true, uint16_t, my_half> {};\n```\n\n----------------------------------------\n\nTITLE: CTA Group Memory Deallocation (Group 1)\nDESCRIPTION: Template function for deallocating memory in CTA group 1. Takes target address and number of columns as parameters. Compatible with PTX ISA 86 and SM_100a/101a architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_alloc.rst#2025-04-23_snippet_2\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.dealloc.cta_group.sync.aligned.b32 taddr, nCols; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1, .cta_group::2 }\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_dealloc(\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t taddr,\n  const uint32_t& nCols);\n```\n\n----------------------------------------\n\nTITLE: Installing pre-commit via pip for CCCL Project\nDESCRIPTION: This snippet demonstrates how to install pre-commit using pip for the CCCL project.\nSOURCE: https://github.com/nvidia/cccl/blob/main/CONTRIBUTING.md#2025-04-23_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npip install pre-commit\n```\n\n----------------------------------------\n\nTITLE: Configuring C/C++ Parallel Test Targets in CMake\nDESCRIPTION: CMake function that creates and configures test targets for C/C++ parallel tests. Sets up executable targets with CUDA dependencies, compile definitions for various include paths, and registers the test with CTest.\nSOURCE: https://github.com/nvidia/cccl/blob/main/c/parallel/test/CMakeLists.txt#2025-04-23_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfunction(cccl_c_parallel_add_test target_name_var source)\n  string(REGEX REPLACE \"test_([^.]*)\" \"cccl.c.parallel.test.\\\\1\" target_name \"${source}\")\n  set(target_name_var ${target_name} PARENT_SCOPE)\n\n  add_executable(${target_name} \"${source}\")\n  cccl_configure_target(${target_name} DIALECT 20)\n\n  target_link_libraries(${target_name} PRIVATE\n    cccl.c.parallel\n    CUDA::cudart\n    CUDA::nvrtc\n    cccl.c2h.main\n    cccl.compiler_interface_cpp20\n  )\n\n  target_compile_definitions(${target_name} PRIVATE\n    TEST_CUB_PATH=\"-I${CCCL_SOURCE_DIR}/cub\"\n    TEST_THRUST_PATH=\"-I${CCCL_SOURCE_DIR}/thrust\"\n    TEST_LIBCUDACXX_PATH=\"-I${CCCL_SOURCE_DIR}/libcudacxx/include\"\n    TEST_CTK_PATH=\"-I${CUDAToolkit_INCLUDE_DIRS}\"\n  )\n\n  add_test(NAME ${target_name} COMMAND ${target_name})\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Store 64-bit Data with L1 No Allocate and L2 Cache Hint in CUDA\nDESCRIPTION: This function stores 64-bit data to global memory with L1 cache no allocate policy and L2 cache hint. It requires SM_80 or later architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/st.rst#2025-04-23_snippet_18\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename B64, enable_if_t<sizeof(B64) == 8, bool> = true>\n__device__ static inline void st_L1_no_allocate_L2_cache_hint(\n  cuda::ptx::space_global_t,\n  B64* addr,\n  B64 src,\n  uint64_t cache_policy);\n```\n\n----------------------------------------\n\nTITLE: Generating Thrust CUDA Test Targets with RDC Variations in CMake\nDESCRIPTION: This CMake script automates the creation of Thrust test targets for CUDA source files found in the current directory. It iterates through a predefined list of `THRUST_TARGETS`, checks if the target's device property is 'CUDA', and for each matching target and source file (`*.cu`, `*.cpp`), generates two test targets using `thrust_add_test`. The first target (`.cdp_0`) is configured with Relocatable Device Code (RDC) explicitly turned OFF using `thrust_configure_cuda_target`. The second target (`.cdp_1`) is created only if `THRUST_ENABLE_RDC_TESTS` is true and is configured with RDC ON. This setup facilitates testing both the serial fallback path (RDC OFF) and the CUDA Dynamic Parallelism (CDP) kernel launch path (RDC ON).\nSOURCE: https://github.com/nvidia/cccl/blob/main/thrust/testing/cuda/CMakeLists.txt#2025-04-23_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB test_srcs\n  RELATIVE \"${CMAKE_CURRENT_LIST_DIR}}\"\n  CONFIGURE_DEPENDS\n  *.cu *.cpp\n)\n\nforeach(thrust_target IN LISTS THRUST_TARGETS)\n  thrust_get_target_property(config_device ${thrust_target} DEVICE)\n  if (NOT config_device STREQUAL \"CUDA\")\n    continue()\n  endif()\n\n  foreach(test_src IN LISTS test_srcs)\n    get_filename_component(test_name \"${test_src}\" NAME_WLE)\n    string(PREPEND test_name \"cuda.\")\n\n    # Create two targets, one with RDC enabled, the other without. This tests\n    # both device-side behaviors -- the CDP kernel launch with RDC, and the\n    # serial fallback path without RDC.\n    thrust_add_test(seq_test_target ${test_name}.cdp_0 \"${test_src}\" ${thrust_target})\n    thrust_configure_cuda_target(${seq_test_target} RDC OFF)\n\n    if (THRUST_ENABLE_RDC_TESTS)\n      thrust_add_test(cdp_test_target ${test_name}.cdp_1 \"${test_src}\" ${thrust_target})\n      thrust_configure_cuda_target(${cdp_test_target} RDC ON)\n    endif()\n  endforeach()\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Accessing Shared Memory Size Information in CUDA\nDESCRIPTION: These functions retrieve shared memory size information. They use inline PTX instructions to access the special registers.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/get_sreg.rst#2025-04-23_snippet_13\n\nLANGUAGE: cuda\nCODE:\n```\n// mov.u32 sreg_value, %%total_smem_size; // PTX ISA 41, SM_35\ntemplate <typename = void>\n__device__ static inline uint32_t get_sreg_total_smem_size();\n```\n\nLANGUAGE: cuda\nCODE:\n```\n// mov.u32 sreg_value, %%aggr_smem_size; // PTX ISA 81, SM_90\ntemplate <typename = void>\n__device__ static inline uint32_t get_sreg_aggr_smem_size();\n```\n\nLANGUAGE: cuda\nCODE:\n```\n// mov.u32 sreg_value, %%dynamic_smem_size; // PTX ISA 41, SM_35\ntemplate <typename = void>\n__device__ static inline uint32_t get_sreg_dynamic_smem_size();\n```\n\n----------------------------------------\n\nTITLE: Creating Launch Tests in CMake\nDESCRIPTION: Sets up tests for the CUDA kernel launch functionality, including basic functionality tests and configuration tests. These verify the proper operation of CUDA kernel launching mechanisms provided by CCCL.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cudax/test/CMakeLists.txt#2025-04-23_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\n  cudax_add_catch2_test(test_target launch ${cn_target}\n    launch/launch_smoke.cu\n    launch/configuration.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: Configuring CUDA Language Support\nDESCRIPTION: Creates an option to enable or disable CUDA language support, which is enabled by default, and enables the CUDA language when the option is ON.\nSOURCE: https://github.com/nvidia/cccl/blob/main/libcudacxx/CMakeLists.txt#2025-04-23_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\n# Configuration options.\noption(LIBCUDACXX_ENABLE_CUDA \"Enable the CUDA language support.\" ON)\nif (LIBCUDACXX_ENABLE_CUDA)\n  enable_language(CUDA)\nendif ()\n```\n\n----------------------------------------\n\nTITLE: Combining Multidimensional Spaces and Random Sequences in CUB Tests with C++\nDESCRIPTION: Demonstrates how to combine multidimensional configuration spaces with multiple random sequence generations in CUB tests.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/test_overview.rst#2025-04-23_snippet_6\n\nLANGUAGE: C++\nCODE:\n```\nusing block_sizes = c2h::enum_type_list<int, 128, 256>;\nusing types = c2h::type_list<std::uint8_t, std::int32_t>;\n\nC2H_TEST(\"SCOPE FACILITY works with CONDITION\", \"[FACILITY][SCOPE]\",\n         types, block_sizes)\n{\n  using type = typename c2h::get<0, TestType>;\n  constexpr int threads_per_block = c2h::get<1, TestType>::value;\n  // ...\n  c2h::device_vector<type> d_input(5);\n  c2h::gen(C2H_SEED(2), d_input);\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Basic CCCL Example Compilation Test\nDESCRIPTION: Sets up a basic CCCL example compilation test using previously defined build options. This validates that the basic example can be correctly compiled with the current CCCL configuration.\nSOURCE: https://github.com/nvidia/cccl/blob/main/examples/CMakeLists.txt#2025-04-23_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\ncccl_add_compile_test(test_name\n  cccl.example\n  basic\n  \"default\"\n  ${cmake_opts}\n  ${cmake_cpm_opts}\n)\n```\n\n----------------------------------------\n\nTITLE: Tensormap Copy Fence Proxy - System Scope\nDESCRIPTION: Template function for tensormap copy operation with release semantics and system scope synchronization. Handles aligned memory transfers between global and shared memory.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tensormap_cp_fenceproxy.rst#2025-04-23_snippet_3\n\nLANGUAGE: cuda\nCODE:\n```\n// tensormap.cp_fenceproxy.global.shared::cta.tensormap::generic.sem.scope.sync.aligned  [dst], [src], size; // PTX ISA 83, SM_90\n// .sem       = { .release }\n// .scope     = { .cta, .cluster, .gpu, .sys }\ntemplate <int N32, cuda::ptx::dot_scope Scope>\n__device__ static inline void tensormap_cp_fenceproxy(\n  cuda::ptx::sem_release_t,\n  cuda::ptx::scope_t<Scope> scope,\n  void* dst,\n  const void* src,\n  cuda::ptx::n32_t<N32> size);\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Bulk ADD Reduction (Signed 64-bit) in CUDA\nDESCRIPTION: Performs an asynchronous bulk ADD reduction operation from CTA-shared to cluster-shared memory for signed 64-bit integers. Uses a memory barrier for synchronization.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_reduce_async_bulk.rst#2025-04-23_snippet_12\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline void cp_reduce_async_bulk(\n  cuda::ptx::space_cluster_t,\n  cuda::ptx::space_shared_t,\n  cuda::ptx::op_add_t,\n  int64_t* dstMem,\n  const int64_t* srcMem,\n  uint32_t size,\n  uint64_t* rdsmem_bar);\n```\n\n----------------------------------------\n\nTITLE: Defining CUDA Binary Semaphore Template\nDESCRIPTION: Shows the namespace and template definition for cuda::binary_semaphore, which is an alias template for counting_semaphore with a max count of 1.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/synchronization_primitives/binary_semaphore.rst#2025-04-23_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nnamespace cuda {\n\ntemplate <cuda::thread_scope Scope>\nusing binary_semaphore = cuda::std::counting_semaphore<Scope, 1>;\n\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling <nv/target> for C and C++98 Compilation (C/C++)\nDESCRIPTION: Issue #177 fixed in libcu++ 1.6.0. Enables the `<nv/target>` header to build correctly when compiled under C and C++98 standards. Contribution by David Olsen.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/releases/changelog.rst#2025-04-23_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\n<nv/target>\n```\n\n----------------------------------------\n\nTITLE: Declaring CUDA PTX Multimem Reduction XOR Template with 64-bit Values\nDESCRIPTION: Template declaration for performing atomic XOR reduction operations on 64-bit values with different memory semantics and memory scopes. The template accepts semantics parameters (relaxed/release), scope parameters (cta/cluster/gpu/sys), and uses the PTX multimem.red instruction.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_red.rst#2025-04-23_snippet_39\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.red.sem.scope.global.op.b64 [addr], val; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .release }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .xor }\ntemplate <typename B64, enable_if_t<sizeof(B64) == 8, bool> = true, cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline void multimem_red(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_xor_op_t,\n  B64* addr,\n  B64 val);\n```\n\n----------------------------------------\n\nTITLE: Basic Memory Barrier Try Wait Parity Function\nDESCRIPTION: Basic implementation of memory barrier try wait with parity checking. Takes a memory address and phase parity as parameters.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/mbarrier_try_wait_parity.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline bool mbarrier_try_wait_parity(\n  uint64_t* addr,\n  const uint32_t& phaseParity);\n```\n\n----------------------------------------\n\nTITLE: Configuring Thrust Target from Cache Options\nDESCRIPTION: Creates a configurable Thrust target using the FROM_OPTIONS flag with customizable parameters for host system, device system, and dispatch type via CMake cache variables.\nSOURCE: https://github.com/nvidia/cccl/blob/main/lib/cmake/thrust/README.md#2025-04-23_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nthrust_create_target(Thrust FROM_OPTIONS\n  [HOST_OPTION <option name>]\n  [DEVICE_OPTION <option name>]\n  [DISPATCH_OPTION <option name>]\n  [HOST_OPTION_DOC <doc string>]\n  [DEVICE_OPTION_DOC <doc string>]\n  [DISPATCH_OPTION_DOC <doc string>]\n  [HOST <default host system name>]\n  [DEVICE <default device system name>]\n  [DISPATCH <default dispatch type>]\n  [ADVANCED]\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring reStructuredText toctree for Thrust System Backends\nDESCRIPTION: Sets up a documentation tree for Thrust system backends. This RST directive creates a table of contents that globs all system backend typedef documentation files, displaying them with a maxdepth of 1.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/api_docs/system/systems.rst#2025-04-23_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. toctree::\n   :glob:\n   :maxdepth: 1\n\n   ${repo_docs_api_path}/*typedef_group__system__backends*\n```\n\n----------------------------------------\n\nTITLE: Configuring NVBench Helper Library in CMake\nDESCRIPTION: Defines and configures the nvbench_helper library, setting up its source files, dependencies, and compiler properties.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cub/benchmarks/nvbench_helper/CMakeLists.txt#2025-04-23_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(nvbench_helper OBJECT nvbench_helper/nvbench_helper.cuh\n                                  nvbench_helper/nvbench_helper.cu)\n\ntarget_link_libraries(nvbench_helper PUBLIC CUB::CUB\n                                            Thrust::Thrust\n                                            CUB::libcudacxx\n                                            nvbench::nvbench\n                                     PRIVATE CUDA::curand)\n\ntarget_include_directories(nvbench_helper PUBLIC \"${CMAKE_CURRENT_LIST_DIR}/nvbench_helper\")\nset_target_properties(nvbench_helper PROPERTIES CUDA_STANDARD 17 CXX_STANDARD 17)\n```\n\n----------------------------------------\n\nTITLE: Implementing multimem.red.release.gpu.global.add.u32 Operation in CUDA PTX\nDESCRIPTION: Template function declaration for performing a 32-bit unsigned integer addition reduction with release semantics and GPU scope in global memory. This function is available for SM_90 and requires PTX ISA 8.1 or higher.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_red.rst#2025-04-23_snippet_24\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.red.sem.scope.global.op.u32 [addr], val; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .release }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .add }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline void multimem_red(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_add_t,\n  uint32_t* addr,\n  uint32_t val);\n```\n\n----------------------------------------\n\nTITLE: CTA Group Memory Deallocation (Group 2)\nDESCRIPTION: Template function for deallocating memory in CTA group 2. Takes target address and number of columns as parameters. Compatible with PTX ISA 86 and SM_100a/101a architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_alloc.rst#2025-04-23_snippet_3\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.dealloc.cta_group.sync.aligned.b32 taddr, nCols; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1, .cta_group::2 }\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_dealloc(\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t taddr,\n  const uint32_t& nCols);\n```\n\n----------------------------------------\n\nTITLE: Including C Standard Library Utilities Header in CUDA C++\nDESCRIPTION: Includes the `<cuda/std/cstdlib>` header, providing common utilities like memory management and numeric conversions. Available since CCCL 2.2.0 and CUDA Toolkit 12.3.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/standard_api/c_library.rst#2025-04-23_snippet_9\n\nLANGUAGE: C++\nCODE:\n```\n#include <cuda/std/cstdlib>\n```\n\n----------------------------------------\n\nTITLE: Defining Toctree for Thrust Copying Algorithms Documentation\nDESCRIPTION: ReStructuredText directive that defines the table of contents tree for the copying algorithms section of the Thrust documentation. It includes specific pages for gather and scatter operations, as well as auto-generated API documentation for copying functions.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/api_docs/algorithms/copying.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :glob:\n   :maxdepth: 1\n\n   copying/gather\n   copying/scatter\n   ${repo_docs_api_path}/*function_group__copying*\n```\n\n----------------------------------------\n\nTITLE: Setting Up libcudacxx Testing Infrastructure\nDESCRIPTION: Configures testing for libcudacxx, including finding Python interpreter, determining host and target triples, and setting up LIT arguments for test execution.\nSOURCE: https://github.com/nvidia/cccl/blob/main/libcudacxx/CMakeLists.txt#2025-04-23_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\noption(LIBCUDACXX_ENABLE_LIBCUDACXX_TESTS \"Enable libcu++ tests.\" ON)\nif (LIBCUDACXX_ENABLE_LIBCUDACXX_TESTS)\n  enable_testing()\n\n  find_package (Python COMPONENTS Interpreter)\n  if (NOT Python_Interpreter_FOUND)\n    message(FATAL_ERROR\n      \"Failed to find python interpreter, which is required for running tests and \"\n      \"building a libcu++ static library.\")\n  endif ()\n\n  # Determine the host triple to avoid invoking `${CXX} -dumpmachine`.\n  include(GetHostTriple)\n  get_host_triple(LLVM_INFERRED_HOST_TRIPLE)\n\n  set(LLVM_HOST_TRIPLE \"${LLVM_INFERRED_HOST_TRIPLE}\" CACHE STRING\n      \"Host on which LLVM binaries will run\")\n\n  # By default, we target the host, but this can be overridden at CMake\n  # invocation time.\n  set(LLVM_DEFAULT_TARGET_TRIPLE \"${LLVM_HOST_TRIPLE}\" CACHE STRING\n    \"Default target for which LLVM will generate code.\" )\n  set(TARGET_TRIPLE \"${LLVM_DEFAULT_TARGET_TRIPLE}\")\n  message(STATUS \"LLVM host triple: ${LLVM_HOST_TRIPLE}\")\n  message(STATUS \"LLVM default target triple: ${LLVM_DEFAULT_TARGET_TRIPLE}\")\n\n  set(LIT_EXTRA_ARGS \"\" CACHE STRING \"Use for additional options (e.g. -j12)\")\n  find_program(LLVM_DEFAULT_EXTERNAL_LIT lit)\n  set(LLVM_LIT_ARGS \"-sv ${LIT_EXTRA_ARGS}\")\n\n  set(LIBCUDACXX_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR})\n  add_subdirectory(test)\n```\n\n----------------------------------------\n\nTITLE: Defining TCGEN05 MMA Workspace TMEM A Collector B2 Fill Function Template with Zero Column Mask\nDESCRIPTION: Template function for TCGen05 MMA workspace operations using tmem for operand A with collector B2 fill mode. Takes d_tmem, a_tmem, b_desc, idesc, enable_input_d flag, and zero_column_mask_desc. Supports multiple data types via the Kind template parameter.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_34\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.ws.cta_group.kind.collector::b2::fill [d_tmem], [a_tmem], b_desc, idesc, enable_input_d, zero_column_mask_desc; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1 }\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_tmem_a_collector_b2_fill(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint32_t a_tmem,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d,\n  uint64_t zero_column_mask_desc);\n```\n\n----------------------------------------\n\nTITLE: Global Address Replacement in Shared Memory\nDESCRIPTION: Device function to replace global address in tensormap stored in shared memory. Takes a 64-bit value for the new address.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tensormap_replace.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename B64, enable_if_t<sizeof(B64) == 8, bool> = true>\n__device__ static inline void tensormap_replace_global_address(\n  cuda::ptx::space_shared_t,\n  void* tm_addr,\n  B64 new_val);\n```\n\n----------------------------------------\n\nTITLE: Configuring MSVC-Specific Compiler Options\nDESCRIPTION: Sets C++20 standard and compiler-specific options required for Windows MSVC compatibility with cudax and mdspan.\nSOURCE: https://github.com/nvidia/cccl/blob/main/examples/cudax/CMakeLists.txt#2025-04-23_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nif (\"MSVC\" STREQUAL \"${CMAKE_CXX_COMPILER_ID}\")\n  # mdspan on windows only works in C++20 mode\n  target_compile_features(cudax_samples_interface INTERFACE cxx_std_20)\n\n  # cudax requires dim3 to be usable from a constexpr context, and the CUDART\n  # headers require __cplusplus to be defined for this to work:\n  target_compile_options(cudax_samples_interface INTERFACE\n    $<$<COMPILE_LANGUAGE:CXX>:/Zc:__cplusplus /Zc:preprocessor>\n    $<$<COMPILE_LANG_AND_ID:CUDA,NVIDIA>:-Xcompiler=/Zc:__cplusplus -Xcompiler=/Zc:preprocessor>\n  )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Templated Matrix Multiply Accumulate with TMEM Collector B2 Lastuse - Standard\nDESCRIPTION: Device function template for tensor core matrix multiply accumulate (MMA) operations with shared memory (TMEM) operands. This standard variation omits zero column masking and works with various data types through template specialization.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_53\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.ws.cta_group.kind.collector::b2::lastuse [d_tmem], [a_tmem], b_desc, idesc, enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1 }\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_tmem_a_collector_b2_lastuse(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint32_t a_tmem,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Freezing Logical Data Example\nDESCRIPTION: Shows how to freeze logical data for read-only access across multiple data places.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_48\n\nLANGUAGE: cpp\nCODE:\n```\nauto frozen_ld = ctx.freeze(ld);\nauto dX = frozen_ld.get(data_place::current_device(), stream);\nkernel<<<..., stream>>>(dX);\n\n// Get a read-only copy of the frozen data on other data places\nauto dX1 = frozen_ld.get(data_place::device(1), stream);\nauto hX = frozen_ld.get(data_place::host(), stream);\n\nfx.unfreeze(stream);\n```\n\n----------------------------------------\n\nTITLE: Defining CTA Group Tensor Core Function for 64x128b WarpX2 Variant - CUDA\nDESCRIPTION: This device inline CUDA C++ template declares a function for performing a tensor core operation (tcgen05_cp) on a 64x128 bit CTA group with WarpX2 layout. It uses NVIDIA's PTX dot_cta_group abstraction as a type template and takes parameters for the CTA group object, the operation address, and the operation descriptor. Requires CUDA PTX (PTX ISA 86), template support, and relevant types from cuda::ptx; outputs are via side-effect, and there are no return values.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_cp.rst#2025-04-23_snippet_4\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.cp.cta_group.64x128b.warpx2::01_23.b8x16.b6x16_p32 [taddr], s_desc; // PTX ISA 86, SM_100a, SM_101a\\n// .cta_group = { .cta_group::1, .cta_group::2 }\\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\\n__device__ static inline void tcgen05_cp_64x128b_warpx2_01_23_b8x16_b6x16_p32(\\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\\n  uint32_t taddr,\\n  uint64_t s_desc);\n```\n\n----------------------------------------\n\nTITLE: Setting Minimum CMake Version Requirements for CUB\nDESCRIPTION: Sets minimum CMake version requirements depending on project needs. Requires 3.15 for basic inclusion, 3.21 if CUB is enabled, and mentions 3.27.5 for MSVC builds with RDC.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cub/CMakeLists.txt#2025-04-23_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\n# 3.15 is the minimum for including the project with add_subdirectory.\n# 3.21 is the minimum for the developer build.\n# 3.27.5 is the minimum for MSVC build with RDC=true.\ncmake_minimum_required(VERSION 3.15)\n\n# This must be done before any languages are enabled:\nif (CCCL_ENABLE_CUB)\n  cmake_minimum_required(VERSION 3.21)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Analyzing CUB Benchmark Coverage in Bash\nDESCRIPTION: Shows how to use the analyze.py script to display the coverage of variants per compile-time workload.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/tuning.rst#2025-04-23_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n$ ../benchmarks/scripts/analyze.py --coverage\n    cub.bench.radix_sort.keys[T{ct}=I8, OffsetT{ct}=I32] coverage: 167 / 522 (31.9923%)\n    cub.bench.radix_sort.keys[T{ct}=I8, OffsetT{ct}=I64] coverage: 152 / 522 (29.1188%)\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake Test for CUB Source Code Pattern Checking\nDESCRIPTION: Defines a CMake test that checks CUB source code for issues that can be identified through pattern matching. The test invokes a separate CMake script with the CUB source directory as a parameter.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cub/test/cmake/CMakeLists.txt#2025-04-23_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\n# Check source code for issues that can be found by pattern matching:\nadd_test(\n  NAME cub.test.cmake.check_source_files\n  COMMAND\n    \"${CMAKE_COMMAND}\"\n      -D \"CUB_SOURCE_DIR=${CUB_SOURCE_DIR}\"\n      -P \"${CMAKE_CURRENT_LIST_DIR}/check_source_files.cmake\"\n)\n```\n\n----------------------------------------\n\nTITLE: Replacing Tile Swizzle Atomicity in Global Memory with CUDA C++\nDESCRIPTION: This CUDA device function template performs a tile replacement using swizzle atomicity mode in the global memory space of a tensor map, relying on PTX ISA 86 and architectures SM_100a or SM_101a. Requires the PTX global space type, tensor map address, and a new 32-entry value. It must be run on compatible hardware and within device kernels, with dependencies on CUDA and PTX constructs.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tensormap_replace.rst#2025-04-23_snippet_7\n\nLANGUAGE: cuda\nCODE:\n```\n// tensormap.replace.tile.swizzle_atomicity.space.b1024.b32 [tm_addr], new_val; // PTX ISA 86, SM_100a, SM_101a\\n// .space     = { .global }\\ntemplate <int N32>\\n__device__ static inline void tensormap_replace_swizzle_atomicity(\\n  cuda::ptx::space_global_t,\\n  void* tm_addr,\\n  cuda::ptx::n32_t<N32> new_val);\n```\n\n----------------------------------------\n\nTITLE: Implementing elect.sync Operation in CUDA\nDESCRIPTION: This code snippet defines a device function template for the elect.sync operation. It takes a membermask as input and returns a boolean indicating whether the current thread is elected. This operation is specific to PTX ISA 80 and SM_90 architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/elect_sync.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\n// elect.sync _|is_elected, membermask; // PTX ISA 80, SM_90\ntemplate <typename = void>\n__device__ static inline bool elect_sync(\n  const uint32_t& membermask);\n```\n\n----------------------------------------\n\nTITLE: Including C Standard Float Type Support Header in CUDA C++\nDESCRIPTION: Includes the `<cuda/std/cfloat>` header for type support library functionalities related to floating-point types. Available since CCCL 2.2.0 and CUDA Toolkit 12.3.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/standard_api/c_library.rst#2025-04-23_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\n#include <cuda/std/cfloat>\n```\n\n----------------------------------------\n\nTITLE: Declaring multimem_ld_reduce with Relaxed/CTA Scope and OR Operation in CUDA PTX\nDESCRIPTION: Template function declaration for multimem.ld_reduce with relaxed memory semantics, CTA scope, and OR operation on 64-bit values in global memory. Supports PTX ISA 8.1 on SM_90 architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_61\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.b64 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .or }\ntemplate <typename B64, enable_if_t<sizeof(B64) == 8, bool> = true, cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline B64 multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_or_op_t,\n  const B64* addr);\n```\n\n----------------------------------------\n\nTITLE: Enabling <cuda/std/barrier> and <cuda/std/atomic> with NVRTC (C++)\nDESCRIPTION: Issue #194 fixed in libcu++ 1.6.0. Resolved compilation failures encountered when using the `<cuda/std/barrier>` and `<cuda/std/atomic>` headers with the NVRTC (NVIDIA Runtime Compilation) library.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/releases/changelog.rst#2025-04-23_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\n<cuda/std/barrier>\n```\n\nLANGUAGE: cpp\nCODE:\n```\n<cuda/std/atomic>\n```\n\n----------------------------------------\n\nTITLE: MDSpan Accessor Conversion Example\nDESCRIPTION: Example demonstrating conversion between different mdspan accessor types and alignment requirements.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/mdspan/host_device_accessor.rst#2025-04-23_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\n#include <cuda/mdspan>\n\nusing dim = cuda::std::dims<1>;\n\nint main() {\n    using cuda::std::layout_right;\n    using cuda::std::aligned_accessor;\n    int               h_ptr[4];\n    cuda::std::mdspan md{h_ptr};\n    cuda::host_mdspan h_md = md; // ok\n\n    cuda::std::mdspan<int, dim, layout_right, aligned_accessor<int, 8>> md_a{h_ptr, 4};\n    // cuda::host_mdspan h_md = md_a; // compile-error\n    cuda::host_mdspan    h_md{md_a};  // ok\n}\n```\n\n----------------------------------------\n\nTITLE: Disabling __builtin_is_constant_evaluated with NVCC in C++11 (C++)\nDESCRIPTION: Issue #21 fixed in libcu++ 1.3.0. Disabled the usage of `__builtin_is_constant_evaluated` when compiling with NVCC in C++11 mode due to known issues with the builtin in that configuration.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/releases/changelog.rst#2025-04-23_snippet_19\n\nLANGUAGE: cpp\nCODE:\n```\n__builtin_is_constant_evaluated // Disabled with NVCC in C++11 mode\n```\n\n----------------------------------------\n\nTITLE: Defining Thrust Complex Number Structure Reference in reStructuredText\nDESCRIPTION: A reStructuredText directive that references the thrust::complex structure, which is part of the Thrust numerics module. This provides a link to the complex number implementation documentation.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/api_docs/numerics.rst#2025-04-23_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n  - :cpp:struct:`thrust::complex <thrust::complex>`\n```\n\n----------------------------------------\n\nTITLE: Running Build and Test Scripts\nDESCRIPTION: Commands for executing CCCL build and test scripts with required parameters for compiler, C++ standard, and GPU architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/ci-overview.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./ci/build_cub.sh <HOST_COMPILER> <CXX_STANDARD> <GPU_ARCHS>\n./ci/test_cub.sh <HOST_COMPILER> <CXX_STANDARD> <GPU_ARCHS>\n```\n\n----------------------------------------\n\nTITLE: Implementing mbarrier.arrive.expect_tx with Relaxed Semantics for Cluster Scope in Shared Cluster Space\nDESCRIPTION: This CUDA template function implements the mbarrier.arrive.expect_tx operation with relaxed semantics for cluster scope in shared cluster space. It takes a 64-bit address and a transaction count as parameters.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/mbarrier_arrive_expect_tx.rst#2025-04-23_snippet_3\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline void mbarrier_arrive_expect_tx(\n  cuda::ptx::sem_relaxed_t,\n  cuda::ptx::scope_cluster_t,\n  cuda::ptx::space_cluster_t,\n  uint64_t* addr,\n  const uint32_t& txCount);\n```\n\n----------------------------------------\n\nTITLE: Storing and Unpacking 16x256b Data with tcgen05 in CUDA (16 values)\nDESCRIPTION: This function performs a 16x256b store operation with 16-bit unpacking using tcgen05. It takes a 32-bit address and an array of 16 B32 values as input. The function is constrained to B32 types of 4 bytes.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_st.rst#2025-04-23_snippet_10\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.st.sync.aligned.16x256b.x4.unpack::16b.b32 [taddr], values; // PTX ISA 86, SM_100a, SM_101a\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline void tcgen05_st_16x256b_unpack_16b(\n  uint32_t taddr,\n  const B32 (&values)[16]);\n```\n\n----------------------------------------\n\nTITLE: Defining ABI Version Switching Macro (C++)\nDESCRIPTION: New feature in libcu++ 1.2.0. Users can select a supported ABI version by defining the `_LIBCUDACXX_CUDA_ABI_VERSION` macro before including library headers. The macro `_LIBCUDACXX_CUDA_ABI_VERSION_LATEST` is defined to the latest (and default) ABI version.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/releases/changelog.rst#2025-04-23_snippet_24\n\nLANGUAGE: cpp\nCODE:\n```\n_LIBCUDACXX_CUDA_ABI_VERSION // Define to select ABI version (e.g., 2 or 3)\n```\n\nLANGUAGE: cpp\nCODE:\n```\n_LIBCUDACXX_CUDA_ABI_VERSION_LATEST // Represents the latest ABI version\n```\n\n----------------------------------------\n\nTITLE: Implementing multimem.ld_reduce for relaxed GPU global add operations on 32-bit integers\nDESCRIPTION: Template implementation for multimem atomic load-and-reduce operations with relaxed semantics, GPU scope, global memory, and add operation on 32-bit integers. Compatible with PTX ISA 81 on SM_90 architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_34\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.s32 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .add }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline int32_t multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_add_t,\n  const int32_t* addr);\n```\n\n----------------------------------------\n\nTITLE: Configuring Table of Contents for Predicate Functions in RST Documentation\nDESCRIPTION: Sets up a table of contents (toctree) directive that includes all documentation files matching the pattern '*function_group__predicates*'. The toctree is configured to display files alphabetically with a maximum depth of 1.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/api_docs/algorithms/reductions/predicates.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :glob:\n   :maxdepth: 1\n\n   ${repo_docs_api_path}/*function_group__predicates*\n```\n\n----------------------------------------\n\nTITLE: Initializing libcudacxx Project Parameters\nDESCRIPTION: Sets up the package name, version information, and initializes the project with C++ language support.\nSOURCE: https://github.com/nvidia/cccl/blob/main/libcudacxx/CMakeLists.txt#2025-04-23_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(PACKAGE_NAME libcudacxx)\nset(PACKAGE_VERSION 11.0)\nset(PACKAGE_STRING \"${PACKAGE_NAME} ${PACKAGE_VERSION}\")\nproject(libcudacxx LANGUAGES CXX)\n```\n\n----------------------------------------\n\nTITLE: Integer Base-10 Logarithm Function Declaration in CUDA\nDESCRIPTION: Template function declaration for computing the floor of logarithm base 10 of an integer value. The function is available for both host and device execution.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/math/ilog.rst#2025-04-23_snippet_2\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename T>\n[[nodiscard]] __host__ __device__ inline constexpr\nint ilog10(T value) noexcept;\n```\n\n----------------------------------------\n\nTITLE: Building CUDA Samples with Debug Symbols\nDESCRIPTION: Make command for building CUDA samples with debug symbols enabled using the dbg flag.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cudax/test/stf/cuda-samples/0_Introduction/vectorAdd/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ make dbg=1\n```\n\n----------------------------------------\n\nTITLE: MMA Workstream Collector B0 Discard Operations\nDESCRIPTION: Template function implementing MMA workstream collector operations with b0 discard mode. Supports multiple data types including f16, tf32, f8f6f4, and i8. Handles tensor memory operations with optional zero column masking.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_14\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_tmem_a_collector_b0_discard(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint32_t a_tmem,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d,\n  uint64_t zero_column_mask_desc);\n```\n\n----------------------------------------\n\nTITLE: Processing CUDA Example Sources\nDESCRIPTION: Configures source file processing for CUDA examples, including version-specific handling and iteration over targets.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cudax/examples/CMakeLists.txt#2025-04-23_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB example_srcs\n  RELATIVE \"${cudax_SOURCE_DIR}/examples\"\n  CONFIGURE_DEPENDS\n  *.cu *.cpp\n)\n\n# Example requires pinned_memory_resource.\nif(CUDAToolkit_VERSION VERSION_LESS 12.6)\n  list(REMOVE_ITEM example_srcs async_buffer_add.cu)\nendif()\n\nforeach(cudax_target IN LISTS cudax_TARGETS)\n  cudax_get_target_property(config_prefix ${cudax_target} PREFIX)\n\n  # Metatarget for the current configuration's tests:\n  set(config_meta_target ${config_prefix}.examples)\n  add_custom_target(${config_meta_target})\n  add_dependencies(${config_prefix}.all ${config_meta_target})\n\n  foreach (example_src IN LISTS example_srcs)\n    cudax_add_example(example_target \"${example_src}\" ${cudax_target})\n  endforeach()\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Enabling <nv/target> Usage with NVRTC (C++)\nDESCRIPTION: Issue #186 fixed in libcu++ 1.6.0. Allows the `<nv/target>` header to be used successfully within code compiled by NVRTC.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/releases/changelog.rst#2025-04-23_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\n<nv/target>\n```\n\n----------------------------------------\n\nTITLE: Installing Specific CCCL Version with CUDA using Conda (Bash)\nDESCRIPTION: This command shows how to install a specific version of CCCL that corresponds to a particular CUDA Toolkit version using Conda.\nSOURCE: https://github.com/nvidia/cccl/blob/main/README.md#2025-04-23_snippet_5\n\nLANGUAGE: Bash\nCODE:\n```\nconda config --add channels conda-forge\nconda install cuda-cccl cuda-version=12.4\n```\n\n----------------------------------------\n\nTITLE: Globbing Test Source Files in CMake\nDESCRIPTION: Uses the `file(GLOB ...)` command to find all files ending in `.cu` or `.cpp` within the current directory (`${CMAKE_CURRENT_LIST_DIR}`). The paths, relative to the current directory, are stored in the `test_srcs` variable. The `CONFIGURE_DEPENDS` flag ensures that CMake re-runs the configuration step if the list of matching files changes (files added or removed). These files represent the common tests to be added.\nSOURCE: https://github.com/nvidia/cccl/blob/main/thrust/testing/CMakeLists.txt#2025-04-23_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB test_srcs\n  RELATIVE \"${CMAKE_CURRENT_LIST_DIR}\"\n  CONFIGURE_DEPENDS\n  *.cu *.cpp\n)\n```\n\n----------------------------------------\n\nTITLE: Storing and Unpacking 16x256b Data with tcgen05 in CUDA (32 values)\nDESCRIPTION: This function performs a 16x256b store operation with 16-bit unpacking using tcgen05. It takes a 32-bit address and an array of 32 B32 values as input. The function is constrained to B32 types of 4 bytes.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_st.rst#2025-04-23_snippet_12\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.st.sync.aligned.16x256b.x8.unpack::16b.b32 [taddr], values; // PTX ISA 86, SM_100a, SM_101a\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline void tcgen05_st_16x256b_unpack_16b(\n  uint32_t taddr,\n  const B32 (&values)[32]);\n```\n\n----------------------------------------\n\nTITLE: Implementing mbarrier.arrive.expect_tx with Relaxed Semantics for CTA/Cluster Scope in Shared CTA Space\nDESCRIPTION: This CUDA template function implements the mbarrier.arrive.expect_tx operation with relaxed semantics for CTA or cluster scope in shared CTA space. It takes a 64-bit address and a transaction count as parameters.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/mbarrier_arrive_expect_tx.rst#2025-04-23_snippet_2\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_scope Scope>\n__device__ static inline uint64_t mbarrier_arrive_expect_tx(\n  cuda::ptx::sem_relaxed_t,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::space_shared_t,\n  uint64_t* addr,\n  const uint32_t& txCount);\n```\n\n----------------------------------------\n\nTITLE: Including C Standard Integer Limits Header in CUDA C++\nDESCRIPTION: Includes the `<cuda/std/climits>` header, providing access to the limits of integral types. Available since CCCL 2.0.0 and CUDA Toolkit 10.2.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/standard_api/c_library.rst#2025-04-23_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\n#include <cuda/std/climits>\n```\n\n----------------------------------------\n\nTITLE: XOR Operation for 32-bit Type\nDESCRIPTION: Template function implementing asynchronous XOR reduction operation on cluster memory barrier for b32 type. Takes destination pointer, value and remote barrier pointer as parameters. Uses SFINAE to ensure 4-byte size.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/red_async.rst#2025-04-23_snippet_10\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline void red_async(\n  cuda::ptx::op_xor_op_t,\n  B32* dest,\n  const B32& value,\n  uint64_t* remote_bar);\n```\n\n----------------------------------------\n\nTITLE: Backporting C++17 <cuda/std/type_traits> Features to C++14 (C++)\nDESCRIPTION: Feature #44 added in libcu++ 1.4.0. C++17 features from `<type_traits>` are backported to C++14 within the `<cuda/std/type_traits>` header. Contribution by Jake Hemstad and Paul Taylor.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/releases/changelog.rst#2025-04-23_snippet_13\n\nLANGUAGE: cpp\nCODE:\n```\n<cuda/std/type_traits>\n```\n\n----------------------------------------\n\nTITLE: Parameterized Test Definition with Block Sizes\nDESCRIPTION: Demonstrates how to define parameterized tests using %PARAM% directives to generate multiple test executables with different block sizes. Uses type lists and enum type lists for template parameters.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/test_overview.rst#2025-04-23_snippet_7\n\nLANGUAGE: c++\nCODE:\n```\n// %PARAM% BLOCK_SIZE bs 128:256\nusing block_sizes = c2h::enum_type_list<int, BLOCK_SIZE>;\nusing types = c2h::type_list<std::uint8_t, std::int32_t>;\n\nC2H_TEST(\"SCOPE FACILITY works with CONDITION\", \"[FACILITY][SCOPE]\",\n         types, block_sizes)\n{\n  using type = typename c2h::get<0, TestType>;\n  constexpr int threads_per_block = c2h::get<1, TestType>::value;\n  // ...\n}\n```\n\n----------------------------------------\n\nTITLE: Testing CCCL Components\nDESCRIPTION: Command pattern for running tests on CCCL components. Takes same arguments as build script and requires GPU.\nSOURCE: https://github.com/nvidia/cccl/blob/main/CONTRIBUTING.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n./ci/test_cub.sh -cxx g++ -std 17 -arch \"70;75;80-virtual\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Documentation Structure for Transformed Prefix Sums in Thrust\nDESCRIPTION: Sets up the documentation tree structure for transformed prefix sums algorithms in the Thrust library. Uses the toctree directive to glob and include all related function documentation files.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/api_docs/algorithms/prefix_sums/transformed.rst#2025-04-23_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. toctree::\n   :glob:\n   :maxdepth: 1\n\n   ${repo_docs_api_path}/*function_group__transformed__prefixsums*\n```\n\n----------------------------------------\n\nTITLE: Conditional CUDA STF Examples Configuration\nDESCRIPTION: Conditionally adds CUDA STF examples based on compiler and feature flags.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cudax/examples/CMakeLists.txt#2025-04-23_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\n# FIXME: Enable MSVC\nif (cudax_ENABLE_CUDASTF AND\n    NOT \"MSVC\" STREQUAL \"${CMAKE_CXX_COMPILER_ID}\")\n  # STF examples are handled separately:\n  add_subdirectory(stf)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Storing and Unpacking 16x128b Data with tcgen05 in CUDA (64 values)\nDESCRIPTION: This function performs a 16x128b store operation with 16-bit unpacking using tcgen05. It takes a 32-bit address and an array of 64 B32 values as input. The function is constrained to B32 types of 4 bytes.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_st.rst#2025-04-23_snippet_2\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.st.sync.aligned.16x128b.x32.unpack::16b.b32 [taddr], values; // PTX ISA 86, SM_100a, SM_101a\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline void tcgen05_st_16x128b_unpack_16b(\n  uint32_t taddr,\n  const B32 (&values)[64]);\n```\n\n----------------------------------------\n\nTITLE: MMA Workspace Template for B0 Discard Operations\nDESCRIPTION: Template device function for MMA workspace operations with discard collector pattern. Handles descriptor-based inputs and supports zero column masking.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_13\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_collector_b0_discard(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d,\n  uint64_t zero_column_mask_desc);\n```\n\n----------------------------------------\n\nTITLE: Declaring Load-Reduce Minimum Operation (acquire/sys/global/u64) in CUDA C++\nDESCRIPTION: Defines a templated device function to perform a global atomic minimum reduction on 64-bit unsigned integers, supporting PTX acquire or relaxed semantics and system-wide visibility. Expects PTX type templates for synchronization and scope properties, and integration with other CUDA library constructs.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_17\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.u64 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .min }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline uint64_t multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_min_t,\n  const uint64_t* addr);\n```\n\n----------------------------------------\n\nTITLE: Configuring toctree for Thrust Transformed Reductions Documentation\nDESCRIPTION: A toctree directive that includes all function group documentation files related to transformed reductions. The directive uses a glob pattern to automatically include all matching files from the repository's API documentation path.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/api_docs/algorithms/reductions/transformed.rst#2025-04-23_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\n   :glob:\n   :maxdepth: 1\n\n   ${repo_docs_api_path}/*function_group__transformed__reductions*\n```\n\n----------------------------------------\n\nTITLE: Setting up Code Generation Executable for libcudacxx Atomics\nDESCRIPTION: Defines a CMake executable target for the codegen tool that will generate atomic operation implementations. The target is configured to use C++17 standard and depends on the fmt library for formatting.\nSOURCE: https://github.com/nvidia/cccl/blob/main/libcudacxx/codegen/CMakeLists.txt#2025-04-23_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncccl_get_fmt()\n\nadd_executable(\n    codegen\n    EXCLUDE_FROM_ALL\n    codegen.cpp\n)\n\ntarget_link_libraries(codegen PRIVATE fmt)\n\nset_property(TARGET codegen PROPERTY CXX_STANDARD 17)\n```\n\n----------------------------------------\n\nTITLE: Implementing MMA Workspace Operations with F16 Data Type in CUDA\nDESCRIPTION: Template function for tensor compute generation 5 matrix multiplication with workspaces using F16 data type. This variant operates on tensor memory and descriptors without zero column masking. Targets PTX ISA 86 on SM_100a and SM_101a architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_40\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.ws.cta_group.kind.collector::b2::use [d_tmem], [a_tmem], b_desc, idesc, enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1 }\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_tmem_a_collector_b2_use(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint32_t a_tmem,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Overriding Default Host Compiler for CUDA Samples\nDESCRIPTION: Make command showing how to override the default g++ host compiler when building CUDA samples on Linux.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cudax/test/stf/cuda-samples/0_Introduction/vectorAdd/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ make HOST_COMPILER=g++\n```\n\n----------------------------------------\n\nTITLE: MMA Block Scale Vector 4X Collector Fill with mxf4nvf4 Type (CTA Group 2)\nDESCRIPTION: Template function for matrix multiplication with block scaling using vector 4X with collector A fill operation for CTA group 2. The function uses mxf4nvf4 data type and requires PTX ISA 86 with SM_100a or SM_101a architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma.rst#2025-04-23_snippet_23\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.cta_group.kind.block_scale.scale_vec::4X.collector::a::fill [d_tmem], a_desc, b_desc, idesc, [scale_A_tmem], [scale_B_tmem], enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .kind      = { .kind::mxf4nvf4 }\n// .cta_group = { .cta_group::1, .cta_group::2 }\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_mma_block_scale_vec_4x_collector_a_fill(\n  cuda::ptx::kind_mxf4nvf4_t,\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  uint32_t scale_A_tmem,\n  uint32_t scale_B_tmem,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Querying Thread Hierarchy Properties in C++\nDESCRIPTION: Shows how to query various properties of a thread hierarchy, including scope, available memory, and hierarchy depth.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_39\n\nLANGUAGE: cpp\nCODE:\n```\nth.get_scope(i); // returns the scope of the i-th level\n\nth.get_mem(i); // returns the amount of memory available at i-th level\n\nslice<char> smem = th.template storage<char>(1);\n\nth.depth(); // (constexpr) return the number of levels in the hierarchy\n```\n\n----------------------------------------\n\nTITLE: Improving <cuda/pipeline> Documentation (Documentation)\nDESCRIPTION: Issues #53, #80, #81 fixed in libcu++ 1.4.0. Enhanced the documentation for the `<cuda/pipeline>` header and the associated asynchronous operations API.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/releases/changelog.rst#2025-04-23_snippet_16\n\nLANGUAGE: cpp\nCODE:\n```\n<cuda/pipeline>\n```\n\n----------------------------------------\n\nTITLE: Introducing ABI Version 4 for Performance (C++)\nDESCRIPTION: Issue #172 addressed in libcu++ 1.6.0. Introduces ABI version 4, which forces `cuda::std::complex` alignment for better performance and sets the internal representation of `cuda::std::chrono` literals to `double`.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/releases/changelog.rst#2025-04-23_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\ncuda::std::complex\n```\n\nLANGUAGE: cpp\nCODE:\n```\ncuda::std::chrono\n```\n\n----------------------------------------\n\nTITLE: CMake Find Package Example for CUB\nDESCRIPTION: Example showing how to use CMake's find_package functionality to include CUB in a CMake project.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/releases/changelog.rst#2025-04-23_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncmake -DCUB_DIR=/usr/local/cuda/include/cub/cmake/ .\nfind_package(CUB REQUIRED CONFIG)\n```\n\n----------------------------------------\n\nTITLE: Setting up CUDA C++ Atomic Tests with CMake\nDESCRIPTION: Complete CMake script for configuring and building CUDA C++ atomic codegen verification tests. It discovers test files, compiles them with specific flags for SM80 architecture, and uses custom scripts to dump and validate the generated PTX/SASS code against expected patterns.\nSOURCE: https://github.com/nvidia/cccl/blob/main/libcudacxx/test/atomic_codegen/CMakeLists.txt#2025-04-23_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_custom_target(libcudacxx.test.atomics.ptx)\n\nfind_program(filecheck \"FileCheck\")\n\nif (filecheck)\n  message(\"-- ${filecheck} found... building atomic codegen tests\")\nelse()\n  return()\nendif()\n\nfind_program(cuobjdump \"cuobjdump\" REQUIRED)\nfind_program(bash \"bash\" REQUIRED)\n\nset(libcudacxx_atomic_codegen_tests)\nif (NOT \"NVHPC\" STREQUAL \"${CMAKE_CXX_COMPILER_ID}\")\n  file(GLOB libcudacxx_atomic_codegen_tests \"*.cu\")\nendif()\n\n# For every atomic API compile the TU and check if the SASS/PTX matches the expected result\nforeach(test_path IN LISTS libcudacxx_atomic_codegen_tests)\n    cmake_path(GET test_path FILENAME test_file)\n    cmake_path(REMOVE_EXTENSION test_file LAST_ONLY OUTPUT_VARIABLE test_name)\n\n    add_library(\n        atomic_codegen_${test_name}\n        STATIC ${test_path}\n    )\n\n    set_target_properties(\n        atomic_codegen_${test_name}\n        PROPERTIES\n          CUDA_ARCHITECTURES \"80\"\n          COMPILE_DEFINITIONS \"_LIBCUDACXX_ATOMIC_UNSAFE_AUTOMATIC_STORAGE=1\"\n    )\n\n    target_compile_options(atomic_codegen_${test_name} PRIVATE \"-Wno-comment\")\n\n    ## Important for testing the local headers\n    target_include_directories(atomic_codegen_${test_name} PRIVATE \"${libcudacxx_SOURCE_DIR}/include\")\n    add_dependencies(libcudacxx.test.atomics.ptx atomic_codegen_${test_name})\n\n    # Add output path to object directory\n    add_custom_command(\n        TARGET libcudacxx.test.atomics.ptx\n        POST_BUILD\n        COMMAND ${CMAKE_CURRENT_SOURCE_DIR}/dump_and_check.bash $<TARGET_FILE:atomic_codegen_${test_name}> ${test_path} SM8X\n    )\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: PRMT B32 Backward 4-Byte Template Function\nDESCRIPTION: Template function declaration for 32-bit backward 4-byte permute operation (b4e mode). Operates on 32-bit inputs with backward byte ordering.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/prmt.rst#2025-04-23_snippet_2\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline uint32_t prmt_b4e(\n  B32 a_reg,\n  B32 b_reg,\n  uint32_t c_reg);\n```\n\n----------------------------------------\n\nTITLE: Acquire Memory Barrier Try Wait with Suspend Hint\nDESCRIPTION: Memory barrier try wait with acquire semantics and suspend time hint, supporting both CTA and cluster scopes.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/mbarrier_try_wait_parity.rst#2025-04-23_snippet_3\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_scope Scope>\n__device__ static inline bool mbarrier_try_wait_parity(\n  cuda::ptx::sem_acquire_t,\n  cuda::ptx::scope_t<Scope> scope,\n  uint64_t* addr,\n  const uint32_t& phaseParity,\n  const uint32_t& suspendTimeHint);\n```\n\n----------------------------------------\n\nTITLE: Fixing DeviceHistogram null-pointer exception in CUB 1.7.3\nDESCRIPTION: Bug fix for cub::DeviceHistogram null-pointer exception when using iterator inputs.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/releases/changelog.rst#2025-04-23_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\ncub::DeviceHistogram\n```\n\n----------------------------------------\n\nTITLE: Packed 16x64b Tensor Load with 16-bit Output (CUDA)\nDESCRIPTION: Template function for loading and packing 16x64b tensor data into a single 32-bit output array with 16-bit packing. Compatible with PTX ISA 86.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_ld.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.ld.sync.aligned.16x64b.x1.pack::16b.b32 out, [taddr]; // PTX ISA 86, SM_100a, SM_101a\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline void tcgen05_ld_16x64b_pack_16b(\n  B32 (&out)[1],\n  uint32_t taddr);\n```\n\n----------------------------------------\n\nTITLE: Overriding CMake Preset Options\nDESCRIPTION: Example of overriding preset CUDA architecture settings during CMake configuration.\nSOURCE: https://github.com/nvidia/cccl/blob/main/CONTRIBUTING.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncmake --preset=thrust-cpp20 \"-DCMAKE_CUDA_ARCHITECTURES=89\"\n```\n\n----------------------------------------\n\nTITLE: Installing CCCL using CMake (Bash)\nDESCRIPTION: This set of commands shows how to clone the CCCL repository and install it using CMake. It uses the 'install' preset to configure the build.\nSOURCE: https://github.com/nvidia/cccl/blob/main/README.md#2025-04-23_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\ngit clone https://github.com/NVIDIA/cccl.git\ncd cccl\ncmake --preset install -DCMAKE_INSTALL_PREFIX=/usr/local/\ncd build/install\nninja install\n```\n\n----------------------------------------\n\nTITLE: Configuring NVRTC Runtime Compilation Test Mode\nDESCRIPTION: Sets up the test environment to use NVRTC (NVIDIA Runtime Compilation) instead of offline compilation, which only runs device-side tests. This configures special compiler and executor settings.\nSOURCE: https://github.com/nvidia/cccl/blob/main/libcudacxx/test/libcudacxx/CMakeLists.txt#2025-04-23_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\noption(LIBCUDACXX_TEST_WITH_NVRTC\n  \"Test libcu++ with runtime compilation instead of offline compilation. Only runs device side tests.\"\n  OFF)\n\nif (LIBCUDACXX_TEST_WITH_NVRTC)\n  # TODO: Use project properties to get path to binary.\n  # Should also set up dependency on the project when NVRTC is enabled\n  set(LIBCUDACXX_CUDA_COMPILER \"${CMAKE_BINARY_DIR}/libcudacxx/test/utils/nvidia/nvrtc/nvrtcc\")\n  set(LIBCUDACXX_CUDA_COMPILER_ARG1 \"\")\n  set(LIBCUDACXX_CUDA_TEST_WITH_NVRTC \"True\")\n  set(LIBCUDACXX_FORCE_INCLUDE \"\")\n  set(LIBCUDACXX_TEST_COMPILER_FLAGS \"-I'${CUDA_INCLUDE_DIR}'\")\n  # Use the NVRTCC utility to run the built test outputs\n  set(LIBCUDACXX_EXECUTOR \"PrefixExecutor(['${LIBCUDACXX_CUDA_COMPILER}'], LocalExecutor())\")\nelse() # NOT LIBCUDACXX_TEST_WITH_NVRTC\n  set(LIBCUDACXX_FORCE_INCLUDE \"-include ${libcudacxx_SOURCE_DIR}/test/libcudacxx/force_include.h\")\n  set(LIBCUDACXX_CUDA_COMPILER \"${CMAKE_CUDA_COMPILER}\")\n  set(LIBCUDACXX_CUDA_TEST_WITH_NVRTC \"False\")\n  set(LIBCUDACXX_TEST_COMPILER_FLAGS \"-DLIBCUDACXX_ENABLE_EXPERIMENTAL_MEMORY_RESOURCE\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents for Iterator Tags Documentation in reStructuredText\nDESCRIPTION: This snippet creates a table of contents directive in reStructuredText that automatically includes all files matching the 'iterator_tag' pattern. The glob pattern ensures all iterator tag documentation is included with a maximum depth of 1 level.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/api_docs/iterators/tags.rst#2025-04-23_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\n   :glob:\n   :maxdepth: 1\n\n   ${repo_docs_api_path}/*iterator__tag*\n```\n\n----------------------------------------\n\nTITLE: Including CPM Package Manager\nDESCRIPTION: Includes the CMake Package Manager (CPM) to simplify fetching CCCL from GitHub.\nSOURCE: https://github.com/nvidia/cccl/blob/main/examples/cudax/CMakeLists.txt#2025-04-23_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ninclude(cmake/CPM.cmake)\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Bulk XOR Reduction (32-bit) in CUDA\nDESCRIPTION: Performs an asynchronous bulk XOR reduction operation from CTA-shared to cluster-shared memory for 32-bit data types. Uses a memory barrier for synchronization.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_reduce_async_bulk.rst#2025-04-23_snippet_2\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline void cp_reduce_async_bulk(\n  cuda::ptx::space_cluster_t,\n  cuda::ptx::space_shared_t,\n  cuda::ptx::op_xor_op_t,\n  B32* dstMem,\n  const B32* srcMem,\n  uint32_t size,\n  uint64_t* rdsmem_bar);\n```\n\n----------------------------------------\n\nTITLE: Rank Replacement Operations\nDESCRIPTION: Device functions to replace rank value in tensormap for both global and shared memory spaces. Takes a 32-bit value.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tensormap_replace.rst#2025-04-23_snippet_2\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline void tensormap_replace_rank(\n  cuda::ptx::space_global_t,\n  void* tm_addr,\n  B32 new_val);\n```\n\n----------------------------------------\n\nTITLE: Loading 32x32b Data with tcgen05 in CUDA\nDESCRIPTION: This template function loads 32x32b data using the tcgen05 instruction. It supports different output sizes (32, 64, 128) and has variants for packed 16-bit data.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_ld.rst#2025-04-23_snippet_2\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline void tcgen05_ld_32x32b(\n  B32 (&out)[64],\n  uint32_t taddr);\n```\n\n----------------------------------------\n\nTITLE: CUDA Execution Policy Example\nDESCRIPTION: Example of thrust execution policy syntax for CUDA backend\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/releases/changelog.rst#2025-04-23_snippet_9\n\nLANGUAGE: CUDA\nCODE:\n```\nthrust::cuda::par\n```\n\n----------------------------------------\n\nTITLE: Configuring Warning Levels for Clang CUDA Compiler\nDESCRIPTION: Sets warning flags specifically for Clang when used as the CUDA device compiler, including flags to handle CUDA version warnings and support for variadic functions.\nSOURCE: https://github.com/nvidia/cccl/blob/main/libcudacxx/test/CMakeLists.txt#2025-04-23_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nelseif(\"${CMAKE_CUDA_COMPILER_ID}\" STREQUAL \"Clang\")\n  set(headertest_warning_levels_device -Wall -Werror -Wno-unknown-cuda-version -Xclang=-fcuda-allow-variadic-functions)\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies for Plotting (Bash)\nDESCRIPTION: Shows the command to install necessary Python packages for plotting benchmark results. These packages include fpzip, pandas, matplotlib, seaborn, tabulate, and PyQt5.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/benchmarking.rst#2025-04-23_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npip install fpzip pandas matplotlib seaborn tabulate PyQt5\n```\n\n----------------------------------------\n\nTITLE: Running All Benchmarks via Command Line\nDESCRIPTION: Script to automatically run all available benchmarks and save their results to separate files\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/benchmarking.rst#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nninja cub.all.benches\nbenchmarks=$(ls bin | grep cub.bench); n=$(echo $benchmarks | wc -w); i=1; \\\nfor b in $benchmarks; do \\\n  echo \"=== Running $b ($i/$n) ===\"; \\\n  ./bin/$b -d 0 --stopping-criterion entropy --json $b.json --md $b.md; \\\n  ((i++)); \\\ndone\n```\n\n----------------------------------------\n\nTITLE: MMA Block Scale Vector 1X Collector Fill with mxf8f6f4 Type (CTA Group 2)\nDESCRIPTION: Template function for matrix multiplication with block scaling using vector 1X with collector A fill operation for CTA group 2. The function uses mxf8f6f4 data type and requires PTX ISA 86 with SM_100a or SM_101a architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma.rst#2025-04-23_snippet_25\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.cta_group.kind.block_scale.scale_vec::1X.collector::a::fill [d_tmem], a_desc, b_desc, idesc, [scale_A_tmem], [scale_B_tmem], enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .kind      = { .kind::mxf8f6f4 }\n// .cta_group = { .cta_group::1, .cta_group::2 }\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_mma_block_scale_vec_1x_tmem_a_collector_a_fill(\n  cuda::ptx::kind_mxf8f6f4_t,\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  uint32_t scale_A_tmem,\n  uint32_t scale_B_tmem,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Configuring CUDA Example Target Function\nDESCRIPTION: CMake function that creates and configures a build target for a CUDA example. Sets up compiler options, dependencies, and test configuration.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cudax/examples/CMakeLists.txt#2025-04-23_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nfunction(cudax_add_example target_name_var example_src cudax_target)\n  cudax_get_target_property(config_prefix ${cudax_target} PREFIX)\n  cudax_get_target_property(config_dialect ${cudax_target} DIALECT)\n\n  get_filename_component(example_name ${example_src} NAME_WE)\n\n  # The actual name of the test's target:\n  set(example_target ${config_prefix}.example.${example_name})\n  set(${target_name_var} ${example_target} PARENT_SCOPE)\n\n  # Related target names:\n  set(config_meta_target ${config_prefix}.examples)\n  set(example_meta_target cudax.all.example.${example_name})\n\n  add_executable(${example_target} \"${example_src}\")\n  cccl_configure_target(${example_target} DIALECT ${config_dialect})\n  target_link_libraries(${example_target} PRIVATE\n    ${cudax_target}\n    cudax.examples.thrust\n  )\n  target_compile_options(${example_target} PRIVATE\n    \"-DLIBCUDACXX_ENABLE_EXPERIMENTAL_MEMORY_RESOURCE\"\n  )\n\n  cudax_clone_target_properties(${example_target} ${cudax_target})\n  target_include_directories(${example_target} PRIVATE \"${CUB_SOURCE_DIR}/examples\")\n\n  # Add to the active configuration's meta target\n  add_dependencies(${config_meta_target} ${example_target})\n\n  # Meta target that builds examples with this name for all configurations:\n  if (NOT TARGET ${example_meta_target})\n    add_custom_target(${example_meta_target})\n  endif()\n  add_dependencies(${example_meta_target} ${example_target})\n\n  add_test(NAME ${example_target}\n    COMMAND \"$<TARGET_FILE:${example_target}>\"\n  )\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Plotting Benchmark Results (Bash)\nDESCRIPTION: Illustrates how to plot benchmark results from one or more tuning databases as a bar chart or box plot using the sol.py script.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/benchmarking.rst#2025-04-23_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n../benchmarks/scripts/sol.py cccl_meta_bench.db ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic STF Example Sources\nDESCRIPTION: Defines a list of basic CUDA STF example source files that demonstrate core functionality like AXPY operations, Fibonacci calculations, and data handling.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cudax/examples/stf/CMakeLists.txt#2025-04-23_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(stf_example_sources\n  01-axpy.cu\n  01-axpy-cuda_kernel.cu\n  01-axpy-cuda_kernel_chain.cu\n  02-axpy-host_launch.cu\n  03-temporary-data.cu\n  04-fibonacci.cu\n  04-fibonacci-run_once.cu\n  08-cub-reduce.cu\n  axpy-annotated.cu\n  void_data_interface.cu\n  explicit_data_places.cu\n  thrust_zip_iterator.cu\n  1f1b.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Including NVBench Helper Header for CUB Benchmarks\nDESCRIPTION: Starting point for creating CUB benchmarks, which includes all necessary header files and definitions from the NVBench framework.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/tuning.rst#2025-04-23_snippet_0\n\nLANGUAGE: c++\nCODE:\n```\n#include <nvbench_helper.cuh>\n```\n\n----------------------------------------\n\nTITLE: Declaring Load-Reduce Minimum Operation (relaxed/gpu/global/u32) in CUDA C++\nDESCRIPTION: This snippet defines a device-side inline function for atomic-like min reduction loads on global memory for 32-bit unsigned integers, using PTX relaxed or acquire semaphore semantics and configurable memory scope, including GPU-wide. Requires PTX semaphore and scope types, and a pointer to the memory location. Ensures multi-threaded reduction operations honor desired visibility.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_3\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.u32 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .min }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline uint32_t multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_min_t,\n  const uint32_t* addr);\n```\n\n----------------------------------------\n\nTITLE: Running pre-commit hooks for CCCL Project\nDESCRIPTION: This command runs pre-commit hooks on staged files in the CCCL project before committing code.\nSOURCE: https://github.com/nvidia/cccl/blob/main/CONTRIBUTING.md#2025-04-23_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npre-commit run\n```\n\n----------------------------------------\n\nTITLE: CTA Group 2 Multicast Barrier Commit Operation\nDESCRIPTION: Template device function for committing to a shared memory barrier with multicast support in CTA group 2. Includes CTA mask parameter for selective synchronization.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_commit.rst#2025-04-23_snippet_3\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.commit.cta_group.mbarrier::arrive::one.shared::cluster.multicast::cluster.b64 [smem_bar], ctaMask; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1, .cta_group::2 }\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_commit_multicast(\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint64_t* smem_bar,\n  uint16_t ctaMask);\n```\n\n----------------------------------------\n\nTITLE: Updating WarpReduce for uint64_t in CUB 1.7.1\nDESCRIPTION: Bug fix for uint64_t cub::WarpReduce which was broken in CUB 1.7.0 on CUDA 8 and older.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/releases/changelog.rst#2025-04-23_snippet_5\n\nLANGUAGE: C++\nCODE:\n```\nuint64_t cub::WarpReduce\n```\n\n----------------------------------------\n\nTITLE: Release Semantic Barrier Arrive with Count\nDESCRIPTION: Memory barrier arrive operation with release semantics and count parameter. Supports both CTA and cluster scopes.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/mbarrier_arrive.rst#2025-04-23_snippet_3\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_scope Scope>\n__device__ static inline uint64_t mbarrier_arrive(\n  cuda::ptx::sem_release_t,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::space_shared_t,\n  uint64_t* addr,\n  const uint32_t& count);\n```\n\n----------------------------------------\n\nTITLE: ReStructuredText Exit Instruction Reference\nDESCRIPTION: ReStructuredText markup defining a documentation section for the PTX 'exit' instruction, including a reference link to the official NVIDIA documentation.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/exit.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _libcudacxx-ptx-instructions-exit:\n\nexit\n====\n\n-  PTX ISA:\n   `exit <https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#control-flow-instructions-exit>`__\n\n.. include:: generated/exit.rst\n```\n\n----------------------------------------\n\nTITLE: Relaxed Memory Barrier Try Wait\nDESCRIPTION: Memory barrier try_wait with relaxed semantics, supporting both CTA and cluster scopes.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/mbarrier_try_wait.rst#2025-04-23_snippet_4\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_scope Scope>\n__device__ static inline bool mbarrier_try_wait(\n  cuda::ptx::sem_relaxed_t,\n  cuda::ptx::scope_t<Scope> scope,\n  uint64_t* addr,\n  const uint64_t& state);\n```\n\n----------------------------------------\n\nTITLE: Implementing multimem.red.relaxed.sys.global.add.u32 Operation in CUDA PTX\nDESCRIPTION: Template function declaration for performing a 32-bit unsigned integer addition reduction with relaxed semantics and system-wide scope in global memory. This function is available for SM_90 and requires PTX ISA 8.1 or higher.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_red.rst#2025-04-23_snippet_21\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.red.sem.scope.global.op.u32 [addr], val; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .release }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .add }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline void multimem_red(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_add_t,\n  uint32_t* addr,\n  uint32_t val);\n```\n\n----------------------------------------\n\nTITLE: Declaring cuda::device::barrier_expect_tx Function in CUDA\nDESCRIPTION: Function declaration for cuda::device::barrier_expect_tx, which increments the expected transaction count of a barrier in shared memory. It takes a reference to a block-scoped barrier and a transaction count update as parameters.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/synchronization_primitives/barrier/barrier_expect_tx.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\n__device__\nvoid cuda::device::barrier_expect_tx(\n  cuda::barrier<cuda::thread_scope_block>& bar,\n  ptrdiff_t transaction_count_update);\n```\n\n----------------------------------------\n\nTITLE: Multimem Reduction Template for 64-bit Integer Min Operation\nDESCRIPTION: Template function implementing multimem reduction with minimum operation for 64-bit integers. Supports different memory semantics (relaxed/release) and scopes (cta/cluster/gpu/sys).\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_red.rst#2025-04-23_snippet_4\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.red.sem.scope.global.op.s64 [addr], val; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .release }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .min }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline void multimem_red(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_min_t,\n  int64_t* addr,\n  int64_t val);\n```\n\n----------------------------------------\n\nTITLE: Include Memory Resource Header in CUDA C++\nDESCRIPTION: Header inclusion path for CUDA memory resource functionality. Requires LIBCUDACXX_ENABLE_EXPERIMENTAL_MEMORY_RESOURCE to be defined as the feature is experimental.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/memory_resource.rst#2025-04-23_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n#include <cuda/memory_resource>\n```\n\n----------------------------------------\n\nTITLE: Execution Policy Listing\nDESCRIPTION: Supported execution policies in Thrust\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/releases/changelog.rst#2025-04-23_snippet_10\n\nLANGUAGE: C++\nCODE:\n```\nthrust::host\nthrust::device\nthrust::cpp::par\nthrust::cuda::par\nthrust::omp::par\nthrust::tbb::par\n```\n\n----------------------------------------\n\nTITLE: Implementing Multimem Load Reduce XOR Operation in CUDA\nDESCRIPTION: This template function performs an atomic XOR operation using CUDA's multimem load reduce instruction. It supports 64-bit data types and allows specifying memory semantics and scope. The function is designed for use in device code.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_77\n\nLANGUAGE: CUDA\nCODE:\n```\ntemplate <typename B64, enable_if_t<sizeof(B64) == 8, bool> = true, cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline B64 multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_xor_op_t,\n  const B64* addr);\n```\n\nLANGUAGE: CUDA\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.b64 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .xor }\ntemplate <typename B64, enable_if_t<sizeof(B64) == 8, bool> = true, cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline B64 multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_xor_op_t,\n  const B64* addr);\n```\n\n----------------------------------------\n\nTITLE: Configuring and Building libcudacxx Tests with NVRTC using CMake and lit\nDESCRIPTION: Shell commands demonstrating the process to configure a CMake project (specifically libcudacxx) to enable NVRTC testing via `nvrtcc`, followed by building the project and running the tests using the `lit` command-line tool. This requires CMake, `lit`, and the relevant project source.\nSOURCE: https://github.com/nvidia/cccl/blob/main/libcudacxx/test/utils/nvidia/nvrtc/README.md#2025-04-23_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ncmake ... -DLIBCUDACXX_TEST_WITH_NVRTC=ON\ncmake --build $BUILD_DIR\nlit ... $TEST_DIR\n```\n\n----------------------------------------\n\nTITLE: Compiling Benchmarks with Line Info for Profiling (Bash)\nDESCRIPTION: Shows how to recompile benchmarks with the -lineinfo option using CMake to enable source-level profiling metrics.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/benchmarking.rst#2025-04-23_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\ncmake .. --preset=cub-benchmark -DCMAKE_CUDA_FLAGS=-lineinfo -DCMAKE_CUDA_ARCHITECTURES=90 # TODO: Set your GPU architecture\n```\n\n----------------------------------------\n\nTITLE: Declaring tcgen05_mma_block_scale_vec_4x_collector_a_discard for MXF4NVF4 kind\nDESCRIPTION: Template function declaration for 4X vector scaling operations with collector A discard. Specifically for MXF4NVF4 kind with support for CTA groups 1 and 2. Implements tensor compute operations for PTX ISA 86 and SM_100a/SM_101a architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma.rst#2025-04-23_snippet_48\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_mma_block_scale_vec_4x_collector_a_discard(\n  cuda::ptx::kind_mxf4nvf4_t,\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  uint32_t scale_A_tmem,\n  uint32_t scale_B_tmem,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Implementing Block-Scaled MMA for mxf4nvf4 with 4X Vector Scaling in CUDA\nDESCRIPTION: This template function implements a block-scaled matrix multiply-accumulate operation for mxf4nvf4 data type with 4X vector scaling. It supports different CTA group configurations.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma.rst#2025-04-23_snippet_16\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.cta_group.kind.block_scale.scale_vec::4X [d_tmem], a_desc, b_desc, idesc, [scale_A_tmem], [scale_B_tmem], enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .kind      = { .kind::mxf4nvf4 }\n// .cta_group = { .cta_group::1, .cta_group::2 }\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_mma_block_scale_vec_4x(\n  cuda::ptx::kind_mxf4nvf4_t,\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  uint32_t scale_A_tmem,\n  uint32_t scale_B_tmem,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Setting up Sphinx toctree for Thrust Transformations Documentation\nDESCRIPTION: A reStructuredText directive that creates a table of contents for transformation algorithm documentation. It includes subcategories and a variable path reference to the repository documentation API path.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/api_docs/algorithms/transformations.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :glob:\n   :maxdepth: 1\n\n   transformations/filling\n   transformations/modifying\n   transformations/replacing\n   ${repo_docs_api_path}/*function_group__transformations*\n```\n\n----------------------------------------\n\nTITLE: Declaring cuda::pipeline::consumer_release Function in CUDA C++\nDESCRIPTION: This snippet shows the declaration of the consumer_release function template for the cuda::pipeline class. It is a member function that releases the current pipeline stage and is available for different thread scopes.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/synchronization_primitives/pipeline/consumer_release.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::thread_scope Scope>\n__host__ __device__\nvoid cuda::pipeline<Scope>::consumer_release();\n```\n\n----------------------------------------\n\nTITLE: Installing pre-commit hooks for automatic execution in CCCL Project\nDESCRIPTION: This command sets up pre-commit hooks to run automatically when making a git commit in the CCCL project.\nSOURCE: https://github.com/nvidia/cccl/blob/main/CONTRIBUTING.md#2025-04-23_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npre-commit install\n```\n\n----------------------------------------\n\nTITLE: Declaring Load-Reduce Minimum Operation (acquire/gpu/global/u32) in CUDA C++\nDESCRIPTION: Declares a device inline template for atomic min reduction on global uint32_t values, leveraging acquire or relaxed PTX semaphore semantics and any specified scope, including entire GPU. Designed to match hardware-level PTX instructions (PTX ISA 81) and requires correct PTX type usage for templates.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_7\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.u32 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .min }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline uint32_t multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_min_t,\n  const uint32_t* addr);\n```\n\n----------------------------------------\n\nTITLE: Linux Debug Build Command\nDESCRIPTION: Make command for building the sample with debug symbols enabled.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cudax/test/stf/cuda-samples/3_CUDA_Features/jacobiCudaGraphs/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ make dbg=1\n```\n\n----------------------------------------\n\nTITLE: Release Barrier Cluster Arrive Operation\nDESCRIPTION: Device function for cluster barrier arrive with release semantics. Requires PTX ISA 80 and SM_90. Implementation is volatile and clobbers memory.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/barrier_cluster.rst#2025-04-23_snippet_2\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline void barrier_cluster_arrive(\n  cuda::ptx::sem_release_t);\n```\n\n----------------------------------------\n\nTITLE: Including Subdirectory for CUDA-Specific Examples - CMake\nDESCRIPTION: This line adds the 'cuda' subdirectory to the current CMake build tree, which is expected to contain CUDA-specific example projects or further configurations. This ensures any additional CUDA-targeted logic or sources are integrated for build and test. Requires a valid 'cuda' subdirectory with its own CMakeLists.txt.\nSOURCE: https://github.com/nvidia/cccl/blob/main/thrust/examples/CMakeLists.txt#2025-04-23_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(cuda)\n\n```\n\n----------------------------------------\n\nTITLE: Defining cudax_add_catch2_test Function in CMake\nDESCRIPTION: A CMake function that creates Catch2 test executables and registers them with CTest. It sets up the test target with appropriate include directories, link libraries, and compiler options. The function also clones properties from a reference target and adds the test to a meta-target for the current configuration.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cudax/test/CMakeLists.txt#2025-04-23_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(cudax_add_catch2_test target_name_var test_name cn_target) # ARGN=test sources\n  cudax_get_target_property(config_prefix ${cn_target} PREFIX)\n\n  set(test_target ${config_prefix}.test.${test_name})\n  set(test_sources ${ARGN})\n\n  add_executable(${test_target} ${test_sources})\n  target_include_directories(${test_target} PRIVATE \"common\")\n  target_link_libraries(${test_target} PRIVATE\n    ${cn_target}\n    cccl.c2h.main\n  )\n  target_compile_options(${test_target} PRIVATE\n    \"-DLIBCUDACXX_ENABLE_EXPERIMENTAL_MEMORY_RESOURCE\"\n    $<$<COMPILE_LANG_AND_ID:CUDA,NVIDIA>:--extended-lambda>\n  )\n\n  cudax_clone_target_properties(${test_target} ${cn_target})\n  set_target_properties(${test_target} PROPERTIES\n    CUDA_ARCHITECTURES \"${CMAKE_CUDA_ARCHITECTURES}\"\n  )\n\n  set(config_meta_target ${config_prefix}.tests)\n  add_dependencies(${config_meta_target} ${test_target})\n\n  add_test(NAME ${test_target} COMMAND \"$<TARGET_FILE:${test_target}>\")\n\n  set(${target_name_var} ${test_target} PARENT_SCOPE)\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Windows Build Command\nDESCRIPTION: Command format for Visual Studio solution files used to build the Windows samples. Solutions are provided for different Visual Studio versions.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cudax/test/stf/cuda-samples/3_CUDA_Features/jacobiCudaGraphs/README.md#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n*_vs<version>.sln - for Visual Studio <version>\n```\n\n----------------------------------------\n\nTITLE: Test Source File Discovery and Registration\nDESCRIPTION: CMake code that discovers all test source files (*.cu and *.cpp) in the current directory and registers them as tests using the previously defined configuration function.\nSOURCE: https://github.com/nvidia/cccl/blob/main/c/parallel/test/CMakeLists.txt#2025-04-23_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB test_srcs\n  RELATIVE \"${CMAKE_CURRENT_LIST_DIR}\"\n  CONFIGURE_DEPENDS\n  *.cu *.cpp\n)\n\nforeach(test_src IN LISTS test_srcs)\n  cccl_c_parallel_add_test(test_target \"${test_src}\")\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Running Benchmark with Filtered Workloads\nDESCRIPTION: Command showing how to run benchmarks with specific axis restrictions to test only certain workload configurations\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/benchmarking.rst#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n./bin/cub.bench.adjacent_difference.subtract_left.base ...\\\n    -a 'T{ct}=I32'\\\n    -a 'OffsetT{ct}=I32'\\\n    -a 'Elements{io}[pow2]=[24,28]'\n```\n\n----------------------------------------\n\nTITLE: Creating the C2H Test Runner Library\nDESCRIPTION: Defines an object library for the C2H test runner, including both C++ and CUDA source files, and links it with the main C2H library.\nSOURCE: https://github.com/nvidia/cccl/blob/main/c2h/CMakeLists.txt#2025-04-23_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(cccl.c2h.main OBJECT\n  catch2_runner.cpp\n  catch2_runner_helper.cu\n)\ntarget_link_libraries(cccl.c2h.main PUBLIC cccl.c2h)\n```\n\n----------------------------------------\n\nTITLE: Async Store with Barrier Completion - 32-bit Vector2\nDESCRIPTION: Template for asynchronous weak shared memory store operation with cluster barrier completion tracking for 2-element vectors of 32-bit values. Compatible with PTX ISA 81 and SM_90 architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/st_async.rst#2025-04-23_snippet_2\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename Type>\n__device__ static inline void st_async(\n  Type* addr,\n  const Type (&value)[2],\n  uint64_t* remote_bar);\n```\n\n----------------------------------------\n\nTITLE: CTA Group Template Function for 64x128b with Warp Configuration 02_13\nDESCRIPTION: Template function declaration for 64x128b memory layout with warp configuration 02_13 supporting CTA groups 1 and 2. Takes a CTA group parameter, target address, and descriptor as inputs.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_cp.rst#2025-04-23_snippet_3\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.cp.cta_group.64x128b.warpx2::02_13 [taddr], s_desc; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1, .cta_group::2 }\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_cp_64x128b_warpx2_02_13(\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t taddr,\n  uint64_t s_desc);\n```\n\n----------------------------------------\n\nTITLE: Modifying scan and reduce interfaces in CUB 1.6.0\nDESCRIPTION: Interface changes for device/block/warp-wide exclusive scans to accept an 'initial value' instead of an 'identity value'. Device-wide reductions and scans now support different input and output sequence types.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/releases/changelog.rst#2025-04-23_snippet_8\n\nLANGUAGE: C++\nCODE:\n```\n// Device/block/warp-wide exclusive scans\n// Device-wide reductions and scans\n```\n\n----------------------------------------\n\nTITLE: Copying and Viewing Nsight Compute Report (Bash)\nDESCRIPTION: Shows how to copy the Nsight Compute report from a remote machine and view it using ncu-ui on a local workstation.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/benchmarking.rst#2025-04-23_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nscp <remote hostname>:<cccl repo directory>/build/base.ncu-rep .\nncu-ui base.ncu-rep\n```\n\n----------------------------------------\n\nTITLE: Basic Shared Memory Barrier Arrive\nDESCRIPTION: Basic shared memory barrier arrive operation for SM_80 architecture. Takes a memory address and returns barrier state.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/mbarrier_arrive.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline uint64_t mbarrier_arrive(\n  uint64_t* addr);\n```\n\n----------------------------------------\n\nTITLE: CMake Function for Creating Individual CUDA C++ Header Tests\nDESCRIPTION: Defines a function that creates a test target for a single header file. It configures a test source file from a template, sets up the necessary compiler flags and definitions, and links against the appropriate CUDA runtime library based on the compiler being used.\nSOURCE: https://github.com/nvidia/cccl/blob/main/libcudacxx/test/public_headers_host_only/CMakeLists.txt#2025-04-23_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(libcudacxx_add_std_header_test header)\n  # ${header} contains the \"/\" from the subfolder, replace by \"_\" for actual names\n  string(REPLACE \"/\" \"_\" header_name \"${header}\")\n\n  # Create the source file for the header target from the template and add the file to the global project\n  set(headertest_src \"headers/${header_name}\")\n  configure_file(\"${CMAKE_CURRENT_SOURCE_DIR}/header_test.cpp.in\" \"${headertest_src}.cpp\")\n\n  # Create the default target for that file\n  set(headertest_std_${header_name} verify_${header_name})\n  add_library(headertest_std_${header_name} SHARED \"${headertest_src}.cpp\")\n  target_include_directories(headertest_std_${header_name} PRIVATE \"${libcudacxx_SOURCE_DIR}/include\")\n  target_compile_options(headertest_std_${header_name} PRIVATE ${headertest_warning_levels_host})\n  target_compile_definitions(headertest_std_${header_name} PRIVATE CCCL_ENABLE_ASSERTIONS)\n  target_compile_definitions(headertest_std_${header_name} PRIVATE CCCL_IGNORE_DEPRECATED_CPP_DIALECT)\n  target_compile_definitions(headertest_std_${header_name} PRIVATE CCCL_ENABLE_OPTIONAL_REF)\n\n  # We want to ensure that we can build headers within <cuda/> with a host compiler but we need cuda_runtime_api.h\n  if (\"NVHPC\" STREQUAL \"${CMAKE_CXX_COMPILER_ID}\")\n    target_link_libraries(headertest_std_${header_name} NVHPC::CUDART)\n  else()\n    target_link_libraries(headertest_std_${header_name} CUDA::cudart)\n  endif()\n\n  add_dependencies(libcudacxx.test.public_headers_host_only headertest_std_${header_name})\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Setting up RST Table of Contents for Memory Management Documentation\nDESCRIPTION: Configures a reStructuredText table of contents for the Memory Management module, organizing documentation for allocators, memory resources, and other memory management functions.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/api_docs/memory_management.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :glob:\n   :maxdepth: 2\n\n   memory_management/allocators\n   memory_management/memory_resources\n   ${repo_docs_api_path}/*function_group__memory__management*\n```\n\n----------------------------------------\n\nTITLE: Defining CUDA Device Barrier API Function\nDESCRIPTION: Function signature for barrier_arrive_tx that handles barrier arrival with transaction count updates in shared memory.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/synchronization_primitives/barrier/barrier_arrive_tx.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\n__device__\ncuda::barrier<cuda::thread_scope_block>::arrival_token\ncuda::device::barrier_arrive_tx(\n  cuda::barrier<cuda::thread_scope_block>& bar,\n  ptrdiff_t arrive_count_update,\n  ptrdiff_t transaction_count_update);\n```\n\n----------------------------------------\n\nTITLE: Including C Standard Integer Types Header in CUDA C++ (Fundamental)\nDESCRIPTION: Includes the `<cuda/std/cstdint>` header, providing fundamental integer types. Available since CCCL 2.2.0 and CUDA Toolkit 12.3.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/standard_api/c_library.rst#2025-04-23_snippet_7\n\nLANGUAGE: C++\nCODE:\n```\n#include <cuda/std/cstdint>\n```\n\n----------------------------------------\n\nTITLE: Conditionally Adding CUDA 12+ Extension Tests\nDESCRIPTION: Checks if CUDA Toolkit version is 12 or higher and conditionally adds tests for CUDA extensions and STF features that are only available in newer CUDA versions.\nSOURCE: https://github.com/nvidia/cccl/blob/main/examples/CMakeLists.txt#2025-04-23_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nif (CUDAToolkit_VERSION_MAJOR VERSION_GREATER_EQUAL 12)\n  cccl_add_compile_test(test_name\n    cccl.example\n    cudax\n    \"default\"\n    ${cmake_opts}\n    ${cmake_cpm_opts}\n  )\n\n  cccl_add_compile_test(test_name\n    cccl.example\n    cudax_stf\n    \"default\"\n    ${cmake_opts}\n    ${cmake_cpm_opts}\n  )\nendif() # CTK > 12.0\n```\n\n----------------------------------------\n\nTITLE: Implementing multimem.ld_reduce for weak global add operations on 64-bit integers\nDESCRIPTION: Template implementation for multimem atomic load-and-reduce operations with weak semantics, global memory, and add operation on 64-bit integers. Compatible with PTX ISA 81 on SM_90 architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_40\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.global.op.u64 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .weak }\n// .op        = { .add }\ntemplate <typename = void>\n__device__ static inline int64_t multimem_ld_reduce(\n  cuda::ptx::sem_weak_t,\n  cuda::ptx::op_add_t,\n  const int64_t* addr);\n```\n\n----------------------------------------\n\nTITLE: Generating DOT Graph for AXPY Example in CUDASTF\nDESCRIPTION: Command for generating a DOT graph file from the AXPY example and converting it to PNG format.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_52\n\nLANGUAGE: bash\nCODE:\n```\nCUDASTF_DOT_FILE=heat.dot build/examples/heat_mgpu 1000 8 4\ndot -Tpng heat.dot -o heat.png\n```\n\n----------------------------------------\n\nTITLE: Binary Find Shift Amount for 64-bit Signed Integer\nDESCRIPTION: Device function template implementing bfind shift amount operation for int64_t type. Returns shift amount for the highest set bit in a 64-bit signed integer.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/bfind.rst#2025-04-23_snippet_7\n\nLANGUAGE: cuda\nCODE:\n```\n// bfind.shiftamt.s64 dest, a_reg; // PTX ISA 20, SM_50\ntemplate <typename = void>\n__device__ static inline uint32_t bfind_shiftamt(\n  int64_t a_reg);\n```\n\n----------------------------------------\n\nTITLE: Running NCU Performance Analysis on CUDASTF Application\nDESCRIPTION: Command to analyze kernel performance of a CUDASTF application using NVIDIA Compute Profiler (ncu) with NVTX kernel renaming.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_56\n\nLANGUAGE: bash\nCODE:\n```\nncu --section=ComputeWorkloadAnalysis --print-nvtx-rename=kernel --nvtx -o output build/examples/miniWeather\n```\n\n----------------------------------------\n\nTITLE: Defining STF Example Build Function\nDESCRIPTION: CMake function that configures and builds an STF example executable, sets up proper linking and testing, and registers it with CTest.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cudax/examples/stf/CMakeLists.txt#2025-04-23_snippet_3\n\nLANGUAGE: cmake-function\nCODE:\n```\nfunction(cudax_add_stf_example target_name_var source cn_target)\n  cudax_get_target_property(config_dialect ${cn_target} DIALECT)\n  cudax_get_target_property(config_prefix ${cn_target} PREFIX)\n\n  get_filename_component(dir ${source} DIRECTORY)\n  get_filename_component(filename ${source} NAME_WE)\n  if (dir)\n    set(filename \"${dir}/${filename}\")\n  endif()\n  string(REPLACE \"/\" \".\" example_name \"stf/${filename}\")\n\n  set(example_target ${config_prefix}.example.${example_name})\n\n  add_executable(${example_target} ${source})\n  cccl_configure_target(${example_target} DIALECT ${config_dialect})\n  cudax_clone_target_properties(${example_target} ${cn_target})\n  cudax_stf_configure_target(${example_target} ${ARGN})\n  target_link_libraries(${example_target} PRIVATE\n    cudax.examples.thrust\n    CUB::CUB\n  )\n\n  set(stf_meta_target ${config_prefix}.examples.stf)\n  add_dependencies(${stf_meta_target} ${example_target})\n\n  add_test(NAME ${example_target} COMMAND \"$<TARGET_FILE:${example_target}>\")\n\n  set(${target_name_var} ${example_target} PARENT_SCOPE)\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Initializing CMake Project for C2H with CUDA and C++ Support\nDESCRIPTION: Sets up the CMake project named C2H, specifying minimum CMake version and enabling C++ and CUDA languages. It also retrieves Catch2 for testing purposes.\nSOURCE: https://github.com/nvidia/cccl/blob/main/c2h/CMakeLists.txt#2025-04-23_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.21)\n\nproject(C2H LANGUAGES CXX CUDA)\n\ncccl_get_catch2()\n```\n\n----------------------------------------\n\nTITLE: Configuring CCCL CPM Repository Settings\nDESCRIPTION: Sets up CPM (CMake Package Manager) repository configuration for CCCL examples. Defines the source repository and Git tag to be used when building examples via CPM.\nSOURCE: https://github.com/nvidia/cccl/blob/main/examples/CMakeLists.txt#2025-04-23_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nset(CCCL_EXAMPLE_CPM_REPOSITORY \"${CCCL_SOURCE_DIR}\" CACHE STRING \"Git repository used for CPM examples.\")\nset(CCCL_EXAMPLE_CPM_TAG \"HEAD\" CACHE STRING \"Git tag/branch used for CPM examples.\")\n```\n\n----------------------------------------\n\nTITLE: Defining tcgen05_mma_ws_collector_b0_use Template (with mask) in CUDA\nDESCRIPTION: Defines a templated CUDA `__device__` static inline function `tcgen05_mma_ws_collector_b0_use`. This function handles collector `b0` use operations within `cta_group::1`, supporting various `dot_kind` types (f16, tf32, f8f6f4, i8). It operates on destination memory (`d_tmem`) using descriptors (`a_desc`, `b_desc`, `idesc`), an enable flag (`enable_input_d`), and a zero column mask descriptor (`zero_column_mask_desc`). Designed for PTX ISA 86, SM_100a, SM_101a.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_5\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.ws.cta_group.kind.collector::b0::use [d_tmem], a_desc, b_desc, idesc, enable_input_d, zero_column_mask_desc; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1 }\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_collector_b0_use(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d,\n  uint64_t zero_column_mask_desc);\n```\n\n----------------------------------------\n\nTITLE: Implementing multimem.red.release.sys.global.max.s64 Operation in CUDA PTX\nDESCRIPTION: Template function declaration for performing a 64-bit signed integer max reduction with release semantics and system-wide scope in global memory. This function is available for SM_90 and requires PTX ISA 8.1 or higher.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_red.rst#2025-04-23_snippet_17\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.red.sem.scope.global.op.s64 [addr], val; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .release }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .max }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline void multimem_red(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_max_t,\n  int64_t* addr,\n  int64_t val);\n```\n\n----------------------------------------\n\nTITLE: Adding STF Test Executable in CMake\nDESCRIPTION: Defines a function to add an STF test executable and register it with CTest. It configures the target with the specified dialect and properties.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cudax/test/stf/CMakeLists.txt#2025-04-23_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(cudax_add_stf_test target_name_var source cn_target)\n  cudax_get_target_property(config_dialect ${cn_target} DIALECT)\n  cudax_get_target_property(config_prefix ${cn_target} PREFIX)\n\n  # ... (function implementation)\n\n  add_test(NAME ${test_target} COMMAND \"$<TARGET_FILE:${test_target}>\")\n\n  set(${target_name_var} ${test_target} PARENT_SCOPE)\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Implementation of Type Info Mapping for JIT Templates\nDESCRIPTION: Shows how to define an argument mapping structure for c.parallel types to be used in NVRTC-compiled device code, with archetype definition and parameter mapping functions.\nSOURCE: https://github.com/nvidia/cccl/blob/main/c/parallel/src/jit_templates/README.md#2025-04-23_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n// The mapping structure itself. This defines what is passed as template arguments.\ntemplate <typename T>\nstruct cccl_type_info_mapping\n{\n  using Type = T;\n};\n\n// For why this ifndef is necessary, please see the CMake section.\n#ifndef _CCCL_C_PARALLEL_JIT_TEMPLATES_PREPROCESS\n#  include \"../traits.h\"\n\n// To register a mapping for a given type (generally a c.parallel type, but this can be anything), define a\n// specialization of the `parameter_mapping` template declared in `traits.h`.\ntemplate <>\nstruct parameter_mapping<cccl_type_info>\n{\n  // Every parameter type must provide an archetype value. This value will be used by `get_specialization` to perform\n  // its rudimentary type checking, by instantiating the given template with it as one of the template arguments.\n  static const constexpr auto archetype = cccl_type_info_mapping<int>{};\n\n  // This function defines what the mapping is. It receives the template id of the template being instantiated (this\n  // allows the mapping to handle certain target templates in a special way) and the argument that was passed to\n  // `get_specialization`.\n  //\n  // The returned string will be used in the constructed C++ type name, in the position corresponding to the position of\n  // the original argument in the call to `get_specialization`.\n  template <typename TplId>\n  static std::string map(TplId, cccl_type_info arg)\n  {\n    return std::format(\"cccl_type_info_mapping<{}>{{}}\", cccl_type_enum_to_string(arg.type));\n  }\n\n  // This function defines any additional code that is required for the value returned from `map` to be well formed when\n  // compiled. It is invoked with the same arguments as `map`.\n  //\n  // The common use case for this feature is declaring extern functions, whose names are carried by the argument.\n  template <typename TplId>\n  static std::string aux(TplId, cccl_type_info)\n  {\n    return {};\n  }\n};\n#endif\n```\n\n----------------------------------------\n\nTITLE: Implementing CUDA PTX Multimemory AND Reduction Template Function for 64-bit Values\nDESCRIPTION: Template function for 64-bit AND reduction operations with configurable memory semantics and scope. This function supports atomic AND operations on 64-bit values in global memory with different synchronization scopes.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_red.rst#2025-04-23_snippet_36\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.red.sem.scope.global.op.b64 [addr], val; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .release }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .and }\ntemplate <typename B64, enable_if_t<sizeof(B64) == 8, bool> = true, cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline void multimem_red(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_and_op_t,\n  B64* addr,\n  B64 val);\n```\n\n----------------------------------------\n\nTITLE: Running pre-commit checks on all files for CCCL Project\nDESCRIPTION: This command executes pre-commit checks on all files in the CCCL project, not just staged changes.\nSOURCE: https://github.com/nvidia/cccl/blob/main/CONTRIBUTING.md#2025-04-23_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npre-commit run --all-files\n```\n\n----------------------------------------\n\nTITLE: Example %PARAM% Comments in CUB Test File\nDESCRIPTION: Example showing how to define multiple parameters in a test file. This will generate six different test executables with various combinations of preprocessor definitions.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cub/test/README.md#2025-04-23_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n// %PARAM% TEST_FOO foo 0:1:2\n// %PARAM% TEST_LAUNCH lid 0:1\n```\n\n----------------------------------------\n\nTITLE: Basic Memory Barrier Try Wait\nDESCRIPTION: Basic memory barrier try_wait operation without semantics specification. Takes memory address and state as parameters.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/mbarrier_try_wait.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline bool mbarrier_try_wait(\n  uint64_t* addr,\n  const uint64_t& state);\n```\n\n----------------------------------------\n\nTITLE: Creating Miscellaneous Utility Tests in CMake\nDESCRIPTION: Configures tests for various utility functions and classes in the CCCL library, including basic_any implementation, driver API interactions, and device context management.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cudax/test/CMakeLists.txt#2025-04-23_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\n  cudax_add_catch2_test(test_target misc ${cn_target}\n    utility/basic_any.cu\n    utility/driver_api.cu\n    utility/ensure_current_device.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: Installing libcudacxx Headers with CMake\nDESCRIPTION: Installs the libcudacxx header files. It copies the *contents* of the directory `${libcudacxx_SOURCE_DIR}/include/` into the destination directory specified by `${_dest_incl_dir}`. The presence of a trailing slash (`/`) in the source path signifies that only the content of the `include` directory should be copied, not the `include` directory itself.\nSOURCE: https://github.com/nvidia/cccl/blob/main/python/cuda_cccl/CMakeLists.txt#2025-04-23_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\n# Slash at the end: copy content of\n#                   include/ into ${_dest_inc_dir}/\ninstall(\n    DIRECTORY ${libcudacxx_SOURCE_DIR}/include/\n    DESTINATION ${_dest_incl_dir}\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents for Segmented Prefix Sum Documentation in RST\nDESCRIPTION: This RST code creates a table of contents for the segmented prefix sum documentation in the Thrust module. It uses a glob pattern to include all files that match the segmented prefix function group pattern.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/api_docs/algorithms/prefix_sums/segmented.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :glob:\n   :maxdepth: 1\n\n   ${repo_docs_api_path}/*function_group__segmentedprefix*\n```\n\n----------------------------------------\n\nTITLE: ReStructuredText Documentation Reference for elect.sync\nDESCRIPTION: ReStructuredText markup defining documentation links and structure for the elect.sync PTX instruction documentation.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/elect_sync.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _libcudacxx-ptx-instructions-elect_sync:\n\nelect.sync\n==========\n\n-  PTX ISA:\n   `elect.sync <https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-elect-sync>`__\n\n.. include:: generated/elect_sync.rst\n```\n\n----------------------------------------\n\nTITLE: Declaring cuda::pipeline::quit Function in CUDA C++\nDESCRIPTION: This code snippet declares the cuda::pipeline::quit function template. It is a member function of the cuda::pipeline class template, which takes a thread_scope template parameter. The function is marked as both __host__ and __device__, allowing it to be called from both CPU and GPU code.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/synchronization_primitives/pipeline/quit.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::thread_scope Scope>\n__host__ __device__\nbool cuda::pipeline<Scope>::quit();\n```\n\n----------------------------------------\n\nTITLE: Configuring Testing Support\nDESCRIPTION: Enables CTest support and adds a test for the vector_add sample, primarily for internal testing purposes.\nSOURCE: https://github.com/nvidia/cccl/blob/main/examples/cudax/CMakeLists.txt#2025-04-23_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\ninclude(CTest)\nenable_testing()\nadd_test(NAME vector_add COMMAND vector_add)\n```\n\n----------------------------------------\n\nTITLE: Declaring multimem_ld_reduce with Weak Semantics and XOR Operation in CUDA PTX\nDESCRIPTION: Template function declaration for multimem.ld_reduce with weak memory semantics and XOR operation on 64-bit values in global memory. Designed for PTX ISA 8.1 and SM_90 hardware.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_69\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.global.op.b64 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .weak }\n// .op        = { .xor }\ntemplate <typename B64, enable_if_t<sizeof(B64) == 8, bool> = true>\n__device__ static inline B64 multimem_ld_reduce(\n  cuda::ptx::sem_weak_t,\n  cuda::ptx::op_xor_op_t,\n  const B64* addr);\n```\n\n----------------------------------------\n\nTITLE: Configuring toctree for Shuffling Algorithms in Thrust Module\nDESCRIPTION: This RST directive creates a table of contents for shuffling-related function documentation. It uses a glob pattern to include all documentation files that match the shuffling function group pattern.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/api_docs/algorithms/reordering/shuffling.rst#2025-04-23_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. toctree::\n   :glob:\n   :maxdepth: 1\n\n   ${repo_docs_api_path}/*function_group__shuffling*\n```\n\n----------------------------------------\n\nTITLE: Binary Find Operation for 64-bit Unsigned Integer\nDESCRIPTION: Device function template implementing bfind operation for uint64_t type. Returns position of the highest set bit in a 64-bit unsigned integer.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/bfind.rst#2025-04-23_snippet_2\n\nLANGUAGE: cuda\nCODE:\n```\n// bfind.u64 dest, a_reg; // PTX ISA 20, SM_50\ntemplate <typename = void>\n__device__ static inline uint32_t bfind(\n  uint64_t a_reg);\n```\n\n----------------------------------------\n\nTITLE: Setting up Tests for Atomics Code Generation\nDESCRIPTION: Creates a test target that verifies the installed file matches the generated file. The test uses CMake's compare_files command to ensure the files are identical. Additional test properties specify required files for the test to run.\nSOURCE: https://github.com/nvidia/cccl/blob/main/libcudacxx/codegen/CMakeLists.txt#2025-04-23_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nadd_test(\n    NAME libcudacxx.test.atomics.codegen.diff\n    COMMAND ${CMAKE_COMMAND} -E compare_files \"${atomic_install_location}/cuda_ptx_generated.h\" \"${atomic_generated_output}\"\n)\n\nset_tests_properties(\n    libcudacxx.test.atomics.codegen.diff\n    PROPERTIES REQUIRED_FILES \"${atomic_generated_output}\"\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing Thrust Dependencies in CMake\nDESCRIPTION: Sets up Thrust package dependencies with specific version matching and creates a target for CUDA examples.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cudax/examples/CMakeLists.txt#2025-04-23_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfind_package(cudax) # already found, bring in version info.\nfind_package(Thrust ${cudax_VERSION} EXACT CONFIG\n  NO_DEFAULT_PATH # Only check the explicit path in HINTS:\n  HINTS \"${CCCL_SOURCE_DIR}/lib/cmake/thrust/\"\n)\nthrust_create_target(cudax.examples.thrust)\n```\n\n----------------------------------------\n\nTITLE: Installing Thrust Headers with CMake\nDESCRIPTION: Installs the Thrust header files. It copies the entire `thrust` directory located at `${Thrust_SOURCE_DIR}/thrust` into the destination directory specified by `${_dest_incl_dir}`. Similar to the CUB installation, the lack of a trailing slash means a `thrust` directory is created within the destination.\nSOURCE: https://github.com/nvidia/cccl/blob/main/python/cuda_cccl/CMakeLists.txt#2025-04-23_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\n# No end slash: create ${_dest_inc_dir}/thrust\ninstall(\n    DIRECTORY ${Thrust_SOURCE_DIR}/thrust\n    DESTINATION ${_dest_incl_dir}\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring CCCL Repository Variables\nDESCRIPTION: Sets variables for CCCL repository URL and branch tag that can be overridden in CI environments.\nSOURCE: https://github.com/nvidia/cccl/blob/main/examples/cudax/CMakeLists.txt#2025-04-23_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nset(CCCL_REPOSITORY \"https://github.com/NVIDIA/cccl\" CACHE STRING \"Git repository to fetch CCCL from\")\nset(CCCL_TAG \"main\" CACHE STRING \"Git tag/branch to fetch from CCCL repository\")\n```\n\n----------------------------------------\n\nTITLE: CUDA Kernel Launch Sequence\nDESCRIPTION: Sequential kernel launches to update multiple arrays using the same coefficients.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/memory_access_properties/apply_access_property.rst#2025-04-23_snippet_3\n\nLANGUAGE: cuda\nCODE:\n```\nupdate<<<grid, block>>>(x, a, b, N);\nupdate<<<grid, block>>>(y, a, b, N);\nupdate<<<grid, block>>>(z, a, b, N);\n```\n\n----------------------------------------\n\nTITLE: Setting Common CMake Build Options for CCCL\nDESCRIPTION: Configures common CMake build options including build type, compiler settings, and CUDA architectures. These options will be passed to all compile tests to ensure consistent build configuration.\nSOURCE: https://github.com/nvidia/cccl/blob/main/examples/CMakeLists.txt#2025-04-23_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(cmake_opts\n  -D \"CMAKE_BUILD_TYPE=${CMAKE_BUILD_TYPE}\"\n  -D \"CMAKE_MAKE_PROGRAM=${CMAKE_MAKE_PROGRAM}\"\n  -D \"CMAKE_CXX_COMPILER=${CMAKE_CXX_COMPILER}\"\n  -D \"CMAKE_CUDA_COMPILER=${CMAKE_CUDA_COMPILER}\"\n  -D \"CMAKE_CUDA_HOST_COMPILER=${CMAKE_CUDA_HOST_COMPILER}\"\n  -D \"CMAKE_CUDA_ARCHITECTURES=${arches_escaped}\"\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing multimem.red.release.sys.global.max.s32 Operation in CUDA PTX\nDESCRIPTION: Template function declaration for performing a 32-bit signed integer max reduction with release semantics and system-wide scope in global memory. This function is available for SM_90 and requires PTX ISA 8.1 or higher.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_red.rst#2025-04-23_snippet_9\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.red.sem.scope.global.op.s32 [addr], val; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .release }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .max }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline void multimem_red(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_max_t,\n  int32_t* addr,\n  int32_t val);\n```\n\n----------------------------------------\n\nTITLE: Declaring Pipeline Producer Commit Function - CUDA\nDESCRIPTION: Function declaration for pipeline_producer_commit that binds pipeline operations to a CUDA barrier. Takes a thread-scoped pipeline and barrier as parameters.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/synchronization_primitives/pipeline/pipeline_producer_commit.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::thread_scope Scope>\n__host__ __device__\nvoid cuda::pipeline_producer_commit(cuda::pipeline<cuda::thread_scope_thread>& pipe,\n                                     cuda::barrier<Scope>& bar);\n```\n\n----------------------------------------\n\nTITLE: Installing pre-commit via conda for CCCL Project\nDESCRIPTION: This snippet shows how to install pre-commit using conda for the CCCL project. It adds the conda-forge channel and then installs pre-commit.\nSOURCE: https://github.com/nvidia/cccl/blob/main/CONTRIBUTING.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nconda config --add channels conda-forge\nconda install pre-commit\n```\n\n----------------------------------------\n\nTITLE: Displaying Asynchronous Operations in DOT Graph\nDESCRIPTION: Command to include internally generated asynchronous operations in the visualization by disabling the PREREQS ignore flag.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_53\n\nLANGUAGE: c++\nCODE:\n```\nCUDASTF_DOT_IGNORE_PREREQS=0 CUDASTF_DOT_FILE=axpy-with-events.dot build/examples/01-axpy\ndot -Tpng axpy-with-events.dot -o axpy-with-events.png\n```\n\n----------------------------------------\n\nTITLE: Filtering Headers Not Suitable for Standalone Testing\nDESCRIPTION: Applies several filters to exclude headers that shouldn't be tested in isolation. Excludes CUDA extension headers, MSVC-incompatible mdspan headers, and generated PTX headers.\nSOURCE: https://github.com/nvidia/cccl/blob/main/libcudacxx/test/internal_headers/CMakeLists.txt#2025-04-23_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\n# headers in `__cuda` are meant to come after the related \"cuda\" headers so they do not compile on their own\nlist(FILTER internal_headers EXCLUDE REGEX \"__cuda/*\")\n\n# mdspan is currently not supported on msvc outside of C++20\nif (\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"MSVC\" AND NOT \"${CMAKE_CXX_STANDARD}\" MATCHES \"20\")\n  list(FILTER internal_headers EXCLUDE REGEX \"mdspan\")\nendif()\n\n# generated cuda::ptx headers are not standalone\nlist(FILTER internal_headers EXCLUDE REGEX \"__ptx/instructions/generated\")\n```\n\n----------------------------------------\n\nTITLE: Setting up table of contents for Thrust comparison algorithms documentation\nDESCRIPTION: This snippet configures a reStructuredText toctree directive to include all documentation files that match the specified glob pattern for comparison functions. The toctree is configured with maxdepth 1 to show only direct child pages.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/api_docs/algorithms/reductions/comparisons.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :glob:\n   :maxdepth: 1\n\n   ${repo_docs_api_path}/*function_group__comparisons*\n```\n\n----------------------------------------\n\nTITLE: Finding Test Source Files\nDESCRIPTION: Uses glob to find all test source files in the project, looking for files with test_*.cu or catch2_test_*.cu naming patterns. The CONFIGURE_DEPENDS flag ensures CMake reconfiguration when files are added or removed.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cub/test/CMakeLists.txt#2025-04-23_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB_RECURSE test_srcs\n  RELATIVE \"${CUB_SOURCE_DIR}/test\"\n  CONFIGURE_DEPENDS\n  test_*.cu\n  catch2_test_*.cu\n)\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Link for SHL PTX Instruction\nDESCRIPTION: ReStructuredText markup defining a reference label and external link to the SHL PTX instruction documentation in the NVIDIA PTX ISA guide.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/shl.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _libcudacxx-ptx-instructions-shl:\n\nshl\n===\n\n-  PTX ISA:\n   `shl <https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#logic-and-shift-instructions-shl>`__\n\n.. include:: generated/shl.rst\n```\n\n----------------------------------------\n\nTITLE: Running CUDASTF Examples\nDESCRIPTION: Shows how to run a compiled CUDASTF example.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n./bin/cudax.cpp17.example.stf.01-axpy\n```\n\n----------------------------------------\n\nTITLE: Defining cuda::pipeline Destructor in CUDA C++\nDESCRIPTION: This code snippet defines the destructor for the cuda::pipeline class. It is templated on the thread_scope and can be called from both host and device code. The destructor calls the quit() method if it hasn't been called by the current thread before destroying the pipeline object.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/synchronization_primitives/pipeline/destructor.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::thread_scope Scope>\n__host__ __device__\ncuda::pipeline<Scope>::~pipeline();\n```\n\n----------------------------------------\n\nTITLE: Fixing cuda::std::complex for NVRTC (C++)\nDESCRIPTION: Issue #101 fixed in libcu++ 1.4.1. Corrected issues with using `cuda::std::complex` specifically within the NVRTC environment.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/releases/changelog.rst#2025-04-23_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\ncuda::std::complex\n```\n\n----------------------------------------\n\nTITLE: Declaring multimem_ld_reduce with Relaxed/Cluster Scope and XOR Operation in CUDA PTX\nDESCRIPTION: Template function declaration for multimem.ld_reduce with relaxed memory semantics, cluster scope, and XOR operation on 64-bit values in global memory. Compatible with PTX ISA 8.1 on SM_90 hardware.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_71\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.b64 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .xor }\ntemplate <typename B64, enable_if_t<sizeof(B64) == 8, bool> = true, cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline B64 multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_xor_op_t,\n  const B64* addr);\n```\n\n----------------------------------------\n\nTITLE: RST Section Reference for SHR PTX Instruction\nDESCRIPTION: RestructuredText markup defining documentation section for the SHR PTX instruction with cross-references and includes.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/shr.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _libcudacxx-ptx-instructions-shr:\n\nshr\n===\n\n-  PTX ISA:\n   `shr <https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#logic-and-shift-instructions-shr>`__\n\n.. include:: generated/shr.rst\n```\n\n----------------------------------------\n\nTITLE: Before Thread Sync Fence Implementation\nDESCRIPTION: Device function template that implements a fence operation before thread synchronization. Targets PTX ISA 86 and is compatible with SM_100a and SM_101a architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_fence.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.fence::before_thread_sync; // PTX ISA 86, SM_100a, SM_101a\ntemplate <typename = void>\n__device__ static inline void tcgen05_fence_before_thread_sync();\n```\n\n----------------------------------------\n\nTITLE: Setting CMake Variables for JIT Template File Paths\nDESCRIPTION: Defines variables for file paths used in JIT template generation, including output header, source, dependency file, and input entry point.\nSOURCE: https://github.com/nvidia/cccl/blob/main/c/parallel/src/jit_templates/CMakeLists.txt#2025-04-23_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(jit_template \"${CMAKE_CURRENT_BINARY_DIR}/jit_template.h\")\nset(jit_template_src \"${CMAKE_CURRENT_BINARY_DIR}/jit_template.cpp\")\nset(jit_template_depfile \"${CMAKE_CURRENT_BINARY_DIR}/jit_template.d\")\nset(cpp_entry \"${CMAKE_CURRENT_LIST_DIR}/jit_entry.h\")\n\nfile(MAKE_DIRECTORY \"${CMAKE_CURRENT_BINARY_DIR}\")\n```\n\n----------------------------------------\n\nTITLE: Compiling CUDASTF Applications with G++\nDESCRIPTION: Shows how to compile CUDASTF applications using G++ without NVCC, useful for applications using existing CUDA libraries.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Compilation flags\ng++ -I$(cudastf_path)\n# Linking flags\ng++ -lcuda -lcudart\n```\n\n----------------------------------------\n\nTITLE: CUB Example Addition Function Definition\nDESCRIPTION: Defines a function that adds an example executable to the build system, configures its properties, and registers it with CTest. Handles target naming, dependencies, and test registration.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cub/examples/CMakeLists.txt#2025-04-23_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nfunction(cub_add_example target_name_var example_name example_src cub_target)\n  cub_get_target_property(config_prefix ${cub_target} PREFIX)\n\n  # The actual name of the test's target:\n  set(example_target ${config_prefix}.example.${example_name})\n  set(${target_name_var} ${example_target} PARENT_SCOPE)\n\n  # Related target names:\n  set(config_meta_target ${config_prefix}.examples)\n  set(example_meta_target cub.all.example.${example_name})\n\n  add_executable(${example_target} \"${example_src}\")\n  target_link_libraries(${example_target} PRIVATE\n    ${cub_target}\n    cccl.c2h\n  )\n  cub_clone_target_properties(${example_target} ${cub_target})\n  cub_configure_cuda_target(${example_target} RDC ${CUB_FORCE_RDC})\n  target_include_directories(${example_target} PRIVATE \"${CUB_SOURCE_DIR}/examples\")\n\n  # Add to the active configuration's meta target\n  add_dependencies(${config_meta_target} ${example_target})\n\n  # Meta target that builds examples with this name for all configurations:\n  if (NOT TARGET ${example_meta_target})\n    add_custom_target(${example_meta_target})\n  endif()\n  add_dependencies(${example_meta_target} ${example_target})\n\n  add_test(NAME ${example_target}\n    COMMAND \"$<TARGET_FILE:${example_target}>\"\n  )\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Fixing Version Parsing in Legacy FindThrust.cmake\nDESCRIPTION: Shows how to patch a version parsing bug in the legacy FindThrust.cmake module by replacing the regex matching code with mathematical expressions.\nSOURCE: https://github.com/nvidia/cccl/blob/main/lib/cmake/thrust/README.md#2025-04-23_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nstring( REGEX MATCH \"^[0-9]\" major ${version} )\nstring( REGEX REPLACE \"^${major}00\" \"\" version \"${version}\" )\nstring( REGEX MATCH \"^[0-9]\" minor ${version} )\nstring( REGEX REPLACE \"^${minor}0\" \"\" version \"${version}\" )\n```\n\nLANGUAGE: cmake\nCODE:\n```\nmath(EXPR major \"${version} / 100000\")\nmath(EXPR minor \"(${version} / 100) % 1000\")\nmath(EXPR version \"${version} % 100\")\n```\n\n----------------------------------------\n\nTITLE: Storing 16x256b Data with tcgen05 in CUDA (4 values)\nDESCRIPTION: This function performs a 16x256b store operation using tcgen05. It takes a 32-bit address and an array of 4 B32 values as input. The function is constrained to B32 types of 4 bytes.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_st.rst#2025-04-23_snippet_5\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.st.sync.aligned.16x256b.x1.b32 [taddr], values; // PTX ISA 86, SM_100a, SM_101a\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline void tcgen05_st_16x256b(\n  uint32_t taddr,\n  const B32 (&values)[4]);\n```\n\n----------------------------------------\n\nTITLE: Creating Hierarchy Tests in CMake\nDESCRIPTION: Creates a test target for hierarchy-related functionality in CCCL. This includes tests for basic functionality (smoke tests) and custom type support, using the previously defined cudax_add_catch2_test function.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cudax/test/CMakeLists.txt#2025-04-23_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\n  cudax_add_catch2_test(test_target hierarchy ${cn_target}\n    hierarchy/hierarchy_smoke.cu\n    hierarchy/hierarchy_custom_types.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: Building Windows Samples with Visual Studio\nDESCRIPTION: Instructions for building Windows samples using Visual Studio solution files. Each sample has individual solution files in its directory.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cudax/test/stf/cuda-samples/0_Introduction/vectorAdd/README.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n*_vs<version>.sln - for Visual Studio <version>\n```\n\n----------------------------------------\n\nTITLE: Implementing Multimem Reduction XOR Operations with Memory Semantics in CUDA PTX\nDESCRIPTION: This template function declaration enables 64-bit XOR reduction operations with configurable memory semantics and scope. It supports relaxed or release semantics and various scope levels (CTA, cluster, GPU, or system). This is part of the PTX ISA 8.1 for SM_90 architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_red.rst#2025-04-23_snippet_40\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.red.sem.scope.global.op.b64 [addr], val; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .release }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .xor }\ntemplate <typename B64, enable_if_t<sizeof(B64) == 8, bool> = true, cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline void multimem_red(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_xor_op_t,\n  B64* addr,\n  B64 val);\n```\n\n----------------------------------------\n\nTITLE: Configuring Table of Contents for Thrust Filling Algorithms Documentation in reST\nDESCRIPTION: A Sphinx toctree directive that collects and organizes all API documentation files related to filling algorithms in the Thrust library. It uses a glob pattern to automatically include relevant files.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/api_docs/algorithms/transformations/filling.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :glob:\n   :maxdepth: 1\n\n   ${repo_docs_api_path}/*function_group__filling*\n```\n\n----------------------------------------\n\nTITLE: CUDA Compiler Warning Message\nDESCRIPTION: Warning message from NVCC compiler regarding dynamic initialization of shared variables in device/global functions.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/synchronization_primitives/barrier/init.rst#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nwarning: dynamic initialization is not supported for a function-scope static\n__shared__ variable within a __device__/__global__ function\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents for Extended API Documentation in RST\nDESCRIPTION: This RST code snippet defines the structure of the extended API documentation for the CUDA C++ library. It uses the toctree directive to create a hierarchical table of contents, listing various components and features of the extended API.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api.rst#2025-04-23_snippet_0\n\nLANGUAGE: RST\nCODE:\n```\n.. _libcudacxx-extended-api:\n\nExtended API\n============\n\n.. toctree::\n   :maxdepth: 2\n\n   extended_api/bit\n   extended_api/execution_model\n   extended_api/memory_model\n   extended_api/thread_groups\n   extended_api/shapes\n   extended_api/synchronization_primitives\n   extended_api/asynchronous_operations\n   extended_api/memory_access_properties\n   extended_api/functional\n   extended_api/streams\n   extended_api/memory_resource\n   extended_api/math\n   extended_api/mdspan\n   extended_api/warp\n   extended_api/work_stealing\n```\n\n----------------------------------------\n\nTITLE: Finding CUDA Toolkit for Version Checks\nDESCRIPTION: Locates the CUDA Toolkit installation to determine available features and version compatibility. This is used to conditionally enable CUDA version-specific tests.\nSOURCE: https://github.com/nvidia/cccl/blob/main/examples/CMakeLists.txt#2025-04-23_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nfind_package(CUDAToolkit REQUIRED)\n```\n\n----------------------------------------\n\nTITLE: Adding CCCL Package with CPM\nDESCRIPTION: Uses CPM (CMake Package Manager) to fetch CCCL from GitHub and make its targets available, enabling unstable features to access the CUDAX extension.\nSOURCE: https://github.com/nvidia/cccl/blob/main/examples/cudax_stf/CMakeLists.txt#2025-04-23_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nCPMAddPackage(\n  NAME CCCL\n  GIT_REPOSITORY \"${CCCL_REPOSITORY}\"\n  GIT_TAG ${CCCL_TAG}\n  # The following is required to make the `CCCL::cudax` target available:\n  OPTIONS \"CCCL_ENABLE_UNSTABLE ON\"\n)\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure for shfl.sync PTX Instruction\nDESCRIPTION: ReStructuredText documentation structure defining reference and include directives for the shfl.sync PTX instruction documentation\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/shfl_sync.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _libcudacxx-ptx-instructions-shfl_sync:\n\nshfl.sync\n=========\n\n-  PTX ISA:\n   `shfl.sync <https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-shfl-sync>`__\n\n.. include:: manual/shfl_sync.rst\n```\n\n----------------------------------------\n\nTITLE: Thread Hierarchy Specification Syntax\nDESCRIPTION: Syntax for defining thread hierarchies in the launch construct. This shows how to specify parallel or concurrent thread groups with optional static width, dynamic width, scope, and memory parameters.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_65\n\nLANGUAGE: cpp\nCODE:\n```\n{par|con}<[static_width]>([width], [scope], [mem(size)] [nested hierarchy specification])\n```\n\n----------------------------------------\n\nTITLE: Referencing cp.async.mbarrier.arrive in reStructuredText\nDESCRIPTION: This snippet creates a reference link to the official PTX ISA documentation for the cp.async.mbarrier.arrive instruction. It uses reStructuredText syntax to format the link.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/cp_async_mbarrier_arrive.rst#2025-04-23_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n`cp.async.mbarrier.arrive <https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-cp-async-mbarrier-arrive>`__\n```\n\n----------------------------------------\n\nTITLE: Configuring C2H Dependency for CUB Tests\nDESCRIPTION: Retrieves the C2H dependency required by CUB test utilities.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cub/examples/CMakeLists.txt#2025-04-23_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncccl_get_c2h()\n```\n\n----------------------------------------\n\nTITLE: Implementing multimem.red.release.gpu.global.max.s64 Operation in CUDA PTX\nDESCRIPTION: Template function declaration for performing a 64-bit signed integer max reduction with release semantics and GPU scope in global memory. This function is available for SM_90 and requires PTX ISA 8.1 or higher.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_red.rst#2025-04-23_snippet_16\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.red.sem.scope.global.op.s64 [addr], val; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .release }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .max }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline void multimem_red(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_max_t,\n  int64_t* addr,\n  int64_t val);\n```\n\n----------------------------------------\n\nTITLE: Documenting mbarrier.test_wait PTX Instruction\nDESCRIPTION: RST markup documentation structure for the CUDA PTX mbarrier.test_wait instruction, including a link to official PTX ISA documentation and placeholders for generated content.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/mbarrier_test_wait.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _libcudacxx-ptx-instructions-mbarrier-test_wait:\n\nmbarrier.test_wait\n==================\n\n-  PTX ISA:\n   `mbarrier.test_wait <https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-mbarrier-test-wait-mbarrier-try-wait>`__\n\n.. _mbarrier.test_wait-1:\n\nmbarrier.test_wait\n------------------\n\n.. include:: generated/mbarrier_test_wait.rst\n\nmbarrier.test_wait.parity\n-------------------------\n\n.. include:: generated/mbarrier_test_wait_parity.rst\n```\n\n----------------------------------------\n\nTITLE: Implementing cp.async.bulk.wait_group in CUDA\nDESCRIPTION: This code snippet defines a template function for the cp.async.bulk.wait_group PTX instruction. It takes a template parameter N32 and uses cuda::ptx::n32_t to represent the wait group count.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_async_bulk_wait_group.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\n// cp.async.bulk.wait_group N; // PTX ISA 80, SM_90\ntemplate <int N32>\n__device__ static inline void cp_async_bulk_wait_group(\n  cuda::ptx::n32_t<N32> N);\n```\n\n----------------------------------------\n\nTITLE: PRMT B32 Rotate by 16 Template Function\nDESCRIPTION: Template function declaration for 32-bit rotate by 16 bits permute operation (rc16 mode). Performs byte-wise rotation by 16 bits on input registers.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/prmt.rst#2025-04-23_snippet_6\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline uint32_t prmt_rc16(\n  B32 a_reg,\n  B32 b_reg,\n  uint32_t c_reg);\n```\n\n----------------------------------------\n\nTITLE: CUB Version Macros Definition\nDESCRIPTION: Version identification macros available in cub/version.cuh for determining CUB library version.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/releases/changelog.rst#2025-04-23_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\nCUB_VERSION\nCUB_VERSION_MAJOR\nCUB_VERSION_MINOR\nCUB_VERSION_SUBMINOR\nCUB_PATCH_NUMBER\n```\n\n----------------------------------------\n\nTITLE: CMake Integration Guidelines\nDESCRIPTION: Instructions for integrating CCCL into projects using CMake. CMake is the recommended and officially tested build system for CCCL integration.\nSOURCE: https://github.com/nvidia/cccl/blob/main/README.md#2025-04-23_snippet_6\n\nLANGUAGE: markdown\nCODE:\n```\n##### CMake Integration\n\nCCCL uses [CMake](https://cmake.org/) for all build and installation infrastructure, including tests as well as targets to link against in other CMake projects.\nTherefore, CMake is the recommended way to integrate CCCL into another project.\n\nFor a complete example of how to do this using CMake Package Manager see [our basic example project](examples/basic).\n\nOther build systems should work, but only CMake is tested.\nContributions to simplify integrating CCCL into other build systems are welcome.\n```\n\n----------------------------------------\n\nTITLE: Enabling Testing and Codegen for libcudacxx\nDESCRIPTION: Enables CMake testing functionality and optionally adds the codegen subdirectory based on the libcudacxx_ENABLE_CODEGEN option.\nSOURCE: https://github.com/nvidia/cccl/blob/main/libcudacxx/CMakeLists.txt#2025-04-23_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\n# Must be called in root CMakeLists.txt\ninclude(CTest)\nenable_testing()\n\n# Add codegen module\noption(libcudacxx_ENABLE_CODEGEN \"Enable libcudacxx's atomics backend codegen and tests.\" OFF)\nif (libcudacxx_ENABLE_CODEGEN)\n  add_subdirectory(codegen)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring LIT Regression Tests Target\nDESCRIPTION: Sets up a comprehensive testing target named 'check-all' using the LLVM LIT testing framework, gathering all test suites, parameters, dependencies, and arguments from global properties.\nSOURCE: https://github.com/nvidia/cccl/blob/main/libcudacxx/CMakeLists.txt#2025-04-23_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\n  include(AddLLVM)\n  get_property(LLVM_LIT_TESTSUITES          GLOBAL PROPERTY LLVM_LIT_TESTSUITES)\n  get_property(LLVM_LIT_PARAMS              GLOBAL PROPERTY LLVM_LIT_PARAMS)\n  get_property(LLVM_LIT_DEPENDS             GLOBAL PROPERTY LLVM_LIT_DEPENDS)\n  get_property(LLVM_LIT_EXTRA_ARGS          GLOBAL PROPERTY LLVM_LIT_EXTRA_ARGS)\n  get_property(LLVM_ADDITIONAL_TEST_TARGETS GLOBAL PROPERTY LLVM_ADDITIONAL_TEST_TARGETS)\n  get_property(LLVM_ADDITIONAL_TEST_DEPENDS GLOBAL PROPERTY LLVM_ADDITIONAL_TEST_DEPENDS)\n  add_lit_target(check-all\n    \"Running all regression tests\"\n    ${LLVM_LIT_TESTSUITES}\n    PARAMS ${LLVM_LIT_PARAMS}\n    DEPENDS ${LLVM_LIT_DEPENDS} ${LLVM_ADDITIONAL_TEST_TARGETS}\n    ARGS ${LLVM_LIT_EXTRA_ARGS}\n  )\nendif ()\n```\n\n----------------------------------------\n\nTITLE: Complex Number Type Definition\nDESCRIPTION: References to thrust::complex type for complex number operations in CUDA code\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/releases/changelog.rst#2025-04-23_snippet_7\n\nLANGUAGE: C++\nCODE:\n```\nthrust::complex<double>\n```\n\n----------------------------------------\n\nTITLE: Escaping CUDA Architectures for CMake Configuration\nDESCRIPTION: Prepares CUDA architecture values by properly escaping semicolons for use in CMake variables. This ensures architecture specifications are correctly passed to downstream build processes.\nSOURCE: https://github.com/nvidia/cccl/blob/main/examples/CMakeLists.txt#2025-04-23_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nstring(REPLACE \";\" \"\\\\;\" arches_escaped \"${CMAKE_CUDA_ARCHITECTURES}\")\n```\n\n----------------------------------------\n\nTITLE: Implementing mbarrier.arrive.expect_tx with Release Semantics for Cluster Scope in Shared Cluster Space\nDESCRIPTION: This CUDA template function implements the mbarrier.arrive.expect_tx operation with release semantics for cluster scope in shared cluster space. It takes a 64-bit address and a transaction count as parameters.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/mbarrier_arrive_expect_tx.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline void mbarrier_arrive_expect_tx(\n  cuda::ptx::sem_release_t,\n  cuda::ptx::scope_cluster_t,\n  cuda::ptx::space_cluster_t,\n  uint64_t* addr,\n  const uint32_t& tx_count);\n```\n\n----------------------------------------\n\nTITLE: Including C Standard Complex Number Header in CUDA C++\nDESCRIPTION: Includes the `<cuda/std/ccomplex>` header, providing C-style complex number arithmetic support. Available since CCCL 2.0.0 and CUDA Toolkit 11.4.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/standard_api/c_library.rst#2025-04-23_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\n#include <cuda/std/ccomplex>\n```\n\n----------------------------------------\n\nTITLE: Linux SM Architecture Build Command\nDESCRIPTION: Make command showing how to specify target SM architectures for the build process.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cudax/test/stf/cuda-samples/3_CUDA_Features/jacobiCudaGraphs/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ make SMS=\"50 60\"\n```\n\n----------------------------------------\n\nTITLE: Renaming __is_convertible due to NVCC Keyword Conflict (C++)\nDESCRIPTION: Issue #118 fixed in libcu++ 1.4.1. Renamed the internal identifier `__is_convertible` because NVCC treats it as a context-sensitive keyword, causing potential compilation conflicts.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/releases/changelog.rst#2025-04-23_snippet_10\n\nLANGUAGE: cpp\nCODE:\n```\n__is_convertible // Renamed due to NVCC keyword conflict\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents for Contributing Documentation in reStructuredText\nDESCRIPTION: This snippet defines a table of contents for the contributing documentation using reStructuredText syntax. It includes a link to the code of conduct document.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/contributing.rst#2025-04-23_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\n   :maxdepth: 1\n\n   contributing/code_of_conduct\n```\n\n----------------------------------------\n\nTITLE: Creating Event Tests with Extended Lambda Support\nDESCRIPTION: Configures tests for CUDA event functionality with the extended lambda compiler option enabled. This test validates the basic functionality of CUDA events in the CCCL library.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cudax/test/CMakeLists.txt#2025-04-23_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\n  cudax_add_catch2_test(test_target event ${cn_target}\n    event/event_smoke.cu\n  )\n  target_compile_options(${test_target} PRIVATE $<$<COMPILE_LANG_AND_ID:CUDA,NVIDIA>:--extended-lambda>)\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication Template - CTA Group 1 with F8F6F4\nDESCRIPTION: CUDA device function template for matrix multiplication using F8F6F4 data type in CTA group 1. Optimized for low precision computation.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma.rst#2025-04-23_snippet_2\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma(\n  cuda::ptx::kind_t<Kind> kind,\n  cuda::ptx::cta_group_1_t,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  const uint32_t (&disable_output_lane)[4],\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Accessing Block Dimensions in CUDA\nDESCRIPTION: These functions retrieve the x, y, and z dimensions of the thread block. They use inline PTX instructions to access the special registers.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/get_sreg.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n// mov.u32 sreg_value, %%ntid.x; // PTX ISA 20\ntemplate <typename = void>\n__device__ static inline uint32_t get_sreg_ntid_x();\n```\n\nLANGUAGE: cuda\nCODE:\n```\n// mov.u32 sreg_value, %%ntid.y; // PTX ISA 20\ntemplate <typename = void>\n__device__ static inline uint32_t get_sreg_ntid_y();\n```\n\nLANGUAGE: cuda\nCODE:\n```\n// mov.u32 sreg_value, %%ntid.z; // PTX ISA 20\ntemplate <typename = void>\n__device__ static inline uint32_t get_sreg_ntid_z();\n```\n\n----------------------------------------\n\nTITLE: Defining Thrust API Documentation Structure in reStructuredText\nDESCRIPTION: A reStructuredText document that defines a labeled section for Thrust API documentation and creates a table of contents with links to various API subsections. The document uses directive syntax to organize documentation hierarchically.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/api.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _thrust-module-api:\n\nThrust API documentation\n=========================\n\n.. toctree::\n   :maxdepth: 1\n\n   api_docs/algorithms\n   api_docs/containers\n   api_docs/function_objects\n   api_docs/iterators\n   api_docs/memory_management\n   api_docs/numerics\n   api_docs/parallel_execution_policies\n   api_docs/random\n   api_docs/system\n   api_docs/utility\n```\n\n----------------------------------------\n\nTITLE: Setting CMake Module Path and LLVM Path\nDESCRIPTION: Configures the CMake module path to find custom modules and sets the LLVM path to the source directory.\nSOURCE: https://github.com/nvidia/cccl/blob/main/libcudacxx/CMakeLists.txt#2025-04-23_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nset(CMAKE_MODULE_PATH \"${libcudacxx_SOURCE_DIR}/cmake\")\nset(LLVM_PATH \"${libcudacxx_SOURCE_DIR}\" CACHE STRING \"\" FORCE)\n```\n\n----------------------------------------\n\nTITLE: Scoped Global 32-bit Store Operations\nDESCRIPTION: Template functions for 32-bit store operations with relaxed and release semantics across different scopes (CTA, cluster, GPU, system).\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_st.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true, cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline void multimem_st(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  B32* addr,\n  B32 val);\n```\n\n----------------------------------------\n\nTITLE: Including Generated Documentation in reStructuredText\nDESCRIPTION: This snippet uses the reStructuredText 'include' directive to insert the content of an external file named 'cp_async_bulk_wait_group.rst'. This likely contains detailed documentation for the cp.async.bulk.wait_group instruction.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/cp_async_bulk_wait_group.rst#2025-04-23_snippet_1\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. include:: generated/cp_async_bulk_wait_group.rst\n```\n\n----------------------------------------\n\nTITLE: Initializing Memory Barrier with Release Semantics and Cluster Scope in CUDA PTX\nDESCRIPTION: This code snippet defines a device function template for initializing a memory barrier with release semantics and cluster scope. It uses CUDA PTX ISA version 80 and is intended for SM_90 architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/fence_mbarrier_init.rst#2025-04-23_snippet_0\n\nLANGUAGE: CUDA\nCODE:\n```\n// fence.mbarrier_init.sem.scope; // 3. PTX ISA 80, SM_90\n// .sem       = { .release }\n// .scope     = { .cluster }\ntemplate <typename = void>\n__device__ static inline void fence_mbarrier_init(\n  cuda::ptx::sem_release_t,\n  cuda::ptx::scope_cluster_t);\n```\n\n----------------------------------------\n\nTITLE: Structuring RST Documentation for Thrust Reordering Algorithms\nDESCRIPTION: This RST (reStructuredText) snippet defines the structure for the reordering algorithms section in the Thrust documentation. It creates a section title, adds a table of contents, and links to three subcategories of reordering algorithms.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/api_docs/algorithms/reordering.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _thrust-module-api-algorithms-reordering:\n\nReordering\n----------\n\n.. toctree::\n   :glob:\n   :maxdepth: 1\n\n   reordering/partitioning\n   reordering/shuffling\n   reordering/stream_compaction\n```\n\n----------------------------------------\n\nTITLE: Configuring Test Target Metatargets in CMake\nDESCRIPTION: Loop through the enabled CUDA configuration targets to create a meta-target for each configuration's tests and add it as a dependency to the configuration's 'all' target. This sets up the organization structure for the test targets that will be defined later.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cudax/test/CMakeLists.txt#2025-04-23_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nforeach(cn_target IN LISTS cudax_TARGETS)\n  cudax_get_target_property(config_prefix ${cn_target} PREFIX)\n\n  # Metatarget for the current configuration's tests:\n  set(config_meta_target ${config_prefix}.tests)\n  add_custom_target(${config_meta_target})\n  add_dependencies(${config_prefix}.all ${config_meta_target})\n```\n\n----------------------------------------\n\nTITLE: Referencing cp.async.bulk.wait_group PTX Instruction in reStructuredText\nDESCRIPTION: This snippet creates a reference link to the cp.async.bulk.wait_group PTX instruction documentation using reStructuredText syntax. It provides a clickable link to the official NVIDIA documentation for this specific instruction.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/cp_async_bulk_wait_group.rst#2025-04-23_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n`cp.async.bulk.wait_group <https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async-bulk-wait-group>`__\n```\n\n----------------------------------------\n\nTITLE: Decrement Operation for Unsigned 32-bit Integer\nDESCRIPTION: Template function implementing asynchronous decrement reduction operation on cluster memory barrier for u32 type. Takes destination pointer, value and remote barrier pointer as parameters.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/red_async.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline void red_async(\n  cuda::ptx::op_dec_t,\n  uint32_t* dest,\n  const uint32_t& value,\n  uint64_t* remote_bar);\n```\n\n----------------------------------------\n\nTITLE: Transform Output Iterator Usage\nDESCRIPTION: Shows the usage of thrust::transform_output_iterator for applying a function to output before storing results\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/releases/changelog.rst#2025-04-23_snippet_6\n\nLANGUAGE: C++\nCODE:\n```\nthrust::transform_output_iterator\n```\n\n----------------------------------------\n\nTITLE: Implementing Acquire Fence for Tensor Map Proxy in CUDA\nDESCRIPTION: This template function implements an acquire fence operation for tensor map proxies. It supports various scopes and takes additional parameters for address and size. The function uses PTX ISA 83 instructions and is available on SM_90 architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/fence_proxy_tensormap_generic.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n// fence.proxy.tensormap::generic.sem.scope [addr], size; // 8. PTX ISA 83, SM_90\n// .sem       = { .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\ntemplate <int N32, cuda::ptx::dot_scope Scope>\n__device__ static inline void fence_proxy_tensormap_generic(\n  cuda::ptx::sem_acquire_t,\n  cuda::ptx::scope_t<Scope> scope,\n  const void* addr,\n  cuda::ptx::n32_t<N32> size);\n```\n\n----------------------------------------\n\nTITLE: Defining PTX L1 First Eviction Store Instructions - CUDA\nDESCRIPTION: Declares device function templates for PTX global memory store instructions with L1::evict_first for 8, 16, 32, 64, and 128-bit types. Each function uses template specialization by sizeof to select the correct overload, and operates on a global space pointer, the value to store, and does not require a cache policy argument. For use in tuning memory access patterns and eviction on CUDA architectures SM_70 and above.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/st.rst#2025-04-23_snippet_5\n\nLANGUAGE: cuda\nCODE:\n```\n// st.space.L1::evict_first.b8 [addr], src; // PTX ISA 74, SM_70\n// .space     = { .global }\ntemplate <typename B8, enable_if_t<sizeof(B8) == 1, bool> = true>\n__device__ static inline void st_L1_evict_first(\n  cuda::ptx::space_global_t,\n  B8* addr,\n  B8 src);\n```\n\nLANGUAGE: cuda\nCODE:\n```\n// st.space.L1::evict_first.b16 [addr], src; // PTX ISA 74, SM_70\n// .space     = { .global }\ntemplate <typename B16, enable_if_t<sizeof(B16) == 2, bool> = true>\n__device__ static inline void st_L1_evict_first(\n  cuda::ptx::space_global_t,\n  B16* addr,\n  B16 src);\n```\n\nLANGUAGE: cuda\nCODE:\n```\n// st.space.L1::evict_first.b32 [addr], src; // PTX ISA 74, SM_70\n// .space     = { .global }\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline void st_L1_evict_first(\n  cuda::ptx::space_global_t,\n  B32* addr,\n  B32 src);\n```\n\nLANGUAGE: cuda\nCODE:\n```\n// st.space.L1::evict_first.b64 [addr], src; // PTX ISA 74, SM_70\n// .space     = { .global }\ntemplate <typename B64, enable_if_t<sizeof(B64) == 8, bool> = true>\n__device__ static inline void st_L1_evict_first(\n  cuda::ptx::space_global_t,\n  B64* addr,\n  B64 src);\n```\n\nLANGUAGE: cuda\nCODE:\n```\n// st.space.L1::evict_first.b128 [addr], src; // PTX ISA 83, SM_70\n// .space     = { .global }\ntemplate <typename B128, enable_if_t<sizeof(B128) == 16, bool> = true>\n__device__ static inline void st_L1_evict_first(\n  cuda::ptx::space_global_t,\n  B128* addr,\n  B128 src);\n```\n\n----------------------------------------\n\nTITLE: Extracting Test Parameters from Source Files\nDESCRIPTION: Function that reads %PARAM% comments from test source files and generates parameter variant labels and definitions. This is used to create multiple test configurations from a single source file based on specified parameters.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cub/test/CMakeLists.txt#2025-04-23_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\n# The function below reads the filepath `src`, extracts the %PARAM% comments,\n# and fills `labels_var` with a list of `label1_value1.label2_value2...`\n# strings, and puts the corresponding `DEFINITION=value1:DEFINITION=value2`\n# entries into `defs_var`.\n#\n# See the README.md file in this directory for background info.\nfunction(cub_get_test_params src labels_var defs_var)\n  file(READ \"${src}\" file_data)\n  set(param_regex \"//[ ]+%PARAM%[ ]+([^ ]+)[ ]+([^ ]+)[ ]+([^\\n]*)\")\n\n  string(REGEX MATCHALL\n    \"${param_regex}\"\n    matches\n    \"${file_data}\"\n  )\n\n  set(variant_labels)\n  set(variant_defs)\n\n  foreach(match IN LISTS matches)\n    string(REGEX MATCH\n      \"${param_regex}\"\n      unused\n      \"${match}\"\n    )\n\n    set(def ${CMAKE_MATCH_1})\n    set(label ${CMAKE_MATCH_2})\n    set(values \"${CMAKE_MATCH_3}\")\n    string(REPLACE \":\" \";\" values \"${values}\")\n\n    # Build lists of test name suffixes (labels) and preprocessor definitions\n    # (defs) containing the cartesian product of all param values:\n    if (NOT variant_labels)\n      foreach(value IN LISTS values)\n        list(APPEND variant_labels ${label}_${value})\n      endforeach()\n    else()\n      set(tmp_labels)\n      foreach(old_label IN LISTS variant_labels)\n        foreach(value IN LISTS values)\n          list(APPEND tmp_labels ${old_label}.${label}_${value})\n        endforeach()\n      endforeach()\n      set(variant_labels \"${tmp_labels}\")\n    endif()\n\n    if (NOT variant_defs)\n      foreach(value IN LISTS values)\n        list(APPEND variant_defs ${def}=${value})\n      endforeach()\n    else()\n      set(tmp_defs)\n      foreach(old_def IN LISTS variant_defs)\n        foreach(value IN LISTS values)\n          list(APPEND tmp_defs ${old_def}:${def}=${value})\n        endforeach()\n      endforeach()\n      set(variant_defs \"${tmp_defs}\")\n    endif()\n  endforeach()\n\n  set(${labels_var} \"${variant_labels}\" PARENT_SCOPE)\n  set(${defs_var} \"${variant_defs}\" PARENT_SCOPE)\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Adding API Version Macros in <cuda/std/version> (C++)\nDESCRIPTION: New feature in libcu++ 1.2.0. The `<cuda/std/version>` header now provides macros to check the libcu++ API version: `_LIBCUDACXX_CUDA_API_VERSION`, `_LIBCUDACXX_CUDA_API_VERSION_MAJOR`, `_LIBCUDACXX_CUDA_API_VERSION_MINOR`, and `_LIBCUDACXX_CUDA_API_VERSION_PATCH`.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/releases/changelog.rst#2025-04-23_snippet_23\n\nLANGUAGE: cpp\nCODE:\n```\n<cuda/std/version>\n```\n\nLANGUAGE: cpp\nCODE:\n```\n_LIBCUDACXX_CUDA_API_VERSION\n```\n\nLANGUAGE: cpp\nCODE:\n```\n_LIBCUDACXX_CUDA_API_VERSION_MAJOR\n```\n\nLANGUAGE: cpp\nCODE:\n```\n_LIBCUDACXX_CUDA_API_VERSION_MINOR\n```\n\nLANGUAGE: cpp\nCODE:\n```\n_LIBCUDACXX_CUDA_API_VERSION_PATCH\n```\n\n----------------------------------------\n\nTITLE: Defining CUDA Source Wrapping Function for Thrust Benchmarks in CMake\nDESCRIPTION: Defines a CMake function `thrust_wrap_bench_in_cpp` used to wrap a CUDA benchmark source file (`.cu`) within a C++ file (`.cpp`). This is necessary for Thrust backends that are not CUDA-based. It uses `configure_file` with a template (`wrap_source_file.cpp.in`) located in the Thrust source directory to generate the wrapper C++ file in the build directory. The path to the generated C++ file is returned via the `cpp_file_var` argument.\nSOURCE: https://github.com/nvidia/cccl/blob/main/thrust/benchmarks/CMakeLists.txt#2025-04-23_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(thrust_wrap_bench_in_cpp cpp_file_var cu_file thrust_target)\n  thrust_get_target_property(prefix ${thrust_target} PREFIX)\n  set(wrapped_source_file \"${cu_file}\")\n  set(cpp_file \"${CMAKE_CURRENT_BINARY_DIR}/${prefix}/${cu_file}.cpp\")\n  configure_file(\"${Thrust_SOURCE_DIR}/cmake/wrap_source_file.cpp.in\" \"${cpp_file}\")\n  set(${cpp_file_var} \"${cpp_file}\" PARENT_SCOPE)\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake Project for CUDA Extensions\nDESCRIPTION: Sets up the CMake project with CUDA and C++ support, defining the minimum required CMake version and project name.\nSOURCE: https://github.com/nvidia/cccl/blob/main/examples/cudax_stf/CMakeLists.txt#2025-04-23_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.15 FATAL_ERROR)\n\nproject(CUDAX_SAMPLES CUDA CXX)\n```\n\n----------------------------------------\n\nTITLE: Accessing Cluster CTA Rank Information in CUDA\nDESCRIPTION: These functions retrieve cluster CTA rank information. They use inline PTX instructions to access the special registers.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/get_sreg.rst#2025-04-23_snippet_9\n\nLANGUAGE: cuda\nCODE:\n```\n// mov.u32 sreg_value, %%cluster_ctarank; // PTX ISA 78, SM_90\ntemplate <typename = void>\n__device__ static inline uint32_t get_sreg_cluster_ctarank();\n```\n\nLANGUAGE: cuda\nCODE:\n```\n// mov.u32 sreg_value, %%cluster_nctarank; // PTX ISA 78, SM_90\ntemplate <typename = void>\n__device__ static inline uint32_t get_sreg_cluster_nctarank();\n```\n\n----------------------------------------\n\nTITLE: Linux Basic Build Commands\nDESCRIPTION: Basic make commands for building the sample on Linux systems. Shows directory navigation and make execution.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cudax/test/stf/cuda-samples/3_CUDA_Features/jacobiCudaGraphs/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ cd <sample_dir>\n$ make\n```\n\n----------------------------------------\n\nTITLE: Defining Unittested Headers for STF in CMake\nDESCRIPTION: Specifies a list of header files that contain unit tests for the STF framework.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cudax/test/stf/CMakeLists.txt#2025-04-23_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(stf_unittested_headers\n  cuda/experimental/__stf/allocators/buddy_allocator.cuh\n  cuda/experimental/__stf/graph/graph_ctx.cuh\n  # ... (additional header files)\n  cuda/experimental/__stf/utility/unstable_unique.cuh\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Green Context Tests in CMake\nDESCRIPTION: Configures tests for the green context functionality in CCCL. This test verifies the basic operation of green contexts, which are likely related to lightweight execution contexts.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cudax/test/CMakeLists.txt#2025-04-23_snippet_12\n\nLANGUAGE: CMake\nCODE:\n```\n  cudax_add_catch2_test(test_target green_context ${cn_target}\n    green_context/green_ctx_smoke.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: Defining PTX L1 First Eviction Store With L2 Cache Hint - CUDA\nDESCRIPTION: Provides device-side inline templates for PTX global store (st) with L1::evict_first and L2::cache_hint variants for types of size 8, 16, 32, 64, and 128 bits. Each version includes a uint64_t cache_policy argument for L2, template SFINAE for type safety, and is designed for explicit control over both L1 and L2 caching for fine-grained, optimized memory stores on recent CUDA GPUs.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/st.rst#2025-04-23_snippet_6\n\nLANGUAGE: cuda\nCODE:\n```\n// st.space.L1::evict_first.L2::cache_hint.b8 [addr], src, cache_policy; // PTX ISA 74, SM_80\n// .space     = { .global }\ntemplate <typename B8, enable_if_t<sizeof(B8) == 1, bool> = true>\n__device__ static inline void st_L1_evict_first_L2_cache_hint(\n  cuda::ptx::space_global_t,\n  B8* addr,\n  B8 src,\n  uint64_t cache_policy);\n```\n\nLANGUAGE: cuda\nCODE:\n```\n// st.space.L1::evict_first.L2::cache_hint.b16 [addr], src, cache_policy; // PTX ISA 74, SM_80\n// .space     = { .global }\ntemplate <typename B16, enable_if_t<sizeof(B16) == 2, bool> = true>\n__device__ static inline void st_L1_evict_first_L2_cache_hint(\n  cuda::ptx::space_global_t,\n  B16* addr,\n  B16 src,\n  uint64_t cache_policy);\n```\n\nLANGUAGE: cuda\nCODE:\n```\n// st.space.L1::evict_first.L2::cache_hint.b32 [addr], src, cache_policy; // PTX ISA 74, SM_80\n// .space     = { .global }\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline void st_L1_evict_first_L2_cache_hint(\n  cuda::ptx::space_global_t,\n  B32* addr,\n  B32 src,\n  uint64_t cache_policy);\n```\n\nLANGUAGE: cuda\nCODE:\n```\n// st.space.L1::evict_first.L2::cache_hint.b64 [addr], src, cache_policy; // PTX ISA 74, SM_80\n// .space     = { .global }\ntemplate <typename B64, enable_if_t<sizeof(B64) == 8, bool> = true>\n__device__ static inline void st_L1_evict_first_L2_cache_hint(\n  cuda::ptx::space_global_t,\n  B64* addr,\n  B64 src,\n  uint64_t cache_policy);\n```\n\nLANGUAGE: cuda\nCODE:\n```\n// st.space.L1::evict_first.L2::cache_hint.b128 [addr], src, cache_policy; // PTX ISA 83, SM_80\n// .space     = { .global }\ntemplate <typename B128, enable_if_t<sizeof(B128) == 16, bool> = true>\n__device__ static inline void st_L1_evict_first_L2_cache_hint(\n  cuda::ptx::space_global_t,\n  B128* addr,\n  B128 src,\n  uint64_t cache_policy);\n```\n\n----------------------------------------\n\nTITLE: Configuring toctree for Scatter algorithm documentation in reStructuredText\nDESCRIPTION: Sets up a table of contents for Scatter algorithm documentation using reStructuredText. The configuration uses glob pattern matching to include all documentation files related to scatter function groups within the repository's API path.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/api_docs/algorithms/copying/scatter.rst#2025-04-23_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\n   :glob:\n   :maxdepth: 1\n\n   ${repo_docs_api_path}/*function_group__scatter*\n```\n\n----------------------------------------\n\nTITLE: Setting CMake Minimum Requirements for CUDAX Project\nDESCRIPTION: Sets the minimum CMake version to 3.15 for standard inclusion and elevates it to 3.21 when CCCL_ENABLE_CUDAX is enabled. Initializes the project with C++ language support.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cudax/CMakeLists.txt#2025-04-23_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\n# 3.15 is the minimum for including the project with add_subdirectory.\n# 3.21 is the minimum for the developer build.\ncmake_minimum_required(VERSION 3.15)\n\n# This must be done before any languages are enabled:\nif (CCCL_ENABLE_CUDAX)\n  cmake_minimum_required(VERSION 3.21)\nendif()\n\nproject(cudax LANGUAGES CXX)\n```\n\n----------------------------------------\n\nTITLE: Auto-Discovery and Registration of Example Sources with Meta-Targets - CMake\nDESCRIPTION: These snippets use file globbing and iteration to automatically register every Thrust example source file as a build and test target for each configuration, invoking the thrust_add_example function with deduced arguments. This pattern ensures all examples are discovered dynamically without manual enumeration and are grouped by meta-targets for orchestrated building and testing. Dependencies are thrust_add_example and valid example sources under the current list directory.\nSOURCE: https://github.com/nvidia/cccl/blob/main/thrust/examples/CMakeLists.txt#2025-04-23_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB example_srcs\n  RELATIVE \"${CMAKE_CURRENT_LIST_DIR}\"\n  CONFIGURE_DEPENDS\n  *.cu *.cpp\n)\n\nforeach(thrust_target IN LISTS THRUST_TARGETS)\n  foreach(example_src IN LISTS example_srcs)\n    get_filename_component(example_name \"${example_src}\" NAME_WLE)\n    thrust_add_example(example_target ${example_name} \"${example_src}\" ${thrust_target})\n  endforeach()\nendforeach()\n\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure for mbarrier.try_wait\nDESCRIPTION: ReStructuredText documentation structure defining sections and references for the mbarrier.try_wait PTX instruction documentation.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/mbarrier_try_wait.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _libcudacxx-ptx-instructions-mbarrier-try_wait:\n\nmbarrier.try_wait\n=================\n\n-  PTX ISA:\n   `mbarrier.try_wait <https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-mbarrier-test-wait-mbarrier-try-wait>`__\n\n\n.. _mbarrier.try_wait-1:\n\nmbarrier.try_wait\n-----------------\n\n.. include:: generated/mbarrier_try_wait.rst\n\nmbarrier.try_wait.parity\n------------------------\n\n.. include:: generated/mbarrier_try_wait_parity.rst\n```\n\n----------------------------------------\n\nTITLE: 64-bit Unsigned Right Shift Operation in CUDA\nDESCRIPTION: Template function for performing unsigned right shift on 64-bit values. Takes a generic B64 type parameter constrained to 8 bytes and shift amount as parameters.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/shr.rst#2025-04-23_snippet_2\n\nLANGUAGE: cuda\nCODE:\n```\n// shr.b64 dest, a_reg, b_reg; // PTX ISA 10, SM_50\ntemplate <typename B64, enable_if_t<sizeof(B64) == 8, bool> = true>\n__device__ static inline B64 shr(\n  B64 a_reg,\n  uint32_t b_reg);\n```\n\n----------------------------------------\n\nTITLE: Thrust C++11 Compatibility Macros\nDESCRIPTION: Compatibility macros that provide C++11 features with fallbacks for older compilers\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/releases/changelog.rst#2025-04-23_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\nTHRUST_NODISCARD\nTHRUST_CONSTEXPR\nTHRUST_OVERRIDE\nTHRUST_DEFAULT\nTHRUST_NOEXCEPT\nTHRUST_FINAL\nTHRUST_INLINE_CONSTANT\n```\n\n----------------------------------------\n\nTITLE: Function for Adding Internal Header Tests with Fallback Support\nDESCRIPTION: Defines a function that processes each internal header, creates test targets for both normal and fallback configurations if applicable. It handles special cases for MSVC and detects fallback definitions in headers.\nSOURCE: https://github.com/nvidia/cccl/blob/main/libcudacxx/test/internal_headers/CMakeLists.txt#2025-04-23_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(libcudacxx_add_internal_header_test header)\n  # ${header} contains the \"/\" from the subfolder, replace by \"_\" for actual names\n  string(REPLACE \"/\" \"_\" header_name \"${header}\")\n\n  # Create the source file for the header target from the template and add the file to the global project\n  set(headertest_src \"headers/${header_name}\")\n  configure_file(\"${CMAKE_CURRENT_SOURCE_DIR}/header_test.cpp.in\" \"${headertest_src}.cu\")\n\n  # Create the default target for that file\n  libcudacxx_create_internal_header_test(${header_name}, ${headertest_src}, \"\")\n\n  # MSVC cannot handle some of the fallbacks\n  if (\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"MSVC\")\n    if(\"${header}\" MATCHES \"is_base_of\" OR\n       \"${header}\" MATCHES \"is_nothrow_destructible\" OR\n       \"${header}\" MATCHES \"is_polymorphic\")\n      return()\n    endif()\n  endif()\n\n  # Search the file for a fallback definition\n  file(READ ${libcudacxx_SOURCE_DIR}/include/cuda/${header} header_file)\n  string(REGEX MATCH \"_LIBCUDACXX_[A-Z_]*_FALLBACK\" fallback \"${header_file}\")\n  if(fallback)\n    libcudacxx_create_internal_header_test(${header_name}, ${headertest_src}, ${fallback})\n  endif()\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Declaring multimem_ld_reduce with Acquire/System Scope and OR Operation in CUDA PTX\nDESCRIPTION: Template function declaration for multimem.ld_reduce with acquire memory semantics, system scope, and OR operation on 64-bit values in global memory. For PTX ISA 8.1 on SM_90 architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_68\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.b64 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .or }\ntemplate <typename B64, enable_if_t<sizeof(B64) == 8, bool> = true, cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline B64 multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_or_op_t,\n  const B64* addr);\n```\n\n----------------------------------------\n\nTITLE: Including the libcudacxx inplace_vector Header (C++)\nDESCRIPTION: Specifies the header file `<cuda/std/inplace_vector>` required to use the `inplace_vector` container from the libcudacxx library. Most features are available from C++14, but the range-based interface requires C++17 and ranges support.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/standard_api/container_library/inplace_vector.rst#2025-04-23_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n<cuda/std/inplace_vector>\n```\n\n----------------------------------------\n\nTITLE: Adding SSH Signing Key to GitHub\nDESCRIPTION: Command to upload an SSH public key to GitHub for commit signing using the GitHub CLI tool.\nSOURCE: https://github.com/nvidia/cccl/blob/main/ci-overview.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngh ssh-key add ~/.ssh/YOUR_PUBLIC_KEY_FILE_HERE.pub --type signing\n```\n\n----------------------------------------\n\nTITLE: Setting Up FileCheck Verification with CMake - CMake\nDESCRIPTION: This snippet conditionally locates and configures the LLVM FileCheck utility if enabled, ensuring the specified version is available, then runs a smoke test to verify the detected executable before enabling FileCheck-based example output checking. Dependencies include the LLVM FileCheck utility and accessible FileCheck data paths. Variables configured here are used for later test registrations, and errors out with explicit messages if requirements are unmet. The setup expects the environment to be able to locate or be manually provided the FileCheck binary.\nSOURCE: https://github.com/nvidia/cccl/blob/main/thrust/examples/CMakeLists.txt#2025-04-23_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\n# Setup FileCheck if requested and available:\noption(THRUST_ENABLE_EXAMPLE_FILECHECK\n  \"Check example output with the LLVM FileCheck utility.\"\n  OFF\n)\nset(filecheck_data_path \"${Thrust_SOURCE_DIR}/internal/test\")\n\nif (THRUST_ENABLE_EXAMPLE_FILECHECK)\n  # TODO this should go into a find module\n  find_program(THRUST_FILECHECK_EXECUTABLE\n    DOC \"Path to the LLVM FileCheck utility.\"\n    NAMES\n      FileCheck\n      FileCheck-3.9\n      FileCheck-4.0\n      FileCheck-5.0\n      FileCheck-6.0\n      FileCheck-7\n      FileCheck-8\n      FileCheck-9\n  )\n\n  if (NOT THRUST_FILECHECK_EXECUTABLE)\n    message(FATAL_ERROR\n      \"Could not find the LLVM FileCheck utility. Set THRUST_FILECHECK_EXECUTABLE manually, \"\n      \"or disable THRUST_ENABLE_EXAMPLE_FILECHECK.\"\n    )\n  endif()\n\n  execute_process(\n    COMMAND \"${THRUST_FILECHECK_EXECUTABLE}\" \"${filecheck_data_path}/thrust.smoke.filecheck\"\n    INPUT_FILE \"${Thrust_SOURCE_DIR}/cmake/filecheck_smoke_test\"\n    RESULT_VARIABLE exit_code\n  )\n\n  if (0 EQUAL exit_code)\n    message(STATUS \"FileCheck enabled: ${THRUST_FILECHECK_EXECUTABLE}\")\n  else()\n    message(FATAL_ERROR\n      \"The current THRUST_FILECHECK_EXECUTABLE ('${THRUST_FILECHECK_EXECUTABLE}') \"\n      \"does not seem to be a valid FileCheck executable.\"\n    )\n  endif()\nendif()\n\n```\n\n----------------------------------------\n\nTITLE: Defining cuda::pipeline_consumer_wait_prior Function in CUDA\nDESCRIPTION: Function signature for cuda::pipeline_consumer_wait_prior, which waits for pipeline operations to complete up to a specified prior stage.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/synchronization_primitives/pipeline/consumer_wait_prior.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::std::uint8_t Prior>\n__host__ __device__\nvoid cuda::pipeline_consumer_wait_prior(cuda::pipeline<thread_scope_thread>& pipe);\n```\n\n----------------------------------------\n\nTITLE: Weak Global 64-bit Store Operation\nDESCRIPTION: Template function for 64-bit weak store operations in global memory. Uses weak memory semantics without specific scope requirements.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_st.rst#2025-04-23_snippet_2\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename B64, enable_if_t<sizeof(B64) == 8, bool> = true>\n__device__ static inline void multimem_st(\n  cuda::ptx::sem_weak_t,\n  B64* addr,\n  B64 val);\n```\n\n----------------------------------------\n\nTITLE: Configuring STF Test Sources in CMake\nDESCRIPTION: Defines lists of source files for various STF test categories, including standard tests, codegen tests, and math library tests.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cudax/test/stf/CMakeLists.txt#2025-04-23_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(stf_test_sources\n  allocators/buddy_allocator.cu\n  cpp/concurrency_test.cu\n  cpp/redundant_data.cu\n  # ... (additional source files)\n  utility/source_location_map.cu\n)\n\nset(stf_test_codegen_sources\n  allocators/adapter.cu\n  allocators/cap_tmp_buffers.cu\n  # ... (additional codegen source files)\n  tools/auto_dump/auto_dump.cu\n)\n\nset(stf_test_mathlib_sources\n  cuda-samples/0_Introduction/vectorAdd/vectorAdd_cudastf.cu\n  # ... (additional math library source files)\n  gnu/07-cholesky.cpp\n)\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Reference for multimem.red\nDESCRIPTION: ReStructuredText markup defining documentation references and includes for the multimem.red PTX instruction documentation.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/multimem_red.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _libcudacxx-ptx-instructions-multimem-red:\n\nmultimem.red\n============\n\n-  PTX ISA:\n   `multimem.red <https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-multimem-ld-reduce-multimem-st-multimem-red>`__\n\n.. include:: generated/multimem_red.rst\n```\n\n----------------------------------------\n\nTITLE: Store 16-bit Data with L1 No Allocate in CUDA\nDESCRIPTION: This function stores 16-bit data to global memory with L1 cache no allocate policy. It requires SM_70 or later architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/st.rst#2025-04-23_snippet_11\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename B16, enable_if_t<sizeof(B16) == 2, bool> = true>\n__device__ static inline void st_L1_no_allocate(\n  cuda::ptx::space_global_t,\n  B16* addr,\n  B16 src);\n```\n\n----------------------------------------\n\nTITLE: Table of Contents Configuration for Complex Number Functions\nDESCRIPTION: A reStructuredText toctree directive that includes all documentation files matching the complex function group pattern. It sets the maximum depth to 2 and uses glob pattern matching.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/api_docs/numerics.rst#2025-04-23_snippet_1\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\n   :glob:\n   :maxdepth: 2\n\n   ${repo_docs_api_path}/*function_group__complex*\n```\n\n----------------------------------------\n\nTITLE: Enabling CUDA Language Support for Developer Build\nDESCRIPTION: Enables CUDA language support only when required for the developer build.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cudax/CMakeLists.txt#2025-04-23_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\n# Enable CUDA only when required for the developer build, see #2609\nenable_language(CUDA)\n```\n\n----------------------------------------\n\nTITLE: Setting Up CUB Project with Language Requirements\nDESCRIPTION: Initializes the CUB project without specifying languages initially, then enables C++ language support which is required for the GNUInstallDirs CMake module.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cub/CMakeLists.txt#2025-04-23_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nproject(CUB LANGUAGES NONE)\n\n# This must appear before the installation rules, as it is required by the\n# GNUInstallDirs CMake module.\nenable_language(CXX)\n```\n\n----------------------------------------\n\nTITLE: Specifying PTX Memory Barrier Templates Explicitly in CUDA C++\nDESCRIPTION: Demonstrates invoking a cuda::ptx memory barrier function by explicitly specifying the template parameter. While this is valid, it is discouraged since template order or arity could change in minor releases, risking forward compatibility. Users should prefer overload resolution via arguments to maintain compatibility with future versions of the cuda/ptx API.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\ncuda::ptx::mbarrier_arrive_expect_tx<cuda::ptx::sem_release_t>(\n  cuda::ptx::sem_release, cuda::ptx::scope_cta, cuda::ptx::space_shared, &bar, 1\n);\n\n```\n\n----------------------------------------\n\nTITLE: Implementing multimem_ld_reduce with weak semantics for XOR operation in CUDA\nDESCRIPTION: Template function declaration for multimem load-reduce XOR operation on 32-bit values with weak memory semantics. This variant doesn't require a specific memory scope parameter.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_55\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.global.op.b32 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .weak }\n// .op        = { .xor }\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline B32 multimem_ld_reduce(\n  cuda::ptx::sem_weak_t,\n  cuda::ptx::op_xor_op_t,\n  const B32* addr);\n```\n\n----------------------------------------\n\nTITLE: Applying Header Tests to Each Internal Header\nDESCRIPTION: Iterates through all identified internal headers and creates test targets for each one using the previously defined functions.\nSOURCE: https://github.com/nvidia/cccl/blob/main/libcudacxx/test/internal_headers/CMakeLists.txt#2025-04-23_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nforeach(header IN LISTS internal_headers)\n  libcudacxx_add_internal_header_test(${header})\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Basic PRMT B32 Template Function\nDESCRIPTION: Template function declaration for basic 32-bit permute operation. Takes two source registers and a control register as inputs.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/prmt.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline uint32_t prmt(\n  B32 a_reg,\n  B32 b_reg,\n  uint32_t c_reg);\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories for Tests, Examples, and Benchmarks\nDESCRIPTION: Conditionally adds subdirectories for tests, examples, and benchmarks based on the enabled build options, only including components that are explicitly enabled.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cub/CMakeLists.txt#2025-04-23_snippet_9\n\nLANGUAGE: CMake\nCODE:\n```\nif (CUB_ENABLE_TESTING)\n  add_subdirectory(test)\nendif()\n\nif (CUB_ENABLE_EXAMPLES)\n  add_subdirectory(examples)\nendif()\n\nif (CCCL_ENABLE_BENCHMARKS OR CUB_ENABLE_TUNING)\n  add_subdirectory(benchmarks)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring CCCL Repository Settings\nDESCRIPTION: Defines variables for the CCCL GitHub repository URL and branch/tag to use, allowing for overrides in CI environments.\nSOURCE: https://github.com/nvidia/cccl/blob/main/examples/cudax_stf/CMakeLists.txt#2025-04-23_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(CCCL_REPOSITORY \"https://github.com/NVIDIA/cccl\" CACHE STRING \"Git repository to fetch CCCL from\")\nset(CCCL_TAG \"main\" CACHE STRING \"Git tag/branch to fetch from CCCL repository\")\n```\n\n----------------------------------------\n\nTITLE: Committing Changes in Git\nDESCRIPTION: Command to commit changes with a descriptive message.\nSOURCE: https://github.com/nvidia/cccl/blob/main/CONTRIBUTING.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit commit -m \"Brief description of the change\"\n```\n\n----------------------------------------\n\nTITLE: CTA Group Allocation Permit Relinquishment (Group 1)\nDESCRIPTION: Template function for relinquishing allocation permits in CTA group 1. Takes only the CTA group parameter. Compatible with PTX ISA 86 and SM_100a/101a architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_alloc.rst#2025-04-23_snippet_4\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.relinquish_alloc_permit.cta_group.sync.aligned; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1, .cta_group::2 }\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_relinquish_alloc_permit(\n  cuda::ptx::cta_group_t<Cta_Group> cta_group);\n```\n\n----------------------------------------\n\nTITLE: Creating Feature Branch in Git\nDESCRIPTION: Command to create and checkout a new feature branch for development.\nSOURCE: https://github.com/nvidia/cccl/blob/main/CONTRIBUTING.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit checkout -b your-feature-branch\n```\n\n----------------------------------------\n\nTITLE: Configuring toctree for Binary Search Documentation in RST\nDESCRIPTION: A restructured text (RST) directive that sets up a table of contents tree for binary search algorithm documentation. The directive is configured to include all files matching the pattern '*function_group__binary__search*' with a maximum depth of 1 level.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/api_docs/algorithms/searching/binary_search.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :glob:\n   :maxdepth: 1\n\n   ${repo_docs_api_path}/*function_group__binary__search*\n```\n\n----------------------------------------\n\nTITLE: Variable Declarations for CUDA Memory Example\nDESCRIPTION: Basic variable declarations for input/output vectors and coefficient arrays used in the example.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/memory_access_properties/apply_access_property.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\nsize_t N;\nint* x, *y, *z;\nint* a, *b;\n```\n\n----------------------------------------\n\nTITLE: Configuring Example Builds for Each Target\nDESCRIPTION: Main loop that processes each CUDA target and builds all relevant examples based on enabled features and configurations.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cudax/examples/stf/CMakeLists.txt#2025-04-23_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nforeach(cn_target IN LISTS cudax_TARGETS)\n  cudax_get_target_property(config_prefix ${cn_target} PREFIX)\n\n  set(config_meta_target ${config_prefix}.examples)\n  set(stf_meta_target ${config_prefix}.examples.stf)\n  add_custom_target(${stf_meta_target})\n  add_dependencies(${config_meta_target} ${stf_meta_target})\n\n  foreach(source IN LISTS stf_example_sources)\n    cudax_add_stf_example(example_target \"${source}\" ${cn_target})\n  endforeach()\n\n  if (cudax_ENABLE_CUDASTF_CODE_GENERATION)\n     foreach(source IN LISTS stf_example_codegen_sources)\n         cudax_add_stf_example(example_target \"${source}\" ${cn_target})\n     endforeach()\n  endif()\n\n  if (cudax_ENABLE_CUDASTF_MATHLIBS)\n    foreach(source IN LISTS stf_example_mathlib_sources)\n      cudax_add_stf_example(example_target \"${source}\" ${cn_target} LINK_MATHLIBS)\n    endforeach()\n  endif()\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Creating Public Header Test Function in CMake\nDESCRIPTION: Defines a function to create test targets for each public header. It handles special cases for headers requiring SM70+ architecture and sets up compilation flags and include paths.\nSOURCE: https://github.com/nvidia/cccl/blob/main/libcudacxx/test/public_headers/CMakeLists.txt#2025-04-23_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(libcudacxx_add_public_header_test header)\n  # ${header} contains the \"/\" from the subfolder, replace by \"_\" for actual names\n  string(REPLACE \"/\" \"_\" header_name \"${header}\")\n\n  # Create the source file for the header target from the template and add the file to the global project\n  set(headertest_src \"headers/${header_name}\")\n  configure_file(\"${CMAKE_CURRENT_SOURCE_DIR}/header_test.cpp.in\" \"${headertest_src}.cu\")\n\n  # Create the default target for that file\n  set(headertest_${header_name} verify_${header_name})\n  add_library(headertest_${header_name} SHARED \"${headertest_src}.cu\")\n  target_include_directories(headertest_${header_name} PRIVATE \"${libcudacxx_SOURCE_DIR}/include\")\n  target_compile_options(headertest_${header_name}\n                         PRIVATE\n                         ${headertest_warning_levels_device}\n                         -DLIBCUDACXX_ENABLE_EXPERIMENTAL_MEMORY_RESOURCE)\n  target_compile_definitions(headertest_${header_name} PRIVATE CCCL_ENABLE_ASSERTIONS)\n  target_compile_definitions(headertest_${header_name} PRIVATE CCCL_IGNORE_DEPRECATED_CPP_DIALECT)\n  target_compile_definitions(headertest_${header_name} PRIVATE CCCL_ENABLE_OPTIONAL_REF)\n\n  # Ensure that if this is an atomic header, we only include the right architectures\n  string(REGEX MATCH \"atomic|barrier|latch|semaphore|annotated_ptr|pipeline\" match \"${header}\")\n  if(match)\n    # Ensure that we only compile the header when we have some architectures enabled\n    if (NOT architectures_at_least_sm70)\n      return()\n    endif()\n    set_target_properties(headertest_${header_name} PROPERTIES CUDA_ARCHITECTURES \"${architectures_at_least_sm70}\")\n  endif()\n\n  add_dependencies(libcudacxx.test.public_headers headertest_${header_name})\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Implementing MMA Workspace Last Use Operations with F16 Data Type (No Masking) in CUDA\nDESCRIPTION: Template function for the final tensor compute operation (lastuse) with F16 data type. This simplified variant handles tensor descriptors without zero column masking. Targets PTX ISA 86 on SM_100a and SM_101a architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_47\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.ws.cta_group.kind.collector::b2::lastuse [d_tmem], a_desc, b_desc, idesc, enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1 }\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_collector_b2_lastuse(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Running Specific libcu++ Tests or Folders with lit (Bash)\nDESCRIPTION: Demonstrates using `lit` to run tests. The first command runs all tests within the `views.span` directory for the `libcudacxx-cpp17` preset. The second command runs a single specific test file (`array.pass.cpp`) within that directory.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/setup/building_and_testing.rst#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd build\n\n# Builds all tests within libcudacxx/test/libcudacxx/std/containers/views/views.span\nlit libcudacxx-cpp17/libcudacxx/test/libcudacxx/std/containers/views/views.span -sv\n\n# Builds the individual test array.pass.cpp\nlit libcudacxx-cpp17/libcudacxx/test/libcudacxx/std/containers/views/views.span/span.cons/array.pass.cpp -sv\n```\n\n----------------------------------------\n\nTITLE: Configuring Warning Levels for NVIDIA CUDA Compiler\nDESCRIPTION: Sets appropriate warning levels for the NVIDIA CUDA compiler with special handling for MSVC integration and version-specific flags. Includes compatibility check for CUDA 11.6+ support for the --use-local-env flag.\nSOURCE: https://github.com/nvidia/cccl/blob/main/libcudacxx/test/CMakeLists.txt#2025-04-23_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif(\"${CMAKE_CUDA_COMPILER_ID}\" STREQUAL \"NVIDIA\")\n  # CUDA 11.5 and down do not support '-use-local-env'\n  if(MSVC)\n    set(headertest_warning_levels_device -Xcompiler=/W4 -Xcompiler=/WX -Wno-deprecated-gpu-targets)\n    if (\"${CMAKE_CUDA_COMPILER_VERSION}\" GREATER_EQUAL \"11.6.0\")\n      list(APPEND headertest_warning_levels_device --use-local-env)\n    endif()\n  else()\n    set(headertest_warning_levels_device -Wall -Werror all-warnings -Wno-deprecated-gpu-targets)\n  endif()\n```\n\n----------------------------------------\n\nTITLE: Declaring tcgen05_mma_tmem_a with Disable Output Lane for CTA Group 1\nDESCRIPTION: Template function declaration for tcgen05 matrix multiply-accumulate operation with CTA group 1 configuration, supporting various data types (f16, tf32, f8f6f4, i8). This variant includes disable_output_lane parameter as an array of 4 uint32_t values.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma.rst#2025-04-23_snippet_9\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.cta_group.kind [d_tmem], [a_tmem], b_desc, idesc, disable_output_lane, enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\n// .cta_group = { .cta_group::1 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_tmem_a(\n  cuda::ptx::kind_t<Kind> kind,\n  cuda::ptx::cta_group_1_t,\n  uint32_t d_tmem,\n  uint32_t a_tmem,\n  uint64_t b_desc,\n  uint32_t idesc,\n  const uint32_t (&disable_output_lane)[4],\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Subsumption Restriction Workarounds in CUDA Concepts - C++\nDESCRIPTION: Shows how to handle the lack of concept subsumption support before C++20 in template overload resolution, substituting concept requirements via enable_if_t and logical operators. Depends on the CUDA C++ Standard Concepts Library and a user-defined subsuming_concept. The code illustrates ambiguous overloads in C++17 and explains how proper subsumption behavior would resolve the ambiguity, outlining limitations with nvcc compiler versions.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/standard_api/concepts_library.rst#2025-04-23_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\ntemplate<class Integer, cuda::std::enable_if_t<subsuming_concept<Integer> && true, int> = 0>\nvoid would_be_preferred_overload_in_cpp20(Integer&& i) {...}\n\ntemplate<class Integer, cuda::std::enable_if_t<cuda::std::integral<Integer>, int> = 0>\nvoid is_always_ambiguous_in_cpp17(Integer&& i) {...}\n```\n\n----------------------------------------\n\nTITLE: Including Generated Documentation in reStructuredText\nDESCRIPTION: These snippets include generated documentation files for cp.async.mbarrier.arrive and cp.async.mbarrier.arrive_noinc instructions using the reStructuredText include directive.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/cp_async_mbarrier_arrive.rst#2025-04-23_snippet_1\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. include:: generated/cp_async_mbarrier_arrive.rst\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. include:: generated/cp_async_mbarrier_arrive_noinc.rst\n```\n\n----------------------------------------\n\nTITLE: Defining Function to Create Internal Header Test\nDESCRIPTION: Creates a function that generates a test target for a specific internal header. The function creates a shared library that includes just the header, with appropriate compile options and link dependencies.\nSOURCE: https://github.com/nvidia/cccl/blob/main/libcudacxx/test/internal_headers/CMakeLists.txt#2025-04-23_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(libcudacxx_create_internal_header_test header_name, headertest_src, fallback)\n  if(fallback)\n    set(header_name \"${header_name}_fallback\")\n  endif()\n\n  if(header_name MATCHES \"__memory_resource.*\")\n    return()\n  endif()\n\n  # cuda internal headers should not be tested against a host compiler\n  add_library(headertest_${header_name} SHARED \"${headertest_src}.cu\")\n\n  target_include_directories(headertest_${header_name} PRIVATE \"${libcudacxx_SOURCE_DIR}/include\")\n  target_compile_options(headertest_${header_name}\n    PRIVATE\n    $<$<COMPILE_LANGUAGE:CUDA>:${headertest_warning_levels_device}>\n    $<$<COMPILE_LANGUAGE:CXX>:${headertest_warning_levels_host}>\n    -DLIBCUDACXX_ENABLE_EXPERIMENTAL_MEMORY_RESOURCE)\n  target_compile_definitions(headertest_${header_name} PRIVATE CCCL_ENABLE_ASSERTIONS)\n  target_compile_definitions(headertest_${header_name} PRIVATE CCCL_ENABLE_OPTIONAL_REF)\n  if (\"NVHPC\" STREQUAL \"${CMAKE_CXX_COMPILER_ID}\")\n    target_link_libraries(headertest_${header_name} NVHPC::CUDART)\n  else()\n    target_link_libraries(headertest_${header_name} CUDA::cudart)\n  endif()\n\n  if(fallback)\n    target_compile_definitions(headertest_${header_name} PRIVATE \"-D${fallback}\")\n  endif()\n  target_compile_definitions(headertest_${header_name} PRIVATE \"-DLIBCUDACXX_ENABLE_EXPERIMENTAL_MEMORY_RESOURCE\")\n\n  add_dependencies(libcudacxx.test.internal_headers headertest_${header_name})\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Conditionally Including CUDASTF Tests in CMake\nDESCRIPTION: Conditionally adds the STF (Standard Task Framework) tests subdirectory if CUDASTF is enabled and the compiler is not MSVC. This ensures that platform-specific tests are only built on supported configurations.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cudax/test/CMakeLists.txt#2025-04-23_snippet_14\n\nLANGUAGE: CMake\nCODE:\n```\n# FIXME: Enable MSVC\nif (cudax_ENABLE_CUDASTF AND\n    NOT \"MSVC\" STREQUAL \"${CMAKE_CXX_COMPILER_ID}\")\n  # STF tests are handled separately:\n  add_subdirectory(stf)\nendif()\n```\n\n----------------------------------------\n\nTITLE: CTA Group 2 Shift Down Operation\nDESCRIPTION: Template function implementing downward shift operation for CTA group type 2. Takes a CTA group parameter and target address, targeting PTX ISA 86 and SM_100a/SM_101a architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_shift.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.shift.cta_group.down [taddr]; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1, .cta_group::2 }\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_shift_down(\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t taddr);\n```\n\n----------------------------------------\n\nTITLE: Implementing cp.async.mbarrier.arrive.noinc.b64 in CUDA\nDESCRIPTION: This snippet defines a device function template for the cp.async.mbarrier.arrive.noinc.b64 instruction. It takes a 64-bit address as a parameter and is available for PTX ISA 70 and SM_80 architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_async_mbarrier_arrive_noinc.rst#2025-04-23_snippet_0\n\nLANGUAGE: CUDA\nCODE:\n```\n// cp.async.mbarrier.arrive.noinc.b64 [addr]; // PTX ISA 70, SM_80\ntemplate <typename = void>\n__device__ static inline void cp_async_mbarrier_arrive_noinc(\n  uint64_t* addr);\n```\n\n----------------------------------------\n\nTITLE: Basic Shared Memory Barrier Test Wait\nDESCRIPTION: Basic shared memory barrier test wait operation for PTX ISA 70 and SM_80. Takes a memory address and state parameter.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/mbarrier_test_wait.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline bool mbarrier_test_wait(\n  uint64_t* addr,\n  const uint64_t& state);\n```\n\n----------------------------------------\n\nTITLE: Applying Static Assertion Tests to CUDA Targets in CMake\nDESCRIPTION: This snippet iterates over a list of CUDA targets and applies the static assertion test to each source file for each target. It uses the previously defined function to set up the tests.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cudax/test/stf/static_error_checks/CMakeLists.txt#2025-04-23_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nforeach(cn_target IN LISTS cudax_TARGETS)\n  foreach(source IN LISTS static_assert_tests)\n    cudax_stf_add_static_assert_test(test_target \"${source}\" ${cn_target})\n  endforeach()\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Setting CMake Minimum Version Requirements for libcudacxx\nDESCRIPTION: Establishes the minimum CMake version required for the project, with a base requirement of 3.15 and higher requirements (3.21) when CCCL_ENABLE_LIBCUDACXX is enabled.\nSOURCE: https://github.com/nvidia/cccl/blob/main/libcudacxx/CMakeLists.txt#2025-04-23_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\n# 3.15 is the minimum for including the project with add_subdirectory.\n# 3.18 for C++17 + CUDA and clang-cuda support.\n# 3.21 is the minimum for the developer build.\ncmake_minimum_required(VERSION 3.15)\n\nif (CCCL_ENABLE_LIBCUDACXX)\n  cmake_minimum_required(VERSION 3.21)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Declaring MMA Block Scale Vector 1X Collector for MXF8F6F4 Kind\nDESCRIPTION: Defines a device function template for matrix multiply operation with 1X vector scaling for MXF8F6F4 data type. The function uses CTA groups and handles matrix descriptors with scaling options.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma.rst#2025-04-23_snippet_34\n\nLANGUAGE: cuda\nCODE:\n```\n// .cta_group = { .cta_group::1, .cta_group::2 }\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_mma_block_scale_vec_1x_collector_a_use(\n  cuda::ptx::kind_mxf8f6f4_t,\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  uint32_t scale_A_tmem,\n  uint32_t scale_B_tmem,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Referencing cp.async.bulk.commit_group PTX Instruction in reStructuredText\nDESCRIPTION: This snippet creates a reStructuredText reference for the cp.async.bulk.commit_group PTX instruction, including a link to the official NVIDIA documentation and an include directive for a generated file with additional details.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/cp_async_bulk_commit_group.rst#2025-04-23_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. _libcudacxx-ptx-instructions-cp-async-bulk-commit_group:\n\ncp.async.bulk.commit_group\n==========================\n\n-  PTX ISA:\n   `cp.async.bulk.commit_group <https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async-bulk-commit-group>`__\n\n.. include:: generated/cp_async_bulk_commit_group.rst\n```\n\n----------------------------------------\n\nTITLE: Task Graph Visualization Commands\nDESCRIPTION: Shows commands for generating task graph visualizations using Graphviz.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_50\n\nLANGUAGE: bash\nCODE:\n```\n# Run the application with CUDASTF_DOT_FILE set to the filename\nCUDASTF_DOT_FILE=axpy.dot build/examples/01-axpy\n\n# Generate the visualization from this dot file\n## PDF format\ndot -Tpdf axpy.dot -o axpy.pdf\n## PNG format\ndot -Tpng axpy.dot -o axpy.png\n```\n\n----------------------------------------\n\nTITLE: Filtering Headers for NVCC and Clang Compatibility in CMake\nDESCRIPTION: Filters out incompatible headers (annotated_ptr) for NVCC 11.x and Clang CUDA compilers due to feature support limitations.\nSOURCE: https://github.com/nvidia/cccl/blob/main/libcudacxx/test/public_headers/CMakeLists.txt#2025-04-23_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nif (\"${CMAKE_CUDA_COMPILER_VERSION}\" MATCHES \"11\\.*\\.*\" OR\n    \"${CMAKE_CUDA_COMPILER_ID}\" MATCHES \"Clang\")\n  list(FILTER public_headers EXCLUDE REGEX \"annotated_ptr\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Retrieving First CTA ID X-coordinate in CUDA\nDESCRIPTION: Defines a device function to get the X-coordinate of the first CTA ID from a cancellation response. It takes a 128-bit response and returns a 32-bit value.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/clusterlaunchcontrol.rst#2025-04-23_snippet_3\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true, typename B128, enable_if_t<sizeof(B128) == 16, bool> = true>\n__device__ static inline B32 clusterlaunchcontrol_query_cancel_get_first_ctaid_x(\n  B128 try_cancel_response);\n```\n\n----------------------------------------\n\nTITLE: Thread Hierarchy Methods\nDESCRIPTION: Methods available on thread hierarchy objects that allow threads to interact or query parallelism structure. These methods enable operations like getting thread ranks, sizes, synchronization barriers, and accessing shared memory.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_66\n\nLANGUAGE: cpp\nCODE:\n```\nth.rank(); // get thread rank within the entire hierarchy\nth.rank(i); // get thread rank at i-th level\nth.template rank<i>(); // (constexpr) get thread rank at the i-th level at compile time\n\nth.size(); // get the total number of threads\nth.size(i); // get the number of threads at the i-th level\nth.template size<i>(); // (constexpr) get the number of threads at the i-th level at compile time\n\nth.get_scope(i); // get the affinity of the i-th level\nth.template storage<T>(i); // get the local storage associated to the i-th level as a slice<T>\n\nth.sync(); // issue a barrier among all threads\nth.sync(i); // issue a barrier among threads of the i-th level\nth.is_synchronizable(i); // check if we can call sync(i)\nth.template is_synchronizable<i>(); // (constexpr) check if we can call sync(i) at compile time\n\nth.depth(); // (constexpr) get the depth of the thread hierarchy\nth.inner(); // get the thread hierarchy subset obtained by removing the top-most level of the hierarchy\n```\n\n----------------------------------------\n\nTITLE: Defining TCGEN05 MMA Workspace Collector B2 Fill Function Template without Zero Column Mask\nDESCRIPTION: Simplified template function for TCGen05 MMA workspace operations with collector B2 fill mode. Takes d_tmem, a_desc, b_desc, idesc, and enable_input_d flag without the zero column mask descriptor. Supports multiple data types via the Kind template parameter.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_33\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.ws.cta_group.kind.collector::b2::fill [d_tmem], a_desc, b_desc, idesc, enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1 }\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_collector_b2_fill(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Creating Table of Contents for Thrust Set Operations Documentation\nDESCRIPTION: RST directive that creates a table of contents (toctree) for Thrust set operations API documentation. The directive uses glob pattern matching to include all documentation files containing 'function_group__set__operations' in their name.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/api_docs/algorithms/set_operations.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :glob:\n   :maxdepth: 1\n\n   ${repo_docs_api_path}/*function_group__set__operations*\n```\n\n----------------------------------------\n\nTITLE: MMA Collector Fill Operation with Zero Column Mask\nDESCRIPTION: Template function for matrix multiply-accumulate collector fill operation with zero column mask support. Handles multiple data types and uses both descriptor and tensor memory inputs.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_collector_b0_fill(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d,\n  uint64_t zero_column_mask_desc);\n```\n\n----------------------------------------\n\nTITLE: Initializing Variables in CUDA\nDESCRIPTION: Initializes two integer variables x and f to 0. These variables will be used in the subsequent thread operations.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/memory_model.rst#2025-04-23_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\nint x = 0;\nint f = 0;\n```\n\n----------------------------------------\n\nTITLE: Memory Comparison Function\nDESCRIPTION: Inline memory comparison function to prevent ODR violations in multiple translation units.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/releases/changelog.rst#2025-04-23_snippet_33\n\nLANGUAGE: C++\nCODE:\n```\n__cuda_memcmp\n```\n\n----------------------------------------\n\nTITLE: Retrieving First CTA ID Y-coordinate in CUDA\nDESCRIPTION: Defines a device function to get the Y-coordinate of the first CTA ID from a cancellation response. It takes a 128-bit response and returns a 32-bit value.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/clusterlaunchcontrol.rst#2025-04-23_snippet_4\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true, typename B128, enable_if_t<sizeof(B128) == 16, bool> = true>\n__device__ static inline B32 clusterlaunchcontrol_query_cancel_get_first_ctaid_y(\n  B128 try_cancel_response);\n```\n\n----------------------------------------\n\nTITLE: Basic Memory Barrier Test Wait Parity Operation\nDESCRIPTION: Basic implementation of memory barrier test wait parity operation for SM_80. Takes a memory address and phase parity as parameters.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/mbarrier_test_wait_parity.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline bool mbarrier_test_wait_parity(\n  uint64_t* addr,\n  const uint32_t& phaseParity);\n```\n\n----------------------------------------\n\nTITLE: Creating TOC for Type Traits Documentation in reStructuredText\nDESCRIPTION: Sets up a table of contents structure that links to API documentation for type traits in the Thrust library. The documentation specifically references contiguous iterator proclamations and type traits typedefs through glob patterns.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/api_docs/utility/type_traits.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :glob:\n   :maxdepth: 1\n\n   ${repo_docs_api_path}/*struct*proclaim__contiguous__iterator*\n   ${repo_docs_api_path}/*typedef_group__type__traits*\n```\n\n----------------------------------------\n\nTITLE: Implementing tcgen05 MMA tmem_a_collector_b3_lastuse PTX Operation\nDESCRIPTION: Template function for tensor core matrix multiply accumulate operation with tensor memory for A input and B3 collector with last use option. Supports multiple data types through the template parameter.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_72\n\nLANGUAGE: cuda\nCODE:\n```\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_tmem_a_collector_b3_lastuse(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint32_t a_tmem,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Implementing MMA Workspace Last Use Operations with Tensor Memory in CUDA\nDESCRIPTION: Template function for the final tensor compute operation (lastuse) using tensor memory for both destination and input A. This variant supports zero column masking and works with various data types. Targets PTX ISA 86 on SM_100a and SM_101a architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_51\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.ws.cta_group.kind.collector::b2::lastuse [d_tmem], [a_tmem], b_desc, idesc, enable_input_d, zero_column_mask_desc; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1 }\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_tmem_a_collector_b2_lastuse(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint32_t a_tmem,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d,\n  uint64_t zero_column_mask_desc);\n```\n\n----------------------------------------\n\nTITLE: Declaring tcgen05_mma_ws_tmem_a_collector_b3_lastuse with zero_column_mask_desc for CUDA\nDESCRIPTION: Template function declaration for tensor core operations using tensor memory for A operand with zero column masking support. This variant works with all four data types for CTA group 1 with b3 collector in last-use mode.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_70\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.ws.cta_group.kind.collector::b3::lastuse [d_tmem], [a_tmem], b_desc, idesc, enable_input_d, zero_column_mask_desc; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1 }\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_tmem_a_collector_b3_lastuse(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint32_t a_tmem,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d,\n  uint64_t zero_column_mask_desc);\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure for Math Functions\nDESCRIPTION: ReStructuredText documentation defining the structure and availability of mathematical functions in the CCCL library, including version compatibility with CUDA toolkit.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/math.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _libcudacxx-extended-api-math:\n\nMath\n====\n\n.. toctree::\n   :hidden:\n   :maxdepth: 1\n\n   math/ceil_div\n   math/round_up\n   math/round_down\n   math/ilog\n   math/isqrt\n   math/uabs\n\n.. list-table::\n   :widths: 25 45 30 30\n   :header-rows: 1\n\n   * - **Header**\n     - **Content**\n     - **CCCL Availability**\n     - **CUDA Toolkit Availability**\n\n   * - :ref:`ceil_div <libcudacxx-extended-api-math-ceil-div>`\n     - Ceiling division\n     - CCCL 2.7.0\n     - CUDA 12.8\n\n   * - :ref:`round_up <libcudacxx-extended-api-math-round-up>`\n     - Round up to the next multiple\n     - CCCL 2.9.0\n     - CUDA 12.9\n\n   * - :ref:`round_down <libcudacxx-extended-api-math-round-down>`\n     - Round down to the previous multiple\n     - CCCL 2.9.0\n     - CUDA 12.9\n\n   * - :ref:`ilog2 <libcudacxx-extended-api-math-ilog>`\n     - Integer logarithm to the base 2\n     - CCCL 3.0.0\n     - CUDA 13.0\n\n   * - :ref:`ilog10 <libcudacxx-extended-api-math-ilog>`\n     - Integer logarithm to the base 10\n     - CCCL 3.0.0\n     - CUDA 13.0\n\n   * - :ref:`isqrt <libcudacxx-extended-api-math-isqrt>`\n     - Integer square root\n     - CCCL 3.1.0\n     - CUDA 13.1\n\n   * - :ref:`uabs <libcudacxx-extended-api-math-uabs>`\n     - Unsigned absolute value\n     - CCCL 3.1.0\n     - CUDA 13.1\n```\n\n----------------------------------------\n\nTITLE: Setting Up LLVM Lit Testing Framework Configuration\nDESCRIPTION: Configures the LLVM Lit testing framework for running libcu++ tests, including target information, executors, timeout values, and site configuration files.\nSOURCE: https://github.com/nvidia/cccl/blob/main/libcudacxx/test/libcudacxx/CMakeLists.txt#2025-04-23_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nset(LIBCUDACXX_COMPUTE_ARCHS_STRING \"${CMAKE_CUDA_ARCHITECTURES}\")\n\ninclude(AddLLVM)\n\nset(LIBCUDACXX_BINARY_DIR \"${CMAKE_CURRENT_BINARY_DIR}\")\n\nset(LIBCUDACXX_TARGET_INFO \"libcudacxx.test.target_info.LocalTI\" CACHE STRING\n\"TargetInfo to use when setting up test environment.\")\nset(LIBCUDACXX_EXECUTOR \"None\" CACHE STRING\n\"Executor to use when running tests.\")\n\nset(LIBCUDACXX_TEST_TIMEOUT \"200\" CACHE STRING \"Enable test timeouts (Default = 200, Off = 0)\")\n\nset(AUTO_GEN_COMMENT \"## Autogenerated by libcudacxx configuration.\\n# Do not edit!\")\n\nset(lit_site_cfg_path \"${CMAKE_CURRENT_BINARY_DIR}/lit.site.cfg\")\nconfigure_lit_site_cfg(\n  \"${CMAKE_CURRENT_SOURCE_DIR}/lit.site.cfg.in\"\n  \"${lit_site_cfg_path}\")\n\nadd_lit_testsuite(check-cudacxx\n  \"Running libcu++ tests\"\n  \"${CMAKE_CURRENT_BINARY_DIR}\")\n\nfind_program(libcudacxx_LIT lit REQUIRED)\n\nset(libcudacxx_LIT_FLAGS \"\" CACHE STRING \"Semi-colon separated list of flags passed to the invocation of lit.\")\nmessage(STATUS \"libcudacxx_LIT_FLAGS: ${libcudacxx_LIT_FLAGS}\")\n```\n\n----------------------------------------\n\nTITLE: Accessing Global Timer Information in CUDA\nDESCRIPTION: These functions retrieve global timer information. They use inline PTX instructions to access the special registers.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/get_sreg.rst#2025-04-23_snippet_12\n\nLANGUAGE: cuda\nCODE:\n```\n// mov.u64 sreg_value, %%globaltimer; // PTX ISA 31, SM_35\ntemplate <typename = void>\n__device__ static inline uint64_t get_sreg_globaltimer();\n```\n\nLANGUAGE: cuda\nCODE:\n```\n// mov.u32 sreg_value, %%globaltimer_lo; // PTX ISA 31, SM_35\ntemplate <typename = void>\n__device__ static inline uint32_t get_sreg_globaltimer_lo();\n```\n\nLANGUAGE: cuda\nCODE:\n```\n// mov.u32 sreg_value, %%globaltimer_hi; // PTX ISA 31, SM_35\ntemplate <typename = void>\n__device__ static inline uint32_t get_sreg_globaltimer_hi();\n```\n\n----------------------------------------\n\nTITLE: MMA Workstream Collector Implementation (Base)\nDESCRIPTION: Base template implementation for matrix multiplication workstream collector with descriptor-based inputs. Supports multiple data types and includes zero column masking.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_65\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_collector_b3_use(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d,\n  uint64_t zero_column_mask_desc);\n```\n\n----------------------------------------\n\nTITLE: Defining RestructuredText Table of Contents\nDESCRIPTION: Creates a hierarchical documentation structure using RestructuredText toctree directive to organize PTX code examples, limiting depth to 1 level and including specific example pages.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/examples.rst#2025-04-23_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. toctree::\n   :maxdepth: 1\n\n   examples/st.async\n   examples/device_tensormap_initialization\n```\n\n----------------------------------------\n\nTITLE: PRMT B32 Edge Clamp Left Template Function\nDESCRIPTION: Template function declaration for 32-bit edge clamp left permute operation (ecl mode). Clamps bytes at the left edge of the register.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/prmt.rst#2025-04-23_snippet_4\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline uint32_t prmt_ecl(\n  B32 a_reg,\n  B32 b_reg,\n  uint32_t c_reg);\n```\n\n----------------------------------------\n\nTITLE: Memory Fence with SC Semantics (Cluster Scope)\nDESCRIPTION: Template function implementing memory fence with sequential consistency semantics for cluster scope. Requires PTX ISA 7.8 and SM_90 or higher.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/fence.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline void fence(\n  cuda::ptx::sem_sc_t,\n  cuda::ptx::scope_cluster_t);\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Concept-like Macros in C++\nDESCRIPTION: This code snippet shows how to use the _CCCL_TEMPLATE, _CCCL_REQUIRES, and _CCCL_TRAIT macros to create concept-like constraints in C++.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cccl/development/macro.rst#2025-04-23_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\n_CCCL_TEMPLATE(typename T)\n_CCCL_REQUIRES(_CCCL_TRAIT(is_integral, T) _CCCL_AND(sizeof(T) > 1))\n```\n\nLANGUAGE: C++\nCODE:\n```\n_CCCL_TEMPLATE(typename T)\n_CCCL_REQUIRES(_CCCL_TRAIT(is_arithmetic, T) _CCCL_AND (!_CCCL_TRAIT(is_integral, T)))\n```\n\n----------------------------------------\n\nTITLE: Implementing tcgen05 MMA collector_b3_discard PTX Operation with zero_column_mask\nDESCRIPTION: Template function for tensor core matrix multiply accumulate operation with B3 collector and discard option with zero column masking. Supports multiple data types through the template parameter.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_73\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.ws.cta_group.kind.collector::b3::discard [d_tmem], a_desc, b_desc, idesc, enable_input_d, zero_column_mask_desc; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1 }\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_collector_b3_discard(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d,\n  uint64_t zero_column_mask_desc);\n```\n\n----------------------------------------\n\nTITLE: Setting CMake Options for Thrust Build in Bash\nDESCRIPTION: Basic command for passing CMake options to configure a Thrust build. This demonstrates how to specify configuration options when initiating a CMake build of Thrust.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/cmake_options.rst#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncmake -D<option_name>=<value> /path/to/thrust/sources\n```\n\n----------------------------------------\n\nTITLE: Creating Pre-compilation Target for CI Caching\nDESCRIPTION: Creates a custom target that builds but doesn't run the tests, which is used by CI systems to pre-seed compiler caches for improved performance. Only enabled in non-NVRTC mode.\nSOURCE: https://github.com/nvidia/cccl/blob/main/libcudacxx/test/libcudacxx/CMakeLists.txt#2025-04-23_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\nif (NOT LIBCUDACXX_TEST_WITH_NVRTC)\n  # Build but don't run the tests. Used by CI to pre-seed sccache for the test machines.\n  # Only executed if explicitly requested.\n  add_custom_target(libcudacxx.test.lit.precompile\n    DEPENDS libcudacxx.test.public_headers libcudacxx.test.internal_headers libcudacxx.test.public_headers_host_only\n    COMMAND \"${CMAKE_COMMAND}\" -E env \"LIBCUDACXX_SITE_CONFIG=${lit_site_cfg_path}\"\n    \"${libcudacxx_LIT}\" -vv --no-progress-bar --time-tests ${libcudacxx_LIT_FLAGS}\n    \"-Dexecutor=\\\"NoopExecutor()\\\"\" \"${libcudacxx_SOURCE_DIR}/test/libcudacxx\"\n  )\n\nendif()\n```\n\n----------------------------------------\n\nTITLE: Replacing Tile Fill Mode in Global Memory with CUDA C++\nDESCRIPTION: This snippet defines a templated __device__ inline function that performs a tile replacement operation using the fill mode in the global memory space for tensor maps, utilizing PTX ISA 83 extensions. Dependencies include the PTX interface and CUDA device-side compilation. It requires the global space type, a pointer to the tensor map (tm_addr), and a new value (new_val) as a 32-element vector. The function operates strictly on the device and expects correct PTX and hardware support (SM_90a, SM_100a, SM_101a).\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tensormap_replace.rst#2025-04-23_snippet_5\n\nLANGUAGE: cuda\nCODE:\n```\n// tensormap.replace.tile.fill_mode.space.b1024.b32 [tm_addr], new_val; // PTX ISA 83, SM_90a, SM_100a, SM_101a\\n// .space     = { .global }\\ntemplate <int N32>\\n__device__ static inline void tensormap_replace_fill_mode(\\n  cuda::ptx::space_global_t,\\n  void* tm_addr,\\n  cuda::ptx::n32_t<N32> new_val);\n```\n\n----------------------------------------\n\nTITLE: Defining RST Toctree Directive for Thrust Developer Build Documentation\nDESCRIPTION: An RST toctree directive that lists additional documentation pages related to Thrust module setup, specifically referencing the cmake_options page with a maximum depth of 1.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/developer_build.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :maxdepth: 1\n\n   cmake_options\n```\n\n----------------------------------------\n\nTITLE: Storing and Unpacking 16x128b Data with tcgen05 in CUDA (128 values)\nDESCRIPTION: This function performs a 16x128b store operation with 16-bit unpacking using tcgen05. It takes a 32-bit address and an array of 128 B32 values as input. The function is constrained to B32 types of 4 bytes.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_st.rst#2025-04-23_snippet_4\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.st.sync.aligned.16x128b.x64.unpack::16b.b32 [taddr], values; // PTX ISA 86, SM_100a, SM_101a\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline void tcgen05_st_16x128b_unpack_16b(\n  uint32_t taddr,\n  const B32 (&values)[128]);\n```\n\n----------------------------------------\n\nTITLE: Enabling Host-Side __half/__nv_bfloat16 Support in C++\nDESCRIPTION: Defines a preprocessor macro required when compiling host-only code directly with the host compiler (not NVCC) to enable support for `__half` (and consequently `__nv_bfloat16`) types from `<cuda_fp16.h>` and `<cuda_bf16.h>`. This must be defined before including any libcu++ headers, and requires CUDA toolkit 12.2 or higher.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/standard_api/numerics_library/complex.rst#2025-04-23_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\n// Define before including libcu++ headers for host-only compilation\n#define LIBCUDACXX_ENABLE_HOST_NVFP16\n```\n\n----------------------------------------\n\nTITLE: Building libcu++ Tests Without Running using lit (Bash)\nDESCRIPTION: This command uses `lit` to build the tests within a specified path for the `libcudacxx-cpp17` preset but prevents their execution by overriding the executor with `NoopExecutor()`. This is useful for build verification on systems without GPUs or when preparing tests for a different architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/setup/building_and_testing.rst#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd build\nlit libcudacxx-cpp17/RELATIVE_PATH_TO_TEST_OR_SUBFOLDER -sv -Dexecutor=\"NoopExecutor()\"\n```\n\n----------------------------------------\n\nTITLE: Storing and Unpacking 16x256b Data with tcgen05 in CUDA (8 values)\nDESCRIPTION: This function performs a 16x256b store operation with 16-bit unpacking using tcgen05. It takes a 32-bit address and an array of 8 B32 values as input. The function is constrained to B32 types of 4 bytes.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_st.rst#2025-04-23_snippet_8\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.st.sync.aligned.16x256b.x2.unpack::16b.b32 [taddr], values; // PTX ISA 86, SM_100a, SM_101a\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline void tcgen05_st_16x256b_unpack_16b(\n  uint32_t taddr,\n  const B32 (&values)[8]);\n```\n\n----------------------------------------\n\nTITLE: Minimum Operation for Unsigned 32-bit Integer\nDESCRIPTION: Template function implementing asynchronous minimum reduction operation on cluster memory barrier for u32 type. Takes destination pointer, value and remote barrier pointer as parameters.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/red_async.rst#2025-04-23_snippet_2\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline void red_async(\n  cuda::ptx::op_min_t,\n  uint32_t* dest,\n  const uint32_t& value,\n  uint64_t* remote_bar);\n```\n\n----------------------------------------\n\nTITLE: Declaring multimem_ld_reduce with Acquire/CTA Scope and OR Operation in CUDA PTX\nDESCRIPTION: Template function declaration for multimem.ld_reduce with acquire memory semantics, CTA scope, and OR operation on 64-bit values in global memory. Targets PTX ISA 8.1 on SM_90 architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_65\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.b64 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .or }\ntemplate <typename B64, enable_if_t<sizeof(B64) == 8, bool> = true, cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline B64 multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_or_op_t,\n  const B64* addr);\n```\n\n----------------------------------------\n\nTITLE: RST Link Definition for PTX PRMT Instruction\nDESCRIPTION: ReStructuredText anchor and link definition for the PRMT instruction documentation, including a reference to the official NVIDIA PTX documentation.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/prmt.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _libcudacxx-ptx-instructions-prmt:\n\nprmt\n====\n\n-  PTX ISA:\n   `prmt <https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-prmt>`__\n\n.. include:: generated/prmt.rst\n```\n\n----------------------------------------\n\nTITLE: Basic CUDA Variable Declaration\nDESCRIPTION: Declaration of input/output vectors and coefficient arrays used in the example.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/memory_access_properties/annotated_ptr.rst#2025-04-23_snippet_7\n\nLANGUAGE: cuda\nCODE:\n```\nsize_t N;\nint* x, *y, *z;\nint* a, *b;\n```\n\n----------------------------------------\n\nTITLE: Defining CMake Function for Adding Static Assertion Tests\nDESCRIPTION: This function, cudax_stf_add_static_assert_test, sets up a build target and CTest for a given source file. It configures the target to be excluded from regular builds and sets up a test that checks for static assertions in the compilation output.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cudax/test/stf/static_error_checks/CMakeLists.txt#2025-04-23_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(cudax_stf_add_static_assert_test target_name_var source cn_target)\n  cudax_get_target_property(config_dialect ${cn_target} DIALECT)\n  cudax_get_target_property(config_prefix ${cn_target} PREFIX)\n\n  get_filename_component(filename ${source} NAME_WE)\n\n  set(test_target \"${config_prefix}.tests.stf.error.${filename}\")\n\n  add_executable(${test_target} ${source})\n  cccl_configure_target(${test_target} DIALECT ${config_dialect})\n  cudax_clone_target_properties(${test_target} ${cn_target})\n  cudax_stf_configure_target(${test_target} ${ARGN})\n  set_target_properties(${test_target} PROPERTIES\n    EXCLUDE_FROM_ALL true\n    EXCLUDE_FROM_DEFAULT_BUILD true\n  )\n\n  add_test(NAME ${test_target}\n    COMMAND ${CMAKE_COMMAND} --build \"${CMAKE_BINARY_DIR}\"\n                             --target ${test_target}\n                             --config $<CONFIGURATION>\n  )\n  set_tests_properties(${test_target} PROPERTIES\n    PASS_REGULAR_EXPRESSION \"static_assert|static assertion\"\n  )\n\n  set(${target_name_var} ${test_target} PARENT_SCOPE)\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Creating Interface Library for Common Settings\nDESCRIPTION: Creates an interface library that centralizes common compiler settings and dependencies for all samples.\nSOURCE: https://github.com/nvidia/cccl/blob/main/examples/cudax/CMakeLists.txt#2025-04-23_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(cudax_samples_interface INTERFACE)\n\ntarget_compile_definitions(\n  cudax_samples_interface INTERFACE\n  LIBCUDACXX_ENABLE_EXPERIMENTAL_MEMORY_RESOURCE\n)\n\ntarget_link_libraries(cudax_samples_interface INTERFACE CCCL::CCCL CCCL::cudax)\n```\n\n----------------------------------------\n\nTITLE: Declaring CUDA C++ Device Function for PTX 'trap' Instruction\nDESCRIPTION: This snippet declares a CUDA C++ device-side inline function template `trap()`. It maps to the PTX `trap` instruction, which causes the executing thread to enter a trap state. This instruction is available starting from PTX ISA 10 and Compute Capability SM_50. The template parameter `<typename = void>` allows the function to be called without explicit template arguments.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/trap.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\n// trap; // PTX ISA 10, SM_50\ntemplate <typename = void>\n__device__ static inline void trap();\n```\n\n----------------------------------------\n\nTITLE: Storing 16x256b Data with tcgen05 in CUDA (16 values)\nDESCRIPTION: This function performs a 16x256b store operation using tcgen05. It takes a 32-bit address and an array of 16 B32 values as input. The function is constrained to B32 types of 4 bytes.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_st.rst#2025-04-23_snippet_9\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.st.sync.aligned.16x256b.x4.b32 [taddr], values; // PTX ISA 86, SM_100a, SM_101a\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline void tcgen05_st_16x256b(\n  uint32_t taddr,\n  const B32 (&values)[16]);\n```\n\n----------------------------------------\n\nTITLE: Defining RST Documentation Structure for CUDA Functional API\nDESCRIPTION: This RST code snippet defines the structure of the documentation for the CUDA C++ Core Libraries Functional API. It includes a table of contents and a list table describing various functions, their purposes, and version information.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/functional.rst#2025-04-23_snippet_0\n\nLANGUAGE: RST\nCODE:\n```\n.. _libcudacxx-extended-api-functional:\n\nFunctional\n----------\n\n.. toctree::\n   :hidden:\n   :maxdepth: 1\n\n   functional/proclaim_return_type\n   functional/get_device_address\n   functional/maximum_minimum\n\n.. list-table::\n   :widths: 25 45 30 30\n   :header-rows: 0\n\n   * - :ref:`cuda::maximum <libcudacxx-extended-api-functional-maximum-minimum>`\n     - Returns the maximum of two values\n     - CCCL 2.8.0\n     - CUDA 12.9\n\n   * - :ref:`cuda::minimum <libcudacxx-extended-api-functional-maximum-minimum>`\n     - Returns the minimum of two values\n     - CCCL 2.8.0\n     - CUDA 12.9\n\n   * - :ref:`cuda::proclaim_return_type <libcudacxx-extended-api-functional-proclaim-return-type>`\n     - Creates a forwarding call wrapper that proclaims return type\n     - libcu++ 1.9.0 / CCCL 2.0.0\n     - CUDA 11.8\n\n   * - ``cuda::proclaim_copyable_arguments``\n     - Creates a forwarding call wrapper that proclaims that arguments can be freely copied before an invocation of the wrapped callable\n     - CCCL 2.8.0\n     - CUDA 12.9\n\n   * - :ref:`cuda::get_device_address <libcudacxx-extended-api-functional-get-device-address>`\n     - Returns a valid address to a device object\n     - CCCL 2.8.0\n     - CUDA 12.9\n```\n\n----------------------------------------\n\nTITLE: Partitioning Policy Visualization: Blocked Layout - CUDASTF Text\nDESCRIPTION: Provides an ASCII illustration of a 2D shape split over three execution places using the blocked partitioning policy. This visual helps explain contiguous range assignment in grid partitioning, independent of any code execution. No programmatic dependencies required.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_28\n\nLANGUAGE: text\nCODE:\n```\n    __________________________________\n   |           |           |         |\n   |           |           |         |\n   |           |           |         |\n   |    P 0    |    P 1    |   P 2   |\n   |           |           |         |\n   |           |           |         |\n   |___________|___________|_________|\n```\n\n----------------------------------------\n\nTITLE: Generated Test Executable Paths\nDESCRIPTION: Shows the resulting test executable paths generated from the parameterized test compilation. Each executable corresponds to a different block size parameter.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/test_overview.rst#2025-04-23_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nbin/cub.test.scope_algorithm.bs_128\nbin/cub.test.scope_algorithm.bs_256\n```\n\n----------------------------------------\n\nTITLE: Using CMake Presets\nDESCRIPTION: Commands for configuring, building and testing using CMake presets in CCCL.\nSOURCE: https://github.com/nvidia/cccl/blob/main/CONTRIBUTING.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncmake --preset=thrust-cpp17\ncmake --build --preset=thrust-cpp17\nctest --preset=thrust-cpp17\n```\n\n----------------------------------------\n\nTITLE: Including CUDAX Build Configuration Files\nDESCRIPTION: Includes CMake files responsible for building compiler targets and target lists, as well as configuring CUDASTF target when enabled.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cudax/CMakeLists.txt#2025-04-23_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\ninclude(cmake/cudaxBuildCompilerTargets.cmake)\ninclude(cmake/cudaxBuildTargetList.cmake)\nif (cudax_ENABLE_CUDASTF)\n  include(cmake/cudaxSTFConfigureTarget.cmake)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Maximum Operation for Signed 32-bit Integer\nDESCRIPTION: Template function implementing asynchronous maximum reduction operation on cluster memory barrier for s32 type. Takes destination pointer, value and remote barrier pointer as parameters.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/red_async.rst#2025-04-23_snippet_6\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline void red_async(\n  cuda::ptx::op_max_t,\n  int32_t* dest,\n  const int32_t& value,\n  uint64_t* remote_bar);\n```\n\n----------------------------------------\n\nTITLE: Setting Up Dependencies and Build Options\nDESCRIPTION: Configures the necessary dependencies (c2h, nvtx, CUDAToolkit) and determines whether to build NVRTC tests based on the compiler being used. NVRTC tests are disabled when using the NVHPC compiler.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cub/test/CMakeLists.txt#2025-04-23_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\ncccl_get_c2h()\ncccl_get_nvtx()\n\nfind_package(CUDAToolkit)\n\nset(build_nvrtc_tests ON)\nif (\"NVHPC\" STREQUAL \"${CMAKE_CXX_COMPILER_ID}\")\n  set(build_nvrtc_tests OFF)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Storing 16x256b Data with tcgen05 in CUDA (64 values)\nDESCRIPTION: This function performs a 16x256b store operation using tcgen05. It takes a 32-bit address and an array of 64 B32 values as input. The function is constrained to B32 types of 4 bytes.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_st.rst#2025-04-23_snippet_13\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.st.sync.aligned.16x256b.x16.b32 [taddr], values; // PTX ISA 86, SM_100a, SM_101a\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline void tcgen05_st_16x256b(\n  uint32_t taddr,\n  const B32 (&values)[64]);\n```\n\n----------------------------------------\n\nTITLE: Declaring Exit Instruction Template for CUDA Device Code\nDESCRIPTION: This code snippet defines a template for the 'exit' instruction in CUDA device code. It is compatible with PTX ISA 10 and SM_50 or higher. The exit instruction is used to terminate the current thread.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/exit.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\n// exit; // PTX ISA 10, SM_50\ntemplate <typename = void>\n__device__ static inline void exit();\n```\n\n----------------------------------------\n\nTITLE: Including Parent CMake Directory\nDESCRIPTION: Adds a parent directory (two levels up, `../../`) to the build process. This allows the current project to inherit settings, variables, or targets defined in the higher-level `CMakeLists.txt` file. The logical name `_parent_cccl` is assigned to this subdirectory within the CMake context.\nSOURCE: https://github.com/nvidia/cccl/blob/main/python/cuda_cccl/CMakeLists.txt#2025-04-23_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(../.. _parent_cccl)\n```\n\n----------------------------------------\n\nTITLE: Configuring CUDA Build Project for SM Detection\nDESCRIPTION: Sets up a CMake project that builds a CUDA executable called 'getsm' and creates a custom target to run the executable during build time. This is likely used to detect or report the SM (Streaming Multiprocessor) version of the installed NVIDIA GPU.\nSOURCE: https://github.com/nvidia/cccl/blob/main/libcudacxx/test/utils/nvidia/getsm/CMakeLists.txt#2025-04-23_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.8)\n\nproject(getsm CUDA)\n\nadd_executable(getsm\n    main.cu)\n\nadd_custom_target(\n    get-sm ALL\n    COMMAND getsm\n)\n```\n\n----------------------------------------\n\nTITLE: Cluster Shared Memory Barrier Arrive\nDESCRIPTION: Memory barrier arrive operation for cluster-shared memory space. Supports release semantics with cluster scope.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/mbarrier_arrive.rst#2025-04-23_snippet_4\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline void mbarrier_arrive(\n  cuda::ptx::sem_release_t,\n  cuda::ptx::scope_cluster_t,\n  cuda::ptx::space_cluster_t,\n  uint64_t* addr);\n```\n\n----------------------------------------\n\nTITLE: Removing WarpAll and WarpAny functions in CUB 1.7.0\nDESCRIPTION: Removal of cub::WarpAll and cub::WarpAny functions which were used to emulate __all and __any functionality for deprecated SM1x devices.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/releases/changelog.rst#2025-04-23_snippet_6\n\nLANGUAGE: C++\nCODE:\n```\ncub::WarpAll\ncub::WarpAny\n```\n\n----------------------------------------\n\nTITLE: Setting Minimum Required CMake Version\nDESCRIPTION: Specifies the minimum required version of CMake needed to process this CMakeLists.txt file. It ensures compatibility by setting a range from 3.21 up to (but not including) 3.31. If the CMake version used is outside this range, a fatal error is triggered.\nSOURCE: https://github.com/nvidia/cccl/blob/main/python/cuda_cccl/CMakeLists.txt#2025-04-23_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.21...3.31 FATAL_ERROR)\n```\n\n----------------------------------------\n\nTITLE: Storing 16x256b Data with tcgen05 in CUDA (128 values)\nDESCRIPTION: This function performs a 16x256b store operation using tcgen05. It takes a 32-bit address and an array of 128 B32 values as input. The function is constrained to B32 types of 4 bytes.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_st.rst#2025-04-23_snippet_15\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.st.sync.aligned.16x256b.x32.b32 [taddr], values; // PTX ISA 86, SM_100a, SM_101a\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline void tcgen05_st_16x256b(\n  uint32_t taddr,\n  const B32 (&values)[128]);\n```\n\n----------------------------------------\n\nTITLE: Defining CTA Group Tensor Core Function for 128x128b b8x16 b4x16 p64 Variant - CUDA\nDESCRIPTION: This CUDA template function describes a tensor core operation for a 128x128b CTA group, generalizing access to both .cta_group::1 and .cta_group::2. Input parameters consist of a PTX dot_cta_group object, a 32-bit address, and a 64-bit descriptor. This function requires CUDA PTX (PTX ISA 86) configuration and is designed for specialized matrix/tensor operations with given tile size constraints.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_cp.rst#2025-04-23_snippet_8\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.cp.cta_group.128x128b.b8x16.b4x16_p64 [taddr], s_desc; // PTX ISA 86, SM_100a, SM_101a\\n// .cta_group = { .cta_group::1, .cta_group::2 }\\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\\n__device__ static inline void tcgen05_cp_128x128b_b8x16_b4x16_p64(\\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\\n  uint32_t taddr,\\n  uint64_t s_desc);\n```\n\n----------------------------------------\n\nTITLE: Viewing Extracted JSON Benchmark Results (Bash)\nDESCRIPTION: Demonstrates how to view the contents of an extracted JSON file containing benchmark results for a specific variant.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/benchmarking.rst#2025-04-23_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\ncat cub_bench_scan_exclusive_sum_base_T_ct__I32___OffsetT_ct__U32___Elements_io__pow2__28.json\n```\n\n----------------------------------------\n\nTITLE: Configuring Test Directories for libcudacxx\nDESCRIPTION: Sets up the various test directories for libcudacxx, including internal headers, public headers, host-only tests, and conditionally adding NVRTC tests if enabled.\nSOURCE: https://github.com/nvidia/cccl/blob/main/libcudacxx/test/CMakeLists.txt#2025-04-23_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\n# Libcudacxx's main lit tests\nadd_subdirectory(libcudacxx)\n\n# Libcudacxx auto-generated internal header tests\nadd_subdirectory(internal_headers)\n\n# Libcudacxx auto-generated public header tests\nadd_subdirectory(public_headers)\n\n# Libcudacxx auto-generated public header as std tests\nadd_subdirectory(public_headers_host_only)\n\n# Enable building the nvrtcc project if NVRTC is enabled\nif (LIBCUDACXX_TEST_WITH_NVRTC)\n  add_subdirectory(utils/nvidia/nvrtc)\nendif()\n\nadd_subdirectory(atomic_codegen)\n```\n\n----------------------------------------\n\nTITLE: RST Link Definition for cp.reduce.async.bulk.tensor Documentation\nDESCRIPTION: ReStructuredText directive defining a reference label and including external documentation for the cp.reduce.async.bulk.tensor PTX instruction.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/cp_reduce_async_bulk_tensor.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _libcudacxx-ptx-instructions-cp-reduce-async-bulk-tensor:\n\ncp.reduce.async.bulk.tensor\n===========================\n\n-  PTX ISA:\n   `cp.reduce.async.bulk.tensor <https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-reduce-async-bulk-tensor>`__\n\n.. include:: generated/cp_reduce_async_bulk_tensor.rst\n```\n\n----------------------------------------\n\nTITLE: Implementing MMA collector with descriptor inputs (without zero_column_mask) in CUDA\nDESCRIPTION: This function template is a variation of the MMA collector operation that doesn't include the zero_column_mask_desc parameter. It's used for simpler MMA operations that don't require column masking.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_28\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_collector_b1_discard(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Replacing Tile Fill Mode in Shared::CTA Memory with CUDA C++\nDESCRIPTION: This snippet provides a templated __device__ inline function to execute a tile replace operation in fill mode within the shared::cta memory space for tensor maps, using PTX ISA 83. The function depends on the presence of CUDA PTX interfaces, and proper device hardware support. It accepts a shared::cta space specifier, a pointer to the tensor map, and a new 32-value vector, to be executed on compatible NVIDIA architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tensormap_replace.rst#2025-04-23_snippet_6\n\nLANGUAGE: cuda\nCODE:\n```\n// tensormap.replace.tile.fill_mode.space.b1024.b32 [tm_addr], new_val; // PTX ISA 83, SM_90a, SM_100a, SM_101a\\n// .space     = { .shared::cta }\\ntemplate <int N32>\\n__device__ static inline void tensormap_replace_fill_mode(\\n  cuda::ptx::space_shared_t,\\n  void* tm_addr,\\n  cuda::ptx::n32_t<N32> new_val);\n```\n\n----------------------------------------\n\nTITLE: Configuring Thrust/CUB Library Versions in Compiler Explorer Libraries YAML\nDESCRIPTION: YAML configuration in the 'libraries.yaml' file that tells Compiler Explorer how to pull Thrust and CUB library files and defines which versions to fetch. The file specifies the GitHub repository, clone method, and target versions.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/release_process.rst#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nthrustcub:\n  type: github\n  method: clone_branch\n  repo: NVIDIA/thrust\n  check_file: dependencies/cub/cub/cub.cuh\n  targets:\n    - 1.9.9\n    - 1.9.10\n    - 1.9.10-1\n    - 1.10.0\n```\n\n----------------------------------------\n\nTITLE: Adding Common Tests Based on Configurations and Restrictions in CMake\nDESCRIPTION: Iterates through all defined Thrust targets (`THRUST_TARGETS`) and then through all common test source files found previously (`test_srcs`). For each test source, it extracts the base name (`test_name`). It then checks if this `test_name` exists in the `restricted_tests` list. If it does, it further checks if the current configuration (`${config_host}.${config_device}`) is present in the allowed list (`${test_name}_host.device_allowed`). If the test is restricted and the current configuration is not allowed, it skips adding the test using `continue()`. Otherwise, it calls the `thrust_add_test` function to add and configure the test for the current Thrust target. If the device configuration is CUDA, it calls `thrust_configure_cuda_target` to potentially enable Relocatable Device Code (RDC) based on the `THRUST_FORCE_RDC` variable.\nSOURCE: https://github.com/nvidia/cccl/blob/main/thrust/testing/CMakeLists.txt#2025-04-23_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\n# Add common tests to all configs:\nforeach(thrust_target IN LISTS THRUST_TARGETS)\n  thrust_get_target_property(config_host ${thrust_target} HOST)\n  thrust_get_target_property(config_device ${thrust_target} DEVICE)\n  thrust_get_target_property(config_prefix ${thrust_target} PREFIX)\n\n  foreach(test_src IN LISTS test_srcs)\n    get_filename_component(test_name \"${test_src}\" NAME_WLE)\n\n    # Is this test restricted to only certain host/device combinations?\n    if(${test_name} IN_LIST restricted_tests)\n      # Is the current host/device combination supported?\n      if (NOT \"${config_host}.${config_device}\" IN_LIST\n            ${test_name}_host.device_allowed)\n        continue()\n      endif()\n    endif()\n\n    thrust_add_test(test_target ${test_name} \"${test_src}\" ${thrust_target})\n\n    if (\"CUDA\" STREQUAL \"${config_device}\")\n      thrust_configure_cuda_target(${test_target} RDC ${THRUST_FORCE_RDC})\n    endif()\n  endforeach()\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Multimem Reduction Template for 32-bit Integer Min Operation\nDESCRIPTION: Template function implementing multimem reduction with minimum operation for 32-bit integers. Supports different memory semantics (relaxed/release) and scopes (cta/cluster/gpu/sys).\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_red.rst#2025-04-23_snippet_3\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.red.sem.scope.global.op.s32 [addr], val; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .release }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .min }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline void multimem_red(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_min_t,\n  int32_t* addr,\n  int32_t val);\n```\n\n----------------------------------------\n\nTITLE: Defining Documentation Structure for Stream Compaction in reStructuredText\nDESCRIPTION: This snippet configures a documentation toctree that automatically includes all documentation files related to stream compaction functions. It uses glob pattern matching to find files that match the stream_compaction function group.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/api_docs/algorithms/reordering/stream_compaction.rst#2025-04-23_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\n   :glob:\n   :maxdepth: 1\n\n   ${repo_docs_api_path}/*function_group__stream__compaction*\n```\n\n----------------------------------------\n\nTITLE: ReStructuredText Link Definition for multimem.st Documentation\nDESCRIPTION: RST markup defining the documentation reference ID and link to official NVIDIA PTX documentation for the multimem.st instruction.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/multimem_st.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _libcudacxx-ptx-instructions-multimem-st:\n\nmultimem.st\n===========\n\n-  PTX ISA:\n   `multimem.st <https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-multimem-ld-reduce-multimem-st-multimem-red>`__\n\n.. include:: generated/multimem_st.rst\n```\n\n----------------------------------------\n\nTITLE: Add Operation for Unsigned 64-bit Integer\nDESCRIPTION: Template function implementing asynchronous addition reduction operation on cluster memory barrier for u64 type. Takes destination pointer, value and remote barrier pointer as parameters.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/red_async.rst#2025-04-23_snippet_11\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline void red_async(\n  cuda::ptx::op_add_t,\n  uint64_t* dest,\n  const uint64_t& value,\n  uint64_t* remote_bar);\n```\n\n----------------------------------------\n\nTITLE: Creating Memory Resource Tests in CMake\nDESCRIPTION: Configures tests for various CUDA memory resource implementations, including device, managed, and pinned memory resources. These tests verify the proper operation of memory allocation and management functionality.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cudax/test/CMakeLists.txt#2025-04-23_snippet_10\n\nLANGUAGE: CMake\nCODE:\n```\n  cudax_add_catch2_test(test_target memory_resource ${cn_target}\n    memory_resource/any_async_resource.cu\n    memory_resource/any_resource.cu\n    memory_resource/memory_pools.cu\n    memory_resource/device_memory_resource.cu\n    memory_resource/get_memory_resource.cu\n    memory_resource/managed_memory_resource.cu\n    memory_resource/pinned_memory_resource.cu\n    memory_resource/shared_resource.cu\n  )\n```\n\n----------------------------------------\n\nTITLE: MMA Collector Fill Operation without Zero Column Mask\nDESCRIPTION: Template function for basic matrix multiply-accumulate collector fill operation without zero column mask. Supports multiple data types using descriptor-based memory access.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_collector_b0_fill(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Global Minimum Reduction for uint32 with Different Memory Scopes\nDESCRIPTION: Template implementation for global minimum reduction operations on uint32_t values. Supports both relaxed and release semantics across CTA, cluster, GPU and system memory scopes. Requires PTX ISA 8.1 and SM_90.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_red.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.red.sem.scope.global.op.u32 [addr], val; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .release }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .min }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline void multimem_red(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_min_t,\n  uint32_t* addr,\n  uint32_t val);\n```\n\n----------------------------------------\n\nTITLE: Processing Test Source Files and Creating Test Targets in CMake\nDESCRIPTION: Main loop that processes each test source file, extracts test name and variants, and creates appropriate test targets. It handles configuration for different CUB targets, regular and variant tests, and sets up RDC (Relocatable Device Code) appropriately based on test requirements.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cub/test/CMakeLists.txt#2025-04-23_snippet_11\n\nLANGUAGE: cmake\nCODE:\n```\nforeach (test_src IN LISTS test_srcs)\n  get_filename_component(test_name \"${test_src}\" NAME_WE)\n  string(REGEX REPLACE \"^catch2_test_\" \"\" test_name \"${test_name}\")\n  string(REGEX REPLACE \"^test_\" \"\" test_name \"${test_name}\")\n\n  cub_get_test_params(\"${test_src}\" variant_labels variant_defs)\n  list(LENGTH variant_labels num_variants)\n\n  if (\"${test_name}\" MATCHES \"nvrtc\")\n    if (NOT build_nvrtc_tests)\n      continue()\n    endif()\n  endif()\n\n  # Subtract 1 to support the inclusive endpoint of foreach(...RANGE...):\n  math(EXPR range_end \"${num_variants} - 1\")\n\n  # Verbose output:\n  if (num_variants GREATER 0)\n    message(VERBOSE \"Detected ${num_variants} variants of test '${test_src}':\")\n    foreach(var_idx RANGE ${range_end})\n      math(EXPR i \"${var_idx} + 1\")\n      list(GET variant_labels ${var_idx} label)\n      list(GET variant_defs ${var_idx} defs)\n      message(VERBOSE \"  ${i}: ${test_name} ${label} ${defs}\")\n    endforeach()\n  endif()\n\n  foreach(cub_target IN LISTS CUB_TARGETS)\n    cub_get_target_property(config_prefix ${cub_target} PREFIX)\n\n    if (num_variants EQUAL 0)\n      if (${CUB_FORCE_RDC})\n        set(launcher 1)\n      else()\n        set(launcher 0)\n      endif()\n\n      # FIXME: There are a few remaining device algorithm tests that have not been ported to\n      # use Catch2 and lid variants. Mark these as `lid_0/1` so they'll run in the appropriate\n      # CI configs:\n      string(REGEX MATCH \"^device_\" is_device_test \"${test_name}\")\n      _cub_is_fail_test(is_fail_test \"%{test_name}\")\n      if (is_device_test AND NOT is_fail_test)\n        string(APPEND test_name \".lid_${launcher}\")\n      endif()\n\n      # Only one version of this test.\n      cub_add_test(test_target ${test_name} \"${test_src}\" ${cub_target} ${launcher})\n      cub_configure_cuda_target(${test_target} RDC ${CUB_FORCE_RDC})\n    else() # has variants:\n      # Meta target to build all parametrizations of the current test for the\n      # current CUB_TARGET config\n      set(variant_meta_target ${config_prefix}.test.${test_name}.all)\n      if (NOT TARGET ${variant_meta_target})\n        add_custom_target(${variant_meta_target})\n      endif()\n\n      # Meta target to build all parametrizations of the current test for all\n      # CUB_TARGET configs\n      set(cub_variant_meta_target cub.all.test.${test_name}.all)\n      if (NOT TARGET ${cub_variant_meta_target})\n        add_custom_target(${cub_variant_meta_target})\n      endif()\n\n      # Generate multiple tests, one per variant.\n      # See `cub_get_test_params` for details.\n      foreach(var_idx RANGE ${range_end})\n        list(GET variant_labels ${var_idx} label)\n        list(GET variant_defs ${var_idx} defs)\n        string(REPLACE \":\" \";\" defs \"${defs}\")\n        # A unique index per variant:\n        list(APPEND defs VAR_IDX=${var_idx})\n\n        # Check if the test explicitly specifies launcher id:\n        _cub_has_lid_variant(explicit_launcher \"${label}\")\n        _cub_launcher_id(explicit_launcher_id \"${label}\")\n\n        if (${explicit_launcher})\n          set(launcher_id \"${explicit_launcher_id}\")\n        else()\n          if (${CUB_FORCE_RDC})\n            set(launcher_id 1)\n          else()\n            set(launcher_id 0)\n          endif()\n        endif()\n\n        _cub_launcher_requires_rdc(cdp_val \"${launcher_id}\")\n\n        if (cdp_val AND NOT CUB_ENABLE_RDC_TESTS)\n          continue()\n        endif()\n\n        cub_add_test(test_target ${test_name}.${label} \"${test_src}\" ${cub_target} ${launcher_id})\n\n        # Enable RDC if the test either:\n        # 1. Explicitly requests it (lid_1 label)\n        # 2. Does not have an explicit CDP variant (no lid_0, lid_1, or lid_2) but\n        #    RDC testing is forced\n        #\n        # Tests that explicitly request no cdp (lid_0 label) should never enable\n        # RDC.\n        cub_configure_cuda_target(${test_target} RDC ${cdp_val})\n        add_dependencies(${variant_meta_target} ${test_target})\n        add_dependencies(${cub_variant_meta_target} ${test_target})\n        target_compile_definitions(${test_target} PRIVATE ${defs})\n      endforeach() # Variant\n    endif() # Has variants\n  endforeach() # CUB targets\nendforeach() # Source file\n\nadd_subdirectory(cmake)\n```\n\n----------------------------------------\n\nTITLE: CTA Group Template Function for 4x256b Configuration\nDESCRIPTION: Template function declaration for 4x256b memory layout supporting CTA groups 1 and 2. Takes a CTA group parameter, target address, and descriptor as inputs.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_cp.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.cp.cta_group.4x256b [taddr], s_desc; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1, .cta_group::2 }\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_cp_4x256b(\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t taddr,\n  uint64_t s_desc);\n```\n\n----------------------------------------\n\nTITLE: Defining Toctree Structure for Thrust Gather Algorithm Documentation\nDESCRIPTION: RST directive that creates a table of contents tree (toctree) for Gather algorithm documentation. It includes all files matching the pattern '*function_group__gather*' in the API documentation path, with a maximum depth of 1 level.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/api_docs/algorithms/copying/gather.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :glob:\n   :maxdepth: 1\n\n   ${repo_docs_api_path}/*function_group__gather*\n```\n\n----------------------------------------\n\nTITLE: Defining the CCCL Headers Project in CMake\nDESCRIPTION: Defines the CMake project named `CCCL_HEADERS`. It sets the project version using the `SKBUILD_PROJECT_VERSION` variable (likely provided externally, possibly by scikit-build), specifies C and CXX as the project languages, and provides a brief description.\nSOURCE: https://github.com/nvidia/cccl/blob/main/python/cuda_cccl/CMakeLists.txt#2025-04-23_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\n# include_guard(GLOBAL)\n\nproject(\n    CCCL_HEADERS\n    VERSION ${SKBUILD_PROJECT_VERSION}\n    LANGUAGES C CXX\n    DESCRIPTION \"Headers of NVIDIA CUDA Core Compute Libraries\"\n)\n```\n\n----------------------------------------\n\nTITLE: RST Documentation for PTX clusterlaunchcontrol Instructions\nDESCRIPTION: ReStructuredText documentation defining reference links and descriptions for PTX clusterlaunchcontrol instructions used in CUDA parallel thread execution.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/clusterlaunchcontrol.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _libcudacxx-ptx-instructions-clusterlaunchcontrol:\n\nclusterlaunchcontrol\n====================\n\n-  PTX ISA:\n   `clusterlaunchcontrol.try_cancel <https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-clusterlaunchcontrol-try-cancel>`__\n-  PTX ISA:\n   `clusterlaunchcontrol.query_cancel <https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-clusterlaunchcontrol-query-cancel>`__\n\n.. include:: generated/clusterlaunchcontrol.rst\n```\n\n----------------------------------------\n\nTITLE: Declaring Load-Reduce Minimum Operation (relaxed/cluster/global/u64) in CUDA C++\nDESCRIPTION: Declares a device inline template function for performing minimum-reduction loads into a 64-bit unsigned integer location from global memory, with specified relaxed or acquire PTX semaphore and cluster-wide scope. Requires PTX type definitions for correct usage and supports versatile synchronization models in CUDA libraries.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_11\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.u64 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .min }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline uint64_t multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_min_t,\n  const uint64_t* addr);\n```\n\n----------------------------------------\n\nTITLE: Basic Asynchronous Bulk Tensor Copy (5D)\nDESCRIPTION: Template function for copying 5D tensor data from global memory to cluster shared memory asynchronously. Implements the basic version without CTA group specification.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_async_bulk_tensor_multicast.rst#2025-04-23_snippet_6\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline void cp_async_bulk_tensor(\n  cuda::ptx::space_cluster_t,\n  cuda::ptx::space_global_t,\n  void* dstMem,\n  const void* tensorMap,\n  const int32_t (&tensorCoords)[5],\n  uint64_t* smem_bar,\n  const uint16_t& ctaMask);\n```\n\n----------------------------------------\n\nTITLE: Configuring CUDA Architecture Support\nDESCRIPTION: Processes the list of CUDA compute architectures to build for, creating a formatted string for display and configuration purposes.\nSOURCE: https://github.com/nvidia/cccl/blob/main/libcudacxx/test/libcudacxx/CMakeLists.txt#2025-04-23_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nset(LIBCUDACXX_HIGHEST_COMPUTE_ARCH 90)\n\nforeach (COMPUTE_ARCH ${LIBCUDACXX_COMPUTE_ARCHS})\n  set(_compute_message \"${_compute_message} sm_${COMPUTE_ARCH}\")\n  set(LIBCUDACXX_COMPUTE_ARCHS_STRING \"${LIBCUDACXX_COMPUTE_ARCHS_STRING} ${COMPUTE_ARCH}\")\nendforeach ()\n\nmessage(STATUS \"Enabled CUDA architectures:${_compute_message}\")\n```\n\n----------------------------------------\n\nTITLE: Storing and Unpacking 16x256b Data with tcgen05 in CUDA (4 values)\nDESCRIPTION: This function performs a 16x256b store operation with 16-bit unpacking using tcgen05. It takes a 32-bit address and an array of 4 B32 values as input. The function is constrained to B32 types of 4 bytes.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_st.rst#2025-04-23_snippet_6\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.st.sync.aligned.16x256b.x1.unpack::16b.b32 [taddr], values; // PTX ISA 86, SM_100a, SM_101a\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline void tcgen05_st_16x256b_unpack_16b(\n  uint32_t taddr,\n  const B32 (&values)[4]);\n```\n\n----------------------------------------\n\nTITLE: Referencing bmsk PTX Instruction in RST Documentation\nDESCRIPTION: This RST code snippet creates a reference link to the bmsk PTX instruction in the NVIDIA CUDA documentation and includes a generated file with detailed information about the instruction.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/bmsk.rst#2025-04-23_snippet_0\n\nLANGUAGE: RST\nCODE:\n```\n.. _libcudacxx-ptx-instructions-bmsk:\n\nbmsk\n====\n\n-  PTX ISA:\n   `bmsk <https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#integer-arithmetic-instructions-bmsk>`__\n\n.. include:: generated/bmsk.rst\n```\n\n----------------------------------------\n\nTITLE: Implementing CUDA PTX Multimemory XOR Reduction Template Function\nDESCRIPTION: Template function for 32-bit XOR reduction operations with configurable memory semantics and scope. This function enables atomic XOR operations in global memory with different synchronization scopes.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_red.rst#2025-04-23_snippet_35\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.red.sem.scope.global.op.b32 [addr], val; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .release }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .xor }\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true, cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline void multimem_red(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_xor_op_t,\n  B32* addr,\n  B32 val);\n```\n\n----------------------------------------\n\nTITLE: Add Operation for Unsigned 32-bit Integer\nDESCRIPTION: Template function implementing asynchronous addition reduction operation on cluster memory barrier for u32 type. Takes destination pointer, value and remote barrier pointer as parameters.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/red_async.rst#2025-04-23_snippet_4\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline void red_async(\n  cuda::ptx::op_add_t,\n  uint32_t* dest,\n  const uint32_t& value,\n  uint64_t* remote_bar);\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Target for Testing Internal Headers in CMake\nDESCRIPTION: Creates a custom target for testing all internal headers in libcudacxx to verify modularity. This target will collect all individual header tests.\nSOURCE: https://github.com/nvidia/cccl/blob/main/libcudacxx/test/internal_headers/CMakeLists.txt#2025-04-23_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_custom_target(libcudacxx.test.internal_headers)\n```\n\n----------------------------------------\n\nTITLE: ReStructuredText Reference Link to PTX Special Registers\nDESCRIPTION: RST markup linking to the PTX ISA documentation section on special registers and including a generated file with special register details.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/special_registers.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _libcudacxx-ptx-instructions-special-registers:\n\nSpecial registers\n=================\n\n-  PTX ISA:\n   `Special Register <https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#special-registers>`__\n\n.. include:: generated/get_sreg.rst\n```\n\n----------------------------------------\n\nTITLE: CUDA Memory Space Type Traits\nDESCRIPTION: Trait definitions for checking accessor types and memory space accessibility.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/mdspan/host_device_accessor.rst#2025-04-23_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\ntemplate <typename T>\ninline constexpr bool is_host_accessor_v = /* true if T is a host accessor, false otherwise */\n\ntemplate <typename T>\ninline constexpr bool is_device_accessor_v = /* true if T is a device accessor, false otherwise */\n\ntemplate <typename T>\ninline constexpr bool is_managed_accessor_v = /* true if T is a managed accessor, false otherwise */\n\ntemplate <typename T>\ninline constexpr bool is_host_accessible_v = /* true if T is a mdspan/accessor accessible from the host, false otherwise */\n\ntemplate <typename T>\ninline constexpr bool is_device_accessible_v = /* true if T is a mdspan/accessor accessible from the device, false otherwise */\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Bulk MAX Reduction (Signed 32-bit) in CUDA\nDESCRIPTION: Performs an asynchronous bulk MAX reduction operation from CTA-shared to cluster-shared memory for signed 32-bit integers. Uses a memory barrier for synchronization.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/cp_reduce_async_bulk.rst#2025-04-23_snippet_9\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline void cp_reduce_async_bulk(\n  cuda::ptx::space_cluster_t,\n  cuda::ptx::space_shared_t,\n  cuda::ptx::op_max_t,\n  int32_t* dstMem,\n  const int32_t* srcMem,\n  uint32_t size,\n  uint64_t* rdsmem_bar);\n```\n\n----------------------------------------\n\nTITLE: Including C Standard Math Functions Header in CUDA C++\nDESCRIPTION: Includes the `<cuda/std/cmath>` header, providing common mathematical functions for use in CUDA C++. Available since CCCL 2.2.0 and CUDA Toolkit 12.3.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/standard_api/c_library.rst#2025-04-23_snippet_5\n\nLANGUAGE: C++\nCODE:\n```\n#include <cuda/std/cmath>\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Empty Loop Limitations in CUDA Device Code\nDESCRIPTION: Example showing how a trivial infinite loop without any operations may not guarantee forward progress in device threads, unlike with host threads.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/execution_model.rst#2025-04-23_snippet_4\n\nLANGUAGE: cuda\nCODE:\n```\n__global void ex4() {\n    while(true) { /* empty */ }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Multimem Reduction Max Operations for u32 Type in CUDA\nDESCRIPTION: Template function declarations for multimem reduction max operations on uint32_t data. These functions support different memory semantics (relaxed, release) and memory scopes (cta, cluster, gpu, sys). These PTX operations are available in PTX ISA 8.1 on SM_90 architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_red.rst#2025-04-23_snippet_6\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.red.sem.scope.global.op.u32 [addr], val; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .release }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .max }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline void multimem_red(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_max_t,\n  uint32_t* addr,\n  uint32_t val);\n```\n\n----------------------------------------\n\nTITLE: Using cuda::device::barrier_native_handle in a CUDA Kernel\nDESCRIPTION: Example of using cuda::device::barrier_native_handle in a CUDA kernel to obtain a pointer to the native barrier handle and use it with inline PTX assembly.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/synchronization_primitives/barrier/barrier_native_handle.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\n#include <cuda/barrier>\n\n__global__ void example_kernel(cuda::barrier<cuda::thread_scope_block>& bar) {\n  auto ptr = cuda::device::barrier_native_handle(bar);\n\n  asm volatile (\n      \"mbarrier.arrive.b64 _, [%0];\"\n      :\n      : \"l\" (ptr)\n      : \"memory\");\n  // Equivalent to: `(void)b.arrive()`.\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Git SSH Signing\nDESCRIPTION: Git configuration commands for setting up SSH key-based commit signing to enable automatic CI triggering for internal NVIDIA contributors.\nSOURCE: https://github.com/nvidia/cccl/blob/main/ci-overview.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit config --global gpg.format ssh\ngit config --global user.signingKey ~/.ssh/YOUR_PUBLIC_KEY_FILE_HERE.pub\n\n# These settings are optional. They tell git to automatically sign all new commits and tags.\n# If these are set to false, use `git commit -S` to manually sign each commit.\ngit config --global commit.gpgsign true\ngit config --global tag.gpgsign true\n```\n\n----------------------------------------\n\nTITLE: Finding Compiler-Specific CUDA Dependencies\nDESCRIPTION: Detects the compiler type and finds the appropriate CUDA dependencies. Uses NVHPC package for NVHPC compiler, otherwise finds the CUDAToolkit package.\nSOURCE: https://github.com/nvidia/cccl/blob/main/libcudacxx/test/internal_headers/CMakeLists.txt#2025-04-23_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif (\"NVHPC\" STREQUAL \"${CMAKE_CXX_COMPILER_ID}\")\n  find_package(NVHPC)\nelse()\n  find_package(CUDAToolkit)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Declaring Weak-Semantic Multimem Load-Reduce (min, s64) with CUDA PTX (C++)\nDESCRIPTION: This template specializes a device function for atomic minimal reduction (min) for 64-bit signed integers, employing weak semantic guarantees. It accepts a semantic type, reduction op, and pointer to a 64-bit int global memory location. Returns the reduced int64_t value. This function depends on PTX types and is intended for usage on PTX ISA 81, SM_90 hardware.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_20\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.global.op.s64 dest, [addr]; // PTX ISA 81, SM_90\\n// .sem       = { .weak }\\n// .op        = { .min }\\ntemplate <typename = void>\\n__device__ static inline int64_t multimem_ld_reduce(\\n  cuda::ptx::sem_weak_t,\\n  cuda::ptx::op_min_t,\\n  const int64_t* addr);\n```\n\n----------------------------------------\n\nTITLE: Compiling CUDASTF Applications with NVCC\nDESCRIPTION: Demonstrates the compilation and linking flags required for building CUDASTF applications using NVCC.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Compilation flags\nnvcc -std=c++17 --expt-relaxed-constexpr --extended-lambda -I$(cudastf_path)\n# Linking flags\nnvcc -lcuda\n```\n\n----------------------------------------\n\nTITLE: Defining toctree for Thrust Iterators Documentation in reStructuredText\nDESCRIPTION: A reStructuredText directive that defines the table of contents for the Iterators documentation section of the Thrust module API. It includes links to fancy iterators and tags documentation pages.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/api_docs/iterators.rst#2025-04-23_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. toctree::\n   :glob:\n   :maxdepth: 2\n\n   iterators/fancy_iterators\n   iterators/tags\n```\n\n----------------------------------------\n\nTITLE: Configuring Matrix Override in YAML\nDESCRIPTION: Example YAML configuration showing how to override the default PR matrix for specific testing scenarios. This allows reducing resource usage by limiting tests to relevant configurations.\nSOURCE: https://github.com/nvidia/cccl/blob/main/ci-overview.md#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nworkflows:\n  override:\n    - {jobs: ['build'], project: 'cudax', ctk: '12.0', std: 'all', cxx: ['msvc14.39', 'gcc10', 'clang14']}\n  pull_request:\n    - <...>\n```\n\n----------------------------------------\n\nTITLE: Implementing multimem.red.relaxed.cluster.global.max.s64 Operation in CUDA PTX\nDESCRIPTION: Template function declaration for performing a 64-bit signed integer max reduction with relaxed semantics and cluster scope in global memory. This function is available for SM_90 and requires PTX ISA 8.1 or higher.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_red.rst#2025-04-23_snippet_11\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.red.sem.scope.global.op.s64 [addr], val; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .release }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .max }\ntemplate <cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline void multimem_red(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_max_t,\n  int64_t* addr,\n  int64_t val);\n```\n\n----------------------------------------\n\nTITLE: Global Address Replacement in Global Memory\nDESCRIPTION: Device function to replace global address in tensormap stored in global memory. Takes a 64-bit value for the new address.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tensormap_replace.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename B64, enable_if_t<sizeof(B64) == 8, bool> = true>\n__device__ static inline void tensormap_replace_global_address(\n  cuda::ptx::space_global_t,\n  void* tm_addr,\n  B64 new_val);\n```\n\n----------------------------------------\n\nTITLE: Creating Table of Contents for Thrust Partitioning Algorithms in reStructuredText\nDESCRIPTION: Creates a toctree directive that includes all documentation files related to partitioning functions in the Thrust module. The toctree uses glob pattern matching to find all files matching the partitioning function group pattern.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/api_docs/algorithms/reordering/partitioning.rst#2025-04-23_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. toctree::\n   :glob:\n   :maxdepth: 1\n\n   ${repo_docs_api_path}/*function_group__partitioning*\n```\n\n----------------------------------------\n\nTITLE: Configuring Optional Testing Components\nDESCRIPTION: Conditionally enables testing and header testing based on the specified CMake options. If enabled, adds the test directory or includes the header testing configuration.\nSOURCE: https://github.com/nvidia/cccl/blob/main/c/parallel/CMakeLists.txt#2025-04-23_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nif (CCCL_C_Parallel_ENABLE_TESTING)\n  add_subdirectory(test)\nendif()\n\nif (CCCL_C_Parallel_ENABLE_HEADER_TESTING)\n  include(cmake/CParallelHeaderTesting.cmake)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Extracting Launcher ID from Test Label in CMake\nDESCRIPTION: Helper function that extracts a launcher ID from a test label using regex matching. If a match is found in the format 'lid_X' where X is a number, it returns that number as the launcher ID; otherwise, it returns 0.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cub/test/CMakeLists.txt#2025-04-23_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\nfunction(_cub_launcher_id out_var label)\n  string(REGEX MATCH \"lid_([0-9]+)\" MATCH_RESULT \"${label}\")\n  if(MATCH_RESULT)\n    set(${out_var} ${CMAKE_MATCH_1} PARENT_SCOPE)\n  else()\n    set(${out_var} 0 PARENT_SCOPE)\n  endif()\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Building Linux Samples with Make\nDESCRIPTION: Commands for building Linux samples using makefiles. The code shows how to navigate to a sample directory and run the make command.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cudax/test/stf/cuda-samples/0_Introduction/vectorAdd/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ cd <sample_dir>\n$ make\n```\n\n----------------------------------------\n\nTITLE: Accessing Grid Dimensions in CUDA\nDESCRIPTION: These functions retrieve the x, y, and z dimensions of the grid. They use inline PTX instructions to access the special registers.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/get_sreg.rst#2025-04-23_snippet_4\n\nLANGUAGE: cuda\nCODE:\n```\n// mov.u32 sreg_value, %%nctaid.x; // PTX ISA 20\ntemplate <typename = void>\n__device__ static inline uint32_t get_sreg_nctaid_x();\n```\n\nLANGUAGE: cuda\nCODE:\n```\n// mov.u32 sreg_value, %%nctaid.y; // PTX ISA 20\ntemplate <typename = void>\n__device__ static inline uint32_t get_sreg_nctaid_y();\n```\n\nLANGUAGE: cuda\nCODE:\n```\n// mov.u32 sreg_value, %%nctaid.z; // PTX ISA 20\ntemplate <typename = void>\n__device__ static inline uint32_t get_sreg_nctaid_z();\n```\n\n----------------------------------------\n\nTITLE: Complete CUDA Kernel Launch Sequence with Memory Pinning\nDESCRIPTION: Full sequence of kernel launches including memory pinning and unpinning operations.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/memory_access_properties/apply_access_property.rst#2025-04-23_snippet_6\n\nLANGUAGE: cuda\nCODE:\n```\npin<<<grid, block>>>(a, b, N);\nupdate<<<grid, block>>>(x, a, b, N);\nupdate<<<grid, block>>>(y, a, b, N);\nupdate<<<grid, block>>>(z, a, b, N);\nunpin<<<grid, block>>>(a, b, N);\n```\n\n----------------------------------------\n\nTITLE: Defining for_each_canceled_block Function in CUDA\nDESCRIPTION: Defines the for_each_canceled_block function template in the cuda namespace. This function is used for implementing work-stealing at the thread-block level in CUDA kernels.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/work_stealing.rst#2025-04-23_snippet_0\n\nLANGUAGE: cuda\nCODE:\n```\nnamespace cuda {\n\n    template <int ThreadBlockRank = 3, typename UnaryFunction = ..unspecified..>\n    __device__ void for_each_canceled_block(UnaryFunction uf);\n\n} // namespace cuda\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents for Extrema Algorithm Documentation in reStructuredText\nDESCRIPTION: This snippet creates a table of contents (toctree) that uses glob patterns to include all documentation files that contain 'function_group__extrema' in their names. The toctree is set to display at maximum depth of 1.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/api_docs/algorithms/reductions/extrema.rst#2025-04-23_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\n   :glob:\n   :maxdepth: 1\n\n   ${repo_docs_api_path}/*function_group__extrema*\n```\n\n----------------------------------------\n\nTITLE: Declaring cuda::bit_reverse Function in C++\nDESCRIPTION: Function signature for cuda::bit_reverse, which takes a template parameter T and returns the bit-reversed value of the input.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/bit/bit_reverse.rst#2025-04-23_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\ntemplate <typename T>\n[[nodiscard]] constexpr T\nbit_reverse(T value) noexcept;\n```\n\n----------------------------------------\n\nTITLE: Tensormap Copy Fence Proxy - GPU Scope\nDESCRIPTION: Template function for tensormap copy operation with release semantics and GPU scope synchronization. Handles aligned memory transfers between global and shared memory.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tensormap_cp_fenceproxy.rst#2025-04-23_snippet_2\n\nLANGUAGE: cuda\nCODE:\n```\n// tensormap.cp_fenceproxy.global.shared::cta.tensormap::generic.sem.scope.sync.aligned  [dst], [src], size; // PTX ISA 83, SM_90\n// .sem       = { .release }\n// .scope     = { .cta, .cluster, .gpu, .sys }\ntemplate <int N32, cuda::ptx::dot_scope Scope>\n__device__ static inline void tensormap_cp_fenceproxy(\n  cuda::ptx::sem_release_t,\n  cuda::ptx::scope_t<Scope> scope,\n  void* dst,\n  const void* src,\n  cuda::ptx::n32_t<N32> size);\n```\n\n----------------------------------------\n\nTITLE: Cross-compiling CUDA Samples for Different Architectures\nDESCRIPTION: Make commands for cross-compiling CUDA samples targeting specific architectures including x86_64, ppc64le, and armv7l.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cudax/test/stf/cuda-samples/0_Introduction/vectorAdd/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ make TARGET_ARCH=x86_64\n$ make TARGET_ARCH=ppc64le\n$ make TARGET_ARCH=armv7l\n```\n\n----------------------------------------\n\nTITLE: Setting Host Compiler Warning Levels\nDESCRIPTION: Configures warning levels for host compilers, handling both MSVC and GCC-style compilers differently.\nSOURCE: https://github.com/nvidia/cccl/blob/main/libcudacxx/test/CMakeLists.txt#2025-04-23_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nif(MSVC)\n  set(headertest_warning_levels_host /W4 /WX)\nelse()\n  set(headertest_warning_levels_host -Wall -Werror)\nendif()\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Link for mbarrier.init\nDESCRIPTION: ReStructuredText markup that defines a reference link for the mbarrier.init PTX instruction documentation\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/mbarrier_init.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _libcudacxx-ptx-instructions-mbarrier-init:\n\nmbarrier.init\n=============\n\n-  PTX ISA:\n   `mbarrier.arrive <https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and-communication-instructions-mbarrier-init>`__\n\n.. include:: generated/mbarrier_init.rst\n```\n\n----------------------------------------\n\nTITLE: Creating a Table of Contents for Replacing Algorithms in reStructuredText\nDESCRIPTION: This code snippet creates a table of contents in reStructuredText format. It uses the toctree directive to generate links to all documentation files matching the pattern 'function_group__replacing*' in the repository's API documentation path.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/api_docs/algorithms/transformations/replacing.rst#2025-04-23_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. toctree::\n   :glob:\n   :maxdepth: 1\n\n   ${repo_docs_api_path}/*function_group__replacing*\n```\n\n----------------------------------------\n\nTITLE: Handling Conditional Include for libcudacxx\nDESCRIPTION: Includes a separate CMake file for handling the project as a subdirectory when CCCL_ENABLE_LIBCUDACXX is not enabled, and returns early to prevent further processing.\nSOURCE: https://github.com/nvidia/cccl/blob/main/libcudacxx/CMakeLists.txt#2025-04-23_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nif (NOT CCCL_ENABLE_LIBCUDACXX)\n  include(cmake/libcudacxxAddSubdir.cmake)\n  return()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining tcgen05_mma_ws_collector_b3_fill Template Function for CUDA PTX\nDESCRIPTION: Template function declaration for filling b3 collector with various data types (f16, tf32, f8f6f4, i8) in MMA workstation operations. The function takes descriptors for matrices and enables input D configuration.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_61\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.ws.cta_group.kind.collector::b3::fill [d_tmem], a_desc, b_desc, idesc, enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1 }\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_collector_b3_fill(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Implementing Restrict Accessor Template Alias in C++\nDESCRIPTION: Template alias definition for creating an accessor with restrict aliasing policy from an existing accessor type.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/extended_api/mdspan/restrict_accessor.rst#2025-04-23_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\ntemplate <typename Accessor>\nusing restrict_accessor;\n```\n\n----------------------------------------\n\nTITLE: Storing and Unpacking 16x256b Data with tcgen05 in CUDA (64 values)\nDESCRIPTION: This function performs a 16x256b store operation with 16-bit unpacking using tcgen05. It takes a 32-bit address and an array of 64 B32 values as input. The function is constrained to B32 types of 4 bytes.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_st.rst#2025-04-23_snippet_14\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.st.sync.aligned.16x256b.x16.unpack::16b.b32 [taddr], values; // PTX ISA 86, SM_100a, SM_101a\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline void tcgen05_st_16x256b_unpack_16b(\n  uint32_t taddr,\n  const B32 (&values)[64]);\n```\n\n----------------------------------------\n\nTITLE: Conditionally Including Header Testing, Tests, and Examples\nDESCRIPTION: Conditionally includes header testing, tests, and examples based on the options configured earlier. This controls what components of the project are built.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cudax/CMakeLists.txt#2025-04-23_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nif (cudax_ENABLE_HEADER_TESTING)\n  include(cmake/cudaxHeaderTesting.cmake)\nendif()\n\nif (cudax_ENABLE_TESTING)\n  add_subdirectory(test)\nendif()\n\nif (cudax_ENABLE_EXAMPLES)\n  add_subdirectory(examples)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Declaring multimem_ld_reduce with Relaxed/CTA Scope and XOR Operation in CUDA PTX\nDESCRIPTION: Template function declaration for multimem.ld_reduce with relaxed memory semantics, CTA scope, and XOR operation on 64-bit values in global memory. For PTX ISA 8.1 on SM_90 architecture.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/multimem_ld_reduce.rst#2025-04-23_snippet_70\n\nLANGUAGE: cuda\nCODE:\n```\n// multimem.ld_reduce.sem.scope.global.op.b64 dest, [addr]; // PTX ISA 81, SM_90\n// .sem       = { .relaxed, .acquire }\n// .scope     = { .cta, .cluster, .gpu, .sys }\n// .op        = { .xor }\ntemplate <typename B64, enable_if_t<sizeof(B64) == 8, bool> = true, cuda::ptx::dot_sem Sem, cuda::ptx::dot_scope Scope>\n__device__ static inline B64 multimem_ld_reduce(\n  cuda::ptx::sem_t<Sem> sem,\n  cuda::ptx::scope_t<Scope> scope,\n  cuda::ptx::op_xor_op_t,\n  const B64* addr);\n```\n\n----------------------------------------\n\nTITLE: Defining CUDA Device Function Template tcgen05_mma_ws_collector_b0_lastuse (Version 2)\nDESCRIPTION: Defines a templated CUDA `__device__` inline function `tcgen05_mma_ws_collector_b0_lastuse` for tensor core MMA operations (lastuse stage) within a CTA group 1. This version takes a shared memory pointer/descriptor for D (`d_tmem`), descriptors for A and B (`a_desc`, `b_desc`), an index descriptor (`idesc`), and an enable flag (`enable_input_d`), but does not take `zero_column_mask_desc`. The template parameter `Kind` specifies the data type (f16, tf32, f8f6f4, i8). Requires PTX ISA 86 and targets SM_100a/SM_101a architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_10\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.ws.cta_group.kind.collector::b0::lastuse [d_tmem], a_desc, b_desc, idesc, enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1 }\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_collector_b0_lastuse(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Creating toctree for Thrust merging algorithms documentation\nDESCRIPTION: This reStructuredText snippet creates a toctree that includes all documentation files matching the pattern '*function_group__merging*' from the repository's API documentation path. The toctree is configured to display files alphabetically with a maximum depth of 1.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/thrust/api_docs/algorithms/merging.rst#2025-04-23_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. toctree::\n   :glob:\n   :maxdepth: 1\n\n   ${repo_docs_api_path}/*function_group__merging*\n```\n\n----------------------------------------\n\nTITLE: Configuring C++ Extensions and Building Targets\nDESCRIPTION: Disables C++ extensions to ensure standard-compliant code and calls custom functions to set up compiler targets and build target lists for the project.\nSOURCE: https://github.com/nvidia/cccl/blob/main/cub/CMakeLists.txt#2025-04-23_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nset(CMAKE_CXX_EXTENSIONS OFF)\n\ncub_build_compiler_targets()\ncub_build_target_list()\n```\n\n----------------------------------------\n\nTITLE: Defining CUDA Device Function Template tcgen05_mma_ws_collector_b0_lastuse (Version 1)\nDESCRIPTION: Defines a templated CUDA `__device__` inline function `tcgen05_mma_ws_collector_b0_lastuse` for tensor core MMA operations (lastuse stage) within a CTA group 1. It takes a shared memory pointer/descriptor for D (`d_tmem`), descriptors for A and B (`a_desc`, `b_desc`), an index descriptor (`idesc`), an enable flag (`enable_input_d`), and a zero column mask descriptor (`zero_column_mask_desc`). The template parameter `Kind` specifies the data type (f16, tf32, f8f6f4, i8). Requires PTX ISA 86 and targets SM_100a/SM_101a architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_9\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.ws.cta_group.kind.collector::b0::lastuse [d_tmem], a_desc, b_desc, idesc, enable_input_d, zero_column_mask_desc; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1 }\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_collector_b0_lastuse(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d,\n  uint64_t zero_column_mask_desc);\n```\n\n----------------------------------------\n\nTITLE: Implementing tcgen05 MMA Block Scale with mxf4/mxf4nvf4 2X Vector Scaling and A Discard\nDESCRIPTION: Function declaration for tcgen05 matrix multiplication with templated Kind and Cta_Group parameters, using mxf4 or mxf4nvf4 data types with 2X vector scaling and discard strategy for matrix A. Requires CUDA PTX ISA 86 and SM_100a or SM_101a.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma.rst#2025-04-23_snippet_46\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.cta_group.kind.block_scale.scale_vec::2X.collector::a::discard [d_tmem], a_desc, b_desc, idesc, [scale_A_tmem], [scale_B_tmem], enable_input_d; // PTX ISA 86, SM_100a, SM_101a\n// .kind      = { .kind::mxf4, .kind::mxf4nvf4 }\n// .cta_group = { .cta_group::1, .cta_group::2 }\ntemplate <cuda::ptx::dot_kind Kind, cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_mma_block_scale_vec_2x_collector_a_discard(\n  cuda::ptx::kind_t<Kind> kind,\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  uint32_t scale_A_tmem,\n  uint32_t scale_B_tmem,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Converting Tasks Between Typed and Untyped\nDESCRIPTION: Demonstrates implicit conversion from typed to untyped tasks.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cudax/stf.rst#2025-04-23_snippet_47\n\nLANGUAGE: cpp\nCODE:\n```\nstream_task<> t = ctx.task(lX.read());\n```\n\n----------------------------------------\n\nTITLE: Declaring tcgen05_mma_block_scale_vec_1x_tmem_a_collector_a_discard for MXF8F6F4 kind\nDESCRIPTION: Template function declaration for 1X vector scaling operations with collector A discard. Specifically for MXF8F6F4 kind with support for CTA groups 1 and 2. Implements tensor compute operations with memory allocation for PTX ISA 86 and SM_100a/SM_101a architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma.rst#2025-04-23_snippet_49\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_mma_block_scale_vec_1x_tmem_a_collector_a_discard(\n  cuda::ptx::kind_mxf8f6f4_t,\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  uint32_t scale_A_tmem,\n  uint32_t scale_B_tmem,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Declaring tcgen05_mma_block_scale_vec_2x_collector_a_discard for MXF4/MXF4NVF4 kinds\nDESCRIPTION: Template function declaration for 2X vector scaling operations with collector A discard. Supports multiple dot kinds (MXF4, MXF4NVF4) and CTA groups (1, 2). Designed for PTX ISA 86 and SM_100a/SM_101a architectures.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma.rst#2025-04-23_snippet_47\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_kind Kind, cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_mma_block_scale_vec_2x_collector_a_discard(\n  cuda::ptx::kind_t<Kind> kind,\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  uint32_t scale_A_tmem,\n  uint32_t scale_B_tmem,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Minimum Operation for Signed 32-bit Integer\nDESCRIPTION: Template function implementing asynchronous minimum reduction operation on cluster memory barrier for s32 type. Takes destination pointer, value and remote barrier pointer as parameters.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/red_async.rst#2025-04-23_snippet_5\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename = void>\n__device__ static inline void red_async(\n  cuda::ptx::op_min_t,\n  int32_t* dest,\n  const int32_t& value,\n  uint64_t* remote_bar);\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication with 4X Vector Scaling for MXF4NVF4\nDESCRIPTION: Specialized template function for 4X vector scaling matrix multiplication operations. Exclusively supports mxf4nvf4 kind with configurable CTA groups. Takes memory descriptors and scaling parameters as inputs.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma.rst#2025-04-23_snippet_40\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_mma_block_scale_vec_4x_tmem_a_collector_a_use(\n  cuda::ptx::kind_mxf4nvf4_t,\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  uint32_t scale_A_tmem,\n  uint32_t scale_B_tmem,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: CUDA MMA Last Use Template\nDESCRIPTION: Final usage template for matrix multiplication that includes zero column masking and handles cleanup operations.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_21\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_collector_b1_lastuse(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d,\n  uint64_t zero_column_mask_desc);\n```\n\n----------------------------------------\n\nTITLE: Comparing Benchmark Results\nDESCRIPTION: Command to compare two benchmark result files using the NVBench comparison script\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/benchmarking.rst#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nPYTHONPATH=./_deps/nvbench-src/scripts ./_deps/nvbench-src/scripts/nvbench_compare.py base.json new.json\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication with 1X Vector Scaling for MXF8F6F4\nDESCRIPTION: Template function for matrix multiplication with 1X vector scaling, specifically for mxf8f6f4 operations. Includes last-use collector pattern and supports configurable CTA groups.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma.rst#2025-04-23_snippet_41\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <cuda::ptx::dot_cta_group Cta_Group>\n__device__ static inline void tcgen05_mma_block_scale_vec_1x_collector_a_lastuse(\n  cuda::ptx::kind_mxf8f6f4_t,\n  cuda::ptx::cta_group_t<Cta_Group> cta_group,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  uint32_t scale_A_tmem,\n  uint32_t scale_B_tmem,\n  bool enable_input_d);\n```\n\n----------------------------------------\n\nTITLE: Declaring tcgen05_mma_ws_tmem_a_collector_b2_discard with Zero Column Mask\nDESCRIPTION: Template function for MMA workload shaping using tmem buffer for matrix A with b2 collector discard mode. Includes zero column masking functionality through the zero_column_mask_desc parameter.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_57\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.ws.cta_group.kind.collector::b2::discard [d_tmem], [a_tmem], b_desc, idesc, enable_input_d, zero_column_mask_desc; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1 }\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_tmem_a_collector_b2_discard(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint32_t a_tmem,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d,\n  uint64_t zero_column_mask_desc);\n```\n\n----------------------------------------\n\nTITLE: PRMT B32 Forward 4-Byte Template Function\nDESCRIPTION: Template function declaration for 32-bit forward 4-byte permute operation (f4e mode). Operates on 32-bit inputs with forward byte ordering.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/prmt.rst#2025-04-23_snippet_1\n\nLANGUAGE: cuda\nCODE:\n```\ntemplate <typename B32, enable_if_t<sizeof(B32) == 4, bool> = true>\n__device__ static inline uint32_t prmt_f4e(\n  B32 a_reg,\n  B32 b_reg,\n  uint32_t c_reg);\n```\n\n----------------------------------------\n\nTITLE: Declaring tcgen05_mma_ws_collector_b3_lastuse with zero_column_mask_desc for CUDA\nDESCRIPTION: Template function declaration for tensor core operations using descriptor-based operands with zero column masking support. This variant handles f8f6f4 and i8 data types for CTA group 1 with b3 collector in last-use mode.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/ptx/instructions/generated/tcgen05_mma_ws.rst#2025-04-23_snippet_68\n\nLANGUAGE: cuda\nCODE:\n```\n// tcgen05.mma.ws.cta_group.kind.collector::b3::lastuse [d_tmem], a_desc, b_desc, idesc, enable_input_d, zero_column_mask_desc; // PTX ISA 86, SM_100a, SM_101a\n// .cta_group = { .cta_group::1 }\n// .kind      = { .kind::f16, .kind::tf32, .kind::f8f6f4, .kind::i8 }\ntemplate <cuda::ptx::dot_kind Kind>\n__device__ static inline void tcgen05_mma_ws_collector_b3_lastuse(\n  cuda::ptx::cta_group_1_t,\n  cuda::ptx::kind_t<Kind> kind,\n  uint32_t d_tmem,\n  uint64_t a_desc,\n  uint64_t b_desc,\n  uint32_t idesc,\n  bool enable_input_d,\n  uint64_t zero_column_mask_desc);\n```\n\n----------------------------------------\n\nTITLE: Filtering mdspan Headers for MSVC Compatibility in CMake\nDESCRIPTION: Excludes mdspan headers when using MSVC compiler without C++20 standard enabled, as mdspan requires C++20 on MSVC.\nSOURCE: https://github.com/nvidia/cccl/blob/main/libcudacxx/test/public_headers/CMakeLists.txt#2025-04-23_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nif (\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"MSVC\" AND NOT \"${CMAKE_CXX_STANDARD}\" MATCHES \"20\")\n  list(FILTER public_headers EXCLUDE REGEX \"mdspan\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: CUB Platform Detection Headers\nDESCRIPTION: Core configuration and platform detection header files in CUB library.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/cub/releases/changelog.rst#2025-04-23_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\n<cub/util_cpp_dialect.cuh>\n<cub/util_compiler.cuh>\n<cub/util_deprecated.cuh>\n<cub/config.cuh>\n```\n\n----------------------------------------\n\nTITLE: Introducing cuda::memcpy_async for Asynchronous Local Copies (C++)\nDESCRIPTION: libcu++ 1.1.0 introduces `cuda::memcpy_async` (available via `<cuda/barrier>`) as an extension for managing asynchronous local copies. This is intended for specific intra-thread or local memory operations, not for inter-thread or host-device transfers.\nSOURCE: https://github.com/nvidia/cccl/blob/main/docs/libcudacxx/releases/changelog.rst#2025-04-23_snippet_27\n\nLANGUAGE: cpp\nCODE:\n```\n#include <cuda/barrier> // Header providing cuda::memcpy_async\n```\n\nLANGUAGE: cpp\nCODE:\n```\ncuda::memcpy_async(/* ... */);\n```"
  }
]